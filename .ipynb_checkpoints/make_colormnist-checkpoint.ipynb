{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torchvision.datasets as dset\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "# from matplotlib import rcParams\n",
    "# rcParams.update({'figure.autolayout': True})\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as tforms\n",
    "\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import Dataset\n",
    "from IPython.core.debugger import Tracer\n",
    "\n",
    "from PIL import Image\n",
    "import os\n",
    "import os.path\n",
    "import errno\n",
    "import numpy as np\n",
    "import torch\n",
    "import codecs\n",
    "\n",
    "path_to_mnist = '/tancode/repos/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorMNIST(data.Dataset):\n",
    "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
    "\n",
    "    Args:\n",
    "        root (string): Root directory of dataset where ``processed/training.pt``\n",
    "            and  ``processed/test.pt`` exist.\n",
    "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
    "            otherwise from ``test.pt``.\n",
    "        download (bool, optional): If true, downloads the dataset from the internet and\n",
    "            puts it in root directory. If dataset is already downloaded, it is not\n",
    "            downloaded again.\n",
    "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
    "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
    "        target_transform (callable, optional): A function/transform that takes in the\n",
    "            target and transforms it.\n",
    "    \"\"\"\n",
    "    urls = [\n",
    "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
    "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
    "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
    "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
    "    ]\n",
    "    raw_folder = 'raw'\n",
    "    processed_folder = 'processed'\n",
    "    training_file = 'training.pt'\n",
    "    test_file = 'test.pt'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        self.train = train  # training set or test set\n",
    "\n",
    "        if download:\n",
    "            self.download()\n",
    "\n",
    "        if not self._check_exists():\n",
    "            raise RuntimeError('Dataset not found.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "        if self.train:\n",
    "            self.train_data, self.train_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
    "            \n",
    "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
    "        else:\n",
    "            self.test_data, self.test_labels = torch.load(\n",
    "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "            \n",
    "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
    "        \n",
    "        self.pallette = [[31, 119, 180],\n",
    "                         [255, 127, 14],\n",
    "                         [44, 160, 44],\n",
    "                         [214, 39, 40],\n",
    "                         [148, 103, 189],\n",
    "                         [140, 86, 75],\n",
    "                         [227, 119, 194],\n",
    "                         [127, 127, 127],\n",
    "                         [188, 189, 34],\n",
    "                         [23, 190, 207]]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Index\n",
    "\n",
    "        Returns:\n",
    "            tuple: (image, target) where target is index of the target class.\n",
    "        \"\"\"\n",
    "        if self.train:\n",
    "            img, target = self.train_data[index], self.train_labels[index]\n",
    "        else:\n",
    "            img, target = self.test_data[index], self.test_labels[index]\n",
    "        \n",
    "        # doing this so that it is consistent with all other datasets\n",
    "        # to return a PIL Image\n",
    "        y_color_digit = np.random.randint(0, 10)\n",
    "        c_digit = self.pallette[y_color_digit]\n",
    "        \n",
    "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
    "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
    "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
    "        \n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.train:\n",
    "            return len(self.train_data)\n",
    "        else:\n",
    "            return len(self.test_data)\n",
    "\n",
    "    def _check_exists(self):\n",
    "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
    "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
    "\n",
    "    def download(self):\n",
    "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
    "        from six.moves import urllib\n",
    "        import gzip\n",
    "\n",
    "        if self._check_exists():\n",
    "            return\n",
    "\n",
    "        # download files\n",
    "        try:\n",
    "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
    "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST:\n",
    "                pass\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "        for url in self.urls:\n",
    "            print('Downloading ' + url)\n",
    "            data = urllib.request.urlopen(url)\n",
    "            filename = url.rpartition('/')[2]\n",
    "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
    "            with open(file_path, 'wb') as f:\n",
    "                f.write(data.read())\n",
    "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
    "                    gzip.GzipFile(file_path) as zip_f:\n",
    "                out_f.write(zip_f.read())\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        # process and save as torch files\n",
    "        print('Processing...')\n",
    "\n",
    "        training_set = (\n",
    "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
    "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
    "        )\n",
    "        test_set = (\n",
    "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
    "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
    "        )\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
    "            torch.save(training_set, f)\n",
    "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
    "            torch.save(test_set, f)\n",
    "\n",
    "        print('Done!')\n",
    "\n",
    "    def __repr__(self):\n",
    "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
    "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
    "        tmp = 'train' if self.train is True else 'test'\n",
    "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
    "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
    "        tmp = '    Transforms (if any): '\n",
    "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        tmp = '    Target Transforms (if any): '\n",
    "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
    "        return fmt_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(x):\n",
    "    \"\"\"\n",
    "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
    "    \"\"\"\n",
    "    noise = x.new().resize_as_(x).uniform_()\n",
    "    x = x * 255 + noise\n",
    "    x = x / 256\n",
    "    return x\n",
    "\n",
    "def get_train_loader(train_set, epoch):\n",
    "    if args.batch_size_schedule != \"\":\n",
    "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
    "        n_passed = sum(np.array(epochs) <= epoch)\n",
    "        current_batch_size = int(args.batch_size * n_passed)\n",
    "    else:\n",
    "        current_batch_size = args.batch_size\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
    "    )\n",
    "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
    "    return train_loader\n",
    "\n",
    "trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
    "im_dim = 1\n",
    "im_size = 28\n",
    "\n",
    "train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
    "test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
    "data_shape = (im_dim, im_size, im_size)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_set, batch_size=500, shuffle=False, drop_last=True)\n",
    "\n",
    "for epoch in range(1, 11):\n",
    "    train_loader = torch.utils.data.DataLoader(dataset=train_set, batch_size=10, shuffle=True, drop_last=True, pin_memory=True)\n",
    "    for _, (x, y) in enumerate(train_loader):\n",
    "        ximg = x.numpy().transpose([1, 2, 0])\n",
    "        plt.imshow(ximg)\n",
    "        plt.show()\n",
    "        Tracer()()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ColorMNIST('back', 'train', path_to_mnist, randomcolor=False)\n",
    "\n",
    "x_all = []\n",
    "for i in [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]:\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "x_all = np.hstack(x_all)\n",
    "\n",
    "plt.imshow(x_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ColorMNIST('num', 'train', path_to_mnist, randomcolor=False)\n",
    "\n",
    "x_all = []\n",
    "for i in [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]:\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "x_all = np.hstack(x_all)\n",
    "\n",
    "plt.imshow(x_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ColorMNIST('both', 'train', path_to_mnist, randomcolor=False)\n",
    "\n",
    "x_all = []\n",
    "for i in [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]:\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "x_all = np.hstack(x_all)\n",
    "\n",
    "plt.imshow(x_all)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = ColorMNIST('both', 'train', path_to_mnist, randomcolor=True)\n",
    "\n",
    "x_all = []\n",
    "for i in [1, 3, 5, 7, 2, 0, 13, 15, 17, 4]:\n",
    "    x_all.append(dataloader[i][0].numpy().transpose([1, 2, 0]))\n",
    "x_all = np.hstack(x_all)\n",
    "\n",
    "plt.imshow(x_all)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
