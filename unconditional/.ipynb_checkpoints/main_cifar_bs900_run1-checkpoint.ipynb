{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=False, controlled_tol=False, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_published_baseline_bs900_1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 7.3297(16.2122) | Bit/dim 17.8241(19.1159) | Xent 0.0000(0.0000) | Loss 17.8241(19.1159) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 127.7706(142.2465) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 7.4826(13.8974) | Bit/dim 14.6661(18.3023) | Xent 0.0000(0.0000) | Loss 14.6661(18.3023) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 86.5604(132.5856) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 7.7449(12.2742) | Bit/dim 12.0965(16.9360) | Xent 0.0000(0.0000) | Loss 12.0965(16.9360) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 43.2771(113.6154) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 7.5779(11.0481) | Bit/dim 10.1691(15.3759) | Xent 0.0000(0.0000) | Loss 10.1691(15.3759) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 21.2672(91.3649) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 7.6463(10.1330) | Bit/dim 8.6873(13.7861) | Xent 0.0000(0.0000) | Loss 8.6873(13.7861) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 14.4938(71.6291) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 7.6519(9.4664) | Bit/dim 7.4117(12.2689) | Xent 0.0000(0.0000) | Loss 7.4117(12.2689) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 15.0524(56.7452) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 40.2560, Epoch Time 564.7481(564.7481), Bit/dim 6.5140(best: inf), Xent 0.0000, Loss 6.5140, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 7.6884(8.9899) | Bit/dim 6.0581(10.7915) | Xent 0.0000(0.0000) | Loss 6.0581(10.7915) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 12.0403(45.3365) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 7.6204(8.6128) | Bit/dim 4.8616(9.3655) | Xent 0.0000(0.0000) | Loss 4.8616(9.3655) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 10.6821(36.3649) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 7.6217(8.3244) | Bit/dim 3.8491(8.0207) | Xent 0.0000(0.0000) | Loss 3.8491(8.0207) | Error 0.0000(0.0000) Steps 416(410.35) | Grad Norm 8.7924(29.3723) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 8.1516(8.2645) | Bit/dim 3.1326(6.8106) | Xent 0.0000(0.0000) | Loss 3.1326(6.8106) | Error 0.0000(0.0000) Steps 422(413.13) | Grad Norm 5.6859(23.4874) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 8.0515(8.2247) | Bit/dim 2.7730(5.7867) | Xent 0.0000(0.0000) | Loss 2.7730(5.7867) | Error 0.0000(0.0000) Steps 428(415.99) | Grad Norm 3.7788(18.5042) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 8.7922(8.3322) | Bit/dim 2.5318(4.9557) | Xent 0.0000(0.0000) | Loss 2.5318(4.9557) | Error 0.0000(0.0000) Steps 440(421.59) | Grad Norm 2.3998(14.4160) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 9.1077(8.4428) | Bit/dim 2.3957(4.2996) | Xent 0.0000(0.0000) | Loss 2.3957(4.2996) | Error 0.0000(0.0000) Steps 440(426.42) | Grad Norm 1.6654(11.1438) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 40.1784, Epoch Time 590.1271(565.5095), Bit/dim 2.3763(best: 6.5140), Xent 0.0000, Loss 2.3763, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.8899(8.5602) | Bit/dim 2.3470(3.7908) | Xent 0.0000(0.0000) | Loss 2.3470(3.7908) | Error 0.0000(0.0000) Steps 446(431.13) | Grad Norm 1.2447(8.5923) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 8.8302(8.6463) | Bit/dim 2.2773(3.3994) | Xent 0.0000(0.0000) | Loss 2.2773(3.3994) | Error 0.0000(0.0000) Steps 446(435.04) | Grad Norm 0.9164(6.6064) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 8.7310(8.6926) | Bit/dim 2.2456(3.1019) | Xent 0.0000(0.0000) | Loss 2.2456(3.1019) | Error 0.0000(0.0000) Steps 440(437.40) | Grad Norm 0.7228(5.0823) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 8.6880(8.6929) | Bit/dim 2.2363(2.8777) | Xent 0.0000(0.0000) | Loss 2.2363(2.8777) | Error 0.0000(0.0000) Steps 440(438.08) | Grad Norm 0.6348(3.9220) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 8.6943(8.7079) | Bit/dim 2.2209(2.7062) | Xent 0.0000(0.0000) | Loss 2.2209(2.7062) | Error 0.0000(0.0000) Steps 440(438.58) | Grad Norm 0.5696(3.0465) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 8.5503(8.7275) | Bit/dim 2.1962(2.5754) | Xent 0.0000(0.0000) | Loss 2.1962(2.5754) | Error 0.0000(0.0000) Steps 440(438.96) | Grad Norm 0.5644(2.3953) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 39.4547, Epoch Time 635.4251(567.6070), Bit/dim 2.1839(best: 2.3763), Xent 0.0000, Loss 2.1839, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 8.7321(8.7216) | Bit/dim 2.1857(2.4763) | Xent 0.0000(0.0000) | Loss 2.1857(2.4763) | Error 0.0000(0.0000) Steps 440(439.23) | Grad Norm 0.5021(1.9040) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 8.5464(8.7102) | Bit/dim 2.1725(2.3986) | Xent 0.0000(0.0000) | Loss 2.1725(2.3986) | Error 0.0000(0.0000) Steps 440(439.43) | Grad Norm 0.4825(1.5383) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 9.0647(8.7101) | Bit/dim 2.1556(2.3378) | Xent 0.0000(0.0000) | Loss 2.1556(2.3378) | Error 0.0000(0.0000) Steps 440(439.58) | Grad Norm 0.4651(1.2573) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 8.8665(8.7114) | Bit/dim 2.1353(2.2881) | Xent 0.0000(0.0000) | Loss 2.1353(2.2881) | Error 0.0000(0.0000) Steps 446(439.87) | Grad Norm 0.4393(1.0441) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 8.6854(8.7523) | Bit/dim 2.1410(2.2476) | Xent 0.0000(0.0000) | Loss 2.1410(2.2476) | Error 0.0000(0.0000) Steps 440(440.98) | Grad Norm 0.4261(0.8859) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 8.5036(8.7191) | Bit/dim 2.1137(2.2143) | Xent 0.0000(0.0000) | Loss 2.1137(2.2143) | Error 0.0000(0.0000) Steps 440(441.03) | Grad Norm 0.4280(0.7694) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 8.3210(8.6818) | Bit/dim 2.0991(2.1854) | Xent 0.0000(0.0000) | Loss 2.0991(2.1854) | Error 0.0000(0.0000) Steps 440(440.76) | Grad Norm 0.3993(0.6752) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 39.3132, Epoch Time 627.7560(569.4115), Bit/dim 2.0876(best: 2.1839), Xent 0.0000, Loss 2.0876, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.2690(8.6024) | Bit/dim 2.0791(2.1593) | Xent 0.0000(0.0000) | Loss 2.0791(2.1593) | Error 0.0000(0.0000) Steps 428(438.28) | Grad Norm 0.3854(0.6036) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 8.2505(8.5410) | Bit/dim 2.0762(2.1377) | Xent 0.0000(0.0000) | Loss 2.0762(2.1377) | Error 0.0000(0.0000) Steps 428(435.88) | Grad Norm 0.3763(0.5457) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 8.1843(8.4710) | Bit/dim 2.0537(2.1175) | Xent 0.0000(0.0000) | Loss 2.0537(2.1175) | Error 0.0000(0.0000) Steps 428(433.81) | Grad Norm 0.3538(0.5000) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 8.5243(8.4724) | Bit/dim 2.0322(2.0979) | Xent 0.0000(0.0000) | Loss 2.0322(2.0979) | Error 0.0000(0.0000) Steps 434(433.14) | Grad Norm 0.3602(0.4632) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 8.7935(8.5966) | Bit/dim 2.0171(2.0789) | Xent 0.0000(0.0000) | Loss 2.0171(2.0789) | Error 0.0000(0.0000) Steps 440(434.80) | Grad Norm 0.3624(0.4361) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 9.0416(8.7218) | Bit/dim 2.0241(2.0614) | Xent 0.0000(0.0000) | Loss 2.0241(2.0614) | Error 0.0000(0.0000) Steps 440(436.50) | Grad Norm 0.4073(0.4167) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 9.0951(8.8225) | Bit/dim 2.0103(2.0463) | Xent 0.0000(0.0000) | Loss 2.0103(2.0463) | Error 0.0000(0.0000) Steps 452(439.33) | Grad Norm 0.3545(0.4038) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 39.0010, Epoch Time 625.8684(571.1052), Bit/dim 1.9859(best: 2.0876), Xent 0.0000, Loss 1.9859, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 8.7559(8.8741) | Bit/dim 1.9864(2.0309) | Xent 0.0000(0.0000) | Loss 1.9864(2.0309) | Error 0.0000(0.0000) Steps 440(440.42) | Grad Norm 0.3756(0.3948) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 9.0992(8.9777) | Bit/dim 1.9602(2.0144) | Xent 0.0000(0.0000) | Loss 1.9602(2.0144) | Error 0.0000(0.0000) Steps 458(445.04) | Grad Norm 0.3441(0.3932) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 9.3107(9.0612) | Bit/dim 1.9582(1.9990) | Xent 0.0000(0.0000) | Loss 1.9582(1.9990) | Error 0.0000(0.0000) Steps 458(448.44) | Grad Norm 0.3076(0.3792) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 9.4904(9.1120) | Bit/dim 1.9282(1.9832) | Xent 0.0000(0.0000) | Loss 1.9282(1.9832) | Error 0.0000(0.0000) Steps 458(450.95) | Grad Norm 0.3754(0.3712) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 9.5199(9.1864) | Bit/dim 1.9278(1.9691) | Xent 0.0000(0.0000) | Loss 1.9278(1.9691) | Error 0.0000(0.0000) Steps 464(453.33) | Grad Norm 0.5677(0.3803) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 9.5107(9.3114) | Bit/dim 1.9170(1.9547) | Xent 0.0000(0.0000) | Loss 1.9170(1.9547) | Error 0.0000(0.0000) Steps 470(456.31) | Grad Norm 0.3619(0.3808) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 41.5316, Epoch Time 674.2943(574.2008), Bit/dim 1.9026(best: 1.9859), Xent 0.0000, Loss 1.9026, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.8533(9.4364) | Bit/dim 1.8951(1.9419) | Xent 0.0000(0.0000) | Loss 1.8951(1.9419) | Error 0.0000(0.0000) Steps 470(459.90) | Grad Norm 0.3287(0.3541) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 9.5438(9.5079) | Bit/dim 1.9094(1.9312) | Xent 0.0000(0.0000) | Loss 1.9094(1.9312) | Error 0.0000(0.0000) Steps 470(462.55) | Grad Norm 0.2424(0.3385) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 9.5666(9.5744) | Bit/dim 1.8850(1.9217) | Xent 0.0000(0.0000) | Loss 1.8850(1.9217) | Error 0.0000(0.0000) Steps 470(464.51) | Grad Norm 0.3126(0.3226) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 10.5490(9.6780) | Bit/dim 1.8823(1.9119) | Xent 0.0000(0.0000) | Loss 1.8823(1.9119) | Error 0.0000(0.0000) Steps 476(466.48) | Grad Norm 0.4436(0.3367) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 10.1768(9.8049) | Bit/dim 1.8711(1.9007) | Xent 0.0000(0.0000) | Loss 1.8711(1.9007) | Error 0.0000(0.0000) Steps 482(469.98) | Grad Norm 0.4140(0.3499) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 10.0990(9.9063) | Bit/dim 1.8571(1.8910) | Xent 0.0000(0.0000) | Loss 1.8571(1.8910) | Error 0.0000(0.0000) Steps 482(473.13) | Grad Norm 0.3186(0.3588) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 10.5836(9.9983) | Bit/dim 1.8415(1.8804) | Xent 0.0000(0.0000) | Loss 1.8415(1.8804) | Error 0.0000(0.0000) Steps 482(475.46) | Grad Norm 0.5718(0.3666) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 42.8515, Epoch Time 718.0039(578.5149), Bit/dim 1.8389(best: 1.9026), Xent 0.0000, Loss 1.8389, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 10.0969(10.0729) | Bit/dim 1.8383(1.8691) | Xent 0.0000(0.0000) | Loss 1.8383(1.8691) | Error 0.0000(0.0000) Steps 482(477.18) | Grad Norm 0.4159(0.3983) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 10.0229(10.1021) | Bit/dim 1.8011(1.8555) | Xent 0.0000(0.0000) | Loss 1.8011(1.8555) | Error 0.0000(0.0000) Steps 482(478.45) | Grad Norm 0.5048(0.4231) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 10.4814(10.1673) | Bit/dim 1.7930(1.8395) | Xent 0.0000(0.0000) | Loss 1.7930(1.8395) | Error 0.0000(0.0000) Steps 482(479.38) | Grad Norm 0.3902(0.4387) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 10.1579(10.1713) | Bit/dim 1.7419(1.8178) | Xent 0.0000(0.0000) | Loss 1.7419(1.8178) | Error 0.0000(0.0000) Steps 482(480.07) | Grad Norm 0.3423(0.4255) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 10.6688(10.2036) | Bit/dim 1.7041(1.7921) | Xent 0.0000(0.0000) | Loss 1.7041(1.7921) | Error 0.0000(0.0000) Steps 488(480.93) | Grad Norm 0.6088(0.4383) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 10.5180(10.3005) | Bit/dim 1.6665(1.7629) | Xent 0.0000(0.0000) | Loss 1.6665(1.7629) | Error 0.0000(0.0000) Steps 488(482.34) | Grad Norm 1.2406(0.4998) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 43.3230, Epoch Time 740.2859(583.3681), Bit/dim 1.6404(best: 1.8389), Xent 0.0000, Loss 1.6404, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 10.5095(10.3406) | Bit/dim 1.6445(1.7339) | Xent 0.0000(0.0000) | Loss 1.6445(1.7339) | Error 0.0000(0.0000) Steps 488(483.36) | Grad Norm 4.5308(1.0644) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 10.1767(10.3519) | Bit/dim 1.6128(1.7059) | Xent 0.0000(0.0000) | Loss 1.6128(1.7059) | Error 0.0000(0.0000) Steps 482(483.42) | Grad Norm 4.2720(1.8124) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 9.9017(10.2826) | Bit/dim 1.5853(1.6783) | Xent 0.0000(0.0000) | Loss 1.5853(1.6783) | Error 0.0000(0.0000) Steps 482(482.93) | Grad Norm 0.3133(2.4787) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 10.3879(10.2654) | Bit/dim 1.5875(1.6540) | Xent 0.0000(0.0000) | Loss 1.5875(1.6540) | Error 0.0000(0.0000) Steps 488(483.48) | Grad Norm 6.2060(2.9221) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 10.3062(10.2886) | Bit/dim 1.5702(1.6320) | Xent 0.0000(0.0000) | Loss 1.5702(1.6320) | Error 0.0000(0.0000) Steps 488(484.66) | Grad Norm 7.9435(3.0831) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 10.3413(10.2964) | Bit/dim 1.5412(1.6141) | Xent 0.0000(0.0000) | Loss 1.5412(1.6141) | Error 0.0000(0.0000) Steps 488(485.40) | Grad Norm 3.4247(4.5421) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 10.0193(10.2852) | Bit/dim 1.5536(1.5972) | Xent 0.0000(0.0000) | Loss 1.5536(1.5972) | Error 0.0000(0.0000) Steps 482(485.57) | Grad Norm 7.5591(4.8347) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 43.1807, Epoch Time 736.7586(587.9698), Bit/dim 1.5327(best: 1.6404), Xent 0.0000, Loss 1.5327, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 10.3524(10.2990) | Bit/dim 1.5236(1.5800) | Xent 0.0000(0.0000) | Loss 1.5236(1.5800) | Error 0.0000(0.0000) Steps 488(485.31) | Grad Norm 1.2289(4.7674) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 10.4130(10.3136) | Bit/dim 1.5036(1.5673) | Xent 0.0000(0.0000) | Loss 1.5036(1.5673) | Error 0.0000(0.0000) Steps 488(486.02) | Grad Norm 0.8356(5.7686) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 10.2332(10.3269) | Bit/dim 1.5047(1.5540) | Xent 0.0000(0.0000) | Loss 1.5047(1.5540) | Error 0.0000(0.0000) Steps 488(486.71) | Grad Norm 7.3420(6.2486) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 10.6019(10.3433) | Bit/dim 1.4906(1.5406) | Xent 0.0000(0.0000) | Loss 1.4906(1.5406) | Error 0.0000(0.0000) Steps 494(488.16) | Grad Norm 7.1113(6.6863) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 10.4702(10.3883) | Bit/dim 1.4937(1.5293) | Xent 0.0000(0.0000) | Loss 1.4937(1.5293) | Error 0.0000(0.0000) Steps 494(489.70) | Grad Norm 4.1039(7.3428) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 10.4808(10.4497) | Bit/dim 1.4827(1.5172) | Xent 0.0000(0.0000) | Loss 1.4827(1.5172) | Error 0.0000(0.0000) Steps 494(490.83) | Grad Norm 5.5843(7.4726) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 10.4823(10.4775) | Bit/dim 1.4676(1.5085) | Xent 0.0000(0.0000) | Loss 1.4676(1.5085) | Error 0.0000(0.0000) Steps 494(492.14) | Grad Norm 2.8254(7.4604) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 45.5479, Epoch Time 750.9800(592.8601), Bit/dim 1.4659(best: 1.5327), Xent 0.0000, Loss 1.4659, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 10.7048(10.5139) | Bit/dim 1.4831(1.5009) | Xent 0.0000(0.0000) | Loss 1.4831(1.5009) | Error 0.0000(0.0000) Steps 500(493.76) | Grad Norm 15.9847(8.7093) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 10.3390(10.5515) | Bit/dim 1.4471(1.4911) | Xent 0.0000(0.0000) | Loss 1.4471(1.4911) | Error 0.0000(0.0000) Steps 500(495.40) | Grad Norm 7.9515(8.3543) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 10.5295(10.6198) | Bit/dim 1.4651(1.4824) | Xent 0.0000(0.0000) | Loss 1.4651(1.4824) | Error 0.0000(0.0000) Steps 500(496.95) | Grad Norm 16.0624(8.9489) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 11.0215(10.6544) | Bit/dim 1.4509(1.4756) | Xent 0.0000(0.0000) | Loss 1.4509(1.4756) | Error 0.0000(0.0000) Steps 512(498.89) | Grad Norm 12.4555(9.6322) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 10.6003(10.7065) | Bit/dim 1.4320(1.4667) | Xent 0.0000(0.0000) | Loss 1.4320(1.4667) | Error 0.0000(0.0000) Steps 500(500.17) | Grad Norm 7.9035(9.3830) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 10.3991(10.6774) | Bit/dim 1.4491(1.4652) | Xent 0.0000(0.0000) | Loss 1.4491(1.4652) | Error 0.0000(0.0000) Steps 500(500.87) | Grad Norm 8.2724(10.2599) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 46.4300, Epoch Time 768.7566(598.1370), Bit/dim 1.4284(best: 1.4659), Xent 0.0000, Loss 1.4284, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 10.7632(10.7031) | Bit/dim 1.4373(1.4607) | Xent 0.0000(0.0000) | Loss 1.4373(1.4607) | Error 0.0000(0.0000) Steps 500(501.72) | Grad Norm 9.6424(10.7889) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 11.0709(10.7539) | Bit/dim 1.4317(1.4529) | Xent 0.0000(0.0000) | Loss 1.4317(1.4529) | Error 0.0000(0.0000) Steps 506(502.71) | Grad Norm 0.8179(9.8625) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 10.9002(10.8343) | Bit/dim 1.4234(1.4448) | Xent 0.0000(0.0000) | Loss 1.4234(1.4448) | Error 0.0000(0.0000) Steps 500(503.58) | Grad Norm 11.5055(9.5548) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 11.0449(10.8332) | Bit/dim 1.4081(1.4414) | Xent 0.0000(0.0000) | Loss 1.4081(1.4414) | Error 0.0000(0.0000) Steps 512(503.93) | Grad Norm 10.5601(10.9001) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 11.8266(10.8898) | Bit/dim 1.4327(1.4364) | Xent 0.0000(0.0000) | Loss 1.4327(1.4364) | Error 0.0000(0.0000) Steps 512(505.08) | Grad Norm 12.8795(10.4478) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 11.2714(10.9006) | Bit/dim 1.3996(1.4320) | Xent 0.0000(0.0000) | Loss 1.3996(1.4320) | Error 0.0000(0.0000) Steps 512(505.19) | Grad Norm 7.2190(10.6811) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 10.6473(10.9012) | Bit/dim 1.4392(1.4300) | Xent 0.0000(0.0000) | Loss 1.4392(1.4300) | Error 0.0000(0.0000) Steps 494(505.70) | Grad Norm 18.0433(11.4475) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 47.6517, Epoch Time 784.8154(603.7373), Bit/dim 1.3947(best: 1.4284), Xent 0.0000, Loss 1.3947, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 11.2979(10.9589) | Bit/dim 1.4067(1.4243) | Xent 0.0000(0.0000) | Loss 1.4067(1.4243) | Error 0.0000(0.0000) Steps 518(508.02) | Grad Norm 7.8339(10.2775) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 10.9043(10.9785) | Bit/dim 1.4153(1.4204) | Xent 0.0000(0.0000) | Loss 1.4153(1.4204) | Error 0.0000(0.0000) Steps 494(508.41) | Grad Norm 18.9104(10.7845) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 10.9582(10.9520) | Bit/dim 1.4211(1.4195) | Xent 0.0000(0.0000) | Loss 1.4211(1.4195) | Error 0.0000(0.0000) Steps 512(508.36) | Grad Norm 10.2267(12.0323) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 10.8594(10.9583) | Bit/dim 1.3868(1.4128) | Xent 0.0000(0.0000) | Loss 1.3868(1.4128) | Error 0.0000(0.0000) Steps 512(509.23) | Grad Norm 7.5191(11.5344) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 11.5463(11.0347) | Bit/dim 1.3951(1.4070) | Xent 0.0000(0.0000) | Loss 1.3951(1.4070) | Error 0.0000(0.0000) Steps 518(510.88) | Grad Norm 8.8351(11.1863) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 10.5574(10.9944) | Bit/dim 1.4232(1.4052) | Xent 0.0000(0.0000) | Loss 1.4232(1.4052) | Error 0.0000(0.0000) Steps 500(510.60) | Grad Norm 19.7190(12.2225) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 47.0343, Epoch Time 789.6505(609.3147), Bit/dim 1.3938(best: 1.3947), Xent 0.0000, Loss 1.3938, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 10.7088(11.0033) | Bit/dim 1.4119(1.4021) | Xent 0.0000(0.0000) | Loss 1.4119(1.4021) | Error 0.0000(0.0000) Steps 506(510.51) | Grad Norm 16.1614(12.6083) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 11.3021(11.0473) | Bit/dim 1.3714(1.3947) | Xent 0.0000(0.0000) | Loss 1.3714(1.3947) | Error 0.0000(0.0000) Steps 518(512.18) | Grad Norm 5.0027(11.3248) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 11.2623(11.0445) | Bit/dim 1.3774(1.3909) | Xent 0.0000(0.0000) | Loss 1.3774(1.3909) | Error 0.0000(0.0000) Steps 512(512.49) | Grad Norm 18.7976(11.9737) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 11.0560(11.0448) | Bit/dim 1.3881(1.3907) | Xent 0.0000(0.0000) | Loss 1.3881(1.3907) | Error 0.0000(0.0000) Steps 512(512.10) | Grad Norm 15.0592(12.9874) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 11.2466(11.0860) | Bit/dim 1.3688(1.3860) | Xent 0.0000(0.0000) | Loss 1.3688(1.3860) | Error 0.0000(0.0000) Steps 518(512.91) | Grad Norm 14.2783(12.4119) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 10.8166(11.0993) | Bit/dim 1.3616(1.3800) | Xent 0.0000(0.0000) | Loss 1.3616(1.3800) | Error 0.0000(0.0000) Steps 512(513.79) | Grad Norm 17.6825(11.9430) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 11.2010(11.0921) | Bit/dim 1.3747(1.3776) | Xent 0.0000(0.0000) | Loss 1.3747(1.3776) | Error 0.0000(0.0000) Steps 518(513.89) | Grad Norm 2.1433(12.4328) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 48.6400, Epoch Time 795.8509(614.9108), Bit/dim 1.3487(best: 1.3938), Xent 0.0000, Loss 1.3487, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 10.8441(11.0850) | Bit/dim 1.3864(1.3780) | Xent 0.0000(0.0000) | Loss 1.3864(1.3780) | Error 0.0000(0.0000) Steps 512(514.99) | Grad Norm 20.0517(13.5451) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 11.2417(11.1211) | Bit/dim 1.3528(1.3723) | Xent 0.0000(0.0000) | Loss 1.3528(1.3723) | Error 0.0000(0.0000) Steps 518(516.07) | Grad Norm 14.2537(12.7591) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 11.2582(11.1507) | Bit/dim 1.3226(1.3643) | Xent 0.0000(0.0000) | Loss 1.3226(1.3643) | Error 0.0000(0.0000) Steps 518(516.57) | Grad Norm 6.8969(11.8216) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 11.2024(11.1373) | Bit/dim 1.3626(1.3647) | Xent 0.0000(0.0000) | Loss 1.3626(1.3647) | Error 0.0000(0.0000) Steps 524(516.91) | Grad Norm 16.8806(13.1017) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 11.4293(11.1547) | Bit/dim 1.3116(1.3615) | Xent 0.0000(0.0000) | Loss 1.3116(1.3615) | Error 0.0000(0.0000) Steps 524(518.00) | Grad Norm 5.7932(13.1374) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 11.7289(11.1973) | Bit/dim 1.3245(1.3569) | Xent 0.0000(0.0000) | Loss 1.3245(1.3569) | Error 0.0000(0.0000) Steps 524(518.78) | Grad Norm 5.7492(13.1865) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 11.8788(11.2309) | Bit/dim 1.3489(1.3508) | Xent 0.0000(0.0000) | Loss 1.3489(1.3508) | Error 0.0000(0.0000) Steps 524(520.15) | Grad Norm 14.2214(12.4717) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 48.7560, Epoch Time 804.2709(620.5916), Bit/dim 1.3308(best: 1.3487), Xent 0.0000, Loss 1.3308, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 11.3620(11.2586) | Bit/dim 1.3512(1.3472) | Xent 0.0000(0.0000) | Loss 1.3512(1.3472) | Error 0.0000(0.0000) Steps 524(520.99) | Grad Norm 18.2127(12.4626) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 10.6341(11.2634) | Bit/dim 1.4272(1.3507) | Xent 0.0000(0.0000) | Loss 1.4272(1.3507) | Error 0.0000(0.0000) Steps 500(521.07) | Grad Norm 31.8107(13.6836) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 11.6843(11.2755) | Bit/dim 1.3170(1.3474) | Xent 0.0000(0.0000) | Loss 1.3170(1.3474) | Error 0.0000(0.0000) Steps 524(521.35) | Grad Norm 8.1198(13.4960) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 11.1988(11.2846) | Bit/dim 1.3307(1.3421) | Xent 0.0000(0.0000) | Loss 1.3307(1.3421) | Error 0.0000(0.0000) Steps 524(521.88) | Grad Norm 12.7534(12.7280) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 11.2983(11.2930) | Bit/dim 1.3027(1.3356) | Xent 0.0000(0.0000) | Loss 1.3027(1.3356) | Error 0.0000(0.0000) Steps 524(522.44) | Grad Norm 2.4388(11.3776) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 11.5463(11.2964) | Bit/dim 1.3128(1.3305) | Xent 0.0000(0.0000) | Loss 1.3128(1.3305) | Error 0.0000(0.0000) Steps 524(522.85) | Grad Norm 12.3033(10.9066) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 48.1557, Epoch Time 809.8578(626.2696), Bit/dim 1.3193(best: 1.3308), Xent 0.0000, Loss 1.3193, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 11.3238(11.2991) | Bit/dim 1.3298(1.3281) | Xent 0.0000(0.0000) | Loss 1.3298(1.3281) | Error 0.0000(0.0000) Steps 524(523.15) | Grad Norm 1.5638(11.2959) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 11.2287(11.3153) | Bit/dim 1.3392(1.3266) | Xent 0.0000(0.0000) | Loss 1.3392(1.3266) | Error 0.0000(0.0000) Steps 524(523.37) | Grad Norm 16.7776(11.9831) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 11.1497(11.2923) | Bit/dim 1.2971(1.3335) | Xent 0.0000(0.0000) | Loss 1.2971(1.3335) | Error 0.0000(0.0000) Steps 524(523.64) | Grad Norm 5.1391(14.1266) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 11.2921(11.3686) | Bit/dim 1.3290(1.3380) | Xent 0.0000(0.0000) | Loss 1.3290(1.3380) | Error 0.0000(0.0000) Steps 524(524.82) | Grad Norm 13.7306(15.1016) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 11.2121(11.3382) | Bit/dim 1.3145(1.3332) | Xent 0.0000(0.0000) | Loss 1.3145(1.3332) | Error 0.0000(0.0000) Steps 524(524.46) | Grad Norm 12.1920(13.7003) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 11.2866(11.3136) | Bit/dim 1.3069(1.3264) | Xent 0.0000(0.0000) | Loss 1.3069(1.3264) | Error 0.0000(0.0000) Steps 524(524.34) | Grad Norm 2.3445(11.3263) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 11.4856(11.3208) | Bit/dim 1.2928(1.3183) | Xent 0.0000(0.0000) | Loss 1.2928(1.3183) | Error 0.0000(0.0000) Steps 524(524.25) | Grad Norm 0.8938(8.8318) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 47.8516, Epoch Time 811.4149(631.8240), Bit/dim 1.2912(best: 1.3193), Xent 0.0000, Loss 1.2912, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 11.2742(11.3256) | Bit/dim 1.2953(1.3110) | Xent 0.0000(0.0000) | Loss 1.2953(1.3110) | Error 0.0000(0.0000) Steps 524(524.19) | Grad Norm 1.1501(6.8695) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 11.0521(11.3450) | Bit/dim 1.3526(1.3098) | Xent 0.0000(0.0000) | Loss 1.3526(1.3098) | Error 0.0000(0.0000) Steps 518(523.96) | Grad Norm 29.5902(7.4648) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 12.5832(11.4468) | Bit/dim 1.4377(1.3271) | Xent 0.0000(0.0000) | Loss 1.4377(1.3271) | Error 0.0000(0.0000) Steps 554(526.64) | Grad Norm 35.1574(11.8936) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 12.0631(11.4561) | Bit/dim 1.3445(1.3282) | Xent 0.0000(0.0000) | Loss 1.3445(1.3282) | Error 0.0000(0.0000) Steps 542(527.24) | Grad Norm 20.5146(12.9382) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 11.4321(11.4163) | Bit/dim 1.3072(1.3230) | Xent 0.0000(0.0000) | Loss 1.3072(1.3230) | Error 0.0000(0.0000) Steps 524(525.92) | Grad Norm 10.8195(11.8704) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 11.4722(11.4017) | Bit/dim 1.2883(1.3151) | Xent 0.0000(0.0000) | Loss 1.2883(1.3151) | Error 0.0000(0.0000) Steps 524(525.42) | Grad Norm 8.7872(10.7909) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 48.9195, Epoch Time 817.6765(637.3995), Bit/dim 1.2886(best: 1.2912), Xent 0.0000, Loss 1.2886, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 11.2039(11.3903) | Bit/dim 1.2884(1.3085) | Xent 0.0000(0.0000) | Loss 1.2884(1.3085) | Error 0.0000(0.0000) Steps 524(525.05) | Grad Norm 7.2562(9.7693) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 11.2986(11.3835) | Bit/dim 1.2841(1.3027) | Xent 0.0000(0.0000) | Loss 1.2841(1.3027) | Error 0.0000(0.0000) Steps 524(524.77) | Grad Norm 1.7244(8.5360) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 11.3400(11.3806) | Bit/dim 1.2762(1.2973) | Xent 0.0000(0.0000) | Loss 1.2762(1.2973) | Error 0.0000(0.0000) Steps 524(524.57) | Grad Norm 5.6932(7.4114) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 11.2513(11.3615) | Bit/dim 1.2784(1.2932) | Xent 0.0000(0.0000) | Loss 1.2784(1.2932) | Error 0.0000(0.0000) Steps 524(524.42) | Grad Norm 12.9704(8.5674) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 11.1852(11.3577) | Bit/dim 1.2779(1.2887) | Xent 0.0000(0.0000) | Loss 1.2779(1.2887) | Error 0.0000(0.0000) Steps 524(524.31) | Grad Norm 6.0077(7.2755) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 11.1351(11.3959) | Bit/dim 1.3940(1.2992) | Xent 0.0000(0.0000) | Loss 1.3940(1.2992) | Error 0.0000(0.0000) Steps 518(525.64) | Grad Norm 27.4358(11.2761) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 11.0362(11.4105) | Bit/dim 1.3227(1.3060) | Xent 0.0000(0.0000) | Loss 1.3227(1.3060) | Error 0.0000(0.0000) Steps 518(526.56) | Grad Norm 14.2323(12.5915) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 48.6880, Epoch Time 816.9848(642.7871), Bit/dim 1.2827(best: 1.2886), Xent 0.0000, Loss 1.2827, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 11.5867(11.4159) | Bit/dim 1.3013(1.3031) | Xent 0.0000(0.0000) | Loss 1.3013(1.3031) | Error 0.0000(0.0000) Steps 536(527.11) | Grad Norm 14.1579(12.2900) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 10.7157(11.3359) | Bit/dim 1.2947(1.2995) | Xent 0.0000(0.0000) | Loss 1.2947(1.2995) | Error 0.0000(0.0000) Steps 518(526.30) | Grad Norm 14.2605(11.7435) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 11.5754(11.3198) | Bit/dim 1.2730(1.2934) | Xent 0.0000(0.0000) | Loss 1.2730(1.2934) | Error 0.0000(0.0000) Steps 536(526.22) | Grad Norm 11.9334(11.1110) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 11.3054(11.3127) | Bit/dim 1.2670(1.2877) | Xent 0.0000(0.0000) | Loss 1.2670(1.2877) | Error 0.0000(0.0000) Steps 524(525.64) | Grad Norm 3.1394(9.8372) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 11.9757(11.3803) | Bit/dim 1.2966(1.2837) | Xent 0.0000(0.0000) | Loss 1.2966(1.2837) | Error 0.0000(0.0000) Steps 542(526.04) | Grad Norm 15.2703(9.8734) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 11.9931(11.4136) | Bit/dim 1.3274(1.2851) | Xent 0.0000(0.0000) | Loss 1.3274(1.2851) | Error 0.0000(0.0000) Steps 542(527.00) | Grad Norm 29.4370(11.1257) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 10.9182(11.4254) | Bit/dim 1.3120(1.2937) | Xent 0.0000(0.0000) | Loss 1.3120(1.2937) | Error 0.0000(0.0000) Steps 518(527.62) | Grad Norm 21.2295(13.3182) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 47.9835, Epoch Time 812.5029(647.8786), Bit/dim 1.2784(best: 1.2827), Xent 0.0000, Loss 1.2784, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 11.3580(11.4600) | Bit/dim 1.3078(1.2973) | Xent 0.0000(0.0000) | Loss 1.3078(1.2973) | Error 0.0000(0.0000) Steps 524(528.54) | Grad Norm 17.5450(14.2713) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 10.8839(11.4565) | Bit/dim 1.2949(1.2941) | Xent 0.0000(0.0000) | Loss 1.2949(1.2941) | Error 0.0000(0.0000) Steps 518(528.24) | Grad Norm 11.0788(13.2889) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 11.1743(11.4472) | Bit/dim 1.2664(1.2870) | Xent 0.0000(0.0000) | Loss 1.2664(1.2870) | Error 0.0000(0.0000) Steps 524(527.24) | Grad Norm 5.0514(11.2274) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 11.7983(11.4605) | Bit/dim 1.2524(1.2806) | Xent 0.0000(0.0000) | Loss 1.2524(1.2806) | Error 0.0000(0.0000) Steps 524(526.39) | Grad Norm 3.0945(9.1803) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 11.2559(11.4304) | Bit/dim 1.2572(1.2753) | Xent 0.0000(0.0000) | Loss 1.2572(1.2753) | Error 0.0000(0.0000) Steps 524(525.76) | Grad Norm 2.6140(7.4056) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 11.3765(11.4282) | Bit/dim 1.2505(1.2697) | Xent 0.0000(0.0000) | Loss 1.2505(1.2697) | Error 0.0000(0.0000) Steps 524(525.30) | Grad Norm 1.9455(6.2845) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 48.6633, Epoch Time 819.4385(653.0254), Bit/dim 1.2493(best: 1.2784), Xent 0.0000, Loss 1.2493, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 11.8173(11.4389) | Bit/dim 1.2952(1.2666) | Xent 0.0000(0.0000) | Loss 1.2952(1.2666) | Error 0.0000(0.0000) Steps 542(525.84) | Grad Norm 21.5453(6.2835) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 11.9588(11.5202) | Bit/dim 1.2856(1.2768) | Xent 0.0000(0.0000) | Loss 1.2856(1.2768) | Error 0.0000(0.0000) Steps 548(528.90) | Grad Norm 14.5312(10.1271) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 12.4238(11.5518) | Bit/dim 1.3603(1.2836) | Xent 0.0000(0.0000) | Loss 1.3603(1.2836) | Error 0.0000(0.0000) Steps 554(529.67) | Grad Norm 29.3904(12.1249) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 11.9352(11.5156) | Bit/dim 1.3205(1.2877) | Xent 0.0000(0.0000) | Loss 1.3205(1.2877) | Error 0.0000(0.0000) Steps 542(529.15) | Grad Norm 25.3295(13.4411) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 11.8500(11.5631) | Bit/dim 1.2830(1.2852) | Xent 0.0000(0.0000) | Loss 1.2830(1.2852) | Error 0.0000(0.0000) Steps 542(530.04) | Grad Norm 18.8827(13.5158) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 11.7174(11.4787) | Bit/dim 1.2868(1.2806) | Xent 0.0000(0.0000) | Loss 1.2868(1.2806) | Error 0.0000(0.0000) Steps 536(529.33) | Grad Norm 8.0135(12.1151) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 11.6622(11.4879) | Bit/dim 1.2591(1.2740) | Xent 0.0000(0.0000) | Loss 1.2591(1.2740) | Error 0.0000(0.0000) Steps 524(528.90) | Grad Norm 10.4116(10.7322) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 49.0376, Epoch Time 825.1293(658.1885), Bit/dim 1.2488(best: 1.2493), Xent 0.0000, Loss 1.2488, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 11.2881(11.4521) | Bit/dim 1.2452(1.2675) | Xent 0.0000(0.0000) | Loss 1.2452(1.2675) | Error 0.0000(0.0000) Steps 524(527.94) | Grad Norm 4.0443(9.1372) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 11.7834(11.4647) | Bit/dim 1.2330(1.2619) | Xent 0.0000(0.0000) | Loss 1.2330(1.2619) | Error 0.0000(0.0000) Steps 524(526.91) | Grad Norm 3.8484(7.5907) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 11.2860(11.4625) | Bit/dim 1.2383(1.2565) | Xent 0.0000(0.0000) | Loss 1.2383(1.2565) | Error 0.0000(0.0000) Steps 524(526.77) | Grad Norm 5.6579(6.5091) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 11.2882(11.5100) | Bit/dim 1.2754(1.2558) | Xent 0.0000(0.0000) | Loss 1.2754(1.2558) | Error 0.0000(0.0000) Steps 524(528.09) | Grad Norm 16.1351(7.6475) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 11.7981(11.5155) | Bit/dim 1.2703(1.2527) | Xent 0.0000(0.0000) | Loss 1.2703(1.2527) | Error 0.0000(0.0000) Steps 542(528.60) | Grad Norm 16.7537(7.7457) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 12.7348(11.5982) | Bit/dim 1.3147(1.2604) | Xent 0.0000(0.0000) | Loss 1.3147(1.2604) | Error 0.0000(0.0000) Steps 554(531.24) | Grad Norm 32.1442(11.0898) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 50.6467, Epoch Time 829.3410(663.3231), Bit/dim 1.2551(best: 1.2488), Xent 0.0000, Loss 1.2551, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 12.0167(11.6421) | Bit/dim 1.2624(1.2705) | Xent 0.0000(0.0000) | Loss 1.2624(1.2705) | Error 0.0000(0.0000) Steps 548(533.02) | Grad Norm 12.3120(13.1134) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 11.6468(11.6131) | Bit/dim 1.2330(1.2690) | Xent 0.0000(0.0000) | Loss 1.2330(1.2690) | Error 0.0000(0.0000) Steps 536(532.45) | Grad Norm 2.1900(13.2785) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 11.4042(11.5869) | Bit/dim 1.2418(1.2633) | Xent 0.0000(0.0000) | Loss 1.2418(1.2633) | Error 0.0000(0.0000) Steps 524(532.43) | Grad Norm 9.6801(11.8407) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 11.8259(11.5625) | Bit/dim 1.2357(1.2591) | Xent 0.0000(0.0000) | Loss 1.2357(1.2591) | Error 0.0000(0.0000) Steps 542(532.12) | Grad Norm 7.9426(10.4259) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 11.6265(11.5557) | Bit/dim 1.2289(1.2528) | Xent 0.0000(0.0000) | Loss 1.2289(1.2528) | Error 0.0000(0.0000) Steps 524(531.39) | Grad Norm 1.8358(9.0692) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 11.8818(11.5812) | Bit/dim 1.2482(1.2487) | Xent 0.0000(0.0000) | Loss 1.2482(1.2487) | Error 0.0000(0.0000) Steps 548(531.87) | Grad Norm 11.3525(8.5604) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 11.9396(11.5832) | Bit/dim 1.2424(1.2468) | Xent 0.0000(0.0000) | Loss 1.2424(1.2468) | Error 0.0000(0.0000) Steps 548(532.07) | Grad Norm 7.7008(8.0209) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 49.0330, Epoch Time 827.2342(668.2404), Bit/dim 1.2318(best: 1.2488), Xent 0.0000, Loss 1.2318, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 12.3594(11.5974) | Bit/dim 1.2201(1.2429) | Xent 0.0000(0.0000) | Loss 1.2201(1.2429) | Error 0.0000(0.0000) Steps 542(532.64) | Grad Norm 6.5187(8.1239) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 11.3204(11.6285) | Bit/dim 1.2461(1.2427) | Xent 0.0000(0.0000) | Loss 1.2461(1.2427) | Error 0.0000(0.0000) Steps 524(533.69) | Grad Norm 12.9292(9.2943) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 11.8150(11.6973) | Bit/dim 1.2440(1.2531) | Xent 0.0000(0.0000) | Loss 1.2440(1.2531) | Error 0.0000(0.0000) Steps 542(536.67) | Grad Norm 8.3301(12.0835) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 11.7402(11.6791) | Bit/dim 1.2520(1.2534) | Xent 0.0000(0.0000) | Loss 1.2520(1.2534) | Error 0.0000(0.0000) Steps 548(536.27) | Grad Norm 10.0228(11.8013) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 11.5948(11.6478) | Bit/dim 1.2504(1.2478) | Xent 0.0000(0.0000) | Loss 1.2504(1.2478) | Error 0.0000(0.0000) Steps 536(535.89) | Grad Norm 1.9976(10.0096) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 12.0450(11.6427) | Bit/dim 1.2291(1.2433) | Xent 0.0000(0.0000) | Loss 1.2291(1.2433) | Error 0.0000(0.0000) Steps 548(536.08) | Grad Norm 11.1401(9.2293) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 11.3221(11.6307) | Bit/dim 1.2663(1.2423) | Xent 0.0000(0.0000) | Loss 1.2663(1.2423) | Error 0.0000(0.0000) Steps 524(535.58) | Grad Norm 22.4156(9.4869) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 51.2385, Epoch Time 836.2772(673.2815), Bit/dim 1.2379(best: 1.2318), Xent 0.0000, Loss 1.2379, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 12.4301(11.6224) | Bit/dim 1.3596(1.2536) | Xent 0.0000(0.0000) | Loss 1.3596(1.2536) | Error 0.0000(0.0000) Steps 560(536.33) | Grad Norm 41.0388(12.5985) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 12.1689(11.6862) | Bit/dim 1.2599(1.2646) | Xent 0.0000(0.0000) | Loss 1.2599(1.2646) | Error 0.0000(0.0000) Steps 548(536.24) | Grad Norm 12.5192(13.4785) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 12.4923(11.7210) | Bit/dim 1.2603(1.2608) | Xent 0.0000(0.0000) | Loss 1.2603(1.2608) | Error 0.0000(0.0000) Steps 554(538.12) | Grad Norm 10.3335(12.6828) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 11.2410(11.6397) | Bit/dim 1.2468(1.2592) | Xent 0.0000(0.0000) | Loss 1.2468(1.2592) | Error 0.0000(0.0000) Steps 524(537.00) | Grad Norm 11.0898(12.7414) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 11.2946(11.6468) | Bit/dim 1.2340(1.2539) | Xent 0.0000(0.0000) | Loss 1.2340(1.2539) | Error 0.0000(0.0000) Steps 524(535.92) | Grad Norm 3.3634(11.5595) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 11.6550(11.6274) | Bit/dim 1.2241(1.2479) | Xent 0.0000(0.0000) | Loss 1.2241(1.2479) | Error 0.0000(0.0000) Steps 542(536.12) | Grad Norm 2.1752(10.5229) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 51.1305, Epoch Time 834.5993(678.1210), Bit/dim 1.2177(best: 1.2318), Xent 0.0000, Loss 1.2177, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 11.8187(11.6031) | Bit/dim 1.2095(1.2404) | Xent 0.0000(0.0000) | Loss 1.2095(1.2404) | Error 0.0000(0.0000) Steps 542(535.46) | Grad Norm 5.2313(9.1542) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 11.5193(11.5806) | Bit/dim 1.2167(1.2350) | Xent 0.0000(0.0000) | Loss 1.2167(1.2350) | Error 0.0000(0.0000) Steps 530(535.24) | Grad Norm 0.8613(7.4100) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 12.5164(11.6009) | Bit/dim 1.2085(1.2310) | Xent 0.0000(0.0000) | Loss 1.2085(1.2310) | Error 0.0000(0.0000) Steps 548(535.93) | Grad Norm 6.7066(6.2719) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 10.9609(11.5969) | Bit/dim 1.3058(1.2327) | Xent 0.0000(0.0000) | Loss 1.3058(1.2327) | Error 0.0000(0.0000) Steps 518(536.52) | Grad Norm 26.7683(8.6014) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 12.6312(11.6684) | Bit/dim 1.2890(1.2401) | Xent 0.0000(0.0000) | Loss 1.2890(1.2401) | Error 0.0000(0.0000) Steps 560(538.66) | Grad Norm 26.6431(11.2474) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 12.0336(11.6639) | Bit/dim 1.2312(1.2483) | Xent 0.0000(0.0000) | Loss 1.2312(1.2483) | Error 0.0000(0.0000) Steps 542(538.20) | Grad Norm 15.5975(13.1349) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 12.2009(11.6748) | Bit/dim 1.2317(1.2488) | Xent 0.0000(0.0000) | Loss 1.2317(1.2488) | Error 0.0000(0.0000) Steps 542(537.83) | Grad Norm 3.5637(13.3600) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 50.1970, Epoch Time 834.4299(682.8103), Bit/dim 1.2200(best: 1.2177), Xent 0.0000, Loss 1.2200, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 11.5727(11.7149) | Bit/dim 1.2398(1.2431) | Xent 0.0000(0.0000) | Loss 1.2398(1.2431) | Error 0.0000(0.0000) Steps 530(537.23) | Grad Norm 9.6028(12.0642) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 11.5814(11.6731) | Bit/dim 1.2199(1.2396) | Xent 0.0000(0.0000) | Loss 1.2199(1.2396) | Error 0.0000(0.0000) Steps 542(536.72) | Grad Norm 3.8329(11.5608) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 12.2611(11.7043) | Bit/dim 1.2491(1.2378) | Xent 0.0000(0.0000) | Loss 1.2491(1.2378) | Error 0.0000(0.0000) Steps 560(538.02) | Grad Norm 20.7549(11.8752) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 12.1104(11.7036) | Bit/dim 1.2189(1.2396) | Xent 0.0000(0.0000) | Loss 1.2189(1.2396) | Error 0.0000(0.0000) Steps 542(538.06) | Grad Norm 4.6425(13.0436) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 11.4230(11.6906) | Bit/dim 1.2283(1.2364) | Xent 0.0000(0.0000) | Loss 1.2283(1.2364) | Error 0.0000(0.0000) Steps 524(537.20) | Grad Norm 14.0091(12.2750) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 11.3500(11.6849) | Bit/dim 1.2229(1.2356) | Xent 0.0000(0.0000) | Loss 1.2229(1.2356) | Error 0.0000(0.0000) Steps 530(536.73) | Grad Norm 10.2624(12.6998) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 48.9125, Epoch Time 835.7917(687.3997), Bit/dim 1.2113(best: 1.2177), Xent 0.0000, Loss 1.2113, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 11.7399(11.6794) | Bit/dim 1.2149(1.2319) | Xent 0.0000(0.0000) | Loss 1.2149(1.2319) | Error 0.0000(0.0000) Steps 548(537.13) | Grad Norm 8.8954(11.9756) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 11.6880(11.6529) | Bit/dim 1.2320(1.2276) | Xent 0.0000(0.0000) | Loss 1.2320(1.2276) | Error 0.0000(0.0000) Steps 530(536.72) | Grad Norm 6.0829(9.9982) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 11.3590(11.6388) | Bit/dim 1.2181(1.2222) | Xent 0.0000(0.0000) | Loss 1.2181(1.2222) | Error 0.0000(0.0000) Steps 530(536.81) | Grad Norm 1.6816(8.1210) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 11.5377(11.6195) | Bit/dim 1.2060(1.2177) | Xent 0.0000(0.0000) | Loss 1.2060(1.2177) | Error 0.0000(0.0000) Steps 530(536.58) | Grad Norm 5.6690(7.0571) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 11.6616(11.6321) | Bit/dim 1.2080(1.2158) | Xent 0.0000(0.0000) | Loss 1.2080(1.2158) | Error 0.0000(0.0000) Steps 530(537.22) | Grad Norm 2.2116(6.4046) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 11.3551(11.7039) | Bit/dim 1.1981(1.2148) | Xent 0.0000(0.0000) | Loss 1.1981(1.2148) | Error 0.0000(0.0000) Steps 530(538.32) | Grad Norm 5.2911(7.3600) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 11.1830(11.7270) | Bit/dim 1.2184(1.2146) | Xent 0.0000(0.0000) | Loss 1.2184(1.2146) | Error 0.0000(0.0000) Steps 530(539.14) | Grad Norm 5.6677(7.8183) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 48.9672, Epoch Time 836.6178(691.8763), Bit/dim 1.2121(best: 1.2113), Xent 0.0000, Loss 1.2121, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 13.0790(11.7787) | Bit/dim 1.2753(1.2265) | Xent 0.0000(0.0000) | Loss 1.2753(1.2265) | Error 0.0000(0.0000) Steps 572(540.96) | Grad Norm 28.7286(10.6193) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 13.0397(11.8966) | Bit/dim 1.2931(1.2550) | Xent 0.0000(0.0000) | Loss 1.2931(1.2550) | Error 0.0000(0.0000) Steps 602(547.76) | Grad Norm 12.4108(13.1454) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 10.7587(11.8192) | Bit/dim 1.2357(1.2621) | Xent 0.0000(0.0000) | Loss 1.2357(1.2621) | Error 0.0000(0.0000) Steps 524(546.18) | Grad Norm 10.6942(13.6485) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 11.1448(11.7837) | Bit/dim 1.2122(1.2542) | Xent 0.0000(0.0000) | Loss 1.2122(1.2542) | Error 0.0000(0.0000) Steps 530(543.22) | Grad Norm 6.8597(11.8404) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 11.7817(11.7625) | Bit/dim 1.2250(1.2450) | Xent 0.0000(0.0000) | Loss 1.2250(1.2450) | Error 0.0000(0.0000) Steps 548(542.16) | Grad Norm 3.7639(10.1966) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 12.0059(11.7655) | Bit/dim 1.2065(1.2357) | Xent 0.0000(0.0000) | Loss 1.2065(1.2357) | Error 0.0000(0.0000) Steps 548(542.09) | Grad Norm 2.9833(8.7243) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 12.1330(11.7362) | Bit/dim 1.1975(1.2275) | Xent 0.0000(0.0000) | Loss 1.1975(1.2275) | Error 0.0000(0.0000) Steps 548(541.33) | Grad Norm 8.1600(7.7062) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 50.1976, Epoch Time 842.7538(696.4026), Bit/dim 1.1982(best: 1.2113), Xent 0.0000, Loss 1.1982, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 11.9345(11.7245) | Bit/dim 1.1944(1.2205) | Xent 0.0000(0.0000) | Loss 1.1944(1.2205) | Error 0.0000(0.0000) Steps 548(540.61) | Grad Norm 8.3052(7.0008) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 11.5376(11.7497) | Bit/dim 1.2055(1.2173) | Xent 0.0000(0.0000) | Loss 1.2055(1.2173) | Error 0.0000(0.0000) Steps 530(540.66) | Grad Norm 3.1296(6.8706) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 11.9933(11.7404) | Bit/dim 1.1821(1.2141) | Xent 0.0000(0.0000) | Loss 1.1821(1.2141) | Error 0.0000(0.0000) Steps 536(540.43) | Grad Norm 5.9286(7.1218) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 11.2171(11.7818) | Bit/dim 1.2471(1.2152) | Xent 0.0000(0.0000) | Loss 1.2471(1.2152) | Error 0.0000(0.0000) Steps 524(541.94) | Grad Norm 26.5201(9.0773) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 13.2101(11.8010) | Bit/dim 1.2902(1.2290) | Xent 0.0000(0.0000) | Loss 1.2902(1.2290) | Error 0.0000(0.0000) Steps 566(542.42) | Grad Norm 11.7328(11.8896) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 12.3018(11.7906) | Bit/dim 1.2253(1.2323) | Xent 0.0000(0.0000) | Loss 1.2253(1.2323) | Error 0.0000(0.0000) Steps 548(542.07) | Grad Norm 12.7021(12.4255) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 52.0744, Epoch Time 845.3330(700.8705), Bit/dim 1.2077(best: 1.1982), Xent 0.0000, Loss 1.2077, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 11.4126(11.8057) | Bit/dim 1.2058(1.2286) | Xent 0.0000(0.0000) | Loss 1.2058(1.2286) | Error 0.0000(0.0000) Steps 542(543.33) | Grad Norm 3.2331(11.0420) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 11.7946(11.7794) | Bit/dim 1.1961(1.2212) | Xent 0.0000(0.0000) | Loss 1.1961(1.2212) | Error 0.0000(0.0000) Steps 542(542.71) | Grad Norm 0.8748(9.0982) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 11.8226(11.7675) | Bit/dim 1.1968(1.2148) | Xent 0.0000(0.0000) | Loss 1.1968(1.2148) | Error 0.0000(0.0000) Steps 542(542.03) | Grad Norm 2.4055(7.6492) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 12.4649(11.7839) | Bit/dim 1.1983(1.2095) | Xent 0.0000(0.0000) | Loss 1.1983(1.2095) | Error 0.0000(0.0000) Steps 554(541.88) | Grad Norm 4.2461(6.6112) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 12.4793(11.8119) | Bit/dim 1.2111(1.2079) | Xent 0.0000(0.0000) | Loss 1.2111(1.2079) | Error 0.0000(0.0000) Steps 560(542.31) | Grad Norm 11.5118(7.3434) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 12.6358(11.8417) | Bit/dim 1.2118(1.2046) | Xent 0.0000(0.0000) | Loss 1.2118(1.2046) | Error 0.0000(0.0000) Steps 566(542.97) | Grad Norm 16.8780(7.7922) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 12.5457(11.8593) | Bit/dim 1.1967(1.2038) | Xent 0.0000(0.0000) | Loss 1.1967(1.2038) | Error 0.0000(0.0000) Steps 566(543.60) | Grad Norm 14.5849(8.7914) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 49.2491, Epoch Time 846.0486(705.2259), Bit/dim 1.1937(best: 1.1982), Xent 0.0000, Loss 1.1937, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 11.9820(11.8517) | Bit/dim 1.1774(1.2020) | Xent 0.0000(0.0000) | Loss 1.1774(1.2020) | Error 0.0000(0.0000) Steps 554(543.17) | Grad Norm 9.1858(8.4204) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 12.7488(11.9346) | Bit/dim 1.2513(1.2128) | Xent 0.0000(0.0000) | Loss 1.2513(1.2128) | Error 0.0000(0.0000) Steps 566(544.91) | Grad Norm 11.2361(10.9916) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 11.8257(11.9079) | Bit/dim 1.3811(1.2398) | Xent 0.0000(0.0000) | Loss 1.3811(1.2398) | Error 0.0000(0.0000) Steps 554(544.73) | Grad Norm 15.2802(14.0224) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 11.9699(11.9147) | Bit/dim 1.2432(1.2538) | Xent 0.0000(0.0000) | Loss 1.2432(1.2538) | Error 0.0000(0.0000) Steps 548(546.08) | Grad Norm 6.9272(14.7401) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 11.6591(11.8139) | Bit/dim 1.2259(1.2502) | Xent 0.0000(0.0000) | Loss 1.2259(1.2502) | Error 0.0000(0.0000) Steps 524(542.14) | Grad Norm 9.0245(13.8724) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 11.4588(11.7798) | Bit/dim 1.2069(1.2396) | Xent 0.0000(0.0000) | Loss 1.2069(1.2396) | Error 0.0000(0.0000) Steps 524(539.90) | Grad Norm 7.7126(11.8938) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 48.5847, Epoch Time 842.5295(709.3450), Bit/dim 1.1890(best: 1.1937), Xent 0.0000, Loss 1.1890, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 11.3292(11.7288) | Bit/dim 1.1950(1.2283) | Xent 0.0000(0.0000) | Loss 1.1950(1.2283) | Error 0.0000(0.0000) Steps 524(537.73) | Grad Norm 3.3659(9.8701) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 11.4690(11.7023) | Bit/dim 1.1902(1.2193) | Xent 0.0000(0.0000) | Loss 1.1902(1.2193) | Error 0.0000(0.0000) Steps 542(537.18) | Grad Norm 3.6500(8.4658) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 11.7653(11.6454) | Bit/dim 1.1697(1.2106) | Xent 0.0000(0.0000) | Loss 1.1697(1.2106) | Error 0.0000(0.0000) Steps 542(536.11) | Grad Norm 3.2900(7.1925) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 11.5836(11.6080) | Bit/dim 1.1890(1.2042) | Xent 0.0000(0.0000) | Loss 1.1890(1.2042) | Error 0.0000(0.0000) Steps 530(534.97) | Grad Norm 1.5161(5.8642) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 12.3100(11.6332) | Bit/dim 1.1924(1.1995) | Xent 0.0000(0.0000) | Loss 1.1924(1.1995) | Error 0.0000(0.0000) Steps 548(535.44) | Grad Norm 5.1363(5.3482) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 12.4671(11.7582) | Bit/dim 1.1751(1.1973) | Xent 0.0000(0.0000) | Loss 1.1751(1.1973) | Error 0.0000(0.0000) Steps 542(538.41) | Grad Norm 2.4647(6.3312) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 12.2534(11.8046) | Bit/dim 1.1952(1.1976) | Xent 0.0000(0.0000) | Loss 1.1952(1.1976) | Error 0.0000(0.0000) Steps 554(540.58) | Grad Norm 4.3491(7.0132) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 53.9124, Epoch Time 842.5754(713.3419), Bit/dim 1.2619(best: 1.1890), Xent 0.0000, Loss 1.2619, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 11.7184(11.8297) | Bit/dim 1.3786(1.2171) | Xent 0.0000(0.0000) | Loss 1.3786(1.2171) | Error 0.0000(0.0000) Steps 530(540.71) | Grad Norm 14.5246(11.0778) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 12.0217(11.8850) | Bit/dim 1.2408(1.2342) | Xent 0.0000(0.0000) | Loss 1.2408(1.2342) | Error 0.0000(0.0000) Steps 542(542.30) | Grad Norm 16.9816(11.4138) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 10.9930(11.8133) | Bit/dim 1.2127(1.2300) | Xent 0.0000(0.0000) | Loss 1.2127(1.2300) | Error 0.0000(0.0000) Steps 524(540.51) | Grad Norm 11.3689(10.8421) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 11.3518(11.7638) | Bit/dim 1.2161(1.2253) | Xent 0.0000(0.0000) | Loss 1.2161(1.2253) | Error 0.0000(0.0000) Steps 518(539.58) | Grad Norm 12.9115(11.0930) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 11.3065(11.7280) | Bit/dim 1.2155(1.2215) | Xent 0.0000(0.0000) | Loss 1.2155(1.2215) | Error 0.0000(0.0000) Steps 524(538.08) | Grad Norm 11.8343(11.6009) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 11.8164(11.7849) | Bit/dim 1.1977(1.2172) | Xent 0.0000(0.0000) | Loss 1.1977(1.2172) | Error 0.0000(0.0000) Steps 548(538.38) | Grad Norm 8.3544(11.4154) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 11.1482(11.7459) | Bit/dim 1.1885(1.2111) | Xent 0.0000(0.0000) | Loss 1.1885(1.2111) | Error 0.0000(0.0000) Steps 530(538.41) | Grad Norm 9.9407(10.1928) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 51.3634, Epoch Time 843.1640(717.2365), Bit/dim 1.1806(best: 1.1890), Xent 0.0000, Loss 1.1806, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 11.8781(11.7102) | Bit/dim 1.1919(1.2044) | Xent 0.0000(0.0000) | Loss 1.1919(1.2044) | Error 0.0000(0.0000) Steps 530(537.99) | Grad Norm 3.8465(8.7431) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 11.3652(11.6683) | Bit/dim 1.1941(1.1988) | Xent 0.0000(0.0000) | Loss 1.1941(1.1988) | Error 0.0000(0.0000) Steps 530(536.22) | Grad Norm 2.2975(7.3710) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 11.8062(11.6817) | Bit/dim 1.1900(1.1941) | Xent 0.0000(0.0000) | Loss 1.1900(1.1941) | Error 0.0000(0.0000) Steps 542(536.24) | Grad Norm 4.7240(6.4616) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 11.5819(11.7526) | Bit/dim 1.1683(1.1907) | Xent 0.0000(0.0000) | Loss 1.1683(1.1907) | Error 0.0000(0.0000) Steps 542(538.11) | Grad Norm 5.3489(7.1412) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 13.0680(11.8470) | Bit/dim 1.1922(1.1907) | Xent 0.0000(0.0000) | Loss 1.1922(1.1907) | Error 0.0000(0.0000) Steps 566(539.94) | Grad Norm 6.5914(7.7142) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 12.3673(11.9510) | Bit/dim 1.1837(1.1978) | Xent 0.0000(0.0000) | Loss 1.1837(1.1978) | Error 0.0000(0.0000) Steps 566(543.19) | Grad Norm 4.9776(10.0144) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 47.7767, Epoch Time 846.7808(721.1229), Bit/dim 1.2299(best: 1.1806), Xent 0.0000, Loss 1.2299, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 12.4503(11.9390) | Bit/dim 1.2248(1.2104) | Xent 0.0000(0.0000) | Loss 1.2248(1.2104) | Error 0.0000(0.0000) Steps 554(542.63) | Grad Norm 10.7225(11.7354) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 11.0436(11.9130) | Bit/dim 1.2046(1.2111) | Xent 0.0000(0.0000) | Loss 1.2046(1.2111) | Error 0.0000(0.0000) Steps 518(541.54) | Grad Norm 9.2534(11.9516) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 11.3790(11.8034) | Bit/dim 1.2893(1.2211) | Xent 0.0000(0.0000) | Loss 1.2893(1.2211) | Error 0.0000(0.0000) Steps 536(539.75) | Grad Norm 14.7321(13.7263) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 11.3913(11.8243) | Bit/dim 1.2088(1.2192) | Xent 0.0000(0.0000) | Loss 1.2088(1.2192) | Error 0.0000(0.0000) Steps 518(539.95) | Grad Norm 7.7181(13.3884) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 11.7616(11.8436) | Bit/dim 1.1937(1.2118) | Xent 0.0000(0.0000) | Loss 1.1937(1.2118) | Error 0.0000(0.0000) Steps 542(540.07) | Grad Norm 4.3521(11.4477) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 11.2898(11.7958) | Bit/dim 1.1811(1.2047) | Xent 0.0000(0.0000) | Loss 1.1811(1.2047) | Error 0.0000(0.0000) Steps 530(539.11) | Grad Norm 2.2075(9.4470) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 11.4202(11.7249) | Bit/dim 1.1927(1.1981) | Xent 0.0000(0.0000) | Loss 1.1927(1.1981) | Error 0.0000(0.0000) Steps 530(537.13) | Grad Norm 3.0053(7.6570) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 49.2672, Epoch Time 837.0242(724.5999), Bit/dim 1.1712(best: 1.1806), Xent 0.0000, Loss 1.1712, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.7376(11.6431) | Bit/dim 1.1655(1.1925) | Xent 0.0000(0.0000) | Loss 1.1655(1.1925) | Error 0.0000(0.0000) Steps 530(535.07) | Grad Norm 4.6761(6.3145) | Total Time 10.00(10.00)\n",
      "Iter 2460 | Time 12.6349(11.6874) | Bit/dim 1.1680(1.1877) | Xent 0.0000(0.0000) | Loss 1.1680(1.1877) | Error 0.0000(0.0000) Steps 566(536.72) | Grad Norm 11.3805(6.3298) | Total Time 10.00(10.00)\n",
      "Iter 2470 | Time 11.3402(11.7608) | Bit/dim 1.1745(1.1850) | Xent 0.0000(0.0000) | Loss 1.1745(1.1850) | Error 0.0000(0.0000) Steps 524(538.33) | Grad Norm 1.4965(6.9805) | Total Time 10.00(10.00)\n",
      "Iter 2480 | Time 11.5905(11.8171) | Bit/dim 1.3568(1.2040) | Xent 0.0000(0.0000) | Loss 1.3568(1.2040) | Error 0.0000(0.0000) Steps 530(540.34) | Grad Norm 14.7881(10.5237) | Total Time 10.00(10.00)\n",
      "Iter 2490 | Time 11.9257(11.8626) | Bit/dim 1.2303(1.2180) | Xent 0.0000(0.0000) | Loss 1.2303(1.2180) | Error 0.0000(0.0000) Steps 536(541.03) | Grad Norm 15.5281(10.6400) | Total Time 10.00(10.00)\n",
      "Iter 2500 | Time 10.7286(11.7869) | Bit/dim 1.2766(1.2216) | Xent 0.0000(0.0000) | Loss 1.2766(1.2216) | Error 0.0000(0.0000) Steps 512(538.88) | Grad Norm 16.3953(12.0834) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 48.3625, Epoch Time 841.4864(728.1065), Bit/dim 1.2304(best: 1.1712), Xent 0.0000, Loss 1.2304, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.6178(11.7596) | Bit/dim 1.1964(1.2245) | Xent 0.0000(0.0000) | Loss 1.1964(1.2245) | Error 0.0000(0.0000) Steps 542(537.33) | Grad Norm 5.8415(12.3402) | Total Time 10.00(10.00)\n",
      "Iter 2520 | Time 11.0101(11.6452) | Bit/dim 1.1904(1.2207) | Xent 0.0000(0.0000) | Loss 1.1904(1.2207) | Error 0.0000(0.0000) Steps 518(534.55) | Grad Norm 8.9987(12.4023) | Total Time 10.00(10.00)\n",
      "Iter 2530 | Time 11.8337(11.6936) | Bit/dim 1.1957(1.2137) | Xent 0.0000(0.0000) | Loss 1.1957(1.2137) | Error 0.0000(0.0000) Steps 536(535.34) | Grad Norm 6.6792(11.3755) | Total Time 10.00(10.00)\n",
      "Iter 2540 | Time 11.9146(11.6816) | Bit/dim 1.1728(1.2045) | Xent 0.0000(0.0000) | Loss 1.1728(1.2045) | Error 0.0000(0.0000) Steps 536(534.64) | Grad Norm 3.9836(9.7611) | Total Time 10.00(10.00)\n",
      "Iter 2550 | Time 11.6449(11.6034) | Bit/dim 1.1774(1.1966) | Xent 0.0000(0.0000) | Loss 1.1774(1.1966) | Error 0.0000(0.0000) Steps 524(532.78) | Grad Norm 2.7282(7.8771) | Total Time 10.00(10.00)\n",
      "Iter 2560 | Time 11.0715(11.5035) | Bit/dim 1.1894(1.1893) | Xent 0.0000(0.0000) | Loss 1.1894(1.1893) | Error 0.0000(0.0000) Steps 524(531.25) | Grad Norm 2.2944(6.2942) | Total Time 10.00(10.00)\n",
      "Iter 2570 | Time 11.3883(11.4593) | Bit/dim 1.1812(1.1842) | Xent 0.0000(0.0000) | Loss 1.1812(1.1842) | Error 0.0000(0.0000) Steps 524(529.96) | Grad Norm 1.9987(5.0388) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 49.4655, Epoch Time 821.5535(730.9099), Bit/dim 1.1649(best: 1.1712), Xent 0.0000, Loss 1.1649, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.5975(11.5530) | Bit/dim 1.1879(1.1802) | Xent 0.0000(0.0000) | Loss 1.1879(1.1802) | Error 0.0000(0.0000) Steps 530(532.06) | Grad Norm 11.8994(5.7169) | Total Time 10.00(10.00)\n",
      "Iter 2590 | Time 11.6405(11.6086) | Bit/dim 1.1856(1.1795) | Xent 0.0000(0.0000) | Loss 1.1856(1.1795) | Error 0.0000(0.0000) Steps 530(533.73) | Grad Norm 6.9800(6.1757) | Total Time 10.00(10.00)\n",
      "Iter 2600 | Time 12.1866(11.7602) | Bit/dim 1.3161(1.1882) | Xent 0.0000(0.0000) | Loss 1.3161(1.1882) | Error 0.0000(0.0000) Steps 524(537.12) | Grad Norm 29.5860(9.1409) | Total Time 10.00(10.00)\n",
      "Iter 2610 | Time 12.0508(11.8041) | Bit/dim 1.2036(1.2051) | Xent 0.0000(0.0000) | Loss 1.2036(1.2051) | Error 0.0000(0.0000) Steps 554(539.29) | Grad Norm 9.7829(11.5204) | Total Time 10.00(10.00)\n",
      "Iter 2620 | Time 12.6191(11.7965) | Bit/dim 1.3025(1.2254) | Xent 0.0000(0.0000) | Loss 1.3025(1.2254) | Error 0.0000(0.0000) Steps 572(540.52) | Grad Norm 30.0235(13.6899) | Total Time 10.00(10.00)\n",
      "Iter 2630 | Time 11.2789(11.7259) | Bit/dim 1.1927(1.2265) | Xent 0.0000(0.0000) | Loss 1.1927(1.2265) | Error 0.0000(0.0000) Steps 518(537.36) | Grad Norm 6.6262(13.3570) | Total Time 10.00(10.00)\n",
      "Iter 2640 | Time 11.9920(11.6826) | Bit/dim 1.1997(1.2198) | Xent 0.0000(0.0000) | Loss 1.1997(1.2198) | Error 0.0000(0.0000) Steps 542(535.45) | Grad Norm 11.9594(12.8015) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 50.6388, Epoch Time 845.2480(734.3401), Bit/dim 1.1822(best: 1.1649), Xent 0.0000, Loss 1.1822, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 11.5229(11.6845) | Bit/dim 1.1878(1.2109) | Xent 0.0000(0.0000) | Loss 1.1878(1.2109) | Error 0.0000(0.0000) Steps 530(536.10) | Grad Norm 5.0047(11.4389) | Total Time 10.00(10.00)\n",
      "Iter 2660 | Time 11.4383(11.6688) | Bit/dim 1.1816(1.2019) | Xent 0.0000(0.0000) | Loss 1.1816(1.2019) | Error 0.0000(0.0000) Steps 530(535.48) | Grad Norm 1.0338(9.8930) | Total Time 10.00(10.00)\n",
      "Iter 2670 | Time 12.9034(11.6752) | Bit/dim 1.1737(1.1949) | Xent 0.0000(0.0000) | Loss 1.1737(1.1949) | Error 0.0000(0.0000) Steps 566(535.63) | Grad Norm 14.0394(9.3426) | Total Time 10.00(10.00)\n",
      "Iter 2680 | Time 10.9182(11.7320) | Bit/dim 1.1776(1.1942) | Xent 0.0000(0.0000) | Loss 1.1776(1.1942) | Error 0.0000(0.0000) Steps 524(536.70) | Grad Norm 2.9606(10.6394) | Total Time 10.00(10.00)\n",
      "Iter 2690 | Time 11.7569(11.7928) | Bit/dim 1.1961(1.1888) | Xent 0.0000(0.0000) | Loss 1.1961(1.1888) | Error 0.0000(0.0000) Steps 530(537.22) | Grad Norm 15.3875(10.3836) | Total Time 10.00(10.00)\n",
      "Iter 2700 | Time 11.1410(11.7838) | Bit/dim 1.1721(1.1881) | Xent 0.0000(0.0000) | Loss 1.1721(1.1881) | Error 0.0000(0.0000) Steps 524(537.12) | Grad Norm 10.0703(11.0457) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 49.3962, Epoch Time 842.0625(737.5717), Bit/dim 1.1719(best: 1.1649), Xent 0.0000, Loss 1.1719, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 11.2928(11.8122) | Bit/dim 1.1870(1.1883) | Xent 0.0000(0.0000) | Loss 1.1870(1.1883) | Error 0.0000(0.0000) Steps 530(537.90) | Grad Norm 9.6045(11.2149) | Total Time 10.00(10.00)\n",
      "Iter 2720 | Time 12.3629(11.8722) | Bit/dim 1.1898(1.1872) | Xent 0.0000(0.0000) | Loss 1.1898(1.1872) | Error 0.0000(0.0000) Steps 566(539.48) | Grad Norm 8.9781(11.3599) | Total Time 10.00(10.00)\n",
      "Iter 2730 | Time 11.0177(11.7844) | Bit/dim 1.1612(1.1827) | Xent 0.0000(0.0000) | Loss 1.1612(1.1827) | Error 0.0000(0.0000) Steps 524(536.51) | Grad Norm 3.1603(9.4317) | Total Time 10.00(10.00)\n",
      "Iter 2740 | Time 11.3943(11.7212) | Bit/dim 1.1580(1.1781) | Xent 0.0000(0.0000) | Loss 1.1580(1.1781) | Error 0.0000(0.0000) Steps 524(533.38) | Grad Norm 3.4846(7.7743) | Total Time 10.00(10.00)\n",
      "Iter 2750 | Time 11.5949(11.6468) | Bit/dim 1.1445(1.1731) | Xent 0.0000(0.0000) | Loss 1.1445(1.1731) | Error 0.0000(0.0000) Steps 524(531.55) | Grad Norm 2.4871(6.3981) | Total Time 10.00(10.00)\n",
      "Iter 2760 | Time 11.2829(11.6188) | Bit/dim 1.1640(1.1695) | Xent 0.0000(0.0000) | Loss 1.1640(1.1695) | Error 0.0000(0.0000) Steps 530(530.86) | Grad Norm 2.6093(5.2889) | Total Time 10.00(10.00)\n",
      "Iter 2770 | Time 11.5684(11.6061) | Bit/dim 1.1589(1.1678) | Xent 0.0000(0.0000) | Loss 1.1589(1.1678) | Error 0.0000(0.0000) Steps 536(530.66) | Grad Norm 10.9827(5.5849) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 49.0905, Epoch Time 832.1399(740.4088), Bit/dim 1.1589(best: 1.1649), Xent 0.0000, Loss 1.1589, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 13.0359(11.6880) | Bit/dim 1.1778(1.1678) | Xent 0.0000(0.0000) | Loss 1.1778(1.1678) | Error 0.0000(0.0000) Steps 572(533.63) | Grad Norm 14.3877(6.8261) | Total Time 10.00(10.00)\n",
      "Iter 2790 | Time 11.7485(11.7634) | Bit/dim 1.3037(1.1811) | Xent 0.0000(0.0000) | Loss 1.3037(1.1811) | Error 0.0000(0.0000) Steps 530(535.02) | Grad Norm 12.8155(9.5848) | Total Time 10.00(10.00)\n",
      "Iter 2800 | Time 11.0384(11.8377) | Bit/dim 1.2306(1.1973) | Xent 0.0000(0.0000) | Loss 1.2306(1.1973) | Error 0.0000(0.0000) Steps 518(539.25) | Grad Norm 18.3794(10.7000) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 11.6228(11.7592) | Bit/dim 1.1807(1.1988) | Xent 0.0000(0.0000) | Loss 1.1807(1.1988) | Error 0.0000(0.0000) Steps 518(538.38) | Grad Norm 6.0897(11.1154) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 12.1415(11.7026) | Bit/dim 1.1766(1.1960) | Xent 0.0000(0.0000) | Loss 1.1766(1.1960) | Error 0.0000(0.0000) Steps 548(536.39) | Grad Norm 5.0491(11.0511) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 11.3471(11.6893) | Bit/dim 1.2394(1.2017) | Xent 0.0000(0.0000) | Loss 1.2394(1.2017) | Error 0.0000(0.0000) Steps 512(534.69) | Grad Norm 17.9314(12.9282) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 49.2814, Epoch Time 840.3500(743.4070), Bit/dim 1.1865(best: 1.1589), Xent 0.0000, Loss 1.1865, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 11.7192(11.6722) | Bit/dim 1.2193(1.2063) | Xent 0.0000(0.0000) | Loss 1.2193(1.2063) | Error 0.0000(0.0000) Steps 548(534.21) | Grad Norm 22.8058(13.7624) | Total Time 10.00(10.00)\n",
      "Iter 2850 | Time 11.6316(11.6185) | Bit/dim 1.1754(1.2022) | Xent 0.0000(0.0000) | Loss 1.1754(1.2022) | Error 0.0000(0.0000) Steps 530(532.33) | Grad Norm 3.1065(13.2861) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 11.2591(11.6373) | Bit/dim 1.1759(1.1958) | Xent 0.0000(0.0000) | Loss 1.1759(1.1958) | Error 0.0000(0.0000) Steps 530(533.10) | Grad Norm 7.0198(12.3832) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 11.1532(11.6297) | Bit/dim 1.1578(1.1887) | Xent 0.0000(0.0000) | Loss 1.1578(1.1887) | Error 0.0000(0.0000) Steps 524(532.15) | Grad Norm 4.8777(10.8444) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 11.3604(11.5627) | Bit/dim 1.1629(1.1815) | Xent 0.0000(0.0000) | Loss 1.1629(1.1815) | Error 0.0000(0.0000) Steps 524(530.16) | Grad Norm 3.4021(8.8076) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 11.3780(11.5171) | Bit/dim 1.1518(1.1755) | Xent 0.0000(0.0000) | Loss 1.1518(1.1755) | Error 0.0000(0.0000) Steps 524(528.84) | Grad Norm 1.1990(7.2128) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 11.4916(11.4827) | Bit/dim 1.1598(1.1712) | Xent 0.0000(0.0000) | Loss 1.1598(1.1712) | Error 0.0000(0.0000) Steps 524(527.57) | Grad Norm 4.0387(6.2883) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 48.3719, Epoch Time 821.8527(745.7604), Bit/dim 1.1512(best: 1.1589), Xent 0.0000, Loss 1.1512, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 11.5157(11.4529) | Bit/dim 1.1587(1.1669) | Xent 0.0000(0.0000) | Loss 1.1587(1.1669) | Error 0.0000(0.0000) Steps 524(527.07) | Grad Norm 5.9773(5.5943) | Total Time 10.00(10.00)\n",
      "Iter 2920 | Time 11.6388(11.4579) | Bit/dim 1.1450(1.1654) | Xent 0.0000(0.0000) | Loss 1.1450(1.1654) | Error 0.0000(0.0000) Steps 524(527.22) | Grad Norm 3.8913(6.0175) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 12.2701(11.5199) | Bit/dim 1.1803(1.1647) | Xent 0.0000(0.0000) | Loss 1.1803(1.1647) | Error 0.0000(0.0000) Steps 548(528.76) | Grad Norm 8.6739(6.4690) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 11.9362(11.6657) | Bit/dim 1.2592(1.1737) | Xent 0.0000(0.0000) | Loss 1.2592(1.1737) | Error 0.0000(0.0000) Steps 524(532.85) | Grad Norm 18.8945(9.4441) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 10.8316(11.6956) | Bit/dim 1.1951(1.1914) | Xent 0.0000(0.0000) | Loss 1.1951(1.1914) | Error 0.0000(0.0000) Steps 518(534.30) | Grad Norm 7.8403(10.8146) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 11.3690(11.6245) | Bit/dim 1.3010(1.2190) | Xent 0.0000(0.0000) | Loss 1.3010(1.2190) | Error 0.0000(0.0000) Steps 536(533.42) | Grad Norm 38.7899(13.2691) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 11.4177(11.5082) | Bit/dim 1.2067(1.2298) | Xent 0.0000(0.0000) | Loss 1.2067(1.2298) | Error 0.0000(0.0000) Steps 530(530.47) | Grad Norm 6.1607(13.0716) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 46.3770, Epoch Time 825.9806(748.1670), Bit/dim 1.2183(best: 1.1512), Xent 0.0000, Loss 1.2183, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 11.5280(11.4301) | Bit/dim 1.1871(1.2216) | Xent 0.0000(0.0000) | Loss 1.1871(1.2216) | Error 0.0000(0.0000) Steps 524(526.80) | Grad Norm 2.4514(11.6138) | Total Time 10.00(10.00)\n",
      "Iter 2990 | Time 11.1743(11.4101) | Bit/dim 1.1781(1.2103) | Xent 0.0000(0.0000) | Loss 1.1781(1.2103) | Error 0.0000(0.0000) Steps 524(526.87) | Grad Norm 2.8574(9.8613) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 11.6343(11.4307) | Bit/dim 1.1562(1.1981) | Xent 0.0000(0.0000) | Loss 1.1562(1.1981) | Error 0.0000(0.0000) Steps 530(527.46) | Grad Norm 6.4040(8.5518) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 11.9616(11.4497) | Bit/dim 1.1669(1.1879) | Xent 0.0000(0.0000) | Loss 1.1669(1.1879) | Error 0.0000(0.0000) Steps 524(527.18) | Grad Norm 7.3282(7.7088) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 11.2400(11.4247) | Bit/dim 1.1592(1.1791) | Xent 0.0000(0.0000) | Loss 1.1592(1.1791) | Error 0.0000(0.0000) Steps 524(526.34) | Grad Norm 1.6285(6.4897) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 11.6798(11.4458) | Bit/dim 1.1508(1.1723) | Xent 0.0000(0.0000) | Loss 1.1508(1.1723) | Error 0.0000(0.0000) Steps 524(525.88) | Grad Norm 3.4503(5.9212) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 48.9102, Epoch Time 816.1783(750.2073), Bit/dim 1.1490(best: 1.1512), Xent 0.0000, Loss 1.1490, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 11.3944(11.4490) | Bit/dim 1.1589(1.1672) | Xent 0.0000(0.0000) | Loss 1.1589(1.1672) | Error 0.0000(0.0000) Steps 524(525.53) | Grad Norm 4.2233(5.4947) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 11.1306(11.4725) | Bit/dim 1.1600(1.1653) | Xent 0.0000(0.0000) | Loss 1.1600(1.1653) | Error 0.0000(0.0000) Steps 524(526.28) | Grad Norm 4.3895(5.9595) | Total Time 10.00(10.00)\n",
      "Iter 3060 | Time 11.5984(11.5630) | Bit/dim 1.3825(1.1889) | Xent 0.0000(0.0000) | Loss 1.3825(1.1889) | Error 0.0000(0.0000) Steps 536(528.95) | Grad Norm 10.8912(9.3411) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 11.7465(11.6765) | Bit/dim 1.1997(1.2104) | Xent 0.0000(0.0000) | Loss 1.1997(1.2104) | Error 0.0000(0.0000) Steps 524(531.99) | Grad Norm 3.2229(9.0289) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 11.3224(11.7100) | Bit/dim 1.1692(1.2035) | Xent 0.0000(0.0000) | Loss 1.1692(1.2035) | Error 0.0000(0.0000) Steps 524(533.82) | Grad Norm 4.3603(8.3379) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 11.7259(11.6558) | Bit/dim 1.1558(1.1940) | Xent 0.0000(0.0000) | Loss 1.1558(1.1940) | Error 0.0000(0.0000) Steps 530(532.89) | Grad Norm 2.0864(6.8079) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 11.2709(11.5592) | Bit/dim 1.1545(1.1841) | Xent 0.0000(0.0000) | Loss 1.1545(1.1841) | Error 0.0000(0.0000) Steps 524(529.87) | Grad Norm 3.3856(5.6240) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 48.9589, Epoch Time 832.5191(752.6767), Bit/dim 1.1483(best: 1.1490), Xent 0.0000, Loss 1.1483, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 11.8130(11.5658) | Bit/dim 1.1556(1.1758) | Xent 0.0000(0.0000) | Loss 1.1556(1.1758) | Error 0.0000(0.0000) Steps 530(529.44) | Grad Norm 6.6417(5.1100) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 11.4064(11.5232) | Bit/dim 1.1482(1.1689) | Xent 0.0000(0.0000) | Loss 1.1482(1.1689) | Error 0.0000(0.0000) Steps 524(528.17) | Grad Norm 5.0552(4.5307) | Total Time 10.00(10.00)\n",
      "Iter 3130 | Time 11.5854(11.5375) | Bit/dim 1.1546(1.1656) | Xent 0.0000(0.0000) | Loss 1.1546(1.1656) | Error 0.0000(0.0000) Steps 530(527.56) | Grad Norm 4.3470(5.1909) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 11.1571(11.4995) | Bit/dim 1.1511(1.1637) | Xent 0.0000(0.0000) | Loss 1.1511(1.1637) | Error 0.0000(0.0000) Steps 524(527.11) | Grad Norm 7.8091(7.2645) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 12.3834(11.6207) | Bit/dim 1.2275(1.1902) | Xent 0.0000(0.0000) | Loss 1.2275(1.1902) | Error 0.0000(0.0000) Steps 548(530.96) | Grad Norm 7.1196(9.5053) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 12.0858(11.5077) | Bit/dim 1.2864(1.2099) | Xent 0.0000(0.0000) | Loss 1.2864(1.2099) | Error 0.0000(0.0000) Steps 560(528.48) | Grad Norm 8.6755(10.8934) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 50.0169, Epoch Time 825.2685(754.8544), Bit/dim 1.2412(best: 1.1483), Xent 0.0000, Loss 1.2412, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 12.1917(11.5193) | Bit/dim 1.2006(1.2198) | Xent 0.0000(0.0000) | Loss 1.2006(1.2198) | Error 0.0000(0.0000) Steps 542(528.05) | Grad Norm 6.9749(11.8392) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 11.3563(11.4101) | Bit/dim 1.1820(1.2153) | Xent 0.0000(0.0000) | Loss 1.1820(1.2153) | Error 0.0000(0.0000) Steps 530(524.26) | Grad Norm 14.4179(11.7875) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 11.7433(11.3820) | Bit/dim 1.1735(1.2043) | Xent 0.0000(0.0000) | Loss 1.1735(1.2043) | Error 0.0000(0.0000) Steps 536(523.87) | Grad Norm 8.9617(10.2642) | Total Time 10.00(10.00)\n",
      "Iter 3200 | Time 10.7877(11.3152) | Bit/dim 1.1575(1.1921) | Xent 0.0000(0.0000) | Loss 1.1575(1.1921) | Error 0.0000(0.0000) Steps 512(521.89) | Grad Norm 7.1411(8.8234) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 10.8954(11.2983) | Bit/dim 1.1544(1.1820) | Xent 0.0000(0.0000) | Loss 1.1544(1.1820) | Error 0.0000(0.0000) Steps 524(521.97) | Grad Norm 5.4354(7.8281) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 11.1219(11.2733) | Bit/dim 1.1465(1.1728) | Xent 0.0000(0.0000) | Loss 1.1465(1.1728) | Error 0.0000(0.0000) Steps 524(521.91) | Grad Norm 7.1115(7.0890) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 11.3732(11.2863) | Bit/dim 1.1411(1.1658) | Xent 0.0000(0.0000) | Loss 1.1411(1.1658) | Error 0.0000(0.0000) Steps 524(522.49) | Grad Norm 7.4043(6.4063) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 48.7881, Epoch Time 806.3303(756.3987), Bit/dim 1.1413(best: 1.1483), Xent 0.0000, Loss 1.1413, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 10.8639(11.3005) | Bit/dim 1.1536(1.1620) | Xent 0.0000(0.0000) | Loss 1.1536(1.1620) | Error 0.0000(0.0000) Steps 518(522.72) | Grad Norm 9.2418(6.4995) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 10.9042(11.3130) | Bit/dim 1.1556(1.1577) | Xent 0.0000(0.0000) | Loss 1.1556(1.1577) | Error 0.0000(0.0000) Steps 518(522.87) | Grad Norm 11.4547(6.8821) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 11.6609(11.3306) | Bit/dim 1.1509(1.1544) | Xent 0.0000(0.0000) | Loss 1.1509(1.1544) | Error 0.0000(0.0000) Steps 530(523.49) | Grad Norm 6.0165(6.4651) | Total Time 10.00(10.00)\n",
      "Iter 3270 | Time 12.8167(11.4548) | Bit/dim 1.2463(1.1675) | Xent 0.0000(0.0000) | Loss 1.2463(1.1675) | Error 0.0000(0.0000) Steps 578(527.43) | Grad Norm 30.1413(9.3209) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 11.3618(11.5040) | Bit/dim 1.2617(1.1918) | Xent 0.0000(0.0000) | Loss 1.2617(1.1918) | Error 0.0000(0.0000) Steps 512(528.14) | Grad Norm 17.7366(11.9357) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 10.7789(11.4968) | Bit/dim 1.2803(1.2050) | Xent 0.0000(0.0000) | Loss 1.2803(1.2050) | Error 0.0000(0.0000) Steps 512(527.49) | Grad Norm 12.4374(13.0009) | Total Time 10.00(10.00)\n",
      "Iter 3300 | Time 10.7511(11.4587) | Bit/dim 1.1828(1.2062) | Xent 0.0000(0.0000) | Loss 1.1828(1.2062) | Error 0.0000(0.0000) Steps 512(526.03) | Grad Norm 5.1343(12.2905) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 50.0375, Epoch Time 823.6960(758.4176), Bit/dim 1.2021(best: 1.1413), Xent 0.0000, Loss 1.2021, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 10.8542(11.4397) | Bit/dim 1.1679(1.2001) | Xent 0.0000(0.0000) | Loss 1.1679(1.2001) | Error 0.0000(0.0000) Steps 512(526.53) | Grad Norm 9.3140(12.1524) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 10.9585(11.3608) | Bit/dim 1.1630(1.1901) | Xent 0.0000(0.0000) | Loss 1.1630(1.1901) | Error 0.0000(0.0000) Steps 512(524.39) | Grad Norm 10.6989(11.2762) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 10.9942(11.2839) | Bit/dim 1.1493(1.1804) | Xent 0.0000(0.0000) | Loss 1.1493(1.1804) | Error 0.0000(0.0000) Steps 518(523.43) | Grad Norm 3.9581(10.2060) | Total Time 10.00(10.00)\n",
      "Iter 3340 | Time 11.2410(11.2838) | Bit/dim 1.1430(1.1721) | Xent 0.0000(0.0000) | Loss 1.1430(1.1721) | Error 0.0000(0.0000) Steps 524(522.96) | Grad Norm 8.5606(8.9586) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 11.6939(11.3141) | Bit/dim 1.2040(1.1713) | Xent 0.0000(0.0000) | Loss 1.2040(1.1713) | Error 0.0000(0.0000) Steps 518(523.54) | Grad Norm 16.8866(10.1846) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 11.0545(11.3838) | Bit/dim 1.1761(1.1794) | Xent 0.0000(0.0000) | Loss 1.1761(1.1794) | Error 0.0000(0.0000) Steps 524(524.75) | Grad Norm 5.5595(11.3559) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 45.2313, Epoch Time 811.4965(760.0100), Bit/dim 1.2248(best: 1.1413), Xent 0.0000, Loss 1.2248, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 12.4663(11.5133) | Bit/dim 1.2348(1.1878) | Xent 0.0000(0.0000) | Loss 1.2348(1.1878) | Error 0.0000(0.0000) Steps 566(528.17) | Grad Norm 23.0343(12.8582) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 10.5336(11.4605) | Bit/dim 1.1830(1.1881) | Xent 0.0000(0.0000) | Loss 1.1830(1.1881) | Error 0.0000(0.0000) Steps 500(526.19) | Grad Norm 12.7300(13.0638) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 11.0174(11.3830) | Bit/dim 1.1443(1.1829) | Xent 0.0000(0.0000) | Loss 1.1443(1.1829) | Error 0.0000(0.0000) Steps 524(525.40) | Grad Norm 2.6408(12.3380) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 10.8601(11.3075) | Bit/dim 1.1732(1.1788) | Xent 0.0000(0.0000) | Loss 1.1732(1.1788) | Error 0.0000(0.0000) Steps 512(524.62) | Grad Norm 9.5106(12.1616) | Total Time 10.00(10.00)\n",
      "Iter 3410 | Time 10.8940(11.2839) | Bit/dim 1.1439(1.1716) | Xent 0.0000(0.0000) | Loss 1.1439(1.1716) | Error 0.0000(0.0000) Steps 518(523.64) | Grad Norm 6.5404(10.8874) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 11.3156(11.2519) | Bit/dim 1.1495(1.1657) | Xent 0.0000(0.0000) | Loss 1.1495(1.1657) | Error 0.0000(0.0000) Steps 524(522.84) | Grad Norm 4.1586(9.8361) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 11.3617(11.2591) | Bit/dim 1.1609(1.1619) | Xent 0.0000(0.0000) | Loss 1.1609(1.1619) | Error 0.0000(0.0000) Steps 524(522.68) | Grad Norm 18.9486(9.7563) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 51.1520, Epoch Time 807.6887(761.4404), Bit/dim 1.1754(best: 1.1413), Xent 0.0000, Loss 1.1754, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 11.1560(11.2864) | Bit/dim 1.1446(1.1618) | Xent 0.0000(0.0000) | Loss 1.1446(1.1618) | Error 0.0000(0.0000) Steps 530(524.76) | Grad Norm 10.6273(10.0870) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 11.2092(11.2952) | Bit/dim 1.1458(1.1611) | Xent 0.0000(0.0000) | Loss 1.1458(1.1611) | Error 0.0000(0.0000) Steps 530(524.69) | Grad Norm 2.0011(10.5955) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 11.4913(11.3123) | Bit/dim 1.1578(1.1594) | Xent 0.0000(0.0000) | Loss 1.1578(1.1594) | Error 0.0000(0.0000) Steps 530(524.45) | Grad Norm 9.5004(10.7553) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 11.2313(11.3505) | Bit/dim 1.1470(1.1552) | Xent 0.0000(0.0000) | Loss 1.1470(1.1552) | Error 0.0000(0.0000) Steps 524(523.88) | Grad Norm 2.9767(9.9898) | Total Time 10.00(10.00)\n",
      "Iter 3480 | Time 12.9378(11.4080) | Bit/dim 1.2040(1.1548) | Xent 0.0000(0.0000) | Loss 1.2040(1.1548) | Error 0.0000(0.0000) Steps 572(525.06) | Grad Norm 28.1197(10.5350) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 10.7856(11.4312) | Bit/dim 1.1569(1.1601) | Xent 0.0000(0.0000) | Loss 1.1569(1.1601) | Error 0.0000(0.0000) Steps 512(526.16) | Grad Norm 9.2354(11.3176) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 49.2862, Epoch Time 820.6380(763.2163), Bit/dim 1.1624(best: 1.1413), Xent 0.0000, Loss 1.1624, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 10.5881(11.4687) | Bit/dim 1.1609(1.1654) | Xent 0.0000(0.0000) | Loss 1.1609(1.1654) | Error 0.0000(0.0000) Steps 506(526.10) | Grad Norm 11.0772(12.2478) | Total Time 10.00(10.00)\n",
      "Iter 3510 | Time 11.4019(11.3881) | Bit/dim 1.1478(1.1636) | Xent 0.0000(0.0000) | Loss 1.1478(1.1636) | Error 0.0000(0.0000) Steps 524(525.15) | Grad Norm 9.9561(11.8362) | Total Time 10.00(10.00)\n",
      "Iter 3520 | Time 12.7973(11.3646) | Bit/dim 1.1616(1.1635) | Xent 0.0000(0.0000) | Loss 1.1616(1.1635) | Error 0.0000(0.0000) Steps 572(525.34) | Grad Norm 21.1787(11.9825) | Total Time 10.00(10.00)\n",
      "Iter 3530 | Time 11.1094(11.3818) | Bit/dim 1.1475(1.1619) | Xent 0.0000(0.0000) | Loss 1.1475(1.1619) | Error 0.0000(0.0000) Steps 530(525.67) | Grad Norm 2.8805(11.6394) | Total Time 10.00(10.00)\n",
      "Iter 3540 | Time 11.1173(11.3350) | Bit/dim 1.1418(1.1602) | Xent 0.0000(0.0000) | Loss 1.1418(1.1602) | Error 0.0000(0.0000) Steps 518(524.72) | Grad Norm 9.4477(11.5082) | Total Time 10.00(10.00)\n",
      "Iter 3550 | Time 10.5936(11.2925) | Bit/dim 1.1442(1.1575) | Xent 0.0000(0.0000) | Loss 1.1442(1.1575) | Error 0.0000(0.0000) Steps 512(523.81) | Grad Norm 5.9630(10.7037) | Total Time 10.00(10.00)\n",
      "Iter 3560 | Time 11.3661(11.2882) | Bit/dim 1.1439(1.1550) | Xent 0.0000(0.0000) | Loss 1.1439(1.1550) | Error 0.0000(0.0000) Steps 524(523.75) | Grad Norm 10.3945(10.9790) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 49.2573, Epoch Time 807.1776(764.5351), Bit/dim 1.1328(best: 1.1413), Xent 0.0000, Loss 1.1328, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 11.1739(11.2783) | Bit/dim 1.1372(1.1512) | Xent 0.0000(0.0000) | Loss 1.1372(1.1512) | Error 0.0000(0.0000) Steps 518(523.32) | Grad Norm 8.7942(9.6185) | Total Time 10.00(10.00)\n",
      "Iter 3580 | Time 11.3184(11.2512) | Bit/dim 1.1369(1.1496) | Xent 0.0000(0.0000) | Loss 1.1369(1.1496) | Error 0.0000(0.0000) Steps 524(523.59) | Grad Norm 1.2130(8.8724) | Total Time 10.00(10.00)\n",
      "Iter 3590 | Time 11.1699(11.3692) | Bit/dim 1.1786(1.1558) | Xent 0.0000(0.0000) | Loss 1.1786(1.1558) | Error 0.0000(0.0000) Steps 530(527.01) | Grad Norm 11.7667(10.7045) | Total Time 10.00(10.00)\n",
      "Iter 3600 | Time 12.3757(11.3979) | Bit/dim 1.2098(1.1666) | Xent 0.0000(0.0000) | Loss 1.2098(1.1666) | Error 0.0000(0.0000) Steps 560(528.47) | Grad Norm 25.2634(11.8773) | Total Time 10.00(10.00)\n",
      "Iter 3610 | Time 10.6714(11.3756) | Bit/dim 1.1934(1.1713) | Xent 0.0000(0.0000) | Loss 1.1934(1.1713) | Error 0.0000(0.0000) Steps 512(527.43) | Grad Norm 12.3249(12.1038) | Total Time 10.00(10.00)\n",
      "Iter 3620 | Time 12.6189(11.3987) | Bit/dim 1.1946(1.1742) | Xent 0.0000(0.0000) | Loss 1.1946(1.1742) | Error 0.0000(0.0000) Steps 560(527.58) | Grad Norm 20.7205(12.8941) | Total Time 10.00(10.00)\n",
      "Iter 3630 | Time 10.9324(11.3513) | Bit/dim 1.1555(1.1730) | Xent 0.0000(0.0000) | Loss 1.1555(1.1730) | Error 0.0000(0.0000) Steps 512(527.00) | Grad Norm 12.5214(12.5423) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 48.9705, Epoch Time 815.2867(766.0577), Bit/dim 1.1411(best: 1.1328), Xent 0.0000, Loss 1.1411, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 11.4170(11.3630) | Bit/dim 1.1253(1.1680) | Xent 0.0000(0.0000) | Loss 1.1253(1.1680) | Error 0.0000(0.0000) Steps 530(526.88) | Grad Norm 3.0350(11.7141) | Total Time 10.00(10.00)\n",
      "Iter 3650 | Time 11.9287(11.3714) | Bit/dim 1.1574(1.1626) | Xent 0.0000(0.0000) | Loss 1.1574(1.1626) | Error 0.0000(0.0000) Steps 524(525.38) | Grad Norm 11.6461(10.7628) | Total Time 10.00(10.00)\n",
      "Iter 3660 | Time 11.5665(11.3256) | Bit/dim 1.1288(1.1573) | Xent 0.0000(0.0000) | Loss 1.1288(1.1573) | Error 0.0000(0.0000) Steps 524(524.27) | Grad Norm 4.4353(9.8187) | Total Time 10.00(10.00)\n",
      "Iter 3670 | Time 11.2657(11.3321) | Bit/dim 1.1393(1.1525) | Xent 0.0000(0.0000) | Loss 1.1393(1.1525) | Error 0.0000(0.0000) Steps 524(524.19) | Grad Norm 0.5921(8.5233) | Total Time 10.00(10.00)\n",
      "Iter 3680 | Time 13.3372(11.3940) | Bit/dim 1.1858(1.1505) | Xent 0.0000(0.0000) | Loss 1.1858(1.1505) | Error 0.0000(0.0000) Steps 578(526.39) | Grad Norm 28.9852(9.3104) | Total Time 10.00(10.00)\n",
      "Iter 3690 | Time 11.0050(11.4411) | Bit/dim 1.2111(1.1596) | Xent 0.0000(0.0000) | Loss 1.2111(1.1596) | Error 0.0000(0.0000) Steps 512(528.86) | Grad Norm 15.7489(11.0072) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 50.8726, Epoch Time 820.1579(767.6807), Bit/dim 1.1983(best: 1.1328), Xent 0.0000, Loss 1.1983, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 11.4634(11.4641) | Bit/dim 1.1695(1.1676) | Xent 0.0000(0.0000) | Loss 1.1695(1.1676) | Error 0.0000(0.0000) Steps 524(528.90) | Grad Norm 8.2788(11.7924) | Total Time 10.00(10.00)\n",
      "Iter 3710 | Time 11.8364(11.4588) | Bit/dim 1.1793(1.1679) | Xent 0.0000(0.0000) | Loss 1.1793(1.1679) | Error 0.0000(0.0000) Steps 542(528.76) | Grad Norm 22.3799(12.0261) | Total Time 10.00(10.00)\n",
      "Iter 3720 | Time 11.2940(11.4027) | Bit/dim 1.1415(1.1642) | Xent 0.0000(0.0000) | Loss 1.1415(1.1642) | Error 0.0000(0.0000) Steps 518(526.91) | Grad Norm 6.3727(11.3828) | Total Time 10.00(10.00)\n",
      "Iter 3730 | Time 10.7455(11.3611) | Bit/dim 1.1333(1.1600) | Xent 0.0000(0.0000) | Loss 1.1333(1.1600) | Error 0.0000(0.0000) Steps 524(526.77) | Grad Norm 9.0878(10.6480) | Total Time 10.00(10.00)\n",
      "Iter 3740 | Time 11.4307(11.3163) | Bit/dim 1.1377(1.1551) | Xent 0.0000(0.0000) | Loss 1.1377(1.1551) | Error 0.0000(0.0000) Steps 524(526.47) | Grad Norm 5.9619(9.8596) | Total Time 10.00(10.00)\n",
      "Iter 3750 | Time 11.4249(11.3062) | Bit/dim 1.1353(1.1508) | Xent 0.0000(0.0000) | Loss 1.1353(1.1508) | Error 0.0000(0.0000) Steps 530(526.05) | Grad Norm 12.7612(9.5436) | Total Time 10.00(10.00)\n",
      "Iter 3760 | Time 11.4650(11.3588) | Bit/dim 1.1434(1.1489) | Xent 0.0000(0.0000) | Loss 1.1434(1.1489) | Error 0.0000(0.0000) Steps 524(526.01) | Grad Norm 10.9163(9.8808) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 48.6871, Epoch Time 810.6912(768.9710), Bit/dim 1.1289(best: 1.1328), Xent 0.0000, Loss 1.1289, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 11.4750(11.3854) | Bit/dim 1.1268(1.1451) | Xent 0.0000(0.0000) | Loss 1.1268(1.1451) | Error 0.0000(0.0000) Steps 524(526.56) | Grad Norm 4.6187(9.1669) | Total Time 10.00(10.00)\n",
      "Iter 3780 | Time 11.3599(11.3684) | Bit/dim 1.1268(1.1413) | Xent 0.0000(0.0000) | Loss 1.1268(1.1413) | Error 0.0000(0.0000) Steps 530(527.16) | Grad Norm 2.6249(7.8953) | Total Time 10.00(10.00)\n",
      "Iter 3790 | Time 11.6326(11.3748) | Bit/dim 1.1387(1.1387) | Xent 0.0000(0.0000) | Loss 1.1387(1.1387) | Error 0.0000(0.0000) Steps 530(528.19) | Grad Norm 1.2156(6.5298) | Total Time 10.00(10.00)\n",
      "Iter 3800 | Time 11.4132(11.3872) | Bit/dim 1.1244(1.1360) | Xent 0.0000(0.0000) | Loss 1.1244(1.1360) | Error 0.0000(0.0000) Steps 530(528.51) | Grad Norm 2.6052(5.2844) | Total Time 10.00(10.00)\n",
      "Iter 3810 | Time 11.6913(11.4332) | Bit/dim 1.1208(1.1333) | Xent 0.0000(0.0000) | Loss 1.1208(1.1333) | Error 0.0000(0.0000) Steps 530(528.90) | Grad Norm 0.9624(4.6142) | Total Time 10.00(10.00)\n",
      "Iter 3820 | Time 11.3854(11.4541) | Bit/dim 1.1345(1.1328) | Xent 0.0000(0.0000) | Loss 1.1345(1.1328) | Error 0.0000(0.0000) Steps 530(529.02) | Grad Norm 4.2977(4.8922) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 48.6556, Epoch Time 820.2913(770.5106), Bit/dim 1.1221(best: 1.1289), Xent 0.0000, Loss 1.1221, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 11.9269(11.4723) | Bit/dim 1.1342(1.1322) | Xent 0.0000(0.0000) | Loss 1.1342(1.1322) | Error 0.0000(0.0000) Steps 530(528.82) | Grad Norm 1.2039(5.1271) | Total Time 10.00(10.00)\n",
      "Iter 3840 | Time 11.5262(11.4873) | Bit/dim 1.1401(1.1315) | Xent 0.0000(0.0000) | Loss 1.1401(1.1315) | Error 0.0000(0.0000) Steps 530(528.53) | Grad Norm 13.3272(5.8475) | Total Time 10.00(10.00)\n",
      "Iter 3850 | Time 12.1159(11.5341) | Bit/dim 1.2503(1.1521) | Xent 0.0000(0.0000) | Loss 1.2503(1.1521) | Error 0.0000(0.0000) Steps 548(530.08) | Grad Norm 7.7143(8.5243) | Total Time 10.00(10.00)\n",
      "Iter 3860 | Time 11.8427(11.5019) | Bit/dim 1.1779(1.1620) | Xent 0.0000(0.0000) | Loss 1.1779(1.1620) | Error 0.0000(0.0000) Steps 530(528.78) | Grad Norm 4.5732(8.8350) | Total Time 10.00(10.00)\n",
      "Iter 3870 | Time 11.0033(11.5067) | Bit/dim 1.2773(1.1757) | Xent 0.0000(0.0000) | Loss 1.2773(1.1757) | Error 0.0000(0.0000) Steps 512(528.02) | Grad Norm 13.8862(11.2388) | Total Time 10.00(10.00)\n",
      "Iter 3880 | Time 10.7066(11.5296) | Bit/dim 1.1652(1.1824) | Xent 0.0000(0.0000) | Loss 1.1652(1.1824) | Error 0.0000(0.0000) Steps 518(528.89) | Grad Norm 3.6651(10.4251) | Total Time 10.00(10.00)\n",
      "Iter 3890 | Time 10.9182(11.3807) | Bit/dim 1.2046(1.1860) | Xent 0.0000(0.0000) | Loss 1.2046(1.1860) | Error 0.0000(0.0000) Steps 506(525.10) | Grad Norm 15.5647(12.1855) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 47.3654, Epoch Time 818.3203(771.9449), Bit/dim 1.1544(best: 1.1221), Xent 0.0000, Loss 1.1544, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 11.9915(11.4053) | Bit/dim 1.1428(1.1813) | Xent 0.0000(0.0000) | Loss 1.1428(1.1813) | Error 0.0000(0.0000) Steps 536(525.63) | Grad Norm 12.2519(11.6711) | Total Time 10.00(10.00)\n",
      "Iter 3910 | Time 12.0856(11.3419) | Bit/dim 1.1307(1.1710) | Xent 0.0000(0.0000) | Loss 1.1307(1.1710) | Error 0.0000(0.0000) Steps 542(525.63) | Grad Norm 9.9296(10.8458) | Total Time 10.00(10.00)\n",
      "Iter 3920 | Time 11.4722(11.2911) | Bit/dim 1.1342(1.1623) | Xent 0.0000(0.0000) | Loss 1.1342(1.1623) | Error 0.0000(0.0000) Steps 536(525.26) | Grad Norm 2.0393(8.9095) | Total Time 10.00(10.00)\n",
      "Iter 3930 | Time 11.1151(11.2507) | Bit/dim 1.1322(1.1535) | Xent 0.0000(0.0000) | Loss 1.1322(1.1535) | Error 0.0000(0.0000) Steps 524(525.37) | Grad Norm 1.8990(7.0521) | Total Time 10.00(10.00)\n",
      "Iter 3940 | Time 11.4827(11.2840) | Bit/dim 1.1314(1.1465) | Xent 0.0000(0.0000) | Loss 1.1314(1.1465) | Error 0.0000(0.0000) Steps 524(525.69) | Grad Norm 1.6918(5.7327) | Total Time 10.00(10.00)\n",
      "Iter 3950 | Time 11.4075(11.3064) | Bit/dim 1.1237(1.1418) | Xent 0.0000(0.0000) | Loss 1.1237(1.1418) | Error 0.0000(0.0000) Steps 530(526.84) | Grad Norm 6.1760(5.0536) | Total Time 10.00(10.00)\n",
      "Iter 3960 | Time 11.3133(11.3273) | Bit/dim 1.1361(1.1368) | Xent 0.0000(0.0000) | Loss 1.1361(1.1368) | Error 0.0000(0.0000) Steps 524(527.01) | Grad Norm 2.4547(4.6674) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 48.9769, Epoch Time 808.9460(773.0549), Bit/dim 1.1192(best: 1.1221), Xent 0.0000, Loss 1.1192, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 11.1880(11.3192) | Bit/dim 1.1220(1.1333) | Xent 0.0000(0.0000) | Loss 1.1220(1.1333) | Error 0.0000(0.0000) Steps 530(528.59) | Grad Norm 4.9477(4.7319) | Total Time 10.00(10.00)\n",
      "Iter 3980 | Time 11.3752(11.3167) | Bit/dim 1.1322(1.1311) | Xent 0.0000(0.0000) | Loss 1.1322(1.1311) | Error 0.0000(0.0000) Steps 530(528.52) | Grad Norm 7.3840(5.4305) | Total Time 10.00(10.00)\n",
      "Iter 3990 | Time 12.9919(11.3303) | Bit/dim 1.2104(1.1347) | Xent 0.0000(0.0000) | Loss 1.2104(1.1347) | Error 0.0000(0.0000) Steps 578(529.86) | Grad Norm 37.7150(7.7678) | Total Time 10.00(10.00)\n",
      "Iter 4000 | Time 11.9889(11.4445) | Bit/dim 1.2018(1.1545) | Xent 0.0000(0.0000) | Loss 1.2018(1.1545) | Error 0.0000(0.0000) Steps 518(531.01) | Grad Norm 11.4107(9.8364) | Total Time 10.00(10.00)\n",
      "Iter 4010 | Time 11.2948(11.3999) | Bit/dim 1.2602(1.1894) | Xent 0.0000(0.0000) | Loss 1.2602(1.1894) | Error 0.0000(0.0000) Steps 524(529.22) | Grad Norm 4.9288(10.4181) | Total Time 10.00(10.00)\n",
      "Iter 4020 | Time 11.4706(11.3477) | Bit/dim 1.1754(1.1930) | Xent 0.0000(0.0000) | Loss 1.1754(1.1930) | Error 0.0000(0.0000) Steps 524(528.48) | Grad Norm 18.2323(9.7888) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 45.7447, Epoch Time 810.4175(774.1758), Bit/dim 1.1950(best: 1.1192), Xent 0.0000, Loss 1.1950, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 12.6059(11.3504) | Bit/dim 1.2218(1.1946) | Xent 0.0000(0.0000) | Loss 1.2218(1.1946) | Error 0.0000(0.0000) Steps 566(527.46) | Grad Norm 26.8715(11.6268) | Total Time 10.00(10.00)\n",
      "Iter 4040 | Time 10.8319(11.3091) | Bit/dim 1.1457(1.1874) | Xent 0.0000(0.0000) | Loss 1.1457(1.1874) | Error 0.0000(0.0000) Steps 518(527.08) | Grad Norm 10.0211(11.0990) | Total Time 10.00(10.00)\n",
      "Iter 4050 | Time 11.2182(11.2243) | Bit/dim 1.1387(1.1752) | Xent 0.0000(0.0000) | Loss 1.1387(1.1752) | Error 0.0000(0.0000) Steps 518(525.03) | Grad Norm 10.0950(10.5554) | Total Time 10.00(10.00)\n",
      "Iter 4060 | Time 11.1233(11.2365) | Bit/dim 1.1162(1.1646) | Xent 0.0000(0.0000) | Loss 1.1162(1.1646) | Error 0.0000(0.0000) Steps 524(525.98) | Grad Norm 5.5981(9.6148) | Total Time 10.00(10.00)\n",
      "Iter 4070 | Time 11.5703(11.2375) | Bit/dim 1.1160(1.1544) | Xent 0.0000(0.0000) | Loss 1.1160(1.1544) | Error 0.0000(0.0000) Steps 518(525.90) | Grad Norm 1.3807(8.2705) | Total Time 10.00(10.00)\n",
      "Iter 4080 | Time 12.2911(11.2890) | Bit/dim 1.1616(1.1519) | Xent 0.0000(0.0000) | Loss 1.1616(1.1519) | Error 0.0000(0.0000) Steps 560(527.45) | Grad Norm 14.7516(9.2785) | Total Time 10.00(10.00)\n",
      "Iter 4090 | Time 11.2079(11.3487) | Bit/dim 1.1891(1.1573) | Xent 0.0000(0.0000) | Loss 1.1891(1.1573) | Error 0.0000(0.0000) Steps 506(528.75) | Grad Norm 17.9011(10.5081) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 48.3148, Epoch Time 807.9215(775.1882), Bit/dim 1.1595(best: 1.1192), Xent 0.0000, Loss 1.1595, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 11.3677(11.3293) | Bit/dim 1.1401(1.1584) | Xent 0.0000(0.0000) | Loss 1.1401(1.1584) | Error 0.0000(0.0000) Steps 530(528.87) | Grad Norm 8.0151(11.0357) | Total Time 10.00(10.00)\n",
      "Iter 4110 | Time 11.3525(11.2601) | Bit/dim 1.1220(1.1507) | Xent 0.0000(0.0000) | Loss 1.1220(1.1507) | Error 0.0000(0.0000) Steps 524(527.34) | Grad Norm 2.5027(10.2633) | Total Time 10.00(10.00)\n",
      "Iter 4120 | Time 10.9501(11.2309) | Bit/dim 1.1310(1.1445) | Xent 0.0000(0.0000) | Loss 1.1310(1.1445) | Error 0.0000(0.0000) Steps 524(526.50) | Grad Norm 2.4163(9.0379) | Total Time 10.00(10.00)\n",
      "Iter 4130 | Time 11.4619(11.2513) | Bit/dim 1.1524(1.1461) | Xent 0.0000(0.0000) | Loss 1.1524(1.1461) | Error 0.0000(0.0000) Steps 530(526.82) | Grad Norm 8.2584(9.9387) | Total Time 10.00(10.00)\n",
      "Iter 4140 | Time 11.1612(11.3804) | Bit/dim 1.1750(1.1548) | Xent 0.0000(0.0000) | Loss 1.1750(1.1548) | Error 0.0000(0.0000) Steps 530(530.40) | Grad Norm 11.8673(11.8596) | Total Time 10.00(10.00)\n",
      "Iter 4150 | Time 10.9048(11.3919) | Bit/dim 1.1550(1.1583) | Xent 0.0000(0.0000) | Loss 1.1550(1.1583) | Error 0.0000(0.0000) Steps 524(530.49) | Grad Norm 10.1973(11.6340) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 48.3505, Epoch Time 809.1732(776.2077), Bit/dim 1.1345(best: 1.1192), Xent 0.0000, Loss 1.1345, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 10.9650(11.3138) | Bit/dim 1.1255(1.1529) | Xent 0.0000(0.0000) | Loss 1.1255(1.1529) | Error 0.0000(0.0000) Steps 518(528.41) | Grad Norm 5.4198(10.6693) | Total Time 10.00(10.00)\n",
      "Iter 4170 | Time 10.7627(11.2568) | Bit/dim 1.1265(1.1461) | Xent 0.0000(0.0000) | Loss 1.1265(1.1461) | Error 0.0000(0.0000) Steps 524(527.27) | Grad Norm 4.4145(9.1879) | Total Time 10.00(10.00)\n",
      "Iter 4180 | Time 11.4399(11.2261) | Bit/dim 1.1217(1.1399) | Xent 0.0000(0.0000) | Loss 1.1217(1.1399) | Error 0.0000(0.0000) Steps 524(526.44) | Grad Norm 1.6135(7.7108) | Total Time 10.00(10.00)\n",
      "Iter 4190 | Time 11.4434(11.2558) | Bit/dim 1.1236(1.1356) | Xent 0.0000(0.0000) | Loss 1.1236(1.1356) | Error 0.0000(0.0000) Steps 530(526.60) | Grad Norm 2.2776(7.0714) | Total Time 10.00(10.00)\n",
      "Iter 4200 | Time 11.6307(11.2928) | Bit/dim 1.1316(1.1325) | Xent 0.0000(0.0000) | Loss 1.1316(1.1325) | Error 0.0000(0.0000) Steps 530(526.08) | Grad Norm 3.4448(6.0670) | Total Time 10.00(10.00)\n",
      "Iter 4210 | Time 11.3166(11.3057) | Bit/dim 1.1210(1.1287) | Xent 0.0000(0.0000) | Loss 1.1210(1.1287) | Error 0.0000(0.0000) Steps 530(526.18) | Grad Norm 3.0083(4.9187) | Total Time 10.00(10.00)\n",
      "Iter 4220 | Time 11.4240(11.3502) | Bit/dim 1.1196(1.1260) | Xent 0.0000(0.0000) | Loss 1.1196(1.1260) | Error 0.0000(0.0000) Steps 530(527.03) | Grad Norm 2.4767(4.0512) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 48.2082, Epoch Time 807.7139(777.1529), Bit/dim 1.1132(best: 1.1192), Xent 0.0000, Loss 1.1132, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4230 | Time 11.3890(11.3218) | Bit/dim 1.1109(1.1233) | Xent 0.0000(0.0000) | Loss 1.1109(1.1233) | Error 0.0000(0.0000) Steps 530(526.91) | Grad Norm 4.5203(4.1708) | Total Time 10.00(10.00)\n",
      "Iter 4240 | Time 10.9558(11.2892) | Bit/dim 1.1500(1.1234) | Xent 0.0000(0.0000) | Loss 1.1500(1.1234) | Error 0.0000(0.0000) Steps 524(526.80) | Grad Norm 16.1899(5.4482) | Total Time 10.00(10.00)\n",
      "Iter 4250 | Time 12.8939(11.4365) | Bit/dim 1.2379(1.1452) | Xent 0.0000(0.0000) | Loss 1.2379(1.1452) | Error 0.0000(0.0000) Steps 572(530.69) | Grad Norm 39.3460(8.6741) | Total Time 10.00(10.00)\n",
      "Iter 4260 | Time 11.4043(11.4317) | Bit/dim 1.1943(1.1654) | Xent 0.0000(0.0000) | Loss 1.1943(1.1654) | Error 0.0000(0.0000) Steps 530(529.23) | Grad Norm 14.1403(9.2044) | Total Time 10.00(10.00)\n",
      "Iter 4270 | Time 11.0043(11.3413) | Bit/dim 1.1595(1.1641) | Xent 0.0000(0.0000) | Loss 1.1595(1.1641) | Error 0.0000(0.0000) Steps 524(526.89) | Grad Norm 20.8909(10.0551) | Total Time 10.00(10.00)\n",
      "Iter 4280 | Time 11.3982(11.3218) | Bit/dim 1.1435(1.1595) | Xent 0.0000(0.0000) | Loss 1.1435(1.1595) | Error 0.0000(0.0000) Steps 530(525.75) | Grad Norm 11.5369(10.4218) | Total Time 10.00(10.00)\n",
      "Iter 4290 | Time 12.3569(11.2912) | Bit/dim 1.1539(1.1543) | Xent 0.0000(0.0000) | Loss 1.1539(1.1543) | Error 0.0000(0.0000) Steps 548(525.81) | Grad Norm 23.4927(10.3431) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 46.5860, Epoch Time 809.5064(778.1235), Bit/dim 1.1435(best: 1.1132), Xent 0.0000, Loss 1.1435, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4300 | Time 11.1080(11.2376) | Bit/dim 1.1465(1.1483) | Xent 0.0000(0.0000) | Loss 1.1465(1.1483) | Error 0.0000(0.0000) Steps 530(525.67) | Grad Norm 17.9021(10.2674) | Total Time 10.00(10.00)\n",
      "Iter 4310 | Time 11.3250(11.2799) | Bit/dim 1.1241(1.1440) | Xent 0.0000(0.0000) | Loss 1.1241(1.1440) | Error 0.0000(0.0000) Steps 530(526.15) | Grad Norm 12.5211(10.2563) | Total Time 10.00(10.00)\n",
      "Iter 4320 | Time 10.9995(11.2775) | Bit/dim 1.1277(1.1414) | Xent 0.0000(0.0000) | Loss 1.1277(1.1414) | Error 0.0000(0.0000) Steps 518(525.45) | Grad Norm 1.8448(10.3828) | Total Time 10.00(10.00)\n",
      "Iter 4330 | Time 10.7020(11.2232) | Bit/dim 1.1256(1.1353) | Xent 0.0000(0.0000) | Loss 1.1256(1.1353) | Error 0.0000(0.0000) Steps 518(524.72) | Grad Norm 0.6974(8.5523) | Total Time 10.00(10.00)\n",
      "Iter 4340 | Time 11.3394(11.2106) | Bit/dim 1.0968(1.1295) | Xent 0.0000(0.0000) | Loss 1.0968(1.1295) | Error 0.0000(0.0000) Steps 524(524.10) | Grad Norm 0.8675(6.9119) | Total Time 10.00(10.00)\n",
      "Iter 4350 | Time 11.4692(11.2199) | Bit/dim 1.1027(1.1259) | Xent 0.0000(0.0000) | Loss 1.1027(1.1259) | Error 0.0000(0.0000) Steps 524(525.03) | Grad Norm 1.6415(5.6152) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 49.1827, Epoch Time 804.0939(778.9026), Bit/dim 1.1087(best: 1.1132), Xent 0.0000, Loss 1.1087, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4360 | Time 11.2853(11.2262) | Bit/dim 1.1066(1.1238) | Xent 0.0000(0.0000) | Loss 1.1066(1.1238) | Error 0.0000(0.0000) Steps 530(526.35) | Grad Norm 2.1187(4.8643) | Total Time 10.00(10.00)\n",
      "Iter 4370 | Time 11.0339(11.2190) | Bit/dim 1.1107(1.1221) | Xent 0.0000(0.0000) | Loss 1.1107(1.1221) | Error 0.0000(0.0000) Steps 530(527.14) | Grad Norm 11.4201(5.0027) | Total Time 10.00(10.00)\n",
      "Iter 4380 | Time 11.4215(11.2497) | Bit/dim 1.1091(1.1206) | Xent 0.0000(0.0000) | Loss 1.1091(1.1206) | Error 0.0000(0.0000) Steps 536(528.54) | Grad Norm 10.6435(5.5620) | Total Time 10.00(10.00)\n",
      "Iter 4390 | Time 11.1770(11.3153) | Bit/dim 1.1213(1.1265) | Xent 0.0000(0.0000) | Loss 1.1213(1.1265) | Error 0.0000(0.0000) Steps 524(529.71) | Grad Norm 10.3022(7.7120) | Total Time 10.00(10.00)\n",
      "Iter 4400 | Time 11.2601(11.3565) | Bit/dim 1.1945(1.1610) | Xent 0.0000(0.0000) | Loss 1.1945(1.1610) | Error 0.0000(0.0000) Steps 518(529.55) | Grad Norm 5.7402(8.7437) | Total Time 10.00(10.00)\n",
      "Iter 4410 | Time 10.9061(11.3064) | Bit/dim 1.1601(1.1615) | Xent 0.0000(0.0000) | Loss 1.1601(1.1615) | Error 0.0000(0.0000) Steps 512(527.57) | Grad Norm 6.9855(7.9417) | Total Time 10.00(10.00)\n",
      "Iter 4420 | Time 10.6939(11.2256) | Bit/dim 1.1286(1.1546) | Xent 0.0000(0.0000) | Loss 1.1286(1.1546) | Error 0.0000(0.0000) Steps 524(526.06) | Grad Norm 9.5461(7.9962) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 47.0944, Epoch Time 806.8481(779.7410), Bit/dim 1.1347(best: 1.1087), Xent 0.0000, Loss 1.1347, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4430 | Time 11.4206(11.2566) | Bit/dim 1.1387(1.1508) | Xent 0.0000(0.0000) | Loss 1.1387(1.1508) | Error 0.0000(0.0000) Steps 524(525.71) | Grad Norm 13.3390(8.9118) | Total Time 10.00(10.00)\n",
      "Iter 4440 | Time 10.9775(11.2801) | Bit/dim 1.1228(1.1493) | Xent 0.0000(0.0000) | Loss 1.1228(1.1493) | Error 0.0000(0.0000) Steps 518(525.60) | Grad Norm 10.5280(10.3957) | Total Time 10.00(10.00)\n",
      "Iter 4450 | Time 11.1734(11.2591) | Bit/dim 1.1269(1.1435) | Xent 0.0000(0.0000) | Loss 1.1269(1.1435) | Error 0.0000(0.0000) Steps 524(525.94) | Grad Norm 9.0936(10.0419) | Total Time 10.00(10.00)\n",
      "Iter 4460 | Time 11.3443(11.3419) | Bit/dim 1.1536(1.1465) | Xent 0.0000(0.0000) | Loss 1.1536(1.1465) | Error 0.0000(0.0000) Steps 530(528.37) | Grad Norm 11.7663(11.4739) | Total Time 10.00(10.00)\n",
      "Iter 4470 | Time 12.6008(11.3878) | Bit/dim 1.1933(1.1505) | Xent 0.0000(0.0000) | Loss 1.1933(1.1505) | Error 0.0000(0.0000) Steps 572(530.05) | Grad Norm 34.3466(12.8092) | Total Time 10.00(10.00)\n",
      "Iter 4480 | Time 11.0757(11.4169) | Bit/dim 1.1600(1.1569) | Xent 0.0000(0.0000) | Loss 1.1600(1.1569) | Error 0.0000(0.0000) Steps 530(530.48) | Grad Norm 6.8312(12.7220) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 47.3767, Epoch Time 812.3503(780.7193), Bit/dim 1.1250(best: 1.1087), Xent 0.0000, Loss 1.1250, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4490 | Time 10.7918(11.2991) | Bit/dim 1.1234(1.1512) | Xent 0.0000(0.0000) | Loss 1.1234(1.1512) | Error 0.0000(0.0000) Steps 512(528.22) | Grad Norm 4.1075(11.4974) | Total Time 10.00(10.00)\n",
      "Iter 4500 | Time 11.2462(11.2468) | Bit/dim 1.1180(1.1437) | Xent 0.0000(0.0000) | Loss 1.1180(1.1437) | Error 0.0000(0.0000) Steps 524(527.12) | Grad Norm 4.8304(10.6036) | Total Time 10.00(10.00)\n",
      "Iter 4510 | Time 11.6212(11.2037) | Bit/dim 1.0968(1.1361) | Xent 0.0000(0.0000) | Loss 1.0968(1.1361) | Error 0.0000(0.0000) Steps 524(526.60) | Grad Norm 1.6374(8.6983) | Total Time 10.00(10.00)\n",
      "Iter 4520 | Time 11.0195(11.1808) | Bit/dim 1.1087(1.1299) | Xent 0.0000(0.0000) | Loss 1.1087(1.1299) | Error 0.0000(0.0000) Steps 524(526.08) | Grad Norm 0.5030(6.9235) | Total Time 10.00(10.00)\n",
      "Iter 4530 | Time 11.0338(11.1788) | Bit/dim 1.1023(1.1252) | Xent 0.0000(0.0000) | Loss 1.1023(1.1252) | Error 0.0000(0.0000) Steps 524(526.17) | Grad Norm 0.6609(5.6715) | Total Time 10.00(10.00)\n",
      "Iter 4540 | Time 11.3299(11.1986) | Bit/dim 1.1325(1.1221) | Xent 0.0000(0.0000) | Loss 1.1325(1.1221) | Error 0.0000(0.0000) Steps 536(527.04) | Grad Norm 6.3061(5.3736) | Total Time 10.00(10.00)\n",
      "Iter 4550 | Time 11.5026(11.2065) | Bit/dim 1.1066(1.1192) | Xent 0.0000(0.0000) | Loss 1.1066(1.1192) | Error 0.0000(0.0000) Steps 524(527.67) | Grad Norm 2.2602(4.8911) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 49.4555, Epoch Time 800.4704(781.3118), Bit/dim 1.1051(best: 1.1087), Xent 0.0000, Loss 1.1051, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4560 | Time 11.6578(11.2085) | Bit/dim 1.1163(1.1172) | Xent 0.0000(0.0000) | Loss 1.1163(1.1172) | Error 0.0000(0.0000) Steps 536(528.14) | Grad Norm 6.1147(4.9341) | Total Time 10.00(10.00)\n",
      "Iter 4570 | Time 12.4893(11.2914) | Bit/dim 1.1298(1.1211) | Xent 0.0000(0.0000) | Loss 1.1298(1.1211) | Error 0.0000(0.0000) Steps 566(530.34) | Grad Norm 17.8320(6.9665) | Total Time 10.00(10.00)\n",
      "Iter 4580 | Time 12.2334(11.3559) | Bit/dim 1.2303(1.1436) | Xent 0.0000(0.0000) | Loss 1.2303(1.1436) | Error 0.0000(0.0000) Steps 554(531.60) | Grad Norm 25.1459(8.6674) | Total Time 10.00(10.00)\n",
      "Iter 4590 | Time 11.4082(11.3676) | Bit/dim 1.1625(1.1570) | Xent 0.0000(0.0000) | Loss 1.1625(1.1570) | Error 0.0000(0.0000) Steps 536(532.40) | Grad Norm 8.3792(9.4535) | Total Time 10.00(10.00)\n",
      "Iter 4600 | Time 12.0194(11.3786) | Bit/dim 1.2502(1.1801) | Xent 0.0000(0.0000) | Loss 1.2502(1.1801) | Error 0.0000(0.0000) Steps 554(531.44) | Grad Norm 6.8419(11.2611) | Total Time 10.00(10.00)\n",
      "Iter 4610 | Time 11.0743(11.3598) | Bit/dim 1.1623(1.1820) | Xent 0.0000(0.0000) | Loss 1.1623(1.1820) | Error 0.0000(0.0000) Steps 524(530.74) | Grad Norm 18.3038(10.6681) | Total Time 10.00(10.00)\n",
      "Iter 4620 | Time 11.5111(11.3188) | Bit/dim 1.1595(1.1750) | Xent 0.0000(0.0000) | Loss 1.1595(1.1750) | Error 0.0000(0.0000) Steps 542(530.96) | Grad Norm 26.0711(11.1445) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 46.2653, Epoch Time 812.3895(782.2441), Bit/dim 1.1488(best: 1.1051), Xent 0.0000, Loss 1.1488, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4630 | Time 10.8638(11.2713) | Bit/dim 1.1321(1.1671) | Xent 0.0000(0.0000) | Loss 1.1321(1.1671) | Error 0.0000(0.0000) Steps 518(529.87) | Grad Norm 8.6538(10.9518) | Total Time 10.00(10.00)\n",
      "Iter 4640 | Time 10.7736(11.1941) | Bit/dim 1.1338(1.1579) | Xent 0.0000(0.0000) | Loss 1.1338(1.1579) | Error 0.0000(0.0000) Steps 524(529.09) | Grad Norm 8.7153(10.3822) | Total Time 10.00(10.00)\n",
      "Iter 4650 | Time 10.7420(11.1590) | Bit/dim 1.1323(1.1495) | Xent 0.0000(0.0000) | Loss 1.1323(1.1495) | Error 0.0000(0.0000) Steps 524(528.55) | Grad Norm 10.0134(10.4133) | Total Time 10.00(10.00)\n",
      "Iter 4660 | Time 10.9178(11.1377) | Bit/dim 1.1008(1.1404) | Xent 0.0000(0.0000) | Loss 1.1008(1.1404) | Error 0.0000(0.0000) Steps 524(528.09) | Grad Norm 2.3683(9.1905) | Total Time 10.00(10.00)\n",
      "Iter 4670 | Time 11.5807(11.2494) | Bit/dim 1.2091(1.1442) | Xent 0.0000(0.0000) | Loss 1.2091(1.1442) | Error 0.0000(0.0000) Steps 518(530.02) | Grad Norm 14.6108(10.9434) | Total Time 10.00(10.00)\n",
      "Iter 4680 | Time 11.4527(11.3102) | Bit/dim 1.1284(1.1457) | Xent 0.0000(0.0000) | Loss 1.1284(1.1457) | Error 0.0000(0.0000) Steps 530(531.31) | Grad Norm 11.7618(11.2490) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 49.4124, Epoch Time 803.2672(782.8748), Bit/dim 1.1277(best: 1.1051), Xent 0.0000, Loss 1.1277, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4690 | Time 11.2065(11.2263) | Bit/dim 1.1415(1.1401) | Xent 0.0000(0.0000) | Loss 1.1415(1.1401) | Error 0.0000(0.0000) Steps 536(528.80) | Grad Norm 16.8515(10.9732) | Total Time 10.00(10.00)\n",
      "Iter 4700 | Time 11.0698(11.1717) | Bit/dim 1.1271(1.1348) | Xent 0.0000(0.0000) | Loss 1.1271(1.1348) | Error 0.0000(0.0000) Steps 524(527.99) | Grad Norm 1.4122(9.4773) | Total Time 10.00(10.00)\n",
      "Iter 4710 | Time 11.3781(11.1399) | Bit/dim 1.1262(1.1277) | Xent 0.0000(0.0000) | Loss 1.1262(1.1277) | Error 0.0000(0.0000) Steps 524(526.95) | Grad Norm 0.9889(7.5887) | Total Time 10.00(10.00)\n",
      "Iter 4720 | Time 10.8974(11.1137) | Bit/dim 1.0991(1.1230) | Xent 0.0000(0.0000) | Loss 1.0991(1.1230) | Error 0.0000(0.0000) Steps 524(526.17) | Grad Norm 1.2775(5.9004) | Total Time 10.00(10.00)\n",
      "Iter 4730 | Time 10.8352(11.0927) | Bit/dim 1.0905(1.1184) | Xent 0.0000(0.0000) | Loss 1.0905(1.1184) | Error 0.0000(0.0000) Steps 524(525.77) | Grad Norm 0.7161(4.8205) | Total Time 10.00(10.00)\n",
      "Iter 4740 | Time 11.0265(11.1072) | Bit/dim 1.1016(1.1161) | Xent 0.0000(0.0000) | Loss 1.1016(1.1161) | Error 0.0000(0.0000) Steps 524(525.47) | Grad Norm 1.7735(3.9839) | Total Time 10.00(10.00)\n",
      "Iter 4750 | Time 11.6000(11.1006) | Bit/dim 1.0927(1.1118) | Xent 0.0000(0.0000) | Loss 1.0927(1.1118) | Error 0.0000(0.0000) Steps 536(525.77) | Grad Norm 5.0668(3.5489) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 48.7814, Epoch Time 794.0913(783.2113), Bit/dim 1.1039(best: 1.1051), Xent 0.0000, Loss 1.1039, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4760 | Time 11.4255(11.1287) | Bit/dim 1.1099(1.1111) | Xent 0.0000(0.0000) | Loss 1.1099(1.1111) | Error 0.0000(0.0000) Steps 536(526.58) | Grad Norm 8.9687(4.3494) | Total Time 10.00(10.00)\n",
      "Iter 4770 | Time 11.2695(11.1683) | Bit/dim 1.1267(1.1109) | Xent 0.0000(0.0000) | Loss 1.1267(1.1109) | Error 0.0000(0.0000) Steps 536(527.32) | Grad Norm 13.8859(5.3039) | Total Time 10.00(10.00)\n",
      "Iter 4780 | Time 13.1463(11.3470) | Bit/dim 1.2822(1.1316) | Xent 0.0000(0.0000) | Loss 1.2822(1.1316) | Error 0.0000(0.0000) Steps 578(531.19) | Grad Norm 44.4985(8.9343) | Total Time 10.00(10.00)\n",
      "Iter 4790 | Time 11.1805(11.3237) | Bit/dim 1.1548(1.1525) | Xent 0.0000(0.0000) | Loss 1.1548(1.1525) | Error 0.0000(0.0000) Steps 518(528.95) | Grad Norm 5.9789(8.9114) | Total Time 10.00(10.00)\n",
      "Iter 4800 | Time 10.9467(11.2790) | Bit/dim 1.1487(1.1504) | Xent 0.0000(0.0000) | Loss 1.1487(1.1504) | Error 0.0000(0.0000) Steps 530(528.05) | Grad Norm 12.3380(8.3161) | Total Time 10.00(10.00)\n",
      "Iter 4810 | Time 10.5664(11.2886) | Bit/dim 1.1589(1.1552) | Xent 0.0000(0.0000) | Loss 1.1589(1.1552) | Error 0.0000(0.0000) Steps 518(529.54) | Grad Norm 9.0827(9.9423) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 47.7564, Epoch Time 811.6450(784.0643), Bit/dim 1.1320(best: 1.1039), Xent 0.0000, Loss 1.1320, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4820 | Time 10.9274(11.2688) | Bit/dim 1.1268(1.1516) | Xent 0.0000(0.0000) | Loss 1.1268(1.1516) | Error 0.0000(0.0000) Steps 530(530.18) | Grad Norm 3.7391(9.7558) | Total Time 10.00(10.00)\n",
      "Iter 4830 | Time 10.7568(11.1799) | Bit/dim 1.1250(1.1440) | Xent 0.0000(0.0000) | Loss 1.1250(1.1440) | Error 0.0000(0.0000) Steps 518(527.42) | Grad Norm 6.3537(9.2952) | Total Time 10.00(10.00)\n",
      "Iter 4840 | Time 10.9586(11.2792) | Bit/dim 1.1601(1.1477) | Xent 0.0000(0.0000) | Loss 1.1601(1.1477) | Error 0.0000(0.0000) Steps 524(530.74) | Grad Norm 11.9107(10.9273) | Total Time 10.00(10.00)\n",
      "Iter 4850 | Time 11.1508(11.2484) | Bit/dim 1.1005(1.1434) | Xent 0.0000(0.0000) | Loss 1.1005(1.1434) | Error 0.0000(0.0000) Steps 530(531.05) | Grad Norm 6.4463(10.8024) | Total Time 10.00(10.00)\n",
      "Iter 4860 | Time 11.5062(11.2675) | Bit/dim 1.1189(1.1379) | Xent 0.0000(0.0000) | Loss 1.1189(1.1379) | Error 0.0000(0.0000) Steps 518(530.80) | Grad Norm 5.0370(10.6348) | Total Time 10.00(10.00)\n",
      "Iter 4870 | Time 11.5978(11.3217) | Bit/dim 1.1578(1.1388) | Xent 0.0000(0.0000) | Loss 1.1578(1.1388) | Error 0.0000(0.0000) Steps 536(532.69) | Grad Norm 12.7463(11.6305) | Total Time 10.00(10.00)\n",
      "Iter 4880 | Time 11.1576(11.3135) | Bit/dim 1.1267(1.1392) | Xent 0.0000(0.0000) | Loss 1.1267(1.1392) | Error 0.0000(0.0000) Steps 530(533.46) | Grad Norm 11.2429(12.0302) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 47.5387, Epoch Time 807.1929(784.7582), Bit/dim 1.1084(best: 1.1039), Xent 0.0000, Loss 1.1084, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4890 | Time 11.8451(11.3167) | Bit/dim 1.1692(1.1399) | Xent 0.0000(0.0000) | Loss 1.1692(1.1399) | Error 0.0000(0.0000) Steps 542(533.83) | Grad Norm 23.0608(12.3253) | Total Time 10.00(10.00)\n",
      "Iter 4900 | Time 10.8980(11.2918) | Bit/dim 1.1215(1.1353) | Xent 0.0000(0.0000) | Loss 1.1215(1.1353) | Error 0.0000(0.0000) Steps 524(532.94) | Grad Norm 10.3554(11.7275) | Total Time 10.00(10.00)\n",
      "Iter 4910 | Time 11.0415(11.2892) | Bit/dim 1.1261(1.1318) | Xent 0.0000(0.0000) | Loss 1.1261(1.1318) | Error 0.0000(0.0000) Steps 536(532.98) | Grad Norm 3.6717(10.9965) | Total Time 10.00(10.00)\n",
      "Iter 4920 | Time 11.2279(11.2720) | Bit/dim 1.0978(1.1267) | Xent 0.0000(0.0000) | Loss 1.0978(1.1267) | Error 0.0000(0.0000) Steps 536(533.83) | Grad Norm 2.2667(10.1422) | Total Time 10.00(10.00)\n",
      "Iter 4930 | Time 10.7395(11.2550) | Bit/dim 1.1387(1.1229) | Xent 0.0000(0.0000) | Loss 1.1387(1.1229) | Error 0.0000(0.0000) Steps 524(534.17) | Grad Norm 12.1907(9.6977) | Total Time 10.00(10.00)\n",
      "Iter 4940 | Time 11.4863(11.2664) | Bit/dim 1.0987(1.1195) | Xent 0.0000(0.0000) | Loss 1.0987(1.1195) | Error 0.0000(0.0000) Steps 536(534.80) | Grad Norm 4.5652(9.0709) | Total Time 10.00(10.00)\n",
      "Iter 4950 | Time 11.4455(11.2494) | Bit/dim 1.1105(1.1155) | Xent 0.0000(0.0000) | Loss 1.1105(1.1155) | Error 0.0000(0.0000) Steps 536(534.05) | Grad Norm 2.2452(7.2403) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 48.0441, Epoch Time 804.9918(785.3652), Bit/dim 1.0961(best: 1.1039), Xent 0.0000, Loss 1.0961, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4960 | Time 11.0780(11.2256) | Bit/dim 1.1017(1.1113) | Xent 0.0000(0.0000) | Loss 1.1017(1.1113) | Error 0.0000(0.0000) Steps 536(533.63) | Grad Norm 1.3091(5.6979) | Total Time 10.00(10.00)\n",
      "Iter 4970 | Time 11.1827(11.2135) | Bit/dim 1.1033(1.1085) | Xent 0.0000(0.0000) | Loss 1.1033(1.1085) | Error 0.0000(0.0000) Steps 530(533.57) | Grad Norm 3.0705(4.9440) | Total Time 10.00(10.00)\n",
      "Iter 4980 | Time 11.5039(11.1917) | Bit/dim 1.0817(1.1060) | Xent 0.0000(0.0000) | Loss 1.0817(1.1060) | Error 0.0000(0.0000) Steps 530(533.08) | Grad Norm 2.6626(4.1396) | Total Time 10.00(10.00)\n",
      "Iter 4990 | Time 11.3467(11.1983) | Bit/dim 1.1031(1.1053) | Xent 0.0000(0.0000) | Loss 1.1031(1.1053) | Error 0.0000(0.0000) Steps 536(532.74) | Grad Norm 3.6998(3.9389) | Total Time 10.00(10.00)\n",
      "Iter 5000 | Time 11.3320(11.2213) | Bit/dim 1.1004(1.1037) | Xent 0.0000(0.0000) | Loss 1.1004(1.1037) | Error 0.0000(0.0000) Steps 536(533.28) | Grad Norm 5.8297(4.4893) | Total Time 10.00(10.00)\n",
      "Iter 5010 | Time 11.5100(11.2364) | Bit/dim 1.0916(1.1035) | Xent 0.0000(0.0000) | Loss 1.0916(1.1035) | Error 0.0000(0.0000) Steps 536(533.98) | Grad Norm 10.9608(5.2315) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 48.6596, Epoch Time 804.2563(785.9319), Bit/dim 1.1131(best: 1.0961), Xent 0.0000, Loss 1.1131, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5020 | Time 10.7038(11.2751) | Bit/dim 1.2581(1.1165) | Xent 0.0000(0.0000) | Loss 1.2581(1.1165) | Error 0.0000(0.0000) Steps 506(533.79) | Grad Norm 12.2993(7.6549) | Total Time 10.00(10.00)\n",
      "Iter 5030 | Time 11.0139(11.3427) | Bit/dim 1.1346(1.1345) | Xent 0.0000(0.0000) | Loss 1.1346(1.1345) | Error 0.0000(0.0000) Steps 536(535.86) | Grad Norm 9.4772(8.6421) | Total Time 10.00(10.00)\n",
      "Iter 5040 | Time 10.7663(11.3499) | Bit/dim 1.1504(1.1437) | Xent 0.0000(0.0000) | Loss 1.1504(1.1437) | Error 0.0000(0.0000) Steps 524(535.77) | Grad Norm 11.2961(10.9963) | Total Time 10.00(10.00)\n",
      "Iter 5050 | Time 11.8083(11.3531) | Bit/dim 1.1235(1.1431) | Xent 0.0000(0.0000) | Loss 1.1235(1.1431) | Error 0.0000(0.0000) Steps 542(536.30) | Grad Norm 4.8669(11.0104) | Total Time 10.00(10.00)\n",
      "Iter 5060 | Time 11.0244(11.3469) | Bit/dim 1.1265(1.1419) | Xent 0.0000(0.0000) | Loss 1.1265(1.1419) | Error 0.0000(0.0000) Steps 530(537.13) | Grad Norm 10.9271(11.6859) | Total Time 10.00(10.00)\n",
      "Iter 5070 | Time 11.1858(11.3814) | Bit/dim 1.1202(1.1399) | Xent 0.0000(0.0000) | Loss 1.1202(1.1399) | Error 0.0000(0.0000) Steps 536(538.63) | Grad Norm 5.8091(11.5340) | Total Time 10.00(10.00)\n",
      "Iter 5080 | Time 11.1098(11.3361) | Bit/dim 1.1305(1.1341) | Xent 0.0000(0.0000) | Loss 1.1305(1.1341) | Error 0.0000(0.0000) Steps 530(538.36) | Grad Norm 10.1407(10.9095) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 48.3027, Epoch Time 815.0983(786.8069), Bit/dim 1.1111(best: 1.0961), Xent 0.0000, Loss 1.1111, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5090 | Time 11.6569(11.3658) | Bit/dim 1.1142(1.1282) | Xent 0.0000(0.0000) | Loss 1.1142(1.1282) | Error 0.0000(0.0000) Steps 524(538.77) | Grad Norm 10.4266(10.6111) | Total Time 10.00(10.00)\n",
      "Iter 5100 | Time 11.4882(11.3615) | Bit/dim 1.1229(1.1255) | Xent 0.0000(0.0000) | Loss 1.1229(1.1255) | Error 0.0000(0.0000) Steps 542(538.20) | Grad Norm 23.6939(11.0318) | Total Time 10.00(10.00)\n",
      "Iter 5110 | Time 11.0009(11.4305) | Bit/dim 1.1677(1.1309) | Xent 0.0000(0.0000) | Loss 1.1677(1.1309) | Error 0.0000(0.0000) Steps 512(539.16) | Grad Norm 12.4887(11.8393) | Total Time 10.00(10.00)\n",
      "Iter 5120 | Time 11.0061(11.4032) | Bit/dim 1.1053(1.1312) | Xent 0.0000(0.0000) | Loss 1.1053(1.1312) | Error 0.0000(0.0000) Steps 524(539.00) | Grad Norm 6.4382(11.4946) | Total Time 10.00(10.00)\n",
      "Iter 5130 | Time 11.2959(11.3792) | Bit/dim 1.1125(1.1272) | Xent 0.0000(0.0000) | Loss 1.1125(1.1272) | Error 0.0000(0.0000) Steps 542(537.39) | Grad Norm 7.9750(10.8096) | Total Time 10.00(10.00)\n",
      "Iter 5140 | Time 11.0986(11.3550) | Bit/dim 1.1014(1.1220) | Xent 0.0000(0.0000) | Loss 1.1014(1.1220) | Error 0.0000(0.0000) Steps 530(537.03) | Grad Norm 4.3019(9.4925) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 48.4595, Epoch Time 813.8205(787.6173), Bit/dim 1.0940(best: 1.0961), Xent 0.0000, Loss 1.0940, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5150 | Time 11.0433(11.3097) | Bit/dim 1.1010(1.1173) | Xent 0.0000(0.0000) | Loss 1.1010(1.1173) | Error 0.0000(0.0000) Steps 536(537.22) | Grad Norm 1.8312(7.6199) | Total Time 10.00(10.00)\n",
      "Iter 5160 | Time 10.9991(11.2828) | Bit/dim 1.0879(1.1119) | Xent 0.0000(0.0000) | Loss 1.0879(1.1119) | Error 0.0000(0.0000) Steps 542(537.55) | Grad Norm 2.6890(6.1099) | Total Time 10.00(10.00)\n",
      "Iter 5170 | Time 11.2122(11.2360) | Bit/dim 1.1026(1.1081) | Xent 0.0000(0.0000) | Loss 1.1026(1.1081) | Error 0.0000(0.0000) Steps 542(537.36) | Grad Norm 3.1248(5.2924) | Total Time 10.00(10.00)\n",
      "Iter 5180 | Time 11.3427(11.2452) | Bit/dim 1.1015(1.1041) | Xent 0.0000(0.0000) | Loss 1.1015(1.1041) | Error 0.0000(0.0000) Steps 536(537.01) | Grad Norm 0.7019(4.6668) | Total Time 10.00(10.00)\n",
      "Iter 5190 | Time 11.1646(11.2077) | Bit/dim 1.0897(1.1014) | Xent 0.0000(0.0000) | Loss 1.0897(1.1014) | Error 0.0000(0.0000) Steps 530(535.33) | Grad Norm 3.2631(4.4042) | Total Time 10.00(10.00)\n",
      "Iter 5200 | Time 11.1599(11.2145) | Bit/dim 1.0960(1.1009) | Xent 0.0000(0.0000) | Loss 1.0960(1.1009) | Error 0.0000(0.0000) Steps 530(535.49) | Grad Norm 3.9469(4.7571) | Total Time 10.00(10.00)\n",
      "Iter 5210 | Time 10.9847(11.1918) | Bit/dim 1.0897(1.0992) | Xent 0.0000(0.0000) | Loss 1.0897(1.0992) | Error 0.0000(0.0000) Steps 530(534.36) | Grad Norm 6.2488(5.0942) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 49.2918, Epoch Time 802.4149(788.0613), Bit/dim 1.0888(best: 1.0940), Xent 0.0000, Loss 1.0888, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5220 | Time 11.5051(11.2130) | Bit/dim 1.1011(1.0973) | Xent 0.0000(0.0000) | Loss 1.1011(1.0973) | Error 0.0000(0.0000) Steps 524(533.79) | Grad Norm 11.6584(5.7356) | Total Time 10.00(10.00)\n",
      "Iter 5230 | Time 11.2121(11.1784) | Bit/dim 1.1040(1.0982) | Xent 0.0000(0.0000) | Loss 1.1040(1.0982) | Error 0.0000(0.0000) Steps 530(533.54) | Grad Norm 8.9586(6.0694) | Total Time 10.00(10.00)\n",
      "Iter 5240 | Time 11.4724(11.2916) | Bit/dim 1.1715(1.1183) | Xent 0.0000(0.0000) | Loss 1.1715(1.1183) | Error 0.0000(0.0000) Steps 542(535.65) | Grad Norm 15.0299(8.4072) | Total Time 10.00(10.00)\n",
      "Iter 5250 | Time 11.2660(11.3337) | Bit/dim 1.2250(1.1383) | Xent 0.0000(0.0000) | Loss 1.2250(1.1383) | Error 0.0000(0.0000) Steps 524(536.39) | Grad Norm 10.6904(10.1650) | Total Time 10.00(10.00)\n",
      "Iter 5260 | Time 11.3039(11.3421) | Bit/dim 1.1220(1.1434) | Xent 0.0000(0.0000) | Loss 1.1220(1.1434) | Error 0.0000(0.0000) Steps 530(535.38) | Grad Norm 5.5481(10.0170) | Total Time 10.00(10.00)\n",
      "Iter 5270 | Time 11.4600(11.2960) | Bit/dim 1.1195(1.1408) | Xent 0.0000(0.0000) | Loss 1.1195(1.1408) | Error 0.0000(0.0000) Steps 530(535.44) | Grad Norm 8.9350(10.3950) | Total Time 10.00(10.00)\n",
      "Iter 5280 | Time 11.2905(11.3293) | Bit/dim 1.1333(1.1422) | Xent 0.0000(0.0000) | Loss 1.1333(1.1422) | Error 0.0000(0.0000) Steps 542(536.84) | Grad Norm 4.5171(11.0570) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 47.1975, Epoch Time 810.6446(788.7388), Bit/dim 1.1220(best: 1.0888), Xent 0.0000, Loss 1.1220, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5290 | Time 11.3119(11.3460) | Bit/dim 1.1248(1.1353) | Xent 0.0000(0.0000) | Loss 1.1248(1.1353) | Error 0.0000(0.0000) Steps 548(538.55) | Grad Norm 19.9896(10.8491) | Total Time 10.00(10.00)\n",
      "Iter 5300 | Time 11.8869(11.3742) | Bit/dim 1.1100(1.1301) | Xent 0.0000(0.0000) | Loss 1.1100(1.1301) | Error 0.0000(0.0000) Steps 548(538.47) | Grad Norm 14.0074(10.7454) | Total Time 10.00(10.00)\n",
      "Iter 5310 | Time 11.8366(11.3295) | Bit/dim 1.1237(1.1244) | Xent 0.0000(0.0000) | Loss 1.1237(1.1244) | Error 0.0000(0.0000) Steps 554(538.05) | Grad Norm 15.0528(10.4926) | Total Time 10.00(10.00)\n",
      "Iter 5320 | Time 11.0396(11.2636) | Bit/dim 1.0861(1.1178) | Xent 0.0000(0.0000) | Loss 1.0861(1.1178) | Error 0.0000(0.0000) Steps 530(537.02) | Grad Norm 5.0119(8.9662) | Total Time 10.00(10.00)\n",
      "Iter 5330 | Time 11.6979(11.2315) | Bit/dim 1.0941(1.1110) | Xent 0.0000(0.0000) | Loss 1.0941(1.1110) | Error 0.0000(0.0000) Steps 536(536.29) | Grad Norm 2.8181(7.4884) | Total Time 10.00(10.00)\n",
      "Iter 5340 | Time 11.0864(11.2409) | Bit/dim 1.1048(1.1077) | Xent 0.0000(0.0000) | Loss 1.1048(1.1077) | Error 0.0000(0.0000) Steps 536(536.00) | Grad Norm 2.5607(6.6496) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 48.9631, Epoch Time 806.7105(789.2779), Bit/dim 1.0868(best: 1.0888), Xent 0.0000, Loss 1.0868, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5350 | Time 11.5914(11.2288) | Bit/dim 1.0867(1.1024) | Xent 0.0000(0.0000) | Loss 1.0867(1.1024) | Error 0.0000(0.0000) Steps 536(535.72) | Grad Norm 5.6831(5.9894) | Total Time 10.00(10.00)\n",
      "Iter 5360 | Time 11.5992(11.2083) | Bit/dim 1.0929(1.0998) | Xent 0.0000(0.0000) | Loss 1.0929(1.0998) | Error 0.0000(0.0000) Steps 536(535.47) | Grad Norm 8.2005(5.5869) | Total Time 10.00(10.00)\n",
      "Iter 5370 | Time 11.2144(11.2237) | Bit/dim 1.0783(1.0979) | Xent 0.0000(0.0000) | Loss 1.0783(1.0979) | Error 0.0000(0.0000) Steps 536(535.29) | Grad Norm 1.8033(5.5900) | Total Time 10.00(10.00)\n",
      "Iter 5380 | Time 11.7935(11.2855) | Bit/dim 1.1964(1.1219) | Xent 0.0000(0.0000) | Loss 1.1964(1.1219) | Error 0.0000(0.0000) Steps 536(535.62) | Grad Norm 5.5443(7.8234) | Total Time 10.00(10.00)\n",
      "Iter 5390 | Time 11.7136(11.3560) | Bit/dim 1.1742(1.1349) | Xent 0.0000(0.0000) | Loss 1.1742(1.1349) | Error 0.0000(0.0000) Steps 542(538.95) | Grad Norm 13.5666(8.9459) | Total Time 10.00(10.00)\n",
      "Iter 5400 | Time 11.2173(11.3658) | Bit/dim 1.2073(1.1549) | Xent 0.0000(0.0000) | Loss 1.2073(1.1549) | Error 0.0000(0.0000) Steps 524(538.68) | Grad Norm 14.2245(10.6826) | Total Time 10.00(10.00)\n",
      "Iter 5410 | Time 11.4449(11.3674) | Bit/dim 1.1284(1.1516) | Xent 0.0000(0.0000) | Loss 1.1284(1.1516) | Error 0.0000(0.0000) Steps 554(540.03) | Grad Norm 4.4241(10.1163) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 47.2710, Epoch Time 812.0801(789.9620), Bit/dim 1.1137(best: 1.0868), Xent 0.0000, Loss 1.1137, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5420 | Time 11.4226(11.3394) | Bit/dim 1.0968(1.1417) | Xent 0.0000(0.0000) | Loss 1.0968(1.1417) | Error 0.0000(0.0000) Steps 542(540.58) | Grad Norm 10.4813(9.8240) | Total Time 10.00(10.00)\n",
      "Iter 5430 | Time 10.9398(11.2968) | Bit/dim 1.0992(1.1318) | Xent 0.0000(0.0000) | Loss 1.0992(1.1318) | Error 0.0000(0.0000) Steps 536(539.09) | Grad Norm 5.6159(8.6151) | Total Time 10.00(10.00)\n",
      "Iter 5440 | Time 11.4789(11.2958) | Bit/dim 1.1527(1.1287) | Xent 0.0000(0.0000) | Loss 1.1527(1.1287) | Error 0.0000(0.0000) Steps 536(539.08) | Grad Norm 13.2201(9.6739) | Total Time 10.00(10.00)\n",
      "Iter 5450 | Time 11.9704(11.3727) | Bit/dim 1.1464(1.1312) | Xent 0.0000(0.0000) | Loss 1.1464(1.1312) | Error 0.0000(0.0000) Steps 566(540.63) | Grad Norm 27.1421(11.0789) | Total Time 10.00(10.00)\n",
      "Iter 5460 | Time 10.8282(11.3153) | Bit/dim 1.1232(1.1280) | Xent 0.0000(0.0000) | Loss 1.1232(1.1280) | Error 0.0000(0.0000) Steps 524(539.82) | Grad Norm 11.3383(10.9722) | Total Time 10.00(10.00)\n",
      "Iter 5470 | Time 12.4698(11.3865) | Bit/dim 1.1665(1.1295) | Xent 0.0000(0.0000) | Loss 1.1665(1.1295) | Error 0.0000(0.0000) Steps 578(541.02) | Grad Norm 28.0566(11.7795) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 48.6475, Epoch Time 812.1034(790.6262), Bit/dim 1.1140(best: 1.0868), Xent 0.0000, Loss 1.1140, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5480 | Time 10.7532(11.3781) | Bit/dim 1.1061(1.1260) | Xent 0.0000(0.0000) | Loss 1.1061(1.1260) | Error 0.0000(0.0000) Steps 530(541.49) | Grad Norm 8.0789(11.2678) | Total Time 10.00(10.00)\n",
      "Iter 5490 | Time 11.2089(11.3871) | Bit/dim 1.0938(1.1236) | Xent 0.0000(0.0000) | Loss 1.0938(1.1236) | Error 0.0000(0.0000) Steps 530(541.12) | Grad Norm 3.9221(11.1263) | Total Time 10.00(10.00)\n",
      "Iter 5500 | Time 10.9616(11.4084) | Bit/dim 1.1085(1.1218) | Xent 0.0000(0.0000) | Loss 1.1085(1.1218) | Error 0.0000(0.0000) Steps 536(542.73) | Grad Norm 7.7329(11.0866) | Total Time 10.00(10.00)\n",
      "Iter 5510 | Time 12.0697(11.4143) | Bit/dim 1.1162(1.1199) | Xent 0.0000(0.0000) | Loss 1.1162(1.1199) | Error 0.0000(0.0000) Steps 560(543.20) | Grad Norm 6.2538(10.7636) | Total Time 10.00(10.00)\n",
      "Iter 5520 | Time 11.1353(11.4077) | Bit/dim 1.1021(1.1158) | Xent 0.0000(0.0000) | Loss 1.1021(1.1158) | Error 0.0000(0.0000) Steps 536(543.17) | Grad Norm 8.7794(10.5110) | Total Time 10.00(10.00)\n",
      "Iter 5530 | Time 11.0499(11.3106) | Bit/dim 1.1049(1.1104) | Xent 0.0000(0.0000) | Loss 1.1049(1.1104) | Error 0.0000(0.0000) Steps 530(540.94) | Grad Norm 8.5901(9.8447) | Total Time 10.00(10.00)\n",
      "Iter 5540 | Time 11.2497(11.2526) | Bit/dim 1.0924(1.1051) | Xent 0.0000(0.0000) | Loss 1.0924(1.1051) | Error 0.0000(0.0000) Steps 530(538.81) | Grad Norm 3.7034(9.0124) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 48.0953, Epoch Time 808.9260(791.1752), Bit/dim 1.0886(best: 1.0868), Xent 0.0000, Loss 1.0886, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5550 | Time 11.8267(11.2562) | Bit/dim 1.1139(1.1025) | Xent 0.0000(0.0000) | Loss 1.1139(1.1025) | Error 0.0000(0.0000) Steps 554(538.44) | Grad Norm 23.0661(8.8865) | Total Time 10.00(10.00)\n",
      "Iter 5560 | Time 11.0632(11.3253) | Bit/dim 1.1491(1.1118) | Xent 0.0000(0.0000) | Loss 1.1491(1.1118) | Error 0.0000(0.0000) Steps 518(539.92) | Grad Norm 11.4962(9.9615) | Total Time 10.00(10.00)\n",
      "Iter 5570 | Time 12.3275(11.3104) | Bit/dim 1.1166(1.1144) | Xent 0.0000(0.0000) | Loss 1.1166(1.1144) | Error 0.0000(0.0000) Steps 548(539.89) | Grad Norm 16.6238(10.2501) | Total Time 10.00(10.00)\n",
      "Iter 5580 | Time 11.3771(11.2817) | Bit/dim 1.1080(1.1126) | Xent 0.0000(0.0000) | Loss 1.1080(1.1126) | Error 0.0000(0.0000) Steps 548(539.12) | Grad Norm 4.8812(10.0345) | Total Time 10.00(10.00)\n",
      "Iter 5590 | Time 10.9726(11.3118) | Bit/dim 1.0997(1.1119) | Xent 0.0000(0.0000) | Loss 1.0997(1.1119) | Error 0.0000(0.0000) Steps 536(539.96) | Grad Norm 8.6595(10.2637) | Total Time 10.00(10.00)\n",
      "Iter 5600 | Time 11.0272(11.3316) | Bit/dim 1.1202(1.1110) | Xent 0.0000(0.0000) | Loss 1.1202(1.1110) | Error 0.0000(0.0000) Steps 530(541.27) | Grad Norm 11.9582(10.2774) | Total Time 10.00(10.00)\n",
      "Iter 5610 | Time 11.1268(11.3245) | Bit/dim 1.0813(1.1087) | Xent 0.0000(0.0000) | Loss 1.0813(1.1087) | Error 0.0000(0.0000) Steps 536(540.81) | Grad Norm 3.7689(10.1333) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 47.7607, Epoch Time 811.1728(791.7751), Bit/dim 1.0965(best: 1.0868), Xent 0.0000, Loss 1.0965, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5620 | Time 11.5181(11.3367) | Bit/dim 1.1072(1.1066) | Xent 0.0000(0.0000) | Loss 1.1072(1.1066) | Error 0.0000(0.0000) Steps 548(540.11) | Grad Norm 19.4952(10.0502) | Total Time 10.00(10.00)\n",
      "Iter 5630 | Time 11.1614(11.3497) | Bit/dim 1.0962(1.1075) | Xent 0.0000(0.0000) | Loss 1.0962(1.1075) | Error 0.0000(0.0000) Steps 530(540.23) | Grad Norm 3.4513(10.0211) | Total Time 10.00(10.00)\n",
      "Iter 5640 | Time 11.3740(11.3415) | Bit/dim 1.1018(1.1073) | Xent 0.0000(0.0000) | Loss 1.1018(1.1073) | Error 0.0000(0.0000) Steps 530(540.32) | Grad Norm 9.0569(10.3703) | Total Time 10.00(10.00)\n",
      "Iter 5650 | Time 11.0945(11.3440) | Bit/dim 1.1056(1.1057) | Xent 0.0000(0.0000) | Loss 1.1056(1.1057) | Error 0.0000(0.0000) Steps 530(540.52) | Grad Norm 5.9897(9.8125) | Total Time 10.00(10.00)\n",
      "Iter 5660 | Time 11.1392(11.2995) | Bit/dim 1.0810(1.1012) | Xent 0.0000(0.0000) | Loss 1.0810(1.1012) | Error 0.0000(0.0000) Steps 536(539.02) | Grad Norm 1.8487(7.9863) | Total Time 10.00(10.00)\n",
      "Iter 5670 | Time 10.9166(11.2290) | Bit/dim 1.0854(1.0966) | Xent 0.0000(0.0000) | Loss 1.0854(1.0966) | Error 0.0000(0.0000) Steps 530(536.81) | Grad Norm 2.0293(6.2799) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 47.7234, Epoch Time 806.0983(792.2048), Bit/dim 1.0794(best: 1.0868), Xent 0.0000, Loss 1.0794, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5680 | Time 11.5431(11.2121) | Bit/dim 1.0947(1.0928) | Xent 0.0000(0.0000) | Loss 1.0947(1.0928) | Error 0.0000(0.0000) Steps 536(536.71) | Grad Norm 2.3830(5.2250) | Total Time 10.00(10.00)\n",
      "Iter 5690 | Time 11.0281(11.2262) | Bit/dim 1.0881(1.0905) | Xent 0.0000(0.0000) | Loss 1.0881(1.0905) | Error 0.0000(0.0000) Steps 536(537.28) | Grad Norm 1.2929(4.1863) | Total Time 10.00(10.00)\n",
      "Iter 5700 | Time 11.0985(11.2135) | Bit/dim 1.0937(1.0894) | Xent 0.0000(0.0000) | Loss 1.0937(1.0894) | Error 0.0000(0.0000) Steps 536(537.44) | Grad Norm 4.2103(4.4391) | Total Time 10.00(10.00)\n",
      "Iter 5710 | Time 11.4656(11.1970) | Bit/dim 1.0856(1.0877) | Xent 0.0000(0.0000) | Loss 1.0856(1.0877) | Error 0.0000(0.0000) Steps 542(537.55) | Grad Norm 4.0191(4.4635) | Total Time 10.00(10.00)\n",
      "Iter 5720 | Time 12.9097(11.2650) | Bit/dim 1.1474(1.0913) | Xent 0.0000(0.0000) | Loss 1.1474(1.0913) | Error 0.0000(0.0000) Steps 596(539.72) | Grad Norm 31.3857(6.4660) | Total Time 10.00(10.00)\n",
      "Iter 5730 | Time 11.6664(11.4121) | Bit/dim 1.1518(1.1119) | Xent 0.0000(0.0000) | Loss 1.1518(1.1119) | Error 0.0000(0.0000) Steps 566(543.58) | Grad Norm 13.3667(8.6089) | Total Time 10.00(10.00)\n",
      "Iter 5740 | Time 10.9551(11.3845) | Bit/dim 1.1746(1.1217) | Xent 0.0000(0.0000) | Loss 1.1746(1.1217) | Error 0.0000(0.0000) Steps 536(541.96) | Grad Norm 12.1039(10.0093) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 49.8383, Epoch Time 814.1967(792.8646), Bit/dim 1.1224(best: 1.0794), Xent 0.0000, Loss 1.1224, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5750 | Time 11.6536(11.4092) | Bit/dim 1.0982(1.1239) | Xent 0.0000(0.0000) | Loss 1.0982(1.1239) | Error 0.0000(0.0000) Steps 542(543.36) | Grad Norm 4.2681(9.8064) | Total Time 10.00(10.00)\n",
      "Iter 5760 | Time 11.1740(11.3695) | Bit/dim 1.0913(1.1194) | Xent 0.0000(0.0000) | Loss 1.0913(1.1194) | Error 0.0000(0.0000) Steps 542(543.14) | Grad Norm 5.0399(9.2066) | Total Time 10.00(10.00)\n",
      "Iter 5770 | Time 11.0270(11.3632) | Bit/dim 1.1045(1.1157) | Xent 0.0000(0.0000) | Loss 1.1045(1.1157) | Error 0.0000(0.0000) Steps 530(543.05) | Grad Norm 10.5390(9.5758) | Total Time 10.00(10.00)\n",
      "Iter 5780 | Time 11.1335(11.3499) | Bit/dim 1.0958(1.1122) | Xent 0.0000(0.0000) | Loss 1.0958(1.1122) | Error 0.0000(0.0000) Steps 536(543.53) | Grad Norm 12.4097(10.0134) | Total Time 10.00(10.00)\n",
      "Iter 5790 | Time 11.7049(11.3324) | Bit/dim 1.0980(1.1093) | Xent 0.0000(0.0000) | Loss 1.0980(1.1093) | Error 0.0000(0.0000) Steps 566(543.81) | Grad Norm 11.3090(10.0040) | Total Time 10.00(10.00)\n",
      "Iter 5800 | Time 11.7008(11.3996) | Bit/dim 1.0988(1.1114) | Xent 0.0000(0.0000) | Loss 1.0988(1.1114) | Error 0.0000(0.0000) Steps 548(543.23) | Grad Norm 6.8468(10.8588) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 49.3814, Epoch Time 816.2739(793.5669), Bit/dim 1.1064(best: 1.0794), Xent 0.0000, Loss 1.1064, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5810 | Time 11.1521(11.4347) | Bit/dim 1.1052(1.1103) | Xent 0.0000(0.0000) | Loss 1.1052(1.1103) | Error 0.0000(0.0000) Steps 548(545.19) | Grad Norm 2.3387(11.0141) | Total Time 10.00(10.00)\n",
      "Iter 5820 | Time 12.0128(11.4274) | Bit/dim 1.0986(1.1075) | Xent 0.0000(0.0000) | Loss 1.0986(1.1075) | Error 0.0000(0.0000) Steps 572(545.25) | Grad Norm 13.9677(10.6907) | Total Time 10.00(10.00)\n",
      "Iter 5830 | Time 11.4645(11.4524) | Bit/dim 1.0852(1.1039) | Xent 0.0000(0.0000) | Loss 1.0852(1.1039) | Error 0.0000(0.0000) Steps 548(546.30) | Grad Norm 2.4759(9.7577) | Total Time 10.00(10.00)\n",
      "Iter 5840 | Time 11.1358(11.5004) | Bit/dim 1.1160(1.1072) | Xent 0.0000(0.0000) | Loss 1.1160(1.1072) | Error 0.0000(0.0000) Steps 530(546.90) | Grad Norm 12.4019(10.8157) | Total Time 10.00(10.00)\n",
      "Iter 5850 | Time 11.4004(11.5248) | Bit/dim 1.0897(1.1101) | Xent 0.0000(0.0000) | Loss 1.0897(1.1101) | Error 0.0000(0.0000) Steps 554(546.83) | Grad Norm 10.5285(10.7879) | Total Time 10.00(10.00)\n",
      "Iter 5860 | Time 12.2420(11.5435) | Bit/dim 1.1086(1.1086) | Xent 0.0000(0.0000) | Loss 1.1086(1.1086) | Error 0.0000(0.0000) Steps 572(548.44) | Grad Norm 18.0683(10.8287) | Total Time 10.00(10.00)\n",
      "Iter 5870 | Time 11.1910(11.5317) | Bit/dim 1.0884(1.1066) | Xent 0.0000(0.0000) | Loss 1.0884(1.1066) | Error 0.0000(0.0000) Steps 548(548.55) | Grad Norm 7.4268(10.2587) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 48.4374, Epoch Time 825.1464(794.5143), Bit/dim 1.0885(best: 1.0794), Xent 0.0000, Loss 1.0885, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5880 | Time 11.2057(11.5342) | Bit/dim 1.1040(1.1030) | Xent 0.0000(0.0000) | Loss 1.1040(1.1030) | Error 0.0000(0.0000) Steps 542(548.52) | Grad Norm 6.3906(9.4203) | Total Time 10.00(10.00)\n",
      "Iter 5890 | Time 11.1324(11.4854) | Bit/dim 1.0875(1.0995) | Xent 0.0000(0.0000) | Loss 1.0875(1.0995) | Error 0.0000(0.0000) Steps 536(546.12) | Grad Norm 0.9087(7.8690) | Total Time 10.00(10.00)\n",
      "Iter 5900 | Time 11.0330(11.4176) | Bit/dim 1.0719(1.0947) | Xent 0.0000(0.0000) | Loss 1.0719(1.0947) | Error 0.0000(0.0000) Steps 542(543.82) | Grad Norm 1.7890(6.8767) | Total Time 10.00(10.00)\n",
      "Iter 5910 | Time 11.1797(11.3530) | Bit/dim 1.0763(1.0901) | Xent 0.0000(0.0000) | Loss 1.0763(1.0901) | Error 0.0000(0.0000) Steps 542(542.12) | Grad Norm 1.0078(5.7510) | Total Time 10.00(10.00)\n",
      "Iter 5920 | Time 11.3516(11.3670) | Bit/dim 1.0770(1.0879) | Xent 0.0000(0.0000) | Loss 1.0770(1.0879) | Error 0.0000(0.0000) Steps 530(539.86) | Grad Norm 4.1908(5.1834) | Total Time 10.00(10.00)\n",
      "Iter 5930 | Time 11.4978(11.3579) | Bit/dim 1.0781(1.0860) | Xent 0.0000(0.0000) | Loss 1.0781(1.0860) | Error 0.0000(0.0000) Steps 548(540.48) | Grad Norm 2.2241(4.8308) | Total Time 10.00(10.00)\n",
      "Iter 5940 | Time 11.5822(11.3779) | Bit/dim 1.0843(1.0844) | Xent 0.0000(0.0000) | Loss 1.0843(1.0844) | Error 0.0000(0.0000) Steps 548(542.03) | Grad Norm 4.9247(4.9830) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 48.1520, Epoch Time 812.0939(795.0416), Bit/dim 1.0732(best: 1.0794), Xent 0.0000, Loss 1.0732, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 5950 | Time 11.6385(11.3819) | Bit/dim 1.0753(1.0829) | Xent 0.0000(0.0000) | Loss 1.0753(1.0829) | Error 0.0000(0.0000) Steps 548(542.71) | Grad Norm 1.2350(4.9337) | Total Time 10.00(10.00)\n",
      "Iter 5960 | Time 12.0439(11.4082) | Bit/dim 1.1618(1.0982) | Xent 0.0000(0.0000) | Loss 1.1618(1.0982) | Error 0.0000(0.0000) Steps 566(543.77) | Grad Norm 29.1851(7.3370) | Total Time 10.00(10.00)\n",
      "Iter 5970 | Time 11.5968(11.4352) | Bit/dim 1.1405(1.1126) | Xent 0.0000(0.0000) | Loss 1.1405(1.1126) | Error 0.0000(0.0000) Steps 548(546.71) | Grad Norm 9.8709(8.9654) | Total Time 10.00(10.00)\n",
      "Iter 5980 | Time 11.8325(11.5042) | Bit/dim 1.1187(1.1138) | Xent 0.0000(0.0000) | Loss 1.1187(1.1138) | Error 0.0000(0.0000) Steps 548(548.21) | Grad Norm 9.2794(9.3275) | Total Time 10.00(10.00)\n",
      "Iter 5990 | Time 11.1547(11.4971) | Bit/dim 1.0803(1.1086) | Xent 0.0000(0.0000) | Loss 1.0803(1.1086) | Error 0.0000(0.0000) Steps 548(549.99) | Grad Norm 7.3630(8.5280) | Total Time 10.00(10.00)\n",
      "Iter 6000 | Time 11.0896(11.5549) | Bit/dim 1.1225(1.1105) | Xent 0.0000(0.0000) | Loss 1.1225(1.1105) | Error 0.0000(0.0000) Steps 542(552.51) | Grad Norm 14.5109(9.8915) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 49.9838, Epoch Time 827.8383(796.0255), Bit/dim 1.1232(best: 1.0732), Xent 0.0000, Loss 1.1232, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6010 | Time 11.4441(11.5872) | Bit/dim 1.0956(1.1132) | Xent 0.0000(0.0000) | Loss 1.0956(1.1132) | Error 0.0000(0.0000) Steps 560(555.75) | Grad Norm 6.3407(10.2760) | Total Time 10.00(10.00)\n",
      "Iter 6020 | Time 11.3706(11.5775) | Bit/dim 1.0927(1.1085) | Xent 0.0000(0.0000) | Loss 1.0927(1.1085) | Error 0.0000(0.0000) Steps 542(554.96) | Grad Norm 7.7195(9.7360) | Total Time 10.00(10.00)\n",
      "Iter 6030 | Time 11.2284(11.5223) | Bit/dim 1.0942(1.1032) | Xent 0.0000(0.0000) | Loss 1.0942(1.1032) | Error 0.0000(0.0000) Steps 536(552.45) | Grad Norm 9.5638(9.5300) | Total Time 10.00(10.00)\n",
      "Iter 6040 | Time 10.9923(11.5129) | Bit/dim 1.0922(1.0999) | Xent 0.0000(0.0000) | Loss 1.0922(1.0999) | Error 0.0000(0.0000) Steps 542(552.29) | Grad Norm 7.0173(9.2697) | Total Time 10.00(10.00)\n",
      "Iter 6050 | Time 11.7711(11.5138) | Bit/dim 1.0919(1.0986) | Xent 0.0000(0.0000) | Loss 1.0919(1.0986) | Error 0.0000(0.0000) Steps 566(551.17) | Grad Norm 9.1431(9.6954) | Total Time 10.00(10.00)\n",
      "Iter 6060 | Time 11.6272(11.5852) | Bit/dim 1.1672(1.1066) | Xent 0.0000(0.0000) | Loss 1.1672(1.1066) | Error 0.0000(0.0000) Steps 548(552.64) | Grad Norm 14.8653(10.9858) | Total Time 10.00(10.00)\n",
      "Iter 6070 | Time 11.1091(11.6057) | Bit/dim 1.1231(1.1108) | Xent 0.0000(0.0000) | Loss 1.1231(1.1108) | Error 0.0000(0.0000) Steps 548(554.19) | Grad Norm 10.4113(10.8962) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 47.6841, Epoch Time 826.5686(796.9418), Bit/dim 1.1223(best: 1.0732), Xent 0.0000, Loss 1.1223, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6080 | Time 11.8300(11.6068) | Bit/dim 1.1167(1.1122) | Xent 0.0000(0.0000) | Loss 1.1167(1.1122) | Error 0.0000(0.0000) Steps 560(554.24) | Grad Norm 11.6349(10.9612) | Total Time 10.00(10.00)\n",
      "Iter 6090 | Time 11.3246(11.5387) | Bit/dim 1.0867(1.1060) | Xent 0.0000(0.0000) | Loss 1.0867(1.1060) | Error 0.0000(0.0000) Steps 548(552.89) | Grad Norm 4.0454(9.3308) | Total Time 10.00(10.00)\n",
      "Iter 6100 | Time 11.3896(11.5205) | Bit/dim 1.0921(1.1002) | Xent 0.0000(0.0000) | Loss 1.0921(1.1002) | Error 0.0000(0.0000) Steps 554(552.22) | Grad Norm 10.8970(8.5883) | Total Time 10.00(10.00)\n",
      "Iter 6110 | Time 11.4752(11.5295) | Bit/dim 1.0723(1.0975) | Xent 0.0000(0.0000) | Loss 1.0723(1.0975) | Error 0.0000(0.0000) Steps 548(551.76) | Grad Norm 4.7808(9.0862) | Total Time 10.00(10.00)\n",
      "Iter 6120 | Time 11.0745(11.4760) | Bit/dim 1.0750(1.0936) | Xent 0.0000(0.0000) | Loss 1.0750(1.0936) | Error 0.0000(0.0000) Steps 554(551.25) | Grad Norm 2.0250(8.3682) | Total Time 10.00(10.00)\n",
      "Iter 6130 | Time 11.1966(11.5273) | Bit/dim 1.1545(1.1004) | Xent 0.0000(0.0000) | Loss 1.1545(1.1004) | Error 0.0000(0.0000) Steps 536(552.75) | Grad Norm 12.6054(10.1053) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 48.1489, Epoch Time 825.2201(797.7902), Bit/dim 1.0959(best: 1.0732), Xent 0.0000, Loss 1.0959, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6140 | Time 11.5476(11.6038) | Bit/dim 1.0968(1.1023) | Xent 0.0000(0.0000) | Loss 1.0968(1.1023) | Error 0.0000(0.0000) Steps 560(554.74) | Grad Norm 5.6621(9.8859) | Total Time 10.00(10.00)\n",
      "Iter 6150 | Time 11.2759(11.5637) | Bit/dim 1.0829(1.1005) | Xent 0.0000(0.0000) | Loss 1.0829(1.1005) | Error 0.0000(0.0000) Steps 542(552.73) | Grad Norm 7.0088(9.8244) | Total Time 10.00(10.00)\n",
      "Iter 6160 | Time 11.1447(11.5223) | Bit/dim 1.1001(1.0976) | Xent 0.0000(0.0000) | Loss 1.1001(1.0976) | Error 0.0000(0.0000) Steps 542(550.38) | Grad Norm 7.7504(9.3493) | Total Time 10.00(10.00)\n",
      "Iter 6170 | Time 11.4897(11.4804) | Bit/dim 1.0887(1.0933) | Xent 0.0000(0.0000) | Loss 1.0887(1.0933) | Error 0.0000(0.0000) Steps 542(548.91) | Grad Norm 4.7412(8.1055) | Total Time 10.00(10.00)\n",
      "Iter 6180 | Time 12.0054(11.4761) | Bit/dim 1.0727(1.0879) | Xent 0.0000(0.0000) | Loss 1.0727(1.0879) | Error 0.0000(0.0000) Steps 554(546.67) | Grad Norm 7.0632(7.1453) | Total Time 10.00(10.00)\n",
      "Iter 6190 | Time 11.7908(11.4518) | Bit/dim 1.0720(1.0844) | Xent 0.0000(0.0000) | Loss 1.0720(1.0844) | Error 0.0000(0.0000) Steps 542(545.94) | Grad Norm 2.6841(6.2145) | Total Time 10.00(10.00)\n",
      "Iter 6200 | Time 11.5979(11.4223) | Bit/dim 1.0718(1.0813) | Xent 0.0000(0.0000) | Loss 1.0718(1.0813) | Error 0.0000(0.0000) Steps 542(545.20) | Grad Norm 7.4941(5.7240) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 48.6816, Epoch Time 816.5519(798.3530), Bit/dim 1.0714(best: 1.0732), Xent 0.0000, Loss 1.0714, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6210 | Time 11.3268(11.4520) | Bit/dim 1.0905(1.0806) | Xent 0.0000(0.0000) | Loss 1.0905(1.0806) | Error 0.0000(0.0000) Steps 548(545.81) | Grad Norm 12.0825(5.9738) | Total Time 10.00(10.00)\n",
      "Iter 6220 | Time 11.7129(11.4958) | Bit/dim 1.1089(1.0848) | Xent 0.0000(0.0000) | Loss 1.1089(1.0848) | Error 0.0000(0.0000) Steps 566(547.43) | Grad Norm 7.3553(7.0674) | Total Time 10.00(10.00)\n",
      "Iter 6230 | Time 11.5272(11.5486) | Bit/dim 1.1638(1.1089) | Xent 0.0000(0.0000) | Loss 1.1638(1.1089) | Error 0.0000(0.0000) Steps 572(551.57) | Grad Norm 8.1265(9.1305) | Total Time 10.00(10.00)\n",
      "Iter 6240 | Time 11.5521(11.5833) | Bit/dim 1.1204(1.1155) | Xent 0.0000(0.0000) | Loss 1.1204(1.1155) | Error 0.0000(0.0000) Steps 554(553.54) | Grad Norm 5.9660(9.5944) | Total Time 10.00(10.00)\n",
      "Iter 6250 | Time 11.2131(11.5207) | Bit/dim 1.0941(1.1119) | Xent 0.0000(0.0000) | Loss 1.0941(1.1119) | Error 0.0000(0.0000) Steps 548(552.37) | Grad Norm 3.2609(8.6712) | Total Time 10.00(10.00)\n",
      "Iter 6260 | Time 11.2271(11.4961) | Bit/dim 1.0829(1.1042) | Xent 0.0000(0.0000) | Loss 1.0829(1.1042) | Error 0.0000(0.0000) Steps 548(551.80) | Grad Norm 2.9124(7.8179) | Total Time 10.00(10.00)\n",
      "Iter 6270 | Time 11.0817(11.4142) | Bit/dim 1.0826(1.0970) | Xent 0.0000(0.0000) | Loss 1.0826(1.0970) | Error 0.0000(0.0000) Steps 542(548.60) | Grad Norm 2.3387(6.3611) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 47.6159, Epoch Time 822.5322(799.0784), Bit/dim 1.0708(best: 1.0714), Xent 0.0000, Loss 1.0708, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6280 | Time 11.5432(11.3879) | Bit/dim 1.0725(1.0907) | Xent 0.0000(0.0000) | Loss 1.0725(1.0907) | Error 0.0000(0.0000) Steps 548(547.97) | Grad Norm 0.9124(5.0390) | Total Time 10.00(10.00)\n",
      "Iter 6290 | Time 11.3577(11.3636) | Bit/dim 1.0719(1.0858) | Xent 0.0000(0.0000) | Loss 1.0719(1.0858) | Error 0.0000(0.0000) Steps 542(547.64) | Grad Norm 4.5524(4.3622) | Total Time 10.00(10.00)\n",
      "Iter 6300 | Time 11.3423(11.3851) | Bit/dim 1.0905(1.0826) | Xent 0.0000(0.0000) | Loss 1.0905(1.0826) | Error 0.0000(0.0000) Steps 542(547.62) | Grad Norm 4.5884(4.5930) | Total Time 10.00(10.00)\n",
      "Iter 6310 | Time 11.1293(11.4203) | Bit/dim 1.0666(1.0797) | Xent 0.0000(0.0000) | Loss 1.0666(1.0797) | Error 0.0000(0.0000) Steps 554(548.99) | Grad Norm 3.7866(5.0986) | Total Time 10.00(10.00)\n",
      "Iter 6320 | Time 11.3972(11.4223) | Bit/dim 1.0707(1.0785) | Xent 0.0000(0.0000) | Loss 1.0707(1.0785) | Error 0.0000(0.0000) Steps 548(549.50) | Grad Norm 9.2486(5.6553) | Total Time 10.00(10.00)\n",
      "Iter 6330 | Time 11.6249(11.4434) | Bit/dim 1.0778(1.0782) | Xent 0.0000(0.0000) | Loss 1.0778(1.0782) | Error 0.0000(0.0000) Steps 548(550.17) | Grad Norm 8.7101(6.0820) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 50.2791, Epoch Time 821.1525(799.7406), Bit/dim 1.2333(best: 1.0708), Xent 0.0000, Loss 1.2333, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6340 | Time 12.2582(11.5516) | Bit/dim 1.1329(1.1022) | Xent 0.0000(0.0000) | Loss 1.1329(1.1022) | Error 0.0000(0.0000) Steps 566(553.76) | Grad Norm 4.9924(8.0579) | Total Time 10.00(10.00)\n",
      "Iter 6350 | Time 11.7991(11.5661) | Bit/dim 1.1413(1.1127) | Xent 0.0000(0.0000) | Loss 1.1413(1.1127) | Error 0.0000(0.0000) Steps 560(556.42) | Grad Norm 12.2177(8.6290) | Total Time 10.00(10.00)\n",
      "Iter 6360 | Time 13.0356(11.6542) | Bit/dim 1.1637(1.1227) | Xent 0.0000(0.0000) | Loss 1.1637(1.1227) | Error 0.0000(0.0000) Steps 602(557.04) | Grad Norm 17.5242(10.1766) | Total Time 10.00(10.00)\n",
      "Iter 6370 | Time 11.2165(11.6363) | Bit/dim 1.1023(1.1188) | Xent 0.0000(0.0000) | Loss 1.1023(1.1188) | Error 0.0000(0.0000) Steps 554(556.49) | Grad Norm 7.3697(9.4125) | Total Time 10.00(10.00)\n",
      "Iter 6380 | Time 11.2735(11.6155) | Bit/dim 1.0959(1.1111) | Xent 0.0000(0.0000) | Loss 1.0959(1.1111) | Error 0.0000(0.0000) Steps 542(553.67) | Grad Norm 1.9147(8.2950) | Total Time 10.00(10.00)\n",
      "Iter 6390 | Time 11.4408(11.6331) | Bit/dim 1.1800(1.1130) | Xent 0.0000(0.0000) | Loss 1.1800(1.1130) | Error 0.0000(0.0000) Steps 548(554.29) | Grad Norm 12.0653(9.8861) | Total Time 10.00(10.00)\n",
      "Iter 6400 | Time 11.4244(11.6833) | Bit/dim 1.1095(1.1138) | Xent 0.0000(0.0000) | Loss 1.1095(1.1138) | Error 0.0000(0.0000) Steps 548(557.04) | Grad Norm 8.8918(10.1384) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 47.4766, Epoch Time 835.1354(800.8025), Bit/dim 1.0920(best: 1.0708), Xent 0.0000, Loss 1.0920, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6410 | Time 11.2362(11.6028) | Bit/dim 1.0781(1.1088) | Xent 0.0000(0.0000) | Loss 1.0781(1.1088) | Error 0.0000(0.0000) Steps 554(554.78) | Grad Norm 4.8870(9.6307) | Total Time 10.00(10.00)\n",
      "Iter 6420 | Time 11.3034(11.5206) | Bit/dim 1.0713(1.1023) | Xent 0.0000(0.0000) | Loss 1.0713(1.1023) | Error 0.0000(0.0000) Steps 554(553.21) | Grad Norm 1.8549(8.5778) | Total Time 10.00(10.00)\n",
      "Iter 6430 | Time 11.7663(11.5008) | Bit/dim 1.0673(1.0953) | Xent 0.0000(0.0000) | Loss 1.0673(1.0953) | Error 0.0000(0.0000) Steps 554(552.15) | Grad Norm 8.7212(7.6509) | Total Time 10.00(10.00)\n",
      "Iter 6440 | Time 11.4676(11.5316) | Bit/dim 1.0800(1.0942) | Xent 0.0000(0.0000) | Loss 1.0800(1.0942) | Error 0.0000(0.0000) Steps 560(553.29) | Grad Norm 9.0258(8.7571) | Total Time 10.00(10.00)\n",
      "Iter 6450 | Time 11.6195(11.5309) | Bit/dim 1.1067(1.0927) | Xent 0.0000(0.0000) | Loss 1.1067(1.0927) | Error 0.0000(0.0000) Steps 554(553.91) | Grad Norm 10.6747(8.9768) | Total Time 10.00(10.00)\n",
      "Iter 6460 | Time 11.4973(11.5017) | Bit/dim 1.0635(1.0878) | Xent 0.0000(0.0000) | Loss 1.0635(1.0878) | Error 0.0000(0.0000) Steps 548(552.48) | Grad Norm 1.5338(7.9883) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 48.1325, Epoch Time 819.0212(801.3490), Bit/dim 1.1196(best: 1.0708), Xent 0.0000, Loss 1.1196, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6470 | Time 11.6315(11.5270) | Bit/dim 1.0994(1.0873) | Xent 0.0000(0.0000) | Loss 1.0994(1.0873) | Error 0.0000(0.0000) Steps 554(552.00) | Grad Norm 6.1772(8.4170) | Total Time 10.00(10.00)\n",
      "Iter 6480 | Time 11.6937(11.6130) | Bit/dim 1.1268(1.1030) | Xent 0.0000(0.0000) | Loss 1.1268(1.1030) | Error 0.0000(0.0000) Steps 584(557.66) | Grad Norm 4.7599(9.4921) | Total Time 10.00(10.00)\n",
      "Iter 6490 | Time 11.6639(11.6196) | Bit/dim 1.0969(1.1051) | Xent 0.0000(0.0000) | Loss 1.0969(1.1051) | Error 0.0000(0.0000) Steps 536(558.05) | Grad Norm 10.7085(9.9672) | Total Time 10.00(10.00)\n",
      "Iter 6500 | Time 11.2265(11.5743) | Bit/dim 1.0856(1.1008) | Xent 0.0000(0.0000) | Loss 1.0856(1.1008) | Error 0.0000(0.0000) Steps 560(555.80) | Grad Norm 10.0973(9.3604) | Total Time 10.00(10.00)\n",
      "Iter 6510 | Time 11.8019(11.5698) | Bit/dim 1.0765(1.0947) | Xent 0.0000(0.0000) | Loss 1.0765(1.0947) | Error 0.0000(0.0000) Steps 560(556.63) | Grad Norm 6.2995(8.0598) | Total Time 10.00(10.00)\n",
      "Iter 6520 | Time 11.5066(11.5714) | Bit/dim 1.0710(1.0920) | Xent 0.0000(0.0000) | Loss 1.0710(1.0920) | Error 0.0000(0.0000) Steps 560(556.81) | Grad Norm 1.0807(8.3202) | Total Time 10.00(10.00)\n",
      "Iter 6530 | Time 11.6526(11.6011) | Bit/dim 1.0948(1.0901) | Xent 0.0000(0.0000) | Loss 1.0948(1.0901) | Error 0.0000(0.0000) Steps 554(557.25) | Grad Norm 10.0371(8.8179) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 48.3043, Epoch Time 830.9604(802.2374), Bit/dim 1.0693(best: 1.0708), Xent 0.0000, Loss 1.0693, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6540 | Time 11.7981(11.5672) | Bit/dim 1.0721(1.0865) | Xent 0.0000(0.0000) | Loss 1.0721(1.0865) | Error 0.0000(0.0000) Steps 548(556.18) | Grad Norm 0.8889(7.8116) | Total Time 10.00(10.00)\n",
      "Iter 6550 | Time 11.2887(11.5265) | Bit/dim 1.0625(1.0818) | Xent 0.0000(0.0000) | Loss 1.0625(1.0818) | Error 0.0000(0.0000) Steps 542(554.61) | Grad Norm 3.9470(6.4560) | Total Time 10.00(10.00)\n",
      "Iter 6560 | Time 11.2163(11.4860) | Bit/dim 1.0789(1.0783) | Xent 0.0000(0.0000) | Loss 1.0789(1.0783) | Error 0.0000(0.0000) Steps 542(553.00) | Grad Norm 1.9631(5.3058) | Total Time 10.00(10.00)\n",
      "Iter 6570 | Time 11.6242(11.4916) | Bit/dim 1.0746(1.0764) | Xent 0.0000(0.0000) | Loss 1.0746(1.0764) | Error 0.0000(0.0000) Steps 560(553.61) | Grad Norm 6.0309(4.7544) | Total Time 10.00(10.00)\n",
      "Iter 6580 | Time 11.6491(11.5087) | Bit/dim 1.0569(1.0736) | Xent 0.0000(0.0000) | Loss 1.0569(1.0736) | Error 0.0000(0.0000) Steps 554(554.34) | Grad Norm 2.6054(4.6765) | Total Time 10.00(10.00)\n",
      "Iter 6590 | Time 11.4567(11.5244) | Bit/dim 1.0714(1.0731) | Xent 0.0000(0.0000) | Loss 1.0714(1.0731) | Error 0.0000(0.0000) Steps 560(555.34) | Grad Norm 7.8066(5.5600) | Total Time 10.00(10.00)\n",
      "Iter 6600 | Time 11.7955(11.5632) | Bit/dim 1.1642(1.1024) | Xent 0.0000(0.0000) | Loss 1.1642(1.1024) | Error 0.0000(0.0000) Steps 578(558.50) | Grad Norm 8.7342(7.6814) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 51.6793, Epoch Time 825.7939(802.9441), Bit/dim 1.1485(best: 1.0693), Xent 0.0000, Loss 1.1485, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6610 | Time 11.5744(11.6504) | Bit/dim 1.1317(1.1064) | Xent 0.0000(0.0000) | Loss 1.1317(1.1064) | Error 0.0000(0.0000) Steps 554(562.53) | Grad Norm 12.7142(8.1608) | Total Time 10.00(10.00)\n",
      "Iter 6620 | Time 12.0410(11.6828) | Bit/dim 1.1111(1.1118) | Xent 0.0000(0.0000) | Loss 1.1111(1.1118) | Error 0.0000(0.0000) Steps 554(560.20) | Grad Norm 15.8081(9.4094) | Total Time 10.00(10.00)\n",
      "Iter 6630 | Time 11.7331(11.6227) | Bit/dim 1.0854(1.1067) | Xent 0.0000(0.0000) | Loss 1.0854(1.1067) | Error 0.0000(0.0000) Steps 542(556.99) | Grad Norm 5.0490(8.6726) | Total Time 10.00(10.00)\n",
      "Iter 6640 | Time 11.0494(11.6109) | Bit/dim 1.0810(1.1005) | Xent 0.0000(0.0000) | Loss 1.0810(1.1005) | Error 0.0000(0.0000) Steps 548(555.22) | Grad Norm 1.6516(8.8972) | Total Time 10.00(10.00)\n",
      "Iter 6650 | Time 11.0705(11.6707) | Bit/dim 1.0820(1.1009) | Xent 0.0000(0.0000) | Loss 1.0820(1.1009) | Error 0.0000(0.0000) Steps 554(556.82) | Grad Norm 8.1445(10.0646) | Total Time 10.00(10.00)\n",
      "Iter 6660 | Time 11.6441(11.6820) | Bit/dim 1.0780(1.1006) | Xent 0.0000(0.0000) | Loss 1.0780(1.1006) | Error 0.0000(0.0000) Steps 554(557.71) | Grad Norm 9.2081(10.2218) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 48.0771, Epoch Time 837.3784(803.9771), Bit/dim 1.0846(best: 1.0693), Xent 0.0000, Loss 1.0846, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6670 | Time 12.1628(11.7016) | Bit/dim 1.0974(1.0979) | Xent 0.0000(0.0000) | Loss 1.0974(1.0979) | Error 0.0000(0.0000) Steps 572(557.78) | Grad Norm 20.7741(10.2627) | Total Time 10.00(10.00)\n",
      "Iter 6680 | Time 11.3118(11.6965) | Bit/dim 1.0876(1.0966) | Xent 0.0000(0.0000) | Loss 1.0876(1.0966) | Error 0.0000(0.0000) Steps 554(557.73) | Grad Norm 9.1407(10.0213) | Total Time 10.00(10.00)\n",
      "Iter 6690 | Time 11.5627(11.6986) | Bit/dim 1.0733(1.0921) | Xent 0.0000(0.0000) | Loss 1.0733(1.0921) | Error 0.0000(0.0000) Steps 554(557.49) | Grad Norm 9.4739(9.5272) | Total Time 10.00(10.00)\n",
      "Iter 6700 | Time 11.5993(11.6416) | Bit/dim 1.0867(1.0870) | Xent 0.0000(0.0000) | Loss 1.0867(1.0870) | Error 0.0000(0.0000) Steps 560(556.86) | Grad Norm 12.2293(9.0146) | Total Time 10.00(10.00)\n",
      "Iter 6710 | Time 11.8152(11.6182) | Bit/dim 1.0668(1.0823) | Xent 0.0000(0.0000) | Loss 1.0668(1.0823) | Error 0.0000(0.0000) Steps 554(555.80) | Grad Norm 6.6081(8.3248) | Total Time 10.00(10.00)\n",
      "Iter 6720 | Time 11.5956(11.6544) | Bit/dim 1.1430(1.0853) | Xent 0.0000(0.0000) | Loss 1.1430(1.0853) | Error 0.0000(0.0000) Steps 566(557.03) | Grad Norm 15.1595(9.3800) | Total Time 10.00(10.00)\n",
      "Iter 6730 | Time 11.5859(11.6871) | Bit/dim 1.0908(1.0903) | Xent 0.0000(0.0000) | Loss 1.0908(1.0903) | Error 0.0000(0.0000) Steps 566(560.32) | Grad Norm 7.4605(9.8616) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 49.4034, Epoch Time 833.3488(804.8582), Bit/dim 1.0969(best: 1.0693), Xent 0.0000, Loss 1.0969, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6740 | Time 11.1670(11.6308) | Bit/dim 1.0779(1.0901) | Xent 0.0000(0.0000) | Loss 1.0779(1.0901) | Error 0.0000(0.0000) Steps 554(559.95) | Grad Norm 6.4789(9.3734) | Total Time 10.00(10.00)\n",
      "Iter 6750 | Time 11.7017(11.5577) | Bit/dim 1.0545(1.0852) | Xent 0.0000(0.0000) | Loss 1.0545(1.0852) | Error 0.0000(0.0000) Steps 548(556.36) | Grad Norm 5.8585(8.1237) | Total Time 10.00(10.00)\n",
      "Iter 6760 | Time 11.7773(11.5856) | Bit/dim 1.0799(1.0830) | Xent 0.0000(0.0000) | Loss 1.0799(1.0830) | Error 0.0000(0.0000) Steps 554(555.99) | Grad Norm 9.2016(8.7028) | Total Time 10.00(10.00)\n",
      "Iter 6770 | Time 11.3719(11.5715) | Bit/dim 1.0523(1.0794) | Xent 0.0000(0.0000) | Loss 1.0523(1.0794) | Error 0.0000(0.0000) Steps 554(555.93) | Grad Norm 0.9140(7.7738) | Total Time 10.00(10.00)\n",
      "Iter 6780 | Time 12.0342(11.6406) | Bit/dim 1.1656(1.0861) | Xent 0.0000(0.0000) | Loss 1.1656(1.0861) | Error 0.0000(0.0000) Steps 572(558.50) | Grad Norm 11.6332(9.3082) | Total Time 10.00(10.00)\n",
      "Iter 6790 | Time 11.7607(11.7210) | Bit/dim 1.0878(1.0928) | Xent 0.0000(0.0000) | Loss 1.0878(1.0928) | Error 0.0000(0.0000) Steps 584(564.31) | Grad Norm 7.1998(9.5605) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 49.2454, Epoch Time 831.3029(805.6516), Bit/dim 1.0763(best: 1.0693), Xent 0.0000, Loss 1.0763, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6800 | Time 11.3685(11.6563) | Bit/dim 1.0664(1.0915) | Xent 0.0000(0.0000) | Loss 1.0664(1.0915) | Error 0.0000(0.0000) Steps 548(562.91) | Grad Norm 5.0752(8.8428) | Total Time 10.00(10.00)\n",
      "Iter 6810 | Time 11.7379(11.5988) | Bit/dim 1.0659(1.0859) | Xent 0.0000(0.0000) | Loss 1.0659(1.0859) | Error 0.0000(0.0000) Steps 560(560.93) | Grad Norm 4.7937(7.5252) | Total Time 10.00(10.00)\n",
      "Iter 6820 | Time 11.8621(11.5918) | Bit/dim 1.0779(1.0827) | Xent 0.0000(0.0000) | Loss 1.0779(1.0827) | Error 0.0000(0.0000) Steps 566(559.93) | Grad Norm 10.0692(7.5515) | Total Time 10.00(10.00)\n",
      "Iter 6830 | Time 11.5304(11.6566) | Bit/dim 1.0681(1.0812) | Xent 0.0000(0.0000) | Loss 1.0681(1.0812) | Error 0.0000(0.0000) Steps 566(560.55) | Grad Norm 5.6358(7.9902) | Total Time 10.00(10.00)\n",
      "Iter 6840 | Time 11.9161(11.6630) | Bit/dim 1.0482(1.0763) | Xent 0.0000(0.0000) | Loss 1.0482(1.0763) | Error 0.0000(0.0000) Steps 560(560.07) | Grad Norm 2.9554(6.7015) | Total Time 10.00(10.00)\n",
      "Iter 6850 | Time 11.8703(11.6590) | Bit/dim 1.0552(1.0728) | Xent 0.0000(0.0000) | Loss 1.0552(1.0728) | Error 0.0000(0.0000) Steps 560(559.43) | Grad Norm 6.2293(5.7599) | Total Time 10.00(10.00)\n",
      "Iter 6860 | Time 12.6191(11.6853) | Bit/dim 1.0978(1.0742) | Xent 0.0000(0.0000) | Loss 1.0978(1.0742) | Error 0.0000(0.0000) Steps 590(560.80) | Grad Norm 29.6371(6.5872) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 52.9878, Epoch Time 837.6105(806.6104), Bit/dim 1.1346(best: 1.0693), Xent 0.0000, Loss 1.1346, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6870 | Time 12.4112(11.8365) | Bit/dim 1.1262(1.0944) | Xent 0.0000(0.0000) | Loss 1.1262(1.0944) | Error 0.0000(0.0000) Steps 590(566.85) | Grad Norm 12.7621(8.3166) | Total Time 10.00(10.00)\n",
      "Iter 6880 | Time 11.5839(11.8587) | Bit/dim 1.0940(1.0990) | Xent 0.0000(0.0000) | Loss 1.0940(1.0990) | Error 0.0000(0.0000) Steps 572(570.67) | Grad Norm 8.3514(8.8444) | Total Time 10.00(10.00)\n",
      "Iter 6890 | Time 11.7605(11.7966) | Bit/dim 1.0711(1.0944) | Xent 0.0000(0.0000) | Loss 1.0711(1.0944) | Error 0.0000(0.0000) Steps 554(567.05) | Grad Norm 6.5675(8.1754) | Total Time 10.00(10.00)\n",
      "Iter 6900 | Time 11.9147(11.7515) | Bit/dim 1.0606(1.0889) | Xent 0.0000(0.0000) | Loss 1.0606(1.0889) | Error 0.0000(0.0000) Steps 560(565.72) | Grad Norm 2.6144(7.7188) | Total Time 10.00(10.00)\n",
      "Iter 6910 | Time 11.9656(11.7719) | Bit/dim 1.0908(1.0862) | Xent 0.0000(0.0000) | Loss 1.0908(1.0862) | Error 0.0000(0.0000) Steps 572(565.69) | Grad Norm 16.3961(8.6918) | Total Time 10.00(10.00)\n",
      "Iter 6920 | Time 11.7213(11.7041) | Bit/dim 1.0687(1.0814) | Xent 0.0000(0.0000) | Loss 1.0687(1.0814) | Error 0.0000(0.0000) Steps 572(565.18) | Grad Norm 5.3410(7.7519) | Total Time 10.00(10.00)\n",
      "Iter 6930 | Time 12.4432(11.7108) | Bit/dim 1.1748(1.0846) | Xent 0.0000(0.0000) | Loss 1.1748(1.0846) | Error 0.0000(0.0000) Steps 572(565.67) | Grad Norm 11.4996(8.7071) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 50.5486, Epoch Time 842.2320(807.6790), Bit/dim 1.1471(best: 1.0693), Xent 0.0000, Loss 1.1471, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 6940 | Time 11.4796(11.7373) | Bit/dim 1.0898(1.0910) | Xent 0.0000(0.0000) | Loss 1.0898(1.0910) | Error 0.0000(0.0000) Steps 566(568.74) | Grad Norm 4.2393(8.3515) | Total Time 10.00(10.00)\n",
      "Iter 6950 | Time 12.1358(11.8015) | Bit/dim 1.1241(1.0934) | Xent 0.0000(0.0000) | Loss 1.1241(1.0934) | Error 0.0000(0.0000) Steps 566(569.48) | Grad Norm 11.7715(9.4237) | Total Time 10.00(10.00)\n",
      "Iter 6960 | Time 12.0806(11.8696) | Bit/dim 1.0941(1.0981) | Xent 0.0000(0.0000) | Loss 1.0941(1.0981) | Error 0.0000(0.0000) Steps 584(571.98) | Grad Norm 6.5406(9.9233) | Total Time 10.00(10.00)\n",
      "Iter 6970 | Time 11.8883(11.8578) | Bit/dim 1.0966(1.0951) | Xent 0.0000(0.0000) | Loss 1.0966(1.0951) | Error 0.0000(0.0000) Steps 566(571.00) | Grad Norm 13.3198(9.2770) | Total Time 10.00(10.00)\n",
      "Iter 6980 | Time 11.2028(11.7214) | Bit/dim 1.0611(1.0891) | Xent 0.0000(0.0000) | Loss 1.0611(1.0891) | Error 0.0000(0.0000) Steps 554(566.24) | Grad Norm 2.6752(8.3747) | Total Time 10.00(10.00)\n",
      "Iter 6990 | Time 10.9672(11.6880) | Bit/dim 1.0714(1.0840) | Xent 0.0000(0.0000) | Loss 1.0714(1.0840) | Error 0.0000(0.0000) Steps 554(563.17) | Grad Norm 13.0127(7.9904) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 48.3898, Epoch Time 840.1225(808.6523), Bit/dim 1.0602(best: 1.0693), Xent 0.0000, Loss 1.0602, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7000 | Time 12.0635(11.7116) | Bit/dim 1.0853(1.0817) | Xent 0.0000(0.0000) | Loss 1.0853(1.0817) | Error 0.0000(0.0000) Steps 572(563.52) | Grad Norm 17.6676(8.4599) | Total Time 10.00(10.00)\n",
      "Iter 7010 | Time 11.6135(11.7935) | Bit/dim 1.1198(1.0857) | Xent 0.0000(0.0000) | Loss 1.1198(1.0857) | Error 0.0000(0.0000) Steps 554(565.94) | Grad Norm 11.8923(9.6311) | Total Time 10.00(10.00)\n",
      "Iter 7020 | Time 11.9976(11.8634) | Bit/dim 1.0986(1.0926) | Xent 0.0000(0.0000) | Loss 1.0986(1.0926) | Error 0.0000(0.0000) Steps 572(570.69) | Grad Norm 7.3101(9.7508) | Total Time 10.00(10.00)\n",
      "Iter 7030 | Time 11.3499(11.8110) | Bit/dim 1.0677(1.0897) | Xent 0.0000(0.0000) | Loss 1.0677(1.0897) | Error 0.0000(0.0000) Steps 560(570.56) | Grad Norm 2.5613(8.6655) | Total Time 10.00(10.00)\n",
      "Iter 7040 | Time 11.2962(11.7505) | Bit/dim 1.0557(1.0838) | Xent 0.0000(0.0000) | Loss 1.0557(1.0838) | Error 0.0000(0.0000) Steps 566(569.63) | Grad Norm 1.4668(6.9803) | Total Time 10.00(10.00)\n",
      "Iter 7050 | Time 11.5176(11.7054) | Bit/dim 1.0756(1.0792) | Xent 0.0000(0.0000) | Loss 1.0756(1.0792) | Error 0.0000(0.0000) Steps 554(567.00) | Grad Norm 1.4318(5.6204) | Total Time 10.00(10.00)\n",
      "Iter 7060 | Time 11.7319(11.6364) | Bit/dim 1.0588(1.0748) | Xent 0.0000(0.0000) | Loss 1.0588(1.0748) | Error 0.0000(0.0000) Steps 560(564.81) | Grad Norm 0.6256(4.4058) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 49.1323, Epoch Time 837.5623(809.5196), Bit/dim 1.0553(best: 1.0602), Xent 0.0000, Loss 1.0553, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7070 | Time 11.9756(11.5896) | Bit/dim 1.0548(1.0705) | Xent 0.0000(0.0000) | Loss 1.0548(1.0705) | Error 0.0000(0.0000) Steps 560(563.55) | Grad Norm 0.5292(3.4505) | Total Time 10.00(10.00)\n",
      "Iter 7080 | Time 11.6325(11.5983) | Bit/dim 1.0540(1.0682) | Xent 0.0000(0.0000) | Loss 1.0540(1.0682) | Error 0.0000(0.0000) Steps 566(562.31) | Grad Norm 2.6044(2.8248) | Total Time 10.00(10.00)\n",
      "Iter 7090 | Time 11.3932(11.6332) | Bit/dim 1.0562(1.0681) | Xent 0.0000(0.0000) | Loss 1.0562(1.0681) | Error 0.0000(0.0000) Steps 566(562.54) | Grad Norm 6.4183(4.1331) | Total Time 10.00(10.00)\n",
      "Iter 7100 | Time 11.5007(11.6839) | Bit/dim 1.1244(1.0932) | Xent 0.0000(0.0000) | Loss 1.1244(1.0932) | Error 0.0000(0.0000) Steps 578(566.17) | Grad Norm 7.1944(6.3423) | Total Time 10.00(10.00)\n",
      "Iter 7110 | Time 11.2466(11.7167) | Bit/dim 1.0772(1.0936) | Xent 0.0000(0.0000) | Loss 1.0772(1.0936) | Error 0.0000(0.0000) Steps 560(570.30) | Grad Norm 4.7636(6.8172) | Total Time 10.00(10.00)\n",
      "Iter 7120 | Time 11.8045(11.7493) | Bit/dim 1.1274(1.0971) | Xent 0.0000(0.0000) | Loss 1.1274(1.0971) | Error 0.0000(0.0000) Steps 584(570.89) | Grad Norm 11.2706(8.6311) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 49.9121, Epoch Time 841.3691(810.4751), Bit/dim 1.0743(best: 1.0553), Xent 0.0000, Loss 1.0743, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7130 | Time 12.0092(11.8242) | Bit/dim 1.0872(1.0940) | Xent 0.0000(0.0000) | Loss 1.0872(1.0940) | Error 0.0000(0.0000) Steps 578(574.32) | Grad Norm 6.5738(8.3119) | Total Time 10.00(10.00)\n",
      "Iter 7140 | Time 11.4339(11.7591) | Bit/dim 1.0789(1.0897) | Xent 0.0000(0.0000) | Loss 1.0789(1.0897) | Error 0.0000(0.0000) Steps 560(571.62) | Grad Norm 11.6565(7.8128) | Total Time 10.00(10.00)\n",
      "Iter 7150 | Time 11.2241(11.7142) | Bit/dim 1.0649(1.0847) | Xent 0.0000(0.0000) | Loss 1.0649(1.0847) | Error 0.0000(0.0000) Steps 560(568.90) | Grad Norm 1.9908(8.1042) | Total Time 10.00(10.00)\n",
      "Iter 7160 | Time 11.4953(11.6992) | Bit/dim 1.0941(1.0813) | Xent 0.0000(0.0000) | Loss 1.0941(1.0813) | Error 0.0000(0.0000) Steps 566(568.62) | Grad Norm 10.5074(8.6032) | Total Time 10.00(10.00)\n",
      "Iter 7170 | Time 11.9445(11.7243) | Bit/dim 1.0901(1.0800) | Xent 0.0000(0.0000) | Loss 1.0901(1.0800) | Error 0.0000(0.0000) Steps 584(569.39) | Grad Norm 19.5495(9.1808) | Total Time 10.00(10.00)\n",
      "Iter 7180 | Time 11.8970(11.7562) | Bit/dim 1.1154(1.0818) | Xent 0.0000(0.0000) | Loss 1.1154(1.0818) | Error 0.0000(0.0000) Steps 572(569.88) | Grad Norm 11.4077(9.6909) | Total Time 10.00(10.00)\n",
      "Iter 7190 | Time 11.5861(11.7951) | Bit/dim 1.0810(1.0816) | Xent 0.0000(0.0000) | Loss 1.0810(1.0816) | Error 0.0000(0.0000) Steps 578(572.79) | Grad Norm 7.7247(9.3482) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 49.8965, Epoch Time 840.2244(811.3676), Bit/dim 1.0616(best: 1.0553), Xent 0.0000, Loss 1.0616, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7200 | Time 11.4369(11.7389) | Bit/dim 1.0529(1.0766) | Xent 0.0000(0.0000) | Loss 1.0529(1.0766) | Error 0.0000(0.0000) Steps 566(572.98) | Grad Norm 1.4275(8.0489) | Total Time 10.00(10.00)\n",
      "Iter 7210 | Time 12.0611(11.7167) | Bit/dim 1.0675(1.0746) | Xent 0.0000(0.0000) | Loss 1.0675(1.0746) | Error 0.0000(0.0000) Steps 560(570.95) | Grad Norm 17.3872(7.9003) | Total Time 10.00(10.00)\n",
      "Iter 7220 | Time 11.8423(11.7276) | Bit/dim 1.0610(1.0728) | Xent 0.0000(0.0000) | Loss 1.0610(1.0728) | Error 0.0000(0.0000) Steps 566(569.95) | Grad Norm 4.0784(7.8529) | Total Time 10.00(10.00)\n",
      "Iter 7230 | Time 11.9286(11.7340) | Bit/dim 1.0999(1.0731) | Xent 0.0000(0.0000) | Loss 1.0999(1.0731) | Error 0.0000(0.0000) Steps 572(569.61) | Grad Norm 10.8027(8.2462) | Total Time 10.00(10.00)\n",
      "Iter 7240 | Time 12.1108(11.8221) | Bit/dim 1.0892(1.0761) | Xent 0.0000(0.0000) | Loss 1.0892(1.0761) | Error 0.0000(0.0000) Steps 596(573.08) | Grad Norm 17.5404(9.1004) | Total Time 10.00(10.00)\n",
      "Iter 7250 | Time 11.4249(11.9442) | Bit/dim 1.0864(1.0794) | Xent 0.0000(0.0000) | Loss 1.0864(1.0794) | Error 0.0000(0.0000) Steps 578(575.26) | Grad Norm 8.2789(9.5449) | Total Time 10.00(10.00)\n",
      "Iter 7260 | Time 11.5280(11.9154) | Bit/dim 1.0603(1.0768) | Xent 0.0000(0.0000) | Loss 1.0603(1.0768) | Error 0.0000(0.0000) Steps 578(575.52) | Grad Norm 5.5155(8.3547) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 50.5113, Epoch Time 848.1143(812.4700), Bit/dim 1.0608(best: 1.0553), Xent 0.0000, Loss 1.0608, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7270 | Time 12.1012(11.8628) | Bit/dim 1.0610(1.0725) | Xent 0.0000(0.0000) | Loss 1.0610(1.0725) | Error 0.0000(0.0000) Steps 584(575.44) | Grad Norm 6.5037(7.4612) | Total Time 10.00(10.00)\n",
      "Iter 7280 | Time 11.8655(11.8361) | Bit/dim 1.0921(1.0714) | Xent 0.0000(0.0000) | Loss 1.0921(1.0714) | Error 0.0000(0.0000) Steps 578(574.53) | Grad Norm 10.5226(8.1706) | Total Time 10.00(10.00)\n",
      "Iter 7290 | Time 12.7829(11.8805) | Bit/dim 1.1208(1.0769) | Xent 0.0000(0.0000) | Loss 1.1208(1.0769) | Error 0.0000(0.0000) Steps 614(577.52) | Grad Norm 19.5975(9.3583) | Total Time 10.00(10.00)\n",
      "Iter 7300 | Time 11.1465(11.8313) | Bit/dim 1.0656(1.0762) | Xent 0.0000(0.0000) | Loss 1.0656(1.0762) | Error 0.0000(0.0000) Steps 578(578.80) | Grad Norm 5.3922(8.5391) | Total Time 10.00(10.00)\n",
      "Iter 7310 | Time 11.5010(11.7917) | Bit/dim 1.0562(1.0737) | Xent 0.0000(0.0000) | Loss 1.0562(1.0737) | Error 0.0000(0.0000) Steps 578(578.31) | Grad Norm 2.0627(7.1236) | Total Time 10.00(10.00)\n",
      "Iter 7320 | Time 11.3339(11.7343) | Bit/dim 1.0503(1.0703) | Xent 0.0000(0.0000) | Loss 1.0503(1.0703) | Error 0.0000(0.0000) Steps 560(575.81) | Grad Norm 1.8338(5.9955) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 49.6185, Epoch Time 837.5549(813.2225), Bit/dim 1.0520(best: 1.0553), Xent 0.0000, Loss 1.0520, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7330 | Time 11.4893(11.6684) | Bit/dim 1.0486(1.0664) | Xent 0.0000(0.0000) | Loss 1.0486(1.0664) | Error 0.0000(0.0000) Steps 572(573.05) | Grad Norm 1.3466(5.1101) | Total Time 10.00(10.00)\n",
      "Iter 7340 | Time 11.9755(11.6929) | Bit/dim 1.0465(1.0633) | Xent 0.0000(0.0000) | Loss 1.0465(1.0633) | Error 0.0000(0.0000) Steps 578(572.10) | Grad Norm 5.6512(4.7665) | Total Time 10.00(10.00)\n",
      "Iter 7350 | Time 11.9310(11.6638) | Bit/dim 1.0665(1.0625) | Xent 0.0000(0.0000) | Loss 1.0665(1.0625) | Error 0.0000(0.0000) Steps 578(570.20) | Grad Norm 9.4517(4.9692) | Total Time 10.00(10.00)\n",
      "Iter 7360 | Time 11.6095(11.6491) | Bit/dim 1.0700(1.0603) | Xent 0.0000(0.0000) | Loss 1.0700(1.0603) | Error 0.0000(0.0000) Steps 560(569.27) | Grad Norm 6.8121(5.2584) | Total Time 10.00(10.00)\n",
      "Iter 7370 | Time 11.2782(11.6351) | Bit/dim 1.0564(1.0601) | Xent 0.0000(0.0000) | Loss 1.0564(1.0601) | Error 0.0000(0.0000) Steps 560(568.45) | Grad Norm 2.8791(5.5728) | Total Time 10.00(10.00)\n",
      "Iter 7380 | Time 12.1286(11.7233) | Bit/dim 1.1505(1.0848) | Xent 0.0000(0.0000) | Loss 1.1505(1.0848) | Error 0.0000(0.0000) Steps 590(572.32) | Grad Norm 5.5953(7.7746) | Total Time 10.00(10.00)\n",
      "Iter 7390 | Time 11.9849(11.8295) | Bit/dim 1.0864(1.0945) | Xent 0.0000(0.0000) | Loss 1.0864(1.0945) | Error 0.0000(0.0000) Steps 578(576.11) | Grad Norm 3.0434(8.1521) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 49.5375, Epoch Time 841.3784(814.0672), Bit/dim 1.0883(best: 1.0520), Xent 0.0000, Loss 1.0883, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7400 | Time 11.4636(11.7752) | Bit/dim 1.0722(1.0917) | Xent 0.0000(0.0000) | Loss 1.0722(1.0917) | Error 0.0000(0.0000) Steps 554(573.96) | Grad Norm 1.6352(7.4887) | Total Time 10.00(10.00)\n",
      "Iter 7410 | Time 11.7320(11.7063) | Bit/dim 1.0704(1.0855) | Xent 0.0000(0.0000) | Loss 1.0704(1.0855) | Error 0.0000(0.0000) Steps 566(571.88) | Grad Norm 3.6705(6.3201) | Total Time 10.00(10.00)\n",
      "Iter 7420 | Time 11.3780(11.6997) | Bit/dim 1.0590(1.0787) | Xent 0.0000(0.0000) | Loss 1.0590(1.0787) | Error 0.0000(0.0000) Steps 560(570.48) | Grad Norm 6.8201(5.8415) | Total Time 10.00(10.00)\n",
      "Iter 7430 | Time 12.0504(11.7058) | Bit/dim 1.0732(1.0767) | Xent 0.0000(0.0000) | Loss 1.0732(1.0767) | Error 0.0000(0.0000) Steps 578(571.04) | Grad Norm 9.9364(6.3097) | Total Time 10.00(10.00)\n",
      "Iter 7440 | Time 12.4349(11.8480) | Bit/dim 1.1131(1.0889) | Xent 0.0000(0.0000) | Loss 1.1131(1.0889) | Error 0.0000(0.0000) Steps 602(576.56) | Grad Norm 16.5062(7.9591) | Total Time 10.00(10.00)\n",
      "Iter 7450 | Time 11.0511(11.8262) | Bit/dim 1.0755(1.0875) | Xent 0.0000(0.0000) | Loss 1.0755(1.0875) | Error 0.0000(0.0000) Steps 572(577.41) | Grad Norm 8.5683(8.3896) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 49.8106, Epoch Time 841.4746(814.8894), Bit/dim 1.0604(best: 1.0520), Xent 0.0000, Loss 1.0604, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7460 | Time 12.0641(11.8191) | Bit/dim 1.0680(1.0820) | Xent 0.0000(0.0000) | Loss 1.0680(1.0820) | Error 0.0000(0.0000) Steps 578(576.67) | Grad Norm 7.0526(8.0600) | Total Time 10.00(10.00)\n",
      "Iter 7470 | Time 11.2951(11.7800) | Bit/dim 1.0599(1.0764) | Xent 0.0000(0.0000) | Loss 1.0599(1.0764) | Error 0.0000(0.0000) Steps 572(574.84) | Grad Norm 6.9196(7.7522) | Total Time 10.00(10.00)\n",
      "Iter 7480 | Time 11.7981(11.8653) | Bit/dim 1.0792(1.0746) | Xent 0.0000(0.0000) | Loss 1.0792(1.0746) | Error 0.0000(0.0000) Steps 584(576.31) | Grad Norm 9.7190(8.4174) | Total Time 10.00(10.00)\n",
      "Iter 7490 | Time 11.8701(11.8761) | Bit/dim 1.0562(1.0737) | Xent 0.0000(0.0000) | Loss 1.0562(1.0737) | Error 0.0000(0.0000) Steps 578(577.69) | Grad Norm 7.3199(8.9846) | Total Time 10.00(10.00)\n",
      "Iter 7500 | Time 11.8627(11.8688) | Bit/dim 1.0654(1.0732) | Xent 0.0000(0.0000) | Loss 1.0654(1.0732) | Error 0.0000(0.0000) Steps 584(577.29) | Grad Norm 17.1456(8.9315) | Total Time 10.00(10.00)\n",
      "Iter 7510 | Time 11.4578(11.8722) | Bit/dim 1.0774(1.0727) | Xent 0.0000(0.0000) | Loss 1.0774(1.0727) | Error 0.0000(0.0000) Steps 578(577.93) | Grad Norm 3.4230(8.8498) | Total Time 10.00(10.00)\n",
      "Iter 7520 | Time 11.9757(11.8996) | Bit/dim 1.0753(1.0727) | Xent 0.0000(0.0000) | Loss 1.0753(1.0727) | Error 0.0000(0.0000) Steps 578(578.74) | Grad Norm 7.3802(8.9975) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 50.9946, Epoch Time 850.9264(815.9705), Bit/dim 1.0643(best: 1.0520), Xent 0.0000, Loss 1.0643, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7530 | Time 11.5682(11.8712) | Bit/dim 1.0606(1.0689) | Xent 0.0000(0.0000) | Loss 1.0606(1.0689) | Error 0.0000(0.0000) Steps 572(578.21) | Grad Norm 5.6922(8.3866) | Total Time 10.00(10.00)\n",
      "Iter 7540 | Time 11.5865(11.8468) | Bit/dim 1.0763(1.0676) | Xent 0.0000(0.0000) | Loss 1.0763(1.0676) | Error 0.0000(0.0000) Steps 578(578.01) | Grad Norm 10.1728(8.4889) | Total Time 10.00(10.00)\n",
      "Iter 7550 | Time 12.0674(11.9246) | Bit/dim 1.1066(1.0701) | Xent 0.0000(0.0000) | Loss 1.1066(1.0701) | Error 0.0000(0.0000) Steps 578(579.75) | Grad Norm 12.6271(9.3231) | Total Time 10.00(10.00)\n",
      "Iter 7560 | Time 12.0422(11.9517) | Bit/dim 1.0781(1.0739) | Xent 0.0000(0.0000) | Loss 1.0781(1.0739) | Error 0.0000(0.0000) Steps 596(581.58) | Grad Norm 6.2173(9.0710) | Total Time 10.00(10.00)\n",
      "Iter 7570 | Time 11.7325(11.8433) | Bit/dim 1.0593(1.0710) | Xent 0.0000(0.0000) | Loss 1.0593(1.0710) | Error 0.0000(0.0000) Steps 572(580.43) | Grad Norm 3.8389(7.8465) | Total Time 10.00(10.00)\n",
      "Iter 7580 | Time 11.8312(11.7985) | Bit/dim 1.0523(1.0676) | Xent 0.0000(0.0000) | Loss 1.0523(1.0676) | Error 0.0000(0.0000) Steps 572(580.14) | Grad Norm 1.8598(6.4457) | Total Time 10.00(10.00)\n",
      "Iter 7590 | Time 11.5816(11.7151) | Bit/dim 1.0600(1.0642) | Xent 0.0000(0.0000) | Loss 1.0600(1.0642) | Error 0.0000(0.0000) Steps 566(576.34) | Grad Norm 1.2374(5.3293) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 49.8276, Epoch Time 842.0014(816.7515), Bit/dim 1.0480(best: 1.0520), Xent 0.0000, Loss 1.0480, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7600 | Time 11.5594(11.6770) | Bit/dim 1.0511(1.0612) | Xent 0.0000(0.0000) | Loss 1.0511(1.0612) | Error 0.0000(0.0000) Steps 566(573.32) | Grad Norm 7.8953(4.6357) | Total Time 10.00(10.00)\n",
      "Iter 7610 | Time 12.1546(11.6806) | Bit/dim 1.0950(1.0594) | Xent 0.0000(0.0000) | Loss 1.0950(1.0594) | Error 0.0000(0.0000) Steps 584(572.71) | Grad Norm 33.3298(6.1471) | Total Time 10.00(10.00)\n",
      "Iter 7620 | Time 12.1374(11.7654) | Bit/dim 1.1787(1.0873) | Xent 0.0000(0.0000) | Loss 1.1787(1.0873) | Error 0.0000(0.0000) Steps 590(575.80) | Grad Norm 7.5123(7.5865) | Total Time 10.00(10.00)\n",
      "Iter 7630 | Time 11.5441(11.8437) | Bit/dim 1.0917(1.0914) | Xent 0.0000(0.0000) | Loss 1.0917(1.0914) | Error 0.0000(0.0000) Steps 572(578.06) | Grad Norm 3.3664(7.2223) | Total Time 10.00(10.00)\n",
      "Iter 7640 | Time 11.8859(11.7869) | Bit/dim 1.0760(1.0885) | Xent 0.0000(0.0000) | Loss 1.0760(1.0885) | Error 0.0000(0.0000) Steps 572(576.60) | Grad Norm 2.8594(6.4804) | Total Time 10.00(10.00)\n",
      "Iter 7650 | Time 11.6888(11.8484) | Bit/dim 1.1529(1.0917) | Xent 0.0000(0.0000) | Loss 1.1529(1.0917) | Error 0.0000(0.0000) Steps 572(576.38) | Grad Norm 11.0762(8.1868) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 51.1384, Epoch Time 849.4654(817.7329), Bit/dim 1.0876(best: 1.0480), Xent 0.0000, Loss 1.0876, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7660 | Time 12.3448(11.9915) | Bit/dim 1.1156(1.0976) | Xent 0.0000(0.0000) | Loss 1.1156(1.0976) | Error 0.0000(0.0000) Steps 584(581.50) | Grad Norm 7.2024(8.8394) | Total Time 10.00(10.00)\n",
      "Iter 7670 | Time 11.4857(11.9701) | Bit/dim 1.0698(1.0926) | Xent 0.0000(0.0000) | Loss 1.0698(1.0926) | Error 0.0000(0.0000) Steps 584(582.86) | Grad Norm 2.7155(7.6304) | Total Time 10.00(10.00)\n",
      "Iter 7680 | Time 11.7471(11.9346) | Bit/dim 1.0519(1.0844) | Xent 0.0000(0.0000) | Loss 1.0519(1.0844) | Error 0.0000(0.0000) Steps 584(583.60) | Grad Norm 1.1096(6.2076) | Total Time 10.00(10.00)\n",
      "Iter 7690 | Time 11.5607(11.8752) | Bit/dim 1.0551(1.0764) | Xent 0.0000(0.0000) | Loss 1.0551(1.0764) | Error 0.0000(0.0000) Steps 566(580.47) | Grad Norm 3.0204(5.1784) | Total Time 10.00(10.00)\n",
      "Iter 7700 | Time 11.6212(11.8053) | Bit/dim 1.0511(1.0703) | Xent 0.0000(0.0000) | Loss 1.0511(1.0703) | Error 0.0000(0.0000) Steps 566(576.68) | Grad Norm 1.0665(4.6504) | Total Time 10.00(10.00)\n",
      "Iter 7710 | Time 11.1486(11.7216) | Bit/dim 1.0399(1.0655) | Xent 0.0000(0.0000) | Loss 1.0399(1.0655) | Error 0.0000(0.0000) Steps 560(573.22) | Grad Norm 3.4410(4.5057) | Total Time 10.00(10.00)\n",
      "Iter 7720 | Time 11.3836(11.6838) | Bit/dim 1.0669(1.0627) | Xent 0.0000(0.0000) | Loss 1.0669(1.0627) | Error 0.0000(0.0000) Steps 566(571.82) | Grad Norm 4.6692(5.1756) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 49.7282, Epoch Time 839.8977(818.3978), Bit/dim 1.0473(best: 1.0480), Xent 0.0000, Loss 1.0473, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 7730 | Time 11.8207(11.7177) | Bit/dim 1.0488(1.0593) | Xent 0.0000(0.0000) | Loss 1.0488(1.0593) | Error 0.0000(0.0000) Steps 566(570.96) | Grad Norm 13.2840(5.3059) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_published_baseline_bs900_1 --seed 1 --conditional False --controlled_tol False  --log_freq 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_published_baseline_bs900_1 --resume ../experiments_published/cnf_published_baseline_bs900_1/current_checkpt.pth --seed 1 --conditional False --controlled_tol False  --log_freq 10\n",
    "# #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
