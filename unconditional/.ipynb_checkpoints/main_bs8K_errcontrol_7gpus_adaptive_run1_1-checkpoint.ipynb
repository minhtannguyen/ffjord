{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=False, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_published_bs8K_errcontrol_7gpus_1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 60.4748(60.4748) | Bit/dim 33.9420(33.9420) | Xent 0.0000(0.0000) | Loss 33.9420(33.9420) | Error 0.0000(0.0000) Steps 290(290.00) | Grad Norm 272.9114(272.9114) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 22.7339(59.3426) | Bit/dim 24.7959(33.6677) | Xent 0.0000(0.0000) | Loss 24.7959(33.6677) | Error 0.0000(0.0000) Steps 290(290.00) | Grad Norm 197.9274(270.6618) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 24.7947(58.3062) | Bit/dim 16.6920(33.1584) | Xent 0.0000(0.0000) | Loss 16.6920(33.1584) | Error 0.0000(0.0000) Steps 314(290.72) | Grad Norm 109.2051(265.8181) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 27.3259(57.3768) | Bit/dim 13.7901(32.5773) | Xent 0.0000(0.0000) | Loss 13.7901(32.5773) | Error 0.0000(0.0000) Steps 374(293.22) | Grad Norm 46.6063(259.2418) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 31.2288(56.5923) | Bit/dim 15.5340(32.0660) | Xent 0.0000(0.0000) | Loss 15.5340(32.0660) | Error 0.0000(0.0000) Steps 410(296.72) | Grad Norm 111.4972(254.8095) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 32.3863(55.8662) | Bit/dim 15.6230(31.5727) | Xent 0.0000(0.0000) | Loss 15.6230(31.5727) | Error 0.0000(0.0000) Steps 416(300.30) | Grad Norm 128.5398(251.0214) | Total Time 10.00(10.00)\n",
      "Iter 0007 | Time 28.7875(55.0538) | Bit/dim 13.0216(31.0162) | Xent 0.0000(0.0000) | Loss 13.0216(31.0162) | Error 0.0000(0.0000) Steps 404(303.41) | Grad Norm 100.5661(246.5077) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 23.3447, Epoch Time 263.6855(263.6855), Bit/dim 10.2876(best: inf), Xent 0.0000, Loss 10.2876, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0008 | Time 32.6200(54.3808) | Bit/dim 10.1413(30.3900) | Xent 0.0000(0.0000) | Loss 10.1413(30.3900) | Error 0.0000(0.0000) Steps 392(306.07) | Grad Norm 62.8276(240.9973) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 29.8090(53.6436) | Bit/dim 8.3689(29.7293) | Xent 0.0000(0.0000) | Loss 8.3689(29.7293) | Error 0.0000(0.0000) Steps 380(308.29) | Grad Norm 32.8532(234.7530) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 28.0668(52.8763) | Bit/dim 7.8069(29.0717) | Xent 0.0000(0.0000) | Loss 7.8069(29.0717) | Error 0.0000(0.0000) Steps 344(309.36) | Grad Norm 24.7525(228.4530) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 27.8852(52.1266) | Bit/dim 7.8073(28.4337) | Xent 0.0000(0.0000) | Loss 7.8073(28.4337) | Error 0.0000(0.0000) Steps 338(310.22) | Grad Norm 33.4442(222.6027) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 25.9269(51.3406) | Bit/dim 7.6509(27.8102) | Xent 0.0000(0.0000) | Loss 7.6509(27.8102) | Error 0.0000(0.0000) Steps 338(311.05) | Grad Norm 38.4128(217.0770) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 27.7892(50.6341) | Bit/dim 6.9293(27.1838) | Xent 0.0000(0.0000) | Loss 6.9293(27.1838) | Error 0.0000(0.0000) Steps 338(311.86) | Grad Norm 35.9472(211.6431) | Total Time 10.00(10.00)\n",
      "Iter 0014 | Time 26.1876(49.9007) | Bit/dim 5.8219(26.5430) | Xent 0.0000(0.0000) | Loss 5.8219(26.5430) | Error 0.0000(0.0000) Steps 350(313.00) | Grad Norm 28.2630(206.1417) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 12.1852, Epoch Time 222.8682(262.4610), Bit/dim 4.7332(best: 10.2876), Xent 0.0000, Loss 4.7332, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0015 | Time 28.3931(49.2554) | Bit/dim 4.7154(25.8881) | Xent 0.0000(0.0000) | Loss 4.7154(25.8881) | Error 0.0000(0.0000) Steps 350(314.11) | Grad Norm 18.7810(200.5209) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 29.7998(48.6718) | Bit/dim 3.9311(25.2294) | Xent 0.0000(0.0000) | Loss 3.9311(25.2294) | Error 0.0000(0.0000) Steps 374(315.91) | Grad Norm 10.6608(194.8251) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 28.3864(48.0632) | Bit/dim 3.4953(24.5774) | Xent 0.0000(0.0000) | Loss 3.4953(24.5774) | Error 0.0000(0.0000) Steps 362(317.29) | Grad Norm 6.6927(189.1811) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 28.8420(47.4866) | Bit/dim 3.3183(23.9396) | Xent 0.0000(0.0000) | Loss 3.3183(23.9396) | Error 0.0000(0.0000) Steps 356(318.45) | Grad Norm 7.3067(183.7249) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 29.1998(46.9380) | Bit/dim 3.2681(23.3195) | Xent 0.0000(0.0000) | Loss 3.2681(23.3195) | Error 0.0000(0.0000) Steps 356(319.58) | Grad Norm 8.6178(178.4717) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 27.5828(46.3573) | Bit/dim 3.1944(22.7157) | Xent 0.0000(0.0000) | Loss 3.1944(22.7157) | Error 0.0000(0.0000) Steps 368(321.03) | Grad Norm 8.8840(173.3840) | Total Time 10.00(10.00)\n",
      "Iter 0021 | Time 31.8393(45.9218) | Bit/dim 3.0981(22.1272) | Xent 0.0000(0.0000) | Loss 3.0981(22.1272) | Error 0.0000(0.0000) Steps 386(322.98) | Grad Norm 8.1896(168.4282) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 13.3920, Epoch Time 229.7862(261.4807), Bit/dim 2.9594(best: 4.7332), Xent 0.0000, Loss 2.9594, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0022 | Time 30.3028(45.4532) | Bit/dim 2.9723(21.5526) | Xent 0.0000(0.0000) | Loss 2.9723(21.5526) | Error 0.0000(0.0000) Steps 392(325.05) | Grad Norm 6.9567(163.5841) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 34.8330(45.1346) | Bit/dim 2.8682(20.9920) | Xent 0.0000(0.0000) | Loss 2.8682(20.9920) | Error 0.0000(0.0000) Steps 404(327.42) | Grad Norm 5.6941(158.8474) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 35.0883(44.8332) | Bit/dim 2.8099(20.4466) | Xent 0.0000(0.0000) | Loss 2.8099(20.4466) | Error 0.0000(0.0000) Steps 404(329.72) | Grad Norm 4.7655(154.2249) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 36.7185(44.5898) | Bit/dim 2.7906(19.9169) | Xent 0.0000(0.0000) | Loss 2.7906(19.9169) | Error 0.0000(0.0000) Steps 410(332.13) | Grad Norm 4.3850(149.7297) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 38.5096(44.4074) | Bit/dim 2.8185(19.4039) | Xent 0.0000(0.0000) | Loss 2.8185(19.4039) | Error 0.0000(0.0000) Steps 416(334.64) | Grad Norm 4.4370(145.3709) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 39.5804(44.2626) | Bit/dim 2.8530(18.9074) | Xent 0.0000(0.0000) | Loss 2.8530(18.9074) | Error 0.0000(0.0000) Steps 416(337.08) | Grad Norm 4.6348(141.1488) | Total Time 10.00(10.00)\n",
      "Iter 0028 | Time 40.0771(44.1370) | Bit/dim 2.8946(18.4270) | Xent 0.0000(0.0000) | Loss 2.8946(18.4270) | Error 0.0000(0.0000) Steps 434(339.99) | Grad Norm 4.7082(137.0556) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 14.9673, Epoch Time 282.2824(262.1048), Bit/dim 2.9111(best: 2.9594), Xent 0.0000, Loss 2.9111, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0029 | Time 40.5691(44.0300) | Bit/dim 2.9235(17.9619) | Xent 0.0000(0.0000) | Loss 2.9235(17.9619) | Error 0.0000(0.0000) Steps 440(342.99) | Grad Norm 4.6187(133.0825) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 39.0511(43.8806) | Bit/dim 2.9212(17.5107) | Xent 0.0000(0.0000) | Loss 2.9212(17.5107) | Error 0.0000(0.0000) Steps 434(345.72) | Grad Norm 4.4553(129.2237) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 42.5674(43.8412) | Bit/dim 2.9091(17.0726) | Xent 0.0000(0.0000) | Loss 2.9091(17.0726) | Error 0.0000(0.0000) Steps 446(348.73) | Grad Norm 4.3643(125.4779) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 44.8725(43.8721) | Bit/dim 2.8803(16.6469) | Xent 0.0000(0.0000) | Loss 2.8803(16.6469) | Error 0.0000(0.0000) Steps 446(351.65) | Grad Norm 4.3741(121.8448) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 39.9424(43.7542) | Bit/dim 2.8269(16.2323) | Xent 0.0000(0.0000) | Loss 2.8269(16.2323) | Error 0.0000(0.0000) Steps 440(354.30) | Grad Norm 4.3778(118.3208) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 37.5330(43.5676) | Bit/dim 2.7551(15.8280) | Xent 0.0000(0.0000) | Loss 2.7551(15.8280) | Error 0.0000(0.0000) Steps 434(356.69) | Grad Norm 4.2369(114.8983) | Total Time 10.00(10.00)\n",
      "Iter 0035 | Time 43.2195(43.5572) | Bit/dim 2.6722(15.4333) | Xent 0.0000(0.0000) | Loss 2.6722(15.4333) | Error 0.0000(0.0000) Steps 452(359.55) | Grad Norm 3.9084(111.5686) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 14.6586, Epoch Time 314.8124(263.6860), Bit/dim 2.5741(best: 2.9111), Xent 0.0000, Loss 2.5741, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0036 | Time 38.2999(43.3994) | Bit/dim 2.5831(15.0478) | Xent 0.0000(0.0000) | Loss 2.5831(15.0478) | Error 0.0000(0.0000) Steps 440(361.96) | Grad Norm 3.4251(108.3243) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 36.2498(43.1850) | Bit/dim 2.5007(14.6714) | Xent 0.0000(0.0000) | Loss 2.5007(14.6714) | Error 0.0000(0.0000) Steps 434(364.12) | Grad Norm 2.8370(105.1597) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 36.3239(42.9791) | Bit/dim 2.4401(14.3044) | Xent 0.0000(0.0000) | Loss 2.4401(14.3044) | Error 0.0000(0.0000) Steps 422(365.86) | Grad Norm 2.2569(102.0726) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 34.6040(42.7279) | Bit/dim 2.3931(13.9471) | Xent 0.0000(0.0000) | Loss 2.3931(13.9471) | Error 0.0000(0.0000) Steps 410(367.18) | Grad Norm 1.8046(99.0645) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 34.4597(42.4798) | Bit/dim 2.3753(13.5999) | Xent 0.0000(0.0000) | Loss 2.3753(13.5999) | Error 0.0000(0.0000) Steps 392(367.93) | Grad Norm 1.5122(96.1380) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 33.9843(42.2250) | Bit/dim 2.3545(13.2626) | Xent 0.0000(0.0000) | Loss 2.3545(13.2626) | Error 0.0000(0.0000) Steps 392(368.65) | Grad Norm 1.4608(93.2976) | Total Time 10.00(10.00)\n",
      "Iter 0042 | Time 33.4460(41.9616) | Bit/dim 2.3489(12.9352) | Xent 0.0000(0.0000) | Loss 2.3489(12.9352) | Error 0.0000(0.0000) Steps 386(369.17) | Grad Norm 1.7838(90.5522) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 12.8537, Epoch Time 272.5116(263.9508), Bit/dim 2.3374(best: 2.5741), Xent 0.0000, Loss 2.3374, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0043 | Time 34.0508(41.7243) | Bit/dim 2.3428(12.6174) | Xent 0.0000(0.0000) | Loss 2.3428(12.6174) | Error 0.0000(0.0000) Steps 386(369.68) | Grad Norm 2.4493(87.9091) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 36.3939(41.5644) | Bit/dim 2.3307(12.3088) | Xent 0.0000(0.0000) | Loss 2.3307(12.3088) | Error 0.0000(0.0000) Steps 398(370.53) | Grad Norm 2.8791(85.3582) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 31.7696(41.2705) | Bit/dim 2.3057(12.0087) | Xent 0.0000(0.0000) | Loss 2.3057(12.0087) | Error 0.0000(0.0000) Steps 380(370.81) | Grad Norm 2.5130(82.8729) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 33.4122(41.0348) | Bit/dim 2.2763(11.7167) | Xent 0.0000(0.0000) | Loss 2.2763(11.7167) | Error 0.0000(0.0000) Steps 380(371.09) | Grad Norm 1.5912(80.4344) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 33.2167(40.8002) | Bit/dim 2.2549(11.4329) | Xent 0.0000(0.0000) | Loss 2.2549(11.4329) | Error 0.0000(0.0000) Steps 380(371.35) | Grad Norm 1.5328(78.0674) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 33.1084(40.5695) | Bit/dim 2.2361(11.1570) | Xent 0.0000(0.0000) | Loss 2.2361(11.1570) | Error 0.0000(0.0000) Steps 380(371.61) | Grad Norm 1.9075(75.7826) | Total Time 10.00(10.00)\n",
      "Iter 0049 | Time 31.3803(40.2938) | Bit/dim 2.2141(10.8887) | Xent 0.0000(0.0000) | Loss 2.2141(10.8887) | Error 0.0000(0.0000) Steps 386(372.04) | Grad Norm 1.7363(73.5612) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 12.9437, Epoch Time 259.0387(263.8034), Bit/dim 2.2003(best: 2.3374), Xent 0.0000, Loss 2.2003, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0050 | Time 33.4822(40.0894) | Bit/dim 2.2034(10.6281) | Xent 0.0000(0.0000) | Loss 2.2034(10.6281) | Error 0.0000(0.0000) Steps 374(372.10) | Grad Norm 1.4587(71.3981) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 29.9388(39.7849) | Bit/dim 2.1992(10.3753) | Xent 0.0000(0.0000) | Loss 2.1992(10.3753) | Error 0.0000(0.0000) Steps 368(371.98) | Grad Norm 1.6674(69.3062) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 29.5605(39.4782) | Bit/dim 2.1850(10.1295) | Xent 0.0000(0.0000) | Loss 2.1850(10.1295) | Error 0.0000(0.0000) Steps 362(371.68) | Grad Norm 1.8074(67.2812) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 32.4058(39.2660) | Bit/dim 2.1497(9.8902) | Xent 0.0000(0.0000) | Loss 2.1497(9.8902) | Error 0.0000(0.0000) Steps 362(371.39) | Grad Norm 1.9578(65.3215) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 32.2938(39.0569) | Bit/dim 2.1336(9.6575) | Xent 0.0000(0.0000) | Loss 2.1336(9.6575) | Error 0.0000(0.0000) Steps 362(371.11) | Grad Norm 1.7476(63.4143) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 31.9959(38.8450) | Bit/dim 2.1135(9.4311) | Xent 0.0000(0.0000) | Loss 2.1135(9.4311) | Error 0.0000(0.0000) Steps 362(370.84) | Grad Norm 1.0586(61.5436) | Total Time 10.00(10.00)\n",
      "Iter 0056 | Time 29.6349(38.5687) | Bit/dim 2.0990(9.2112) | Xent 0.0000(0.0000) | Loss 2.0990(9.2112) | Error 0.0000(0.0000) Steps 362(370.57) | Grad Norm 1.2548(59.7350) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 12.3856, Epoch Time 243.7746(263.2026), Bit/dim 2.0888(best: 2.2003), Xent 0.0000, Loss 2.0888, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0057 | Time 29.5232(38.2974) | Bit/dim 2.0915(8.9976) | Xent 0.0000(0.0000) | Loss 2.0915(8.9976) | Error 0.0000(0.0000) Steps 362(370.31) | Grad Norm 1.5755(57.9902) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 31.0305(38.0794) | Bit/dim 2.0769(8.7900) | Xent 0.0000(0.0000) | Loss 2.0769(8.7900) | Error 0.0000(0.0000) Steps 368(370.24) | Grad Norm 1.3949(56.2923) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 30.6134(37.8554) | Bit/dim 2.0613(8.5881) | Xent 0.0000(0.0000) | Loss 2.0613(8.5881) | Error 0.0000(0.0000) Steps 368(370.18) | Grad Norm 1.4557(54.6472) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 32.3680(37.6907) | Bit/dim 2.0448(8.3918) | Xent 0.0000(0.0000) | Loss 2.0448(8.3918) | Error 0.0000(0.0000) Steps 368(370.11) | Grad Norm 1.1448(53.0422) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 31.9752(37.5193) | Bit/dim 2.0342(8.2011) | Xent 0.0000(0.0000) | Loss 2.0342(8.2011) | Error 0.0000(0.0000) Steps 374(370.23) | Grad Norm 1.1239(51.4846) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 31.2085(37.3300) | Bit/dim 2.0107(8.0154) | Xent 0.0000(0.0000) | Loss 2.0107(8.0154) | Error 0.0000(0.0000) Steps 374(370.34) | Grad Norm 1.6967(49.9910) | Total Time 10.00(10.00)\n",
      "Iter 0063 | Time 33.6667(37.2201) | Bit/dim 2.0006(7.8349) | Xent 0.0000(0.0000) | Loss 2.0006(7.8349) | Error 0.0000(0.0000) Steps 374(370.45) | Grad Norm 1.7136(48.5427) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 12.9057, Epoch Time 245.5567(262.6732), Bit/dim 1.9821(best: 2.0888), Xent 0.0000, Loss 1.9821, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0064 | Time 33.7049(37.1146) | Bit/dim 1.9838(7.6594) | Xent 0.0000(0.0000) | Loss 1.9838(7.6594) | Error 0.0000(0.0000) Steps 380(370.74) | Grad Norm 1.4216(47.1290) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 31.7176(36.9527) | Bit/dim 1.9755(7.4889) | Xent 0.0000(0.0000) | Loss 1.9755(7.4889) | Error 0.0000(0.0000) Steps 368(370.65) | Grad Norm 1.5998(45.7632) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 31.1074(36.7773) | Bit/dim 1.9585(7.3230) | Xent 0.0000(0.0000) | Loss 1.9585(7.3230) | Error 0.0000(0.0000) Steps 374(370.76) | Grad Norm 1.3990(44.4322) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 29.3620(36.5549) | Bit/dim 1.9559(7.1619) | Xent 0.0000(0.0000) | Loss 1.9559(7.1619) | Error 0.0000(0.0000) Steps 374(370.85) | Grad Norm 2.6202(43.1779) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 31.7374(36.4104) | Bit/dim 1.9467(7.0055) | Xent 0.0000(0.0000) | Loss 1.9467(7.0055) | Error 0.0000(0.0000) Steps 374(370.95) | Grad Norm 1.4895(41.9272) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 31.1210(36.2517) | Bit/dim 1.9377(6.8535) | Xent 0.0000(0.0000) | Loss 1.9377(6.8535) | Error 0.0000(0.0000) Steps 374(371.04) | Grad Norm 1.4798(40.7138) | Total Time 10.00(10.00)\n",
      "Iter 0070 | Time 31.8604(36.1199) | Bit/dim 1.9252(6.7056) | Xent 0.0000(0.0000) | Loss 1.9252(6.7056) | Error 0.0000(0.0000) Steps 392(371.67) | Grad Norm 1.8948(39.5492) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 13.3997, Epoch Time 246.3720(262.1841), Bit/dim 1.9136(best: 1.9821), Xent 0.0000, Loss 1.9136, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0071 | Time 31.7336(35.9883) | Bit/dim 1.9196(6.5620) | Xent 0.0000(0.0000) | Loss 1.9196(6.5620) | Error 0.0000(0.0000) Steps 392(372.28) | Grad Norm 1.0245(38.3935) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 33.7511(35.9212) | Bit/dim 1.9049(6.4223) | Xent 0.0000(0.0000) | Loss 1.9049(6.4223) | Error 0.0000(0.0000) Steps 380(372.51) | Grad Norm 1.2046(37.2778) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 34.4048(35.8757) | Bit/dim 1.9094(6.2869) | Xent 0.0000(0.0000) | Loss 1.9094(6.2869) | Error 0.0000(0.0000) Steps 386(372.91) | Grad Norm 1.7368(36.2116) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 31.9615(35.7583) | Bit/dim 1.8927(6.1551) | Xent 0.0000(0.0000) | Loss 1.8927(6.1551) | Error 0.0000(0.0000) Steps 386(373.31) | Grad Norm 1.4944(35.1701) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 32.1527(35.6501) | Bit/dim 1.8872(6.0271) | Xent 0.0000(0.0000) | Loss 1.8872(6.0271) | Error 0.0000(0.0000) Steps 380(373.51) | Grad Norm 1.0488(34.1464) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 33.5536(35.5872) | Bit/dim 1.8787(5.9026) | Xent 0.0000(0.0000) | Loss 1.8787(5.9026) | Error 0.0000(0.0000) Steps 386(373.88) | Grad Norm 2.0158(33.1825) | Total Time 10.00(10.00)\n",
      "Iter 0077 | Time 32.0561(35.4813) | Bit/dim 1.8768(5.7818) | Xent 0.0000(0.0000) | Loss 1.8768(5.7818) | Error 0.0000(0.0000) Steps 380(374.07) | Grad Norm 0.9643(32.2160) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 13.5738, Epoch Time 255.5818(261.9861), Bit/dim 1.8640(best: 1.9136), Xent 0.0000, Loss 1.8640, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0078 | Time 32.1240(35.3806) | Bit/dim 1.8644(5.6643) | Xent 0.0000(0.0000) | Loss 1.8644(5.6643) | Error 0.0000(0.0000) Steps 380(374.24) | Grad Norm 1.1265(31.2833) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 32.2681(35.2872) | Bit/dim 1.8648(5.5503) | Xent 0.0000(0.0000) | Loss 1.8648(5.5503) | Error 0.0000(0.0000) Steps 380(374.42) | Grad Norm 1.7652(30.3977) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 33.5678(35.2356) | Bit/dim 1.8589(5.4396) | Xent 0.0000(0.0000) | Loss 1.8589(5.4396) | Error 0.0000(0.0000) Steps 386(374.76) | Grad Norm 0.7124(29.5072) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 33.3025(35.1776) | Bit/dim 1.8549(5.3320) | Xent 0.0000(0.0000) | Loss 1.8549(5.3320) | Error 0.0000(0.0000) Steps 392(375.28) | Grad Norm 1.4641(28.6659) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 31.7286(35.0742) | Bit/dim 1.8498(5.2276) | Xent 0.0000(0.0000) | Loss 1.8498(5.2276) | Error 0.0000(0.0000) Steps 392(375.78) | Grad Norm 4.4259(27.9387) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 32.4437(34.9953) | Bit/dim 1.8956(5.1276) | Xent 0.0000(0.0000) | Loss 1.8956(5.1276) | Error 0.0000(0.0000) Steps 386(376.09) | Grad Norm 13.3536(27.5011) | Total Time 10.00(10.00)\n",
      "Iter 0084 | Time 32.6322(34.9244) | Bit/dim 2.2107(5.0401) | Xent 0.0000(0.0000) | Loss 2.2107(5.0401) | Error 0.0000(0.0000) Steps 380(376.21) | Grad Norm 36.9036(27.7832) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 14.4375, Epoch Time 255.0760(261.7788), Bit/dim 2.5705(best: 1.8640), Xent 0.0000, Loss 2.5705, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0085 | Time 36.5391(34.9728) | Bit/dim 2.5791(4.9663) | Xent 0.0000(0.0000) | Loss 2.5791(4.9663) | Error 0.0000(0.0000) Steps 398(376.86) | Grad Norm 44.3429(28.2800) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 37.6986(35.0546) | Bit/dim 2.2778(4.8856) | Xent 0.0000(0.0000) | Loss 2.2778(4.8856) | Error 0.0000(0.0000) Steps 422(378.21) | Grad Norm 10.2010(27.7376) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 34.5344(35.0390) | Bit/dim 2.1765(4.8044) | Xent 0.0000(0.0000) | Loss 2.1765(4.8044) | Error 0.0000(0.0000) Steps 392(378.63) | Grad Norm 7.7870(27.1391) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 34.1448(35.0121) | Bit/dim 2.1629(4.7251) | Xent 0.0000(0.0000) | Loss 2.1629(4.7251) | Error 0.0000(0.0000) Steps 392(379.03) | Grad Norm 11.9637(26.6838) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 32.7707(34.9449) | Bit/dim 2.1037(4.6465) | Xent 0.0000(0.0000) | Loss 2.1037(4.6465) | Error 0.0000(0.0000) Steps 380(379.06) | Grad Norm 9.9724(26.1825) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 30.2335(34.8036) | Bit/dim 2.0139(4.5675) | Xent 0.0000(0.0000) | Loss 2.0139(4.5675) | Error 0.0000(0.0000) Steps 386(379.27) | Grad Norm 3.3627(25.4979) | Total Time 10.00(10.00)\n",
      "Iter 0091 | Time 34.0382(34.7806) | Bit/dim 2.0379(4.4916) | Xent 0.0000(0.0000) | Loss 2.0379(4.4916) | Error 0.0000(0.0000) Steps 386(379.47) | Grad Norm 3.6713(24.8431) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 13.7217, Epoch Time 266.0299(261.9063), Bit/dim 2.0493(best: 1.8640), Xent 0.0000, Loss 2.0493, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0092 | Time 37.4590(34.8610) | Bit/dim 2.0518(4.4184) | Xent 0.0000(0.0000) | Loss 2.0518(4.4184) | Error 0.0000(0.0000) Steps 416(380.56) | Grad Norm 4.3376(24.2279) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 33.2203(34.8117) | Bit/dim 2.0249(4.3466) | Xent 0.0000(0.0000) | Loss 2.0249(4.3466) | Error 0.0000(0.0000) Steps 416(381.63) | Grad Norm 4.0286(23.6220) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 34.0347(34.7884) | Bit/dim 2.0193(4.2768) | Xent 0.0000(0.0000) | Loss 2.0193(4.2768) | Error 0.0000(0.0000) Steps 434(383.20) | Grad Norm 7.5736(23.1405) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 32.5708(34.7219) | Bit/dim 1.9971(4.2084) | Xent 0.0000(0.0000) | Loss 1.9971(4.2084) | Error 0.0000(0.0000) Steps 416(384.18) | Grad Norm 1.9600(22.5051) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 33.2965(34.6791) | Bit/dim 2.0265(4.1429) | Xent 0.0000(0.0000) | Loss 2.0265(4.1429) | Error 0.0000(0.0000) Steps 416(385.14) | Grad Norm 3.4386(21.9331) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 34.8837(34.6853) | Bit/dim 2.0307(4.0796) | Xent 0.0000(0.0000) | Loss 2.0307(4.0796) | Error 0.0000(0.0000) Steps 410(385.88) | Grad Norm 3.4228(21.3778) | Total Time 10.00(10.00)\n",
      "Iter 0098 | Time 36.4769(34.7390) | Bit/dim 2.0003(4.0172) | Xent 0.0000(0.0000) | Loss 2.0003(4.0172) | Error 0.0000(0.0000) Steps 416(386.79) | Grad Norm 2.5268(20.8123) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 13.8255, Epoch Time 268.0863(262.0917), Bit/dim 1.9575(best: 1.8640), Xent 0.0000, Loss 1.9575, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0099 | Time 34.1829(34.7223) | Bit/dim 1.9632(3.9556) | Xent 0.0000(0.0000) | Loss 1.9632(3.9556) | Error 0.0000(0.0000) Steps 416(387.66) | Grad Norm 2.2252(20.2547) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 32.0058(34.6408) | Bit/dim 1.9406(3.8951) | Xent 0.0000(0.0000) | Loss 1.9406(3.8951) | Error 0.0000(0.0000) Steps 410(388.33) | Grad Norm 2.3016(19.7161) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 32.0928(34.5644) | Bit/dim 1.9339(3.8363) | Xent 0.0000(0.0000) | Loss 1.9339(3.8363) | Error 0.0000(0.0000) Steps 404(388.80) | Grad Norm 1.9309(19.1825) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 28.2357(34.3745) | Bit/dim 1.9287(3.7791) | Xent 0.0000(0.0000) | Loss 1.9287(3.7791) | Error 0.0000(0.0000) Steps 380(388.54) | Grad Norm 2.0254(18.6678) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 29.5918(34.2311) | Bit/dim 1.9098(3.7230) | Xent 0.0000(0.0000) | Loss 1.9098(3.7230) | Error 0.0000(0.0000) Steps 386(388.46) | Grad Norm 1.5486(18.1542) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 30.3229(34.1138) | Bit/dim 1.9046(3.6684) | Xent 0.0000(0.0000) | Loss 1.9046(3.6684) | Error 0.0000(0.0000) Steps 380(388.21) | Grad Norm 1.7615(17.6624) | Total Time 10.00(10.00)\n",
      "Iter 0105 | Time 28.7866(33.9540) | Bit/dim 1.9029(3.6155) | Xent 0.0000(0.0000) | Loss 1.9029(3.6155) | Error 0.0000(0.0000) Steps 368(387.60) | Grad Norm 2.5326(17.2085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 12.6521, Epoch Time 240.2025(261.4350), Bit/dim 1.8789(best: 1.8640), Xent 0.0000, Loss 1.8789, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0106 | Time 30.5217(33.8510) | Bit/dim 1.8805(3.5634) | Xent 0.0000(0.0000) | Loss 1.8805(3.5634) | Error 0.0000(0.0000) Steps 380(387.37) | Grad Norm 1.2084(16.7285) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 32.0775(33.7978) | Bit/dim 1.8798(3.5129) | Xent 0.0000(0.0000) | Loss 1.8798(3.5129) | Error 0.0000(0.0000) Steps 392(387.51) | Grad Norm 2.2138(16.2931) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 30.6637(33.7038) | Bit/dim 1.8727(3.4637) | Xent 0.0000(0.0000) | Loss 1.8727(3.4637) | Error 0.0000(0.0000) Steps 380(387.29) | Grad Norm 1.0590(15.8361) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 28.6855(33.5532) | Bit/dim 1.8657(3.4158) | Xent 0.0000(0.0000) | Loss 1.8657(3.4158) | Error 0.0000(0.0000) Steps 380(387.07) | Grad Norm 2.1176(15.4245) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 28.4587(33.4004) | Bit/dim 1.8481(3.3687) | Xent 0.0000(0.0000) | Loss 1.8481(3.3687) | Error 0.0000(0.0000) Steps 380(386.86) | Grad Norm 0.9375(14.9899) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 30.9401(33.3266) | Bit/dim 1.8397(3.3229) | Xent 0.0000(0.0000) | Loss 1.8397(3.3229) | Error 0.0000(0.0000) Steps 392(387.01) | Grad Norm 2.0333(14.6012) | Total Time 10.00(10.00)\n",
      "Iter 0112 | Time 32.1184(33.2904) | Bit/dim 1.8321(3.2781) | Xent 0.0000(0.0000) | Loss 1.8321(3.2781) | Error 0.0000(0.0000) Steps 392(387.16) | Grad Norm 1.4051(14.2053) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 13.3695, Epoch Time 239.0986(260.7649), Bit/dim 1.8221(best: 1.8640), Xent 0.0000, Loss 1.8221, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0113 | Time 30.5312(33.2076) | Bit/dim 1.8285(3.2346) | Xent 0.0000(0.0000) | Loss 1.8285(3.2346) | Error 0.0000(0.0000) Steps 386(387.13) | Grad Norm 2.2369(13.8463) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 29.9932(33.1112) | Bit/dim 1.8205(3.1922) | Xent 0.0000(0.0000) | Loss 1.8205(3.1922) | Error 0.0000(0.0000) Steps 392(387.27) | Grad Norm 1.7963(13.4848) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 31.5177(33.0633) | Bit/dim 1.8151(3.1509) | Xent 0.0000(0.0000) | Loss 1.8151(3.1509) | Error 0.0000(0.0000) Steps 392(387.41) | Grad Norm 1.4207(13.1228) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 33.1069(33.0647) | Bit/dim 1.8096(3.1107) | Xent 0.0000(0.0000) | Loss 1.8096(3.1107) | Error 0.0000(0.0000) Steps 398(387.73) | Grad Norm 2.3479(12.7996) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 30.5536(32.9893) | Bit/dim 1.8079(3.0716) | Xent 0.0000(0.0000) | Loss 1.8079(3.0716) | Error 0.0000(0.0000) Steps 392(387.86) | Grad Norm 2.1584(12.4804) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 34.0936(33.0225) | Bit/dim 1.7967(3.0333) | Xent 0.0000(0.0000) | Loss 1.7967(3.0333) | Error 0.0000(0.0000) Steps 392(387.98) | Grad Norm 1.2489(12.1434) | Total Time 10.00(10.00)\n",
      "Iter 0119 | Time 32.0834(32.9943) | Bit/dim 1.7956(2.9962) | Xent 0.0000(0.0000) | Loss 1.7956(2.9962) | Error 0.0000(0.0000) Steps 392(388.10) | Grad Norm 1.9819(11.8386) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 13.5396, Epoch Time 247.5803(260.3694), Bit/dim 1.7991(best: 1.8221), Xent 0.0000, Loss 1.7991, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0120 | Time 31.5457(32.9508) | Bit/dim 1.7998(2.9603) | Xent 0.0000(0.0000) | Loss 1.7998(2.9603) | Error 0.0000(0.0000) Steps 398(388.40) | Grad Norm 3.7939(11.5972) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 32.5481(32.9387) | Bit/dim 1.8019(2.9256) | Xent 0.0000(0.0000) | Loss 1.8019(2.9256) | Error 0.0000(0.0000) Steps 398(388.69) | Grad Norm 4.9553(11.3980) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 36.5460(33.0470) | Bit/dim 1.8233(2.8925) | Xent 0.0000(0.0000) | Loss 1.8233(2.8925) | Error 0.0000(0.0000) Steps 416(389.51) | Grad Norm 6.3992(11.2480) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 31.9008(33.0126) | Bit/dim 1.8202(2.8603) | Xent 0.0000(0.0000) | Loss 1.8202(2.8603) | Error 0.0000(0.0000) Steps 398(389.76) | Grad Norm 7.7975(11.1445) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 33.6723(33.0324) | Bit/dim 1.8494(2.8300) | Xent 0.0000(0.0000) | Loss 1.8494(2.8300) | Error 0.0000(0.0000) Steps 410(390.37) | Grad Norm 7.8143(11.0446) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 32.1114(33.0047) | Bit/dim 1.7944(2.7989) | Xent 0.0000(0.0000) | Loss 1.7944(2.7989) | Error 0.0000(0.0000) Steps 398(390.60) | Grad Norm 4.0580(10.8350) | Total Time 10.00(10.00)\n",
      "Iter 0126 | Time 31.8551(32.9702) | Bit/dim 1.7768(2.7683) | Xent 0.0000(0.0000) | Loss 1.7768(2.7683) | Error 0.0000(0.0000) Steps 398(390.82) | Grad Norm 1.0264(10.5407) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 14.1951, Epoch Time 256.6108(260.2566), Bit/dim 1.7877(best: 1.7991), Xent 0.0000, Loss 1.7877, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0127 | Time 33.1403(32.9754) | Bit/dim 1.7938(2.7390) | Xent 0.0000(0.0000) | Loss 1.7938(2.7390) | Error 0.0000(0.0000) Steps 416(391.58) | Grad Norm 4.7114(10.3659) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 32.4051(32.9582) | Bit/dim 1.8079(2.7111) | Xent 0.0000(0.0000) | Loss 1.8079(2.7111) | Error 0.0000(0.0000) Steps 398(391.77) | Grad Norm 8.5105(10.3102) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 33.9314(32.9874) | Bit/dim 1.8687(2.6858) | Xent 0.0000(0.0000) | Loss 1.8687(2.6858) | Error 0.0000(0.0000) Steps 416(392.50) | Grad Norm 9.2152(10.2773) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 31.9743(32.9570) | Bit/dim 1.7671(2.6583) | Xent 0.0000(0.0000) | Loss 1.7671(2.6583) | Error 0.0000(0.0000) Steps 398(392.66) | Grad Norm 2.2380(10.0362) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 31.7434(32.9206) | Bit/dim 1.8100(2.6328) | Xent 0.0000(0.0000) | Loss 1.8100(2.6328) | Error 0.0000(0.0000) Steps 398(392.82) | Grad Norm 8.0293(9.9760) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 35.4112(32.9954) | Bit/dim 1.8690(2.6099) | Xent 0.0000(0.0000) | Loss 1.8690(2.6099) | Error 0.0000(0.0000) Steps 422(393.70) | Grad Norm 8.8007(9.9407) | Total Time 10.00(10.00)\n",
      "Iter 0133 | Time 31.0966(32.9384) | Bit/dim 1.7690(2.5847) | Xent 0.0000(0.0000) | Loss 1.7690(2.5847) | Error 0.0000(0.0000) Steps 398(393.83) | Grad Norm 3.1476(9.7369) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 13.0369, Epoch Time 255.0036(260.0990), Bit/dim 1.8943(best: 1.7877), Xent 0.0000, Loss 1.8943, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0134 | Time 30.5955(32.8681) | Bit/dim 1.8987(2.5641) | Xent 0.0000(0.0000) | Loss 1.8987(2.5641) | Error 0.0000(0.0000) Steps 380(393.41) | Grad Norm 13.0877(9.8374) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 33.5102(32.8874) | Bit/dim 1.8079(2.5414) | Xent 0.0000(0.0000) | Loss 1.8079(2.5414) | Error 0.0000(0.0000) Steps 416(394.09) | Grad Norm 6.1828(9.7278) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 36.8815(33.0072) | Bit/dim 1.8327(2.5201) | Xent 0.0000(0.0000) | Loss 1.8327(2.5201) | Error 0.0000(0.0000) Steps 422(394.93) | Grad Norm 6.8306(9.6409) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 31.6381(32.9661) | Bit/dim 1.7857(2.4981) | Xent 0.0000(0.0000) | Loss 1.7857(2.4981) | Error 0.0000(0.0000) Steps 392(394.84) | Grad Norm 3.7489(9.4641) | Total Time 10.00(10.00)\n",
      "Iter 0138 | Time 29.6314(32.8661) | Bit/dim 1.8267(2.4780) | Xent 0.0000(0.0000) | Loss 1.8267(2.4780) | Error 0.0000(0.0000) Steps 374(394.21) | Grad Norm 6.3451(9.3706) | Total Time 10.00(10.00)\n",
      "Iter 0139 | Time 31.7926(32.8339) | Bit/dim 1.7685(2.4567) | Xent 0.0000(0.0000) | Loss 1.7685(2.4567) | Error 0.0000(0.0000) Steps 398(394.33) | Grad Norm 1.1934(9.1252) | Total Time 10.00(10.00)\n",
      "Iter 0140 | Time 34.8653(32.8948) | Bit/dim 1.8117(2.4373) | Xent 0.0000(0.0000) | Loss 1.8117(2.4373) | Error 0.0000(0.0000) Steps 410(394.80) | Grad Norm 5.4566(9.0152) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 13.4215, Epoch Time 254.8088(259.9403), Bit/dim 1.7614(best: 1.7877), Xent 0.0000, Loss 1.7614, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0141 | Time 32.2686(32.8760) | Bit/dim 1.7707(2.4173) | Xent 0.0000(0.0000) | Loss 1.7707(2.4173) | Error 0.0000(0.0000) Steps 380(394.35) | Grad Norm 1.4755(8.7890) | Total Time 10.00(10.00)\n",
      "Iter 0142 | Time 29.8367(32.7848) | Bit/dim 1.7829(2.3983) | Xent 0.0000(0.0000) | Loss 1.7829(2.3983) | Error 0.0000(0.0000) Steps 362(393.38) | Grad Norm 3.8737(8.6415) | Total Time 10.00(10.00)\n",
      "Iter 0143 | Time 29.5793(32.6887) | Bit/dim 1.7827(2.3798) | Xent 0.0000(0.0000) | Loss 1.7827(2.3798) | Error 0.0000(0.0000) Steps 362(392.44) | Grad Norm 3.4584(8.4860) | Total Time 10.00(10.00)\n",
      "Iter 0144 | Time 32.9207(32.6956) | Bit/dim 1.7633(2.3613) | Xent 0.0000(0.0000) | Loss 1.7633(2.3613) | Error 0.0000(0.0000) Steps 386(392.25) | Grad Norm 1.3761(8.2727) | Total Time 10.00(10.00)\n",
      "Iter 0145 | Time 32.8344(32.6998) | Bit/dim 1.7792(2.3439) | Xent 0.0000(0.0000) | Loss 1.7792(2.3439) | Error 0.0000(0.0000) Steps 392(392.24) | Grad Norm 3.8297(8.1395) | Total Time 10.00(10.00)\n",
      "Iter 0146 | Time 31.5218(32.6645) | Bit/dim 1.7563(2.3262) | Xent 0.0000(0.0000) | Loss 1.7563(2.3262) | Error 0.0000(0.0000) Steps 380(391.87) | Grad Norm 1.1457(7.9296) | Total Time 10.00(10.00)\n",
      "Iter 0147 | Time 28.2092(32.5308) | Bit/dim 1.7651(2.3094) | Xent 0.0000(0.0000) | Loss 1.7651(2.3094) | Error 0.0000(0.0000) Steps 368(391.16) | Grad Norm 2.7078(7.7730) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 12.5667, Epoch Time 241.9920(259.4019), Bit/dim 1.7555(best: 1.7614), Xent 0.0000, Loss 1.7555, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0148 | Time 28.8915(32.4216) | Bit/dim 1.7610(2.2930) | Xent 0.0000(0.0000) | Loss 1.7610(2.2930) | Error 0.0000(0.0000) Steps 368(390.46) | Grad Norm 2.6175(7.6183) | Total Time 10.00(10.00)\n",
      "Iter 0149 | Time 32.9023(32.4360) | Bit/dim 1.7431(2.2765) | Xent 0.0000(0.0000) | Loss 1.7431(2.2765) | Error 0.0000(0.0000) Steps 392(390.51) | Grad Norm 1.2905(7.4285) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 35.5081(32.5282) | Bit/dim 1.7566(2.2609) | Xent 0.0000(0.0000) | Loss 1.7566(2.2609) | Error 0.0000(0.0000) Steps 404(390.91) | Grad Norm 3.3387(7.3058) | Total Time 10.00(10.00)\n",
      "Iter 0151 | Time 31.8006(32.5064) | Bit/dim 1.7371(2.2452) | Xent 0.0000(0.0000) | Loss 1.7371(2.2452) | Error 0.0000(0.0000) Steps 392(390.95) | Grad Norm 0.3602(7.0974) | Total Time 10.00(10.00)\n",
      "Iter 0152 | Time 28.1002(32.3742) | Bit/dim 1.7520(2.2304) | Xent 0.0000(0.0000) | Loss 1.7520(2.2304) | Error 0.0000(0.0000) Steps 374(390.44) | Grad Norm 2.9692(6.9736) | Total Time 10.00(10.00)\n",
      "Iter 0153 | Time 31.9489(32.3614) | Bit/dim 1.7333(2.2155) | Xent 0.0000(0.0000) | Loss 1.7333(2.2155) | Error 0.0000(0.0000) Steps 392(390.48) | Grad Norm 0.9151(6.7918) | Total Time 10.00(10.00)\n",
      "Iter 0154 | Time 36.1405(32.4748) | Bit/dim 1.7391(2.2012) | Xent 0.0000(0.0000) | Loss 1.7391(2.2012) | Error 0.0000(0.0000) Steps 404(390.89) | Grad Norm 2.9285(6.6759) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 14.4198, Epoch Time 252.0565(259.1815), Bit/dim 1.7206(best: 1.7555), Xent 0.0000, Loss 1.7206, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0155 | Time 36.5096(32.5959) | Bit/dim 1.7224(2.1868) | Xent 0.0000(0.0000) | Loss 1.7224(2.1868) | Error 0.0000(0.0000) Steps 422(391.82) | Grad Norm 0.9204(6.5033) | Total Time 10.00(10.00)\n",
      "Iter 0156 | Time 36.7664(32.7210) | Bit/dim 1.7251(2.1729) | Xent 0.0000(0.0000) | Loss 1.7251(2.1729) | Error 0.0000(0.0000) Steps 416(392.55) | Grad Norm 2.5423(6.3844) | Total Time 10.00(10.00)\n",
      "Iter 0157 | Time 35.0698(32.7914) | Bit/dim 1.7186(2.1593) | Xent 0.0000(0.0000) | Loss 1.7186(2.1593) | Error 0.0000(0.0000) Steps 422(393.43) | Grad Norm 0.7837(6.2164) | Total Time 10.00(10.00)\n",
      "Iter 0158 | Time 35.8737(32.8839) | Bit/dim 1.7217(2.1462) | Xent 0.0000(0.0000) | Loss 1.7217(2.1462) | Error 0.0000(0.0000) Steps 422(394.29) | Grad Norm 2.7247(6.1117) | Total Time 10.00(10.00)\n",
      "Iter 0159 | Time 35.6212(32.9660) | Bit/dim 1.7118(2.1332) | Xent 0.0000(0.0000) | Loss 1.7118(2.1332) | Error 0.0000(0.0000) Steps 422(395.12) | Grad Norm 0.2630(5.9362) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 36.4460(33.0704) | Bit/dim 1.7149(2.1206) | Xent 0.0000(0.0000) | Loss 1.7149(2.1206) | Error 0.0000(0.0000) Steps 422(395.93) | Grad Norm 2.6872(5.8387) | Total Time 10.00(10.00)\n",
      "Iter 0161 | Time 34.2197(33.1049) | Bit/dim 1.6974(2.1079) | Xent 0.0000(0.0000) | Loss 1.6974(2.1079) | Error 0.0000(0.0000) Steps 422(396.71) | Grad Norm 1.6034(5.7117) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 14.5073, Epoch Time 277.6735(259.7363), Bit/dim 1.6872(best: 1.7206), Xent 0.0000, Loss 1.6872, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0162 | Time 34.9988(33.1617) | Bit/dim 1.6916(2.0954) | Xent 0.0000(0.0000) | Loss 1.6916(2.0954) | Error 0.0000(0.0000) Steps 422(397.47) | Grad Norm 1.4968(5.5852) | Total Time 10.00(10.00)\n",
      "Iter 0163 | Time 35.2934(33.2257) | Bit/dim 1.6901(2.0833) | Xent 0.0000(0.0000) | Loss 1.6901(2.0833) | Error 0.0000(0.0000) Steps 422(398.20) | Grad Norm 2.8908(5.5044) | Total Time 10.00(10.00)\n",
      "Iter 0164 | Time 36.3043(33.3180) | Bit/dim 1.6856(2.0713) | Xent 0.0000(0.0000) | Loss 1.6856(2.0713) | Error 0.0000(0.0000) Steps 422(398.92) | Grad Norm 1.5808(5.3867) | Total Time 10.00(10.00)\n",
      "Iter 0165 | Time 35.3654(33.3794) | Bit/dim 1.6688(2.0593) | Xent 0.0000(0.0000) | Loss 1.6688(2.0593) | Error 0.0000(0.0000) Steps 422(399.61) | Grad Norm 1.1486(5.2595) | Total Time 10.00(10.00)\n",
      "Iter 0166 | Time 35.9491(33.4565) | Bit/dim 1.6708(2.0476) | Xent 0.0000(0.0000) | Loss 1.6708(2.0476) | Error 0.0000(0.0000) Steps 422(400.28) | Grad Norm 3.0852(5.1943) | Total Time 10.00(10.00)\n",
      "Iter 0167 | Time 38.7836(33.6163) | Bit/dim 1.6626(2.0361) | Xent 0.0000(0.0000) | Loss 1.6626(2.0361) | Error 0.0000(0.0000) Steps 428(401.11) | Grad Norm 4.0468(5.1599) | Total Time 10.00(10.00)\n",
      "Iter 0168 | Time 34.9782(33.6572) | Bit/dim 1.6551(2.0246) | Xent 0.0000(0.0000) | Loss 1.6551(2.0246) | Error 0.0000(0.0000) Steps 422(401.74) | Grad Norm 4.6019(5.1431) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 14.6747, Epoch Time 278.8843(260.3107), Bit/dim 1.6535(best: 1.6872), Xent 0.0000, Loss 1.6535, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0169 | Time 36.5878(33.7451) | Bit/dim 1.6577(2.0136) | Xent 0.0000(0.0000) | Loss 1.6577(2.0136) | Error 0.0000(0.0000) Steps 422(402.35) | Grad Norm 5.8310(5.1638) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 34.9186(33.7803) | Bit/dim 1.6662(2.0032) | Xent 0.0000(0.0000) | Loss 1.6662(2.0032) | Error 0.0000(0.0000) Steps 422(402.94) | Grad Norm 7.8890(5.2455) | Total Time 10.00(10.00)\n",
      "Iter 0171 | Time 35.0716(33.8191) | Bit/dim 1.7011(1.9941) | Xent 0.0000(0.0000) | Loss 1.7011(1.9941) | Error 0.0000(0.0000) Steps 422(403.51) | Grad Norm 9.6842(5.3787) | Total Time 10.00(10.00)\n",
      "Iter 0172 | Time 35.6516(33.8740) | Bit/dim 1.6870(1.9849) | Xent 0.0000(0.0000) | Loss 1.6870(1.9849) | Error 0.0000(0.0000) Steps 422(404.06) | Grad Norm 9.9618(5.5162) | Total Time 10.00(10.00)\n",
      "Iter 0173 | Time 36.8537(33.9634) | Bit/dim 1.6578(1.9751) | Xent 0.0000(0.0000) | Loss 1.6578(1.9751) | Error 0.0000(0.0000) Steps 428(404.78) | Grad Norm 7.3905(5.5724) | Total Time 10.00(10.00)\n",
      "Iter 0174 | Time 36.1568(34.0292) | Bit/dim 1.6078(1.9641) | Xent 0.0000(0.0000) | Loss 1.6078(1.9641) | Error 0.0000(0.0000) Steps 416(405.12) | Grad Norm 2.2584(5.4730) | Total Time 10.00(10.00)\n",
      "Iter 0175 | Time 32.5917(33.9861) | Bit/dim 1.6159(1.9536) | Xent 0.0000(0.0000) | Loss 1.6159(1.9536) | Error 0.0000(0.0000) Steps 416(405.44) | Grad Norm 3.6053(5.4170) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 14.3217, Epoch Time 274.5607(260.7382), Bit/dim 1.6248(best: 1.6535), Xent 0.0000, Loss 1.6248, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0176 | Time 35.4530(34.0301) | Bit/dim 1.6282(1.9439) | Xent 0.0000(0.0000) | Loss 1.6282(1.9439) | Error 0.0000(0.0000) Steps 422(405.94) | Grad Norm 6.0837(5.4370) | Total Time 10.00(10.00)\n",
      "Iter 0177 | Time 35.1919(34.0650) | Bit/dim 1.6003(1.9336) | Xent 0.0000(0.0000) | Loss 1.6003(1.9336) | Error 0.0000(0.0000) Steps 416(406.24) | Grad Norm 3.6711(5.3840) | Total Time 10.00(10.00)\n",
      "Iter 0178 | Time 34.8750(34.0893) | Bit/dim 1.5916(1.9233) | Xent 0.0000(0.0000) | Loss 1.5916(1.9233) | Error 0.0000(0.0000) Steps 416(406.54) | Grad Norm 1.8223(5.2771) | Total Time 10.00(10.00)\n",
      "Iter 0179 | Time 35.4753(34.1308) | Bit/dim 1.6004(1.9136) | Xent 0.0000(0.0000) | Loss 1.6004(1.9136) | Error 0.0000(0.0000) Steps 416(406.82) | Grad Norm 4.6451(5.2582) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 35.5112(34.1723) | Bit/dim 1.5799(1.9036) | Xent 0.0000(0.0000) | Loss 1.5799(1.9036) | Error 0.0000(0.0000) Steps 422(407.28) | Grad Norm 3.0912(5.1932) | Total Time 10.00(10.00)\n",
      "Iter 0181 | Time 33.8741(34.1633) | Bit/dim 1.5604(1.8933) | Xent 0.0000(0.0000) | Loss 1.5604(1.8933) | Error 0.0000(0.0000) Steps 416(407.54) | Grad Norm 1.1840(5.0729) | Total Time 10.00(10.00)\n",
      "Iter 0182 | Time 35.3746(34.1997) | Bit/dim 1.5689(1.8836) | Xent 0.0000(0.0000) | Loss 1.5689(1.8836) | Error 0.0000(0.0000) Steps 416(407.79) | Grad Norm 4.3196(5.0503) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 14.2623, Epoch Time 272.1775(261.0814), Bit/dim 1.5614(best: 1.6248), Xent 0.0000, Loss 1.5614, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0183 | Time 35.3661(34.2346) | Bit/dim 1.5708(1.8742) | Xent 0.0000(0.0000) | Loss 1.5708(1.8742) | Error 0.0000(0.0000) Steps 416(408.04) | Grad Norm 4.4851(5.0333) | Total Time 10.00(10.00)\n",
      "Iter 0184 | Time 34.2462(34.2350) | Bit/dim 1.5491(1.8644) | Xent 0.0000(0.0000) | Loss 1.5491(1.8644) | Error 0.0000(0.0000) Steps 416(408.28) | Grad Norm 2.6708(4.9625) | Total Time 10.00(10.00)\n",
      "Iter 0185 | Time 36.0589(34.2897) | Bit/dim 1.5364(1.8546) | Xent 0.0000(0.0000) | Loss 1.5364(1.8546) | Error 0.0000(0.0000) Steps 416(408.51) | Grad Norm 0.7184(4.8351) | Total Time 10.00(10.00)\n",
      "Iter 0186 | Time 36.5987(34.3590) | Bit/dim 1.5348(1.8450) | Xent 0.0000(0.0000) | Loss 1.5348(1.8450) | Error 0.0000(0.0000) Steps 416(408.73) | Grad Norm 2.4837(4.7646) | Total Time 10.00(10.00)\n",
      "Iter 0187 | Time 36.8133(34.4326) | Bit/dim 1.5448(1.8360) | Xent 0.0000(0.0000) | Loss 1.5448(1.8360) | Error 0.0000(0.0000) Steps 422(409.13) | Grad Norm 5.6106(4.7900) | Total Time 10.00(10.00)\n",
      "Iter 0188 | Time 31.8978(34.3566) | Bit/dim 1.6081(1.8292) | Xent 0.0000(0.0000) | Loss 1.6081(1.8292) | Error 0.0000(0.0000) Steps 386(408.44) | Grad Norm 11.4417(4.9895) | Total Time 10.00(10.00)\n",
      "Iter 0189 | Time 38.2798(34.4743) | Bit/dim 1.7906(1.8280) | Xent 0.0000(0.0000) | Loss 1.7906(1.8280) | Error 0.0000(0.0000) Steps 428(409.02) | Grad Norm 12.5774(5.2172) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 14.5500, Epoch Time 276.0217(261.5296), Bit/dim 1.5582(best: 1.5614), Xent 0.0000, Loss 1.5582, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0190 | Time 34.8914(34.4868) | Bit/dim 1.5675(1.8202) | Xent 0.0000(0.0000) | Loss 1.5675(1.8202) | Error 0.0000(0.0000) Steps 422(409.41) | Grad Norm 3.0910(5.1534) | Total Time 10.00(10.00)\n",
      "Iter 0191 | Time 33.2194(34.4487) | Bit/dim 1.8039(1.8197) | Xent 0.0000(0.0000) | Loss 1.8039(1.8197) | Error 0.0000(0.0000) Steps 380(408.53) | Grad Norm 15.4266(5.4616) | Total Time 10.00(10.00)\n",
      "Iter 0192 | Time 36.9422(34.5236) | Bit/dim 1.5955(1.8130) | Xent 0.0000(0.0000) | Loss 1.5955(1.8130) | Error 0.0000(0.0000) Steps 416(408.75) | Grad Norm 6.3865(5.4893) | Total Time 10.00(10.00)\n",
      "Iter 0193 | Time 35.2725(34.5460) | Bit/dim 1.6693(1.8087) | Xent 0.0000(0.0000) | Loss 1.6693(1.8087) | Error 0.0000(0.0000) Steps 410(408.79) | Grad Norm 7.8544(5.5603) | Total Time 10.00(10.00)\n",
      "Iter 0194 | Time 32.5921(34.4874) | Bit/dim 1.5597(1.8012) | Xent 0.0000(0.0000) | Loss 1.5597(1.8012) | Error 0.0000(0.0000) Steps 398(408.47) | Grad Norm 2.7866(5.4771) | Total Time 10.00(10.00)\n",
      "Iter 0195 | Time 31.1187(34.3863) | Bit/dim 1.6283(1.7960) | Xent 0.0000(0.0000) | Loss 1.6283(1.7960) | Error 0.0000(0.0000) Steps 380(407.61) | Grad Norm 4.9705(5.4619) | Total Time 10.00(10.00)\n",
      "Iter 0196 | Time 33.8746(34.3710) | Bit/dim 1.5965(1.7900) | Xent 0.0000(0.0000) | Loss 1.5965(1.7900) | Error 0.0000(0.0000) Steps 380(406.79) | Grad Norm 3.8538(5.4136) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 13.7984, Epoch Time 264.0206(261.6043), Bit/dim 1.5587(best: 1.5582), Xent 0.0000, Loss 1.5587, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0197 | Time 33.7446(34.3522) | Bit/dim 1.5647(1.7833) | Xent 0.0000(0.0000) | Loss 1.5647(1.7833) | Error 0.0000(0.0000) Steps 398(406.52) | Grad Norm 1.3902(5.2929) | Total Time 10.00(10.00)\n",
      "Iter 0198 | Time 34.4592(34.3554) | Bit/dim 1.5967(1.7777) | Xent 0.0000(0.0000) | Loss 1.5967(1.7777) | Error 0.0000(0.0000) Steps 398(406.27) | Grad Norm 4.5298(5.2700) | Total Time 10.00(10.00)\n",
      "Iter 0199 | Time 34.9702(34.3739) | Bit/dim 1.5602(1.7711) | Xent 0.0000(0.0000) | Loss 1.5602(1.7711) | Error 0.0000(0.0000) Steps 398(406.02) | Grad Norm 2.6745(5.1922) | Total Time 10.00(10.00)\n",
      "Iter 0200 | Time 31.6587(34.2924) | Bit/dim 1.5532(1.7646) | Xent 0.0000(0.0000) | Loss 1.5532(1.7646) | Error 0.0000(0.0000) Steps 380(405.24) | Grad Norm 2.2984(5.1053) | Total Time 10.00(10.00)\n",
      "Iter 0201 | Time 31.7514(34.2162) | Bit/dim 1.5701(1.7588) | Xent 0.0000(0.0000) | Loss 1.5701(1.7588) | Error 0.0000(0.0000) Steps 380(404.48) | Grad Norm 4.0646(5.0741) | Total Time 10.00(10.00)\n",
      "Iter 0202 | Time 29.3217(34.0693) | Bit/dim 1.5281(1.7519) | Xent 0.0000(0.0000) | Loss 1.5281(1.7519) | Error 0.0000(0.0000) Steps 368(403.39) | Grad Norm 1.9192(4.9795) | Total Time 10.00(10.00)\n",
      "Iter 0203 | Time 32.7608(34.0301) | Bit/dim 1.5481(1.7457) | Xent 0.0000(0.0000) | Loss 1.5481(1.7457) | Error 0.0000(0.0000) Steps 374(402.50) | Grad Norm 3.8456(4.9455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 13.7813, Epoch Time 254.5460(261.3926), Bit/dim 1.5232(best: 1.5582), Xent 0.0000, Loss 1.5232, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0204 | Time 34.3116(34.0385) | Bit/dim 1.5265(1.7392) | Xent 0.0000(0.0000) | Loss 1.5265(1.7392) | Error 0.0000(0.0000) Steps 392(402.19) | Grad Norm 3.6843(4.9076) | Total Time 10.00(10.00)\n",
      "Iter 0205 | Time 29.3768(33.8987) | Bit/dim 1.5245(1.7327) | Xent 0.0000(0.0000) | Loss 1.5245(1.7327) | Error 0.0000(0.0000) Steps 368(401.16) | Grad Norm 3.4879(4.8650) | Total Time 10.00(10.00)\n",
      "Iter 0206 | Time 29.6646(33.7716) | Bit/dim 1.5065(1.7259) | Xent 0.0000(0.0000) | Loss 1.5065(1.7259) | Error 0.0000(0.0000) Steps 362(399.99) | Grad Norm 2.3871(4.7907) | Total Time 10.00(10.00)\n",
      "Iter 0207 | Time 32.9776(33.7478) | Bit/dim 1.5131(1.7195) | Xent 0.0000(0.0000) | Loss 1.5131(1.7195) | Error 0.0000(0.0000) Steps 380(399.39) | Grad Norm 2.6140(4.7254) | Total Time 10.00(10.00)\n",
      "Iter 0208 | Time 34.9509(33.7839) | Bit/dim 1.4948(1.7128) | Xent 0.0000(0.0000) | Loss 1.4948(1.7128) | Error 0.0000(0.0000) Steps 380(398.81) | Grad Norm 1.4081(4.6259) | Total Time 10.00(10.00)\n",
      "Iter 0209 | Time 33.0779(33.7627) | Bit/dim 1.5020(1.7065) | Xent 0.0000(0.0000) | Loss 1.5020(1.7065) | Error 0.0000(0.0000) Steps 380(398.24) | Grad Norm 2.4919(4.5619) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 33.6990(33.7608) | Bit/dim 1.4969(1.7002) | Xent 0.0000(0.0000) | Loss 1.4969(1.7002) | Error 0.0000(0.0000) Steps 392(398.06) | Grad Norm 4.5257(4.5608) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 13.4998, Epoch Time 254.0491(261.1723), Bit/dim 1.4804(best: 1.5232), Xent 0.0000, Loss 1.4804, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0211 | Time 32.6265(33.7268) | Bit/dim 1.4840(1.6937) | Xent 0.0000(0.0000) | Loss 1.4840(1.6937) | Error 0.0000(0.0000) Steps 380(397.51) | Grad Norm 3.5771(4.5313) | Total Time 10.00(10.00)\n",
      "Iter 0212 | Time 32.5992(33.6930) | Bit/dim 1.4621(1.6868) | Xent 0.0000(0.0000) | Loss 1.4621(1.6868) | Error 0.0000(0.0000) Steps 380(396.99) | Grad Norm 1.8626(4.4512) | Total Time 10.00(10.00)\n",
      "Iter 0213 | Time 33.7965(33.6961) | Bit/dim 1.4815(1.6806) | Xent 0.0000(0.0000) | Loss 1.4815(1.6806) | Error 0.0000(0.0000) Steps 398(397.02) | Grad Norm 5.3225(4.4773) | Total Time 10.00(10.00)\n",
      "Iter 0214 | Time 34.4856(33.7198) | Bit/dim 1.5133(1.6756) | Xent 0.0000(0.0000) | Loss 1.5133(1.6756) | Error 0.0000(0.0000) Steps 398(397.05) | Grad Norm 7.8455(4.5784) | Total Time 10.00(10.00)\n",
      "Iter 0215 | Time 36.1136(33.7916) | Bit/dim 1.5960(1.6732) | Xent 0.0000(0.0000) | Loss 1.5960(1.6732) | Error 0.0000(0.0000) Steps 416(397.62) | Grad Norm 9.8710(4.7372) | Total Time 10.00(10.00)\n",
      "Iter 0216 | Time 35.0715(33.8300) | Bit/dim 1.4785(1.6674) | Xent 0.0000(0.0000) | Loss 1.4785(1.6674) | Error 0.0000(0.0000) Steps 380(397.09) | Grad Norm 5.1298(4.7489) | Total Time 10.00(10.00)\n",
      "Iter 0217 | Time 34.2212(33.8417) | Bit/dim 1.4737(1.6615) | Xent 0.0000(0.0000) | Loss 1.4737(1.6615) | Error 0.0000(0.0000) Steps 398(397.12) | Grad Norm 4.5468(4.7429) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 14.4878, Epoch Time 265.7253(261.3089), Bit/dim 1.5275(best: 1.4804), Xent 0.0000, Loss 1.5275, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0218 | Time 36.7104(33.9278) | Bit/dim 1.5309(1.6576) | Xent 0.0000(0.0000) | Loss 1.5309(1.6576) | Error 0.0000(0.0000) Steps 410(397.50) | Grad Norm 8.1091(4.8439) | Total Time 10.00(10.00)\n",
      "Iter 0219 | Time 34.6210(33.9486) | Bit/dim 1.4608(1.6517) | Xent 0.0000(0.0000) | Loss 1.4608(1.6517) | Error 0.0000(0.0000) Steps 410(397.88) | Grad Norm 2.7599(4.7813) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 33.4657(33.9341) | Bit/dim 1.4732(1.6464) | Xent 0.0000(0.0000) | Loss 1.4732(1.6464) | Error 0.0000(0.0000) Steps 410(398.24) | Grad Norm 5.3462(4.7983) | Total Time 10.00(10.00)\n",
      "Iter 0221 | Time 33.5022(33.9211) | Bit/dim 1.5255(1.6427) | Xent 0.0000(0.0000) | Loss 1.5255(1.6427) | Error 0.0000(0.0000) Steps 392(398.05) | Grad Norm 8.1575(4.8991) | Total Time 10.00(10.00)\n",
      "Iter 0222 | Time 34.2796(33.9319) | Bit/dim 1.4622(1.6373) | Xent 0.0000(0.0000) | Loss 1.4622(1.6373) | Error 0.0000(0.0000) Steps 410(398.41) | Grad Norm 5.0557(4.9038) | Total Time 10.00(10.00)\n",
      "Iter 0223 | Time 33.8800(33.9303) | Bit/dim 1.4471(1.6316) | Xent 0.0000(0.0000) | Loss 1.4471(1.6316) | Error 0.0000(0.0000) Steps 404(398.58) | Grad Norm 3.3695(4.8577) | Total Time 10.00(10.00)\n",
      "Iter 0224 | Time 32.4472(33.8858) | Bit/dim 1.5023(1.6277) | Xent 0.0000(0.0000) | Loss 1.5023(1.6277) | Error 0.0000(0.0000) Steps 380(398.02) | Grad Norm 8.0706(4.9541) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 13.9335, Epoch Time 265.1116(261.4230), Bit/dim 1.4421(best: 1.4804), Xent 0.0000, Loss 1.4421, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0225 | Time 33.6752(33.8795) | Bit/dim 1.4434(1.6222) | Xent 0.0000(0.0000) | Loss 1.4434(1.6222) | Error 0.0000(0.0000) Steps 404(398.20) | Grad Norm 3.8335(4.9205) | Total Time 10.00(10.00)\n",
      "Iter 0226 | Time 36.1923(33.9489) | Bit/dim 1.4404(1.6168) | Xent 0.0000(0.0000) | Loss 1.4404(1.6168) | Error 0.0000(0.0000) Steps 410(398.56) | Grad Norm 3.6061(4.8811) | Total Time 10.00(10.00)\n",
      "Iter 0227 | Time 33.8847(33.9470) | Bit/dim 1.4690(1.6123) | Xent 0.0000(0.0000) | Loss 1.4690(1.6123) | Error 0.0000(0.0000) Steps 380(398.00) | Grad Norm 6.0436(4.9160) | Total Time 10.00(10.00)\n",
      "Iter 0228 | Time 35.0777(33.9809) | Bit/dim 1.4523(1.6075) | Xent 0.0000(0.0000) | Loss 1.4523(1.6075) | Error 0.0000(0.0000) Steps 404(398.18) | Grad Norm 5.3217(4.9281) | Total Time 10.00(10.00)\n",
      "Iter 0229 | Time 33.9528(33.9800) | Bit/dim 1.4299(1.6022) | Xent 0.0000(0.0000) | Loss 1.4299(1.6022) | Error 0.0000(0.0000) Steps 410(398.53) | Grad Norm 3.0559(4.8720) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 35.9784(34.0400) | Bit/dim 1.4161(1.5966) | Xent 0.0000(0.0000) | Loss 1.4161(1.5966) | Error 0.0000(0.0000) Steps 410(398.88) | Grad Norm 0.9939(4.7556) | Total Time 10.00(10.00)\n",
      "Iter 0231 | Time 33.5423(34.0251) | Bit/dim 1.4271(1.5915) | Xent 0.0000(0.0000) | Loss 1.4271(1.5915) | Error 0.0000(0.0000) Steps 404(399.03) | Grad Norm 4.8407(4.7582) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 13.7302, Epoch Time 268.1283(261.6241), Bit/dim 1.4697(best: 1.4421), Xent 0.0000, Loss 1.4697, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0232 | Time 34.1509(34.0288) | Bit/dim 1.4784(1.5881) | Xent 0.0000(0.0000) | Loss 1.4784(1.5881) | Error 0.0000(0.0000) Steps 392(398.82) | Grad Norm 8.6099(4.8737) | Total Time 10.00(10.00)\n",
      "Iter 0233 | Time 31.3606(33.9488) | Bit/dim 1.5356(1.5866) | Xent 0.0000(0.0000) | Loss 1.5356(1.5866) | Error 0.0000(0.0000) Steps 398(398.80) | Grad Norm 10.5783(5.0449) | Total Time 10.00(10.00)\n",
      "Iter 0234 | Time 36.9866(34.0399) | Bit/dim 1.4540(1.5826) | Xent 0.0000(0.0000) | Loss 1.4540(1.5826) | Error 0.0000(0.0000) Steps 416(399.31) | Grad Norm 7.2471(5.1109) | Total Time 10.00(10.00)\n",
      "Iter 0235 | Time 33.9784(34.0381) | Bit/dim 1.4053(1.5773) | Xent 0.0000(0.0000) | Loss 1.4053(1.5773) | Error 0.0000(0.0000) Steps 410(399.63) | Grad Norm 1.5978(5.0055) | Total Time 10.00(10.00)\n",
      "Iter 0236 | Time 34.7167(34.0584) | Bit/dim 1.4220(1.5726) | Xent 0.0000(0.0000) | Loss 1.4220(1.5726) | Error 0.0000(0.0000) Steps 398(399.58) | Grad Norm 4.8572(5.0011) | Total Time 10.00(10.00)\n",
      "Iter 0237 | Time 34.6578(34.0764) | Bit/dim 1.4581(1.5692) | Xent 0.0000(0.0000) | Loss 1.4581(1.5692) | Error 0.0000(0.0000) Steps 410(399.90) | Grad Norm 7.1797(5.0664) | Total Time 10.00(10.00)\n",
      "Iter 0238 | Time 32.8451(34.0395) | Bit/dim 1.4312(1.5650) | Xent 0.0000(0.0000) | Loss 1.4312(1.5650) | Error 0.0000(0.0000) Steps 398(399.84) | Grad Norm 5.5097(5.0797) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 14.2831, Epoch Time 265.4224(261.7381), Bit/dim 1.3938(best: 1.4421), Xent 0.0000, Loss 1.3938, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0239 | Time 34.3463(34.0487) | Bit/dim 1.3979(1.5600) | Xent 0.0000(0.0000) | Loss 1.3979(1.5600) | Error 0.0000(0.0000) Steps 404(399.96) | Grad Norm 2.2106(4.9937) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 37.4164(34.1497) | Bit/dim 1.4030(1.5553) | Xent 0.0000(0.0000) | Loss 1.4030(1.5553) | Error 0.0000(0.0000) Steps 404(400.09) | Grad Norm 1.8437(4.8992) | Total Time 10.00(10.00)\n",
      "Iter 0241 | Time 33.7194(34.1368) | Bit/dim 1.4042(1.5508) | Xent 0.0000(0.0000) | Loss 1.4042(1.5508) | Error 0.0000(0.0000) Steps 398(400.02) | Grad Norm 5.1569(4.9069) | Total Time 10.00(10.00)\n",
      "Iter 0242 | Time 33.4192(34.1153) | Bit/dim 1.4391(1.5474) | Xent 0.0000(0.0000) | Loss 1.4391(1.5474) | Error 0.0000(0.0000) Steps 398(399.96) | Grad Norm 8.0527(5.0013) | Total Time 10.00(10.00)\n",
      "Iter 0243 | Time 33.0007(34.0818) | Bit/dim 1.4636(1.5449) | Xent 0.0000(0.0000) | Loss 1.4636(1.5449) | Error 0.0000(0.0000) Steps 398(399.90) | Grad Norm 8.9173(5.1188) | Total Time 10.00(10.00)\n",
      "Iter 0244 | Time 33.6408(34.0686) | Bit/dim 1.4329(1.5415) | Xent 0.0000(0.0000) | Loss 1.4329(1.5415) | Error 0.0000(0.0000) Steps 410(400.21) | Grad Norm 6.7820(5.1686) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_published_bs8K_errcontrol_7gpus_1 --seed 0 --conditional False --controlled_tol True --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
