{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=False, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_cifar10_bs900_rl_stdscale_6_run3/epoch_165_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_cifar10_bs900_rl_stdscale_6_run3', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 9080 | Time 19.0109(19.2797) | Bit/dim 3.4273(3.4334) | Xent 0.0000(0.0000) | Loss 8.5987(9.4457) | Error 0.0000(0.0000) Steps 766(756.30) | Grad Norm 6.5633(5.0049) | Total Time 0.00(0.00)\n",
      "Iter 9090 | Time 18.7959(19.1556) | Bit/dim 3.4557(3.4341) | Xent 0.0000(0.0000) | Loss 8.5565(9.2114) | Error 0.0000(0.0000) Steps 784(756.38) | Grad Norm 3.3005(5.0115) | Total Time 0.00(0.00)\n",
      "Iter 9100 | Time 18.9424(19.0052) | Bit/dim 3.4275(3.4316) | Xent 0.0000(0.0000) | Loss 8.4586(9.0289) | Error 0.0000(0.0000) Steps 688(750.04) | Grad Norm 6.0497(4.9248) | Total Time 0.00(0.00)\n",
      "Iter 9110 | Time 19.0549(18.9464) | Bit/dim 3.4664(3.4305) | Xent 0.0000(0.0000) | Loss 8.6880(8.8969) | Error 0.0000(0.0000) Steps 772(750.16) | Grad Norm 4.3214(4.8162) | Total Time 0.00(0.00)\n",
      "Iter 9120 | Time 19.6677(18.9366) | Bit/dim 3.4453(3.4319) | Xent 0.0000(0.0000) | Loss 8.6928(8.8188) | Error 0.0000(0.0000) Steps 760(751.07) | Grad Norm 6.9740(4.8789) | Total Time 0.00(0.00)\n",
      "Iter 9130 | Time 18.8082(18.9007) | Bit/dim 3.4453(3.4343) | Xent 0.0000(0.0000) | Loss 8.6337(8.7568) | Error 0.0000(0.0000) Steps 748(751.60) | Grad Norm 5.8203(4.7505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 100.4818, Epoch Time 1176.8683(1072.9132), Bit/dim 3.4333(best: inf), Xent 0.0000, Loss 3.4333, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 18.6357(18.9558) | Bit/dim 3.4244(3.4312) | Xent 0.0000(0.0000) | Loss 8.6905(9.3530) | Error 0.0000(0.0000) Steps 778(755.90) | Grad Norm 5.5168(4.9069) | Total Time 0.00(0.00)\n",
      "Iter 9150 | Time 18.3095(18.9204) | Bit/dim 3.4212(3.4292) | Xent 0.0000(0.0000) | Loss 8.5046(9.1448) | Error 0.0000(0.0000) Steps 724(757.90) | Grad Norm 4.3091(4.8950) | Total Time 0.00(0.00)\n",
      "Iter 9160 | Time 19.7715(18.9856) | Bit/dim 3.4183(3.4307) | Xent 0.0000(0.0000) | Loss 8.5368(9.0115) | Error 0.0000(0.0000) Steps 802(763.78) | Grad Norm 8.6294(5.0198) | Total Time 0.00(0.00)\n",
      "Iter 9170 | Time 19.3532(19.1493) | Bit/dim 3.4070(3.4303) | Xent 0.0000(0.0000) | Loss 8.4882(8.9085) | Error 0.0000(0.0000) Steps 754(771.76) | Grad Norm 2.8224(4.9174) | Total Time 0.00(0.00)\n",
      "Iter 9180 | Time 19.5912(19.2758) | Bit/dim 3.4130(3.4326) | Xent 0.0000(0.0000) | Loss 8.5783(8.8254) | Error 0.0000(0.0000) Steps 784(775.77) | Grad Norm 3.0365(4.3279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 94.6212, Epoch Time 1171.6249(1075.8745), Bit/dim 3.4407(best: 3.4333), Xent 0.0000, Loss 3.4407, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 18.6167(19.1969) | Bit/dim 3.4229(3.4340) | Xent 0.0000(0.0000) | Loss 8.5793(9.5561) | Error 0.0000(0.0000) Steps 802(773.63) | Grad Norm 5.5949(4.7335) | Total Time 0.00(0.00)\n",
      "Iter 9200 | Time 18.4039(19.1384) | Bit/dim 3.4356(3.4319) | Xent 0.0000(0.0000) | Loss 8.5349(9.2972) | Error 0.0000(0.0000) Steps 760(771.76) | Grad Norm 6.3204(4.9686) | Total Time 0.00(0.00)\n",
      "Iter 9210 | Time 19.5545(19.1949) | Bit/dim 3.4505(3.4308) | Xent 0.0000(0.0000) | Loss 8.5826(9.1009) | Error 0.0000(0.0000) Steps 730(772.01) | Grad Norm 4.3705(4.9274) | Total Time 0.00(0.00)\n",
      "Iter 9220 | Time 18.4205(19.1278) | Bit/dim 3.3916(3.4302) | Xent 0.0000(0.0000) | Loss 8.4819(8.9534) | Error 0.0000(0.0000) Steps 754(770.43) | Grad Norm 4.1717(4.7944) | Total Time 0.00(0.00)\n",
      "Iter 9230 | Time 20.2669(19.1852) | Bit/dim 3.4500(3.4342) | Xent 0.0000(0.0000) | Loss 8.6105(8.8754) | Error 0.0000(0.0000) Steps 766(770.71) | Grad Norm 7.1947(4.9958) | Total Time 0.00(0.00)\n",
      "Iter 9240 | Time 19.8705(19.2003) | Bit/dim 3.4510(3.4332) | Xent 0.0000(0.0000) | Loss 8.7137(8.8044) | Error 0.0000(0.0000) Steps 778(771.68) | Grad Norm 6.2299(5.1007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 95.8886, Epoch Time 1166.7012(1078.5993), Bit/dim 3.4373(best: 3.4333), Xent 0.0000, Loss 3.4373, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 18.9081(19.2300) | Bit/dim 3.4504(3.4323) | Xent 0.0000(0.0000) | Loss 8.6537(9.3973) | Error 0.0000(0.0000) Steps 754(772.16) | Grad Norm 3.5138(4.8664) | Total Time 0.00(0.00)\n",
      "Iter 9260 | Time 19.3320(19.1883) | Bit/dim 3.4185(3.4311) | Xent 0.0000(0.0000) | Loss 8.6449(9.1862) | Error 0.0000(0.0000) Steps 796(774.34) | Grad Norm 7.5102(5.0293) | Total Time 0.00(0.00)\n",
      "Iter 9270 | Time 19.2101(19.1734) | Bit/dim 3.4492(3.4322) | Xent 0.0000(0.0000) | Loss 8.5818(9.0224) | Error 0.0000(0.0000) Steps 796(774.73) | Grad Norm 3.3459(4.9585) | Total Time 0.00(0.00)\n",
      "Iter 9280 | Time 19.2924(19.2168) | Bit/dim 3.4532(3.4324) | Xent 0.0000(0.0000) | Loss 8.6143(8.8967) | Error 0.0000(0.0000) Steps 796(776.65) | Grad Norm 6.0009(4.8864) | Total Time 0.00(0.00)\n",
      "Iter 9290 | Time 19.7192(19.2686) | Bit/dim 3.4139(3.4318) | Xent 0.0000(0.0000) | Loss 8.5875(8.8109) | Error 0.0000(0.0000) Steps 766(777.70) | Grad Norm 3.5151(4.7569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 94.9580, Epoch Time 1173.2749(1081.4396), Bit/dim 3.4373(best: 3.4333), Xent 0.0000, Loss 3.4373, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 19.5156(19.2807) | Bit/dim 3.3961(3.4306) | Xent 0.0000(0.0000) | Loss 8.5839(9.5085) | Error 0.0000(0.0000) Steps 820(780.79) | Grad Norm 2.4631(4.6370) | Total Time 0.00(0.00)\n",
      "Iter 9310 | Time 19.2885(19.3272) | Bit/dim 3.4250(3.4302) | Xent 0.0000(0.0000) | Loss 8.5431(9.2528) | Error 0.0000(0.0000) Steps 766(780.14) | Grad Norm 2.7522(4.5573) | Total Time 0.00(0.00)\n",
      "Iter 9320 | Time 19.4334(19.3916) | Bit/dim 3.4016(3.4300) | Xent 0.0000(0.0000) | Loss 8.6926(9.0759) | Error 0.0000(0.0000) Steps 802(777.92) | Grad Norm 6.1926(4.8822) | Total Time 0.00(0.00)\n",
      "Iter 9330 | Time 19.5314(19.4029) | Bit/dim 3.4434(3.4302) | Xent 0.0000(0.0000) | Loss 8.5124(8.9459) | Error 0.0000(0.0000) Steps 784(779.79) | Grad Norm 7.8384(4.9689) | Total Time 0.00(0.00)\n",
      "Iter 9340 | Time 20.0902(19.3564) | Bit/dim 3.4098(3.4313) | Xent 0.0000(0.0000) | Loss 8.5922(8.8488) | Error 0.0000(0.0000) Steps 802(781.14) | Grad Norm 4.7351(5.0702) | Total Time 0.00(0.00)\n",
      "Iter 9350 | Time 19.1351(19.3507) | Bit/dim 3.3703(3.4292) | Xent 0.0000(0.0000) | Loss 8.5106(8.7768) | Error 0.0000(0.0000) Steps 784(783.82) | Grad Norm 2.5456(4.5855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 96.2773, Epoch Time 1180.3763(1084.4077), Bit/dim 3.4306(best: 3.4333), Xent 0.0000, Loss 3.4306, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 18.9246(19.3991) | Bit/dim 3.4551(3.4297) | Xent 0.0000(0.0000) | Loss 8.5362(9.3901) | Error 0.0000(0.0000) Steps 754(783.65) | Grad Norm 7.9059(4.8817) | Total Time 0.00(0.00)\n",
      "Iter 9370 | Time 19.3839(19.3029) | Bit/dim 3.4239(3.4316) | Xent 0.0000(0.0000) | Loss 8.5148(9.1717) | Error 0.0000(0.0000) Steps 808(781.78) | Grad Norm 3.4459(4.6125) | Total Time 0.00(0.00)\n",
      "Iter 9380 | Time 19.9249(19.2659) | Bit/dim 3.4165(3.4320) | Xent 0.0000(0.0000) | Loss 8.5219(9.0159) | Error 0.0000(0.0000) Steps 820(782.26) | Grad Norm 4.3453(4.4628) | Total Time 0.00(0.00)\n",
      "Iter 9390 | Time 19.5952(19.3480) | Bit/dim 3.4505(3.4313) | Xent 0.0000(0.0000) | Loss 8.6394(8.9028) | Error 0.0000(0.0000) Steps 748(780.04) | Grad Norm 4.6694(4.5810) | Total Time 0.00(0.00)\n",
      "Iter 9400 | Time 20.0253(19.2930) | Bit/dim 3.4530(3.4338) | Xent 0.0000(0.0000) | Loss 8.6183(8.8172) | Error 0.0000(0.0000) Steps 808(780.16) | Grad Norm 2.7312(4.9332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 96.2563, Epoch Time 1173.7388(1087.0876), Bit/dim 3.4378(best: 3.4306), Xent 0.0000, Loss 3.4378, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 19.5202(19.2698) | Bit/dim 3.4165(3.4297) | Xent 0.0000(0.0000) | Loss 8.6605(9.5302) | Error 0.0000(0.0000) Steps 808(778.65) | Grad Norm 2.9879(4.9216) | Total Time 0.00(0.00)\n",
      "Iter 9420 | Time 21.0210(19.3854) | Bit/dim 3.4262(3.4298) | Xent 0.0000(0.0000) | Loss 8.5997(9.2765) | Error 0.0000(0.0000) Steps 802(781.68) | Grad Norm 4.7292(4.8011) | Total Time 0.00(0.00)\n",
      "Iter 9430 | Time 19.8398(19.3998) | Bit/dim 3.4288(3.4301) | Xent 0.0000(0.0000) | Loss 8.5475(9.0938) | Error 0.0000(0.0000) Steps 802(782.13) | Grad Norm 8.9172(5.0024) | Total Time 0.00(0.00)\n",
      "Iter 9440 | Time 18.6800(19.4536) | Bit/dim 3.4424(3.4313) | Xent 0.0000(0.0000) | Loss 8.7074(8.9665) | Error 0.0000(0.0000) Steps 796(783.91) | Grad Norm 5.6224(5.2059) | Total Time 0.00(0.00)\n",
      "Iter 9450 | Time 19.3235(19.4642) | Bit/dim 3.4214(3.4303) | Xent 0.0000(0.0000) | Loss 8.4733(8.8584) | Error 0.0000(0.0000) Steps 748(782.72) | Grad Norm 3.0244(4.9042) | Total Time 0.00(0.00)\n",
      "Iter 9460 | Time 18.9919(19.4923) | Bit/dim 3.4490(3.4301) | Xent 0.0000(0.0000) | Loss 8.6074(8.7804) | Error 0.0000(0.0000) Steps 766(782.79) | Grad Norm 5.7884(4.6307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 96.9548, Epoch Time 1190.0164(1090.1755), Bit/dim 3.4368(best: 3.4306), Xent 0.0000, Loss 3.4368, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 20.4738(19.5641) | Bit/dim 3.4394(3.4304) | Xent 0.0000(0.0000) | Loss 8.5808(9.3944) | Error 0.0000(0.0000) Steps 742(784.20) | Grad Norm 3.0939(4.8349) | Total Time 0.00(0.00)\n",
      "Iter 9480 | Time 20.7687(19.6367) | Bit/dim 3.4656(3.4320) | Xent 0.0000(0.0000) | Loss 8.7952(9.1921) | Error 0.0000(0.0000) Steps 844(784.86) | Grad Norm 6.5719(5.0361) | Total Time 0.00(0.00)\n",
      "Iter 9490 | Time 19.1224(19.6601) | Bit/dim 3.4243(3.4303) | Xent 0.0000(0.0000) | Loss 8.5076(9.0305) | Error 0.0000(0.0000) Steps 796(785.54) | Grad Norm 7.5199(5.2030) | Total Time 0.00(0.00)\n",
      "Iter 9500 | Time 19.4703(19.5954) | Bit/dim 3.4405(3.4295) | Xent 0.0000(0.0000) | Loss 8.6458(8.9107) | Error 0.0000(0.0000) Steps 766(785.01) | Grad Norm 5.0439(4.9404) | Total Time 0.00(0.00)\n",
      "Iter 9510 | Time 19.5226(19.6089) | Bit/dim 3.4265(3.4289) | Xent 0.0000(0.0000) | Loss 8.5834(8.8125) | Error 0.0000(0.0000) Steps 796(788.83) | Grad Norm 5.1263(4.8116) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 98.2202, Epoch Time 1200.0180(1093.4708), Bit/dim 3.4314(best: 3.4306), Xent 0.0000, Loss 3.4314, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 19.4869(19.6798) | Bit/dim 3.4109(3.4285) | Xent 0.0000(0.0000) | Loss 8.5570(9.5335) | Error 0.0000(0.0000) Steps 796(792.07) | Grad Norm 4.8471(4.5593) | Total Time 0.00(0.00)\n",
      "Iter 9530 | Time 19.4857(19.7312) | Bit/dim 3.4581(3.4327) | Xent 0.0000(0.0000) | Loss 8.6412(9.2975) | Error 0.0000(0.0000) Steps 796(794.01) | Grad Norm 4.5815(4.7029) | Total Time 0.00(0.00)\n",
      "Iter 9540 | Time 20.0154(19.7408) | Bit/dim 3.4188(3.4289) | Xent 0.0000(0.0000) | Loss 8.4987(9.0990) | Error 0.0000(0.0000) Steps 832(793.88) | Grad Norm 4.6337(4.7335) | Total Time 0.00(0.00)\n",
      "Iter 9550 | Time 20.2566(19.8586) | Bit/dim 3.4390(3.4308) | Xent 0.0000(0.0000) | Loss 8.6900(8.9663) | Error 0.0000(0.0000) Steps 790(794.97) | Grad Norm 3.1899(4.9042) | Total Time 0.00(0.00)\n",
      "Iter 9560 | Time 20.7676(19.8926) | Bit/dim 3.4825(3.4328) | Xent 0.0000(0.0000) | Loss 8.7395(8.8676) | Error 0.0000(0.0000) Steps 796(793.83) | Grad Norm 3.9013(4.9865) | Total Time 0.00(0.00)\n",
      "Iter 9570 | Time 20.5193(19.9353) | Bit/dim 3.4428(3.4327) | Xent 0.0000(0.0000) | Loss 8.6225(8.7945) | Error 0.0000(0.0000) Steps 778(791.12) | Grad Norm 6.3638(5.0893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 97.8467, Epoch Time 1213.1200(1097.0603), Bit/dim 3.4344(best: 3.4306), Xent 0.0000, Loss 3.4344, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9580 | Time 19.4951(19.9629) | Bit/dim 3.4052(3.4309) | Xent 0.0000(0.0000) | Loss 8.4280(9.3939) | Error 0.0000(0.0000) Steps 826(794.99) | Grad Norm 3.8279(4.7429) | Total Time 0.00(0.00)\n",
      "Iter 9590 | Time 21.1786(20.0153) | Bit/dim 3.4053(3.4302) | Xent 0.0000(0.0000) | Loss 8.5775(9.1724) | Error 0.0000(0.0000) Steps 796(796.44) | Grad Norm 5.6822(4.9538) | Total Time 0.00(0.00)\n",
      "Iter 9600 | Time 21.2611(20.1555) | Bit/dim 3.4120(3.4327) | Xent 0.0000(0.0000) | Loss 8.5446(9.0256) | Error 0.0000(0.0000) Steps 838(800.68) | Grad Norm 4.0768(4.9256) | Total Time 0.00(0.00)\n",
      "Iter 9610 | Time 20.1774(20.2361) | Bit/dim 3.4687(3.4345) | Xent 0.0000(0.0000) | Loss 8.6754(8.9188) | Error 0.0000(0.0000) Steps 832(801.80) | Grad Norm 8.3824(4.8761) | Total Time 0.00(0.00)\n",
      "Iter 9620 | Time 20.5071(20.3424) | Bit/dim 3.5018(3.4401) | Xent 0.0000(0.0000) | Loss 8.7417(8.8493) | Error 0.0000(0.0000) Steps 820(803.89) | Grad Norm 12.4555(5.5704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 99.4652, Epoch Time 1240.0572(1101.3502), Bit/dim 3.4741(best: 3.4306), Xent 0.0000, Loss 3.4741, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9630 | Time 20.6777(20.4268) | Bit/dim 3.4517(3.4507) | Xent 0.0000(0.0000) | Loss 8.6384(9.6019) | Error 0.0000(0.0000) Steps 808(803.97) | Grad Norm 4.6384(5.7837) | Total Time 0.00(0.00)\n",
      "Iter 9640 | Time 21.5376(20.6516) | Bit/dim 3.5122(3.4547) | Xent 0.0000(0.0000) | Loss 8.8559(9.3643) | Error 0.0000(0.0000) Steps 826(809.85) | Grad Norm 6.6487(5.8720) | Total Time 0.00(0.00)\n",
      "Iter 9650 | Time 21.6055(20.8704) | Bit/dim 4.0073(3.5024) | Xent 0.0000(0.0000) | Loss 9.9233(9.2863) | Error 0.0000(0.0000) Steps 832(816.48) | Grad Norm 19.5101(8.8051) | Total Time 0.00(0.00)\n",
      "Iter 9660 | Time 23.3044(21.2476) | Bit/dim 3.7102(3.5674) | Xent 0.0000(0.0000) | Loss 9.2542(9.2803) | Error 0.0000(0.0000) Steps 838(820.70) | Grad Norm 5.8357(9.5594) | Total Time 0.00(0.00)\n",
      "Iter 9670 | Time 23.0171(21.9360) | Bit/dim 3.6929(3.6002) | Xent 0.0000(0.0000) | Loss 9.1252(9.2502) | Error 0.0000(0.0000) Steps 850(826.22) | Grad Norm 144.3192(17.9820) | Total Time 0.00(0.00)\n",
      "Iter 9680 | Time 20.1588(21.5726) | Bit/dim 3.6526(3.6177) | Xent 0.0000(0.0000) | Loss 8.8602(9.1922) | Error 0.0000(0.0000) Steps 784(813.86) | Grad Norm 3.0350(14.3367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 95.8139, Epoch Time 1311.7803(1107.6631), Bit/dim 3.6233(best: 3.4306), Xent 0.0000, Loss 3.6233, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9690 | Time 19.6162(21.3567) | Bit/dim 3.5483(3.6075) | Xent 0.0000(0.0000) | Loss 8.8140(9.7208) | Error 0.0000(0.0000) Steps 736(804.75) | Grad Norm 1.9218(11.1266) | Total Time 0.00(0.00)\n",
      "Iter 9700 | Time 20.2892(21.0497) | Bit/dim 3.5237(3.5912) | Xent 0.0000(0.0000) | Loss 8.6019(9.4596) | Error 0.0000(0.0000) Steps 736(797.32) | Grad Norm 1.4571(8.6326) | Total Time 0.00(0.00)\n",
      "Iter 9710 | Time 19.9638(20.9242) | Bit/dim 3.5198(3.5731) | Xent 0.0000(0.0000) | Loss 8.7282(9.2683) | Error 0.0000(0.0000) Steps 802(796.44) | Grad Norm 1.2964(6.7367) | Total Time 0.00(0.00)\n",
      "Iter 9720 | Time 20.8895(20.9230) | Bit/dim 3.4546(3.5524) | Xent 0.0000(0.0000) | Loss 8.6650(9.1136) | Error 0.0000(0.0000) Steps 808(796.64) | Grad Norm 1.2855(5.3191) | Total Time 0.00(0.00)\n",
      "Iter 9730 | Time 20.4458(20.9241) | Bit/dim 3.4934(3.5398) | Xent 0.0000(0.0000) | Loss 8.4907(8.9951) | Error 0.0000(0.0000) Steps 802(798.32) | Grad Norm 1.0169(4.2016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 94.0016, Epoch Time 1249.9356(1111.9313), Bit/dim 3.4914(best: 3.4306), Xent 0.0000, Loss 3.4914, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9740 | Time 20.6587(20.8286) | Bit/dim 3.4451(3.5256) | Xent 0.0000(0.0000) | Loss 8.6425(9.6287) | Error 0.0000(0.0000) Steps 784(796.16) | Grad Norm 1.0272(3.4031) | Total Time 0.00(0.00)\n",
      "Iter 9750 | Time 21.0753(20.8260) | Bit/dim 3.4658(3.5123) | Xent 0.0000(0.0000) | Loss 8.5630(9.3627) | Error 0.0000(0.0000) Steps 742(795.00) | Grad Norm 1.3281(2.9144) | Total Time 0.00(0.00)\n",
      "Iter 9760 | Time 21.0082(20.7318) | Bit/dim 3.4421(3.5008) | Xent 0.0000(0.0000) | Loss 8.6038(9.1625) | Error 0.0000(0.0000) Steps 820(795.85) | Grad Norm 1.0684(2.4925) | Total Time 0.00(0.00)\n",
      "Iter 9770 | Time 19.7313(20.6895) | Bit/dim 3.4388(3.4917) | Xent 0.0000(0.0000) | Loss 8.5175(9.0165) | Error 0.0000(0.0000) Steps 802(794.76) | Grad Norm 2.8724(2.2638) | Total Time 0.00(0.00)\n",
      "Iter 9780 | Time 19.7842(20.6245) | Bit/dim 3.4726(3.4856) | Xent 0.0000(0.0000) | Loss 8.6280(8.9091) | Error 0.0000(0.0000) Steps 772(789.94) | Grad Norm 1.9261(2.5234) | Total Time 0.00(0.00)\n",
      "Iter 9790 | Time 20.0129(20.4927) | Bit/dim 3.4071(3.4796) | Xent 0.0000(0.0000) | Loss 8.3868(8.8282) | Error 0.0000(0.0000) Steps 802(787.15) | Grad Norm 1.6220(2.2442) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 94.3768, Epoch Time 1238.2121(1115.7197), Bit/dim 3.4640(best: 3.4306), Xent 0.0000, Loss 3.4640, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9800 | Time 19.6240(20.5001) | Bit/dim 3.4641(3.4751) | Xent 0.0000(0.0000) | Loss 8.5384(9.4344) | Error 0.0000(0.0000) Steps 808(791.44) | Grad Norm 4.9118(2.3003) | Total Time 0.00(0.00)\n",
      "Iter 9810 | Time 20.0091(20.4500) | Bit/dim 3.4655(3.4728) | Xent 0.0000(0.0000) | Loss 8.6648(9.2160) | Error 0.0000(0.0000) Steps 760(788.78) | Grad Norm 2.6154(2.4614) | Total Time 0.00(0.00)\n",
      "Iter 9820 | Time 20.1924(20.4517) | Bit/dim 3.4618(3.4688) | Xent 0.0000(0.0000) | Loss 8.6661(9.0477) | Error 0.0000(0.0000) Steps 790(788.36) | Grad Norm 3.2394(2.8953) | Total Time 0.00(0.00)\n",
      "Iter 9830 | Time 19.8134(20.4826) | Bit/dim 3.4843(3.4644) | Xent 0.0000(0.0000) | Loss 8.7054(8.9249) | Error 0.0000(0.0000) Steps 796(790.01) | Grad Norm 5.3309(3.1212) | Total Time 0.00(0.00)\n",
      "Iter 9840 | Time 19.9339(20.3209) | Bit/dim 3.4733(3.4620) | Xent 0.0000(0.0000) | Loss 8.6310(8.8453) | Error 0.0000(0.0000) Steps 796(791.02) | Grad Norm 4.2648(3.2717) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 96.5261, Epoch Time 1233.5592(1119.2549), Bit/dim 3.4578(best: 3.4306), Xent 0.0000, Loss 3.4578, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9850 | Time 19.6532(20.2785) | Bit/dim 3.4508(3.4619) | Xent 0.0000(0.0000) | Loss 8.5129(9.5834) | Error 0.0000(0.0000) Steps 784(792.71) | Grad Norm 3.4534(3.5848) | Total Time 0.00(0.00)\n",
      "Iter 9860 | Time 20.9139(20.4584) | Bit/dim 3.4575(3.4609) | Xent 0.0000(0.0000) | Loss 8.7047(9.3350) | Error 0.0000(0.0000) Steps 838(798.03) | Grad Norm 2.2486(3.6753) | Total Time 0.00(0.00)\n",
      "Iter 9870 | Time 22.9391(20.8948) | Bit/dim 3.5207(3.4649) | Xent 0.0000(0.0000) | Loss 8.7817(9.1540) | Error 0.0000(0.0000) Steps 844(805.75) | Grad Norm 7.4124(12.0643) | Total Time 0.00(0.00)\n",
      "Iter 9880 | Time 21.0492(21.6034) | Bit/dim 3.8448(3.5086) | Xent 0.0000(0.0000) | Loss 9.4971(9.1172) | Error 0.0000(0.0000) Steps 802(809.47) | Grad Norm 9.8904(983489.7396) | Total Time 0.00(0.00)\n",
      "Iter 9890 | Time 19.8787(20.9401) | Bit/dim 3.8219(3.5995) | Xent 0.0000(0.0000) | Loss 9.4239(9.2066) | Error 0.0000(0.0000) Steps 820(799.89) | Grad Norm 4.9722(725250.8055) | Total Time 0.00(0.00)\n",
      "Iter 9900 | Time 20.0467(20.7456) | Bit/dim 3.7064(3.6434) | Xent 0.0000(0.0000) | Loss 9.2378(9.2338) | Error 0.0000(0.0000) Steps 802(799.94) | Grad Norm 4.4638(534818.5844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 93.6469, Epoch Time 1270.8180(1123.8018), Bit/dim 3.7165(best: 3.4306), Xent 0.0000, Loss 3.7165, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9910 | Time 19.0343(20.2859) | Bit/dim 3.6471(3.6527) | Xent 0.0000(0.0000) | Loss 9.0267(9.8548) | Error 0.0000(0.0000) Steps 742(791.70) | Grad Norm 5.6266(394389.0487) | Total Time 0.00(0.00)\n",
      "Iter 9920 | Time 19.7720(19.9527) | Bit/dim 3.6188(3.6477) | Xent 0.0000(0.0000) | Loss 9.0042(9.6363) | Error 0.0000(0.0000) Steps 808(786.80) | Grad Norm 5.4708(290833.9468) | Total Time 0.00(0.00)\n",
      "Iter 9930 | Time 19.2651(19.7535) | Bit/dim 3.5694(3.6356) | Xent 0.0000(0.0000) | Loss 8.9060(9.4545) | Error 0.0000(0.0000) Steps 760(785.65) | Grad Norm 3.6792(214469.2143) | Total Time 0.00(0.00)\n",
      "Iter 9940 | Time 19.0014(19.6095) | Bit/dim 3.5579(3.6190) | Xent 0.0000(0.0000) | Loss 8.8028(9.2940) | Error 0.0000(0.0000) Steps 760(782.91) | Grad Norm 7.2709(158156.3030) | Total Time 0.00(0.00)\n",
      "Iter 9950 | Time 18.7919(19.3905) | Bit/dim 3.5486(3.6068) | Xent 0.0000(0.0000) | Loss 8.8387(9.1743) | Error 0.0000(0.0000) Steps 784(781.13) | Grad Norm 7.0459(116629.7559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 94.0240, Epoch Time 1160.1020(1124.8908), Bit/dim 3.5548(best: 3.4306), Xent 0.0000, Loss 3.5548, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9960 | Time 19.2593(19.3309) | Bit/dim 3.5312(3.5917) | Xent 0.0000(0.0000) | Loss 8.7186(9.8329) | Error 0.0000(0.0000) Steps 784(780.77) | Grad Norm 5.5366(86007.1671) | Total Time 0.00(0.00)\n",
      "Iter 9970 | Time 19.6047(19.3051) | Bit/dim 3.5202(3.5801) | Xent 0.0000(0.0000) | Loss 8.7429(9.5576) | Error 0.0000(0.0000) Steps 814(780.87) | Grad Norm 4.5622(63425.1217) | Total Time 0.00(0.00)\n",
      "Iter 9980 | Time 18.7226(19.3003) | Bit/dim 3.5410(3.5702) | Xent 0.0000(0.0000) | Loss 8.8381(9.3583) | Error 0.0000(0.0000) Steps 778(781.36) | Grad Norm 7.1924(46772.6350) | Total Time 0.00(0.00)\n",
      "Iter 9990 | Time 19.8091(19.3860) | Bit/dim 3.4930(3.5578) | Xent 0.0000(0.0000) | Loss 8.7296(9.1979) | Error 0.0000(0.0000) Steps 814(786.56) | Grad Norm 2.2130(34492.7276) | Total Time 0.00(0.00)\n",
      "Iter 10000 | Time 18.9544(19.4493) | Bit/dim 3.5755(3.5502) | Xent 0.0000(0.0000) | Loss 8.8148(9.0887) | Error 0.0000(0.0000) Steps 778(791.78) | Grad Norm 6.1814(25437.2267) | Total Time 0.00(0.00)\n",
      "Iter 10010 | Time 19.3212(19.4894) | Bit/dim 3.5205(3.5454) | Xent 0.0000(0.0000) | Loss 8.7752(9.0134) | Error 0.0000(0.0000) Steps 820(793.15) | Grad Norm 4.9409(18759.5053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 94.4570, Epoch Time 1182.0242(1126.6048), Bit/dim 3.5273(best: 3.4306), Xent 0.0000, Loss 3.5273, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10020 | Time 19.7883(19.4692) | Bit/dim 3.5192(3.5415) | Xent 0.0000(0.0000) | Loss 8.8123(9.6049) | Error 0.0000(0.0000) Steps 778(789.85) | Grad Norm 6.2014(13834.9803) | Total Time 0.00(0.00)\n",
      "Iter 10030 | Time 19.5186(19.4826) | Bit/dim 3.5357(3.5374) | Xent 0.0000(0.0000) | Loss 8.7265(9.3903) | Error 0.0000(0.0000) Steps 784(791.63) | Grad Norm 8.0950(10204.1071) | Total Time 0.00(0.00)\n",
      "Iter 10040 | Time 19.8547(19.5595) | Bit/dim 3.5446(3.5315) | Xent 0.0000(0.0000) | Loss 8.7838(9.2201) | Error 0.0000(0.0000) Steps 748(790.22) | Grad Norm 7.1004(7526.4415) | Total Time 0.00(0.00)\n",
      "Iter 10050 | Time 19.6845(19.5903) | Bit/dim 3.4685(3.5234) | Xent 0.0000(0.0000) | Loss 8.6470(9.0883) | Error 0.0000(0.0000) Steps 790(791.34) | Grad Norm 5.0325(5551.5805) | Total Time 0.00(0.00)\n",
      "Iter 10060 | Time 19.9551(19.6625) | Bit/dim 3.5384(3.5204) | Xent 0.0000(0.0000) | Loss 8.7519(8.9880) | Error 0.0000(0.0000) Steps 802(794.05) | Grad Norm 3.7471(4095.1034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 94.2318, Epoch Time 1192.7714(1128.5898), Bit/dim 3.5130(best: 3.4306), Xent 0.0000, Loss 3.5130, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10070 | Time 19.5561(19.6549) | Bit/dim 3.4582(3.5146) | Xent 0.0000(0.0000) | Loss 8.6146(9.6887) | Error 0.0000(0.0000) Steps 790(794.54) | Grad Norm 6.3902(3020.9030) | Total Time 0.00(0.00)\n",
      "Iter 10080 | Time 19.3163(19.7258) | Bit/dim 3.5111(3.5113) | Xent 0.0000(0.0000) | Loss 8.7165(9.4318) | Error 0.0000(0.0000) Steps 814(796.74) | Grad Norm 6.0075(2229.5622) | Total Time 0.00(0.00)\n",
      "Iter 10090 | Time 19.9707(19.7822) | Bit/dim 3.5261(3.5132) | Xent 0.0000(0.0000) | Loss 8.7928(9.2511) | Error 0.0000(0.0000) Steps 814(799.45) | Grad Norm 3.7754(1645.3828) | Total Time 0.00(0.00)\n",
      "Iter 10100 | Time 19.1261(19.7297) | Bit/dim 3.4611(3.5081) | Xent 0.0000(0.0000) | Loss 8.6506(9.1115) | Error 0.0000(0.0000) Steps 802(801.13) | Grad Norm 2.9542(1214.7143) | Total Time 0.00(0.00)\n",
      "Iter 10110 | Time 19.9209(19.7514) | Bit/dim 3.4975(3.5095) | Xent 0.0000(0.0000) | Loss 8.7104(9.0044) | Error 0.0000(0.0000) Steps 832(803.12) | Grad Norm 3.4981(897.1038) | Total Time 0.00(0.00)\n",
      "Iter 10120 | Time 20.0609(19.7887) | Bit/dim 3.5030(3.5073) | Xent 0.0000(0.0000) | Loss 8.7454(8.9294) | Error 0.0000(0.0000) Steps 808(803.19) | Grad Norm 5.3467(663.0843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 95.8185, Epoch Time 1203.8122(1130.8464), Bit/dim 3.5049(best: 3.4306), Xent 0.0000, Loss 3.5049, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10130 | Time 20.5408(19.8734) | Bit/dim 3.5122(3.5029) | Xent 0.0000(0.0000) | Loss 8.7425(9.5065) | Error 0.0000(0.0000) Steps 790(802.79) | Grad Norm 4.1901(490.3863) | Total Time 0.00(0.00)\n",
      "Iter 10140 | Time 19.9416(19.8884) | Bit/dim 3.5023(3.5026) | Xent 0.0000(0.0000) | Loss 8.7916(9.3018) | Error 0.0000(0.0000) Steps 814(803.99) | Grad Norm 3.7511(362.7486) | Total Time 0.00(0.00)\n",
      "Iter 10150 | Time 20.2201(19.9210) | Bit/dim 3.4812(3.5001) | Xent 0.0000(0.0000) | Loss 8.6570(9.1462) | Error 0.0000(0.0000) Steps 790(802.29) | Grad Norm 6.8763(268.9346) | Total Time 0.00(0.00)\n",
      "Iter 10160 | Time 21.0578(19.9667) | Bit/dim 3.4925(3.4994) | Xent 0.0000(0.0000) | Loss 8.7584(9.0329) | Error 0.0000(0.0000) Steps 826(804.60) | Grad Norm 5.1685(199.6006) | Total Time 0.00(0.00)\n",
      "Iter 10170 | Time 20.0368(20.1224) | Bit/dim 3.4864(3.4979) | Xent 0.0000(0.0000) | Loss 8.5538(8.9454) | Error 0.0000(0.0000) Steps 832(805.43) | Grad Norm 5.3439(148.6781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 96.1576, Epoch Time 1222.2388(1133.5882), Bit/dim 3.4928(best: 3.4306), Xent 0.0000, Loss 3.4928, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10180 | Time 20.3091(20.0975) | Bit/dim 3.4838(3.4970) | Xent 0.0000(0.0000) | Loss 8.6518(9.6516) | Error 0.0000(0.0000) Steps 808(805.15) | Grad Norm 7.3261(111.0754) | Total Time 0.00(0.00)\n",
      "Iter 10190 | Time 19.4368(20.0709) | Bit/dim 3.4898(3.4942) | Xent 0.0000(0.0000) | Loss 8.6256(9.3984) | Error 0.0000(0.0000) Steps 772(803.56) | Grad Norm 2.1358(83.0436) | Total Time 0.00(0.00)\n",
      "Iter 10200 | Time 19.2634(19.9637) | Bit/dim 3.5272(3.4954) | Xent 0.0000(0.0000) | Loss 8.7818(9.2156) | Error 0.0000(0.0000) Steps 802(804.23) | Grad Norm 6.4192(62.5724) | Total Time 0.00(0.00)\n",
      "Iter 10210 | Time 20.7164(20.0037) | Bit/dim 3.4525(3.4944) | Xent 0.0000(0.0000) | Loss 8.7345(9.0799) | Error 0.0000(0.0000) Steps 832(804.46) | Grad Norm 3.0715(47.5479) | Total Time 0.00(0.00)\n",
      "Iter 10220 | Time 20.4196(20.0079) | Bit/dim 3.4638(3.4921) | Xent 0.0000(0.0000) | Loss 8.4520(8.9641) | Error 0.0000(0.0000) Steps 784(804.40) | Grad Norm 7.1235(36.7261) | Total Time 0.00(0.00)\n",
      "Iter 10230 | Time 20.0244(20.0654) | Bit/dim 3.4821(3.4877) | Xent 0.0000(0.0000) | Loss 8.6729(8.8893) | Error 0.0000(0.0000) Steps 790(805.58) | Grad Norm 4.6094(28.5506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 96.5214, Epoch Time 1214.7614(1136.0234), Bit/dim 3.4892(best: 3.4306), Xent 0.0000, Loss 3.4892, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10240 | Time 19.8059(20.0926) | Bit/dim 3.4947(3.4865) | Xent 0.0000(0.0000) | Loss 8.5889(9.5035) | Error 0.0000(0.0000) Steps 814(803.70) | Grad Norm 3.6168(22.4486) | Total Time 0.00(0.00)\n",
      "Iter 10250 | Time 19.8019(20.0143) | Bit/dim 3.4571(3.4866) | Xent 0.0000(0.0000) | Loss 8.6006(9.2871) | Error 0.0000(0.0000) Steps 802(803.57) | Grad Norm 6.3600(17.8248) | Total Time 0.00(0.00)\n",
      "Iter 10260 | Time 19.1752(19.9258) | Bit/dim 3.4522(3.4852) | Xent 0.0000(0.0000) | Loss 8.5202(9.1234) | Error 0.0000(0.0000) Steps 808(803.58) | Grad Norm 5.4348(14.6323) | Total Time 0.00(0.00)\n",
      "Iter 10270 | Time 19.2020(19.9208) | Bit/dim 3.4555(3.4859) | Xent 0.0000(0.0000) | Loss 8.5310(9.0115) | Error 0.0000(0.0000) Steps 784(799.36) | Grad Norm 4.3770(12.3246) | Total Time 0.00(0.00)\n",
      "Iter 10280 | Time 19.6176(19.9596) | Bit/dim 3.4722(3.4843) | Xent 0.0000(0.0000) | Loss 8.5985(8.9301) | Error 0.0000(0.0000) Steps 808(801.44) | Grad Norm 8.3649(10.5763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 94.9980, Epoch Time 1209.6419(1138.2320), Bit/dim 3.4856(best: 3.4306), Xent 0.0000, Loss 3.4856, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10290 | Time 20.2786(19.9020) | Bit/dim 3.4631(3.4841) | Xent 0.0000(0.0000) | Loss 8.6606(9.5987) | Error 0.0000(0.0000) Steps 808(799.92) | Grad Norm 5.2712(9.3201) | Total Time 0.00(0.00)\n",
      "Iter 10300 | Time 20.5472(20.0189) | Bit/dim 3.4384(3.4856) | Xent 0.0000(0.0000) | Loss 8.6183(9.3622) | Error 0.0000(0.0000) Steps 790(800.41) | Grad Norm 3.4227(8.1274) | Total Time 0.00(0.00)\n",
      "Iter 10310 | Time 19.3877(19.9840) | Bit/dim 3.4705(3.4830) | Xent 0.0000(0.0000) | Loss 8.6819(9.1787) | Error 0.0000(0.0000) Steps 808(798.82) | Grad Norm 6.0616(7.5918) | Total Time 0.00(0.00)\n",
      "Iter 10320 | Time 21.1543(20.0636) | Bit/dim 3.5004(3.4819) | Xent 0.0000(0.0000) | Loss 8.6882(9.0432) | Error 0.0000(0.0000) Steps 790(802.24) | Grad Norm 5.0296(6.6337) | Total Time 0.00(0.00)\n",
      "Iter 10330 | Time 20.0616(20.0639) | Bit/dim 3.4873(3.4822) | Xent 0.0000(0.0000) | Loss 8.6435(8.9477) | Error 0.0000(0.0000) Steps 814(802.39) | Grad Norm 3.6887(6.3942) | Total Time 0.00(0.00)\n",
      "Iter 10340 | Time 20.2237(20.0234) | Bit/dim 3.5047(3.4817) | Xent 0.0000(0.0000) | Loss 8.7893(8.8755) | Error 0.0000(0.0000) Steps 814(801.45) | Grad Norm 7.3640(6.2006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 96.9346, Epoch Time 1217.9189(1140.6226), Bit/dim 3.4840(best: 3.4306), Xent 0.0000, Loss 3.4840, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10350 | Time 19.8943(20.0156) | Bit/dim 3.4561(3.4821) | Xent 0.0000(0.0000) | Loss 8.6409(9.4957) | Error 0.0000(0.0000) Steps 790(800.80) | Grad Norm 4.0331(5.9882) | Total Time 0.00(0.00)\n",
      "Iter 10360 | Time 19.6210(20.0694) | Bit/dim 3.4950(3.4814) | Xent 0.0000(0.0000) | Loss 8.6895(9.2816) | Error 0.0000(0.0000) Steps 766(800.90) | Grad Norm 5.3436(5.8126) | Total Time 0.00(0.00)\n",
      "Iter 10370 | Time 19.9899(20.0594) | Bit/dim 3.4993(3.4808) | Xent 0.0000(0.0000) | Loss 8.7111(9.1265) | Error 0.0000(0.0000) Steps 802(802.27) | Grad Norm 2.9381(5.6047) | Total Time 0.00(0.00)\n",
      "Iter 10380 | Time 20.1716(20.1005) | Bit/dim 3.4549(3.4807) | Xent 0.0000(0.0000) | Loss 8.5699(9.0058) | Error 0.0000(0.0000) Steps 808(803.32) | Grad Norm 5.6254(5.5207) | Total Time 0.00(0.00)\n",
      "Iter 10390 | Time 19.2749(20.1418) | Bit/dim 3.4951(3.4764) | Xent 0.0000(0.0000) | Loss 8.6938(8.9112) | Error 0.0000(0.0000) Steps 820(805.17) | Grad Norm 8.0310(5.6057) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 95.9367, Epoch Time 1221.4469(1143.0473), Bit/dim 3.4766(best: 3.4306), Xent 0.0000, Loss 3.4766, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10400 | Time 20.1246(20.0975) | Bit/dim 3.5198(3.4787) | Xent 0.0000(0.0000) | Loss 8.6697(9.6300) | Error 0.0000(0.0000) Steps 802(806.78) | Grad Norm 5.1980(5.3707) | Total Time 0.00(0.00)\n",
      "Iter 10410 | Time 21.4260(20.1341) | Bit/dim 3.4603(3.4770) | Xent 0.0000(0.0000) | Loss 8.6379(9.3789) | Error 0.0000(0.0000) Steps 832(808.50) | Grad Norm 5.3211(5.2529) | Total Time 0.00(0.00)\n",
      "Iter 10420 | Time 20.6117(20.1897) | Bit/dim 3.4565(3.4767) | Xent 0.0000(0.0000) | Loss 8.6853(9.1910) | Error 0.0000(0.0000) Steps 814(808.22) | Grad Norm 4.5985(5.4116) | Total Time 0.00(0.00)\n",
      "Iter 10430 | Time 19.9081(20.2123) | Bit/dim 3.4803(3.4743) | Xent 0.0000(0.0000) | Loss 8.5979(9.0518) | Error 0.0000(0.0000) Steps 790(808.58) | Grad Norm 10.4990(5.5061) | Total Time 0.00(0.00)\n",
      "Iter 10440 | Time 19.8411(20.1517) | Bit/dim 3.4691(3.4730) | Xent 0.0000(0.0000) | Loss 8.7981(8.9480) | Error 0.0000(0.0000) Steps 820(807.90) | Grad Norm 6.1162(5.7380) | Total Time 0.00(0.00)\n",
      "Iter 10450 | Time 20.3798(20.1624) | Bit/dim 3.4466(3.4763) | Xent 0.0000(0.0000) | Loss 8.6184(8.8843) | Error 0.0000(0.0000) Steps 808(806.81) | Grad Norm 3.8177(5.6446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 96.9983, Epoch Time 1225.1264(1145.5097), Bit/dim 3.4763(best: 3.4306), Xent 0.0000, Loss 3.4763, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10460 | Time 19.1064(20.0969) | Bit/dim 3.4743(3.4765) | Xent 0.0000(0.0000) | Loss 8.5888(9.4998) | Error 0.0000(0.0000) Steps 784(805.00) | Grad Norm 6.3957(5.3379) | Total Time 0.00(0.00)\n",
      "Iter 10470 | Time 20.3874(20.0935) | Bit/dim 3.4524(3.4754) | Xent 0.0000(0.0000) | Loss 8.6360(9.2836) | Error 0.0000(0.0000) Steps 790(804.78) | Grad Norm 5.0067(5.1801) | Total Time 0.00(0.00)\n",
      "Iter 10480 | Time 20.0061(20.0783) | Bit/dim 3.4459(3.4754) | Xent 0.0000(0.0000) | Loss 8.5792(9.1213) | Error 0.0000(0.0000) Steps 808(804.34) | Grad Norm 5.9496(5.3500) | Total Time 0.00(0.00)\n",
      "Iter 10490 | Time 20.2942(20.0802) | Bit/dim 3.4811(3.4711) | Xent 0.0000(0.0000) | Loss 8.6784(8.9962) | Error 0.0000(0.0000) Steps 802(803.12) | Grad Norm 7.4661(5.4640) | Total Time 0.00(0.00)\n",
      "Iter 10500 | Time 20.7181(20.0733) | Bit/dim 3.5041(3.4729) | Xent 0.0000(0.0000) | Loss 8.7085(8.9143) | Error 0.0000(0.0000) Steps 802(802.51) | Grad Norm 4.5919(5.2876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 95.6979, Epoch Time 1218.4803(1147.6988), Bit/dim 3.4731(best: 3.4306), Xent 0.0000, Loss 3.4731, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10510 | Time 20.3037(20.1211) | Bit/dim 3.4855(3.4728) | Xent 0.0000(0.0000) | Loss 8.7394(9.6139) | Error 0.0000(0.0000) Steps 814(801.36) | Grad Norm 6.2648(5.5608) | Total Time 0.00(0.00)\n",
      "Iter 10520 | Time 19.3629(20.1260) | Bit/dim 3.4789(3.4696) | Xent 0.0000(0.0000) | Loss 8.5719(9.3497) | Error 0.0000(0.0000) Steps 784(801.77) | Grad Norm 4.3255(5.5256) | Total Time 0.00(0.00)\n",
      "Iter 10530 | Time 19.9635(20.1282) | Bit/dim 3.4632(3.4710) | Xent 0.0000(0.0000) | Loss 8.6887(9.1698) | Error 0.0000(0.0000) Steps 808(802.13) | Grad Norm 3.1030(5.4095) | Total Time 0.00(0.00)\n",
      "Iter 10540 | Time 19.8928(20.1804) | Bit/dim 3.5182(3.4717) | Xent 0.0000(0.0000) | Loss 8.7766(9.0413) | Error 0.0000(0.0000) Steps 790(805.81) | Grad Norm 5.5046(5.2458) | Total Time 0.00(0.00)\n",
      "Iter 10550 | Time 20.1853(20.2661) | Bit/dim 3.4632(3.4709) | Xent 0.0000(0.0000) | Loss 8.6533(8.9438) | Error 0.0000(0.0000) Steps 808(806.50) | Grad Norm 5.8637(5.4545) | Total Time 0.00(0.00)\n",
      "Iter 10560 | Time 21.1374(20.3531) | Bit/dim 3.4712(3.4712) | Xent 0.0000(0.0000) | Loss 8.6919(8.8768) | Error 0.0000(0.0000) Steps 826(808.99) | Grad Norm 4.7108(5.4892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 96.1708, Epoch Time 1231.8189(1150.2224), Bit/dim 3.4725(best: 3.4306), Xent 0.0000, Loss 3.4725, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10570 | Time 20.7068(20.3410) | Bit/dim 3.4871(3.4705) | Xent 0.0000(0.0000) | Loss 8.6827(9.4757) | Error 0.0000(0.0000) Steps 820(810.57) | Grad Norm 3.9503(5.2194) | Total Time 0.00(0.00)\n",
      "Iter 10580 | Time 19.4535(20.3019) | Bit/dim 3.4587(3.4685) | Xent 0.0000(0.0000) | Loss 8.6622(9.2683) | Error 0.0000(0.0000) Steps 784(811.79) | Grad Norm 6.4407(5.3879) | Total Time 0.00(0.00)\n",
      "Iter 10590 | Time 19.6909(20.2804) | Bit/dim 3.4842(3.4683) | Xent 0.0000(0.0000) | Loss 8.7036(9.1112) | Error 0.0000(0.0000) Steps 784(810.56) | Grad Norm 5.5442(5.2040) | Total Time 0.00(0.00)\n",
      "Iter 10600 | Time 20.4356(20.3142) | Bit/dim 3.4646(3.4663) | Xent 0.0000(0.0000) | Loss 8.7164(8.9921) | Error 0.0000(0.0000) Steps 808(810.77) | Grad Norm 6.0849(5.5073) | Total Time 0.00(0.00)\n",
      "Iter 10610 | Time 19.6120(20.3449) | Bit/dim 3.4733(3.4667) | Xent 0.0000(0.0000) | Loss 8.6090(8.9022) | Error 0.0000(0.0000) Steps 790(808.94) | Grad Norm 5.1470(5.3438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 96.0060, Epoch Time 1232.2645(1152.6837), Bit/dim 3.4742(best: 3.4306), Xent 0.0000, Loss 3.4742, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10620 | Time 20.1992(20.3929) | Bit/dim 3.4784(3.4704) | Xent 0.0000(0.0000) | Loss 8.4674(9.6125) | Error 0.0000(0.0000) Steps 808(809.33) | Grad Norm 2.6338(5.4265) | Total Time 0.00(0.00)\n",
      "Iter 10630 | Time 21.3088(20.4393) | Bit/dim 3.4449(3.4669) | Xent 0.0000(0.0000) | Loss 8.6388(9.3558) | Error 0.0000(0.0000) Steps 838(810.72) | Grad Norm 5.9348(5.4351) | Total Time 0.00(0.00)\n",
      "Iter 10640 | Time 21.1086(20.4876) | Bit/dim 3.4637(3.4669) | Xent 0.0000(0.0000) | Loss 8.6842(9.1616) | Error 0.0000(0.0000) Steps 796(808.67) | Grad Norm 7.8426(5.3597) | Total Time 0.00(0.00)\n",
      "Iter 10650 | Time 20.3287(20.4463) | Bit/dim 3.4584(3.4672) | Xent 0.0000(0.0000) | Loss 8.7299(9.0287) | Error 0.0000(0.0000) Steps 808(807.55) | Grad Norm 5.5892(5.3837) | Total Time 0.00(0.00)\n",
      "Iter 10660 | Time 20.7828(20.4516) | Bit/dim 3.4385(3.4677) | Xent 0.0000(0.0000) | Loss 8.5975(8.9378) | Error 0.0000(0.0000) Steps 808(809.53) | Grad Norm 6.5664(5.3368) | Total Time 0.00(0.00)\n",
      "Iter 10670 | Time 20.4996(20.4723) | Bit/dim 3.4571(3.4668) | Xent 0.0000(0.0000) | Loss 8.6415(8.8625) | Error 0.0000(0.0000) Steps 808(809.75) | Grad Norm 5.7007(5.1432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 96.5685, Epoch Time 1242.3260(1155.3729), Bit/dim 3.4715(best: 3.4306), Xent 0.0000, Loss 3.4715, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10680 | Time 20.3122(20.4133) | Bit/dim 3.4605(3.4691) | Xent 0.0000(0.0000) | Loss 8.7029(9.4637) | Error 0.0000(0.0000) Steps 832(807.94) | Grad Norm 5.0747(5.4064) | Total Time 0.00(0.00)\n",
      "Iter 10690 | Time 19.6875(20.4192) | Bit/dim 3.4808(3.4690) | Xent 0.0000(0.0000) | Loss 8.5642(9.2574) | Error 0.0000(0.0000) Steps 790(806.76) | Grad Norm 5.0086(5.5112) | Total Time 0.00(0.00)\n",
      "Iter 10700 | Time 21.4447(20.5040) | Bit/dim 3.4771(3.4667) | Xent 0.0000(0.0000) | Loss 8.6408(9.0881) | Error 0.0000(0.0000) Steps 802(808.66) | Grad Norm 3.6402(5.2359) | Total Time 0.00(0.00)\n",
      "Iter 10710 | Time 19.9798(20.4180) | Bit/dim 3.4462(3.4620) | Xent 0.0000(0.0000) | Loss 8.6346(8.9707) | Error 0.0000(0.0000) Steps 796(810.31) | Grad Norm 4.7484(5.2960) | Total Time 0.00(0.00)\n",
      "Iter 10720 | Time 20.6320(20.3940) | Bit/dim 3.4426(3.4605) | Xent 0.0000(0.0000) | Loss 8.6403(8.8826) | Error 0.0000(0.0000) Steps 814(810.48) | Grad Norm 4.9586(5.2913) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 96.0151, Epoch Time 1232.4841(1157.6863), Bit/dim 3.4626(best: 3.4306), Xent 0.0000, Loss 3.4626, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10730 | Time 20.3627(20.2732) | Bit/dim 3.4528(3.4605) | Xent 0.0000(0.0000) | Loss 8.6998(9.5991) | Error 0.0000(0.0000) Steps 832(809.65) | Grad Norm 5.3791(5.3681) | Total Time 0.00(0.00)\n",
      "Iter 10740 | Time 20.6556(20.2792) | Bit/dim 3.4818(3.4613) | Xent 0.0000(0.0000) | Loss 8.7953(9.3544) | Error 0.0000(0.0000) Steps 826(809.44) | Grad Norm 3.1692(5.2486) | Total Time 0.00(0.00)\n",
      "Iter 10750 | Time 20.9960(20.3922) | Bit/dim 3.4562(3.4613) | Xent 0.0000(0.0000) | Loss 8.6296(9.1732) | Error 0.0000(0.0000) Steps 802(810.38) | Grad Norm 5.6954(5.3670) | Total Time 0.00(0.00)\n",
      "Iter 10760 | Time 20.9630(20.5244) | Bit/dim 3.4630(3.4622) | Xent 0.0000(0.0000) | Loss 8.5202(9.0289) | Error 0.0000(0.0000) Steps 796(813.67) | Grad Norm 5.2000(5.5252) | Total Time 0.00(0.00)\n",
      "Iter 10770 | Time 19.8467(20.4878) | Bit/dim 3.4444(3.4628) | Xent 0.0000(0.0000) | Loss 8.6373(8.9280) | Error 0.0000(0.0000) Steps 796(812.53) | Grad Norm 6.4783(5.4076) | Total Time 0.00(0.00)\n",
      "Iter 10780 | Time 20.9521(20.4406) | Bit/dim 3.4629(3.4618) | Xent 0.0000(0.0000) | Loss 8.6727(8.8595) | Error 0.0000(0.0000) Steps 808(812.87) | Grad Norm 6.8182(5.5550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 95.3493, Epoch Time 1239.7440(1160.1480), Bit/dim 3.4679(best: 3.4306), Xent 0.0000, Loss 3.4679, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10790 | Time 20.2535(20.4324) | Bit/dim 3.4516(3.4601) | Xent 0.0000(0.0000) | Loss 8.7074(9.4533) | Error 0.0000(0.0000) Steps 820(814.66) | Grad Norm 4.6955(5.5194) | Total Time 0.00(0.00)\n",
      "Iter 10800 | Time 20.7280(20.4747) | Bit/dim 3.4822(3.4644) | Xent 0.0000(0.0000) | Loss 8.5594(9.2535) | Error 0.0000(0.0000) Steps 808(814.53) | Grad Norm 6.1549(5.7014) | Total Time 0.00(0.00)\n",
      "Iter 10810 | Time 20.3790(20.4286) | Bit/dim 3.4737(3.4634) | Xent 0.0000(0.0000) | Loss 8.7379(9.1042) | Error 0.0000(0.0000) Steps 820(816.49) | Grad Norm 4.7131(5.5936) | Total Time 0.00(0.00)\n",
      "Iter 10820 | Time 20.2281(20.4484) | Bit/dim 3.4366(3.4628) | Xent 0.0000(0.0000) | Loss 8.6239(8.9859) | Error 0.0000(0.0000) Steps 802(814.98) | Grad Norm 3.3833(5.5194) | Total Time 0.00(0.00)\n",
      "Iter 10830 | Time 20.5088(20.5203) | Bit/dim 3.4422(3.4613) | Xent 0.0000(0.0000) | Loss 8.5183(8.8909) | Error 0.0000(0.0000) Steps 820(812.11) | Grad Norm 4.5404(5.5702) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 96.6256, Epoch Time 1245.3436(1162.7039), Bit/dim 3.4658(best: 3.4306), Xent 0.0000, Loss 3.4658, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10840 | Time 20.2928(20.5527) | Bit/dim 3.4556(3.4611) | Xent 0.0000(0.0000) | Loss 8.6411(9.6096) | Error 0.0000(0.0000) Steps 808(809.77) | Grad Norm 4.7930(5.4213) | Total Time 0.00(0.00)\n",
      "Iter 10850 | Time 20.6609(20.5956) | Bit/dim 3.5059(3.4624) | Xent 0.0000(0.0000) | Loss 8.8071(9.3650) | Error 0.0000(0.0000) Steps 802(811.21) | Grad Norm 6.7718(5.4480) | Total Time 0.00(0.00)\n",
      "Iter 10860 | Time 20.8732(20.6171) | Bit/dim 3.4668(3.4583) | Xent 0.0000(0.0000) | Loss 8.6950(9.1687) | Error 0.0000(0.0000) Steps 832(815.27) | Grad Norm 4.6572(5.4359) | Total Time 0.00(0.00)\n",
      "Iter 10870 | Time 19.7504(20.5975) | Bit/dim 3.4883(3.4601) | Xent 0.0000(0.0000) | Loss 8.6398(9.0416) | Error 0.0000(0.0000) Steps 790(814.67) | Grad Norm 6.8395(5.6114) | Total Time 0.00(0.00)\n",
      "Iter 10880 | Time 21.8321(20.5472) | Bit/dim 3.4812(3.4630) | Xent 0.0000(0.0000) | Loss 8.7822(8.9444) | Error 0.0000(0.0000) Steps 826(811.70) | Grad Norm 5.1986(5.3703) | Total Time 0.00(0.00)\n",
      "Iter 10890 | Time 20.5009(20.5462) | Bit/dim 3.4602(3.4621) | Xent 0.0000(0.0000) | Loss 8.6641(8.8625) | Error 0.0000(0.0000) Steps 814(813.84) | Grad Norm 4.7051(5.4849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 96.3838, Epoch Time 1244.4263(1165.1555), Bit/dim 3.4620(best: 3.4306), Xent 0.0000, Loss 3.4620, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10900 | Time 20.5451(20.5697) | Bit/dim 3.4307(3.4623) | Xent 0.0000(0.0000) | Loss 8.6215(9.4829) | Error 0.0000(0.0000) Steps 832(813.80) | Grad Norm 3.1004(5.4182) | Total Time 0.00(0.00)\n",
      "Iter 10910 | Time 21.4760(20.5906) | Bit/dim 3.4550(3.4597) | Xent 0.0000(0.0000) | Loss 8.6091(9.2649) | Error 0.0000(0.0000) Steps 844(814.52) | Grad Norm 7.4016(5.4750) | Total Time 0.00(0.00)\n",
      "Iter 10920 | Time 20.5481(20.5549) | Bit/dim 3.4493(3.4585) | Xent 0.0000(0.0000) | Loss 8.6316(9.1149) | Error 0.0000(0.0000) Steps 814(816.34) | Grad Norm 6.2156(5.4149) | Total Time 0.00(0.00)\n",
      "Iter 10930 | Time 20.4712(20.5638) | Bit/dim 3.4183(3.4585) | Xent 0.0000(0.0000) | Loss 8.6477(8.9877) | Error 0.0000(0.0000) Steps 826(816.86) | Grad Norm 3.8154(5.0138) | Total Time 0.00(0.00)\n",
      "Iter 10940 | Time 21.2815(20.6530) | Bit/dim 3.4713(3.4596) | Xent 0.0000(0.0000) | Loss 8.6961(8.9007) | Error 0.0000(0.0000) Steps 856(819.76) | Grad Norm 4.0644(4.9182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 97.4723, Epoch Time 1249.9616(1167.6997), Bit/dim 3.4626(best: 3.4306), Xent 0.0000, Loss 3.4626, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10950 | Time 20.9513(20.6323) | Bit/dim 3.4614(3.4585) | Xent 0.0000(0.0000) | Loss 8.6551(9.6459) | Error 0.0000(0.0000) Steps 820(817.89) | Grad Norm 3.2168(4.9679) | Total Time 0.00(0.00)\n",
      "Iter 10960 | Time 20.3417(20.6176) | Bit/dim 3.4584(3.4580) | Xent 0.0000(0.0000) | Loss 8.7047(9.3898) | Error 0.0000(0.0000) Steps 814(816.98) | Grad Norm 5.7108(4.8770) | Total Time 0.00(0.00)\n",
      "Iter 10970 | Time 20.9458(20.7438) | Bit/dim 3.4466(3.4590) | Xent 0.0000(0.0000) | Loss 8.6216(9.1990) | Error 0.0000(0.0000) Steps 802(817.84) | Grad Norm 2.8047(5.2410) | Total Time 0.00(0.00)\n",
      "Iter 10980 | Time 21.1941(20.8460) | Bit/dim 3.4122(3.4570) | Xent 0.0000(0.0000) | Loss 8.6179(9.0498) | Error 0.0000(0.0000) Steps 850(822.61) | Grad Norm 8.1243(5.0820) | Total Time 0.00(0.00)\n",
      "Iter 10990 | Time 20.5965(20.8299) | Bit/dim 3.4515(3.4554) | Xent 0.0000(0.0000) | Loss 8.6988(8.9452) | Error 0.0000(0.0000) Steps 808(820.68) | Grad Norm 8.2631(5.3078) | Total Time 0.00(0.00)\n",
      "Iter 11000 | Time 20.5771(20.8154) | Bit/dim 3.4229(3.4571) | Xent 0.0000(0.0000) | Loss 8.5948(8.8661) | Error 0.0000(0.0000) Steps 826(819.47) | Grad Norm 6.9925(5.6483) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 96.5121, Epoch Time 1261.8537(1170.5243), Bit/dim 3.4603(best: 3.4306), Xent 0.0000, Loss 3.4603, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11010 | Time 21.5511(20.9389) | Bit/dim 3.4660(3.4565) | Xent 0.0000(0.0000) | Loss 8.6463(9.4757) | Error 0.0000(0.0000) Steps 844(823.41) | Grad Norm 5.8473(5.5700) | Total Time 0.00(0.00)\n",
      "Iter 11020 | Time 21.0888(21.0045) | Bit/dim 3.4837(3.4592) | Xent 0.0000(0.0000) | Loss 8.7021(9.2706) | Error 0.0000(0.0000) Steps 838(826.10) | Grad Norm 3.8885(5.3733) | Total Time 0.00(0.00)\n",
      "Iter 11030 | Time 20.1016(20.9445) | Bit/dim 3.4831(3.4583) | Xent 0.0000(0.0000) | Loss 8.6826(9.1000) | Error 0.0000(0.0000) Steps 814(824.45) | Grad Norm 4.1344(5.2674) | Total Time 0.00(0.00)\n",
      "Iter 11040 | Time 21.8670(20.9458) | Bit/dim 3.4735(3.4541) | Xent 0.0000(0.0000) | Loss 8.7772(8.9772) | Error 0.0000(0.0000) Steps 850(825.45) | Grad Norm 4.2017(5.3822) | Total Time 0.00(0.00)\n",
      "Iter 11050 | Time 21.1928(20.9919) | Bit/dim 3.4528(3.4553) | Xent 0.0000(0.0000) | Loss 8.7561(8.8999) | Error 0.0000(0.0000) Steps 826(825.27) | Grad Norm 6.0572(5.3469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 98.7167, Epoch Time 1277.5287(1173.7345), Bit/dim 3.4607(best: 3.4306), Xent 0.0000, Loss 3.4607, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11060 | Time 21.8185(21.0221) | Bit/dim 3.5049(3.4558) | Xent 0.0000(0.0000) | Loss 8.7595(9.6181) | Error 0.0000(0.0000) Steps 838(825.37) | Grad Norm 4.6060(5.4329) | Total Time 0.00(0.00)\n",
      "Iter 11070 | Time 21.0701(21.1154) | Bit/dim 3.4574(3.4550) | Xent 0.0000(0.0000) | Loss 8.4912(9.3601) | Error 0.0000(0.0000) Steps 802(826.69) | Grad Norm 6.6869(5.5152) | Total Time 0.00(0.00)\n",
      "Iter 11080 | Time 21.0437(21.0251) | Bit/dim 3.4567(3.4519) | Xent 0.0000(0.0000) | Loss 8.6842(9.1691) | Error 0.0000(0.0000) Steps 844(826.19) | Grad Norm 3.7289(5.0955) | Total Time 0.00(0.00)\n",
      "Iter 11090 | Time 20.9479(21.0612) | Bit/dim 3.4636(3.4531) | Xent 0.0000(0.0000) | Loss 8.7487(9.0396) | Error 0.0000(0.0000) Steps 862(828.45) | Grad Norm 7.3752(4.9408) | Total Time 0.00(0.00)\n",
      "Iter 11100 | Time 20.6004(21.0234) | Bit/dim 3.4488(3.4543) | Xent 0.0000(0.0000) | Loss 8.6183(8.9416) | Error 0.0000(0.0000) Steps 832(828.86) | Grad Norm 4.7244(5.4572) | Total Time 0.00(0.00)\n",
      "Iter 11110 | Time 20.9363(21.0852) | Bit/dim 3.4726(3.4548) | Xent 0.0000(0.0000) | Loss 8.6385(8.8582) | Error 0.0000(0.0000) Steps 808(829.97) | Grad Norm 6.4774(5.4766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 97.5231, Epoch Time 1274.9712(1176.7716), Bit/dim 3.4586(best: 3.4306), Xent 0.0000, Loss 3.4586, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11120 | Time 20.9995(21.1121) | Bit/dim 3.4390(3.4528) | Xent 0.0000(0.0000) | Loss 8.7334(9.4805) | Error 0.0000(0.0000) Steps 844(831.21) | Grad Norm 3.6274(5.1777) | Total Time 0.00(0.00)\n",
      "Iter 11130 | Time 21.0789(21.0155) | Bit/dim 3.4711(3.4533) | Xent 0.0000(0.0000) | Loss 8.6774(9.2591) | Error 0.0000(0.0000) Steps 832(830.74) | Grad Norm 5.6976(5.0269) | Total Time 0.00(0.00)\n",
      "Iter 11140 | Time 21.3641(21.0376) | Bit/dim 3.4984(3.4568) | Xent 0.0000(0.0000) | Loss 8.7326(9.1143) | Error 0.0000(0.0000) Steps 826(830.67) | Grad Norm 4.3191(5.1331) | Total Time 0.00(0.00)\n",
      "Iter 11150 | Time 21.5575(21.0959) | Bit/dim 3.4544(3.4562) | Xent 0.0000(0.0000) | Loss 8.5986(8.9978) | Error 0.0000(0.0000) Steps 850(834.05) | Grad Norm 3.9917(5.0904) | Total Time 0.00(0.00)\n",
      "Iter 11160 | Time 21.2238(21.1106) | Bit/dim 3.4369(3.4518) | Xent 0.0000(0.0000) | Loss 8.6107(8.8954) | Error 0.0000(0.0000) Steps 832(834.28) | Grad Norm 6.4387(5.4419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 98.8481, Epoch Time 1276.2430(1179.7557), Bit/dim 3.4599(best: 3.4306), Xent 0.0000, Loss 3.4599, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11170 | Time 21.1582(21.0694) | Bit/dim 3.4745(3.4526) | Xent 0.0000(0.0000) | Loss 8.7677(9.6042) | Error 0.0000(0.0000) Steps 850(838.49) | Grad Norm 7.6472(5.5251) | Total Time 0.00(0.00)\n",
      "Iter 11180 | Time 21.1153(21.1055) | Bit/dim 3.4646(3.4530) | Xent 0.0000(0.0000) | Loss 8.6261(9.3515) | Error 0.0000(0.0000) Steps 844(838.43) | Grad Norm 3.5322(5.4088) | Total Time 0.00(0.00)\n",
      "Iter 11190 | Time 20.7486(21.1148) | Bit/dim 3.4540(3.4519) | Xent 0.0000(0.0000) | Loss 8.6955(9.1660) | Error 0.0000(0.0000) Steps 826(837.49) | Grad Norm 4.3116(5.1296) | Total Time 0.00(0.00)\n",
      "Iter 11200 | Time 21.9192(21.1994) | Bit/dim 3.4348(3.4556) | Xent 0.0000(0.0000) | Loss 8.6126(9.0368) | Error 0.0000(0.0000) Steps 850(837.28) | Grad Norm 3.8090(5.1836) | Total Time 0.00(0.00)\n",
      "Iter 11210 | Time 20.7531(21.1847) | Bit/dim 3.4322(3.4517) | Xent 0.0000(0.0000) | Loss 8.5151(8.9292) | Error 0.0000(0.0000) Steps 844(840.91) | Grad Norm 3.8655(5.2294) | Total Time 0.00(0.00)\n",
      "Iter 11220 | Time 20.9192(21.1753) | Bit/dim 3.4642(3.4532) | Xent 0.0000(0.0000) | Loss 8.6792(8.8572) | Error 0.0000(0.0000) Steps 844(838.62) | Grad Norm 10.3380(5.2933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 98.0990, Epoch Time 1281.7701(1182.8161), Bit/dim 3.4704(best: 3.4306), Xent 0.0000, Loss 3.4704, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11230 | Time 20.7248(21.1486) | Bit/dim 3.4714(3.4551) | Xent 0.0000(0.0000) | Loss 8.6541(9.4966) | Error 0.0000(0.0000) Steps 808(835.75) | Grad Norm 5.7047(5.7190) | Total Time 0.00(0.00)\n",
      "Iter 11240 | Time 22.4221(21.1915) | Bit/dim 3.4578(3.4562) | Xent 0.0000(0.0000) | Loss 8.7715(9.2788) | Error 0.0000(0.0000) Steps 886(838.64) | Grad Norm 4.2648(5.3738) | Total Time 0.00(0.00)\n",
      "Iter 11250 | Time 21.8109(21.3113) | Bit/dim 3.4379(3.4511) | Xent 0.0000(0.0000) | Loss 8.5094(9.1133) | Error 0.0000(0.0000) Steps 850(842.08) | Grad Norm 2.5582(5.1686) | Total Time 0.00(0.00)\n",
      "Iter 11260 | Time 20.9382(21.3280) | Bit/dim 3.4551(3.4512) | Xent 0.0000(0.0000) | Loss 8.6279(8.9967) | Error 0.0000(0.0000) Steps 838(839.86) | Grad Norm 3.7287(5.2097) | Total Time 0.00(0.00)\n",
      "Iter 11270 | Time 21.8313(21.4193) | Bit/dim 3.4320(3.4537) | Xent 0.0000(0.0000) | Loss 8.6129(8.9121) | Error 0.0000(0.0000) Steps 838(840.01) | Grad Norm 3.2204(4.9602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 97.4428, Epoch Time 1294.7808(1186.1751), Bit/dim 3.4577(best: 3.4306), Xent 0.0000, Loss 3.4577, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11280 | Time 20.9804(21.3919) | Bit/dim 3.4455(3.4505) | Xent 0.0000(0.0000) | Loss 8.6037(9.6364) | Error 0.0000(0.0000) Steps 838(838.85) | Grad Norm 5.7150(5.0231) | Total Time 0.00(0.00)\n",
      "Iter 11290 | Time 21.1228(21.2729) | Bit/dim 3.4426(3.4511) | Xent 0.0000(0.0000) | Loss 8.4372(9.3756) | Error 0.0000(0.0000) Steps 838(837.96) | Grad Norm 4.6760(5.0311) | Total Time 0.00(0.00)\n",
      "Iter 11300 | Time 21.1155(21.2490) | Bit/dim 3.4936(3.4523) | Xent 0.0000(0.0000) | Loss 8.6678(9.1855) | Error 0.0000(0.0000) Steps 856(839.04) | Grad Norm 6.2514(5.4229) | Total Time 0.00(0.00)\n",
      "Iter 11310 | Time 21.6674(21.2565) | Bit/dim 3.4153(3.4512) | Xent 0.0000(0.0000) | Loss 8.5308(9.0445) | Error 0.0000(0.0000) Steps 808(837.55) | Grad Norm 5.3160(5.4019) | Total Time 0.00(0.00)\n",
      "Iter 11320 | Time 20.5425(21.2151) | Bit/dim 3.4321(3.4506) | Xent 0.0000(0.0000) | Loss 8.6096(8.9397) | Error 0.0000(0.0000) Steps 838(839.47) | Grad Norm 4.4630(5.3451) | Total Time 0.00(0.00)\n",
      "Iter 11330 | Time 21.7631(21.2081) | Bit/dim 3.4375(3.4487) | Xent 0.0000(0.0000) | Loss 8.6511(8.8581) | Error 0.0000(0.0000) Steps 874(839.60) | Grad Norm 3.1210(5.1951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 97.7974, Epoch Time 1277.9606(1188.9286), Bit/dim 3.4508(best: 3.4306), Xent 0.0000, Loss 3.4508, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11340 | Time 21.0591(21.1980) | Bit/dim 3.4385(3.4488) | Xent 0.0000(0.0000) | Loss 8.5883(9.4999) | Error 0.0000(0.0000) Steps 850(839.74) | Grad Norm 4.1487(5.2969) | Total Time 0.00(0.00)\n",
      "Iter 11350 | Time 21.3618(21.1840) | Bit/dim 3.4803(3.4505) | Xent 0.0000(0.0000) | Loss 8.7720(9.2871) | Error 0.0000(0.0000) Steps 856(839.73) | Grad Norm 3.2267(5.1982) | Total Time 0.00(0.00)\n",
      "Iter 11360 | Time 21.4759(21.2460) | Bit/dim 3.4185(3.4511) | Xent 0.0000(0.0000) | Loss 8.6596(9.1213) | Error 0.0000(0.0000) Steps 862(840.84) | Grad Norm 6.9740(5.3564) | Total Time 0.00(0.00)\n",
      "Iter 11370 | Time 21.6189(21.2515) | Bit/dim 3.4436(3.4501) | Xent 0.0000(0.0000) | Loss 8.6563(8.9960) | Error 0.0000(0.0000) Steps 850(840.39) | Grad Norm 7.3506(5.5444) | Total Time 0.00(0.00)\n",
      "Iter 11380 | Time 21.4022(21.2461) | Bit/dim 3.4105(3.4491) | Xent 0.0000(0.0000) | Loss 8.5490(8.9096) | Error 0.0000(0.0000) Steps 808(839.31) | Grad Norm 5.7751(5.6313) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 97.4076, Epoch Time 1283.5212(1191.7664), Bit/dim 3.4502(best: 3.4306), Xent 0.0000, Loss 3.4502, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11390 | Time 21.2887(21.3011) | Bit/dim 3.4683(3.4496) | Xent 0.0000(0.0000) | Loss 8.6822(9.6457) | Error 0.0000(0.0000) Steps 844(839.34) | Grad Norm 3.5397(5.2030) | Total Time 0.00(0.00)\n",
      "Iter 11400 | Time 20.1668(21.2299) | Bit/dim 3.4351(3.4490) | Xent 0.0000(0.0000) | Loss 8.6060(9.3935) | Error 0.0000(0.0000) Steps 814(839.36) | Grad Norm 6.3497(5.2038) | Total Time 0.00(0.00)\n",
      "Iter 11410 | Time 21.0044(21.2360) | Bit/dim 3.4390(3.4474) | Xent 0.0000(0.0000) | Loss 8.7027(9.1912) | Error 0.0000(0.0000) Steps 850(839.45) | Grad Norm 3.7856(5.2121) | Total Time 0.00(0.00)\n",
      "Iter 11420 | Time 20.7897(21.0885) | Bit/dim 3.4360(3.4470) | Xent 0.0000(0.0000) | Loss 8.6542(9.0443) | Error 0.0000(0.0000) Steps 850(839.31) | Grad Norm 5.0066(5.3036) | Total Time 0.00(0.00)\n",
      "Iter 11430 | Time 20.3933(20.9213) | Bit/dim 3.4134(3.4483) | Xent 0.0000(0.0000) | Loss 8.5503(8.9446) | Error 0.0000(0.0000) Steps 868(838.00) | Grad Norm 5.6142(5.3339) | Total Time 0.00(0.00)\n",
      "Iter 11440 | Time 20.7757(20.8170) | Bit/dim 3.5065(3.4515) | Xent 0.0000(0.0000) | Loss 8.7898(8.8778) | Error 0.0000(0.0000) Steps 856(839.51) | Grad Norm 6.5037(5.3827) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 94.6873, Epoch Time 1260.3766(1193.8247), Bit/dim 3.4523(best: 3.4306), Xent 0.0000, Loss 3.4523, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11450 | Time 20.6148(20.7650) | Bit/dim 3.4027(3.4488) | Xent 0.0000(0.0000) | Loss 8.5271(9.5174) | Error 0.0000(0.0000) Steps 874(843.22) | Grad Norm 5.2012(5.3813) | Total Time 0.00(0.00)\n",
      "Iter 11460 | Time 21.4389(20.8140) | Bit/dim 3.4448(3.4491) | Xent 0.0000(0.0000) | Loss 8.6107(9.2990) | Error 0.0000(0.0000) Steps 880(846.49) | Grad Norm 8.0157(5.4932) | Total Time 0.00(0.00)\n",
      "Iter 11470 | Time 21.5602(20.8469) | Bit/dim 3.4317(3.4495) | Xent 0.0000(0.0000) | Loss 8.6228(9.1312) | Error 0.0000(0.0000) Steps 862(849.14) | Grad Norm 7.1289(5.4929) | Total Time 0.00(0.00)\n",
      "Iter 11480 | Time 20.8014(20.8304) | Bit/dim 3.4362(3.4479) | Xent 0.0000(0.0000) | Loss 8.6337(9.0042) | Error 0.0000(0.0000) Steps 838(846.28) | Grad Norm 4.3710(5.5968) | Total Time 0.00(0.00)\n",
      "Iter 11490 | Time 20.4981(20.7787) | Bit/dim 3.4647(3.4471) | Xent 0.0000(0.0000) | Loss 8.7669(8.9160) | Error 0.0000(0.0000) Steps 856(842.71) | Grad Norm 4.5548(5.5102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 92.9090, Epoch Time 1250.9241(1195.5377), Bit/dim 3.4506(best: 3.4306), Xent 0.0000, Loss 3.4506, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11500 | Time 20.2018(20.6967) | Bit/dim 3.4377(3.4457) | Xent 0.0000(0.0000) | Loss 8.6682(9.6657) | Error 0.0000(0.0000) Steps 844(842.37) | Grad Norm 6.1241(5.4941) | Total Time 0.00(0.00)\n",
      "Iter 11510 | Time 20.7784(20.6963) | Bit/dim 3.4679(3.4453) | Xent 0.0000(0.0000) | Loss 8.5760(9.3930) | Error 0.0000(0.0000) Steps 862(841.90) | Grad Norm 3.2561(5.2692) | Total Time 0.00(0.00)\n",
      "Iter 11520 | Time 21.8827(20.7633) | Bit/dim 3.4518(3.4465) | Xent 0.0000(0.0000) | Loss 8.7458(9.2057) | Error 0.0000(0.0000) Steps 850(844.62) | Grad Norm 3.9558(5.0623) | Total Time 0.00(0.00)\n",
      "Iter 11530 | Time 21.3429(20.7394) | Bit/dim 3.4238(3.4440) | Xent 0.0000(0.0000) | Loss 8.6059(9.0591) | Error 0.0000(0.0000) Steps 880(848.52) | Grad Norm 3.2793(5.3933) | Total Time 0.00(0.00)\n",
      "Iter 11540 | Time 19.6478(20.6034) | Bit/dim 3.4279(3.4466) | Xent 0.0000(0.0000) | Loss 8.6025(8.9602) | Error 0.0000(0.0000) Steps 838(846.99) | Grad Norm 5.6078(5.4396) | Total Time 0.00(0.00)\n",
      "Iter 11550 | Time 20.6029(20.6132) | Bit/dim 3.4506(3.4465) | Xent 0.0000(0.0000) | Loss 8.6906(8.8804) | Error 0.0000(0.0000) Steps 850(848.21) | Grad Norm 6.1008(5.3353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 93.4546, Epoch Time 1243.2370(1196.9687), Bit/dim 3.4517(best: 3.4306), Xent 0.0000, Loss 3.4517, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11560 | Time 21.0960(20.6807) | Bit/dim 3.4241(3.4461) | Xent 0.0000(0.0000) | Loss 8.4677(9.4884) | Error 0.0000(0.0000) Steps 832(848.16) | Grad Norm 4.5219(5.3325) | Total Time 0.00(0.00)\n",
      "Iter 11570 | Time 20.3197(20.6757) | Bit/dim 3.4348(3.4444) | Xent 0.0000(0.0000) | Loss 8.6270(9.2697) | Error 0.0000(0.0000) Steps 856(850.13) | Grad Norm 4.3966(5.1736) | Total Time 0.00(0.00)\n",
      "Iter 11580 | Time 20.8413(20.7427) | Bit/dim 3.4438(3.4467) | Xent 0.0000(0.0000) | Loss 8.7128(9.1184) | Error 0.0000(0.0000) Steps 862(851.91) | Grad Norm 6.5183(5.4508) | Total Time 0.00(0.00)\n",
      "Iter 11590 | Time 20.8742(20.8235) | Bit/dim 3.4163(3.4442) | Xent 0.0000(0.0000) | Loss 8.6385(8.9911) | Error 0.0000(0.0000) Steps 886(853.83) | Grad Norm 6.1873(5.3923) | Total Time 0.00(0.00)\n",
      "Iter 11600 | Time 21.0933(20.8266) | Bit/dim 3.4458(3.4446) | Xent 0.0000(0.0000) | Loss 8.6668(8.9028) | Error 0.0000(0.0000) Steps 850(853.52) | Grad Norm 5.2133(5.2797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 96.1612, Epoch Time 1261.9682(1198.9187), Bit/dim 3.4539(best: 3.4306), Xent 0.0000, Loss 3.4539, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11610 | Time 21.5374(20.8979) | Bit/dim 3.4433(3.4477) | Xent 0.0000(0.0000) | Loss 8.6349(9.6508) | Error 0.0000(0.0000) Steps 844(854.22) | Grad Norm 3.9501(5.2921) | Total Time 0.00(0.00)\n",
      "Iter 11620 | Time 20.5217(20.9504) | Bit/dim 3.4162(3.4454) | Xent 0.0000(0.0000) | Loss 8.3714(9.3822) | Error 0.0000(0.0000) Steps 850(855.77) | Grad Norm 4.3364(5.3637) | Total Time 0.00(0.00)\n",
      "Iter 11630 | Time 21.0596(20.9650) | Bit/dim 3.4276(3.4452) | Xent 0.0000(0.0000) | Loss 8.5537(9.1882) | Error 0.0000(0.0000) Steps 832(854.03) | Grad Norm 6.0164(5.3278) | Total Time 0.00(0.00)\n",
      "Iter 11640 | Time 20.8243(21.0265) | Bit/dim 3.4536(3.4483) | Xent 0.0000(0.0000) | Loss 8.5212(9.0539) | Error 0.0000(0.0000) Steps 844(855.98) | Grad Norm 3.6177(5.3450) | Total Time 0.00(0.00)\n",
      "Iter 11650 | Time 21.2752(20.9578) | Bit/dim 3.4384(3.4476) | Xent 0.0000(0.0000) | Loss 8.5734(8.9408) | Error 0.0000(0.0000) Steps 862(853.91) | Grad Norm 4.9092(5.4649) | Total Time 0.00(0.00)\n",
      "Iter 11660 | Time 21.4757(21.0148) | Bit/dim 3.4257(3.4437) | Xent 0.0000(0.0000) | Loss 8.5951(8.8634) | Error 0.0000(0.0000) Steps 850(855.84) | Grad Norm 5.1556(5.5219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 94.3782, Epoch Time 1268.6181(1201.0097), Bit/dim 3.4479(best: 3.4306), Xent 0.0000, Loss 3.4479, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11670 | Time 21.3392(21.0458) | Bit/dim 3.4718(3.4455) | Xent 0.0000(0.0000) | Loss 8.6156(9.5139) | Error 0.0000(0.0000) Steps 862(856.93) | Grad Norm 6.2022(5.3126) | Total Time 0.00(0.00)\n",
      "Iter 11680 | Time 20.5121(21.0662) | Bit/dim 3.4230(3.4449) | Xent 0.0000(0.0000) | Loss 8.6356(9.2935) | Error 0.0000(0.0000) Steps 850(858.53) | Grad Norm 2.7873(5.0115) | Total Time 0.00(0.00)\n",
      "Iter 11690 | Time 20.5498(21.1364) | Bit/dim 3.4329(3.4441) | Xent 0.0000(0.0000) | Loss 8.6956(9.1327) | Error 0.0000(0.0000) Steps 850(860.34) | Grad Norm 6.3338(5.3069) | Total Time 0.00(0.00)\n",
      "Iter 11700 | Time 21.4624(21.1597) | Bit/dim 3.4457(3.4455) | Xent 0.0000(0.0000) | Loss 8.6810(9.0103) | Error 0.0000(0.0000) Steps 850(862.27) | Grad Norm 6.9460(5.4581) | Total Time 0.00(0.00)\n",
      "Iter 11710 | Time 19.8503(21.0755) | Bit/dim 3.4398(3.4443) | Xent 0.0000(0.0000) | Loss 8.6417(8.9194) | Error 0.0000(0.0000) Steps 874(862.17) | Grad Norm 5.5182(5.5202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 95.4099, Epoch Time 1274.4375(1203.2125), Bit/dim 3.4465(best: 3.4306), Xent 0.0000, Loss 3.4465, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11720 | Time 21.2029(21.0635) | Bit/dim 3.4211(3.4427) | Xent 0.0000(0.0000) | Loss 8.5396(9.6434) | Error 0.0000(0.0000) Steps 874(862.96) | Grad Norm 6.5152(5.2560) | Total Time 0.00(0.00)\n",
      "Iter 11730 | Time 21.1594(21.0348) | Bit/dim 3.4717(3.4449) | Xent 0.0000(0.0000) | Loss 8.6566(9.3891) | Error 0.0000(0.0000) Steps 862(863.12) | Grad Norm 3.8391(5.2812) | Total Time 0.00(0.00)\n",
      "Iter 11740 | Time 20.9376(21.1131) | Bit/dim 3.4285(3.4435) | Xent 0.0000(0.0000) | Loss 8.5845(9.1947) | Error 0.0000(0.0000) Steps 814(863.83) | Grad Norm 5.2316(5.0556) | Total Time 0.00(0.00)\n",
      "Iter 11750 | Time 21.9393(21.1910) | Bit/dim 3.4713(3.4439) | Xent 0.0000(0.0000) | Loss 8.6411(9.0532) | Error 0.0000(0.0000) Steps 904(863.78) | Grad Norm 7.4740(5.0808) | Total Time 0.00(0.00)\n",
      "Iter 11760 | Time 21.6477(21.2232) | Bit/dim 3.4299(3.4412) | Xent 0.0000(0.0000) | Loss 8.7469(8.9515) | Error 0.0000(0.0000) Steps 898(864.71) | Grad Norm 3.9671(5.0415) | Total Time 0.00(0.00)\n",
      "Iter 11770 | Time 21.3633(21.2435) | Bit/dim 3.4523(3.4434) | Xent 0.0000(0.0000) | Loss 8.7277(8.8791) | Error 0.0000(0.0000) Steps 844(866.18) | Grad Norm 4.7043(5.3410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 95.1482, Epoch Time 1278.9473(1205.4845), Bit/dim 3.4456(best: 3.4306), Xent 0.0000, Loss 3.4456, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11780 | Time 21.2884(21.2910) | Bit/dim 3.4478(3.4442) | Xent 0.0000(0.0000) | Loss 8.6950(9.5200) | Error 0.0000(0.0000) Steps 880(868.20) | Grad Norm 4.5706(5.4246) | Total Time 0.00(0.00)\n",
      "Iter 11790 | Time 20.5877(21.2817) | Bit/dim 3.4759(3.4435) | Xent 0.0000(0.0000) | Loss 8.7638(9.2873) | Error 0.0000(0.0000) Steps 880(869.16) | Grad Norm 6.6475(5.3580) | Total Time 0.00(0.00)\n",
      "Iter 11800 | Time 22.1167(21.2499) | Bit/dim 3.4436(3.4426) | Xent 0.0000(0.0000) | Loss 8.8429(9.1306) | Error 0.0000(0.0000) Steps 880(869.45) | Grad Norm 5.9129(5.2251) | Total Time 0.00(0.00)\n",
      "Iter 11810 | Time 21.7265(21.3325) | Bit/dim 3.4046(3.4425) | Xent 0.0000(0.0000) | Loss 8.5352(9.0172) | Error 0.0000(0.0000) Steps 886(873.33) | Grad Norm 4.6149(5.2239) | Total Time 0.00(0.00)\n",
      "Iter 11820 | Time 21.6637(21.3974) | Bit/dim 3.4486(3.4434) | Xent 0.0000(0.0000) | Loss 8.7839(8.9385) | Error 0.0000(0.0000) Steps 904(878.95) | Grad Norm 2.9029(5.2213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 98.2203, Epoch Time 1290.5067(1208.0352), Bit/dim 3.4520(best: 3.4306), Xent 0.0000, Loss 3.4520, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11830 | Time 22.4913(21.4573) | Bit/dim 3.4241(3.4428) | Xent 0.0000(0.0000) | Loss 8.5884(9.7323) | Error 0.0000(0.0000) Steps 856(879.88) | Grad Norm 4.7548(5.3267) | Total Time 0.00(0.00)\n",
      "Iter 11840 | Time 20.6240(21.4511) | Bit/dim 3.4451(3.4403) | Xent 0.0000(0.0000) | Loss 8.5894(9.4484) | Error 0.0000(0.0000) Steps 844(877.65) | Grad Norm 3.5648(5.3577) | Total Time 0.00(0.00)\n",
      "Iter 11850 | Time 20.1751(21.3451) | Bit/dim 3.4514(3.4428) | Xent 0.0000(0.0000) | Loss 8.8200(9.2475) | Error 0.0000(0.0000) Steps 862(870.67) | Grad Norm 7.2312(5.3771) | Total Time 0.00(0.00)\n",
      "Iter 11860 | Time 21.2634(21.2535) | Bit/dim 3.4710(3.4444) | Xent 0.0000(0.0000) | Loss 8.7208(9.0929) | Error 0.0000(0.0000) Steps 880(863.15) | Grad Norm 5.7150(5.0230) | Total Time 0.00(0.00)\n",
      "Iter 11870 | Time 21.2286(21.2224) | Bit/dim 3.4420(3.4435) | Xent 0.0000(0.0000) | Loss 8.6698(8.9838) | Error 0.0000(0.0000) Steps 850(860.41) | Grad Norm 5.3072(5.3609) | Total Time 0.00(0.00)\n",
      "Iter 11880 | Time 19.9262(21.0813) | Bit/dim 3.4679(3.4422) | Xent 0.0000(0.0000) | Loss 8.5352(8.8872) | Error 0.0000(0.0000) Steps 826(857.90) | Grad Norm 5.2149(5.2120) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 95.8539, Epoch Time 1275.5240(1210.0599), Bit/dim 3.4459(best: 3.4306), Xent 0.0000, Loss 3.4459, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11890 | Time 20.1817(20.9888) | Bit/dim 3.4456(3.4420) | Xent 0.0000(0.0000) | Loss 8.6714(9.5073) | Error 0.0000(0.0000) Steps 856(857.44) | Grad Norm 6.9886(5.0549) | Total Time 0.00(0.00)\n",
      "Iter 11900 | Time 21.4744(20.9854) | Bit/dim 3.4199(3.4417) | Xent 0.0000(0.0000) | Loss 8.5601(9.2852) | Error 0.0000(0.0000) Steps 856(857.48) | Grad Norm 6.2070(5.2268) | Total Time 0.00(0.00)\n",
      "Iter 11910 | Time 21.0274(20.9900) | Bit/dim 3.4168(3.4389) | Xent 0.0000(0.0000) | Loss 8.6236(9.1191) | Error 0.0000(0.0000) Steps 856(857.27) | Grad Norm 5.6149(5.2345) | Total Time 0.00(0.00)\n",
      "Iter 11920 | Time 20.3623(20.9847) | Bit/dim 3.3968(3.4387) | Xent 0.0000(0.0000) | Loss 8.5286(8.9949) | Error 0.0000(0.0000) Steps 820(855.11) | Grad Norm 7.0184(5.1619) | Total Time 0.00(0.00)\n",
      "Iter 11930 | Time 21.2878(21.0523) | Bit/dim 3.4606(3.4436) | Xent 0.0000(0.0000) | Loss 8.7478(8.9161) | Error 0.0000(0.0000) Steps 868(854.66) | Grad Norm 4.8306(5.1401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 95.9112, Epoch Time 1267.5922(1211.7858), Bit/dim 3.4427(best: 3.4306), Xent 0.0000, Loss 3.4427, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11940 | Time 20.4060(21.0345) | Bit/dim 3.4177(3.4447) | Xent 0.0000(0.0000) | Loss 8.5785(9.6658) | Error 0.0000(0.0000) Steps 868(854.63) | Grad Norm 6.7028(5.2645) | Total Time 0.00(0.00)\n",
      "Iter 11950 | Time 21.0854(21.0391) | Bit/dim 3.4670(3.4457) | Xent 0.0000(0.0000) | Loss 8.7063(9.4038) | Error 0.0000(0.0000) Steps 862(854.56) | Grad Norm 4.7251(5.3398) | Total Time 0.00(0.00)\n",
      "Iter 11960 | Time 21.3092(21.0161) | Bit/dim 3.4629(3.4429) | Xent 0.0000(0.0000) | Loss 8.7201(9.2111) | Error 0.0000(0.0000) Steps 862(856.55) | Grad Norm 5.6841(5.3404) | Total Time 0.00(0.00)\n",
      "Iter 11970 | Time 21.2842(21.1007) | Bit/dim 3.4355(3.4432) | Xent 0.0000(0.0000) | Loss 8.6559(9.0745) | Error 0.0000(0.0000) Steps 880(857.66) | Grad Norm 5.5246(5.3142) | Total Time 0.00(0.00)\n",
      "Iter 11980 | Time 21.0383(21.0779) | Bit/dim 3.4590(3.4416) | Xent 0.0000(0.0000) | Loss 8.7669(8.9705) | Error 0.0000(0.0000) Steps 874(861.43) | Grad Norm 6.5328(5.4147) | Total Time 0.00(0.00)\n",
      "Iter 11990 | Time 21.6794(21.1333) | Bit/dim 3.4426(3.4408) | Xent 0.0000(0.0000) | Loss 8.7325(8.8897) | Error 0.0000(0.0000) Steps 862(862.64) | Grad Norm 3.7325(5.2022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 96.9793, Epoch Time 1273.1155(1213.6257), Bit/dim 3.4443(best: 3.4306), Xent 0.0000, Loss 3.4443, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12000 | Time 21.4135(21.2348) | Bit/dim 3.4139(3.4402) | Xent 0.0000(0.0000) | Loss 8.5087(9.5226) | Error 0.0000(0.0000) Steps 868(864.59) | Grad Norm 3.5126(5.4313) | Total Time 0.00(0.00)\n",
      "Iter 12010 | Time 21.6682(21.1612) | Bit/dim 3.4420(3.4385) | Xent 0.0000(0.0000) | Loss 8.6309(9.2931) | Error 0.0000(0.0000) Steps 844(862.68) | Grad Norm 5.9789(5.3114) | Total Time 0.00(0.00)\n",
      "Iter 12020 | Time 21.7471(21.0882) | Bit/dim 3.3930(3.4377) | Xent 0.0000(0.0000) | Loss 8.4907(9.1218) | Error 0.0000(0.0000) Steps 862(862.08) | Grad Norm 5.0771(5.0919) | Total Time 0.00(0.00)\n",
      "Iter 12030 | Time 21.7180(21.0548) | Bit/dim 3.4637(3.4397) | Xent 0.0000(0.0000) | Loss 8.8614(9.0015) | Error 0.0000(0.0000) Steps 862(862.18) | Grad Norm 5.8143(5.1053) | Total Time 0.00(0.00)\n",
      "Iter 12040 | Time 20.8665(21.0457) | Bit/dim 3.4112(3.4389) | Xent 0.0000(0.0000) | Loss 8.5706(8.9114) | Error 0.0000(0.0000) Steps 874(863.72) | Grad Norm 3.4928(5.3767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 97.6165, Epoch Time 1272.7029(1215.3980), Bit/dim 3.4416(best: 3.4306), Xent 0.0000, Loss 3.4416, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12050 | Time 22.1690(21.0585) | Bit/dim 3.4376(3.4395) | Xent 0.0000(0.0000) | Loss 8.6430(9.6654) | Error 0.0000(0.0000) Steps 898(866.23) | Grad Norm 6.8977(5.3089) | Total Time 0.00(0.00)\n",
      "Iter 12060 | Time 21.4237(21.1337) | Bit/dim 3.3728(3.4378) | Xent 0.0000(0.0000) | Loss 8.4644(9.3995) | Error 0.0000(0.0000) Steps 856(869.88) | Grad Norm 6.4469(5.5389) | Total Time 0.00(0.00)\n",
      "Iter 12070 | Time 22.2242(21.2334) | Bit/dim 3.4264(3.4362) | Xent 0.0000(0.0000) | Loss 8.6018(9.2083) | Error 0.0000(0.0000) Steps 916(873.52) | Grad Norm 5.5943(5.3776) | Total Time 0.00(0.00)\n",
      "Iter 12080 | Time 21.6857(21.2976) | Bit/dim 3.4554(3.4380) | Xent 0.0000(0.0000) | Loss 8.7751(9.0724) | Error 0.0000(0.0000) Steps 880(876.84) | Grad Norm 3.3595(5.0604) | Total Time 0.00(0.00)\n",
      "Iter 12090 | Time 22.4177(21.3549) | Bit/dim 3.4450(3.4383) | Xent 0.0000(0.0000) | Loss 8.6105(8.9636) | Error 0.0000(0.0000) Steps 886(878.96) | Grad Norm 1.7901(4.8233) | Total Time 0.00(0.00)\n",
      "Iter 12100 | Time 21.5827(21.3910) | Bit/dim 3.4896(3.4400) | Xent 0.0000(0.0000) | Loss 8.6997(8.8926) | Error 0.0000(0.0000) Steps 910(879.97) | Grad Norm 7.9291(5.0101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 98.2619, Epoch Time 1292.2320(1217.7031), Bit/dim 3.4460(best: 3.4306), Xent 0.0000, Loss 3.4460, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12110 | Time 20.6584(21.3401) | Bit/dim 3.4543(3.4406) | Xent 0.0000(0.0000) | Loss 8.7476(9.5592) | Error 0.0000(0.0000) Steps 898(881.25) | Grad Norm 5.9773(5.4001) | Total Time 0.00(0.00)\n",
      "Iter 12120 | Time 20.8226(21.3422) | Bit/dim 3.4164(3.4370) | Xent 0.0000(0.0000) | Loss 8.6096(9.3158) | Error 0.0000(0.0000) Steps 862(879.54) | Grad Norm 7.0716(5.5732) | Total Time 0.00(0.00)\n",
      "Iter 12130 | Time 21.1021(21.3559) | Bit/dim 3.4778(3.4380) | Xent 0.0000(0.0000) | Loss 8.7134(9.1426) | Error 0.0000(0.0000) Steps 886(879.71) | Grad Norm 5.6588(5.5576) | Total Time 0.00(0.00)\n",
      "Iter 12140 | Time 21.8436(21.3682) | Bit/dim 3.4682(3.4407) | Xent 0.0000(0.0000) | Loss 8.7783(9.0282) | Error 0.0000(0.0000) Steps 880(881.88) | Grad Norm 5.1939(5.3564) | Total Time 0.00(0.00)\n",
      "Iter 12150 | Time 21.7097(21.4525) | Bit/dim 3.4645(3.4412) | Xent 0.0000(0.0000) | Loss 8.7080(8.9341) | Error 0.0000(0.0000) Steps 886(882.46) | Grad Norm 3.4099(5.2861) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 98.4169, Epoch Time 1292.3172(1219.9415), Bit/dim 3.4422(best: 3.4306), Xent 0.0000, Loss 3.4422, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12160 | Time 20.0524(21.4453) | Bit/dim 3.3997(3.4405) | Xent 0.0000(0.0000) | Loss 8.4870(9.6888) | Error 0.0000(0.0000) Steps 862(882.83) | Grad Norm 4.2935(5.2273) | Total Time 0.00(0.00)\n",
      "Iter 12170 | Time 21.6198(21.5415) | Bit/dim 3.4642(3.4414) | Xent 0.0000(0.0000) | Loss 8.7876(9.4236) | Error 0.0000(0.0000) Steps 862(880.63) | Grad Norm 5.6853(5.3021) | Total Time 0.00(0.00)\n",
      "Iter 12180 | Time 21.9496(21.5443) | Bit/dim 3.4304(3.4383) | Xent 0.0000(0.0000) | Loss 8.7357(9.2213) | Error 0.0000(0.0000) Steps 874(881.71) | Grad Norm 2.6307(5.0446) | Total Time 0.00(0.00)\n",
      "Iter 12190 | Time 21.7118(21.5502) | Bit/dim 3.4412(3.4382) | Xent 0.0000(0.0000) | Loss 8.5826(9.0699) | Error 0.0000(0.0000) Steps 880(884.65) | Grad Norm 5.2565(5.1872) | Total Time 0.00(0.00)\n",
      "Iter 12200 | Time 21.4479(21.5895) | Bit/dim 3.4319(3.4383) | Xent 0.0000(0.0000) | Loss 8.6536(8.9625) | Error 0.0000(0.0000) Steps 874(883.96) | Grad Norm 5.5734(5.3703) | Total Time 0.00(0.00)\n",
      "Iter 12210 | Time 21.2802(21.5352) | Bit/dim 3.4686(3.4387) | Xent 0.0000(0.0000) | Loss 8.7346(8.8901) | Error 0.0000(0.0000) Steps 904(889.52) | Grad Norm 4.4029(4.9878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 99.2810, Epoch Time 1302.8634(1222.4291), Bit/dim 3.4425(best: 3.4306), Xent 0.0000, Loss 3.4425, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12220 | Time 21.9119(21.5812) | Bit/dim 3.4601(3.4391) | Xent 0.0000(0.0000) | Loss 8.7034(9.5550) | Error 0.0000(0.0000) Steps 928(889.53) | Grad Norm 5.2400(4.9898) | Total Time 0.00(0.00)\n",
      "Iter 12230 | Time 21.4533(21.5157) | Bit/dim 3.4309(3.4361) | Xent 0.0000(0.0000) | Loss 8.6842(9.3106) | Error 0.0000(0.0000) Steps 880(886.27) | Grad Norm 7.7686(4.9799) | Total Time 0.00(0.00)\n",
      "Iter 12240 | Time 21.4388(21.5115) | Bit/dim 3.4653(3.4384) | Xent 0.0000(0.0000) | Loss 8.7205(9.1460) | Error 0.0000(0.0000) Steps 868(886.47) | Grad Norm 4.8222(5.2181) | Total Time 0.00(0.00)\n",
      "Iter 12250 | Time 21.6375(21.5393) | Bit/dim 3.4589(3.4397) | Xent 0.0000(0.0000) | Loss 8.7436(9.0200) | Error 0.0000(0.0000) Steps 886(887.06) | Grad Norm 4.0470(4.7602) | Total Time 0.00(0.00)\n",
      "Iter 12260 | Time 22.4102(21.5463) | Bit/dim 3.4541(3.4406) | Xent 0.0000(0.0000) | Loss 8.6450(8.9331) | Error 0.0000(0.0000) Steps 922(890.64) | Grad Norm 4.6058(5.1552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 98.6064, Epoch Time 1300.0932(1224.7591), Bit/dim 3.4427(best: 3.4306), Xent 0.0000, Loss 3.4427, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12270 | Time 21.3176(21.5875) | Bit/dim 3.4712(3.4433) | Xent 0.0000(0.0000) | Loss 8.7390(9.6683) | Error 0.0000(0.0000) Steps 916(891.05) | Grad Norm 5.2588(5.3586) | Total Time 0.00(0.00)\n",
      "Iter 12280 | Time 22.6757(21.5881) | Bit/dim 3.3954(3.4402) | Xent 0.0000(0.0000) | Loss 8.6733(9.4062) | Error 0.0000(0.0000) Steps 886(890.51) | Grad Norm 3.8737(5.1170) | Total Time 0.00(0.00)\n",
      "Iter 12290 | Time 20.2803(21.5200) | Bit/dim 3.4273(3.4407) | Xent 0.0000(0.0000) | Loss 8.4720(9.2080) | Error 0.0000(0.0000) Steps 886(888.41) | Grad Norm 5.3563(5.5122) | Total Time 0.00(0.00)\n",
      "Iter 12300 | Time 22.5628(21.6580) | Bit/dim 3.4346(3.4369) | Xent 0.0000(0.0000) | Loss 8.7729(9.0584) | Error 0.0000(0.0000) Steps 874(890.98) | Grad Norm 5.2415(5.3108) | Total Time 0.00(0.00)\n",
      "Iter 12310 | Time 21.5836(21.7115) | Bit/dim 3.4490(3.4375) | Xent 0.0000(0.0000) | Loss 8.5823(8.9620) | Error 0.0000(0.0000) Steps 868(893.02) | Grad Norm 5.2372(5.1573) | Total Time 0.00(0.00)\n",
      "Iter 12320 | Time 21.7426(21.7258) | Bit/dim 3.4566(3.4360) | Xent 0.0000(0.0000) | Loss 8.7513(8.8839) | Error 0.0000(0.0000) Steps 916(893.69) | Grad Norm 3.9454(5.2266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 98.6000, Epoch Time 1309.5622(1227.3032), Bit/dim 3.4397(best: 3.4306), Xent 0.0000, Loss 3.4397, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12330 | Time 21.5874(21.6593) | Bit/dim 3.4374(3.4361) | Xent 0.0000(0.0000) | Loss 8.7074(9.5433) | Error 0.0000(0.0000) Steps 910(891.47) | Grad Norm 6.8899(5.2894) | Total Time 0.00(0.00)\n",
      "Iter 12340 | Time 22.6201(21.7354) | Bit/dim 3.4605(3.4368) | Xent 0.0000(0.0000) | Loss 8.8663(9.3226) | Error 0.0000(0.0000) Steps 916(892.43) | Grad Norm 6.1954(5.3403) | Total Time 0.00(0.00)\n",
      "Iter 12350 | Time 22.0034(21.7806) | Bit/dim 3.4784(3.4399) | Xent 0.0000(0.0000) | Loss 8.7746(9.1484) | Error 0.0000(0.0000) Steps 916(891.67) | Grad Norm 5.3288(5.2630) | Total Time 0.00(0.00)\n",
      "Iter 12360 | Time 22.6022(21.8233) | Bit/dim 3.4276(3.4404) | Xent 0.0000(0.0000) | Loss 8.7399(9.0262) | Error 0.0000(0.0000) Steps 928(891.34) | Grad Norm 5.7530(5.6494) | Total Time 0.00(0.00)\n",
      "Iter 12370 | Time 22.4798(21.8999) | Bit/dim 3.4473(3.4405) | Xent 0.0000(0.0000) | Loss 8.6327(8.9335) | Error 0.0000(0.0000) Steps 898(894.49) | Grad Norm 4.7933(5.3579) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 100.5095, Epoch Time 1320.8585(1230.1098), Bit/dim 3.4400(best: 3.4306), Xent 0.0000, Loss 3.4400, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12380 | Time 21.5252(21.9413) | Bit/dim 3.4237(3.4366) | Xent 0.0000(0.0000) | Loss 8.6935(9.6861) | Error 0.0000(0.0000) Steps 898(895.04) | Grad Norm 5.7264(5.2723) | Total Time 0.00(0.00)\n",
      "Iter 12390 | Time 21.4336(21.9249) | Bit/dim 3.4475(3.4371) | Xent 0.0000(0.0000) | Loss 8.6923(9.4206) | Error 0.0000(0.0000) Steps 898(897.06) | Grad Norm 3.5153(5.3219) | Total Time 0.00(0.00)\n",
      "Iter 12400 | Time 22.4947(21.9896) | Bit/dim 3.4322(3.4366) | Xent 0.0000(0.0000) | Loss 8.6031(9.2214) | Error 0.0000(0.0000) Steps 874(899.06) | Grad Norm 6.3348(5.3657) | Total Time 0.00(0.00)\n",
      "Iter 12410 | Time 22.8213(22.1194) | Bit/dim 3.4319(3.4381) | Xent 0.0000(0.0000) | Loss 8.6789(9.0829) | Error 0.0000(0.0000) Steps 928(898.06) | Grad Norm 4.5128(5.3092) | Total Time 0.00(0.00)\n",
      "Iter 12420 | Time 22.4382(22.0704) | Bit/dim 3.4317(3.4364) | Xent 0.0000(0.0000) | Loss 8.6238(8.9724) | Error 0.0000(0.0000) Steps 928(899.66) | Grad Norm 2.9699(5.0093) | Total Time 0.00(0.00)\n",
      "Iter 12430 | Time 21.8827(22.1140) | Bit/dim 3.4704(3.4363) | Xent 0.0000(0.0000) | Loss 8.7459(8.8942) | Error 0.0000(0.0000) Steps 880(900.82) | Grad Norm 4.1174(5.1499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 100.6005, Epoch Time 1334.0871(1233.2291), Bit/dim 3.4399(best: 3.4306), Xent 0.0000, Loss 3.4399, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12440 | Time 22.2779(22.1110) | Bit/dim 3.4583(3.4370) | Xent 0.0000(0.0000) | Loss 8.6449(9.5369) | Error 0.0000(0.0000) Steps 922(904.07) | Grad Norm 4.8951(5.2722) | Total Time 0.00(0.00)\n",
      "Iter 12450 | Time 22.3998(22.1322) | Bit/dim 3.4453(3.4393) | Xent 0.0000(0.0000) | Loss 8.6613(9.3162) | Error 0.0000(0.0000) Steps 898(902.88) | Grad Norm 7.8111(5.4270) | Total Time 0.00(0.00)\n",
      "Iter 12460 | Time 22.6979(22.2582) | Bit/dim 3.3963(3.4356) | Xent 0.0000(0.0000) | Loss 8.4961(9.1482) | Error 0.0000(0.0000) Steps 898(903.99) | Grad Norm 3.9210(5.2755) | Total Time 0.00(0.00)\n",
      "Iter 12470 | Time 22.5966(22.3262) | Bit/dim 3.4518(3.4351) | Xent 0.0000(0.0000) | Loss 8.7254(9.0250) | Error 0.0000(0.0000) Steps 874(902.46) | Grad Norm 4.8264(5.1055) | Total Time 0.00(0.00)\n",
      "Iter 12480 | Time 22.5390(22.2888) | Bit/dim 3.4553(3.4372) | Xent 0.0000(0.0000) | Loss 8.7259(8.9402) | Error 0.0000(0.0000) Steps 922(906.39) | Grad Norm 6.7819(5.2339) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 101.2886, Epoch Time 1346.1975(1236.6182), Bit/dim 3.4386(best: 3.4306), Xent 0.0000, Loss 3.4386, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12490 | Time 22.1864(22.2747) | Bit/dim 3.4354(3.4348) | Xent 0.0000(0.0000) | Loss 8.6883(9.6854) | Error 0.0000(0.0000) Steps 892(903.86) | Grad Norm 6.8473(5.3599) | Total Time 0.00(0.00)\n",
      "Iter 12500 | Time 22.2006(22.2623) | Bit/dim 3.4425(3.4353) | Xent 0.0000(0.0000) | Loss 8.7786(9.4267) | Error 0.0000(0.0000) Steps 916(903.09) | Grad Norm 4.2834(5.2554) | Total Time 0.00(0.00)\n",
      "Iter 12510 | Time 23.0701(22.2376) | Bit/dim 3.4353(3.4345) | Xent 0.0000(0.0000) | Loss 8.7388(9.2239) | Error 0.0000(0.0000) Steps 934(901.47) | Grad Norm 5.2003(5.0566) | Total Time 0.00(0.00)\n",
      "Iter 12520 | Time 22.1769(22.2128) | Bit/dim 3.4528(3.4380) | Xent 0.0000(0.0000) | Loss 8.6630(9.0832) | Error 0.0000(0.0000) Steps 928(904.40) | Grad Norm 4.3709(5.3112) | Total Time 0.00(0.00)\n",
      "Iter 12530 | Time 22.8857(22.2611) | Bit/dim 3.4159(3.4350) | Xent 0.0000(0.0000) | Loss 8.5797(8.9686) | Error 0.0000(0.0000) Steps 904(904.13) | Grad Norm 6.9810(5.4437) | Total Time 0.00(0.00)\n",
      "Iter 12540 | Time 23.2617(22.3987) | Bit/dim 3.4511(3.4341) | Xent 0.0000(0.0000) | Loss 8.6362(8.8792) | Error 0.0000(0.0000) Steps 868(903.17) | Grad Norm 4.7163(5.3892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 101.8871, Epoch Time 1345.0830(1239.8721), Bit/dim 3.4410(best: 3.4306), Xent 0.0000, Loss 3.4410, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12550 | Time 22.6038(22.4602) | Bit/dim 3.4223(3.4353) | Xent 0.0000(0.0000) | Loss 8.7180(9.5442) | Error 0.0000(0.0000) Steps 910(905.96) | Grad Norm 5.3384(5.4436) | Total Time 0.00(0.00)\n",
      "Iter 12560 | Time 22.4331(22.4094) | Bit/dim 3.4459(3.4377) | Xent 0.0000(0.0000) | Loss 8.8053(9.3305) | Error 0.0000(0.0000) Steps 916(908.30) | Grad Norm 3.9859(5.1517) | Total Time 0.00(0.00)\n",
      "Iter 12570 | Time 22.3609(22.3806) | Bit/dim 3.4194(3.4345) | Xent 0.0000(0.0000) | Loss 8.6517(9.1482) | Error 0.0000(0.0000) Steps 910(904.21) | Grad Norm 5.0053(5.2619) | Total Time 0.00(0.00)\n",
      "Iter 12580 | Time 21.9290(22.2674) | Bit/dim 3.3743(3.4329) | Xent 0.0000(0.0000) | Loss 8.5185(9.0156) | Error 0.0000(0.0000) Steps 892(901.44) | Grad Norm 5.9832(5.2651) | Total Time 0.00(0.00)\n",
      "Iter 12590 | Time 22.0891(22.2295) | Bit/dim 3.4101(3.4300) | Xent 0.0000(0.0000) | Loss 8.6009(8.9165) | Error 0.0000(0.0000) Steps 886(900.93) | Grad Norm 6.0281(5.1427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 99.2500, Epoch Time 1340.6808(1242.8964), Bit/dim 3.4365(best: 3.4306), Xent 0.0000, Loss 3.4365, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12600 | Time 23.0903(22.3060) | Bit/dim 3.4294(3.4313) | Xent 0.0000(0.0000) | Loss 8.6600(9.6794) | Error 0.0000(0.0000) Steps 892(902.99) | Grad Norm 3.0298(5.0328) | Total Time 0.00(0.00)\n",
      "Iter 12610 | Time 21.4463(22.2795) | Bit/dim 3.4220(3.4301) | Xent 0.0000(0.0000) | Loss 8.6821(9.4161) | Error 0.0000(0.0000) Steps 892(902.33) | Grad Norm 3.8579(4.7614) | Total Time 0.00(0.00)\n",
      "Iter 12620 | Time 23.2987(22.3073) | Bit/dim 3.4200(3.4337) | Xent 0.0000(0.0000) | Loss 8.7592(9.2332) | Error 0.0000(0.0000) Steps 940(903.85) | Grad Norm 5.7202(4.8712) | Total Time 0.00(0.00)\n",
      "Iter 12630 | Time 21.8254(22.3015) | Bit/dim 3.4409(3.4339) | Xent 0.0000(0.0000) | Loss 8.7246(9.0875) | Error 0.0000(0.0000) Steps 880(905.10) | Grad Norm 6.1315(4.7564) | Total Time 0.00(0.00)\n",
      "Iter 12640 | Time 21.8715(22.2539) | Bit/dim 3.4615(3.4331) | Xent 0.0000(0.0000) | Loss 8.6793(8.9787) | Error 0.0000(0.0000) Steps 874(901.79) | Grad Norm 5.1656(4.9553) | Total Time 0.00(0.00)\n",
      "Iter 12650 | Time 23.0633(22.2493) | Bit/dim 3.4216(3.4278) | Xent 0.0000(0.0000) | Loss 8.7044(8.8906) | Error 0.0000(0.0000) Steps 904(901.26) | Grad Norm 5.3603(5.0620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 101.1085, Epoch Time 1342.2980(1245.8784), Bit/dim 3.4375(best: 3.4306), Xent 0.0000, Loss 3.4375, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12660 | Time 22.1932(22.2659) | Bit/dim 3.4378(3.4271) | Xent 0.0000(0.0000) | Loss 8.7504(9.5385) | Error 0.0000(0.0000) Steps 880(899.74) | Grad Norm 5.5618(5.1681) | Total Time 0.00(0.00)\n",
      "Iter 12670 | Time 21.4002(22.2012) | Bit/dim 3.4101(3.4274) | Xent 0.0000(0.0000) | Loss 8.5676(9.3104) | Error 0.0000(0.0000) Steps 886(900.45) | Grad Norm 3.9825(5.1209) | Total Time 0.00(0.00)\n",
      "Iter 12680 | Time 23.0515(22.1975) | Bit/dim 3.4320(3.4264) | Xent 0.0000(0.0000) | Loss 8.6593(9.1387) | Error 0.0000(0.0000) Steps 880(900.91) | Grad Norm 5.6487(5.0904) | Total Time 0.00(0.00)\n",
      "Iter 12690 | Time 22.0074(22.2001) | Bit/dim 3.4290(3.4270) | Xent 0.0000(0.0000) | Loss 8.5852(9.0137) | Error 0.0000(0.0000) Steps 874(900.97) | Grad Norm 6.3621(5.1190) | Total Time 0.00(0.00)\n",
      "Iter 12700 | Time 21.8597(22.1412) | Bit/dim 3.4317(3.4270) | Xent 0.0000(0.0000) | Loss 8.6668(8.9191) | Error 0.0000(0.0000) Steps 898(902.53) | Grad Norm 4.1754(5.2300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 99.3151, Epoch Time 1332.5863(1248.4797), Bit/dim 3.4341(best: 3.4306), Xent 0.0000, Loss 3.4341, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12710 | Time 22.4148(22.1090) | Bit/dim 3.4352(3.4308) | Xent 0.0000(0.0000) | Loss 8.6788(9.7031) | Error 0.0000(0.0000) Steps 898(903.74) | Grad Norm 3.4426(4.8748) | Total Time 0.00(0.00)\n",
      "Iter 12720 | Time 21.9881(22.1538) | Bit/dim 3.4478(3.4309) | Xent 0.0000(0.0000) | Loss 8.7601(9.4382) | Error 0.0000(0.0000) Steps 922(905.59) | Grad Norm 8.9316(5.2518) | Total Time 0.00(0.00)\n",
      "Iter 12730 | Time 22.2408(22.1707) | Bit/dim 3.4636(3.4295) | Xent 0.0000(0.0000) | Loss 8.8588(9.2364) | Error 0.0000(0.0000) Steps 904(906.11) | Grad Norm 6.7683(5.3642) | Total Time 0.00(0.00)\n",
      "Iter 12740 | Time 22.0075(22.2190) | Bit/dim 3.4463(3.4300) | Xent 0.0000(0.0000) | Loss 8.6735(9.0805) | Error 0.0000(0.0000) Steps 910(904.76) | Grad Norm 3.9307(5.2135) | Total Time 0.00(0.00)\n",
      "Iter 12750 | Time 21.7811(22.1213) | Bit/dim 3.4391(3.4296) | Xent 0.0000(0.0000) | Loss 8.7459(8.9710) | Error 0.0000(0.0000) Steps 898(902.51) | Grad Norm 2.9429(4.9062) | Total Time 0.00(0.00)\n",
      "Iter 12760 | Time 21.7024(22.0543) | Bit/dim 3.4186(3.4283) | Xent 0.0000(0.0000) | Loss 8.6797(8.8966) | Error 0.0000(0.0000) Steps 904(902.13) | Grad Norm 7.6751(5.0268) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 98.9814, Epoch Time 1330.8162(1250.9498), Bit/dim 3.4312(best: 3.4306), Xent 0.0000, Loss 3.4312, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12770 | Time 22.4806(22.0879) | Bit/dim 3.4131(3.4282) | Xent 0.0000(0.0000) | Loss 8.6059(9.5517) | Error 0.0000(0.0000) Steps 880(904.74) | Grad Norm 7.1230(5.1898) | Total Time 0.00(0.00)\n",
      "Iter 12780 | Time 21.4717(22.0138) | Bit/dim 3.4317(3.4294) | Xent 0.0000(0.0000) | Loss 8.6294(9.3216) | Error 0.0000(0.0000) Steps 898(902.05) | Grad Norm 5.7404(5.0153) | Total Time 0.00(0.00)\n",
      "Iter 12790 | Time 22.2922(21.9901) | Bit/dim 3.4160(3.4287) | Xent 0.0000(0.0000) | Loss 8.5313(9.1318) | Error 0.0000(0.0000) Steps 898(903.28) | Grad Norm 4.4297(5.0225) | Total Time 0.00(0.00)\n",
      "Iter 12800 | Time 21.4983(21.9613) | Bit/dim 3.4107(3.4268) | Xent 0.0000(0.0000) | Loss 8.7041(9.0068) | Error 0.0000(0.0000) Steps 904(903.29) | Grad Norm 5.4043(4.9117) | Total Time 0.00(0.00)\n",
      "Iter 12810 | Time 21.9804(22.0415) | Bit/dim 3.4166(3.4306) | Xent 0.0000(0.0000) | Loss 8.5076(8.9217) | Error 0.0000(0.0000) Steps 886(903.09) | Grad Norm 7.7245(5.0916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 99.9834, Epoch Time 1328.6169(1253.2798), Bit/dim 3.4377(best: 3.4306), Xent 0.0000, Loss 3.4377, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12820 | Time 23.4066(22.1682) | Bit/dim 3.4346(3.4298) | Xent 0.0000(0.0000) | Loss 8.6524(9.6916) | Error 0.0000(0.0000) Steps 958(905.26) | Grad Norm 2.4562(4.9546) | Total Time 0.00(0.00)\n",
      "Iter 12830 | Time 22.3948(22.0660) | Bit/dim 3.4407(3.4266) | Xent 0.0000(0.0000) | Loss 8.7036(9.4115) | Error 0.0000(0.0000) Steps 904(904.39) | Grad Norm 5.0332(4.8586) | Total Time 0.00(0.00)\n",
      "Iter 12840 | Time 21.5069(21.9736) | Bit/dim 3.4205(3.4296) | Xent 0.0000(0.0000) | Loss 8.5640(9.2166) | Error 0.0000(0.0000) Steps 880(899.80) | Grad Norm 9.6703(5.0017) | Total Time 0.00(0.00)\n",
      "Iter 12850 | Time 22.3037(21.9992) | Bit/dim 3.4267(3.4288) | Xent 0.0000(0.0000) | Loss 8.6750(9.0773) | Error 0.0000(0.0000) Steps 922(901.49) | Grad Norm 4.6635(5.1874) | Total Time 0.00(0.00)\n",
      "Iter 12860 | Time 20.9605(22.0468) | Bit/dim 3.4271(3.4298) | Xent 0.0000(0.0000) | Loss 8.6863(8.9784) | Error 0.0000(0.0000) Steps 910(902.08) | Grad Norm 5.6110(5.0770) | Total Time 0.00(0.00)\n",
      "Iter 12870 | Time 22.0388(22.1022) | Bit/dim 3.4615(3.4319) | Xent 0.0000(0.0000) | Loss 8.6791(8.9022) | Error 0.0000(0.0000) Steps 910(905.24) | Grad Norm 5.6945(4.8569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 101.4136, Epoch Time 1331.6107(1255.6297), Bit/dim 3.4340(best: 3.4306), Xent 0.0000, Loss 3.4340, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12880 | Time 22.3358(22.0943) | Bit/dim 3.4343(3.4310) | Xent 0.0000(0.0000) | Loss 8.7248(9.5518) | Error 0.0000(0.0000) Steps 880(905.13) | Grad Norm 5.4858(5.2457) | Total Time 0.00(0.00)\n",
      "Iter 12890 | Time 21.9434(22.1842) | Bit/dim 3.3887(3.4327) | Xent 0.0000(0.0000) | Loss 8.5061(9.3275) | Error 0.0000(0.0000) Steps 904(905.85) | Grad Norm 4.4801(5.2304) | Total Time 0.00(0.00)\n",
      "Iter 12900 | Time 22.8234(22.2408) | Bit/dim 3.4292(3.4331) | Xent 0.0000(0.0000) | Loss 8.7302(9.1657) | Error 0.0000(0.0000) Steps 934(910.33) | Grad Norm 5.7412(5.1167) | Total Time 0.00(0.00)\n",
      "Iter 12910 | Time 22.4095(22.2604) | Bit/dim 3.4420(3.4320) | Xent 0.0000(0.0000) | Loss 8.6443(9.0159) | Error 0.0000(0.0000) Steps 898(908.76) | Grad Norm 4.8791(5.2773) | Total Time 0.00(0.00)\n",
      "Iter 12920 | Time 23.1880(22.2468) | Bit/dim 3.4032(3.4289) | Xent 0.0000(0.0000) | Loss 8.7915(8.9313) | Error 0.0000(0.0000) Steps 958(907.45) | Grad Norm 5.6120(5.3821) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 101.5461, Epoch Time 1343.5416(1258.2671), Bit/dim 3.4319(best: 3.4306), Xent 0.0000, Loss 3.4319, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12930 | Time 22.9438(22.3706) | Bit/dim 3.4308(3.4288) | Xent 0.0000(0.0000) | Loss 8.7869(9.7046) | Error 0.0000(0.0000) Steps 916(907.90) | Grad Norm 5.3212(5.3086) | Total Time 0.00(0.00)\n",
      "Iter 12940 | Time 23.1621(22.3151) | Bit/dim 3.4257(3.4293) | Xent 0.0000(0.0000) | Loss 8.7407(9.4421) | Error 0.0000(0.0000) Steps 874(906.90) | Grad Norm 5.6225(5.1447) | Total Time 0.00(0.00)\n",
      "Iter 12950 | Time 21.8275(22.2932) | Bit/dim 3.4266(3.4289) | Xent 0.0000(0.0000) | Loss 8.6011(9.2438) | Error 0.0000(0.0000) Steps 898(907.54) | Grad Norm 5.8594(5.3361) | Total Time 0.00(0.00)\n",
      "Iter 12960 | Time 21.6480(22.2644) | Bit/dim 3.3873(3.4268) | Xent 0.0000(0.0000) | Loss 8.6187(9.0931) | Error 0.0000(0.0000) Steps 892(908.71) | Grad Norm 6.4679(5.1779) | Total Time 0.00(0.00)\n",
      "Iter 12970 | Time 22.4393(22.2120) | Bit/dim 3.4375(3.4285) | Xent 0.0000(0.0000) | Loss 8.6101(8.9863) | Error 0.0000(0.0000) Steps 934(909.38) | Grad Norm 6.5170(5.3125) | Total Time 0.00(0.00)\n",
      "Iter 12980 | Time 22.2207(22.2464) | Bit/dim 3.4742(3.4310) | Xent 0.0000(0.0000) | Loss 8.6494(8.9120) | Error 0.0000(0.0000) Steps 916(914.91) | Grad Norm 4.1474(5.4053) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 101.2267, Epoch Time 1342.1887(1260.7847), Bit/dim 3.4308(best: 3.4306), Xent 0.0000, Loss 3.4308, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12990 | Time 21.7245(22.2328) | Bit/dim 3.4057(3.4319) | Xent 0.0000(0.0000) | Loss 8.5482(9.5447) | Error 0.0000(0.0000) Steps 904(915.47) | Grad Norm 4.8988(5.1557) | Total Time 0.00(0.00)\n",
      "Iter 13000 | Time 22.1523(22.2211) | Bit/dim 3.4228(3.4283) | Xent 0.0000(0.0000) | Loss 8.6750(9.3255) | Error 0.0000(0.0000) Steps 898(914.41) | Grad Norm 4.4854(4.9870) | Total Time 0.00(0.00)\n",
      "Iter 13010 | Time 22.5321(22.1866) | Bit/dim 3.4034(3.4261) | Xent 0.0000(0.0000) | Loss 8.6853(9.1541) | Error 0.0000(0.0000) Steps 940(915.67) | Grad Norm 6.9645(4.9066) | Total Time 0.00(0.00)\n",
      "Iter 13020 | Time 21.6491(22.2559) | Bit/dim 3.4298(3.4277) | Xent 0.0000(0.0000) | Loss 8.7614(9.0359) | Error 0.0000(0.0000) Steps 886(916.77) | Grad Norm 6.2855(5.0967) | Total Time 0.00(0.00)\n",
      "Iter 13030 | Time 22.3717(22.2913) | Bit/dim 3.4477(3.4300) | Xent 0.0000(0.0000) | Loss 8.8235(8.9395) | Error 0.0000(0.0000) Steps 898(915.69) | Grad Norm 5.5289(5.2627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 103.6257, Epoch Time 1344.0753(1263.2834), Bit/dim 3.4336(best: 3.4306), Xent 0.0000, Loss 3.4336, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13040 | Time 23.1099(22.3394) | Bit/dim 3.4158(3.4297) | Xent 0.0000(0.0000) | Loss 8.5973(9.7266) | Error 0.0000(0.0000) Steps 880(915.48) | Grad Norm 4.2837(5.2729) | Total Time 0.00(0.00)\n",
      "Iter 13050 | Time 22.4592(22.3606) | Bit/dim 3.4538(3.4275) | Xent 0.0000(0.0000) | Loss 8.6901(9.4515) | Error 0.0000(0.0000) Steps 898(915.62) | Grad Norm 4.9611(5.3097) | Total Time 0.00(0.00)\n",
      "Iter 13060 | Time 22.3556(22.3436) | Bit/dim 3.4579(3.4296) | Xent 0.0000(0.0000) | Loss 8.7352(9.2480) | Error 0.0000(0.0000) Steps 898(913.95) | Grad Norm 4.3024(5.2012) | Total Time 0.00(0.00)\n",
      "Iter 13070 | Time 23.6192(22.4743) | Bit/dim 3.4269(3.4294) | Xent 0.0000(0.0000) | Loss 8.6775(9.1006) | Error 0.0000(0.0000) Steps 940(917.99) | Grad Norm 4.7542(5.0661) | Total Time 0.00(0.00)\n",
      "Iter 13080 | Time 22.3873(22.5051) | Bit/dim 3.4447(3.4314) | Xent 0.0000(0.0000) | Loss 8.8064(9.0095) | Error 0.0000(0.0000) Steps 910(918.02) | Grad Norm 7.3284(5.3493) | Total Time 0.00(0.00)\n",
      "Iter 13090 | Time 23.7347(22.6232) | Bit/dim 3.4066(3.4295) | Xent 0.0000(0.0000) | Loss 8.6937(8.9279) | Error 0.0000(0.0000) Steps 952(922.36) | Grad Norm 6.4652(5.2477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 102.4115, Epoch Time 1363.1958(1266.2808), Bit/dim 3.4312(best: 3.4306), Xent 0.0000, Loss 3.4312, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13100 | Time 21.9803(22.5630) | Bit/dim 3.3980(3.4269) | Xent 0.0000(0.0000) | Loss 8.6548(9.6080) | Error 0.0000(0.0000) Steps 946(923.19) | Grad Norm 3.5399(5.3550) | Total Time 0.00(0.00)\n",
      "Iter 13110 | Time 21.9320(22.5557) | Bit/dim 3.4754(3.4284) | Xent 0.0000(0.0000) | Loss 8.8495(9.3635) | Error 0.0000(0.0000) Steps 886(922.69) | Grad Norm 1.9868(4.8986) | Total Time 0.00(0.00)\n",
      "Iter 13120 | Time 22.2241(22.6100) | Bit/dim 3.4254(3.4300) | Xent 0.0000(0.0000) | Loss 8.7280(9.1915) | Error 0.0000(0.0000) Steps 916(922.52) | Grad Norm 6.0572(5.1799) | Total Time 0.00(0.00)\n",
      "Iter 13130 | Time 21.9817(22.5358) | Bit/dim 3.4142(3.4270) | Xent 0.0000(0.0000) | Loss 8.6801(9.0452) | Error 0.0000(0.0000) Steps 922(922.90) | Grad Norm 4.3786(5.2035) | Total Time 0.00(0.00)\n",
      "Iter 13140 | Time 22.2302(22.4890) | Bit/dim 3.4280(3.4273) | Xent 0.0000(0.0000) | Loss 8.7769(8.9552) | Error 0.0000(0.0000) Steps 922(921.36) | Grad Norm 3.9606(5.1419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 101.7432, Epoch Time 1354.8542(1268.9380), Bit/dim 3.4273(best: 3.4306), Xent 0.0000, Loss 3.4273, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13150 | Time 22.3211(22.5222) | Bit/dim 3.4085(3.4248) | Xent 0.0000(0.0000) | Loss 8.6111(9.7337) | Error 0.0000(0.0000) Steps 940(923.84) | Grad Norm 5.2456(4.9991) | Total Time 0.00(0.00)\n",
      "Iter 13160 | Time 21.9427(22.6093) | Bit/dim 3.4164(3.4255) | Xent 0.0000(0.0000) | Loss 8.6485(9.4723) | Error 0.0000(0.0000) Steps 946(928.84) | Grad Norm 3.6561(4.9075) | Total Time 0.00(0.00)\n",
      "Iter 13170 | Time 22.8469(22.5913) | Bit/dim 3.4208(3.4282) | Xent 0.0000(0.0000) | Loss 8.8352(9.2877) | Error 0.0000(0.0000) Steps 946(928.22) | Grad Norm 3.7137(4.8176) | Total Time 0.00(0.00)\n",
      "Iter 13180 | Time 22.0855(22.5557) | Bit/dim 3.4066(3.4247) | Xent 0.0000(0.0000) | Loss 8.6128(9.1259) | Error 0.0000(0.0000) Steps 904(927.17) | Grad Norm 5.9419(5.1426) | Total Time 0.00(0.00)\n",
      "Iter 13190 | Time 23.2591(22.6195) | Bit/dim 3.4174(3.4269) | Xent 0.0000(0.0000) | Loss 8.7693(9.0229) | Error 0.0000(0.0000) Steps 958(924.76) | Grad Norm 4.5836(4.9269) | Total Time 0.00(0.00)\n",
      "Iter 13200 | Time 21.2011(22.5985) | Bit/dim 3.4349(3.4280) | Xent 0.0000(0.0000) | Loss 8.7263(8.9468) | Error 0.0000(0.0000) Steps 934(926.95) | Grad Norm 5.3323(5.2770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 102.3958, Epoch Time 1364.0448(1271.7912), Bit/dim 3.4309(best: 3.4273), Xent 0.0000, Loss 3.4309, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13210 | Time 22.7796(22.6987) | Bit/dim 3.3983(3.4276) | Xent 0.0000(0.0000) | Loss 8.7577(9.6086) | Error 0.0000(0.0000) Steps 952(927.76) | Grad Norm 4.9309(5.3194) | Total Time 0.00(0.00)\n",
      "Iter 13220 | Time 23.0545(22.6894) | Bit/dim 3.4280(3.4261) | Xent 0.0000(0.0000) | Loss 8.7495(9.3755) | Error 0.0000(0.0000) Steps 946(925.40) | Grad Norm 5.7884(5.0596) | Total Time 0.00(0.00)\n",
      "Iter 13230 | Time 22.1176(22.6230) | Bit/dim 3.4105(3.4254) | Xent 0.0000(0.0000) | Loss 8.7563(9.2110) | Error 0.0000(0.0000) Steps 946(925.58) | Grad Norm 3.1275(5.0896) | Total Time 0.00(0.00)\n",
      "Iter 13240 | Time 22.9264(22.5707) | Bit/dim 3.4326(3.4242) | Xent 0.0000(0.0000) | Loss 8.7584(9.0803) | Error 0.0000(0.0000) Steps 940(922.92) | Grad Norm 4.3471(5.0409) | Total Time 0.00(0.00)\n",
      "Iter 13250 | Time 22.6422(22.4693) | Bit/dim 3.4468(3.4284) | Xent 0.0000(0.0000) | Loss 8.7794(8.9866) | Error 0.0000(0.0000) Steps 916(922.26) | Grad Norm 5.4099(5.0584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 99.7666, Epoch Time 1351.8042(1274.1916), Bit/dim 3.4304(best: 3.4273), Xent 0.0000, Loss 3.4304, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13260 | Time 21.7057(22.3792) | Bit/dim 3.4076(3.4289) | Xent 0.0000(0.0000) | Loss 8.5302(9.7196) | Error 0.0000(0.0000) Steps 910(921.55) | Grad Norm 3.8849(5.1440) | Total Time 0.00(0.00)\n",
      "Iter 13270 | Time 21.8562(22.3618) | Bit/dim 3.4321(3.4263) | Xent 0.0000(0.0000) | Loss 8.5833(9.4379) | Error 0.0000(0.0000) Steps 910(918.27) | Grad Norm 4.1027(4.9152) | Total Time 0.00(0.00)\n",
      "Iter 13280 | Time 22.1220(22.2240) | Bit/dim 3.4615(3.4251) | Xent 0.0000(0.0000) | Loss 8.8176(9.2431) | Error 0.0000(0.0000) Steps 898(915.98) | Grad Norm 5.2654(4.9899) | Total Time 0.00(0.00)\n",
      "Iter 13290 | Time 22.7363(22.1989) | Bit/dim 3.4252(3.4251) | Xent 0.0000(0.0000) | Loss 8.6290(9.0936) | Error 0.0000(0.0000) Steps 898(914.05) | Grad Norm 5.4923(4.8525) | Total Time 0.00(0.00)\n",
      "Iter 13300 | Time 21.1375(22.1333) | Bit/dim 3.4074(3.4273) | Xent 0.0000(0.0000) | Loss 8.6399(8.9944) | Error 0.0000(0.0000) Steps 880(909.27) | Grad Norm 3.6193(4.9066) | Total Time 0.00(0.00)\n",
      "Iter 13310 | Time 21.8328(22.2011) | Bit/dim 3.4407(3.4258) | Xent 0.0000(0.0000) | Loss 8.6513(8.9097) | Error 0.0000(0.0000) Steps 910(911.08) | Grad Norm 7.1856(5.0076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 101.1486, Epoch Time 1335.3745(1276.0271), Bit/dim 3.4283(best: 3.4273), Xent 0.0000, Loss 3.4283, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13320 | Time 22.7793(22.3315) | Bit/dim 3.4387(3.4253) | Xent 0.0000(0.0000) | Loss 8.8196(9.5811) | Error 0.0000(0.0000) Steps 934(915.99) | Grad Norm 5.1243(5.0140) | Total Time 0.00(0.00)\n",
      "Iter 13330 | Time 23.2417(22.3971) | Bit/dim 3.4539(3.4260) | Xent 0.0000(0.0000) | Loss 8.8662(9.3617) | Error 0.0000(0.0000) Steps 904(918.28) | Grad Norm 8.1526(5.1719) | Total Time 0.00(0.00)\n",
      "Iter 13340 | Time 21.9049(22.3730) | Bit/dim 3.4035(3.4228) | Xent 0.0000(0.0000) | Loss 8.7248(9.1833) | Error 0.0000(0.0000) Steps 910(916.60) | Grad Norm 4.5566(5.0678) | Total Time 0.00(0.00)\n",
      "Iter 13350 | Time 21.5974(22.3655) | Bit/dim 3.4291(3.4252) | Xent 0.0000(0.0000) | Loss 8.6025(9.0490) | Error 0.0000(0.0000) Steps 880(913.68) | Grad Norm 5.0464(5.1891) | Total Time 0.00(0.00)\n",
      "Iter 13360 | Time 22.5101(22.3690) | Bit/dim 3.4881(3.4253) | Xent 0.0000(0.0000) | Loss 8.6601(8.9518) | Error 0.0000(0.0000) Steps 904(914.02) | Grad Norm 5.8755(5.3401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 100.5553, Epoch Time 1348.4465(1278.1997), Bit/dim 3.4275(best: 3.4273), Xent 0.0000, Loss 3.4275, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13370 | Time 22.8368(22.3181) | Bit/dim 3.4226(3.4239) | Xent 0.0000(0.0000) | Loss 8.6717(9.7375) | Error 0.0000(0.0000) Steps 916(911.54) | Grad Norm 3.3248(5.1061) | Total Time 0.00(0.00)\n",
      "Iter 13380 | Time 22.6083(22.2833) | Bit/dim 3.3971(3.4223) | Xent 0.0000(0.0000) | Loss 8.6518(9.4637) | Error 0.0000(0.0000) Steps 928(912.92) | Grad Norm 4.6312(5.1998) | Total Time 0.00(0.00)\n",
      "Iter 13390 | Time 22.4587(22.3146) | Bit/dim 3.4132(3.4235) | Xent 0.0000(0.0000) | Loss 8.7460(9.2706) | Error 0.0000(0.0000) Steps 922(914.26) | Grad Norm 2.8511(5.0595) | Total Time 0.00(0.00)\n",
      "Iter 13400 | Time 21.6998(22.3743) | Bit/dim 3.4158(3.4237) | Xent 0.0000(0.0000) | Loss 8.6927(9.1261) | Error 0.0000(0.0000) Steps 916(917.59) | Grad Norm 4.2863(5.0101) | Total Time 0.00(0.00)\n",
      "Iter 13410 | Time 22.3045(22.3778) | Bit/dim 3.4110(3.4231) | Xent 0.0000(0.0000) | Loss 8.6767(9.0206) | Error 0.0000(0.0000) Steps 886(916.05) | Grad Norm 4.3140(5.0527) | Total Time 0.00(0.00)\n",
      "Iter 13420 | Time 22.1724(22.3647) | Bit/dim 3.4546(3.4244) | Xent 0.0000(0.0000) | Loss 8.7784(8.9312) | Error 0.0000(0.0000) Steps 910(913.24) | Grad Norm 4.9200(5.2742) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 101.4053, Epoch Time 1348.8955(1280.3205), Bit/dim 3.4282(best: 3.4273), Xent 0.0000, Loss 3.4282, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13430 | Time 23.2327(22.4345) | Bit/dim 3.4090(3.4204) | Xent 0.0000(0.0000) | Loss 8.7276(9.6187) | Error 0.0000(0.0000) Steps 946(917.66) | Grad Norm 7.1923(5.1948) | Total Time 0.00(0.00)\n",
      "Iter 13440 | Time 22.2454(22.4350) | Bit/dim 3.4362(3.4235) | Xent 0.0000(0.0000) | Loss 8.8036(9.3967) | Error 0.0000(0.0000) Steps 934(922.85) | Grad Norm 3.3116(5.0471) | Total Time 0.00(0.00)\n",
      "Iter 13450 | Time 23.4849(22.4912) | Bit/dim 3.4487(3.4234) | Xent 0.0000(0.0000) | Loss 8.7424(9.2160) | Error 0.0000(0.0000) Steps 916(922.88) | Grad Norm 3.5616(4.5437) | Total Time 0.00(0.00)\n",
      "Iter 13460 | Time 21.4913(22.3665) | Bit/dim 3.4079(3.4242) | Xent 0.0000(0.0000) | Loss 8.5995(9.0710) | Error 0.0000(0.0000) Steps 880(914.98) | Grad Norm 6.1743(4.9350) | Total Time 0.00(0.00)\n",
      "Iter 13470 | Time 21.6299(22.2703) | Bit/dim 3.4311(3.4240) | Xent 0.0000(0.0000) | Loss 8.7215(8.9636) | Error 0.0000(0.0000) Steps 898(907.67) | Grad Norm 5.2674(4.9895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 101.0080, Epoch Time 1345.8455(1282.2863), Bit/dim 3.4290(best: 3.4273), Xent 0.0000, Loss 3.4290, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13480 | Time 22.6938(22.2443) | Bit/dim 3.3920(3.4237) | Xent 0.0000(0.0000) | Loss 8.6191(9.7335) | Error 0.0000(0.0000) Steps 922(904.66) | Grad Norm 7.2161(5.0856) | Total Time 0.00(0.00)\n",
      "Iter 13490 | Time 22.2101(22.2057) | Bit/dim 3.4088(3.4242) | Xent 0.0000(0.0000) | Loss 8.6112(9.4552) | Error 0.0000(0.0000) Steps 922(904.86) | Grad Norm 2.5649(4.8491) | Total Time 0.00(0.00)\n",
      "Iter 13500 | Time 21.6854(22.1394) | Bit/dim 3.4135(3.4227) | Xent 0.0000(0.0000) | Loss 8.6586(9.2390) | Error 0.0000(0.0000) Steps 886(902.44) | Grad Norm 4.9483(4.9532) | Total Time 0.00(0.00)\n",
      "Iter 13510 | Time 22.0601(22.1412) | Bit/dim 3.3835(3.4198) | Xent 0.0000(0.0000) | Loss 8.6293(9.0894) | Error 0.0000(0.0000) Steps 898(903.08) | Grad Norm 5.7558(5.2129) | Total Time 0.00(0.00)\n",
      "Iter 13520 | Time 21.8441(22.1685) | Bit/dim 3.4047(3.4216) | Xent 0.0000(0.0000) | Loss 8.7446(8.9840) | Error 0.0000(0.0000) Steps 904(904.00) | Grad Norm 4.4748(4.8010) | Total Time 0.00(0.00)\n",
      "Iter 13530 | Time 22.2733(22.1999) | Bit/dim 3.4340(3.4224) | Xent 0.0000(0.0000) | Loss 8.6522(8.9028) | Error 0.0000(0.0000) Steps 880(901.88) | Grad Norm 6.2849(4.8804) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 101.6905, Epoch Time 1336.2661(1283.9057), Bit/dim 3.4247(best: 3.4273), Xent 0.0000, Loss 3.4247, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13540 | Time 22.6076(22.1230) | Bit/dim 3.4287(3.4212) | Xent 0.0000(0.0000) | Loss 8.7058(9.5272) | Error 0.0000(0.0000) Steps 928(901.75) | Grad Norm 4.9246(4.8710) | Total Time 0.00(0.00)\n",
      "Iter 13550 | Time 23.1522(22.1477) | Bit/dim 3.4061(3.4204) | Xent 0.0000(0.0000) | Loss 8.6504(9.3042) | Error 0.0000(0.0000) Steps 940(902.51) | Grad Norm 6.3404(5.1238) | Total Time 0.00(0.00)\n",
      "Iter 13560 | Time 22.1822(22.1530) | Bit/dim 3.4189(3.4224) | Xent 0.0000(0.0000) | Loss 8.6911(9.1472) | Error 0.0000(0.0000) Steps 874(903.58) | Grad Norm 4.6280(5.1564) | Total Time 0.00(0.00)\n",
      "Iter 13570 | Time 21.6288(22.2198) | Bit/dim 3.4147(3.4204) | Xent 0.0000(0.0000) | Loss 8.6912(9.0254) | Error 0.0000(0.0000) Steps 904(906.16) | Grad Norm 5.7945(4.9650) | Total Time 0.00(0.00)\n",
      "Iter 13580 | Time 23.0489(22.2625) | Bit/dim 3.4297(3.4236) | Xent 0.0000(0.0000) | Loss 8.7144(8.9424) | Error 0.0000(0.0000) Steps 886(904.62) | Grad Norm 3.1958(4.9304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 102.1088, Epoch Time 1341.3353(1285.6286), Bit/dim 3.4248(best: 3.4247), Xent 0.0000, Loss 3.4248, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13590 | Time 23.3498(22.3312) | Bit/dim 3.4506(3.4216) | Xent 0.0000(0.0000) | Loss 8.6760(9.7022) | Error 0.0000(0.0000) Steps 916(905.33) | Grad Norm 5.0474(4.8409) | Total Time 0.00(0.00)\n",
      "Iter 13600 | Time 23.2681(22.3061) | Bit/dim 3.4141(3.4238) | Xent 0.0000(0.0000) | Loss 8.7372(9.4328) | Error 0.0000(0.0000) Steps 904(904.75) | Grad Norm 4.3178(4.8312) | Total Time 0.00(0.00)\n",
      "Iter 13610 | Time 23.5699(22.3797) | Bit/dim 3.4019(3.4224) | Xent 0.0000(0.0000) | Loss 8.6793(9.2371) | Error 0.0000(0.0000) Steps 892(906.73) | Grad Norm 4.1289(4.8647) | Total Time 0.00(0.00)\n",
      "Iter 13620 | Time 23.2358(22.3498) | Bit/dim 3.4447(3.4232) | Xent 0.0000(0.0000) | Loss 8.8321(9.0955) | Error 0.0000(0.0000) Steps 946(909.04) | Grad Norm 4.5899(4.7901) | Total Time 0.00(0.00)\n",
      "Iter 13630 | Time 21.5401(22.3702) | Bit/dim 3.3951(3.4204) | Xent 0.0000(0.0000) | Loss 8.7232(8.9874) | Error 0.0000(0.0000) Steps 886(909.35) | Grad Norm 4.6947(4.8362) | Total Time 0.00(0.00)\n",
      "Iter 13640 | Time 22.0789(22.2743) | Bit/dim 3.4117(3.4206) | Xent 0.0000(0.0000) | Loss 8.6873(8.8979) | Error 0.0000(0.0000) Steps 922(904.44) | Grad Norm 5.0665(5.2366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 100.5136, Epoch Time 1344.3679(1287.3908), Bit/dim 3.4281(best: 3.4247), Xent 0.0000, Loss 3.4281, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 22.2942(22.2981) | Bit/dim 3.3960(3.4213) | Xent 0.0000(0.0000) | Loss 8.6736(9.5592) | Error 0.0000(0.0000) Steps 892(901.36) | Grad Norm 5.7675(5.1608) | Total Time 0.00(0.00)\n",
      "Iter 13660 | Time 21.9987(22.1584) | Bit/dim 3.4124(3.4182) | Xent 0.0000(0.0000) | Loss 8.6246(9.3173) | Error 0.0000(0.0000) Steps 898(899.48) | Grad Norm 4.6760(5.1431) | Total Time 0.00(0.00)\n",
      "Iter 13670 | Time 22.1727(22.1716) | Bit/dim 3.4184(3.4183) | Xent 0.0000(0.0000) | Loss 8.6974(9.1419) | Error 0.0000(0.0000) Steps 904(898.30) | Grad Norm 5.0813(5.3620) | Total Time 0.00(0.00)\n",
      "Iter 13680 | Time 22.3810(22.2517) | Bit/dim 3.4259(3.4224) | Xent 0.0000(0.0000) | Loss 8.6118(9.0163) | Error 0.0000(0.0000) Steps 934(898.65) | Grad Norm 3.1155(4.9511) | Total Time 0.00(0.00)\n",
      "Iter 13690 | Time 22.3379(22.2291) | Bit/dim 3.4357(3.4199) | Xent 0.0000(0.0000) | Loss 8.6250(8.9070) | Error 0.0000(0.0000) Steps 886(896.56) | Grad Norm 4.5591(4.8530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 99.6076, Epoch Time 1334.6650(1288.8090), Bit/dim 3.4219(best: 3.4247), Xent 0.0000, Loss 3.4219, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 22.2845(22.1234) | Bit/dim 3.4520(3.4207) | Xent 0.0000(0.0000) | Loss 8.7237(9.6674) | Error 0.0000(0.0000) Steps 916(893.79) | Grad Norm 4.4724(4.8493) | Total Time 0.00(0.00)\n",
      "Iter 13710 | Time 21.9782(22.1353) | Bit/dim 3.4355(3.4219) | Xent 0.0000(0.0000) | Loss 8.7095(9.4027) | Error 0.0000(0.0000) Steps 910(892.73) | Grad Norm 2.9589(4.8088) | Total Time 0.00(0.00)\n",
      "Iter 13720 | Time 22.3031(22.1913) | Bit/dim 3.4113(3.4205) | Xent 0.0000(0.0000) | Loss 8.5931(9.2146) | Error 0.0000(0.0000) Steps 898(897.28) | Grad Norm 3.8477(4.8749) | Total Time 0.00(0.00)\n",
      "Iter 13730 | Time 23.0892(22.2630) | Bit/dim 3.4021(3.4190) | Xent 0.0000(0.0000) | Loss 8.6340(9.0700) | Error 0.0000(0.0000) Steps 916(897.70) | Grad Norm 6.2847(4.8669) | Total Time 0.00(0.00)\n",
      "Iter 13740 | Time 22.9585(22.2710) | Bit/dim 3.4439(3.4212) | Xent 0.0000(0.0000) | Loss 8.7918(8.9711) | Error 0.0000(0.0000) Steps 898(898.38) | Grad Norm 4.7331(5.0025) | Total Time 0.00(0.00)\n",
      "Iter 13750 | Time 22.8612(22.3419) | Bit/dim 3.4350(3.4208) | Xent 0.0000(0.0000) | Loss 8.6601(8.8945) | Error 0.0000(0.0000) Steps 916(901.04) | Grad Norm 5.1465(5.1334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 99.5719, Epoch Time 1342.6455(1290.4241), Bit/dim 3.4277(best: 3.4219), Xent 0.0000, Loss 3.4277, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 22.2750(22.3560) | Bit/dim 3.4443(3.4212) | Xent 0.0000(0.0000) | Loss 8.7017(9.5688) | Error 0.0000(0.0000) Steps 892(900.81) | Grad Norm 7.4996(5.2079) | Total Time 0.00(0.00)\n",
      "Iter 13770 | Time 23.0296(22.3982) | Bit/dim 3.4274(3.4217) | Xent 0.0000(0.0000) | Loss 8.8223(9.3422) | Error 0.0000(0.0000) Steps 904(902.07) | Grad Norm 3.5385(5.0408) | Total Time 0.00(0.00)\n",
      "Iter 13780 | Time 22.3568(22.3413) | Bit/dim 3.4077(3.4212) | Xent 0.0000(0.0000) | Loss 8.6595(9.1647) | Error 0.0000(0.0000) Steps 892(901.44) | Grad Norm 5.5860(4.9358) | Total Time 0.00(0.00)\n",
      "Iter 13790 | Time 23.0412(22.3695) | Bit/dim 3.4495(3.4212) | Xent 0.0000(0.0000) | Loss 8.7329(9.0421) | Error 0.0000(0.0000) Steps 916(903.77) | Grad Norm 5.2931(5.1002) | Total Time 0.00(0.00)\n",
      "Iter 13800 | Time 22.1050(22.4540) | Bit/dim 3.4467(3.4181) | Xent 0.0000(0.0000) | Loss 8.7295(8.9445) | Error 0.0000(0.0000) Steps 910(904.80) | Grad Norm 6.6911(5.3130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 98.9649, Epoch Time 1351.1506(1292.2459), Bit/dim 3.4303(best: 3.4219), Xent 0.0000, Loss 3.4303, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 22.9326(22.4775) | Bit/dim 3.3974(3.4171) | Xent 0.0000(0.0000) | Loss 8.6682(9.7134) | Error 0.0000(0.0000) Steps 934(908.46) | Grad Norm 4.1321(5.1228) | Total Time 0.00(0.00)\n",
      "Iter 13820 | Time 21.9885(22.4060) | Bit/dim 3.4222(3.4189) | Xent 0.0000(0.0000) | Loss 8.6468(9.4480) | Error 0.0000(0.0000) Steps 904(908.29) | Grad Norm 3.4557(5.1446) | Total Time 0.00(0.00)\n",
      "Iter 13830 | Time 22.3389(22.3331) | Bit/dim 3.4184(3.4186) | Xent 0.0000(0.0000) | Loss 8.6651(9.2442) | Error 0.0000(0.0000) Steps 904(907.18) | Grad Norm 7.8580(4.8722) | Total Time 0.00(0.00)\n",
      "Iter 13840 | Time 22.3974(22.3651) | Bit/dim 3.4278(3.4183) | Xent 0.0000(0.0000) | Loss 8.6427(9.1006) | Error 0.0000(0.0000) Steps 892(906.73) | Grad Norm 4.7902(5.0114) | Total Time 0.00(0.00)\n",
      "Iter 13850 | Time 23.2149(22.4331) | Bit/dim 3.4152(3.4224) | Xent 0.0000(0.0000) | Loss 8.7031(8.9900) | Error 0.0000(0.0000) Steps 880(904.60) | Grad Norm 4.1396(5.0041) | Total Time 0.00(0.00)\n",
      "Iter 13860 | Time 21.6487(22.4032) | Bit/dim 3.4312(3.4219) | Xent 0.0000(0.0000) | Loss 8.5506(8.8933) | Error 0.0000(0.0000) Steps 856(902.58) | Grad Norm 6.1215(5.0876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 100.6027, Epoch Time 1345.8543(1293.8541), Bit/dim 3.4224(best: 3.4219), Xent 0.0000, Loss 3.4224, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 22.2104(22.3524) | Bit/dim 3.4533(3.4209) | Xent 0.0000(0.0000) | Loss 8.7644(9.5515) | Error 0.0000(0.0000) Steps 916(905.11) | Grad Norm 3.1558(5.0701) | Total Time 0.00(0.00)\n",
      "Iter 13880 | Time 22.7227(22.3920) | Bit/dim 3.4221(3.4227) | Xent 0.0000(0.0000) | Loss 8.7616(9.3194) | Error 0.0000(0.0000) Steps 910(906.30) | Grad Norm 5.0930(5.0122) | Total Time 0.00(0.00)\n",
      "Iter 13890 | Time 22.2080(22.4191) | Bit/dim 3.4462(3.4240) | Xent 0.0000(0.0000) | Loss 8.8069(9.1626) | Error 0.0000(0.0000) Steps 904(907.40) | Grad Norm 5.5268(4.9597) | Total Time 0.00(0.00)\n",
      "Iter 13900 | Time 22.4873(22.4128) | Bit/dim 3.3865(3.4193) | Xent 0.0000(0.0000) | Loss 8.5024(9.0363) | Error 0.0000(0.0000) Steps 916(908.27) | Grad Norm 6.1260(5.0466) | Total Time 0.00(0.00)\n",
      "Iter 13910 | Time 22.6578(22.4647) | Bit/dim 3.3865(3.4190) | Xent 0.0000(0.0000) | Loss 8.6751(8.9543) | Error 0.0000(0.0000) Steps 928(911.41) | Grad Norm 3.5257(5.0102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 100.2619, Epoch Time 1351.9445(1295.5968), Bit/dim 3.4218(best: 3.4219), Xent 0.0000, Loss 3.4218, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 23.9229(22.5037) | Bit/dim 3.3662(3.4184) | Xent 0.0000(0.0000) | Loss 8.5084(9.7409) | Error 0.0000(0.0000) Steps 892(911.00) | Grad Norm 5.9300(4.8197) | Total Time 0.00(0.00)\n",
      "Iter 13930 | Time 22.6523(22.3989) | Bit/dim 3.4149(3.4203) | Xent 0.0000(0.0000) | Loss 8.7450(9.4710) | Error 0.0000(0.0000) Steps 922(908.64) | Grad Norm 4.2604(4.7676) | Total Time 0.00(0.00)\n",
      "Iter 13940 | Time 21.3440(22.3406) | Bit/dim 3.4488(3.4168) | Xent 0.0000(0.0000) | Loss 8.7943(9.2542) | Error 0.0000(0.0000) Steps 880(906.78) | Grad Norm 5.4212(4.7369) | Total Time 0.00(0.00)\n",
      "Iter 13950 | Time 22.4825(22.3371) | Bit/dim 3.4156(3.4156) | Xent 0.0000(0.0000) | Loss 8.7331(9.1014) | Error 0.0000(0.0000) Steps 904(904.96) | Grad Norm 5.1504(4.8586) | Total Time 0.00(0.00)\n",
      "Iter 13960 | Time 22.2903(22.3092) | Bit/dim 3.4237(3.4150) | Xent 0.0000(0.0000) | Loss 8.6431(8.9888) | Error 0.0000(0.0000) Steps 892(903.55) | Grad Norm 7.4522(5.0104) | Total Time 0.00(0.00)\n",
      "Iter 13970 | Time 21.6263(22.2766) | Bit/dim 3.4180(3.4181) | Xent 0.0000(0.0000) | Loss 8.6039(8.9139) | Error 0.0000(0.0000) Steps 856(900.85) | Grad Norm 4.0892(4.9452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 99.7566, Epoch Time 1339.9807(1296.9284), Bit/dim 3.4228(best: 3.4218), Xent 0.0000, Loss 3.4228, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 21.7432(22.2593) | Bit/dim 3.4295(3.4176) | Xent 0.0000(0.0000) | Loss 8.7195(9.5674) | Error 0.0000(0.0000) Steps 886(897.99) | Grad Norm 6.3059(4.9378) | Total Time 0.00(0.00)\n",
      "Iter 13990 | Time 22.0318(22.2400) | Bit/dim 3.4185(3.4174) | Xent 0.0000(0.0000) | Loss 8.6815(9.3359) | Error 0.0000(0.0000) Steps 916(899.36) | Grad Norm 5.2863(4.9364) | Total Time 0.00(0.00)\n",
      "Iter 14000 | Time 22.4239(22.2040) | Bit/dim 3.4095(3.4186) | Xent 0.0000(0.0000) | Loss 8.6330(9.1764) | Error 0.0000(0.0000) Steps 916(899.95) | Grad Norm 6.0694(5.0538) | Total Time 0.00(0.00)\n",
      "Iter 14010 | Time 22.5921(22.2372) | Bit/dim 3.3898(3.4192) | Xent 0.0000(0.0000) | Loss 8.6460(9.0486) | Error 0.0000(0.0000) Steps 910(899.39) | Grad Norm 5.3337(4.9592) | Total Time 0.00(0.00)\n",
      "Iter 14020 | Time 22.2090(22.2135) | Bit/dim 3.4490(3.4167) | Xent 0.0000(0.0000) | Loss 8.8381(8.9483) | Error 0.0000(0.0000) Steps 910(899.26) | Grad Norm 2.7533(4.8592) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 100.8731, Epoch Time 1338.4768(1298.1748), Bit/dim 3.4246(best: 3.4218), Xent 0.0000, Loss 3.4246, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 22.6370(22.2454) | Bit/dim 3.3834(3.4151) | Xent 0.0000(0.0000) | Loss 8.6084(9.7313) | Error 0.0000(0.0000) Steps 910(900.34) | Grad Norm 6.5988(5.2488) | Total Time 0.00(0.00)\n",
      "Iter 14040 | Time 22.9869(22.2169) | Bit/dim 3.4345(3.4164) | Xent 0.0000(0.0000) | Loss 8.7464(9.4543) | Error 0.0000(0.0000) Steps 862(897.10) | Grad Norm 6.9622(5.3644) | Total Time 0.00(0.00)\n",
      "Iter 14050 | Time 22.5965(22.2686) | Bit/dim 3.4232(3.4153) | Xent 0.0000(0.0000) | Loss 8.7081(9.2512) | Error 0.0000(0.0000) Steps 904(898.43) | Grad Norm 2.6940(5.0111) | Total Time 0.00(0.00)\n",
      "Iter 14060 | Time 23.0026(22.3522) | Bit/dim 3.4252(3.4152) | Xent 0.0000(0.0000) | Loss 8.7807(9.1001) | Error 0.0000(0.0000) Steps 910(901.61) | Grad Norm 5.7439(4.9343) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_cifar10_bs900_rl_stdscale_6_run3 --resume ../experiments_published/cnf_cifar10_bs900_rl_stdscale_6_run3/epoch_250_checkpt.pth --seed 3 --lr 0.0001 --conditional False --controlled_tol False --log_freq 10 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
