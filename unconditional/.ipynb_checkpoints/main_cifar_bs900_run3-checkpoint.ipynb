{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, conditional=False, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_cifar10_published_baseline_bs900_1/current_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_cifar10_published_baseline_bs900_1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 4740 | Time 19.5121(30.7058) | Bit/dim 3.4950(3.5101) | Xent 0.0000(0.0000) | Loss 3.4950(3.5101) | Error 0.0000(0.0000) Steps 796(786.49) | Grad Norm 2.7338(2.6046) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 19.8043(27.7078) | Bit/dim 3.4912(3.5102) | Xent 0.0000(0.0000) | Loss 3.4912(3.5102) | Error 0.0000(0.0000) Steps 778(786.56) | Grad Norm 2.5371(2.6655) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 18.3049(25.4876) | Bit/dim 3.5146(3.5094) | Xent 0.0000(0.0000) | Loss 3.5146(3.5094) | Error 0.0000(0.0000) Steps 778(785.08) | Grad Norm 2.0360(2.6486) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 19.2436(23.8063) | Bit/dim 3.5326(3.5094) | Xent 0.0000(0.0000) | Loss 3.5326(3.5094) | Error 0.0000(0.0000) Steps 778(784.28) | Grad Norm 2.6752(2.5233) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 19.2962(22.6378) | Bit/dim 3.5696(3.5126) | Xent 0.0000(0.0000) | Loss 3.5696(3.5126) | Error 0.0000(0.0000) Steps 790(787.58) | Grad Norm 1.2104(2.6209) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 106.8323, Epoch Time 1205.9759(1905.1833), Bit/dim 3.5147(best: inf), Xent 0.0000, Loss 3.5147, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 18.9045(21.7746) | Bit/dim 3.5055(3.5131) | Xent 0.0000(0.0000) | Loss 3.5055(3.5131) | Error 0.0000(0.0000) Steps 778(786.71) | Grad Norm 1.7496(2.5782) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 19.2657(21.0643) | Bit/dim 3.5283(3.5116) | Xent 0.0000(0.0000) | Loss 3.5283(3.5116) | Error 0.0000(0.0000) Steps 808(788.01) | Grad Norm 2.4768(2.5955) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 19.6485(20.6084) | Bit/dim 3.5118(3.5133) | Xent 0.0000(0.0000) | Loss 3.5118(3.5133) | Error 0.0000(0.0000) Steps 796(786.48) | Grad Norm 1.8519(2.4265) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 19.4236(20.2349) | Bit/dim 3.4899(3.5107) | Xent 0.0000(0.0000) | Loss 3.4899(3.5107) | Error 0.0000(0.0000) Steps 796(785.41) | Grad Norm 3.0560(2.3280) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 19.0191(19.9680) | Bit/dim 3.4976(3.5087) | Xent 0.0000(0.0000) | Loss 3.4976(3.5087) | Error 0.0000(0.0000) Steps 784(786.52) | Grad Norm 2.6784(2.6316) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 19.2943(19.7718) | Bit/dim 3.5139(3.5111) | Xent 0.0000(0.0000) | Loss 3.5139(3.5111) | Error 0.0000(0.0000) Steps 772(788.71) | Grad Norm 2.8371(2.6217) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 100.4314, Epoch Time 1175.6915(1883.2986), Bit/dim 3.5101(best: 3.5147), Xent 0.0000, Loss 3.5101, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 19.9851(19.6693) | Bit/dim 3.4788(3.5113) | Xent 0.0000(0.0000) | Loss 3.4788(3.5113) | Error 0.0000(0.0000) Steps 790(789.06) | Grad Norm 1.8560(2.4148) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 19.5548(19.5800) | Bit/dim 3.5254(3.5097) | Xent 0.0000(0.0000) | Loss 3.5254(3.5097) | Error 0.0000(0.0000) Steps 790(787.39) | Grad Norm 4.2378(2.5074) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 19.3184(19.5304) | Bit/dim 3.5035(3.5084) | Xent 0.0000(0.0000) | Loss 3.5035(3.5084) | Error 0.0000(0.0000) Steps 808(790.39) | Grad Norm 3.5581(2.5464) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 19.3663(19.4318) | Bit/dim 3.4659(3.5072) | Xent 0.0000(0.0000) | Loss 3.4659(3.5072) | Error 0.0000(0.0000) Steps 802(790.94) | Grad Norm 2.3888(2.6517) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 19.5663(19.3848) | Bit/dim 3.5205(3.5100) | Xent 0.0000(0.0000) | Loss 3.5205(3.5100) | Error 0.0000(0.0000) Steps 772(788.25) | Grad Norm 2.4637(2.6232) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 100.4802, Epoch Time 1180.5819(1862.2171), Bit/dim 3.5074(best: 3.5101), Xent 0.0000, Loss 3.5074, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 19.5100(19.3647) | Bit/dim 3.4908(3.5079) | Xent 0.0000(0.0000) | Loss 3.4908(3.5079) | Error 0.0000(0.0000) Steps 778(786.51) | Grad Norm 2.5836(2.5556) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 19.0263(19.3083) | Bit/dim 3.4859(3.5073) | Xent 0.0000(0.0000) | Loss 3.4859(3.5073) | Error 0.0000(0.0000) Steps 790(786.90) | Grad Norm 3.2638(2.6000) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 19.5485(19.3350) | Bit/dim 3.5401(3.5062) | Xent 0.0000(0.0000) | Loss 3.5401(3.5062) | Error 0.0000(0.0000) Steps 796(787.89) | Grad Norm 2.4389(2.6291) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 19.0997(19.2873) | Bit/dim 3.5284(3.5056) | Xent 0.0000(0.0000) | Loss 3.5284(3.5056) | Error 0.0000(0.0000) Steps 778(787.98) | Grad Norm 2.4265(2.4849) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 19.0601(19.3002) | Bit/dim 3.5407(3.5049) | Xent 0.0000(0.0000) | Loss 3.5407(3.5049) | Error 0.0000(0.0000) Steps 784(788.74) | Grad Norm 3.6881(2.5571) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 19.6246(19.2446) | Bit/dim 3.5093(3.5061) | Xent 0.0000(0.0000) | Loss 3.5093(3.5061) | Error 0.0000(0.0000) Steps 808(793.19) | Grad Norm 3.1292(2.6089) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 102.9668, Epoch Time 1178.3677(1841.7016), Bit/dim 3.5106(best: 3.5074), Xent 0.0000, Loss 3.5106, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 19.5456(19.2496) | Bit/dim 3.5087(3.5068) | Xent 0.0000(0.0000) | Loss 3.5087(3.5068) | Error 0.0000(0.0000) Steps 796(794.85) | Grad Norm 2.3199(2.5270) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 19.2670(19.3217) | Bit/dim 3.4983(3.5045) | Xent 0.0000(0.0000) | Loss 3.4983(3.5045) | Error 0.0000(0.0000) Steps 802(794.21) | Grad Norm 2.3608(2.4991) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 19.5463(19.3039) | Bit/dim 3.5067(3.5019) | Xent 0.0000(0.0000) | Loss 3.5067(3.5019) | Error 0.0000(0.0000) Steps 772(792.49) | Grad Norm 3.3843(2.4669) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 19.6842(19.3490) | Bit/dim 3.5129(3.5025) | Xent 0.0000(0.0000) | Loss 3.5129(3.5025) | Error 0.0000(0.0000) Steps 796(792.65) | Grad Norm 2.2297(2.6104) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 19.6992(19.4552) | Bit/dim 3.5331(3.5047) | Xent 0.0000(0.0000) | Loss 3.5331(3.5047) | Error 0.0000(0.0000) Steps 820(793.30) | Grad Norm 4.9382(2.7603) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 99.9887, Epoch Time 1189.8439(1822.1459), Bit/dim 3.5059(best: 3.5074), Xent 0.0000, Loss 3.5059, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 19.4943(19.4781) | Bit/dim 3.4621(3.5038) | Xent 0.0000(0.0000) | Loss 3.4621(3.5038) | Error 0.0000(0.0000) Steps 814(795.47) | Grad Norm 1.8356(2.7531) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 19.4685(19.5056) | Bit/dim 3.5213(3.5013) | Xent 0.0000(0.0000) | Loss 3.5213(3.5013) | Error 0.0000(0.0000) Steps 814(795.91) | Grad Norm 2.4036(2.5478) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 18.7659(19.4978) | Bit/dim 3.5410(3.5054) | Xent 0.0000(0.0000) | Loss 3.5410(3.5054) | Error 0.0000(0.0000) Steps 808(797.60) | Grad Norm 3.9983(2.5929) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 19.5464(19.5060) | Bit/dim 3.4967(3.5043) | Xent 0.0000(0.0000) | Loss 3.4967(3.5043) | Error 0.0000(0.0000) Steps 790(797.57) | Grad Norm 1.7929(2.5671) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 19.3417(19.4806) | Bit/dim 3.5055(3.5015) | Xent 0.0000(0.0000) | Loss 3.5055(3.5015) | Error 0.0000(0.0000) Steps 802(795.77) | Grad Norm 2.7452(2.5655) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 19.4251(19.4251) | Bit/dim 3.4979(3.5020) | Xent 0.0000(0.0000) | Loss 3.4979(3.5020) | Error 0.0000(0.0000) Steps 802(796.65) | Grad Norm 3.7251(2.5865) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 100.4253, Epoch Time 1188.1303(1803.1254), Bit/dim 3.5086(best: 3.5059), Xent 0.0000, Loss 3.5086, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 19.1279(19.4336) | Bit/dim 3.5072(3.5032) | Xent 0.0000(0.0000) | Loss 3.5072(3.5032) | Error 0.0000(0.0000) Steps 790(797.17) | Grad Norm 2.5272(2.6233) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 19.3599(19.3682) | Bit/dim 3.5332(3.5045) | Xent 0.0000(0.0000) | Loss 3.5332(3.5045) | Error 0.0000(0.0000) Steps 796(795.93) | Grad Norm 3.4032(2.5534) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 19.6560(19.3890) | Bit/dim 3.4892(3.5030) | Xent 0.0000(0.0000) | Loss 3.4892(3.5030) | Error 0.0000(0.0000) Steps 802(797.51) | Grad Norm 1.8119(2.4911) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 19.8851(19.4268) | Bit/dim 3.5074(3.5005) | Xent 0.0000(0.0000) | Loss 3.5074(3.5005) | Error 0.0000(0.0000) Steps 808(798.44) | Grad Norm 3.4983(2.5358) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 19.3411(19.4773) | Bit/dim 3.5095(3.5013) | Xent 0.0000(0.0000) | Loss 3.5095(3.5013) | Error 0.0000(0.0000) Steps 808(801.47) | Grad Norm 2.0340(2.4961) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 101.0732, Epoch Time 1189.6829(1784.7221), Bit/dim 3.5023(best: 3.5059), Xent 0.0000, Loss 3.5023, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 19.1792(19.4816) | Bit/dim 3.4974(3.4980) | Xent 0.0000(0.0000) | Loss 3.4974(3.4980) | Error 0.0000(0.0000) Steps 820(802.84) | Grad Norm 3.0693(2.5077) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 19.2251(19.5092) | Bit/dim 3.4797(3.4995) | Xent 0.0000(0.0000) | Loss 3.4797(3.4995) | Error 0.0000(0.0000) Steps 808(803.24) | Grad Norm 1.7763(2.4259) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 19.6138(19.5855) | Bit/dim 3.4848(3.4963) | Xent 0.0000(0.0000) | Loss 3.4848(3.4963) | Error 0.0000(0.0000) Steps 802(803.20) | Grad Norm 2.9723(2.3634) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 19.7556(19.6376) | Bit/dim 3.4884(3.4962) | Xent 0.0000(0.0000) | Loss 3.4884(3.4962) | Error 0.0000(0.0000) Steps 808(805.89) | Grad Norm 3.1088(2.6155) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 19.7256(19.6501) | Bit/dim 3.4880(3.4986) | Xent 0.0000(0.0000) | Loss 3.4880(3.4986) | Error 0.0000(0.0000) Steps 814(805.55) | Grad Norm 1.6420(2.4944) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 19.7586(19.6461) | Bit/dim 3.5045(3.4992) | Xent 0.0000(0.0000) | Loss 3.5045(3.4992) | Error 0.0000(0.0000) Steps 808(806.19) | Grad Norm 1.3004(2.3553) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 99.6958, Epoch Time 1199.7387(1767.1726), Bit/dim 3.4987(best: 3.5023), Xent 0.0000, Loss 3.4987, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 19.0480(19.6512) | Bit/dim 3.4912(3.4991) | Xent 0.0000(0.0000) | Loss 3.4912(3.4991) | Error 0.0000(0.0000) Steps 802(804.20) | Grad Norm 2.1694(2.5002) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 19.4624(19.5908) | Bit/dim 3.4713(3.4981) | Xent 0.0000(0.0000) | Loss 3.4713(3.4981) | Error 0.0000(0.0000) Steps 790(802.01) | Grad Norm 1.7113(2.3357) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 19.9748(19.5301) | Bit/dim 3.4871(3.4949) | Xent 0.0000(0.0000) | Loss 3.4871(3.4949) | Error 0.0000(0.0000) Steps 814(800.82) | Grad Norm 2.0294(2.5048) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 20.1166(19.5069) | Bit/dim 3.4875(3.4970) | Xent 0.0000(0.0000) | Loss 3.4875(3.4970) | Error 0.0000(0.0000) Steps 802(801.23) | Grad Norm 2.5704(2.5064) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 19.8116(19.4860) | Bit/dim 3.5128(3.4981) | Xent 0.0000(0.0000) | Loss 3.5128(3.4981) | Error 0.0000(0.0000) Steps 808(802.38) | Grad Norm 1.7912(2.5102) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 102.9588, Epoch Time 1191.8324(1749.9124), Bit/dim 3.4978(best: 3.4987), Xent 0.0000, Loss 3.4978, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 19.5767(19.5044) | Bit/dim 3.4981(3.4983) | Xent 0.0000(0.0000) | Loss 3.4981(3.4983) | Error 0.0000(0.0000) Steps 808(803.88) | Grad Norm 1.7900(2.4598) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 19.4951(19.5255) | Bit/dim 3.5126(3.4972) | Xent 0.0000(0.0000) | Loss 3.5126(3.4972) | Error 0.0000(0.0000) Steps 802(804.25) | Grad Norm 2.6791(2.3407) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 19.7166(19.5368) | Bit/dim 3.5166(3.4955) | Xent 0.0000(0.0000) | Loss 3.5166(3.4955) | Error 0.0000(0.0000) Steps 796(805.17) | Grad Norm 2.2325(2.4669) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 19.5659(19.5348) | Bit/dim 3.5023(3.4948) | Xent 0.0000(0.0000) | Loss 3.5023(3.4948) | Error 0.0000(0.0000) Steps 808(805.59) | Grad Norm 1.5412(2.3795) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 19.4174(19.5687) | Bit/dim 3.5087(3.4968) | Xent 0.0000(0.0000) | Loss 3.5087(3.4968) | Error 0.0000(0.0000) Steps 790(806.23) | Grad Norm 2.4691(2.4942) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 19.7762(19.6293) | Bit/dim 3.5212(3.4967) | Xent 0.0000(0.0000) | Loss 3.5212(3.4967) | Error 0.0000(0.0000) Steps 826(808.35) | Grad Norm 2.2390(2.5191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 101.0729, Epoch Time 1198.2811(1733.3635), Bit/dim 3.4949(best: 3.4978), Xent 0.0000, Loss 3.4949, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 19.8066(19.6462) | Bit/dim 3.4924(3.4959) | Xent 0.0000(0.0000) | Loss 3.4924(3.4959) | Error 0.0000(0.0000) Steps 790(807.14) | Grad Norm 3.5181(2.5100) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 19.4102(19.6832) | Bit/dim 3.4613(3.4942) | Xent 0.0000(0.0000) | Loss 3.4613(3.4942) | Error 0.0000(0.0000) Steps 820(809.03) | Grad Norm 2.6155(2.4678) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 19.5509(19.6818) | Bit/dim 3.5136(3.4971) | Xent 0.0000(0.0000) | Loss 3.5136(3.4971) | Error 0.0000(0.0000) Steps 814(807.12) | Grad Norm 2.2544(2.4850) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 19.6030(19.6478) | Bit/dim 3.4770(3.4938) | Xent 0.0000(0.0000) | Loss 3.4770(3.4938) | Error 0.0000(0.0000) Steps 808(806.60) | Grad Norm 2.7559(2.5585) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 19.4287(19.6591) | Bit/dim 3.5152(3.4943) | Xent 0.0000(0.0000) | Loss 3.5152(3.4943) | Error 0.0000(0.0000) Steps 796(807.00) | Grad Norm 2.4955(2.4811) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 101.3063, Epoch Time 1201.1159(1717.3961), Bit/dim 3.4970(best: 3.4949), Xent 0.0000, Loss 3.4970, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 19.5261(19.6027) | Bit/dim 3.4870(3.4940) | Xent 0.0000(0.0000) | Loss 3.4870(3.4940) | Error 0.0000(0.0000) Steps 826(808.65) | Grad Norm 2.0639(2.5124) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 19.7105(19.5683) | Bit/dim 3.5068(3.4927) | Xent 0.0000(0.0000) | Loss 3.5068(3.4927) | Error 0.0000(0.0000) Steps 796(808.85) | Grad Norm 1.8467(2.5489) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 19.6608(19.5391) | Bit/dim 3.4665(3.4918) | Xent 0.0000(0.0000) | Loss 3.4665(3.4918) | Error 0.0000(0.0000) Steps 796(808.80) | Grad Norm 3.1546(2.6266) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 19.5929(19.5091) | Bit/dim 3.4822(3.4922) | Xent 0.0000(0.0000) | Loss 3.4822(3.4922) | Error 0.0000(0.0000) Steps 814(809.62) | Grad Norm 2.1755(2.5760) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 19.9177(19.4663) | Bit/dim 3.4985(3.4923) | Xent 0.0000(0.0000) | Loss 3.4985(3.4923) | Error 0.0000(0.0000) Steps 814(810.17) | Grad Norm 2.0929(2.4087) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 19.6885(19.4698) | Bit/dim 3.4943(3.4937) | Xent 0.0000(0.0000) | Loss 3.4943(3.4937) | Error 0.0000(0.0000) Steps 820(811.87) | Grad Norm 1.9365(2.5333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 102.6037, Epoch Time 1189.2886(1701.5528), Bit/dim 3.4929(best: 3.4949), Xent 0.0000, Loss 3.4929, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 19.6175(19.4829) | Bit/dim 3.5013(3.4945) | Xent 0.0000(0.0000) | Loss 3.5013(3.4945) | Error 0.0000(0.0000) Steps 796(809.18) | Grad Norm 2.4577(2.4742) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 19.3858(19.4876) | Bit/dim 3.5060(3.4907) | Xent 0.0000(0.0000) | Loss 3.5060(3.4907) | Error 0.0000(0.0000) Steps 814(810.06) | Grad Norm 2.1642(2.2922) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 19.6535(19.5273) | Bit/dim 3.4588(3.4907) | Xent 0.0000(0.0000) | Loss 3.4588(3.4907) | Error 0.0000(0.0000) Steps 820(810.31) | Grad Norm 2.1180(2.3817) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 19.5364(19.5211) | Bit/dim 3.5064(3.4898) | Xent 0.0000(0.0000) | Loss 3.5064(3.4898) | Error 0.0000(0.0000) Steps 814(811.20) | Grad Norm 2.5186(2.4568) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 19.4738(19.5186) | Bit/dim 3.5024(3.4920) | Xent 0.0000(0.0000) | Loss 3.5024(3.4920) | Error 0.0000(0.0000) Steps 820(810.19) | Grad Norm 2.1379(2.4232) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 101.6651, Epoch Time 1195.3996(1686.3682), Bit/dim 3.4930(best: 3.4929), Xent 0.0000, Loss 3.4930, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 19.9352(19.5859) | Bit/dim 3.4948(3.4903) | Xent 0.0000(0.0000) | Loss 3.4948(3.4903) | Error 0.0000(0.0000) Steps 826(810.67) | Grad Norm 3.0003(2.4930) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 19.9209(19.6001) | Bit/dim 3.4409(3.4885) | Xent 0.0000(0.0000) | Loss 3.4409(3.4885) | Error 0.0000(0.0000) Steps 814(812.36) | Grad Norm 2.4970(2.4340) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 19.1893(19.5905) | Bit/dim 3.4629(3.4878) | Xent 0.0000(0.0000) | Loss 3.4629(3.4878) | Error 0.0000(0.0000) Steps 820(812.74) | Grad Norm 2.5867(2.4663) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 19.5593(19.5738) | Bit/dim 3.4846(3.4879) | Xent 0.0000(0.0000) | Loss 3.4846(3.4879) | Error 0.0000(0.0000) Steps 802(812.86) | Grad Norm 2.8214(2.4490) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 20.2970(19.5866) | Bit/dim 3.5113(3.4891) | Xent 0.0000(0.0000) | Loss 3.5113(3.4891) | Error 0.0000(0.0000) Steps 796(812.06) | Grad Norm 2.0068(2.5225) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 19.2233(19.5957) | Bit/dim 3.4660(3.4887) | Xent 0.0000(0.0000) | Loss 3.4660(3.4887) | Error 0.0000(0.0000) Steps 820(812.89) | Grad Norm 3.0321(2.5061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 102.9545, Epoch Time 1199.5480(1671.7636), Bit/dim 3.4905(best: 3.4929), Xent 0.0000, Loss 3.4905, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 19.6621(19.6249) | Bit/dim 3.4834(3.4886) | Xent 0.0000(0.0000) | Loss 3.4834(3.4886) | Error 0.0000(0.0000) Steps 796(812.73) | Grad Norm 3.8389(2.6788) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 20.1413(19.6688) | Bit/dim 3.4499(3.4874) | Xent 0.0000(0.0000) | Loss 3.4499(3.4874) | Error 0.0000(0.0000) Steps 808(813.44) | Grad Norm 2.1074(2.4864) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 19.3143(19.6280) | Bit/dim 3.5276(3.4880) | Xent 0.0000(0.0000) | Loss 3.5276(3.4880) | Error 0.0000(0.0000) Steps 802(813.29) | Grad Norm 2.5680(2.5917) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 19.2440(19.6120) | Bit/dim 3.4789(3.4885) | Xent 0.0000(0.0000) | Loss 3.4789(3.4885) | Error 0.0000(0.0000) Steps 802(812.45) | Grad Norm 2.3000(2.5481) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 19.5138(19.6312) | Bit/dim 3.5023(3.4894) | Xent 0.0000(0.0000) | Loss 3.5023(3.4894) | Error 0.0000(0.0000) Steps 790(812.87) | Grad Norm 1.9872(2.6033) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 102.6821, Epoch Time 1200.7696(1657.6338), Bit/dim 3.4864(best: 3.4905), Xent 0.0000, Loss 3.4864, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 19.9505(19.6449) | Bit/dim 3.4774(3.4870) | Xent 0.0000(0.0000) | Loss 3.4774(3.4870) | Error 0.0000(0.0000) Steps 838(815.00) | Grad Norm 2.2552(2.4378) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 19.4855(19.6260) | Bit/dim 3.4913(3.4857) | Xent 0.0000(0.0000) | Loss 3.4913(3.4857) | Error 0.0000(0.0000) Steps 808(815.09) | Grad Norm 2.7525(2.5964) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 19.0490(19.5960) | Bit/dim 3.4799(3.4857) | Xent 0.0000(0.0000) | Loss 3.4799(3.4857) | Error 0.0000(0.0000) Steps 814(814.53) | Grad Norm 1.5704(2.4865) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 19.5095(19.5776) | Bit/dim 3.4983(3.4862) | Xent 0.0000(0.0000) | Loss 3.4983(3.4862) | Error 0.0000(0.0000) Steps 814(814.37) | Grad Norm 1.7662(2.4952) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 19.9015(19.5869) | Bit/dim 3.4984(3.4865) | Xent 0.0000(0.0000) | Loss 3.4984(3.4865) | Error 0.0000(0.0000) Steps 814(815.55) | Grad Norm 2.2837(2.4117) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 20.1942(19.6481) | Bit/dim 3.4741(3.4866) | Xent 0.0000(0.0000) | Loss 3.4741(3.4866) | Error 0.0000(0.0000) Steps 820(816.15) | Grad Norm 2.6840(2.3482) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 102.1564, Epoch Time 1200.4343(1643.9178), Bit/dim 3.4885(best: 3.4864), Xent 0.0000, Loss 3.4885, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 19.7902(19.6626) | Bit/dim 3.4958(3.4845) | Xent 0.0000(0.0000) | Loss 3.4958(3.4845) | Error 0.0000(0.0000) Steps 832(816.93) | Grad Norm 2.4415(2.2329) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 19.6052(19.6482) | Bit/dim 3.5207(3.4848) | Xent 0.0000(0.0000) | Loss 3.5207(3.4848) | Error 0.0000(0.0000) Steps 808(815.56) | Grad Norm 2.7833(2.4577) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 19.6910(19.6407) | Bit/dim 3.5051(3.4874) | Xent 0.0000(0.0000) | Loss 3.5051(3.4874) | Error 0.0000(0.0000) Steps 808(815.60) | Grad Norm 3.2434(2.5247) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 19.4525(19.6643) | Bit/dim 3.4782(3.4868) | Xent 0.0000(0.0000) | Loss 3.4782(3.4868) | Error 0.0000(0.0000) Steps 808(816.03) | Grad Norm 1.6767(2.3567) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 19.1618(19.6233) | Bit/dim 3.4394(3.4844) | Xent 0.0000(0.0000) | Loss 3.4394(3.4844) | Error 0.0000(0.0000) Steps 814(815.18) | Grad Norm 1.6803(2.4987) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 100.9840, Epoch Time 1198.7224(1630.5620), Bit/dim 3.4857(best: 3.4864), Xent 0.0000, Loss 3.4857, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 19.3886(19.6070) | Bit/dim 3.4380(3.4810) | Xent 0.0000(0.0000) | Loss 3.4380(3.4810) | Error 0.0000(0.0000) Steps 808(813.65) | Grad Norm 1.4131(2.4340) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 18.9657(19.5903) | Bit/dim 3.5046(3.4841) | Xent 0.0000(0.0000) | Loss 3.5046(3.4841) | Error 0.0000(0.0000) Steps 820(813.54) | Grad Norm 3.7327(2.4192) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 19.3842(19.5762) | Bit/dim 3.4763(3.4837) | Xent 0.0000(0.0000) | Loss 3.4763(3.4837) | Error 0.0000(0.0000) Steps 820(813.57) | Grad Norm 2.2000(2.4370) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 19.4781(19.5546) | Bit/dim 3.4693(3.4824) | Xent 0.0000(0.0000) | Loss 3.4693(3.4824) | Error 0.0000(0.0000) Steps 820(814.88) | Grad Norm 2.4393(2.4613) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 19.8935(19.6163) | Bit/dim 3.4805(3.4827) | Xent 0.0000(0.0000) | Loss 3.4805(3.4827) | Error 0.0000(0.0000) Steps 802(812.91) | Grad Norm 2.2818(2.3882) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 19.8108(19.6081) | Bit/dim 3.5014(3.4808) | Xent 0.0000(0.0000) | Loss 3.5014(3.4808) | Error 0.0000(0.0000) Steps 820(812.26) | Grad Norm 3.9882(2.5519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 101.6959, Epoch Time 1197.7188(1617.5767), Bit/dim 3.4891(best: 3.4857), Xent 0.0000, Loss 3.4891, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 19.7874(19.5722) | Bit/dim 3.4537(3.4799) | Xent 0.0000(0.0000) | Loss 3.4537(3.4799) | Error 0.0000(0.0000) Steps 820(813.28) | Grad Norm 2.1061(2.4562) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 19.3205(19.5759) | Bit/dim 3.5161(3.4831) | Xent 0.0000(0.0000) | Loss 3.5161(3.4831) | Error 0.0000(0.0000) Steps 814(812.12) | Grad Norm 2.8076(2.4809) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 19.2325(19.6224) | Bit/dim 3.4478(3.4816) | Xent 0.0000(0.0000) | Loss 3.4478(3.4816) | Error 0.0000(0.0000) Steps 820(813.57) | Grad Norm 1.9728(2.4276) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 19.2226(19.5643) | Bit/dim 3.4340(3.4816) | Xent 0.0000(0.0000) | Loss 3.4340(3.4816) | Error 0.0000(0.0000) Steps 820(814.81) | Grad Norm 1.9571(2.4969) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 19.5048(19.5427) | Bit/dim 3.4314(3.4790) | Xent 0.0000(0.0000) | Loss 3.4314(3.4790) | Error 0.0000(0.0000) Steps 814(815.38) | Grad Norm 2.0063(2.3958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 98.3903, Epoch Time 1189.7707(1604.7425), Bit/dim 3.4833(best: 3.4857), Xent 0.0000, Loss 3.4833, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 18.7275(19.4515) | Bit/dim 3.4743(3.4773) | Xent 0.0000(0.0000) | Loss 3.4743(3.4773) | Error 0.0000(0.0000) Steps 808(815.49) | Grad Norm 3.1650(2.5319) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 18.8527(19.4193) | Bit/dim 3.4971(3.4777) | Xent 0.0000(0.0000) | Loss 3.4971(3.4777) | Error 0.0000(0.0000) Steps 814(815.05) | Grad Norm 2.1700(2.4882) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 19.4053(19.4265) | Bit/dim 3.4669(3.4764) | Xent 0.0000(0.0000) | Loss 3.4669(3.4764) | Error 0.0000(0.0000) Steps 832(815.71) | Grad Norm 2.1197(2.4391) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 18.8435(19.4254) | Bit/dim 3.4845(3.4790) | Xent 0.0000(0.0000) | Loss 3.4845(3.4790) | Error 0.0000(0.0000) Steps 802(815.93) | Grad Norm 2.0615(2.3840) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 19.9447(19.4348) | Bit/dim 3.5044(3.4830) | Xent 0.0000(0.0000) | Loss 3.5044(3.4830) | Error 0.0000(0.0000) Steps 808(813.72) | Grad Norm 2.6542(2.4098) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 19.1602(19.3701) | Bit/dim 3.4884(3.4798) | Xent 0.0000(0.0000) | Loss 3.4884(3.4798) | Error 0.0000(0.0000) Steps 814(813.99) | Grad Norm 3.2113(2.4643) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 99.4297, Epoch Time 1180.7200(1592.0218), Bit/dim 3.4802(best: 3.4833), Xent 0.0000, Loss 3.4802, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 19.6907(19.4041) | Bit/dim 3.4657(3.4774) | Xent 0.0000(0.0000) | Loss 3.4657(3.4774) | Error 0.0000(0.0000) Steps 814(813.17) | Grad Norm 2.8996(2.4299) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 19.1408(19.3387) | Bit/dim 3.4743(3.4809) | Xent 0.0000(0.0000) | Loss 3.4743(3.4809) | Error 0.0000(0.0000) Steps 802(811.99) | Grad Norm 2.8493(2.4221) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 18.9546(19.3529) | Bit/dim 3.5018(3.4801) | Xent 0.0000(0.0000) | Loss 3.5018(3.4801) | Error 0.0000(0.0000) Steps 826(812.49) | Grad Norm 2.8384(2.5851) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 19.1323(19.3609) | Bit/dim 3.5121(3.4808) | Xent 0.0000(0.0000) | Loss 3.5121(3.4808) | Error 0.0000(0.0000) Steps 808(812.20) | Grad Norm 1.5140(2.4536) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 19.6529(19.3899) | Bit/dim 3.4914(3.4796) | Xent 0.0000(0.0000) | Loss 3.4914(3.4796) | Error 0.0000(0.0000) Steps 832(812.69) | Grad Norm 3.8032(2.4981) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 98.1448, Epoch Time 1180.8747(1579.6874), Bit/dim 3.4785(best: 3.4802), Xent 0.0000, Loss 3.4785, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 19.2293(19.4127) | Bit/dim 3.4645(3.4786) | Xent 0.0000(0.0000) | Loss 3.4645(3.4786) | Error 0.0000(0.0000) Steps 826(814.04) | Grad Norm 2.6394(2.5030) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 19.0551(19.4237) | Bit/dim 3.4704(3.4783) | Xent 0.0000(0.0000) | Loss 3.4704(3.4783) | Error 0.0000(0.0000) Steps 826(815.62) | Grad Norm 2.5867(2.5003) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 18.5470(19.3623) | Bit/dim 3.5162(3.4777) | Xent 0.0000(0.0000) | Loss 3.5162(3.4777) | Error 0.0000(0.0000) Steps 808(816.53) | Grad Norm 1.8291(2.5377) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 19.4235(19.3929) | Bit/dim 3.4791(3.4752) | Xent 0.0000(0.0000) | Loss 3.4791(3.4752) | Error 0.0000(0.0000) Steps 832(817.56) | Grad Norm 2.6771(2.4813) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 19.2556(19.3879) | Bit/dim 3.4889(3.4768) | Xent 0.0000(0.0000) | Loss 3.4889(3.4768) | Error 0.0000(0.0000) Steps 808(816.54) | Grad Norm 1.9963(2.5092) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 19.2027(19.3545) | Bit/dim 3.4554(3.4771) | Xent 0.0000(0.0000) | Loss 3.4554(3.4771) | Error 0.0000(0.0000) Steps 814(816.10) | Grad Norm 2.4247(2.4701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 99.6783, Epoch Time 1182.6787(1567.7771), Bit/dim 3.4772(best: 3.4785), Xent 0.0000, Loss 3.4772, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 19.1190(19.3111) | Bit/dim 3.4596(3.4728) | Xent 0.0000(0.0000) | Loss 3.4596(3.4728) | Error 0.0000(0.0000) Steps 814(816.46) | Grad Norm 2.6556(2.3930) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 19.2618(19.3216) | Bit/dim 3.5238(3.4769) | Xent 0.0000(0.0000) | Loss 3.5238(3.4769) | Error 0.0000(0.0000) Steps 838(815.53) | Grad Norm 2.1881(2.4020) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 20.0512(19.3465) | Bit/dim 3.4812(3.4783) | Xent 0.0000(0.0000) | Loss 3.4812(3.4783) | Error 0.0000(0.0000) Steps 820(814.66) | Grad Norm 1.9059(2.3096) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 18.7263(19.3224) | Bit/dim 3.4744(3.4773) | Xent 0.0000(0.0000) | Loss 3.4744(3.4773) | Error 0.0000(0.0000) Steps 808(814.64) | Grad Norm 2.9395(2.4670) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 19.5518(19.3249) | Bit/dim 3.4651(3.4768) | Xent 0.0000(0.0000) | Loss 3.4651(3.4768) | Error 0.0000(0.0000) Steps 820(813.38) | Grad Norm 2.4232(2.3775) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 98.0565, Epoch Time 1176.4064(1556.0360), Bit/dim 3.4848(best: 3.4772), Xent 0.0000, Loss 3.4848, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 18.4057(19.2375) | Bit/dim 3.4687(3.4777) | Xent 0.0000(0.0000) | Loss 3.4687(3.4777) | Error 0.0000(0.0000) Steps 808(812.17) | Grad Norm 1.7741(2.5159) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 18.7572(19.2859) | Bit/dim 3.4569(3.4748) | Xent 0.0000(0.0000) | Loss 3.4569(3.4748) | Error 0.0000(0.0000) Steps 814(814.55) | Grad Norm 2.2203(2.4092) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 18.5546(19.2590) | Bit/dim 3.4852(3.4751) | Xent 0.0000(0.0000) | Loss 3.4852(3.4751) | Error 0.0000(0.0000) Steps 802(813.32) | Grad Norm 2.2221(2.2407) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 19.4293(19.2346) | Bit/dim 3.4789(3.4746) | Xent 0.0000(0.0000) | Loss 3.4789(3.4746) | Error 0.0000(0.0000) Steps 796(811.71) | Grad Norm 2.4925(2.4212) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 19.2337(19.2801) | Bit/dim 3.4831(3.4757) | Xent 0.0000(0.0000) | Loss 3.4831(3.4757) | Error 0.0000(0.0000) Steps 802(811.05) | Grad Norm 2.7640(2.5435) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 20.1111(19.3184) | Bit/dim 3.4667(3.4746) | Xent 0.0000(0.0000) | Loss 3.4667(3.4746) | Error 0.0000(0.0000) Steps 808(811.93) | Grad Norm 1.5415(2.3965) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 97.7963, Epoch Time 1175.5985(1544.6229), Bit/dim 3.4750(best: 3.4772), Xent 0.0000, Loss 3.4750, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 19.6773(19.3440) | Bit/dim 3.4625(3.4736) | Xent 0.0000(0.0000) | Loss 3.4625(3.4736) | Error 0.0000(0.0000) Steps 808(811.53) | Grad Norm 2.1430(2.4044) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 19.0612(19.3212) | Bit/dim 3.4951(3.4756) | Xent 0.0000(0.0000) | Loss 3.4951(3.4756) | Error 0.0000(0.0000) Steps 778(811.57) | Grad Norm 3.5379(2.4097) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 19.2692(19.2914) | Bit/dim 3.4630(3.4739) | Xent 0.0000(0.0000) | Loss 3.4630(3.4739) | Error 0.0000(0.0000) Steps 808(812.34) | Grad Norm 1.9013(2.3793) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 19.9692(19.3337) | Bit/dim 3.5029(3.4742) | Xent 0.0000(0.0000) | Loss 3.5029(3.4742) | Error 0.0000(0.0000) Steps 796(811.29) | Grad Norm 1.8363(2.3240) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 19.3871(19.3388) | Bit/dim 3.4833(3.4744) | Xent 0.0000(0.0000) | Loss 3.4833(3.4744) | Error 0.0000(0.0000) Steps 808(811.40) | Grad Norm 1.7378(2.4139) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 98.1692, Epoch Time 1179.8023(1533.6783), Bit/dim 3.4737(best: 3.4750), Xent 0.0000, Loss 3.4737, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 19.8559(19.4046) | Bit/dim 3.4664(3.4728) | Xent 0.0000(0.0000) | Loss 3.4664(3.4728) | Error 0.0000(0.0000) Steps 826(812.12) | Grad Norm 1.3067(2.2703) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 19.4074(19.3358) | Bit/dim 3.4465(3.4694) | Xent 0.0000(0.0000) | Loss 3.4465(3.4694) | Error 0.0000(0.0000) Steps 808(812.03) | Grad Norm 2.4186(2.3778) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 19.1808(19.3550) | Bit/dim 3.4595(3.4708) | Xent 0.0000(0.0000) | Loss 3.4595(3.4708) | Error 0.0000(0.0000) Steps 802(810.59) | Grad Norm 2.5121(2.4201) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 19.3065(19.3571) | Bit/dim 3.4689(3.4703) | Xent 0.0000(0.0000) | Loss 3.4689(3.4703) | Error 0.0000(0.0000) Steps 802(809.79) | Grad Norm 3.4828(2.2940) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 20.1384(19.3544) | Bit/dim 3.4905(3.4723) | Xent 0.0000(0.0000) | Loss 3.4905(3.4723) | Error 0.0000(0.0000) Steps 820(809.12) | Grad Norm 2.6600(2.5179) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 19.9069(19.4380) | Bit/dim 3.4723(3.4723) | Xent 0.0000(0.0000) | Loss 3.4723(3.4723) | Error 0.0000(0.0000) Steps 832(810.61) | Grad Norm 1.5702(2.3355) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 97.3344, Epoch Time 1181.4446(1523.1113), Bit/dim 3.4723(best: 3.4737), Xent 0.0000, Loss 3.4723, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 19.2690(19.4746) | Bit/dim 3.4878(3.4710) | Xent 0.0000(0.0000) | Loss 3.4878(3.4710) | Error 0.0000(0.0000) Steps 814(810.41) | Grad Norm 2.6129(2.5186) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 19.4457(19.4899) | Bit/dim 3.4808(3.4717) | Xent 0.0000(0.0000) | Loss 3.4808(3.4717) | Error 0.0000(0.0000) Steps 778(812.69) | Grad Norm 2.1382(2.4499) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 19.3451(19.4443) | Bit/dim 3.4562(3.4715) | Xent 0.0000(0.0000) | Loss 3.4562(3.4715) | Error 0.0000(0.0000) Steps 832(813.43) | Grad Norm 1.5088(2.5065) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 19.5206(19.4454) | Bit/dim 3.4842(3.4721) | Xent 0.0000(0.0000) | Loss 3.4842(3.4721) | Error 0.0000(0.0000) Steps 826(816.61) | Grad Norm 2.9498(2.4993) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 19.2556(19.4078) | Bit/dim 3.4424(3.4709) | Xent 0.0000(0.0000) | Loss 3.4424(3.4709) | Error 0.0000(0.0000) Steps 826(817.80) | Grad Norm 2.0829(2.3622) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 98.2426, Epoch Time 1184.6998(1512.9589), Bit/dim 3.4713(best: 3.4723), Xent 0.0000, Loss 3.4713, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 19.3790(19.4138) | Bit/dim 3.4924(3.4697) | Xent 0.0000(0.0000) | Loss 3.4924(3.4697) | Error 0.0000(0.0000) Steps 814(817.44) | Grad Norm 2.2418(2.3664) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 18.7082(19.3956) | Bit/dim 3.4655(3.4694) | Xent 0.0000(0.0000) | Loss 3.4655(3.4694) | Error 0.0000(0.0000) Steps 832(817.76) | Grad Norm 2.8920(2.2536) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 19.1224(19.4039) | Bit/dim 3.4422(3.4695) | Xent 0.0000(0.0000) | Loss 3.4422(3.4695) | Error 0.0000(0.0000) Steps 814(817.43) | Grad Norm 2.0728(2.3944) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 19.5748(19.4394) | Bit/dim 3.4594(3.4703) | Xent 0.0000(0.0000) | Loss 3.4594(3.4703) | Error 0.0000(0.0000) Steps 796(816.80) | Grad Norm 1.4796(2.2592) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 19.8143(19.4369) | Bit/dim 3.4508(3.4683) | Xent 0.0000(0.0000) | Loss 3.4508(3.4683) | Error 0.0000(0.0000) Steps 832(816.57) | Grad Norm 3.0897(2.2710) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 19.3089(19.4600) | Bit/dim 3.4678(3.4689) | Xent 0.0000(0.0000) | Loss 3.4678(3.4689) | Error 0.0000(0.0000) Steps 808(817.80) | Grad Norm 4.5359(2.4307) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 98.8403, Epoch Time 1185.7224(1503.1418), Bit/dim 3.4763(best: 3.4713), Xent 0.0000, Loss 3.4763, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 19.4653(19.4719) | Bit/dim 3.4645(3.4693) | Xent 0.0000(0.0000) | Loss 3.4645(3.4693) | Error 0.0000(0.0000) Steps 814(818.02) | Grad Norm 2.6583(2.5050) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 19.4560(19.4720) | Bit/dim 3.4845(3.4681) | Xent 0.0000(0.0000) | Loss 3.4845(3.4681) | Error 0.0000(0.0000) Steps 820(815.85) | Grad Norm 2.1274(2.4836) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 18.6336(19.3988) | Bit/dim 3.4850(3.4693) | Xent 0.0000(0.0000) | Loss 3.4850(3.4693) | Error 0.0000(0.0000) Steps 814(816.37) | Grad Norm 2.4352(2.5089) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 19.2344(19.4285) | Bit/dim 3.4854(3.4693) | Xent 0.0000(0.0000) | Loss 3.4854(3.4693) | Error 0.0000(0.0000) Steps 826(817.55) | Grad Norm 2.7319(2.4598) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 20.2339(19.5062) | Bit/dim 3.5032(3.4697) | Xent 0.0000(0.0000) | Loss 3.5032(3.4697) | Error 0.0000(0.0000) Steps 814(818.62) | Grad Norm 3.2427(2.5876) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 98.6218, Epoch Time 1185.9821(1493.6270), Bit/dim 3.4663(best: 3.4713), Xent 0.0000, Loss 3.4663, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 20.4321(19.5167) | Bit/dim 3.4579(3.4662) | Xent 0.0000(0.0000) | Loss 3.4579(3.4662) | Error 0.0000(0.0000) Steps 826(817.31) | Grad Norm 2.9468(2.5258) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 19.6181(19.5856) | Bit/dim 3.5028(3.4661) | Xent 0.0000(0.0000) | Loss 3.5028(3.4661) | Error 0.0000(0.0000) Steps 820(819.10) | Grad Norm 2.0766(2.4416) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 19.2421(19.5983) | Bit/dim 3.4665(3.4667) | Xent 0.0000(0.0000) | Loss 3.4665(3.4667) | Error 0.0000(0.0000) Steps 814(820.59) | Grad Norm 3.4358(2.4097) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 19.3205(19.6167) | Bit/dim 3.4436(3.4665) | Xent 0.0000(0.0000) | Loss 3.4436(3.4665) | Error 0.0000(0.0000) Steps 832(822.74) | Grad Norm 3.3436(2.4664) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 19.8358(19.5925) | Bit/dim 3.4800(3.4657) | Xent 0.0000(0.0000) | Loss 3.4800(3.4657) | Error 0.0000(0.0000) Steps 820(822.41) | Grad Norm 2.6641(2.4017) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 18.9539(19.5637) | Bit/dim 3.4692(3.4671) | Xent 0.0000(0.0000) | Loss 3.4692(3.4671) | Error 0.0000(0.0000) Steps 826(822.13) | Grad Norm 1.7078(2.4192) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 99.4561, Epoch Time 1196.4281(1484.7111), Bit/dim 3.4658(best: 3.4663), Xent 0.0000, Loss 3.4658, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 19.5573(19.5660) | Bit/dim 3.4484(3.4654) | Xent 0.0000(0.0000) | Loss 3.4484(3.4654) | Error 0.0000(0.0000) Steps 832(822.43) | Grad Norm 2.1244(2.2851) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 19.8521(19.5789) | Bit/dim 3.4603(3.4648) | Xent 0.0000(0.0000) | Loss 3.4603(3.4648) | Error 0.0000(0.0000) Steps 832(823.61) | Grad Norm 3.1754(2.4018) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 18.8945(19.6536) | Bit/dim 3.4917(3.4677) | Xent 0.0000(0.0000) | Loss 3.4917(3.4677) | Error 0.0000(0.0000) Steps 832(824.98) | Grad Norm 2.6478(2.3697) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 19.8896(19.5877) | Bit/dim 3.4558(3.4673) | Xent 0.0000(0.0000) | Loss 3.4558(3.4673) | Error 0.0000(0.0000) Steps 832(825.36) | Grad Norm 2.6389(2.5939) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 19.8232(19.6320) | Bit/dim 3.4643(3.4666) | Xent 0.0000(0.0000) | Loss 3.4643(3.4666) | Error 0.0000(0.0000) Steps 832(826.91) | Grad Norm 1.5356(2.3962) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 99.0163, Epoch Time 1195.1208(1476.0234), Bit/dim 3.4665(best: 3.4658), Xent 0.0000, Loss 3.4665, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 18.9346(19.5697) | Bit/dim 3.4651(3.4647) | Xent 0.0000(0.0000) | Loss 3.4651(3.4647) | Error 0.0000(0.0000) Steps 820(825.95) | Grad Norm 2.4827(2.3508) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 19.7568(19.5612) | Bit/dim 3.4672(3.4639) | Xent 0.0000(0.0000) | Loss 3.4672(3.4639) | Error 0.0000(0.0000) Steps 826(824.09) | Grad Norm 1.9826(2.3047) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 19.8252(19.5727) | Bit/dim 3.4460(3.4650) | Xent 0.0000(0.0000) | Loss 3.4460(3.4650) | Error 0.0000(0.0000) Steps 814(824.55) | Grad Norm 3.1494(2.3995) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 19.6897(19.5845) | Bit/dim 3.4496(3.4654) | Xent 0.0000(0.0000) | Loss 3.4496(3.4654) | Error 0.0000(0.0000) Steps 826(824.02) | Grad Norm 2.6704(2.4371) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 19.7567(19.6147) | Bit/dim 3.4569(3.4649) | Xent 0.0000(0.0000) | Loss 3.4569(3.4649) | Error 0.0000(0.0000) Steps 838(824.37) | Grad Norm 2.5116(2.3589) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 19.3611(19.6471) | Bit/dim 3.4560(3.4655) | Xent 0.0000(0.0000) | Loss 3.4560(3.4655) | Error 0.0000(0.0000) Steps 826(825.21) | Grad Norm 1.9286(2.2735) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 99.3647, Epoch Time 1195.5218(1467.6083), Bit/dim 3.4671(best: 3.4658), Xent 0.0000, Loss 3.4671, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 19.3622(19.6419) | Bit/dim 3.4691(3.4656) | Xent 0.0000(0.0000) | Loss 3.4691(3.4656) | Error 0.0000(0.0000) Steps 826(824.43) | Grad Norm 3.2676(2.2504) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 19.4940(19.6390) | Bit/dim 3.4776(3.4667) | Xent 0.0000(0.0000) | Loss 3.4776(3.4667) | Error 0.0000(0.0000) Steps 826(826.11) | Grad Norm 2.9550(2.3344) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 19.6724(19.6372) | Bit/dim 3.4754(3.4685) | Xent 0.0000(0.0000) | Loss 3.4754(3.4685) | Error 0.0000(0.0000) Steps 838(827.25) | Grad Norm 2.4262(2.4123) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 19.8255(19.5919) | Bit/dim 3.4655(3.4645) | Xent 0.0000(0.0000) | Loss 3.4655(3.4645) | Error 0.0000(0.0000) Steps 820(826.10) | Grad Norm 2.7782(2.3671) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 19.1730(19.6712) | Bit/dim 3.4460(3.4623) | Xent 0.0000(0.0000) | Loss 3.4460(3.4623) | Error 0.0000(0.0000) Steps 826(824.90) | Grad Norm 2.0647(2.2539) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 100.4145, Epoch Time 1198.7717(1459.5432), Bit/dim 3.4624(best: 3.4658), Xent 0.0000, Loss 3.4624, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 19.7370(19.7074) | Bit/dim 3.4794(3.4618) | Xent 0.0000(0.0000) | Loss 3.4794(3.4618) | Error 0.0000(0.0000) Steps 832(826.13) | Grad Norm 2.2747(2.1552) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 19.3236(19.7845) | Bit/dim 3.4281(3.4604) | Xent 0.0000(0.0000) | Loss 3.4281(3.4604) | Error 0.0000(0.0000) Steps 832(828.51) | Grad Norm 2.5884(2.2207) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 19.8878(19.7767) | Bit/dim 3.4393(3.4640) | Xent 0.0000(0.0000) | Loss 3.4393(3.4640) | Error 0.0000(0.0000) Steps 808(828.24) | Grad Norm 1.3342(2.3298) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 19.8186(19.8532) | Bit/dim 3.4429(3.4621) | Xent 0.0000(0.0000) | Loss 3.4429(3.4621) | Error 0.0000(0.0000) Steps 832(827.17) | Grad Norm 2.4542(2.2933) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 20.3029(19.8748) | Bit/dim 3.4443(3.4614) | Xent 0.0000(0.0000) | Loss 3.4443(3.4614) | Error 0.0000(0.0000) Steps 850(829.23) | Grad Norm 2.7302(2.3905) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 19.4479(19.8001) | Bit/dim 3.4605(3.4614) | Xent 0.0000(0.0000) | Loss 3.4605(3.4614) | Error 0.0000(0.0000) Steps 832(829.36) | Grad Norm 3.5405(2.4088) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 99.5247, Epoch Time 1209.7734(1452.0501), Bit/dim 3.4683(best: 3.4624), Xent 0.0000, Loss 3.4683, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 19.6920(19.7640) | Bit/dim 3.4234(3.4591) | Xent 0.0000(0.0000) | Loss 3.4234(3.4591) | Error 0.0000(0.0000) Steps 820(828.09) | Grad Norm 1.8221(2.4384) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 19.8841(19.7844) | Bit/dim 3.4556(3.4606) | Xent 0.0000(0.0000) | Loss 3.4556(3.4606) | Error 0.0000(0.0000) Steps 838(829.14) | Grad Norm 1.8441(2.4561) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 19.8304(19.7980) | Bit/dim 3.4928(3.4630) | Xent 0.0000(0.0000) | Loss 3.4928(3.4630) | Error 0.0000(0.0000) Steps 826(830.94) | Grad Norm 1.4527(2.3789) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 19.4293(19.8183) | Bit/dim 3.4551(3.4614) | Xent 0.0000(0.0000) | Loss 3.4551(3.4614) | Error 0.0000(0.0000) Steps 820(830.84) | Grad Norm 2.2628(2.2433) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 20.3089(19.8625) | Bit/dim 3.4552(3.4602) | Xent 0.0000(0.0000) | Loss 3.4552(3.4602) | Error 0.0000(0.0000) Steps 844(830.82) | Grad Norm 2.1860(2.3393) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 99.9195, Epoch Time 1209.0951(1444.7615), Bit/dim 3.4645(best: 3.4624), Xent 0.0000, Loss 3.4645, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 20.2473(19.9042) | Bit/dim 3.4692(3.4602) | Xent 0.0000(0.0000) | Loss 3.4692(3.4602) | Error 0.0000(0.0000) Steps 838(832.87) | Grad Norm 2.5468(2.4014) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 20.0990(19.8583) | Bit/dim 3.4575(3.4599) | Xent 0.0000(0.0000) | Loss 3.4575(3.4599) | Error 0.0000(0.0000) Steps 838(833.50) | Grad Norm 2.2613(2.3177) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 20.0808(19.8733) | Bit/dim 3.4602(3.4606) | Xent 0.0000(0.0000) | Loss 3.4602(3.4606) | Error 0.0000(0.0000) Steps 832(832.36) | Grad Norm 3.0435(2.4059) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_cifar10_published_baseline_bs900_2 --seed 2 --conditional False --controlled_tol False  --log_freq 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_published_baseline_bs900_1 --resume ../experiments_published/cnf_published_baseline_bs900_1/current_checkpt.pth --seed 1 --conditional False --controlled_tol False  --log_freq 10\n",
    "# #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
