{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=False, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_published_bs8K_errcontrol_1/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_published_bs8K_errcontrol_1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2801 | Time 69.0556(31.8099) | Bit/dim 1.1001(1.1060) | Xent 0.0000(0.0000) | Loss 1.1001(1.1060) | Error 0.0000(0.0000) Steps 410(411.08) | Grad Norm 4.6331(5.2128) | Total Time 10.00(10.00)\n",
      "Iter 2802 | Time 30.5716(31.7728) | Bit/dim 1.1096(1.1061) | Xent 0.0000(0.0000) | Loss 1.1096(1.1061) | Error 0.0000(0.0000) Steps 410(411.04) | Grad Norm 4.2600(5.1842) | Total Time 10.00(10.00)\n",
      "Iter 2803 | Time 29.9188(31.7171) | Bit/dim 1.1013(1.1060) | Xent 0.0000(0.0000) | Loss 1.1013(1.1060) | Error 0.0000(0.0000) Steps 404(410.83) | Grad Norm 3.2751(5.1269) | Total Time 10.00(10.00)\n",
      "Iter 2804 | Time 29.8435(31.6609) | Bit/dim 1.1001(1.1058) | Xent 0.0000(0.0000) | Loss 1.1001(1.1058) | Error 0.0000(0.0000) Steps 404(410.63) | Grad Norm 1.5724(5.0203) | Total Time 10.00(10.00)\n",
      "Iter 2805 | Time 29.5456(31.5975) | Bit/dim 1.1000(1.1056) | Xent 0.0000(0.0000) | Loss 1.1000(1.1056) | Error 0.0000(0.0000) Steps 410(410.61) | Grad Norm 0.8373(4.8948) | Total Time 10.00(10.00)\n",
      "Iter 2806 | Time 33.1711(31.6447) | Bit/dim 1.1021(1.1055) | Xent 0.0000(0.0000) | Loss 1.1021(1.1055) | Error 0.0000(0.0000) Steps 410(410.59) | Grad Norm 3.0741(4.8402) | Total Time 10.00(10.00)\n",
      "Iter 2807 | Time 32.8081(31.6796) | Bit/dim 1.0999(1.1054) | Xent 0.0000(0.0000) | Loss 1.0999(1.1054) | Error 0.0000(0.0000) Steps 410(410.57) | Grad Norm 4.4499(4.8285) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 26.6368, Epoch Time 294.4440(246.2780), Bit/dim 1.0956(best: inf), Xent 0.0000, Loss 1.0956, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2808 | Time 34.5175(31.7647) | Bit/dim 1.0980(1.1052) | Xent 0.0000(0.0000) | Loss 1.0980(1.1052) | Error 0.0000(0.0000) Steps 410(410.56) | Grad Norm 4.2122(4.8100) | Total Time 10.00(10.00)\n",
      "Iter 2809 | Time 31.9456(31.7701) | Bit/dim 1.1009(1.1050) | Xent 0.0000(0.0000) | Loss 1.1009(1.1050) | Error 0.0000(0.0000) Steps 410(410.54) | Grad Norm 2.7128(4.7471) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 29.6424(31.7063) | Bit/dim 1.1009(1.1049) | Xent 0.0000(0.0000) | Loss 1.1009(1.1049) | Error 0.0000(0.0000) Steps 410(410.52) | Grad Norm 0.7082(4.6259) | Total Time 10.00(10.00)\n",
      "Iter 2811 | Time 29.7944(31.6490) | Bit/dim 1.0986(1.1047) | Xent 0.0000(0.0000) | Loss 1.0986(1.1047) | Error 0.0000(0.0000) Steps 410(410.51) | Grad Norm 1.2056(4.5233) | Total Time 10.00(10.00)\n",
      "Iter 2812 | Time 29.7668(31.5925) | Bit/dim 1.1005(1.1046) | Xent 0.0000(0.0000) | Loss 1.1005(1.1046) | Error 0.0000(0.0000) Steps 410(410.49) | Grad Norm 2.3297(4.4575) | Total Time 10.00(10.00)\n",
      "Iter 2813 | Time 30.5263(31.5605) | Bit/dim 1.1002(1.1045) | Xent 0.0000(0.0000) | Loss 1.1002(1.1045) | Error 0.0000(0.0000) Steps 410(410.48) | Grad Norm 2.8777(4.4101) | Total Time 10.00(10.00)\n",
      "Iter 2814 | Time 29.6547(31.5033) | Bit/dim 1.1000(1.1043) | Xent 0.0000(0.0000) | Loss 1.1000(1.1043) | Error 0.0000(0.0000) Steps 404(410.28) | Grad Norm 2.7530(4.3604) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 16.1556, Epoch Time 245.2998(246.2487), Bit/dim 1.0938(best: 1.0956), Xent 0.0000, Loss 1.0938, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2815 | Time 29.4835(31.4427) | Bit/dim 1.0946(1.1040) | Xent 0.0000(0.0000) | Loss 1.0946(1.1040) | Error 0.0000(0.0000) Steps 404(410.09) | Grad Norm 2.0174(4.2901) | Total Time 10.00(10.00)\n",
      "Iter 2816 | Time 32.4491(31.4729) | Bit/dim 1.0996(1.1039) | Xent 0.0000(0.0000) | Loss 1.0996(1.1039) | Error 0.0000(0.0000) Steps 410(410.09) | Grad Norm 0.9048(4.1885) | Total Time 10.00(10.00)\n",
      "Iter 2817 | Time 31.4664(31.4727) | Bit/dim 1.0990(1.1037) | Xent 0.0000(0.0000) | Loss 1.0990(1.1037) | Error 0.0000(0.0000) Steps 422(410.45) | Grad Norm 0.5665(4.0799) | Total Time 10.00(10.00)\n",
      "Iter 2818 | Time 32.5138(31.5040) | Bit/dim 1.0975(1.1036) | Xent 0.0000(0.0000) | Loss 1.0975(1.1036) | Error 0.0000(0.0000) Steps 410(410.43) | Grad Norm 1.8992(4.0144) | Total Time 10.00(10.00)\n",
      "Iter 2819 | Time 33.8775(31.5752) | Bit/dim 1.1027(1.1035) | Xent 0.0000(0.0000) | Loss 1.1027(1.1035) | Error 0.0000(0.0000) Steps 410(410.42) | Grad Norm 2.5340(3.9700) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 31.9313(31.5859) | Bit/dim 1.0967(1.1033) | Xent 0.0000(0.0000) | Loss 1.0967(1.1033) | Error 0.0000(0.0000) Steps 410(410.41) | Grad Norm 2.2737(3.9191) | Total Time 10.00(10.00)\n",
      "Iter 2821 | Time 32.3206(31.6079) | Bit/dim 1.1011(1.1033) | Xent 0.0000(0.0000) | Loss 1.1011(1.1033) | Error 0.0000(0.0000) Steps 410(410.40) | Grad Norm 1.3881(3.8432) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 16.3173, Epoch Time 253.3797(246.4626), Bit/dim 1.0928(best: 1.0938), Xent 0.0000, Loss 1.0928, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2822 | Time 32.3472(31.6301) | Bit/dim 1.0993(1.1031) | Xent 0.0000(0.0000) | Loss 1.0993(1.1031) | Error 0.0000(0.0000) Steps 410(410.39) | Grad Norm 0.1986(3.7339) | Total Time 10.00(10.00)\n",
      "Iter 2823 | Time 29.6351(31.5702) | Bit/dim 1.0978(1.1030) | Xent 0.0000(0.0000) | Loss 1.0978(1.1030) | Error 0.0000(0.0000) Steps 404(410.19) | Grad Norm 0.9198(3.6495) | Total Time 10.00(10.00)\n",
      "Iter 2824 | Time 29.9349(31.5212) | Bit/dim 1.1010(1.1029) | Xent 0.0000(0.0000) | Loss 1.1010(1.1029) | Error 0.0000(0.0000) Steps 404(410.01) | Grad Norm 1.6060(3.5882) | Total Time 10.00(10.00)\n",
      "Iter 2825 | Time 29.6038(31.4636) | Bit/dim 1.0942(1.1027) | Xent 0.0000(0.0000) | Loss 1.0942(1.1027) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 1.7167(3.5320) | Total Time 10.00(10.00)\n",
      "Iter 2826 | Time 29.3946(31.4016) | Bit/dim 1.1011(1.1026) | Xent 0.0000(0.0000) | Loss 1.1011(1.1026) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 1.4445(3.4694) | Total Time 10.00(10.00)\n",
      "Iter 2827 | Time 29.8666(31.3555) | Bit/dim 1.0937(1.1023) | Xent 0.0000(0.0000) | Loss 1.0937(1.1023) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.8027(3.3894) | Total Time 10.00(10.00)\n",
      "Iter 2828 | Time 31.5751(31.3621) | Bit/dim 1.0969(1.1022) | Xent 0.0000(0.0000) | Loss 1.0969(1.1022) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.0988(3.2907) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 16.3860, Epoch Time 241.7421(246.3210), Bit/dim 1.0931(best: 1.0928), Xent 0.0000, Loss 1.0931, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2829 | Time 32.7301(31.4032) | Bit/dim 1.1019(1.1022) | Xent 0.0000(0.0000) | Loss 1.1019(1.1022) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.8454(3.2173) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 33.0534(31.4527) | Bit/dim 1.0969(1.1020) | Xent 0.0000(0.0000) | Loss 1.0969(1.1020) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 1.3344(3.1608) | Total Time 10.00(10.00)\n",
      "Iter 2831 | Time 31.5980(31.4570) | Bit/dim 1.0974(1.1019) | Xent 0.0000(0.0000) | Loss 1.0974(1.1019) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 1.4253(3.1088) | Total Time 10.00(10.00)\n",
      "Iter 2832 | Time 32.5208(31.4889) | Bit/dim 1.1003(1.1018) | Xent 0.0000(0.0000) | Loss 1.1003(1.1018) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.9203(3.0431) | Total Time 10.00(10.00)\n",
      "Iter 2833 | Time 30.1367(31.4484) | Bit/dim 1.0960(1.1016) | Xent 0.0000(0.0000) | Loss 1.0960(1.1016) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.3233(2.9615) | Total Time 10.00(10.00)\n",
      "Iter 2834 | Time 30.5871(31.4225) | Bit/dim 1.0958(1.1015) | Xent 0.0000(0.0000) | Loss 1.0958(1.1015) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.3543(2.8833) | Total Time 10.00(10.00)\n",
      "Iter 2835 | Time 29.8568(31.3756) | Bit/dim 1.0993(1.1014) | Xent 0.0000(0.0000) | Loss 1.0993(1.1014) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.8106(2.8211) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 16.0832, Epoch Time 249.3552(246.4120), Bit/dim 1.0921(best: 1.0928), Xent 0.0000, Loss 1.0921, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2836 | Time 29.7752(31.3275) | Bit/dim 1.0990(1.1013) | Xent 0.0000(0.0000) | Loss 1.0990(1.1013) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.9998(2.7665) | Total Time 10.00(10.00)\n",
      "Iter 2837 | Time 29.3077(31.2670) | Bit/dim 1.0969(1.1012) | Xent 0.0000(0.0000) | Loss 1.0969(1.1012) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.8964(2.7104) | Total Time 10.00(10.00)\n",
      "Iter 2838 | Time 31.6948(31.2798) | Bit/dim 1.1015(1.1012) | Xent 0.0000(0.0000) | Loss 1.1015(1.1012) | Error 0.0000(0.0000) Steps 410(410.01) | Grad Norm 0.5673(2.6461) | Total Time 10.00(10.00)\n",
      "Iter 2839 | Time 31.9716(31.3005) | Bit/dim 1.0964(1.1011) | Xent 0.0000(0.0000) | Loss 1.0964(1.1011) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.1108(2.5700) | Total Time 10.00(10.00)\n",
      "Iter 2840 | Time 30.1226(31.2652) | Bit/dim 1.0998(1.1010) | Xent 0.0000(0.0000) | Loss 1.0998(1.1010) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.4172(2.5054) | Total Time 10.00(10.00)\n",
      "Iter 2841 | Time 29.4685(31.2113) | Bit/dim 1.0953(1.1009) | Xent 0.0000(0.0000) | Loss 1.0953(1.1009) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.7467(2.4527) | Total Time 10.00(10.00)\n",
      "Iter 2842 | Time 31.3820(31.2164) | Bit/dim 1.0981(1.1008) | Xent 0.0000(0.0000) | Loss 1.0981(1.1008) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.7759(2.4024) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 16.3455, Epoch Time 243.0602(246.3115), Bit/dim 1.0920(best: 1.0921), Xent 0.0000, Loss 1.0920, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2843 | Time 30.0177(31.1805) | Bit/dim 1.0992(1.1007) | Xent 0.0000(0.0000) | Loss 1.0992(1.1007) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.6295(2.3492) | Total Time 10.00(10.00)\n",
      "Iter 2844 | Time 32.6841(31.2256) | Bit/dim 1.1006(1.1007) | Xent 0.0000(0.0000) | Loss 1.1006(1.1007) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.2670(2.2867) | Total Time 10.00(10.00)\n",
      "Iter 2845 | Time 33.3184(31.2884) | Bit/dim 1.0970(1.1006) | Xent 0.0000(0.0000) | Loss 1.0970(1.1006) | Error 0.0000(0.0000) Steps 410(410.00) | Grad Norm 0.1671(2.2231) | Total Time 10.00(10.00)\n",
      "Iter 2846 | Time 32.4456(31.3231) | Bit/dim 1.0958(1.1005) | Xent 0.0000(0.0000) | Loss 1.0958(1.1005) | Error 0.0000(0.0000) Steps 422(410.36) | Grad Norm 0.4244(2.1692) | Total Time 10.00(10.00)\n",
      "Iter 2847 | Time 31.6223(31.3320) | Bit/dim 1.0972(1.1004) | Xent 0.0000(0.0000) | Loss 1.0972(1.1004) | Error 0.0000(0.0000) Steps 404(410.17) | Grad Norm 0.5459(2.1205) | Total Time 10.00(10.00)\n",
      "Iter 2848 | Time 29.4589(31.2759) | Bit/dim 1.0997(1.1003) | Xent 0.0000(0.0000) | Loss 1.0997(1.1003) | Error 0.0000(0.0000) Steps 404(409.99) | Grad Norm 0.6085(2.0751) | Total Time 10.00(10.00)\n",
      "Iter 2849 | Time 29.5172(31.2231) | Bit/dim 1.0960(1.1002) | Xent 0.0000(0.0000) | Loss 1.0960(1.1002) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.4217(2.0255) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 16.5577, Epoch Time 248.6095(246.3804), Bit/dim 1.0928(best: 1.0920), Xent 0.0000, Loss 1.0928, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2850 | Time 29.6425(31.1757) | Bit/dim 1.0981(1.1002) | Xent 0.0000(0.0000) | Loss 1.0981(1.1002) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.0792(1.9671) | Total Time 10.00(10.00)\n",
      "Iter 2851 | Time 28.9336(31.1084) | Bit/dim 1.1037(1.1003) | Xent 0.0000(0.0000) | Loss 1.1037(1.1003) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.2242(1.9148) | Total Time 10.00(10.00)\n",
      "Iter 2852 | Time 30.3735(31.0864) | Bit/dim 1.1029(1.1003) | Xent 0.0000(0.0000) | Loss 1.1029(1.1003) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.4011(1.8694) | Total Time 10.00(10.00)\n",
      "Iter 2853 | Time 32.4653(31.1277) | Bit/dim 1.0915(1.1001) | Xent 0.0000(0.0000) | Loss 1.0915(1.1001) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.5128(1.8287) | Total Time 10.00(10.00)\n",
      "Iter 2854 | Time 31.9700(31.1530) | Bit/dim 1.0984(1.1000) | Xent 0.0000(0.0000) | Loss 1.0984(1.1000) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.3850(1.7854) | Total Time 10.00(10.00)\n",
      "Iter 2855 | Time 29.8572(31.1141) | Bit/dim 1.0968(1.0999) | Xent 0.0000(0.0000) | Loss 1.0968(1.0999) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.1818(1.7373) | Total Time 10.00(10.00)\n",
      "Iter 2856 | Time 32.3641(31.1516) | Bit/dim 1.0948(1.0998) | Xent 0.0000(0.0000) | Loss 1.0948(1.0998) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.0882(1.6878) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 16.1903, Epoch Time 244.5636(246.3259), Bit/dim 1.0925(best: 1.0920), Xent 0.0000, Loss 1.0925, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2857 | Time 29.6205(31.1057) | Bit/dim 1.0981(1.0997) | Xent 0.0000(0.0000) | Loss 1.0981(1.0997) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.2666(1.6452) | Total Time 10.00(10.00)\n",
      "Iter 2858 | Time 29.4648(31.0565) | Bit/dim 1.0953(1.0996) | Xent 0.0000(0.0000) | Loss 1.0953(1.0996) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.3805(1.6073) | Total Time 10.00(10.00)\n",
      "Iter 2859 | Time 29.6085(31.0130) | Bit/dim 1.0976(1.0995) | Xent 0.0000(0.0000) | Loss 1.0976(1.0995) | Error 0.0000(0.0000) Steps 410(409.99) | Grad Norm 0.3573(1.5698) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 31.3008(31.0217) | Bit/dim 1.0978(1.0995) | Xent 0.0000(0.0000) | Loss 1.0978(1.0995) | Error 0.0000(0.0000) Steps 404(409.81) | Grad Norm 0.2862(1.5312) | Total Time 10.00(10.00)\n",
      "Iter 2861 | Time 31.8356(31.0461) | Bit/dim 1.1031(1.0996) | Xent 0.0000(0.0000) | Loss 1.1031(1.0996) | Error 0.0000(0.0000) Steps 410(409.82) | Grad Norm 0.1088(1.4886) | Total Time 10.00(10.00)\n",
      "Iter 2862 | Time 32.4977(31.0896) | Bit/dim 1.0924(1.0994) | Xent 0.0000(0.0000) | Loss 1.0924(1.0994) | Error 0.0000(0.0000) Steps 410(409.82) | Grad Norm 0.1643(1.4488) | Total Time 10.00(10.00)\n",
      "Iter 2863 | Time 31.2539(31.0946) | Bit/dim 1.0981(1.0993) | Xent 0.0000(0.0000) | Loss 1.0981(1.0993) | Error 0.0000(0.0000) Steps 410(409.83) | Grad Norm 0.3388(1.4155) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 16.5666, Epoch Time 245.3459(246.2965), Bit/dim 1.0927(best: 1.0920), Xent 0.0000, Loss 1.0927, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2864 | Time 31.2983(31.1007) | Bit/dim 1.0964(1.0992) | Xent 0.0000(0.0000) | Loss 1.0964(1.0992) | Error 0.0000(0.0000) Steps 410(409.83) | Grad Norm 0.3628(1.3840) | Total Time 10.00(10.00)\n",
      "Iter 2865 | Time 30.6856(31.0882) | Bit/dim 1.0985(1.0992) | Xent 0.0000(0.0000) | Loss 1.0985(1.0992) | Error 0.0000(0.0000) Steps 410(409.84) | Grad Norm 0.2311(1.3494) | Total Time 10.00(10.00)\n",
      "Iter 2866 | Time 31.3398(31.0958) | Bit/dim 1.0936(1.0990) | Xent 0.0000(0.0000) | Loss 1.0936(1.0990) | Error 0.0000(0.0000) Steps 410(409.84) | Grad Norm 0.0735(1.3111) | Total Time 10.00(10.00)\n",
      "Iter 2867 | Time 32.1865(31.1285) | Bit/dim 1.0976(1.0990) | Xent 0.0000(0.0000) | Loss 1.0976(1.0990) | Error 0.0000(0.0000) Steps 404(409.67) | Grad Norm 0.1449(1.2761) | Total Time 10.00(10.00)\n",
      "Iter 2868 | Time 31.3507(31.1352) | Bit/dim 1.1019(1.0991) | Xent 0.0000(0.0000) | Loss 1.1019(1.0991) | Error 0.0000(0.0000) Steps 410(409.68) | Grad Norm 0.1465(1.2422) | Total Time 10.00(10.00)\n",
      "Iter 2869 | Time 31.8403(31.1563) | Bit/dim 1.1011(1.0992) | Xent 0.0000(0.0000) | Loss 1.1011(1.0992) | Error 0.0000(0.0000) Steps 404(409.51) | Grad Norm 0.2571(1.2127) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 29.5127(31.1070) | Bit/dim 1.0932(1.0990) | Xent 0.0000(0.0000) | Loss 1.0932(1.0990) | Error 0.0000(0.0000) Steps 410(409.52) | Grad Norm 0.1574(1.1810) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 16.3219, Epoch Time 247.4928(246.3324), Bit/dim 1.0925(best: 1.0920), Xent 0.0000, Loss 1.0925, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2871 | Time 33.1095(31.1671) | Bit/dim 1.0993(1.0990) | Xent 0.0000(0.0000) | Loss 1.0993(1.0990) | Error 0.0000(0.0000) Steps 410(409.54) | Grad Norm 0.1362(1.1497) | Total Time 10.00(10.00)\n",
      "Iter 2872 | Time 31.5971(31.1800) | Bit/dim 1.0920(1.0988) | Xent 0.0000(0.0000) | Loss 1.0920(1.0988) | Error 0.0000(0.0000) Steps 410(409.55) | Grad Norm 0.0801(1.1176) | Total Time 10.00(10.00)\n",
      "Iter 2873 | Time 29.6199(31.1332) | Bit/dim 1.1034(1.0989) | Xent 0.0000(0.0000) | Loss 1.1034(1.0989) | Error 0.0000(0.0000) Steps 404(409.38) | Grad Norm 0.0675(1.0861) | Total Time 10.00(10.00)\n",
      "Iter 2874 | Time 32.0466(31.1606) | Bit/dim 1.0955(1.0988) | Xent 0.0000(0.0000) | Loss 1.0955(1.0988) | Error 0.0000(0.0000) Steps 410(409.40) | Grad Norm 0.2262(1.0603) | Total Time 10.00(10.00)\n",
      "Iter 2875 | Time 34.3363(31.2558) | Bit/dim 1.0973(1.0988) | Xent 0.0000(0.0000) | Loss 1.0973(1.0988) | Error 0.0000(0.0000) Steps 422(409.78) | Grad Norm 0.1919(1.0342) | Total Time 10.00(10.00)\n",
      "Iter 2876 | Time 31.4624(31.2620) | Bit/dim 1.0932(1.0986) | Xent 0.0000(0.0000) | Loss 1.0932(1.0986) | Error 0.0000(0.0000) Steps 404(409.61) | Grad Norm 0.1588(1.0080) | Total Time 10.00(10.00)\n",
      "Iter 2877 | Time 31.0066(31.2544) | Bit/dim 1.0992(1.0986) | Xent 0.0000(0.0000) | Loss 1.0992(1.0986) | Error 0.0000(0.0000) Steps 410(409.62) | Grad Norm 0.0653(0.9797) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 16.0297, Epoch Time 251.8967(246.4993), Bit/dim 1.0916(best: 1.0920), Xent 0.0000, Loss 1.0916, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2878 | Time 30.8445(31.2421) | Bit/dim 1.0939(1.0985) | Xent 0.0000(0.0000) | Loss 1.0939(1.0985) | Error 0.0000(0.0000) Steps 410(409.63) | Grad Norm 0.0589(0.9521) | Total Time 10.00(10.00)\n",
      "Iter 2879 | Time 31.5396(31.2510) | Bit/dim 1.0981(1.0985) | Xent 0.0000(0.0000) | Loss 1.0981(1.0985) | Error 0.0000(0.0000) Steps 410(409.64) | Grad Norm 0.1424(0.9278) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 29.6807(31.2039) | Bit/dim 1.0992(1.0985) | Xent 0.0000(0.0000) | Loss 1.0992(1.0985) | Error 0.0000(0.0000) Steps 410(409.65) | Grad Norm 0.1840(0.9055) | Total Time 10.00(10.00)\n",
      "Iter 2881 | Time 32.6678(31.2478) | Bit/dim 1.0989(1.0985) | Xent 0.0000(0.0000) | Loss 1.0989(1.0985) | Error 0.0000(0.0000) Steps 428(410.20) | Grad Norm 0.1678(0.8833) | Total Time 10.00(10.00)\n",
      "Iter 2882 | Time 31.6960(31.2613) | Bit/dim 1.0966(1.0984) | Xent 0.0000(0.0000) | Loss 1.0966(1.0984) | Error 0.0000(0.0000) Steps 410(410.20) | Grad Norm 0.0553(0.8585) | Total Time 10.00(10.00)\n",
      "Iter 2883 | Time 29.7824(31.2169) | Bit/dim 1.0987(1.0984) | Xent 0.0000(0.0000) | Loss 1.0987(1.0984) | Error 0.0000(0.0000) Steps 410(410.19) | Grad Norm 0.1321(0.8367) | Total Time 10.00(10.00)\n",
      "Iter 2884 | Time 31.6954(31.2312) | Bit/dim 1.0961(1.0984) | Xent 0.0000(0.0000) | Loss 1.0961(1.0984) | Error 0.0000(0.0000) Steps 410(410.18) | Grad Norm 0.1093(0.8149) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 16.1035, Epoch Time 246.9323(246.5123), Bit/dim 1.0919(best: 1.0916), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2885 | Time 31.9860(31.2539) | Bit/dim 1.1009(1.0985) | Xent 0.0000(0.0000) | Loss 1.1009(1.0985) | Error 0.0000(0.0000) Steps 410(410.18) | Grad Norm 0.1317(0.7944) | Total Time 10.00(10.00)\n",
      "Iter 2886 | Time 32.2999(31.2853) | Bit/dim 1.0954(1.0984) | Xent 0.0000(0.0000) | Loss 1.0954(1.0984) | Error 0.0000(0.0000) Steps 410(410.17) | Grad Norm 0.0659(0.7725) | Total Time 10.00(10.00)\n",
      "Iter 2887 | Time 31.4901(31.2914) | Bit/dim 1.1012(1.0984) | Xent 0.0000(0.0000) | Loss 1.1012(1.0984) | Error 0.0000(0.0000) Steps 410(410.17) | Grad Norm 0.0547(0.7510) | Total Time 10.00(10.00)\n",
      "Iter 2888 | Time 31.9187(31.3102) | Bit/dim 1.0942(1.0983) | Xent 0.0000(0.0000) | Loss 1.0942(1.0983) | Error 0.0000(0.0000) Steps 410(410.16) | Grad Norm 0.0836(0.7310) | Total Time 10.00(10.00)\n",
      "Iter 2889 | Time 32.0499(31.3324) | Bit/dim 1.0985(1.0983) | Xent 0.0000(0.0000) | Loss 1.0985(1.0983) | Error 0.0000(0.0000) Steps 422(410.52) | Grad Norm 0.0667(0.7110) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 32.4260(31.3652) | Bit/dim 1.0972(1.0983) | Xent 0.0000(0.0000) | Loss 1.0972(1.0983) | Error 0.0000(0.0000) Steps 422(410.86) | Grad Norm 0.0866(0.6923) | Total Time 10.00(10.00)\n",
      "Iter 2891 | Time 31.6380(31.3734) | Bit/dim 1.0977(1.0983) | Xent 0.0000(0.0000) | Loss 1.0977(1.0983) | Error 0.0000(0.0000) Steps 410(410.84) | Grad Norm 0.0819(0.6740) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 16.0150, Epoch Time 252.6580(246.6967), Bit/dim 1.0925(best: 1.0916), Xent 0.0000, Loss 1.0925, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2892 | Time 32.5833(31.4097) | Bit/dim 1.0959(1.0982) | Xent 0.0000(0.0000) | Loss 1.0959(1.0982) | Error 0.0000(0.0000) Steps 410(410.81) | Grad Norm 0.0632(0.6557) | Total Time 10.00(10.00)\n",
      "Iter 2893 | Time 32.9069(31.4546) | Bit/dim 1.0938(1.0981) | Xent 0.0000(0.0000) | Loss 1.0938(1.0981) | Error 0.0000(0.0000) Steps 410(410.79) | Grad Norm 0.0506(0.6375) | Total Time 10.00(10.00)\n",
      "Iter 2894 | Time 32.7834(31.4945) | Bit/dim 1.0972(1.0980) | Xent 0.0000(0.0000) | Loss 1.0972(1.0980) | Error 0.0000(0.0000) Steps 410(410.76) | Grad Norm 0.1464(0.6228) | Total Time 10.00(10.00)\n",
      "Iter 2895 | Time 30.1170(31.4532) | Bit/dim 1.0949(1.0980) | Xent 0.0000(0.0000) | Loss 1.0949(1.0980) | Error 0.0000(0.0000) Steps 410(410.74) | Grad Norm 0.0843(0.6066) | Total Time 10.00(10.00)\n",
      "Iter 2896 | Time 29.8553(31.4052) | Bit/dim 1.0975(1.0979) | Xent 0.0000(0.0000) | Loss 1.0975(1.0979) | Error 0.0000(0.0000) Steps 410(410.72) | Grad Norm 0.0796(0.5908) | Total Time 10.00(10.00)\n",
      "Iter 2897 | Time 32.2486(31.4305) | Bit/dim 1.0999(1.0980) | Xent 0.0000(0.0000) | Loss 1.0999(1.0980) | Error 0.0000(0.0000) Steps 422(411.06) | Grad Norm 0.0788(0.5755) | Total Time 10.00(10.00)\n",
      "Iter 2898 | Time 31.1510(31.4221) | Bit/dim 1.1010(1.0981) | Xent 0.0000(0.0000) | Loss 1.1010(1.0981) | Error 0.0000(0.0000) Steps 410(411.03) | Grad Norm 0.0519(0.5598) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 15.8485, Epoch Time 250.2100(246.8021), Bit/dim 1.0918(best: 1.0916), Xent 0.0000, Loss 1.0918, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2899 | Time 30.9270(31.4073) | Bit/dim 1.0981(1.0981) | Xent 0.0000(0.0000) | Loss 1.0981(1.0981) | Error 0.0000(0.0000) Steps 410(410.99) | Grad Norm 0.1334(0.5470) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 29.6870(31.3557) | Bit/dim 1.0950(1.0980) | Xent 0.0000(0.0000) | Loss 1.0950(1.0980) | Error 0.0000(0.0000) Steps 410(410.97) | Grad Norm 0.0920(0.5333) | Total Time 10.00(10.00)\n",
      "Iter 2901 | Time 32.2925(31.3838) | Bit/dim 1.0982(1.0980) | Xent 0.0000(0.0000) | Loss 1.0982(1.0980) | Error 0.0000(0.0000) Steps 410(410.94) | Grad Norm 0.0765(0.5196) | Total Time 10.00(10.00)\n",
      "Iter 2902 | Time 30.2729(31.3505) | Bit/dim 1.0966(1.0980) | Xent 0.0000(0.0000) | Loss 1.0966(1.0980) | Error 0.0000(0.0000) Steps 410(410.91) | Grad Norm 0.0615(0.5059) | Total Time 10.00(10.00)\n",
      "Iter 2903 | Time 32.2547(31.3776) | Bit/dim 1.0935(1.0978) | Xent 0.0000(0.0000) | Loss 1.0935(1.0978) | Error 0.0000(0.0000) Steps 416(411.06) | Grad Norm 0.0605(0.4925) | Total Time 10.00(10.00)\n",
      "Iter 2904 | Time 31.4522(31.3798) | Bit/dim 1.0961(1.0978) | Xent 0.0000(0.0000) | Loss 1.0961(1.0978) | Error 0.0000(0.0000) Steps 410(411.03) | Grad Norm 0.0683(0.4798) | Total Time 10.00(10.00)\n",
      "Iter 2905 | Time 33.4506(31.4420) | Bit/dim 1.1014(1.0979) | Xent 0.0000(0.0000) | Loss 1.1014(1.0979) | Error 0.0000(0.0000) Steps 428(411.54) | Grad Norm 0.0548(0.4670) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 16.2855, Epoch Time 249.4776(246.8824), Bit/dim 1.0919(best: 1.0916), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2906 | Time 33.0683(31.4907) | Bit/dim 1.0949(1.0978) | Xent 0.0000(0.0000) | Loss 1.0949(1.0978) | Error 0.0000(0.0000) Steps 410(411.49) | Grad Norm 0.0578(0.4548) | Total Time 10.00(10.00)\n",
      "Iter 2907 | Time 32.4991(31.5210) | Bit/dim 1.0962(1.0977) | Xent 0.0000(0.0000) | Loss 1.0962(1.0977) | Error 0.0000(0.0000) Steps 410(411.45) | Grad Norm 0.0619(0.4430) | Total Time 10.00(10.00)\n",
      "Iter 2908 | Time 31.7309(31.5273) | Bit/dim 1.1015(1.0979) | Xent 0.0000(0.0000) | Loss 1.1015(1.0979) | Error 0.0000(0.0000) Steps 422(411.76) | Grad Norm 0.0678(0.4317) | Total Time 10.00(10.00)\n",
      "Iter 2909 | Time 32.3037(31.5506) | Bit/dim 1.0991(1.0979) | Xent 0.0000(0.0000) | Loss 1.0991(1.0979) | Error 0.0000(0.0000) Steps 410(411.71) | Grad Norm 0.0617(0.4206) | Total Time 10.00(10.00)\n",
      "Iter 2910 | Time 32.6742(31.5843) | Bit/dim 1.0974(1.0979) | Xent 0.0000(0.0000) | Loss 1.0974(1.0979) | Error 0.0000(0.0000) Steps 410(411.66) | Grad Norm 0.0629(0.4099) | Total Time 10.00(10.00)\n",
      "Iter 2911 | Time 33.2548(31.6344) | Bit/dim 1.0941(1.0978) | Xent 0.0000(0.0000) | Loss 1.0941(1.0978) | Error 0.0000(0.0000) Steps 416(411.79) | Grad Norm 0.0904(0.4003) | Total Time 10.00(10.00)\n",
      "Iter 2912 | Time 32.3938(31.6572) | Bit/dim 1.0973(1.0978) | Xent 0.0000(0.0000) | Loss 1.0973(1.0978) | Error 0.0000(0.0000) Steps 416(411.92) | Grad Norm 0.0843(0.3908) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 16.4651, Epoch Time 257.4101(247.1982), Bit/dim 1.0923(best: 1.0916), Xent 0.0000, Loss 1.0923, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2913 | Time 32.6053(31.6856) | Bit/dim 1.0979(1.0978) | Xent 0.0000(0.0000) | Loss 1.0979(1.0978) | Error 0.0000(0.0000) Steps 410(411.86) | Grad Norm 0.0540(0.3807) | Total Time 10.00(10.00)\n",
      "Iter 2914 | Time 31.4544(31.6787) | Bit/dim 1.0992(1.0978) | Xent 0.0000(0.0000) | Loss 1.0992(1.0978) | Error 0.0000(0.0000) Steps 416(411.98) | Grad Norm 0.0582(0.3710) | Total Time 10.00(10.00)\n",
      "Iter 2915 | Time 33.9360(31.7464) | Bit/dim 1.0978(1.0978) | Xent 0.0000(0.0000) | Loss 1.0978(1.0978) | Error 0.0000(0.0000) Steps 416(412.10) | Grad Norm 0.1054(0.3631) | Total Time 10.00(10.00)\n",
      "Iter 2916 | Time 32.7856(31.7776) | Bit/dim 1.0958(1.0977) | Xent 0.0000(0.0000) | Loss 1.0958(1.0977) | Error 0.0000(0.0000) Steps 416(412.22) | Grad Norm 0.0658(0.3542) | Total Time 10.00(10.00)\n",
      "Iter 2917 | Time 32.9993(31.8142) | Bit/dim 1.0984(1.0978) | Xent 0.0000(0.0000) | Loss 1.0984(1.0978) | Error 0.0000(0.0000) Steps 416(412.33) | Grad Norm 0.0546(0.3452) | Total Time 10.00(10.00)\n",
      "Iter 2918 | Time 32.3257(31.8296) | Bit/dim 1.0942(1.0977) | Xent 0.0000(0.0000) | Loss 1.0942(1.0977) | Error 0.0000(0.0000) Steps 416(412.44) | Grad Norm 0.0701(0.3369) | Total Time 10.00(10.00)\n",
      "Iter 2919 | Time 31.9516(31.8332) | Bit/dim 1.0977(1.0977) | Xent 0.0000(0.0000) | Loss 1.0977(1.0977) | Error 0.0000(0.0000) Steps 416(412.55) | Grad Norm 0.0517(0.3284) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 15.8888, Epoch Time 256.9301(247.4901), Bit/dim 1.0919(best: 1.0916), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2920 | Time 32.3856(31.8498) | Bit/dim 1.0946(1.0976) | Xent 0.0000(0.0000) | Loss 1.0946(1.0976) | Error 0.0000(0.0000) Steps 428(413.01) | Grad Norm 0.0609(0.3203) | Total Time 10.00(10.00)\n",
      "Iter 2921 | Time 32.9043(31.8814) | Bit/dim 1.0961(1.0975) | Xent 0.0000(0.0000) | Loss 1.0961(1.0975) | Error 0.0000(0.0000) Steps 428(413.46) | Grad Norm 0.0860(0.3133) | Total Time 10.00(10.00)\n",
      "Iter 2922 | Time 32.0936(31.8878) | Bit/dim 1.0957(1.0975) | Xent 0.0000(0.0000) | Loss 1.0957(1.0975) | Error 0.0000(0.0000) Steps 428(413.90) | Grad Norm 0.0695(0.3060) | Total Time 10.00(10.00)\n",
      "Iter 2923 | Time 31.2535(31.8688) | Bit/dim 1.0977(1.0975) | Xent 0.0000(0.0000) | Loss 1.0977(1.0975) | Error 0.0000(0.0000) Steps 416(413.96) | Grad Norm 0.0913(0.2995) | Total Time 10.00(10.00)\n",
      "Iter 2924 | Time 32.4253(31.8855) | Bit/dim 1.1004(1.0976) | Xent 0.0000(0.0000) | Loss 1.1004(1.0976) | Error 0.0000(0.0000) Steps 416(414.02) | Grad Norm 0.0583(0.2923) | Total Time 10.00(10.00)\n",
      "Iter 2925 | Time 32.1696(31.8940) | Bit/dim 1.1014(1.0977) | Xent 0.0000(0.0000) | Loss 1.1014(1.0977) | Error 0.0000(0.0000) Steps 416(414.08) | Grad Norm 0.0531(0.2851) | Total Time 10.00(10.00)\n",
      "Iter 2926 | Time 30.9328(31.8652) | Bit/dim 1.0981(1.0977) | Xent 0.0000(0.0000) | Loss 1.0981(1.0977) | Error 0.0000(0.0000) Steps 416(414.14) | Grad Norm 0.0610(0.2784) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 16.2751, Epoch Time 253.2751(247.6637), Bit/dim 1.0924(best: 1.0916), Xent 0.0000, Loss 1.0924, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2927 | Time 30.0650(31.8112) | Bit/dim 1.0990(1.0977) | Xent 0.0000(0.0000) | Loss 1.0990(1.0977) | Error 0.0000(0.0000) Steps 416(414.20) | Grad Norm 0.0636(0.2720) | Total Time 10.00(10.00)\n",
      "Iter 2928 | Time 30.6218(31.7755) | Bit/dim 1.0945(1.0976) | Xent 0.0000(0.0000) | Loss 1.0945(1.0976) | Error 0.0000(0.0000) Steps 416(414.25) | Grad Norm 0.0617(0.2657) | Total Time 10.00(10.00)\n",
      "Iter 2929 | Time 32.7588(31.8050) | Bit/dim 1.1031(1.0978) | Xent 0.0000(0.0000) | Loss 1.1031(1.0978) | Error 0.0000(0.0000) Steps 416(414.30) | Grad Norm 0.0716(0.2598) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 32.1399(31.8150) | Bit/dim 1.0932(1.0977) | Xent 0.0000(0.0000) | Loss 1.0932(1.0977) | Error 0.0000(0.0000) Steps 410(414.17) | Grad Norm 0.0725(0.2542) | Total Time 10.00(10.00)\n",
      "Iter 2931 | Time 31.8034(31.8147) | Bit/dim 1.0980(1.0977) | Xent 0.0000(0.0000) | Loss 1.0980(1.0977) | Error 0.0000(0.0000) Steps 416(414.23) | Grad Norm 0.0651(0.2485) | Total Time 10.00(10.00)\n",
      "Iter 2932 | Time 31.4874(31.8049) | Bit/dim 1.1010(1.0978) | Xent 0.0000(0.0000) | Loss 1.1010(1.0978) | Error 0.0000(0.0000) Steps 416(414.28) | Grad Norm 0.1086(0.2443) | Total Time 10.00(10.00)\n",
      "Iter 2933 | Time 30.0620(31.7526) | Bit/dim 1.0939(1.0977) | Xent 0.0000(0.0000) | Loss 1.0939(1.0977) | Error 0.0000(0.0000) Steps 416(414.33) | Grad Norm 0.0566(0.2387) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 16.2747, Epoch Time 248.0862(247.6764), Bit/dim 1.0914(best: 1.0916), Xent 0.0000, Loss 1.0914, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2934 | Time 31.9900(31.7597) | Bit/dim 1.0975(1.0976) | Xent 0.0000(0.0000) | Loss 1.0975(1.0976) | Error 0.0000(0.0000) Steps 410(414.20) | Grad Norm 0.0633(0.2334) | Total Time 10.00(10.00)\n",
      "Iter 2935 | Time 32.8609(31.7927) | Bit/dim 1.0931(1.0975) | Xent 0.0000(0.0000) | Loss 1.0931(1.0975) | Error 0.0000(0.0000) Steps 416(414.26) | Grad Norm 0.0739(0.2287) | Total Time 10.00(10.00)\n",
      "Iter 2936 | Time 32.9071(31.8262) | Bit/dim 1.0965(1.0975) | Xent 0.0000(0.0000) | Loss 1.0965(1.0975) | Error 0.0000(0.0000) Steps 428(414.67) | Grad Norm 0.0662(0.2238) | Total Time 10.00(10.00)\n",
      "Iter 2937 | Time 32.8743(31.8576) | Bit/dim 1.0975(1.0975) | Xent 0.0000(0.0000) | Loss 1.0975(1.0975) | Error 0.0000(0.0000) Steps 416(414.71) | Grad Norm 0.0782(0.2194) | Total Time 10.00(10.00)\n",
      "Iter 2938 | Time 32.8282(31.8867) | Bit/dim 1.1005(1.0976) | Xent 0.0000(0.0000) | Loss 1.1005(1.0976) | Error 0.0000(0.0000) Steps 416(414.75) | Grad Norm 0.0813(0.2153) | Total Time 10.00(10.00)\n",
      "Iter 2939 | Time 32.9854(31.9197) | Bit/dim 1.0959(1.0975) | Xent 0.0000(0.0000) | Loss 1.0959(1.0975) | Error 0.0000(0.0000) Steps 428(415.15) | Grad Norm 0.0543(0.2104) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 32.9801(31.9515) | Bit/dim 1.0968(1.0975) | Xent 0.0000(0.0000) | Loss 1.0968(1.0975) | Error 0.0000(0.0000) Steps 416(415.17) | Grad Norm 0.0606(0.2060) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 16.2302, Epoch Time 258.6062(248.0043), Bit/dim 1.0917(best: 1.0914), Xent 0.0000, Loss 1.0917, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2941 | Time 32.5836(31.9705) | Bit/dim 1.1003(1.0976) | Xent 0.0000(0.0000) | Loss 1.1003(1.0976) | Error 0.0000(0.0000) Steps 416(415.20) | Grad Norm 0.0554(0.2014) | Total Time 10.00(10.00)\n",
      "Iter 2942 | Time 32.0853(31.9739) | Bit/dim 1.0940(1.0975) | Xent 0.0000(0.0000) | Loss 1.0940(1.0975) | Error 0.0000(0.0000) Steps 416(415.22) | Grad Norm 0.0907(0.1981) | Total Time 10.00(10.00)\n",
      "Iter 2943 | Time 32.2605(31.9825) | Bit/dim 1.0983(1.0975) | Xent 0.0000(0.0000) | Loss 1.0983(1.0975) | Error 0.0000(0.0000) Steps 410(415.06) | Grad Norm 0.0905(0.1949) | Total Time 10.00(10.00)\n",
      "Iter 2944 | Time 32.8214(32.0077) | Bit/dim 1.0972(1.0975) | Xent 0.0000(0.0000) | Loss 1.0972(1.0975) | Error 0.0000(0.0000) Steps 416(415.09) | Grad Norm 0.0742(0.1913) | Total Time 10.00(10.00)\n",
      "Iter 2945 | Time 31.6960(31.9983) | Bit/dim 1.0990(1.0975) | Xent 0.0000(0.0000) | Loss 1.0990(1.0975) | Error 0.0000(0.0000) Steps 416(415.12) | Grad Norm 0.0634(0.1874) | Total Time 10.00(10.00)\n",
      "Iter 2946 | Time 32.3044(32.0075) | Bit/dim 1.0982(1.0976) | Xent 0.0000(0.0000) | Loss 1.0982(1.0976) | Error 0.0000(0.0000) Steps 428(415.51) | Grad Norm 0.0643(0.1837) | Total Time 10.00(10.00)\n",
      "Iter 2947 | Time 30.3210(31.9569) | Bit/dim 1.0976(1.0976) | Xent 0.0000(0.0000) | Loss 1.0976(1.0976) | Error 0.0000(0.0000) Steps 416(415.52) | Grad Norm 0.0758(0.1805) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 16.1287, Epoch Time 253.1734(248.1593), Bit/dim 1.0919(best: 1.0914), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2948 | Time 29.8929(31.8950) | Bit/dim 1.0976(1.0976) | Xent 0.0000(0.0000) | Loss 1.0976(1.0976) | Error 0.0000(0.0000) Steps 416(415.53) | Grad Norm 0.0534(0.1767) | Total Time 10.00(10.00)\n",
      "Iter 2949 | Time 32.0806(31.9006) | Bit/dim 1.0931(1.0974) | Xent 0.0000(0.0000) | Loss 1.0931(1.0974) | Error 0.0000(0.0000) Steps 416(415.55) | Grad Norm 0.1088(0.1747) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 31.9991(31.9035) | Bit/dim 1.0978(1.0974) | Xent 0.0000(0.0000) | Loss 1.0978(1.0974) | Error 0.0000(0.0000) Steps 410(415.38) | Grad Norm 0.0820(0.1719) | Total Time 10.00(10.00)\n",
      "Iter 2951 | Time 31.7378(31.8985) | Bit/dim 1.0976(1.0974) | Xent 0.0000(0.0000) | Loss 1.0976(1.0974) | Error 0.0000(0.0000) Steps 416(415.40) | Grad Norm 0.0703(0.1688) | Total Time 10.00(10.00)\n",
      "Iter 2952 | Time 32.9549(31.9302) | Bit/dim 1.1009(1.0975) | Xent 0.0000(0.0000) | Loss 1.1009(1.0975) | Error 0.0000(0.0000) Steps 428(415.78) | Grad Norm 0.0988(0.1667) | Total Time 10.00(10.00)\n",
      "Iter 2953 | Time 32.3230(31.9420) | Bit/dim 1.0972(1.0975) | Xent 0.0000(0.0000) | Loss 1.0972(1.0975) | Error 0.0000(0.0000) Steps 416(415.79) | Grad Norm 0.0502(0.1632) | Total Time 10.00(10.00)\n",
      "Iter 2954 | Time 31.7781(31.9371) | Bit/dim 1.0969(1.0975) | Xent 0.0000(0.0000) | Loss 1.0969(1.0975) | Error 0.0000(0.0000) Steps 416(415.79) | Grad Norm 0.0629(0.1602) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 16.1067, Epoch Time 251.7590(248.2673), Bit/dim 1.0916(best: 1.0914), Xent 0.0000, Loss 1.0916, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2955 | Time 32.1856(31.9445) | Bit/dim 1.0955(1.0975) | Xent 0.0000(0.0000) | Loss 1.0955(1.0975) | Error 0.0000(0.0000) Steps 416(415.80) | Grad Norm 0.0638(0.1573) | Total Time 10.00(10.00)\n",
      "Iter 2956 | Time 32.4707(31.9603) | Bit/dim 1.0924(1.0973) | Xent 0.0000(0.0000) | Loss 1.0924(1.0973) | Error 0.0000(0.0000) Steps 410(415.62) | Grad Norm 0.0559(0.1543) | Total Time 10.00(10.00)\n",
      "Iter 2957 | Time 31.8845(31.9581) | Bit/dim 1.0984(1.0973) | Xent 0.0000(0.0000) | Loss 1.0984(1.0973) | Error 0.0000(0.0000) Steps 416(415.64) | Grad Norm 0.0705(0.1518) | Total Time 10.00(10.00)\n",
      "Iter 2958 | Time 33.8745(32.0156) | Bit/dim 1.0972(1.0973) | Xent 0.0000(0.0000) | Loss 1.0972(1.0973) | Error 0.0000(0.0000) Steps 410(415.47) | Grad Norm 0.0581(0.1490) | Total Time 10.00(10.00)\n",
      "Iter 2959 | Time 33.0818(32.0475) | Bit/dim 1.1025(1.0975) | Xent 0.0000(0.0000) | Loss 1.1025(1.0975) | Error 0.0000(0.0000) Steps 416(415.48) | Grad Norm 0.0536(0.1461) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 32.8147(32.0706) | Bit/dim 1.1016(1.0976) | Xent 0.0000(0.0000) | Loss 1.1016(1.0976) | Error 0.0000(0.0000) Steps 410(415.32) | Grad Norm 0.0548(0.1434) | Total Time 10.00(10.00)\n",
      "Iter 2961 | Time 31.2625(32.0463) | Bit/dim 1.0957(1.0976) | Xent 0.0000(0.0000) | Loss 1.0957(1.0976) | Error 0.0000(0.0000) Steps 416(415.34) | Grad Norm 0.0699(0.1412) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 16.3732, Epoch Time 256.7863(248.5229), Bit/dim 1.0919(best: 1.0914), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2962 | Time 32.1073(32.0481) | Bit/dim 1.0963(1.0975) | Xent 0.0000(0.0000) | Loss 1.0963(1.0975) | Error 0.0000(0.0000) Steps 416(415.36) | Grad Norm 0.1058(0.1401) | Total Time 10.00(10.00)\n",
      "Iter 2963 | Time 31.9403(32.0449) | Bit/dim 1.0962(1.0975) | Xent 0.0000(0.0000) | Loss 1.0962(1.0975) | Error 0.0000(0.0000) Steps 410(415.20) | Grad Norm 0.0668(0.1379) | Total Time 10.00(10.00)\n",
      "Iter 2964 | Time 33.8102(32.0979) | Bit/dim 1.1029(1.0976) | Xent 0.0000(0.0000) | Loss 1.1029(1.0976) | Error 0.0000(0.0000) Steps 416(415.22) | Grad Norm 0.0858(0.1363) | Total Time 10.00(10.00)\n",
      "Iter 2965 | Time 32.7136(32.1163) | Bit/dim 1.0949(1.0976) | Xent 0.0000(0.0000) | Loss 1.0949(1.0976) | Error 0.0000(0.0000) Steps 416(415.24) | Grad Norm 0.0906(0.1350) | Total Time 10.00(10.00)\n",
      "Iter 2966 | Time 32.3341(32.1229) | Bit/dim 1.0995(1.0976) | Xent 0.0000(0.0000) | Loss 1.0995(1.0976) | Error 0.0000(0.0000) Steps 416(415.27) | Grad Norm 0.0827(0.1334) | Total Time 10.00(10.00)\n",
      "Iter 2967 | Time 31.8079(32.1134) | Bit/dim 1.0941(1.0975) | Xent 0.0000(0.0000) | Loss 1.0941(1.0975) | Error 0.0000(0.0000) Steps 428(415.65) | Grad Norm 0.0563(0.1311) | Total Time 10.00(10.00)\n",
      "Iter 2968 | Time 33.4888(32.1547) | Bit/dim 1.0945(1.0974) | Xent 0.0000(0.0000) | Loss 1.0945(1.0974) | Error 0.0000(0.0000) Steps 416(415.66) | Grad Norm 0.0675(0.1292) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 16.2524, Epoch Time 257.4740(248.7914), Bit/dim 1.0910(best: 1.0914), Xent 0.0000, Loss 1.0910, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2969 | Time 32.0854(32.1526) | Bit/dim 1.0963(1.0974) | Xent 0.0000(0.0000) | Loss 1.0963(1.0974) | Error 0.0000(0.0000) Steps 416(415.67) | Grad Norm 0.0770(0.1276) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 31.5248(32.1338) | Bit/dim 1.0934(1.0973) | Xent 0.0000(0.0000) | Loss 1.0934(1.0973) | Error 0.0000(0.0000) Steps 416(415.68) | Grad Norm 0.0554(0.1254) | Total Time 10.00(10.00)\n",
      "Iter 2971 | Time 32.2134(32.1362) | Bit/dim 1.1025(1.0974) | Xent 0.0000(0.0000) | Loss 1.1025(1.0974) | Error 0.0000(0.0000) Steps 416(415.69) | Grad Norm 0.0531(0.1233) | Total Time 10.00(10.00)\n",
      "Iter 2972 | Time 32.7600(32.1549) | Bit/dim 1.0967(1.0974) | Xent 0.0000(0.0000) | Loss 1.0967(1.0974) | Error 0.0000(0.0000) Steps 416(415.70) | Grad Norm 0.0731(0.1218) | Total Time 10.00(10.00)\n",
      "Iter 2973 | Time 31.7371(32.1423) | Bit/dim 1.0951(1.0973) | Xent 0.0000(0.0000) | Loss 1.0951(1.0973) | Error 0.0000(0.0000) Steps 416(415.71) | Grad Norm 0.0514(0.1197) | Total Time 10.00(10.00)\n",
      "Iter 2974 | Time 32.3618(32.1489) | Bit/dim 1.0952(1.0973) | Xent 0.0000(0.0000) | Loss 1.0952(1.0973) | Error 0.0000(0.0000) Steps 422(415.90) | Grad Norm 0.0765(0.1184) | Total Time 10.00(10.00)\n",
      "Iter 2975 | Time 32.0375(32.1456) | Bit/dim 1.0992(1.0973) | Xent 0.0000(0.0000) | Loss 1.0992(1.0973) | Error 0.0000(0.0000) Steps 428(416.26) | Grad Norm 0.0703(0.1169) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 16.2124, Epoch Time 253.7380(248.9398), Bit/dim 1.0920(best: 1.0910), Xent 0.0000, Loss 1.0920, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2976 | Time 32.4935(32.1560) | Bit/dim 1.0970(1.0973) | Xent 0.0000(0.0000) | Loss 1.0970(1.0973) | Error 0.0000(0.0000) Steps 416(416.25) | Grad Norm 0.0873(0.1160) | Total Time 10.00(10.00)\n",
      "Iter 2977 | Time 33.5347(32.1974) | Bit/dim 1.0949(1.0972) | Xent 0.0000(0.0000) | Loss 1.0949(1.0972) | Error 0.0000(0.0000) Steps 416(416.24) | Grad Norm 0.1087(0.1158) | Total Time 10.00(10.00)\n",
      "Iter 2978 | Time 32.3129(32.2009) | Bit/dim 1.0979(1.0973) | Xent 0.0000(0.0000) | Loss 1.0979(1.0973) | Error 0.0000(0.0000) Steps 422(416.42) | Grad Norm 0.0546(0.1140) | Total Time 10.00(10.00)\n",
      "Iter 2979 | Time 32.2395(32.2020) | Bit/dim 1.1014(1.0974) | Xent 0.0000(0.0000) | Loss 1.1014(1.0974) | Error 0.0000(0.0000) Steps 416(416.40) | Grad Norm 0.0525(0.1121) | Total Time 10.00(10.00)\n",
      "Iter 2980 | Time 32.8147(32.2204) | Bit/dim 1.0965(1.0974) | Xent 0.0000(0.0000) | Loss 1.0965(1.0974) | Error 0.0000(0.0000) Steps 410(416.21) | Grad Norm 0.0556(0.1104) | Total Time 10.00(10.00)\n",
      "Iter 2981 | Time 32.8782(32.2401) | Bit/dim 1.0937(1.0973) | Xent 0.0000(0.0000) | Loss 1.0937(1.0973) | Error 0.0000(0.0000) Steps 416(416.21) | Grad Norm 0.0665(0.1091) | Total Time 10.00(10.00)\n",
      "Iter 2982 | Time 32.4209(32.2455) | Bit/dim 1.0934(1.0971) | Xent 0.0000(0.0000) | Loss 1.0934(1.0971) | Error 0.0000(0.0000) Steps 416(416.20) | Grad Norm 0.0804(0.1083) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 16.0367, Epoch Time 257.5658(249.1986), Bit/dim 1.0919(best: 1.0910), Xent 0.0000, Loss 1.0919, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2983 | Time 29.8865(32.1748) | Bit/dim 1.1012(1.0973) | Xent 0.0000(0.0000) | Loss 1.1012(1.0973) | Error 0.0000(0.0000) Steps 416(416.19) | Grad Norm 0.0539(0.1066) | Total Time 10.00(10.00)\n",
      "Iter 2984 | Time 31.9344(32.1676) | Bit/dim 1.0977(1.0973) | Xent 0.0000(0.0000) | Loss 1.0977(1.0973) | Error 0.0000(0.0000) Steps 416(416.19) | Grad Norm 0.0755(0.1057) | Total Time 10.00(10.00)\n",
      "Iter 2985 | Time 32.6850(32.1831) | Bit/dim 1.0964(1.0972) | Xent 0.0000(0.0000) | Loss 1.0964(1.0972) | Error 0.0000(0.0000) Steps 416(416.18) | Grad Norm 0.0572(0.1042) | Total Time 10.00(10.00)\n",
      "Iter 2986 | Time 31.9624(32.1765) | Bit/dim 1.0988(1.0973) | Xent 0.0000(0.0000) | Loss 1.0988(1.0973) | Error 0.0000(0.0000) Steps 416(416.18) | Grad Norm 0.0547(0.1027) | Total Time 10.00(10.00)\n",
      "Iter 2987 | Time 33.3790(32.2125) | Bit/dim 1.0952(1.0972) | Xent 0.0000(0.0000) | Loss 1.0952(1.0972) | Error 0.0000(0.0000) Steps 428(416.53) | Grad Norm 0.0528(0.1013) | Total Time 10.00(10.00)\n",
      "Iter 2988 | Time 32.7342(32.2282) | Bit/dim 1.0927(1.0971) | Xent 0.0000(0.0000) | Loss 1.0927(1.0971) | Error 0.0000(0.0000) Steps 416(416.52) | Grad Norm 0.0694(0.1003) | Total Time 10.00(10.00)\n",
      "Iter 2989 | Time 33.6784(32.2717) | Bit/dim 1.0995(1.0972) | Xent 0.0000(0.0000) | Loss 1.0995(1.0972) | Error 0.0000(0.0000) Steps 410(416.32) | Grad Norm 0.1088(0.1006) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 16.2216, Epoch Time 255.2379(249.3798), Bit/dim 1.0916(best: 1.0910), Xent 0.0000, Loss 1.0916, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2990 | Time 32.4450(32.2769) | Bit/dim 1.1005(1.0973) | Xent 0.0000(0.0000) | Loss 1.1005(1.0973) | Error 0.0000(0.0000) Steps 416(416.31) | Grad Norm 0.0721(0.0997) | Total Time 10.00(10.00)\n",
      "Iter 2991 | Time 34.1802(32.3340) | Bit/dim 1.0932(1.0971) | Xent 0.0000(0.0000) | Loss 1.0932(1.0971) | Error 0.0000(0.0000) Steps 416(416.30) | Grad Norm 0.0954(0.0996) | Total Time 10.00(10.00)\n",
      "Iter 2992 | Time 32.1367(32.3281) | Bit/dim 1.0984(1.0972) | Xent 0.0000(0.0000) | Loss 1.0984(1.0972) | Error 0.0000(0.0000) Steps 428(416.65) | Grad Norm 0.0744(0.0988) | Total Time 10.00(10.00)\n",
      "Iter 2993 | Time 32.8597(32.3440) | Bit/dim 1.1018(1.0973) | Xent 0.0000(0.0000) | Loss 1.1018(1.0973) | Error 0.0000(0.0000) Steps 416(416.63) | Grad Norm 0.0564(0.0975) | Total Time 10.00(10.00)\n",
      "Iter 2994 | Time 32.0970(32.3366) | Bit/dim 1.0939(1.0972) | Xent 0.0000(0.0000) | Loss 1.0939(1.0972) | Error 0.0000(0.0000) Steps 416(416.61) | Grad Norm 0.0890(0.0973) | Total Time 10.00(10.00)\n",
      "Iter 2995 | Time 31.9797(32.3259) | Bit/dim 1.0978(1.0972) | Xent 0.0000(0.0000) | Loss 1.0978(1.0972) | Error 0.0000(0.0000) Steps 416(416.60) | Grad Norm 0.0813(0.0968) | Total Time 10.00(10.00)\n",
      "Iter 2996 | Time 31.8723(32.3123) | Bit/dim 1.0924(1.0971) | Xent 0.0000(0.0000) | Loss 1.0924(1.0971) | Error 0.0000(0.0000) Steps 416(416.58) | Grad Norm 0.0504(0.0954) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 16.4188, Epoch Time 256.8577(249.6041), Bit/dim 1.0917(best: 1.0910), Xent 0.0000, Loss 1.0917, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2997 | Time 31.8525(32.2985) | Bit/dim 1.0994(1.0972) | Xent 0.0000(0.0000) | Loss 1.0994(1.0972) | Error 0.0000(0.0000) Steps 416(416.56) | Grad Norm 0.0719(0.0947) | Total Time 10.00(10.00)\n",
      "Iter 2998 | Time 33.3023(32.3286) | Bit/dim 1.0929(1.0970) | Xent 0.0000(0.0000) | Loss 1.0929(1.0970) | Error 0.0000(0.0000) Steps 428(416.90) | Grad Norm 0.0588(0.0936) | Total Time 10.00(10.00)\n",
      "Iter 2999 | Time 33.2166(32.3553) | Bit/dim 1.0969(1.0970) | Xent 0.0000(0.0000) | Loss 1.0969(1.0970) | Error 0.0000(0.0000) Steps 416(416.88) | Grad Norm 0.0596(0.0926) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 31.8553(32.3403) | Bit/dim 1.0939(1.0969) | Xent 0.0000(0.0000) | Loss 1.0939(1.0969) | Error 0.0000(0.0000) Steps 410(416.67) | Grad Norm 0.0699(0.0919) | Total Time 10.00(10.00)\n",
      "Iter 3001 | Time 33.0912(32.3628) | Bit/dim 1.0979(1.0970) | Xent 0.0000(0.0000) | Loss 1.0979(1.0970) | Error 0.0000(0.0000) Steps 428(417.01) | Grad Norm 0.0578(0.0909) | Total Time 10.00(10.00)\n",
      "Iter 3002 | Time 32.1541(32.3565) | Bit/dim 1.0944(1.0969) | Xent 0.0000(0.0000) | Loss 1.0944(1.0969) | Error 0.0000(0.0000) Steps 416(416.98) | Grad Norm 0.0850(0.0907) | Total Time 10.00(10.00)\n",
      "Iter 3003 | Time 34.0078(32.4061) | Bit/dim 1.1005(1.0970) | Xent 0.0000(0.0000) | Loss 1.1005(1.0970) | Error 0.0000(0.0000) Steps 416(416.95) | Grad Norm 0.1202(0.0916) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 16.4699, Epoch Time 259.0435(249.8873), Bit/dim 1.0912(best: 1.0910), Xent 0.0000, Loss 1.0912, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3004 | Time 33.4371(32.4370) | Bit/dim 1.0963(1.0970) | Xent 0.0000(0.0000) | Loss 1.0963(1.0970) | Error 0.0000(0.0000) Steps 428(417.28) | Grad Norm 0.1164(0.0924) | Total Time 10.00(10.00)\n",
      "Iter 3005 | Time 32.9080(32.4511) | Bit/dim 1.0961(1.0969) | Xent 0.0000(0.0000) | Loss 1.0961(1.0969) | Error 0.0000(0.0000) Steps 416(417.24) | Grad Norm 0.0487(0.0910) | Total Time 10.00(10.00)\n",
      "Iter 3006 | Time 32.3066(32.4468) | Bit/dim 1.0935(1.0968) | Xent 0.0000(0.0000) | Loss 1.0935(1.0968) | Error 0.0000(0.0000) Steps 416(417.21) | Grad Norm 0.0678(0.0903) | Total Time 10.00(10.00)\n",
      "Iter 3007 | Time 32.8579(32.4591) | Bit/dim 1.0966(1.0968) | Xent 0.0000(0.0000) | Loss 1.0966(1.0968) | Error 0.0000(0.0000) Steps 434(417.71) | Grad Norm 0.1010(0.0907) | Total Time 10.00(10.00)\n",
      "Iter 3008 | Time 33.2358(32.4824) | Bit/dim 1.0976(1.0969) | Xent 0.0000(0.0000) | Loss 1.0976(1.0969) | Error 0.0000(0.0000) Steps 428(418.02) | Grad Norm 0.1603(0.0928) | Total Time 10.00(10.00)\n",
      "Iter 3009 | Time 32.3490(32.4784) | Bit/dim 1.0977(1.0969) | Xent 0.0000(0.0000) | Loss 1.0977(1.0969) | Error 0.0000(0.0000) Steps 422(418.14) | Grad Norm 0.0545(0.0916) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 29.7241(32.3958) | Bit/dim 1.0979(1.0969) | Xent 0.0000(0.0000) | Loss 1.0979(1.0969) | Error 0.0000(0.0000) Steps 416(418.07) | Grad Norm 0.0543(0.0905) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 16.2375, Epoch Time 255.7353(250.0627), Bit/dim 1.0911(best: 1.0910), Xent 0.0000, Loss 1.0911, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3011 | Time 32.0104(32.3842) | Bit/dim 1.0948(1.0968) | Xent 0.0000(0.0000) | Loss 1.0948(1.0968) | Error 0.0000(0.0000) Steps 416(418.01) | Grad Norm 0.0946(0.0906) | Total Time 10.00(10.00)\n",
      "Iter 3012 | Time 32.5120(32.3881) | Bit/dim 1.0941(1.0968) | Xent 0.0000(0.0000) | Loss 1.0941(1.0968) | Error 0.0000(0.0000) Steps 416(417.95) | Grad Norm 0.1793(0.0933) | Total Time 10.00(10.00)\n",
      "Iter 3013 | Time 31.9278(32.3743) | Bit/dim 1.0990(1.0968) | Xent 0.0000(0.0000) | Loss 1.0990(1.0968) | Error 0.0000(0.0000) Steps 416(417.89) | Grad Norm 0.0488(0.0919) | Total Time 10.00(10.00)\n",
      "Iter 3014 | Time 32.5200(32.3786) | Bit/dim 1.0924(1.0967) | Xent 0.0000(0.0000) | Loss 1.0924(1.0967) | Error 0.0000(0.0000) Steps 416(417.84) | Grad Norm 0.0636(0.0911) | Total Time 10.00(10.00)\n",
      "Iter 3015 | Time 33.2940(32.4061) | Bit/dim 1.0980(1.0967) | Xent 0.0000(0.0000) | Loss 1.0980(1.0967) | Error 0.0000(0.0000) Steps 422(417.96) | Grad Norm 0.0988(0.0913) | Total Time 10.00(10.00)\n",
      "Iter 3016 | Time 32.0297(32.3948) | Bit/dim 1.0995(1.0968) | Xent 0.0000(0.0000) | Loss 1.0995(1.0968) | Error 0.0000(0.0000) Steps 416(417.90) | Grad Norm 0.0884(0.0912) | Total Time 10.00(10.00)\n",
      "Iter 3017 | Time 32.3646(32.3939) | Bit/dim 1.0959(1.0968) | Xent 0.0000(0.0000) | Loss 1.0959(1.0968) | Error 0.0000(0.0000) Steps 416(417.84) | Grad Norm 0.0536(0.0901) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 16.1681, Epoch Time 255.7747(250.2341), Bit/dim 1.0913(best: 1.0910), Xent 0.0000, Loss 1.0913, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3018 | Time 32.8067(32.4063) | Bit/dim 1.0948(1.0967) | Xent 0.0000(0.0000) | Loss 1.0948(1.0967) | Error 0.0000(0.0000) Steps 410(417.61) | Grad Norm 0.0815(0.0898) | Total Time 10.00(10.00)\n",
      "Iter 3019 | Time 31.9668(32.3931) | Bit/dim 1.0992(1.0968) | Xent 0.0000(0.0000) | Loss 1.0992(1.0968) | Error 0.0000(0.0000) Steps 416(417.56) | Grad Norm 0.0596(0.0889) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 32.3892(32.3930) | Bit/dim 1.1004(1.0969) | Xent 0.0000(0.0000) | Loss 1.1004(1.0969) | Error 0.0000(0.0000) Steps 416(417.51) | Grad Norm 0.0644(0.0882) | Total Time 10.00(10.00)\n",
      "Iter 3021 | Time 32.7218(32.4028) | Bit/dim 1.0966(1.0969) | Xent 0.0000(0.0000) | Loss 1.0966(1.0969) | Error 0.0000(0.0000) Steps 410(417.29) | Grad Norm 0.0875(0.0882) | Total Time 10.00(10.00)\n",
      "Iter 3022 | Time 32.5326(32.4067) | Bit/dim 1.0946(1.0968) | Xent 0.0000(0.0000) | Loss 1.0946(1.0968) | Error 0.0000(0.0000) Steps 410(417.07) | Grad Norm 0.0883(0.0882) | Total Time 10.00(10.00)\n",
      "Iter 3023 | Time 34.2453(32.4619) | Bit/dim 1.0958(1.0968) | Xent 0.0000(0.0000) | Loss 1.0958(1.0968) | Error 0.0000(0.0000) Steps 410(416.86) | Grad Norm 0.0603(0.0873) | Total Time 10.00(10.00)\n",
      "Iter 3024 | Time 32.2273(32.4548) | Bit/dim 1.0971(1.0968) | Xent 0.0000(0.0000) | Loss 1.0971(1.0968) | Error 0.0000(0.0000) Steps 416(416.83) | Grad Norm 0.0504(0.0862) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 16.4533, Epoch Time 258.3075(250.4763), Bit/dim 1.0911(best: 1.0910), Xent 0.0000, Loss 1.0911, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3025 | Time 30.6913(32.4019) | Bit/dim 1.0939(1.0967) | Xent 0.0000(0.0000) | Loss 1.0939(1.0967) | Error 0.0000(0.0000) Steps 416(416.81) | Grad Norm 0.1142(0.0871) | Total Time 10.00(10.00)\n",
      "Iter 3026 | Time 32.3502(32.4004) | Bit/dim 1.0959(1.0967) | Xent 0.0000(0.0000) | Loss 1.0959(1.0967) | Error 0.0000(0.0000) Steps 428(417.14) | Grad Norm 0.0595(0.0863) | Total Time 10.00(10.00)\n",
      "Iter 3027 | Time 31.9126(32.3858) | Bit/dim 1.0977(1.0967) | Xent 0.0000(0.0000) | Loss 1.0977(1.0967) | Error 0.0000(0.0000) Steps 416(417.11) | Grad Norm 0.0721(0.0858) | Total Time 10.00(10.00)\n",
      "Iter 3028 | Time 33.0920(32.4069) | Bit/dim 1.0983(1.0968) | Xent 0.0000(0.0000) | Loss 1.0983(1.0968) | Error 0.0000(0.0000) Steps 416(417.08) | Grad Norm 0.0767(0.0856) | Total Time 10.00(10.00)\n",
      "Iter 3029 | Time 33.7748(32.4480) | Bit/dim 1.0994(1.0969) | Xent 0.0000(0.0000) | Loss 1.0994(1.0969) | Error 0.0000(0.0000) Steps 428(417.40) | Grad Norm 0.0605(0.0848) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 32.4659(32.4485) | Bit/dim 1.0978(1.0969) | Xent 0.0000(0.0000) | Loss 1.0978(1.0969) | Error 0.0000(0.0000) Steps 428(417.72) | Grad Norm 0.0729(0.0844) | Total Time 10.00(10.00)\n",
      "Iter 3031 | Time 32.4512(32.4486) | Bit/dim 1.0959(1.0969) | Xent 0.0000(0.0000) | Loss 1.0959(1.0969) | Error 0.0000(0.0000) Steps 416(417.67) | Grad Norm 0.0731(0.0841) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 16.3527, Epoch Time 255.9236(250.6397), Bit/dim 1.0916(best: 1.0910), Xent 0.0000, Loss 1.0916, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3032 | Time 33.1987(32.4711) | Bit/dim 1.1005(1.0970) | Xent 0.0000(0.0000) | Loss 1.1005(1.0970) | Error 0.0000(0.0000) Steps 428(417.98) | Grad Norm 0.0941(0.0844) | Total Time 10.00(10.00)\n",
      "Iter 3033 | Time 32.0178(32.4575) | Bit/dim 1.0940(1.0969) | Xent 0.0000(0.0000) | Loss 1.0940(1.0969) | Error 0.0000(0.0000) Steps 416(417.92) | Grad Norm 0.0882(0.0845) | Total Time 10.00(10.00)\n",
      "Iter 3034 | Time 33.4796(32.4882) | Bit/dim 1.0992(1.0970) | Xent 0.0000(0.0000) | Loss 1.0992(1.0970) | Error 0.0000(0.0000) Steps 416(417.86) | Grad Norm 0.0743(0.0842) | Total Time 10.00(10.00)\n",
      "Iter 3035 | Time 33.1361(32.5076) | Bit/dim 1.0921(1.0968) | Xent 0.0000(0.0000) | Loss 1.0921(1.0968) | Error 0.0000(0.0000) Steps 428(418.17) | Grad Norm 0.0914(0.0844) | Total Time 10.00(10.00)\n",
      "Iter 3036 | Time 32.0092(32.4926) | Bit/dim 1.0949(1.0967) | Xent 0.0000(0.0000) | Loss 1.0949(1.0967) | Error 0.0000(0.0000) Steps 416(418.10) | Grad Norm 0.0724(0.0841) | Total Time 10.00(10.00)\n",
      "Iter 3037 | Time 32.5995(32.4959) | Bit/dim 1.0985(1.0968) | Xent 0.0000(0.0000) | Loss 1.0985(1.0968) | Error 0.0000(0.0000) Steps 428(418.40) | Grad Norm 0.0540(0.0832) | Total Time 10.00(10.00)\n",
      "Iter 3038 | Time 32.8379(32.5061) | Bit/dim 1.0996(1.0969) | Xent 0.0000(0.0000) | Loss 1.0996(1.0969) | Error 0.0000(0.0000) Steps 416(418.33) | Grad Norm 0.0497(0.0822) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 15.9823, Epoch Time 258.1571(250.8652), Bit/dim 1.0910(best: 1.0910), Xent 0.0000, Loss 1.0910, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3039 | Time 32.3287(32.5008) | Bit/dim 1.0970(1.0969) | Xent 0.0000(0.0000) | Loss 1.0970(1.0969) | Error 0.0000(0.0000) Steps 416(418.26) | Grad Norm 0.0539(0.0813) | Total Time 10.00(10.00)\n",
      "Iter 3040 | Time 33.6762(32.5361) | Bit/dim 1.1032(1.0971) | Xent 0.0000(0.0000) | Loss 1.1032(1.0971) | Error 0.0000(0.0000) Steps 416(418.19) | Grad Norm 0.0689(0.0809) | Total Time 10.00(10.00)\n",
      "Iter 3041 | Time 33.7684(32.5730) | Bit/dim 1.0903(1.0969) | Xent 0.0000(0.0000) | Loss 1.0903(1.0969) | Error 0.0000(0.0000) Steps 410(417.94) | Grad Norm 0.0792(0.0809) | Total Time 10.00(10.00)\n",
      "Iter 3042 | Time 31.9802(32.5552) | Bit/dim 1.0962(1.0969) | Xent 0.0000(0.0000) | Loss 1.0962(1.0969) | Error 0.0000(0.0000) Steps 410(417.71) | Grad Norm 0.1082(0.0817) | Total Time 10.00(10.00)\n",
      "Iter 3043 | Time 31.8418(32.5338) | Bit/dim 1.0961(1.0968) | Xent 0.0000(0.0000) | Loss 1.0961(1.0968) | Error 0.0000(0.0000) Steps 416(417.65) | Grad Norm 0.0650(0.0812) | Total Time 10.00(10.00)\n",
      "Iter 3044 | Time 32.9575(32.5465) | Bit/dim 1.0996(1.0969) | Xent 0.0000(0.0000) | Loss 1.0996(1.0969) | Error 0.0000(0.0000) Steps 428(417.96) | Grad Norm 0.0852(0.0813) | Total Time 10.00(10.00)\n",
      "Iter 3045 | Time 32.1259(32.5339) | Bit/dim 1.0965(1.0969) | Xent 0.0000(0.0000) | Loss 1.0965(1.0969) | Error 0.0000(0.0000) Steps 422(418.09) | Grad Norm 0.0557(0.0806) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 16.2369, Epoch Time 257.8312(251.0742), Bit/dim 1.0915(best: 1.0910), Xent 0.0000, Loss 1.0915, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3046 | Time 32.2283(32.5248) | Bit/dim 1.0977(1.0969) | Xent 0.0000(0.0000) | Loss 1.0977(1.0969) | Error 0.0000(0.0000) Steps 416(418.02) | Grad Norm 0.0953(0.0810) | Total Time 10.00(10.00)\n",
      "Iter 3047 | Time 32.6588(32.5288) | Bit/dim 1.0990(1.0970) | Xent 0.0000(0.0000) | Loss 1.0990(1.0970) | Error 0.0000(0.0000) Steps 416(417.96) | Grad Norm 0.0712(0.0807) | Total Time 10.00(10.00)\n",
      "Iter 3048 | Time 33.9074(32.5701) | Bit/dim 1.0974(1.0970) | Xent 0.0000(0.0000) | Loss 1.0974(1.0970) | Error 0.0000(0.0000) Steps 416(417.90) | Grad Norm 0.0547(0.0799) | Total Time 10.00(10.00)\n",
      "Iter 3049 | Time 30.2091(32.4993) | Bit/dim 1.0967(1.0970) | Xent 0.0000(0.0000) | Loss 1.0967(1.0970) | Error 0.0000(0.0000) Steps 416(417.85) | Grad Norm 0.0550(0.0792) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 32.0796(32.4867) | Bit/dim 1.0952(1.0969) | Xent 0.0000(0.0000) | Loss 1.0952(1.0969) | Error 0.0000(0.0000) Steps 422(417.97) | Grad Norm 0.0723(0.0790) | Total Time 10.00(10.00)\n",
      "Iter 3051 | Time 33.1527(32.5067) | Bit/dim 1.0926(1.0968) | Xent 0.0000(0.0000) | Loss 1.0926(1.0968) | Error 0.0000(0.0000) Steps 416(417.91) | Grad Norm 0.0674(0.0786) | Total Time 10.00(10.00)\n",
      "Iter 3052 | Time 32.2560(32.4992) | Bit/dim 1.0956(1.0968) | Xent 0.0000(0.0000) | Loss 1.0956(1.0968) | Error 0.0000(0.0000) Steps 416(417.85) | Grad Norm 0.1029(0.0794) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 16.2824, Epoch Time 255.7567(251.2147), Bit/dim 1.0909(best: 1.0910), Xent 0.0000, Loss 1.0909, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3053 | Time 32.7021(32.5053) | Bit/dim 1.0986(1.0968) | Xent 0.0000(0.0000) | Loss 1.0986(1.0968) | Error 0.0000(0.0000) Steps 428(418.16) | Grad Norm 0.0583(0.0787) | Total Time 10.00(10.00)\n",
      "Iter 3054 | Time 33.6800(32.5405) | Bit/dim 1.0982(1.0969) | Xent 0.0000(0.0000) | Loss 1.0982(1.0969) | Error 0.0000(0.0000) Steps 428(418.45) | Grad Norm 0.0506(0.0779) | Total Time 10.00(10.00)\n",
      "Iter 3055 | Time 33.2435(32.5616) | Bit/dim 1.0937(1.0968) | Xent 0.0000(0.0000) | Loss 1.0937(1.0968) | Error 0.0000(0.0000) Steps 416(418.38) | Grad Norm 0.0557(0.0772) | Total Time 10.00(10.00)\n",
      "Iter 3056 | Time 32.3109(32.5541) | Bit/dim 1.0955(1.0967) | Xent 0.0000(0.0000) | Loss 1.0955(1.0967) | Error 0.0000(0.0000) Steps 416(418.31) | Grad Norm 0.0535(0.0765) | Total Time 10.00(10.00)\n",
      "Iter 3057 | Time 33.7655(32.5904) | Bit/dim 1.0952(1.0967) | Xent 0.0000(0.0000) | Loss 1.0952(1.0967) | Error 0.0000(0.0000) Steps 422(418.42) | Grad Norm 0.0638(0.0761) | Total Time 10.00(10.00)\n",
      "Iter 3058 | Time 32.3394(32.5829) | Bit/dim 1.1003(1.0968) | Xent 0.0000(0.0000) | Loss 1.1003(1.0968) | Error 0.0000(0.0000) Steps 416(418.35) | Grad Norm 0.0624(0.0757) | Total Time 10.00(10.00)\n",
      "Iter 3059 | Time 33.7087(32.6167) | Bit/dim 1.0958(1.0968) | Xent 0.0000(0.0000) | Loss 1.0958(1.0968) | Error 0.0000(0.0000) Steps 428(418.64) | Grad Norm 0.0828(0.0759) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 16.3046, Epoch Time 260.8272(251.5031), Bit/dim 1.0909(best: 1.0909), Xent 0.0000, Loss 1.0909, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3060 | Time 32.9305(32.6261) | Bit/dim 1.1026(1.0969) | Xent 0.0000(0.0000) | Loss 1.1026(1.0969) | Error 0.0000(0.0000) Steps 416(418.56) | Grad Norm 0.0555(0.0753) | Total Time 10.00(10.00)\n",
      "Iter 3061 | Time 31.9855(32.6069) | Bit/dim 1.0965(1.0969) | Xent 0.0000(0.0000) | Loss 1.0965(1.0969) | Error 0.0000(0.0000) Steps 428(418.84) | Grad Norm 0.0657(0.0750) | Total Time 10.00(10.00)\n",
      "Iter 3062 | Time 32.3828(32.6001) | Bit/dim 1.0924(1.0968) | Xent 0.0000(0.0000) | Loss 1.0924(1.0968) | Error 0.0000(0.0000) Steps 428(419.12) | Grad Norm 0.0650(0.0747) | Total Time 10.00(10.00)\n",
      "Iter 3063 | Time 32.4209(32.5948) | Bit/dim 1.0964(1.0968) | Xent 0.0000(0.0000) | Loss 1.0964(1.0968) | Error 0.0000(0.0000) Steps 428(419.38) | Grad Norm 0.0611(0.0743) | Total Time 10.00(10.00)\n",
      "Iter 3064 | Time 32.8983(32.6039) | Bit/dim 1.0948(1.0967) | Xent 0.0000(0.0000) | Loss 1.0948(1.0967) | Error 0.0000(0.0000) Steps 416(419.28) | Grad Norm 0.0571(0.0738) | Total Time 10.00(10.00)\n",
      "Iter 3065 | Time 32.4710(32.5999) | Bit/dim 1.0954(1.0967) | Xent 0.0000(0.0000) | Loss 1.0954(1.0967) | Error 0.0000(0.0000) Steps 416(419.18) | Grad Norm 0.0503(0.0731) | Total Time 10.00(10.00)\n",
      "Iter 3066 | Time 31.4903(32.5666) | Bit/dim 1.0981(1.0967) | Xent 0.0000(0.0000) | Loss 1.0981(1.0967) | Error 0.0000(0.0000) Steps 410(418.91) | Grad Norm 0.0741(0.0731) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 16.4256, Epoch Time 256.0232(251.6387), Bit/dim 1.0915(best: 1.0909), Xent 0.0000, Loss 1.0915, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3067 | Time 32.6495(32.5691) | Bit/dim 1.0983(1.0968) | Xent 0.0000(0.0000) | Loss 1.0983(1.0968) | Error 0.0000(0.0000) Steps 410(418.64) | Grad Norm 0.0637(0.0728) | Total Time 10.00(10.00)\n",
      "Iter 3068 | Time 31.7500(32.5445) | Bit/dim 1.0931(1.0967) | Xent 0.0000(0.0000) | Loss 1.0931(1.0967) | Error 0.0000(0.0000) Steps 428(418.92) | Grad Norm 0.0524(0.0722) | Total Time 10.00(10.00)\n",
      "Iter 3069 | Time 32.3979(32.5401) | Bit/dim 1.0934(1.0966) | Xent 0.0000(0.0000) | Loss 1.0934(1.0966) | Error 0.0000(0.0000) Steps 416(418.83) | Grad Norm 0.0968(0.0730) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 33.5530(32.5705) | Bit/dim 1.1024(1.0967) | Xent 0.0000(0.0000) | Loss 1.1024(1.0967) | Error 0.0000(0.0000) Steps 428(419.11) | Grad Norm 0.0576(0.0725) | Total Time 10.00(10.00)\n",
      "Iter 3071 | Time 31.8542(32.5490) | Bit/dim 1.1002(1.0968) | Xent 0.0000(0.0000) | Loss 1.1002(1.0968) | Error 0.0000(0.0000) Steps 422(419.19) | Grad Norm 0.0810(0.0728) | Total Time 10.00(10.00)\n",
      "Iter 3072 | Time 32.6443(32.5519) | Bit/dim 1.0942(1.0968) | Xent 0.0000(0.0000) | Loss 1.0942(1.0968) | Error 0.0000(0.0000) Steps 416(419.10) | Grad Norm 0.0978(0.0735) | Total Time 10.00(10.00)\n",
      "Iter 3073 | Time 31.8498(32.5308) | Bit/dim 1.0973(1.0968) | Xent 0.0000(0.0000) | Loss 1.0973(1.0968) | Error 0.0000(0.0000) Steps 416(419.01) | Grad Norm 0.0648(0.0732) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 16.3541, Epoch Time 255.7960(251.7634), Bit/dim 1.0914(best: 1.0909), Xent 0.0000, Loss 1.0914, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3074 | Time 32.3401(32.5251) | Bit/dim 1.0965(1.0968) | Xent 0.0000(0.0000) | Loss 1.0965(1.0968) | Error 0.0000(0.0000) Steps 416(418.92) | Grad Norm 0.0721(0.0732) | Total Time 10.00(10.00)\n",
      "Iter 3075 | Time 32.5231(32.5250) | Bit/dim 1.0958(1.0967) | Xent 0.0000(0.0000) | Loss 1.0958(1.0967) | Error 0.0000(0.0000) Steps 422(419.01) | Grad Norm 0.0589(0.0728) | Total Time 10.00(10.00)\n",
      "Iter 3076 | Time 31.8788(32.5056) | Bit/dim 1.0980(1.0968) | Xent 0.0000(0.0000) | Loss 1.0980(1.0968) | Error 0.0000(0.0000) Steps 416(418.92) | Grad Norm 0.0510(0.0721) | Total Time 10.00(10.00)\n",
      "Iter 3077 | Time 29.8517(32.4260) | Bit/dim 1.0967(1.0968) | Xent 0.0000(0.0000) | Loss 1.0967(1.0968) | Error 0.0000(0.0000) Steps 416(418.83) | Grad Norm 0.0581(0.0717) | Total Time 10.00(10.00)\n",
      "Iter 3078 | Time 32.8914(32.4400) | Bit/dim 1.0950(1.0967) | Xent 0.0000(0.0000) | Loss 1.0950(1.0967) | Error 0.0000(0.0000) Steps 416(418.75) | Grad Norm 0.0636(0.0715) | Total Time 10.00(10.00)\n",
      "Iter 3079 | Time 32.0675(32.4288) | Bit/dim 1.0968(1.0967) | Xent 0.0000(0.0000) | Loss 1.0968(1.0967) | Error 0.0000(0.0000) Steps 416(418.66) | Grad Norm 0.1118(0.0727) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 32.8978(32.4429) | Bit/dim 1.0969(1.0967) | Xent 0.0000(0.0000) | Loss 1.0969(1.0967) | Error 0.0000(0.0000) Steps 416(418.58) | Grad Norm 0.0581(0.0722) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 16.2509, Epoch Time 253.5276(251.8163), Bit/dim 1.0908(best: 1.0909), Xent 0.0000, Loss 1.0908, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3081 | Time 33.4429(32.4729) | Bit/dim 1.0919(1.0966) | Xent 0.0000(0.0000) | Loss 1.0919(1.0966) | Error 0.0000(0.0000) Steps 428(418.87) | Grad Norm 0.0750(0.0723) | Total Time 10.00(10.00)\n",
      "Iter 3082 | Time 32.3208(32.4683) | Bit/dim 1.0953(1.0966) | Xent 0.0000(0.0000) | Loss 1.0953(1.0966) | Error 0.0000(0.0000) Steps 416(418.78) | Grad Norm 0.1336(0.0742) | Total Time 10.00(10.00)\n",
      "Iter 3083 | Time 34.3007(32.5233) | Bit/dim 1.1033(1.0968) | Xent 0.0000(0.0000) | Loss 1.1033(1.0968) | Error 0.0000(0.0000) Steps 428(419.06) | Grad Norm 0.0640(0.0739) | Total Time 10.00(10.00)\n",
      "Iter 3084 | Time 32.8245(32.5323) | Bit/dim 1.0983(1.0968) | Xent 0.0000(0.0000) | Loss 1.0983(1.0968) | Error 0.0000(0.0000) Steps 416(418.96) | Grad Norm 0.0514(0.0732) | Total Time 10.00(10.00)\n",
      "Iter 3085 | Time 32.0967(32.5193) | Bit/dim 1.0937(1.0967) | Xent 0.0000(0.0000) | Loss 1.0937(1.0967) | Error 0.0000(0.0000) Steps 416(418.88) | Grad Norm 0.0532(0.0726) | Total Time 10.00(10.00)\n",
      "Iter 3086 | Time 33.9141(32.5611) | Bit/dim 1.0953(1.0967) | Xent 0.0000(0.0000) | Loss 1.0953(1.0967) | Error 0.0000(0.0000) Steps 416(418.79) | Grad Norm 0.0495(0.0719) | Total Time 10.00(10.00)\n",
      "Iter 3087 | Time 32.1456(32.5486) | Bit/dim 1.0977(1.0967) | Xent 0.0000(0.0000) | Loss 1.0977(1.0967) | Error 0.0000(0.0000) Steps 416(418.71) | Grad Norm 0.0705(0.0718) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 16.2560, Epoch Time 260.2492(252.0693), Bit/dim 1.0906(best: 1.0908), Xent 0.0000, Loss 1.0906, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3088 | Time 33.2237(32.5689) | Bit/dim 1.1024(1.0969) | Xent 0.0000(0.0000) | Loss 1.1024(1.0969) | Error 0.0000(0.0000) Steps 416(418.62) | Grad Norm 0.0646(0.0716) | Total Time 10.00(10.00)\n",
      "Iter 3089 | Time 32.2394(32.5590) | Bit/dim 1.0984(1.0969) | Xent 0.0000(0.0000) | Loss 1.0984(1.0969) | Error 0.0000(0.0000) Steps 416(418.55) | Grad Norm 0.0712(0.0716) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 32.6758(32.5625) | Bit/dim 1.0989(1.0970) | Xent 0.0000(0.0000) | Loss 1.0989(1.0970) | Error 0.0000(0.0000) Steps 428(418.83) | Grad Norm 0.0602(0.0713) | Total Time 10.00(10.00)\n",
      "Iter 3091 | Time 32.2346(32.5527) | Bit/dim 1.0917(1.0968) | Xent 0.0000(0.0000) | Loss 1.0917(1.0968) | Error 0.0000(0.0000) Steps 410(418.56) | Grad Norm 0.0631(0.0710) | Total Time 10.00(10.00)\n",
      "Iter 3092 | Time 32.0680(32.5381) | Bit/dim 1.0921(1.0967) | Xent 0.0000(0.0000) | Loss 1.0921(1.0967) | Error 0.0000(0.0000) Steps 428(418.85) | Grad Norm 0.0669(0.0709) | Total Time 10.00(10.00)\n",
      "Iter 3093 | Time 32.1794(32.5274) | Bit/dim 1.0984(1.0967) | Xent 0.0000(0.0000) | Loss 1.0984(1.0967) | Error 0.0000(0.0000) Steps 422(418.94) | Grad Norm 0.0752(0.0710) | Total Time 10.00(10.00)\n",
      "Iter 3094 | Time 31.9164(32.5090) | Bit/dim 1.0954(1.0967) | Xent 0.0000(0.0000) | Loss 1.0954(1.0967) | Error 0.0000(0.0000) Steps 428(419.21) | Grad Norm 0.0753(0.0712) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 16.5008, Epoch Time 255.8251(252.1820), Bit/dim 1.0913(best: 1.0906), Xent 0.0000, Loss 1.0913, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3095 | Time 32.9521(32.5223) | Bit/dim 1.0960(1.0967) | Xent 0.0000(0.0000) | Loss 1.0960(1.0967) | Error 0.0000(0.0000) Steps 416(419.12) | Grad Norm 0.0632(0.0709) | Total Time 10.00(10.00)\n",
      "Iter 3096 | Time 32.9046(32.5338) | Bit/dim 1.0943(1.0966) | Xent 0.0000(0.0000) | Loss 1.0943(1.0966) | Error 0.0000(0.0000) Steps 422(419.20) | Grad Norm 0.0482(0.0702) | Total Time 10.00(10.00)\n",
      "Iter 3097 | Time 33.0686(32.5498) | Bit/dim 1.0931(1.0965) | Xent 0.0000(0.0000) | Loss 1.0931(1.0965) | Error 0.0000(0.0000) Steps 428(419.47) | Grad Norm 0.0789(0.0705) | Total Time 10.00(10.00)\n",
      "Iter 3098 | Time 32.2500(32.5408) | Bit/dim 1.0947(1.0964) | Xent 0.0000(0.0000) | Loss 1.0947(1.0964) | Error 0.0000(0.0000) Steps 422(419.54) | Grad Norm 0.0674(0.0704) | Total Time 10.00(10.00)\n",
      "Iter 3099 | Time 32.4022(32.5367) | Bit/dim 1.0953(1.0964) | Xent 0.0000(0.0000) | Loss 1.0953(1.0964) | Error 0.0000(0.0000) Steps 428(419.80) | Grad Norm 0.0547(0.0699) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 32.7207(32.5422) | Bit/dim 1.1025(1.0966) | Xent 0.0000(0.0000) | Loss 1.1025(1.0966) | Error 0.0000(0.0000) Steps 416(419.68) | Grad Norm 0.0711(0.0700) | Total Time 10.00(10.00)\n",
      "Iter 3101 | Time 31.4275(32.5088) | Bit/dim 1.0935(1.0965) | Xent 0.0000(0.0000) | Loss 1.0935(1.0965) | Error 0.0000(0.0000) Steps 416(419.57) | Grad Norm 0.0613(0.0697) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 15.9926, Epoch Time 256.8829(252.3230), Bit/dim 1.0914(best: 1.0906), Xent 0.0000, Loss 1.0914, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3102 | Time 33.0825(32.5260) | Bit/dim 1.0959(1.0965) | Xent 0.0000(0.0000) | Loss 1.0959(1.0965) | Error 0.0000(0.0000) Steps 428(419.83) | Grad Norm 0.0504(0.0691) | Total Time 10.00(10.00)\n",
      "Iter 3103 | Time 34.1548(32.5748) | Bit/dim 1.0976(1.0965) | Xent 0.0000(0.0000) | Loss 1.0976(1.0965) | Error 0.0000(0.0000) Steps 428(420.07) | Grad Norm 0.0895(0.0697) | Total Time 10.00(10.00)\n",
      "Iter 3104 | Time 32.4540(32.5712) | Bit/dim 1.0982(1.0966) | Xent 0.0000(0.0000) | Loss 1.0982(1.0966) | Error 0.0000(0.0000) Steps 428(420.31) | Grad Norm 0.0655(0.0696) | Total Time 10.00(10.00)\n",
      "Iter 3105 | Time 32.2576(32.5618) | Bit/dim 1.0939(1.0965) | Xent 0.0000(0.0000) | Loss 1.0939(1.0965) | Error 0.0000(0.0000) Steps 428(420.54) | Grad Norm 0.0575(0.0692) | Total Time 10.00(10.00)\n",
      "Iter 3106 | Time 32.9108(32.5723) | Bit/dim 1.0981(1.0965) | Xent 0.0000(0.0000) | Loss 1.0981(1.0965) | Error 0.0000(0.0000) Steps 428(420.76) | Grad Norm 0.0843(0.0697) | Total Time 10.00(10.00)\n",
      "Iter 3107 | Time 32.3930(32.5669) | Bit/dim 1.0971(1.0965) | Xent 0.0000(0.0000) | Loss 1.0971(1.0965) | Error 0.0000(0.0000) Steps 416(420.62) | Grad Norm 0.0986(0.0706) | Total Time 10.00(10.00)\n",
      "Iter 3108 | Time 33.1419(32.5842) | Bit/dim 1.0920(1.0964) | Xent 0.0000(0.0000) | Loss 1.0920(1.0964) | Error 0.0000(0.0000) Steps 416(420.48) | Grad Norm 0.0601(0.0703) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 16.4902, Epoch Time 260.2219(252.5600), Bit/dim 1.0915(best: 1.0906), Xent 0.0000, Loss 1.0915, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3109 | Time 32.0267(32.5674) | Bit/dim 1.0961(1.0964) | Xent 0.0000(0.0000) | Loss 1.0961(1.0964) | Error 0.0000(0.0000) Steps 410(420.17) | Grad Norm 0.0560(0.0698) | Total Time 10.00(10.00)\n",
      "Iter 3110 | Time 32.6891(32.5711) | Bit/dim 1.0984(1.0965) | Xent 0.0000(0.0000) | Loss 1.0984(1.0965) | Error 0.0000(0.0000) Steps 428(420.40) | Grad Norm 0.0535(0.0693) | Total Time 10.00(10.00)\n",
      "Iter 3111 | Time 33.1035(32.5870) | Bit/dim 1.0969(1.0965) | Xent 0.0000(0.0000) | Loss 1.0969(1.0965) | Error 0.0000(0.0000) Steps 422(420.45) | Grad Norm 0.0527(0.0688) | Total Time 10.00(10.00)\n",
      "Iter 3112 | Time 32.8577(32.5952) | Bit/dim 1.0960(1.0965) | Xent 0.0000(0.0000) | Loss 1.0960(1.0965) | Error 0.0000(0.0000) Steps 416(420.32) | Grad Norm 0.0755(0.0690) | Total Time 10.00(10.00)\n",
      "Iter 3113 | Time 32.6973(32.5982) | Bit/dim 1.0958(1.0964) | Xent 0.0000(0.0000) | Loss 1.0958(1.0964) | Error 0.0000(0.0000) Steps 416(420.19) | Grad Norm 0.0830(0.0695) | Total Time 10.00(10.00)\n",
      "Iter 3114 | Time 32.8218(32.6049) | Bit/dim 1.0942(1.0964) | Xent 0.0000(0.0000) | Loss 1.0942(1.0964) | Error 0.0000(0.0000) Steps 410(419.88) | Grad Norm 0.0560(0.0691) | Total Time 10.00(10.00)\n",
      "Iter 3115 | Time 31.5629(32.5737) | Bit/dim 1.0962(1.0964) | Xent 0.0000(0.0000) | Loss 1.0962(1.0964) | Error 0.0000(0.0000) Steps 416(419.77) | Grad Norm 0.0621(0.0688) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 16.0404, Epoch Time 256.6025(252.6813), Bit/dim 1.0911(best: 1.0906), Xent 0.0000, Loss 1.0911, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3116 | Time 31.9866(32.5561) | Bit/dim 1.0990(1.0964) | Xent 0.0000(0.0000) | Loss 1.0990(1.0964) | Error 0.0000(0.0000) Steps 428(420.01) | Grad Norm 0.0648(0.0687) | Total Time 10.00(10.00)\n",
      "Iter 3117 | Time 32.8703(32.5655) | Bit/dim 1.0942(1.0964) | Xent 0.0000(0.0000) | Loss 1.0942(1.0964) | Error 0.0000(0.0000) Steps 428(420.25) | Grad Norm 0.0721(0.0688) | Total Time 10.00(10.00)\n",
      "Iter 3118 | Time 33.2165(32.5850) | Bit/dim 1.0961(1.0964) | Xent 0.0000(0.0000) | Loss 1.0961(1.0964) | Error 0.0000(0.0000) Steps 416(420.12) | Grad Norm 0.0716(0.0689) | Total Time 10.00(10.00)\n",
      "Iter 3119 | Time 33.4947(32.6123) | Bit/dim 1.0944(1.0963) | Xent 0.0000(0.0000) | Loss 1.0944(1.0963) | Error 0.0000(0.0000) Steps 416(420.00) | Grad Norm 0.0499(0.0683) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 32.1910(32.5997) | Bit/dim 1.1003(1.0964) | Xent 0.0000(0.0000) | Loss 1.1003(1.0964) | Error 0.0000(0.0000) Steps 416(419.88) | Grad Norm 0.0795(0.0687) | Total Time 10.00(10.00)\n",
      "Iter 3121 | Time 32.9421(32.6099) | Bit/dim 1.0927(1.0963) | Xent 0.0000(0.0000) | Loss 1.0927(1.0963) | Error 0.0000(0.0000) Steps 416(419.76) | Grad Norm 0.0852(0.0692) | Total Time 10.00(10.00)\n",
      "Iter 3122 | Time 32.4952(32.6065) | Bit/dim 1.0957(1.0963) | Xent 0.0000(0.0000) | Loss 1.0957(1.0963) | Error 0.0000(0.0000) Steps 428(420.01) | Grad Norm 0.0608(0.0689) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 16.1668, Epoch Time 258.1907(252.8465), Bit/dim 1.0905(best: 1.0906), Xent 0.0000, Loss 1.0905, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3123 | Time 31.7764(32.5816) | Bit/dim 1.0997(1.0964) | Xent 0.0000(0.0000) | Loss 1.0997(1.0964) | Error 0.0000(0.0000) Steps 416(419.89) | Grad Norm 0.0491(0.0683) | Total Time 10.00(10.00)\n",
      "Iter 3124 | Time 32.2038(32.5703) | Bit/dim 1.0943(1.0963) | Xent 0.0000(0.0000) | Loss 1.0943(1.0963) | Error 0.0000(0.0000) Steps 428(420.13) | Grad Norm 0.0730(0.0685) | Total Time 10.00(10.00)\n",
      "Iter 3125 | Time 32.6145(32.5716) | Bit/dim 1.0989(1.0964) | Xent 0.0000(0.0000) | Loss 1.0989(1.0964) | Error 0.0000(0.0000) Steps 416(420.01) | Grad Norm 0.0686(0.0685) | Total Time 10.00(10.00)\n",
      "Iter 3126 | Time 32.7843(32.5780) | Bit/dim 1.0988(1.0965) | Xent 0.0000(0.0000) | Loss 1.0988(1.0965) | Error 0.0000(0.0000) Steps 428(420.25) | Grad Norm 0.0970(0.0693) | Total Time 10.00(10.00)\n",
      "Iter 3127 | Time 32.2468(32.5680) | Bit/dim 1.0974(1.0965) | Xent 0.0000(0.0000) | Loss 1.0974(1.0965) | Error 0.0000(0.0000) Steps 410(419.94) | Grad Norm 0.0533(0.0688) | Total Time 10.00(10.00)\n",
      "Iter 3128 | Time 32.2291(32.5579) | Bit/dim 1.0949(1.0965) | Xent 0.0000(0.0000) | Loss 1.0949(1.0965) | Error 0.0000(0.0000) Steps 422(420.00) | Grad Norm 0.0489(0.0682) | Total Time 10.00(10.00)\n",
      "Iter 3129 | Time 32.2678(32.5492) | Bit/dim 1.0938(1.0964) | Xent 0.0000(0.0000) | Loss 1.0938(1.0964) | Error 0.0000(0.0000) Steps 416(419.88) | Grad Norm 0.0543(0.0678) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 15.8892, Epoch Time 254.8344(252.9062), Bit/dim 1.0914(best: 1.0905), Xent 0.0000, Loss 1.0914, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3130 | Time 32.3765(32.5440) | Bit/dim 1.0959(1.0964) | Xent 0.0000(0.0000) | Loss 1.0959(1.0964) | Error 0.0000(0.0000) Steps 428(420.13) | Grad Norm 0.0560(0.0675) | Total Time 10.00(10.00)\n",
      "Iter 3131 | Time 33.1892(32.5633) | Bit/dim 1.0986(1.0964) | Xent 0.0000(0.0000) | Loss 1.0986(1.0964) | Error 0.0000(0.0000) Steps 416(420.00) | Grad Norm 0.0458(0.0668) | Total Time 10.00(10.00)\n",
      "Iter 3132 | Time 33.3423(32.5867) | Bit/dim 1.0946(1.0964) | Xent 0.0000(0.0000) | Loss 1.0946(1.0964) | Error 0.0000(0.0000) Steps 428(420.24) | Grad Norm 0.0588(0.0666) | Total Time 10.00(10.00)\n",
      "Iter 3133 | Time 32.2470(32.5765) | Bit/dim 1.0946(1.0963) | Xent 0.0000(0.0000) | Loss 1.0946(1.0963) | Error 0.0000(0.0000) Steps 428(420.48) | Grad Norm 0.0541(0.0662) | Total Time 10.00(10.00)\n",
      "Iter 3134 | Time 32.3592(32.5700) | Bit/dim 1.0934(1.0962) | Xent 0.0000(0.0000) | Loss 1.0934(1.0962) | Error 0.0000(0.0000) Steps 416(420.34) | Grad Norm 0.1017(0.0673) | Total Time 10.00(10.00)\n",
      "Iter 3135 | Time 31.9654(32.5519) | Bit/dim 1.0983(1.0963) | Xent 0.0000(0.0000) | Loss 1.0983(1.0963) | Error 0.0000(0.0000) Steps 416(420.21) | Grad Norm 0.0618(0.0671) | Total Time 10.00(10.00)\n",
      "Iter 3136 | Time 32.4357(32.5484) | Bit/dim 1.0977(1.0963) | Xent 0.0000(0.0000) | Loss 1.0977(1.0963) | Error 0.0000(0.0000) Steps 416(420.09) | Grad Norm 0.0844(0.0676) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 16.2899, Epoch Time 257.0703(253.0311), Bit/dim 1.0910(best: 1.0905), Xent 0.0000, Loss 1.0910, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3137 | Time 32.1443(32.5363) | Bit/dim 1.0991(1.0964) | Xent 0.0000(0.0000) | Loss 1.0991(1.0964) | Error 0.0000(0.0000) Steps 416(419.96) | Grad Norm 0.0498(0.0671) | Total Time 10.00(10.00)\n",
      "Iter 3138 | Time 31.9439(32.5185) | Bit/dim 1.0998(1.0965) | Xent 0.0000(0.0000) | Loss 1.0998(1.0965) | Error 0.0000(0.0000) Steps 416(419.84) | Grad Norm 0.0749(0.0673) | Total Time 10.00(10.00)\n",
      "Iter 3139 | Time 34.6883(32.5836) | Bit/dim 1.0931(1.0964) | Xent 0.0000(0.0000) | Loss 1.0931(1.0964) | Error 0.0000(0.0000) Steps 428(420.09) | Grad Norm 0.0727(0.0675) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 31.7046(32.5572) | Bit/dim 1.0965(1.0964) | Xent 0.0000(0.0000) | Loss 1.0965(1.0964) | Error 0.0000(0.0000) Steps 428(420.33) | Grad Norm 0.1273(0.0693) | Total Time 10.00(10.00)\n",
      "Iter 3141 | Time 32.7084(32.5617) | Bit/dim 1.0969(1.0964) | Xent 0.0000(0.0000) | Loss 1.0969(1.0964) | Error 0.0000(0.0000) Steps 416(420.20) | Grad Norm 0.0587(0.0690) | Total Time 10.00(10.00)\n",
      "Iter 3142 | Time 31.7754(32.5381) | Bit/dim 1.0937(1.0964) | Xent 0.0000(0.0000) | Loss 1.0937(1.0964) | Error 0.0000(0.0000) Steps 416(420.07) | Grad Norm 0.0751(0.0692) | Total Time 10.00(10.00)\n",
      "Iter 3143 | Time 33.9186(32.5796) | Bit/dim 1.0971(1.0964) | Xent 0.0000(0.0000) | Loss 1.0971(1.0964) | Error 0.0000(0.0000) Steps 428(420.31) | Grad Norm 0.0466(0.0685) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 16.1681, Epoch Time 257.9815(253.1796), Bit/dim 1.0908(best: 1.0905), Xent 0.0000, Loss 1.0908, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3144 | Time 33.4086(32.6044) | Bit/dim 1.0954(1.0964) | Xent 0.0000(0.0000) | Loss 1.0954(1.0964) | Error 0.0000(0.0000) Steps 422(420.36) | Grad Norm 0.0763(0.0687) | Total Time 10.00(10.00)\n",
      "Iter 3145 | Time 31.8689(32.5824) | Bit/dim 1.0989(1.0964) | Xent 0.0000(0.0000) | Loss 1.0989(1.0964) | Error 0.0000(0.0000) Steps 422(420.41) | Grad Norm 0.0535(0.0683) | Total Time 10.00(10.00)\n",
      "Iter 3146 | Time 32.3356(32.5750) | Bit/dim 1.1003(1.0965) | Xent 0.0000(0.0000) | Loss 1.1003(1.0965) | Error 0.0000(0.0000) Steps 428(420.64) | Grad Norm 0.0739(0.0684) | Total Time 10.00(10.00)\n",
      "Iter 3147 | Time 32.9206(32.5853) | Bit/dim 1.0930(1.0964) | Xent 0.0000(0.0000) | Loss 1.0930(1.0964) | Error 0.0000(0.0000) Steps 428(420.86) | Grad Norm 0.0825(0.0689) | Total Time 10.00(10.00)\n",
      "Iter 3148 | Time 32.3396(32.5780) | Bit/dim 1.0901(1.0963) | Xent 0.0000(0.0000) | Loss 1.0901(1.0963) | Error 0.0000(0.0000) Steps 428(421.07) | Grad Norm 0.0615(0.0686) | Total Time 10.00(10.00)\n",
      "Iter 3149 | Time 32.0363(32.5617) | Bit/dim 1.0972(1.0963) | Xent 0.0000(0.0000) | Loss 1.0972(1.0963) | Error 0.0000(0.0000) Steps 416(420.92) | Grad Norm 0.1115(0.0699) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 32.6326(32.5638) | Bit/dim 1.1003(1.0964) | Xent 0.0000(0.0000) | Loss 1.1003(1.0964) | Error 0.0000(0.0000) Steps 428(421.13) | Grad Norm 0.0988(0.0708) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 16.2041, Epoch Time 256.6131(253.2826), Bit/dim 1.0906(best: 1.0905), Xent 0.0000, Loss 1.0906, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3151 | Time 32.8227(32.5716) | Bit/dim 1.1021(1.0966) | Xent 0.0000(0.0000) | Loss 1.1021(1.0966) | Error 0.0000(0.0000) Steps 416(420.98) | Grad Norm 0.0507(0.0702) | Total Time 10.00(10.00)\n",
      "Iter 3152 | Time 32.7230(32.5761) | Bit/dim 1.0940(1.0965) | Xent 0.0000(0.0000) | Loss 1.0940(1.0965) | Error 0.0000(0.0000) Steps 428(421.19) | Grad Norm 0.1161(0.0716) | Total Time 10.00(10.00)\n",
      "Iter 3153 | Time 32.8156(32.5833) | Bit/dim 1.0950(1.0964) | Xent 0.0000(0.0000) | Loss 1.0950(1.0964) | Error 0.0000(0.0000) Steps 416(421.03) | Grad Norm 0.0861(0.0720) | Total Time 10.00(10.00)\n",
      "Iter 3154 | Time 32.1799(32.5712) | Bit/dim 1.0933(1.0964) | Xent 0.0000(0.0000) | Loss 1.0933(1.0964) | Error 0.0000(0.0000) Steps 416(420.88) | Grad Norm 0.0689(0.0719) | Total Time 10.00(10.00)\n",
      "Iter 3155 | Time 32.3371(32.5642) | Bit/dim 1.0969(1.0964) | Xent 0.0000(0.0000) | Loss 1.0969(1.0964) | Error 0.0000(0.0000) Steps 416(420.74) | Grad Norm 0.0840(0.0723) | Total Time 10.00(10.00)\n",
      "Iter 3156 | Time 33.6242(32.5960) | Bit/dim 1.0977(1.0964) | Xent 0.0000(0.0000) | Loss 1.0977(1.0964) | Error 0.0000(0.0000) Steps 422(420.77) | Grad Norm 0.0551(0.0717) | Total Time 10.00(10.00)\n",
      "Iter 3157 | Time 31.9771(32.5774) | Bit/dim 1.0947(1.0964) | Xent 0.0000(0.0000) | Loss 1.0947(1.0964) | Error 0.0000(0.0000) Steps 416(420.63) | Grad Norm 0.0828(0.0721) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 16.1981, Epoch Time 257.4504(253.4076), Bit/dim 1.0908(best: 1.0905), Xent 0.0000, Loss 1.0908, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3158 | Time 32.5461(32.5765) | Bit/dim 1.0969(1.0964) | Xent 0.0000(0.0000) | Loss 1.0969(1.0964) | Error 0.0000(0.0000) Steps 416(420.49) | Grad Norm 0.0711(0.0720) | Total Time 10.00(10.00)\n",
      "Iter 3159 | Time 33.6390(32.6084) | Bit/dim 1.0914(1.0962) | Xent 0.0000(0.0000) | Loss 1.0914(1.0962) | Error 0.0000(0.0000) Steps 416(420.36) | Grad Norm 0.0726(0.0721) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 32.6713(32.6103) | Bit/dim 1.0975(1.0963) | Xent 0.0000(0.0000) | Loss 1.0975(1.0963) | Error 0.0000(0.0000) Steps 428(420.59) | Grad Norm 0.0550(0.0716) | Total Time 10.00(10.00)\n",
      "Iter 3161 | Time 32.9763(32.6212) | Bit/dim 1.1003(1.0964) | Xent 0.0000(0.0000) | Loss 1.1003(1.0964) | Error 0.0000(0.0000) Steps 416(420.45) | Grad Norm 0.0931(0.0722) | Total Time 10.00(10.00)\n",
      "Iter 3162 | Time 31.8457(32.5980) | Bit/dim 1.0962(1.0964) | Xent 0.0000(0.0000) | Loss 1.0962(1.0964) | Error 0.0000(0.0000) Steps 410(420.13) | Grad Norm 0.0590(0.0718) | Total Time 10.00(10.00)\n",
      "Iter 3163 | Time 32.5555(32.5967) | Bit/dim 1.0954(1.0964) | Xent 0.0000(0.0000) | Loss 1.0954(1.0964) | Error 0.0000(0.0000) Steps 428(420.37) | Grad Norm 0.1143(0.0731) | Total Time 10.00(10.00)\n",
      "Iter 3164 | Time 32.4993(32.5938) | Bit/dim 1.0953(1.0963) | Xent 0.0000(0.0000) | Loss 1.0953(1.0963) | Error 0.0000(0.0000) Steps 428(420.60) | Grad Norm 0.0446(0.0722) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 16.4042, Epoch Time 258.0287(253.5463), Bit/dim 1.0911(best: 1.0905), Xent 0.0000, Loss 1.0911, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3165 | Time 31.8899(32.5727) | Bit/dim 1.0946(1.0963) | Xent 0.0000(0.0000) | Loss 1.0946(1.0963) | Error 0.0000(0.0000) Steps 416(420.46) | Grad Norm 0.0767(0.0724) | Total Time 10.00(10.00)\n",
      "Iter 3166 | Time 32.5597(32.5723) | Bit/dim 1.0932(1.0962) | Xent 0.0000(0.0000) | Loss 1.0932(1.0962) | Error 0.0000(0.0000) Steps 410(420.15) | Grad Norm 0.0755(0.0725) | Total Time 10.00(10.00)\n",
      "Iter 3167 | Time 33.1260(32.5889) | Bit/dim 1.0972(1.0962) | Xent 0.0000(0.0000) | Loss 1.0972(1.0962) | Error 0.0000(0.0000) Steps 428(420.38) | Grad Norm 0.0866(0.0729) | Total Time 10.00(10.00)\n",
      "Iter 3168 | Time 31.8885(32.5679) | Bit/dim 1.0961(1.0962) | Xent 0.0000(0.0000) | Loss 1.0961(1.0962) | Error 0.0000(0.0000) Steps 428(420.61) | Grad Norm 0.0604(0.0725) | Total Time 10.00(10.00)\n",
      "Iter 3169 | Time 32.1302(32.5547) | Bit/dim 1.0968(1.0962) | Xent 0.0000(0.0000) | Loss 1.0968(1.0962) | Error 0.0000(0.0000) Steps 428(420.83) | Grad Norm 0.0513(0.0719) | Total Time 10.00(10.00)\n",
      "Iter 3170 | Time 32.7224(32.5598) | Bit/dim 1.0960(1.0962) | Xent 0.0000(0.0000) | Loss 1.0960(1.0962) | Error 0.0000(0.0000) Steps 416(420.69) | Grad Norm 0.0782(0.0721) | Total Time 10.00(10.00)\n",
      "Iter 3171 | Time 32.3104(32.5523) | Bit/dim 1.1002(1.0963) | Xent 0.0000(0.0000) | Loss 1.1002(1.0963) | Error 0.0000(0.0000) Steps 428(420.91) | Grad Norm 0.0601(0.0717) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 16.3803, Epoch Time 255.9056(253.6171), Bit/dim 1.0903(best: 1.0905), Xent 0.0000, Loss 1.0903, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3172 | Time 32.4265(32.5485) | Bit/dim 1.0940(1.0963) | Xent 0.0000(0.0000) | Loss 1.0940(1.0963) | Error 0.0000(0.0000) Steps 428(421.12) | Grad Norm 0.0721(0.0717) | Total Time 10.00(10.00)\n",
      "Iter 3173 | Time 31.9773(32.5314) | Bit/dim 1.0977(1.0963) | Xent 0.0000(0.0000) | Loss 1.0977(1.0963) | Error 0.0000(0.0000) Steps 428(421.33) | Grad Norm 0.0565(0.0713) | Total Time 10.00(10.00)\n",
      "Iter 3174 | Time 31.4383(32.4986) | Bit/dim 1.0905(1.0961) | Xent 0.0000(0.0000) | Loss 1.0905(1.0961) | Error 0.0000(0.0000) Steps 416(421.17) | Grad Norm 0.0777(0.0715) | Total Time 10.00(10.00)\n",
      "Iter 3175 | Time 32.5406(32.4999) | Bit/dim 1.0998(1.0962) | Xent 0.0000(0.0000) | Loss 1.0998(1.0962) | Error 0.0000(0.0000) Steps 428(421.37) | Grad Norm 0.0723(0.0715) | Total Time 10.00(10.00)\n",
      "Iter 3176 | Time 32.8224(32.5095) | Bit/dim 1.0939(1.0962) | Xent 0.0000(0.0000) | Loss 1.0939(1.0962) | Error 0.0000(0.0000) Steps 416(421.21) | Grad Norm 0.0450(0.0707) | Total Time 10.00(10.00)\n",
      "Iter 3177 | Time 32.2645(32.5022) | Bit/dim 1.1001(1.0963) | Xent 0.0000(0.0000) | Loss 1.1001(1.0963) | Error 0.0000(0.0000) Steps 428(421.41) | Grad Norm 0.0685(0.0706) | Total Time 10.00(10.00)\n",
      "Iter 3178 | Time 34.5838(32.5646) | Bit/dim 1.0958(1.0963) | Xent 0.0000(0.0000) | Loss 1.0958(1.0963) | Error 0.0000(0.0000) Steps 428(421.61) | Grad Norm 0.1025(0.0716) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 16.1829, Epoch Time 257.3693(253.7296), Bit/dim 1.0907(best: 1.0903), Xent 0.0000, Loss 1.0907, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3179 | Time 33.6043(32.5958) | Bit/dim 1.0962(1.0963) | Xent 0.0000(0.0000) | Loss 1.0962(1.0963) | Error 0.0000(0.0000) Steps 434(421.98) | Grad Norm 0.0521(0.0710) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 31.1302(32.5518) | Bit/dim 1.0938(1.0962) | Xent 0.0000(0.0000) | Loss 1.0938(1.0962) | Error 0.0000(0.0000) Steps 416(421.80) | Grad Norm 0.0532(0.0705) | Total Time 10.00(10.00)\n",
      "Iter 3181 | Time 32.0349(32.5363) | Bit/dim 1.0903(1.0960) | Xent 0.0000(0.0000) | Loss 1.0903(1.0960) | Error 0.0000(0.0000) Steps 410(421.45) | Grad Norm 0.1390(0.0725) | Total Time 10.00(10.00)\n",
      "Iter 3182 | Time 32.2986(32.5292) | Bit/dim 1.1005(1.0962) | Xent 0.0000(0.0000) | Loss 1.1005(1.0962) | Error 0.0000(0.0000) Steps 416(421.29) | Grad Norm 0.1167(0.0738) | Total Time 10.00(10.00)\n",
      "Iter 3183 | Time 32.6095(32.5316) | Bit/dim 1.0971(1.0962) | Xent 0.0000(0.0000) | Loss 1.0971(1.0962) | Error 0.0000(0.0000) Steps 416(421.13) | Grad Norm 0.0573(0.0733) | Total Time 10.00(10.00)\n",
      "Iter 3184 | Time 32.8600(32.5415) | Bit/dim 1.0979(1.0962) | Xent 0.0000(0.0000) | Loss 1.0979(1.0962) | Error 0.0000(0.0000) Steps 428(421.33) | Grad Norm 0.1219(0.0748) | Total Time 10.00(10.00)\n",
      "Iter 3185 | Time 32.5323(32.5412) | Bit/dim 1.0985(1.0963) | Xent 0.0000(0.0000) | Loss 1.0985(1.0963) | Error 0.0000(0.0000) Steps 416(421.17) | Grad Norm 0.1099(0.0759) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 16.3405, Epoch Time 256.4242(253.8105), Bit/dim 1.0902(best: 1.0903), Xent 0.0000, Loss 1.0902, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3186 | Time 31.6977(32.5159) | Bit/dim 1.0911(1.0961) | Xent 0.0000(0.0000) | Loss 1.0911(1.0961) | Error 0.0000(0.0000) Steps 416(421.02) | Grad Norm 0.0846(0.0761) | Total Time 10.00(10.00)\n",
      "Iter 3187 | Time 33.5589(32.5472) | Bit/dim 1.0996(1.0963) | Xent 0.0000(0.0000) | Loss 1.0996(1.0963) | Error 0.0000(0.0000) Steps 410(420.69) | Grad Norm 0.0478(0.0753) | Total Time 10.00(10.00)\n",
      "Iter 3188 | Time 32.0461(32.5321) | Bit/dim 1.0950(1.0962) | Xent 0.0000(0.0000) | Loss 1.0950(1.0962) | Error 0.0000(0.0000) Steps 416(420.55) | Grad Norm 0.0559(0.0747) | Total Time 10.00(10.00)\n",
      "Iter 3189 | Time 33.1186(32.5497) | Bit/dim 1.0932(1.0961) | Xent 0.0000(0.0000) | Loss 1.0932(1.0961) | Error 0.0000(0.0000) Steps 428(420.77) | Grad Norm 0.0879(0.0751) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 32.1430(32.5375) | Bit/dim 1.1002(1.0962) | Xent 0.0000(0.0000) | Loss 1.1002(1.0962) | Error 0.0000(0.0000) Steps 410(420.45) | Grad Norm 0.1149(0.0763) | Total Time 10.00(10.00)\n",
      "Iter 3191 | Time 32.4274(32.5342) | Bit/dim 1.0971(1.0963) | Xent 0.0000(0.0000) | Loss 1.0971(1.0963) | Error 0.0000(0.0000) Steps 416(420.31) | Grad Norm 0.0860(0.0766) | Total Time 10.00(10.00)\n",
      "Iter 3192 | Time 32.4669(32.5322) | Bit/dim 1.0971(1.0963) | Xent 0.0000(0.0000) | Loss 1.0971(1.0963) | Error 0.0000(0.0000) Steps 416(420.19) | Grad Norm 0.1229(0.0780) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 16.1180, Epoch Time 256.2847(253.8847), Bit/dim 1.0905(best: 1.0902), Xent 0.0000, Loss 1.0905, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3193 | Time 32.5742(32.5335) | Bit/dim 1.0987(1.0964) | Xent 0.0000(0.0000) | Loss 1.0987(1.0964) | Error 0.0000(0.0000) Steps 416(420.06) | Grad Norm 0.0464(0.0770) | Total Time 10.00(10.00)\n",
      "Iter 3194 | Time 33.6073(32.5657) | Bit/dim 1.0967(1.0964) | Xent 0.0000(0.0000) | Loss 1.0967(1.0964) | Error 0.0000(0.0000) Steps 428(420.30) | Grad Norm 0.1094(0.0780) | Total Time 10.00(10.00)\n",
      "Iter 3195 | Time 33.1324(32.5827) | Bit/dim 1.0983(1.0964) | Xent 0.0000(0.0000) | Loss 1.0983(1.0964) | Error 0.0000(0.0000) Steps 434(420.71) | Grad Norm 0.0895(0.0783) | Total Time 10.00(10.00)\n",
      "Iter 3196 | Time 30.4893(32.5199) | Bit/dim 1.0954(1.0964) | Xent 0.0000(0.0000) | Loss 1.0954(1.0964) | Error 0.0000(0.0000) Steps 416(420.57) | Grad Norm 0.0483(0.0774) | Total Time 10.00(10.00)\n",
      "Iter 3197 | Time 32.7669(32.5273) | Bit/dim 1.0973(1.0964) | Xent 0.0000(0.0000) | Loss 1.0973(1.0964) | Error 0.0000(0.0000) Steps 428(420.79) | Grad Norm 0.0523(0.0767) | Total Time 10.00(10.00)\n",
      "Iter 3198 | Time 31.7738(32.5047) | Bit/dim 1.0927(1.0963) | Xent 0.0000(0.0000) | Loss 1.0927(1.0963) | Error 0.0000(0.0000) Steps 422(420.83) | Grad Norm 0.0447(0.0757) | Total Time 10.00(10.00)\n",
      "Iter 3199 | Time 33.1142(32.5230) | Bit/dim 1.0956(1.0963) | Xent 0.0000(0.0000) | Loss 1.0956(1.0963) | Error 0.0000(0.0000) Steps 410(420.50) | Grad Norm 0.0696(0.0755) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 16.1322, Epoch Time 256.6162(253.9666), Bit/dim 1.0907(best: 1.0902), Xent 0.0000, Loss 1.0907, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3200 | Time 34.1080(32.5705) | Bit/dim 1.0975(1.0963) | Xent 0.0000(0.0000) | Loss 1.0975(1.0963) | Error 0.0000(0.0000) Steps 416(420.37) | Grad Norm 0.0695(0.0754) | Total Time 10.00(10.00)\n",
      "Iter 3201 | Time 32.7733(32.5766) | Bit/dim 1.0978(1.0964) | Xent 0.0000(0.0000) | Loss 1.0978(1.0964) | Error 0.0000(0.0000) Steps 428(420.60) | Grad Norm 0.0599(0.0749) | Total Time 10.00(10.00)\n",
      "Iter 3202 | Time 33.0433(32.5906) | Bit/dim 1.0993(1.0965) | Xent 0.0000(0.0000) | Loss 1.0993(1.0965) | Error 0.0000(0.0000) Steps 416(420.46) | Grad Norm 0.0599(0.0744) | Total Time 10.00(10.00)\n",
      "Iter 3203 | Time 31.6509(32.5624) | Bit/dim 1.0954(1.0964) | Xent 0.0000(0.0000) | Loss 1.0954(1.0964) | Error 0.0000(0.0000) Steps 428(420.68) | Grad Norm 0.1199(0.0758) | Total Time 10.00(10.00)\n",
      "Iter 3204 | Time 31.7028(32.5366) | Bit/dim 1.0942(1.0964) | Xent 0.0000(0.0000) | Loss 1.0942(1.0964) | Error 0.0000(0.0000) Steps 428(420.90) | Grad Norm 0.0680(0.0756) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_published_bs8K_errcontrol_1 --resume ../experiments_published/cnf_published_bs8K_errcontrol_1/epoch_400_checkpt.pth --seed 0 --conditional False --controlled_tol True --lr 0.0001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
