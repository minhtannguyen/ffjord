{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=False, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_cifar10_bs900_rl_stdscale_15_run3', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 10.7832(28.5990) | Bit/dim 8.7874(8.9815) | Xent 0.0000(0.0000) | Loss 18.4821(18.5222) | Error 0.0000(0.0000) Steps 442(443.85) | Grad Norm 18.0949(27.3419) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 10.7094(23.9536) | Bit/dim 8.5188(8.8913) | Xent 0.0000(0.0000) | Loss 17.6593(18.3599) | Error 0.0000(0.0000) Steps 418(442.25) | Grad Norm 8.5186(23.0513) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 10.6661(20.5283) | Bit/dim 8.4071(8.7660) | Xent 0.0000(0.0000) | Loss 17.3736(18.1215) | Error 0.0000(0.0000) Steps 424(444.24) | Grad Norm 6.8081(18.9803) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.1499(18.0614) | Bit/dim 8.0802(8.6202) | Xent 0.0000(0.0000) | Loss 16.9807(17.8581) | Error 0.0000(0.0000) Steps 448(444.52) | Grad Norm 4.9981(15.3489) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 10.4278(16.2580) | Bit/dim 7.8754(8.4522) | Xent 0.0000(0.0000) | Loss 16.1871(17.5527) | Error 0.0000(0.0000) Steps 454(447.77) | Grad Norm 4.8943(12.6015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 65.9664, Epoch Time 715.5325(715.5325), Bit/dim 7.7071(best: inf), Xent 0.0000, Loss 7.7071, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 11.0290(14.8709) | Bit/dim 7.5991(8.2606) | Xent 0.0000(0.0000) | Loss 15.9493(17.6440) | Error 0.0000(0.0000) Steps 442(447.26) | Grad Norm 4.5112(10.5470) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 11.4912(13.9283) | Bit/dim 7.3310(8.0430) | Xent 0.0000(0.0000) | Loss 15.5115(17.1089) | Error 0.0000(0.0000) Steps 490(447.73) | Grad Norm 3.6043(8.8375) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 11.3284(13.2873) | Bit/dim 7.1521(7.8257) | Xent 0.0000(0.0000) | Loss 15.0039(16.6011) | Error 0.0000(0.0000) Steps 448(450.53) | Grad Norm 2.2014(7.2066) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 11.6419(12.8329) | Bit/dim 7.0692(7.6332) | Xent 0.0000(0.0000) | Loss 14.8817(16.1669) | Error 0.0000(0.0000) Steps 466(453.64) | Grad Norm 1.3622(5.7562) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 11.8987(12.5075) | Bit/dim 6.9956(7.4765) | Xent 0.0000(0.0000) | Loss 14.6607(15.8092) | Error 0.0000(0.0000) Steps 472(456.44) | Grad Norm 0.9595(4.5517) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 11.4539(12.3237) | Bit/dim 7.0037(7.3517) | Xent 0.0000(0.0000) | Loss 14.7695(15.5263) | Error 0.0000(0.0000) Steps 472(460.27) | Grad Norm 0.9575(3.6257) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 67.2318, Epoch Time 718.7176(715.6281), Bit/dim 6.9840(best: 7.7071), Xent 0.0000, Loss 6.9840, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 12.6557(12.2287) | Bit/dim 6.9425(7.2502) | Xent 0.0000(0.0000) | Loss 14.7481(15.7448) | Error 0.0000(0.0000) Steps 490(462.51) | Grad Norm 1.0164(2.9094) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 12.0478(12.1841) | Bit/dim 6.9206(7.1651) | Xent 0.0000(0.0000) | Loss 14.5820(15.4428) | Error 0.0000(0.0000) Steps 478(466.33) | Grad Norm 0.6279(2.3595) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 11.8684(12.1758) | Bit/dim 6.8603(7.0907) | Xent 0.0000(0.0000) | Loss 14.2369(15.1785) | Error 0.0000(0.0000) Steps 508(470.78) | Grad Norm 0.6956(1.9460) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 12.5047(12.2349) | Bit/dim 6.7560(7.0182) | Xent 0.0000(0.0000) | Loss 14.2811(14.9824) | Error 0.0000(0.0000) Steps 490(474.08) | Grad Norm 0.9353(1.6922) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 12.7094(12.4158) | Bit/dim 6.6444(6.9385) | Xent 0.0000(0.0000) | Loss 14.0856(14.7864) | Error 0.0000(0.0000) Steps 472(475.78) | Grad Norm 2.3817(1.6432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 68.7774, Epoch Time 764.9721(717.1084), Bit/dim 6.5496(best: 6.9840), Xent 0.0000, Loss 6.5496, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 12.9164(12.4760) | Bit/dim 6.4857(6.8358) | Xent 0.0000(0.0000) | Loss 13.5886(15.0228) | Error 0.0000(0.0000) Steps 508(479.83) | Grad Norm 29.0510(3.7526) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 12.8842(12.5282) | Bit/dim 6.2428(6.7080) | Xent 0.0000(0.0000) | Loss 13.3051(14.6262) | Error 0.0000(0.0000) Steps 490(484.32) | Grad Norm 41.3541(11.8072) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 12.9320(12.6370) | Bit/dim 5.9615(6.5439) | Xent 0.0000(0.0000) | Loss 12.7328(14.1917) | Error 0.0000(0.0000) Steps 496(488.51) | Grad Norm 18.9187(19.4821) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 12.4193(12.6086) | Bit/dim 5.8502(6.3686) | Xent 0.0000(0.0000) | Loss 12.5724(13.7499) | Error 0.0000(0.0000) Steps 472(488.78) | Grad Norm 73.9365(24.3357) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.1316(12.6274) | Bit/dim 5.7252(6.2065) | Xent 0.0000(0.0000) | Loss 12.2797(13.3491) | Error 0.0000(0.0000) Steps 472(491.72) | Grad Norm 23.1945(26.2615) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 12.5163(12.6173) | Bit/dim 5.6401(6.0676) | Xent 0.0000(0.0000) | Loss 11.9846(13.0344) | Error 0.0000(0.0000) Steps 514(492.41) | Grad Norm 9.5416(22.9584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 69.3629, Epoch Time 785.4595(719.1589), Bit/dim 5.6415(best: 6.5496), Xent 0.0000, Loss 5.6415, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 12.3873(12.5703) | Bit/dim 5.6506(5.9501) | Xent 0.0000(0.0000) | Loss 12.0793(13.1854) | Error 0.0000(0.0000) Steps 484(496.52) | Grad Norm 5.3569(19.7926) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 11.6926(12.6184) | Bit/dim 5.5806(5.8536) | Xent 0.0000(0.0000) | Loss 11.6471(12.8463) | Error 0.0000(0.0000) Steps 508(501.15) | Grad Norm 8.8987(16.5041) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.3840(12.6896) | Bit/dim 5.5526(5.7756) | Xent 0.0000(0.0000) | Loss 11.9944(12.6083) | Error 0.0000(0.0000) Steps 544(504.03) | Grad Norm 3.8507(14.9268) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.4423(12.7577) | Bit/dim 5.5156(5.7106) | Xent 0.0000(0.0000) | Loss 11.9187(12.4151) | Error 0.0000(0.0000) Steps 520(504.62) | Grad Norm 21.4451(13.9513) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 12.8232(12.7623) | Bit/dim 5.4649(5.6548) | Xent 0.0000(0.0000) | Loss 11.7331(12.2453) | Error 0.0000(0.0000) Steps 502(502.59) | Grad Norm 7.6367(15.6034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 71.5296, Epoch Time 793.8215(721.3988), Bit/dim 5.4586(best: 5.6415), Xent 0.0000, Loss 5.4586, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 12.5178(12.8721) | Bit/dim 5.4517(5.6030) | Xent 0.0000(0.0000) | Loss 11.8162(12.6140) | Error 0.0000(0.0000) Steps 520(505.18) | Grad Norm 18.5350(14.1846) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 12.4992(12.8697) | Bit/dim 5.4058(5.5541) | Xent 0.0000(0.0000) | Loss 11.4389(12.3529) | Error 0.0000(0.0000) Steps 520(506.54) | Grad Norm 18.4941(14.9545) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.9285(12.8788) | Bit/dim 5.3621(5.5039) | Xent 0.0000(0.0000) | Loss 11.4823(12.1294) | Error 0.0000(0.0000) Steps 520(508.78) | Grad Norm 8.1535(14.2305) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.5472(12.9524) | Bit/dim 5.3433(5.4573) | Xent 0.0000(0.0000) | Loss 11.4544(11.9588) | Error 0.0000(0.0000) Steps 556(510.98) | Grad Norm 13.7619(15.0271) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.8702(13.0983) | Bit/dim 5.3421(5.4154) | Xent 0.0000(0.0000) | Loss 11.4679(11.8186) | Error 0.0000(0.0000) Steps 550(515.08) | Grad Norm 45.1140(16.6587) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.1139(13.1805) | Bit/dim 5.2864(5.3765) | Xent 0.0000(0.0000) | Loss 11.2751(11.6972) | Error 0.0000(0.0000) Steps 496(513.75) | Grad Norm 36.7530(19.4780) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 72.5432, Epoch Time 814.8982(724.2038), Bit/dim 5.2424(best: 5.4586), Xent 0.0000, Loss 5.2424, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 12.4301(13.1975) | Bit/dim 5.1689(5.3357) | Xent 0.0000(0.0000) | Loss 11.1297(12.0226) | Error 0.0000(0.0000) Steps 502(516.57) | Grad Norm 9.9665(18.3156) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.0264(13.2029) | Bit/dim 5.1675(5.2933) | Xent 0.0000(0.0000) | Loss 11.1515(11.8018) | Error 0.0000(0.0000) Steps 508(513.21) | Grad Norm 12.6673(16.8987) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.6762(13.1342) | Bit/dim 5.1476(5.2508) | Xent 0.0000(0.0000) | Loss 11.0364(11.6059) | Error 0.0000(0.0000) Steps 532(513.48) | Grad Norm 22.0231(15.2449) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 13.6109(13.1596) | Bit/dim 5.0929(5.2130) | Xent 0.0000(0.0000) | Loss 10.9442(11.4597) | Error 0.0000(0.0000) Steps 526(514.92) | Grad Norm 20.6023(16.4604) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 12.7312(13.1665) | Bit/dim 5.0619(5.1734) | Xent 0.0000(0.0000) | Loss 10.9032(11.3105) | Error 0.0000(0.0000) Steps 526(514.68) | Grad Norm 25.3657(15.9200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 71.7566, Epoch Time 817.6586(727.0074), Bit/dim 5.0071(best: 5.2424), Xent 0.0000, Loss 5.0071, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.3037(13.2967) | Bit/dim 5.0355(5.1326) | Xent 0.0000(0.0000) | Loss 10.9412(11.6972) | Error 0.0000(0.0000) Steps 526(515.56) | Grad Norm 2.0782(13.0054) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.6832(13.3790) | Bit/dim 5.0088(5.0951) | Xent 0.0000(0.0000) | Loss 10.7769(11.4491) | Error 0.0000(0.0000) Steps 544(517.48) | Grad Norm 38.8048(14.7499) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.8001(13.5341) | Bit/dim 5.1234(5.0820) | Xent 0.0000(0.0000) | Loss 11.1197(11.3119) | Error 0.0000(0.0000) Steps 526(519.07) | Grad Norm 63.9815(21.5072) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.6921(13.7603) | Bit/dim 4.9150(5.0561) | Xent 0.0000(0.0000) | Loss 10.5827(11.1740) | Error 0.0000(0.0000) Steps 502(520.81) | Grad Norm 16.9935(21.5631) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 15.2025(13.8878) | Bit/dim 4.8852(5.0175) | Xent 0.0000(0.0000) | Loss 10.4995(11.0284) | Error 0.0000(0.0000) Steps 520(521.57) | Grad Norm 5.7172(18.6517) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.2924(14.1015) | Bit/dim 4.8558(4.9777) | Xent 0.0000(0.0000) | Loss 10.5824(10.9080) | Error 0.0000(0.0000) Steps 538(525.06) | Grad Norm 13.7564(16.5512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 74.0852, Epoch Time 868.5916(731.2549), Bit/dim 4.8755(best: 5.0071), Xent 0.0000, Loss 4.8755, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.9677(14.1906) | Bit/dim 4.8660(4.9440) | Xent 0.0000(0.0000) | Loss 10.7079(11.2565) | Error 0.0000(0.0000) Steps 568(528.57) | Grad Norm 25.0955(16.6163) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.4613(14.2196) | Bit/dim 4.7960(4.9131) | Xent 0.0000(0.0000) | Loss 10.4707(11.0507) | Error 0.0000(0.0000) Steps 520(531.04) | Grad Norm 7.2183(17.0042) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.1010(14.2678) | Bit/dim 4.8029(4.8829) | Xent 0.0000(0.0000) | Loss 10.5352(10.8858) | Error 0.0000(0.0000) Steps 538(534.61) | Grad Norm 32.1374(18.4299) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 14.6828(14.4263) | Bit/dim 4.9870(4.8744) | Xent 0.0000(0.0000) | Loss 10.7772(10.8095) | Error 0.0000(0.0000) Steps 520(539.23) | Grad Norm 72.3103(23.6200) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 14.1687(14.4148) | Bit/dim 4.8141(4.8679) | Xent 0.0000(0.0000) | Loss 10.4061(10.7296) | Error 0.0000(0.0000) Steps 532(539.31) | Grad Norm 22.4343(24.3731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 74.9445, Epoch Time 890.4652(736.0313), Bit/dim 4.7726(best: 4.8755), Xent 0.0000, Loss 4.7726, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.1882(14.5123) | Bit/dim 4.7551(4.8438) | Xent 0.0000(0.0000) | Loss 10.2492(11.1632) | Error 0.0000(0.0000) Steps 574(539.07) | Grad Norm 13.7314(21.7899) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 14.6556(14.5301) | Bit/dim 4.6840(4.8119) | Xent 0.0000(0.0000) | Loss 10.0238(10.9219) | Error 0.0000(0.0000) Steps 496(537.14) | Grad Norm 2.8196(18.4534) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 15.1342(14.5474) | Bit/dim 4.6949(4.7836) | Xent 0.0000(0.0000) | Loss 10.1375(10.7480) | Error 0.0000(0.0000) Steps 568(539.08) | Grad Norm 2.9899(15.2176) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.3318(14.6449) | Bit/dim 4.6816(4.7583) | Xent 0.0000(0.0000) | Loss 10.2946(10.6179) | Error 0.0000(0.0000) Steps 592(539.50) | Grad Norm 14.7365(14.7963) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.6879(14.7043) | Bit/dim 5.1090(4.7622) | Xent 0.0000(0.0000) | Loss 11.0695(10.5591) | Error 0.0000(0.0000) Steps 550(541.05) | Grad Norm 116.0250(20.1065) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 14.2795(14.6376) | Bit/dim 4.9964(4.8364) | Xent 0.0000(0.0000) | Loss 10.7362(10.6622) | Error 0.0000(0.0000) Steps 526(538.67) | Grad Norm 13.6693(21.6927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 74.4323, Epoch Time 900.5350(740.9664), Bit/dim 4.9509(best: 4.7726), Xent 0.0000, Loss 4.9509, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 14.1140(14.5147) | Bit/dim 4.7465(4.8348) | Xent 0.0000(0.0000) | Loss 10.3125(11.0564) | Error 0.0000(0.0000) Steps 520(534.89) | Grad Norm 8.5646(18.9009) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 14.7844(14.4850) | Bit/dim 4.6489(4.8015) | Xent 0.0000(0.0000) | Loss 10.1549(10.8446) | Error 0.0000(0.0000) Steps 526(531.39) | Grad Norm 8.2743(15.5629) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.4042(14.3631) | Bit/dim 4.6168(4.7666) | Xent 0.0000(0.0000) | Loss 9.9157(10.6618) | Error 0.0000(0.0000) Steps 484(528.27) | Grad Norm 3.6291(12.8502) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.5797(14.3939) | Bit/dim 4.6087(4.7307) | Xent 0.0000(0.0000) | Loss 10.1962(10.5270) | Error 0.0000(0.0000) Steps 526(526.88) | Grad Norm 3.5031(11.1791) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 14.2708(14.4131) | Bit/dim 4.6042(4.6978) | Xent 0.0000(0.0000) | Loss 10.0023(10.4111) | Error 0.0000(0.0000) Steps 538(529.31) | Grad Norm 4.7308(9.2710) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 74.0185, Epoch Time 878.8594(745.1032), Bit/dim 4.5975(best: 4.7726), Xent 0.0000, Loss 4.5975, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 14.7597(14.3932) | Bit/dim 4.5649(4.6698) | Xent 0.0000(0.0000) | Loss 10.0233(10.8043) | Error 0.0000(0.0000) Steps 550(527.77) | Grad Norm 3.3476(7.9517) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 14.0221(14.3342) | Bit/dim 4.6063(4.6541) | Xent 0.0000(0.0000) | Loss 9.9238(10.6134) | Error 0.0000(0.0000) Steps 514(527.38) | Grad Norm 4.4701(10.5902) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 14.6336(14.3615) | Bit/dim 4.5725(4.6339) | Xent 0.0000(0.0000) | Loss 10.1825(10.4539) | Error 0.0000(0.0000) Steps 538(526.66) | Grad Norm 13.7571(11.1808) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 14.5370(14.3977) | Bit/dim 4.5647(4.6136) | Xent 0.0000(0.0000) | Loss 9.9626(10.3332) | Error 0.0000(0.0000) Steps 550(529.94) | Grad Norm 8.3555(10.6323) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 14.2482(14.4666) | Bit/dim 4.5683(4.5995) | Xent 0.0000(0.0000) | Loss 10.0497(10.2504) | Error 0.0000(0.0000) Steps 538(535.26) | Grad Norm 18.3073(11.2854) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 14.4342(14.5027) | Bit/dim 4.5521(4.5865) | Xent 0.0000(0.0000) | Loss 10.0026(10.1661) | Error 0.0000(0.0000) Steps 520(533.39) | Grad Norm 27.2082(12.1423) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 74.5144, Epoch Time 887.9792(749.3894), Bit/dim 4.5659(best: 4.5975), Xent 0.0000, Loss 4.5659, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 14.2076(14.5686) | Bit/dim 4.5326(4.5759) | Xent 0.0000(0.0000) | Loss 9.9654(10.5642) | Error 0.0000(0.0000) Steps 520(534.42) | Grad Norm 15.6639(13.7032) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 14.6073(14.5811) | Bit/dim 4.5264(4.5616) | Xent 0.0000(0.0000) | Loss 10.0187(10.3881) | Error 0.0000(0.0000) Steps 520(537.41) | Grad Norm 16.2561(13.9763) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.1210(14.5798) | Bit/dim 4.4943(4.5474) | Xent 0.0000(0.0000) | Loss 9.9110(10.2654) | Error 0.0000(0.0000) Steps 550(539.72) | Grad Norm 17.1400(14.2938) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.5571(14.6042) | Bit/dim 4.5092(4.5318) | Xent 0.0000(0.0000) | Loss 9.9531(10.1552) | Error 0.0000(0.0000) Steps 568(538.30) | Grad Norm 17.6146(12.9497) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 14.2434(14.4886) | Bit/dim 4.5746(4.5629) | Xent 0.0000(0.0000) | Loss 10.0385(10.1614) | Error 0.0000(0.0000) Steps 544(538.04) | Grad Norm 14.5756(18.6506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 74.5952, Epoch Time 890.5506(753.6243), Bit/dim 4.5548(best: 4.5659), Xent 0.0000, Loss 4.5548, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 13.9748(14.3147) | Bit/dim 4.5394(4.5681) | Xent 0.0000(0.0000) | Loss 9.8073(10.6331) | Error 0.0000(0.0000) Steps 508(533.96) | Grad Norm 11.9761(17.2688) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 14.5704(14.3771) | Bit/dim 4.4784(4.5508) | Xent 0.0000(0.0000) | Loss 9.8502(10.4325) | Error 0.0000(0.0000) Steps 562(537.94) | Grad Norm 7.5185(14.8567) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 14.3351(14.3610) | Bit/dim 4.4218(4.5260) | Xent 0.0000(0.0000) | Loss 9.5189(10.2454) | Error 0.0000(0.0000) Steps 508(535.78) | Grad Norm 4.7339(12.2843) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 14.9323(14.3501) | Bit/dim 4.4233(4.4997) | Xent 0.0000(0.0000) | Loss 9.6462(10.0898) | Error 0.0000(0.0000) Steps 556(534.83) | Grad Norm 6.8005(10.3217) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 13.5483(14.4551) | Bit/dim 4.4016(4.4793) | Xent 0.0000(0.0000) | Loss 9.5066(9.9885) | Error 0.0000(0.0000) Steps 520(538.33) | Grad Norm 16.6133(9.8822) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 14.4564(14.4863) | Bit/dim 4.4416(4.4690) | Xent 0.0000(0.0000) | Loss 9.7799(9.9200) | Error 0.0000(0.0000) Steps 574(539.46) | Grad Norm 22.0554(12.1657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 75.5310, Epoch Time 887.1992(757.6315), Bit/dim 4.4025(best: 4.5548), Xent 0.0000, Loss 4.4025, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 14.9929(14.5163) | Bit/dim 4.4112(4.4614) | Xent 0.0000(0.0000) | Loss 9.5896(10.3548) | Error 0.0000(0.0000) Steps 532(538.14) | Grad Norm 13.8676(13.9087) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 15.2330(14.5516) | Bit/dim 4.3806(4.4399) | Xent 0.0000(0.0000) | Loss 9.5127(10.1648) | Error 0.0000(0.0000) Steps 550(537.80) | Grad Norm 3.3930(12.2740) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 13.7403(14.5302) | Bit/dim 4.3674(4.4277) | Xent 0.0000(0.0000) | Loss 9.6300(10.0223) | Error 0.0000(0.0000) Steps 538(537.33) | Grad Norm 10.1012(13.4218) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 13.8872(14.4305) | Bit/dim 4.3949(4.4273) | Xent 0.0000(0.0000) | Loss 9.5219(9.9394) | Error 0.0000(0.0000) Steps 514(534.98) | Grad Norm 11.5694(15.2899) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 14.0728(14.4067) | Bit/dim 4.3831(4.4120) | Xent 0.0000(0.0000) | Loss 9.5215(9.8588) | Error 0.0000(0.0000) Steps 514(536.03) | Grad Norm 7.6548(13.6308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 75.7754, Epoch Time 889.1720(761.5777), Bit/dim 4.3345(best: 4.4025), Xent 0.0000, Loss 4.3345, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 13.2953(14.3874) | Bit/dim 4.3127(4.3893) | Xent 0.0000(0.0000) | Loss 9.4813(10.2952) | Error 0.0000(0.0000) Steps 526(536.19) | Grad Norm 9.8153(11.9348) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 14.5776(14.4604) | Bit/dim 4.3909(4.3779) | Xent 0.0000(0.0000) | Loss 9.6595(10.1074) | Error 0.0000(0.0000) Steps 556(539.27) | Grad Norm 29.9093(12.1349) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 14.8299(14.4873) | Bit/dim 4.3091(4.3686) | Xent 0.0000(0.0000) | Loss 9.4919(9.9644) | Error 0.0000(0.0000) Steps 502(537.46) | Grad Norm 7.5804(12.8983) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 14.3488(14.5241) | Bit/dim 4.2909(4.3537) | Xent 0.0000(0.0000) | Loss 9.4325(9.8352) | Error 0.0000(0.0000) Steps 520(537.20) | Grad Norm 12.7831(13.2663) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 14.3404(14.4469) | Bit/dim 4.2505(4.3390) | Xent 0.0000(0.0000) | Loss 9.3093(9.7277) | Error 0.0000(0.0000) Steps 508(535.34) | Grad Norm 8.1288(14.4303) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 14.8753(14.4597) | Bit/dim 4.2610(4.3226) | Xent 0.0000(0.0000) | Loss 9.3504(9.6475) | Error 0.0000(0.0000) Steps 538(537.92) | Grad Norm 17.6550(13.5709) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 75.1088, Epoch Time 889.9103(765.4277), Bit/dim 4.2737(best: 4.3345), Xent 0.0000, Loss 4.2737, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 14.8109(14.5328) | Bit/dim 4.2921(4.3076) | Xent 0.0000(0.0000) | Loss 9.5994(10.0224) | Error 0.0000(0.0000) Steps 562(539.18) | Grad Norm 31.7213(14.4288) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 14.0049(14.4879) | Bit/dim 4.3501(4.3109) | Xent 0.0000(0.0000) | Loss 9.4090(9.8832) | Error 0.0000(0.0000) Steps 526(538.62) | Grad Norm 18.6089(16.6013) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 13.5876(14.4399) | Bit/dim 4.2052(4.3030) | Xent 0.0000(0.0000) | Loss 9.1573(9.7579) | Error 0.0000(0.0000) Steps 532(536.56) | Grad Norm 5.0730(15.4604) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 15.1072(14.6628) | Bit/dim 4.2367(4.2851) | Xent 0.0000(0.0000) | Loss 9.3026(9.6583) | Error 0.0000(0.0000) Steps 514(540.82) | Grad Norm 6.9260(13.4986) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 14.8268(14.6931) | Bit/dim 4.3228(4.2685) | Xent 0.0000(0.0000) | Loss 9.4861(9.5705) | Error 0.0000(0.0000) Steps 538(540.16) | Grad Norm 35.7567(13.0427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 75.6839, Epoch Time 900.8285(769.4897), Bit/dim 4.2547(best: 4.2737), Xent 0.0000, Loss 4.2547, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 15.2366(14.6854) | Bit/dim 4.2380(4.2684) | Xent 0.0000(0.0000) | Loss 9.3358(10.0465) | Error 0.0000(0.0000) Steps 574(541.00) | Grad Norm 10.0614(14.5522) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 13.8595(14.7154) | Bit/dim 4.2000(4.2529) | Xent 0.0000(0.0000) | Loss 9.2529(9.8530) | Error 0.0000(0.0000) Steps 538(540.88) | Grad Norm 7.3372(12.9244) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 14.6134(14.7570) | Bit/dim 4.1365(4.2301) | Xent 0.0000(0.0000) | Loss 9.1346(9.6777) | Error 0.0000(0.0000) Steps 556(542.87) | Grad Norm 8.1897(11.1770) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 14.3155(14.7827) | Bit/dim 4.1991(4.2303) | Xent 0.0000(0.0000) | Loss 9.2179(9.5888) | Error 0.0000(0.0000) Steps 544(542.87) | Grad Norm 12.4732(13.8772) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 13.8227(14.7436) | Bit/dim 4.1492(4.2148) | Xent 0.0000(0.0000) | Loss 8.9991(9.4858) | Error 0.0000(0.0000) Steps 514(541.16) | Grad Norm 5.3281(12.7234) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 14.7389(14.7584) | Bit/dim 4.1351(4.1971) | Xent 0.0000(0.0000) | Loss 9.0878(9.3918) | Error 0.0000(0.0000) Steps 550(537.90) | Grad Norm 4.3627(11.2625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 75.6957, Epoch Time 908.2444(773.6524), Bit/dim 4.1347(best: 4.2547), Xent 0.0000, Loss 4.1347, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 14.7544(14.7368) | Bit/dim 4.1693(4.1801) | Xent 0.0000(0.0000) | Loss 9.1070(9.7696) | Error 0.0000(0.0000) Steps 544(539.61) | Grad Norm 30.6338(11.8567) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 15.2512(14.7585) | Bit/dim 4.1785(4.1831) | Xent 0.0000(0.0000) | Loss 9.1834(9.6286) | Error 0.0000(0.0000) Steps 556(539.23) | Grad Norm 17.9792(13.8188) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 15.5166(14.8072) | Bit/dim 4.1133(4.1727) | Xent 0.0000(0.0000) | Loss 9.1393(9.5062) | Error 0.0000(0.0000) Steps 544(539.64) | Grad Norm 7.9854(12.6983) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 15.5359(14.9092) | Bit/dim 4.0916(4.1541) | Xent 0.0000(0.0000) | Loss 9.0926(9.3984) | Error 0.0000(0.0000) Steps 580(545.07) | Grad Norm 4.8894(10.8984) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 14.1467(14.7809) | Bit/dim 4.1300(4.1370) | Xent 0.0000(0.0000) | Loss 9.1198(9.2963) | Error 0.0000(0.0000) Steps 532(546.75) | Grad Norm 21.2793(10.0917) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 74.7636, Epoch Time 904.0439(777.5641), Bit/dim 4.1555(best: 4.1347), Xent 0.0000, Loss 4.1555, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 14.8376(14.7121) | Bit/dim 4.1346(4.1332) | Xent 0.0000(0.0000) | Loss 9.1420(9.7388) | Error 0.0000(0.0000) Steps 520(542.60) | Grad Norm 16.1998(12.5394) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 15.0365(14.7198) | Bit/dim 4.0784(4.1224) | Xent 0.0000(0.0000) | Loss 9.1283(9.5509) | Error 0.0000(0.0000) Steps 538(541.83) | Grad Norm 9.8391(12.1310) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 14.9521(14.7899) | Bit/dim 4.0498(4.1088) | Xent 0.0000(0.0000) | Loss 8.8763(9.4075) | Error 0.0000(0.0000) Steps 544(544.82) | Grad Norm 5.5100(10.6738) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 14.5393(14.8874) | Bit/dim 4.0711(4.0961) | Xent 0.0000(0.0000) | Loss 8.9885(9.2985) | Error 0.0000(0.0000) Steps 556(547.56) | Grad Norm 9.6660(10.3381) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 14.0412(14.8118) | Bit/dim 4.0370(4.0798) | Xent 0.0000(0.0000) | Loss 8.9811(9.2103) | Error 0.0000(0.0000) Steps 544(546.58) | Grad Norm 15.2730(9.7982) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 14.9285(14.7701) | Bit/dim 4.0552(4.0709) | Xent 0.0000(0.0000) | Loss 8.9112(9.1466) | Error 0.0000(0.0000) Steps 568(550.08) | Grad Norm 9.2700(9.2512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 76.7940, Epoch Time 910.2906(781.5459), Bit/dim 4.0435(best: 4.1347), Xent 0.0000, Loss 4.0435, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 14.7320(14.7686) | Bit/dim 4.0343(4.0624) | Xent 0.0000(0.0000) | Loss 8.9980(9.5503) | Error 0.0000(0.0000) Steps 568(553.29) | Grad Norm 11.2074(9.9589) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 14.7897(14.8089) | Bit/dim 4.0502(4.0513) | Xent 0.0000(0.0000) | Loss 8.9367(9.3805) | Error 0.0000(0.0000) Steps 568(553.54) | Grad Norm 15.8823(9.7603) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 14.2322(14.8500) | Bit/dim 4.0142(4.0456) | Xent 0.0000(0.0000) | Loss 8.8476(9.2552) | Error 0.0000(0.0000) Steps 550(555.01) | Grad Norm 4.9728(10.0000) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 14.3847(14.7958) | Bit/dim 4.0366(4.0375) | Xent 0.0000(0.0000) | Loss 8.9367(9.1588) | Error 0.0000(0.0000) Steps 562(556.07) | Grad Norm 18.8029(9.9842) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 14.4618(14.7201) | Bit/dim 4.0080(4.0325) | Xent 0.0000(0.0000) | Loss 8.8344(9.0848) | Error 0.0000(0.0000) Steps 538(554.38) | Grad Norm 11.3754(11.1757) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 76.1422, Epoch Time 906.9120(785.3069), Bit/dim 4.0024(best: 4.0435), Xent 0.0000, Loss 4.0024, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 13.9725(14.7272) | Bit/dim 4.0359(4.0206) | Xent 0.0000(0.0000) | Loss 8.8821(9.5563) | Error 0.0000(0.0000) Steps 538(555.39) | Grad Norm 11.5377(10.7258) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 14.6532(14.7599) | Bit/dim 3.9792(4.0131) | Xent 0.0000(0.0000) | Loss 8.8526(9.3674) | Error 0.0000(0.0000) Steps 580(554.80) | Grad Norm 8.2983(10.1456) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 14.7069(14.7215) | Bit/dim 4.0671(4.0075) | Xent 0.0000(0.0000) | Loss 8.9178(9.2247) | Error 0.0000(0.0000) Steps 574(553.63) | Grad Norm 28.7364(10.4956) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 14.9980(14.7585) | Bit/dim 3.9899(4.0024) | Xent 0.0000(0.0000) | Loss 8.8972(9.1262) | Error 0.0000(0.0000) Steps 574(555.64) | Grad Norm 4.6415(10.7073) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 13.7909(14.7469) | Bit/dim 3.9698(3.9964) | Xent 0.0000(0.0000) | Loss 8.7493(9.0516) | Error 0.0000(0.0000) Steps 538(554.45) | Grad Norm 9.1024(10.4783) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 14.7526(14.7534) | Bit/dim 3.9633(3.9893) | Xent 0.0000(0.0000) | Loss 8.7447(8.9901) | Error 0.0000(0.0000) Steps 544(554.30) | Grad Norm 10.9824(10.2952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 77.0412, Epoch Time 906.9755(788.9570), Bit/dim 3.9726(best: 4.0024), Xent 0.0000, Loss 3.9726, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 14.3637(14.7543) | Bit/dim 3.9690(3.9818) | Xent 0.0000(0.0000) | Loss 8.7957(9.3678) | Error 0.0000(0.0000) Steps 568(552.06) | Grad Norm 11.5937(9.6725) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.9687(14.7677) | Bit/dim 3.9447(3.9730) | Xent 0.0000(0.0000) | Loss 8.6935(9.2105) | Error 0.0000(0.0000) Steps 574(553.59) | Grad Norm 6.0725(9.1012) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.1371(14.8178) | Bit/dim 3.9440(3.9694) | Xent 0.0000(0.0000) | Loss 8.7590(9.0972) | Error 0.0000(0.0000) Steps 580(555.57) | Grad Norm 12.8597(9.9608) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 13.7264(14.7294) | Bit/dim 4.0161(3.9696) | Xent 0.0000(0.0000) | Loss 8.8473(9.0158) | Error 0.0000(0.0000) Steps 538(552.87) | Grad Norm 21.5211(10.7619) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.8255(14.7819) | Bit/dim 4.0200(3.9730) | Xent 0.0000(0.0000) | Loss 8.9273(8.9681) | Error 0.0000(0.0000) Steps 592(552.89) | Grad Norm 15.0667(12.1629) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 76.8279, Epoch Time 906.9250(792.4960), Bit/dim 3.9603(best: 3.9726), Xent 0.0000, Loss 3.9603, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 14.2659(14.7230) | Bit/dim 3.9399(3.9696) | Xent 0.0000(0.0000) | Loss 8.7225(9.4524) | Error 0.0000(0.0000) Steps 544(551.76) | Grad Norm 7.8215(10.9996) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 14.8776(14.7758) | Bit/dim 3.9162(3.9598) | Xent 0.0000(0.0000) | Loss 8.6737(9.2661) | Error 0.0000(0.0000) Steps 580(550.54) | Grad Norm 4.8414(9.4305) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 14.0945(14.7567) | Bit/dim 3.9510(3.9502) | Xent 0.0000(0.0000) | Loss 8.7334(9.1203) | Error 0.0000(0.0000) Steps 520(548.38) | Grad Norm 3.8602(7.8840) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 14.1431(14.6295) | Bit/dim 3.9259(3.9410) | Xent 0.0000(0.0000) | Loss 8.7745(9.0121) | Error 0.0000(0.0000) Steps 538(545.42) | Grad Norm 2.3300(6.6671) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 14.3564(14.5837) | Bit/dim 3.8997(3.9314) | Xent 0.0000(0.0000) | Loss 8.6902(8.9199) | Error 0.0000(0.0000) Steps 562(543.77) | Grad Norm 6.9059(6.3000) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 14.4212(14.5941) | Bit/dim 3.8898(3.9281) | Xent 0.0000(0.0000) | Loss 8.6683(8.8550) | Error 0.0000(0.0000) Steps 538(542.68) | Grad Norm 5.6485(7.9925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 76.2071, Epoch Time 897.2224(795.6378), Bit/dim 3.9167(best: 3.9603), Xent 0.0000, Loss 3.9167, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 14.6951(14.5860) | Bit/dim 3.9227(3.9244) | Xent 0.0000(0.0000) | Loss 8.7155(9.2634) | Error 0.0000(0.0000) Steps 568(545.55) | Grad Norm 5.8280(8.2336) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 15.1141(14.6388) | Bit/dim 3.8821(3.9192) | Xent 0.0000(0.0000) | Loss 8.6959(9.1190) | Error 0.0000(0.0000) Steps 550(545.74) | Grad Norm 4.1195(8.0168) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 14.2697(14.5084) | Bit/dim 3.9068(3.9124) | Xent 0.0000(0.0000) | Loss 8.7137(8.9963) | Error 0.0000(0.0000) Steps 574(547.35) | Grad Norm 13.3901(9.0426) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.6059(14.5757) | Bit/dim 3.9244(3.9074) | Xent 0.0000(0.0000) | Loss 8.7412(8.9040) | Error 0.0000(0.0000) Steps 562(549.97) | Grad Norm 17.0694(9.3616) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 14.6892(14.5353) | Bit/dim 3.8709(3.9034) | Xent 0.0000(0.0000) | Loss 8.6331(8.8382) | Error 0.0000(0.0000) Steps 562(552.31) | Grad Norm 3.5755(9.4079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 77.1543, Epoch Time 894.5709(798.6058), Bit/dim 3.8873(best: 3.9167), Xent 0.0000, Loss 3.8873, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 14.6252(14.5112) | Bit/dim 3.8742(3.9016) | Xent 0.0000(0.0000) | Loss 8.6694(9.3259) | Error 0.0000(0.0000) Steps 574(550.75) | Grad Norm 8.5038(9.2022) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 14.0411(14.5197) | Bit/dim 3.8803(3.8985) | Xent 0.0000(0.0000) | Loss 8.5977(9.1495) | Error 0.0000(0.0000) Steps 544(551.35) | Grad Norm 8.6320(9.9006) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 14.3915(14.5163) | Bit/dim 3.8875(3.8942) | Xent 0.0000(0.0000) | Loss 8.6165(9.0131) | Error 0.0000(0.0000) Steps 562(550.59) | Grad Norm 13.0449(10.0029) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 14.0392(14.5661) | Bit/dim 3.8444(3.8867) | Xent 0.0000(0.0000) | Loss 8.4391(8.9062) | Error 0.0000(0.0000) Steps 538(550.94) | Grad Norm 14.9343(9.7314) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 14.6787(14.5816) | Bit/dim 3.8458(3.8804) | Xent 0.0000(0.0000) | Loss 8.6748(8.8304) | Error 0.0000(0.0000) Steps 562(548.89) | Grad Norm 10.1517(9.9511) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 14.1238(14.5105) | Bit/dim 3.8171(3.8760) | Xent 0.0000(0.0000) | Loss 8.4116(8.7592) | Error 0.0000(0.0000) Steps 538(549.62) | Grad Norm 4.1825(8.9753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 76.5927, Epoch Time 892.8526(801.4332), Bit/dim 3.8481(best: 3.8873), Xent 0.0000, Loss 3.8481, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 13.9724(14.4511) | Bit/dim 3.8512(3.8712) | Xent 0.0000(0.0000) | Loss 8.4721(9.1437) | Error 0.0000(0.0000) Steps 508(543.03) | Grad Norm 3.9118(9.3297) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 14.8181(14.3741) | Bit/dim 3.8285(3.8652) | Xent 0.0000(0.0000) | Loss 8.5171(8.9910) | Error 0.0000(0.0000) Steps 538(541.12) | Grad Norm 8.3157(9.4421) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 13.4272(14.3515) | Bit/dim 3.8419(3.8632) | Xent 0.0000(0.0000) | Loss 8.6007(8.8789) | Error 0.0000(0.0000) Steps 538(542.48) | Grad Norm 13.9516(9.9623) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 14.7283(14.3719) | Bit/dim 3.8531(3.8628) | Xent 0.0000(0.0000) | Loss 8.6133(8.8046) | Error 0.0000(0.0000) Steps 556(542.67) | Grad Norm 14.4736(10.1843) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.8036(14.3939) | Bit/dim 3.8394(3.8568) | Xent 0.0000(0.0000) | Loss 8.6252(8.7424) | Error 0.0000(0.0000) Steps 520(543.28) | Grad Norm 12.4150(9.9076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 76.2400, Epoch Time 881.4492(803.8337), Bit/dim 3.8396(best: 3.8481), Xent 0.0000, Loss 3.8396, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 14.1077(14.3606) | Bit/dim 3.8078(3.8525) | Xent 0.0000(0.0000) | Loss 8.5380(9.2066) | Error 0.0000(0.0000) Steps 520(542.61) | Grad Norm 4.9680(9.5395) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 13.6974(14.3635) | Bit/dim 3.8108(3.8481) | Xent 0.0000(0.0000) | Loss 8.5263(9.0396) | Error 0.0000(0.0000) Steps 526(542.63) | Grad Norm 15.2828(9.9840) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 14.4350(14.4210) | Bit/dim 3.8449(3.8427) | Xent 0.0000(0.0000) | Loss 8.4877(8.9031) | Error 0.0000(0.0000) Steps 568(544.69) | Grad Norm 9.5675(9.8151) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 13.8599(14.3820) | Bit/dim 3.8187(3.8396) | Xent 0.0000(0.0000) | Loss 8.5163(8.8110) | Error 0.0000(0.0000) Steps 544(545.73) | Grad Norm 8.4096(10.0617) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 14.5050(14.3351) | Bit/dim 3.8503(3.8351) | Xent 0.0000(0.0000) | Loss 8.4779(8.7290) | Error 0.0000(0.0000) Steps 520(546.14) | Grad Norm 19.4986(10.3307) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 14.6465(14.3405) | Bit/dim 3.8126(3.8342) | Xent 0.0000(0.0000) | Loss 8.6067(8.6796) | Error 0.0000(0.0000) Steps 574(545.39) | Grad Norm 7.6948(10.3168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 76.5735, Epoch Time 884.1034(806.2418), Bit/dim 3.8163(best: 3.8396), Xent 0.0000, Loss 3.8163, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.0193(14.3384) | Bit/dim 3.8201(3.8299) | Xent 0.0000(0.0000) | Loss 8.5698(9.0693) | Error 0.0000(0.0000) Steps 562(545.14) | Grad Norm 10.4076(9.2750) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 13.6593(14.2870) | Bit/dim 3.7880(3.8233) | Xent 0.0000(0.0000) | Loss 8.2682(8.9027) | Error 0.0000(0.0000) Steps 544(544.99) | Grad Norm 8.4249(9.3258) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 14.8553(14.2750) | Bit/dim 3.7778(3.8212) | Xent 0.0000(0.0000) | Loss 8.4141(8.7916) | Error 0.0000(0.0000) Steps 580(546.21) | Grad Norm 11.5775(9.3879) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 14.6477(14.2479) | Bit/dim 3.8548(3.8208) | Xent 0.0000(0.0000) | Loss 8.5988(8.7227) | Error 0.0000(0.0000) Steps 574(547.31) | Grad Norm 21.3440(10.2116) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.7167(14.3050) | Bit/dim 3.8187(3.8195) | Xent 0.0000(0.0000) | Loss 8.6013(8.6708) | Error 0.0000(0.0000) Steps 586(552.18) | Grad Norm 10.2178(10.5369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 75.9776, Epoch Time 876.9738(808.3637), Bit/dim 3.8023(best: 3.8163), Xent 0.0000, Loss 3.8023, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 13.7144(14.2239) | Bit/dim 3.7679(3.8153) | Xent 0.0000(0.0000) | Loss 8.3852(9.1355) | Error 0.0000(0.0000) Steps 550(550.40) | Grad Norm 8.0457(9.7742) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.3236(14.2863) | Bit/dim 3.8342(3.8111) | Xent 0.0000(0.0000) | Loss 8.5874(8.9678) | Error 0.0000(0.0000) Steps 568(550.09) | Grad Norm 14.6062(10.0483) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 13.8717(14.2919) | Bit/dim 3.8217(3.8102) | Xent 0.0000(0.0000) | Loss 8.5195(8.8349) | Error 0.0000(0.0000) Steps 544(549.17) | Grad Norm 9.6874(9.9709) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 14.1753(14.3536) | Bit/dim 3.7663(3.8047) | Xent 0.0000(0.0000) | Loss 8.5064(8.7322) | Error 0.0000(0.0000) Steps 568(549.52) | Grad Norm 7.5489(10.5177) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 13.7046(14.2308) | Bit/dim 3.7735(3.7981) | Xent 0.0000(0.0000) | Loss 8.4263(8.6488) | Error 0.0000(0.0000) Steps 538(547.64) | Grad Norm 9.8708(10.3542) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 13.9175(14.1909) | Bit/dim 3.7751(3.7951) | Xent 0.0000(0.0000) | Loss 8.4380(8.5910) | Error 0.0000(0.0000) Steps 538(543.31) | Grad Norm 5.3353(9.2910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 75.9366, Epoch Time 877.5054(810.4380), Bit/dim 3.7805(best: 3.8023), Xent 0.0000, Loss 3.7805, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 14.5271(14.2274) | Bit/dim 3.7488(3.7893) | Xent 0.0000(0.0000) | Loss 8.5002(8.9946) | Error 0.0000(0.0000) Steps 550(543.21) | Grad Norm 9.0615(9.1343) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 14.3657(14.2224) | Bit/dim 3.8048(3.7899) | Xent 0.0000(0.0000) | Loss 8.4274(8.8507) | Error 0.0000(0.0000) Steps 532(542.07) | Grad Norm 12.5597(10.0933) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 14.1992(14.3049) | Bit/dim 3.7748(3.7881) | Xent 0.0000(0.0000) | Loss 8.3230(8.7417) | Error 0.0000(0.0000) Steps 526(541.45) | Grad Norm 7.5366(9.6906) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 13.2622(14.3269) | Bit/dim 3.7823(3.7875) | Xent 0.0000(0.0000) | Loss 8.5054(8.6579) | Error 0.0000(0.0000) Steps 544(543.49) | Grad Norm 7.1237(10.6049) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 14.0369(14.3105) | Bit/dim 3.7621(3.7854) | Xent 0.0000(0.0000) | Loss 8.4815(8.6041) | Error 0.0000(0.0000) Steps 532(541.82) | Grad Norm 11.2962(10.7185) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 76.6906, Epoch Time 882.6379(812.6040), Bit/dim 3.7724(best: 3.7805), Xent 0.0000, Loss 3.7724, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 13.7480(14.2854) | Bit/dim 3.7779(3.7807) | Xent 0.0000(0.0000) | Loss 8.3676(9.0710) | Error 0.0000(0.0000) Steps 544(540.56) | Grad Norm 4.8411(9.9202) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 14.4007(14.2015) | Bit/dim 3.7911(3.7787) | Xent 0.0000(0.0000) | Loss 8.4793(8.9038) | Error 0.0000(0.0000) Steps 562(542.39) | Grad Norm 9.6589(9.6232) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 13.8002(14.1907) | Bit/dim 3.7669(3.7753) | Xent 0.0000(0.0000) | Loss 8.4325(8.7805) | Error 0.0000(0.0000) Steps 550(542.09) | Grad Norm 14.7261(9.8967) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 14.2197(14.3009) | Bit/dim 3.7782(3.7731) | Xent 0.0000(0.0000) | Loss 8.4747(8.6826) | Error 0.0000(0.0000) Steps 568(545.08) | Grad Norm 9.2158(10.1815) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 13.2046(14.3012) | Bit/dim 3.7410(3.7700) | Xent 0.0000(0.0000) | Loss 8.2721(8.6016) | Error 0.0000(0.0000) Steps 520(543.05) | Grad Norm 7.9481(9.7592) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 13.9466(14.2942) | Bit/dim 3.7330(3.7671) | Xent 0.0000(0.0000) | Loss 8.3647(8.5403) | Error 0.0000(0.0000) Steps 562(542.76) | Grad Norm 3.6042(8.9538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 76.7913, Epoch Time 879.8677(814.6219), Bit/dim 3.7532(best: 3.7724), Xent 0.0000, Loss 3.7532, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 14.5442(14.2760) | Bit/dim 3.7682(3.7662) | Xent 0.0000(0.0000) | Loss 8.4070(8.9533) | Error 0.0000(0.0000) Steps 514(543.59) | Grad Norm 15.2146(9.2928) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 13.9471(14.3843) | Bit/dim 3.7265(3.7607) | Xent 0.0000(0.0000) | Loss 8.3192(8.8021) | Error 0.0000(0.0000) Steps 532(546.00) | Grad Norm 10.1164(9.3021) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 14.6331(14.4374) | Bit/dim 3.7599(3.7579) | Xent 0.0000(0.0000) | Loss 8.4636(8.6908) | Error 0.0000(0.0000) Steps 562(545.18) | Grad Norm 4.7323(9.1882) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 13.7925(14.4120) | Bit/dim 3.7401(3.7590) | Xent 0.0000(0.0000) | Loss 8.4401(8.6120) | Error 0.0000(0.0000) Steps 562(545.00) | Grad Norm 9.6829(9.4375) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 14.3768(14.4391) | Bit/dim 3.7535(3.7560) | Xent 0.0000(0.0000) | Loss 8.2558(8.5429) | Error 0.0000(0.0000) Steps 562(546.06) | Grad Norm 4.8466(8.2739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 76.1741, Epoch Time 891.3240(816.9229), Bit/dim 3.7681(best: 3.7532), Xent 0.0000, Loss 3.7681, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 14.0227(14.4463) | Bit/dim 3.7681(3.7587) | Xent 0.0000(0.0000) | Loss 8.4076(9.0359) | Error 0.0000(0.0000) Steps 526(547.40) | Grad Norm 14.2688(9.5309) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.7002(14.4451) | Bit/dim 3.7491(3.7527) | Xent 0.0000(0.0000) | Loss 8.4523(8.8570) | Error 0.0000(0.0000) Steps 604(548.11) | Grad Norm 9.0889(9.9336) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 14.1776(14.4280) | Bit/dim 3.7347(3.7486) | Xent 0.0000(0.0000) | Loss 8.3470(8.7326) | Error 0.0000(0.0000) Steps 562(551.52) | Grad Norm 7.8401(9.3926) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 13.4229(14.3647) | Bit/dim 3.7432(3.7448) | Xent 0.0000(0.0000) | Loss 8.3273(8.6265) | Error 0.0000(0.0000) Steps 550(550.96) | Grad Norm 8.5844(8.3561) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.0879(14.3917) | Bit/dim 3.7580(3.7439) | Xent 0.0000(0.0000) | Loss 8.4799(8.5556) | Error 0.0000(0.0000) Steps 568(554.07) | Grad Norm 5.7555(8.7255) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 14.7397(14.4017) | Bit/dim 3.7431(3.7414) | Xent 0.0000(0.0000) | Loss 8.3658(8.4925) | Error 0.0000(0.0000) Steps 550(553.14) | Grad Norm 9.9386(8.7773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 77.6468, Epoch Time 885.4694(818.9793), Bit/dim 3.7402(best: 3.7532), Xent 0.0000, Loss 3.7402, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 14.7980(14.4296) | Bit/dim 3.7431(3.7408) | Xent 0.0000(0.0000) | Loss 8.3637(8.9200) | Error 0.0000(0.0000) Steps 556(555.40) | Grad Norm 17.7245(9.4997) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 14.0891(14.4978) | Bit/dim 3.7552(3.7402) | Xent 0.0000(0.0000) | Loss 8.3931(8.7711) | Error 0.0000(0.0000) Steps 562(554.71) | Grad Norm 9.6907(9.9826) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.2925(14.5414) | Bit/dim 3.7214(3.7399) | Xent 0.0000(0.0000) | Loss 8.2427(8.6545) | Error 0.0000(0.0000) Steps 532(552.60) | Grad Norm 8.6710(9.5881) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 14.8108(14.5639) | Bit/dim 3.7057(3.7351) | Xent 0.0000(0.0000) | Loss 8.3065(8.5656) | Error 0.0000(0.0000) Steps 562(551.69) | Grad Norm 8.3185(9.5005) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 13.7606(14.5076) | Bit/dim 3.7373(3.7333) | Xent 0.0000(0.0000) | Loss 8.3247(8.5096) | Error 0.0000(0.0000) Steps 526(550.73) | Grad Norm 6.5024(9.2360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 75.6981, Epoch Time 895.1779(821.2653), Bit/dim 3.7302(best: 3.7402), Xent 0.0000, Loss 3.7302, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 15.0590(14.5209) | Bit/dim 3.7189(3.7293) | Xent 0.0000(0.0000) | Loss 8.3218(8.9685) | Error 0.0000(0.0000) Steps 556(551.47) | Grad Norm 5.4485(9.0515) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 13.9470(14.5362) | Bit/dim 3.7449(3.7277) | Xent 0.0000(0.0000) | Loss 8.3226(8.8040) | Error 0.0000(0.0000) Steps 574(550.22) | Grad Norm 8.8797(8.8349) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 14.2217(14.4814) | Bit/dim 3.6970(3.7253) | Xent 0.0000(0.0000) | Loss 8.1949(8.6706) | Error 0.0000(0.0000) Steps 544(547.77) | Grad Norm 3.1237(7.9179) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 14.7354(14.4650) | Bit/dim 3.7059(3.7225) | Xent 0.0000(0.0000) | Loss 8.3811(8.5754) | Error 0.0000(0.0000) Steps 526(547.71) | Grad Norm 13.5075(8.7221) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.4459(14.4121) | Bit/dim 3.7347(3.7212) | Xent 0.0000(0.0000) | Loss 8.3917(8.5071) | Error 0.0000(0.0000) Steps 598(548.76) | Grad Norm 7.9001(8.9557) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 14.9344(14.4256) | Bit/dim 3.7353(3.7210) | Xent 0.0000(0.0000) | Loss 8.4682(8.4584) | Error 0.0000(0.0000) Steps 586(552.22) | Grad Norm 9.1422(8.5163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 74.8148, Epoch Time 885.6281(823.1962), Bit/dim 3.7123(best: 3.7302), Xent 0.0000, Loss 3.7123, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 14.4953(14.4588) | Bit/dim 3.7088(3.7177) | Xent 0.0000(0.0000) | Loss 8.2492(8.8565) | Error 0.0000(0.0000) Steps 532(553.04) | Grad Norm 7.6788(8.9454) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 14.9255(14.4671) | Bit/dim 3.7012(3.7153) | Xent 0.0000(0.0000) | Loss 8.4092(8.7037) | Error 0.0000(0.0000) Steps 538(549.96) | Grad Norm 7.0315(8.4789) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.2112(14.4269) | Bit/dim 3.7051(3.7133) | Xent 0.0000(0.0000) | Loss 8.3599(8.5932) | Error 0.0000(0.0000) Steps 544(547.84) | Grad Norm 12.0343(8.6641) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.1023(14.3681) | Bit/dim 3.7256(3.7134) | Xent 0.0000(0.0000) | Loss 8.2306(8.5138) | Error 0.0000(0.0000) Steps 544(548.12) | Grad Norm 13.1716(8.5023) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 13.9663(14.3919) | Bit/dim 3.7343(3.7110) | Xent 0.0000(0.0000) | Loss 8.2953(8.4455) | Error 0.0000(0.0000) Steps 526(545.53) | Grad Norm 6.9438(8.9542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 76.1176, Epoch Time 886.6358(825.0994), Bit/dim 3.7056(best: 3.7123), Xent 0.0000, Loss 3.7056, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 14.6042(14.4569) | Bit/dim 3.6701(3.7099) | Xent 0.0000(0.0000) | Loss 8.0126(8.9215) | Error 0.0000(0.0000) Steps 526(544.77) | Grad Norm 7.0811(8.7141) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 14.4379(14.4149) | Bit/dim 3.6828(3.7082) | Xent 0.0000(0.0000) | Loss 8.1564(8.7510) | Error 0.0000(0.0000) Steps 574(545.64) | Grad Norm 3.6198(8.7358) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 13.9427(14.3942) | Bit/dim 3.7194(3.7071) | Xent 0.0000(0.0000) | Loss 8.3441(8.6255) | Error 0.0000(0.0000) Steps 556(546.31) | Grad Norm 4.6799(7.9782) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 14.0271(14.4335) | Bit/dim 3.7196(3.7078) | Xent 0.0000(0.0000) | Loss 8.2335(8.5390) | Error 0.0000(0.0000) Steps 556(547.96) | Grad Norm 7.6993(8.4331) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.2988(14.3843) | Bit/dim 3.7073(3.7054) | Xent 0.0000(0.0000) | Loss 8.2679(8.4616) | Error 0.0000(0.0000) Steps 544(548.36) | Grad Norm 15.5391(8.4781) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.2527(14.4347) | Bit/dim 3.6947(3.7033) | Xent 0.0000(0.0000) | Loss 8.3598(8.4104) | Error 0.0000(0.0000) Steps 562(548.70) | Grad Norm 11.1284(8.7808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 75.3326, Epoch Time 886.1790(826.9318), Bit/dim 3.7108(best: 3.7056), Xent 0.0000, Loss 3.7108, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 14.2114(14.4222) | Bit/dim 3.6779(3.7019) | Xent 0.0000(0.0000) | Loss 8.1002(8.7963) | Error 0.0000(0.0000) Steps 502(548.21) | Grad Norm 3.9106(8.5516) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 15.3767(14.3744) | Bit/dim 3.7075(3.7011) | Xent 0.0000(0.0000) | Loss 8.2872(8.6544) | Error 0.0000(0.0000) Steps 526(548.80) | Grad Norm 15.4765(8.3379) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 13.7696(14.3012) | Bit/dim 3.7301(3.7007) | Xent 0.0000(0.0000) | Loss 8.3382(8.5476) | Error 0.0000(0.0000) Steps 568(547.22) | Grad Norm 9.7941(8.5341) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 14.4807(14.3415) | Bit/dim 3.6919(3.6987) | Xent 0.0000(0.0000) | Loss 8.3097(8.4730) | Error 0.0000(0.0000) Steps 532(549.35) | Grad Norm 8.8796(8.9927) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.3115(14.3543) | Bit/dim 3.6940(3.6954) | Xent 0.0000(0.0000) | Loss 8.2563(8.4020) | Error 0.0000(0.0000) Steps 550(549.56) | Grad Norm 11.3369(8.7797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 75.2880, Epoch Time 881.2551(828.5615), Bit/dim 3.6872(best: 3.7056), Xent 0.0000, Loss 3.6872, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 14.2618(14.3629) | Bit/dim 3.6761(3.6944) | Xent 0.0000(0.0000) | Loss 8.1330(8.8848) | Error 0.0000(0.0000) Steps 526(546.81) | Grad Norm 7.5330(8.0226) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 14.9055(14.3648) | Bit/dim 3.7011(3.6976) | Xent 0.0000(0.0000) | Loss 8.2970(8.7211) | Error 0.0000(0.0000) Steps 544(548.28) | Grad Norm 4.6953(8.7254) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 14.4133(14.3599) | Bit/dim 3.6682(3.6934) | Xent 0.0000(0.0000) | Loss 8.2136(8.5960) | Error 0.0000(0.0000) Steps 580(546.66) | Grad Norm 11.5787(9.0280) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 14.9993(14.3919) | Bit/dim 3.6689(3.6900) | Xent 0.0000(0.0000) | Loss 8.2265(8.5023) | Error 0.0000(0.0000) Steps 568(547.90) | Grad Norm 3.5307(8.5866) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 14.2158(14.4460) | Bit/dim 3.6972(3.6924) | Xent 0.0000(0.0000) | Loss 8.2614(8.4393) | Error 0.0000(0.0000) Steps 562(549.35) | Grad Norm 7.5199(8.6120) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 14.4322(14.4650) | Bit/dim 3.6593(3.6867) | Xent 0.0000(0.0000) | Loss 8.1584(8.3810) | Error 0.0000(0.0000) Steps 544(549.00) | Grad Norm 12.3920(8.1813) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 76.0892, Epoch Time 888.6486(830.3641), Bit/dim 3.6943(best: 3.6872), Xent 0.0000, Loss 3.6943, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 14.5269(14.5275) | Bit/dim 3.6835(3.6847) | Xent 0.0000(0.0000) | Loss 8.3372(8.7949) | Error 0.0000(0.0000) Steps 544(549.11) | Grad Norm 2.4100(7.5715) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 14.5264(14.5062) | Bit/dim 3.6684(3.6819) | Xent 0.0000(0.0000) | Loss 8.2127(8.6368) | Error 0.0000(0.0000) Steps 532(548.86) | Grad Norm 11.2945(7.1784) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 14.0903(14.5178) | Bit/dim 3.6917(3.6840) | Xent 0.0000(0.0000) | Loss 8.0823(8.5318) | Error 0.0000(0.0000) Steps 544(551.88) | Grad Norm 7.7123(7.0007) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 14.4503(14.5493) | Bit/dim 3.7176(3.6838) | Xent 0.0000(0.0000) | Loss 8.1655(8.4509) | Error 0.0000(0.0000) Steps 526(550.77) | Grad Norm 9.0004(8.3564) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 14.2272(14.5288) | Bit/dim 3.6700(3.6812) | Xent 0.0000(0.0000) | Loss 8.2122(8.3843) | Error 0.0000(0.0000) Steps 526(548.11) | Grad Norm 9.0633(8.2809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 76.0757, Epoch Time 895.7800(832.3265), Bit/dim 3.6787(best: 3.6872), Xent 0.0000, Loss 3.6787, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 14.2476(14.5607) | Bit/dim 3.6933(3.6812) | Xent 0.0000(0.0000) | Loss 8.2597(8.8806) | Error 0.0000(0.0000) Steps 532(546.20) | Grad Norm 4.1494(8.0040) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 13.9140(14.5082) | Bit/dim 3.6881(3.6788) | Xent 0.0000(0.0000) | Loss 8.2337(8.7085) | Error 0.0000(0.0000) Steps 544(549.17) | Grad Norm 6.3955(7.4342) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 13.8104(14.4550) | Bit/dim 3.6385(3.6768) | Xent 0.0000(0.0000) | Loss 8.1602(8.5691) | Error 0.0000(0.0000) Steps 538(546.95) | Grad Norm 6.5261(7.3571) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 14.3525(14.4393) | Bit/dim 3.6743(3.6767) | Xent 0.0000(0.0000) | Loss 8.1617(8.4796) | Error 0.0000(0.0000) Steps 586(548.03) | Grad Norm 11.3912(8.5134) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 14.2459(14.3971) | Bit/dim 3.6768(3.6778) | Xent 0.0000(0.0000) | Loss 8.1674(8.4140) | Error 0.0000(0.0000) Steps 538(547.19) | Grad Norm 3.9692(8.4598) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 14.2801(14.4326) | Bit/dim 3.6928(3.6764) | Xent 0.0000(0.0000) | Loss 8.2396(8.3657) | Error 0.0000(0.0000) Steps 574(548.16) | Grad Norm 3.3328(8.3360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 75.3433, Epoch Time 885.0195(833.9073), Bit/dim 3.6659(best: 3.6787), Xent 0.0000, Loss 3.6659, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 14.5647(14.3922) | Bit/dim 3.6662(3.6737) | Xent 0.0000(0.0000) | Loss 8.2534(8.7712) | Error 0.0000(0.0000) Steps 538(546.59) | Grad Norm 8.3394(7.6551) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 14.8446(14.3805) | Bit/dim 3.6254(3.6692) | Xent 0.0000(0.0000) | Loss 8.1737(8.6192) | Error 0.0000(0.0000) Steps 562(546.07) | Grad Norm 5.4632(7.4166) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 14.5698(14.3532) | Bit/dim 3.6646(3.6684) | Xent 0.0000(0.0000) | Loss 8.1815(8.5036) | Error 0.0000(0.0000) Steps 556(544.53) | Grad Norm 5.0896(7.1666) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 14.9336(14.3251) | Bit/dim 3.6628(3.6663) | Xent 0.0000(0.0000) | Loss 8.2324(8.4157) | Error 0.0000(0.0000) Steps 556(545.08) | Grad Norm 14.9229(7.0268) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 14.2439(14.3720) | Bit/dim 3.6466(3.6697) | Xent 0.0000(0.0000) | Loss 8.0341(8.3690) | Error 0.0000(0.0000) Steps 526(545.32) | Grad Norm 5.2422(7.9966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 75.5014, Epoch Time 882.0065(835.3503), Bit/dim 3.6645(best: 3.6659), Xent 0.0000, Loss 3.6645, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 14.1347(14.4040) | Bit/dim 3.6989(3.6696) | Xent 0.0000(0.0000) | Loss 8.1923(8.8470) | Error 0.0000(0.0000) Steps 544(546.75) | Grad Norm 12.4839(7.9960) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 14.5313(14.3629) | Bit/dim 3.6602(3.6690) | Xent 0.0000(0.0000) | Loss 8.2481(8.6829) | Error 0.0000(0.0000) Steps 550(546.91) | Grad Norm 4.9015(7.9372) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 14.9835(14.3935) | Bit/dim 3.7000(3.6681) | Xent 0.0000(0.0000) | Loss 8.2661(8.5519) | Error 0.0000(0.0000) Steps 538(542.79) | Grad Norm 6.7457(7.4390) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 14.6427(14.3969) | Bit/dim 3.6606(3.6650) | Xent 0.0000(0.0000) | Loss 8.2670(8.4528) | Error 0.0000(0.0000) Steps 526(541.79) | Grad Norm 5.0054(7.7077) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 14.2084(14.3122) | Bit/dim 3.6374(3.6619) | Xent 0.0000(0.0000) | Loss 8.1455(8.3760) | Error 0.0000(0.0000) Steps 532(541.25) | Grad Norm 5.6797(7.2705) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 14.2702(14.2640) | Bit/dim 3.6714(3.6619) | Xent 0.0000(0.0000) | Loss 8.2051(8.3324) | Error 0.0000(0.0000) Steps 544(540.92) | Grad Norm 12.4679(7.6453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 74.9053, Epoch Time 879.2227(836.6665), Bit/dim 3.6609(best: 3.6645), Xent 0.0000, Loss 3.6609, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 14.5106(14.3033) | Bit/dim 3.6773(3.6585) | Xent 0.0000(0.0000) | Loss 8.2902(8.7381) | Error 0.0000(0.0000) Steps 544(541.76) | Grad Norm 14.0584(7.8407) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 13.9474(14.3528) | Bit/dim 3.7000(3.6607) | Xent 0.0000(0.0000) | Loss 8.2891(8.5881) | Error 0.0000(0.0000) Steps 550(544.87) | Grad Norm 6.1807(7.8152) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 14.8942(14.3307) | Bit/dim 3.6391(3.6583) | Xent 0.0000(0.0000) | Loss 8.1749(8.4741) | Error 0.0000(0.0000) Steps 586(544.72) | Grad Norm 4.8196(7.7903) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 14.4702(14.3506) | Bit/dim 3.6754(3.6557) | Xent 0.0000(0.0000) | Loss 8.2704(8.3939) | Error 0.0000(0.0000) Steps 550(543.52) | Grad Norm 6.4016(7.4316) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 14.0751(14.3951) | Bit/dim 3.6584(3.6551) | Xent 0.0000(0.0000) | Loss 8.0729(8.3335) | Error 0.0000(0.0000) Steps 544(544.16) | Grad Norm 9.8522(7.9066) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 74.8569, Epoch Time 885.4111(838.1288), Bit/dim 3.6538(best: 3.6609), Xent 0.0000, Loss 3.6538, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 14.9366(14.4093) | Bit/dim 3.7057(3.6565) | Xent 0.0000(0.0000) | Loss 8.2308(8.8000) | Error 0.0000(0.0000) Steps 532(542.53) | Grad Norm 8.4561(8.0936) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 14.3279(14.4495) | Bit/dim 3.6152(3.6542) | Xent 0.0000(0.0000) | Loss 8.0969(8.6252) | Error 0.0000(0.0000) Steps 538(545.69) | Grad Norm 3.9621(7.3400) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 14.8225(14.4343) | Bit/dim 3.6370(3.6533) | Xent 0.0000(0.0000) | Loss 8.1876(8.5075) | Error 0.0000(0.0000) Steps 556(546.03) | Grad Norm 9.0839(7.6864) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 14.1139(14.4079) | Bit/dim 3.6296(3.6514) | Xent 0.0000(0.0000) | Loss 8.0408(8.4074) | Error 0.0000(0.0000) Steps 550(544.81) | Grad Norm 7.3419(7.2895) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 14.9032(14.4242) | Bit/dim 3.6740(3.6501) | Xent 0.0000(0.0000) | Loss 8.2435(8.3401) | Error 0.0000(0.0000) Steps 538(545.29) | Grad Norm 3.8829(7.5049) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 14.6650(14.4592) | Bit/dim 3.6463(3.6495) | Xent 0.0000(0.0000) | Loss 8.2269(8.2949) | Error 0.0000(0.0000) Steps 562(545.77) | Grad Norm 9.5532(7.0546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 75.4491, Epoch Time 889.1970(839.6609), Bit/dim 3.6521(best: 3.6538), Xent 0.0000, Loss 3.6521, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.3545(14.4636) | Bit/dim 3.6046(3.6496) | Xent 0.0000(0.0000) | Loss 8.0561(8.7092) | Error 0.0000(0.0000) Steps 538(546.94) | Grad Norm 10.5200(7.6328) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.3324(14.4568) | Bit/dim 3.6554(3.6472) | Xent 0.0000(0.0000) | Loss 8.2687(8.5508) | Error 0.0000(0.0000) Steps 568(547.87) | Grad Norm 14.7392(7.6658) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 14.3115(14.4784) | Bit/dim 3.6726(3.6516) | Xent 0.0000(0.0000) | Loss 8.1581(8.4508) | Error 0.0000(0.0000) Steps 544(546.85) | Grad Norm 6.8702(7.9237) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.2915(14.5291) | Bit/dim 3.6092(3.6500) | Xent 0.0000(0.0000) | Loss 8.0654(8.3636) | Error 0.0000(0.0000) Steps 562(545.81) | Grad Norm 6.1033(7.3366) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 14.6568(14.5556) | Bit/dim 3.6509(3.6470) | Xent 0.0000(0.0000) | Loss 8.2348(8.2964) | Error 0.0000(0.0000) Steps 526(544.45) | Grad Norm 10.0118(7.0685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 76.3061, Epoch Time 895.9361(841.3491), Bit/dim 3.6504(best: 3.6521), Xent 0.0000, Loss 3.6504, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 14.1286(14.5670) | Bit/dim 3.6358(3.6454) | Xent 0.0000(0.0000) | Loss 8.1282(8.7873) | Error 0.0000(0.0000) Steps 556(545.65) | Grad Norm 8.3717(7.6825) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 14.6402(14.5634) | Bit/dim 3.6054(3.6430) | Xent 0.0000(0.0000) | Loss 8.0602(8.6121) | Error 0.0000(0.0000) Steps 520(547.68) | Grad Norm 3.8710(7.1010) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.9444(14.6147) | Bit/dim 3.6450(3.6435) | Xent 0.0000(0.0000) | Loss 8.1695(8.4938) | Error 0.0000(0.0000) Steps 562(550.00) | Grad Norm 6.3138(7.0140) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 14.2301(14.6069) | Bit/dim 3.6449(3.6422) | Xent 0.0000(0.0000) | Loss 8.0766(8.4034) | Error 0.0000(0.0000) Steps 544(553.84) | Grad Norm 10.8408(7.3369) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 14.8467(14.5790) | Bit/dim 3.6010(3.6418) | Xent 0.0000(0.0000) | Loss 8.0693(8.3326) | Error 0.0000(0.0000) Steps 532(554.20) | Grad Norm 5.7356(7.8514) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 14.3821(14.5331) | Bit/dim 3.5961(3.6388) | Xent 0.0000(0.0000) | Loss 8.0639(8.2777) | Error 0.0000(0.0000) Steps 544(552.67) | Grad Norm 4.6231(7.4467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 75.1743, Epoch Time 893.7058(842.9198), Bit/dim 3.6379(best: 3.6504), Xent 0.0000, Loss 3.6379, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 14.1686(14.5795) | Bit/dim 3.6296(3.6373) | Xent 0.0000(0.0000) | Loss 8.0906(8.6786) | Error 0.0000(0.0000) Steps 550(552.20) | Grad Norm 4.2758(6.5883) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 15.7928(14.6505) | Bit/dim 3.5956(3.6384) | Xent 0.0000(0.0000) | Loss 8.0524(8.5334) | Error 0.0000(0.0000) Steps 598(556.73) | Grad Norm 6.8829(6.6264) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 14.5537(14.7655) | Bit/dim 3.6130(3.6371) | Xent 0.0000(0.0000) | Loss 8.1021(8.4313) | Error 0.0000(0.0000) Steps 568(560.85) | Grad Norm 10.0922(6.9280) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 13.8816(14.7052) | Bit/dim 3.5844(3.6344) | Xent 0.0000(0.0000) | Loss 8.0142(8.3348) | Error 0.0000(0.0000) Steps 532(557.04) | Grad Norm 4.0029(6.3184) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 15.4632(14.7224) | Bit/dim 3.6120(3.6331) | Xent 0.0000(0.0000) | Loss 8.1076(8.2796) | Error 0.0000(0.0000) Steps 574(557.20) | Grad Norm 11.7077(7.2573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 74.6370, Epoch Time 905.1502(844.7867), Bit/dim 3.6395(best: 3.6379), Xent 0.0000, Loss 3.6395, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 14.4502(14.6718) | Bit/dim 3.6335(3.6348) | Xent 0.0000(0.0000) | Loss 8.1233(8.7489) | Error 0.0000(0.0000) Steps 562(558.24) | Grad Norm 6.6555(7.2108) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.0347(14.6081) | Bit/dim 3.6278(3.6368) | Xent 0.0000(0.0000) | Loss 8.1558(8.5773) | Error 0.0000(0.0000) Steps 538(555.33) | Grad Norm 6.5044(7.3335) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 14.3076(14.6584) | Bit/dim 3.6103(3.6339) | Xent 0.0000(0.0000) | Loss 8.0135(8.4511) | Error 0.0000(0.0000) Steps 520(555.81) | Grad Norm 9.0622(7.1519) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.1352(14.6686) | Bit/dim 3.6118(3.6317) | Xent 0.0000(0.0000) | Loss 8.0767(8.3688) | Error 0.0000(0.0000) Steps 556(556.55) | Grad Norm 6.8343(7.0049) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 14.1831(14.7263) | Bit/dim 3.6523(3.6310) | Xent 0.0000(0.0000) | Loss 8.1064(8.3040) | Error 0.0000(0.0000) Steps 556(558.16) | Grad Norm 10.3659(6.6410) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.8555(14.7020) | Bit/dim 3.6217(3.6313) | Xent 0.0000(0.0000) | Loss 8.1554(8.2545) | Error 0.0000(0.0000) Steps 544(557.35) | Grad Norm 12.9878(7.3873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 76.0168, Epoch Time 900.7306(846.4651), Bit/dim 3.6351(best: 3.6379), Xent 0.0000, Loss 3.6351, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 14.4136(14.6376) | Bit/dim 3.6274(3.6284) | Xent 0.0000(0.0000) | Loss 8.1159(8.6733) | Error 0.0000(0.0000) Steps 556(557.79) | Grad Norm 4.2797(7.2954) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.0045(14.7044) | Bit/dim 3.6246(3.6273) | Xent 0.0000(0.0000) | Loss 8.1424(8.5251) | Error 0.0000(0.0000) Steps 568(556.79) | Grad Norm 9.3834(7.1836) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 14.3565(14.7008) | Bit/dim 3.6159(3.6279) | Xent 0.0000(0.0000) | Loss 8.0904(8.4164) | Error 0.0000(0.0000) Steps 556(557.80) | Grad Norm 6.4712(6.9878) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 14.6960(14.7034) | Bit/dim 3.6142(3.6293) | Xent 0.0000(0.0000) | Loss 8.2138(8.3454) | Error 0.0000(0.0000) Steps 580(557.60) | Grad Norm 8.9880(7.2173) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 15.5193(14.7852) | Bit/dim 3.6599(3.6285) | Xent 0.0000(0.0000) | Loss 8.1798(8.2877) | Error 0.0000(0.0000) Steps 556(556.05) | Grad Norm 5.9337(7.3850) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 74.5689, Epoch Time 903.7515(848.1836), Bit/dim 3.6239(best: 3.6351), Xent 0.0000, Loss 3.6239, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.5013(14.8056) | Bit/dim 3.6285(3.6245) | Xent 0.0000(0.0000) | Loss 8.1599(8.7411) | Error 0.0000(0.0000) Steps 556(555.55) | Grad Norm 5.5678(6.8453) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 14.9489(14.8067) | Bit/dim 3.6334(3.6255) | Xent 0.0000(0.0000) | Loss 8.0370(8.5801) | Error 0.0000(0.0000) Steps 574(555.61) | Grad Norm 8.8192(6.8949) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 14.9301(14.7686) | Bit/dim 3.6183(3.6201) | Xent 0.0000(0.0000) | Loss 8.1518(8.4531) | Error 0.0000(0.0000) Steps 586(558.19) | Grad Norm 11.5924(6.8885) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.1279(14.8114) | Bit/dim 3.6620(3.6223) | Xent 0.0000(0.0000) | Loss 8.1990(8.3609) | Error 0.0000(0.0000) Steps 604(559.98) | Grad Norm 8.8712(7.2055) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 14.5783(14.8029) | Bit/dim 3.6381(3.6236) | Xent 0.0000(0.0000) | Loss 8.1651(8.2984) | Error 0.0000(0.0000) Steps 568(562.56) | Grad Norm 7.2680(7.6719) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.2626(14.8178) | Bit/dim 3.6324(3.6250) | Xent 0.0000(0.0000) | Loss 8.0717(8.2522) | Error 0.0000(0.0000) Steps 562(562.35) | Grad Norm 8.8248(7.5660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 75.0497, Epoch Time 907.9898(849.9778), Bit/dim 3.6181(best: 3.6239), Xent 0.0000, Loss 3.6181, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 14.3092(14.7872) | Bit/dim 3.6174(3.6261) | Xent 0.0000(0.0000) | Loss 7.9719(8.6753) | Error 0.0000(0.0000) Steps 556(561.01) | Grad Norm 6.9489(6.7921) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 13.8551(14.7707) | Bit/dim 3.6032(3.6213) | Xent 0.0000(0.0000) | Loss 8.1185(8.5221) | Error 0.0000(0.0000) Steps 538(558.30) | Grad Norm 5.7386(6.2534) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 14.7131(14.7544) | Bit/dim 3.6557(3.6225) | Xent 0.0000(0.0000) | Loss 8.2605(8.4261) | Error 0.0000(0.0000) Steps 556(559.58) | Grad Norm 12.6362(6.8975) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.1511(14.7243) | Bit/dim 3.5915(3.6197) | Xent 0.0000(0.0000) | Loss 7.9880(8.3349) | Error 0.0000(0.0000) Steps 562(558.58) | Grad Norm 7.0822(7.3321) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.2829(14.7484) | Bit/dim 3.6352(3.6177) | Xent 0.0000(0.0000) | Loss 8.0969(8.2717) | Error 0.0000(0.0000) Steps 568(559.75) | Grad Norm 8.9465(7.0405) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 76.5323, Epoch Time 904.5798(851.6159), Bit/dim 3.6124(best: 3.6181), Xent 0.0000, Loss 3.6124, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 14.2570(14.7668) | Bit/dim 3.6031(3.6177) | Xent 0.0000(0.0000) | Loss 7.9892(8.7476) | Error 0.0000(0.0000) Steps 544(560.03) | Grad Norm 7.3685(7.2904) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.1611(14.7546) | Bit/dim 3.6325(3.6141) | Xent 0.0000(0.0000) | Loss 8.2128(8.5692) | Error 0.0000(0.0000) Steps 592(561.08) | Grad Norm 5.8167(6.8286) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 14.4108(14.7777) | Bit/dim 3.5960(3.6120) | Xent 0.0000(0.0000) | Loss 8.0639(8.4374) | Error 0.0000(0.0000) Steps 562(561.53) | Grad Norm 10.0854(6.2906) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 14.4376(14.8037) | Bit/dim 3.6188(3.6127) | Xent 0.0000(0.0000) | Loss 8.1913(8.3439) | Error 0.0000(0.0000) Steps 550(559.75) | Grad Norm 3.3912(6.7522) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.7566(14.8017) | Bit/dim 3.6042(3.6118) | Xent 0.0000(0.0000) | Loss 8.0700(8.2747) | Error 0.0000(0.0000) Steps 556(560.31) | Grad Norm 5.1412(6.2054) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 13.7563(14.7654) | Bit/dim 3.5895(3.6131) | Xent 0.0000(0.0000) | Loss 8.0488(8.2333) | Error 0.0000(0.0000) Steps 568(561.74) | Grad Norm 5.3543(6.2734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 77.3319, Epoch Time 908.3003(853.3164), Bit/dim 3.6156(best: 3.6124), Xent 0.0000, Loss 3.6156, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 14.9733(14.7490) | Bit/dim 3.6179(3.6112) | Xent 0.0000(0.0000) | Loss 8.1597(8.6512) | Error 0.0000(0.0000) Steps 568(563.06) | Grad Norm 12.3280(6.8629) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 14.5805(14.8036) | Bit/dim 3.6137(3.6102) | Xent 0.0000(0.0000) | Loss 8.1298(8.5077) | Error 0.0000(0.0000) Steps 550(564.48) | Grad Norm 7.4238(6.9612) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 15.3499(14.8515) | Bit/dim 3.6004(3.6091) | Xent 0.0000(0.0000) | Loss 8.0557(8.3981) | Error 0.0000(0.0000) Steps 586(565.70) | Grad Norm 7.7078(6.7651) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 14.7134(14.8624) | Bit/dim 3.6494(3.6116) | Xent 0.0000(0.0000) | Loss 8.1187(8.3232) | Error 0.0000(0.0000) Steps 568(564.59) | Grad Norm 5.3501(6.7316) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 14.3218(14.7939) | Bit/dim 3.6438(3.6120) | Xent 0.0000(0.0000) | Loss 8.1598(8.2590) | Error 0.0000(0.0000) Steps 562(563.62) | Grad Norm 7.2597(6.9383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 76.8842, Epoch Time 910.4669(855.0309), Bit/dim 3.6077(best: 3.6124), Xent 0.0000, Loss 3.6077, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 14.8111(14.8317) | Bit/dim 3.5992(3.6097) | Xent 0.0000(0.0000) | Loss 8.1664(8.7625) | Error 0.0000(0.0000) Steps 592(567.36) | Grad Norm 8.3871(6.7062) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 14.8170(14.8414) | Bit/dim 3.6119(3.6082) | Xent 0.0000(0.0000) | Loss 8.0668(8.5769) | Error 0.0000(0.0000) Steps 562(566.91) | Grad Norm 6.6175(6.9198) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.3466(14.8542) | Bit/dim 3.5955(3.6037) | Xent 0.0000(0.0000) | Loss 8.0194(8.4431) | Error 0.0000(0.0000) Steps 568(566.86) | Grad Norm 6.2105(6.7484) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.3468(14.9105) | Bit/dim 3.6043(3.6062) | Xent 0.0000(0.0000) | Loss 8.0418(8.3597) | Error 0.0000(0.0000) Steps 556(567.77) | Grad Norm 10.4783(7.1809) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 14.8642(14.8973) | Bit/dim 3.5874(3.6062) | Xent 0.0000(0.0000) | Loss 8.0119(8.2910) | Error 0.0000(0.0000) Steps 532(568.48) | Grad Norm 9.1837(7.0031) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 14.3867(14.9078) | Bit/dim 3.6191(3.6079) | Xent 0.0000(0.0000) | Loss 8.1131(8.2425) | Error 0.0000(0.0000) Steps 556(570.52) | Grad Norm 6.3283(7.0300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 77.6545, Epoch Time 916.8864(856.8866), Bit/dim 3.6003(best: 3.6077), Xent 0.0000, Loss 3.6003, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 14.9029(14.9847) | Bit/dim 3.6014(3.6073) | Xent 0.0000(0.0000) | Loss 8.0076(8.6516) | Error 0.0000(0.0000) Steps 544(573.48) | Grad Norm 7.8399(7.0874) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 14.9989(14.9795) | Bit/dim 3.5769(3.6044) | Xent 0.0000(0.0000) | Loss 8.0413(8.4995) | Error 0.0000(0.0000) Steps 574(574.37) | Grad Norm 6.3540(6.9106) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.1850(15.0125) | Bit/dim 3.6134(3.6030) | Xent 0.0000(0.0000) | Loss 8.0415(8.3858) | Error 0.0000(0.0000) Steps 550(573.42) | Grad Norm 9.0381(6.9748) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.1301(14.9840) | Bit/dim 3.6057(3.6036) | Xent 0.0000(0.0000) | Loss 8.0243(8.2941) | Error 0.0000(0.0000) Steps 562(571.59) | Grad Norm 3.3042(6.3956) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.0734(14.9825) | Bit/dim 3.6354(3.6012) | Xent 0.0000(0.0000) | Loss 8.0790(8.2344) | Error 0.0000(0.0000) Steps 586(574.66) | Grad Norm 12.8272(6.4922) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 78.5821, Epoch Time 923.9716(858.8992), Bit/dim 3.6119(best: 3.6003), Xent 0.0000, Loss 3.6119, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 13.9667(14.9568) | Bit/dim 3.5679(3.6030) | Xent 0.0000(0.0000) | Loss 8.0595(8.7407) | Error 0.0000(0.0000) Steps 580(573.11) | Grad Norm 4.6814(7.1851) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.8539(14.9860) | Bit/dim 3.5540(3.6005) | Xent 0.0000(0.0000) | Loss 8.0543(8.5670) | Error 0.0000(0.0000) Steps 604(575.60) | Grad Norm 3.0399(6.5010) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 14.3760(14.9314) | Bit/dim 3.6335(3.5999) | Xent 0.0000(0.0000) | Loss 8.2063(8.4359) | Error 0.0000(0.0000) Steps 580(575.36) | Grad Norm 3.2597(6.6161) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 14.9816(14.9616) | Bit/dim 3.5717(3.6008) | Xent 0.0000(0.0000) | Loss 7.9934(8.3430) | Error 0.0000(0.0000) Steps 580(576.41) | Grad Norm 5.6556(6.6510) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.1387(14.9318) | Bit/dim 3.5673(3.5986) | Xent 0.0000(0.0000) | Loss 7.9670(8.2637) | Error 0.0000(0.0000) Steps 562(575.47) | Grad Norm 5.3559(6.2720) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 14.9733(14.9382) | Bit/dim 3.6128(3.6005) | Xent 0.0000(0.0000) | Loss 8.1732(8.2191) | Error 0.0000(0.0000) Steps 586(576.99) | Grad Norm 5.0479(6.5750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 77.6131, Epoch Time 915.6963(860.6031), Bit/dim 3.5917(best: 3.6003), Xent 0.0000, Loss 3.5917, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 15.3437(14.9776) | Bit/dim 3.5869(3.6010) | Xent 0.0000(0.0000) | Loss 8.2161(8.6409) | Error 0.0000(0.0000) Steps 616(575.78) | Grad Norm 7.8825(6.2633) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 15.2438(15.0860) | Bit/dim 3.5844(3.5990) | Xent 0.0000(0.0000) | Loss 8.0983(8.4989) | Error 0.0000(0.0000) Steps 580(577.33) | Grad Norm 7.1827(6.5633) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.1312(15.0197) | Bit/dim 3.5827(3.5958) | Xent 0.0000(0.0000) | Loss 8.0940(8.3789) | Error 0.0000(0.0000) Steps 592(575.88) | Grad Norm 6.1740(6.4418) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 14.5090(15.0038) | Bit/dim 3.6055(3.5944) | Xent 0.0000(0.0000) | Loss 8.1765(8.2981) | Error 0.0000(0.0000) Steps 586(579.02) | Grad Norm 11.4898(6.5233) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 14.7177(15.0134) | Bit/dim 3.5971(3.5951) | Xent 0.0000(0.0000) | Loss 8.1480(8.2444) | Error 0.0000(0.0000) Steps 568(579.92) | Grad Norm 3.9038(6.5816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 77.5256, Epoch Time 923.3888(862.4866), Bit/dim 3.5926(best: 3.5917), Xent 0.0000, Loss 3.5926, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.1072(14.9817) | Bit/dim 3.6135(3.5930) | Xent 0.0000(0.0000) | Loss 8.0407(8.7394) | Error 0.0000(0.0000) Steps 592(579.89) | Grad Norm 8.3699(6.4604) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 14.3244(14.9843) | Bit/dim 3.5755(3.5916) | Xent 0.0000(0.0000) | Loss 7.9685(8.5568) | Error 0.0000(0.0000) Steps 550(578.04) | Grad Norm 11.1453(7.1054) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.1725(14.9946) | Bit/dim 3.6031(3.5946) | Xent 0.0000(0.0000) | Loss 8.0755(8.4412) | Error 0.0000(0.0000) Steps 574(578.04) | Grad Norm 3.8894(7.0018) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 14.9281(15.0453) | Bit/dim 3.5808(3.5984) | Xent 0.0000(0.0000) | Loss 8.0500(8.3516) | Error 0.0000(0.0000) Steps 568(580.09) | Grad Norm 10.3040(6.9708) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.1143(15.0877) | Bit/dim 3.5696(3.5952) | Xent 0.0000(0.0000) | Loss 8.0446(8.2716) | Error 0.0000(0.0000) Steps 574(580.16) | Grad Norm 6.3920(6.6931) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 14.7839(15.0498) | Bit/dim 3.5812(3.5920) | Xent 0.0000(0.0000) | Loss 8.0379(8.2156) | Error 0.0000(0.0000) Steps 562(580.85) | Grad Norm 7.3743(6.8945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 78.8540, Epoch Time 924.6344(864.3511), Bit/dim 3.5885(best: 3.5917), Xent 0.0000, Loss 3.5885, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 14.3464(15.0465) | Bit/dim 3.5821(3.5899) | Xent 0.0000(0.0000) | Loss 8.1355(8.6364) | Error 0.0000(0.0000) Steps 592(581.83) | Grad Norm 5.9328(6.5141) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.7546(15.0950) | Bit/dim 3.5548(3.5903) | Xent 0.0000(0.0000) | Loss 8.0171(8.4929) | Error 0.0000(0.0000) Steps 598(581.75) | Grad Norm 8.6289(6.5711) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 15.4710(15.1136) | Bit/dim 3.6104(3.5895) | Xent 0.0000(0.0000) | Loss 8.1675(8.3846) | Error 0.0000(0.0000) Steps 592(580.52) | Grad Norm 9.2860(6.4840) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 14.2967(15.0761) | Bit/dim 3.5714(3.5913) | Xent 0.0000(0.0000) | Loss 8.0612(8.3099) | Error 0.0000(0.0000) Steps 568(580.30) | Grad Norm 7.6304(7.1672) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 14.5581(15.1107) | Bit/dim 3.6086(3.5918) | Xent 0.0000(0.0000) | Loss 8.0583(8.2482) | Error 0.0000(0.0000) Steps 568(580.32) | Grad Norm 2.9987(6.7156) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 79.8179, Epoch Time 930.0014(866.3206), Bit/dim 3.5935(best: 3.5885), Xent 0.0000, Loss 3.5935, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 14.8263(15.1494) | Bit/dim 3.5644(3.5894) | Xent 0.0000(0.0000) | Loss 8.0594(8.7351) | Error 0.0000(0.0000) Steps 562(581.82) | Grad Norm 6.0425(6.6768) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.0616(15.1396) | Bit/dim 3.6186(3.5908) | Xent 0.0000(0.0000) | Loss 8.1777(8.5663) | Error 0.0000(0.0000) Steps 586(581.92) | Grad Norm 3.6414(6.0810) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 15.2633(15.1089) | Bit/dim 3.6084(3.5892) | Xent 0.0000(0.0000) | Loss 8.2358(8.4377) | Error 0.0000(0.0000) Steps 610(582.07) | Grad Norm 10.1496(6.5375) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 14.5294(15.1650) | Bit/dim 3.5526(3.5857) | Xent 0.0000(0.0000) | Loss 7.9576(8.3329) | Error 0.0000(0.0000) Steps 568(582.62) | Grad Norm 7.4922(6.3563) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.0965(15.1763) | Bit/dim 3.5599(3.5874) | Xent 0.0000(0.0000) | Loss 8.0266(8.2715) | Error 0.0000(0.0000) Steps 586(580.54) | Grad Norm 5.6436(6.5836) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 14.6447(15.1403) | Bit/dim 3.5733(3.5843) | Xent 0.0000(0.0000) | Loss 8.0867(8.2121) | Error 0.0000(0.0000) Steps 568(579.28) | Grad Norm 3.1999(6.0082) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 80.1288, Epoch Time 932.1425(868.2952), Bit/dim 3.5823(best: 3.5885), Xent 0.0000, Loss 3.5823, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 14.8213(15.1640) | Bit/dim 3.5602(3.5852) | Xent 0.0000(0.0000) | Loss 7.8311(8.6409) | Error 0.0000(0.0000) Steps 556(576.41) | Grad Norm 8.3056(6.3911) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 14.7806(15.2158) | Bit/dim 3.5569(3.5818) | Xent 0.0000(0.0000) | Loss 7.9963(8.4869) | Error 0.0000(0.0000) Steps 592(577.01) | Grad Norm 4.1963(6.3377) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 14.8392(15.1643) | Bit/dim 3.5653(3.5842) | Xent 0.0000(0.0000) | Loss 7.8995(8.3763) | Error 0.0000(0.0000) Steps 580(577.76) | Grad Norm 5.2900(6.4099) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 14.9043(15.1762) | Bit/dim 3.5706(3.5852) | Xent 0.0000(0.0000) | Loss 7.9809(8.2936) | Error 0.0000(0.0000) Steps 574(577.84) | Grad Norm 7.0614(6.5561) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 14.8252(15.1637) | Bit/dim 3.5745(3.5810) | Xent 0.0000(0.0000) | Loss 8.0136(8.2290) | Error 0.0000(0.0000) Steps 568(579.49) | Grad Norm 9.9104(6.5158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 80.0270, Epoch Time 934.6875(870.2870), Bit/dim 3.5825(best: 3.5823), Xent 0.0000, Loss 3.5825, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 14.4169(15.1670) | Bit/dim 3.5830(3.5836) | Xent 0.0000(0.0000) | Loss 7.9786(8.7372) | Error 0.0000(0.0000) Steps 580(581.49) | Grad Norm 6.6221(6.6889) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 15.2440(15.1881) | Bit/dim 3.5883(3.5840) | Xent 0.0000(0.0000) | Loss 8.1106(8.5657) | Error 0.0000(0.0000) Steps 598(581.71) | Grad Norm 6.1000(6.7504) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.8071(15.3248) | Bit/dim 3.5622(3.5838) | Xent 0.0000(0.0000) | Loss 7.8994(8.4336) | Error 0.0000(0.0000) Steps 568(582.83) | Grad Norm 4.5500(6.2824) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 15.2845(15.3690) | Bit/dim 3.5884(3.5814) | Xent 0.0000(0.0000) | Loss 8.0578(8.3405) | Error 0.0000(0.0000) Steps 568(582.27) | Grad Norm 8.0816(6.3354) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.4883(15.3720) | Bit/dim 3.5723(3.5783) | Xent 0.0000(0.0000) | Loss 8.0293(8.2608) | Error 0.0000(0.0000) Steps 580(582.06) | Grad Norm 5.9736(6.1069) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 16.4998(15.3667) | Bit/dim 3.5641(3.5783) | Xent 0.0000(0.0000) | Loss 8.0205(8.2084) | Error 0.0000(0.0000) Steps 544(581.14) | Grad Norm 10.6934(6.7308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 80.5800, Epoch Time 946.5033(872.5735), Bit/dim 3.5852(best: 3.5823), Xent 0.0000, Loss 3.5852, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.5368(15.3649) | Bit/dim 3.5636(3.5775) | Xent 0.0000(0.0000) | Loss 8.0758(8.6600) | Error 0.0000(0.0000) Steps 604(583.55) | Grad Norm 8.3842(6.6523) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.5847(15.3793) | Bit/dim 3.5867(3.5795) | Xent 0.0000(0.0000) | Loss 8.0274(8.5118) | Error 0.0000(0.0000) Steps 574(583.09) | Grad Norm 7.5813(6.6228) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 15.8050(15.3863) | Bit/dim 3.5615(3.5815) | Xent 0.0000(0.0000) | Loss 8.0527(8.4012) | Error 0.0000(0.0000) Steps 616(583.52) | Grad Norm 7.8528(6.7505) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.8166(15.3572) | Bit/dim 3.5617(3.5806) | Xent 0.0000(0.0000) | Loss 8.1174(8.3198) | Error 0.0000(0.0000) Steps 568(584.36) | Grad Norm 7.4527(6.4975) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 14.6573(15.2887) | Bit/dim 3.5442(3.5762) | Xent 0.0000(0.0000) | Loss 8.0140(8.2468) | Error 0.0000(0.0000) Steps 562(583.44) | Grad Norm 4.7867(6.8260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 80.4371, Epoch Time 941.3879(874.6379), Bit/dim 3.5750(best: 3.5823), Xent 0.0000, Loss 3.5750, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.5474(15.3289) | Bit/dim 3.5195(3.5724) | Xent 0.0000(0.0000) | Loss 8.0062(8.7405) | Error 0.0000(0.0000) Steps 610(586.21) | Grad Norm 3.8334(5.9851) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.1419(15.3168) | Bit/dim 3.5942(3.5739) | Xent 0.0000(0.0000) | Loss 8.1318(8.5670) | Error 0.0000(0.0000) Steps 586(587.37) | Grad Norm 2.8905(6.2660) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.9530(15.3394) | Bit/dim 3.5646(3.5725) | Xent 0.0000(0.0000) | Loss 8.1062(8.4343) | Error 0.0000(0.0000) Steps 586(586.38) | Grad Norm 4.9717(6.4510) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 14.9009(15.3021) | Bit/dim 3.5646(3.5714) | Xent 0.0000(0.0000) | Loss 8.0253(8.3300) | Error 0.0000(0.0000) Steps 568(586.76) | Grad Norm 4.1259(5.9722) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.9936(15.3595) | Bit/dim 3.5985(3.5707) | Xent 0.0000(0.0000) | Loss 8.1602(8.2612) | Error 0.0000(0.0000) Steps 568(586.68) | Grad Norm 8.0311(6.5653) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.7101(15.4146) | Bit/dim 3.5847(3.5713) | Xent 0.0000(0.0000) | Loss 8.0816(8.2123) | Error 0.0000(0.0000) Steps 586(588.65) | Grad Norm 3.6088(5.8720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 80.3580, Epoch Time 945.6915(876.7695), Bit/dim 3.5722(best: 3.5750), Xent 0.0000, Loss 3.5722, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.8826(15.4501) | Bit/dim 3.5876(3.5743) | Xent 0.0000(0.0000) | Loss 8.0093(8.6655) | Error 0.0000(0.0000) Steps 556(588.94) | Grad Norm 6.8989(6.1630) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 16.0109(15.5279) | Bit/dim 3.5841(3.5719) | Xent 0.0000(0.0000) | Loss 8.1106(8.5109) | Error 0.0000(0.0000) Steps 574(590.07) | Grad Norm 9.4897(6.4593) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.0963(15.5483) | Bit/dim 3.5695(3.5718) | Xent 0.0000(0.0000) | Loss 8.0243(8.3902) | Error 0.0000(0.0000) Steps 604(590.46) | Grad Norm 4.4715(6.4617) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 15.4740(15.5239) | Bit/dim 3.5713(3.5703) | Xent 0.0000(0.0000) | Loss 8.0141(8.2967) | Error 0.0000(0.0000) Steps 592(590.80) | Grad Norm 5.2718(5.7938) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.3862(15.4717) | Bit/dim 3.5462(3.5698) | Xent 0.0000(0.0000) | Loss 8.0708(8.2345) | Error 0.0000(0.0000) Steps 616(590.15) | Grad Norm 4.9586(6.0393) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 81.4465, Epoch Time 954.0846(879.0890), Bit/dim 3.5632(best: 3.5722), Xent 0.0000, Loss 3.5632, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 14.4914(15.3854) | Bit/dim 3.5967(3.5694) | Xent 0.0000(0.0000) | Loss 7.9993(8.7565) | Error 0.0000(0.0000) Steps 550(589.33) | Grad Norm 5.2526(6.3662) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.5454(15.4001) | Bit/dim 3.5856(3.5691) | Xent 0.0000(0.0000) | Loss 8.0777(8.5724) | Error 0.0000(0.0000) Steps 598(589.23) | Grad Norm 5.4301(5.9147) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.5844(15.3887) | Bit/dim 3.5680(3.5699) | Xent 0.0000(0.0000) | Loss 8.0341(8.4373) | Error 0.0000(0.0000) Steps 610(592.30) | Grad Norm 5.1884(6.2961) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 15.8823(15.4057) | Bit/dim 3.5681(3.5684) | Xent 0.0000(0.0000) | Loss 8.0820(8.3320) | Error 0.0000(0.0000) Steps 580(590.44) | Grad Norm 3.9085(6.0533) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.1412(15.4384) | Bit/dim 3.6038(3.5695) | Xent 0.0000(0.0000) | Loss 8.1892(8.2638) | Error 0.0000(0.0000) Steps 634(591.67) | Grad Norm 6.6321(6.1173) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.4953(15.4633) | Bit/dim 3.5411(3.5687) | Xent 0.0000(0.0000) | Loss 8.0847(8.2083) | Error 0.0000(0.0000) Steps 592(591.53) | Grad Norm 4.7373(6.4307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 80.5455, Epoch Time 946.1520(881.1009), Bit/dim 3.5710(best: 3.5632), Xent 0.0000, Loss 3.5710, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.3874(15.5208) | Bit/dim 3.5347(3.5664) | Xent 0.0000(0.0000) | Loss 7.9446(8.6469) | Error 0.0000(0.0000) Steps 610(593.65) | Grad Norm 8.1703(6.7099) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 14.7241(15.4330) | Bit/dim 3.5745(3.5653) | Xent 0.0000(0.0000) | Loss 7.8597(8.4838) | Error 0.0000(0.0000) Steps 580(592.59) | Grad Norm 2.8708(6.0046) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.5768(15.4643) | Bit/dim 3.5910(3.5670) | Xent 0.0000(0.0000) | Loss 8.1638(8.3809) | Error 0.0000(0.0000) Steps 592(592.35) | Grad Norm 3.7011(5.6969) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 15.7661(15.4720) | Bit/dim 3.5632(3.5661) | Xent 0.0000(0.0000) | Loss 8.1563(8.3014) | Error 0.0000(0.0000) Steps 604(592.33) | Grad Norm 12.1073(6.2175) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 15.6248(15.5203) | Bit/dim 3.5219(3.5658) | Xent 0.0000(0.0000) | Loss 8.0912(8.2425) | Error 0.0000(0.0000) Steps 592(591.89) | Grad Norm 6.2572(6.3625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 80.5675, Epoch Time 953.2670(883.2659), Bit/dim 3.5705(best: 3.5632), Xent 0.0000, Loss 3.5705, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.2733(15.5623) | Bit/dim 3.5584(3.5682) | Xent 0.0000(0.0000) | Loss 8.0727(8.7437) | Error 0.0000(0.0000) Steps 568(590.97) | Grad Norm 3.5737(6.2923) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 14.6689(15.4931) | Bit/dim 3.5755(3.5690) | Xent 0.0000(0.0000) | Loss 8.1326(8.5725) | Error 0.0000(0.0000) Steps 562(588.01) | Grad Norm 8.9553(6.0383) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.8400(15.4857) | Bit/dim 3.5628(3.5671) | Xent 0.0000(0.0000) | Loss 8.0531(8.4352) | Error 0.0000(0.0000) Steps 604(589.51) | Grad Norm 4.3781(5.9065) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.5027(15.5106) | Bit/dim 3.5848(3.5666) | Xent 0.0000(0.0000) | Loss 8.1068(8.3265) | Error 0.0000(0.0000) Steps 610(592.03) | Grad Norm 8.3570(5.7810) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.9978(15.4970) | Bit/dim 3.5735(3.5660) | Xent 0.0000(0.0000) | Loss 8.0799(8.2493) | Error 0.0000(0.0000) Steps 580(590.38) | Grad Norm 7.9380(6.2961) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.8278(15.4779) | Bit/dim 3.5507(3.5630) | Xent 0.0000(0.0000) | Loss 8.0764(8.2015) | Error 0.0000(0.0000) Steps 604(591.17) | Grad Norm 4.5104(6.1699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 80.7724, Epoch Time 949.6890(885.2586), Bit/dim 3.5587(best: 3.5632), Xent 0.0000, Loss 3.5587, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.5262(15.4628) | Bit/dim 3.5541(3.5627) | Xent 0.0000(0.0000) | Loss 8.0904(8.6380) | Error 0.0000(0.0000) Steps 628(592.48) | Grad Norm 10.8421(6.2186) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.4790(15.4998) | Bit/dim 3.5824(3.5620) | Xent 0.0000(0.0000) | Loss 7.9850(8.4876) | Error 0.0000(0.0000) Steps 586(593.55) | Grad Norm 3.3444(6.1439) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 15.6730(15.5024) | Bit/dim 3.5660(3.5613) | Xent 0.0000(0.0000) | Loss 8.1516(8.3748) | Error 0.0000(0.0000) Steps 598(593.71) | Grad Norm 6.0561(5.6308) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.4071(15.4582) | Bit/dim 3.5787(3.5607) | Xent 0.0000(0.0000) | Loss 8.1065(8.2830) | Error 0.0000(0.0000) Steps 568(593.20) | Grad Norm 4.7132(6.2204) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 16.3988(15.5027) | Bit/dim 3.5751(3.5611) | Xent 0.0000(0.0000) | Loss 8.1110(8.2275) | Error 0.0000(0.0000) Steps 616(595.41) | Grad Norm 7.4761(6.2838) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 81.5708, Epoch Time 952.7139(887.2822), Bit/dim 3.5615(best: 3.5587), Xent 0.0000, Loss 3.5615, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 14.8302(15.4864) | Bit/dim 3.5748(3.5637) | Xent 0.0000(0.0000) | Loss 8.0578(8.7148) | Error 0.0000(0.0000) Steps 556(594.65) | Grad Norm 3.5554(6.4374) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 16.7853(15.5077) | Bit/dim 3.5249(3.5618) | Xent 0.0000(0.0000) | Loss 7.9878(8.5455) | Error 0.0000(0.0000) Steps 622(593.53) | Grad Norm 9.0729(6.5364) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.0584(15.5001) | Bit/dim 3.5727(3.5591) | Xent 0.0000(0.0000) | Loss 8.0222(8.4052) | Error 0.0000(0.0000) Steps 604(590.27) | Grad Norm 4.6598(6.1244) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.8779(15.5400) | Bit/dim 3.5716(3.5598) | Xent 0.0000(0.0000) | Loss 7.9811(8.3180) | Error 0.0000(0.0000) Steps 610(592.92) | Grad Norm 4.4383(6.1487) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.6355(15.4768) | Bit/dim 3.5676(3.5579) | Xent 0.0000(0.0000) | Loss 7.9958(8.2354) | Error 0.0000(0.0000) Steps 580(592.05) | Grad Norm 6.8175(6.4137) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.1355(15.4313) | Bit/dim 3.5695(3.5624) | Xent 0.0000(0.0000) | Loss 8.0292(8.1864) | Error 0.0000(0.0000) Steps 586(591.99) | Grad Norm 6.3892(6.3862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 81.2950, Epoch Time 949.2110(889.1401), Bit/dim 3.5585(best: 3.5587), Xent 0.0000, Loss 3.5585, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.3752(15.5311) | Bit/dim 3.5365(3.5588) | Xent 0.0000(0.0000) | Loss 8.0477(8.6303) | Error 0.0000(0.0000) Steps 604(592.74) | Grad Norm 8.4753(6.7145) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.4324(15.6175) | Bit/dim 3.5777(3.5587) | Xent 0.0000(0.0000) | Loss 8.0752(8.4690) | Error 0.0000(0.0000) Steps 592(592.71) | Grad Norm 6.9861(6.4797) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 15.3808(15.6196) | Bit/dim 3.5711(3.5597) | Xent 0.0000(0.0000) | Loss 8.1035(8.3644) | Error 0.0000(0.0000) Steps 598(592.28) | Grad Norm 4.8231(6.3190) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.7207(15.6322) | Bit/dim 3.5903(3.5601) | Xent 0.0000(0.0000) | Loss 8.1644(8.2824) | Error 0.0000(0.0000) Steps 616(593.19) | Grad Norm 4.3861(6.2453) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 14.8912(15.5822) | Bit/dim 3.5768(3.5595) | Xent 0.0000(0.0000) | Loss 8.0391(8.2242) | Error 0.0000(0.0000) Steps 586(591.94) | Grad Norm 4.3049(5.5470) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 81.7501, Epoch Time 963.6917(891.3766), Bit/dim 3.5630(best: 3.5585), Xent 0.0000, Loss 3.5630, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 15.2499(15.5508) | Bit/dim 3.5546(3.5576) | Xent 0.0000(0.0000) | Loss 7.9822(8.7586) | Error 0.0000(0.0000) Steps 574(591.59) | Grad Norm 4.0443(6.1561) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 15.4723(15.5323) | Bit/dim 3.5504(3.5558) | Xent 0.0000(0.0000) | Loss 8.0710(8.5671) | Error 0.0000(0.0000) Steps 580(592.27) | Grad Norm 4.1070(6.0971) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 16.0470(15.5316) | Bit/dim 3.5553(3.5558) | Xent 0.0000(0.0000) | Loss 7.9225(8.4150) | Error 0.0000(0.0000) Steps 628(590.33) | Grad Norm 5.3733(5.8716) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.5034(15.5472) | Bit/dim 3.5475(3.5544) | Xent 0.0000(0.0000) | Loss 7.9875(8.3157) | Error 0.0000(0.0000) Steps 580(591.17) | Grad Norm 7.3206(5.7979) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.1716(15.5221) | Bit/dim 3.5403(3.5542) | Xent 0.0000(0.0000) | Loss 7.9585(8.2308) | Error 0.0000(0.0000) Steps 598(591.57) | Grad Norm 4.6295(5.8125) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 15.8155(15.5699) | Bit/dim 3.5703(3.5541) | Xent 0.0000(0.0000) | Loss 8.0868(8.1918) | Error 0.0000(0.0000) Steps 568(593.33) | Grad Norm 6.3126(5.7608) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 80.6901, Epoch Time 952.5026(893.2104), Bit/dim 3.5516(best: 3.5585), Xent 0.0000, Loss 3.5516, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.4818(15.5621) | Bit/dim 3.5683(3.5501) | Xent 0.0000(0.0000) | Loss 8.0414(8.6222) | Error 0.0000(0.0000) Steps 610(593.49) | Grad Norm 5.9106(6.1653) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 17.1486(15.5966) | Bit/dim 3.5688(3.5550) | Xent 0.0000(0.0000) | Loss 8.1620(8.4761) | Error 0.0000(0.0000) Steps 664(594.57) | Grad Norm 7.5963(5.6971) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 15.8576(15.5396) | Bit/dim 3.5541(3.5530) | Xent 0.0000(0.0000) | Loss 8.1003(8.3614) | Error 0.0000(0.0000) Steps 622(594.55) | Grad Norm 4.3081(5.9636) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.0699(15.5415) | Bit/dim 3.5752(3.5527) | Xent 0.0000(0.0000) | Loss 8.0804(8.2675) | Error 0.0000(0.0000) Steps 592(594.80) | Grad Norm 4.3042(5.9573) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 15.0297(15.5534) | Bit/dim 3.5526(3.5508) | Xent 0.0000(0.0000) | Loss 7.8911(8.2068) | Error 0.0000(0.0000) Steps 574(596.38) | Grad Norm 6.8369(5.6474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 82.0364, Epoch Time 954.3772(895.0454), Bit/dim 3.5520(best: 3.5516), Xent 0.0000, Loss 3.5520, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.0024(15.5387) | Bit/dim 3.5909(3.5539) | Xent 0.0000(0.0000) | Loss 8.1941(8.7116) | Error 0.0000(0.0000) Steps 592(595.31) | Grad Norm 7.1098(6.3175) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.9316(15.5953) | Bit/dim 3.5374(3.5525) | Xent 0.0000(0.0000) | Loss 8.0398(8.5392) | Error 0.0000(0.0000) Steps 574(594.63) | Grad Norm 5.2574(6.5205) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 15.4615(15.5692) | Bit/dim 3.5265(3.5516) | Xent 0.0000(0.0000) | Loss 8.0239(8.4132) | Error 0.0000(0.0000) Steps 580(595.04) | Grad Norm 6.4489(6.0953) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 16.1082(15.5765) | Bit/dim 3.5565(3.5479) | Xent 0.0000(0.0000) | Loss 8.0784(8.3181) | Error 0.0000(0.0000) Steps 574(596.21) | Grad Norm 4.0983(6.0480) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.4903(15.6446) | Bit/dim 3.5870(3.5522) | Xent 0.0000(0.0000) | Loss 8.1995(8.2628) | Error 0.0000(0.0000) Steps 592(598.21) | Grad Norm 3.5387(6.1863) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.6725(15.6460) | Bit/dim 3.5586(3.5534) | Xent 0.0000(0.0000) | Loss 8.0960(8.2050) | Error 0.0000(0.0000) Steps 586(596.22) | Grad Norm 3.8104(6.0820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 81.6002, Epoch Time 961.1492(897.0285), Bit/dim 3.5523(best: 3.5516), Xent 0.0000, Loss 3.5523, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.5839(15.6071) | Bit/dim 3.5780(3.5518) | Xent 0.0000(0.0000) | Loss 8.1094(8.6181) | Error 0.0000(0.0000) Steps 592(595.58) | Grad Norm 6.8279(5.6862) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.9452(15.6547) | Bit/dim 3.5311(3.5491) | Xent 0.0000(0.0000) | Loss 8.0193(8.4594) | Error 0.0000(0.0000) Steps 610(595.79) | Grad Norm 6.7935(5.7881) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.4864(15.7128) | Bit/dim 3.5485(3.5504) | Xent 0.0000(0.0000) | Loss 8.0494(8.3564) | Error 0.0000(0.0000) Steps 592(598.96) | Grad Norm 4.1503(6.1814) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 15.5445(15.6871) | Bit/dim 3.5752(3.5503) | Xent 0.0000(0.0000) | Loss 7.8744(8.2696) | Error 0.0000(0.0000) Steps 604(598.53) | Grad Norm 6.3783(5.9268) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 15.6253(15.7249) | Bit/dim 3.5281(3.5492) | Xent 0.0000(0.0000) | Loss 7.9935(8.2100) | Error 0.0000(0.0000) Steps 568(598.99) | Grad Norm 5.3135(5.7873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 83.3046, Epoch Time 964.9007(899.0647), Bit/dim 3.5456(best: 3.5516), Xent 0.0000, Loss 3.5456, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 15.9380(15.6997) | Bit/dim 3.5528(3.5478) | Xent 0.0000(0.0000) | Loss 8.1644(8.7366) | Error 0.0000(0.0000) Steps 610(602.50) | Grad Norm 8.6811(5.9370) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 15.3113(15.6837) | Bit/dim 3.5521(3.5485) | Xent 0.0000(0.0000) | Loss 8.0010(8.5551) | Error 0.0000(0.0000) Steps 586(598.39) | Grad Norm 8.7454(6.3180) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.3219(15.6757) | Bit/dim 3.5860(3.5471) | Xent 0.0000(0.0000) | Loss 8.0754(8.4268) | Error 0.0000(0.0000) Steps 598(601.09) | Grad Norm 6.6128(6.4821) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 15.1883(15.6261) | Bit/dim 3.5382(3.5449) | Xent 0.0000(0.0000) | Loss 7.9358(8.3134) | Error 0.0000(0.0000) Steps 580(599.52) | Grad Norm 7.3656(6.2447) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.0511(15.6795) | Bit/dim 3.5254(3.5446) | Xent 0.0000(0.0000) | Loss 7.9330(8.2422) | Error 0.0000(0.0000) Steps 598(600.06) | Grad Norm 7.9105(6.1640) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 15.6341(15.6286) | Bit/dim 3.5481(3.5446) | Xent 0.0000(0.0000) | Loss 8.0403(8.1868) | Error 0.0000(0.0000) Steps 592(598.74) | Grad Norm 3.9261(6.0036) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 83.4487, Epoch Time 962.1417(900.9570), Bit/dim 3.5442(best: 3.5456), Xent 0.0000, Loss 3.5442, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.0791(15.6730) | Bit/dim 3.5341(3.5421) | Xent 0.0000(0.0000) | Loss 7.8755(8.6414) | Error 0.0000(0.0000) Steps 580(597.38) | Grad Norm 6.8761(5.9598) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 15.3018(15.7317) | Bit/dim 3.5205(3.5406) | Xent 0.0000(0.0000) | Loss 8.0794(8.4871) | Error 0.0000(0.0000) Steps 634(599.36) | Grad Norm 6.8381(5.8923) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 14.9248(15.6482) | Bit/dim 3.5209(3.5425) | Xent 0.0000(0.0000) | Loss 8.0738(8.3800) | Error 0.0000(0.0000) Steps 592(598.90) | Grad Norm 5.7655(6.0819) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 15.4337(15.6135) | Bit/dim 3.5489(3.5417) | Xent 0.0000(0.0000) | Loss 8.1158(8.2950) | Error 0.0000(0.0000) Steps 604(600.14) | Grad Norm 3.3964(6.0672) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 15.3423(15.6000) | Bit/dim 3.5096(3.5408) | Xent 0.0000(0.0000) | Loss 7.8930(8.2190) | Error 0.0000(0.0000) Steps 598(600.35) | Grad Norm 7.0750(5.8748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 82.8918, Epoch Time 961.0201(902.7589), Bit/dim 3.5426(best: 3.5442), Xent 0.0000, Loss 3.5426, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 16.4973(15.6382) | Bit/dim 3.5347(3.5413) | Xent 0.0000(0.0000) | Loss 8.1851(8.7504) | Error 0.0000(0.0000) Steps 586(600.58) | Grad Norm 2.9853(5.8169) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 15.3893(15.6530) | Bit/dim 3.5263(3.5398) | Xent 0.0000(0.0000) | Loss 7.9675(8.5612) | Error 0.0000(0.0000) Steps 604(599.78) | Grad Norm 5.8622(5.9685) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 15.0045(15.6433) | Bit/dim 3.5556(3.5412) | Xent 0.0000(0.0000) | Loss 8.0476(8.4363) | Error 0.0000(0.0000) Steps 592(600.21) | Grad Norm 7.1686(5.9355) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 16.1502(15.6407) | Bit/dim 3.5062(3.5405) | Xent 0.0000(0.0000) | Loss 8.0611(8.3348) | Error 0.0000(0.0000) Steps 586(601.17) | Grad Norm 7.2345(5.8336) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 15.5036(15.6778) | Bit/dim 3.5319(3.5400) | Xent 0.0000(0.0000) | Loss 8.0482(8.2572) | Error 0.0000(0.0000) Steps 610(600.72) | Grad Norm 9.7296(6.0250) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 15.5770(15.6547) | Bit/dim 3.5335(3.5405) | Xent 0.0000(0.0000) | Loss 8.1017(8.2028) | Error 0.0000(0.0000) Steps 604(602.69) | Grad Norm 4.2192(6.2854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 83.1029, Epoch Time 964.8555(904.6218), Bit/dim 3.5477(best: 3.5426), Xent 0.0000, Loss 3.5477, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 16.2149(15.7033) | Bit/dim 3.5579(3.5409) | Xent 0.0000(0.0000) | Loss 8.1254(8.6778) | Error 0.0000(0.0000) Steps 610(602.80) | Grad Norm 5.5016(5.9712) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 15.1053(15.6505) | Bit/dim 3.5222(3.5405) | Xent 0.0000(0.0000) | Loss 7.9529(8.5071) | Error 0.0000(0.0000) Steps 562(601.88) | Grad Norm 4.0346(5.6964) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 15.1863(15.6429) | Bit/dim 3.5185(3.5381) | Xent 0.0000(0.0000) | Loss 8.0367(8.3838) | Error 0.0000(0.0000) Steps 616(603.47) | Grad Norm 8.3271(6.0096) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 16.6206(15.7028) | Bit/dim 3.5351(3.5358) | Xent 0.0000(0.0000) | Loss 8.1470(8.3011) | Error 0.0000(0.0000) Steps 604(604.35) | Grad Norm 4.9738(5.8844) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 15.6309(15.7946) | Bit/dim 3.5009(3.5372) | Xent 0.0000(0.0000) | Loss 8.0123(8.2310) | Error 0.0000(0.0000) Steps 622(608.40) | Grad Norm 8.9118(5.6447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 84.1887, Epoch Time 972.2035(906.6492), Bit/dim 3.5356(best: 3.5426), Xent 0.0000, Loss 3.5356, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 15.5967(15.8385) | Bit/dim 3.5020(3.5369) | Xent 0.0000(0.0000) | Loss 8.0126(8.7529) | Error 0.0000(0.0000) Steps 616(606.31) | Grad Norm 5.3070(5.8246) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 16.1434(15.8208) | Bit/dim 3.5400(3.5377) | Xent 0.0000(0.0000) | Loss 8.1009(8.5665) | Error 0.0000(0.0000) Steps 598(604.26) | Grad Norm 7.7014(6.1239) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 15.1434(15.8112) | Bit/dim 3.5314(3.5380) | Xent 0.0000(0.0000) | Loss 8.0988(8.4356) | Error 0.0000(0.0000) Steps 598(604.46) | Grad Norm 5.9890(5.9264) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 16.0602(15.9038) | Bit/dim 3.5426(3.5369) | Xent 0.0000(0.0000) | Loss 8.0696(8.3291) | Error 0.0000(0.0000) Steps 604(608.26) | Grad Norm 4.6837(6.0905) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 16.2217(15.9325) | Bit/dim 3.5084(3.5359) | Xent 0.0000(0.0000) | Loss 7.9617(8.2508) | Error 0.0000(0.0000) Steps 604(607.80) | Grad Norm 7.1593(5.9623) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 15.9125(15.9070) | Bit/dim 3.5489(3.5370) | Xent 0.0000(0.0000) | Loss 8.0351(8.2007) | Error 0.0000(0.0000) Steps 628(606.70) | Grad Norm 7.9011(6.2084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 83.5475, Epoch Time 976.9111(908.7571), Bit/dim 3.5476(best: 3.5356), Xent 0.0000, Loss 3.5476, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 15.4134(15.8974) | Bit/dim 3.5604(3.5398) | Xent 0.0000(0.0000) | Loss 8.0401(8.6451) | Error 0.0000(0.0000) Steps 574(604.20) | Grad Norm 6.8499(6.4300) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 15.8240(15.9309) | Bit/dim 3.5262(3.5399) | Xent 0.0000(0.0000) | Loss 8.0052(8.4894) | Error 0.0000(0.0000) Steps 616(602.33) | Grad Norm 4.0685(6.0967) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 15.7993(15.9335) | Bit/dim 3.5551(3.5376) | Xent 0.0000(0.0000) | Loss 8.0499(8.3696) | Error 0.0000(0.0000) Steps 586(602.55) | Grad Norm 6.8766(6.0545) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 15.1620(15.8729) | Bit/dim 3.5251(3.5369) | Xent 0.0000(0.0000) | Loss 7.9162(8.2892) | Error 0.0000(0.0000) Steps 592(603.63) | Grad Norm 3.8575(5.8695) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 15.6013(15.8643) | Bit/dim 3.5138(3.5329) | Xent 0.0000(0.0000) | Loss 7.9962(8.2083) | Error 0.0000(0.0000) Steps 640(606.46) | Grad Norm 3.9327(5.3966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 84.8317, Epoch Time 977.3809(910.8158), Bit/dim 3.5360(best: 3.5356), Xent 0.0000, Loss 3.5360, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 15.5067(15.8654) | Bit/dim 3.5234(3.5329) | Xent 0.0000(0.0000) | Loss 8.0732(8.7541) | Error 0.0000(0.0000) Steps 574(607.15) | Grad Norm 6.7906(5.8642) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 16.5451(15.8455) | Bit/dim 3.5196(3.5305) | Xent 0.0000(0.0000) | Loss 7.9425(8.5576) | Error 0.0000(0.0000) Steps 568(603.77) | Grad Norm 6.9268(5.7844) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 15.3788(15.7731) | Bit/dim 3.5297(3.5323) | Xent 0.0000(0.0000) | Loss 7.9067(8.4219) | Error 0.0000(0.0000) Steps 580(603.02) | Grad Norm 4.5411(5.9263) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 15.5923(15.7992) | Bit/dim 3.5428(3.5302) | Xent 0.0000(0.0000) | Loss 8.0939(8.3155) | Error 0.0000(0.0000) Steps 628(603.31) | Grad Norm 5.3083(5.8324) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 16.2695(15.8152) | Bit/dim 3.5260(3.5315) | Xent 0.0000(0.0000) | Loss 8.0665(8.2447) | Error 0.0000(0.0000) Steps 640(607.51) | Grad Norm 7.9493(5.6164) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 16.1740(15.9375) | Bit/dim 3.5154(3.5307) | Xent 0.0000(0.0000) | Loss 8.0658(8.1850) | Error 0.0000(0.0000) Steps 610(607.47) | Grad Norm 5.7103(5.8094) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 84.5849, Epoch Time 975.0833(912.7438), Bit/dim 3.5309(best: 3.5356), Xent 0.0000, Loss 3.5309, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 15.9649(15.8801) | Bit/dim 3.5328(3.5306) | Xent 0.0000(0.0000) | Loss 8.0505(8.6344) | Error 0.0000(0.0000) Steps 628(604.94) | Grad Norm 5.6511(5.9897) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 16.0468(15.8665) | Bit/dim 3.5313(3.5316) | Xent 0.0000(0.0000) | Loss 7.9841(8.4831) | Error 0.0000(0.0000) Steps 586(605.39) | Grad Norm 8.6006(6.0067) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 16.0398(15.8417) | Bit/dim 3.5175(3.5301) | Xent 0.0000(0.0000) | Loss 8.0701(8.3643) | Error 0.0000(0.0000) Steps 610(605.73) | Grad Norm 4.4897(5.6205) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 16.1396(15.9059) | Bit/dim 3.5487(3.5284) | Xent 0.0000(0.0000) | Loss 8.0719(8.2764) | Error 0.0000(0.0000) Steps 616(607.30) | Grad Norm 7.0727(5.5343) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 15.6409(15.8963) | Bit/dim 3.4949(3.5272) | Xent 0.0000(0.0000) | Loss 7.9293(8.2096) | Error 0.0000(0.0000) Steps 616(609.89) | Grad Norm 4.7722(5.7113) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 84.0399, Epoch Time 975.3760(914.6228), Bit/dim 3.5346(best: 3.5309), Xent 0.0000, Loss 3.5346, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 15.7785(15.9417) | Bit/dim 3.5388(3.5298) | Xent 0.0000(0.0000) | Loss 8.0149(8.7453) | Error 0.0000(0.0000) Steps 628(610.94) | Grad Norm 6.3576(6.0775) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 15.4213(15.9192) | Bit/dim 3.5325(3.5281) | Xent 0.0000(0.0000) | Loss 8.0120(8.5572) | Error 0.0000(0.0000) Steps 616(611.80) | Grad Norm 4.2293(5.8935) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 15.5145(15.8892) | Bit/dim 3.5305(3.5296) | Xent 0.0000(0.0000) | Loss 8.1315(8.4253) | Error 0.0000(0.0000) Steps 622(610.82) | Grad Norm 6.6172(5.8749) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 16.8248(15.9239) | Bit/dim 3.5302(3.5281) | Xent 0.0000(0.0000) | Loss 8.0491(8.3230) | Error 0.0000(0.0000) Steps 634(612.32) | Grad Norm 8.8508(5.7436) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 15.7855(15.8845) | Bit/dim 3.5329(3.5262) | Xent 0.0000(0.0000) | Loss 8.0145(8.2459) | Error 0.0000(0.0000) Steps 604(612.20) | Grad Norm 7.7380(5.9292) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 14.8227(15.8484) | Bit/dim 3.5274(3.5268) | Xent 0.0000(0.0000) | Loss 7.9468(8.1927) | Error 0.0000(0.0000) Steps 604(612.66) | Grad Norm 6.5731(6.0414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 83.0107, Epoch Time 973.9397(916.4023), Bit/dim 3.5342(best: 3.5309), Xent 0.0000, Loss 3.5342, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 15.9459(15.8627) | Bit/dim 3.5374(3.5252) | Xent 0.0000(0.0000) | Loss 8.1704(8.6508) | Error 0.0000(0.0000) Steps 622(614.91) | Grad Norm 6.2883(6.0809) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 14.8230(15.8567) | Bit/dim 3.4839(3.5256) | Xent 0.0000(0.0000) | Loss 7.8947(8.4884) | Error 0.0000(0.0000) Steps 604(615.93) | Grad Norm 3.7396(5.4002) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 16.9111(15.9107) | Bit/dim 3.5058(3.5263) | Xent 0.0000(0.0000) | Loss 8.0925(8.3811) | Error 0.0000(0.0000) Steps 640(614.14) | Grad Norm 4.0676(5.8548) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 16.4566(15.9348) | Bit/dim 3.5458(3.5263) | Xent 0.0000(0.0000) | Loss 8.0716(8.2842) | Error 0.0000(0.0000) Steps 640(614.43) | Grad Norm 3.2000(5.4023) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 16.1808(16.0030) | Bit/dim 3.5130(3.5264) | Xent 0.0000(0.0000) | Loss 7.9038(8.2040) | Error 0.0000(0.0000) Steps 586(610.18) | Grad Norm 7.7829(5.4387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 84.6554, Epoch Time 984.2524(918.4378), Bit/dim 3.5248(best: 3.5309), Xent 0.0000, Loss 3.5248, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 15.0095(15.9971) | Bit/dim 3.5029(3.5266) | Xent 0.0000(0.0000) | Loss 7.9114(8.7195) | Error 0.0000(0.0000) Steps 592(610.11) | Grad Norm 6.3989(5.4725) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 16.1764(15.9158) | Bit/dim 3.5390(3.5296) | Xent 0.0000(0.0000) | Loss 8.0229(8.5357) | Error 0.0000(0.0000) Steps 616(610.41) | Grad Norm 4.6282(5.6765) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 16.8993(15.9961) | Bit/dim 3.5643(3.5308) | Xent 0.0000(0.0000) | Loss 8.0813(8.4065) | Error 0.0000(0.0000) Steps 628(613.45) | Grad Norm 3.8280(5.4964) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 16.8921(16.0282) | Bit/dim 3.4665(3.5277) | Xent 0.0000(0.0000) | Loss 7.9258(8.3138) | Error 0.0000(0.0000) Steps 616(613.13) | Grad Norm 7.6795(5.3759) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 16.5285(16.1192) | Bit/dim 3.5426(3.5261) | Xent 0.0000(0.0000) | Loss 8.0837(8.2481) | Error 0.0000(0.0000) Steps 616(616.48) | Grad Norm 5.3158(5.9558) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 16.1019(16.0876) | Bit/dim 3.5115(3.5243) | Xent 0.0000(0.0000) | Loss 7.9768(8.1888) | Error 0.0000(0.0000) Steps 616(617.69) | Grad Norm 3.5979(5.4107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 85.3287, Epoch Time 986.1237(920.4684), Bit/dim 3.5208(best: 3.5248), Xent 0.0000, Loss 3.5208, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 16.3154(16.0175) | Bit/dim 3.5239(3.5239) | Xent 0.0000(0.0000) | Loss 8.1432(8.6561) | Error 0.0000(0.0000) Steps 598(617.40) | Grad Norm 7.0700(5.2818) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 15.7358(15.9884) | Bit/dim 3.5217(3.5230) | Xent 0.0000(0.0000) | Loss 8.0780(8.4793) | Error 0.0000(0.0000) Steps 604(615.26) | Grad Norm 6.1057(5.5567) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 16.0140(16.0214) | Bit/dim 3.4888(3.5208) | Xent 0.0000(0.0000) | Loss 8.0231(8.3617) | Error 0.0000(0.0000) Steps 634(617.45) | Grad Norm 7.6319(5.4965) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 16.7050(16.0066) | Bit/dim 3.5406(3.5206) | Xent 0.0000(0.0000) | Loss 8.1125(8.2744) | Error 0.0000(0.0000) Steps 592(615.62) | Grad Norm 3.1701(5.8874) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 16.1249(16.0077) | Bit/dim 3.4979(3.5167) | Xent 0.0000(0.0000) | Loss 8.0356(8.2041) | Error 0.0000(0.0000) Steps 610(614.95) | Grad Norm 6.7626(5.5439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 84.2159, Epoch Time 981.0943(922.2872), Bit/dim 3.5254(best: 3.5208), Xent 0.0000, Loss 3.5254, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 15.5141(15.9896) | Bit/dim 3.5397(3.5216) | Xent 0.0000(0.0000) | Loss 8.0521(8.7383) | Error 0.0000(0.0000) Steps 598(613.85) | Grad Norm 4.0712(5.6901) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 15.5531(16.0258) | Bit/dim 3.5471(3.5201) | Xent 0.0000(0.0000) | Loss 8.0363(8.5491) | Error 0.0000(0.0000) Steps 634(618.02) | Grad Norm 9.5927(5.7572) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 15.8727(16.0021) | Bit/dim 3.5441(3.5206) | Xent 0.0000(0.0000) | Loss 8.1156(8.4194) | Error 0.0000(0.0000) Steps 610(618.07) | Grad Norm 4.4938(5.8462) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 15.1722(16.0152) | Bit/dim 3.4968(3.5201) | Xent 0.0000(0.0000) | Loss 7.9001(8.3242) | Error 0.0000(0.0000) Steps 592(618.34) | Grad Norm 4.2196(5.5311) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 15.9545(16.0483) | Bit/dim 3.5486(3.5200) | Xent 0.0000(0.0000) | Loss 8.1376(8.2533) | Error 0.0000(0.0000) Steps 616(618.17) | Grad Norm 6.2056(5.5224) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 15.6567(16.0160) | Bit/dim 3.5406(3.5196) | Xent 0.0000(0.0000) | Loss 8.0747(8.1915) | Error 0.0000(0.0000) Steps 622(617.85) | Grad Norm 6.3781(5.8135) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 85.2279, Epoch Time 984.7936(924.1624), Bit/dim 3.5239(best: 3.5208), Xent 0.0000, Loss 3.5239, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 15.7721(16.0350) | Bit/dim 3.5352(3.5195) | Xent 0.0000(0.0000) | Loss 8.0243(8.6449) | Error 0.0000(0.0000) Steps 604(619.05) | Grad Norm 5.6811(5.9066) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 16.1984(16.0436) | Bit/dim 3.4698(3.5169) | Xent 0.0000(0.0000) | Loss 7.9750(8.4827) | Error 0.0000(0.0000) Steps 634(619.24) | Grad Norm 6.3920(5.8164) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 16.0324(16.1057) | Bit/dim 3.5270(3.5175) | Xent 0.0000(0.0000) | Loss 8.0170(8.3641) | Error 0.0000(0.0000) Steps 610(618.53) | Grad Norm 4.6420(5.8417) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 15.9645(16.1103) | Bit/dim 3.5463(3.5175) | Xent 0.0000(0.0000) | Loss 8.0423(8.2725) | Error 0.0000(0.0000) Steps 610(618.36) | Grad Norm 6.8581(5.5948) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 16.1430(16.1481) | Bit/dim 3.5106(3.5185) | Xent 0.0000(0.0000) | Loss 7.9090(8.2031) | Error 0.0000(0.0000) Steps 604(618.66) | Grad Norm 6.8412(5.5397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 85.8093, Epoch Time 993.4543(926.2411), Bit/dim 3.5206(best: 3.5208), Xent 0.0000, Loss 3.5206, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 16.7289(16.1619) | Bit/dim 3.5074(3.5211) | Xent 0.0000(0.0000) | Loss 7.9621(8.7547) | Error 0.0000(0.0000) Steps 616(619.02) | Grad Norm 5.4838(6.0461) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.3429(16.1442) | Bit/dim 3.4979(3.5179) | Xent 0.0000(0.0000) | Loss 7.9847(8.5646) | Error 0.0000(0.0000) Steps 622(617.77) | Grad Norm 3.8065(5.8885) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 16.0188(16.1146) | Bit/dim 3.4810(3.5152) | Xent 0.0000(0.0000) | Loss 8.0175(8.4192) | Error 0.0000(0.0000) Steps 604(615.96) | Grad Norm 4.9590(5.8685) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 16.1164(16.1194) | Bit/dim 3.5134(3.5165) | Xent 0.0000(0.0000) | Loss 7.9917(8.3216) | Error 0.0000(0.0000) Steps 628(617.52) | Grad Norm 3.6256(6.0817) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 15.8810(16.1432) | Bit/dim 3.5176(3.5180) | Xent 0.0000(0.0000) | Loss 7.8890(8.2358) | Error 0.0000(0.0000) Steps 598(617.38) | Grad Norm 3.9454(5.5068) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 15.9027(16.2191) | Bit/dim 3.5040(3.5177) | Xent 0.0000(0.0000) | Loss 8.1360(8.1912) | Error 0.0000(0.0000) Steps 622(618.64) | Grad Norm 4.2802(5.6962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 84.5541, Epoch Time 993.0244(928.2446), Bit/dim 3.5180(best: 3.5206), Xent 0.0000, Loss 3.5180, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 16.9601(16.2456) | Bit/dim 3.5400(3.5178) | Xent 0.0000(0.0000) | Loss 8.1882(8.6792) | Error 0.0000(0.0000) Steps 652(619.75) | Grad Norm 5.6288(5.7103) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 16.0821(16.1745) | Bit/dim 3.5038(3.5158) | Xent 0.0000(0.0000) | Loss 8.0107(8.5040) | Error 0.0000(0.0000) Steps 658(620.68) | Grad Norm 6.2833(5.9852) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 16.0091(16.1653) | Bit/dim 3.5023(3.5137) | Xent 0.0000(0.0000) | Loss 7.8000(8.3732) | Error 0.0000(0.0000) Steps 628(623.08) | Grad Norm 4.1944(5.6298) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 16.4898(16.1542) | Bit/dim 3.5109(3.5146) | Xent 0.0000(0.0000) | Loss 7.9575(8.2818) | Error 0.0000(0.0000) Steps 640(624.72) | Grad Norm 5.9232(5.7367) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 15.9569(16.1050) | Bit/dim 3.5217(3.5138) | Xent 0.0000(0.0000) | Loss 7.9208(8.2113) | Error 0.0000(0.0000) Steps 622(624.28) | Grad Norm 5.2682(5.6662) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 85.2917, Epoch Time 989.8223(930.0919), Bit/dim 3.5130(best: 3.5180), Xent 0.0000, Loss 3.5130, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 16.6006(16.1629) | Bit/dim 3.5298(3.5150) | Xent 0.0000(0.0000) | Loss 7.8910(8.7560) | Error 0.0000(0.0000) Steps 610(622.10) | Grad Norm 6.9739(5.9996) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 16.2369(16.1803) | Bit/dim 3.5110(3.5139) | Xent 0.0000(0.0000) | Loss 7.9058(8.5594) | Error 0.0000(0.0000) Steps 604(622.26) | Grad Norm 7.5406(5.9730) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 16.1097(16.1765) | Bit/dim 3.4956(3.5122) | Xent 0.0000(0.0000) | Loss 7.9863(8.4221) | Error 0.0000(0.0000) Steps 628(625.22) | Grad Norm 7.1419(6.0132) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 16.4686(16.2018) | Bit/dim 3.5256(3.5118) | Xent 0.0000(0.0000) | Loss 8.0208(8.3160) | Error 0.0000(0.0000) Steps 622(625.81) | Grad Norm 3.7146(5.8160) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 16.1069(16.2621) | Bit/dim 3.5147(3.5133) | Xent 0.0000(0.0000) | Loss 8.1056(8.2477) | Error 0.0000(0.0000) Steps 610(627.19) | Grad Norm 4.8958(5.6381) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 16.3306(16.2805) | Bit/dim 3.5216(3.5154) | Xent 0.0000(0.0000) | Loss 8.0283(8.1961) | Error 0.0000(0.0000) Steps 640(628.04) | Grad Norm 6.6817(6.0047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 85.0032, Epoch Time 999.7608(932.1820), Bit/dim 3.5159(best: 3.5130), Xent 0.0000, Loss 3.5159, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 16.3358(16.3071) | Bit/dim 3.4968(3.5168) | Xent 0.0000(0.0000) | Loss 8.0712(8.6627) | Error 0.0000(0.0000) Steps 592(626.81) | Grad Norm 3.2187(5.4982) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 16.8793(16.3911) | Bit/dim 3.5150(3.5168) | Xent 0.0000(0.0000) | Loss 8.0866(8.4940) | Error 0.0000(0.0000) Steps 676(628.70) | Grad Norm 5.4776(5.7087) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 15.9898(16.4336) | Bit/dim 3.5007(3.5156) | Xent 0.0000(0.0000) | Loss 7.9896(8.3615) | Error 0.0000(0.0000) Steps 640(628.06) | Grad Norm 4.6539(5.3931) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 16.5339(16.4652) | Bit/dim 3.4807(3.5124) | Xent 0.0000(0.0000) | Loss 7.9779(8.2669) | Error 0.0000(0.0000) Steps 610(626.12) | Grad Norm 6.4073(5.1349) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 17.2091(16.4852) | Bit/dim 3.4821(3.5080) | Xent 0.0000(0.0000) | Loss 7.9470(8.1963) | Error 0.0000(0.0000) Steps 622(625.88) | Grad Norm 4.4808(5.6014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 86.0794, Epoch Time 1015.4694(934.6806), Bit/dim 3.5136(best: 3.5130), Xent 0.0000, Loss 3.5136, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 16.8363(16.5682) | Bit/dim 3.5010(3.5050) | Xent 0.0000(0.0000) | Loss 8.0052(8.7410) | Error 0.0000(0.0000) Steps 622(627.73) | Grad Norm 5.4160(5.4181) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 16.1642(16.5213) | Bit/dim 3.5139(3.5062) | Xent 0.0000(0.0000) | Loss 8.0816(8.5497) | Error 0.0000(0.0000) Steps 628(629.17) | Grad Norm 5.8326(5.6899) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 16.8377(16.5954) | Bit/dim 3.5145(3.5066) | Xent 0.0000(0.0000) | Loss 8.1152(8.4095) | Error 0.0000(0.0000) Steps 640(632.17) | Grad Norm 7.7276(5.8268) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 15.7796(16.6386) | Bit/dim 3.4997(3.5105) | Xent 0.0000(0.0000) | Loss 7.8600(8.3068) | Error 0.0000(0.0000) Steps 598(630.69) | Grad Norm 4.6515(5.8937) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 16.8051(16.6938) | Bit/dim 3.4849(3.5118) | Xent 0.0000(0.0000) | Loss 7.9829(8.2379) | Error 0.0000(0.0000) Steps 634(633.10) | Grad Norm 8.4470(5.6900) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 15.9466(16.6806) | Bit/dim 3.4859(3.5111) | Xent 0.0000(0.0000) | Loss 7.9344(8.1850) | Error 0.0000(0.0000) Steps 622(636.84) | Grad Norm 4.4334(5.4424) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 85.6200, Epoch Time 1022.4218(937.3129), Bit/dim 3.5058(best: 3.5130), Xent 0.0000, Loss 3.5058, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 15.8958(16.6386) | Bit/dim 3.4823(3.5085) | Xent 0.0000(0.0000) | Loss 7.8361(8.6301) | Error 0.0000(0.0000) Steps 610(634.11) | Grad Norm 6.1926(5.6586) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 16.7706(16.6786) | Bit/dim 3.5141(3.5066) | Xent 0.0000(0.0000) | Loss 7.9460(8.4592) | Error 0.0000(0.0000) Steps 646(633.51) | Grad Norm 4.8655(5.4725) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 16.3537(16.6927) | Bit/dim 3.5044(3.5083) | Xent 0.0000(0.0000) | Loss 7.9066(8.3426) | Error 0.0000(0.0000) Steps 646(633.29) | Grad Norm 5.7292(5.3791) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 16.4646(16.6880) | Bit/dim 3.4895(3.5091) | Xent 0.0000(0.0000) | Loss 8.0450(8.2666) | Error 0.0000(0.0000) Steps 670(635.88) | Grad Norm 3.5787(5.4409) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 16.8945(16.7038) | Bit/dim 3.5120(3.5098) | Xent 0.0000(0.0000) | Loss 8.0004(8.2005) | Error 0.0000(0.0000) Steps 622(635.35) | Grad Norm 4.6834(5.5521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 86.2180, Epoch Time 1023.4398(939.8967), Bit/dim 3.5053(best: 3.5058), Xent 0.0000, Loss 3.5053, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 16.0035(16.6801) | Bit/dim 3.5019(3.5071) | Xent 0.0000(0.0000) | Loss 7.8887(8.7409) | Error 0.0000(0.0000) Steps 616(634.62) | Grad Norm 5.3002(5.3492) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 17.6806(16.6795) | Bit/dim 3.5092(3.5038) | Xent 0.0000(0.0000) | Loss 8.0769(8.5418) | Error 0.0000(0.0000) Steps 676(637.35) | Grad Norm 6.9095(5.4658) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.2902(16.7323) | Bit/dim 3.5417(3.5029) | Xent 0.0000(0.0000) | Loss 8.0535(8.3940) | Error 0.0000(0.0000) Steps 610(636.22) | Grad Norm 5.7993(5.6730) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 16.1283(16.7424) | Bit/dim 3.5135(3.5048) | Xent 0.0000(0.0000) | Loss 8.0972(8.2883) | Error 0.0000(0.0000) Steps 616(634.19) | Grad Norm 6.4611(5.6256) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 17.6536(16.8460) | Bit/dim 3.5313(3.5059) | Xent 0.0000(0.0000) | Loss 8.0565(8.2195) | Error 0.0000(0.0000) Steps 640(633.25) | Grad Norm 6.8680(5.4081) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 17.5505(16.8750) | Bit/dim 3.4558(3.5060) | Xent 0.0000(0.0000) | Loss 7.9684(8.1695) | Error 0.0000(0.0000) Steps 658(635.39) | Grad Norm 5.9819(5.7477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 85.6359, Epoch Time 1029.9747(942.5990), Bit/dim 3.5066(best: 3.5053), Xent 0.0000, Loss 3.5066, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 16.1340(16.8624) | Bit/dim 3.5419(3.5081) | Xent 0.0000(0.0000) | Loss 7.9754(8.6233) | Error 0.0000(0.0000) Steps 634(636.40) | Grad Norm 9.6689(5.6386) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 16.8923(16.8880) | Bit/dim 3.5143(3.5045) | Xent 0.0000(0.0000) | Loss 8.0972(8.4458) | Error 0.0000(0.0000) Steps 640(636.73) | Grad Norm 5.4627(5.5024) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 16.8667(16.8815) | Bit/dim 3.5386(3.5052) | Xent 0.0000(0.0000) | Loss 7.9426(8.3304) | Error 0.0000(0.0000) Steps 616(633.94) | Grad Norm 4.9800(5.4191) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 17.5550(16.9442) | Bit/dim 3.5405(3.5062) | Xent 0.0000(0.0000) | Loss 8.0004(8.2496) | Error 0.0000(0.0000) Steps 682(638.72) | Grad Norm 4.0830(5.5959) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 16.7763(17.0337) | Bit/dim 3.5084(3.5051) | Xent 0.0000(0.0000) | Loss 8.0450(8.1907) | Error 0.0000(0.0000) Steps 622(641.03) | Grad Norm 4.4645(5.2719) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 86.3597, Epoch Time 1042.8001(945.6050), Bit/dim 3.5142(best: 3.5053), Xent 0.0000, Loss 3.5142, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 17.0769(17.1509) | Bit/dim 3.5006(3.5009) | Xent 0.0000(0.0000) | Loss 8.0397(8.7565) | Error 0.0000(0.0000) Steps 664(643.12) | Grad Norm 5.7274(5.9240) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 16.9917(17.0809) | Bit/dim 3.4766(3.5007) | Xent 0.0000(0.0000) | Loss 7.8794(8.5552) | Error 0.0000(0.0000) Steps 622(644.08) | Grad Norm 3.8982(5.4800) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 18.2810(17.1262) | Bit/dim 3.5103(3.5010) | Xent 0.0000(0.0000) | Loss 8.0232(8.4135) | Error 0.0000(0.0000) Steps 646(642.62) | Grad Norm 8.9389(5.4161) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 17.0577(17.1056) | Bit/dim 3.4642(3.5020) | Xent 0.0000(0.0000) | Loss 7.8820(8.3026) | Error 0.0000(0.0000) Steps 640(643.47) | Grad Norm 6.7632(5.5338) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 17.6868(17.1434) | Bit/dim 3.4988(3.5026) | Xent 0.0000(0.0000) | Loss 8.0536(8.2264) | Error 0.0000(0.0000) Steps 640(645.86) | Grad Norm 5.4823(5.5217) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 16.6591(17.0423) | Bit/dim 3.4928(3.5024) | Xent 0.0000(0.0000) | Loss 7.9675(8.1640) | Error 0.0000(0.0000) Steps 628(644.33) | Grad Norm 3.2972(5.4189) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 87.4115, Epoch Time 1044.8524(948.5825), Bit/dim 3.5030(best: 3.5053), Xent 0.0000, Loss 3.5030, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.1727(17.0606) | Bit/dim 3.4917(3.5036) | Xent 0.0000(0.0000) | Loss 8.0088(8.6451) | Error 0.0000(0.0000) Steps 652(644.77) | Grad Norm 7.0331(5.6537) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.5159(17.0919) | Bit/dim 3.4592(3.5001) | Xent 0.0000(0.0000) | Loss 7.8160(8.4691) | Error 0.0000(0.0000) Steps 610(643.00) | Grad Norm 5.4336(5.4550) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 16.8476(17.0653) | Bit/dim 3.5194(3.5017) | Xent 0.0000(0.0000) | Loss 7.9551(8.3322) | Error 0.0000(0.0000) Steps 622(642.27) | Grad Norm 4.5255(5.1904) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 17.6807(17.1116) | Bit/dim 3.4891(3.5019) | Xent 0.0000(0.0000) | Loss 8.0459(8.2425) | Error 0.0000(0.0000) Steps 688(646.89) | Grad Norm 5.0926(5.4211) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 17.2526(17.1476) | Bit/dim 3.5196(3.5004) | Xent 0.0000(0.0000) | Loss 8.0293(8.1799) | Error 0.0000(0.0000) Steps 664(647.63) | Grad Norm 4.1735(5.2671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 87.3623, Epoch Time 1049.9337(951.6230), Bit/dim 3.5098(best: 3.5030), Xent 0.0000, Loss 3.5098, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 16.4054(17.1230) | Bit/dim 3.5317(3.5004) | Xent 0.0000(0.0000) | Loss 7.9692(8.7151) | Error 0.0000(0.0000) Steps 634(646.83) | Grad Norm 5.2083(5.5597) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 16.8135(17.1028) | Bit/dim 3.5286(3.5019) | Xent 0.0000(0.0000) | Loss 7.9590(8.5172) | Error 0.0000(0.0000) Steps 652(644.79) | Grad Norm 10.1757(5.6043) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.1182(17.0724) | Bit/dim 3.4872(3.5001) | Xent 0.0000(0.0000) | Loss 7.9398(8.3808) | Error 0.0000(0.0000) Steps 670(646.04) | Grad Norm 5.7571(5.6740) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.4055(17.0684) | Bit/dim 3.4657(3.4987) | Xent 0.0000(0.0000) | Loss 7.9443(8.2740) | Error 0.0000(0.0000) Steps 658(645.20) | Grad Norm 3.9019(5.5790) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 17.7771(17.1563) | Bit/dim 3.5199(3.4980) | Xent 0.0000(0.0000) | Loss 7.9844(8.1968) | Error 0.0000(0.0000) Steps 640(645.84) | Grad Norm 3.9802(5.4055) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 17.0549(17.1088) | Bit/dim 3.4722(3.4978) | Xent 0.0000(0.0000) | Loss 7.9230(8.1421) | Error 0.0000(0.0000) Steps 670(644.25) | Grad Norm 4.5533(5.5254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 85.4763, Epoch Time 1042.4285(954.3472), Bit/dim 3.5034(best: 3.5030), Xent 0.0000, Loss 3.5034, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 16.7039(17.0567) | Bit/dim 3.4781(3.4994) | Xent 0.0000(0.0000) | Loss 7.9877(8.6219) | Error 0.0000(0.0000) Steps 628(647.77) | Grad Norm 6.6788(5.5531) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 16.6018(17.1204) | Bit/dim 3.4934(3.4978) | Xent 0.0000(0.0000) | Loss 7.9888(8.4564) | Error 0.0000(0.0000) Steps 646(648.41) | Grad Norm 7.4464(5.7266) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 17.3144(17.2374) | Bit/dim 3.4806(3.4967) | Xent 0.0000(0.0000) | Loss 8.0054(8.3394) | Error 0.0000(0.0000) Steps 640(651.08) | Grad Norm 5.3793(5.8781) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 16.9211(17.2240) | Bit/dim 3.4696(3.4953) | Xent 0.0000(0.0000) | Loss 7.9477(8.2496) | Error 0.0000(0.0000) Steps 670(653.34) | Grad Norm 6.9881(5.9680) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.3622(17.2807) | Bit/dim 3.4586(3.4967) | Xent 0.0000(0.0000) | Loss 7.8816(8.1835) | Error 0.0000(0.0000) Steps 634(654.61) | Grad Norm 7.2335(5.7542) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 86.6545, Epoch Time 1056.1674(957.4018), Bit/dim 3.5016(best: 3.5030), Xent 0.0000, Loss 3.5016, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 17.5063(17.2776) | Bit/dim 3.5046(3.4966) | Xent 0.0000(0.0000) | Loss 8.1196(8.7290) | Error 0.0000(0.0000) Steps 664(654.70) | Grad Norm 6.6416(5.5598) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 17.6419(17.2812) | Bit/dim 3.4977(3.4965) | Xent 0.0000(0.0000) | Loss 8.0058(8.5429) | Error 0.0000(0.0000) Steps 664(655.01) | Grad Norm 4.3419(5.3657) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 17.0703(17.3101) | Bit/dim 3.4701(3.4944) | Xent 0.0000(0.0000) | Loss 8.0800(8.3936) | Error 0.0000(0.0000) Steps 676(655.15) | Grad Norm 6.1966(5.6453) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 16.6174(17.2985) | Bit/dim 3.4871(3.4942) | Xent 0.0000(0.0000) | Loss 7.8103(8.2753) | Error 0.0000(0.0000) Steps 658(656.41) | Grad Norm 4.9265(5.6862) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 16.8614(17.2969) | Bit/dim 3.5232(3.4950) | Xent 0.0000(0.0000) | Loss 7.9696(8.2073) | Error 0.0000(0.0000) Steps 652(657.03) | Grad Norm 7.5573(5.6314) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 17.0853(17.3458) | Bit/dim 3.4942(3.4972) | Xent 0.0000(0.0000) | Loss 7.9788(8.1628) | Error 0.0000(0.0000) Steps 622(656.23) | Grad Norm 3.6306(5.6563) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 87.5413, Epoch Time 1059.4843(960.4643), Bit/dim 3.4974(best: 3.5016), Xent 0.0000, Loss 3.4974, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 17.7699(17.3506) | Bit/dim 3.5507(3.4974) | Xent 0.0000(0.0000) | Loss 8.1020(8.6449) | Error 0.0000(0.0000) Steps 694(657.81) | Grad Norm 4.5787(5.7374) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 16.7283(17.3966) | Bit/dim 3.5034(3.4970) | Xent 0.0000(0.0000) | Loss 7.9815(8.4788) | Error 0.0000(0.0000) Steps 634(657.45) | Grad Norm 4.8362(5.4398) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.4879(17.4110) | Bit/dim 3.4795(3.4950) | Xent 0.0000(0.0000) | Loss 8.0451(8.3646) | Error 0.0000(0.0000) Steps 652(656.56) | Grad Norm 7.3896(5.5588) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 18.1206(17.3917) | Bit/dim 3.4663(3.4936) | Xent 0.0000(0.0000) | Loss 7.9006(8.2613) | Error 0.0000(0.0000) Steps 694(657.41) | Grad Norm 3.7282(5.2783) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 17.6198(17.4214) | Bit/dim 3.4720(3.4913) | Xent 0.0000(0.0000) | Loss 8.0450(8.1877) | Error 0.0000(0.0000) Steps 682(658.27) | Grad Norm 4.7908(5.0950) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 87.3941, Epoch Time 1063.2778(963.5487), Bit/dim 3.4967(best: 3.4974), Xent 0.0000, Loss 3.4967, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 17.3198(17.3901) | Bit/dim 3.5014(3.4949) | Xent 0.0000(0.0000) | Loss 8.0816(8.7610) | Error 0.0000(0.0000) Steps 688(661.24) | Grad Norm 5.6699(5.2848) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 16.6474(17.3857) | Bit/dim 3.4895(3.4958) | Xent 0.0000(0.0000) | Loss 7.8813(8.5662) | Error 0.0000(0.0000) Steps 664(664.40) | Grad Norm 5.0026(5.3404) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 17.6019(17.3477) | Bit/dim 3.5550(3.4962) | Xent 0.0000(0.0000) | Loss 8.1860(8.4318) | Error 0.0000(0.0000) Steps 652(665.00) | Grad Norm 5.7845(5.5136) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 16.9285(17.3963) | Bit/dim 3.4376(3.4943) | Xent 0.0000(0.0000) | Loss 8.0757(8.3208) | Error 0.0000(0.0000) Steps 676(664.34) | Grad Norm 5.8550(5.4163) | Total Time 0.00(0.00)\n",
      "Iter 5820 | Time 17.6566(17.4779) | Bit/dim 3.5034(3.4932) | Xent 0.0000(0.0000) | Loss 8.0593(8.2492) | Error 0.0000(0.0000) Steps 694(665.12) | Grad Norm 10.2686(5.4103) | Total Time 0.00(0.00)\n",
      "Iter 5830 | Time 17.4461(17.5336) | Bit/dim 3.5110(3.4929) | Xent 0.0000(0.0000) | Loss 8.0479(8.1962) | Error 0.0000(0.0000) Steps 664(667.05) | Grad Norm 7.0275(5.8934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 90.3465, Epoch Time 1071.0451(966.7736), Bit/dim 3.4939(best: 3.4967), Xent 0.0000, Loss 3.4939, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 17.2056(17.5784) | Bit/dim 3.5257(3.4932) | Xent 0.0000(0.0000) | Loss 8.1276(8.6950) | Error 0.0000(0.0000) Steps 658(669.95) | Grad Norm 7.7102(5.5432) | Total Time 0.00(0.00)\n",
      "Iter 5850 | Time 17.7585(17.5694) | Bit/dim 3.5186(3.4933) | Xent 0.0000(0.0000) | Loss 8.0598(8.5058) | Error 0.0000(0.0000) Steps 652(668.19) | Grad Norm 6.8300(5.4042) | Total Time 0.00(0.00)\n",
      "Iter 5860 | Time 17.8915(17.5639) | Bit/dim 3.4548(3.4909) | Xent 0.0000(0.0000) | Loss 7.9466(8.3624) | Error 0.0000(0.0000) Steps 640(661.98) | Grad Norm 5.8260(5.4870) | Total Time 0.00(0.00)\n",
      "Iter 5870 | Time 16.6815(17.4361) | Bit/dim 3.5185(3.4906) | Xent 0.0000(0.0000) | Loss 7.9724(8.2573) | Error 0.0000(0.0000) Steps 646(659.45) | Grad Norm 6.5215(5.4858) | Total Time 0.00(0.00)\n",
      "Iter 5880 | Time 16.5592(17.3988) | Bit/dim 3.5221(3.4898) | Xent 0.0000(0.0000) | Loss 7.8610(8.1788) | Error 0.0000(0.0000) Steps 628(657.64) | Grad Norm 8.9858(5.5540) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 86.3736, Epoch Time 1062.7320(969.6523), Bit/dim 3.4970(best: 3.4939), Xent 0.0000, Loss 3.4970, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 17.7424(17.3717) | Bit/dim 3.4997(3.4908) | Xent 0.0000(0.0000) | Loss 7.9234(8.6855) | Error 0.0000(0.0000) Steps 658(655.47) | Grad Norm 4.1626(5.5364) | Total Time 0.00(0.00)\n",
      "Iter 5900 | Time 17.5923(17.3868) | Bit/dim 3.4666(3.4903) | Xent 0.0000(0.0000) | Loss 7.9811(8.5104) | Error 0.0000(0.0000) Steps 664(655.68) | Grad Norm 4.4765(5.2878) | Total Time 0.00(0.00)\n",
      "Iter 5910 | Time 17.5757(17.4680) | Bit/dim 3.4859(3.4924) | Xent 0.0000(0.0000) | Loss 8.0588(8.3693) | Error 0.0000(0.0000) Steps 646(655.22) | Grad Norm 4.8174(5.4483) | Total Time 0.00(0.00)\n",
      "Iter 5920 | Time 17.1757(17.4985) | Bit/dim 3.5007(3.4895) | Xent 0.0000(0.0000) | Loss 7.9886(8.2684) | Error 0.0000(0.0000) Steps 664(657.26) | Grad Norm 4.6455(5.6621) | Total Time 0.00(0.00)\n",
      "Iter 5930 | Time 17.3211(17.4148) | Bit/dim 3.4749(3.4881) | Xent 0.0000(0.0000) | Loss 8.0206(8.1734) | Error 0.0000(0.0000) Steps 694(659.20) | Grad Norm 3.4263(5.4399) | Total Time 0.00(0.00)\n",
      "Iter 5940 | Time 16.9735(17.4503) | Bit/dim 3.5002(3.4875) | Xent 0.0000(0.0000) | Loss 7.9853(8.1231) | Error 0.0000(0.0000) Steps 658(657.57) | Grad Norm 7.1588(5.4876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 86.2955, Epoch Time 1065.1362(972.5168), Bit/dim 3.4997(best: 3.4939), Xent 0.0000, Loss 3.4997, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 16.5737(17.4069) | Bit/dim 3.4902(3.4898) | Xent 0.0000(0.0000) | Loss 7.9337(8.5954) | Error 0.0000(0.0000) Steps 640(657.02) | Grad Norm 4.1241(5.7329) | Total Time 0.00(0.00)\n",
      "Iter 5960 | Time 18.6250(17.4023) | Bit/dim 3.4840(3.4908) | Xent 0.0000(0.0000) | Loss 8.0530(8.4299) | Error 0.0000(0.0000) Steps 712(656.78) | Grad Norm 5.2537(5.6082) | Total Time 0.00(0.00)\n",
      "Iter 5970 | Time 17.0856(17.4103) | Bit/dim 3.4520(3.4886) | Xent 0.0000(0.0000) | Loss 7.8624(8.3011) | Error 0.0000(0.0000) Steps 640(656.88) | Grad Norm 3.3532(5.6114) | Total Time 0.00(0.00)\n",
      "Iter 5980 | Time 17.3154(17.4538) | Bit/dim 3.4928(3.4878) | Xent 0.0000(0.0000) | Loss 7.9788(8.2190) | Error 0.0000(0.0000) Steps 688(657.23) | Grad Norm 7.5639(5.4151) | Total Time 0.00(0.00)\n",
      "Iter 5990 | Time 17.4708(17.4922) | Bit/dim 3.5003(3.4875) | Xent 0.0000(0.0000) | Loss 7.9153(8.1558) | Error 0.0000(0.0000) Steps 664(660.61) | Grad Norm 5.7859(5.7023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 89.5023, Epoch Time 1069.9272(975.4391), Bit/dim 3.4869(best: 3.4939), Xent 0.0000, Loss 3.4869, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 17.4831(17.5546) | Bit/dim 3.5359(3.4893) | Xent 0.0000(0.0000) | Loss 8.0363(8.7212) | Error 0.0000(0.0000) Steps 658(661.91) | Grad Norm 3.1761(5.2449) | Total Time 0.00(0.00)\n",
      "Iter 6010 | Time 18.1313(17.6008) | Bit/dim 3.4922(3.4886) | Xent 0.0000(0.0000) | Loss 7.9961(8.5242) | Error 0.0000(0.0000) Steps 664(662.07) | Grad Norm 7.0123(5.1422) | Total Time 0.00(0.00)\n",
      "Iter 6020 | Time 17.6705(17.6486) | Bit/dim 3.4945(3.4864) | Xent 0.0000(0.0000) | Loss 7.9416(8.3769) | Error 0.0000(0.0000) Steps 682(664.90) | Grad Norm 3.8382(4.9830) | Total Time 0.00(0.00)\n",
      "Iter 6030 | Time 17.8752(17.6646) | Bit/dim 3.4784(3.4839) | Xent 0.0000(0.0000) | Loss 7.9893(8.2733) | Error 0.0000(0.0000) Steps 694(669.18) | Grad Norm 7.9798(5.2591) | Total Time 0.00(0.00)\n",
      "Iter 6040 | Time 18.2176(17.6673) | Bit/dim 3.5173(3.4851) | Xent 0.0000(0.0000) | Loss 8.0555(8.2079) | Error 0.0000(0.0000) Steps 646(669.23) | Grad Norm 7.5568(5.6460) | Total Time 0.00(0.00)\n",
      "Iter 6050 | Time 17.2064(17.7114) | Bit/dim 3.4881(3.4881) | Xent 0.0000(0.0000) | Loss 7.9133(8.1690) | Error 0.0000(0.0000) Steps 664(672.22) | Grad Norm 6.7174(5.8203) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 88.2847, Epoch Time 1081.7151(978.6274), Bit/dim 3.4931(best: 3.4869), Xent 0.0000, Loss 3.4931, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 17.1114(17.6617) | Bit/dim 3.4471(3.4852) | Xent 0.0000(0.0000) | Loss 8.0041(8.6531) | Error 0.0000(0.0000) Steps 700(673.74) | Grad Norm 3.7825(5.5108) | Total Time 0.00(0.00)\n",
      "Iter 6070 | Time 17.5278(17.7069) | Bit/dim 3.5055(3.4886) | Xent 0.0000(0.0000) | Loss 8.1224(8.4890) | Error 0.0000(0.0000) Steps 676(674.38) | Grad Norm 6.3880(5.3463) | Total Time 0.00(0.00)\n",
      "Iter 6080 | Time 18.1218(17.6973) | Bit/dim 3.4847(3.4879) | Xent 0.0000(0.0000) | Loss 8.0620(8.3604) | Error 0.0000(0.0000) Steps 688(674.53) | Grad Norm 5.2207(5.5476) | Total Time 0.00(0.00)\n",
      "Iter 6090 | Time 17.7750(17.6984) | Bit/dim 3.4706(3.4887) | Xent 0.0000(0.0000) | Loss 7.8988(8.2703) | Error 0.0000(0.0000) Steps 676(674.14) | Grad Norm 4.8567(5.5383) | Total Time 0.00(0.00)\n",
      "Iter 6100 | Time 17.6546(17.8469) | Bit/dim 3.4724(3.4869) | Xent 0.0000(0.0000) | Loss 7.9325(8.2021) | Error 0.0000(0.0000) Steps 706(678.44) | Grad Norm 3.6109(5.4002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 89.9427, Epoch Time 1088.4620(981.9225), Bit/dim 3.4835(best: 3.4869), Xent 0.0000, Loss 3.4835, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 17.8460(17.8641) | Bit/dim 3.4653(3.4819) | Xent 0.0000(0.0000) | Loss 8.1431(8.7912) | Error 0.0000(0.0000) Steps 700(682.31) | Grad Norm 8.4582(5.8582) | Total Time 0.00(0.00)\n",
      "Iter 6120 | Time 18.4179(17.8357) | Bit/dim 3.4563(3.4818) | Xent 0.0000(0.0000) | Loss 7.9811(8.5805) | Error 0.0000(0.0000) Steps 652(679.35) | Grad Norm 5.7998(5.3573) | Total Time 0.00(0.00)\n",
      "Iter 6130 | Time 17.0019(17.7389) | Bit/dim 3.4847(3.4821) | Xent 0.0000(0.0000) | Loss 7.9793(8.4090) | Error 0.0000(0.0000) Steps 682(675.99) | Grad Norm 5.0866(5.4456) | Total Time 0.00(0.00)\n",
      "Iter 6140 | Time 17.9174(17.7573) | Bit/dim 3.4830(3.4812) | Xent 0.0000(0.0000) | Loss 8.0739(8.3047) | Error 0.0000(0.0000) Steps 682(673.27) | Grad Norm 3.8815(5.6152) | Total Time 0.00(0.00)\n",
      "Iter 6150 | Time 18.0952(17.7710) | Bit/dim 3.4612(3.4816) | Xent 0.0000(0.0000) | Loss 7.9354(8.2146) | Error 0.0000(0.0000) Steps 676(671.25) | Grad Norm 3.0859(5.1587) | Total Time 0.00(0.00)\n",
      "Iter 6160 | Time 17.4887(17.7315) | Bit/dim 3.4778(3.4856) | Xent 0.0000(0.0000) | Loss 7.9396(8.1615) | Error 0.0000(0.0000) Steps 676(668.86) | Grad Norm 5.2945(5.4308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 86.4863, Epoch Time 1078.6840(984.8253), Bit/dim 3.4828(best: 3.4835), Xent 0.0000, Loss 3.4828, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 18.1288(17.6909) | Bit/dim 3.4556(3.4832) | Xent 0.0000(0.0000) | Loss 7.8587(8.6088) | Error 0.0000(0.0000) Steps 670(666.63) | Grad Norm 5.1668(5.3579) | Total Time 0.00(0.00)\n",
      "Iter 6180 | Time 16.5299(17.5527) | Bit/dim 3.4860(3.4870) | Xent 0.0000(0.0000) | Loss 7.9872(8.4414) | Error 0.0000(0.0000) Steps 640(665.62) | Grad Norm 5.2734(5.6936) | Total Time 0.00(0.00)\n",
      "Iter 6190 | Time 17.9903(17.6050) | Bit/dim 3.4816(3.4829) | Xent 0.0000(0.0000) | Loss 7.9111(8.3220) | Error 0.0000(0.0000) Steps 658(666.07) | Grad Norm 7.1308(5.6264) | Total Time 0.00(0.00)\n",
      "Iter 6200 | Time 17.6884(17.5649) | Bit/dim 3.4654(3.4844) | Xent 0.0000(0.0000) | Loss 7.8965(8.2294) | Error 0.0000(0.0000) Steps 682(669.27) | Grad Norm 5.9488(5.3779) | Total Time 0.00(0.00)\n",
      "Iter 6210 | Time 17.5168(17.6205) | Bit/dim 3.4864(3.4825) | Xent 0.0000(0.0000) | Loss 7.9304(8.1470) | Error 0.0000(0.0000) Steps 682(669.59) | Grad Norm 3.6328(5.1154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 87.7144, Epoch Time 1070.5285(987.3964), Bit/dim 3.4847(best: 3.4828), Xent 0.0000, Loss 3.4847, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 17.9032(17.6195) | Bit/dim 3.4873(3.4826) | Xent 0.0000(0.0000) | Loss 8.0805(8.7158) | Error 0.0000(0.0000) Steps 694(670.75) | Grad Norm 6.6045(5.2889) | Total Time 0.00(0.00)\n",
      "Iter 6230 | Time 18.4228(17.6347) | Bit/dim 3.5099(3.4837) | Xent 0.0000(0.0000) | Loss 8.0833(8.5216) | Error 0.0000(0.0000) Steps 676(668.88) | Grad Norm 3.9981(5.4343) | Total Time 0.00(0.00)\n",
      "Iter 6240 | Time 17.5359(17.6867) | Bit/dim 3.4784(3.4843) | Xent 0.0000(0.0000) | Loss 7.9183(8.3781) | Error 0.0000(0.0000) Steps 688(668.98) | Grad Norm 5.6097(5.2470) | Total Time 0.00(0.00)\n",
      "Iter 6250 | Time 18.6920(17.6575) | Bit/dim 3.5072(3.4827) | Xent 0.0000(0.0000) | Loss 8.1766(8.2691) | Error 0.0000(0.0000) Steps 694(667.83) | Grad Norm 2.8715(5.4107) | Total Time 0.00(0.00)\n",
      "Iter 6260 | Time 17.3210(17.6970) | Bit/dim 3.4905(3.4812) | Xent 0.0000(0.0000) | Loss 7.9354(8.1919) | Error 0.0000(0.0000) Steps 670(667.92) | Grad Norm 4.9738(5.1961) | Total Time 0.00(0.00)\n",
      "Iter 6270 | Time 17.0268(17.7318) | Bit/dim 3.5210(3.4812) | Xent 0.0000(0.0000) | Loss 8.1051(8.1442) | Error 0.0000(0.0000) Steps 634(668.20) | Grad Norm 4.3346(5.3438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 87.6170, Epoch Time 1081.4617(990.2184), Bit/dim 3.4835(best: 3.4828), Xent 0.0000, Loss 3.4835, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 18.4999(17.8091) | Bit/dim 3.4627(3.4827) | Xent 0.0000(0.0000) | Loss 7.9323(8.6281) | Error 0.0000(0.0000) Steps 676(670.45) | Grad Norm 3.7097(5.4036) | Total Time 0.00(0.00)\n",
      "Iter 6290 | Time 17.9881(17.8537) | Bit/dim 3.4659(3.4767) | Xent 0.0000(0.0000) | Loss 8.0867(8.4573) | Error 0.0000(0.0000) Steps 688(673.99) | Grad Norm 5.7859(5.3944) | Total Time 0.00(0.00)\n",
      "Iter 6300 | Time 17.8480(17.7221) | Bit/dim 3.4825(3.4788) | Xent 0.0000(0.0000) | Loss 7.9025(8.3253) | Error 0.0000(0.0000) Steps 670(672.03) | Grad Norm 3.2038(5.3755) | Total Time 0.00(0.00)\n",
      "Iter 6310 | Time 17.8659(17.6756) | Bit/dim 3.4999(3.4771) | Xent 0.0000(0.0000) | Loss 8.0395(8.2297) | Error 0.0000(0.0000) Steps 652(670.16) | Grad Norm 5.4900(5.3428) | Total Time 0.00(0.00)\n",
      "Iter 6320 | Time 18.2612(17.6517) | Bit/dim 3.5000(3.4807) | Xent 0.0000(0.0000) | Loss 8.0657(8.1800) | Error 0.0000(0.0000) Steps 658(668.26) | Grad Norm 4.0928(5.2684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 88.7544, Epoch Time 1082.3252(992.9816), Bit/dim 3.4779(best: 3.4828), Xent 0.0000, Loss 3.4779, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 17.7936(17.7061) | Bit/dim 3.4845(3.4790) | Xent 0.0000(0.0000) | Loss 7.9872(8.7509) | Error 0.0000(0.0000) Steps 658(670.49) | Grad Norm 6.4442(5.2324) | Total Time 0.00(0.00)\n",
      "Iter 6340 | Time 17.5933(17.7381) | Bit/dim 3.5047(3.4777) | Xent 0.0000(0.0000) | Loss 8.0196(8.5531) | Error 0.0000(0.0000) Steps 676(674.25) | Grad Norm 7.6011(5.4923) | Total Time 0.00(0.00)\n",
      "Iter 6350 | Time 18.2205(17.8432) | Bit/dim 3.5034(3.4791) | Xent 0.0000(0.0000) | Loss 8.1265(8.4159) | Error 0.0000(0.0000) Steps 676(676.97) | Grad Norm 6.4427(5.5306) | Total Time 0.00(0.00)\n",
      "Iter 6360 | Time 18.6860(17.9143) | Bit/dim 3.4695(3.4792) | Xent 0.0000(0.0000) | Loss 7.9571(8.3123) | Error 0.0000(0.0000) Steps 718(680.72) | Grad Norm 4.9921(5.4965) | Total Time 0.00(0.00)\n",
      "Iter 6370 | Time 17.6472(17.8969) | Bit/dim 3.4662(3.4798) | Xent 0.0000(0.0000) | Loss 7.9470(8.2358) | Error 0.0000(0.0000) Steps 700(682.60) | Grad Norm 6.5636(5.4881) | Total Time 0.00(0.00)\n",
      "Iter 6380 | Time 18.0563(17.9413) | Bit/dim 3.5300(3.4796) | Xent 0.0000(0.0000) | Loss 8.0672(8.1764) | Error 0.0000(0.0000) Steps 676(682.64) | Grad Norm 4.4812(5.7537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 89.7835, Epoch Time 1096.6908(996.0928), Bit/dim 3.4846(best: 3.4779), Xent 0.0000, Loss 3.4846, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 17.9467(17.9534) | Bit/dim 3.4676(3.4777) | Xent 0.0000(0.0000) | Loss 8.0484(8.6686) | Error 0.0000(0.0000) Steps 682(682.54) | Grad Norm 3.9680(5.3347) | Total Time 0.00(0.00)\n",
      "Iter 6400 | Time 18.1706(17.9993) | Bit/dim 3.4825(3.4769) | Xent 0.0000(0.0000) | Loss 8.1230(8.4943) | Error 0.0000(0.0000) Steps 700(684.49) | Grad Norm 5.1584(5.2869) | Total Time 0.00(0.00)\n",
      "Iter 6410 | Time 19.1065(18.0293) | Bit/dim 3.4778(3.4770) | Xent 0.0000(0.0000) | Loss 8.1347(8.3729) | Error 0.0000(0.0000) Steps 700(685.52) | Grad Norm 4.5399(5.6378) | Total Time 0.00(0.00)\n",
      "Iter 6420 | Time 18.5014(18.0076) | Bit/dim 3.4643(3.4776) | Xent 0.0000(0.0000) | Loss 8.0138(8.2702) | Error 0.0000(0.0000) Steps 688(684.44) | Grad Norm 3.3940(5.0785) | Total Time 0.00(0.00)\n",
      "Iter 6430 | Time 18.4109(18.0073) | Bit/dim 3.4930(3.4788) | Xent 0.0000(0.0000) | Loss 8.0176(8.1963) | Error 0.0000(0.0000) Steps 694(684.30) | Grad Norm 8.4339(5.3761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 89.0256, Epoch Time 1099.1767(999.1854), Bit/dim 3.4827(best: 3.4779), Xent 0.0000, Loss 3.4827, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 18.5228(18.0840) | Bit/dim 3.4884(3.4798) | Xent 0.0000(0.0000) | Loss 7.9489(8.7683) | Error 0.0000(0.0000) Steps 688(684.67) | Grad Norm 4.7168(5.4156) | Total Time 0.00(0.00)\n",
      "Iter 6450 | Time 17.4979(18.0756) | Bit/dim 3.4711(3.4791) | Xent 0.0000(0.0000) | Loss 8.0361(8.5696) | Error 0.0000(0.0000) Steps 694(684.02) | Grad Norm 4.8313(5.2475) | Total Time 0.00(0.00)\n",
      "Iter 6460 | Time 18.6257(18.0984) | Bit/dim 3.4589(3.4750) | Xent 0.0000(0.0000) | Loss 7.9719(8.4106) | Error 0.0000(0.0000) Steps 700(687.35) | Grad Norm 5.5624(5.3786) | Total Time 0.00(0.00)\n",
      "Iter 6470 | Time 17.5511(18.1588) | Bit/dim 3.4905(3.4781) | Xent 0.0000(0.0000) | Loss 8.0254(8.3153) | Error 0.0000(0.0000) Steps 694(691.77) | Grad Norm 7.3018(5.5289) | Total Time 0.00(0.00)\n",
      "Iter 6480 | Time 18.5432(18.1449) | Bit/dim 3.4654(3.4772) | Xent 0.0000(0.0000) | Loss 7.9709(8.2358) | Error 0.0000(0.0000) Steps 676(692.99) | Grad Norm 4.6235(5.7507) | Total Time 0.00(0.00)\n",
      "Iter 6490 | Time 18.5908(18.2289) | Bit/dim 3.4767(3.4747) | Xent 0.0000(0.0000) | Loss 8.0285(8.1803) | Error 0.0000(0.0000) Steps 724(696.62) | Grad Norm 6.3018(5.4561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 89.0531, Epoch Time 1110.8210(1002.5344), Bit/dim 3.4779(best: 3.4779), Xent 0.0000, Loss 3.4779, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 17.7972(18.2968) | Bit/dim 3.5129(3.4755) | Xent 0.0000(0.0000) | Loss 8.1269(8.6793) | Error 0.0000(0.0000) Steps 694(698.08) | Grad Norm 2.6871(5.5286) | Total Time 0.00(0.00)\n",
      "Iter 6510 | Time 17.8353(18.2895) | Bit/dim 3.4514(3.4723) | Xent 0.0000(0.0000) | Loss 7.9921(8.5027) | Error 0.0000(0.0000) Steps 712(698.03) | Grad Norm 4.9826(5.3514) | Total Time 0.00(0.00)\n",
      "Iter 6520 | Time 18.1463(18.2358) | Bit/dim 3.4700(3.4723) | Xent 0.0000(0.0000) | Loss 8.0424(8.3679) | Error 0.0000(0.0000) Steps 694(696.01) | Grad Norm 6.3176(5.6585) | Total Time 0.00(0.00)\n",
      "Iter 6530 | Time 19.0452(18.3196) | Bit/dim 3.4842(3.4777) | Xent 0.0000(0.0000) | Loss 8.1353(8.2862) | Error 0.0000(0.0000) Steps 742(696.82) | Grad Norm 2.7334(5.2900) | Total Time 0.00(0.00)\n",
      "Iter 6540 | Time 18.6387(18.3079) | Bit/dim 3.4417(3.4751) | Xent 0.0000(0.0000) | Loss 7.9446(8.2084) | Error 0.0000(0.0000) Steps 700(697.92) | Grad Norm 8.1900(5.4926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 89.4917, Epoch Time 1115.7470(1005.9308), Bit/dim 3.4733(best: 3.4779), Xent 0.0000, Loss 3.4733, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 17.9401(18.2944) | Bit/dim 3.4445(3.4730) | Xent 0.0000(0.0000) | Loss 8.0271(8.7918) | Error 0.0000(0.0000) Steps 700(699.26) | Grad Norm 5.2758(5.3470) | Total Time 0.00(0.00)\n",
      "Iter 6560 | Time 18.5855(18.3317) | Bit/dim 3.4983(3.4728) | Xent 0.0000(0.0000) | Loss 8.1346(8.5822) | Error 0.0000(0.0000) Steps 688(702.39) | Grad Norm 3.9378(5.4196) | Total Time 0.00(0.00)\n",
      "Iter 6570 | Time 18.6489(18.3515) | Bit/dim 3.4953(3.4743) | Xent 0.0000(0.0000) | Loss 8.0648(8.4365) | Error 0.0000(0.0000) Steps 718(702.13) | Grad Norm 7.4540(5.3734) | Total Time 0.00(0.00)\n",
      "Iter 6580 | Time 18.4997(18.3637) | Bit/dim 3.4739(3.4737) | Xent 0.0000(0.0000) | Loss 8.0072(8.3356) | Error 0.0000(0.0000) Steps 718(701.55) | Grad Norm 5.9778(5.2681) | Total Time 0.00(0.00)\n",
      "Iter 6590 | Time 19.2067(18.4348) | Bit/dim 3.4801(3.4751) | Xent 0.0000(0.0000) | Loss 8.1176(8.2629) | Error 0.0000(0.0000) Steps 712(700.69) | Grad Norm 3.3347(5.3432) | Total Time 0.00(0.00)\n",
      "Iter 6600 | Time 18.2366(18.4078) | Bit/dim 3.4730(3.4731) | Xent 0.0000(0.0000) | Loss 8.0289(8.1869) | Error 0.0000(0.0000) Steps 712(700.07) | Grad Norm 6.2149(5.5078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 89.3989, Epoch Time 1120.7721(1009.3760), Bit/dim 3.4818(best: 3.4733), Xent 0.0000, Loss 3.4818, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 18.9255(18.5663) | Bit/dim 3.4803(3.4725) | Xent 0.0000(0.0000) | Loss 8.0535(8.6826) | Error 0.0000(0.0000) Steps 712(706.90) | Grad Norm 5.8371(5.5008) | Total Time 0.00(0.00)\n",
      "Iter 6620 | Time 19.8902(18.5981) | Bit/dim 3.4529(3.4722) | Xent 0.0000(0.0000) | Loss 8.0995(8.5159) | Error 0.0000(0.0000) Steps 730(709.37) | Grad Norm 3.8833(5.6811) | Total Time 0.00(0.00)\n",
      "Iter 6630 | Time 18.8705(18.5817) | Bit/dim 3.4743(3.4727) | Xent 0.0000(0.0000) | Loss 8.1337(8.3922) | Error 0.0000(0.0000) Steps 730(710.68) | Grad Norm 4.4543(5.3903) | Total Time 0.00(0.00)\n",
      "Iter 6640 | Time 18.7427(18.6454) | Bit/dim 3.4725(3.4718) | Xent 0.0000(0.0000) | Loss 7.9855(8.2992) | Error 0.0000(0.0000) Steps 712(711.93) | Grad Norm 5.2209(5.3430) | Total Time 0.00(0.00)\n",
      "Iter 6650 | Time 19.8886(18.6780) | Bit/dim 3.4649(3.4716) | Xent 0.0000(0.0000) | Loss 8.1070(8.2218) | Error 0.0000(0.0000) Steps 730(711.75) | Grad Norm 3.6102(5.1490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 90.9192, Epoch Time 1138.7315(1013.2567), Bit/dim 3.4754(best: 3.4733), Xent 0.0000, Loss 3.4754, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 19.0029(18.6474) | Bit/dim 3.4975(3.4740) | Xent 0.0000(0.0000) | Loss 7.9959(8.8229) | Error 0.0000(0.0000) Steps 688(713.31) | Grad Norm 4.5910(5.1180) | Total Time 0.00(0.00)\n",
      "Iter 6670 | Time 19.3681(18.6489) | Bit/dim 3.4414(3.4714) | Xent 0.0000(0.0000) | Loss 8.0513(8.6180) | Error 0.0000(0.0000) Steps 706(713.05) | Grad Norm 3.4844(5.5264) | Total Time 0.00(0.00)\n",
      "Iter 6680 | Time 19.8143(18.7097) | Bit/dim 3.4669(3.4696) | Xent 0.0000(0.0000) | Loss 8.0135(8.4641) | Error 0.0000(0.0000) Steps 772(717.17) | Grad Norm 6.7296(5.2702) | Total Time 0.00(0.00)\n",
      "Iter 6690 | Time 18.5403(18.6717) | Bit/dim 3.4607(3.4741) | Xent 0.0000(0.0000) | Loss 8.0323(8.3637) | Error 0.0000(0.0000) Steps 694(715.42) | Grad Norm 4.7527(5.2147) | Total Time 0.00(0.00)\n",
      "Iter 6700 | Time 18.7096(18.7370) | Bit/dim 3.4431(3.4734) | Xent 0.0000(0.0000) | Loss 8.0929(8.2913) | Error 0.0000(0.0000) Steps 760(718.41) | Grad Norm 7.2112(5.1094) | Total Time 0.00(0.00)\n",
      "Iter 6710 | Time 18.7579(18.7824) | Bit/dim 3.4933(3.4734) | Xent 0.0000(0.0000) | Loss 8.1783(8.2328) | Error 0.0000(0.0000) Steps 742(719.46) | Grad Norm 4.5184(5.3480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 91.4391, Epoch Time 1142.9049(1017.1462), Bit/dim 3.4717(best: 3.4733), Xent 0.0000, Loss 3.4717, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 18.1546(18.7452) | Bit/dim 3.4389(3.4720) | Xent 0.0000(0.0000) | Loss 7.9557(8.7266) | Error 0.0000(0.0000) Steps 712(718.27) | Grad Norm 7.0623(5.5828) | Total Time 0.00(0.00)\n",
      "Iter 6730 | Time 18.8136(18.8067) | Bit/dim 3.4479(3.4710) | Xent 0.0000(0.0000) | Loss 8.0203(8.5369) | Error 0.0000(0.0000) Steps 706(718.25) | Grad Norm 3.6417(5.3212) | Total Time 0.00(0.00)\n",
      "Iter 6740 | Time 19.3977(18.9726) | Bit/dim 3.4597(3.4716) | Xent 0.0000(0.0000) | Loss 8.1210(8.4179) | Error 0.0000(0.0000) Steps 748(722.85) | Grad Norm 5.6335(5.5091) | Total Time 0.00(0.00)\n",
      "Iter 6750 | Time 20.3495(19.0858) | Bit/dim 3.4621(3.4689) | Xent 0.0000(0.0000) | Loss 8.1234(8.3201) | Error 0.0000(0.0000) Steps 712(725.61) | Grad Norm 4.1855(5.0991) | Total Time 0.00(0.00)\n",
      "Iter 6760 | Time 19.4135(19.0242) | Bit/dim 3.5010(3.4700) | Xent 0.0000(0.0000) | Loss 8.1343(8.2464) | Error 0.0000(0.0000) Steps 772(724.86) | Grad Norm 6.5631(5.4194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 89.9591, Epoch Time 1153.9016(1021.2488), Bit/dim 3.4776(best: 3.4717), Xent 0.0000, Loss 3.4776, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 18.9169(18.8786) | Bit/dim 3.4604(3.4697) | Xent 0.0000(0.0000) | Loss 7.9397(8.8160) | Error 0.0000(0.0000) Steps 694(719.45) | Grad Norm 4.7320(5.3054) | Total Time 0.00(0.00)\n",
      "Iter 6780 | Time 18.8016(18.8341) | Bit/dim 3.4392(3.4666) | Xent 0.0000(0.0000) | Loss 8.0225(8.6049) | Error 0.0000(0.0000) Steps 742(718.76) | Grad Norm 5.1352(5.1910) | Total Time 0.00(0.00)\n",
      "Iter 6790 | Time 18.0966(18.8091) | Bit/dim 3.4326(3.4672) | Xent 0.0000(0.0000) | Loss 7.9252(8.4377) | Error 0.0000(0.0000) Steps 730(720.47) | Grad Norm 5.3073(5.1898) | Total Time 0.00(0.00)\n",
      "Iter 6800 | Time 19.4077(18.8568) | Bit/dim 3.4666(3.4675) | Xent 0.0000(0.0000) | Loss 8.0208(8.3213) | Error 0.0000(0.0000) Steps 700(721.52) | Grad Norm 2.8430(5.4743) | Total Time 0.00(0.00)\n",
      "Iter 6810 | Time 18.4091(18.7781) | Bit/dim 3.4574(3.4667) | Xent 0.0000(0.0000) | Loss 8.0932(8.2378) | Error 0.0000(0.0000) Steps 700(720.41) | Grad Norm 3.8975(5.6727) | Total Time 0.00(0.00)\n",
      "Iter 6820 | Time 17.8391(18.7255) | Bit/dim 3.4738(3.4688) | Xent 0.0000(0.0000) | Loss 8.0437(8.1894) | Error 0.0000(0.0000) Steps 706(722.08) | Grad Norm 2.7804(5.2765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 90.8672, Epoch Time 1137.2211(1024.7280), Bit/dim 3.4640(best: 3.4717), Xent 0.0000, Loss 3.4640, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 18.6109(18.7605) | Bit/dim 3.4568(3.4699) | Xent 0.0000(0.0000) | Loss 7.8753(8.7067) | Error 0.0000(0.0000) Steps 724(726.39) | Grad Norm 5.2610(5.1212) | Total Time 0.00(0.00)\n",
      "Iter 6840 | Time 18.7026(18.8064) | Bit/dim 3.4996(3.4696) | Xent 0.0000(0.0000) | Loss 8.2134(8.5418) | Error 0.0000(0.0000) Steps 730(726.63) | Grad Norm 9.0634(5.5771) | Total Time 0.00(0.00)\n",
      "Iter 6850 | Time 19.2804(18.8138) | Bit/dim 3.4551(3.4675) | Xent 0.0000(0.0000) | Loss 7.9020(8.4001) | Error 0.0000(0.0000) Steps 742(726.98) | Grad Norm 6.9689(5.5469) | Total Time 0.00(0.00)\n",
      "Iter 6860 | Time 18.5462(18.8114) | Bit/dim 3.4355(3.4668) | Xent 0.0000(0.0000) | Loss 7.9529(8.3043) | Error 0.0000(0.0000) Steps 724(726.39) | Grad Norm 6.8322(5.3472) | Total Time 0.00(0.00)\n",
      "Iter 6870 | Time 19.4019(18.8641) | Bit/dim 3.5156(3.4668) | Xent 0.0000(0.0000) | Loss 8.1477(8.2258) | Error 0.0000(0.0000) Steps 742(727.15) | Grad Norm 7.1534(5.5452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 88.8073, Epoch Time 1144.4839(1028.3207), Bit/dim 3.4674(best: 3.4640), Xent 0.0000, Loss 3.4674, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 18.1063(18.7886) | Bit/dim 3.4438(3.4672) | Xent 0.0000(0.0000) | Loss 7.8384(8.7915) | Error 0.0000(0.0000) Steps 706(723.56) | Grad Norm 5.0741(5.6314) | Total Time 0.00(0.00)\n",
      "Iter 6890 | Time 18.9019(18.8630) | Bit/dim 3.4435(3.4636) | Xent 0.0000(0.0000) | Loss 7.9504(8.5800) | Error 0.0000(0.0000) Steps 736(724.94) | Grad Norm 3.6990(5.3436) | Total Time 0.00(0.00)\n",
      "Iter 6900 | Time 18.8431(18.8317) | Bit/dim 3.4362(3.4643) | Xent 0.0000(0.0000) | Loss 7.8410(8.4312) | Error 0.0000(0.0000) Steps 724(722.39) | Grad Norm 3.4940(5.2485) | Total Time 0.00(0.00)\n",
      "Iter 6910 | Time 19.5254(18.8490) | Bit/dim 3.4649(3.4628) | Xent 0.0000(0.0000) | Loss 8.0430(8.3147) | Error 0.0000(0.0000) Steps 766(721.42) | Grad Norm 6.0573(5.2283) | Total Time 0.00(0.00)\n",
      "Iter 6920 | Time 18.4234(18.8667) | Bit/dim 3.4505(3.4655) | Xent 0.0000(0.0000) | Loss 7.9202(8.2322) | Error 0.0000(0.0000) Steps 730(721.29) | Grad Norm 5.9591(5.5222) | Total Time 0.00(0.00)\n",
      "Iter 6930 | Time 18.9553(18.9411) | Bit/dim 3.4888(3.4670) | Xent 0.0000(0.0000) | Loss 8.0157(8.1676) | Error 0.0000(0.0000) Steps 724(719.42) | Grad Norm 7.3016(5.5467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 92.1445, Epoch Time 1151.2635(1032.0089), Bit/dim 3.4667(best: 3.4640), Xent 0.0000, Loss 3.4667, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 20.1481(19.0445) | Bit/dim 3.4885(3.4673) | Xent 0.0000(0.0000) | Loss 8.0650(8.6998) | Error 0.0000(0.0000) Steps 742(722.36) | Grad Norm 3.1114(5.3184) | Total Time 0.00(0.00)\n",
      "Iter 6950 | Time 18.8706(19.1116) | Bit/dim 3.4811(3.4662) | Xent 0.0000(0.0000) | Loss 8.0976(8.5238) | Error 0.0000(0.0000) Steps 736(725.40) | Grad Norm 6.6206(5.3600) | Total Time 0.00(0.00)\n",
      "Iter 6960 | Time 19.2616(19.1808) | Bit/dim 3.4591(3.4646) | Xent 0.0000(0.0000) | Loss 8.0504(8.3914) | Error 0.0000(0.0000) Steps 730(729.94) | Grad Norm 4.7095(5.2261) | Total Time 0.00(0.00)\n",
      "Iter 6970 | Time 19.0030(19.2915) | Bit/dim 3.4752(3.4658) | Xent 0.0000(0.0000) | Loss 8.0729(8.3058) | Error 0.0000(0.0000) Steps 742(734.43) | Grad Norm 4.0425(5.2081) | Total Time 0.00(0.00)\n",
      "Iter 6980 | Time 20.2463(19.4262) | Bit/dim 3.4889(3.4648) | Xent 0.0000(0.0000) | Loss 8.0291(8.2385) | Error 0.0000(0.0000) Steps 760(736.08) | Grad Norm 5.6991(5.4013) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 92.5729, Epoch Time 1180.4633(1036.4626), Bit/dim 3.4732(best: 3.4640), Xent 0.0000, Loss 3.4732, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 20.3578(19.3801) | Bit/dim 3.4381(3.4651) | Xent 0.0000(0.0000) | Loss 7.9447(8.8323) | Error 0.0000(0.0000) Steps 742(737.16) | Grad Norm 4.4586(5.3461) | Total Time 0.00(0.00)\n",
      "Iter 7000 | Time 19.0085(19.4256) | Bit/dim 3.4687(3.4659) | Xent 0.0000(0.0000) | Loss 8.0965(8.6363) | Error 0.0000(0.0000) Steps 742(736.78) | Grad Norm 3.4170(5.1301) | Total Time 0.00(0.00)\n",
      "Iter 7010 | Time 18.9784(19.4614) | Bit/dim 3.5245(3.4659) | Xent 0.0000(0.0000) | Loss 8.1464(8.4856) | Error 0.0000(0.0000) Steps 730(738.47) | Grad Norm 3.5704(5.0455) | Total Time 0.00(0.00)\n",
      "Iter 7020 | Time 20.2454(19.5537) | Bit/dim 3.4597(3.4652) | Xent 0.0000(0.0000) | Loss 8.1240(8.3747) | Error 0.0000(0.0000) Steps 772(743.70) | Grad Norm 6.1830(5.0810) | Total Time 0.00(0.00)\n",
      "Iter 7030 | Time 19.8484(19.5642) | Bit/dim 3.4517(3.4643) | Xent 0.0000(0.0000) | Loss 7.9383(8.2875) | Error 0.0000(0.0000) Steps 718(742.66) | Grad Norm 6.9294(5.4400) | Total Time 0.00(0.00)\n",
      "Iter 7040 | Time 20.3655(19.6762) | Bit/dim 3.4659(3.4653) | Xent 0.0000(0.0000) | Loss 8.1301(8.2270) | Error 0.0000(0.0000) Steps 766(743.77) | Grad Norm 3.3111(5.1621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 92.7158, Epoch Time 1193.4751(1041.1729), Bit/dim 3.4615(best: 3.4640), Xent 0.0000, Loss 3.4615, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 18.7217(19.6499) | Bit/dim 3.4829(3.4675) | Xent 0.0000(0.0000) | Loss 8.0946(8.7640) | Error 0.0000(0.0000) Steps 754(743.96) | Grad Norm 5.3700(5.1097) | Total Time 0.00(0.00)\n",
      "Iter 7060 | Time 19.6498(19.6837) | Bit/dim 3.4732(3.4677) | Xent 0.0000(0.0000) | Loss 8.1132(8.5846) | Error 0.0000(0.0000) Steps 760(747.68) | Grad Norm 4.3062(5.1349) | Total Time 0.00(0.00)\n",
      "Iter 7070 | Time 19.5471(19.6902) | Bit/dim 3.4671(3.4645) | Xent 0.0000(0.0000) | Loss 8.0381(8.4363) | Error 0.0000(0.0000) Steps 748(747.04) | Grad Norm 5.7165(5.1334) | Total Time 0.00(0.00)\n",
      "Iter 7080 | Time 20.2916(19.7347) | Bit/dim 3.4734(3.4628) | Xent 0.0000(0.0000) | Loss 8.0858(8.3329) | Error 0.0000(0.0000) Steps 754(747.75) | Grad Norm 7.0246(5.0623) | Total Time 0.00(0.00)\n",
      "Iter 7090 | Time 19.6645(19.7449) | Bit/dim 3.4501(3.4601) | Xent 0.0000(0.0000) | Loss 7.9824(8.2588) | Error 0.0000(0.0000) Steps 772(747.35) | Grad Norm 8.1579(5.1520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 92.1512, Epoch Time 1196.7137(1045.8392), Bit/dim 3.4673(best: 3.4615), Xent 0.0000, Loss 3.4673, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 21.2211(19.7830) | Bit/dim 3.4605(3.4588) | Xent 0.0000(0.0000) | Loss 8.1784(8.8290) | Error 0.0000(0.0000) Steps 796(747.50) | Grad Norm 4.8247(5.2535) | Total Time 0.00(0.00)\n",
      "Iter 7110 | Time 19.7940(19.8365) | Bit/dim 3.4425(3.4581) | Xent 0.0000(0.0000) | Loss 7.9813(8.6184) | Error 0.0000(0.0000) Steps 766(748.68) | Grad Norm 3.9136(5.3809) | Total Time 0.00(0.00)\n",
      "Iter 7120 | Time 18.8297(19.8738) | Bit/dim 3.4856(3.4625) | Xent 0.0000(0.0000) | Loss 8.0656(8.4717) | Error 0.0000(0.0000) Steps 730(750.38) | Grad Norm 4.0642(5.1908) | Total Time 0.00(0.00)\n",
      "Iter 7130 | Time 20.0346(19.8636) | Bit/dim 3.5171(3.4635) | Xent 0.0000(0.0000) | Loss 8.0008(8.3601) | Error 0.0000(0.0000) Steps 748(752.42) | Grad Norm 3.9853(5.1412) | Total Time 0.00(0.00)\n",
      "Iter 7140 | Time 20.1973(19.8361) | Bit/dim 3.4430(3.4603) | Xent 0.0000(0.0000) | Loss 8.0689(8.2847) | Error 0.0000(0.0000) Steps 754(751.47) | Grad Norm 3.7623(5.2385) | Total Time 0.00(0.00)\n",
      "Iter 7150 | Time 19.7202(19.8702) | Bit/dim 3.4742(3.4655) | Xent 0.0000(0.0000) | Loss 8.1647(8.2228) | Error 0.0000(0.0000) Steps 754(752.44) | Grad Norm 3.3630(5.0667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 91.4434, Epoch Time 1205.5149(1050.6294), Bit/dim 3.4674(best: 3.4615), Xent 0.0000, Loss 3.4674, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 19.4780(19.8375) | Bit/dim 3.4583(3.4632) | Xent 0.0000(0.0000) | Loss 7.8890(8.7336) | Error 0.0000(0.0000) Steps 724(749.94) | Grad Norm 7.5598(4.9364) | Total Time 0.00(0.00)\n",
      "Iter 7170 | Time 19.3145(19.8326) | Bit/dim 3.4572(3.4625) | Xent 0.0000(0.0000) | Loss 8.0944(8.5425) | Error 0.0000(0.0000) Steps 754(749.56) | Grad Norm 3.6984(4.8694) | Total Time 0.00(0.00)\n",
      "Iter 7180 | Time 19.8145(19.9420) | Bit/dim 3.4622(3.4606) | Xent 0.0000(0.0000) | Loss 7.9737(8.3929) | Error 0.0000(0.0000) Steps 754(747.57) | Grad Norm 3.5224(4.7955) | Total Time 0.00(0.00)\n",
      "Iter 7190 | Time 20.0234(20.0206) | Bit/dim 3.4648(3.4636) | Xent 0.0000(0.0000) | Loss 8.0227(8.3060) | Error 0.0000(0.0000) Steps 754(751.86) | Grad Norm 7.7316(5.0992) | Total Time 0.00(0.00)\n",
      "Iter 7200 | Time 19.9342(19.9889) | Bit/dim 3.4289(3.4626) | Xent 0.0000(0.0000) | Loss 7.9563(8.2313) | Error 0.0000(0.0000) Steps 754(750.17) | Grad Norm 6.1143(5.0597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 92.0201, Epoch Time 1213.0661(1055.5025), Bit/dim 3.4560(best: 3.4615), Xent 0.0000, Loss 3.4560, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 20.2224(20.1040) | Bit/dim 3.4745(3.4630) | Xent 0.0000(0.0000) | Loss 8.0874(8.8323) | Error 0.0000(0.0000) Steps 754(752.78) | Grad Norm 6.5450(5.2792) | Total Time 0.00(0.00)\n",
      "Iter 7220 | Time 20.1036(20.0832) | Bit/dim 3.4756(3.4608) | Xent 0.0000(0.0000) | Loss 8.0619(8.6184) | Error 0.0000(0.0000) Steps 736(751.16) | Grad Norm 3.2705(5.0377) | Total Time 0.00(0.00)\n",
      "Iter 7230 | Time 19.7884(19.9750) | Bit/dim 3.4669(3.4615) | Xent 0.0000(0.0000) | Loss 7.9520(8.4483) | Error 0.0000(0.0000) Steps 742(748.03) | Grad Norm 7.0329(5.3543) | Total Time 0.00(0.00)\n",
      "Iter 7240 | Time 19.6826(19.9645) | Bit/dim 3.4444(3.4611) | Xent 0.0000(0.0000) | Loss 7.8645(8.3390) | Error 0.0000(0.0000) Steps 742(745.83) | Grad Norm 3.9768(5.2683) | Total Time 0.00(0.00)\n",
      "Iter 7250 | Time 19.4862(19.9480) | Bit/dim 3.4478(3.4591) | Xent 0.0000(0.0000) | Loss 7.8965(8.2567) | Error 0.0000(0.0000) Steps 718(747.59) | Grad Norm 7.3102(5.2259) | Total Time 0.00(0.00)\n",
      "Iter 7260 | Time 20.1999(20.0720) | Bit/dim 3.4524(3.4591) | Xent 0.0000(0.0000) | Loss 7.9018(8.2007) | Error 0.0000(0.0000) Steps 748(748.55) | Grad Norm 6.6050(5.2098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 92.3343, Epoch Time 1211.9622(1060.1963), Bit/dim 3.4683(best: 3.4560), Xent 0.0000, Loss 3.4683, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 20.1944(20.1550) | Bit/dim 3.4751(3.4610) | Xent 0.0000(0.0000) | Loss 8.0259(8.7031) | Error 0.0000(0.0000) Steps 736(747.63) | Grad Norm 4.6153(5.3742) | Total Time 0.00(0.00)\n",
      "Iter 7280 | Time 19.9795(20.1008) | Bit/dim 3.4100(3.4595) | Xent 0.0000(0.0000) | Loss 7.9476(8.5221) | Error 0.0000(0.0000) Steps 754(750.24) | Grad Norm 5.7334(5.2043) | Total Time 0.00(0.00)\n",
      "Iter 7290 | Time 20.3187(20.1703) | Bit/dim 3.4903(3.4604) | Xent 0.0000(0.0000) | Loss 8.0245(8.3953) | Error 0.0000(0.0000) Steps 772(751.21) | Grad Norm 5.5514(5.0766) | Total Time 0.00(0.00)\n",
      "Iter 7300 | Time 19.9726(20.2069) | Bit/dim 3.4647(3.4593) | Xent 0.0000(0.0000) | Loss 8.0062(8.2890) | Error 0.0000(0.0000) Steps 766(755.12) | Grad Norm 5.6688(5.1237) | Total Time 0.00(0.00)\n",
      "Iter 7310 | Time 20.3653(20.2197) | Bit/dim 3.4342(3.4574) | Xent 0.0000(0.0000) | Loss 7.9667(8.2107) | Error 0.0000(0.0000) Steps 754(755.02) | Grad Norm 4.5907(4.9355) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 92.7047, Epoch Time 1224.7860(1065.1340), Bit/dim 3.4664(best: 3.4560), Xent 0.0000, Loss 3.4664, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 20.1975(20.2481) | Bit/dim 3.4708(3.4571) | Xent 0.0000(0.0000) | Loss 8.1128(8.8129) | Error 0.0000(0.0000) Steps 736(755.81) | Grad Norm 7.7825(5.3005) | Total Time 0.00(0.00)\n",
      "Iter 7330 | Time 20.3277(20.2570) | Bit/dim 3.4603(3.4590) | Xent 0.0000(0.0000) | Loss 8.0470(8.6069) | Error 0.0000(0.0000) Steps 772(753.49) | Grad Norm 4.3717(5.1747) | Total Time 0.00(0.00)\n",
      "Iter 7340 | Time 19.3161(20.2466) | Bit/dim 3.4728(3.4591) | Xent 0.0000(0.0000) | Loss 8.0146(8.4509) | Error 0.0000(0.0000) Steps 742(754.07) | Grad Norm 3.0042(4.9312) | Total Time 0.00(0.00)\n",
      "Iter 7350 | Time 19.4699(20.2366) | Bit/dim 3.4408(3.4583) | Xent 0.0000(0.0000) | Loss 7.8184(8.3278) | Error 0.0000(0.0000) Steps 736(751.83) | Grad Norm 6.0913(5.1842) | Total Time 0.00(0.00)\n",
      "Iter 7360 | Time 20.2720(20.2436) | Bit/dim 3.4478(3.4566) | Xent 0.0000(0.0000) | Loss 8.1578(8.2563) | Error 0.0000(0.0000) Steps 766(754.88) | Grad Norm 3.4486(5.1749) | Total Time 0.00(0.00)\n",
      "Iter 7370 | Time 19.7391(20.2259) | Bit/dim 3.4638(3.4589) | Xent 0.0000(0.0000) | Loss 8.0546(8.1917) | Error 0.0000(0.0000) Steps 772(753.33) | Grad Norm 4.4357(5.2720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 91.3285, Epoch Time 1223.5093(1069.8853), Bit/dim 3.4575(best: 3.4560), Xent 0.0000, Loss 3.4575, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 20.8599(20.1775) | Bit/dim 3.4808(3.4595) | Xent 0.0000(0.0000) | Loss 7.9585(8.7023) | Error 0.0000(0.0000) Steps 772(751.91) | Grad Norm 4.2119(5.0679) | Total Time 0.00(0.00)\n",
      "Iter 7390 | Time 20.2019(20.2075) | Bit/dim 3.4390(3.4545) | Xent 0.0000(0.0000) | Loss 7.9169(8.5032) | Error 0.0000(0.0000) Steps 754(754.21) | Grad Norm 4.2353(4.9138) | Total Time 0.00(0.00)\n",
      "Iter 7400 | Time 20.3536(20.3258) | Bit/dim 3.4695(3.4564) | Xent 0.0000(0.0000) | Loss 7.9681(8.3804) | Error 0.0000(0.0000) Steps 730(755.94) | Grad Norm 4.9011(5.1041) | Total Time 0.00(0.00)\n",
      "Iter 7410 | Time 20.6263(20.3684) | Bit/dim 3.5049(3.4597) | Xent 0.0000(0.0000) | Loss 8.1165(8.2982) | Error 0.0000(0.0000) Steps 760(756.09) | Grad Norm 4.1568(4.9677) | Total Time 0.00(0.00)\n",
      "Iter 7420 | Time 20.4176(20.3944) | Bit/dim 3.4496(3.4570) | Xent 0.0000(0.0000) | Loss 7.9990(8.2294) | Error 0.0000(0.0000) Steps 748(755.24) | Grad Norm 4.6773(5.3079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 94.5130, Epoch Time 1231.9757(1074.7480), Bit/dim 3.4597(best: 3.4560), Xent 0.0000, Loss 3.4597, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.4523(20.3438) | Bit/dim 3.4447(3.4561) | Xent 0.0000(0.0000) | Loss 8.0415(8.8473) | Error 0.0000(0.0000) Steps 760(754.50) | Grad Norm 3.1207(5.1643) | Total Time 0.00(0.00)\n",
      "Iter 7440 | Time 19.7986(20.3547) | Bit/dim 3.4668(3.4580) | Xent 0.0000(0.0000) | Loss 7.9882(8.6363) | Error 0.0000(0.0000) Steps 760(756.33) | Grad Norm 6.2870(5.2254) | Total Time 0.00(0.00)\n",
      "Iter 7450 | Time 21.3182(20.3274) | Bit/dim 3.4554(3.4572) | Xent 0.0000(0.0000) | Loss 8.1551(8.4649) | Error 0.0000(0.0000) Steps 808(758.06) | Grad Norm 6.2777(5.4232) | Total Time 0.00(0.00)\n",
      "Iter 7460 | Time 19.8503(20.4090) | Bit/dim 3.4452(3.4565) | Xent 0.0000(0.0000) | Loss 8.0368(8.3470) | Error 0.0000(0.0000) Steps 766(757.34) | Grad Norm 5.1711(5.4095) | Total Time 0.00(0.00)\n",
      "Iter 7470 | Time 20.2136(20.4222) | Bit/dim 3.4701(3.4566) | Xent 0.0000(0.0000) | Loss 8.0414(8.2611) | Error 0.0000(0.0000) Steps 742(756.48) | Grad Norm 5.9945(5.3646) | Total Time 0.00(0.00)\n",
      "Iter 7480 | Time 19.9547(20.3052) | Bit/dim 3.4474(3.4559) | Xent 0.0000(0.0000) | Loss 8.0534(8.1930) | Error 0.0000(0.0000) Steps 742(754.64) | Grad Norm 4.1377(5.4950) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 93.2421, Epoch Time 1231.1321(1079.4395), Bit/dim 3.4568(best: 3.4560), Xent 0.0000, Loss 3.4568, Error 1.0000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 20.1831(20.2455) | Bit/dim 3.4395(3.4551) | Xent 0.0000(0.0000) | Loss 7.9753(8.7126) | Error 0.0000(0.0000) Steps 748(755.64) | Grad Norm 4.6469(5.0513) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_cifar10_bs900_rl_stdscale_15_annealing_run3 --seed 3 --lr 0.001 --conditional False --controlled_tol False --log_freq 10 --scale_fac 1.0 --scale_std 15.0 --annealing_std True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
