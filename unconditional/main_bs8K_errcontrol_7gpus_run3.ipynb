{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=False, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_published_bs8K_errcontrol_7gpus_3/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_published_bs8K_errcontrol_7gpus_3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2801 | Time 114.2703(85.8731) | Bit/dim 2.2448(1.7303) | Xent 0.0000(0.0000) | Loss 2.2448(1.7303) | Error 0.0000(0.0000) Steps 668(733.84) | Grad Norm 3.6450(772.1310) | Total Time 10.00(10.00)\n",
      "Iter 2802 | Time 77.2227(85.6136) | Bit/dim 2.2421(1.7457) | Xent 0.0000(0.0000) | Loss 2.2421(1.7457) | Error 0.0000(0.0000) Steps 662(731.69) | Grad Norm 3.6186(749.0756) | Total Time 10.00(10.00)\n",
      "Iter 2803 | Time 77.9208(85.3828) | Bit/dim 2.2414(1.7606) | Xent 0.0000(0.0000) | Loss 2.2414(1.7606) | Error 0.0000(0.0000) Steps 650(729.24) | Grad Norm 3.4442(726.7067) | Total Time 10.00(10.00)\n",
      "Iter 2804 | Time 76.6293(85.1202) | Bit/dim 2.2333(1.7747) | Xent 0.0000(0.0000) | Loss 2.2333(1.7747) | Error 0.0000(0.0000) Steps 650(726.86) | Grad Norm 3.2012(705.0015) | Total Time 10.00(10.00)\n",
      "Iter 2805 | Time 75.5553(84.8332) | Bit/dim 2.2297(1.7884) | Xent 0.0000(0.0000) | Loss 2.2297(1.7884) | Error 0.0000(0.0000) Steps 656(724.74) | Grad Norm 2.9302(683.9394) | Total Time 10.00(10.00)\n",
      "Iter 2806 | Time 75.1586(84.5430) | Bit/dim 2.2188(1.8013) | Xent 0.0000(0.0000) | Loss 2.2188(1.8013) | Error 0.0000(0.0000) Steps 656(722.67) | Grad Norm 2.6988(663.5022) | Total Time 10.00(10.00)\n",
      "Iter 2807 | Time 74.5414(84.2429) | Bit/dim 2.2155(1.8137) | Xent 0.0000(0.0000) | Loss 2.2155(1.8137) | Error 0.0000(0.0000) Steps 656(720.67) | Grad Norm 2.4793(643.6715) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 29.8461, Epoch Time 614.1909(623.5488), Bit/dim 2.2069(best: inf), Xent 0.0000, Loss 2.2069, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2808 | Time 76.9398(84.0238) | Bit/dim 2.2099(1.8256) | Xent 0.0000(0.0000) | Loss 2.2099(1.8256) | Error 0.0000(0.0000) Steps 656(718.73) | Grad Norm 2.3421(624.4316) | Total Time 10.00(10.00)\n",
      "Iter 2809 | Time 75.2075(83.7593) | Bit/dim 2.2083(1.8371) | Xent 0.0000(0.0000) | Loss 2.2083(1.8371) | Error 0.0000(0.0000) Steps 656(716.85) | Grad Norm 2.2142(605.7651) | Total Time 10.00(10.00)\n",
      "Iter 2810 | Time 75.4291(83.5094) | Bit/dim 2.2064(1.8482) | Xent 0.0000(0.0000) | Loss 2.2064(1.8482) | Error 0.0000(0.0000) Steps 656(715.03) | Grad Norm 2.1097(587.6554) | Total Time 10.00(10.00)\n",
      "Iter 2811 | Time 73.9930(83.2239) | Bit/dim 2.1985(1.8587) | Xent 0.0000(0.0000) | Loss 2.1985(1.8587) | Error 0.0000(0.0000) Steps 662(713.44) | Grad Norm 2.0466(570.0871) | Total Time 10.00(10.00)\n",
      "Iter 2812 | Time 74.3221(82.9569) | Bit/dim 2.1910(1.8686) | Xent 0.0000(0.0000) | Loss 2.1910(1.8686) | Error 0.0000(0.0000) Steps 656(711.71) | Grad Norm 2.0198(553.0451) | Total Time 10.00(10.00)\n",
      "Iter 2813 | Time 75.4528(82.7318) | Bit/dim 2.1919(1.8783) | Xent 0.0000(0.0000) | Loss 2.1919(1.8783) | Error 0.0000(0.0000) Steps 656(710.04) | Grad Norm 1.9595(536.5126) | Total Time 10.00(10.00)\n",
      "Iter 2814 | Time 75.7769(82.5231) | Bit/dim 2.1902(1.8877) | Xent 0.0000(0.0000) | Loss 2.1902(1.8877) | Error 0.0000(0.0000) Steps 656(708.42) | Grad Norm 1.9610(520.4760) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 20.8150, Epoch Time 560.6434(621.6616), Bit/dim 2.1809(best: 2.2069), Xent 0.0000, Loss 2.1809, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2815 | Time 73.1818(82.2429) | Bit/dim 2.1884(1.8967) | Xent 0.0000(0.0000) | Loss 2.1884(1.8967) | Error 0.0000(0.0000) Steps 656(706.85) | Grad Norm 1.9252(504.9195) | Total Time 10.00(10.00)\n",
      "Iter 2816 | Time 74.4915(82.0103) | Bit/dim 2.1857(1.9054) | Xent 0.0000(0.0000) | Loss 2.1857(1.9054) | Error 0.0000(0.0000) Steps 656(705.32) | Grad Norm 1.9343(489.8299) | Total Time 10.00(10.00)\n",
      "Iter 2817 | Time 79.7807(81.9435) | Bit/dim 2.1826(1.9137) | Xent 0.0000(0.0000) | Loss 2.1826(1.9137) | Error 0.0000(0.0000) Steps 656(703.84) | Grad Norm 1.9200(475.1926) | Total Time 10.00(10.00)\n",
      "Iter 2818 | Time 74.6322(81.7241) | Bit/dim 2.1781(1.9216) | Xent 0.0000(0.0000) | Loss 2.1781(1.9216) | Error 0.0000(0.0000) Steps 662(702.59) | Grad Norm 1.9202(460.9945) | Total Time 10.00(10.00)\n",
      "Iter 2819 | Time 76.6701(81.5725) | Bit/dim 2.1718(1.9291) | Xent 0.0000(0.0000) | Loss 2.1718(1.9291) | Error 0.0000(0.0000) Steps 656(701.19) | Grad Norm 1.8560(447.2203) | Total Time 10.00(10.00)\n",
      "Iter 2820 | Time 74.4380(81.3585) | Bit/dim 2.1721(1.9364) | Xent 0.0000(0.0000) | Loss 2.1721(1.9364) | Error 0.0000(0.0000) Steps 656(699.83) | Grad Norm 1.7934(433.8575) | Total Time 10.00(10.00)\n",
      "Iter 2821 | Time 73.4103(81.1200) | Bit/dim 2.1642(1.9433) | Xent 0.0000(0.0000) | Loss 2.1642(1.9433) | Error 0.0000(0.0000) Steps 650(698.34) | Grad Norm 1.7338(420.8938) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 20.7709, Epoch Time 560.0917(619.8145), Bit/dim 2.1599(best: 2.1809), Xent 0.0000, Loss 2.1599, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2822 | Time 73.7028(80.8975) | Bit/dim 2.1657(1.9499) | Xent 0.0000(0.0000) | Loss 2.1657(1.9499) | Error 0.0000(0.0000) Steps 650(696.89) | Grad Norm 1.6714(408.3171) | Total Time 10.00(10.00)\n",
      "Iter 2823 | Time 74.1855(80.6961) | Bit/dim 2.1603(1.9563) | Xent 0.0000(0.0000) | Loss 2.1603(1.9563) | Error 0.0000(0.0000) Steps 656(695.66) | Grad Norm 1.6237(396.1163) | Total Time 10.00(10.00)\n",
      "Iter 2824 | Time 75.0936(80.5281) | Bit/dim 2.1601(1.9624) | Xent 0.0000(0.0000) | Loss 2.1601(1.9624) | Error 0.0000(0.0000) Steps 656(694.47) | Grad Norm 1.6022(384.2809) | Total Time 10.00(10.00)\n",
      "Iter 2825 | Time 74.6042(80.3504) | Bit/dim 2.1559(1.9682) | Xent 0.0000(0.0000) | Loss 2.1559(1.9682) | Error 0.0000(0.0000) Steps 656(693.32) | Grad Norm 1.5651(372.7994) | Total Time 10.00(10.00)\n",
      "Iter 2826 | Time 77.4440(80.2632) | Bit/dim 2.1599(1.9739) | Xent 0.0000(0.0000) | Loss 2.1599(1.9739) | Error 0.0000(0.0000) Steps 650(692.02) | Grad Norm 1.5736(361.6626) | Total Time 10.00(10.00)\n",
      "Iter 2827 | Time 76.0514(80.1368) | Bit/dim 2.1515(1.9793) | Xent 0.0000(0.0000) | Loss 2.1515(1.9793) | Error 0.0000(0.0000) Steps 656(690.94) | Grad Norm 1.5200(350.8584) | Total Time 10.00(10.00)\n",
      "Iter 2828 | Time 73.5354(79.9388) | Bit/dim 2.1467(1.9843) | Xent 0.0000(0.0000) | Loss 2.1467(1.9843) | Error 0.0000(0.0000) Steps 650(689.71) | Grad Norm 1.5191(340.3782) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 20.5908, Epoch Time 558.0497(617.9616), Bit/dim 2.1406(best: 2.1599), Xent 0.0000, Loss 2.1406, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2829 | Time 73.7290(79.7525) | Bit/dim 2.1442(1.9891) | Xent 0.0000(0.0000) | Loss 2.1442(1.9891) | Error 0.0000(0.0000) Steps 650(688.52) | Grad Norm 1.4729(330.2110) | Total Time 10.00(10.00)\n",
      "Iter 2830 | Time 75.7575(79.6326) | Bit/dim 2.1393(1.9936) | Xent 0.0000(0.0000) | Loss 2.1393(1.9936) | Error 0.0000(0.0000) Steps 650(687.36) | Grad Norm 1.4357(320.3478) | Total Time 10.00(10.00)\n",
      "Iter 2831 | Time 78.9825(79.6131) | Bit/dim 2.1396(1.9980) | Xent 0.0000(0.0000) | Loss 2.1396(1.9980) | Error 0.0000(0.0000) Steps 650(686.24) | Grad Norm 1.4175(310.7799) | Total Time 10.00(10.00)\n",
      "Iter 2832 | Time 76.7691(79.5278) | Bit/dim 2.1367(2.0021) | Xent 0.0000(0.0000) | Loss 2.1367(2.0021) | Error 0.0000(0.0000) Steps 650(685.15) | Grad Norm 1.3818(301.4979) | Total Time 10.00(10.00)\n",
      "Iter 2833 | Time 78.2297(79.4889) | Bit/dim 2.1430(2.0063) | Xent 0.0000(0.0000) | Loss 2.1430(2.0063) | Error 0.0000(0.0000) Steps 650(684.10) | Grad Norm 1.3832(292.4945) | Total Time 10.00(10.00)\n",
      "Iter 2834 | Time 77.0458(79.4156) | Bit/dim 2.1346(2.0102) | Xent 0.0000(0.0000) | Loss 2.1346(2.0102) | Error 0.0000(0.0000) Steps 662(683.44) | Grad Norm 1.3920(283.7614) | Total Time 10.00(10.00)\n",
      "Iter 2835 | Time 75.6682(79.3031) | Bit/dim 2.1360(2.0140) | Xent 0.0000(0.0000) | Loss 2.1360(2.0140) | Error 0.0000(0.0000) Steps 662(682.79) | Grad Norm 1.3779(275.2899) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 20.7690, Epoch Time 569.8846(616.5193), Bit/dim 2.1235(best: 2.1406), Xent 0.0000, Loss 2.1235, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2836 | Time 77.4196(79.2466) | Bit/dim 2.1272(2.0174) | Xent 0.0000(0.0000) | Loss 2.1272(2.0174) | Error 0.0000(0.0000) Steps 662(682.17) | Grad Norm 1.3636(267.0721) | Total Time 10.00(10.00)\n",
      "Iter 2837 | Time 77.1641(79.1842) | Bit/dim 2.1289(2.0207) | Xent 0.0000(0.0000) | Loss 2.1289(2.0207) | Error 0.0000(0.0000) Steps 662(681.56) | Grad Norm 1.3531(259.1005) | Total Time 10.00(10.00)\n",
      "Iter 2838 | Time 77.1295(79.1225) | Bit/dim 2.1260(2.0239) | Xent 0.0000(0.0000) | Loss 2.1260(2.0239) | Error 0.0000(0.0000) Steps 662(680.98) | Grad Norm 1.3428(251.3678) | Total Time 10.00(10.00)\n",
      "Iter 2839 | Time 77.9316(79.0868) | Bit/dim 2.1197(2.0267) | Xent 0.0000(0.0000) | Loss 2.1197(2.0267) | Error 0.0000(0.0000) Steps 662(680.41) | Grad Norm 1.3455(243.8671) | Total Time 10.00(10.00)\n",
      "Iter 2840 | Time 78.0431(79.0555) | Bit/dim 2.1188(2.0295) | Xent 0.0000(0.0000) | Loss 2.1188(2.0295) | Error 0.0000(0.0000) Steps 662(679.86) | Grad Norm 1.3556(236.5918) | Total Time 10.00(10.00)\n",
      "Iter 2841 | Time 79.3881(79.0655) | Bit/dim 2.1176(2.0322) | Xent 0.0000(0.0000) | Loss 2.1176(2.0322) | Error 0.0000(0.0000) Steps 662(679.32) | Grad Norm 1.3688(229.5351) | Total Time 10.00(10.00)\n",
      "Iter 2842 | Time 75.7895(78.9672) | Bit/dim 2.1198(2.0348) | Xent 0.0000(0.0000) | Loss 2.1198(2.0348) | Error 0.0000(0.0000) Steps 668(678.98) | Grad Norm 1.3695(222.6901) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 20.8950, Epoch Time 576.8554(615.3294), Bit/dim 2.1082(best: 2.1235), Xent 0.0000, Loss 2.1082, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2843 | Time 77.6552(78.9278) | Bit/dim 2.1094(2.0370) | Xent 0.0000(0.0000) | Loss 2.1094(2.0370) | Error 0.0000(0.0000) Steps 662(678.47) | Grad Norm 1.3717(216.0506) | Total Time 10.00(10.00)\n",
      "Iter 2844 | Time 80.4800(78.9744) | Bit/dim 2.1108(2.0392) | Xent 0.0000(0.0000) | Loss 2.1108(2.0392) | Error 0.0000(0.0000) Steps 662(677.98) | Grad Norm 1.3473(209.6095) | Total Time 10.00(10.00)\n",
      "Iter 2845 | Time 77.7204(78.9368) | Bit/dim 2.1055(2.0412) | Xent 0.0000(0.0000) | Loss 2.1055(2.0412) | Error 0.0000(0.0000) Steps 662(677.50) | Grad Norm 1.3515(203.3617) | Total Time 10.00(10.00)\n",
      "Iter 2846 | Time 77.7589(78.9014) | Bit/dim 2.1048(2.0431) | Xent 0.0000(0.0000) | Loss 2.1048(2.0431) | Error 0.0000(0.0000) Steps 668(677.21) | Grad Norm 1.3407(197.3011) | Total Time 10.00(10.00)\n",
      "Iter 2847 | Time 77.3610(78.8552) | Bit/dim 2.1051(2.0450) | Xent 0.0000(0.0000) | Loss 2.1051(2.0450) | Error 0.0000(0.0000) Steps 662(676.76) | Grad Norm 1.3268(191.4219) | Total Time 10.00(10.00)\n",
      "Iter 2848 | Time 77.6363(78.8187) | Bit/dim 2.1056(2.0468) | Xent 0.0000(0.0000) | Loss 2.1056(2.0468) | Error 0.0000(0.0000) Steps 662(676.31) | Grad Norm 1.3077(185.7185) | Total Time 10.00(10.00)\n",
      "Iter 2849 | Time 77.7582(78.7868) | Bit/dim 2.1143(2.0488) | Xent 0.0000(0.0000) | Loss 2.1143(2.0488) | Error 0.0000(0.0000) Steps 662(675.88) | Grad Norm 1.3022(180.1860) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 20.7863, Epoch Time 579.7977(614.2634), Bit/dim 2.0952(best: 2.1082), Xent 0.0000, Loss 2.0952, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2850 | Time 78.5483(78.7797) | Bit/dim 2.0936(2.0502) | Xent 0.0000(0.0000) | Loss 2.0936(2.0502) | Error 0.0000(0.0000) Steps 662(675.47) | Grad Norm 1.2877(174.8190) | Total Time 10.00(10.00)\n",
      "Iter 2851 | Time 77.2658(78.7343) | Bit/dim 2.1043(2.0518) | Xent 0.0000(0.0000) | Loss 2.1043(2.0518) | Error 0.0000(0.0000) Steps 662(675.06) | Grad Norm 1.2695(169.6125) | Total Time 10.00(10.00)\n",
      "Iter 2852 | Time 76.8209(78.6769) | Bit/dim 2.0945(2.0531) | Xent 0.0000(0.0000) | Loss 2.0945(2.0531) | Error 0.0000(0.0000) Steps 662(674.67) | Grad Norm 1.2790(164.5625) | Total Time 10.00(10.00)\n",
      "Iter 2853 | Time 78.9013(78.6836) | Bit/dim 2.0956(2.0543) | Xent 0.0000(0.0000) | Loss 2.0956(2.0543) | Error 0.0000(0.0000) Steps 662(674.29) | Grad Norm 1.2735(159.6639) | Total Time 10.00(10.00)\n",
      "Iter 2854 | Time 77.2898(78.6418) | Bit/dim 2.0917(2.0555) | Xent 0.0000(0.0000) | Loss 2.0917(2.0555) | Error 0.0000(0.0000) Steps 662(673.92) | Grad Norm 1.2763(154.9122) | Total Time 10.00(10.00)\n",
      "Iter 2855 | Time 78.1018(78.6256) | Bit/dim 2.0916(2.0566) | Xent 0.0000(0.0000) | Loss 2.0916(2.0566) | Error 0.0000(0.0000) Steps 662(673.57) | Grad Norm 1.2733(150.3031) | Total Time 10.00(10.00)\n",
      "Iter 2856 | Time 75.3984(78.5288) | Bit/dim 2.0889(2.0575) | Xent 0.0000(0.0000) | Loss 2.0889(2.0575) | Error 0.0000(0.0000) Steps 662(673.22) | Grad Norm 1.2581(145.8317) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 20.6971, Epoch Time 575.5850(613.1031), Bit/dim 2.0816(best: 2.0952), Xent 0.0000, Loss 2.0816, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2857 | Time 76.2813(78.4613) | Bit/dim 2.0933(2.0586) | Xent 0.0000(0.0000) | Loss 2.0933(2.0586) | Error 0.0000(0.0000) Steps 662(672.88) | Grad Norm 1.2339(141.4938) | Total Time 10.00(10.00)\n",
      "Iter 2858 | Time 76.1912(78.3932) | Bit/dim 2.0848(2.0594) | Xent 0.0000(0.0000) | Loss 2.0848(2.0594) | Error 0.0000(0.0000) Steps 662(672.56) | Grad Norm 1.2397(137.2862) | Total Time 10.00(10.00)\n",
      "Iter 2859 | Time 77.1941(78.3573) | Bit/dim 2.0825(2.0601) | Xent 0.0000(0.0000) | Loss 2.0825(2.0601) | Error 0.0000(0.0000) Steps 668(672.42) | Grad Norm 1.2302(133.2045) | Total Time 10.00(10.00)\n",
      "Iter 2860 | Time 77.6101(78.3349) | Bit/dim 2.0812(2.0607) | Xent 0.0000(0.0000) | Loss 2.0812(2.0607) | Error 0.0000(0.0000) Steps 668(672.29) | Grad Norm 1.2218(129.2450) | Total Time 10.00(10.00)\n",
      "Iter 2861 | Time 77.9783(78.3242) | Bit/dim 2.0765(2.0612) | Xent 0.0000(0.0000) | Loss 2.0765(2.0612) | Error 0.0000(0.0000) Steps 662(671.98) | Grad Norm 1.2202(125.4042) | Total Time 10.00(10.00)\n",
      "Iter 2862 | Time 78.9610(78.3433) | Bit/dim 2.0719(2.0615) | Xent 0.0000(0.0000) | Loss 2.0719(2.0615) | Error 0.0000(0.0000) Steps 668(671.86) | Grad Norm 1.2049(121.6783) | Total Time 10.00(10.00)\n",
      "Iter 2863 | Time 77.1163(78.3065) | Bit/dim 2.0762(2.0619) | Xent 0.0000(0.0000) | Loss 2.0762(2.0619) | Error 0.0000(0.0000) Steps 668(671.74) | Grad Norm 1.2060(118.0641) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 20.5271, Epoch Time 574.6166(611.9485), Bit/dim 2.0697(best: 2.0816), Xent 0.0000, Loss 2.0697, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2864 | Time 76.8434(78.2626) | Bit/dim 2.0719(2.0622) | Xent 0.0000(0.0000) | Loss 2.0719(2.0622) | Error 0.0000(0.0000) Steps 662(671.45) | Grad Norm 1.2161(114.5587) | Total Time 10.00(10.00)\n",
      "Iter 2865 | Time 77.1606(78.2295) | Bit/dim 2.0727(2.0626) | Xent 0.0000(0.0000) | Loss 2.0727(2.0626) | Error 0.0000(0.0000) Steps 662(671.17) | Grad Norm 1.2168(111.1584) | Total Time 10.00(10.00)\n",
      "Iter 2866 | Time 75.7635(78.1555) | Bit/dim 2.0748(2.0629) | Xent 0.0000(0.0000) | Loss 2.0748(2.0629) | Error 0.0000(0.0000) Steps 668(671.07) | Grad Norm 1.2096(107.8599) | Total Time 10.00(10.00)\n",
      "Iter 2867 | Time 77.7360(78.1429) | Bit/dim 2.0702(2.0631) | Xent 0.0000(0.0000) | Loss 2.0702(2.0631) | Error 0.0000(0.0000) Steps 662(670.80) | Grad Norm 1.2210(104.6608) | Total Time 10.00(10.00)\n",
      "Iter 2868 | Time 78.1409(78.1429) | Bit/dim 2.0694(2.0633) | Xent 0.0000(0.0000) | Loss 2.0694(2.0633) | Error 0.0000(0.0000) Steps 668(670.72) | Grad Norm 1.2139(101.5574) | Total Time 10.00(10.00)\n",
      "Iter 2869 | Time 75.9978(78.0785) | Bit/dim 2.0649(2.0634) | Xent 0.0000(0.0000) | Loss 2.0649(2.0634) | Error 0.0000(0.0000) Steps 668(670.63) | Grad Norm 1.2097(98.5469) | Total Time 10.00(10.00)\n",
      "Iter 2870 | Time 79.0426(78.1075) | Bit/dim 2.0598(2.0633) | Xent 0.0000(0.0000) | Loss 2.0598(2.0633) | Error 0.0000(0.0000) Steps 668(670.56) | Grad Norm 1.2065(95.6267) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 20.5616, Epoch Time 573.8879(610.8066), Bit/dim 2.0571(best: 2.0697), Xent 0.0000, Loss 2.0571, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2871 | Time 76.7531(78.0668) | Bit/dim 2.0633(2.0633) | Xent 0.0000(0.0000) | Loss 2.0633(2.0633) | Error 0.0000(0.0000) Steps 668(670.48) | Grad Norm 1.2017(92.7940) | Total Time 10.00(10.00)\n",
      "Iter 2872 | Time 76.3062(78.0140) | Bit/dim 2.0636(2.0633) | Xent 0.0000(0.0000) | Loss 2.0636(2.0633) | Error 0.0000(0.0000) Steps 668(670.40) | Grad Norm 1.1867(90.0458) | Total Time 10.00(10.00)\n",
      "Iter 2873 | Time 76.4804(77.9680) | Bit/dim 2.0563(2.0631) | Xent 0.0000(0.0000) | Loss 2.0563(2.0631) | Error 0.0000(0.0000) Steps 662(670.15) | Grad Norm 1.1861(87.3800) | Total Time 10.00(10.00)\n",
      "Iter 2874 | Time 76.4720(77.9231) | Bit/dim 2.0520(2.0627) | Xent 0.0000(0.0000) | Loss 2.0520(2.0627) | Error 0.0000(0.0000) Steps 662(669.91) | Grad Norm 1.2014(84.7946) | Total Time 10.00(10.00)\n",
      "Iter 2875 | Time 77.0449(77.8968) | Bit/dim 2.0625(2.0627) | Xent 0.0000(0.0000) | Loss 2.0625(2.0627) | Error 0.0000(0.0000) Steps 662(669.67) | Grad Norm 1.1789(82.2861) | Total Time 10.00(10.00)\n",
      "Iter 2876 | Time 77.4525(77.8834) | Bit/dim 2.0573(2.0626) | Xent 0.0000(0.0000) | Loss 2.0573(2.0626) | Error 0.0000(0.0000) Steps 662(669.44) | Grad Norm 1.1949(79.8534) | Total Time 10.00(10.00)\n",
      "Iter 2877 | Time 76.2802(77.8353) | Bit/dim 2.0499(2.0622) | Xent 0.0000(0.0000) | Loss 2.0499(2.0622) | Error 0.0000(0.0000) Steps 656(669.04) | Grad Norm 1.1772(77.4931) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 20.6467, Epoch Time 570.0798(609.5848), Bit/dim 2.0460(best: 2.0571), Xent 0.0000, Loss 2.0460, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2878 | Time 75.8278(77.7751) | Bit/dim 2.0572(2.0620) | Xent 0.0000(0.0000) | Loss 2.0572(2.0620) | Error 0.0000(0.0000) Steps 662(668.83) | Grad Norm 1.1826(75.2038) | Total Time 10.00(10.00)\n",
      "Iter 2879 | Time 78.1106(77.7852) | Bit/dim 2.0501(2.0617) | Xent 0.0000(0.0000) | Loss 2.0501(2.0617) | Error 0.0000(0.0000) Steps 668(668.80) | Grad Norm 1.2000(72.9837) | Total Time 10.00(10.00)\n",
      "Iter 2880 | Time 77.6999(77.7826) | Bit/dim 2.0527(2.0614) | Xent 0.0000(0.0000) | Loss 2.0527(2.0614) | Error 0.0000(0.0000) Steps 668(668.78) | Grad Norm 1.1684(70.8292) | Total Time 10.00(10.00)\n",
      "Iter 2881 | Time 77.6877(77.7798) | Bit/dim 2.0492(2.0610) | Xent 0.0000(0.0000) | Loss 2.0492(2.0610) | Error 0.0000(0.0000) Steps 662(668.57) | Grad Norm 1.1547(68.7390) | Total Time 10.00(10.00)\n",
      "Iter 2882 | Time 77.2214(77.7630) | Bit/dim 2.0422(2.0605) | Xent 0.0000(0.0000) | Loss 2.0422(2.0605) | Error 0.0000(0.0000) Steps 662(668.38) | Grad Norm 1.1629(66.7117) | Total Time 10.00(10.00)\n",
      "Iter 2883 | Time 77.2902(77.7488) | Bit/dim 2.0402(2.0599) | Xent 0.0000(0.0000) | Loss 2.0402(2.0599) | Error 0.0000(0.0000) Steps 662(668.19) | Grad Norm 1.1583(64.7451) | Total Time 10.00(10.00)\n",
      "Iter 2884 | Time 78.7617(77.7792) | Bit/dim 2.0401(2.0593) | Xent 0.0000(0.0000) | Loss 2.0401(2.0593) | Error 0.0000(0.0000) Steps 656(667.82) | Grad Norm 1.1440(62.8371) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 20.3565, Epoch Time 575.5854(608.5649), Bit/dim 2.0362(best: 2.0460), Xent 0.0000, Loss 2.0362, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2885 | Time 78.0425(77.7871) | Bit/dim 2.0405(2.0587) | Xent 0.0000(0.0000) | Loss 2.0405(2.0587) | Error 0.0000(0.0000) Steps 662(667.65) | Grad Norm 1.1648(60.9869) | Total Time 10.00(10.00)\n",
      "Iter 2886 | Time 77.6444(77.7828) | Bit/dim 2.0399(2.0582) | Xent 0.0000(0.0000) | Loss 2.0399(2.0582) | Error 0.0000(0.0000) Steps 668(667.66) | Grad Norm 1.1729(59.1925) | Total Time 10.00(10.00)\n",
      "Iter 2887 | Time 77.4446(77.7727) | Bit/dim 2.0367(2.0575) | Xent 0.0000(0.0000) | Loss 2.0367(2.0575) | Error 0.0000(0.0000) Steps 668(667.67) | Grad Norm 1.1577(57.4514) | Total Time 10.00(10.00)\n",
      "Iter 2888 | Time 77.0315(77.7505) | Bit/dim 2.0368(2.0569) | Xent 0.0000(0.0000) | Loss 2.0368(2.0569) | Error 0.0000(0.0000) Steps 662(667.50) | Grad Norm 1.1334(55.7619) | Total Time 10.00(10.00)\n",
      "Iter 2889 | Time 75.8020(77.6920) | Bit/dim 2.0403(2.0564) | Xent 0.0000(0.0000) | Loss 2.0403(2.0564) | Error 0.0000(0.0000) Steps 662(667.33) | Grad Norm 1.1406(54.1233) | Total Time 10.00(10.00)\n",
      "Iter 2890 | Time 76.1442(77.6456) | Bit/dim 2.0320(2.0557) | Xent 0.0000(0.0000) | Loss 2.0320(2.0557) | Error 0.0000(0.0000) Steps 662(667.17) | Grad Norm 1.1359(52.5336) | Total Time 10.00(10.00)\n",
      "Iter 2891 | Time 75.9404(77.5944) | Bit/dim 2.0243(2.0547) | Xent 0.0000(0.0000) | Loss 2.0243(2.0547) | Error 0.0000(0.0000) Steps 662(667.02) | Grad Norm 1.1400(50.9918) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 20.0596, Epoch Time 570.7698(607.4310), Bit/dim 2.0265(best: 2.0362), Xent 0.0000, Loss 2.0265, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2892 | Time 75.7939(77.5404) | Bit/dim 2.0247(2.0538) | Xent 0.0000(0.0000) | Loss 2.0247(2.0538) | Error 0.0000(0.0000) Steps 656(666.69) | Grad Norm 1.1283(49.4959) | Total Time 10.00(10.00)\n",
      "Iter 2893 | Time 77.6107(77.5425) | Bit/dim 2.0283(2.0531) | Xent 0.0000(0.0000) | Loss 2.0283(2.0531) | Error 0.0000(0.0000) Steps 662(666.55) | Grad Norm 1.1249(48.0448) | Total Time 10.00(10.00)\n",
      "Iter 2894 | Time 76.5048(77.5114) | Bit/dim 2.0251(2.0522) | Xent 0.0000(0.0000) | Loss 2.0251(2.0522) | Error 0.0000(0.0000) Steps 656(666.23) | Grad Norm 1.1148(46.6369) | Total Time 10.00(10.00)\n",
      "Iter 2895 | Time 77.7281(77.5179) | Bit/dim 2.0277(2.0515) | Xent 0.0000(0.0000) | Loss 2.0277(2.0515) | Error 0.0000(0.0000) Steps 656(665.92) | Grad Norm 1.1282(45.2716) | Total Time 10.00(10.00)\n",
      "Iter 2896 | Time 76.7696(77.4954) | Bit/dim 2.0245(2.0507) | Xent 0.0000(0.0000) | Loss 2.0245(2.0507) | Error 0.0000(0.0000) Steps 650(665.44) | Grad Norm 1.1180(43.9470) | Total Time 10.00(10.00)\n",
      "Iter 2897 | Time 76.5750(77.4678) | Bit/dim 2.0288(2.0500) | Xent 0.0000(0.0000) | Loss 2.0288(2.0500) | Error 0.0000(0.0000) Steps 650(664.98) | Grad Norm 1.1168(42.6621) | Total Time 10.00(10.00)\n",
      "Iter 2898 | Time 77.9416(77.4820) | Bit/dim 2.0244(2.0492) | Xent 0.0000(0.0000) | Loss 2.0244(2.0492) | Error 0.0000(0.0000) Steps 650(664.53) | Grad Norm 1.1055(41.4154) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 20.3260, Epoch Time 571.8934(606.3649), Bit/dim 2.0153(best: 2.0265), Xent 0.0000, Loss 2.0153, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2899 | Time 76.2722(77.4457) | Bit/dim 2.0173(2.0483) | Xent 0.0000(0.0000) | Loss 2.0173(2.0483) | Error 0.0000(0.0000) Steps 656(664.28) | Grad Norm 1.1077(40.2062) | Total Time 10.00(10.00)\n",
      "Iter 2900 | Time 77.4685(77.4464) | Bit/dim 2.0208(2.0475) | Xent 0.0000(0.0000) | Loss 2.0208(2.0475) | Error 0.0000(0.0000) Steps 650(663.85) | Grad Norm 1.0915(39.0327) | Total Time 10.00(10.00)\n",
      "Iter 2901 | Time 77.0959(77.4359) | Bit/dim 2.0174(2.0466) | Xent 0.0000(0.0000) | Loss 2.0174(2.0466) | Error 0.0000(0.0000) Steps 656(663.61) | Grad Norm 1.1072(37.8950) | Total Time 10.00(10.00)\n",
      "Iter 2902 | Time 77.8364(77.4479) | Bit/dim 2.0143(2.0456) | Xent 0.0000(0.0000) | Loss 2.0143(2.0456) | Error 0.0000(0.0000) Steps 650(663.20) | Grad Norm 1.0863(36.7907) | Total Time 10.00(10.00)\n",
      "Iter 2903 | Time 76.7327(77.4265) | Bit/dim 2.0208(2.0449) | Xent 0.0000(0.0000) | Loss 2.0208(2.0449) | Error 0.0000(0.0000) Steps 656(662.99) | Grad Norm 1.0885(35.7197) | Total Time 10.00(10.00)\n",
      "Iter 2904 | Time 77.4414(77.4269) | Bit/dim 2.0136(2.0439) | Xent 0.0000(0.0000) | Loss 2.0136(2.0439) | Error 0.0000(0.0000) Steps 650(662.60) | Grad Norm 1.0798(34.6805) | Total Time 10.00(10.00)\n",
      "Iter 2905 | Time 76.2585(77.3919) | Bit/dim 2.0141(2.0430) | Xent 0.0000(0.0000) | Loss 2.0141(2.0430) | Error 0.0000(0.0000) Steps 650(662.22) | Grad Norm 1.0909(33.6728) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 19.8187, Epoch Time 571.5739(605.3211), Bit/dim 2.0071(best: 2.0153), Xent 0.0000, Loss 2.0071, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2906 | Time 74.5524(77.3067) | Bit/dim 2.0179(2.0423) | Xent 0.0000(0.0000) | Loss 2.0179(2.0423) | Error 0.0000(0.0000) Steps 650(661.85) | Grad Norm 1.0843(32.6951) | Total Time 10.00(10.00)\n",
      "Iter 2907 | Time 76.6456(77.2868) | Bit/dim 2.0111(2.0413) | Xent 0.0000(0.0000) | Loss 2.0111(2.0413) | Error 0.0000(0.0000) Steps 650(661.50) | Grad Norm 1.1049(31.7474) | Total Time 10.00(10.00)\n",
      "Iter 2908 | Time 77.5891(77.2959) | Bit/dim 2.0024(2.0402) | Xent 0.0000(0.0000) | Loss 2.0024(2.0402) | Error 0.0000(0.0000) Steps 650(661.15) | Grad Norm 1.0997(30.8280) | Total Time 10.00(10.00)\n",
      "Iter 2909 | Time 78.3992(77.3290) | Bit/dim 2.0088(2.0392) | Xent 0.0000(0.0000) | Loss 2.0088(2.0392) | Error 0.0000(0.0000) Steps 650(660.82) | Grad Norm 1.0699(29.9352) | Total Time 10.00(10.00)\n",
      "Iter 2910 | Time 76.0053(77.2893) | Bit/dim 2.0092(2.0383) | Xent 0.0000(0.0000) | Loss 2.0092(2.0383) | Error 0.0000(0.0000) Steps 650(660.49) | Grad Norm 1.0569(29.0689) | Total Time 10.00(10.00)\n",
      "Iter 2911 | Time 77.0513(77.2822) | Bit/dim 2.0051(2.0373) | Xent 0.0000(0.0000) | Loss 2.0051(2.0373) | Error 0.0000(0.0000) Steps 650(660.18) | Grad Norm 1.0563(28.2285) | Total Time 10.00(10.00)\n",
      "Iter 2912 | Time 75.6053(77.2318) | Bit/dim 1.9998(2.0362) | Xent 0.0000(0.0000) | Loss 1.9998(2.0362) | Error 0.0000(0.0000) Steps 650(659.87) | Grad Norm 1.0725(27.4138) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 19.8653, Epoch Time 568.6947(604.2224), Bit/dim 1.9968(best: 2.0071), Xent 0.0000, Loss 1.9968, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2913 | Time 75.1970(77.1708) | Bit/dim 1.9997(2.0351) | Xent 0.0000(0.0000) | Loss 1.9997(2.0351) | Error 0.0000(0.0000) Steps 650(659.58) | Grad Norm 1.0610(26.6232) | Total Time 10.00(10.00)\n",
      "Iter 2914 | Time 75.2032(77.1118) | Bit/dim 2.0081(2.0343) | Xent 0.0000(0.0000) | Loss 2.0081(2.0343) | Error 0.0000(0.0000) Steps 650(659.29) | Grad Norm 1.0603(25.8564) | Total Time 10.00(10.00)\n",
      "Iter 2915 | Time 77.2584(77.1162) | Bit/dim 2.0032(2.0334) | Xent 0.0000(0.0000) | Loss 2.0032(2.0334) | Error 0.0000(0.0000) Steps 656(659.19) | Grad Norm 1.0601(25.1125) | Total Time 10.00(10.00)\n",
      "Iter 2916 | Time 76.4965(77.0976) | Bit/dim 1.9929(2.0322) | Xent 0.0000(0.0000) | Loss 1.9929(2.0322) | Error 0.0000(0.0000) Steps 656(659.10) | Grad Norm 1.0847(24.3916) | Total Time 10.00(10.00)\n",
      "Iter 2917 | Time 75.9162(77.0621) | Bit/dim 1.9945(2.0310) | Xent 0.0000(0.0000) | Loss 1.9945(2.0310) | Error 0.0000(0.0000) Steps 656(659.00) | Grad Norm 1.0396(23.6911) | Total Time 10.00(10.00)\n",
      "Iter 2918 | Time 76.7690(77.0533) | Bit/dim 1.9960(2.0300) | Xent 0.0000(0.0000) | Loss 1.9960(2.0300) | Error 0.0000(0.0000) Steps 656(658.91) | Grad Norm 1.0314(23.0113) | Total Time 10.00(10.00)\n",
      "Iter 2919 | Time 77.1109(77.0551) | Bit/dim 1.9955(2.0289) | Xent 0.0000(0.0000) | Loss 1.9955(2.0289) | Error 0.0000(0.0000) Steps 644(658.47) | Grad Norm 1.0429(22.3522) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 20.1498, Epoch Time 566.8931(603.1025), Bit/dim 1.9889(best: 1.9968), Xent 0.0000, Loss 1.9889, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2920 | Time 75.4258(77.0062) | Bit/dim 1.9951(2.0279) | Xent 0.0000(0.0000) | Loss 1.9951(2.0279) | Error 0.0000(0.0000) Steps 644(658.03) | Grad Norm 1.0206(21.7123) | Total Time 10.00(10.00)\n",
      "Iter 2921 | Time 74.9162(76.9435) | Bit/dim 1.9940(2.0269) | Xent 0.0000(0.0000) | Loss 1.9940(2.0269) | Error 0.0000(0.0000) Steps 644(657.61) | Grad Norm 1.0365(21.0920) | Total Time 10.00(10.00)\n",
      "Iter 2922 | Time 73.7133(76.8466) | Bit/dim 1.9930(2.0259) | Xent 0.0000(0.0000) | Loss 1.9930(2.0259) | Error 0.0000(0.0000) Steps 644(657.20) | Grad Norm 1.0615(20.4911) | Total Time 10.00(10.00)\n",
      "Iter 2923 | Time 73.6383(76.7503) | Bit/dim 1.9922(2.0249) | Xent 0.0000(0.0000) | Loss 1.9922(2.0249) | Error 0.0000(0.0000) Steps 644(656.81) | Grad Norm 1.0528(19.9079) | Total Time 10.00(10.00)\n",
      "Iter 2924 | Time 75.0374(76.6990) | Bit/dim 1.9836(2.0236) | Xent 0.0000(0.0000) | Loss 1.9836(2.0236) | Error 0.0000(0.0000) Steps 644(656.42) | Grad Norm 1.0261(19.3415) | Total Time 10.00(10.00)\n",
      "Iter 2925 | Time 73.7778(76.6113) | Bit/dim 1.9893(2.0226) | Xent 0.0000(0.0000) | Loss 1.9893(2.0226) | Error 0.0000(0.0000) Steps 644(656.05) | Grad Norm 1.0043(18.7914) | Total Time 10.00(10.00)\n",
      "Iter 2926 | Time 75.4870(76.5776) | Bit/dim 1.9839(2.0214) | Xent 0.0000(0.0000) | Loss 1.9839(2.0214) | Error 0.0000(0.0000) Steps 644(655.69) | Grad Norm 1.0229(18.2583) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 20.2152, Epoch Time 554.7366(601.6515), Bit/dim 1.9813(best: 1.9889), Xent 0.0000, Loss 1.9813, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2927 | Time 73.2582(76.4780) | Bit/dim 1.9890(2.0205) | Xent 0.0000(0.0000) | Loss 1.9890(2.0205) | Error 0.0000(0.0000) Steps 650(655.52) | Grad Norm 1.0008(17.7406) | Total Time 10.00(10.00)\n",
      "Iter 2928 | Time 73.5508(76.3902) | Bit/dim 1.9793(2.0192) | Xent 0.0000(0.0000) | Loss 1.9793(2.0192) | Error 0.0000(0.0000) Steps 650(655.35) | Grad Norm 1.0013(17.2384) | Total Time 10.00(10.00)\n",
      "Iter 2929 | Time 74.0203(76.3191) | Bit/dim 1.9785(2.0180) | Xent 0.0000(0.0000) | Loss 1.9785(2.0180) | Error 0.0000(0.0000) Steps 650(655.19) | Grad Norm 1.0632(16.7532) | Total Time 10.00(10.00)\n",
      "Iter 2930 | Time 76.5105(76.3248) | Bit/dim 1.9859(2.0171) | Xent 0.0000(0.0000) | Loss 1.9859(2.0171) | Error 0.0000(0.0000) Steps 650(655.04) | Grad Norm 1.0292(16.2814) | Total Time 10.00(10.00)\n",
      "Iter 2931 | Time 80.7678(76.4581) | Bit/dim 1.9920(2.0163) | Xent 0.0000(0.0000) | Loss 1.9920(2.0163) | Error 0.0000(0.0000) Steps 668(655.42) | Grad Norm 0.9796(15.8224) | Total Time 10.00(10.00)\n",
      "Iter 2932 | Time 78.7900(76.5281) | Bit/dim 1.9778(2.0151) | Xent 0.0000(0.0000) | Loss 1.9778(2.0151) | Error 0.0000(0.0000) Steps 656(655.44) | Grad Norm 0.9807(15.3771) | Total Time 10.00(10.00)\n",
      "Iter 2933 | Time 78.7925(76.5960) | Bit/dim 1.9765(2.0140) | Xent 0.0000(0.0000) | Loss 1.9765(2.0140) | Error 0.0000(0.0000) Steps 656(655.46) | Grad Norm 0.9718(14.9450) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 20.3966, Epoch Time 568.4989(600.6569), Bit/dim 1.9726(best: 1.9813), Xent 0.0000, Loss 1.9726, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2934 | Time 79.1374(76.6722) | Bit/dim 1.9746(2.0128) | Xent 0.0000(0.0000) | Loss 1.9746(2.0128) | Error 0.0000(0.0000) Steps 668(655.83) | Grad Norm 0.9694(14.5257) | Total Time 10.00(10.00)\n",
      "Iter 2935 | Time 78.1332(76.7161) | Bit/dim 1.9740(2.0116) | Xent 0.0000(0.0000) | Loss 1.9740(2.0116) | Error 0.0000(0.0000) Steps 668(656.20) | Grad Norm 1.0331(14.1209) | Total Time 10.00(10.00)\n",
      "Iter 2936 | Time 79.3461(76.7950) | Bit/dim 1.9766(2.0106) | Xent 0.0000(0.0000) | Loss 1.9766(2.0106) | Error 0.0000(0.0000) Steps 656(656.19) | Grad Norm 0.9953(13.7272) | Total Time 10.00(10.00)\n",
      "Iter 2937 | Time 80.3554(76.9018) | Bit/dim 1.9707(2.0094) | Xent 0.0000(0.0000) | Loss 1.9707(2.0094) | Error 0.0000(0.0000) Steps 656(656.19) | Grad Norm 0.9895(13.3450) | Total Time 10.00(10.00)\n",
      "Iter 2938 | Time 76.8447(76.9001) | Bit/dim 1.9746(2.0084) | Xent 0.0000(0.0000) | Loss 1.9746(2.0084) | Error 0.0000(0.0000) Steps 656(656.18) | Grad Norm 0.9627(12.9736) | Total Time 10.00(10.00)\n",
      "Iter 2939 | Time 77.9220(76.9307) | Bit/dim 1.9780(2.0074) | Xent 0.0000(0.0000) | Loss 1.9780(2.0074) | Error 0.0000(0.0000) Steps 656(656.18) | Grad Norm 0.9837(12.6139) | Total Time 10.00(10.00)\n",
      "Iter 2940 | Time 76.9131(76.9302) | Bit/dim 1.9711(2.0064) | Xent 0.0000(0.0000) | Loss 1.9711(2.0064) | Error 0.0000(0.0000) Steps 656(656.17) | Grad Norm 0.9435(12.2638) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 20.3319, Epoch Time 581.8399(600.0924), Bit/dim 1.9646(best: 1.9726), Xent 0.0000, Loss 1.9646, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2941 | Time 77.2049(76.9384) | Bit/dim 1.9786(2.0055) | Xent 0.0000(0.0000) | Loss 1.9786(2.0055) | Error 0.0000(0.0000) Steps 668(656.53) | Grad Norm 0.9585(11.9246) | Total Time 10.00(10.00)\n",
      "Iter 2942 | Time 77.8966(76.9672) | Bit/dim 1.9698(2.0044) | Xent 0.0000(0.0000) | Loss 1.9698(2.0044) | Error 0.0000(0.0000) Steps 668(656.87) | Grad Norm 1.0338(11.5979) | Total Time 10.00(10.00)\n",
      "Iter 2943 | Time 77.9816(76.9976) | Bit/dim 1.9600(2.0031) | Xent 0.0000(0.0000) | Loss 1.9600(2.0031) | Error 0.0000(0.0000) Steps 668(657.20) | Grad Norm 0.9932(11.2797) | Total Time 10.00(10.00)\n",
      "Iter 2944 | Time 77.7221(77.0194) | Bit/dim 1.9688(2.0021) | Xent 0.0000(0.0000) | Loss 1.9688(2.0021) | Error 0.0000(0.0000) Steps 656(657.17) | Grad Norm 0.9433(10.9696) | Total Time 10.00(10.00)\n",
      "Iter 2945 | Time 79.0857(77.0813) | Bit/dim 1.9670(2.0010) | Xent 0.0000(0.0000) | Loss 1.9670(2.0010) | Error 0.0000(0.0000) Steps 656(657.13) | Grad Norm 0.9516(10.6691) | Total Time 10.00(10.00)\n",
      "Iter 2946 | Time 78.5157(77.1244) | Bit/dim 1.9673(2.0000) | Xent 0.0000(0.0000) | Loss 1.9673(2.0000) | Error 0.0000(0.0000) Steps 656(657.10) | Grad Norm 0.9348(10.3771) | Total Time 10.00(10.00)\n",
      "Iter 2947 | Time 77.5222(77.1363) | Bit/dim 1.9573(1.9987) | Xent 0.0000(0.0000) | Loss 1.9573(1.9987) | Error 0.0000(0.0000) Steps 662(657.25) | Grad Norm 0.9606(10.0946) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 20.4617, Epoch Time 579.2442(599.4670), Bit/dim 1.9583(best: 1.9646), Xent 0.0000, Loss 1.9583, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2948 | Time 79.0954(77.1951) | Bit/dim 1.9583(1.9975) | Xent 0.0000(0.0000) | Loss 1.9583(1.9975) | Error 0.0000(0.0000) Steps 650(657.03) | Grad Norm 0.9792(9.8211) | Total Time 10.00(10.00)\n",
      "Iter 2949 | Time 79.0388(77.2504) | Bit/dim 1.9592(1.9964) | Xent 0.0000(0.0000) | Loss 1.9592(1.9964) | Error 0.0000(0.0000) Steps 662(657.18) | Grad Norm 0.9741(9.5557) | Total Time 10.00(10.00)\n",
      "Iter 2950 | Time 79.8255(77.3276) | Bit/dim 1.9692(1.9956) | Xent 0.0000(0.0000) | Loss 1.9692(1.9956) | Error 0.0000(0.0000) Steps 650(656.96) | Grad Norm 0.9335(9.2970) | Total Time 10.00(10.00)\n",
      "Iter 2951 | Time 77.5568(77.3345) | Bit/dim 1.9613(1.9945) | Xent 0.0000(0.0000) | Loss 1.9613(1.9945) | Error 0.0000(0.0000) Steps 662(657.11) | Grad Norm 0.9299(9.0460) | Total Time 10.00(10.00)\n",
      "Iter 2952 | Time 79.8805(77.4109) | Bit/dim 1.9594(1.9935) | Xent 0.0000(0.0000) | Loss 1.9594(1.9935) | Error 0.0000(0.0000) Steps 650(656.90) | Grad Norm 0.9162(8.8021) | Total Time 10.00(10.00)\n",
      "Iter 2953 | Time 76.4922(77.3833) | Bit/dim 1.9605(1.9925) | Xent 0.0000(0.0000) | Loss 1.9605(1.9925) | Error 0.0000(0.0000) Steps 650(656.69) | Grad Norm 0.9403(8.5663) | Total Time 10.00(10.00)\n",
      "Iter 2954 | Time 79.1879(77.4375) | Bit/dim 1.9540(1.9913) | Xent 0.0000(0.0000) | Loss 1.9540(1.9913) | Error 0.0000(0.0000) Steps 662(656.85) | Grad Norm 0.9661(8.3383) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 20.5106, Epoch Time 584.1677(599.0080), Bit/dim 1.9508(best: 1.9583), Xent 0.0000, Loss 1.9508, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2955 | Time 79.7788(77.5077) | Bit/dim 1.9560(1.9903) | Xent 0.0000(0.0000) | Loss 1.9560(1.9903) | Error 0.0000(0.0000) Steps 656(656.83) | Grad Norm 0.9222(8.1158) | Total Time 10.00(10.00)\n",
      "Iter 2956 | Time 78.1750(77.5277) | Bit/dim 1.9616(1.9894) | Xent 0.0000(0.0000) | Loss 1.9616(1.9894) | Error 0.0000(0.0000) Steps 662(656.98) | Grad Norm 0.9187(7.8999) | Total Time 10.00(10.00)\n",
      "Iter 2957 | Time 78.2705(77.5500) | Bit/dim 1.9472(1.9882) | Xent 0.0000(0.0000) | Loss 1.9472(1.9882) | Error 0.0000(0.0000) Steps 656(656.95) | Grad Norm 0.9124(7.6903) | Total Time 10.00(10.00)\n",
      "Iter 2958 | Time 77.5363(77.5496) | Bit/dim 1.9584(1.9873) | Xent 0.0000(0.0000) | Loss 1.9584(1.9873) | Error 0.0000(0.0000) Steps 662(657.10) | Grad Norm 0.9075(7.4868) | Total Time 10.00(10.00)\n",
      "Iter 2959 | Time 76.8271(77.5279) | Bit/dim 1.9548(1.9863) | Xent 0.0000(0.0000) | Loss 1.9548(1.9863) | Error 0.0000(0.0000) Steps 656(657.07) | Grad Norm 0.9406(7.2904) | Total Time 10.00(10.00)\n",
      "Iter 2960 | Time 78.9524(77.5707) | Bit/dim 1.9477(1.9851) | Xent 0.0000(0.0000) | Loss 1.9477(1.9851) | Error 0.0000(0.0000) Steps 650(656.86) | Grad Norm 0.9164(7.0992) | Total Time 10.00(10.00)\n",
      "Iter 2961 | Time 77.8329(77.5785) | Bit/dim 1.9473(1.9840) | Xent 0.0000(0.0000) | Loss 1.9473(1.9840) | Error 0.0000(0.0000) Steps 650(656.65) | Grad Norm 0.9028(6.9133) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 20.3791, Epoch Time 580.6164(598.4562), Bit/dim 1.9449(best: 1.9508), Xent 0.0000, Loss 1.9449, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2962 | Time 79.1229(77.6249) | Bit/dim 1.9512(1.9830) | Xent 0.0000(0.0000) | Loss 1.9512(1.9830) | Error 0.0000(0.0000) Steps 656(656.63) | Grad Norm 0.8860(6.7325) | Total Time 10.00(10.00)\n",
      "Iter 2963 | Time 79.7965(77.6900) | Bit/dim 1.9495(1.9820) | Xent 0.0000(0.0000) | Loss 1.9495(1.9820) | Error 0.0000(0.0000) Steps 650(656.43) | Grad Norm 0.8958(6.5574) | Total Time 10.00(10.00)\n",
      "Iter 2964 | Time 77.6790(77.6897) | Bit/dim 1.9428(1.9808) | Xent 0.0000(0.0000) | Loss 1.9428(1.9808) | Error 0.0000(0.0000) Steps 650(656.24) | Grad Norm 0.9314(6.3886) | Total Time 10.00(10.00)\n",
      "Iter 2965 | Time 79.1797(77.7344) | Bit/dim 1.9470(1.9798) | Xent 0.0000(0.0000) | Loss 1.9470(1.9798) | Error 0.0000(0.0000) Steps 674(656.77) | Grad Norm 0.8822(6.2234) | Total Time 10.00(10.00)\n",
      "Iter 2966 | Time 79.4429(77.7856) | Bit/dim 1.9427(1.9787) | Xent 0.0000(0.0000) | Loss 1.9427(1.9787) | Error 0.0000(0.0000) Steps 662(656.93) | Grad Norm 0.9087(6.0639) | Total Time 10.00(10.00)\n",
      "Iter 2967 | Time 78.4231(77.8048) | Bit/dim 1.9461(1.9777) | Xent 0.0000(0.0000) | Loss 1.9461(1.9777) | Error 0.0000(0.0000) Steps 656(656.90) | Grad Norm 0.8857(5.9086) | Total Time 10.00(10.00)\n",
      "Iter 2968 | Time 79.4443(77.8539) | Bit/dim 1.9460(1.9768) | Xent 0.0000(0.0000) | Loss 1.9460(1.9768) | Error 0.0000(0.0000) Steps 662(657.06) | Grad Norm 0.8885(5.7580) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 20.6522, Epoch Time 586.2735(598.0908), Bit/dim 1.9379(best: 1.9449), Xent 0.0000, Loss 1.9379, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2969 | Time 77.7704(77.8514) | Bit/dim 1.9404(1.9757) | Xent 0.0000(0.0000) | Loss 1.9404(1.9757) | Error 0.0000(0.0000) Steps 662(657.20) | Grad Norm 0.8845(5.6118) | Total Time 10.00(10.00)\n",
      "Iter 2970 | Time 77.9857(77.8555) | Bit/dim 1.9457(1.9748) | Xent 0.0000(0.0000) | Loss 1.9457(1.9748) | Error 0.0000(0.0000) Steps 668(657.53) | Grad Norm 0.8763(5.4697) | Total Time 10.00(10.00)\n",
      "Iter 2971 | Time 79.4553(77.9035) | Bit/dim 1.9432(1.9738) | Xent 0.0000(0.0000) | Loss 1.9432(1.9738) | Error 0.0000(0.0000) Steps 668(657.84) | Grad Norm 0.8764(5.3319) | Total Time 10.00(10.00)\n",
      "Iter 2972 | Time 77.9060(77.9035) | Bit/dim 1.9381(1.9728) | Xent 0.0000(0.0000) | Loss 1.9381(1.9728) | Error 0.0000(0.0000) Steps 668(658.15) | Grad Norm 0.9129(5.1994) | Total Time 10.00(10.00)\n",
      "Iter 2973 | Time 78.4498(77.9199) | Bit/dim 1.9388(1.9717) | Xent 0.0000(0.0000) | Loss 1.9388(1.9717) | Error 0.0000(0.0000) Steps 668(658.44) | Grad Norm 0.8751(5.0696) | Total Time 10.00(10.00)\n",
      "Iter 2974 | Time 78.3657(77.9333) | Bit/dim 1.9349(1.9706) | Xent 0.0000(0.0000) | Loss 1.9349(1.9706) | Error 0.0000(0.0000) Steps 668(658.73) | Grad Norm 0.8829(4.9440) | Total Time 10.00(10.00)\n",
      "Iter 2975 | Time 76.7429(77.8976) | Bit/dim 1.9394(1.9697) | Xent 0.0000(0.0000) | Loss 1.9394(1.9697) | Error 0.0000(0.0000) Steps 674(659.19) | Grad Norm 0.8871(4.8223) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 20.9217, Epoch Time 580.1030(597.5511), Bit/dim 1.9314(best: 1.9379), Xent 0.0000, Loss 1.9314, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2976 | Time 77.2905(77.8794) | Bit/dim 1.9322(1.9686) | Xent 0.0000(0.0000) | Loss 1.9322(1.9686) | Error 0.0000(0.0000) Steps 674(659.63) | Grad Norm 0.8657(4.7036) | Total Time 10.00(10.00)\n",
      "Iter 2977 | Time 78.6866(77.9036) | Bit/dim 1.9356(1.9676) | Xent 0.0000(0.0000) | Loss 1.9356(1.9676) | Error 0.0000(0.0000) Steps 680(660.24) | Grad Norm 0.8816(4.5890) | Total Time 10.00(10.00)\n",
      "Iter 2978 | Time 78.3303(77.9164) | Bit/dim 1.9329(1.9665) | Xent 0.0000(0.0000) | Loss 1.9329(1.9665) | Error 0.0000(0.0000) Steps 668(660.48) | Grad Norm 0.9315(4.4792) | Total Time 10.00(10.00)\n",
      "Iter 2979 | Time 75.1443(77.8332) | Bit/dim 1.9396(1.9657) | Xent 0.0000(0.0000) | Loss 1.9396(1.9657) | Error 0.0000(0.0000) Steps 668(660.70) | Grad Norm 0.8546(4.3705) | Total Time 10.00(10.00)\n",
      "Iter 2980 | Time 80.4370(77.9113) | Bit/dim 1.9307(1.9647) | Xent 0.0000(0.0000) | Loss 1.9307(1.9647) | Error 0.0000(0.0000) Steps 680(661.28) | Grad Norm 0.8615(4.2652) | Total Time 10.00(10.00)\n",
      "Iter 2981 | Time 77.2573(77.8917) | Bit/dim 1.9290(1.9636) | Xent 0.0000(0.0000) | Loss 1.9290(1.9636) | Error 0.0000(0.0000) Steps 674(661.66) | Grad Norm 0.9116(4.1646) | Total Time 10.00(10.00)\n",
      "Iter 2982 | Time 76.8902(77.8617) | Bit/dim 1.9359(1.9628) | Xent 0.0000(0.0000) | Loss 1.9359(1.9628) | Error 0.0000(0.0000) Steps 674(662.03) | Grad Norm 0.9747(4.0689) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 21.0810, Epoch Time 577.7734(596.9578), Bit/dim 1.9249(best: 1.9314), Xent 0.0000, Loss 1.9249, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2983 | Time 75.8236(77.8005) | Bit/dim 1.9336(1.9619) | Xent 0.0000(0.0000) | Loss 1.9336(1.9619) | Error 0.0000(0.0000) Steps 668(662.21) | Grad Norm 0.8852(3.9734) | Total Time 10.00(10.00)\n",
      "Iter 2984 | Time 75.9369(77.7446) | Bit/dim 1.9319(1.9610) | Xent 0.0000(0.0000) | Loss 1.9319(1.9610) | Error 0.0000(0.0000) Steps 668(662.38) | Grad Norm 0.8784(3.8806) | Total Time 10.00(10.00)\n",
      "Iter 2985 | Time 76.8562(77.7180) | Bit/dim 1.9201(1.9598) | Xent 0.0000(0.0000) | Loss 1.9201(1.9598) | Error 0.0000(0.0000) Steps 674(662.73) | Grad Norm 0.8756(3.7904) | Total Time 10.00(10.00)\n",
      "Iter 2986 | Time 75.9259(77.6642) | Bit/dim 1.9234(1.9587) | Xent 0.0000(0.0000) | Loss 1.9234(1.9587) | Error 0.0000(0.0000) Steps 674(663.07) | Grad Norm 0.9199(3.7043) | Total Time 10.00(10.00)\n",
      "Iter 2987 | Time 75.0002(77.5843) | Bit/dim 1.9267(1.9577) | Xent 0.0000(0.0000) | Loss 1.9267(1.9577) | Error 0.0000(0.0000) Steps 668(663.22) | Grad Norm 0.8596(3.6190) | Total Time 10.00(10.00)\n",
      "Iter 2988 | Time 75.1100(77.5101) | Bit/dim 1.9231(1.9567) | Xent 0.0000(0.0000) | Loss 1.9231(1.9567) | Error 0.0000(0.0000) Steps 668(663.36) | Grad Norm 0.8981(3.5373) | Total Time 10.00(10.00)\n",
      "Iter 2989 | Time 78.6153(77.5432) | Bit/dim 1.9303(1.9559) | Xent 0.0000(0.0000) | Loss 1.9303(1.9559) | Error 0.0000(0.0000) Steps 668(663.50) | Grad Norm 0.8696(3.4573) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 21.2232, Epoch Time 566.9404(596.0573), Bit/dim 1.9195(best: 1.9249), Xent 0.0000, Loss 1.9195, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2990 | Time 77.6922(77.5477) | Bit/dim 1.9276(1.9551) | Xent 0.0000(0.0000) | Loss 1.9276(1.9551) | Error 0.0000(0.0000) Steps 668(663.64) | Grad Norm 0.8728(3.3798) | Total Time 10.00(10.00)\n",
      "Iter 2991 | Time 77.4352(77.5443) | Bit/dim 1.9237(1.9541) | Xent 0.0000(0.0000) | Loss 1.9237(1.9541) | Error 0.0000(0.0000) Steps 674(663.95) | Grad Norm 0.8660(3.3043) | Total Time 10.00(10.00)\n",
      "Iter 2992 | Time 79.2272(77.5948) | Bit/dim 1.9231(1.9532) | Xent 0.0000(0.0000) | Loss 1.9231(1.9532) | Error 0.0000(0.0000) Steps 674(664.25) | Grad Norm 0.8436(3.2305) | Total Time 10.00(10.00)\n",
      "Iter 2993 | Time 76.1596(77.5517) | Bit/dim 1.9251(1.9523) | Xent 0.0000(0.0000) | Loss 1.9251(1.9523) | Error 0.0000(0.0000) Steps 674(664.54) | Grad Norm 0.8519(3.1592) | Total Time 10.00(10.00)\n",
      "Iter 2994 | Time 75.2528(77.4828) | Bit/dim 1.9199(1.9514) | Xent 0.0000(0.0000) | Loss 1.9199(1.9514) | Error 0.0000(0.0000) Steps 674(664.83) | Grad Norm 0.8323(3.0894) | Total Time 10.00(10.00)\n",
      "Iter 2995 | Time 75.4493(77.4218) | Bit/dim 1.9180(1.9504) | Xent 0.0000(0.0000) | Loss 1.9180(1.9504) | Error 0.0000(0.0000) Steps 674(665.10) | Grad Norm 0.8205(3.0213) | Total Time 10.00(10.00)\n",
      "Iter 2996 | Time 77.0550(77.4108) | Bit/dim 1.9193(1.9494) | Xent 0.0000(0.0000) | Loss 1.9193(1.9494) | Error 0.0000(0.0000) Steps 668(665.19) | Grad Norm 0.8998(2.9577) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 21.1749, Epoch Time 572.5091(595.3508), Bit/dim 1.9132(best: 1.9195), Xent 0.0000, Loss 1.9132, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2997 | Time 77.6572(77.4182) | Bit/dim 1.9117(1.9483) | Xent 0.0000(0.0000) | Loss 1.9117(1.9483) | Error 0.0000(0.0000) Steps 680(665.63) | Grad Norm 0.8588(2.8947) | Total Time 10.00(10.00)\n",
      "Iter 2998 | Time 77.8754(77.4319) | Bit/dim 1.9167(1.9474) | Xent 0.0000(0.0000) | Loss 1.9167(1.9474) | Error 0.0000(0.0000) Steps 674(665.88) | Grad Norm 0.8390(2.8330) | Total Time 10.00(10.00)\n",
      "Iter 2999 | Time 80.0456(77.5103) | Bit/dim 1.9183(1.9465) | Xent 0.0000(0.0000) | Loss 1.9183(1.9465) | Error 0.0000(0.0000) Steps 668(665.95) | Grad Norm 0.8672(2.7740) | Total Time 10.00(10.00)\n",
      "Iter 3000 | Time 79.8562(77.5807) | Bit/dim 1.9121(1.9455) | Xent 0.0000(0.0000) | Loss 1.9121(1.9455) | Error 0.0000(0.0000) Steps 680(666.37) | Grad Norm 0.8614(2.7167) | Total Time 10.00(10.00)\n",
      "Iter 3001 | Time 77.5305(77.5792) | Bit/dim 1.9136(1.9445) | Xent 0.0000(0.0000) | Loss 1.9136(1.9445) | Error 0.0000(0.0000) Steps 668(666.42) | Grad Norm 0.9301(2.6631) | Total Time 10.00(10.00)\n",
      "Iter 3002 | Time 78.4080(77.6040) | Bit/dim 1.9177(1.9437) | Xent 0.0000(0.0000) | Loss 1.9177(1.9437) | Error 0.0000(0.0000) Steps 668(666.46) | Grad Norm 0.8432(2.6085) | Total Time 10.00(10.00)\n",
      "Iter 3003 | Time 79.3980(77.6578) | Bit/dim 1.9116(1.9427) | Xent 0.0000(0.0000) | Loss 1.9116(1.9427) | Error 0.0000(0.0000) Steps 668(666.51) | Grad Norm 0.8188(2.5548) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 20.9104, Epoch Time 584.1393(595.0145), Bit/dim 1.9077(best: 1.9132), Xent 0.0000, Loss 1.9077, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3004 | Time 79.5019(77.7132) | Bit/dim 1.9201(1.9421) | Xent 0.0000(0.0000) | Loss 1.9201(1.9421) | Error 0.0000(0.0000) Steps 680(666.92) | Grad Norm 0.8381(2.5033) | Total Time 10.00(10.00)\n",
      "Iter 3005 | Time 79.4934(77.7666) | Bit/dim 1.9114(1.9411) | Xent 0.0000(0.0000) | Loss 1.9114(1.9411) | Error 0.0000(0.0000) Steps 674(667.13) | Grad Norm 0.8310(2.4531) | Total Time 10.00(10.00)\n",
      "Iter 3006 | Time 80.7904(77.8573) | Bit/dim 1.9033(1.9400) | Xent 0.0000(0.0000) | Loss 1.9033(1.9400) | Error 0.0000(0.0000) Steps 680(667.51) | Grad Norm 0.8264(2.4043) | Total Time 10.00(10.00)\n",
      "Iter 3007 | Time 78.9394(77.8898) | Bit/dim 1.9128(1.9392) | Xent 0.0000(0.0000) | Loss 1.9128(1.9392) | Error 0.0000(0.0000) Steps 668(667.53) | Grad Norm 0.9606(2.3610) | Total Time 10.00(10.00)\n",
      "Iter 3008 | Time 82.1669(78.0181) | Bit/dim 1.9037(1.9381) | Xent 0.0000(0.0000) | Loss 1.9037(1.9381) | Error 0.0000(0.0000) Steps 668(667.54) | Grad Norm 0.9178(2.3177) | Total Time 10.00(10.00)\n",
      "Iter 3009 | Time 77.8203(78.0121) | Bit/dim 1.9107(1.9373) | Xent 0.0000(0.0000) | Loss 1.9107(1.9373) | Error 0.0000(0.0000) Steps 686(668.10) | Grad Norm 0.8473(2.2736) | Total Time 10.00(10.00)\n",
      "Iter 3010 | Time 81.1681(78.1068) | Bit/dim 1.9114(1.9365) | Xent 0.0000(0.0000) | Loss 1.9114(1.9365) | Error 0.0000(0.0000) Steps 680(668.45) | Grad Norm 0.8900(2.2321) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 21.1447, Epoch Time 593.7769(594.9774), Bit/dim 1.9004(best: 1.9077), Xent 0.0000, Loss 1.9004, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3011 | Time 78.9992(78.1336) | Bit/dim 1.9092(1.9357) | Xent 0.0000(0.0000) | Loss 1.9092(1.9357) | Error 0.0000(0.0000) Steps 668(668.44) | Grad Norm 0.8016(2.1892) | Total Time 10.00(10.00)\n",
      "Iter 3012 | Time 78.2728(78.1378) | Bit/dim 1.9025(1.9347) | Xent 0.0000(0.0000) | Loss 1.9025(1.9347) | Error 0.0000(0.0000) Steps 680(668.79) | Grad Norm 1.0160(2.1540) | Total Time 10.00(10.00)\n",
      "Iter 3013 | Time 78.5978(78.1516) | Bit/dim 1.9071(1.9339) | Xent 0.0000(0.0000) | Loss 1.9071(1.9339) | Error 0.0000(0.0000) Steps 668(668.76) | Grad Norm 0.8220(2.1140) | Total Time 10.00(10.00)\n",
      "Iter 3014 | Time 80.3187(78.2166) | Bit/dim 1.9066(1.9331) | Xent 0.0000(0.0000) | Loss 1.9066(1.9331) | Error 0.0000(0.0000) Steps 668(668.74) | Grad Norm 0.8048(2.0747) | Total Time 10.00(10.00)\n",
      "Iter 3015 | Time 77.5788(78.1974) | Bit/dim 1.9006(1.9321) | Xent 0.0000(0.0000) | Loss 1.9006(1.9321) | Error 0.0000(0.0000) Steps 674(668.90) | Grad Norm 0.8332(2.0375) | Total Time 10.00(10.00)\n",
      "Iter 3016 | Time 77.3372(78.1716) | Bit/dim 1.9034(1.9312) | Xent 0.0000(0.0000) | Loss 1.9034(1.9312) | Error 0.0000(0.0000) Steps 668(668.87) | Grad Norm 0.7821(1.9998) | Total Time 10.00(10.00)\n",
      "Iter 3017 | Time 78.9071(78.1937) | Bit/dim 1.9027(1.9304) | Xent 0.0000(0.0000) | Loss 1.9027(1.9304) | Error 0.0000(0.0000) Steps 674(669.02) | Grad Norm 0.8550(1.9655) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 21.1712, Epoch Time 583.8318(594.6430), Bit/dim 1.8954(best: 1.9004), Xent 0.0000, Loss 1.8954, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3018 | Time 79.1991(78.2239) | Bit/dim 1.9004(1.9295) | Xent 0.0000(0.0000) | Loss 1.9004(1.9295) | Error 0.0000(0.0000) Steps 668(668.99) | Grad Norm 0.8501(1.9320) | Total Time 10.00(10.00)\n",
      "Iter 3019 | Time 78.7502(78.2397) | Bit/dim 1.9001(1.9286) | Xent 0.0000(0.0000) | Loss 1.9001(1.9286) | Error 0.0000(0.0000) Steps 680(669.32) | Grad Norm 0.7739(1.8973) | Total Time 10.00(10.00)\n",
      "Iter 3020 | Time 78.2897(78.2412) | Bit/dim 1.9003(1.9277) | Xent 0.0000(0.0000) | Loss 1.9003(1.9277) | Error 0.0000(0.0000) Steps 686(669.82) | Grad Norm 0.9468(1.8688) | Total Time 10.00(10.00)\n",
      "Iter 3021 | Time 79.7641(78.2868) | Bit/dim 1.8990(1.9269) | Xent 0.0000(0.0000) | Loss 1.8990(1.9269) | Error 0.0000(0.0000) Steps 680(670.13) | Grad Norm 0.7980(1.8366) | Total Time 10.00(10.00)\n",
      "Iter 3022 | Time 79.1186(78.3118) | Bit/dim 1.8975(1.9260) | Xent 0.0000(0.0000) | Loss 1.8975(1.9260) | Error 0.0000(0.0000) Steps 686(670.61) | Grad Norm 0.8302(1.8065) | Total Time 10.00(10.00)\n",
      "Iter 3023 | Time 78.2350(78.3095) | Bit/dim 1.8959(1.9251) | Xent 0.0000(0.0000) | Loss 1.8959(1.9251) | Error 0.0000(0.0000) Steps 680(670.89) | Grad Norm 0.8075(1.7765) | Total Time 10.00(10.00)\n",
      "Iter 3024 | Time 79.3931(78.3420) | Bit/dim 1.8935(1.9242) | Xent 0.0000(0.0000) | Loss 1.8935(1.9242) | Error 0.0000(0.0000) Steps 680(671.16) | Grad Norm 1.0398(1.7544) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 20.7215, Epoch Time 586.4044(594.3958), Bit/dim 1.8890(best: 1.8954), Xent 0.0000, Loss 1.8890, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3025 | Time 78.0484(78.3332) | Bit/dim 1.8935(1.9232) | Xent 0.0000(0.0000) | Loss 1.8935(1.9232) | Error 0.0000(0.0000) Steps 680(671.43) | Grad Norm 0.9147(1.7292) | Total Time 10.00(10.00)\n",
      "Iter 3026 | Time 81.1506(78.4177) | Bit/dim 1.8913(1.9223) | Xent 0.0000(0.0000) | Loss 1.8913(1.9223) | Error 0.0000(0.0000) Steps 692(672.04) | Grad Norm 0.7824(1.7008) | Total Time 10.00(10.00)\n",
      "Iter 3027 | Time 79.0367(78.4363) | Bit/dim 1.8922(1.9214) | Xent 0.0000(0.0000) | Loss 1.8922(1.9214) | Error 0.0000(0.0000) Steps 692(672.64) | Grad Norm 0.7750(1.6730) | Total Time 10.00(10.00)\n",
      "Iter 3028 | Time 80.8172(78.5077) | Bit/dim 1.8905(1.9204) | Xent 0.0000(0.0000) | Loss 1.8905(1.9204) | Error 0.0000(0.0000) Steps 692(673.22) | Grad Norm 1.0153(1.6533) | Total Time 10.00(10.00)\n",
      "Iter 3029 | Time 78.9099(78.5198) | Bit/dim 1.8949(1.9197) | Xent 0.0000(0.0000) | Loss 1.8949(1.9197) | Error 0.0000(0.0000) Steps 674(673.25) | Grad Norm 1.0331(1.6347) | Total Time 10.00(10.00)\n",
      "Iter 3030 | Time 79.6421(78.5534) | Bit/dim 1.8926(1.9189) | Xent 0.0000(0.0000) | Loss 1.8926(1.9189) | Error 0.0000(0.0000) Steps 686(673.63) | Grad Norm 0.7582(1.6084) | Total Time 10.00(10.00)\n",
      "Iter 3031 | Time 80.9762(78.6261) | Bit/dim 1.8942(1.9181) | Xent 0.0000(0.0000) | Loss 1.8942(1.9181) | Error 0.0000(0.0000) Steps 686(674.00) | Grad Norm 1.0223(1.5908) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 20.9804, Epoch Time 592.1084(594.3272), Bit/dim 1.8830(best: 1.8890), Xent 0.0000, Loss 1.8830, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3032 | Time 80.0648(78.6693) | Bit/dim 1.8863(1.9172) | Xent 0.0000(0.0000) | Loss 1.8863(1.9172) | Error 0.0000(0.0000) Steps 692(674.54) | Grad Norm 0.7637(1.5660) | Total Time 10.00(10.00)\n",
      "Iter 3033 | Time 80.7458(78.7316) | Bit/dim 1.8862(1.9162) | Xent 0.0000(0.0000) | Loss 1.8862(1.9162) | Error 0.0000(0.0000) Steps 692(675.06) | Grad Norm 0.8583(1.5448) | Total Time 10.00(10.00)\n",
      "Iter 3034 | Time 78.7360(78.7317) | Bit/dim 1.8884(1.9154) | Xent 0.0000(0.0000) | Loss 1.8884(1.9154) | Error 0.0000(0.0000) Steps 692(675.57) | Grad Norm 0.7537(1.5210) | Total Time 10.00(10.00)\n",
      "Iter 3035 | Time 79.1487(78.7442) | Bit/dim 1.8885(1.9146) | Xent 0.0000(0.0000) | Loss 1.8885(1.9146) | Error 0.0000(0.0000) Steps 692(676.06) | Grad Norm 1.1959(1.5113) | Total Time 10.00(10.00)\n",
      "Iter 3036 | Time 82.3806(78.8533) | Bit/dim 1.8914(1.9139) | Xent 0.0000(0.0000) | Loss 1.8914(1.9139) | Error 0.0000(0.0000) Steps 692(676.54) | Grad Norm 1.0247(1.4967) | Total Time 10.00(10.00)\n",
      "Iter 3037 | Time 79.3921(78.8695) | Bit/dim 1.8894(1.9132) | Xent 0.0000(0.0000) | Loss 1.8894(1.9132) | Error 0.0000(0.0000) Steps 692(677.01) | Grad Norm 0.7688(1.4748) | Total Time 10.00(10.00)\n",
      "Iter 3038 | Time 78.1064(78.8466) | Bit/dim 1.8835(1.9123) | Xent 0.0000(0.0000) | Loss 1.8835(1.9123) | Error 0.0000(0.0000) Steps 692(677.46) | Grad Norm 1.0653(1.4626) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 21.3043, Epoch Time 592.6916(594.2781), Bit/dim 1.8782(best: 1.8830), Xent 0.0000, Loss 1.8782, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3039 | Time 80.1854(78.8868) | Bit/dim 1.8813(1.9113) | Xent 0.0000(0.0000) | Loss 1.8813(1.9113) | Error 0.0000(0.0000) Steps 692(677.89) | Grad Norm 0.7695(1.4418) | Total Time 10.00(10.00)\n",
      "Iter 3040 | Time 78.1853(78.8657) | Bit/dim 1.8841(1.9105) | Xent 0.0000(0.0000) | Loss 1.8841(1.9105) | Error 0.0000(0.0000) Steps 692(678.32) | Grad Norm 0.9568(1.4272) | Total Time 10.00(10.00)\n",
      "Iter 3041 | Time 78.5159(78.8552) | Bit/dim 1.8929(1.9100) | Xent 0.0000(0.0000) | Loss 1.8929(1.9100) | Error 0.0000(0.0000) Steps 698(678.91) | Grad Norm 1.0082(1.4146) | Total Time 10.00(10.00)\n",
      "Iter 3042 | Time 78.9393(78.8577) | Bit/dim 1.8810(1.9091) | Xent 0.0000(0.0000) | Loss 1.8810(1.9091) | Error 0.0000(0.0000) Steps 692(679.30) | Grad Norm 0.8145(1.3966) | Total Time 10.00(10.00)\n",
      "Iter 3043 | Time 79.5166(78.8775) | Bit/dim 1.8766(1.9082) | Xent 0.0000(0.0000) | Loss 1.8766(1.9082) | Error 0.0000(0.0000) Steps 692(679.68) | Grad Norm 0.9953(1.3846) | Total Time 10.00(10.00)\n",
      "Iter 3044 | Time 80.8483(78.9366) | Bit/dim 1.8797(1.9073) | Xent 0.0000(0.0000) | Loss 1.8797(1.9073) | Error 0.0000(0.0000) Steps 692(680.05) | Grad Norm 0.7573(1.3658) | Total Time 10.00(10.00)\n",
      "Iter 3045 | Time 79.7819(78.9620) | Bit/dim 1.8792(1.9065) | Xent 0.0000(0.0000) | Loss 1.8792(1.9065) | Error 0.0000(0.0000) Steps 698(680.59) | Grad Norm 1.0403(1.3560) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 21.4976, Epoch Time 590.0061(594.1500), Bit/dim 1.8732(best: 1.8782), Xent 0.0000, Loss 1.8732, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3046 | Time 78.3386(78.9433) | Bit/dim 1.8790(1.9056) | Xent 0.0000(0.0000) | Loss 1.8790(1.9056) | Error 0.0000(0.0000) Steps 692(680.93) | Grad Norm 0.8840(1.3419) | Total Time 10.00(10.00)\n",
      "Iter 3047 | Time 78.3843(78.9265) | Bit/dim 1.8809(1.9049) | Xent 0.0000(0.0000) | Loss 1.8809(1.9049) | Error 0.0000(0.0000) Steps 692(681.26) | Grad Norm 0.7461(1.3240) | Total Time 10.00(10.00)\n",
      "Iter 3048 | Time 79.0751(78.9310) | Bit/dim 1.8753(1.9040) | Xent 0.0000(0.0000) | Loss 1.8753(1.9040) | Error 0.0000(0.0000) Steps 698(681.76) | Grad Norm 1.1543(1.3189) | Total Time 10.00(10.00)\n",
      "Iter 3049 | Time 79.3447(78.9434) | Bit/dim 1.8717(1.9030) | Xent 0.0000(0.0000) | Loss 1.8717(1.9030) | Error 0.0000(0.0000) Steps 692(682.07) | Grad Norm 1.0235(1.3100) | Total Time 10.00(10.00)\n",
      "Iter 3050 | Time 80.6840(78.9956) | Bit/dim 1.8762(1.9022) | Xent 0.0000(0.0000) | Loss 1.8762(1.9022) | Error 0.0000(0.0000) Steps 692(682.37) | Grad Norm 0.7721(1.2939) | Total Time 10.00(10.00)\n",
      "Iter 3051 | Time 80.3119(79.0351) | Bit/dim 1.8778(1.9015) | Xent 0.0000(0.0000) | Loss 1.8778(1.9015) | Error 0.0000(0.0000) Steps 698(682.84) | Grad Norm 1.3513(1.2956) | Total Time 10.00(10.00)\n",
      "Iter 3052 | Time 78.7058(79.0252) | Bit/dim 1.8744(1.9007) | Xent 0.0000(0.0000) | Loss 1.8744(1.9007) | Error 0.0000(0.0000) Steps 692(683.11) | Grad Norm 0.8735(1.2830) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 21.5329, Epoch Time 589.0047(593.9956), Bit/dim 1.8688(best: 1.8732), Xent 0.0000, Loss 1.8688, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3053 | Time 77.7597(78.9872) | Bit/dim 1.8675(1.8997) | Xent 0.0000(0.0000) | Loss 1.8675(1.8997) | Error 0.0000(0.0000) Steps 692(683.38) | Grad Norm 0.8156(1.2689) | Total Time 10.00(10.00)\n",
      "Iter 3054 | Time 80.3664(79.0286) | Bit/dim 1.8777(1.8990) | Xent 0.0000(0.0000) | Loss 1.8777(1.8990) | Error 0.0000(0.0000) Steps 692(683.64) | Grad Norm 0.8468(1.2563) | Total Time 10.00(10.00)\n",
      "Iter 3055 | Time 81.1553(79.0924) | Bit/dim 1.8691(1.8981) | Xent 0.0000(0.0000) | Loss 1.8691(1.8981) | Error 0.0000(0.0000) Steps 692(683.89) | Grad Norm 0.9449(1.2469) | Total Time 10.00(10.00)\n",
      "Iter 3056 | Time 78.8218(79.0843) | Bit/dim 1.8761(1.8975) | Xent 0.0000(0.0000) | Loss 1.8761(1.8975) | Error 0.0000(0.0000) Steps 692(684.13) | Grad Norm 1.3526(1.2501) | Total Time 10.00(10.00)\n",
      "Iter 3057 | Time 79.3583(79.0925) | Bit/dim 1.8708(1.8967) | Xent 0.0000(0.0000) | Loss 1.8708(1.8967) | Error 0.0000(0.0000) Steps 692(684.37) | Grad Norm 1.1778(1.2479) | Total Time 10.00(10.00)\n",
      "Iter 3058 | Time 78.1845(79.0653) | Bit/dim 1.8705(1.8959) | Xent 0.0000(0.0000) | Loss 1.8705(1.8959) | Error 0.0000(0.0000) Steps 692(684.60) | Grad Norm 0.7181(1.2320) | Total Time 10.00(10.00)\n",
      "Iter 3059 | Time 79.5727(79.0805) | Bit/dim 1.8705(1.8951) | Xent 0.0000(0.0000) | Loss 1.8705(1.8951) | Error 0.0000(0.0000) Steps 692(684.82) | Grad Norm 0.8779(1.2214) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 21.6792, Epoch Time 589.5232(593.8614), Bit/dim 1.8631(best: 1.8688), Xent 0.0000, Loss 1.8631, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3060 | Time 79.1271(79.0819) | Bit/dim 1.8687(1.8943) | Xent 0.0000(0.0000) | Loss 1.8687(1.8943) | Error 0.0000(0.0000) Steps 692(685.04) | Grad Norm 0.8982(1.2117) | Total Time 10.00(10.00)\n",
      "Iter 3061 | Time 80.9074(79.1367) | Bit/dim 1.8690(1.8936) | Xent 0.0000(0.0000) | Loss 1.8690(1.8936) | Error 0.0000(0.0000) Steps 692(685.24) | Grad Norm 0.7586(1.1981) | Total Time 10.00(10.00)\n",
      "Iter 3062 | Time 81.7535(79.2152) | Bit/dim 1.8709(1.8929) | Xent 0.0000(0.0000) | Loss 1.8709(1.8929) | Error 0.0000(0.0000) Steps 692(685.45) | Grad Norm 0.9528(1.1908) | Total Time 10.00(10.00)\n",
      "Iter 3063 | Time 79.3586(79.2195) | Bit/dim 1.8618(1.8920) | Xent 0.0000(0.0000) | Loss 1.8618(1.8920) | Error 0.0000(0.0000) Steps 692(685.64) | Grad Norm 1.0632(1.1869) | Total Time 10.00(10.00)\n",
      "Iter 3064 | Time 79.5897(79.2306) | Bit/dim 1.8658(1.8912) | Xent 0.0000(0.0000) | Loss 1.8658(1.8912) | Error 0.0000(0.0000) Steps 692(685.83) | Grad Norm 0.7070(1.1725) | Total Time 10.00(10.00)\n",
      "Iter 3065 | Time 80.1208(79.2573) | Bit/dim 1.8666(1.8904) | Xent 0.0000(0.0000) | Loss 1.8666(1.8904) | Error 0.0000(0.0000) Steps 692(686.02) | Grad Norm 0.9542(1.1660) | Total Time 10.00(10.00)\n",
      "Iter 3066 | Time 79.7841(79.2731) | Bit/dim 1.8633(1.8896) | Xent 0.0000(0.0000) | Loss 1.8633(1.8896) | Error 0.0000(0.0000) Steps 692(686.20) | Grad Norm 1.0099(1.1613) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 21.4680, Epoch Time 594.7036(593.8867), Bit/dim 1.8587(best: 1.8631), Xent 0.0000, Loss 1.8587, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3067 | Time 78.4258(79.2477) | Bit/dim 1.8636(1.8888) | Xent 0.0000(0.0000) | Loss 1.8636(1.8888) | Error 0.0000(0.0000) Steps 692(686.37) | Grad Norm 0.8138(1.1509) | Total Time 10.00(10.00)\n",
      "Iter 3068 | Time 81.9346(79.3283) | Bit/dim 1.8626(1.8881) | Xent 0.0000(0.0000) | Loss 1.8626(1.8881) | Error 0.0000(0.0000) Steps 692(686.54) | Grad Norm 1.5524(1.1629) | Total Time 10.00(10.00)\n",
      "Iter 3069 | Time 82.0157(79.4089) | Bit/dim 1.8604(1.8872) | Xent 0.0000(0.0000) | Loss 1.8604(1.8872) | Error 0.0000(0.0000) Steps 698(686.89) | Grad Norm 1.6163(1.1765) | Total Time 10.00(10.00)\n",
      "Iter 3070 | Time 79.7122(79.4180) | Bit/dim 1.8598(1.8864) | Xent 0.0000(0.0000) | Loss 1.8598(1.8864) | Error 0.0000(0.0000) Steps 692(687.04) | Grad Norm 0.7188(1.1628) | Total Time 10.00(10.00)\n",
      "Iter 3071 | Time 79.9965(79.4354) | Bit/dim 1.8613(1.8856) | Xent 0.0000(0.0000) | Loss 1.8613(1.8856) | Error 0.0000(0.0000) Steps 692(687.19) | Grad Norm 1.2949(1.1668) | Total Time 10.00(10.00)\n",
      "Iter 3072 | Time 79.3994(79.4343) | Bit/dim 1.8574(1.8848) | Xent 0.0000(0.0000) | Loss 1.8574(1.8848) | Error 0.0000(0.0000) Steps 692(687.33) | Grad Norm 1.3984(1.1737) | Total Time 10.00(10.00)\n",
      "Iter 3073 | Time 81.1729(79.4864) | Bit/dim 1.8598(1.8841) | Xent 0.0000(0.0000) | Loss 1.8598(1.8841) | Error 0.0000(0.0000) Steps 692(687.47) | Grad Norm 0.7933(1.1623) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 21.2237, Epoch Time 596.7136(593.9715), Bit/dim 1.8531(best: 1.8587), Xent 0.0000, Loss 1.8531, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3074 | Time 79.8795(79.4982) | Bit/dim 1.8573(1.8833) | Xent 0.0000(0.0000) | Loss 1.8573(1.8833) | Error 0.0000(0.0000) Steps 692(687.61) | Grad Norm 0.8406(1.1526) | Total Time 10.00(10.00)\n",
      "Iter 3075 | Time 77.8867(79.4499) | Bit/dim 1.8596(1.8825) | Xent 0.0000(0.0000) | Loss 1.8596(1.8825) | Error 0.0000(0.0000) Steps 692(687.74) | Grad Norm 1.3695(1.1592) | Total Time 10.00(10.00)\n",
      "Iter 3076 | Time 78.3591(79.4172) | Bit/dim 1.8582(1.8818) | Xent 0.0000(0.0000) | Loss 1.8582(1.8818) | Error 0.0000(0.0000) Steps 692(687.87) | Grad Norm 0.9459(1.1528) | Total Time 10.00(10.00)\n",
      "Iter 3077 | Time 81.7740(79.4879) | Bit/dim 1.8597(1.8811) | Xent 0.0000(0.0000) | Loss 1.8597(1.8811) | Error 0.0000(0.0000) Steps 692(687.99) | Grad Norm 0.7559(1.1408) | Total Time 10.00(10.00)\n",
      "Iter 3078 | Time 79.7368(79.4953) | Bit/dim 1.8544(1.8803) | Xent 0.0000(0.0000) | Loss 1.8544(1.8803) | Error 0.0000(0.0000) Steps 692(688.11) | Grad Norm 1.2941(1.1454) | Total Time 10.00(10.00)\n",
      "Iter 3079 | Time 81.0605(79.5423) | Bit/dim 1.8548(1.8796) | Xent 0.0000(0.0000) | Loss 1.8548(1.8796) | Error 0.0000(0.0000) Steps 692(688.23) | Grad Norm 0.9334(1.1391) | Total Time 10.00(10.00)\n",
      "Iter 3080 | Time 81.9895(79.6157) | Bit/dim 1.8552(1.8788) | Xent 0.0000(0.0000) | Loss 1.8552(1.8788) | Error 0.0000(0.0000) Steps 692(688.34) | Grad Norm 0.9383(1.1331) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 21.5916, Epoch Time 594.7326(593.9943), Bit/dim 1.8469(best: 1.8531), Xent 0.0000, Loss 1.8469, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3081 | Time 80.3195(79.6368) | Bit/dim 1.8516(1.8780) | Xent 0.0000(0.0000) | Loss 1.8516(1.8780) | Error 0.0000(0.0000) Steps 698(688.63) | Grad Norm 1.6636(1.1490) | Total Time 10.00(10.00)\n",
      "Iter 3082 | Time 80.0230(79.6484) | Bit/dim 1.8524(1.8773) | Xent 0.0000(0.0000) | Loss 1.8524(1.8773) | Error 0.0000(0.0000) Steps 686(688.55) | Grad Norm 1.0612(1.1463) | Total Time 10.00(10.00)\n",
      "Iter 3083 | Time 80.3356(79.6690) | Bit/dim 1.8542(1.8766) | Xent 0.0000(0.0000) | Loss 1.8542(1.8766) | Error 0.0000(0.0000) Steps 686(688.48) | Grad Norm 0.8812(1.1384) | Total Time 10.00(10.00)\n",
      "Iter 3084 | Time 80.4490(79.6924) | Bit/dim 1.8553(1.8759) | Xent 0.0000(0.0000) | Loss 1.8553(1.8759) | Error 0.0000(0.0000) Steps 692(688.58) | Grad Norm 2.5064(1.1794) | Total Time 10.00(10.00)\n",
      "Iter 3085 | Time 79.9924(79.7014) | Bit/dim 1.8501(1.8752) | Xent 0.0000(0.0000) | Loss 1.8501(1.8752) | Error 0.0000(0.0000) Steps 692(688.68) | Grad Norm 2.9608(1.2329) | Total Time 10.00(10.00)\n",
      "Iter 3086 | Time 79.7679(79.7034) | Bit/dim 1.8464(1.8743) | Xent 0.0000(0.0000) | Loss 1.8464(1.8743) | Error 0.0000(0.0000) Steps 692(688.78) | Grad Norm 2.3618(1.2667) | Total Time 10.00(10.00)\n",
      "Iter 3087 | Time 79.9194(79.7099) | Bit/dim 1.8501(1.8736) | Xent 0.0000(0.0000) | Loss 1.8501(1.8736) | Error 0.0000(0.0000) Steps 692(688.88) | Grad Norm 1.0233(1.2594) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 21.7506, Epoch Time 595.2369(594.0316), Bit/dim 1.8422(best: 1.8469), Xent 0.0000, Loss 1.8422, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3088 | Time 79.3367(79.6987) | Bit/dim 1.8472(1.8728) | Xent 0.0000(0.0000) | Loss 1.8472(1.8728) | Error 0.0000(0.0000) Steps 692(688.97) | Grad Norm 1.0165(1.2521) | Total Time 10.00(10.00)\n",
      "Iter 3089 | Time 80.2078(79.7140) | Bit/dim 1.8398(1.8718) | Xent 0.0000(0.0000) | Loss 1.8398(1.8718) | Error 0.0000(0.0000) Steps 692(689.06) | Grad Norm 2.5793(1.2920) | Total Time 10.00(10.00)\n",
      "Iter 3090 | Time 82.4966(79.7974) | Bit/dim 1.8514(1.8712) | Xent 0.0000(0.0000) | Loss 1.8514(1.8712) | Error 0.0000(0.0000) Steps 692(689.15) | Grad Norm 3.2424(1.3505) | Total Time 10.00(10.00)\n",
      "Iter 3091 | Time 79.8422(79.7988) | Bit/dim 1.8486(1.8705) | Xent 0.0000(0.0000) | Loss 1.8486(1.8705) | Error 0.0000(0.0000) Steps 698(689.42) | Grad Norm 2.7864(1.3936) | Total Time 10.00(10.00)\n",
      "Iter 3092 | Time 80.5250(79.8206) | Bit/dim 1.8500(1.8699) | Xent 0.0000(0.0000) | Loss 1.8500(1.8699) | Error 0.0000(0.0000) Steps 686(689.32) | Grad Norm 0.8210(1.3764) | Total Time 10.00(10.00)\n",
      "Iter 3093 | Time 78.6703(79.7861) | Bit/dim 1.8473(1.8692) | Xent 0.0000(0.0000) | Loss 1.8473(1.8692) | Error 0.0000(0.0000) Steps 686(689.22) | Grad Norm 1.5875(1.3827) | Total Time 10.00(10.00)\n",
      "Iter 3094 | Time 81.7413(79.8447) | Bit/dim 1.8434(1.8684) | Xent 0.0000(0.0000) | Loss 1.8434(1.8684) | Error 0.0000(0.0000) Steps 692(689.30) | Grad Norm 2.7227(1.4229) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 21.2018, Epoch Time 596.8894(594.1174), Bit/dim 1.8371(best: 1.8422), Xent 0.0000, Loss 1.8371, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3095 | Time 79.3904(79.8311) | Bit/dim 1.8409(1.8676) | Xent 0.0000(0.0000) | Loss 1.8409(1.8676) | Error 0.0000(0.0000) Steps 686(689.20) | Grad Norm 2.7337(1.4622) | Total Time 10.00(10.00)\n",
      "Iter 3096 | Time 78.1608(79.7810) | Bit/dim 1.8392(1.8667) | Xent 0.0000(0.0000) | Loss 1.8392(1.8667) | Error 0.0000(0.0000) Steps 686(689.10) | Grad Norm 1.9711(1.4775) | Total Time 10.00(10.00)\n",
      "Iter 3097 | Time 78.6315(79.7465) | Bit/dim 1.8398(1.8659) | Xent 0.0000(0.0000) | Loss 1.8398(1.8659) | Error 0.0000(0.0000) Steps 686(689.01) | Grad Norm 0.6930(1.4540) | Total Time 10.00(10.00)\n",
      "Iter 3100 | Time 78.8419(79.7409) | Bit/dim 1.8362(1.8638) | Xent 0.0000(0.0000) | Loss 1.8362(1.8638) | Error 0.0000(0.0000) Steps 686(688.92) | Grad Norm 2.8807(1.5695) | Total Time 10.00(10.00)\n",
      "Iter 3101 | Time 80.1691(79.7538) | Bit/dim 1.8467(1.8633) | Xent 0.0000(0.0000) | Loss 1.8467(1.8633) | Error 0.0000(0.0000) Steps 686(688.84) | Grad Norm 1.2046(1.5586) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 21.0807, Epoch Time 589.2254(593.9706), Bit/dim 1.8333(best: 1.8371), Xent 0.0000, Loss 1.8333, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3102 | Time 78.4260(79.7139) | Bit/dim 1.8421(1.8627) | Xent 0.0000(0.0000) | Loss 1.8421(1.8627) | Error 0.0000(0.0000) Steps 686(688.75) | Grad Norm 1.6917(1.5626) | Total Time 10.00(10.00)\n",
      "Iter 3103 | Time 80.0527(79.7241) | Bit/dim 1.8325(1.8618) | Xent 0.0000(0.0000) | Loss 1.8325(1.8618) | Error 0.0000(0.0000) Steps 686(688.67) | Grad Norm 3.1949(1.6115) | Total Time 10.00(10.00)\n",
      "Iter 3104 | Time 78.7655(79.6953) | Bit/dim 1.8373(1.8611) | Xent 0.0000(0.0000) | Loss 1.8373(1.8611) | Error 0.0000(0.0000) Steps 692(688.77) | Grad Norm 3.4701(1.6673) | Total Time 10.00(10.00)\n",
      "Iter 3105 | Time 80.1689(79.7095) | Bit/dim 1.8313(1.8602) | Xent 0.0000(0.0000) | Loss 1.8313(1.8602) | Error 0.0000(0.0000) Steps 686(688.68) | Grad Norm 2.4912(1.6920) | Total Time 10.00(10.00)\n",
      "Iter 3106 | Time 79.6500(79.7077) | Bit/dim 1.8336(1.8594) | Xent 0.0000(0.0000) | Loss 1.8336(1.8594) | Error 0.0000(0.0000) Steps 692(688.78) | Grad Norm 1.1549(1.6759) | Total Time 10.00(10.00)\n",
      "Iter 3107 | Time 79.8373(79.7116) | Bit/dim 1.8440(1.8589) | Xent 0.0000(0.0000) | Loss 1.8440(1.8589) | Error 0.0000(0.0000) Steps 686(688.70) | Grad Norm 1.4320(1.6686) | Total Time 10.00(10.00)\n",
      "Iter 3108 | Time 79.0809(79.6927) | Bit/dim 1.8334(1.8581) | Xent 0.0000(0.0000) | Loss 1.8334(1.8581) | Error 0.0000(0.0000) Steps 686(688.62) | Grad Norm 3.2358(1.7156) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 21.1589, Epoch Time 589.7429(593.8438), Bit/dim 1.8269(best: 1.8333), Xent 0.0000, Loss 1.8269, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3109 | Time 79.0575(79.6737) | Bit/dim 1.8363(1.8575) | Xent 0.0000(0.0000) | Loss 1.8363(1.8575) | Error 0.0000(0.0000) Steps 692(688.72) | Grad Norm 3.9176(1.7817) | Total Time 10.00(10.00)\n",
      "Iter 3110 | Time 77.8291(79.6183) | Bit/dim 1.8314(1.8567) | Xent 0.0000(0.0000) | Loss 1.8314(1.8567) | Error 0.0000(0.0000) Steps 686(688.64) | Grad Norm 2.9691(1.8173) | Total Time 10.00(10.00)\n",
      "Iter 3111 | Time 79.2590(79.6075) | Bit/dim 1.8304(1.8559) | Xent 0.0000(0.0000) | Loss 1.8304(1.8559) | Error 0.0000(0.0000) Steps 686(688.56) | Grad Norm 1.1785(1.7981) | Total Time 10.00(10.00)\n",
      "Iter 3112 | Time 78.9546(79.5879) | Bit/dim 1.8312(1.8552) | Xent 0.0000(0.0000) | Loss 1.8312(1.8552) | Error 0.0000(0.0000) Steps 686(688.48) | Grad Norm 1.0785(1.7765) | Total Time 10.00(10.00)\n",
      "Iter 3113 | Time 78.9702(79.5694) | Bit/dim 1.8321(1.8545) | Xent 0.0000(0.0000) | Loss 1.8321(1.8545) | Error 0.0000(0.0000) Steps 686(688.41) | Grad Norm 2.5179(1.7988) | Total Time 10.00(10.00)\n",
      "Iter 3114 | Time 80.1347(79.5864) | Bit/dim 1.8305(1.8538) | Xent 0.0000(0.0000) | Loss 1.8305(1.8538) | Error 0.0000(0.0000) Steps 692(688.52) | Grad Norm 3.8169(1.8593) | Total Time 10.00(10.00)\n",
      "Iter 3115 | Time 80.6244(79.6175) | Bit/dim 1.8270(1.8530) | Xent 0.0000(0.0000) | Loss 1.8270(1.8530) | Error 0.0000(0.0000) Steps 686(688.44) | Grad Norm 3.9079(1.9208) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 21.4437, Epoch Time 588.8799(593.6949), Bit/dim 1.8209(best: 1.8269), Xent 0.0000, Loss 1.8209, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3116 | Time 79.8258(79.6238) | Bit/dim 1.8254(1.8521) | Xent 0.0000(0.0000) | Loss 1.8254(1.8521) | Error 0.0000(0.0000) Steps 692(688.55) | Grad Norm 3.0818(1.9556) | Total Time 10.00(10.00)\n",
      "Iter 3117 | Time 79.7862(79.6286) | Bit/dim 1.8219(1.8512) | Xent 0.0000(0.0000) | Loss 1.8219(1.8512) | Error 0.0000(0.0000) Steps 686(688.47) | Grad Norm 2.0795(1.9593) | Total Time 10.00(10.00)\n",
      "Iter 3118 | Time 78.2506(79.5873) | Bit/dim 1.8274(1.8505) | Xent 0.0000(0.0000) | Loss 1.8274(1.8505) | Error 0.0000(0.0000) Steps 686(688.40) | Grad Norm 0.9697(1.9296) | Total Time 10.00(10.00)\n",
      "Iter 3119 | Time 77.5740(79.5269) | Bit/dim 1.8274(1.8498) | Xent 0.0000(0.0000) | Loss 1.8274(1.8498) | Error 0.0000(0.0000) Steps 686(688.33) | Grad Norm 0.9280(1.8996) | Total Time 10.00(10.00)\n",
      "Iter 3120 | Time 78.2398(79.4883) | Bit/dim 1.8292(1.8492) | Xent 0.0000(0.0000) | Loss 1.8292(1.8492) | Error 0.0000(0.0000) Steps 686(688.26) | Grad Norm 1.9594(1.9014) | Total Time 10.00(10.00)\n",
      "Iter 3121 | Time 81.6868(79.5542) | Bit/dim 1.8251(1.8485) | Xent 0.0000(0.0000) | Loss 1.8251(1.8485) | Error 0.0000(0.0000) Steps 686(688.19) | Grad Norm 2.9309(1.9323) | Total Time 10.00(10.00)\n",
      "Iter 3122 | Time 77.6421(79.4969) | Bit/dim 1.8254(1.8478) | Xent 0.0000(0.0000) | Loss 1.8254(1.8478) | Error 0.0000(0.0000) Steps 686(688.12) | Grad Norm 3.3812(1.9757) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 21.3061, Epoch Time 586.9091(593.4913), Bit/dim 1.8175(best: 1.8209), Xent 0.0000, Loss 1.8175, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3123 | Time 79.2039(79.4881) | Bit/dim 1.8241(1.8471) | Xent 0.0000(0.0000) | Loss 1.8241(1.8471) | Error 0.0000(0.0000) Steps 692(688.24) | Grad Norm 3.1095(2.0097) | Total Time 10.00(10.00)\n",
      "Iter 3124 | Time 78.7648(79.4664) | Bit/dim 1.8256(1.8464) | Xent 0.0000(0.0000) | Loss 1.8256(1.8464) | Error 0.0000(0.0000) Steps 686(688.17) | Grad Norm 2.4818(2.0239) | Total Time 10.00(10.00)\n",
      "Iter 3125 | Time 79.0265(79.4532) | Bit/dim 1.8252(1.8458) | Xent 0.0000(0.0000) | Loss 1.8252(1.8458) | Error 0.0000(0.0000) Steps 686(688.11) | Grad Norm 1.5656(2.0102) | Total Time 10.00(10.00)\n",
      "Iter 3126 | Time 77.9107(79.4069) | Bit/dim 1.8160(1.8449) | Xent 0.0000(0.0000) | Loss 1.8160(1.8449) | Error 0.0000(0.0000) Steps 686(688.04) | Grad Norm 0.6925(1.9706) | Total Time 10.00(10.00)\n",
      "Iter 3127 | Time 78.9039(79.3918) | Bit/dim 1.8166(1.8440) | Xent 0.0000(0.0000) | Loss 1.8166(1.8440) | Error 0.0000(0.0000) Steps 686(687.98) | Grad Norm 1.2486(1.9490) | Total Time 10.00(10.00)\n",
      "Iter 3128 | Time 80.8126(79.4344) | Bit/dim 1.8198(1.8433) | Xent 0.0000(0.0000) | Loss 1.8198(1.8433) | Error 0.0000(0.0000) Steps 686(687.92) | Grad Norm 2.5057(1.9657) | Total Time 10.00(10.00)\n",
      "Iter 3129 | Time 78.9424(79.4197) | Bit/dim 1.8208(1.8426) | Xent 0.0000(0.0000) | Loss 1.8208(1.8426) | Error 0.0000(0.0000) Steps 686(687.86) | Grad Norm 3.7715(2.0198) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 21.2869, Epoch Time 587.3475(593.3070), Bit/dim 1.8132(best: 1.8175), Xent 0.0000, Loss 1.8132, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3130 | Time 78.5937(79.3949) | Bit/dim 1.8189(1.8419) | Xent 0.0000(0.0000) | Loss 1.8189(1.8419) | Error 0.0000(0.0000) Steps 692(687.99) | Grad Norm 4.7179(2.1008) | Total Time 10.00(10.00)\n",
      "Iter 3131 | Time 79.2188(79.3896) | Bit/dim 1.8152(1.8411) | Xent 0.0000(0.0000) | Loss 1.8152(1.8411) | Error 0.0000(0.0000) Steps 686(687.93) | Grad Norm 5.0334(2.1888) | Total Time 10.00(10.00)\n",
      "Iter 3132 | Time 79.9304(79.4058) | Bit/dim 1.8174(1.8404) | Xent 0.0000(0.0000) | Loss 1.8174(1.8404) | Error 0.0000(0.0000) Steps 692(688.05) | Grad Norm 4.3055(2.2523) | Total Time 10.00(10.00)\n",
      "Iter 3133 | Time 79.6068(79.4119) | Bit/dim 1.8192(1.8398) | Xent 0.0000(0.0000) | Loss 1.8192(1.8398) | Error 0.0000(0.0000) Steps 686(687.99) | Grad Norm 2.5461(2.2611) | Total Time 10.00(10.00)\n",
      "Iter 3134 | Time 80.3040(79.4386) | Bit/dim 1.8195(1.8392) | Xent 0.0000(0.0000) | Loss 1.8195(1.8392) | Error 0.0000(0.0000) Steps 686(687.93) | Grad Norm 0.8697(2.2193) | Total Time 10.00(10.00)\n",
      "Iter 3135 | Time 79.0393(79.4267) | Bit/dim 1.8154(1.8385) | Xent 0.0000(0.0000) | Loss 1.8154(1.8385) | Error 0.0000(0.0000) Steps 674(687.51) | Grad Norm 1.0996(2.1857) | Total Time 10.00(10.00)\n",
      "Iter 3136 | Time 77.8576(79.3796) | Bit/dim 1.8173(1.8378) | Xent 0.0000(0.0000) | Loss 1.8173(1.8378) | Error 0.0000(0.0000) Steps 680(687.29) | Grad Norm 2.4714(2.1943) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 21.2299, Epoch Time 588.4793(593.1621), Bit/dim 1.8093(best: 1.8132), Xent 0.0000, Loss 1.8093, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3137 | Time 78.7098(79.3595) | Bit/dim 1.8168(1.8372) | Xent 0.0000(0.0000) | Loss 1.8168(1.8372) | Error 0.0000(0.0000) Steps 692(687.43) | Grad Norm 3.5408(2.2347) | Total Time 10.00(10.00)\n",
      "Iter 3138 | Time 79.2983(79.3577) | Bit/dim 1.8157(1.8366) | Xent 0.0000(0.0000) | Loss 1.8157(1.8366) | Error 0.0000(0.0000) Steps 680(687.21) | Grad Norm 4.0300(2.2886) | Total Time 10.00(10.00)\n",
      "Iter 3139 | Time 80.6521(79.3965) | Bit/dim 1.8176(1.8360) | Xent 0.0000(0.0000) | Loss 1.8176(1.8360) | Error 0.0000(0.0000) Steps 686(687.17) | Grad Norm 3.9239(2.3376) | Total Time 10.00(10.00)\n",
      "Iter 3140 | Time 79.2422(79.3919) | Bit/dim 1.8122(1.8353) | Xent 0.0000(0.0000) | Loss 1.8122(1.8353) | Error 0.0000(0.0000) Steps 680(686.95) | Grad Norm 3.1886(2.3632) | Total Time 10.00(10.00)\n",
      "Iter 3141 | Time 79.0092(79.3804) | Bit/dim 1.8103(1.8345) | Xent 0.0000(0.0000) | Loss 1.8103(1.8345) | Error 0.0000(0.0000) Steps 680(686.75) | Grad Norm 2.1586(2.3570) | Total Time 10.00(10.00)\n",
      "Iter 3142 | Time 80.2532(79.4066) | Bit/dim 1.8092(1.8338) | Xent 0.0000(0.0000) | Loss 1.8092(1.8338) | Error 0.0000(0.0000) Steps 680(686.54) | Grad Norm 1.4038(2.3284) | Total Time 10.00(10.00)\n",
      "Iter 3143 | Time 81.6480(79.4738) | Bit/dim 1.8101(1.8331) | Xent 0.0000(0.0000) | Loss 1.8101(1.8331) | Error 0.0000(0.0000) Steps 668(685.99) | Grad Norm 0.9704(2.2877) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 21.2220, Epoch Time 592.8266(593.1521), Bit/dim 1.8044(best: 1.8093), Xent 0.0000, Loss 1.8044, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3144 | Time 80.2156(79.4961) | Bit/dim 1.8134(1.8325) | Xent 0.0000(0.0000) | Loss 1.8134(1.8325) | Error 0.0000(0.0000) Steps 680(685.81) | Grad Norm 0.6953(2.2399) | Total Time 10.00(10.00)\n",
      "Iter 3145 | Time 79.1186(79.4847) | Bit/dim 1.8092(1.8318) | Xent 0.0000(0.0000) | Loss 1.8092(1.8318) | Error 0.0000(0.0000) Steps 674(685.45) | Grad Norm 0.6725(2.1929) | Total Time 10.00(10.00)\n",
      "Iter 3146 | Time 78.8034(79.4643) | Bit/dim 1.8132(1.8312) | Xent 0.0000(0.0000) | Loss 1.8132(1.8312) | Error 0.0000(0.0000) Steps 674(685.11) | Grad Norm 1.1145(2.1605) | Total Time 10.00(10.00)\n",
      "Iter 3147 | Time 78.3771(79.4317) | Bit/dim 1.8051(1.8304) | Xent 0.0000(0.0000) | Loss 1.8051(1.8304) | Error 0.0000(0.0000) Steps 680(684.96) | Grad Norm 1.6364(2.1448) | Total Time 10.00(10.00)\n",
      "Iter 3148 | Time 78.2971(79.3976) | Bit/dim 1.8059(1.8297) | Xent 0.0000(0.0000) | Loss 1.8059(1.8297) | Error 0.0000(0.0000) Steps 674(684.63) | Grad Norm 2.3946(2.1523) | Total Time 10.00(10.00)\n",
      "Iter 3149 | Time 77.9423(79.3540) | Bit/dim 1.8045(1.8289) | Xent 0.0000(0.0000) | Loss 1.8045(1.8289) | Error 0.0000(0.0000) Steps 674(684.31) | Grad Norm 3.7653(2.2007) | Total Time 10.00(10.00)\n",
      "Iter 3150 | Time 78.0612(79.3152) | Bit/dim 1.8057(1.8282) | Xent 0.0000(0.0000) | Loss 1.8057(1.8282) | Error 0.0000(0.0000) Steps 680(684.18) | Grad Norm 5.3997(2.2967) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 21.1565, Epoch Time 584.6614(592.8974), Bit/dim 1.8013(best: 1.8044), Xent 0.0000, Loss 1.8013, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3151 | Time 79.6790(79.3261) | Bit/dim 1.8080(1.8276) | Xent 0.0000(0.0000) | Loss 1.8080(1.8276) | Error 0.0000(0.0000) Steps 668(683.69) | Grad Norm 7.3921(2.4495) | Total Time 10.00(10.00)\n",
      "Iter 3152 | Time 79.1240(79.3201) | Bit/dim 1.8044(1.8269) | Xent 0.0000(0.0000) | Loss 1.8044(1.8269) | Error 0.0000(0.0000) Steps 686(683.76) | Grad Norm 9.4720(2.6602) | Total Time 10.00(10.00)\n",
      "Iter 3153 | Time 78.6565(79.3001) | Bit/dim 1.8048(1.8263) | Xent 0.0000(0.0000) | Loss 1.8048(1.8263) | Error 0.0000(0.0000) Steps 680(683.65) | Grad Norm 10.8281(2.9052) | Total Time 10.00(10.00)\n",
      "Iter 3154 | Time 80.5699(79.3382) | Bit/dim 1.8043(1.8256) | Xent 0.0000(0.0000) | Loss 1.8043(1.8256) | Error 0.0000(0.0000) Steps 698(684.08) | Grad Norm 10.2977(3.1270) | Total Time 10.00(10.00)\n",
      "Iter 3155 | Time 78.9916(79.3278) | Bit/dim 1.8033(1.8249) | Xent 0.0000(0.0000) | Loss 1.8033(1.8249) | Error 0.0000(0.0000) Steps 674(683.78) | Grad Norm 7.2313(3.2501) | Total Time 10.00(10.00)\n",
      "Iter 3156 | Time 78.4911(79.3027) | Bit/dim 1.8077(1.8244) | Xent 0.0000(0.0000) | Loss 1.8077(1.8244) | Error 0.0000(0.0000) Steps 674(683.48) | Grad Norm 2.4602(3.2264) | Total Time 10.00(10.00)\n",
      "Iter 3157 | Time 79.7054(79.3148) | Bit/dim 1.8039(1.8238) | Xent 0.0000(0.0000) | Loss 1.8039(1.8238) | Error 0.0000(0.0000) Steps 680(683.38) | Grad Norm 2.4086(3.2019) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 21.3113, Epoch Time 589.3916(592.7922), Bit/dim 1.7960(best: 1.8013), Xent 0.0000, Loss 1.7960, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3158 | Time 76.7244(79.2371) | Bit/dim 1.8070(1.8233) | Xent 0.0000(0.0000) | Loss 1.8070(1.8233) | Error 0.0000(0.0000) Steps 674(683.10) | Grad Norm 5.9110(3.2832) | Total Time 10.00(10.00)\n",
      "Iter 3159 | Time 77.6935(79.1908) | Bit/dim 1.7975(1.8225) | Xent 0.0000(0.0000) | Loss 1.7975(1.8225) | Error 0.0000(0.0000) Steps 680(683.01) | Grad Norm 8.1446(3.4290) | Total Time 10.00(10.00)\n",
      "Iter 3160 | Time 79.4273(79.1979) | Bit/dim 1.8022(1.8219) | Xent 0.0000(0.0000) | Loss 1.8022(1.8219) | Error 0.0000(0.0000) Steps 662(682.38) | Grad Norm 8.1731(3.5713) | Total Time 10.00(10.00)\n",
      "Iter 3161 | Time 78.8511(79.1875) | Bit/dim 1.8051(1.8214) | Xent 0.0000(0.0000) | Loss 1.8051(1.8214) | Error 0.0000(0.0000) Steps 674(682.12) | Grad Norm 5.6089(3.6325) | Total Time 10.00(10.00)\n",
      "Iter 3162 | Time 79.6333(79.2009) | Bit/dim 1.7955(1.8206) | Xent 0.0000(0.0000) | Loss 1.7955(1.8206) | Error 0.0000(0.0000) Steps 674(681.88) | Grad Norm 0.9936(3.5533) | Total Time 10.00(10.00)\n",
      "Iter 3163 | Time 79.6411(79.2141) | Bit/dim 1.7985(1.8200) | Xent 0.0000(0.0000) | Loss 1.7985(1.8200) | Error 0.0000(0.0000) Steps 674(681.64) | Grad Norm 4.7197(3.5883) | Total Time 10.00(10.00)\n",
      "Iter 3164 | Time 77.8916(79.1744) | Bit/dim 1.7995(1.8194) | Xent 0.0000(0.0000) | Loss 1.7995(1.8194) | Error 0.0000(0.0000) Steps 680(681.60) | Grad Norm 8.4997(3.7356) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 20.9720, Epoch Time 583.4633(592.5123), Bit/dim 1.7941(best: 1.7960), Xent 0.0000, Loss 1.7941, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3165 | Time 80.1194(79.2027) | Bit/dim 1.7971(1.8187) | Xent 0.0000(0.0000) | Loss 1.7971(1.8187) | Error 0.0000(0.0000) Steps 668(681.19) | Grad Norm 9.4012(3.9056) | Total Time 10.00(10.00)\n",
      "Iter 3166 | Time 77.8546(79.1623) | Bit/dim 1.8011(1.8182) | Xent 0.0000(0.0000) | Loss 1.8011(1.8182) | Error 0.0000(0.0000) Steps 674(680.97) | Grad Norm 7.1836(4.0040) | Total Time 10.00(10.00)\n",
      "Iter 3167 | Time 77.8019(79.1215) | Bit/dim 1.7939(1.8174) | Xent 0.0000(0.0000) | Loss 1.7939(1.8174) | Error 0.0000(0.0000) Steps 674(680.76) | Grad Norm 2.6904(3.9645) | Total Time 10.00(10.00)\n",
      "Iter 3168 | Time 79.7475(79.1403) | Bit/dim 1.8007(1.8169) | Xent 0.0000(0.0000) | Loss 1.8007(1.8169) | Error 0.0000(0.0000) Steps 674(680.56) | Grad Norm 2.2645(3.9135) | Total Time 10.00(10.00)\n",
      "Iter 3169 | Time 78.8002(79.1301) | Bit/dim 1.7981(1.8164) | Xent 0.0000(0.0000) | Loss 1.7981(1.8164) | Error 0.0000(0.0000) Steps 674(680.36) | Grad Norm 5.6177(3.9647) | Total Time 10.00(10.00)\n",
      "Iter 3170 | Time 77.7576(79.0889) | Bit/dim 1.7945(1.8157) | Xent 0.0000(0.0000) | Loss 1.7945(1.8157) | Error 0.0000(0.0000) Steps 674(680.17) | Grad Norm 7.0440(4.0570) | Total Time 10.00(10.00)\n",
      "Iter 3171 | Time 79.9080(79.1135) | Bit/dim 1.7975(1.8152) | Xent 0.0000(0.0000) | Loss 1.7975(1.8152) | Error 0.0000(0.0000) Steps 680(680.17) | Grad Norm 6.3647(4.1263) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 21.5191, Epoch Time 586.2062(592.3231), Bit/dim 1.7883(best: 1.7941), Xent 0.0000, Loss 1.7883, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3172 | Time 78.6978(79.1010) | Bit/dim 1.7959(1.8146) | Xent 0.0000(0.0000) | Loss 1.7959(1.8146) | Error 0.0000(0.0000) Steps 674(679.98) | Grad Norm 3.9443(4.1208) | Total Time 10.00(10.00)\n",
      "Iter 3173 | Time 78.4104(79.0803) | Bit/dim 1.7937(1.8140) | Xent 0.0000(0.0000) | Loss 1.7937(1.8140) | Error 0.0000(0.0000) Steps 674(679.80) | Grad Norm 0.7067(4.0184) | Total Time 10.00(10.00)\n",
      "Iter 3174 | Time 77.1183(79.0214) | Bit/dim 1.7958(1.8134) | Xent 0.0000(0.0000) | Loss 1.7958(1.8134) | Error 0.0000(0.0000) Steps 674(679.63) | Grad Norm 2.8074(3.9821) | Total Time 10.00(10.00)\n",
      "Iter 3175 | Time 78.3977(79.0027) | Bit/dim 1.7937(1.8128) | Xent 0.0000(0.0000) | Loss 1.7937(1.8128) | Error 0.0000(0.0000) Steps 674(679.46) | Grad Norm 4.5557(3.9993) | Total Time 10.00(10.00)\n",
      "Iter 3176 | Time 80.1966(79.0385) | Bit/dim 1.7925(1.8122) | Xent 0.0000(0.0000) | Loss 1.7925(1.8122) | Error 0.0000(0.0000) Steps 674(679.30) | Grad Norm 4.4149(4.0117) | Total Time 10.00(10.00)\n",
      "Iter 3177 | Time 79.5468(79.0538) | Bit/dim 1.7902(1.8116) | Xent 0.0000(0.0000) | Loss 1.7902(1.8116) | Error 0.0000(0.0000) Steps 674(679.14) | Grad Norm 2.6982(3.9723) | Total Time 10.00(10.00)\n",
      "Iter 3178 | Time 77.8999(79.0192) | Bit/dim 1.7911(1.8109) | Xent 0.0000(0.0000) | Loss 1.7911(1.8109) | Error 0.0000(0.0000) Steps 674(678.98) | Grad Norm 0.7139(3.8746) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 21.1414, Epoch Time 584.1810(592.0789), Bit/dim 1.7859(best: 1.7883), Xent 0.0000, Loss 1.7859, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3179 | Time 78.5747(79.0058) | Bit/dim 1.7926(1.8104) | Xent 0.0000(0.0000) | Loss 1.7926(1.8104) | Error 0.0000(0.0000) Steps 674(678.83) | Grad Norm 1.8542(3.8140) | Total Time 10.00(10.00)\n",
      "Iter 3180 | Time 78.3138(78.9851) | Bit/dim 1.7881(1.8097) | Xent 0.0000(0.0000) | Loss 1.7881(1.8097) | Error 0.0000(0.0000) Steps 674(678.69) | Grad Norm 3.0542(3.7912) | Total Time 10.00(10.00)\n",
      "Iter 3181 | Time 78.5832(78.9730) | Bit/dim 1.7883(1.8091) | Xent 0.0000(0.0000) | Loss 1.7883(1.8091) | Error 0.0000(0.0000) Steps 674(678.55) | Grad Norm 3.2507(3.7750) | Total Time 10.00(10.00)\n",
      "Iter 3182 | Time 78.4043(78.9559) | Bit/dim 1.7889(1.8085) | Xent 0.0000(0.0000) | Loss 1.7889(1.8085) | Error 0.0000(0.0000) Steps 674(678.41) | Grad Norm 3.0659(3.7537) | Total Time 10.00(10.00)\n",
      "Iter 3183 | Time 77.5302(78.9132) | Bit/dim 1.7839(1.8077) | Xent 0.0000(0.0000) | Loss 1.7839(1.8077) | Error 0.0000(0.0000) Steps 674(678.28) | Grad Norm 2.6437(3.7204) | Total Time 10.00(10.00)\n",
      "Iter 3184 | Time 78.6977(78.9067) | Bit/dim 1.7926(1.8073) | Xent 0.0000(0.0000) | Loss 1.7926(1.8073) | Error 0.0000(0.0000) Steps 674(678.15) | Grad Norm 2.2792(3.6772) | Total Time 10.00(10.00)\n",
      "Iter 3185 | Time 78.0510(78.8810) | Bit/dim 1.7899(1.8068) | Xent 0.0000(0.0000) | Loss 1.7899(1.8068) | Error 0.0000(0.0000) Steps 674(678.03) | Grad Norm 2.1136(3.6302) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 21.2828, Epoch Time 581.8483(591.7719), Bit/dim 1.7796(best: 1.7859), Xent 0.0000, Loss 1.7796, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3186 | Time 80.4339(78.9276) | Bit/dim 1.7854(1.8061) | Xent 0.0000(0.0000) | Loss 1.7854(1.8061) | Error 0.0000(0.0000) Steps 674(677.91) | Grad Norm 1.7005(3.5724) | Total Time 10.00(10.00)\n",
      "Iter 3187 | Time 76.7238(78.8615) | Bit/dim 1.7895(1.8056) | Xent 0.0000(0.0000) | Loss 1.7895(1.8056) | Error 0.0000(0.0000) Steps 674(677.79) | Grad Norm 1.0591(3.4970) | Total Time 10.00(10.00)\n",
      "Iter 3188 | Time 77.5686(78.8227) | Bit/dim 1.7842(1.8050) | Xent 0.0000(0.0000) | Loss 1.7842(1.8050) | Error 0.0000(0.0000) Steps 674(677.67) | Grad Norm 0.6525(3.4116) | Total Time 10.00(10.00)\n",
      "Iter 3189 | Time 78.3156(78.8075) | Bit/dim 1.7845(1.8044) | Xent 0.0000(0.0000) | Loss 1.7845(1.8044) | Error 0.0000(0.0000) Steps 674(677.56) | Grad Norm 0.7292(3.3312) | Total Time 10.00(10.00)\n",
      "Iter 3190 | Time 77.4598(78.7671) | Bit/dim 1.7830(1.8037) | Xent 0.0000(0.0000) | Loss 1.7830(1.8037) | Error 0.0000(0.0000) Steps 674(677.46) | Grad Norm 1.0925(3.2640) | Total Time 10.00(10.00)\n",
      "Iter 3191 | Time 77.9716(78.7432) | Bit/dim 1.7851(1.8032) | Xent 0.0000(0.0000) | Loss 1.7851(1.8032) | Error 0.0000(0.0000) Steps 674(677.35) | Grad Norm 1.4402(3.2093) | Total Time 10.00(10.00)\n",
      "Iter 3192 | Time 79.7421(78.7732) | Bit/dim 1.7852(1.8026) | Xent 0.0000(0.0000) | Loss 1.7852(1.8026) | Error 0.0000(0.0000) Steps 674(677.25) | Grad Norm 1.6524(3.1626) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 21.0308, Epoch Time 581.9843(591.4783), Bit/dim 1.7763(best: 1.7796), Xent 0.0000, Loss 1.7763, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3193 | Time 76.6040(78.7081) | Bit/dim 1.7872(1.8022) | Xent 0.0000(0.0000) | Loss 1.7872(1.8022) | Error 0.0000(0.0000) Steps 674(677.16) | Grad Norm 1.5023(3.1128) | Total Time 10.00(10.00)\n",
      "Iter 3194 | Time 80.6611(78.7667) | Bit/dim 1.7873(1.8017) | Xent 0.0000(0.0000) | Loss 1.7873(1.8017) | Error 0.0000(0.0000) Steps 674(677.06) | Grad Norm 1.2266(3.0562) | Total Time 10.00(10.00)\n",
      "Iter 3195 | Time 77.7321(78.7357) | Bit/dim 1.7807(1.8011) | Xent 0.0000(0.0000) | Loss 1.7807(1.8011) | Error 0.0000(0.0000) Steps 674(676.97) | Grad Norm 1.0189(2.9951) | Total Time 10.00(10.00)\n",
      "Iter 3196 | Time 79.4905(78.7583) | Bit/dim 1.7827(1.8005) | Xent 0.0000(0.0000) | Loss 1.7827(1.8005) | Error 0.0000(0.0000) Steps 674(676.88) | Grad Norm 1.1348(2.9393) | Total Time 10.00(10.00)\n",
      "Iter 3197 | Time 80.6283(78.8144) | Bit/dim 1.7779(1.7999) | Xent 0.0000(0.0000) | Loss 1.7779(1.7999) | Error 0.0000(0.0000) Steps 674(676.79) | Grad Norm 1.6117(2.8994) | Total Time 10.00(10.00)\n",
      "Iter 3198 | Time 78.2644(78.7979) | Bit/dim 1.7792(1.7992) | Xent 0.0000(0.0000) | Loss 1.7792(1.7992) | Error 0.0000(0.0000) Steps 674(676.71) | Grad Norm 2.3711(2.8836) | Total Time 10.00(10.00)\n",
      "Iter 3199 | Time 78.7781(78.7973) | Bit/dim 1.7750(1.7985) | Xent 0.0000(0.0000) | Loss 1.7750(1.7985) | Error 0.0000(0.0000) Steps 674(676.63) | Grad Norm 3.5166(2.9026) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 21.0437, Epoch Time 585.8936(591.3108), Bit/dim 1.7744(best: 1.7763), Xent 0.0000, Loss 1.7744, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3200 | Time 76.8557(78.7391) | Bit/dim 1.7755(1.7978) | Xent 0.0000(0.0000) | Loss 1.7755(1.7978) | Error 0.0000(0.0000) Steps 674(676.55) | Grad Norm 4.9800(2.9649) | Total Time 10.00(10.00)\n",
      "Iter 3201 | Time 79.9945(78.7767) | Bit/dim 1.7808(1.7973) | Xent 0.0000(0.0000) | Loss 1.7808(1.7973) | Error 0.0000(0.0000) Steps 674(676.47) | Grad Norm 7.1421(3.0902) | Total Time 10.00(10.00)\n",
      "Iter 3202 | Time 78.2819(78.7619) | Bit/dim 1.7817(1.7968) | Xent 0.0000(0.0000) | Loss 1.7817(1.7968) | Error 0.0000(0.0000) Steps 680(676.58) | Grad Norm 10.0348(3.2985) | Total Time 10.00(10.00)\n",
      "Iter 3203 | Time 78.7129(78.7604) | Bit/dim 1.7739(1.7962) | Xent 0.0000(0.0000) | Loss 1.7739(1.7962) | Error 0.0000(0.0000) Steps 662(676.14) | Grad Norm 13.3463(3.6000) | Total Time 10.00(10.00)\n",
      "Iter 3204 | Time 79.4539(78.7812) | Bit/dim 1.7840(1.7958) | Xent 0.0000(0.0000) | Loss 1.7840(1.7958) | Error 0.0000(0.0000) Steps 686(676.44) | Grad Norm 16.0391(3.9732) | Total Time 10.00(10.00)\n",
      "Iter 3205 | Time 77.7437(78.7501) | Bit/dim 1.7792(1.7953) | Xent 0.0000(0.0000) | Loss 1.7792(1.7953) | Error 0.0000(0.0000) Steps 668(676.18) | Grad Norm 16.1320(4.3379) | Total Time 10.00(10.00)\n",
      "Iter 3206 | Time 79.7677(78.7806) | Bit/dim 1.7831(1.7949) | Xent 0.0000(0.0000) | Loss 1.7831(1.7949) | Error 0.0000(0.0000) Steps 674(676.12) | Grad Norm 12.3868(4.5794) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 20.9486, Epoch Time 584.2341(591.0985), Bit/dim 1.7698(best: 1.7744), Xent 0.0000, Loss 1.7698, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3207 | Time 77.3787(78.7386) | Bit/dim 1.7754(1.7943) | Xent 0.0000(0.0000) | Loss 1.7754(1.7943) | Error 0.0000(0.0000) Steps 674(676.05) | Grad Norm 4.9380(4.5901) | Total Time 10.00(10.00)\n",
      "Iter 3208 | Time 79.3744(78.7576) | Bit/dim 1.7808(1.7939) | Xent 0.0000(0.0000) | Loss 1.7808(1.7939) | Error 0.0000(0.0000) Steps 674(675.99) | Grad Norm 3.8285(4.5673) | Total Time 10.00(10.00)\n",
      "Iter 3209 | Time 79.0445(78.7662) | Bit/dim 1.7753(1.7934) | Xent 0.0000(0.0000) | Loss 1.7753(1.7934) | Error 0.0000(0.0000) Steps 674(675.93) | Grad Norm 10.3965(4.7422) | Total Time 10.00(10.00)\n",
      "Iter 3210 | Time 79.7802(78.7967) | Bit/dim 1.7741(1.7928) | Xent 0.0000(0.0000) | Loss 1.7741(1.7928) | Error 0.0000(0.0000) Steps 668(675.70) | Grad Norm 12.6109(4.9782) | Total Time 10.00(10.00)\n",
      "Iter 3211 | Time 77.4698(78.7569) | Bit/dim 1.7779(1.7924) | Xent 0.0000(0.0000) | Loss 1.7779(1.7924) | Error 0.0000(0.0000) Steps 668(675.46) | Grad Norm 10.1679(5.1339) | Total Time 10.00(10.00)\n",
      "Iter 3212 | Time 79.3838(78.7757) | Bit/dim 1.7721(1.7917) | Xent 0.0000(0.0000) | Loss 1.7721(1.7917) | Error 0.0000(0.0000) Steps 668(675.24) | Grad Norm 4.2936(5.1087) | Total Time 10.00(10.00)\n",
      "Iter 3213 | Time 77.1026(78.7255) | Bit/dim 1.7737(1.7912) | Xent 0.0000(0.0000) | Loss 1.7737(1.7912) | Error 0.0000(0.0000) Steps 668(675.02) | Grad Norm 2.8563(5.0411) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 21.0612, Epoch Time 583.4702(590.8696), Bit/dim 1.7669(best: 1.7698), Xent 0.0000, Loss 1.7669, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3214 | Time 78.0570(78.7054) | Bit/dim 1.7720(1.7906) | Xent 0.0000(0.0000) | Loss 1.7720(1.7906) | Error 0.0000(0.0000) Steps 668(674.81) | Grad Norm 8.4084(5.1422) | Total Time 10.00(10.00)\n",
      "Iter 3215 | Time 78.1126(78.6876) | Bit/dim 1.7703(1.7900) | Xent 0.0000(0.0000) | Loss 1.7703(1.7900) | Error 0.0000(0.0000) Steps 668(674.61) | Grad Norm 10.5184(5.3035) | Total Time 10.00(10.00)\n",
      "Iter 3216 | Time 77.6669(78.6570) | Bit/dim 1.7702(1.7894) | Xent 0.0000(0.0000) | Loss 1.7702(1.7894) | Error 0.0000(0.0000) Steps 668(674.41) | Grad Norm 8.4866(5.3989) | Total Time 10.00(10.00)\n",
      "Iter 3217 | Time 77.5485(78.6238) | Bit/dim 1.7725(1.7889) | Xent 0.0000(0.0000) | Loss 1.7725(1.7889) | Error 0.0000(0.0000) Steps 668(674.22) | Grad Norm 3.0733(5.3292) | Total Time 10.00(10.00)\n",
      "Iter 3218 | Time 79.0728(78.6372) | Bit/dim 1.7725(1.7884) | Xent 0.0000(0.0000) | Loss 1.7725(1.7884) | Error 0.0000(0.0000) Steps 662(673.85) | Grad Norm 3.4846(5.2738) | Total Time 10.00(10.00)\n",
      "Iter 3219 | Time 77.5764(78.6054) | Bit/dim 1.7728(1.7880) | Xent 0.0000(0.0000) | Loss 1.7728(1.7880) | Error 0.0000(0.0000) Steps 668(673.68) | Grad Norm 7.9964(5.3555) | Total Time 10.00(10.00)\n",
      "Iter 3220 | Time 78.1361(78.5913) | Bit/dim 1.7668(1.7873) | Xent 0.0000(0.0000) | Loss 1.7668(1.7873) | Error 0.0000(0.0000) Steps 662(673.33) | Grad Norm 9.1543(5.4695) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 21.0767, Epoch Time 579.8708(590.5397), Bit/dim 1.7646(best: 1.7669), Xent 0.0000, Loss 1.7646, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3221 | Time 76.8221(78.5382) | Bit/dim 1.7700(1.7868) | Xent 0.0000(0.0000) | Loss 1.7700(1.7868) | Error 0.0000(0.0000) Steps 662(672.99) | Grad Norm 6.7937(5.5092) | Total Time 10.00(10.00)\n",
      "Iter 3222 | Time 79.1010(78.5551) | Bit/dim 1.7678(1.7862) | Xent 0.0000(0.0000) | Loss 1.7678(1.7862) | Error 0.0000(0.0000) Steps 662(672.66) | Grad Norm 2.3340(5.4140) | Total Time 10.00(10.00)\n",
      "Iter 3223 | Time 78.6702(78.5586) | Bit/dim 1.7709(1.7858) | Xent 0.0000(0.0000) | Loss 1.7709(1.7858) | Error 0.0000(0.0000) Steps 662(672.34) | Grad Norm 2.4616(5.3254) | Total Time 10.00(10.00)\n",
      "Iter 3224 | Time 76.9520(78.5104) | Bit/dim 1.7665(1.7852) | Xent 0.0000(0.0000) | Loss 1.7665(1.7852) | Error 0.0000(0.0000) Steps 662(672.03) | Grad Norm 5.9548(5.3443) | Total Time 10.00(10.00)\n",
      "Iter 3225 | Time 79.1387(78.5292) | Bit/dim 1.7710(1.7848) | Xent 0.0000(0.0000) | Loss 1.7710(1.7848) | Error 0.0000(0.0000) Steps 662(671.73) | Grad Norm 7.6656(5.4139) | Total Time 10.00(10.00)\n",
      "Iter 3226 | Time 78.3033(78.5225) | Bit/dim 1.7663(1.7842) | Xent 0.0000(0.0000) | Loss 1.7663(1.7842) | Error 0.0000(0.0000) Steps 662(671.43) | Grad Norm 6.9780(5.4608) | Total Time 10.00(10.00)\n",
      "Iter 3227 | Time 78.4435(78.5201) | Bit/dim 1.7642(1.7836) | Xent 0.0000(0.0000) | Loss 1.7642(1.7836) | Error 0.0000(0.0000) Steps 650(670.79) | Grad Norm 4.2163(5.4235) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 21.0348, Epoch Time 581.2775(590.2618), Bit/dim 1.7589(best: 1.7646), Xent 0.0000, Loss 1.7589, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3228 | Time 76.6161(78.4630) | Bit/dim 1.7696(1.7832) | Xent 0.0000(0.0000) | Loss 1.7696(1.7832) | Error 0.0000(0.0000) Steps 650(670.17) | Grad Norm 0.7355(5.2829) | Total Time 10.00(10.00)\n",
      "Iter 3229 | Time 81.3305(78.5490) | Bit/dim 1.7690(1.7828) | Xent 0.0000(0.0000) | Loss 1.7690(1.7828) | Error 0.0000(0.0000) Steps 668(670.10) | Grad Norm 2.9025(5.2114) | Total Time 10.00(10.00)\n",
      "Iter 3230 | Time 78.7308(78.5544) | Bit/dim 1.7598(1.7821) | Xent 0.0000(0.0000) | Loss 1.7598(1.7821) | Error 0.0000(0.0000) Steps 662(669.86) | Grad Norm 4.9857(5.2047) | Total Time 10.00(10.00)\n",
      "Iter 3231 | Time 78.1934(78.5436) | Bit/dim 1.7624(1.7815) | Xent 0.0000(0.0000) | Loss 1.7624(1.7815) | Error 0.0000(0.0000) Steps 662(669.62) | Grad Norm 5.3451(5.2089) | Total Time 10.00(10.00)\n",
      "Iter 3234 | Time 76.9369(78.5292) | Bit/dim 1.7526(1.7797) | Xent 0.0000(0.0000) | Loss 1.7526(1.7797) | Error 0.0000(0.0000) Steps 650(668.25) | Grad Norm 2.1793(4.9834) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 21.0903, Epoch Time 583.8177(590.0685), Bit/dim 1.7557(best: 1.7589), Xent 0.0000, Loss 1.7557, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3235 | Time 78.0695(78.5154) | Bit/dim 1.7609(1.7791) | Xent 0.0000(0.0000) | Loss 1.7609(1.7791) | Error 0.0000(0.0000) Steps 656(667.88) | Grad Norm 5.8824(5.0104) | Total Time 10.00(10.00)\n",
      "Iter 3236 | Time 77.8769(78.4963) | Bit/dim 1.7632(1.7786) | Xent 0.0000(0.0000) | Loss 1.7632(1.7786) | Error 0.0000(0.0000) Steps 662(667.70) | Grad Norm 8.3763(5.1114) | Total Time 10.00(10.00)\n",
      "Iter 3237 | Time 77.4216(78.4640) | Bit/dim 1.7607(1.7781) | Xent 0.0000(0.0000) | Loss 1.7607(1.7781) | Error 0.0000(0.0000) Steps 656(667.35) | Grad Norm 8.4448(5.2114) | Total Time 10.00(10.00)\n",
      "Iter 3238 | Time 79.3724(78.4913) | Bit/dim 1.7568(1.7775) | Xent 0.0000(0.0000) | Loss 1.7568(1.7775) | Error 0.0000(0.0000) Steps 662(667.19) | Grad Norm 6.6667(5.2550) | Total Time 10.00(10.00)\n",
      "Iter 3239 | Time 78.1980(78.4825) | Bit/dim 1.7671(1.7772) | Xent 0.0000(0.0000) | Loss 1.7671(1.7772) | Error 0.0000(0.0000) Steps 668(667.22) | Grad Norm 4.0863(5.2200) | Total Time 10.00(10.00)\n",
      "Iter 3240 | Time 79.4983(78.5130) | Bit/dim 1.7614(1.7767) | Xent 0.0000(0.0000) | Loss 1.7614(1.7767) | Error 0.0000(0.0000) Steps 650(666.70) | Grad Norm 1.1097(5.0967) | Total Time 10.00(10.00)\n",
      "Iter 3241 | Time 76.5577(78.4543) | Bit/dim 1.7593(1.7762) | Xent 0.0000(0.0000) | Loss 1.7593(1.7762) | Error 0.0000(0.0000) Steps 650(666.20) | Grad Norm 2.1895(5.0094) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 20.9932, Epoch Time 580.4963(589.7813), Bit/dim 1.7520(best: 1.7557), Xent 0.0000, Loss 1.7520, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3242 | Time 77.7051(78.4318) | Bit/dim 1.7561(1.7756) | Xent 0.0000(0.0000) | Loss 1.7561(1.7756) | Error 0.0000(0.0000) Steps 662(666.07) | Grad Norm 4.3359(4.9892) | Total Time 10.00(10.00)\n",
      "Iter 3243 | Time 79.4176(78.4614) | Bit/dim 1.7577(1.7750) | Xent 0.0000(0.0000) | Loss 1.7577(1.7750) | Error 0.0000(0.0000) Steps 656(665.77) | Grad Norm 5.0572(4.9913) | Total Time 10.00(10.00)\n",
      "Iter 3244 | Time 78.0514(78.4491) | Bit/dim 1.7619(1.7746) | Xent 0.0000(0.0000) | Loss 1.7619(1.7746) | Error 0.0000(0.0000) Steps 650(665.30) | Grad Norm 4.7974(4.9855) | Total Time 10.00(10.00)\n",
      "Iter 3245 | Time 78.2927(78.4444) | Bit/dim 1.7583(1.7741) | Xent 0.0000(0.0000) | Loss 1.7583(1.7741) | Error 0.0000(0.0000) Steps 656(665.02) | Grad Norm 4.0286(4.9568) | Total Time 10.00(10.00)\n",
      "Iter 3246 | Time 78.5390(78.4472) | Bit/dim 1.7594(1.7737) | Xent 0.0000(0.0000) | Loss 1.7594(1.7737) | Error 0.0000(0.0000) Steps 656(664.75) | Grad Norm 2.8472(4.8935) | Total Time 10.00(10.00)\n",
      "Iter 3247 | Time 78.5946(78.4517) | Bit/dim 1.7532(1.7731) | Xent 0.0000(0.0000) | Loss 1.7532(1.7731) | Error 0.0000(0.0000) Steps 650(664.31) | Grad Norm 1.6091(4.7949) | Total Time 10.00(10.00)\n",
      "Iter 3248 | Time 78.3346(78.4481) | Bit/dim 1.7605(1.7727) | Xent 0.0000(0.0000) | Loss 1.7605(1.7727) | Error 0.0000(0.0000) Steps 662(664.24) | Grad Norm 0.7980(4.6750) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 20.7485, Epoch Time 582.5151(589.5633), Bit/dim 1.7494(best: 1.7520), Xent 0.0000, Loss 1.7494, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3249 | Time 76.0425(78.3760) | Bit/dim 1.7598(1.7723) | Xent 0.0000(0.0000) | Loss 1.7598(1.7723) | Error 0.0000(0.0000) Steps 650(663.81) | Grad Norm 0.6588(4.5545) | Total Time 10.00(10.00)\n",
      "Iter 3250 | Time 79.1425(78.3990) | Bit/dim 1.7550(1.7718) | Xent 0.0000(0.0000) | Loss 1.7550(1.7718) | Error 0.0000(0.0000) Steps 650(663.40) | Grad Norm 1.1049(4.4511) | Total Time 10.00(10.00)\n",
      "Iter 3251 | Time 77.7755(78.3803) | Bit/dim 1.7487(1.7711) | Xent 0.0000(0.0000) | Loss 1.7487(1.7711) | Error 0.0000(0.0000) Steps 668(663.53) | Grad Norm 1.6385(4.3667) | Total Time 10.00(10.00)\n",
      "Iter 3252 | Time 79.3018(78.4079) | Bit/dim 1.7549(1.7706) | Xent 0.0000(0.0000) | Loss 1.7549(1.7706) | Error 0.0000(0.0000) Steps 662(663.49) | Grad Norm 2.0325(4.2966) | Total Time 10.00(10.00)\n",
      "Iter 3253 | Time 78.3142(78.4051) | Bit/dim 1.7568(1.7702) | Xent 0.0000(0.0000) | Loss 1.7568(1.7702) | Error 0.0000(0.0000) Steps 668(663.62) | Grad Norm 2.0379(4.2289) | Total Time 10.00(10.00)\n",
      "Iter 3254 | Time 78.3821(78.4044) | Bit/dim 1.7532(1.7697) | Xent 0.0000(0.0000) | Loss 1.7532(1.7697) | Error 0.0000(0.0000) Steps 662(663.57) | Grad Norm 2.7845(4.1856) | Total Time 10.00(10.00)\n",
      "Iter 3255 | Time 77.6409(78.3815) | Bit/dim 1.7515(1.7691) | Xent 0.0000(0.0000) | Loss 1.7515(1.7691) | Error 0.0000(0.0000) Steps 668(663.71) | Grad Norm 4.2329(4.1870) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 20.9093, Epoch Time 580.1078(589.2797), Bit/dim 1.7469(best: 1.7494), Xent 0.0000, Loss 1.7469, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3256 | Time 78.8114(78.3944) | Bit/dim 1.7508(1.7686) | Xent 0.0000(0.0000) | Loss 1.7508(1.7686) | Error 0.0000(0.0000) Steps 656(663.48) | Grad Norm 5.8819(4.2378) | Total Time 10.00(10.00)\n",
      "Iter 3257 | Time 78.2563(78.3903) | Bit/dim 1.7531(1.7681) | Xent 0.0000(0.0000) | Loss 1.7531(1.7681) | Error 0.0000(0.0000) Steps 644(662.89) | Grad Norm 8.3321(4.3607) | Total Time 10.00(10.00)\n",
      "Iter 3258 | Time 79.0869(78.4112) | Bit/dim 1.7519(1.7676) | Xent 0.0000(0.0000) | Loss 1.7519(1.7676) | Error 0.0000(0.0000) Steps 656(662.68) | Grad Norm 11.1205(4.5634) | Total Time 10.00(10.00)\n",
      "Iter 3259 | Time 78.2777(78.4072) | Bit/dim 1.7560(1.7673) | Xent 0.0000(0.0000) | Loss 1.7560(1.7673) | Error 0.0000(0.0000) Steps 644(662.12) | Grad Norm 14.8509(4.8721) | Total Time 10.00(10.00)\n",
      "Iter 3260 | Time 79.4264(78.4377) | Bit/dim 1.7527(1.7669) | Xent 0.0000(0.0000) | Loss 1.7527(1.7669) | Error 0.0000(0.0000) Steps 674(662.48) | Grad Norm 19.3812(5.3073) | Total Time 10.00(10.00)\n",
      "Iter 3261 | Time 79.5801(78.4720) | Bit/dim 1.7541(1.7665) | Xent 0.0000(0.0000) | Loss 1.7541(1.7665) | Error 0.0000(0.0000) Steps 650(662.11) | Grad Norm 22.6649(5.8281) | Total Time 10.00(10.00)\n",
      "Iter 3262 | Time 78.2557(78.4655) | Bit/dim 1.7567(1.7662) | Xent 0.0000(0.0000) | Loss 1.7567(1.7662) | Error 0.0000(0.0000) Steps 674(662.46) | Grad Norm 22.0738(6.3154) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 20.8051, Epoch Time 585.2070(589.1575), Bit/dim 1.7445(best: 1.7469), Xent 0.0000, Loss 1.7445, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3263 | Time 78.8058(78.4757) | Bit/dim 1.7526(1.7658) | Xent 0.0000(0.0000) | Loss 1.7526(1.7658) | Error 0.0000(0.0000) Steps 662(662.45) | Grad Norm 15.5816(6.5934) | Total Time 10.00(10.00)\n",
      "Iter 3264 | Time 77.0946(78.4343) | Bit/dim 1.7520(1.7654) | Xent 0.0000(0.0000) | Loss 1.7520(1.7654) | Error 0.0000(0.0000) Steps 656(662.26) | Grad Norm 4.7340(6.5376) | Total Time 10.00(10.00)\n",
      "Iter 3265 | Time 77.3913(78.4030) | Bit/dim 1.7414(1.7646) | Xent 0.0000(0.0000) | Loss 1.7414(1.7646) | Error 0.0000(0.0000) Steps 668(662.43) | Grad Norm 6.5201(6.5371) | Total Time 10.00(10.00)\n",
      "Iter 3266 | Time 76.6416(78.3502) | Bit/dim 1.7530(1.7643) | Xent 0.0000(0.0000) | Loss 1.7530(1.7643) | Error 0.0000(0.0000) Steps 650(662.06) | Grad Norm 14.8274(6.7858) | Total Time 10.00(10.00)\n",
      "Iter 3267 | Time 74.5571(78.2364) | Bit/dim 1.7523(1.7639) | Xent 0.0000(0.0000) | Loss 1.7523(1.7639) | Error 0.0000(0.0000) Steps 662(662.05) | Grad Norm 17.8160(7.1167) | Total Time 10.00(10.00)\n",
      "Iter 3268 | Time 78.2071(78.2355) | Bit/dim 1.7481(1.7635) | Xent 0.0000(0.0000) | Loss 1.7481(1.7635) | Error 0.0000(0.0000) Steps 650(661.69) | Grad Norm 13.6669(7.3132) | Total Time 10.00(10.00)\n",
      "Iter 3269 | Time 76.0451(78.1698) | Bit/dim 1.7465(1.7630) | Xent 0.0000(0.0000) | Loss 1.7465(1.7630) | Error 0.0000(0.0000) Steps 656(661.52) | Grad Norm 4.7179(7.2354) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 20.9067, Epoch Time 572.4413(588.6560), Bit/dim 1.7407(best: 1.7445), Xent 0.0000, Loss 1.7407, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3270 | Time 78.5687(78.1817) | Bit/dim 1.7469(1.7625) | Xent 0.0000(0.0000) | Loss 1.7469(1.7625) | Error 0.0000(0.0000) Steps 656(661.36) | Grad Norm 5.2438(7.1756) | Total Time 10.00(10.00)\n",
      "Iter 3271 | Time 79.3400(78.2165) | Bit/dim 1.7459(1.7620) | Xent 0.0000(0.0000) | Loss 1.7459(1.7620) | Error 0.0000(0.0000) Steps 650(661.01) | Grad Norm 12.5354(7.3364) | Total Time 10.00(10.00)\n",
      "Iter 3272 | Time 78.9180(78.2375) | Bit/dim 1.7460(1.7615) | Xent 0.0000(0.0000) | Loss 1.7460(1.7615) | Error 0.0000(0.0000) Steps 662(661.04) | Grad Norm 14.2639(7.5442) | Total Time 10.00(10.00)\n",
      "Iter 3273 | Time 77.3036(78.2095) | Bit/dim 1.7492(1.7611) | Xent 0.0000(0.0000) | Loss 1.7492(1.7611) | Error 0.0000(0.0000) Steps 650(660.71) | Grad Norm 9.3574(7.5986) | Total Time 10.00(10.00)\n",
      "Iter 3274 | Time 74.4732(78.0974) | Bit/dim 1.7445(1.7606) | Xent 0.0000(0.0000) | Loss 1.7445(1.7606) | Error 0.0000(0.0000) Steps 656(660.57) | Grad Norm 0.7551(7.3933) | Total Time 10.00(10.00)\n",
      "Iter 3275 | Time 75.5502(78.0210) | Bit/dim 1.7449(1.7602) | Xent 0.0000(0.0000) | Loss 1.7449(1.7602) | Error 0.0000(0.0000) Steps 656(660.43) | Grad Norm 7.9506(7.4101) | Total Time 10.00(10.00)\n",
      "Iter 3276 | Time 79.9720(78.0795) | Bit/dim 1.7470(1.7598) | Xent 0.0000(0.0000) | Loss 1.7470(1.7598) | Error 0.0000(0.0000) Steps 650(660.12) | Grad Norm 12.5274(7.5636) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 20.9486, Epoch Time 577.7149(588.3278), Bit/dim 1.7389(best: 1.7407), Xent 0.0000, Loss 1.7389, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3277 | Time 78.0226(78.0778) | Bit/dim 1.7445(1.7593) | Xent 0.0000(0.0000) | Loss 1.7445(1.7593) | Error 0.0000(0.0000) Steps 656(660.00) | Grad Norm 11.7065(7.6879) | Total Time 10.00(10.00)\n",
      "Iter 3278 | Time 75.8983(78.0124) | Bit/dim 1.7396(1.7587) | Xent 0.0000(0.0000) | Loss 1.7396(1.7587) | Error 0.0000(0.0000) Steps 656(659.88) | Grad Norm 6.1693(7.6423) | Total Time 10.00(10.00)\n",
      "Iter 3279 | Time 75.7496(77.9446) | Bit/dim 1.7415(1.7582) | Xent 0.0000(0.0000) | Loss 1.7415(1.7582) | Error 0.0000(0.0000) Steps 656(659.76) | Grad Norm 1.4699(7.4571) | Total Time 10.00(10.00)\n",
      "Iter 3280 | Time 78.5298(77.9621) | Bit/dim 1.7417(1.7577) | Xent 0.0000(0.0000) | Loss 1.7417(1.7577) | Error 0.0000(0.0000) Steps 668(660.01) | Grad Norm 7.6581(7.4632) | Total Time 10.00(10.00)\n",
      "Iter 3281 | Time 75.3372(77.8834) | Bit/dim 1.7494(1.7575) | Xent 0.0000(0.0000) | Loss 1.7494(1.7575) | Error 0.0000(0.0000) Steps 650(659.71) | Grad Norm 10.6973(7.5602) | Total Time 10.00(10.00)\n",
      "Iter 3282 | Time 75.5623(77.8137) | Bit/dim 1.7403(1.7569) | Xent 0.0000(0.0000) | Loss 1.7403(1.7569) | Error 0.0000(0.0000) Steps 656(659.60) | Grad Norm 9.2109(7.6097) | Total Time 10.00(10.00)\n",
      "Iter 3283 | Time 75.4425(77.7426) | Bit/dim 1.7414(1.7565) | Xent 0.0000(0.0000) | Loss 1.7414(1.7565) | Error 0.0000(0.0000) Steps 656(659.49) | Grad Norm 3.7809(7.4948) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 21.0145, Epoch Time 568.2186(587.7245), Bit/dim 1.7345(best: 1.7389), Xent 0.0000, Loss 1.7345, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3284 | Time 75.0540(77.6619) | Bit/dim 1.7398(1.7560) | Xent 0.0000(0.0000) | Loss 1.7398(1.7560) | Error 0.0000(0.0000) Steps 656(659.38) | Grad Norm 3.1122(7.3634) | Total Time 10.00(10.00)\n",
      "Iter 3285 | Time 76.7083(77.6333) | Bit/dim 1.7384(1.7554) | Xent 0.0000(0.0000) | Loss 1.7384(1.7554) | Error 0.0000(0.0000) Steps 656(659.28) | Grad Norm 8.4179(7.3950) | Total Time 10.00(10.00)\n",
      "Iter 3286 | Time 77.6884(77.6350) | Bit/dim 1.7432(1.7551) | Xent 0.0000(0.0000) | Loss 1.7432(1.7551) | Error 0.0000(0.0000) Steps 650(659.00) | Grad Norm 10.8005(7.4972) | Total Time 10.00(10.00)\n",
      "Iter 3287 | Time 76.8840(77.6125) | Bit/dim 1.7415(1.7547) | Xent 0.0000(0.0000) | Loss 1.7415(1.7547) | Error 0.0000(0.0000) Steps 656(658.91) | Grad Norm 9.8895(7.5689) | Total Time 10.00(10.00)\n",
      "Iter 3288 | Time 76.2866(77.5727) | Bit/dim 1.7373(1.7541) | Xent 0.0000(0.0000) | Loss 1.7373(1.7541) | Error 0.0000(0.0000) Steps 656(658.83) | Grad Norm 6.1629(7.5268) | Total Time 10.00(10.00)\n",
      "Iter 3289 | Time 77.4093(77.5678) | Bit/dim 1.7427(1.7538) | Xent 0.0000(0.0000) | Loss 1.7427(1.7538) | Error 0.0000(0.0000) Steps 656(658.74) | Grad Norm 0.8884(7.3276) | Total Time 10.00(10.00)\n",
      "Iter 3290 | Time 75.8852(77.5173) | Bit/dim 1.7354(1.7533) | Xent 0.0000(0.0000) | Loss 1.7354(1.7533) | Error 0.0000(0.0000) Steps 656(658.66) | Grad Norm 4.6181(7.2463) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 21.0564, Epoch Time 569.7130(587.1841), Bit/dim 1.7316(best: 1.7345), Xent 0.0000, Loss 1.7316, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3291 | Time 75.4898(77.4565) | Bit/dim 1.7413(1.7529) | Xent 0.0000(0.0000) | Loss 1.7413(1.7529) | Error 0.0000(0.0000) Steps 650(658.40) | Grad Norm 7.8583(7.2647) | Total Time 10.00(10.00)\n",
      "Iter 3292 | Time 78.1097(77.4761) | Bit/dim 1.7389(1.7525) | Xent 0.0000(0.0000) | Loss 1.7389(1.7525) | Error 0.0000(0.0000) Steps 656(658.33) | Grad Norm 8.6422(7.3060) | Total Time 10.00(10.00)\n",
      "Iter 3293 | Time 76.7438(77.4541) | Bit/dim 1.7321(1.7519) | Xent 0.0000(0.0000) | Loss 1.7321(1.7519) | Error 0.0000(0.0000) Steps 650(658.08) | Grad Norm 6.9059(7.2940) | Total Time 10.00(10.00)\n",
      "Iter 3294 | Time 77.0307(77.4414) | Bit/dim 1.7386(1.7515) | Xent 0.0000(0.0000) | Loss 1.7386(1.7515) | Error 0.0000(0.0000) Steps 656(658.02) | Grad Norm 3.2643(7.1731) | Total Time 10.00(10.00)\n",
      "Iter 3295 | Time 77.1048(77.4313) | Bit/dim 1.7348(1.7510) | Xent 0.0000(0.0000) | Loss 1.7348(1.7510) | Error 0.0000(0.0000) Steps 656(657.96) | Grad Norm 1.3678(6.9989) | Total Time 10.00(10.00)\n",
      "Iter 3296 | Time 75.4863(77.3730) | Bit/dim 1.7380(1.7506) | Xent 0.0000(0.0000) | Loss 1.7380(1.7506) | Error 0.0000(0.0000) Steps 656(657.90) | Grad Norm 5.5145(6.9544) | Total Time 10.00(10.00)\n",
      "Iter 3297 | Time 76.3969(77.3437) | Bit/dim 1.7326(1.7500) | Xent 0.0000(0.0000) | Loss 1.7326(1.7500) | Error 0.0000(0.0000) Steps 656(657.84) | Grad Norm 8.3413(6.9960) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 20.6440, Epoch Time 569.7084(586.6599), Bit/dim 1.7279(best: 1.7316), Xent 0.0000, Loss 1.7279, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3298 | Time 74.1972(77.2493) | Bit/dim 1.7378(1.7497) | Xent 0.0000(0.0000) | Loss 1.7378(1.7497) | Error 0.0000(0.0000) Steps 650(657.60) | Grad Norm 9.1615(7.0610) | Total Time 10.00(10.00)\n",
      "Iter 3299 | Time 76.1468(77.2162) | Bit/dim 1.7305(1.7491) | Xent 0.0000(0.0000) | Loss 1.7305(1.7491) | Error 0.0000(0.0000) Steps 656(657.56) | Grad Norm 7.7987(7.0831) | Total Time 10.00(10.00)\n",
      "Iter 3300 | Time 77.1703(77.2148) | Bit/dim 1.7313(1.7486) | Xent 0.0000(0.0000) | Loss 1.7313(1.7486) | Error 0.0000(0.0000) Steps 656(657.51) | Grad Norm 4.8954(7.0175) | Total Time 10.00(10.00)\n",
      "Iter 3301 | Time 74.2878(77.1270) | Bit/dim 1.7347(1.7481) | Xent 0.0000(0.0000) | Loss 1.7347(1.7481) | Error 0.0000(0.0000) Steps 656(657.46) | Grad Norm 1.4343(6.8500) | Total Time 10.00(10.00)\n",
      "Iter 3302 | Time 76.0006(77.0932) | Bit/dim 1.7375(1.7478) | Xent 0.0000(0.0000) | Loss 1.7375(1.7478) | Error 0.0000(0.0000) Steps 656(657.42) | Grad Norm 2.2878(6.7131) | Total Time 10.00(10.00)\n",
      "Iter 3303 | Time 75.0429(77.0317) | Bit/dim 1.7324(1.7474) | Xent 0.0000(0.0000) | Loss 1.7324(1.7474) | Error 0.0000(0.0000) Steps 656(657.38) | Grad Norm 5.2757(6.6700) | Total Time 10.00(10.00)\n",
      "Iter 3304 | Time 74.2407(76.9480) | Bit/dim 1.7288(1.7468) | Xent 0.0000(0.0000) | Loss 1.7288(1.7468) | Error 0.0000(0.0000) Steps 656(657.34) | Grad Norm 7.7438(6.7022) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 21.0319, Epoch Time 561.0987(585.8930), Bit/dim 1.7255(best: 1.7279), Xent 0.0000, Loss 1.7255, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3305 | Time 76.5416(76.9358) | Bit/dim 1.7298(1.7463) | Xent 0.0000(0.0000) | Loss 1.7298(1.7463) | Error 0.0000(0.0000) Steps 650(657.12) | Grad Norm 9.2391(6.7783) | Total Time 10.00(10.00)\n",
      "Iter 3306 | Time 74.5331(76.8637) | Bit/dim 1.7330(1.7459) | Xent 0.0000(0.0000) | Loss 1.7330(1.7459) | Error 0.0000(0.0000) Steps 656(657.08) | Grad Norm 10.2853(6.8835) | Total Time 10.00(10.00)\n",
      "Iter 3307 | Time 76.0594(76.8396) | Bit/dim 1.7315(1.7455) | Xent 0.0000(0.0000) | Loss 1.7315(1.7455) | Error 0.0000(0.0000) Steps 650(656.87) | Grad Norm 10.9522(7.0056) | Total Time 10.00(10.00)\n",
      "Iter 3308 | Time 74.8189(76.7790) | Bit/dim 1.7338(1.7451) | Xent 0.0000(0.0000) | Loss 1.7338(1.7451) | Error 0.0000(0.0000) Steps 656(656.84) | Grad Norm 10.2370(7.1025) | Total Time 10.00(10.00)\n",
      "Iter 3309 | Time 74.8599(76.7214) | Bit/dim 1.7327(1.7447) | Xent 0.0000(0.0000) | Loss 1.7327(1.7447) | Error 0.0000(0.0000) Steps 650(656.64) | Grad Norm 8.3129(7.1388) | Total Time 10.00(10.00)\n",
      "Iter 3310 | Time 76.8267(76.7245) | Bit/dim 1.7269(1.7442) | Xent 0.0000(0.0000) | Loss 1.7269(1.7442) | Error 0.0000(0.0000) Steps 656(656.62) | Grad Norm 5.9860(7.1043) | Total Time 10.00(10.00)\n",
      "Iter 3311 | Time 76.1582(76.7076) | Bit/dim 1.7300(1.7438) | Xent 0.0000(0.0000) | Loss 1.7300(1.7438) | Error 0.0000(0.0000) Steps 656(656.60) | Grad Norm 3.4115(6.9935) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 21.0984, Epoch Time 563.5154(585.2217), Bit/dim 1.7225(best: 1.7255), Xent 0.0000, Loss 1.7225, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3312 | Time 75.3424(76.6666) | Bit/dim 1.7271(1.7433) | Xent 0.0000(0.0000) | Loss 1.7271(1.7433) | Error 0.0000(0.0000) Steps 656(656.58) | Grad Norm 0.9486(6.8121) | Total Time 10.00(10.00)\n",
      "Iter 3313 | Time 77.5941(76.6944) | Bit/dim 1.7235(1.7427) | Xent 0.0000(0.0000) | Loss 1.7235(1.7427) | Error 0.0000(0.0000) Steps 656(656.57) | Grad Norm 1.8096(6.6620) | Total Time 10.00(10.00)\n",
      "Iter 3314 | Time 75.5544(76.6602) | Bit/dim 1.7247(1.7421) | Xent 0.0000(0.0000) | Loss 1.7247(1.7421) | Error 0.0000(0.0000) Steps 656(656.55) | Grad Norm 4.2998(6.5912) | Total Time 10.00(10.00)\n",
      "Iter 3315 | Time 77.0529(76.6720) | Bit/dim 1.7287(1.7417) | Xent 0.0000(0.0000) | Loss 1.7287(1.7417) | Error 0.0000(0.0000) Steps 656(656.53) | Grad Norm 7.1773(6.6088) | Total Time 10.00(10.00)\n",
      "Iter 3316 | Time 79.2997(76.7508) | Bit/dim 1.7274(1.7413) | Xent 0.0000(0.0000) | Loss 1.7274(1.7413) | Error 0.0000(0.0000) Steps 650(656.34) | Grad Norm 10.6262(6.7293) | Total Time 10.00(10.00)\n",
      "Iter 3317 | Time 77.1437(76.7626) | Bit/dim 1.7336(1.7411) | Xent 0.0000(0.0000) | Loss 1.7336(1.7411) | Error 0.0000(0.0000) Steps 656(656.33) | Grad Norm 14.5625(6.9643) | Total Time 10.00(10.00)\n",
      "Iter 3318 | Time 78.2152(76.8062) | Bit/dim 1.7259(1.7406) | Xent 0.0000(0.0000) | Loss 1.7259(1.7406) | Error 0.0000(0.0000) Steps 650(656.14) | Grad Norm 18.5250(7.3111) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 21.0156, Epoch Time 573.6485(584.8745), Bit/dim 1.7223(best: 1.7225), Xent 0.0000, Loss 1.7223, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3319 | Time 77.0194(76.8126) | Bit/dim 1.7202(1.7400) | Xent 0.0000(0.0000) | Loss 1.7202(1.7400) | Error 0.0000(0.0000) Steps 662(656.31) | Grad Norm 21.8264(7.7466) | Total Time 10.00(10.00)\n",
      "Iter 3320 | Time 75.3215(76.7679) | Bit/dim 1.7284(1.7397) | Xent 0.0000(0.0000) | Loss 1.7284(1.7397) | Error 0.0000(0.0000) Steps 650(656.12) | Grad Norm 23.1177(8.2077) | Total Time 10.00(10.00)\n",
      "Iter 3321 | Time 75.5822(76.7323) | Bit/dim 1.7300(1.7394) | Xent 0.0000(0.0000) | Loss 1.7300(1.7394) | Error 0.0000(0.0000) Steps 662(656.30) | Grad Norm 21.0690(8.5935) | Total Time 10.00(10.00)\n",
      "Iter 3322 | Time 76.0696(76.7124) | Bit/dim 1.7277(1.7390) | Xent 0.0000(0.0000) | Loss 1.7277(1.7390) | Error 0.0000(0.0000) Steps 650(656.11) | Grad Norm 15.2699(8.7938) | Total Time 10.00(10.00)\n",
      "Iter 3323 | Time 76.2218(76.6977) | Bit/dim 1.7290(1.7387) | Xent 0.0000(0.0000) | Loss 1.7290(1.7387) | Error 0.0000(0.0000) Steps 656(656.11) | Grad Norm 6.6109(8.7283) | Total Time 10.00(10.00)\n",
      "Iter 3324 | Time 75.9386(76.6749) | Bit/dim 1.7256(1.7383) | Xent 0.0000(0.0000) | Loss 1.7256(1.7383) | Error 0.0000(0.0000) Steps 656(656.10) | Grad Norm 3.1766(8.5618) | Total Time 10.00(10.00)\n",
      "Iter 3325 | Time 75.8123(76.6490) | Bit/dim 1.7236(1.7379) | Xent 0.0000(0.0000) | Loss 1.7236(1.7379) | Error 0.0000(0.0000) Steps 650(655.92) | Grad Norm 11.6536(8.6545) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 21.0274, Epoch Time 565.6419(584.2975), Bit/dim 1.7178(best: 1.7223), Xent 0.0000, Loss 1.7178, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3326 | Time 74.7748(76.5928) | Bit/dim 1.7220(1.7374) | Xent 0.0000(0.0000) | Loss 1.7220(1.7374) | Error 0.0000(0.0000) Steps 656(655.92) | Grad Norm 16.8855(8.9015) | Total Time 10.00(10.00)\n",
      "Iter 3327 | Time 74.6549(76.5347) | Bit/dim 1.7200(1.7369) | Xent 0.0000(0.0000) | Loss 1.7200(1.7369) | Error 0.0000(0.0000) Steps 650(655.75) | Grad Norm 17.7342(9.1665) | Total Time 10.00(10.00)\n",
      "Iter 3328 | Time 76.5408(76.5349) | Bit/dim 1.7220(1.7364) | Xent 0.0000(0.0000) | Loss 1.7220(1.7364) | Error 0.0000(0.0000) Steps 656(655.75) | Grad Norm 14.4148(9.3239) | Total Time 10.00(10.00)\n",
      "Iter 3329 | Time 76.8961(76.5457) | Bit/dim 1.7228(1.7360) | Xent 0.0000(0.0000) | Loss 1.7228(1.7360) | Error 0.0000(0.0000) Steps 650(655.58) | Grad Norm 7.9499(9.2827) | Total Time 10.00(10.00)\n",
      "Iter 3330 | Time 74.9137(76.4967) | Bit/dim 1.7237(1.7357) | Xent 0.0000(0.0000) | Loss 1.7237(1.7357) | Error 0.0000(0.0000) Steps 656(655.59) | Grad Norm 0.7101(9.0255) | Total Time 10.00(10.00)\n",
      "Iter 3331 | Time 79.9229(76.5995) | Bit/dim 1.7243(1.7353) | Xent 0.0000(0.0000) | Loss 1.7243(1.7353) | Error 0.0000(0.0000) Steps 656(655.61) | Grad Norm 6.6597(8.9545) | Total Time 10.00(10.00)\n",
      "Iter 3332 | Time 77.0792(76.6139) | Bit/dim 1.7200(1.7349) | Xent 0.0000(0.0000) | Loss 1.7200(1.7349) | Error 0.0000(0.0000) Steps 650(655.44) | Grad Norm 11.8797(9.0423) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 21.0027, Epoch Time 568.4348(583.8216), Bit/dim 1.7153(best: 1.7178), Xent 0.0000, Loss 1.7153, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3333 | Time 75.2797(76.5739) | Bit/dim 1.7145(1.7343) | Xent 0.0000(0.0000) | Loss 1.7145(1.7343) | Error 0.0000(0.0000) Steps 662(655.63) | Grad Norm 14.3814(9.2025) | Total Time 10.00(10.00)\n",
      "Iter 3334 | Time 77.1413(76.5909) | Bit/dim 1.7203(1.7338) | Xent 0.0000(0.0000) | Loss 1.7203(1.7338) | Error 0.0000(0.0000) Steps 650(655.46) | Grad Norm 13.3946(9.3282) | Total Time 10.00(10.00)\n",
      "Iter 3335 | Time 76.0800(76.5756) | Bit/dim 1.7193(1.7334) | Xent 0.0000(0.0000) | Loss 1.7193(1.7334) | Error 0.0000(0.0000) Steps 662(655.66) | Grad Norm 9.2218(9.3250) | Total Time 10.00(10.00)\n",
      "Iter 3336 | Time 78.0677(76.6204) | Bit/dim 1.7208(1.7330) | Xent 0.0000(0.0000) | Loss 1.7208(1.7330) | Error 0.0000(0.0000) Steps 662(655.85) | Grad Norm 2.9296(9.1332) | Total Time 10.00(10.00)\n",
      "Iter 3337 | Time 76.3284(76.6116) | Bit/dim 1.7215(1.7327) | Xent 0.0000(0.0000) | Loss 1.7215(1.7327) | Error 0.0000(0.0000) Steps 656(655.86) | Grad Norm 4.0523(8.9807) | Total Time 10.00(10.00)\n",
      "Iter 3338 | Time 77.2119(76.6296) | Bit/dim 1.7213(1.7323) | Xent 0.0000(0.0000) | Loss 1.7213(1.7323) | Error 0.0000(0.0000) Steps 662(656.04) | Grad Norm 9.4718(8.9955) | Total Time 10.00(10.00)\n",
      "Iter 3339 | Time 76.6287(76.6296) | Bit/dim 1.7211(1.7320) | Xent 0.0000(0.0000) | Loss 1.7211(1.7320) | Error 0.0000(0.0000) Steps 650(655.86) | Grad Norm 12.6937(9.1064) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 21.2690, Epoch Time 570.5758(583.4243), Bit/dim 1.7136(best: 1.7153), Xent 0.0000, Loss 1.7136, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3340 | Time 76.3275(76.6205) | Bit/dim 1.7189(1.7316) | Xent 0.0000(0.0000) | Loss 1.7189(1.7316) | Error 0.0000(0.0000) Steps 662(656.04) | Grad Norm 13.2125(9.2296) | Total Time 10.00(10.00)\n",
      "Iter 3341 | Time 76.6140(76.6203) | Bit/dim 1.7177(1.7312) | Xent 0.0000(0.0000) | Loss 1.7177(1.7312) | Error 0.0000(0.0000) Steps 656(656.04) | Grad Norm 11.1979(9.2887) | Total Time 10.00(10.00)\n",
      "Iter 3342 | Time 75.5498(76.5882) | Bit/dim 1.7119(1.7306) | Xent 0.0000(0.0000) | Loss 1.7119(1.7306) | Error 0.0000(0.0000) Steps 662(656.22) | Grad Norm 7.6203(9.2386) | Total Time 10.00(10.00)\n",
      "Iter 3343 | Time 75.7438(76.5629) | Bit/dim 1.7162(1.7302) | Xent 0.0000(0.0000) | Loss 1.7162(1.7302) | Error 0.0000(0.0000) Steps 662(656.39) | Grad Norm 3.2352(9.0585) | Total Time 10.00(10.00)\n",
      "Iter 3344 | Time 77.1470(76.5804) | Bit/dim 1.7144(1.7297) | Xent 0.0000(0.0000) | Loss 1.7144(1.7297) | Error 0.0000(0.0000) Steps 662(656.56) | Grad Norm 1.5162(8.8322) | Total Time 10.00(10.00)\n",
      "Iter 3345 | Time 77.4999(76.6080) | Bit/dim 1.7189(1.7294) | Xent 0.0000(0.0000) | Loss 1.7189(1.7294) | Error 0.0000(0.0000) Steps 662(656.73) | Grad Norm 5.7376(8.7394) | Total Time 10.00(10.00)\n",
      "Iter 3346 | Time 75.4590(76.5735) | Bit/dim 1.7184(1.7290) | Xent 0.0000(0.0000) | Loss 1.7184(1.7290) | Error 0.0000(0.0000) Steps 656(656.70) | Grad Norm 9.2133(8.7536) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 21.1343, Epoch Time 568.5262(582.9773), Bit/dim 1.7102(best: 1.7136), Xent 0.0000, Loss 1.7102, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3347 | Time 75.8069(76.5505) | Bit/dim 1.7185(1.7287) | Xent 0.0000(0.0000) | Loss 1.7185(1.7287) | Error 0.0000(0.0000) Steps 662(656.86) | Grad Norm 11.1348(8.8250) | Total Time 10.00(10.00)\n",
      "Iter 3348 | Time 76.6647(76.5539) | Bit/dim 1.7140(1.7283) | Xent 0.0000(0.0000) | Loss 1.7140(1.7283) | Error 0.0000(0.0000) Steps 656(656.84) | Grad Norm 11.4223(8.9030) | Total Time 10.00(10.00)\n",
      "Iter 3349 | Time 76.1480(76.5418) | Bit/dim 1.7115(1.7278) | Xent 0.0000(0.0000) | Loss 1.7115(1.7278) | Error 0.0000(0.0000) Steps 662(656.99) | Grad Norm 10.7734(8.9591) | Total Time 10.00(10.00)\n",
      "Iter 3350 | Time 77.3163(76.5650) | Bit/dim 1.7157(1.7274) | Xent 0.0000(0.0000) | Loss 1.7157(1.7274) | Error 0.0000(0.0000) Steps 656(656.96) | Grad Norm 9.0006(8.9603) | Total Time 10.00(10.00)\n",
      "Iter 3351 | Time 76.5203(76.5637) | Bit/dim 1.7106(1.7269) | Xent 0.0000(0.0000) | Loss 1.7106(1.7269) | Error 0.0000(0.0000) Steps 662(657.11) | Grad Norm 6.8249(8.8963) | Total Time 10.00(10.00)\n",
      "Iter 3352 | Time 77.8847(76.6033) | Bit/dim 1.7095(1.7264) | Xent 0.0000(0.0000) | Loss 1.7095(1.7264) | Error 0.0000(0.0000) Steps 662(657.26) | Grad Norm 4.6708(8.7695) | Total Time 10.00(10.00)\n",
      "Iter 3353 | Time 76.7577(76.6079) | Bit/dim 1.7146(1.7260) | Xent 0.0000(0.0000) | Loss 1.7146(1.7260) | Error 0.0000(0.0000) Steps 662(657.40) | Grad Norm 1.8820(8.5629) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 20.8076, Epoch Time 570.5616(582.6049), Bit/dim 1.7058(best: 1.7102), Xent 0.0000, Loss 1.7058, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3354 | Time 78.7476(76.6721) | Bit/dim 1.7127(1.7256) | Xent 0.0000(0.0000) | Loss 1.7127(1.7256) | Error 0.0000(0.0000) Steps 662(657.54) | Grad Norm 1.0613(8.3378) | Total Time 10.00(10.00)\n",
      "Iter 3355 | Time 77.0933(76.6848) | Bit/dim 1.7141(1.7253) | Xent 0.0000(0.0000) | Loss 1.7141(1.7253) | Error 0.0000(0.0000) Steps 656(657.49) | Grad Norm 2.7747(8.1709) | Total Time 10.00(10.00)\n",
      "Iter 3356 | Time 78.5110(76.7395) | Bit/dim 1.7131(1.7249) | Xent 0.0000(0.0000) | Loss 1.7131(1.7249) | Error 0.0000(0.0000) Steps 662(657.63) | Grad Norm 4.9651(8.0748) | Total Time 10.00(10.00)\n",
      "Iter 3359 | Time 78.5012(76.8810) | Bit/dim 1.7119(1.7234) | Xent 0.0000(0.0000) | Loss 1.7119(1.7234) | Error 0.0000(0.0000) Steps 656(657.66) | Grad Norm 12.0033(8.1988) | Total Time 10.00(10.00)\n",
      "Iter 3360 | Time 76.2532(76.8621) | Bit/dim 1.7158(1.7231) | Xent 0.0000(0.0000) | Loss 1.7158(1.7231) | Error 0.0000(0.0000) Steps 662(657.79) | Grad Norm 15.3940(8.4147) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 20.8980, Epoch Time 579.2559(582.5044), Bit/dim 1.7061(best: 1.7058), Xent 0.0000, Loss 1.7061, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3361 | Time 76.7782(76.8596) | Bit/dim 1.7100(1.7227) | Xent 0.0000(0.0000) | Loss 1.7100(1.7227) | Error 0.0000(0.0000) Steps 656(657.74) | Grad Norm 19.0665(8.7343) | Total Time 10.00(10.00)\n",
      "Iter 3362 | Time 76.9691(76.8629) | Bit/dim 1.7108(1.7224) | Xent 0.0000(0.0000) | Loss 1.7108(1.7224) | Error 0.0000(0.0000) Steps 668(658.05) | Grad Norm 23.5306(9.1781) | Total Time 10.00(10.00)\n",
      "Iter 3363 | Time 78.8343(76.9221) | Bit/dim 1.7150(1.7222) | Xent 0.0000(0.0000) | Loss 1.7150(1.7222) | Error 0.0000(0.0000) Steps 656(657.98) | Grad Norm 27.6941(9.7336) | Total Time 10.00(10.00)\n",
      "Iter 3364 | Time 78.5569(76.9711) | Bit/dim 1.7163(1.7220) | Xent 0.0000(0.0000) | Loss 1.7163(1.7220) | Error 0.0000(0.0000) Steps 680(658.64) | Grad Norm 29.6190(10.3302) | Total Time 10.00(10.00)\n",
      "Iter 3365 | Time 76.5195(76.9576) | Bit/dim 1.7076(1.7216) | Xent 0.0000(0.0000) | Loss 1.7076(1.7216) | Error 0.0000(0.0000) Steps 656(658.57) | Grad Norm 27.0879(10.8329) | Total Time 10.00(10.00)\n",
      "Iter 3366 | Time 76.8159(76.9533) | Bit/dim 1.7138(1.7213) | Xent 0.0000(0.0000) | Loss 1.7138(1.7213) | Error 0.0000(0.0000) Steps 668(658.85) | Grad Norm 18.6224(11.0666) | Total Time 10.00(10.00)\n",
      "Iter 3367 | Time 77.7185(76.9763) | Bit/dim 1.7020(1.7207) | Xent 0.0000(0.0000) | Loss 1.7020(1.7207) | Error 0.0000(0.0000) Steps 656(658.76) | Grad Norm 5.9388(10.9128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 20.9236, Epoch Time 575.9188(582.3068), Bit/dim 1.7007(best: 1.7058), Xent 0.0000, Loss 1.7007, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3368 | Time 75.2032(76.9231) | Bit/dim 1.7087(1.7204) | Xent 0.0000(0.0000) | Loss 1.7087(1.7204) | Error 0.0000(0.0000) Steps 656(658.68) | Grad Norm 7.3112(10.8047) | Total Time 10.00(10.00)\n",
      "Iter 3369 | Time 77.9514(76.9539) | Bit/dim 1.7113(1.7201) | Xent 0.0000(0.0000) | Loss 1.7113(1.7201) | Error 0.0000(0.0000) Steps 662(658.78) | Grad Norm 17.9035(11.0177) | Total Time 10.00(10.00)\n",
      "Iter 3370 | Time 77.8392(76.9805) | Bit/dim 1.7001(1.7195) | Xent 0.0000(0.0000) | Loss 1.7001(1.7195) | Error 0.0000(0.0000) Steps 656(658.70) | Grad Norm 22.6346(11.3662) | Total Time 10.00(10.00)\n",
      "Iter 3371 | Time 76.5497(76.9675) | Bit/dim 1.7060(1.7191) | Xent 0.0000(0.0000) | Loss 1.7060(1.7191) | Error 0.0000(0.0000) Steps 662(658.80) | Grad Norm 19.8304(11.6201) | Total Time 10.00(10.00)\n",
      "Iter 3372 | Time 76.5815(76.9560) | Bit/dim 1.7095(1.7188) | Xent 0.0000(0.0000) | Loss 1.7095(1.7188) | Error 0.0000(0.0000) Steps 656(658.71) | Grad Norm 10.6899(11.5922) | Total Time 10.00(10.00)\n",
      "Iter 3373 | Time 77.0242(76.9580) | Bit/dim 1.6997(1.7182) | Xent 0.0000(0.0000) | Loss 1.6997(1.7182) | Error 0.0000(0.0000) Steps 662(658.81) | Grad Norm 1.0746(11.2767) | Total Time 10.00(10.00)\n",
      "Iter 3374 | Time 76.2347(76.9363) | Bit/dim 1.7057(1.7179) | Xent 0.0000(0.0000) | Loss 1.7057(1.7179) | Error 0.0000(0.0000) Steps 662(658.91) | Grad Norm 11.1537(11.2730) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 20.6310, Epoch Time 570.6299(581.9565), Bit/dim 1.7008(best: 1.7007), Xent 0.0000, Loss 1.7008, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3375 | Time 76.9263(76.9360) | Bit/dim 1.7054(1.7175) | Xent 0.0000(0.0000) | Loss 1.7054(1.7175) | Error 0.0000(0.0000) Steps 656(658.82) | Grad Norm 17.1364(11.4489) | Total Time 10.00(10.00)\n",
      "Iter 3376 | Time 77.9324(76.9659) | Bit/dim 1.7026(1.7170) | Xent 0.0000(0.0000) | Loss 1.7026(1.7170) | Error 0.0000(0.0000) Steps 662(658.91) | Grad Norm 17.8531(11.6410) | Total Time 10.00(10.00)\n",
      "Iter 3377 | Time 76.2970(76.9458) | Bit/dim 1.7041(1.7167) | Xent 0.0000(0.0000) | Loss 1.7041(1.7167) | Error 0.0000(0.0000) Steps 656(658.83) | Grad Norm 13.1641(11.6867) | Total Time 10.00(10.00)\n",
      "Iter 3378 | Time 77.4331(76.9605) | Bit/dim 1.7029(1.7162) | Xent 0.0000(0.0000) | Loss 1.7029(1.7162) | Error 0.0000(0.0000) Steps 662(658.92) | Grad Norm 5.4178(11.4986) | Total Time 10.00(10.00)\n",
      "Iter 3379 | Time 79.0375(77.0228) | Bit/dim 1.7059(1.7159) | Xent 0.0000(0.0000) | Loss 1.7059(1.7159) | Error 0.0000(0.0000) Steps 662(659.01) | Grad Norm 2.2229(11.2204) | Total Time 10.00(10.00)\n",
      "Iter 3380 | Time 75.5237(76.9778) | Bit/dim 1.7052(1.7156) | Xent 0.0000(0.0000) | Loss 1.7052(1.7156) | Error 0.0000(0.0000) Steps 656(658.92) | Grad Norm 8.1120(11.1271) | Total Time 10.00(10.00)\n",
      "Iter 3381 | Time 75.5752(76.9357) | Bit/dim 1.6979(1.7151) | Xent 0.0000(0.0000) | Loss 1.6979(1.7151) | Error 0.0000(0.0000) Steps 662(659.02) | Grad Norm 11.7068(11.1445) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 20.8160, Epoch Time 572.1812(581.6633), Bit/dim 1.6965(best: 1.7007), Xent 0.0000, Loss 1.6965, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3382 | Time 75.3612(76.8885) | Bit/dim 1.7063(1.7148) | Xent 0.0000(0.0000) | Loss 1.7063(1.7148) | Error 0.0000(0.0000) Steps 656(658.93) | Grad Norm 11.4910(11.1549) | Total Time 10.00(10.00)\n",
      "Iter 3383 | Time 78.5760(76.9391) | Bit/dim 1.6971(1.7143) | Xent 0.0000(0.0000) | Loss 1.6971(1.7143) | Error 0.0000(0.0000) Steps 662(659.02) | Grad Norm 7.8546(11.0559) | Total Time 10.00(10.00)\n",
      "Iter 3384 | Time 76.8126(76.9353) | Bit/dim 1.6996(1.7138) | Xent 0.0000(0.0000) | Loss 1.6996(1.7138) | Error 0.0000(0.0000) Steps 662(659.11) | Grad Norm 2.3761(10.7955) | Total Time 10.00(10.00)\n",
      "Iter 3385 | Time 77.2228(76.9439) | Bit/dim 1.6979(1.7134) | Xent 0.0000(0.0000) | Loss 1.6979(1.7134) | Error 0.0000(0.0000) Steps 656(659.01) | Grad Norm 3.9106(10.5890) | Total Time 10.00(10.00)\n",
      "Iter 3386 | Time 79.1912(77.0114) | Bit/dim 1.6954(1.7128) | Xent 0.0000(0.0000) | Loss 1.6954(1.7128) | Error 0.0000(0.0000) Steps 662(659.10) | Grad Norm 8.4294(10.5242) | Total Time 10.00(10.00)\n",
      "Iter 3387 | Time 78.9152(77.0685) | Bit/dim 1.7034(1.7125) | Xent 0.0000(0.0000) | Loss 1.7034(1.7125) | Error 0.0000(0.0000) Steps 656(659.01) | Grad Norm 10.8281(10.5333) | Total Time 10.00(10.00)\n",
      "Iter 3388 | Time 76.3287(77.0463) | Bit/dim 1.7000(1.7122) | Xent 0.0000(0.0000) | Loss 1.7000(1.7122) | Error 0.0000(0.0000) Steps 662(659.10) | Grad Norm 11.3694(10.5584) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 20.7727, Epoch Time 575.7239(581.4851), Bit/dim 1.6932(best: 1.6965), Xent 0.0000, Loss 1.6932, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3389 | Time 75.3911(76.9966) | Bit/dim 1.6950(1.7117) | Xent 0.0000(0.0000) | Loss 1.6950(1.7117) | Error 0.0000(0.0000) Steps 656(659.01) | Grad Norm 9.5986(10.5296) | Total Time 10.00(10.00)\n",
      "Iter 3390 | Time 76.9739(76.9959) | Bit/dim 1.6972(1.7112) | Xent 0.0000(0.0000) | Loss 1.6972(1.7112) | Error 0.0000(0.0000) Steps 662(659.10) | Grad Norm 6.6560(10.4134) | Total Time 10.00(10.00)\n",
      "Iter 3391 | Time 76.2778(76.9744) | Bit/dim 1.6993(1.7109) | Xent 0.0000(0.0000) | Loss 1.6993(1.7109) | Error 0.0000(0.0000) Steps 662(659.18) | Grad Norm 3.4985(10.2059) | Total Time 10.00(10.00)\n",
      "Iter 3392 | Time 78.4353(77.0182) | Bit/dim 1.6987(1.7105) | Xent 0.0000(0.0000) | Loss 1.6987(1.7105) | Error 0.0000(0.0000) Steps 662(659.27) | Grad Norm 0.6733(9.9199) | Total Time 10.00(10.00)\n",
      "Iter 3393 | Time 76.8353(77.0127) | Bit/dim 1.7023(1.7103) | Xent 0.0000(0.0000) | Loss 1.7023(1.7103) | Error 0.0000(0.0000) Steps 662(659.35) | Grad Norm 2.3784(9.6937) | Total Time 10.00(10.00)\n",
      "Iter 3394 | Time 77.6659(77.0323) | Bit/dim 1.6974(1.7099) | Xent 0.0000(0.0000) | Loss 1.6974(1.7099) | Error 0.0000(0.0000) Steps 656(659.25) | Grad Norm 4.0533(9.5245) | Total Time 10.00(10.00)\n",
      "Iter 3395 | Time 77.5463(77.0477) | Bit/dim 1.6985(1.7095) | Xent 0.0000(0.0000) | Loss 1.6985(1.7095) | Error 0.0000(0.0000) Steps 662(659.33) | Grad Norm 5.2771(9.3971) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 20.8336, Epoch Time 572.7005(581.2215), Bit/dim 1.6916(best: 1.6932), Xent 0.0000, Loss 1.6916, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3396 | Time 78.8856(77.1029) | Bit/dim 1.6949(1.7091) | Xent 0.0000(0.0000) | Loss 1.6949(1.7091) | Error 0.0000(0.0000) Steps 656(659.23) | Grad Norm 6.2386(9.3023) | Total Time 10.00(10.00)\n",
      "Iter 3397 | Time 76.9903(77.0995) | Bit/dim 1.6948(1.7087) | Xent 0.0000(0.0000) | Loss 1.6948(1.7087) | Error 0.0000(0.0000) Steps 662(659.32) | Grad Norm 7.0374(9.2344) | Total Time 10.00(10.00)\n",
      "Iter 3398 | Time 78.8095(77.1508) | Bit/dim 1.6947(1.7082) | Xent 0.0000(0.0000) | Loss 1.6947(1.7082) | Error 0.0000(0.0000) Steps 656(659.22) | Grad Norm 8.3376(9.2075) | Total Time 10.00(10.00)\n",
      "Iter 3399 | Time 75.6044(77.1044) | Bit/dim 1.6977(1.7079) | Xent 0.0000(0.0000) | Loss 1.6977(1.7079) | Error 0.0000(0.0000) Steps 662(659.30) | Grad Norm 9.6521(9.2208) | Total Time 10.00(10.00)\n",
      "Iter 3400 | Time 76.8716(77.0974) | Bit/dim 1.6946(1.7075) | Xent 0.0000(0.0000) | Loss 1.6946(1.7075) | Error 0.0000(0.0000) Steps 656(659.20) | Grad Norm 11.1783(9.2795) | Total Time 10.00(10.00)\n",
      "Iter 3401 | Time 76.8256(77.0893) | Bit/dim 1.6936(1.7071) | Xent 0.0000(0.0000) | Loss 1.6936(1.7071) | Error 0.0000(0.0000) Steps 662(659.28) | Grad Norm 13.5106(9.4065) | Total Time 10.00(10.00)\n",
      "Iter 3402 | Time 78.2438(77.1239) | Bit/dim 1.6942(1.7067) | Xent 0.0000(0.0000) | Loss 1.6942(1.7067) | Error 0.0000(0.0000) Steps 656(659.19) | Grad Norm 15.9233(9.6020) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 21.0338, Epoch Time 575.6931(581.0557), Bit/dim 1.6906(best: 1.6916), Xent 0.0000, Loss 1.6906, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3403 | Time 75.8203(77.0848) | Bit/dim 1.6954(1.7064) | Xent 0.0000(0.0000) | Loss 1.6954(1.7064) | Error 0.0000(0.0000) Steps 662(659.27) | Grad Norm 19.0167(9.8844) | Total Time 10.00(10.00)\n",
      "Iter 3404 | Time 77.1181(77.0858) | Bit/dim 1.6976(1.7061) | Xent 0.0000(0.0000) | Loss 1.6976(1.7061) | Error 0.0000(0.0000) Steps 656(659.17) | Grad Norm 23.1866(10.2835) | Total Time 10.00(10.00)\n",
      "Iter 3405 | Time 78.8899(77.1399) | Bit/dim 1.7018(1.7060) | Xent 0.0000(0.0000) | Loss 1.7018(1.7060) | Error 0.0000(0.0000) Steps 668(659.44) | Grad Norm 26.4797(10.7694) | Total Time 10.00(10.00)\n",
      "Iter 3406 | Time 78.3667(77.1767) | Bit/dim 1.6940(1.7056) | Xent 0.0000(0.0000) | Loss 1.6940(1.7056) | Error 0.0000(0.0000) Steps 656(659.33) | Grad Norm 27.6969(11.2772) | Total Time 10.00(10.00)\n",
      "Iter 3407 | Time 78.6402(77.2206) | Bit/dim 1.6945(1.7053) | Xent 0.0000(0.0000) | Loss 1.6945(1.7053) | Error 0.0000(0.0000) Steps 668(659.59) | Grad Norm 25.9562(11.7176) | Total Time 10.00(10.00)\n",
      "Iter 3408 | Time 76.5013(77.1991) | Bit/dim 1.6969(1.7050) | Xent 0.0000(0.0000) | Loss 1.6969(1.7050) | Error 0.0000(0.0000) Steps 656(659.49) | Grad Norm 20.0231(11.9667) | Total Time 10.00(10.00)\n",
      "Iter 3409 | Time 77.9092(77.2204) | Bit/dim 1.6858(1.7045) | Xent 0.0000(0.0000) | Loss 1.6858(1.7045) | Error 0.0000(0.0000) Steps 662(659.56) | Grad Norm 11.7301(11.9596) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 21.0502, Epoch Time 576.9313(580.9320), Bit/dim 1.6845(best: 1.6906), Xent 0.0000, Loss 1.6845, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3410 | Time 77.8499(77.2392) | Bit/dim 1.6867(1.7039) | Xent 0.0000(0.0000) | Loss 1.6867(1.7039) | Error 0.0000(0.0000) Steps 662(659.63) | Grad Norm 3.1647(11.6958) | Total Time 10.00(10.00)\n",
      "Iter 3411 | Time 79.8691(77.3181) | Bit/dim 1.6927(1.7036) | Xent 0.0000(0.0000) | Loss 1.6927(1.7036) | Error 0.0000(0.0000) Steps 656(659.53) | Grad Norm 4.8612(11.4907) | Total Time 10.00(10.00)\n",
      "Iter 3412 | Time 77.2951(77.3175) | Bit/dim 1.6902(1.7032) | Xent 0.0000(0.0000) | Loss 1.6902(1.7032) | Error 0.0000(0.0000) Steps 662(659.60) | Grad Norm 10.8493(11.4715) | Total Time 10.00(10.00)\n",
      "Iter 3413 | Time 76.6394(77.2971) | Bit/dim 1.6896(1.7028) | Xent 0.0000(0.0000) | Loss 1.6896(1.7028) | Error 0.0000(0.0000) Steps 656(659.49) | Grad Norm 13.9903(11.5471) | Total Time 10.00(10.00)\n",
      "Iter 3414 | Time 76.7540(77.2808) | Bit/dim 1.6936(1.7025) | Xent 0.0000(0.0000) | Loss 1.6936(1.7025) | Error 0.0000(0.0000) Steps 662(659.57) | Grad Norm 15.1353(11.6547) | Total Time 10.00(10.00)\n",
      "Iter 3415 | Time 76.9609(77.2712) | Bit/dim 1.6898(1.7021) | Xent 0.0000(0.0000) | Loss 1.6898(1.7021) | Error 0.0000(0.0000) Steps 656(659.46) | Grad Norm 14.6729(11.7453) | Total Time 10.00(10.00)\n",
      "Iter 3416 | Time 76.0493(77.2346) | Bit/dim 1.6925(1.7018) | Xent 0.0000(0.0000) | Loss 1.6925(1.7018) | Error 0.0000(0.0000) Steps 662(659.54) | Grad Norm 13.4668(11.7969) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 20.6085, Epoch Time 574.7241(580.7457), Bit/dim 1.6844(best: 1.6845), Xent 0.0000, Loss 1.6844, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3417 | Time 78.4068(77.2697) | Bit/dim 1.6901(1.7015) | Xent 0.0000(0.0000) | Loss 1.6901(1.7015) | Error 0.0000(0.0000) Steps 656(659.43) | Grad Norm 10.8491(11.7685) | Total Time 10.00(10.00)\n",
      "Iter 3418 | Time 79.5196(77.3372) | Bit/dim 1.6903(1.7012) | Xent 0.0000(0.0000) | Loss 1.6903(1.7012) | Error 0.0000(0.0000) Steps 662(659.51) | Grad Norm 6.7928(11.6192) | Total Time 10.00(10.00)\n",
      "Iter 3419 | Time 76.0060(77.2973) | Bit/dim 1.6912(1.7009) | Xent 0.0000(0.0000) | Loss 1.6912(1.7009) | Error 0.0000(0.0000) Steps 656(659.40) | Grad Norm 3.1870(11.3662) | Total Time 10.00(10.00)\n",
      "Iter 3420 | Time 76.9001(77.2854) | Bit/dim 1.6893(1.7005) | Xent 0.0000(0.0000) | Loss 1.6893(1.7005) | Error 0.0000(0.0000) Steps 656(659.30) | Grad Norm 0.9438(11.0536) | Total Time 10.00(10.00)\n",
      "Iter 3421 | Time 76.9633(77.2757) | Bit/dim 1.6856(1.7001) | Xent 0.0000(0.0000) | Loss 1.6856(1.7001) | Error 0.0000(0.0000) Steps 662(659.38) | Grad Norm 1.0407(10.7532) | Total Time 10.00(10.00)\n",
      "Iter 3422 | Time 78.1573(77.3022) | Bit/dim 1.6946(1.6999) | Xent 0.0000(0.0000) | Loss 1.6946(1.6999) | Error 0.0000(0.0000) Steps 662(659.46) | Grad Norm 1.8706(10.4867) | Total Time 10.00(10.00)\n",
      "Iter 3423 | Time 77.8991(77.3201) | Bit/dim 1.6779(1.6992) | Xent 0.0000(0.0000) | Loss 1.6779(1.6992) | Error 0.0000(0.0000) Steps 662(659.54) | Grad Norm 2.5892(10.2498) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 20.8074, Epoch Time 577.3453(580.6437), Bit/dim 1.6809(best: 1.6844), Xent 0.0000, Loss 1.6809, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3424 | Time 75.7090(77.2717) | Bit/dim 1.6860(1.6988) | Xent 0.0000(0.0000) | Loss 1.6860(1.6988) | Error 0.0000(0.0000) Steps 656(659.43) | Grad Norm 2.8332(10.0273) | Total Time 10.00(10.00)\n",
      "Iter 3425 | Time 76.9179(77.2611) | Bit/dim 1.6885(1.6985) | Xent 0.0000(0.0000) | Loss 1.6885(1.6985) | Error 0.0000(0.0000) Steps 662(659.51) | Grad Norm 2.2180(9.7930) | Total Time 10.00(10.00)\n",
      "Iter 3426 | Time 76.6856(77.2439) | Bit/dim 1.6797(1.6980) | Xent 0.0000(0.0000) | Loss 1.6797(1.6980) | Error 0.0000(0.0000) Steps 656(659.40) | Grad Norm 1.2408(9.5364) | Total Time 10.00(10.00)\n",
      "Iter 3427 | Time 77.6090(77.2548) | Bit/dim 1.6931(1.6978) | Xent 0.0000(0.0000) | Loss 1.6931(1.6978) | Error 0.0000(0.0000) Steps 662(659.48) | Grad Norm 0.5542(9.2670) | Total Time 10.00(10.00)\n",
      "Iter 3428 | Time 76.6853(77.2377) | Bit/dim 1.6821(1.6973) | Xent 0.0000(0.0000) | Loss 1.6821(1.6973) | Error 0.0000(0.0000) Steps 662(659.56) | Grad Norm 1.7996(9.0429) | Total Time 10.00(10.00)\n",
      "Iter 3429 | Time 76.1706(77.2057) | Bit/dim 1.6921(1.6972) | Xent 0.0000(0.0000) | Loss 1.6921(1.6972) | Error 0.0000(0.0000) Steps 662(659.63) | Grad Norm 3.5562(8.8783) | Total Time 10.00(10.00)\n",
      "Iter 3430 | Time 75.8614(77.1654) | Bit/dim 1.6780(1.6966) | Xent 0.0000(0.0000) | Loss 1.6780(1.6966) | Error 0.0000(0.0000) Steps 662(659.70) | Grad Norm 5.9554(8.7906) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0490 | Time 20.5732, Epoch Time 568.9928(580.2942), Bit/dim 1.6797(best: 1.6809), Xent 0.0000, Loss 1.6797, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3431 | Time 76.6900(77.1511) | Bit/dim 1.6824(1.6962) | Xent 0.0000(0.0000) | Loss 1.6824(1.6962) | Error 0.0000(0.0000) Steps 656(659.59) | Grad Norm 9.5448(8.8133) | Total Time 10.00(10.00)\n",
      "Iter 3432 | Time 76.7836(77.1401) | Bit/dim 1.6883(1.6959) | Xent 0.0000(0.0000) | Loss 1.6883(1.6959) | Error 0.0000(0.0000) Steps 662(659.66) | Grad Norm 15.2332(9.0059) | Total Time 10.00(10.00)\n",
      "Iter 3433 | Time 78.3131(77.1753) | Bit/dim 1.6777(1.6954) | Xent 0.0000(0.0000) | Loss 1.6777(1.6954) | Error 0.0000(0.0000) Steps 656(659.55) | Grad Norm 23.2734(9.4339) | Total Time 10.00(10.00)\n",
      "Iter 3434 | Time 76.8685(77.1661) | Bit/dim 1.6889(1.6952) | Xent 0.0000(0.0000) | Loss 1.6889(1.6952) | Error 0.0000(0.0000) Steps 680(660.16) | Grad Norm 33.0646(10.1428) | Total Time 10.00(10.00)\n",
      "Iter 3435 | Time 76.6824(77.1516) | Bit/dim 1.6928(1.6951) | Xent 0.0000(0.0000) | Loss 1.6928(1.6951) | Error 0.0000(0.0000) Steps 656(660.04) | Grad Norm 42.9124(11.1259) | Total Time 10.00(10.00)\n",
      "Iter 3436 | Time 77.7489(77.1695) | Bit/dim 1.6958(1.6952) | Xent 0.0000(0.0000) | Loss 1.6958(1.6952) | Error 0.0000(0.0000) Steps 680(660.64) | Grad Norm 46.6955(12.1930) | Total Time 10.00(10.00)\n",
      "Iter 3437 | Time 75.9476(77.1328) | Bit/dim 1.6928(1.6951) | Xent 0.0000(0.0000) | Loss 1.6928(1.6951) | Error 0.0000(0.0000) Steps 656(660.50) | Grad Norm 37.1763(12.9425) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0491 | Time 21.0389, Epoch Time 572.7543(580.0680), Bit/dim 1.6781(best: 1.6797), Xent 0.0000, Loss 1.6781, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3438 | Time 76.9959(77.1287) | Bit/dim 1.6764(1.6945) | Xent 0.0000(0.0000) | Loss 1.6764(1.6945) | Error 0.0000(0.0000) Steps 662(660.54) | Grad Norm 16.8214(13.0589) | Total Time 10.00(10.00)\n",
      "Iter 3440 | Time 77.1894(77.1077) | Bit/dim 1.6842(1.6937) | Xent 0.0000(0.0000) | Loss 1.6842(1.6937) | Error 0.0000(0.0000) Steps 656(660.45) | Grad Norm 24.4197(13.1991) | Total Time 10.00(10.00)\n",
      "Iter 3441 | Time 80.1814(77.1999) | Bit/dim 1.6949(1.6938) | Xent 0.0000(0.0000) | Loss 1.6949(1.6938) | Error 0.0000(0.0000) Steps 668(660.68) | Grad Norm 30.5113(13.7184) | Total Time 10.00(10.00)\n",
      "Iter 3442 | Time 77.0598(77.1957) | Bit/dim 1.6841(1.6935) | Xent 0.0000(0.0000) | Loss 1.6841(1.6935) | Error 0.0000(0.0000) Steps 656(660.54) | Grad Norm 20.7240(13.9286) | Total Time 10.00(10.00)\n",
      "Iter 3443 | Time 78.7098(77.2412) | Bit/dim 1.6827(1.6931) | Xent 0.0000(0.0000) | Loss 1.6827(1.6931) | Error 0.0000(0.0000) Steps 656(660.40) | Grad Norm 1.8912(13.5675) | Total Time 10.00(10.00)\n",
      "Iter 3444 | Time 77.9442(77.2623) | Bit/dim 1.6819(1.6928) | Xent 0.0000(0.0000) | Loss 1.6819(1.6928) | Error 0.0000(0.0000) Steps 662(660.45) | Grad Norm 16.4645(13.6544) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 20.6053, Epoch Time 577.6986(579.9969), Bit/dim 1.6785(best: 1.6781), Xent 0.0000, Loss 1.6785, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3445 | Time 78.4535(77.2980) | Bit/dim 1.6826(1.6925) | Xent 0.0000(0.0000) | Loss 1.6826(1.6925) | Error 0.0000(0.0000) Steps 656(660.32) | Grad Norm 25.2543(14.0024) | Total Time 10.00(10.00)\n",
      "Iter 3446 | Time 77.2876(77.2977) | Bit/dim 1.6800(1.6921) | Xent 0.0000(0.0000) | Loss 1.6800(1.6921) | Error 0.0000(0.0000) Steps 662(660.37) | Grad Norm 21.0044(14.2125) | Total Time 10.00(10.00)\n",
      "Iter 3447 | Time 76.9932(77.2885) | Bit/dim 1.6747(1.6916) | Xent 0.0000(0.0000) | Loss 1.6747(1.6916) | Error 0.0000(0.0000) Steps 656(660.23) | Grad Norm 6.2291(13.9730) | Total Time 10.00(10.00)\n",
      "Iter 3448 | Time 77.8708(77.3060) | Bit/dim 1.6795(1.6912) | Xent 0.0000(0.0000) | Loss 1.6795(1.6912) | Error 0.0000(0.0000) Steps 656(660.11) | Grad Norm 10.0677(13.8558) | Total Time 10.00(10.00)\n",
      "Iter 3449 | Time 77.0550(77.2985) | Bit/dim 1.6828(1.6910) | Xent 0.0000(0.0000) | Loss 1.6828(1.6910) | Error 0.0000(0.0000) Steps 662(660.16) | Grad Norm 19.9647(14.0391) | Total Time 10.00(10.00)\n",
      "Iter 3450 | Time 76.1046(77.2627) | Bit/dim 1.6800(1.6907) | Xent 0.0000(0.0000) | Loss 1.6800(1.6907) | Error 0.0000(0.0000) Steps 656(660.04) | Grad Norm 19.5573(14.2046) | Total Time 10.00(10.00)\n",
      "Iter 3451 | Time 77.8583(77.2805) | Bit/dim 1.6805(1.6904) | Xent 0.0000(0.0000) | Loss 1.6805(1.6904) | Error 0.0000(0.0000) Steps 662(660.10) | Grad Norm 9.5477(14.0649) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 20.7168, Epoch Time 574.9621(579.8459), Bit/dim 1.6720(best: 1.6781), Xent 0.0000, Loss 1.6720, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3452 | Time 78.0331(77.3031) | Bit/dim 1.6809(1.6901) | Xent 0.0000(0.0000) | Loss 1.6809(1.6901) | Error 0.0000(0.0000) Steps 656(659.98) | Grad Norm 4.3035(13.7721) | Total Time 10.00(10.00)\n",
      "Iter 3453 | Time 75.5132(77.2494) | Bit/dim 1.6783(1.6897) | Xent 0.0000(0.0000) | Loss 1.6783(1.6897) | Error 0.0000(0.0000) Steps 656(659.86) | Grad Norm 14.0712(13.7810) | Total Time 10.00(10.00)\n",
      "Iter 3454 | Time 77.4135(77.2543) | Bit/dim 1.6782(1.6894) | Xent 0.0000(0.0000) | Loss 1.6782(1.6894) | Error 0.0000(0.0000) Steps 662(659.92) | Grad Norm 15.3922(13.8294) | Total Time 10.00(10.00)\n",
      "Iter 3455 | Time 77.4870(77.2613) | Bit/dim 1.6759(1.6890) | Xent 0.0000(0.0000) | Loss 1.6759(1.6890) | Error 0.0000(0.0000) Steps 656(659.80) | Grad Norm 8.4573(13.6682) | Total Time 10.00(10.00)\n",
      "Iter 3456 | Time 77.4916(77.2682) | Bit/dim 1.6752(1.6886) | Xent 0.0000(0.0000) | Loss 1.6752(1.6886) | Error 0.0000(0.0000) Steps 656(659.69) | Grad Norm 2.2796(13.3265) | Total Time 10.00(10.00)\n",
      "Iter 3457 | Time 76.5281(77.2460) | Bit/dim 1.6736(1.6881) | Xent 0.0000(0.0000) | Loss 1.6736(1.6881) | Error 0.0000(0.0000) Steps 662(659.76) | Grad Norm 10.5362(13.2428) | Total Time 10.00(10.00)\n",
      "Iter 3458 | Time 76.3349(77.2187) | Bit/dim 1.6795(1.6878) | Xent 0.0000(0.0000) | Loss 1.6795(1.6878) | Error 0.0000(0.0000) Steps 656(659.65) | Grad Norm 12.7736(13.2288) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 21.0213, Epoch Time 572.9885(579.6401), Bit/dim 1.6707(best: 1.6720), Xent 0.0000, Loss 1.6707, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3459 | Time 75.8948(77.1790) | Bit/dim 1.6778(1.6875) | Xent 0.0000(0.0000) | Loss 1.6778(1.6875) | Error 0.0000(0.0000) Steps 662(659.72) | Grad Norm 8.1891(13.0776) | Total Time 10.00(10.00)\n",
      "Iter 3460 | Time 76.7389(77.1658) | Bit/dim 1.6774(1.6872) | Xent 0.0000(0.0000) | Loss 1.6774(1.6872) | Error 0.0000(0.0000) Steps 650(659.42) | Grad Norm 0.9552(12.7139) | Total Time 10.00(10.00)\n",
      "Iter 3461 | Time 78.1579(77.1955) | Bit/dim 1.6714(1.6868) | Xent 0.0000(0.0000) | Loss 1.6714(1.6868) | Error 0.0000(0.0000) Steps 656(659.32) | Grad Norm 5.5337(12.4985) | Total Time 10.00(10.00)\n",
      "Iter 3462 | Time 77.1692(77.1947) | Bit/dim 1.6768(1.6865) | Xent 0.0000(0.0000) | Loss 1.6768(1.6865) | Error 0.0000(0.0000) Steps 662(659.40) | Grad Norm 9.5250(12.4093) | Total Time 10.00(10.00)\n",
      "Iter 3463 | Time 78.2704(77.2270) | Bit/dim 1.6711(1.6860) | Xent 0.0000(0.0000) | Loss 1.6711(1.6860) | Error 0.0000(0.0000) Steps 650(659.12) | Grad Norm 9.7839(12.3305) | Total Time 10.00(10.00)\n",
      "Iter 3464 | Time 79.1425(77.2845) | Bit/dim 1.6777(1.6858) | Xent 0.0000(0.0000) | Loss 1.6777(1.6858) | Error 0.0000(0.0000) Steps 656(659.03) | Grad Norm 6.3860(12.1522) | Total Time 10.00(10.00)\n",
      "Iter 3465 | Time 76.5286(77.2618) | Bit/dim 1.6756(1.6855) | Xent 0.0000(0.0000) | Loss 1.6756(1.6855) | Error 0.0000(0.0000) Steps 656(658.94) | Grad Norm 1.8161(11.8421) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 20.7709, Epoch Time 575.5530(579.5175), Bit/dim 1.6671(best: 1.6707), Xent 0.0000, Loss 1.6671, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3466 | Time 78.5861(77.3015) | Bit/dim 1.6718(1.6850) | Xent 0.0000(0.0000) | Loss 1.6718(1.6850) | Error 0.0000(0.0000) Steps 656(658.85) | Grad Norm 3.3200(11.5864) | Total Time 10.00(10.00)\n",
      "Iter 3467 | Time 76.5082(77.2777) | Bit/dim 1.6725(1.6847) | Xent 0.0000(0.0000) | Loss 1.6725(1.6847) | Error 0.0000(0.0000) Steps 656(658.76) | Grad Norm 7.2913(11.4576) | Total Time 10.00(10.00)\n",
      "Iter 3468 | Time 76.9931(77.2692) | Bit/dim 1.6750(1.6844) | Xent 0.0000(0.0000) | Loss 1.6750(1.6844) | Error 0.0000(0.0000) Steps 650(658.50) | Grad Norm 8.5125(11.3692) | Total Time 10.00(10.00)\n",
      "Iter 3469 | Time 78.1288(77.2950) | Bit/dim 1.6699(1.6839) | Xent 0.0000(0.0000) | Loss 1.6699(1.6839) | Error 0.0000(0.0000) Steps 656(658.42) | Grad Norm 7.4564(11.2518) | Total Time 10.00(10.00)\n",
      "Iter 3470 | Time 76.0523(77.2577) | Bit/dim 1.6796(1.6838) | Xent 0.0000(0.0000) | Loss 1.6796(1.6838) | Error 0.0000(0.0000) Steps 650(658.17) | Grad Norm 4.6583(11.0540) | Total Time 10.00(10.00)\n",
      "Iter 3471 | Time 77.5054(77.2651) | Bit/dim 1.6719(1.6835) | Xent 0.0000(0.0000) | Loss 1.6719(1.6835) | Error 0.0000(0.0000) Steps 650(657.93) | Grad Norm 1.2024(10.7585) | Total Time 10.00(10.00)\n",
      "Iter 3472 | Time 79.3351(77.3272) | Bit/dim 1.6687(1.6830) | Xent 0.0000(0.0000) | Loss 1.6687(1.6830) | Error 0.0000(0.0000) Steps 656(657.87) | Grad Norm 2.3490(10.5062) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 20.8710, Epoch Time 576.8044(579.4361), Bit/dim 1.6664(best: 1.6671), Xent 0.0000, Loss 1.6664, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3473 | Time 77.9840(77.3469) | Bit/dim 1.6731(1.6827) | Xent 0.0000(0.0000) | Loss 1.6731(1.6827) | Error 0.0000(0.0000) Steps 650(657.63) | Grad Norm 5.3422(10.3513) | Total Time 10.00(10.00)\n",
      "Iter 3474 | Time 77.9818(77.3660) | Bit/dim 1.6710(1.6824) | Xent 0.0000(0.0000) | Loss 1.6710(1.6824) | Error 0.0000(0.0000) Steps 656(657.58) | Grad Norm 7.5674(10.2678) | Total Time 10.00(10.00)\n",
      "Iter 3475 | Time 75.9893(77.3247) | Bit/dim 1.6746(1.6821) | Xent 0.0000(0.0000) | Loss 1.6746(1.6821) | Error 0.0000(0.0000) Steps 650(657.36) | Grad Norm 8.3001(10.2087) | Total Time 10.00(10.00)\n",
      "Iter 3476 | Time 76.7081(77.3062) | Bit/dim 1.6699(1.6818) | Xent 0.0000(0.0000) | Loss 1.6699(1.6818) | Error 0.0000(0.0000) Steps 656(657.32) | Grad Norm 8.9963(10.1724) | Total Time 10.00(10.00)\n",
      "Iter 3477 | Time 76.5367(77.2831) | Bit/dim 1.6690(1.6814) | Xent 0.0000(0.0000) | Loss 1.6690(1.6814) | Error 0.0000(0.0000) Steps 650(657.10) | Grad Norm 9.5523(10.1538) | Total Time 10.00(10.00)\n",
      "Iter 3478 | Time 76.9674(77.2736) | Bit/dim 1.6649(1.6809) | Xent 0.0000(0.0000) | Loss 1.6649(1.6809) | Error 0.0000(0.0000) Steps 656(657.06) | Grad Norm 9.5872(10.1368) | Total Time 10.00(10.00)\n",
      "Iter 3479 | Time 78.8060(77.3196) | Bit/dim 1.6733(1.6807) | Xent 0.0000(0.0000) | Loss 1.6733(1.6807) | Error 0.0000(0.0000) Steps 650(656.85) | Grad Norm 9.8160(10.1271) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 21.0533, Epoch Time 575.0497(579.3045), Bit/dim 1.6643(best: 1.6664), Xent 0.0000, Loss 1.6643, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3480 | Time 76.9632(77.3089) | Bit/dim 1.6637(1.6801) | Xent 0.0000(0.0000) | Loss 1.6637(1.6801) | Error 0.0000(0.0000) Steps 656(656.83) | Grad Norm 9.9758(10.1226) | Total Time 10.00(10.00)\n",
      "Iter 3481 | Time 77.6579(77.3194) | Bit/dim 1.6620(1.6796) | Xent 0.0000(0.0000) | Loss 1.6620(1.6796) | Error 0.0000(0.0000) Steps 650(656.62) | Grad Norm 8.8258(10.0837) | Total Time 10.00(10.00)\n",
      "Iter 3482 | Time 75.6910(77.2705) | Bit/dim 1.6703(1.6793) | Xent 0.0000(0.0000) | Loss 1.6703(1.6793) | Error 0.0000(0.0000) Steps 656(656.60) | Grad Norm 7.3806(10.0026) | Total Time 10.00(10.00)\n",
      "Iter 3483 | Time 75.8825(77.2289) | Bit/dim 1.6697(1.6790) | Xent 0.0000(0.0000) | Loss 1.6697(1.6790) | Error 0.0000(0.0000) Steps 650(656.40) | Grad Norm 6.3206(9.8921) | Total Time 10.00(10.00)\n",
      "Iter 3484 | Time 76.8858(77.2186) | Bit/dim 1.6675(1.6787) | Xent 0.0000(0.0000) | Loss 1.6675(1.6787) | Error 0.0000(0.0000) Steps 656(656.39) | Grad Norm 5.4979(9.7603) | Total Time 10.00(10.00)\n",
      "Iter 3485 | Time 76.0306(77.1830) | Bit/dim 1.6727(1.6785) | Xent 0.0000(0.0000) | Loss 1.6727(1.6785) | Error 0.0000(0.0000) Steps 650(656.20) | Grad Norm 5.9082(9.6448) | Total Time 10.00(10.00)\n",
      "Iter 3486 | Time 77.3624(77.1883) | Bit/dim 1.6725(1.6783) | Xent 0.0000(0.0000) | Loss 1.6725(1.6783) | Error 0.0000(0.0000) Steps 656(656.19) | Grad Norm 7.8166(9.5899) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 20.6558, Epoch Time 569.8017(579.0194), Bit/dim 1.6625(best: 1.6643), Xent 0.0000, Loss 1.6625, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3487 | Time 77.1144(77.1861) | Bit/dim 1.6671(1.6780) | Xent 0.0000(0.0000) | Loss 1.6671(1.6780) | Error 0.0000(0.0000) Steps 650(656.01) | Grad Norm 10.3860(9.6138) | Total Time 10.00(10.00)\n",
      "Iter 3488 | Time 75.7272(77.1424) | Bit/dim 1.6716(1.6778) | Xent 0.0000(0.0000) | Loss 1.6716(1.6778) | Error 0.0000(0.0000) Steps 656(656.01) | Grad Norm 15.0885(9.7780) | Total Time 10.00(10.00)\n",
      "Iter 3489 | Time 78.5871(77.1857) | Bit/dim 1.6700(1.6776) | Xent 0.0000(0.0000) | Loss 1.6700(1.6776) | Error 0.0000(0.0000) Steps 650(655.83) | Grad Norm 22.2031(10.1508) | Total Time 10.00(10.00)\n",
      "Iter 3490 | Time 75.9888(77.1498) | Bit/dim 1.6742(1.6775) | Xent 0.0000(0.0000) | Loss 1.6742(1.6775) | Error 0.0000(0.0000) Steps 662(656.01) | Grad Norm 30.6101(10.7646) | Total Time 10.00(10.00)\n",
      "Iter 3491 | Time 77.9175(77.1728) | Bit/dim 1.6690(1.6772) | Xent 0.0000(0.0000) | Loss 1.6690(1.6772) | Error 0.0000(0.0000) Steps 650(655.83) | Grad Norm 39.9103(11.6389) | Total Time 10.00(10.00)\n",
      "Iter 3492 | Time 78.6539(77.2173) | Bit/dim 1.6754(1.6772) | Xent 0.0000(0.0000) | Loss 1.6754(1.6772) | Error 0.0000(0.0000) Steps 680(656.56) | Grad Norm 46.2010(12.6758) | Total Time 10.00(10.00)\n",
      "Iter 3493 | Time 75.2100(77.1570) | Bit/dim 1.6747(1.6771) | Xent 0.0000(0.0000) | Loss 1.6747(1.6771) | Error 0.0000(0.0000) Steps 650(656.36) | Grad Norm 44.4487(13.6290) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 21.1625, Epoch Time 572.9740(578.8381), Bit/dim 1.6664(best: 1.6625), Xent 0.0000, Loss 1.6664, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 3494 | Time 77.1411(77.1566) | Bit/dim 1.6662(1.6768) | Xent 0.0000(0.0000) | Loss 1.6662(1.6768) | Error 0.0000(0.0000) Steps 662(656.53) | Grad Norm 33.9237(14.2378) | Total Time 10.00(10.00)\n",
      "Iter 3495 | Time 78.5035(77.1970) | Bit/dim 1.6713(1.6766) | Xent 0.0000(0.0000) | Loss 1.6713(1.6766) | Error 0.0000(0.0000) Steps 650(656.33) | Grad Norm 15.2251(14.2674) | Total Time 10.00(10.00)\n",
      "Iter 3496 | Time 76.2157(77.1675) | Bit/dim 1.6648(1.6762) | Xent 0.0000(0.0000) | Loss 1.6648(1.6762) | Error 0.0000(0.0000) Steps 650(656.14) | Grad Norm 6.2564(14.0271) | Total Time 10.00(10.00)\n",
      "Iter 3497 | Time 76.8926(77.1593) | Bit/dim 1.6692(1.6760) | Xent 0.0000(0.0000) | Loss 1.6692(1.6760) | Error 0.0000(0.0000) Steps 656(656.14) | Grad Norm 23.1350(14.3004) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_published_bs8K_errcontrol_7gpus_3 --resume ../experiments_published/cnf_published_bs8K_errcontrol_7gpus_3/epoch_400_checkpt.pth --seed 3 --conditional False --controlled_tol True --lr 0.0001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
