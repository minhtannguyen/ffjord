{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=False, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_cifar10_bs8K_rl_stdlearnscale_30_run3/epoch_203_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_cifar10_bs8K_rl_stdlearnscale_30_run3', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 1219 | Time 116.7116(60.1095) | Bit/dim 3.5290(3.5511) | Xent 0.0000(0.0000) | Loss 10.6764(9.0202) | Error 0.0000(0.0000) Steps 628(614.04) | Grad Norm 0.9354(2.2665) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 57.7396(60.0384) | Bit/dim 3.5205(3.5501) | Xent 0.0000(0.0000) | Loss 8.4411(9.0029) | Error 0.0000(0.0000) Steps 616(614.09) | Grad Norm 1.1292(2.2323) | Total Time 0.00(0.00)\n",
      "Iter 1221 | Time 62.7332(60.1193) | Bit/dim 3.5261(3.5494) | Xent 0.0000(0.0000) | Loss 8.5372(8.9889) | Error 0.0000(0.0000) Steps 634(614.69) | Grad Norm 0.8433(2.1907) | Total Time 0.00(0.00)\n",
      "Iter 1222 | Time 57.6630(60.0456) | Bit/dim 3.5139(3.5484) | Xent 0.0000(0.0000) | Loss 8.4008(8.9713) | Error 0.0000(0.0000) Steps 616(614.73) | Grad Norm 0.9093(2.1522) | Total Time 0.00(0.00)\n",
      "Iter 1223 | Time 57.9662(59.9832) | Bit/dim 3.5298(3.5478) | Xent 0.0000(0.0000) | Loss 8.4626(8.9560) | Error 0.0000(0.0000) Steps 628(615.13) | Grad Norm 0.7853(2.1112) | Total Time 0.00(0.00)\n",
      "Iter 1224 | Time 59.2565(59.9614) | Bit/dim 3.5282(3.5472) | Xent 0.0000(0.0000) | Loss 8.3300(8.9372) | Error 0.0000(0.0000) Steps 622(615.34) | Grad Norm 0.8079(2.0721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 36.0283, Epoch Time 464.3318(361.9386), Bit/dim 3.5292(best: inf), Xent 0.0000, Loss 3.5292, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1225 | Time 66.9160(60.1700) | Bit/dim 3.5276(3.5466) | Xent 0.0000(0.0000) | Loss 12.1513(9.0336) | Error 0.0000(0.0000) Steps 610(615.18) | Grad Norm 0.9231(2.0376) | Total Time 0.00(0.00)\n",
      "Iter 1226 | Time 57.9745(60.1042) | Bit/dim 3.5268(3.5460) | Xent 0.0000(0.0000) | Loss 8.3866(9.0142) | Error 0.0000(0.0000) Steps 598(614.66) | Grad Norm 0.7487(1.9990) | Total Time 0.00(0.00)\n",
      "Iter 1227 | Time 59.5668(60.0880) | Bit/dim 3.5262(3.5454) | Xent 0.0000(0.0000) | Loss 8.4095(8.9961) | Error 0.0000(0.0000) Steps 610(614.52) | Grad Norm 0.7947(1.9629) | Total Time 0.00(0.00)\n",
      "Iter 1228 | Time 63.8989(60.2024) | Bit/dim 3.5266(3.5449) | Xent 0.0000(0.0000) | Loss 8.5773(8.9835) | Error 0.0000(0.0000) Steps 646(615.46) | Grad Norm 0.6215(1.9226) | Total Time 0.00(0.00)\n",
      "Iter 1229 | Time 57.4234(60.1190) | Bit/dim 3.5160(3.5440) | Xent 0.0000(0.0000) | Loss 7.9242(8.9517) | Error 0.0000(0.0000) Steps 580(614.40) | Grad Norm 0.8122(1.8893) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 61.7414(60.1677) | Bit/dim 3.5251(3.5434) | Xent 0.0000(0.0000) | Loss 8.2251(8.9299) | Error 0.0000(0.0000) Steps 622(614.63) | Grad Norm 0.9935(1.8624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 22.7520, Epoch Time 406.4703(363.2745), Bit/dim 3.5299(best: 3.5292), Xent 0.0000, Loss 3.5299, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1231 | Time 61.1970(60.1986) | Bit/dim 3.5334(3.5431) | Xent 0.0000(0.0000) | Loss 12.6366(9.0411) | Error 0.0000(0.0000) Steps 634(615.21) | Grad Norm 0.7903(1.8303) | Total Time 0.00(0.00)\n",
      "Iter 1232 | Time 66.3389(60.3828) | Bit/dim 3.5230(3.5425) | Xent 0.0000(0.0000) | Loss 8.4376(9.0230) | Error 0.0000(0.0000) Steps 628(615.59) | Grad Norm 0.6062(1.7935) | Total Time 0.00(0.00)\n",
      "Iter 1233 | Time 58.6126(60.3297) | Bit/dim 3.5124(3.5416) | Xent 0.0000(0.0000) | Loss 8.3145(9.0018) | Error 0.0000(0.0000) Steps 616(615.61) | Grad Norm 0.5381(1.7559) | Total Time 0.00(0.00)\n",
      "Iter 1234 | Time 57.1887(60.2354) | Bit/dim 3.5217(3.5410) | Xent 0.0000(0.0000) | Loss 8.0956(8.9746) | Error 0.0000(0.0000) Steps 628(615.98) | Grad Norm 0.4945(1.7180) | Total Time 0.00(0.00)\n",
      "Iter 1235 | Time 60.1091(60.2316) | Bit/dim 3.5215(3.5404) | Xent 0.0000(0.0000) | Loss 8.3903(8.9571) | Error 0.0000(0.0000) Steps 628(616.34) | Grad Norm 0.5823(1.6840) | Total Time 0.00(0.00)\n",
      "Iter 1236 | Time 61.0634(60.2566) | Bit/dim 3.5304(3.5401) | Xent 0.0000(0.0000) | Loss 8.6475(8.9478) | Error 0.0000(0.0000) Steps 652(617.41) | Grad Norm 0.4749(1.6477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 23.2031, Epoch Time 404.0642(364.4982), Bit/dim 3.5260(best: 3.5292), Xent 0.0000, Loss 3.5260, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1237 | Time 61.7091(60.3002) | Bit/dim 3.5282(3.5398) | Xent 0.0000(0.0000) | Loss 12.3801(9.0507) | Error 0.0000(0.0000) Steps 622(617.55) | Grad Norm 0.3686(1.6093) | Total Time 0.00(0.00)\n",
      "Iter 1238 | Time 60.2633(60.2991) | Bit/dim 3.5190(3.5392) | Xent 0.0000(0.0000) | Loss 8.0767(9.0215) | Error 0.0000(0.0000) Steps 598(616.96) | Grad Norm 0.5735(1.5782) | Total Time 0.00(0.00)\n",
      "Iter 1239 | Time 60.7671(60.3131) | Bit/dim 3.5214(3.5386) | Xent 0.0000(0.0000) | Loss 8.4289(9.0037) | Error 0.0000(0.0000) Steps 634(617.47) | Grad Norm 0.4494(1.5444) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 61.3384(60.3439) | Bit/dim 3.5147(3.5379) | Xent 0.0000(0.0000) | Loss 8.5266(8.9894) | Error 0.0000(0.0000) Steps 628(617.79) | Grad Norm 0.4095(1.5103) | Total Time 0.00(0.00)\n",
      "Iter 1241 | Time 58.8091(60.2978) | Bit/dim 3.5270(3.5376) | Xent 0.0000(0.0000) | Loss 8.4180(8.9723) | Error 0.0000(0.0000) Steps 634(618.27) | Grad Norm 0.8294(1.4899) | Total Time 0.00(0.00)\n",
      "Iter 1242 | Time 60.8114(60.3132) | Bit/dim 3.5336(3.5375) | Xent 0.0000(0.0000) | Loss 8.6077(8.9614) | Error 0.0000(0.0000) Steps 628(618.56) | Grad Norm 0.5000(1.4602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 23.0453, Epoch Time 402.9205(365.6509), Bit/dim 3.5296(best: 3.5260), Xent 0.0000, Loss 3.5296, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1243 | Time 58.6831(60.2643) | Bit/dim 3.5296(3.5372) | Xent 0.0000(0.0000) | Loss 12.1082(9.0558) | Error 0.0000(0.0000) Steps 640(619.21) | Grad Norm 0.8202(1.4410) | Total Time 0.00(0.00)\n",
      "Iter 1244 | Time 56.9319(60.1643) | Bit/dim 3.5260(3.5369) | Xent 0.0000(0.0000) | Loss 8.4297(9.0370) | Error 0.0000(0.0000) Steps 640(619.83) | Grad Norm 0.5979(1.4157) | Total Time 0.00(0.00)\n",
      "Iter 1245 | Time 58.2009(60.1054) | Bit/dim 3.5145(3.5362) | Xent 0.0000(0.0000) | Loss 8.3262(9.0157) | Error 0.0000(0.0000) Steps 628(620.08) | Grad Norm 0.4545(1.3869) | Total Time 0.00(0.00)\n",
      "Iter 1246 | Time 56.7752(60.0055) | Bit/dim 3.5355(3.5362) | Xent 0.0000(0.0000) | Loss 8.3825(8.9967) | Error 0.0000(0.0000) Steps 616(619.95) | Grad Norm 1.0566(1.3770) | Total Time 0.00(0.00)\n",
      "Iter 1247 | Time 60.2658(60.0133) | Bit/dim 3.5123(3.5355) | Xent 0.0000(0.0000) | Loss 8.4984(8.9817) | Error 0.0000(0.0000) Steps 634(620.38) | Grad Norm 0.4051(1.3478) | Total Time 0.00(0.00)\n",
      "Iter 1248 | Time 60.6326(60.0319) | Bit/dim 3.5194(3.5350) | Xent 0.0000(0.0000) | Loss 8.2663(8.9603) | Error 0.0000(0.0000) Steps 628(620.60) | Grad Norm 0.7074(1.3286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 22.8336, Epoch Time 390.5715(366.3985), Bit/dim 3.5297(best: 3.5260), Xent 0.0000, Loss 3.5297, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1249 | Time 59.0857(60.0035) | Bit/dim 3.5182(3.5345) | Xent 0.0000(0.0000) | Loss 12.3376(9.0616) | Error 0.0000(0.0000) Steps 652(621.55) | Grad Norm 0.5914(1.3065) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 58.4825(59.9579) | Bit/dim 3.5251(3.5342) | Xent 0.0000(0.0000) | Loss 8.3260(9.0395) | Error 0.0000(0.0000) Steps 622(621.56) | Grad Norm 0.8385(1.2924) | Total Time 0.00(0.00)\n",
      "Iter 1251 | Time 58.9255(59.9269) | Bit/dim 3.5219(3.5338) | Xent 0.0000(0.0000) | Loss 8.5012(9.0234) | Error 0.0000(0.0000) Steps 640(622.11) | Grad Norm 0.4712(1.2678) | Total Time 0.00(0.00)\n",
      "Iter 1252 | Time 59.6645(59.9191) | Bit/dim 3.5390(3.5340) | Xent 0.0000(0.0000) | Loss 8.4256(9.0054) | Error 0.0000(0.0000) Steps 634(622.47) | Grad Norm 0.4018(1.2418) | Total Time 0.00(0.00)\n",
      "Iter 1253 | Time 59.4208(59.9041) | Bit/dim 3.5238(3.5337) | Xent 0.0000(0.0000) | Loss 8.3890(8.9869) | Error 0.0000(0.0000) Steps 628(622.64) | Grad Norm 0.4767(1.2189) | Total Time 0.00(0.00)\n",
      "Iter 1254 | Time 56.3289(59.7969) | Bit/dim 3.5263(3.5335) | Xent 0.0000(0.0000) | Loss 8.4093(8.9696) | Error 0.0000(0.0000) Steps 634(622.98) | Grad Norm 1.3722(1.2235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 22.6943, Epoch Time 390.5687(367.1236), Bit/dim 3.5285(best: 3.5260), Xent 0.0000, Loss 3.5285, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1255 | Time 59.6589(59.7927) | Bit/dim 3.5124(3.5328) | Xent 0.0000(0.0000) | Loss 11.8485(9.0560) | Error 0.0000(0.0000) Steps 598(622.23) | Grad Norm 0.5960(1.2047) | Total Time 0.00(0.00)\n",
      "Iter 1256 | Time 57.7324(59.7309) | Bit/dim 3.5204(3.5325) | Xent 0.0000(0.0000) | Loss 8.4247(9.0370) | Error 0.0000(0.0000) Steps 622(622.22) | Grad Norm 0.9064(1.1957) | Total Time 0.00(0.00)\n",
      "Iter 1257 | Time 60.4813(59.7534) | Bit/dim 3.5315(3.5324) | Xent 0.0000(0.0000) | Loss 8.4694(9.0200) | Error 0.0000(0.0000) Steps 634(622.57) | Grad Norm 0.8747(1.1861) | Total Time 0.00(0.00)\n",
      "Iter 1258 | Time 59.3882(59.7425) | Bit/dim 3.5258(3.5322) | Xent 0.0000(0.0000) | Loss 8.6119(9.0078) | Error 0.0000(0.0000) Steps 616(622.38) | Grad Norm 0.9605(1.1793) | Total Time 0.00(0.00)\n",
      "Iter 1259 | Time 59.3502(59.7307) | Bit/dim 3.5198(3.5319) | Xent 0.0000(0.0000) | Loss 8.2702(8.9856) | Error 0.0000(0.0000) Steps 622(622.37) | Grad Norm 0.7376(1.1661) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 59.7900(59.7325) | Bit/dim 3.5285(3.5318) | Xent 0.0000(0.0000) | Loss 8.3157(8.9655) | Error 0.0000(0.0000) Steps 610(621.99) | Grad Norm 0.5711(1.1482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 22.4881, Epoch Time 394.9658(367.9589), Bit/dim 3.5264(best: 3.5260), Xent 0.0000, Loss 3.5264, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1261 | Time 57.6827(59.6710) | Bit/dim 3.5285(3.5317) | Xent 0.0000(0.0000) | Loss 11.9854(9.0561) | Error 0.0000(0.0000) Steps 628(622.17) | Grad Norm 1.4624(1.1576) | Total Time 0.00(0.00)\n",
      "Iter 1262 | Time 61.7269(59.7327) | Bit/dim 3.5172(3.5312) | Xent 0.0000(0.0000) | Loss 8.5392(9.0406) | Error 0.0000(0.0000) Steps 652(623.07) | Grad Norm 0.5076(1.1381) | Total Time 0.00(0.00)\n",
      "Iter 1263 | Time 60.5536(59.7573) | Bit/dim 3.5200(3.5309) | Xent 0.0000(0.0000) | Loss 8.4959(9.0243) | Error 0.0000(0.0000) Steps 622(623.04) | Grad Norm 0.4683(1.1180) | Total Time 0.00(0.00)\n",
      "Iter 1264 | Time 64.1134(59.8880) | Bit/dim 3.5254(3.5307) | Xent 0.0000(0.0000) | Loss 8.4797(9.0079) | Error 0.0000(0.0000) Steps 646(623.73) | Grad Norm 0.5082(1.0997) | Total Time 0.00(0.00)\n",
      "Iter 1265 | Time 63.9989(60.0113) | Bit/dim 3.5368(3.5309) | Xent 0.0000(0.0000) | Loss 8.6094(8.9960) | Error 0.0000(0.0000) Steps 646(624.39) | Grad Norm 0.5097(1.0820) | Total Time 0.00(0.00)\n",
      "Iter 1266 | Time 59.1184(59.9845) | Bit/dim 3.5076(3.5302) | Xent 0.0000(0.0000) | Loss 8.5147(8.9815) | Error 0.0000(0.0000) Steps 634(624.68) | Grad Norm 0.5183(1.0651) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 23.0633, Epoch Time 406.0594(369.1019), Bit/dim 3.5208(best: 3.5260), Xent 0.0000, Loss 3.5208, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1267 | Time 62.3739(60.0562) | Bit/dim 3.5346(3.5303) | Xent 0.0000(0.0000) | Loss 12.2394(9.0793) | Error 0.0000(0.0000) Steps 646(625.32) | Grad Norm 0.5799(1.0506) | Total Time 0.00(0.00)\n",
      "Iter 1268 | Time 60.1049(60.0577) | Bit/dim 3.5269(3.5302) | Xent 0.0000(0.0000) | Loss 8.4717(9.0611) | Error 0.0000(0.0000) Steps 628(625.40) | Grad Norm 0.6209(1.0377) | Total Time 0.00(0.00)\n",
      "Iter 1269 | Time 58.0401(59.9971) | Bit/dim 3.5161(3.5298) | Xent 0.0000(0.0000) | Loss 8.4364(9.0423) | Error 0.0000(0.0000) Steps 622(625.30) | Grad Norm 1.4913(1.0513) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 57.5519(59.9238) | Bit/dim 3.5222(3.5296) | Xent 0.0000(0.0000) | Loss 8.4706(9.0252) | Error 0.0000(0.0000) Steps 616(625.02) | Grad Norm 0.5519(1.0363) | Total Time 0.00(0.00)\n",
      "Iter 1271 | Time 58.1962(59.8719) | Bit/dim 3.5172(3.5292) | Xent 0.0000(0.0000) | Loss 8.6019(9.0125) | Error 0.0000(0.0000) Steps 640(625.47) | Grad Norm 0.8171(1.0297) | Total Time 0.00(0.00)\n",
      "Iter 1272 | Time 60.2667(59.8838) | Bit/dim 3.5148(3.5288) | Xent 0.0000(0.0000) | Loss 8.3495(8.9926) | Error 0.0000(0.0000) Steps 628(625.55) | Grad Norm 0.8462(1.0242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 22.8524, Epoch Time 395.5226(369.8945), Bit/dim 3.5252(best: 3.5208), Xent 0.0000, Loss 3.5252, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1273 | Time 64.7171(60.0288) | Bit/dim 3.5310(3.5289) | Xent 0.0000(0.0000) | Loss 12.2631(9.0907) | Error 0.0000(0.0000) Steps 652(626.34) | Grad Norm 0.6867(1.0141) | Total Time 0.00(0.00)\n",
      "Iter 1274 | Time 61.8070(60.0821) | Bit/dim 3.5138(3.5284) | Xent 0.0000(0.0000) | Loss 8.5199(9.0736) | Error 0.0000(0.0000) Steps 640(626.75) | Grad Norm 0.5041(0.9988) | Total Time 0.00(0.00)\n",
      "Iter 1275 | Time 58.5986(60.0376) | Bit/dim 3.5178(3.5281) | Xent 0.0000(0.0000) | Loss 8.5412(9.0576) | Error 0.0000(0.0000) Steps 640(627.15) | Grad Norm 0.7488(0.9913) | Total Time 0.00(0.00)\n",
      "Iter 1276 | Time 59.0950(60.0093) | Bit/dim 3.5364(3.5283) | Xent 0.0000(0.0000) | Loss 8.3087(9.0351) | Error 0.0000(0.0000) Steps 622(626.99) | Grad Norm 1.8831(1.0181) | Total Time 0.00(0.00)\n",
      "Iter 1277 | Time 58.6557(59.9687) | Bit/dim 3.5225(3.5282) | Xent 0.0000(0.0000) | Loss 8.4774(9.0184) | Error 0.0000(0.0000) Steps 634(627.20) | Grad Norm 0.5854(1.0051) | Total Time 0.00(0.00)\n",
      "Iter 1278 | Time 67.7210(60.2013) | Bit/dim 3.5211(3.5279) | Xent 0.0000(0.0000) | Loss 8.5883(9.0055) | Error 0.0000(0.0000) Steps 658(628.13) | Grad Norm 0.9336(1.0029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 23.2240, Epoch Time 409.6535(371.0873), Bit/dim 3.5288(best: 3.5208), Xent 0.0000, Loss 3.5288, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1279 | Time 61.2964(60.2342) | Bit/dim 3.5196(3.5277) | Xent 0.0000(0.0000) | Loss 12.5038(9.1104) | Error 0.0000(0.0000) Steps 628(628.12) | Grad Norm 0.8980(0.9998) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 63.9382(60.3453) | Bit/dim 3.5191(3.5274) | Xent 0.0000(0.0000) | Loss 8.4384(9.0903) | Error 0.0000(0.0000) Steps 640(628.48) | Grad Norm 0.5413(0.9860) | Total Time 0.00(0.00)\n",
      "Iter 1281 | Time 61.9777(60.3943) | Bit/dim 3.5245(3.5274) | Xent 0.0000(0.0000) | Loss 8.6028(9.0757) | Error 0.0000(0.0000) Steps 652(629.19) | Grad Norm 0.5172(0.9720) | Total Time 0.00(0.00)\n",
      "Iter 1282 | Time 60.8749(60.4087) | Bit/dim 3.5297(3.5274) | Xent 0.0000(0.0000) | Loss 8.5454(9.0598) | Error 0.0000(0.0000) Steps 622(628.97) | Grad Norm 0.6995(0.9638) | Total Time 0.00(0.00)\n",
      "Iter 1283 | Time 61.1364(60.4305) | Bit/dim 3.5276(3.5274) | Xent 0.0000(0.0000) | Loss 8.4188(9.0405) | Error 0.0000(0.0000) Steps 628(628.94) | Grad Norm 1.0444(0.9662) | Total Time 0.00(0.00)\n",
      "Iter 1284 | Time 62.0532(60.4792) | Bit/dim 3.5248(3.5274) | Xent 0.0000(0.0000) | Loss 8.5863(9.0269) | Error 0.0000(0.0000) Steps 640(629.27) | Grad Norm 0.4193(0.9498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 23.7593, Epoch Time 410.6510(372.2742), Bit/dim 3.5274(best: 3.5208), Xent 0.0000, Loss 3.5274, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1285 | Time 66.4858(60.6594) | Bit/dim 3.5200(3.5271) | Xent 0.0000(0.0000) | Loss 12.5762(9.1334) | Error 0.0000(0.0000) Steps 652(629.95) | Grad Norm 0.5686(0.9384) | Total Time 0.00(0.00)\n",
      "Iter 1286 | Time 59.0990(60.6126) | Bit/dim 3.5150(3.5268) | Xent 0.0000(0.0000) | Loss 8.3564(9.1101) | Error 0.0000(0.0000) Steps 628(629.90) | Grad Norm 0.7225(0.9319) | Total Time 0.00(0.00)\n",
      "Iter 1287 | Time 60.4983(60.6091) | Bit/dim 3.5212(3.5266) | Xent 0.0000(0.0000) | Loss 8.4116(9.0891) | Error 0.0000(0.0000) Steps 616(629.48) | Grad Norm 0.6804(0.9243) | Total Time 0.00(0.00)\n",
      "Iter 1288 | Time 58.1466(60.5353) | Bit/dim 3.5427(3.5271) | Xent 0.0000(0.0000) | Loss 8.5515(9.0730) | Error 0.0000(0.0000) Steps 634(629.61) | Grad Norm 0.6311(0.9156) | Total Time 0.00(0.00)\n",
      "Iter 1289 | Time 56.8281(60.4241) | Bit/dim 3.5246(3.5270) | Xent 0.0000(0.0000) | Loss 8.3721(9.0520) | Error 0.0000(0.0000) Steps 622(629.39) | Grad Norm 0.9892(0.9178) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 61.8912(60.4681) | Bit/dim 3.5300(3.5271) | Xent 0.0000(0.0000) | Loss 8.4525(9.0340) | Error 0.0000(0.0000) Steps 646(629.88) | Grad Norm 0.8122(0.9146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 23.5413, Epoch Time 402.6844(373.1865), Bit/dim 3.5270(best: 3.5208), Xent 0.0000, Loss 3.5270, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1291 | Time 59.6773(60.4443) | Bit/dim 3.5257(3.5271) | Xent 0.0000(0.0000) | Loss 12.2823(9.1314) | Error 0.0000(0.0000) Steps 616(629.47) | Grad Norm 1.0407(0.9184) | Total Time 0.00(0.00)\n",
      "Iter 1292 | Time 61.4018(60.4731) | Bit/dim 3.5289(3.5271) | Xent 0.0000(0.0000) | Loss 8.5389(9.1137) | Error 0.0000(0.0000) Steps 658(630.32) | Grad Norm 0.7617(0.9137) | Total Time 0.00(0.00)\n",
      "Iter 1293 | Time 61.0885(60.4915) | Bit/dim 3.5186(3.5269) | Xent 0.0000(0.0000) | Loss 8.4979(9.0952) | Error 0.0000(0.0000) Steps 646(630.79) | Grad Norm 0.7914(0.9100) | Total Time 0.00(0.00)\n",
      "Iter 1294 | Time 61.6735(60.5270) | Bit/dim 3.5246(3.5268) | Xent 0.0000(0.0000) | Loss 8.5114(9.0777) | Error 0.0000(0.0000) Steps 640(631.07) | Grad Norm 0.7536(0.9053) | Total Time 0.00(0.00)\n",
      "Iter 1295 | Time 58.5476(60.4676) | Bit/dim 3.5244(3.5267) | Xent 0.0000(0.0000) | Loss 8.4523(9.0589) | Error 0.0000(0.0000) Steps 640(631.34) | Grad Norm 0.6013(0.8962) | Total Time 0.00(0.00)\n",
      "Iter 1296 | Time 60.6272(60.4724) | Bit/dim 3.5296(3.5268) | Xent 0.0000(0.0000) | Loss 8.3476(9.0376) | Error 0.0000(0.0000) Steps 634(631.42) | Grad Norm 1.2050(0.9055) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 23.6461, Epoch Time 402.5884(374.0686), Bit/dim 3.5266(best: 3.5208), Xent 0.0000, Loss 3.5266, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1297 | Time 61.4338(60.5012) | Bit/dim 3.5247(3.5267) | Xent 0.0000(0.0000) | Loss 12.0703(9.1285) | Error 0.0000(0.0000) Steps 628(631.32) | Grad Norm 1.1228(0.9120) | Total Time 0.00(0.00)\n",
      "Iter 1298 | Time 60.5426(60.5025) | Bit/dim 3.5323(3.5269) | Xent 0.0000(0.0000) | Loss 8.3392(9.1049) | Error 0.0000(0.0000) Steps 616(630.86) | Grad Norm 0.6412(0.9039) | Total Time 0.00(0.00)\n",
      "Iter 1299 | Time 59.2152(60.4639) | Bit/dim 3.5239(3.5268) | Xent 0.0000(0.0000) | Loss 8.5777(9.0891) | Error 0.0000(0.0000) Steps 634(630.95) | Grad Norm 0.8466(0.9021) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 62.8529(60.5355) | Bit/dim 3.5289(3.5269) | Xent 0.0000(0.0000) | Loss 8.4954(9.0712) | Error 0.0000(0.0000) Steps 628(630.86) | Grad Norm 0.9449(0.9034) | Total Time 0.00(0.00)\n",
      "Iter 1301 | Time 62.0452(60.5808) | Bit/dim 3.5417(3.5273) | Xent 0.0000(0.0000) | Loss 8.6192(9.0577) | Error 0.0000(0.0000) Steps 640(631.14) | Grad Norm 0.6847(0.8969) | Total Time 0.00(0.00)\n",
      "Iter 1302 | Time 59.3831(60.5449) | Bit/dim 3.5277(3.5273) | Xent 0.0000(0.0000) | Loss 8.5467(9.0424) | Error 0.0000(0.0000) Steps 628(631.04) | Grad Norm 0.5217(0.8856) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 22.6865, Epoch Time 403.6214(374.9552), Bit/dim 3.5315(best: 3.5208), Xent 0.0000, Loss 3.5315, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1303 | Time 61.2757(60.5668) | Bit/dim 3.5294(3.5274) | Xent 0.0000(0.0000) | Loss 12.2876(9.1397) | Error 0.0000(0.0000) Steps 664(632.03) | Grad Norm 0.8882(0.8857) | Total Time 0.00(0.00)\n",
      "Iter 1304 | Time 57.0111(60.4601) | Bit/dim 3.5331(3.5276) | Xent 0.0000(0.0000) | Loss 8.2982(9.1145) | Error 0.0000(0.0000) Steps 610(631.37) | Grad Norm 0.8058(0.8833) | Total Time 0.00(0.00)\n",
      "Iter 1305 | Time 58.9888(60.4160) | Bit/dim 3.5297(3.5276) | Xent 0.0000(0.0000) | Loss 8.3105(9.0903) | Error 0.0000(0.0000) Steps 622(631.09) | Grad Norm 0.9683(0.8858) | Total Time 0.00(0.00)\n",
      "Iter 1306 | Time 58.8535(60.3691) | Bit/dim 3.5303(3.5277) | Xent 0.0000(0.0000) | Loss 8.2977(9.0666) | Error 0.0000(0.0000) Steps 616(630.64) | Grad Norm 1.0124(0.8896) | Total Time 0.00(0.00)\n",
      "Iter 1307 | Time 61.8118(60.4124) | Bit/dim 3.5437(3.5282) | Xent 0.0000(0.0000) | Loss 8.3771(9.0459) | Error 0.0000(0.0000) Steps 640(630.92) | Grad Norm 1.0057(0.8931) | Total Time 0.00(0.00)\n",
      "Iter 1308 | Time 64.1740(60.5253) | Bit/dim 3.5335(3.5284) | Xent 0.0000(0.0000) | Loss 8.5561(9.0312) | Error 0.0000(0.0000) Steps 652(631.55) | Grad Norm 1.1796(0.9017) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 23.3045, Epoch Time 401.5659(375.7535), Bit/dim 3.5342(best: 3.5208), Xent 0.0000, Loss 3.5342, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1309 | Time 62.5674(60.5865) | Bit/dim 3.5395(3.5287) | Xent 0.0000(0.0000) | Loss 12.0824(9.1227) | Error 0.0000(0.0000) Steps 652(632.16) | Grad Norm 1.0264(0.9055) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 65.2268(60.7257) | Bit/dim 3.5347(3.5289) | Xent 0.0000(0.0000) | Loss 8.5966(9.1070) | Error 0.0000(0.0000) Steps 646(632.58) | Grad Norm 0.8011(0.9023) | Total Time 0.00(0.00)\n",
      "Iter 1311 | Time 61.6933(60.7548) | Bit/dim 3.5339(3.5290) | Xent 0.0000(0.0000) | Loss 8.4900(9.0884) | Error 0.0000(0.0000) Steps 634(632.62) | Grad Norm 0.7086(0.8965) | Total Time 0.00(0.00)\n",
      "Iter 1312 | Time 62.0932(60.7949) | Bit/dim 3.5312(3.5291) | Xent 0.0000(0.0000) | Loss 8.5364(9.0719) | Error 0.0000(0.0000) Steps 664(633.56) | Grad Norm 0.5081(0.8849) | Total Time 0.00(0.00)\n",
      "Iter 1313 | Time 66.7259(60.9728) | Bit/dim 3.5386(3.5294) | Xent 0.0000(0.0000) | Loss 8.4356(9.0528) | Error 0.0000(0.0000) Steps 622(633.22) | Grad Norm 0.5281(0.8742) | Total Time 0.00(0.00)\n",
      "Iter 1314 | Time 61.2039(60.9798) | Bit/dim 3.5223(3.5292) | Xent 0.0000(0.0000) | Loss 8.5511(9.0377) | Error 0.0000(0.0000) Steps 646(633.60) | Grad Norm 0.5604(0.8647) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 23.4937, Epoch Time 419.4541(377.0645), Bit/dim 3.5285(best: 3.5208), Xent 0.0000, Loss 3.5285, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1315 | Time 63.0011(61.0404) | Bit/dim 3.5263(3.5291) | Xent 0.0000(0.0000) | Loss 12.0382(9.1278) | Error 0.0000(0.0000) Steps 640(633.79) | Grad Norm 1.0779(0.8711) | Total Time 0.00(0.00)\n",
      "Iter 1316 | Time 57.8130(60.9436) | Bit/dim 3.5289(3.5291) | Xent 0.0000(0.0000) | Loss 8.3317(9.1039) | Error 0.0000(0.0000) Steps 610(633.08) | Grad Norm 1.3684(0.8861) | Total Time 0.00(0.00)\n",
      "Iter 1317 | Time 64.5521(61.0518) | Bit/dim 3.5291(3.5291) | Xent 0.0000(0.0000) | Loss 8.6821(9.0912) | Error 0.0000(0.0000) Steps 670(634.18) | Grad Norm 0.6107(0.8778) | Total Time 0.00(0.00)\n",
      "Iter 1318 | Time 64.1581(61.1450) | Bit/dim 3.5184(3.5287) | Xent 0.0000(0.0000) | Loss 8.4391(9.0717) | Error 0.0000(0.0000) Steps 652(634.72) | Grad Norm 0.8014(0.8755) | Total Time 0.00(0.00)\n",
      "Iter 1319 | Time 58.8016(61.0747) | Bit/dim 3.5430(3.5292) | Xent 0.0000(0.0000) | Loss 8.4661(9.0535) | Error 0.0000(0.0000) Steps 616(634.16) | Grad Norm 0.7612(0.8721) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 60.1235(61.0462) | Bit/dim 3.5401(3.5295) | Xent 0.0000(0.0000) | Loss 8.4471(9.0353) | Error 0.0000(0.0000) Steps 634(634.15) | Grad Norm 0.7358(0.8680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 23.2774, Epoch Time 407.9564(377.9912), Bit/dim 3.5348(best: 3.5208), Xent 0.0000, Loss 3.5348, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1321 | Time 61.9804(61.0742) | Bit/dim 3.5375(3.5297) | Xent 0.0000(0.0000) | Loss 12.2551(9.1319) | Error 0.0000(0.0000) Steps 628(633.97) | Grad Norm 1.2197(0.8785) | Total Time 0.00(0.00)\n",
      "Iter 1322 | Time 61.4147(61.0844) | Bit/dim 3.5397(3.5300) | Xent 0.0000(0.0000) | Loss 8.2828(9.1064) | Error 0.0000(0.0000) Steps 640(634.15) | Grad Norm 0.4852(0.8667) | Total Time 0.00(0.00)\n",
      "Iter 1323 | Time 62.2572(61.1196) | Bit/dim 3.5352(3.5302) | Xent 0.0000(0.0000) | Loss 8.5853(9.0908) | Error 0.0000(0.0000) Steps 652(634.68) | Grad Norm 0.5318(0.8567) | Total Time 0.00(0.00)\n",
      "Iter 1324 | Time 66.6847(61.2866) | Bit/dim 3.5404(3.5305) | Xent 0.0000(0.0000) | Loss 8.5794(9.0754) | Error 0.0000(0.0000) Steps 670(635.74) | Grad Norm 0.9299(0.8589) | Total Time 0.00(0.00)\n",
      "Iter 1325 | Time 64.3808(61.3794) | Bit/dim 3.5325(3.5306) | Xent 0.0000(0.0000) | Loss 8.4557(9.0569) | Error 0.0000(0.0000) Steps 652(636.23) | Grad Norm 1.4182(0.8757) | Total Time 0.00(0.00)\n",
      "Iter 1326 | Time 64.8657(61.4840) | Bit/dim 3.5270(3.5305) | Xent 0.0000(0.0000) | Loss 8.4647(9.0391) | Error 0.0000(0.0000) Steps 646(636.52) | Grad Norm 0.5457(0.8658) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 23.2460, Epoch Time 420.8403(379.2767), Bit/dim 3.5295(best: 3.5208), Xent 0.0000, Loss 3.5295, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1327 | Time 58.2903(61.3882) | Bit/dim 3.5385(3.5307) | Xent 0.0000(0.0000) | Loss 12.5274(9.1437) | Error 0.0000(0.0000) Steps 640(636.63) | Grad Norm 1.2285(0.8766) | Total Time 0.00(0.00)\n",
      "Iter 1328 | Time 63.9543(61.4652) | Bit/dim 3.5380(3.5309) | Xent 0.0000(0.0000) | Loss 8.5852(9.1270) | Error 0.0000(0.0000) Steps 652(637.09) | Grad Norm 1.0959(0.8832) | Total Time 0.00(0.00)\n",
      "Iter 1329 | Time 63.4063(61.5234) | Bit/dim 3.5368(3.5311) | Xent 0.0000(0.0000) | Loss 8.4785(9.1075) | Error 0.0000(0.0000) Steps 652(637.54) | Grad Norm 0.7427(0.8790) | Total Time 0.00(0.00)\n",
      "Iter 1330 | Time 57.7347(61.4097) | Bit/dim 3.5197(3.5307) | Xent 0.0000(0.0000) | Loss 8.0642(9.0762) | Error 0.0000(0.0000) Steps 616(636.89) | Grad Norm 0.6416(0.8719) | Total Time 0.00(0.00)\n",
      "Iter 1331 | Time 63.9990(61.4874) | Bit/dim 3.5283(3.5307) | Xent 0.0000(0.0000) | Loss 8.3839(9.0555) | Error 0.0000(0.0000) Steps 646(637.16) | Grad Norm 1.3893(0.8874) | Total Time 0.00(0.00)\n",
      "Iter 1332 | Time 60.7053(61.4639) | Bit/dim 3.5229(3.5304) | Xent 0.0000(0.0000) | Loss 8.2923(9.0326) | Error 0.0000(0.0000) Steps 628(636.89) | Grad Norm 0.8682(0.8868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 24.0555, Epoch Time 408.4974(380.1533), Bit/dim 3.5275(best: 3.5208), Xent 0.0000, Loss 3.5275, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1333 | Time 65.2118(61.5764) | Bit/dim 3.5264(3.5303) | Xent 0.0000(0.0000) | Loss 12.2971(9.1305) | Error 0.0000(0.0000) Steps 610(636.08) | Grad Norm 1.0028(0.8903) | Total Time 0.00(0.00)\n",
      "Iter 1334 | Time 58.1374(61.4732) | Bit/dim 3.5272(3.5302) | Xent 0.0000(0.0000) | Loss 8.6257(9.1154) | Error 0.0000(0.0000) Steps 634(636.02) | Grad Norm 0.7906(0.8873) | Total Time 0.00(0.00)\n",
      "Iter 1335 | Time 59.7561(61.4217) | Bit/dim 3.5300(3.5302) | Xent 0.0000(0.0000) | Loss 8.1430(9.0862) | Error 0.0000(0.0000) Steps 616(635.42) | Grad Norm 1.1463(0.8951) | Total Time 0.00(0.00)\n",
      "Iter 1336 | Time 61.6770(61.4294) | Bit/dim 3.5197(3.5299) | Xent 0.0000(0.0000) | Loss 8.5079(9.0688) | Error 0.0000(0.0000) Steps 658(636.10) | Grad Norm 0.7485(0.8907) | Total Time 0.00(0.00)\n",
      "Iter 1337 | Time 58.5840(61.3440) | Bit/dim 3.5427(3.5303) | Xent 0.0000(0.0000) | Loss 8.4108(9.0491) | Error 0.0000(0.0000) Steps 610(635.31) | Grad Norm 0.5268(0.8798) | Total Time 0.00(0.00)\n",
      "Iter 1338 | Time 59.6585(61.2934) | Bit/dim 3.5434(3.5307) | Xent 0.0000(0.0000) | Loss 8.4765(9.0319) | Error 0.0000(0.0000) Steps 628(635.10) | Grad Norm 1.1566(0.8881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 23.5467, Epoch Time 402.8442(380.8341), Bit/dim 3.5372(best: 3.5208), Xent 0.0000, Loss 3.5372, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1339 | Time 64.1042(61.3778) | Bit/dim 3.5357(3.5308) | Xent 0.0000(0.0000) | Loss 12.4741(9.1352) | Error 0.0000(0.0000) Steps 640(635.24) | Grad Norm 1.2992(0.9004) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 62.7862(61.4200) | Bit/dim 3.5497(3.5314) | Xent 0.0000(0.0000) | Loss 8.5908(9.1189) | Error 0.0000(0.0000) Steps 640(635.38) | Grad Norm 0.7808(0.8968) | Total Time 0.00(0.00)\n",
      "Iter 1341 | Time 58.8438(61.3427) | Bit/dim 3.5219(3.5311) | Xent 0.0000(0.0000) | Loss 8.2453(9.0926) | Error 0.0000(0.0000) Steps 610(634.62) | Grad Norm 0.7374(0.8920) | Total Time 0.00(0.00)\n",
      "Iter 1342 | Time 61.0218(61.3331) | Bit/dim 3.5348(3.5312) | Xent 0.0000(0.0000) | Loss 8.4224(9.0725) | Error 0.0000(0.0000) Steps 622(634.24) | Grad Norm 2.8531(0.9509) | Total Time 0.00(0.00)\n",
      "Iter 1343 | Time 58.4296(61.2460) | Bit/dim 3.5461(3.5317) | Xent 0.0000(0.0000) | Loss 8.6363(9.0594) | Error 0.0000(0.0000) Steps 634(634.24) | Grad Norm 1.0411(0.9536) | Total Time 0.00(0.00)\n",
      "Iter 1344 | Time 62.1317(61.2726) | Bit/dim 3.5579(3.5325) | Xent 0.0000(0.0000) | Loss 8.4563(9.0414) | Error 0.0000(0.0000) Steps 628(634.05) | Grad Norm 4.1654(1.0499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 24.0863, Epoch Time 407.7570(381.6418), Bit/dim 3.5448(best: 3.5208), Xent 0.0000, Loss 3.5448, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1345 | Time 59.7137(61.2258) | Bit/dim 3.5409(3.5327) | Xent 0.0000(0.0000) | Loss 12.2988(9.1391) | Error 0.0000(0.0000) Steps 640(634.23) | Grad Norm 1.4934(1.0632) | Total Time 0.00(0.00)\n",
      "Iter 1346 | Time 62.9008(61.2760) | Bit/dim 3.5418(3.5330) | Xent 0.0000(0.0000) | Loss 8.5196(9.1205) | Error 0.0000(0.0000) Steps 640(634.40) | Grad Norm 1.2996(1.0703) | Total Time 0.00(0.00)\n",
      "Iter 1347 | Time 61.5462(61.2842) | Bit/dim 3.5310(3.5329) | Xent 0.0000(0.0000) | Loss 8.5572(9.1036) | Error 0.0000(0.0000) Steps 622(634.03) | Grad Norm 1.9039(1.0953) | Total Time 0.00(0.00)\n",
      "Iter 1348 | Time 63.0083(61.3359) | Bit/dim 3.5328(3.5329) | Xent 0.0000(0.0000) | Loss 8.5398(9.0867) | Error 0.0000(0.0000) Steps 652(634.57) | Grad Norm 1.7252(1.1142) | Total Time 0.00(0.00)\n",
      "Iter 1349 | Time 64.6676(61.4358) | Bit/dim 3.5292(3.5328) | Xent 0.0000(0.0000) | Loss 8.3634(9.0650) | Error 0.0000(0.0000) Steps 640(634.73) | Grad Norm 0.9232(1.1085) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 65.0693(61.5448) | Bit/dim 3.5509(3.5334) | Xent 0.0000(0.0000) | Loss 8.6274(9.0519) | Error 0.0000(0.0000) Steps 664(635.61) | Grad Norm 1.7607(1.1281) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 23.4592, Epoch Time 416.6655(382.6925), Bit/dim 3.5422(best: 3.5208), Xent 0.0000, Loss 3.5422, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1351 | Time 60.6742(61.5187) | Bit/dim 3.5250(3.5331) | Xent 0.0000(0.0000) | Loss 12.3661(9.1513) | Error 0.0000(0.0000) Steps 616(635.02) | Grad Norm 1.5852(1.1418) | Total Time 0.00(0.00)\n",
      "Iter 1352 | Time 66.9709(61.6823) | Bit/dim 3.5383(3.5333) | Xent 0.0000(0.0000) | Loss 8.5935(9.1345) | Error 0.0000(0.0000) Steps 652(635.53) | Grad Norm 0.8929(1.1343) | Total Time 0.00(0.00)\n",
      "Iter 1353 | Time 64.0792(61.7542) | Bit/dim 3.5395(3.5335) | Xent 0.0000(0.0000) | Loss 8.4462(9.1139) | Error 0.0000(0.0000) Steps 634(635.48) | Grad Norm 0.9318(1.1282) | Total Time 0.00(0.00)\n",
      "Iter 1354 | Time 60.7263(61.7233) | Bit/dim 3.5457(3.5338) | Xent 0.0000(0.0000) | Loss 8.4818(9.0949) | Error 0.0000(0.0000) Steps 616(634.90) | Grad Norm 2.5786(1.1717) | Total Time 0.00(0.00)\n",
      "Iter 1355 | Time 60.9006(61.6987) | Bit/dim 3.5418(3.5341) | Xent 0.0000(0.0000) | Loss 8.4050(9.0742) | Error 0.0000(0.0000) Steps 616(634.33) | Grad Norm 3.2591(1.2344) | Total Time 0.00(0.00)\n",
      "Iter 1356 | Time 64.2252(61.7745) | Bit/dim 3.5316(3.5340) | Xent 0.0000(0.0000) | Loss 8.6403(9.0612) | Error 0.0000(0.0000) Steps 676(635.58) | Grad Norm 1.3268(1.2371) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 23.4489, Epoch Time 417.1711(383.7268), Bit/dim 3.5517(best: 3.5208), Xent 0.0000, Loss 3.5517, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1357 | Time 57.0075(61.6315) | Bit/dim 3.5609(3.5348) | Xent 0.0000(0.0000) | Loss 12.1504(9.1539) | Error 0.0000(0.0000) Steps 622(635.18) | Grad Norm 4.2312(1.3270) | Total Time 0.00(0.00)\n",
      "Iter 1358 | Time 62.6536(61.6621) | Bit/dim 3.5889(3.5364) | Xent 0.0000(0.0000) | Loss 8.5780(9.1366) | Error 0.0000(0.0000) Steps 652(635.68) | Grad Norm 5.7262(1.4589) | Total Time 0.00(0.00)\n",
      "Iter 1359 | Time 62.9545(61.7009) | Bit/dim 3.5891(3.5380) | Xent 0.0000(0.0000) | Loss 8.6390(9.1217) | Error 0.0000(0.0000) Steps 664(636.53) | Grad Norm 5.0435(1.5665) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 64.2268(61.7767) | Bit/dim 3.5495(3.5383) | Xent 0.0000(0.0000) | Loss 8.5696(9.1051) | Error 0.0000(0.0000) Steps 652(636.99) | Grad Norm 6.2520(1.7070) | Total Time 0.00(0.00)\n",
      "Iter 1361 | Time 63.4072(61.8256) | Bit/dim 3.5572(3.5389) | Xent 0.0000(0.0000) | Loss 8.5082(9.0872) | Error 0.0000(0.0000) Steps 628(636.72) | Grad Norm 7.6542(1.8855) | Total Time 0.00(0.00)\n",
      "Iter 1362 | Time 61.8234(61.8255) | Bit/dim 3.6314(3.5417) | Xent 0.0000(0.0000) | Loss 8.8433(9.0799) | Error 0.0000(0.0000) Steps 634(636.64) | Grad Norm 5.8376(2.0040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 24.6025, Epoch Time 412.7657(384.5980), Bit/dim 3.6659(best: 3.5208), Xent 0.0000, Loss 3.6659, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1363 | Time 60.8592(61.7965) | Bit/dim 3.6638(3.5453) | Xent 0.0000(0.0000) | Loss 12.6497(9.1870) | Error 0.0000(0.0000) Steps 658(637.28) | Grad Norm 11.0527(2.2755) | Total Time 0.00(0.00)\n",
      "Iter 1364 | Time 66.8123(61.9470) | Bit/dim 3.5765(3.5463) | Xent 0.0000(0.0000) | Loss 8.5712(9.1685) | Error 0.0000(0.0000) Steps 682(638.62) | Grad Norm 2.9158(2.2947) | Total Time 0.00(0.00)\n",
      "Iter 1365 | Time 61.9008(61.9456) | Bit/dim 3.5579(3.5466) | Xent 0.0000(0.0000) | Loss 8.3309(9.1434) | Error 0.0000(0.0000) Steps 634(638.49) | Grad Norm 8.5278(2.4817) | Total Time 0.00(0.00)\n",
      "Iter 1366 | Time 67.5608(62.1141) | Bit/dim 3.5539(3.5468) | Xent 0.0000(0.0000) | Loss 8.6923(9.1299) | Error 0.0000(0.0000) Steps 670(639.43) | Grad Norm 3.6885(2.5179) | Total Time 0.00(0.00)\n",
      "Iter 1367 | Time 63.5288(62.1565) | Bit/dim 3.5583(3.5472) | Xent 0.0000(0.0000) | Loss 8.4341(9.1090) | Error 0.0000(0.0000) Steps 634(639.27) | Grad Norm 10.8342(2.7674) | Total Time 0.00(0.00)\n",
      "Iter 1368 | Time 64.4488(62.2253) | Bit/dim 3.6491(3.5502) | Xent 0.0000(0.0000) | Loss 8.7739(9.0989) | Error 0.0000(0.0000) Steps 664(640.01) | Grad Norm 7.3163(2.9038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 25.2793, Epoch Time 426.5084(385.8553), Bit/dim 3.9073(best: 3.5208), Xent 0.0000, Loss 3.9073, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1369 | Time 62.6561(62.2382) | Bit/dim 3.9027(3.5608) | Xent 0.0000(0.0000) | Loss 13.1449(9.2203) | Error 0.0000(0.0000) Steps 652(640.37) | Grad Norm 56.1100(4.5000) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 65.0701(62.3232) | Bit/dim 4.0216(3.5746) | Xent 0.0000(0.0000) | Loss 9.6155(9.2322) | Error 0.0000(0.0000) Steps 682(641.62) | Grad Norm 117.7863(7.8986) | Total Time 0.00(0.00)\n",
      "Iter 1371 | Time 70.6203(62.5721) | Bit/dim 4.0172(3.5879) | Xent 0.0000(0.0000) | Loss 9.6587(9.2450) | Error 0.0000(0.0000) Steps 676(642.65) | Grad Norm 136.9646(11.7706) | Total Time 0.00(0.00)\n",
      "Iter 1372 | Time 69.5372(62.7810) | Bit/dim 3.9287(3.5981) | Xent 0.0000(0.0000) | Loss 9.5741(9.2548) | Error 0.0000(0.0000) Steps 682(643.83) | Grad Norm 114.8888(14.8641) | Total Time 0.00(0.00)\n",
      "Iter 1373 | Time 71.8627(63.0535) | Bit/dim 3.7865(3.6038) | Xent 0.0000(0.0000) | Loss 9.1852(9.2527) | Error 0.0000(0.0000) Steps 706(645.70) | Grad Norm 70.3050(16.5274) | Total Time 0.00(0.00)\n",
      "Iter 1374 | Time 70.1250(63.2656) | Bit/dim 3.7086(3.6069) | Xent 0.0000(0.0000) | Loss 9.0757(9.2474) | Error 0.0000(0.0000) Steps 688(646.97) | Grad Norm 59.9782(17.8309) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 26.0192, Epoch Time 452.0950(387.8425), Bit/dim 3.7111(best: 3.5208), Xent 0.0000, Loss 3.7111, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1375 | Time 73.3957(63.5695) | Bit/dim 3.7025(3.6098) | Xent 0.0000(0.0000) | Loss 13.2268(9.3668) | Error 0.0000(0.0000) Steps 700(648.56) | Grad Norm 57.0067(19.0062) | Total Time 0.00(0.00)\n",
      "Iter 1376 | Time 66.5403(63.6587) | Bit/dim 3.6873(3.6121) | Xent 0.0000(0.0000) | Loss 8.9666(9.3548) | Error 0.0000(0.0000) Steps 670(649.20) | Grad Norm 167.9035(23.4731) | Total Time 0.00(0.00)\n",
      "Iter 1377 | Time 66.2842(63.7374) | Bit/dim 3.7073(3.6150) | Xent 0.0000(0.0000) | Loss 9.0358(9.3452) | Error 0.0000(0.0000) Steps 694(650.54) | Grad Norm 199.5286(28.7548) | Total Time 0.00(0.00)\n",
      "Iter 1378 | Time 75.3070(64.0845) | Bit/dim 3.7138(3.6179) | Xent 0.0000(0.0000) | Loss 9.0030(9.3350) | Error 0.0000(0.0000) Steps 736(653.11) | Grad Norm 293.7670(36.7051) | Total Time 0.00(0.00)\n",
      "Iter 1379 | Time 85.0078(64.7122) | Bit/dim 3.7746(3.6227) | Xent 0.0000(0.0000) | Loss 9.2193(9.3315) | Error 0.0000(0.0000) Steps 760(656.31) | Grad Norm 219.5587(42.1907) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 77.5359(65.0969) | Bit/dim 3.9061(3.6312) | Xent 0.0000(0.0000) | Loss 9.3192(9.3311) | Error 0.0000(0.0000) Steps 742(658.88) | Grad Norm 577.0919(58.2378) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 29.1365, Epoch Time 489.1950(390.8831), Bit/dim 4.1517(best: 3.5208), Xent 0.0000, Loss 4.1517, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1381 | Time 86.6824(65.7445) | Bit/dim 4.1179(3.6458) | Xent 0.0000(0.0000) | Loss 14.5040(9.4863) | Error 0.0000(0.0000) Steps 796(663.00) | Grad Norm 761.5285(79.3365) | Total Time 0.00(0.00)\n",
      "Iter 1382 | Time 84.1099(66.2954) | Bit/dim 4.3916(3.6681) | Xent 0.0000(0.0000) | Loss 10.4952(9.5166) | Error 0.0000(0.0000) Steps 802(667.17) | Grad Norm 5963.5366(255.8625) | Total Time 0.00(0.00)\n",
      "Iter 1383 | Time 85.3394(66.8668) | Bit/dim 4.7175(3.6996) | Xent 0.0000(0.0000) | Loss 11.3305(9.5710) | Error 0.0000(0.0000) Steps 796(671.03) | Grad Norm 17568.7172(775.2481) | Total Time 0.00(0.00)\n",
      "Iter 1384 | Time 84.2713(67.3889) | Bit/dim 5.1983(3.7446) | Xent 0.0000(0.0000) | Loss 11.9944(9.6437) | Error 0.0000(0.0000) Steps 772(674.06) | Grad Norm 93577.3677(3559.3117) | Total Time 0.00(0.00)\n",
      "Iter 1385 | Time 89.8033(68.0613) | Bit/dim 5.6941(3.8031) | Xent 0.0000(0.0000) | Loss 13.0720(9.7466) | Error 0.0000(0.0000) Steps 826(678.62) | Grad Norm 134281.3181(7480.9719) | Total Time 0.00(0.00)\n",
      "Iter 1386 | Time 86.8288(68.6244) | Bit/dim 5.7819(3.8624) | Xent 0.0000(0.0000) | Loss 13.5574(9.8609) | Error 0.0000(0.0000) Steps 844(683.58) | Grad Norm 273024.6489(15447.2822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 33.6205, Epoch Time 567.1300(396.1705), Bit/dim 5.8185(best: 3.5208), Xent 0.0000, Loss 5.8185, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1387 | Time 94.9696(69.4147) | Bit/dim 5.8393(3.9217) | Xent 0.0000(0.0000) | Loss 18.5400(10.1213) | Error 0.0000(0.0000) Steps 880(689.47) | Grad Norm 251382.5198(22525.3393) | Total Time 0.00(0.00)\n",
      "Iter 1388 | Time 105.3219(70.4919) | Bit/dim 5.9412(3.9823) | Xent 0.0000(0.0000) | Loss 13.8524(10.2332) | Error 0.0000(0.0000) Steps 916(696.27) | Grad Norm 189161.0826(27524.4116) | Total Time 0.00(0.00)\n",
      "Iter 1389 | Time 111.8995(71.7342) | Bit/dim 6.1000(4.0458) | Xent 0.0000(0.0000) | Loss 14.3782(10.3575) | Error 0.0000(0.0000) Steps 928(703.22) | Grad Norm 208298.9505(32947.6478) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 103.3787(72.6835) | Bit/dim 6.1495(4.1090) | Xent 0.0000(0.0000) | Loss 14.2524(10.4744) | Error 0.0000(0.0000) Steps 940(710.33) | Grad Norm 203948.8981(38077.6853) | Total Time 0.00(0.00)\n",
      "Iter 1391 | Time 113.4133(73.9054) | Bit/dim 6.1350(4.1697) | Xent 0.0000(0.0000) | Loss 14.2911(10.5889) | Error 0.0000(0.0000) Steps 922(716.68) | Grad Norm 284573.5461(45472.5611) | Total Time 0.00(0.00)\n",
      "Iter 1392 | Time 112.7760(75.0715) | Bit/dim 6.0215(4.2253) | Xent 0.0000(0.0000) | Loss 14.2308(10.6981) | Error 0.0000(0.0000) Steps 910(722.48) | Grad Norm 291946.3172(52866.7738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 34.3751, Epoch Time 692.6755(405.0656), Bit/dim 6.0686(best: 3.5208), Xent 0.0000, Loss 6.0686, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1393 | Time 109.4496(76.1028) | Bit/dim 6.0922(4.2813) | Xent 0.0000(0.0000) | Loss 18.7729(10.9404) | Error 0.0000(0.0000) Steps 910(728.10) | Grad Norm 937798.4743(79414.7248) | Total Time 0.00(0.00)\n",
      "Iter 1394 | Time 114.1261(77.2435) | Bit/dim 6.1301(4.3368) | Xent 0.0000(0.0000) | Loss 14.3420(11.0424) | Error 0.0000(0.0000) Steps 892(733.02) | Grad Norm 839833.2800(102227.2815) | Total Time 0.00(0.00)\n",
      "Iter 1395 | Time 108.4755(78.1805) | Bit/dim 5.9650(4.3856) | Xent 0.0000(0.0000) | Loss 13.8582(11.1269) | Error 0.0000(0.0000) Steps 904(738.15) | Grad Norm 278114.7084(107503.9043) | Total Time 0.00(0.00)\n",
      "Iter 1396 | Time 101.9237(78.8928) | Bit/dim 5.9420(4.4323) | Xent 0.0000(0.0000) | Loss 13.7963(11.2070) | Error 0.0000(0.0000) Steps 850(741.50) | Grad Norm 58440.4201(106031.9998) | Total Time 0.00(0.00)\n",
      "Iter 1397 | Time 117.1175(80.0395) | Bit/dim 5.8504(4.4748) | Xent 0.0000(0.0000) | Loss 13.7391(11.2830) | Error 0.0000(0.0000) Steps 904(746.38) | Grad Norm 6281.5470(103039.4862) | Total Time 0.00(0.00)\n",
      "Iter 1398 | Time 102.2917(80.7071) | Bit/dim 5.7387(4.5128) | Xent 0.0000(0.0000) | Loss 13.6489(11.3539) | Error 0.0000(0.0000) Steps 892(750.75) | Grad Norm 1342.6929(99988.5824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 30.2253, Epoch Time 699.7533(413.9063), Bit/dim 5.2194(best: 3.5208), Xent 0.0000, Loss 5.2194, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1399 | Time 93.8537(81.1015) | Bit/dim 5.1672(4.5324) | Xent 0.0000(0.0000) | Loss 16.8822(11.5198) | Error 0.0000(0.0000) Steps 820(752.82) | Grad Norm 305.4452(96998.0883) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 78.6304(81.0274) | Bit/dim 4.5570(4.5331) | Xent 0.0000(0.0000) | Loss 10.5176(11.4897) | Error 0.0000(0.0000) Steps 718(751.78) | Grad Norm 42.5872(94089.4232) | Total Time 0.00(0.00)\n",
      "Iter 1401 | Time 77.6503(80.9261) | Bit/dim 4.0623(4.5190) | Xent 0.0000(0.0000) | Loss 9.6579(11.4348) | Error 0.0000(0.0000) Steps 724(750.95) | Grad Norm 8.5709(91266.9977) | Total Time 0.00(0.00)\n",
      "Iter 1402 | Time 66.7033(80.4994) | Bit/dim 3.8996(4.5004) | Xent 0.0000(0.0000) | Loss 9.1077(11.3650) | Error 0.0000(0.0000) Steps 676(748.70) | Grad Norm 4.1680(88529.1128) | Total Time 0.00(0.00)\n",
      "Iter 1403 | Time 61.8433(79.9397) | Bit/dim 3.8422(4.4807) | Xent 0.0000(0.0000) | Loss 9.0989(11.2970) | Error 0.0000(0.0000) Steps 676(746.52) | Grad Norm 3.7164(85873.3509) | Total Time 0.00(0.00)\n",
      "Iter 1404 | Time 67.8260(79.5763) | Bit/dim 3.8285(4.4611) | Xent 0.0000(0.0000) | Loss 9.2295(11.2349) | Error 0.0000(0.0000) Steps 712(745.48) | Grad Norm 3.6393(83297.2595) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 24.9798, Epoch Time 487.6478(416.1185), Bit/dim 3.8058(best: 3.5208), Xent 0.0000, Loss 3.8058, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1405 | Time 70.5735(79.3062) | Bit/dim 3.7929(4.4411) | Xent 0.0000(0.0000) | Loss 13.5532(11.3045) | Error 0.0000(0.0000) Steps 724(744.84) | Grad Norm 3.4765(80798.4461) | Total Time 0.00(0.00)\n",
      "Iter 1406 | Time 72.1184(79.0906) | Bit/dim 3.7903(4.4215) | Xent 0.0000(0.0000) | Loss 9.2339(11.2424) | Error 0.0000(0.0000) Steps 706(743.67) | Grad Norm 3.5134(78374.5981) | Total Time 0.00(0.00)\n",
      "Iter 1407 | Time 66.0133(78.6982) | Bit/dim 3.7598(4.4017) | Xent 0.0000(0.0000) | Loss 9.0909(11.1778) | Error 0.0000(0.0000) Steps 682(741.82) | Grad Norm 3.2780(76023.4585) | Total Time 0.00(0.00)\n",
      "Iter 1408 | Time 70.3389(78.4475) | Bit/dim 3.7362(4.3817) | Xent 0.0000(0.0000) | Loss 9.2096(11.1188) | Error 0.0000(0.0000) Steps 742(741.83) | Grad Norm 2.8872(73742.8413) | Total Time 0.00(0.00)\n",
      "Iter 1409 | Time 67.2776(78.1124) | Bit/dim 3.7263(4.3621) | Xent 0.0000(0.0000) | Loss 8.9357(11.0533) | Error 0.0000(0.0000) Steps 694(740.39) | Grad Norm 3.0452(71530.6475) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 71.6049(77.9171) | Bit/dim 3.7086(4.3425) | Xent 0.0000(0.0000) | Loss 8.9085(10.9890) | Error 0.0000(0.0000) Steps 706(739.36) | Grad Norm 2.9144(69384.8155) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 25.2346, Epoch Time 459.3368(417.4151), Bit/dim 3.6998(best: 3.5208), Xent 0.0000, Loss 3.6998, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1411 | Time 66.8301(77.5845) | Bit/dim 3.6992(4.3232) | Xent 0.0000(0.0000) | Loss 13.1259(11.0531) | Error 0.0000(0.0000) Steps 688(737.82) | Grad Norm 2.3880(67303.3426) | Total Time 0.00(0.00)\n",
      "Iter 1412 | Time 66.4638(77.2509) | Bit/dim 3.6869(4.3041) | Xent 0.0000(0.0000) | Loss 8.8743(10.9877) | Error 0.0000(0.0000) Steps 664(735.60) | Grad Norm 2.6605(65284.3222) | Total Time 0.00(0.00)\n",
      "Iter 1413 | Time 64.5021(76.8684) | Bit/dim 3.6830(4.2854) | Xent 0.0000(0.0000) | Loss 8.7983(10.9220) | Error 0.0000(0.0000) Steps 670(733.64) | Grad Norm 2.8261(63325.8773) | Total Time 0.00(0.00)\n",
      "Iter 1414 | Time 69.8733(76.6586) | Bit/dim 3.6713(4.2670) | Xent 0.0000(0.0000) | Loss 8.6432(10.8537) | Error 0.0000(0.0000) Steps 688(732.27) | Grad Norm 2.5134(61426.1764) | Total Time 0.00(0.00)\n",
      "Iter 1415 | Time 68.4063(76.4110) | Bit/dim 3.6601(4.2488) | Xent 0.0000(0.0000) | Loss 8.9134(10.7954) | Error 0.0000(0.0000) Steps 688(730.94) | Grad Norm 2.3048(59583.4602) | Total Time 0.00(0.00)\n",
      "Iter 1416 | Time 67.3513(76.1392) | Bit/dim 3.6452(4.2307) | Xent 0.0000(0.0000) | Loss 8.6253(10.7303) | Error 0.0000(0.0000) Steps 652(728.57) | Grad Norm 1.9401(57796.0146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 24.5323, Epoch Time 443.6110(418.2009), Bit/dim 3.6454(best: 3.5208), Xent 0.0000, Loss 3.6454, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1417 | Time 63.9646(75.7740) | Bit/dim 3.6400(4.2130) | Xent 0.0000(0.0000) | Loss 12.6071(10.7866) | Error 0.0000(0.0000) Steps 658(726.45) | Grad Norm 1.9500(56062.1927) | Total Time 0.00(0.00)\n",
      "Iter 1418 | Time 63.4440(75.4041) | Bit/dim 3.6400(4.1958) | Xent 0.0000(0.0000) | Loss 8.5194(10.7186) | Error 0.0000(0.0000) Steps 640(723.86) | Grad Norm 1.7992(54380.3809) | Total Time 0.00(0.00)\n",
      "Iter 1419 | Time 64.7451(75.0843) | Bit/dim 3.6228(4.1786) | Xent 0.0000(0.0000) | Loss 8.8383(10.6622) | Error 0.0000(0.0000) Steps 664(722.06) | Grad Norm 1.9068(52749.0267) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 67.4959(74.8567) | Bit/dim 3.6148(4.1617) | Xent 0.0000(0.0000) | Loss 8.7636(10.6053) | Error 0.0000(0.0000) Steps 646(719.78) | Grad Norm 1.9375(51166.6140) | Total Time 0.00(0.00)\n",
      "Iter 1421 | Time 69.0774(74.6833) | Bit/dim 3.6250(4.1456) | Xent 0.0000(0.0000) | Loss 8.6261(10.5459) | Error 0.0000(0.0000) Steps 670(718.29) | Grad Norm 1.6246(49631.6643) | Total Time 0.00(0.00)\n",
      "Iter 1422 | Time 63.5011(74.3478) | Bit/dim 3.6126(4.1296) | Xent 0.0000(0.0000) | Loss 8.5127(10.4849) | Error 0.0000(0.0000) Steps 652(716.30) | Grad Norm 1.4075(48142.7566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 23.8835, Epoch Time 431.8795(418.6113), Bit/dim 3.6090(best: 3.5208), Xent 0.0000, Loss 3.6090, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1423 | Time 65.8299(74.0923) | Bit/dim 3.6119(4.1141) | Xent 0.0000(0.0000) | Loss 11.9723(10.5295) | Error 0.0000(0.0000) Steps 634(713.83) | Grad Norm 1.6516(46698.5235) | Total Time 0.00(0.00)\n",
      "Iter 1424 | Time 65.9914(73.8493) | Bit/dim 3.5982(4.0986) | Xent 0.0000(0.0000) | Loss 8.7452(10.4760) | Error 0.0000(0.0000) Steps 664(712.34) | Grad Norm 1.7647(45297.6207) | Total Time 0.00(0.00)\n",
      "Iter 1425 | Time 65.9020(73.6108) | Bit/dim 3.5968(4.0835) | Xent 0.0000(0.0000) | Loss 8.6621(10.4216) | Error 0.0000(0.0000) Steps 658(710.71) | Grad Norm 1.4264(43938.7349) | Total Time 0.00(0.00)\n",
      "Iter 1426 | Time 65.1701(73.3576) | Bit/dim 3.5972(4.0689) | Xent 0.0000(0.0000) | Loss 8.5868(10.3665) | Error 0.0000(0.0000) Steps 622(708.05) | Grad Norm 1.2652(42620.6108) | Total Time 0.00(0.00)\n",
      "Iter 1427 | Time 61.6018(73.0050) | Bit/dim 3.5915(4.0546) | Xent 0.0000(0.0000) | Loss 8.7460(10.3179) | Error 0.0000(0.0000) Steps 646(706.18) | Grad Norm 1.3302(41342.0324) | Total Time 0.00(0.00)\n",
      "Iter 1428 | Time 69.2207(72.8914) | Bit/dim 3.5820(4.0404) | Xent 0.0000(0.0000) | Loss 8.7400(10.2706) | Error 0.0000(0.0000) Steps 652(704.56) | Grad Norm 1.1990(40101.8073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 23.7130, Epoch Time 433.6414(419.0622), Bit/dim 3.5855(best: 3.5208), Xent 0.0000, Loss 3.5855, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1429 | Time 65.1838(72.6602) | Bit/dim 3.5795(4.0266) | Xent 0.0000(0.0000) | Loss 12.6948(10.3433) | Error 0.0000(0.0000) Steps 670(703.52) | Grad Norm 1.0320(38898.7841) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 64.5364(72.4165) | Bit/dim 3.5816(4.0133) | Xent 0.0000(0.0000) | Loss 8.6351(10.2921) | Error 0.0000(0.0000) Steps 640(701.62) | Grad Norm 1.0980(37731.8535) | Total Time 0.00(0.00)\n",
      "Iter 1431 | Time 64.6674(72.1840) | Bit/dim 3.5751(4.0001) | Xent 0.0000(0.0000) | Loss 8.4716(10.2374) | Error 0.0000(0.0000) Steps 640(699.77) | Grad Norm 1.0203(36599.9285) | Total Time 0.00(0.00)\n",
      "Iter 1432 | Time 68.0366(72.0596) | Bit/dim 3.5794(3.9875) | Xent 0.0000(0.0000) | Loss 8.6588(10.1901) | Error 0.0000(0.0000) Steps 664(698.69) | Grad Norm 0.9608(35501.9595) | Total Time 0.00(0.00)\n",
      "Iter 1433 | Time 63.0745(71.7900) | Bit/dim 3.5860(3.9755) | Xent 0.0000(0.0000) | Loss 8.7776(10.1477) | Error 0.0000(0.0000) Steps 658(697.47) | Grad Norm 0.8231(34436.9254) | Total Time 0.00(0.00)\n",
      "Iter 1434 | Time 61.5244(71.4821) | Bit/dim 3.5680(3.9632) | Xent 0.0000(0.0000) | Loss 8.6324(10.1022) | Error 0.0000(0.0000) Steps 640(695.75) | Grad Norm 0.8316(33403.8426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 23.7220, Epoch Time 427.1437(419.3046), Bit/dim 3.5718(best: 3.5208), Xent 0.0000, Loss 3.5718, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1435 | Time 60.7905(71.1613) | Bit/dim 3.5756(3.9516) | Xent 0.0000(0.0000) | Loss 12.5090(10.1744) | Error 0.0000(0.0000) Steps 634(693.90) | Grad Norm 0.8421(32401.7526) | Total Time 0.00(0.00)\n",
      "Iter 1436 | Time 60.6187(70.8450) | Bit/dim 3.5656(3.9400) | Xent 0.0000(0.0000) | Loss 8.5974(10.1271) | Error 0.0000(0.0000) Steps 628(691.92) | Grad Norm 0.7734(31429.7232) | Total Time 0.00(0.00)\n",
      "Iter 1437 | Time 65.9295(70.6976) | Bit/dim 3.5662(3.9288) | Xent 0.0000(0.0000) | Loss 8.4774(10.0776) | Error 0.0000(0.0000) Steps 634(690.18) | Grad Norm 0.7189(30486.8531) | Total Time 0.00(0.00)\n",
      "Iter 1438 | Time 62.6790(70.4570) | Bit/dim 3.5762(3.9182) | Xent 0.0000(0.0000) | Loss 8.3974(10.0272) | Error 0.0000(0.0000) Steps 628(688.32) | Grad Norm 0.7445(29572.2698) | Total Time 0.00(0.00)\n",
      "Iter 1439 | Time 65.2541(70.3009) | Bit/dim 3.5503(3.9072) | Xent 0.0000(0.0000) | Loss 8.5294(9.9823) | Error 0.0000(0.0000) Steps 646(687.05) | Grad Norm 0.6471(28685.1211) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 64.8572(70.1376) | Bit/dim 3.5515(3.8965) | Xent 0.0000(0.0000) | Loss 8.6984(9.9438) | Error 0.0000(0.0000) Steps 652(686.00) | Grad Norm 0.6972(27824.5884) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 23.9400, Epoch Time 420.1121(419.3289), Bit/dim 3.5629(best: 3.5208), Xent 0.0000, Loss 3.5629, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1441 | Time 62.9099(69.9208) | Bit/dim 3.5676(3.8867) | Xent 0.0000(0.0000) | Loss 12.4593(10.0192) | Error 0.0000(0.0000) Steps 634(684.44) | Grad Norm 0.6819(26989.8712) | Total Time 0.00(0.00)\n",
      "Iter 1442 | Time 66.2704(69.8113) | Bit/dim 3.5629(3.8769) | Xent 0.0000(0.0000) | Loss 8.5473(9.9751) | Error 0.0000(0.0000) Steps 646(683.28) | Grad Norm 0.6102(26180.1934) | Total Time 0.00(0.00)\n",
      "Iter 1443 | Time 63.8544(69.6326) | Bit/dim 3.5448(3.8670) | Xent 0.0000(0.0000) | Loss 8.5419(9.9321) | Error 0.0000(0.0000) Steps 658(682.52) | Grad Norm 0.6044(25394.8057) | Total Time 0.00(0.00)\n",
      "Iter 1444 | Time 67.2822(69.5621) | Bit/dim 3.5580(3.8577) | Xent 0.0000(0.0000) | Loss 8.5534(9.8907) | Error 0.0000(0.0000) Steps 652(681.61) | Grad Norm 0.6542(24632.9812) | Total Time 0.00(0.00)\n",
      "Iter 1445 | Time 65.8743(69.4514) | Bit/dim 3.5518(3.8485) | Xent 0.0000(0.0000) | Loss 8.4835(9.8485) | Error 0.0000(0.0000) Steps 646(680.54) | Grad Norm 0.7218(23894.0134) | Total Time 0.00(0.00)\n",
      "Iter 1446 | Time 63.3248(69.2676) | Bit/dim 3.5626(3.8400) | Xent 0.0000(0.0000) | Loss 8.4711(9.8072) | Error 0.0000(0.0000) Steps 634(679.14) | Grad Norm 0.5645(23177.2099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 23.9732, Epoch Time 429.3004(419.6280), Bit/dim 3.5535(best: 3.5208), Xent 0.0000, Loss 3.5535, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1447 | Time 60.7988(69.0136) | Bit/dim 3.5471(3.8312) | Xent 0.0000(0.0000) | Loss 12.7311(9.8949) | Error 0.0000(0.0000) Steps 634(677.79) | Grad Norm 0.5462(22481.9100) | Total Time 0.00(0.00)\n",
      "Iter 1448 | Time 66.9276(68.9510) | Bit/dim 3.5515(3.8228) | Xent 0.0000(0.0000) | Loss 8.2903(9.8468) | Error 0.0000(0.0000) Steps 646(676.84) | Grad Norm 0.4961(21807.4676) | Total Time 0.00(0.00)\n",
      "Iter 1449 | Time 67.6220(68.9111) | Bit/dim 3.5577(3.8148) | Xent 0.0000(0.0000) | Loss 8.6039(9.8095) | Error 0.0000(0.0000) Steps 670(676.63) | Grad Norm 0.5042(21153.2587) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 66.3395(68.8340) | Bit/dim 3.5576(3.8071) | Xent 0.0000(0.0000) | Loss 8.6513(9.7747) | Error 0.0000(0.0000) Steps 676(676.61) | Grad Norm 0.4683(20518.6750) | Total Time 0.00(0.00)\n",
      "Iter 1451 | Time 68.0836(68.8115) | Bit/dim 3.5492(3.7994) | Xent 0.0000(0.0000) | Loss 8.5819(9.7390) | Error 0.0000(0.0000) Steps 652(675.87) | Grad Norm 0.4570(19903.1284) | Total Time 0.00(0.00)\n",
      "Iter 1452 | Time 62.1335(68.6111) | Bit/dim 3.5411(3.7916) | Xent 0.0000(0.0000) | Loss 8.4793(9.7012) | Error 0.0000(0.0000) Steps 622(674.26) | Grad Norm 0.5587(19306.0513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 23.6349, Epoch Time 431.2077(419.9754), Bit/dim 3.5500(best: 3.5208), Xent 0.0000, Loss 3.5500, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1453 | Time 65.1058(68.5060) | Bit/dim 3.5407(3.7841) | Xent 0.0000(0.0000) | Loss 12.4798(9.7845) | Error 0.0000(0.0000) Steps 658(673.77) | Grad Norm 0.4038(18726.8819) | Total Time 0.00(0.00)\n",
      "Iter 1454 | Time 66.5027(68.4459) | Bit/dim 3.5427(3.7769) | Xent 0.0000(0.0000) | Loss 8.6459(9.7504) | Error 0.0000(0.0000) Steps 640(672.76) | Grad Norm 0.4170(18165.0880) | Total Time 0.00(0.00)\n",
      "Iter 1455 | Time 65.1142(68.3459) | Bit/dim 3.5549(3.7702) | Xent 0.0000(0.0000) | Loss 8.3760(9.7091) | Error 0.0000(0.0000) Steps 640(671.77) | Grad Norm 0.4897(17620.1500) | Total Time 0.00(0.00)\n",
      "Iter 1456 | Time 66.7104(68.2968) | Bit/dim 3.5510(3.7636) | Xent 0.0000(0.0000) | Loss 8.4414(9.6711) | Error 0.0000(0.0000) Steps 664(671.54) | Grad Norm 0.4134(17091.5579) | Total Time 0.00(0.00)\n",
      "Iter 1457 | Time 61.3294(68.0878) | Bit/dim 3.5418(3.7570) | Xent 0.0000(0.0000) | Loss 8.4978(9.6359) | Error 0.0000(0.0000) Steps 634(670.41) | Grad Norm 0.4529(16578.8248) | Total Time 0.00(0.00)\n",
      "Iter 1458 | Time 67.6942(68.0760) | Bit/dim 3.5403(3.7505) | Xent 0.0000(0.0000) | Loss 8.2270(9.5936) | Error 0.0000(0.0000) Steps 634(669.32) | Grad Norm 0.4181(16081.4726) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 24.1260, Epoch Time 432.2326(420.3431), Bit/dim 3.5462(best: 3.5208), Xent 0.0000, Loss 3.5462, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1459 | Time 68.2665(68.0817) | Bit/dim 3.5520(3.7445) | Xent 0.0000(0.0000) | Loss 12.2688(9.6739) | Error 0.0000(0.0000) Steps 664(669.16) | Grad Norm 0.6505(15599.0479) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 65.0257(67.9900) | Bit/dim 3.5402(3.7384) | Xent 0.0000(0.0000) | Loss 8.5590(9.6404) | Error 0.0000(0.0000) Steps 652(668.65) | Grad Norm 0.3797(15131.0879) | Total Time 0.00(0.00)\n",
      "Iter 1461 | Time 64.4941(67.8852) | Bit/dim 3.5394(3.7324) | Xent 0.0000(0.0000) | Loss 8.3988(9.6032) | Error 0.0000(0.0000) Steps 658(668.33) | Grad Norm 0.4705(14677.1693) | Total Time 0.00(0.00)\n",
      "Iter 1462 | Time 61.6871(67.6992) | Bit/dim 3.5333(3.7264) | Xent 0.0000(0.0000) | Loss 8.4367(9.5682) | Error 0.0000(0.0000) Steps 646(667.66) | Grad Norm 0.4122(14236.8666) | Total Time 0.00(0.00)\n",
      "Iter 1463 | Time 64.1698(67.5933) | Bit/dim 3.5418(3.7209) | Xent 0.0000(0.0000) | Loss 8.4877(9.5358) | Error 0.0000(0.0000) Steps 652(667.19) | Grad Norm 0.4663(13809.7746) | Total Time 0.00(0.00)\n",
      "Iter 1464 | Time 69.2047(67.6417) | Bit/dim 3.5399(3.7155) | Xent 0.0000(0.0000) | Loss 8.5709(9.5068) | Error 0.0000(0.0000) Steps 646(666.55) | Grad Norm 0.4029(13395.4935) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 23.8733, Epoch Time 432.7647(420.7158), Bit/dim 3.5418(best: 3.5208), Xent 0.0000, Loss 3.5418, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1465 | Time 67.1144(67.6259) | Bit/dim 3.5406(3.7102) | Xent 0.0000(0.0000) | Loss 12.4336(9.5946) | Error 0.0000(0.0000) Steps 670(666.66) | Grad Norm 0.4378(12993.6418) | Total Time 0.00(0.00)\n",
      "Iter 1466 | Time 66.5923(67.5949) | Bit/dim 3.5357(3.7050) | Xent 0.0000(0.0000) | Loss 8.5089(9.5621) | Error 0.0000(0.0000) Steps 646(666.04) | Grad Norm 0.5555(12603.8492) | Total Time 0.00(0.00)\n",
      "Iter 1467 | Time 63.7797(67.4804) | Bit/dim 3.5409(3.7001) | Xent 0.0000(0.0000) | Loss 8.4024(9.5273) | Error 0.0000(0.0000) Steps 652(665.62) | Grad Norm 0.7454(12225.7561) | Total Time 0.00(0.00)\n",
      "Iter 1468 | Time 66.7436(67.4583) | Bit/dim 3.5481(3.6955) | Xent 0.0000(0.0000) | Loss 8.5769(9.4988) | Error 0.0000(0.0000) Steps 676(665.93) | Grad Norm 0.5328(11858.9994) | Total Time 0.00(0.00)\n",
      "Iter 1469 | Time 68.3682(67.4856) | Bit/dim 3.5436(3.6909) | Xent 0.0000(0.0000) | Loss 8.3876(9.4654) | Error 0.0000(0.0000) Steps 628(664.79) | Grad Norm 0.7424(11503.2517) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 68.7564(67.5237) | Bit/dim 3.5471(3.6866) | Xent 0.0000(0.0000) | Loss 8.7176(9.4430) | Error 0.0000(0.0000) Steps 676(665.13) | Grad Norm 0.6299(11158.1730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 23.5155, Epoch Time 441.2739(421.3325), Bit/dim 3.5427(best: 3.5208), Xent 0.0000, Loss 3.5427, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1471 | Time 66.3435(67.4883) | Bit/dim 3.5377(3.6822) | Xent 0.0000(0.0000) | Loss 12.7928(9.5435) | Error 0.0000(0.0000) Steps 676(665.45) | Grad Norm 0.5962(10823.4457) | Total Time 0.00(0.00)\n",
      "Iter 1472 | Time 62.8176(67.3482) | Bit/dim 3.5256(3.6775) | Xent 0.0000(0.0000) | Loss 8.3371(9.5073) | Error 0.0000(0.0000) Steps 640(664.69) | Grad Norm 1.0702(10498.7745) | Total Time 0.00(0.00)\n",
      "Iter 1473 | Time 65.7653(67.3007) | Bit/dim 3.5422(3.6734) | Xent 0.0000(0.0000) | Loss 8.5690(9.4792) | Error 0.0000(0.0000) Steps 640(663.95) | Grad Norm 0.4469(10183.8246) | Total Time 0.00(0.00)\n",
      "Iter 1474 | Time 60.8449(67.1070) | Bit/dim 3.5450(3.6696) | Xent 0.0000(0.0000) | Loss 8.4341(9.4478) | Error 0.0000(0.0000) Steps 640(663.23) | Grad Norm 0.7363(9878.3320) | Total Time 0.00(0.00)\n",
      "Iter 1475 | Time 68.7519(67.1564) | Bit/dim 3.5472(3.6659) | Xent 0.0000(0.0000) | Loss 8.5412(9.4206) | Error 0.0000(0.0000) Steps 682(663.79) | Grad Norm 0.7483(9582.0045) | Total Time 0.00(0.00)\n",
      "Iter 1476 | Time 68.2345(67.1887) | Bit/dim 3.5409(3.6621) | Xent 0.0000(0.0000) | Loss 8.2797(9.3864) | Error 0.0000(0.0000) Steps 652(663.44) | Grad Norm 1.1075(9294.5776) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 23.5680, Epoch Time 432.2153(421.6590), Bit/dim 3.5458(best: 3.5208), Xent 0.0000, Loss 3.5458, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1477 | Time 66.3864(67.1647) | Bit/dim 3.5412(3.6585) | Xent 0.0000(0.0000) | Loss 12.7049(9.4859) | Error 0.0000(0.0000) Steps 664(663.46) | Grad Norm 0.9800(9015.7696) | Total Time 0.00(0.00)\n",
      "Iter 1478 | Time 70.8579(67.2755) | Bit/dim 3.5466(3.6552) | Xent 0.0000(0.0000) | Loss 8.5734(9.4586) | Error 0.0000(0.0000) Steps 700(664.55) | Grad Norm 0.5298(8745.3124) | Total Time 0.00(0.00)\n",
      "Iter 1479 | Time 67.4562(67.2809) | Bit/dim 3.5452(3.6519) | Xent 0.0000(0.0000) | Loss 8.5540(9.4314) | Error 0.0000(0.0000) Steps 652(664.18) | Grad Norm 1.4806(8482.9975) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 69.0155(67.3329) | Bit/dim 3.5436(3.6486) | Xent 0.0000(0.0000) | Loss 8.4427(9.4018) | Error 0.0000(0.0000) Steps 658(663.99) | Grad Norm 0.6466(8228.5270) | Total Time 0.00(0.00)\n",
      "Iter 1481 | Time 63.5930(67.2207) | Bit/dim 3.5332(3.6451) | Xent 0.0000(0.0000) | Loss 8.4368(9.3728) | Error 0.0000(0.0000) Steps 646(663.45) | Grad Norm 0.8221(7981.6958) | Total Time 0.00(0.00)\n",
      "Iter 1482 | Time 66.2613(67.1919) | Bit/dim 3.5497(3.6423) | Xent 0.0000(0.0000) | Loss 8.5312(9.3476) | Error 0.0000(0.0000) Steps 646(662.93) | Grad Norm 1.6160(7742.2934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 23.8105, Epoch Time 443.3691(422.3103), Bit/dim 3.5464(best: 3.5208), Xent 0.0000, Loss 3.5464, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1483 | Time 68.6611(67.2360) | Bit/dim 3.5571(3.6397) | Xent 0.0000(0.0000) | Loss 12.8169(9.4516) | Error 0.0000(0.0000) Steps 682(663.50) | Grad Norm 1.1980(7510.0606) | Total Time 0.00(0.00)\n",
      "Iter 1484 | Time 67.7330(67.2509) | Bit/dim 3.5499(3.6370) | Xent 0.0000(0.0000) | Loss 8.5216(9.4237) | Error 0.0000(0.0000) Steps 676(663.87) | Grad Norm 4.7339(7284.9008) | Total Time 0.00(0.00)\n",
      "Iter 1485 | Time 66.2765(67.2217) | Bit/dim 3.5470(3.6343) | Xent 0.0000(0.0000) | Loss 8.5267(9.3968) | Error 0.0000(0.0000) Steps 670(664.06) | Grad Norm 4.3802(7066.4851) | Total Time 0.00(0.00)\n",
      "Iter 1486 | Time 64.9759(67.1543) | Bit/dim 3.5717(3.6325) | Xent 0.0000(0.0000) | Loss 8.7870(9.3785) | Error 0.0000(0.0000) Steps 682(664.60) | Grad Norm 6.1060(6854.6738) | Total Time 0.00(0.00)\n",
      "Iter 1487 | Time 64.2456(67.0670) | Bit/dim 3.5985(3.6314) | Xent 0.0000(0.0000) | Loss 8.3774(9.3485) | Error 0.0000(0.0000) Steps 640(663.86) | Grad Norm 23.9749(6649.7528) | Total Time 0.00(0.00)\n",
      "Iter 1488 | Time 70.0578(67.1568) | Bit/dim 3.6544(3.6321) | Xent 0.0000(0.0000) | Loss 8.7507(9.3306) | Error 0.0000(0.0000) Steps 682(664.40) | Grad Norm 10.6979(6450.5812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 23.9434, Epoch Time 441.7688(422.8940), Bit/dim 3.6202(best: 3.5208), Xent 0.0000, Loss 3.6202, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1489 | Time 66.1208(67.1257) | Bit/dim 3.6068(3.6314) | Xent 0.0000(0.0000) | Loss 12.1289(9.4145) | Error 0.0000(0.0000) Steps 658(664.21) | Grad Norm 9.8419(6257.3590) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 66.4447(67.1053) | Bit/dim 3.6102(3.6307) | Xent 0.0000(0.0000) | Loss 8.8056(9.3962) | Error 0.0000(0.0000) Steps 688(664.92) | Grad Norm 2.6205(6069.7168) | Total Time 0.00(0.00)\n",
      "Iter 1491 | Time 68.2161(67.1386) | Bit/dim 3.6180(3.6303) | Xent 0.0000(0.0000) | Loss 8.7399(9.3766) | Error 0.0000(0.0000) Steps 676(665.26) | Grad Norm 2.7491(5887.7078) | Total Time 0.00(0.00)\n",
      "Iter 1492 | Time 66.2896(67.1131) | Bit/dim 3.6046(3.6296) | Xent 0.0000(0.0000) | Loss 8.4763(9.3495) | Error 0.0000(0.0000) Steps 640(664.50) | Grad Norm 2.2082(5711.1428) | Total Time 0.00(0.00)\n",
      "Iter 1493 | Time 65.2313(67.0567) | Bit/dim 3.6047(3.6288) | Xent 0.0000(0.0000) | Loss 8.6407(9.3283) | Error 0.0000(0.0000) Steps 682(665.02) | Grad Norm 1.8625(5539.8644) | Total Time 0.00(0.00)\n",
      "Iter 1494 | Time 67.9001(67.0820) | Bit/dim 3.5839(3.6275) | Xent 0.0000(0.0000) | Loss 8.6698(9.3085) | Error 0.0000(0.0000) Steps 646(664.45) | Grad Norm 2.1752(5373.7337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 24.1390, Epoch Time 440.0685(423.4093), Bit/dim 3.5841(best: 3.5208), Xent 0.0000, Loss 3.5841, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1495 | Time 62.6432(66.9488) | Bit/dim 3.5918(3.6264) | Xent 0.0000(0.0000) | Loss 12.1552(9.3939) | Error 0.0000(0.0000) Steps 652(664.08) | Grad Norm 1.8048(5212.5759) | Total Time 0.00(0.00)\n",
      "Iter 1496 | Time 64.1821(66.8658) | Bit/dim 3.5841(3.6251) | Xent 0.0000(0.0000) | Loss 8.5349(9.3682) | Error 0.0000(0.0000) Steps 658(663.90) | Grad Norm 1.5130(5056.2440) | Total Time 0.00(0.00)\n",
      "Iter 1497 | Time 67.0780(66.8722) | Bit/dim 3.5730(3.6236) | Xent 0.0000(0.0000) | Loss 8.6215(9.3458) | Error 0.0000(0.0000) Steps 652(663.54) | Grad Norm 1.5653(4904.6036) | Total Time 0.00(0.00)\n",
      "Iter 1498 | Time 61.5683(66.7131) | Bit/dim 3.5923(3.6226) | Xent 0.0000(0.0000) | Loss 8.6882(9.3260) | Error 0.0000(0.0000) Steps 652(663.19) | Grad Norm 1.6146(4757.5139) | Total Time 0.00(0.00)\n",
      "Iter 1499 | Time 65.1214(66.6653) | Bit/dim 3.5707(3.6211) | Xent 0.0000(0.0000) | Loss 8.5742(9.3035) | Error 0.0000(0.0000) Steps 646(662.68) | Grad Norm 1.9018(4614.8456) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 61.1634(66.5002) | Bit/dim 3.5616(3.6193) | Xent 0.0000(0.0000) | Loss 8.6592(9.2841) | Error 0.0000(0.0000) Steps 646(662.18) | Grad Norm 1.9999(4476.4602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 24.2728, Epoch Time 422.0672(423.3690), Bit/dim 3.5714(best: 3.5208), Xent 0.0000, Loss 3.5714, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1501 | Time 63.9579(66.4240) | Bit/dim 3.5733(3.6179) | Xent 0.0000(0.0000) | Loss 12.4802(9.3800) | Error 0.0000(0.0000) Steps 640(661.51) | Grad Norm 1.7702(4342.2195) | Total Time 0.00(0.00)\n",
      "Iter 1502 | Time 65.2373(66.3884) | Bit/dim 3.5746(3.6166) | Xent 0.0000(0.0000) | Loss 8.6287(9.3575) | Error 0.0000(0.0000) Steps 688(662.31) | Grad Norm 2.1642(4212.0178) | Total Time 0.00(0.00)\n",
      "Iter 1503 | Time 67.7987(66.4307) | Bit/dim 3.5621(3.6150) | Xent 0.0000(0.0000) | Loss 8.5608(9.3336) | Error 0.0000(0.0000) Steps 682(662.90) | Grad Norm 1.6328(4085.7063) | Total Time 0.00(0.00)\n",
      "Iter 1504 | Time 67.0058(66.4479) | Bit/dim 3.5674(3.6136) | Xent 0.0000(0.0000) | Loss 8.6679(9.3136) | Error 0.0000(0.0000) Steps 688(663.65) | Grad Norm 2.0746(3963.1973) | Total Time 0.00(0.00)\n",
      "Iter 1505 | Time 68.5370(66.5106) | Bit/dim 3.5552(3.6118) | Xent 0.0000(0.0000) | Loss 8.7522(9.2968) | Error 0.0000(0.0000) Steps 688(664.38) | Grad Norm 1.6929(3844.3522) | Total Time 0.00(0.00)\n",
      "Iter 1506 | Time 71.5772(66.6626) | Bit/dim 3.5655(3.6104) | Xent 0.0000(0.0000) | Loss 8.4583(9.2716) | Error 0.0000(0.0000) Steps 652(664.01) | Grad Norm 1.6789(3729.0720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 24.1667, Epoch Time 444.2669(423.9960), Bit/dim 3.5668(best: 3.5208), Xent 0.0000, Loss 3.5668, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 69.6059(66.7509) | Bit/dim 3.5704(3.6092) | Xent 0.0000(0.0000) | Loss 12.5373(9.3696) | Error 0.0000(0.0000) Steps 688(664.73) | Grad Norm 1.6568(3617.2495) | Total Time 0.00(0.00)\n",
      "Iter 1508 | Time 68.4806(66.8028) | Bit/dim 3.5782(3.6083) | Xent 0.0000(0.0000) | Loss 8.7734(9.3517) | Error 0.0000(0.0000) Steps 694(665.61) | Grad Norm 1.1042(3508.7652) | Total Time 0.00(0.00)\n",
      "Iter 1509 | Time 63.9475(66.7171) | Bit/dim 3.5530(3.6066) | Xent 0.0000(0.0000) | Loss 8.3484(9.3216) | Error 0.0000(0.0000) Steps 646(665.02) | Grad Norm 1.9196(3403.5598) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 64.0728(66.6378) | Bit/dim 3.5569(3.6051) | Xent 0.0000(0.0000) | Loss 8.2954(9.2908) | Error 0.0000(0.0000) Steps 646(664.45) | Grad Norm 2.4415(3301.5263) | Total Time 0.00(0.00)\n",
      "Iter 1511 | Time 69.3139(66.7181) | Bit/dim 3.5579(3.6037) | Xent 0.0000(0.0000) | Loss 8.5338(9.2681) | Error 0.0000(0.0000) Steps 706(665.70) | Grad Norm 2.7871(3202.5641) | Total Time 0.00(0.00)\n",
      "Iter 1512 | Time 63.7105(66.6279) | Bit/dim 3.5672(3.6026) | Xent 0.0000(0.0000) | Loss 8.3749(9.2413) | Error 0.0000(0.0000) Steps 634(664.74) | Grad Norm 4.3439(3106.6175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 24.2059, Epoch Time 439.2296(424.4530), Bit/dim 3.5782(best: 3.5208), Xent 0.0000, Loss 3.5782, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 72.7224(66.8107) | Bit/dim 3.5849(3.6021) | Xent 0.0000(0.0000) | Loss 12.7728(9.3473) | Error 0.0000(0.0000) Steps 676(665.08) | Grad Norm 2.8031(3013.5031) | Total Time 0.00(0.00)\n",
      "Iter 1514 | Time 57.9296(66.5443) | Bit/dim 3.5743(3.6013) | Xent 0.0000(0.0000) | Loss 8.6157(9.3253) | Error 0.0000(0.0000) Steps 622(663.79) | Grad Norm 4.0752(2923.2202) | Total Time 0.00(0.00)\n",
      "Iter 1515 | Time 64.6371(66.4870) | Bit/dim 3.6087(3.6015) | Xent 0.0000(0.0000) | Loss 8.7110(9.3069) | Error 0.0000(0.0000) Steps 670(663.98) | Grad Norm 3.4800(2835.6280) | Total Time 0.00(0.00)\n",
      "Iter 1516 | Time 62.7323(66.3744) | Bit/dim 3.5951(3.6013) | Xent 0.0000(0.0000) | Loss 8.5753(9.2849) | Error 0.0000(0.0000) Steps 670(664.16) | Grad Norm 4.7418(2750.7014) | Total Time 0.00(0.00)\n",
      "Iter 1517 | Time 67.5096(66.4085) | Bit/dim 3.6396(3.6024) | Xent 0.0000(0.0000) | Loss 8.6153(9.2648) | Error 0.0000(0.0000) Steps 682(664.69) | Grad Norm 10.3140(2668.4898) | Total Time 0.00(0.00)\n",
      "Iter 1518 | Time 62.0678(66.2782) | Bit/dim 3.5927(3.6021) | Xent 0.0000(0.0000) | Loss 8.6652(9.2469) | Error 0.0000(0.0000) Steps 670(664.85) | Grad Norm 5.4541(2588.5987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 24.7275, Epoch Time 428.4743(424.5736), Bit/dim 3.5838(best: 3.5208), Xent 0.0000, Loss 3.5838, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 65.9826(66.2694) | Bit/dim 3.5751(3.6013) | Xent 0.0000(0.0000) | Loss 12.9299(9.3573) | Error 0.0000(0.0000) Steps 670(665.01) | Grad Norm 2.3926(2511.0126) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 66.2950(66.2701) | Bit/dim 3.5871(3.6009) | Xent 0.0000(0.0000) | Loss 8.4813(9.3311) | Error 0.0000(0.0000) Steps 652(664.62) | Grad Norm 3.4448(2435.7855) | Total Time 0.00(0.00)\n",
      "Iter 1521 | Time 60.5493(66.0985) | Bit/dim 3.5889(3.6005) | Xent 0.0000(0.0000) | Loss 8.6384(9.3103) | Error 0.0000(0.0000) Steps 652(664.24) | Grad Norm 4.4695(2362.8460) | Total Time 0.00(0.00)\n",
      "Iter 1522 | Time 69.3018(66.1946) | Bit/dim 3.6729(3.6027) | Xent 0.0000(0.0000) | Loss 9.0586(9.3027) | Error 0.0000(0.0000) Steps 694(665.13) | Grad Norm 13.1369(2292.3548) | Total Time 0.00(0.00)\n",
      "Iter 1523 | Time 72.0838(66.3713) | Bit/dim 3.6508(3.6042) | Xent 0.0000(0.0000) | Loss 8.7037(9.2848) | Error 0.0000(0.0000) Steps 718(666.72) | Grad Norm 11.2735(2223.9223) | Total Time 0.00(0.00)\n",
      "Iter 1524 | Time 63.2689(66.2782) | Bit/dim 3.6087(3.6043) | Xent 0.0000(0.0000) | Loss 8.4631(9.2601) | Error 0.0000(0.0000) Steps 652(666.27) | Grad Norm 14.1339(2157.6287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 25.2642, Epoch Time 438.9122(425.0038), Bit/dim 3.6247(best: 3.5208), Xent 0.0000, Loss 3.6247, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 67.5192(66.3155) | Bit/dim 3.6249(3.6049) | Xent 0.0000(0.0000) | Loss 12.3218(9.3520) | Error 0.0000(0.0000) Steps 688(666.93) | Grad Norm 19.8616(2093.4957) | Total Time 0.00(0.00)\n",
      "Iter 1526 | Time 69.2823(66.4045) | Bit/dim 3.7043(3.6079) | Xent 0.0000(0.0000) | Loss 9.0385(9.3426) | Error 0.0000(0.0000) Steps 724(668.64) | Grad Norm 20.0037(2031.2909) | Total Time 0.00(0.00)\n",
      "Iter 1527 | Time 74.5401(66.6485) | Bit/dim 4.1137(3.6231) | Xent 0.0000(0.0000) | Loss 9.6786(9.3526) | Error 0.0000(0.0000) Steps 724(670.30) | Grad Norm 1979.7256(2029.7439) | Total Time 0.00(0.00)\n",
      "Iter 1528 | Time 82.3884(67.1207) | Bit/dim 5.4468(3.6778) | Xent 0.0000(0.0000) | Loss 12.9581(9.4608) | Error 0.0000(0.0000) Steps 814(674.61) | Grad Norm 61157.7897(3803.5853) | Total Time 0.00(0.00)\n",
      "Iter 1529 | Time 76.1578(67.3918) | Bit/dim 6.9734(3.7766) | Xent 0.0000(0.0000) | Loss 15.9366(9.6551) | Error 0.0000(0.0000) Steps 814(678.79) | Grad Norm 548789.3456(20153.1581) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 95.8124(68.2445) | Bit/dim 6.4433(3.8567) | Xent 0.0000(0.0000) | Loss 14.8175(9.8100) | Error 0.0000(0.0000) Steps 868(684.47) | Grad Norm 455973.2198(33227.7600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 30.7420, Epoch Time 512.4491(427.6271), Bit/dim 5.5893(best: 3.5208), Xent 0.0000, Loss 5.5893, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 83.2719(68.6953) | Bit/dim 5.5427(3.9072) | Xent 0.0000(0.0000) | Loss 17.5257(10.0414) | Error 0.0000(0.0000) Steps 796(687.81) | Grad Norm 538533.8556(48386.9428) | Total Time 0.00(0.00)\n",
      "Iter 1532 | Time 91.8061(69.3886) | Bit/dim 5.1324(3.9440) | Xent 0.0000(0.0000) | Loss 12.0079(10.1004) | Error 0.0000(0.0000) Steps 832(692.14) | Grad Norm 109417.3590(50217.8553) | Total Time 0.00(0.00)\n",
      "Iter 1533 | Time 95.5597(70.1737) | Bit/dim 5.0002(3.9757) | Xent 0.0000(0.0000) | Loss 11.8676(10.1534) | Error 0.0000(0.0000) Steps 826(696.16) | Grad Norm 3504.5090(48816.4549) | Total Time 0.00(0.00)\n",
      "Iter 1534 | Time 95.4449(70.9319) | Bit/dim 5.2319(4.0134) | Xent 0.0000(0.0000) | Loss 12.0090(10.2091) | Error 0.0000(0.0000) Steps 814(699.69) | Grad Norm 695.9671(47372.8403) | Total Time 0.00(0.00)\n",
      "Iter 1535 | Time 90.7450(71.5263) | Bit/dim 5.4517(4.0565) | Xent 0.0000(0.0000) | Loss 12.6603(10.2826) | Error 0.0000(0.0000) Steps 802(702.76) | Grad Norm 29221.2637(46828.2930) | Total Time 0.00(0.00)\n",
      "Iter 1536 | Time 99.2335(72.3575) | Bit/dim 5.7806(4.1082) | Xent 0.0000(0.0000) | Loss 12.9889(10.3638) | Error 0.0000(0.0000) Steps 850(707.18) | Grad Norm 11673.1588(45773.6390) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 31.4768, Epoch Time 603.5524(432.9049), Bit/dim 6.3594(best: 3.5208), Xent 0.0000, Loss 6.3594, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 100.7833(73.2103) | Bit/dim 6.3523(4.1756) | Xent 0.0000(0.0000) | Loss 19.7111(10.6442) | Error 0.0000(0.0000) Steps 868(712.00) | Grad Norm 255885.2800(52076.9882) | Total Time 0.00(0.00)\n",
      "Iter 1538 | Time 96.0746(73.8962) | Bit/dim 6.6296(4.2492) | Xent 0.0000(0.0000) | Loss 15.0932(10.7777) | Error 0.0000(0.0000) Steps 850(716.14) | Grad Norm 235262.8623(57572.5644) | Total Time 0.00(0.00)\n",
      "Iter 1539 | Time 105.6412(74.8485) | Bit/dim 6.5257(4.3175) | Xent 0.0000(0.0000) | Loss 14.9306(10.9023) | Error 0.0000(0.0000) Steps 862(720.52) | Grad Norm 594141.5108(73669.6328) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 98.9587(75.5718) | Bit/dim 6.4152(4.3804) | Xent 0.0000(0.0000) | Loss 14.5164(11.0107) | Error 0.0000(0.0000) Steps 856(724.58) | Grad Norm 12012866.8594(431845.5496) | Total Time 0.00(0.00)\n",
      "Iter 1541 | Time 101.4915(76.3494) | Bit/dim 6.7832(4.4525) | Xent 0.0000(0.0000) | Loss 15.1950(11.1362) | Error 0.0000(0.0000) Steps 880(729.24) | Grad Norm 1387667.2471(460520.2006) | Total Time 0.00(0.00)\n",
      "Iter 1542 | Time 108.6519(77.3185) | Bit/dim 6.9432(4.5272) | Xent 0.0000(0.0000) | Loss 15.8695(11.2782) | Error 0.0000(0.0000) Steps 898(734.31) | Grad Norm 780577.7255(470121.9263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 31.4908, Epoch Time 659.1246(439.6915), Bit/dim 7.4864(best: 3.5208), Xent 0.0000, Loss 7.4864, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 105.7226(78.1706) | Bit/dim 7.4432(4.6147) | Xent 0.0000(0.0000) | Loss 22.4120(11.6123) | Error 0.0000(0.0000) Steps 916(739.76) | Grad Norm 149937.6601(460516.3983) | Total Time 0.00(0.00)\n",
      "Iter 1544 | Time 95.4795(78.6899) | Bit/dim 9.3730(4.7574) | Xent 0.0000(0.0000) | Loss 21.2459(11.9013) | Error 0.0000(0.0000) Steps 892(744.33) | Grad Norm 13113.3622(447094.3072) | Total Time 0.00(0.00)\n",
      "Iter 1545 | Time 97.8272(79.2640) | Bit/dim 9.9429(4.9130) | Xent 0.0000(0.0000) | Loss 21.8664(12.2002) | Error 0.0000(0.0000) Steps 880(748.40) | Grad Norm 109963.7551(436980.3907) | Total Time 0.00(0.00)\n",
      "Iter 1546 | Time 98.4468(79.8395) | Bit/dim 12.1540(5.1302) | Xent 0.0000(0.0000) | Loss 27.1924(12.6500) | Error 0.0000(0.0000) Steps 910(753.24) | Grad Norm 2429.1934(423943.8548) | Total Time 0.00(0.00)\n",
      "Iter 1547 | Time 105.8232(80.6190) | Bit/dim 7.7686(5.2094) | Xent 0.0000(0.0000) | Loss 17.5799(12.7979) | Error 0.0000(0.0000) Steps 898(757.59) | Grad Norm 308964.9165(420494.4866) | Total Time 0.00(0.00)\n",
      "Iter 1548 | Time 114.5566(81.6371) | Bit/dim 7.3252(5.2729) | Xent 0.0000(0.0000) | Loss 16.7159(12.9154) | Error 0.0000(0.0000) Steps 910(762.16) | Grad Norm 362877.7102(418765.9833) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 31.5057, Epoch Time 665.6194(446.4693), Bit/dim 7.2971(best: 3.5208), Xent 0.0000, Loss 7.2971, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 98.3546(82.1387) | Bit/dim 7.2797(5.3331) | Xent 0.0000(0.0000) | Loss 21.2716(13.1661) | Error 0.0000(0.0000) Steps 856(764.97) | Grad Norm 1367493.9246(447227.8215) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 114.8678(83.1205) | Bit/dim 7.3362(5.3932) | Xent 0.0000(0.0000) | Loss 16.6684(13.2712) | Error 0.0000(0.0000) Steps 922(769.68) | Grad Norm 544598.8035(450148.9510) | Total Time 0.00(0.00)\n",
      "Iter 1551 | Time 105.3441(83.7872) | Bit/dim 7.5186(5.4569) | Xent 0.0000(0.0000) | Loss 17.0420(13.3843) | Error 0.0000(0.0000) Steps 910(773.89) | Grad Norm 86717.4995(439246.0075) | Total Time 0.00(0.00)\n",
      "Iter 1552 | Time 102.9090(84.3609) | Bit/dim 7.8399(5.5284) | Xent 0.0000(0.0000) | Loss 17.8088(13.5170) | Error 0.0000(0.0000) Steps 904(777.80) | Grad Norm 164821.3256(431013.2670) | Total Time 0.00(0.00)\n",
      "Iter 1553 | Time 103.0466(84.9215) | Bit/dim 8.2478(5.6100) | Xent 0.0000(0.0000) | Loss 18.4390(13.6647) | Error 0.0000(0.0000) Steps 862(780.32) | Grad Norm 68761.4908(420145.7137) | Total Time 0.00(0.00)\n",
      "Iter 1554 | Time 102.3846(85.4454) | Bit/dim 8.3461(5.6921) | Xent 0.0000(0.0000) | Loss 18.7018(13.8158) | Error 0.0000(0.0000) Steps 868(782.95) | Grad Norm 30803.0423(408465.4336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 31.4841, Epoch Time 674.4876(453.3099), Bit/dim 8.0445(best: 3.5208), Xent 0.0000, Loss 8.0445, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 108.4363(86.1351) | Bit/dim 8.0276(5.7621) | Xent 0.0000(0.0000) | Loss 23.3497(14.1018) | Error 0.0000(0.0000) Steps 910(786.77) | Grad Norm 9371.3298(396492.6105) | Total Time 0.00(0.00)\n",
      "Iter 1556 | Time 102.1399(86.6152) | Bit/dim 7.2190(5.8058) | Xent 0.0000(0.0000) | Loss 16.2983(14.1677) | Error 0.0000(0.0000) Steps 910(790.46) | Grad Norm 27913.8172(385435.2467) | Total Time 0.00(0.00)\n",
      "Iter 1557 | Time 97.4931(86.9416) | Bit/dim 6.6010(5.8297) | Xent 0.0000(0.0000) | Loss 15.1145(14.1961) | Error 0.0000(0.0000) Steps 874(792.97) | Grad Norm 2205614.8408(440040.6345) | Total Time 0.00(0.00)\n",
      "Iter 1558 | Time 99.3811(87.3148) | Bit/dim 6.2153(5.8413) | Xent 0.0000(0.0000) | Loss 14.0998(14.1932) | Error 0.0000(0.0000) Steps 856(794.86) | Grad Norm 16203.3004(427325.5145) | Total Time 0.00(0.00)\n",
      "Iter 1559 | Time 102.5774(87.7726) | Bit/dim 5.8867(5.8426) | Xent 0.0000(0.0000) | Loss 13.4312(14.1704) | Error 0.0000(0.0000) Steps 868(797.05) | Grad Norm 4044.0491(414627.0705) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 98.8868(88.1061) | Bit/dim 5.6571(5.8371) | Xent 0.0000(0.0000) | Loss 13.1262(14.1390) | Error 0.0000(0.0000) Steps 856(798.82) | Grad Norm 6365.1902(402379.2141) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 29.4620, Epoch Time 654.6024(459.3486), Bit/dim 5.5682(best: 3.5208), Xent 0.0000, Loss 5.5682, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 97.7947(88.3967) | Bit/dim 5.5428(5.8282) | Xent 0.0000(0.0000) | Loss 17.0366(14.2260) | Error 0.0000(0.0000) Steps 868(800.90) | Grad Norm 1524290.4998(436036.5527) | Total Time 0.00(0.00)\n",
      "Iter 1562 | Time 93.8365(88.5599) | Bit/dim 5.5099(5.8187) | Xent 0.0000(0.0000) | Loss 12.8161(14.1837) | Error 0.0000(0.0000) Steps 844(802.19) | Grad Norm 9926427.6288(720748.2850) | Total Time 0.00(0.00)\n",
      "Iter 1563 | Time 94.5919(88.7409) | Bit/dim 5.4509(5.8077) | Xent 0.0000(0.0000) | Loss 12.7031(14.1393) | Error 0.0000(0.0000) Steps 838(803.26) | Grad Norm 2492061.0305(773887.6673) | Total Time 0.00(0.00)\n",
      "Iter 1564 | Time 99.0575(89.0504) | Bit/dim 5.3732(5.7946) | Xent 0.0000(0.0000) | Loss 12.6005(14.0931) | Error 0.0000(0.0000) Steps 826(803.95) | Grad Norm 89.0551(750673.7090) | Total Time 0.00(0.00)\n",
      "Iter 1565 | Time 87.4333(89.0019) | Bit/dim 5.0027(5.7709) | Xent 0.0000(0.0000) | Loss 11.7021(14.0214) | Error 0.0000(0.0000) Steps 796(803.71) | Grad Norm 1776.4559(728206.7914) | Total Time 0.00(0.00)\n",
      "Iter 1566 | Time 87.2738(88.9500) | Bit/dim 4.7183(5.7393) | Xent 0.0000(0.0000) | Loss 10.9828(13.9302) | Error 0.0000(0.0000) Steps 784(803.12) | Grad Norm 78.1119(706362.9310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 27.7558, Epoch Time 603.9212(463.6858), Bit/dim 4.3589(best: 3.5208), Xent 0.0000, Loss 4.3589, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 82.8301(88.7664) | Bit/dim 4.3601(5.6979) | Xent 0.0000(0.0000) | Loss 14.6903(13.9530) | Error 0.0000(0.0000) Steps 754(801.64) | Grad Norm 1068.1658(685204.0880) | Total Time 0.00(0.00)\n",
      "Iter 1568 | Time 81.6280(88.5523) | Bit/dim 4.1531(5.6516) | Xent 0.0000(0.0000) | Loss 9.8643(13.8304) | Error 0.0000(0.0000) Steps 748(800.03) | Grad Norm 401029.7201(676678.8570) | Total Time 0.00(0.00)\n",
      "Iter 1569 | Time 82.8528(88.3813) | Bit/dim 4.0545(5.6037) | Xent 0.0000(0.0000) | Loss 9.6495(13.7049) | Error 0.0000(0.0000) Steps 760(798.83) | Grad Norm 571.4276(656395.6341) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 80.4355(88.1429) | Bit/dim 4.0351(5.5566) | Xent 0.0000(0.0000) | Loss 9.3316(13.5737) | Error 0.0000(0.0000) Steps 736(796.95) | Grad Norm 1007170.4205(666918.8777) | Total Time 0.00(0.00)\n",
      "Iter 1571 | Time 78.7195(87.8602) | Bit/dim 4.0423(5.5112) | Xent 0.0000(0.0000) | Loss 9.6549(13.4562) | Error 0.0000(0.0000) Steps 736(795.12) | Grad Norm 72.3994(646913.4833) | Total Time 0.00(0.00)\n",
      "Iter 1572 | Time 76.5867(87.5220) | Bit/dim 4.0538(5.4675) | Xent 0.0000(0.0000) | Loss 9.4779(13.3368) | Error 0.0000(0.0000) Steps 712(792.63) | Grad Norm 596.1140(627523.9623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 26.2068, Epoch Time 525.5569(465.5419), Bit/dim 4.0436(best: 3.5208), Xent 0.0000, Loss 4.0436, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 79.1180(87.2699) | Bit/dim 4.0322(5.4244) | Xent 0.0000(0.0000) | Loss 13.7257(13.3485) | Error 0.0000(0.0000) Steps 748(791.29) | Grad Norm 2154.8158(608762.8879) | Total Time 0.00(0.00)\n",
      "Iter 1574 | Time 77.1381(86.9659) | Bit/dim 4.0225(5.3823) | Xent 0.0000(0.0000) | Loss 9.7195(13.2396) | Error 0.0000(0.0000) Steps 706(788.73) | Grad Norm 5986.0056(590679.5814) | Total Time 0.00(0.00)\n",
      "Iter 1575 | Time 72.6774(86.5373) | Bit/dim 4.0148(5.3413) | Xent 0.0000(0.0000) | Loss 9.5111(13.1278) | Error 0.0000(0.0000) Steps 736(787.15) | Grad Norm 9.4825(572959.4784) | Total Time 0.00(0.00)\n",
      "Iter 1576 | Time 69.2315(86.0181) | Bit/dim 3.8323(5.2960) | Xent 0.0000(0.0000) | Loss 9.2718(13.0121) | Error 0.0000(0.0000) Steps 718(785.07) | Grad Norm 3.9470(555770.8125) | Total Time 0.00(0.00)\n",
      "Iter 1577 | Time 66.8729(85.4437) | Bit/dim 3.7755(5.2504) | Xent 0.0000(0.0000) | Loss 9.0656(12.8937) | Error 0.0000(0.0000) Steps 694(782.34) | Grad Norm 4.3742(539097.8193) | Total Time 0.00(0.00)\n",
      "Iter 1578 | Time 68.3867(84.9320) | Bit/dim 3.8078(5.2071) | Xent 0.0000(0.0000) | Loss 8.9838(12.7764) | Error 0.0000(0.0000) Steps 688(779.51) | Grad Norm 5.1459(522925.0391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 25.8988, Epoch Time 475.3367(465.8358), Bit/dim 3.8021(best: 3.5208), Xent 0.0000, Loss 3.8021, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 67.5072(84.4093) | Bit/dim 3.7852(5.1645) | Xent 0.0000(0.0000) | Loss 13.3894(12.7948) | Error 0.0000(0.0000) Steps 712(777.48) | Grad Norm 4.4556(507237.4216) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 67.7554(83.9097) | Bit/dim 3.7572(5.1223) | Xent 0.0000(0.0000) | Loss 8.7955(12.6748) | Error 0.0000(0.0000) Steps 694(774.98) | Grad Norm 3.5406(492020.4052) | Total Time 0.00(0.00)\n",
      "Iter 1581 | Time 66.3658(83.3834) | Bit/dim 3.7132(5.0800) | Xent 0.0000(0.0000) | Loss 8.8264(12.5594) | Error 0.0000(0.0000) Steps 694(772.55) | Grad Norm 3.5186(477259.8986) | Total Time 0.00(0.00)\n",
      "Iter 1582 | Time 65.9512(82.8604) | Bit/dim 3.6990(5.0386) | Xent 0.0000(0.0000) | Loss 8.5244(12.4383) | Error 0.0000(0.0000) Steps 676(769.65) | Grad Norm 2.9736(462942.1909) | Total Time 0.00(0.00)\n",
      "Iter 1583 | Time 67.2551(82.3922) | Bit/dim 3.6871(4.9980) | Xent 0.0000(0.0000) | Loss 9.0534(12.3368) | Error 0.0000(0.0000) Steps 700(767.56) | Grad Norm 2.7815(449054.0086) | Total Time 0.00(0.00)\n",
      "Iter 1584 | Time 68.1333(81.9645) | Bit/dim 3.6982(4.9590) | Xent 0.0000(0.0000) | Loss 8.9213(12.2343) | Error 0.0000(0.0000) Steps 682(765.00) | Grad Norm 3.1024(435582.4814) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 25.1772, Epoch Time 444.0991(465.1837), Bit/dim 3.6762(best: 3.5208), Xent 0.0000, Loss 3.6762, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 63.5536(81.4121) | Bit/dim 3.6858(4.9208) | Xent 0.0000(0.0000) | Loss 13.0292(12.2581) | Error 0.0000(0.0000) Steps 658(761.79) | Grad Norm 3.2533(422515.1045) | Total Time 0.00(0.00)\n",
      "Iter 1586 | Time 66.1746(80.9550) | Bit/dim 3.6564(4.8829) | Xent 0.0000(0.0000) | Loss 8.8769(12.1567) | Error 0.0000(0.0000) Steps 676(759.21) | Grad Norm 2.6828(409839.7319) | Total Time 0.00(0.00)\n",
      "Iter 1587 | Time 64.1108(80.4497) | Bit/dim 3.6414(4.8457) | Xent 0.0000(0.0000) | Loss 8.9630(12.0609) | Error 0.0000(0.0000) Steps 694(757.26) | Grad Norm 2.1907(397544.6057) | Total Time 0.00(0.00)\n",
      "Iter 1588 | Time 61.3334(79.8762) | Bit/dim 3.6317(4.8092) | Xent 0.0000(0.0000) | Loss 8.7891(11.9627) | Error 0.0000(0.0000) Steps 664(754.46) | Grad Norm 2.5165(385618.3430) | Total Time 0.00(0.00)\n",
      "Iter 1589 | Time 64.9871(79.4295) | Bit/dim 3.6377(4.7741) | Xent 0.0000(0.0000) | Loss 8.6552(11.8635) | Error 0.0000(0.0000) Steps 634(750.85) | Grad Norm 2.5371(374049.8688) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 64.7938(78.9905) | Bit/dim 3.6305(4.7398) | Xent 0.0000(0.0000) | Loss 8.7591(11.7704) | Error 0.0000(0.0000) Steps 670(748.42) | Grad Norm 2.1349(362828.4368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 23.9263, Epoch Time 424.7422(463.9704), Bit/dim 3.6162(best: 3.5208), Xent 0.0000, Loss 3.6162, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 66.3704(78.6119) | Bit/dim 3.6017(4.7056) | Xent 0.0000(0.0000) | Loss 12.8225(11.8019) | Error 0.0000(0.0000) Steps 676(746.25) | Grad Norm 1.7629(351943.6366) | Total Time 0.00(0.00)\n",
      "Iter 1592 | Time 64.3220(78.1832) | Bit/dim 3.6181(4.6730) | Xent 0.0000(0.0000) | Loss 8.6642(11.7078) | Error 0.0000(0.0000) Steps 658(743.60) | Grad Norm 1.8276(341385.3823) | Total Time 0.00(0.00)\n",
      "Iter 1593 | Time 65.2977(77.7966) | Bit/dim 3.6108(4.6411) | Xent 0.0000(0.0000) | Loss 8.7545(11.6192) | Error 0.0000(0.0000) Steps 664(741.21) | Grad Norm 2.0454(331143.8822) | Total Time 0.00(0.00)\n",
      "Iter 1594 | Time 61.1119(77.2961) | Bit/dim 3.5974(4.6098) | Xent 0.0000(0.0000) | Loss 8.6089(11.5289) | Error 0.0000(0.0000) Steps 616(737.46) | Grad Norm 1.7728(321209.6189) | Total Time 0.00(0.00)\n",
      "Iter 1595 | Time 61.5447(76.8235) | Bit/dim 3.5950(4.5794) | Xent 0.0000(0.0000) | Loss 8.5660(11.4400) | Error 0.0000(0.0000) Steps 652(734.89) | Grad Norm 1.4457(311573.3737) | Total Time 0.00(0.00)\n",
      "Iter 1596 | Time 61.0993(76.3518) | Bit/dim 3.5932(4.5498) | Xent 0.0000(0.0000) | Loss 8.5790(11.3542) | Error 0.0000(0.0000) Steps 652(732.41) | Grad Norm 1.6550(302226.2222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 24.2035, Epoch Time 422.4702(462.7254), Bit/dim 3.5898(best: 3.5208), Xent 0.0000, Loss 3.5898, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 63.1989(75.9572) | Bit/dim 3.5908(4.5210) | Xent 0.0000(0.0000) | Loss 12.2629(11.3814) | Error 0.0000(0.0000) Steps 634(729.45) | Grad Norm 1.7769(293159.4888) | Total Time 0.00(0.00)\n",
      "Iter 1598 | Time 60.5832(75.4960) | Bit/dim 3.5882(4.4930) | Xent 0.0000(0.0000) | Loss 8.7186(11.3016) | Error 0.0000(0.0000) Steps 664(727.49) | Grad Norm 1.4915(284364.7489) | Total Time 0.00(0.00)\n",
      "Iter 1599 | Time 60.5901(75.0488) | Bit/dim 3.5745(4.4655) | Xent 0.0000(0.0000) | Loss 8.3928(11.2143) | Error 0.0000(0.0000) Steps 622(724.33) | Grad Norm 1.3516(275833.8470) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 62.5973(74.6753) | Bit/dim 3.5823(4.4390) | Xent 0.0000(0.0000) | Loss 8.7634(11.1408) | Error 0.0000(0.0000) Steps 676(722.88) | Grad Norm 1.3578(267558.8723) | Total Time 0.00(0.00)\n",
      "Iter 1601 | Time 61.7513(74.2875) | Bit/dim 3.5781(4.4132) | Xent 0.0000(0.0000) | Loss 8.6479(11.0660) | Error 0.0000(0.0000) Steps 670(721.29) | Grad Norm 1.2090(259532.1424) | Total Time 0.00(0.00)\n",
      "Iter 1602 | Time 60.4919(73.8737) | Bit/dim 3.5706(4.3879) | Xent 0.0000(0.0000) | Loss 8.2712(10.9821) | Error 0.0000(0.0000) Steps 628(718.49) | Grad Norm 1.0948(251746.2110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 24.7906, Epoch Time 410.2040(461.1498), Bit/dim 3.5734(best: 3.5208), Xent 0.0000, Loss 3.5734, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 61.0598(73.4893) | Bit/dim 3.5707(4.3634) | Xent 0.0000(0.0000) | Loss 12.6006(11.0307) | Error 0.0000(0.0000) Steps 646(716.32) | Grad Norm 1.1976(244193.8606) | Total Time 0.00(0.00)\n",
      "Iter 1604 | Time 65.9735(73.2638) | Bit/dim 3.5756(4.3397) | Xent 0.0000(0.0000) | Loss 8.5833(10.9573) | Error 0.0000(0.0000) Steps 664(714.75) | Grad Norm 1.2509(236868.0823) | Total Time 0.00(0.00)\n",
      "Iter 1605 | Time 63.2880(72.9645) | Bit/dim 3.5635(4.3165) | Xent 0.0000(0.0000) | Loss 8.4591(10.8823) | Error 0.0000(0.0000) Steps 652(712.86) | Grad Norm 1.1050(229762.0730) | Total Time 0.00(0.00)\n",
      "Iter 1606 | Time 60.5165(72.5911) | Bit/dim 3.5671(4.2940) | Xent 0.0000(0.0000) | Loss 8.5602(10.8127) | Error 0.0000(0.0000) Steps 652(711.04) | Grad Norm 0.8739(222869.2370) | Total Time 0.00(0.00)\n",
      "Iter 1607 | Time 55.8651(72.0893) | Bit/dim 3.5640(4.2721) | Xent 0.0000(0.0000) | Loss 8.2072(10.7345) | Error 0.0000(0.0000) Steps 604(707.83) | Grad Norm 0.8879(216183.1865) | Total Time 0.00(0.00)\n",
      "Iter 1608 | Time 63.2119(71.8230) | Bit/dim 3.5557(4.2506) | Xent 0.0000(0.0000) | Loss 8.2664(10.6604) | Error 0.0000(0.0000) Steps 646(705.97) | Grad Norm 0.9534(209697.7195) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 24.6196, Epoch Time 410.6993(459.6363), Bit/dim 3.5644(best: 3.5208), Xent 0.0000, Loss 3.5644, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 58.0625(71.4102) | Bit/dim 3.5453(4.2294) | Xent 0.0000(0.0000) | Loss 12.6658(10.7206) | Error 0.0000(0.0000) Steps 622(703.45) | Grad Norm 0.9743(203406.8172) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 61.6029(71.1159) | Bit/dim 3.5614(4.2094) | Xent 0.0000(0.0000) | Loss 8.6048(10.6571) | Error 0.0000(0.0000) Steps 640(701.55) | Grad Norm 0.8169(197304.6372) | Total Time 0.00(0.00)\n",
      "Iter 1611 | Time 67.4872(71.0071) | Bit/dim 3.5659(4.1901) | Xent 0.0000(0.0000) | Loss 8.5485(10.5939) | Error 0.0000(0.0000) Steps 634(699.52) | Grad Norm 0.7173(191385.5196) | Total Time 0.00(0.00)\n",
      "Iter 1612 | Time 61.7497(70.7294) | Bit/dim 3.5681(4.1714) | Xent 0.0000(0.0000) | Loss 8.4887(10.5307) | Error 0.0000(0.0000) Steps 634(697.56) | Grad Norm 0.7294(185643.9759) | Total Time 0.00(0.00)\n",
      "Iter 1613 | Time 70.0481(70.7089) | Bit/dim 3.5511(4.1528) | Xent 0.0000(0.0000) | Loss 8.4685(10.4689) | Error 0.0000(0.0000) Steps 688(697.27) | Grad Norm 0.7232(180074.6783) | Total Time 0.00(0.00)\n",
      "Iter 1614 | Time 66.2019(70.5737) | Bit/dim 3.5469(4.1346) | Xent 0.0000(0.0000) | Loss 8.3994(10.4068) | Error 0.0000(0.0000) Steps 646(695.73) | Grad Norm 0.7105(174672.4592) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 24.0499, Epoch Time 425.5380(458.6133), Bit/dim 3.5520(best: 3.5208), Xent 0.0000, Loss 3.5520, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 66.3590(70.4473) | Bit/dim 3.5543(4.1172) | Xent 0.0000(0.0000) | Loss 12.4207(10.4672) | Error 0.0000(0.0000) Steps 646(694.24) | Grad Norm 0.6348(169432.3045) | Total Time 0.00(0.00)\n",
      "Iter 1616 | Time 63.4972(70.2388) | Bit/dim 3.5426(4.1000) | Xent 0.0000(0.0000) | Loss 8.7102(10.4145) | Error 0.0000(0.0000) Steps 658(693.15) | Grad Norm 0.5853(164349.3529) | Total Time 0.00(0.00)\n",
      "Iter 1617 | Time 61.2084(69.9678) | Bit/dim 3.5523(4.0836) | Xent 0.0000(0.0000) | Loss 8.6746(10.3623) | Error 0.0000(0.0000) Steps 640(691.56) | Grad Norm 0.5576(159418.8891) | Total Time 0.00(0.00)\n",
      "Iter 1618 | Time 64.3963(69.8007) | Bit/dim 3.5507(4.0676) | Xent 0.0000(0.0000) | Loss 8.5404(10.3076) | Error 0.0000(0.0000) Steps 664(690.73) | Grad Norm 0.5992(154636.3404) | Total Time 0.00(0.00)\n",
      "Iter 1619 | Time 62.9563(69.5954) | Bit/dim 3.5421(4.0518) | Xent 0.0000(0.0000) | Loss 8.2960(10.2473) | Error 0.0000(0.0000) Steps 628(688.85) | Grad Norm 0.8862(149997.2767) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 61.5925(69.3553) | Bit/dim 3.5558(4.0369) | Xent 0.0000(0.0000) | Loss 8.5685(10.1969) | Error 0.0000(0.0000) Steps 628(687.02) | Grad Norm 0.6521(145497.3780) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 23.9916, Epoch Time 420.2427(457.4622), Bit/dim 3.5504(best: 3.5208), Xent 0.0000, Loss 3.5504, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 64.6251(69.2134) | Bit/dim 3.5468(4.0222) | Xent 0.0000(0.0000) | Loss 12.5709(10.2681) | Error 0.0000(0.0000) Steps 658(686.15) | Grad Norm 0.8411(141132.4819) | Total Time 0.00(0.00)\n",
      "Iter 1622 | Time 63.1178(69.0305) | Bit/dim 3.5577(4.0083) | Xent 0.0000(0.0000) | Loss 8.6683(10.2201) | Error 0.0000(0.0000) Steps 664(685.49) | Grad Norm 0.4897(136898.5221) | Total Time 0.00(0.00)\n",
      "Iter 1623 | Time 63.9127(68.8770) | Bit/dim 3.5421(3.9943) | Xent 0.0000(0.0000) | Loss 8.6485(10.1730) | Error 0.0000(0.0000) Steps 664(684.84) | Grad Norm 0.5128(132791.5819) | Total Time 0.00(0.00)\n",
      "Iter 1624 | Time 62.0062(68.6709) | Bit/dim 3.5463(3.9809) | Xent 0.0000(0.0000) | Loss 8.2833(10.1163) | Error 0.0000(0.0000) Steps 640(683.50) | Grad Norm 0.6752(128807.8547) | Total Time 0.00(0.00)\n",
      "Iter 1625 | Time 66.0942(68.5936) | Bit/dim 3.5473(3.9679) | Xent 0.0000(0.0000) | Loss 8.5511(10.0693) | Error 0.0000(0.0000) Steps 676(683.27) | Grad Norm 0.5517(124943.6356) | Total Time 0.00(0.00)\n",
      "Iter 1626 | Time 62.9924(68.4255) | Bit/dim 3.5452(3.9552) | Xent 0.0000(0.0000) | Loss 8.5700(10.0244) | Error 0.0000(0.0000) Steps 658(682.52) | Grad Norm 0.4234(121195.3392) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 24.1025, Epoch Time 422.9341(456.4264), Bit/dim 3.5466(best: 3.5208), Xent 0.0000, Loss 3.5466, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 66.0032(68.3528) | Bit/dim 3.5407(3.9427) | Xent 0.0000(0.0000) | Loss 12.3726(10.0948) | Error 0.0000(0.0000) Steps 640(681.24) | Grad Norm 0.5213(117559.4947) | Total Time 0.00(0.00)\n",
      "Iter 1628 | Time 65.2093(68.2585) | Bit/dim 3.5405(3.9307) | Xent 0.0000(0.0000) | Loss 8.5895(10.0496) | Error 0.0000(0.0000) Steps 658(680.54) | Grad Norm 0.5459(114032.7262) | Total Time 0.00(0.00)\n",
      "Iter 1629 | Time 65.3093(68.1701) | Bit/dim 3.5551(3.9194) | Xent 0.0000(0.0000) | Loss 8.6117(10.0065) | Error 0.0000(0.0000) Steps 670(680.23) | Grad Norm 0.4475(110611.7578) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 67.3378(68.1451) | Bit/dim 3.5383(3.9080) | Xent 0.0000(0.0000) | Loss 8.6140(9.9647) | Error 0.0000(0.0000) Steps 652(679.38) | Grad Norm 0.4266(107293.4179) | Total Time 0.00(0.00)\n",
      "Iter 1631 | Time 63.7647(68.0137) | Bit/dim 3.5474(3.8972) | Xent 0.0000(0.0000) | Loss 8.3158(9.9153) | Error 0.0000(0.0000) Steps 646(678.38) | Grad Norm 0.4559(104074.6290) | Total Time 0.00(0.00)\n",
      "Iter 1632 | Time 63.1871(67.8689) | Bit/dim 3.5357(3.8863) | Xent 0.0000(0.0000) | Loss 8.6865(9.8784) | Error 0.0000(0.0000) Steps 676(678.31) | Grad Norm 0.8874(100952.4168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 24.6677, Epoch Time 431.6135(455.6820), Bit/dim 3.5427(best: 3.5208), Xent 0.0000, Loss 3.5427, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 66.8966(67.8397) | Bit/dim 3.5409(3.8760) | Xent 0.0000(0.0000) | Loss 12.6905(9.9628) | Error 0.0000(0.0000) Steps 670(678.06) | Grad Norm 0.6184(97923.8628) | Total Time 0.00(0.00)\n",
      "Iter 1634 | Time 62.4840(67.6790) | Bit/dim 3.5379(3.8658) | Xent 0.0000(0.0000) | Loss 8.6220(9.9225) | Error 0.0000(0.0000) Steps 658(677.46) | Grad Norm 0.5278(94986.1628) | Total Time 0.00(0.00)\n",
      "Iter 1635 | Time 62.8335(67.5337) | Bit/dim 3.5367(3.8559) | Xent 0.0000(0.0000) | Loss 8.5736(9.8821) | Error 0.0000(0.0000) Steps 676(677.41) | Grad Norm 0.4876(92136.5925) | Total Time 0.00(0.00)\n",
      "Iter 1636 | Time 66.0811(67.4901) | Bit/dim 3.5430(3.8465) | Xent 0.0000(0.0000) | Loss 8.6032(9.8437) | Error 0.0000(0.0000) Steps 664(677.01) | Grad Norm 0.3246(89372.5045) | Total Time 0.00(0.00)\n",
      "Iter 1637 | Time 67.1757(67.4807) | Bit/dim 3.5399(3.8373) | Xent 0.0000(0.0000) | Loss 8.6429(9.8077) | Error 0.0000(0.0000) Steps 676(676.98) | Grad Norm 0.4147(86691.3418) | Total Time 0.00(0.00)\n",
      "Iter 1638 | Time 68.7299(67.5181) | Bit/dim 3.5468(3.8286) | Xent 0.0000(0.0000) | Loss 8.6946(9.7743) | Error 0.0000(0.0000) Steps 682(677.13) | Grad Norm 0.5611(84090.6184) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 24.6900, Epoch Time 435.1984(455.0675), Bit/dim 3.5332(best: 3.5208), Xent 0.0000, Loss 3.5332, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1639 | Time 63.6927(67.4034) | Bit/dim 3.5367(3.8199) | Xent 0.0000(0.0000) | Loss 12.7605(9.8639) | Error 0.0000(0.0000) Steps 646(676.20) | Grad Norm 0.7975(81567.9238) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 60.7591(67.2041) | Bit/dim 3.5258(3.8110) | Xent 0.0000(0.0000) | Loss 8.3860(9.8195) | Error 0.0000(0.0000) Steps 652(675.47) | Grad Norm 0.3736(79120.8973) | Total Time 0.00(0.00)\n",
      "Iter 1641 | Time 65.8932(67.1647) | Bit/dim 3.5447(3.8031) | Xent 0.0000(0.0000) | Loss 8.5601(9.7818) | Error 0.0000(0.0000) Steps 658(674.95) | Grad Norm 0.6534(76747.2899) | Total Time 0.00(0.00)\n",
      "Iter 1642 | Time 67.3370(67.1699) | Bit/dim 3.5433(3.7953) | Xent 0.0000(0.0000) | Loss 8.4649(9.7422) | Error 0.0000(0.0000) Steps 640(673.90) | Grad Norm 0.7286(74444.8931) | Total Time 0.00(0.00)\n",
      "Iter 1643 | Time 66.1088(67.1381) | Bit/dim 3.5319(3.7874) | Xent 0.0000(0.0000) | Loss 8.5352(9.7060) | Error 0.0000(0.0000) Steps 664(673.60) | Grad Norm 0.3659(72211.5573) | Total Time 0.00(0.00)\n",
      "Iter 1644 | Time 65.7650(67.0969) | Bit/dim 3.5377(3.7799) | Xent 0.0000(0.0000) | Loss 8.4717(9.6690) | Error 0.0000(0.0000) Steps 640(672.59) | Grad Norm 0.5864(70045.2282) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 24.6377, Epoch Time 430.2547(454.3231), Bit/dim 3.5425(best: 3.5208), Xent 0.0000, Loss 3.5425, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1645 | Time 62.6294(66.9629) | Bit/dim 3.5304(3.7724) | Xent 0.0000(0.0000) | Loss 12.5350(9.7550) | Error 0.0000(0.0000) Steps 676(672.70) | Grad Norm 0.8810(67943.8977) | Total Time 0.00(0.00)\n",
      "Iter 1646 | Time 66.5306(66.9499) | Bit/dim 3.5313(3.7652) | Xent 0.0000(0.0000) | Loss 8.2942(9.7112) | Error 0.0000(0.0000) Steps 652(672.07) | Grad Norm 0.4409(65905.5940) | Total Time 0.00(0.00)\n",
      "Iter 1647 | Time 65.6734(66.9116) | Bit/dim 3.5368(3.7583) | Xent 0.0000(0.0000) | Loss 8.6273(9.6786) | Error 0.0000(0.0000) Steps 652(671.47) | Grad Norm 0.6991(63928.4472) | Total Time 0.00(0.00)\n",
      "Iter 1648 | Time 61.2885(66.7429) | Bit/dim 3.5273(3.7514) | Xent 0.0000(0.0000) | Loss 8.4277(9.6411) | Error 0.0000(0.0000) Steps 658(671.07) | Grad Norm 0.4105(62010.6061) | Total Time 0.00(0.00)\n",
      "Iter 1649 | Time 63.8665(66.6566) | Bit/dim 3.5477(3.7453) | Xent 0.0000(0.0000) | Loss 8.4011(9.6039) | Error 0.0000(0.0000) Steps 646(670.32) | Grad Norm 0.7600(60150.3107) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 65.4477(66.6203) | Bit/dim 3.5345(3.7389) | Xent 0.0000(0.0000) | Loss 8.4585(9.5696) | Error 0.0000(0.0000) Steps 676(670.49) | Grad Norm 0.3967(58345.8133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 24.1030, Epoch Time 425.8203(453.4680), Bit/dim 3.5366(best: 3.5208), Xent 0.0000, Loss 3.5366, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1651 | Time 63.8664(66.5377) | Bit/dim 3.5306(3.7327) | Xent 0.0000(0.0000) | Loss 12.6185(9.6610) | Error 0.0000(0.0000) Steps 670(670.47) | Grad Norm 0.9813(56595.4683) | Total Time 0.00(0.00)\n",
      "Iter 1652 | Time 66.5330(66.5376) | Bit/dim 3.5265(3.7265) | Xent 0.0000(0.0000) | Loss 8.5331(9.6272) | Error 0.0000(0.0000) Steps 670(670.46) | Grad Norm 0.3038(54897.6134) | Total Time 0.00(0.00)\n",
      "Iter 1653 | Time 65.7043(66.5126) | Bit/dim 3.5305(3.7206) | Xent 0.0000(0.0000) | Loss 8.5347(9.5944) | Error 0.0000(0.0000) Steps 664(670.26) | Grad Norm 0.3764(53250.6963) | Total Time 0.00(0.00)\n",
      "Iter 1654 | Time 63.7996(66.4312) | Bit/dim 3.5364(3.7151) | Xent 0.0000(0.0000) | Loss 8.5963(9.5645) | Error 0.0000(0.0000) Steps 676(670.44) | Grad Norm 0.3070(51653.1846) | Total Time 0.00(0.00)\n",
      "Iter 1655 | Time 63.8558(66.3539) | Bit/dim 3.5379(3.7098) | Xent 0.0000(0.0000) | Loss 8.4773(9.5319) | Error 0.0000(0.0000) Steps 652(669.88) | Grad Norm 0.5315(50103.6050) | Total Time 0.00(0.00)\n",
      "Iter 1656 | Time 65.1898(66.3190) | Bit/dim 3.5465(3.7049) | Xent 0.0000(0.0000) | Loss 8.5973(9.5038) | Error 0.0000(0.0000) Steps 670(669.89) | Grad Norm 0.5684(48600.5139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 24.1672, Epoch Time 429.0815(452.7364), Bit/dim 3.5378(best: 3.5208), Xent 0.0000, Loss 3.5378, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1657 | Time 63.8113(66.2438) | Bit/dim 3.5318(3.6997) | Xent 0.0000(0.0000) | Loss 12.3161(9.5882) | Error 0.0000(0.0000) Steps 646(669.17) | Grad Norm 0.6684(47142.5185) | Total Time 0.00(0.00)\n",
      "Iter 1658 | Time 67.8436(66.2918) | Bit/dim 3.5370(3.6948) | Xent 0.0000(0.0000) | Loss 8.5466(9.5569) | Error 0.0000(0.0000) Steps 694(669.91) | Grad Norm 0.6607(45728.2628) | Total Time 0.00(0.00)\n",
      "Iter 1659 | Time 65.8135(66.2774) | Bit/dim 3.5293(3.6898) | Xent 0.0000(0.0000) | Loss 8.3820(9.5217) | Error 0.0000(0.0000) Steps 658(669.56) | Grad Norm 0.4727(44356.4291) | Total Time 0.00(0.00)\n",
      "Iter 1660 | Time 63.2537(66.1867) | Bit/dim 3.5280(3.6850) | Xent 0.0000(0.0000) | Loss 8.4751(9.4903) | Error 0.0000(0.0000) Steps 670(669.57) | Grad Norm 1.1046(43025.7694) | Total Time 0.00(0.00)\n",
      "Iter 1661 | Time 64.3762(66.1324) | Bit/dim 3.5415(3.6807) | Xent 0.0000(0.0000) | Loss 8.4157(9.4581) | Error 0.0000(0.0000) Steps 634(668.50) | Grad Norm 1.3033(41735.0354) | Total Time 0.00(0.00)\n",
      "Iter 1662 | Time 69.8554(66.2441) | Bit/dim 3.5441(3.6766) | Xent 0.0000(0.0000) | Loss 8.5889(9.4320) | Error 0.0000(0.0000) Steps 682(668.91) | Grad Norm 1.1296(40483.0182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 24.3803, Epoch Time 435.4459(452.2177), Bit/dim 3.5384(best: 3.5208), Xent 0.0000, Loss 3.5384, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1663 | Time 69.3466(66.3372) | Bit/dim 3.5324(3.6723) | Xent 0.0000(0.0000) | Loss 11.9054(9.5062) | Error 0.0000(0.0000) Steps 676(669.12) | Grad Norm 16.4326(39269.0206) | Total Time 0.00(0.00)\n",
      "Iter 1664 | Time 66.5969(66.3450) | Bit/dim 3.5258(3.6679) | Xent 0.0000(0.0000) | Loss 8.5331(9.4770) | Error 0.0000(0.0000) Steps 682(669.51) | Grad Norm 0.9068(38090.9772) | Total Time 0.00(0.00)\n",
      "Iter 1665 | Time 68.3616(66.4055) | Bit/dim 3.5548(3.6645) | Xent 0.0000(0.0000) | Loss 8.4411(9.4459) | Error 0.0000(0.0000) Steps 676(669.70) | Grad Norm 1.5348(36948.2940) | Total Time 0.00(0.00)\n",
      "Iter 1666 | Time 68.5054(66.4685) | Bit/dim 3.5559(3.6612) | Xent 0.0000(0.0000) | Loss 8.6231(9.4212) | Error 0.0000(0.0000) Steps 682(670.07) | Grad Norm 0.9139(35839.8726) | Total Time 0.00(0.00)\n",
      "Iter 1667 | Time 65.7625(66.4473) | Bit/dim 3.5302(3.6573) | Xent 0.0000(0.0000) | Loss 8.2657(9.3866) | Error 0.0000(0.0000) Steps 676(670.25) | Grad Norm 0.7650(34764.6993) | Total Time 0.00(0.00)\n",
      "Iter 1668 | Time 66.0989(66.4368) | Bit/dim 3.5400(3.6538) | Xent 0.0000(0.0000) | Loss 8.4121(9.3573) | Error 0.0000(0.0000) Steps 682(670.60) | Grad Norm 8.3422(33722.0086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 24.3147, Epoch Time 445.7949(452.0250), Bit/dim 3.5505(best: 3.5208), Xent 0.0000, Loss 3.5505, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1669 | Time 68.2584(66.4915) | Bit/dim 3.5490(3.6506) | Xent 0.0000(0.0000) | Loss 12.5909(9.4543) | Error 0.0000(0.0000) Steps 658(670.22) | Grad Norm 1.9351(32710.4064) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 67.7749(66.5300) | Bit/dim 3.5507(3.6476) | Xent 0.0000(0.0000) | Loss 8.5502(9.4272) | Error 0.0000(0.0000) Steps 664(670.04) | Grad Norm 1.3658(31729.1352) | Total Time 0.00(0.00)\n",
      "Iter 1671 | Time 68.5426(66.5903) | Bit/dim 3.5432(3.6445) | Xent 0.0000(0.0000) | Loss 8.6243(9.4031) | Error 0.0000(0.0000) Steps 682(670.40) | Grad Norm 1.7200(30777.3127) | Total Time 0.00(0.00)\n",
      "Iter 1672 | Time 74.5943(66.8305) | Bit/dim 3.5533(3.6418) | Xent 0.0000(0.0000) | Loss 8.4581(9.3748) | Error 0.0000(0.0000) Steps 688(670.92) | Grad Norm 1.0382(29854.0245) | Total Time 0.00(0.00)\n",
      "Iter 1673 | Time 69.1771(66.9009) | Bit/dim 3.5569(3.6392) | Xent 0.0000(0.0000) | Loss 8.5778(9.3509) | Error 0.0000(0.0000) Steps 694(671.62) | Grad Norm 1.0981(28958.4367) | Total Time 0.00(0.00)\n",
      "Iter 1674 | Time 76.2743(67.1821) | Bit/dim 3.5437(3.6364) | Xent 0.0000(0.0000) | Loss 8.5975(9.3283) | Error 0.0000(0.0000) Steps 670(671.57) | Grad Norm 49.6288(28091.1725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 24.8229, Epoch Time 465.4082(452.4265), Bit/dim 3.5484(best: 3.5208), Xent 0.0000, Loss 3.5484, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1675 | Time 75.5702(67.4337) | Bit/dim 3.5388(3.6334) | Xent 0.0000(0.0000) | Loss 12.5901(9.4261) | Error 0.0000(0.0000) Steps 700(672.42) | Grad Norm 123.3218(27252.1370) | Total Time 0.00(0.00)\n",
      "Iter 1676 | Time 73.7769(67.6240) | Bit/dim 3.5452(3.6308) | Xent 0.0000(0.0000) | Loss 8.5783(9.4007) | Error 0.0000(0.0000) Steps 676(672.53) | Grad Norm 1414.5026(26477.0079) | Total Time 0.00(0.00)\n",
      "Iter 1677 | Time 70.6369(67.7144) | Bit/dim 3.5370(3.6280) | Xent 0.0000(0.0000) | Loss 8.5739(9.3759) | Error 0.0000(0.0000) Steps 694(673.17) | Grad Norm 2.1143(25682.7611) | Total Time 0.00(0.00)\n",
      "Iter 1678 | Time 83.0650(68.1749) | Bit/dim 3.5508(3.6256) | Xent 0.0000(0.0000) | Loss 8.5043(9.3497) | Error 0.0000(0.0000) Steps 694(673.80) | Grad Norm 546.2808(24928.6667) | Total Time 0.00(0.00)\n",
      "Iter 1679 | Time 75.9757(68.4089) | Bit/dim 3.5424(3.6232) | Xent 0.0000(0.0000) | Loss 8.4704(9.3233) | Error 0.0000(0.0000) Steps 676(673.86) | Grad Norm 5414.4041(24343.2388) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 76.4061(68.6489) | Bit/dim 3.5489(3.6209) | Xent 0.0000(0.0000) | Loss 8.6532(9.3032) | Error 0.0000(0.0000) Steps 688(674.29) | Grad Norm 31995.1676(24572.7967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 25.5391, Epoch Time 497.3778(453.7750), Bit/dim 3.5498(best: 3.5208), Xent 0.0000, Loss 3.5498, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1681 | Time 74.8816(68.8358) | Bit/dim 3.5524(3.6189) | Xent 0.0000(0.0000) | Loss 12.7665(9.4071) | Error 0.0000(0.0000) Steps 688(674.70) | Grad Norm 422187.2506(36501.2303) | Total Time 0.00(0.00)\n",
      "Iter 1682 | Time 76.6688(69.0708) | Bit/dim 3.5475(3.6167) | Xent 0.0000(0.0000) | Loss 8.5470(9.3813) | Error 0.0000(0.0000) Steps 688(675.10) | Grad Norm 17410.8012(35928.5174) | Total Time 0.00(0.00)\n",
      "Iter 1683 | Time 68.7041(69.0598) | Bit/dim 3.5485(3.6147) | Xent 0.0000(0.0000) | Loss 8.3538(9.3505) | Error 0.0000(0.0000) Steps 652(674.40) | Grad Norm 1078.0548(34883.0035) | Total Time 0.00(0.00)\n",
      "Iter 1684 | Time 73.9300(69.2059) | Bit/dim 3.5606(3.6131) | Xent 0.0000(0.0000) | Loss 8.6683(9.3300) | Error 0.0000(0.0000) Steps 712(675.53) | Grad Norm 1082.9181(33869.0010) | Total Time 0.00(0.00)\n",
      "Iter 1685 | Time 72.9978(69.3197) | Bit/dim 3.5634(3.6116) | Xent 0.0000(0.0000) | Loss 8.4911(9.3049) | Error 0.0000(0.0000) Steps 688(675.91) | Grad Norm 10482.9310(33167.4189) | Total Time 0.00(0.00)\n",
      "Iter 1686 | Time 70.2192(69.3467) | Bit/dim 3.5660(3.6102) | Xent 0.0000(0.0000) | Loss 8.5446(9.2821) | Error 0.0000(0.0000) Steps 658(675.37) | Grad Norm 722.1375(32194.0604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 24.8948, Epoch Time 478.3136(454.5112), Bit/dim 3.5740(best: 3.5208), Xent 0.0000, Loss 3.5740, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1687 | Time 68.2675(69.3143) | Bit/dim 3.5823(3.6094) | Xent 0.0000(0.0000) | Loss 12.6864(9.3842) | Error 0.0000(0.0000) Steps 664(675.03) | Grad Norm 5.2913(31228.3974) | Total Time 0.00(0.00)\n",
      "Iter 1688 | Time 73.1186(69.4284) | Bit/dim 3.5429(3.6074) | Xent 0.0000(0.0000) | Loss 8.6505(9.3622) | Error 0.0000(0.0000) Steps 682(675.24) | Grad Norm 51.0562(30293.0771) | Total Time 0.00(0.00)\n",
      "Iter 1689 | Time 76.2392(69.6328) | Bit/dim 3.5634(3.6061) | Xent 0.0000(0.0000) | Loss 8.4830(9.3358) | Error 0.0000(0.0000) Steps 706(676.16) | Grad Norm 188.7447(29389.9472) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 74.7111(69.7851) | Bit/dim 3.5644(3.6048) | Xent 0.0000(0.0000) | Loss 8.4965(9.3106) | Error 0.0000(0.0000) Steps 700(676.88) | Grad Norm 105936.8786(31686.3551) | Total Time 0.00(0.00)\n",
      "Iter 1691 | Time 81.5299(70.1374) | Bit/dim 3.5863(3.6042) | Xent 0.0000(0.0000) | Loss 8.6918(9.2921) | Error 0.0000(0.0000) Steps 712(677.93) | Grad Norm 2096.8082(30798.6687) | Total Time 0.00(0.00)\n",
      "Iter 1692 | Time 80.5218(70.4490) | Bit/dim 3.6282(3.6050) | Xent 0.0000(0.0000) | Loss 8.5275(9.2691) | Error 0.0000(0.0000) Steps 694(678.41) | Grad Norm 58360.3704(31625.5197) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 25.9568, Epoch Time 496.2702(455.7640), Bit/dim 3.6937(best: 3.5208), Xent 0.0000, Loss 3.6937, Error 1.0000(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_cifar10_bs8K_rl_stdlearnscale_30_run3 --resume ../experiments_published/cnf_cifar10_bs8K_rl_stdlearnscale_30_run3/epoch_203_checkpt.pth --seed 3 --conditional False --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 10.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
