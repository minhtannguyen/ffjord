{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=5400, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run3', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "validating...\n",
      "Epoch 0001 | Time 187.9942, Epoch Time 620.6253(620.6253), Bit/dim 8.7712(best: inf), Xent 2.2766, Loss 9.9095, Error 0.7464(best: inf)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0010 | Time 41.9307(76.5914) | Bit/dim 8.8277(9.0043) | Xent 2.2775(2.2998) | Loss 44.8988(23.0094) | Error 0.7635(0.8602) Steps 514(482.29) | Grad Norm 20.3514(29.8843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 183.2824, Epoch Time 539.5087(618.1918), Bit/dim 8.5304(best: 8.7712), Xent 2.2243, Loss 9.6425, Error 0.7424(best: 0.7464)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0020 | Time 36.9104(66.2092) | Bit/dim 8.5094(8.9023) | Xent 2.2207(2.2856) | Loss 21.2798(23.2544) | Error 0.7480(0.8313) Steps 508(486.52) | Grad Norm 9.1642(25.3044) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 180.2507, Epoch Time 533.5622(615.6530), Bit/dim 8.3585(best: 8.5304), Xent 2.1699, Loss 9.4434, Error 0.7359(best: 0.7424)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0030 | Time 37.1273(58.7383) | Bit/dim 8.3437(8.7738) | Xent 2.1625(2.2596) | Loss 20.9977(23.2699) | Error 0.7417(0.8088) Steps 520(491.35) | Grad Norm 7.2593(20.9106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 183.3665, Epoch Time 538.2798(613.3318), Bit/dim 8.1737(best: 8.3585), Xent 2.1240, Loss 9.2357, Error 0.7187(best: 0.7359)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0040 | Time 38.3371(53.2407) | Bit/dim 8.1072(8.6239) | Xent 2.1181(2.2269) | Loss 20.3016(23.1567) | Error 0.7265(0.7877) Steps 508(497.35) | Grad Norm 5.7533(16.9788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 182.4146, Epoch Time 544.5448(611.2682), Bit/dim 7.9739(best: 8.1737), Xent 2.0889, Loss 9.0183, Error 0.7056(best: 0.7187)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0050 | Time 38.2710(49.1907) | Bit/dim 7.8784(8.4557) | Xent 2.0945(2.1936) | Loss 19.9430(22.9352) | Error 0.7102(0.7684) Steps 514(499.11) | Grad Norm 5.4546(13.9568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 183.9778, Epoch Time 534.0525(608.9517), Bit/dim 7.7299(best: 7.9739), Xent 2.0635, Loss 8.7617, Error 0.6860(best: 0.7056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0060 | Time 40.6703(46.3220) | Bit/dim 7.5857(8.2608) | Xent 2.0650(2.1606) | Loss 19.3936(22.5870) | Error 0.6926(0.7486) Steps 514(503.58) | Grad Norm 4.7810(11.6126) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 185.1409, Epoch Time 547.5047(607.1083), Bit/dim 7.4701(best: 7.7299), Xent 2.0510, Loss 8.4956, Error 0.6660(best: 0.6860)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0070 | Time 38.3416(44.3097) | Bit/dim 7.3206(8.0420) | Xent 2.0519(2.1337) | Loss 18.7263(22.1621) | Error 0.6750(0.7299) Steps 538(508.20) | Grad Norm 3.6087(9.6559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 183.8809, Epoch Time 549.1069(605.3682), Bit/dim 7.2594(best: 7.4701), Xent 2.0551, Loss 8.2869, Error 0.6657(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0080 | Time 39.2638(42.8171) | Bit/dim 7.1547(7.8251) | Xent 2.0734(2.1147) | Loss 18.4813(21.7312) | Error 0.6874(0.7156) Steps 562(512.82) | Grad Norm 2.5527(7.8863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 188.9926, Epoch Time 552.0354(603.7682), Bit/dim 7.1330(best: 7.2594), Xent 2.0639, Loss 8.1650, Error 0.6765(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0090 | Time 39.8608(41.9859) | Bit/dim 7.0661(7.6349) | Xent 2.0694(2.1029) | Loss 18.2872(21.3456) | Error 0.6978(0.7097) Steps 520(517.17) | Grad Norm 1.9926(6.3521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 188.1902, Epoch Time 562.1126(602.5186), Bit/dim 7.0674(best: 7.1330), Xent 2.0601, Loss 8.0974, Error 0.6849(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0011 | Time 191.1057, Epoch Time 558.9265(601.2108), Bit/dim 7.0296(best: 7.0674), Xent 2.0457, Loss 8.0525, Error 0.6830(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0100 | Time 38.2878(41.2002) | Bit/dim 7.0240(7.4797) | Xent 2.0565(2.0918) | Loss 40.6829(21.6730) | Error 0.6956(0.7063) Steps 520(521.62) | Grad Norm 2.1299(5.1125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 189.9975, Epoch Time 561.2804(600.0129), Bit/dim 6.9991(best: 7.0296), Xent 2.0315, Loss 8.0149, Error 0.6795(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0110 | Time 38.9111(40.7045) | Bit/dim 6.9950(7.3558) | Xent 2.0473(2.0800) | Loss 18.0887(21.3742) | Error 0.6843(0.7028) Steps 538(523.49) | Grad Norm 2.1045(4.2042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 190.7041, Epoch Time 562.1349(598.8766), Bit/dim 6.9685(best: 6.9991), Xent 2.0203, Loss 7.9787, Error 0.6713(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0120 | Time 39.5666(40.4578) | Bit/dim 6.9594(7.2558) | Xent 2.0201(2.0672) | Loss 17.8257(21.1047) | Error 0.6785(0.6982) Steps 538(525.02) | Grad Norm 1.9099(3.5786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 192.4446, Epoch Time 565.9624(597.8891), Bit/dim 6.9332(best: 6.9685), Xent 2.0129, Loss 7.9396, Error 0.6757(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0130 | Time 39.9073(40.2854) | Bit/dim 6.9230(7.1714) | Xent 2.0159(2.0554) | Loss 17.7345(20.8645) | Error 0.6822(0.6938) Steps 520(524.79) | Grad Norm 2.1210(3.1196) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 191.5822, Epoch Time 564.5104(596.8878), Bit/dim 6.8904(best: 6.9332), Xent 2.0038, Loss 7.8923, Error 0.6641(best: 0.6657)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0140 | Time 40.3152(40.1523) | Bit/dim 6.8669(7.0969) | Xent 2.0041(2.0444) | Loss 17.7391(20.6447) | Error 0.6754(0.6894) Steps 550(527.46) | Grad Norm 4.8062(3.6686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 193.4074, Epoch Time 573.4991(596.1861), Bit/dim 6.8391(best: 6.8904), Xent 2.0132, Loss 7.8457, Error 0.6786(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0150 | Time 41.2473(40.1627) | Bit/dim 6.8058(7.0269) | Xent 2.0060(2.0361) | Loss 17.4650(20.4145) | Error 0.6728(0.6878) Steps 538(529.58) | Grad Norm 7.4159(6.3142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 191.0781, Epoch Time 572.2070(595.4667), Bit/dim 6.7704(best: 6.8391), Xent 1.9840, Loss 7.7624, Error 0.6596(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0160 | Time 42.2796(40.5310) | Bit/dim 6.7196(6.9554) | Xent 2.0365(2.0298) | Loss 17.3866(20.2080) | Error 0.7200(0.6878) Steps 568(532.57) | Grad Norm 37.2649(10.2786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 191.2947, Epoch Time 577.6502(594.9322), Bit/dim 6.6913(best: 6.7704), Xent 2.0047, Loss 7.6937, Error 0.6971(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0170 | Time 43.0163(40.6971) | Bit/dim 6.5975(6.8762) | Xent 1.9944(2.0211) | Loss 17.1031(19.9775) | Error 0.6869(0.6856) Steps 544(533.88) | Grad Norm 33.3339(12.9831) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 192.2980, Epoch Time 579.4563(594.4680), Bit/dim 6.5807(best: 6.6913), Xent 1.9960, Loss 7.5787, Error 0.6875(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0180 | Time 42.5371(40.9754) | Bit/dim 6.4556(6.7844) | Xent 2.0886(2.0270) | Loss 16.8525(19.7330) | Error 0.7635(0.6950) Steps 556(537.99) | Grad Norm 59.2537(22.5441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 192.2851, Epoch Time 584.6647(594.1739), Bit/dim 6.4597(best: 6.5807), Xent 2.0377, Loss 7.4786, Error 0.7178(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0021 | Time 195.1057, Epoch Time 588.1045(593.9918), Bit/dim 6.3139(best: 6.4597), Xent 2.0076, Loss 7.3177, Error 0.6891(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0190 | Time 39.8165(41.1517) | Bit/dim 6.3096(6.6776) | Xent 2.0203(2.0306) | Loss 39.2169(20.1277) | Error 0.7020(0.7014) Steps 544(540.28) | Grad Norm 55.7196(30.0920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 194.8399, Epoch Time 581.7101(593.6233), Bit/dim 6.1268(best: 6.3139), Xent 1.9896, Loss 7.1216, Error 0.6772(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0200 | Time 41.9333(41.1875) | Bit/dim 6.1316(6.5514) | Xent 2.0219(2.0326) | Loss 16.1170(19.7781) | Error 0.7115(0.7055) Steps 508(539.82) | Grad Norm 65.1474(37.8580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 195.4288, Epoch Time 587.5619(593.4415), Bit/dim 6.0005(best: 6.1268), Xent 2.0223, Loss 7.0116, Error 0.7030(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0210 | Time 40.1618(41.3166) | Bit/dim 5.9652(6.4191) | Xent 2.0347(2.0328) | Loss 15.8375(19.4469) | Error 0.7230(0.7099) Steps 562(540.07) | Grad Norm 53.2248(46.1176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 195.0837, Epoch Time 581.6723(593.0884), Bit/dim 5.8576(best: 6.0005), Xent 1.9900, Loss 6.8526, Error 0.6775(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0220 | Time 42.3037(41.3717) | Bit/dim 5.8323(6.2785) | Xent 1.9952(2.0233) | Loss 15.5009(19.0837) | Error 0.6993(0.7026) Steps 574(542.52) | Grad Norm 55.5577(46.3959) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 198.6226, Epoch Time 585.6033(592.8639), Bit/dim 5.7892(best: 5.8576), Xent 1.9744, Loss 6.7764, Error 0.6459(best: 0.6596)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0230 | Time 40.7392(41.3304) | Bit/dim 5.7562(6.1552) | Xent 1.9968(2.0266) | Loss 15.4788(18.7575) | Error 0.6719(0.7052) Steps 574(545.58) | Grad Norm 38.8037(53.5959) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 198.6310, Epoch Time 584.5662(592.6149), Bit/dim 5.7268(best: 5.7892), Xent 2.0156, Loss 6.7346, Error 0.7055(best: 0.6459)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0240 | Time 42.9524(41.0874) | Bit/dim 5.6974(6.0412) | Xent 1.9975(2.0208) | Loss 15.2353(18.4443) | Error 0.6744(0.7009) Steps 574(545.37) | Grad Norm 36.9192(48.7116) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 198.1916, Epoch Time 577.6509(592.1660), Bit/dim 5.6661(best: 5.7268), Xent 1.9626, Loss 6.6474, Error 0.6483(best: 0.6459)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0250 | Time 41.3334(41.1912) | Bit/dim 5.6805(5.9439) | Xent 2.0056(2.0099) | Loss 15.1229(18.1486) | Error 0.7094(0.6953) Steps 562(545.53) | Grad Norm 81.0964(49.2863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 195.2899, Epoch Time 586.0065(591.9812), Bit/dim 5.6400(best: 5.6661), Xent 1.9484, Loss 6.6143, Error 0.6590(best: 0.6459)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0260 | Time 42.3043(41.3253) | Bit/dim 5.6012(5.8600) | Xent 1.9486(1.9968) | Loss 14.9804(17.9114) | Error 0.6506(0.6877) Steps 562(545.71) | Grad Norm 24.6923(45.2694) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 199.0756, Epoch Time 591.4559(591.9655), Bit/dim 5.6020(best: 5.6400), Xent 1.9298, Loss 6.5669, Error 0.6425(best: 0.6459)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0270 | Time 40.9055(41.7022) | Bit/dim 5.5797(5.7905) | Xent 1.9262(1.9803) | Loss 14.9482(17.6855) | Error 0.6557(0.6799) Steps 562(548.61) | Grad Norm 21.2253(40.7706) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 199.1320, Epoch Time 600.5566(592.2232), Bit/dim 5.5790(best: 5.6020), Xent 1.9098, Loss 6.5339, Error 0.6339(best: 0.6425)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0031 | Time 200.4762, Epoch Time 595.7890(592.3302), Bit/dim 5.5519(best: 5.5790), Xent 1.9094, Loss 6.5066, Error 0.6419(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0280 | Time 43.6638(41.8735) | Bit/dim 5.5457(5.7312) | Xent 1.9298(1.9672) | Loss 38.8343(18.2053) | Error 0.6607(0.6747) Steps 550(550.53) | Grad Norm 32.1599(40.1434) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 200.0687, Epoch Time 597.3345(592.4803), Bit/dim 5.5322(best: 5.5519), Xent 1.9099, Loss 6.4871, Error 0.6513(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0290 | Time 42.6888(41.9718) | Bit/dim 5.5381(5.6810) | Xent 1.9147(1.9534) | Loss 14.8419(18.0113) | Error 0.6743(0.6699) Steps 550(552.64) | Grad Norm 75.3765(39.1023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 199.1744, Epoch Time 598.8558(592.6716), Bit/dim 5.5027(best: 5.5322), Xent 1.9627, Loss 6.4841, Error 0.6960(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0300 | Time 40.4807(41.9959) | Bit/dim 5.4939(5.6435) | Xent 1.9432(1.9629) | Loss 14.6583(17.8559) | Error 0.6796(0.6785) Steps 532(555.80) | Grad Norm 38.6232(47.4010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 201.8478, Epoch Time 595.3682(592.7525), Bit/dim 5.4878(best: 5.5027), Xent 1.9415, Loss 6.4585, Error 0.6603(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0310 | Time 43.7954(42.0692) | Bit/dim 5.4647(5.6013) | Xent 1.9301(1.9630) | Loss 14.6825(17.7031) | Error 0.6602(0.6788) Steps 556(554.11) | Grad Norm 27.8966(44.4862) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 203.7719, Epoch Time 596.8409(592.8751), Bit/dim 5.4284(best: 5.4878), Xent 1.9078, Loss 6.3823, Error 0.6495(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0320 | Time 41.6607(42.0802) | Bit/dim 5.4127(5.5574) | Xent 1.8971(1.9519) | Loss 14.5437(17.5401) | Error 0.6426(0.6730) Steps 574(557.00) | Grad Norm 15.0430(37.6414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 204.7214, Epoch Time 601.7706(593.1420), Bit/dim 5.4029(best: 5.4284), Xent 1.8824, Loss 6.3442, Error 0.6294(best: 0.6339)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0330 | Time 44.9099(42.4209) | Bit/dim 5.3757(5.5139) | Xent 1.8796(1.9364) | Loss 14.4271(17.3864) | Error 0.6615(0.6664) Steps 562(561.17) | Grad Norm 23.3445(32.6360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 209.0341, Epoch Time 618.9170(593.9152), Bit/dim 5.3577(best: 5.4029), Xent 1.8694, Loss 6.2925, Error 0.6416(best: 0.6294)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0340 | Time 45.2066(42.7120) | Bit/dim 5.3276(5.4726) | Xent 1.8676(1.9208) | Loss 14.3389(17.2160) | Error 0.6457(0.6601) Steps 562(565.23) | Grad Norm 18.3351(27.9926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 206.8312, Epoch Time 615.8397(594.5730), Bit/dim 5.3283(best: 5.3577), Xent 1.8443, Loss 6.2505, Error 0.6212(best: 0.6294)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0350 | Time 42.2047(42.9635) | Bit/dim 5.3316(5.4381) | Xent 1.8874(1.9137) | Loss 14.4581(17.0685) | Error 0.6641(0.6598) Steps 568(568.27) | Grad Norm 49.3242(32.1101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 209.5609, Epoch Time 617.7530(595.2684), Bit/dim 5.3122(best: 5.3283), Xent 1.8528, Loss 6.2386, Error 0.6325(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0360 | Time 44.4814(43.1213) | Bit/dim 5.2936(5.4019) | Xent 1.8828(1.9046) | Loss 14.3872(16.9261) | Error 0.6530(0.6570) Steps 586(571.97) | Grad Norm 17.5424(30.4096) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 210.6834, Epoch Time 617.8296(595.9452), Bit/dim 5.2762(best: 5.3122), Xent 1.8527, Loss 6.2026, Error 0.6283(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0041 | Time 205.8802, Epoch Time 617.9145(596.6043), Bit/dim 5.2808(best: 5.2762), Xent 2.0416, Loss 6.3016, Error 0.7271(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0370 | Time 43.2203(43.3454) | Bit/dim 5.2844(5.3690) | Xent 2.0656(1.9052) | Loss 39.4660(17.5303) | Error 0.7324(0.6587) Steps 604(574.16) | Grad Norm 59.7423(31.0659) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 205.4278, Epoch Time 616.1602(597.1910), Bit/dim 5.2910(best: 5.2762), Xent 1.9711, Loss 6.2766, Error 0.6803(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0380 | Time 42.1786(43.3943) | Bit/dim 5.2346(5.3498) | Xent 1.9645(1.9287) | Loss 14.0055(17.4215) | Error 0.6819(0.6702) Steps 556(575.30) | Grad Norm 19.4009(35.2620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 208.2497, Epoch Time 615.9448(597.7536), Bit/dim 5.2388(best: 5.2762), Xent 1.8790, Loss 6.1783, Error 0.6362(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0390 | Time 44.7183(43.5747) | Bit/dim 5.2067(5.3229) | Xent 1.9432(1.9257) | Loss 14.1364(17.2697) | Error 0.6769(0.6678) Steps 568(575.01) | Grad Norm 50.2399(33.2324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 204.1228, Epoch Time 614.8732(598.2672), Bit/dim 5.1977(best: 5.2388), Xent 1.8749, Loss 6.1352, Error 0.6322(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0400 | Time 42.9284(43.6837) | Bit/dim 5.1676(5.2928) | Xent 1.8644(1.9191) | Loss 13.9938(17.1057) | Error 0.6430(0.6658) Steps 580(575.05) | Grad Norm 9.0962(32.8705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 205.8580, Epoch Time 626.1229(599.1028), Bit/dim 5.1719(best: 5.1977), Xent 1.8664, Loss 6.1051, Error 0.6456(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0410 | Time 45.4076(43.9733) | Bit/dim 5.1623(5.2643) | Xent 1.8539(1.9092) | Loss 14.0514(16.9552) | Error 0.6396(0.6614) Steps 610(578.84) | Grad Norm 5.8254(28.8886) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 205.1605, Epoch Time 620.9313(599.7577), Bit/dim 5.1444(best: 5.1719), Xent 1.8376, Loss 6.0631, Error 0.6279(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0420 | Time 42.6532(44.0666) | Bit/dim 5.1437(5.2327) | Xent 1.8656(1.8960) | Loss 13.8651(16.7726) | Error 0.6419(0.6553) Steps 586(578.26) | Grad Norm 33.7151(25.6610) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 205.0908, Epoch Time 616.8672(600.2710), Bit/dim 5.1223(best: 5.1444), Xent 1.8467, Loss 6.0456, Error 0.6421(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0430 | Time 44.3002(44.1614) | Bit/dim 5.0841(5.2034) | Xent 1.8455(1.8862) | Loss 13.6744(16.6161) | Error 0.6480(0.6523) Steps 550(577.23) | Grad Norm 15.4255(25.3865) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 207.5537, Epoch Time 626.7272(601.0647), Bit/dim 5.0973(best: 5.1223), Xent 1.8291, Loss 6.0119, Error 0.6266(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0440 | Time 45.5233(44.3706) | Bit/dim 5.0951(5.1734) | Xent 1.8521(1.8750) | Loss 13.8453(16.4535) | Error 0.6498(0.6494) Steps 574(574.35) | Grad Norm 19.6843(24.8415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 209.3378, Epoch Time 630.3058(601.9419), Bit/dim 5.0641(best: 5.0973), Xent 1.8074, Loss 5.9678, Error 0.6168(best: 0.6212)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0450 | Time 46.5274(44.6774) | Bit/dim 5.0851(5.1509) | Xent 2.0844(1.8918) | Loss 13.9700(16.3500) | Error 0.7381(0.6563) Steps 580(575.26) | Grad Norm 88.4996(31.5538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 208.3436, Epoch Time 633.6614(602.8935), Bit/dim 5.1938(best: 5.0641), Xent 2.0498, Loss 6.2187, Error 0.7210(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0051 | Time 210.2792, Epoch Time 638.0188(603.9472), Bit/dim 5.0518(best: 5.0641), Xent 1.9207, Loss 6.0122, Error 0.6679(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0460 | Time 47.7013(45.0293) | Bit/dim 5.0461(5.1321) | Xent 1.9525(1.9088) | Loss 38.2030(16.9950) | Error 0.6952(0.6647) Steps 598(575.54) | Grad Norm 24.1012(31.8575) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 209.1176, Epoch Time 638.9635(604.9977), Bit/dim 4.9883(best: 5.0518), Xent 1.8891, Loss 5.9329, Error 0.6430(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0470 | Time 46.2355(45.2405) | Bit/dim 4.9818(5.1004) | Xent 1.9087(1.9122) | Loss 13.7219(16.8430) | Error 0.6720(0.6669) Steps 574(577.40) | Grad Norm 8.4489(26.1876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 208.9220, Epoch Time 640.9557(606.0765), Bit/dim 4.9601(best: 4.9883), Xent 1.8531, Loss 5.8867, Error 0.6333(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0480 | Time 46.3194(45.5435) | Bit/dim 4.9628(5.0669) | Xent 1.8747(1.9048) | Loss 13.5956(16.6738) | Error 0.6474(0.6634) Steps 604(580.48) | Grad Norm 17.1356(21.5839) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 212.1552, Epoch Time 645.6349(607.2632), Bit/dim 4.9363(best: 4.9601), Xent 1.8701, Loss 5.8713, Error 0.6588(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0490 | Time 45.9610(45.7327) | Bit/dim 4.9634(5.0354) | Xent 1.8619(1.8984) | Loss 13.4938(16.5147) | Error 0.6439(0.6623) Steps 580(582.83) | Grad Norm 41.3641(24.0561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 214.3583, Epoch Time 653.6539(608.6549), Bit/dim 4.9393(best: 4.9363), Xent 1.8363, Loss 5.8575, Error 0.6281(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0500 | Time 47.8996(46.2212) | Bit/dim 4.9143(5.0057) | Xent 1.8466(1.8877) | Loss 13.3499(16.3564) | Error 0.6552(0.6593) Steps 592(585.50) | Grad Norm 47.5630(26.6257) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 215.3488, Epoch Time 656.7233(610.0970), Bit/dim 4.9930(best: 4.9363), Xent 1.9910, Loss 5.9885, Error 0.6969(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0510 | Time 48.5317(46.5309) | Bit/dim 4.9241(4.9898) | Xent 1.9767(1.9043) | Loss 13.6146(16.2625) | Error 0.7119(0.6668) Steps 616(591.05) | Grad Norm 38.9850(30.7402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 217.6373, Epoch Time 662.5502(611.6706), Bit/dim 4.9102(best: 4.9363), Xent 1.9065, Loss 5.8634, Error 0.6698(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0520 | Time 48.7247(46.9063) | Bit/dim 4.8818(4.9660) | Xent 1.8885(1.9033) | Loss 13.4428(16.1647) | Error 0.6663(0.6681) Steps 556(591.26) | Grad Norm 31.8485(29.7895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 216.4649, Epoch Time 663.0981(613.2134), Bit/dim 4.8917(best: 4.9102), Xent 1.8525, Loss 5.8180, Error 0.6407(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0530 | Time 47.2127(47.1051) | Bit/dim 4.8705(4.9443) | Xent 1.8477(1.8929) | Loss 13.4406(16.0529) | Error 0.6459(0.6637) Steps 580(593.97) | Grad Norm 27.7147(29.9130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 215.5037, Epoch Time 658.7027(614.5781), Bit/dim 4.8561(best: 4.8917), Xent 1.8125, Loss 5.7623, Error 0.6303(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0540 | Time 48.0544(47.3968) | Bit/dim 4.8349(4.9168) | Xent 1.7859(1.8710) | Loss 13.2177(15.9225) | Error 0.6281(0.6560) Steps 634(597.07) | Grad Norm 19.3205(26.6106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 217.0365, Epoch Time 667.6476(616.1702), Bit/dim 4.8200(best: 4.8561), Xent 1.7593, Loss 5.6997, Error 0.6115(best: 0.6168)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0061 | Time 216.9595, Epoch Time 669.4612(617.7689), Bit/dim 4.7998(best: 4.8200), Xent 1.7249, Loss 5.6622, Error 0.5934(best: 0.6115)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0550 | Time 49.0108(47.6981) | Bit/dim 4.8079(4.8888) | Xent 1.7770(1.8463) | Loss 38.3815(16.5311) | Error 0.6222(0.6467) Steps 598(596.71) | Grad Norm 5.6930(22.8211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 216.1681, Epoch Time 669.4306(619.3188), Bit/dim 5.1518(best: 4.7998), Xent 2.1190, Loss 6.2113, Error 0.7487(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0560 | Time 47.8709(47.8868) | Bit/dim 4.8714(4.8926) | Xent 1.9559(1.8667) | Loss 13.4837(16.4808) | Error 0.6950(0.6544) Steps 604(599.85) | Grad Norm 31.8572(30.2648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 217.1905, Epoch Time 667.5400(620.7654), Bit/dim 4.8919(best: 4.7998), Xent 1.7982, Loss 5.7910, Error 0.6274(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0570 | Time 48.8189(47.9821) | Bit/dim 4.8487(4.8962) | Xent 1.8766(1.8768) | Loss 13.4140(16.3931) | Error 0.6635(0.6601) Steps 610(603.28) | Grad Norm 21.1228(29.1719) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 214.9407, Epoch Time 658.1831(621.8879), Bit/dim 4.8047(best: 4.7998), Xent 1.7845, Loss 5.6970, Error 0.6196(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0580 | Time 46.6435(47.9309) | Bit/dim 4.8160(4.8749) | Xent 1.8910(1.8699) | Loss 13.1945(16.2624) | Error 0.6656(0.6585) Steps 592(603.92) | Grad Norm 45.4420(27.7747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 214.2678, Epoch Time 663.5727(623.1385), Bit/dim 4.7805(best: 4.7998), Xent 1.7909, Loss 5.6760, Error 0.6210(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0590 | Time 47.1437(47.8098) | Bit/dim 4.7471(4.8489) | Xent 1.7965(1.8565) | Loss 13.1253(16.1114) | Error 0.6344(0.6544) Steps 610(604.11) | Grad Norm 22.6492(27.0946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 210.9542, Epoch Time 661.4834(624.2888), Bit/dim 4.7506(best: 4.7805), Xent 1.7716, Loss 5.6364, Error 0.6199(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0600 | Time 48.9834(48.1540) | Bit/dim 4.7603(4.8249) | Xent 1.7389(1.8412) | Loss 13.0031(15.9474) | Error 0.6074(0.6486) Steps 622(605.65) | Grad Norm 24.7211(26.9703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 215.3691, Epoch Time 673.4208(625.7628), Bit/dim 4.7425(best: 4.7506), Xent 1.6944, Loss 5.5897, Error 0.5791(best: 0.5934)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0610 | Time 48.6958(48.2565) | Bit/dim 4.7130(4.8019) | Xent 1.7443(1.8180) | Loss 12.8866(15.7958) | Error 0.6187(0.6401) Steps 616(605.36) | Grad Norm 19.8054(26.4689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 216.4780, Epoch Time 669.5900(627.0776), Bit/dim 4.7150(best: 4.7425), Xent 1.7427, Loss 5.5864, Error 0.6206(best: 0.5791)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0620 | Time 49.3523(48.6604) | Bit/dim 4.7230(4.7803) | Xent 1.7121(1.7982) | Loss 12.8293(15.6573) | Error 0.6048(0.6334) Steps 592(610.26) | Grad Norm 23.4955(26.6457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 218.1846, Epoch Time 680.7955(628.6891), Bit/dim 4.7100(best: 4.7150), Xent 1.6624, Loss 5.5412, Error 0.5747(best: 0.5791)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0630 | Time 49.4851(48.8198) | Bit/dim 4.7475(4.7627) | Xent 1.8461(1.7807) | Loss 13.2474(15.5557) | Error 0.6517(0.6275) Steps 598(614.18) | Grad Norm 84.1808(28.2097) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 217.6180, Epoch Time 677.3085(630.1477), Bit/dim 4.7819(best: 4.7100), Xent 1.9947, Loss 5.7792, Error 0.7117(best: 0.5747)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0071 | Time 219.2846, Epoch Time 680.0677(631.6453), Bit/dim 4.7120(best: 4.7100), Xent 1.7117, Loss 5.5679, Error 0.5999(best: 0.5747)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0640 | Time 48.5692(48.9562) | Bit/dim 4.7287(4.7520) | Xent 1.7553(1.7989) | Loss 38.0862(16.2128) | Error 0.6131(0.6345) Steps 646(619.10) | Grad Norm 15.0871(28.7766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 216.7568, Epoch Time 687.6144(633.3244), Bit/dim 4.6822(best: 4.7100), Xent 1.6720, Loss 5.5182, Error 0.5867(best: 0.5747)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0650 | Time 49.1489(49.3665) | Bit/dim 4.6711(4.7333) | Xent 1.6927(1.7827) | Loss 12.8613(16.0705) | Error 0.6087(0.6295) Steps 622(618.10) | Grad Norm 6.0376(24.2812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 219.2028, Epoch Time 676.8765(634.6310), Bit/dim 4.6626(best: 4.6822), Xent 1.6252, Loss 5.4752, Error 0.5728(best: 0.5747)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0660 | Time 50.6082(49.3416) | Bit/dim 4.6791(4.7168) | Xent 1.6815(1.7543) | Loss 12.7906(15.9462) | Error 0.5883(0.6192) Steps 652(619.29) | Grad Norm 18.9153(21.3258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 225.1121, Epoch Time 683.4536(636.0956), Bit/dim 4.6878(best: 4.6626), Xent 1.8703, Loss 5.6229, Error 0.6711(best: 0.5728)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0670 | Time 49.4125(49.3515) | Bit/dim 4.6727(4.7108) | Xent 1.7699(1.7847) | Loss 12.8422(15.8627) | Error 0.6304(0.6291) Steps 664(623.71) | Grad Norm 23.9024(27.0567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 222.5529, Epoch Time 687.6999(637.6438), Bit/dim 4.6457(best: 4.6626), Xent 1.6963, Loss 5.4938, Error 0.5916(best: 0.5728)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0680 | Time 47.9657(49.6273) | Bit/dim 4.6464(4.6952) | Xent 1.6814(1.7730) | Loss 12.6427(15.7182) | Error 0.5907(0.6252) Steps 640(630.25) | Grad Norm 9.2492(23.3731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 218.3125, Epoch Time 688.8224(639.1791), Bit/dim 4.6413(best: 4.6457), Xent 1.6233, Loss 5.4529, Error 0.5706(best: 0.5728)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0690 | Time 48.8780(49.6165) | Bit/dim 4.6194(4.6795) | Xent 1.6596(1.7470) | Loss 12.6800(15.5616) | Error 0.5961(0.6165) Steps 628(630.38) | Grad Norm 6.9450(20.0064) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 218.2370, Epoch Time 678.2906(640.3525), Bit/dim 4.6395(best: 4.6413), Xent 1.6017, Loss 5.4404, Error 0.5713(best: 0.5706)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0700 | Time 51.9013(49.6755) | Bit/dim 4.6718(4.6704) | Xent 1.7446(1.7320) | Loss 13.0110(15.4726) | Error 0.6254(0.6118) Steps 652(635.48) | Grad Norm 38.9198(23.0280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 225.4679, Epoch Time 694.1271(641.9657), Bit/dim 4.6116(best: 4.6395), Xent 1.6580, Loss 5.4406, Error 0.5826(best: 0.5706)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0710 | Time 51.8362(49.9928) | Bit/dim 4.6102(4.6566) | Xent 1.7083(1.7305) | Loss 12.6595(15.3768) | Error 0.6000(0.6120) Steps 664(637.10) | Grad Norm 28.2266(25.5384) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 225.7502, Epoch Time 698.9342(643.6748), Bit/dim 4.6246(best: 4.6116), Xent 1.6726, Loss 5.4609, Error 0.5992(best: 0.5706)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0720 | Time 47.4540(50.0363) | Bit/dim 4.6037(4.6445) | Xent 1.6592(1.7166) | Loss 12.6861(15.2688) | Error 0.5978(0.6075) Steps 664(641.31) | Grad Norm 20.0161(25.1451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 223.9393, Epoch Time 691.6844(645.1150), Bit/dim 4.5914(best: 4.6116), Xent 1.5805, Loss 5.3816, Error 0.5557(best: 0.5706)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0081 | Time 223.6549, Epoch Time 685.7627(646.3345), Bit/dim 4.5778(best: 4.5914), Xent 1.5597, Loss 5.3577, Error 0.5475(best: 0.5557)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0730 | Time 49.5266(49.9011) | Bit/dim 4.5681(4.6292) | Xent 1.5814(1.6916) | Loss 38.4655(15.9188) | Error 0.5676(0.5993) Steps 634(642.27) | Grad Norm 8.2921(22.4172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 225.8581, Epoch Time 702.7152(648.0259), Bit/dim 4.6176(best: 4.5778), Xent 1.6446, Loss 5.4399, Error 0.5769(best: 0.5475)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0740 | Time 50.8832(50.3105) | Bit/dim 4.5818(4.6199) | Xent 1.6294(1.6826) | Loss 12.4656(15.8172) | Error 0.5748(0.5963) Steps 652(642.92) | Grad Norm 26.1763(24.8392) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 226.0905, Epoch Time 693.2499(649.3826), Bit/dim 4.5711(best: 4.5778), Xent 1.6022, Loss 5.3722, Error 0.5565(best: 0.5475)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0750 | Time 51.9975(50.4576) | Bit/dim 4.5684(4.6127) | Xent 1.7149(1.6889) | Loss 12.6609(15.7325) | Error 0.6006(0.5988) Steps 670(644.85) | Grad Norm 26.6933(27.3046) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 223.6366, Epoch Time 702.2189(650.9677), Bit/dim 4.5667(best: 4.5711), Xent 1.5856, Loss 5.3595, Error 0.5563(best: 0.5475)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0760 | Time 48.0041(50.3042) | Bit/dim 4.5646(4.6019) | Xent 1.6076(1.6756) | Loss 12.5061(15.6058) | Error 0.5654(0.5937) Steps 664(642.25) | Grad Norm 13.8743(25.1697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 223.0975, Epoch Time 681.1196(651.8723), Bit/dim 4.5486(best: 4.5667), Xent 1.5400, Loss 5.3186, Error 0.5485(best: 0.5475)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0770 | Time 49.2625(50.0731) | Bit/dim 4.5459(4.5869) | Xent 1.5651(1.6540) | Loss 12.4341(15.4639) | Error 0.5615(0.5867) Steps 628(640.19) | Grad Norm 12.1673(22.2809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 227.5183, Epoch Time 690.7319(653.0381), Bit/dim 4.5368(best: 4.5486), Xent 1.5427, Loss 5.3081, Error 0.5470(best: 0.5475)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0780 | Time 52.0635(50.1293) | Bit/dim 4.5183(4.5732) | Xent 1.6076(1.6400) | Loss 12.4350(15.3356) | Error 0.5769(0.5822) Steps 616(638.85) | Grad Norm 12.6111(21.5125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 227.2501, Epoch Time 697.9795(654.3863), Bit/dim 4.5195(best: 4.5368), Xent 1.5293, Loss 5.2841, Error 0.5379(best: 0.5470)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0790 | Time 50.7491(49.9664) | Bit/dim 4.5222(4.5606) | Xent 1.5783(1.6255) | Loss 12.3419(15.2113) | Error 0.5650(0.5781) Steps 628(638.24) | Grad Norm 15.6396(19.6536) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 227.7698, Epoch Time 690.4002(655.4667), Bit/dim 4.5104(best: 4.5195), Xent 1.5118, Loss 5.2663, Error 0.5394(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0800 | Time 48.9348(50.0077) | Bit/dim 4.5221(4.5455) | Xent 1.5838(1.6084) | Loss 12.3651(15.0806) | Error 0.5743(0.5734) Steps 646(641.26) | Grad Norm 26.8958(17.6029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 226.9151, Epoch Time 689.2791(656.4811), Bit/dim 4.5322(best: 4.5104), Xent 1.5295, Loss 5.2970, Error 0.5513(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0810 | Time 50.4361(49.8221) | Bit/dim 4.5288(4.5396) | Xent 1.6326(1.6167) | Loss 12.5256(14.9938) | Error 0.5876(0.5767) Steps 652(642.49) | Grad Norm 40.9272(22.6733) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 228.2593, Epoch Time 688.5409(657.4429), Bit/dim 4.5019(best: 4.5104), Xent 1.6168, Loss 5.3103, Error 0.5757(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0091 | Time 223.5025, Epoch Time 681.8420(658.1749), Bit/dim 4.4852(best: 4.5019), Xent 1.5180, Loss 5.2442, Error 0.5414(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0820 | Time 50.0493(49.6717) | Bit/dim 4.4877(4.5286) | Xent 1.5758(1.6154) | Loss 37.3231(15.6604) | Error 0.5569(0.5761) Steps 664(639.48) | Grad Norm 10.8522(20.8007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 223.3881, Epoch Time 696.3592(659.3204), Bit/dim 4.5079(best: 4.4852), Xent 1.5186, Loss 5.2672, Error 0.5453(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0830 | Time 47.0278(49.8809) | Bit/dim 4.5971(4.5234) | Xent 1.6475(1.6085) | Loss 12.6199(15.5385) | Error 0.5891(0.5740) Steps 646(639.48) | Grad Norm 38.8948(21.9253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 226.2785, Epoch Time 689.4979(660.2257), Bit/dim 4.6180(best: 4.4852), Xent 1.8730, Loss 5.5544, Error 0.6555(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0840 | Time 48.1716(49.8096) | Bit/dim 4.7465(4.5653) | Xent 1.7793(1.6544) | Loss 13.0953(15.5443) | Error 0.6380(0.5890) Steps 646(636.27) | Grad Norm 27.7166(27.4747) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 218.5450, Epoch Time 681.1064(660.8521), Bit/dim 4.5392(best: 4.4852), Xent 1.6847, Loss 5.3815, Error 0.6055(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0850 | Time 49.9454(49.8423) | Bit/dim 4.5547(4.5727) | Xent 1.6760(1.6785) | Loss 12.4922(15.4538) | Error 0.5974(0.5994) Steps 628(639.64) | Grad Norm 7.6867(24.5513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 216.5363, Epoch Time 680.1269(661.4304), Bit/dim 4.4980(best: 4.4852), Xent 1.5824, Loss 5.2892, Error 0.5670(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0860 | Time 47.3355(49.5320) | Bit/dim 4.4796(4.5539) | Xent 1.6402(1.6706) | Loss 12.5118(15.3058) | Error 0.5917(0.5972) Steps 640(635.96) | Grad Norm 10.2046(21.2130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 223.7668, Epoch Time 673.6052(661.7956), Bit/dim 4.4580(best: 4.4852), Xent 1.5321, Loss 5.2240, Error 0.5457(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0870 | Time 49.1107(49.4445) | Bit/dim 4.4491(4.5278) | Xent 1.5907(1.6507) | Loss 12.1514(15.1378) | Error 0.5722(0.5904) Steps 640(633.61) | Grad Norm 5.5224(17.7601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 227.5225, Epoch Time 685.4020(662.5038), Bit/dim 4.4345(best: 4.4580), Xent 1.5130, Loss 5.1910, Error 0.5411(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0880 | Time 50.1443(49.2913) | Bit/dim 4.4299(4.5016) | Xent 1.5759(1.6295) | Loss 12.2863(14.9803) | Error 0.5674(0.5829) Steps 658(636.39) | Grad Norm 18.3494(15.6633) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 225.5660, Epoch Time 687.5752(663.2560), Bit/dim 4.4168(best: 4.4345), Xent 1.4966, Loss 5.1651, Error 0.5354(best: 0.5379)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0890 | Time 48.5441(49.3513) | Bit/dim 4.3948(4.4768) | Xent 1.5504(1.6082) | Loss 12.0568(14.8321) | Error 0.5506(0.5761) Steps 640(635.80) | Grad Norm 4.4536(13.0637) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 226.1157, Epoch Time 685.0203(663.9089), Bit/dim 4.4007(best: 4.4168), Xent 1.4828, Loss 5.1421, Error 0.5308(best: 0.5354)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0900 | Time 49.9956(49.3783) | Bit/dim 4.3870(4.4544) | Xent 1.5392(1.5880) | Loss 12.0995(14.6931) | Error 0.5406(0.5685) Steps 646(638.61) | Grad Norm 12.6624(12.5372) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 225.0882, Epoch Time 685.6980(664.5626), Bit/dim 4.3879(best: 4.4007), Xent 1.4660, Loss 5.1209, Error 0.5269(best: 0.5308)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0101 | Time 227.6822, Epoch Time 686.4451(665.2190), Bit/dim 4.3746(best: 4.3879), Xent 1.4617, Loss 5.1055, Error 0.5220(best: 0.5269)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0910 | Time 48.9648(49.2934) | Bit/dim 4.3767(4.4344) | Xent 1.4863(1.5689) | Loss 37.3888(15.3384) | Error 0.5291(0.5621) Steps 628(636.74) | Grad Norm 11.6685(11.9383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 225.8581, Epoch Time 692.5615(666.0393), Bit/dim 4.3650(best: 4.3746), Xent 1.4656, Loss 5.0978, Error 0.5258(best: 0.5220)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0920 | Time 48.4447(49.4678) | Bit/dim 4.3532(4.4173) | Xent 1.5401(1.5624) | Loss 11.9598(15.1834) | Error 0.5481(0.5596) Steps 658(634.98) | Grad Norm 10.2540(14.1698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 225.1167, Epoch Time 688.0853(666.7007), Bit/dim 4.3743(best: 4.3650), Xent 1.4963, Loss 5.1225, Error 0.5410(best: 0.5220)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0930 | Time 50.2824(49.4351) | Bit/dim 4.3677(4.4075) | Xent 1.5808(1.5550) | Loss 11.9741(15.0754) | Error 0.5678(0.5575) Steps 646(639.04) | Grad Norm 17.0369(15.9970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 228.3988, Epoch Time 683.6711(667.2098), Bit/dim 4.3876(best: 4.3650), Xent 1.5275, Loss 5.1514, Error 0.5534(best: 0.5220)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0940 | Time 48.1686(49.3672) | Bit/dim 4.3702(4.4018) | Xent 1.5889(1.5624) | Loss 12.0037(14.9959) | Error 0.5672(0.5601) Steps 622(640.27) | Grad Norm 28.6015(19.3096) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 227.2237, Epoch Time 683.9601(667.7123), Bit/dim 4.3507(best: 4.3650), Xent 1.4558, Loss 5.0786, Error 0.5196(best: 0.5220)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0950 | Time 49.4073(49.3141) | Bit/dim 4.3282(4.3873) | Xent 1.5162(1.5513) | Loss 11.9515(14.8627) | Error 0.5426(0.5563) Steps 604(637.43) | Grad Norm 11.4096(17.6366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 222.2179, Epoch Time 679.2800(668.0593), Bit/dim 4.3289(best: 4.3507), Xent 1.4269, Loss 5.0424, Error 0.5171(best: 0.5196)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0960 | Time 48.8232(49.2156) | Bit/dim 4.3248(4.3725) | Xent 1.4761(1.5345) | Loss 11.7822(14.7188) | Error 0.5302(0.5506) Steps 652(635.94) | Grad Norm 6.6656(15.1172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 225.4993, Epoch Time 683.8450(668.5329), Bit/dim 4.3186(best: 4.3289), Xent 1.4159, Loss 5.0266, Error 0.5076(best: 0.5171)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0970 | Time 46.3788(49.1430) | Bit/dim 4.6313(4.3853) | Xent 1.6481(1.5447) | Loss 12.5979(14.6865) | Error 0.5846(0.5537) Steps 646(636.11) | Grad Norm 53.2402(18.9204) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 240.2634, Epoch Time 698.0162(669.4174), Bit/dim 4.6222(best: 4.3186), Xent 1.6125, Loss 5.4284, Error 0.5872(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0980 | Time 55.5727(50.1082) | Bit/dim 4.5562(4.4296) | Xent 1.8894(1.6098) | Loss 13.1966(14.8709) | Error 0.6626(0.5749) Steps 778(658.74) | Grad Norm 25.0967(23.0166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 265.3512, Epoch Time 762.5815(672.2123), Bit/dim 4.5063(best: 4.3186), Xent 1.7133, Loss 5.3630, Error 0.6229(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0990 | Time 50.3406(50.4158) | Bit/dim 4.4018(4.4342) | Xent 1.6864(1.6325) | Loss 12.2174(14.9883) | Error 0.6063(0.5843) Steps 646(667.34) | Grad Norm 9.6888(20.8659) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 226.0361, Epoch Time 702.3554(673.1166), Bit/dim 4.3891(best: 4.3186), Xent 1.5763, Loss 5.1773, Error 0.5722(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0111 | Time 215.3981, Epoch Time 655.7533(672.5957), Bit/dim 4.3370(best: 4.3186), Xent 1.4958, Loss 5.0849, Error 0.5412(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1000 | Time 45.8292(49.5155) | Bit/dim 4.3435(4.4153) | Xent 1.5567(1.6170) | Loss 36.0005(15.5050) | Error 0.5676(0.5805) Steps 616(656.29) | Grad Norm 6.1101(17.0925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 213.0940, Epoch Time 659.0105(672.1882), Bit/dim 4.3081(best: 4.3186), Xent 1.4468, Loss 5.0315, Error 0.5217(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1010 | Time 50.1903(49.2084) | Bit/dim 4.2977(4.3899) | Xent 1.5007(1.5903) | Loss 11.8117(15.2271) | Error 0.5443(0.5720) Steps 598(646.03) | Grad Norm 5.9972(13.8648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 215.3480, Epoch Time 664.3812(671.9540), Bit/dim 4.2890(best: 4.3081), Xent 1.4298, Loss 5.0039, Error 0.5156(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1020 | Time 48.7214(48.9492) | Bit/dim 4.2802(4.3643) | Xent 1.4603(1.5608) | Loss 11.6350(14.9537) | Error 0.5244(0.5616) Steps 622(636.47) | Grad Norm 3.2249(11.3905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 215.7563, Epoch Time 666.9204(671.8029), Bit/dim 4.2687(best: 4.2890), Xent 1.4210, Loss 4.9792, Error 0.5135(best: 0.5076)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1030 | Time 50.4846(48.7723) | Bit/dim 4.2687(4.3394) | Xent 1.4625(1.5378) | Loss 11.6559(14.7256) | Error 0.5304(0.5537) Steps 616(632.30) | Grad Norm 3.9698(10.0097) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 5400 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run3 --seed 3 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
