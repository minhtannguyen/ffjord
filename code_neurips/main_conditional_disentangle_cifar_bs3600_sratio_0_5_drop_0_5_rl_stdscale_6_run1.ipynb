{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=3600, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 27.2426(51.7013) | Bit/dim 8.8051(8.9212) | Xent 2.2775(2.2998) | Loss 21.7621(22.2959) | Error 0.7583(0.8664) Steps 496(499.89) | Grad Norm 22.1319(29.2520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 71.6590, Epoch Time 496.6592(496.6592), Bit/dim 8.6534(best: inf), Xent 2.2557, Loss 9.7813, Error 0.7508(best: inf)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0020 | Time 28.7072(45.8208) | Bit/dim 8.5424(8.8437) | Xent 2.2206(2.2859) | Loss 21.0277(22.6259) | Error 0.7447(0.8378) Steps 478(498.07) | Grad Norm 8.5057(25.2293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 68.8802, Epoch Time 463.8840(495.6759), Bit/dim 8.4044(best: 8.6534), Xent 2.1773, Loss 9.4930, Error 0.7378(best: 0.7508)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0030 | Time 29.2391(41.4427) | Bit/dim 8.3341(8.7352) | Xent 2.1764(2.2605) | Loss 20.5569(22.8167) | Error 0.7592(0.8141) Steps 484(494.73) | Grad Norm 7.1734(20.5145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 66.9798, Epoch Time 458.4627(494.5595), Bit/dim 8.1645(best: 8.4044), Xent 2.1135, Loss 9.2213, Error 0.7156(best: 0.7378)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0040 | Time 30.1401(38.1961) | Bit/dim 8.1865(8.6093) | Xent 2.1177(2.2287) | Loss 42.0313(22.8816) | Error 0.7289(0.7935) Steps 526(496.44) | Grad Norm 5.1248(16.7721) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 28.6472(35.8005) | Bit/dim 7.9441(8.4605) | Xent 2.0864(2.1945) | Loss 19.7199(22.1599) | Error 0.7067(0.7729) Steps 478(495.87) | Grad Norm 5.1128(13.7951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 67.6305, Epoch Time 464.4757(493.6570), Bit/dim 7.8499(best: 8.1645), Xent 2.0683, Loss 8.8840, Error 0.6953(best: 0.7156)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0060 | Time 33.3166(34.2828) | Bit/dim 7.6553(8.2810) | Xent 2.0605(2.1627) | Loss 19.4255(22.0399) | Error 0.6875(0.7539) Steps 514(500.24) | Grad Norm 5.2858(11.5139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 68.8334, Epoch Time 474.9259(493.0951), Bit/dim 7.4726(best: 7.8499), Xent 2.0448, Loss 8.4950, Error 0.6744(best: 0.6953)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0070 | Time 29.2690(33.1530) | Bit/dim 7.3709(8.0716) | Xent 2.0425(2.1331) | Loss 18.6854(21.8124) | Error 0.6714(0.7357) Steps 532(503.75) | Grad Norm 4.1200(9.6684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 71.6567, Epoch Time 479.8228(492.6969), Bit/dim 7.1941(best: 7.4726), Xent 2.0465, Loss 8.2174, Error 0.6677(best: 0.6744)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0080 | Time 29.4156(32.5071) | Bit/dim 7.1864(7.8564) | Xent 2.0474(2.1113) | Loss 18.3463(21.6058) | Error 0.6767(0.7208) Steps 526(505.98) | Grad Norm 2.7458(8.0207) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 30.2550(32.1343) | Bit/dim 7.0725(7.6618) | Xent 2.0551(2.0977) | Loss 17.7838(20.7067) | Error 0.6956(0.7122) Steps 532(510.94) | Grad Norm 1.8664(6.4895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 71.1918, Epoch Time 489.5358(492.6021), Bit/dim 7.0689(best: 7.1941), Xent 2.0537, Loss 8.0957, Error 0.6818(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0100 | Time 31.8641(32.0011) | Bit/dim 7.0295(7.5001) | Xent 2.0628(2.0878) | Loss 18.1230(20.5669) | Error 0.7125(0.7086) Steps 550(517.51) | Grad Norm 1.8171(5.2255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 71.5796, Epoch Time 502.4217(492.8967), Bit/dim 7.0105(best: 7.0689), Xent 2.0456, Loss 8.0333, Error 0.6855(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0110 | Time 30.5313(31.9732) | Bit/dim 6.9893(7.3703) | Xent 2.0584(2.0784) | Loss 17.8570(20.4785) | Error 0.6939(0.7050) Steps 526(523.13) | Grad Norm 1.3747(4.3091) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 72.4154, Epoch Time 513.8064(493.5240), Bit/dim 6.9661(best: 7.0105), Xent 2.0270, Loss 7.9796, Error 0.6739(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0120 | Time 34.0771(32.5617) | Bit/dim 6.9706(7.2663) | Xent 2.0472(2.0687) | Loss 18.0302(20.4791) | Error 0.6867(0.7009) Steps 538(526.31) | Grad Norm 1.1636(3.8328) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 35.2282(33.2496) | Bit/dim 6.9172(7.1782) | Xent 2.0185(2.0577) | Loss 17.9069(19.7981) | Error 0.6783(0.6962) Steps 550(528.86) | Grad Norm 2.3784(3.5763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 73.0020, Epoch Time 545.0887(495.0709), Bit/dim 6.9140(best: 6.9661), Xent 2.0130, Loss 7.9205, Error 0.6724(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0140 | Time 34.1860(33.6203) | Bit/dim 6.8679(7.1021) | Xent 2.0047(2.0468) | Loss 17.7026(19.7457) | Error 0.6733(0.6915) Steps 550(533.33) | Grad Norm 2.5458(3.8088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 73.6078, Epoch Time 540.1386(496.4229), Bit/dim 6.8423(best: 6.9140), Xent 2.0056, Loss 7.8451, Error 0.6794(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0150 | Time 34.4575(33.8970) | Bit/dim 6.7966(7.0296) | Xent 1.9923(2.0374) | Loss 17.5252(19.7501) | Error 0.6658(0.6878) Steps 526(535.76) | Grad Norm 10.7368(5.1677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 74.2786, Epoch Time 543.3952(497.8321), Bit/dim 6.7278(best: 6.8423), Xent 1.9898, Loss 7.7227, Error 0.6756(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0160 | Time 36.6904(34.1779) | Bit/dim 6.7007(6.9538) | Xent 2.0467(2.0297) | Loss 17.4866(19.8057) | Error 0.7258(0.6864) Steps 574(540.45) | Grad Norm 47.8234(8.1059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 74.3277, Epoch Time 544.6416(499.2364), Bit/dim 6.5771(best: 6.7278), Xent 1.9953, Loss 7.5747, Error 0.6769(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0170 | Time 35.6729(34.4218) | Bit/dim 6.5776(6.8705) | Xent 2.0009(2.0301) | Loss 40.4946(19.8452) | Error 0.6847(0.6919) Steps 556(546.89) | Grad Norm 36.6833(16.6281) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 33.6869(34.5234) | Bit/dim 6.3893(6.7678) | Xent 2.0016(2.0211) | Loss 16.7931(19.0648) | Error 0.6950(0.6897) Steps 562(545.36) | Grad Norm 36.3865(18.6010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 71.7957, Epoch Time 537.4019(500.3814), Bit/dim 6.3891(best: 6.5771), Xent 2.0920, Loss 7.4351, Error 0.7493(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0190 | Time 37.4676(34.3525) | Bit/dim 6.2914(6.6609) | Xent 2.2273(2.0551) | Loss 16.6307(19.0134) | Error 0.8114(0.7090) Steps 532(545.48) | Grad Norm 93.2352(39.3530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 73.7532, Epoch Time 534.2540(501.3975), Bit/dim 6.1933(best: 6.3891), Xent 2.0249, Loss 7.2057, Error 0.6739(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0200 | Time 31.2936(34.1301) | Bit/dim 6.1057(6.5389) | Xent 2.0385(2.0491) | Loss 16.1662(18.9182) | Error 0.6878(0.7069) Steps 550(545.24) | Grad Norm 48.0350(39.2548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 75.2660, Epoch Time 519.3909(501.9373), Bit/dim 5.9611(best: 6.1933), Xent 1.9955, Loss 6.9589, Error 0.6667(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0210 | Time 32.9832(33.9153) | Bit/dim 5.9405(6.4011) | Xent 1.9992(2.0421) | Loss 15.5111(18.7945) | Error 0.6881(0.7043) Steps 544(546.46) | Grad Norm 28.3415(38.0824) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 35.6904(34.0444) | Bit/dim 5.9728(6.3067) | Xent 2.0480(2.0680) | Loss 15.9391(18.0837) | Error 0.6933(0.7141) Steps 568(549.24) | Grad Norm 26.4125(59.0768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 74.0281, Epoch Time 535.7483(502.9517), Bit/dim 5.9973(best: 5.9611), Xent 2.0285, Loss 7.0116, Error 0.6794(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0230 | Time 33.1885(33.7922) | Bit/dim 5.8372(6.1994) | Xent 2.0787(2.0685) | Loss 15.5174(17.9937) | Error 0.7211(0.7174) Steps 502(546.32) | Grad Norm 22.9345(51.5989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 73.1975, Epoch Time 514.5186(503.2987), Bit/dim 5.7894(best: 5.9611), Xent 2.0330, Loss 6.8059, Error 0.6844(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0240 | Time 31.3791(33.3226) | Bit/dim 5.7584(6.0919) | Xent 2.0069(2.0601) | Loss 15.3381(17.9045) | Error 0.6814(0.7113) Steps 538(544.34) | Grad Norm 23.4009(43.1939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 72.7126, Epoch Time 498.1416(503.1440), Bit/dim 5.7147(best: 5.7894), Xent 1.9940, Loss 6.7118, Error 0.6752(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0250 | Time 33.6194(32.9385) | Bit/dim 5.7060(5.9959) | Xent 1.9743(2.0453) | Loss 15.1556(17.8639) | Error 0.6594(0.7032) Steps 532(543.70) | Grad Norm 13.2693(35.0182) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 32.2357(32.7372) | Bit/dim 5.6579(5.9138) | Xent 1.9474(2.0264) | Loss 15.1568(17.1538) | Error 0.6528(0.6929) Steps 556(545.88) | Grad Norm 9.2074(28.7490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 72.2008, Epoch Time 509.3202(503.3292), Bit/dim 5.6660(best: 5.7147), Xent 1.9516, Loss 6.6418, Error 0.6437(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0270 | Time 32.3432(32.5861) | Bit/dim 5.6497(5.8456) | Xent 1.9371(2.0058) | Loss 15.1101(17.1012) | Error 0.6539(0.6829) Steps 550(544.61) | Grad Norm 11.7883(23.6029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 73.1130, Epoch Time 510.8232(503.5541), Bit/dim 5.6290(best: 5.6660), Xent 1.9253, Loss 6.5917, Error 0.6364(best: 0.6437)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0280 | Time 32.7989(32.6570) | Bit/dim 5.6113(5.7875) | Xent 1.9286(1.9885) | Loss 14.9791(17.1144) | Error 0.6422(0.6761) Steps 580(543.15) | Grad Norm 10.8075(21.9734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 73.2759, Epoch Time 512.3115(503.8168), Bit/dim 5.6096(best: 5.6290), Xent 1.9695, Loss 6.5944, Error 0.7030(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0290 | Time 32.1750(32.5279) | Bit/dim 5.6284(5.7453) | Xent 2.0029(1.9888) | Loss 14.9390(17.1816) | Error 0.7058(0.6793) Steps 520(540.16) | Grad Norm 61.2657(28.6162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 73.4471, Epoch Time 504.5731(503.8395), Bit/dim 5.5743(best: 5.6096), Xent 1.9583, Loss 6.5535, Error 0.6578(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0300 | Time 30.8914(32.3652) | Bit/dim 5.5673(5.7067) | Xent 1.9638(1.9868) | Loss 38.3873(17.3100) | Error 0.6700(0.6823) Steps 532(540.28) | Grad Norm 10.3395(30.2533) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 31.1028(32.2519) | Bit/dim 5.5791(5.6747) | Xent 2.0021(2.0046) | Loss 14.7330(16.6948) | Error 0.6908(0.6923) Steps 502(541.01) | Grad Norm 39.6451(42.6657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 74.3195, Epoch Time 505.6542(503.8939), Bit/dim 5.5474(best: 5.5743), Xent 1.9614, Loss 6.5281, Error 0.6530(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0320 | Time 34.5109(32.3210) | Bit/dim 5.5574(5.6401) | Xent 1.9663(1.9968) | Loss 14.8695(16.7521) | Error 0.6558(0.6873) Steps 550(538.22) | Grad Norm 21.3338(37.5742) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 73.1757, Epoch Time 510.1973(504.0830), Bit/dim 5.5135(best: 5.5474), Xent 1.9385, Loss 6.4828, Error 0.6402(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0330 | Time 31.9810(32.2636) | Bit/dim 5.4978(5.6052) | Xent 1.9265(1.9840) | Loss 14.6814(16.8574) | Error 0.6489(0.6794) Steps 550(541.59) | Grad Norm 14.8735(30.9443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 73.8382, Epoch Time 502.7492(504.0430), Bit/dim 5.4677(best: 5.5135), Xent 1.9034, Loss 6.4194, Error 0.6290(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0340 | Time 33.9417(32.2099) | Bit/dim 5.4552(5.5723) | Xent 1.9110(1.9690) | Loss 14.5430(16.9873) | Error 0.6533(0.6730) Steps 538(542.35) | Grad Norm 8.8196(25.3267) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 34.2263(32.2501) | Bit/dim 5.4372(5.5388) | Xent 1.8855(1.9511) | Loss 14.5592(16.3492) | Error 0.6461(0.6665) Steps 550(542.84) | Grad Norm 8.3137(20.5240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 74.7147, Epoch Time 509.9703(504.2208), Bit/dim 5.4183(best: 5.4677), Xent 1.8787, Loss 6.3577, Error 0.6301(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0360 | Time 32.6903(32.4597) | Bit/dim 5.3834(5.5039) | Xent 1.9281(1.9364) | Loss 14.4630(16.4054) | Error 0.6694(0.6624) Steps 586(545.16) | Grad Norm 24.0897(19.5277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 74.7358, Epoch Time 522.7453(504.7766), Bit/dim 5.3780(best: 5.4183), Xent 1.9337, Loss 6.3449, Error 0.6884(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0370 | Time 32.2710(32.6559) | Bit/dim 5.4033(5.4711) | Xent 2.0096(1.9373) | Loss 14.8017(16.5207) | Error 0.7031(0.6661) Steps 568(547.63) | Grad Norm 45.2489(24.7288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 76.3955, Epoch Time 526.6949(505.4341), Bit/dim 5.3267(best: 5.3780), Xent 1.9251, Loss 6.2892, Error 0.6643(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0380 | Time 34.3306(33.0078) | Bit/dim 5.3174(5.4342) | Xent 1.9628(1.9399) | Loss 14.4552(16.6569) | Error 0.6772(0.6699) Steps 586(552.22) | Grad Norm 76.6961(28.8987) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 33.9765(33.2266) | Bit/dim 5.3256(5.4055) | Xent 1.8731(1.9303) | Loss 14.1726(16.0393) | Error 0.6433(0.6670) Steps 508(552.91) | Grad Norm 27.5192(32.2525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 74.5129, Epoch Time 532.6941(506.2519), Bit/dim 5.2963(best: 5.3267), Xent 1.8719, Loss 6.2322, Error 0.6305(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0400 | Time 33.3948(33.5022) | Bit/dim 5.2154(5.3693) | Xent 1.8778(1.9186) | Loss 14.0972(16.0946) | Error 0.6583(0.6627) Steps 580(556.42) | Grad Norm 18.9521(29.1803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 74.0089, Epoch Time 539.2892(507.2430), Bit/dim 5.2273(best: 5.2963), Xent 1.8583, Loss 6.1564, Error 0.6277(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0410 | Time 33.3703(33.9005) | Bit/dim 5.2197(5.3354) | Xent 1.8883(1.9069) | Loss 14.0303(16.1693) | Error 0.6486(0.6586) Steps 568(559.94) | Grad Norm 10.0275(25.7785) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 72.8083, Epoch Time 537.3941(508.1476), Bit/dim 5.1901(best: 5.2273), Xent 1.8497, Loss 6.1149, Error 0.6231(best: 0.6277)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0420 | Time 33.6547(34.0379) | Bit/dim 5.2343(5.3012) | Xent 1.8918(1.8956) | Loss 14.1995(16.2393) | Error 0.6536(0.6547) Steps 574(558.45) | Grad Norm 41.5977(24.3625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 74.1674, Epoch Time 538.4800(509.0575), Bit/dim 5.1635(best: 5.1901), Xent 1.8589, Loss 6.0930, Error 0.6431(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0430 | Time 35.6206(34.2006) | Bit/dim 5.1386(5.2654) | Xent 1.8698(1.8874) | Loss 37.1524(16.3356) | Error 0.6531(0.6527) Steps 520(557.57) | Grad Norm 38.2129(25.4557) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 32.4807(34.2459) | Bit/dim 5.4469(5.2491) | Xent 2.2093(1.9133) | Loss 14.8471(15.7502) | Error 0.8006(0.6638) Steps 556(555.61) | Grad Norm 85.4048(33.8205) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 75.8729, Epoch Time 534.3033(509.8149), Bit/dim 5.1870(best: 5.1635), Xent 2.0461, Loss 6.2100, Error 0.7254(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0450 | Time 36.4698(34.1233) | Bit/dim 5.1909(5.2570) | Xent 2.0301(1.9610) | Loss 14.2682(15.9824) | Error 0.7325(0.6828) Steps 574(559.10) | Grad Norm 12.2163(32.9375) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 73.9106, Epoch Time 540.9739(510.7497), Bit/dim 5.1498(best: 5.1635), Xent 1.9539, Loss 6.1268, Error 0.6645(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0460 | Time 35.7500(34.4219) | Bit/dim 5.1275(5.2308) | Xent 1.9581(1.9694) | Loss 13.9316(16.1163) | Error 0.6922(0.6868) Steps 514(558.73) | Grad Norm 14.3033(27.9549) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 73.5670, Epoch Time 552.3936(511.9990), Bit/dim 5.0754(best: 5.1498), Xent 1.8945, Loss 6.0227, Error 0.6477(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0470 | Time 35.1920(34.8218) | Bit/dim 5.0886(5.1972) | Xent 1.9200(1.9575) | Loss 13.7461(16.1743) | Error 0.6622(0.6821) Steps 532(555.54) | Grad Norm 7.7701(23.1433) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 37.7916(35.0534) | Bit/dim 5.0427(5.1594) | Xent 1.8787(1.9383) | Loss 13.6901(15.5217) | Error 0.6556(0.6742) Steps 562(551.94) | Grad Norm 5.1650(18.4446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 75.5866, Epoch Time 554.7393(513.2812), Bit/dim 5.0318(best: 5.0754), Xent 1.8482, Loss 5.9559, Error 0.6270(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0490 | Time 38.4546(35.7696) | Bit/dim 5.0300(5.1239) | Xent 1.8840(1.9180) | Loss 13.8031(15.6026) | Error 0.6497(0.6661) Steps 532(553.55) | Grad Norm 31.1150(15.4497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 75.5887, Epoch Time 573.8211(515.0974), Bit/dim 5.0668(best: 5.0318), Xent 2.0196, Loss 6.0766, Error 0.7315(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0500 | Time 36.0263(35.9983) | Bit/dim 4.9990(5.1071) | Xent 1.9319(1.9579) | Loss 13.5597(15.7565) | Error 0.6742(0.6791) Steps 538(553.26) | Grad Norm 7.0278(27.2630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 76.7272, Epoch Time 583.1850(517.1400), Bit/dim 4.9767(best: 5.0318), Xent 1.9224, Loss 5.9379, Error 0.6674(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0510 | Time 39.2228(36.6409) | Bit/dim 4.9822(5.0770) | Xent 1.9424(1.9567) | Loss 13.6904(15.8557) | Error 0.6736(0.6805) Steps 598(557.94) | Grad Norm 7.3443(24.1613) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 36.6121(37.1694) | Bit/dim 4.9311(5.0425) | Xent 1.9280(1.9487) | Loss 13.5590(15.2492) | Error 0.6778(0.6776) Steps 580(563.45) | Grad Norm 6.3279(20.0172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 77.2315, Epoch Time 597.8127(519.5602), Bit/dim 4.9227(best: 4.9767), Xent 1.8825, Loss 5.8639, Error 0.6547(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0530 | Time 38.2642(37.7651) | Bit/dim 4.8900(5.0077) | Xent 1.8776(1.9338) | Loss 13.4784(15.3233) | Error 0.6544(0.6738) Steps 550(562.80) | Grad Norm 9.6112(17.1346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 78.8499, Epoch Time 604.9933(522.1232), Bit/dim 4.8925(best: 4.9227), Xent 1.8271, Loss 5.8060, Error 0.6271(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0540 | Time 38.7879(38.1144) | Bit/dim 4.8812(4.9754) | Xent 1.8365(1.9116) | Loss 13.3894(15.4158) | Error 0.6447(0.6669) Steps 574(565.86) | Grad Norm 25.1436(16.6391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 77.5279, Epoch Time 600.9388(524.4877), Bit/dim 4.8494(best: 4.8925), Xent 1.7846, Loss 5.7417, Error 0.6113(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0550 | Time 39.7121(38.1773) | Bit/dim 4.8242(4.9455) | Xent 1.7772(1.8868) | Loss 13.0224(15.4829) | Error 0.6264(0.6586) Steps 568(568.39) | Grad Norm 5.4559(16.4440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 78.8181, Epoch Time 588.0461(526.3944), Bit/dim 5.0560(best: 4.8494), Xent 1.8833, Loss 5.9976, Error 0.6697(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0560 | Time 37.7109(38.1217) | Bit/dim 5.0537(4.9873) | Xent 1.8804(1.9037) | Loss 39.0678(15.8357) | Error 0.6617(0.6639) Steps 604(570.59) | Grad Norm 19.2082(27.6571) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 41.6976(38.3624) | Bit/dim 4.9686(4.9957) | Xent 1.9210(1.9099) | Loss 13.6754(15.2856) | Error 0.6706(0.6674) Steps 544(567.17) | Grad Norm 13.2504(25.3961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 75.8773, Epoch Time 594.9308(528.4505), Bit/dim 4.9171(best: 4.8494), Xent 1.8760, Loss 5.8551, Error 0.6512(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0580 | Time 39.8691(38.3594) | Bit/dim 4.8695(4.9703) | Xent 1.9079(1.9074) | Loss 13.4655(15.3516) | Error 0.6792(0.6670) Steps 598(566.03) | Grad Norm 13.5502(21.5635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 75.5011, Epoch Time 587.4118(530.2193), Bit/dim 4.8277(best: 4.8494), Xent 1.8366, Loss 5.7461, Error 0.6363(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0590 | Time 38.0773(38.3276) | Bit/dim 4.8029(4.9342) | Xent 1.8447(1.8976) | Loss 13.1236(15.4107) | Error 0.6419(0.6639) Steps 568(564.13) | Grad Norm 5.5896(17.5134) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 76.5661, Epoch Time 588.4126(531.9651), Bit/dim 4.7839(best: 4.8277), Xent 1.8007, Loss 5.6843, Error 0.6291(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0600 | Time 39.0648(38.3423) | Bit/dim 4.7708(4.8968) | Xent 1.8406(1.8824) | Loss 12.9936(15.4785) | Error 0.6503(0.6593) Steps 550(560.03) | Grad Norm 7.5366(15.4142) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 37.5809(38.5854) | Bit/dim 4.7490(4.8659) | Xent 1.8047(1.8722) | Loss 13.0435(14.8524) | Error 0.6286(0.6558) Steps 574(563.25) | Grad Norm 9.6632(17.5106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 76.9275, Epoch Time 606.2415(534.1934), Bit/dim 4.8069(best: 4.7839), Xent 1.8466, Loss 5.7302, Error 0.6466(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0620 | Time 38.8497(38.7654) | Bit/dim 4.7438(4.8478) | Xent 1.7997(1.8624) | Loss 12.9523(14.9503) | Error 0.6206(0.6520) Steps 550(562.98) | Grad Norm 13.2848(21.5460) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 77.7144, Epoch Time 601.5945(536.2155), Bit/dim 4.8164(best: 4.7839), Xent 1.7739, Loss 5.7034, Error 0.6207(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0630 | Time 38.3176(38.7348) | Bit/dim 4.7239(4.8285) | Xent 1.7811(1.8445) | Loss 12.8637(15.0508) | Error 0.6272(0.6461) Steps 574(565.62) | Grad Norm 14.9647(21.2408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 78.7595, Epoch Time 600.5283(538.1449), Bit/dim 4.7166(best: 4.7839), Xent 1.7265, Loss 5.5799, Error 0.5996(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0640 | Time 39.6424(38.9759) | Bit/dim 4.6912(4.8019) | Xent 1.7552(1.8253) | Loss 12.7916(15.1769) | Error 0.6111(0.6397) Steps 574(569.48) | Grad Norm 8.2857(20.1723) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 41.4801(39.2420) | Bit/dim 4.7192(4.7780) | Xent 1.7403(1.8044) | Loss 12.6946(14.5712) | Error 0.6336(0.6336) Steps 550(572.15) | Grad Norm 18.0599(18.5072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 79.8249, Epoch Time 615.3866(540.4621), Bit/dim 4.6928(best: 4.7166), Xent 1.6896, Loss 5.5376, Error 0.5845(best: 0.5996)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0660 | Time 39.2212(39.5996) | Bit/dim 4.7801(4.7664) | Xent 1.7670(1.8033) | Loss 13.1471(14.7343) | Error 0.6047(0.6337) Steps 592(579.46) | Grad Norm 33.8666(24.4663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 79.4472, Epoch Time 620.3672(542.8593), Bit/dim 4.7308(best: 4.6928), Xent 1.7476, Loss 5.6046, Error 0.6064(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0670 | Time 40.2201(39.6770) | Bit/dim 4.6881(4.7531) | Xent 1.7604(1.8000) | Loss 12.8873(14.8919) | Error 0.6161(0.6332) Steps 568(582.99) | Grad Norm 10.5155(24.4715) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 79.9140, Epoch Time 617.5274(545.0993), Bit/dim 4.6738(best: 4.6928), Xent 1.7021, Loss 5.5248, Error 0.5968(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0680 | Time 41.9699(39.8887) | Bit/dim 4.6736(4.7324) | Xent 1.7287(1.7881) | Loss 12.7123(15.0247) | Error 0.6192(0.6295) Steps 628(588.03) | Grad Norm 8.9682(21.9576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 80.6092, Epoch Time 621.2837(547.3848), Bit/dim 4.6399(best: 4.6738), Xent 1.6562, Loss 5.4680, Error 0.5840(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0690 | Time 42.2017(40.0740) | Bit/dim 4.6338(4.7113) | Xent 1.6971(1.7685) | Loss 36.8417(15.1555) | Error 0.5950(0.6227) Steps 616(594.58) | Grad Norm 13.6920(20.0919) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 39.3673(40.3932) | Bit/dim 4.7063(4.7027) | Xent 1.9273(1.7754) | Loss 13.0885(14.5704) | Error 0.6725(0.6258) Steps 592(594.18) | Grad Norm 82.1988(27.7504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 80.5972, Epoch Time 632.7182(549.9448), Bit/dim 4.6460(best: 4.6399), Xent 1.7549, Loss 5.5234, Error 0.6250(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0710 | Time 38.7453(40.4409) | Bit/dim 4.6359(4.6893) | Xent 1.7912(1.7773) | Loss 12.7370(14.7089) | Error 0.6394(0.6279) Steps 592(593.96) | Grad Norm 29.1324(26.4708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 78.6955, Epoch Time 616.7053(551.9476), Bit/dim 4.6221(best: 4.6399), Xent 1.6964, Loss 5.4703, Error 0.5949(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0720 | Time 40.9025(40.3062) | Bit/dim 4.6178(4.6725) | Xent 1.7343(1.7653) | Loss 12.6995(14.8087) | Error 0.6253(0.6251) Steps 586(592.04) | Grad Norm 19.7503(24.7892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 79.3771, Epoch Time 619.5175(553.9747), Bit/dim 4.5986(best: 4.6221), Xent 1.6248, Loss 5.4110, Error 0.5772(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0730 | Time 41.2319(40.2562) | Bit/dim 4.5704(4.6552) | Xent 1.6655(1.7473) | Loss 12.5550(14.9284) | Error 0.5914(0.6190) Steps 586(591.03) | Grad Norm 24.3481(25.1754) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 42.5807(40.4015) | Bit/dim 4.5872(4.6438) | Xent 1.6577(1.7371) | Loss 12.3599(14.3177) | Error 0.6011(0.6165) Steps 562(588.25) | Grad Norm 5.1423(26.0316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 79.8947, Epoch Time 621.9646(556.0144), Bit/dim 4.6358(best: 4.5986), Xent 1.6725, Loss 5.4720, Error 0.5848(best: 0.5772)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0750 | Time 40.6407(40.5366) | Bit/dim 4.6102(4.6376) | Xent 1.7290(1.7388) | Loss 12.6231(14.4509) | Error 0.6206(0.6175) Steps 592(589.70) | Grad Norm 34.7996(29.9656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 80.2786, Epoch Time 630.0148(558.2345), Bit/dim 4.5814(best: 4.5986), Xent 1.6241, Loss 5.3935, Error 0.5745(best: 0.5772)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0760 | Time 39.6867(40.6702) | Bit/dim 4.5645(4.6217) | Xent 1.6851(1.7264) | Loss 12.6424(14.5893) | Error 0.5972(0.6131) Steps 634(590.33) | Grad Norm 20.1152(27.2567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 80.5820, Epoch Time 620.7488(560.1099), Bit/dim 4.5483(best: 4.5814), Xent 1.6130, Loss 5.3548, Error 0.5735(best: 0.5745)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0770 | Time 42.2866(40.5664) | Bit/dim 4.5496(4.6030) | Xent 1.6656(1.7110) | Loss 12.7164(14.7460) | Error 0.6017(0.6079) Steps 622(594.17) | Grad Norm 23.1231(24.7177) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 40.2695(40.6088) | Bit/dim 4.5543(4.5879) | Xent 1.6613(1.6920) | Loss 12.4828(14.1616) | Error 0.6044(0.6023) Steps 574(597.88) | Grad Norm 39.3329(23.5489) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 81.3241, Epoch Time 626.8365(562.1117), Bit/dim 4.5353(best: 4.5483), Xent 1.6528, Loss 5.3617, Error 0.5946(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0790 | Time 39.4304(40.7397) | Bit/dim 4.5163(4.5726) | Xent 1.6837(1.6928) | Loss 12.4568(14.3005) | Error 0.6022(0.6028) Steps 622(600.51) | Grad Norm 24.3479(27.1677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 82.8344, Epoch Time 634.8395(564.2935), Bit/dim 4.5432(best: 4.5353), Xent 1.6451, Loss 5.3657, Error 0.5916(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0800 | Time 41.0617(40.8452) | Bit/dim 4.5164(4.5638) | Xent 1.6493(1.6969) | Loss 12.3498(14.4622) | Error 0.5825(0.6047) Steps 616(602.20) | Grad Norm 6.8612(28.2251) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.2626, Epoch Time 631.6558(566.3144), Bit/dim 4.4978(best: 4.5353), Xent 1.5806, Loss 5.2881, Error 0.5658(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0810 | Time 39.5316(40.8084) | Bit/dim 4.4891(4.5470) | Xent 1.5984(1.6800) | Loss 12.1575(14.5978) | Error 0.5736(0.5997) Steps 586(604.20) | Grad Norm 6.5636(24.1271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 83.6346, Epoch Time 635.1222(568.3786), Bit/dim 4.6588(best: 4.4978), Xent 1.6515, Loss 5.4845, Error 0.5898(best: 0.5658)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0820 | Time 41.6616(41.0880) | Bit/dim 4.6575(4.5403) | Xent 1.7163(1.6683) | Loss 38.3972(14.8037) | Error 0.5989(0.5949) Steps 610(606.91) | Grad Norm 68.9778(23.8049) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 40.9529(41.1916) | Bit/dim 4.5754(4.5612) | Xent 1.6629(1.6766) | Loss 12.5975(14.2428) | Error 0.5969(0.5974) Steps 586(604.09) | Grad Norm 14.2301(25.5237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 81.5708, Epoch Time 636.4593(570.4210), Bit/dim 4.5360(best: 4.4978), Xent 1.5711, Loss 5.3215, Error 0.5617(best: 0.5658)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0840 | Time 41.0413(41.3632) | Bit/dim 4.4898(4.5501) | Xent 1.5869(1.6628) | Loss 12.2420(14.3530) | Error 0.5578(0.5929) Steps 556(600.95) | Grad Norm 10.2151(22.2487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 79.9973, Epoch Time 642.5546(572.5850), Bit/dim 4.4725(best: 4.4978), Xent 1.5458, Loss 5.2454, Error 0.5520(best: 0.5617)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0850 | Time 40.4247(41.5529) | Bit/dim 4.4659(4.5319) | Xent 1.6024(1.6471) | Loss 12.1663(14.4450) | Error 0.5769(0.5890) Steps 568(601.54) | Grad Norm 19.8602(21.3513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 80.9160, Epoch Time 642.6750(574.6877), Bit/dim 4.4404(best: 4.4725), Xent 1.5322, Loss 5.2065, Error 0.5511(best: 0.5520)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0860 | Time 39.2184(41.6049) | Bit/dim 4.4424(4.5106) | Xent 1.6021(1.6331) | Loss 12.0914(14.5575) | Error 0.5742(0.5846) Steps 604(600.64) | Grad Norm 17.1552(19.9302) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 40.3712(41.4885) | Bit/dim 4.4386(4.4891) | Xent 1.5987(1.6216) | Loss 12.1083(13.9226) | Error 0.5678(0.5806) Steps 580(604.31) | Grad Norm 25.7767(19.8402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 83.2989, Epoch Time 632.3648(576.4181), Bit/dim 4.4455(best: 4.4404), Xent 1.5983, Loss 5.2447, Error 0.5728(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0880 | Time 44.3882(41.8069) | Bit/dim 4.4579(4.4859) | Xent 1.6236(1.6412) | Loss 12.2758(14.1271) | Error 0.5894(0.5862) Steps 628(610.62) | Grad Norm 18.6882(24.8363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 82.1536, Epoch Time 649.0545(578.5971), Bit/dim 4.4224(best: 4.4404), Xent 1.5748, Loss 5.2098, Error 0.5664(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0890 | Time 41.2950(41.7386) | Bit/dim 4.4152(4.4707) | Xent 1.5902(1.6419) | Loss 12.1452(14.2607) | Error 0.5667(0.5880) Steps 610(612.19) | Grad Norm 14.1262(23.1698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 81.4169, Epoch Time 634.7068(580.2804), Bit/dim 4.4070(best: 4.4224), Xent 1.5221, Loss 5.1680, Error 0.5380(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0900 | Time 42.1525(41.6360) | Bit/dim 4.3866(4.4524) | Xent 1.5492(1.6247) | Loss 12.1773(14.3528) | Error 0.5547(0.5827) Steps 634(612.56) | Grad Norm 14.5351(19.9922) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 41.9381(41.6427) | Bit/dim 4.3804(4.4404) | Xent 1.5567(1.6149) | Loss 12.0979(13.7681) | Error 0.5658(0.5799) Steps 598(613.12) | Grad Norm 13.4255(19.8723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 83.7943, Epoch Time 640.6680(582.0921), Bit/dim 4.3872(best: 4.4070), Xent 1.5137, Loss 5.1441, Error 0.5477(best: 0.5380)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0920 | Time 41.7000(41.5268) | Bit/dim 4.3579(4.4231) | Xent 1.5265(1.5947) | Loss 11.9450(13.8650) | Error 0.5425(0.5733) Steps 634(614.15) | Grad Norm 12.3846(17.7586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 83.5486, Epoch Time 634.0506(583.6508), Bit/dim 4.3953(best: 4.3872), Xent 1.5043, Loss 5.1474, Error 0.5371(best: 0.5380)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0930 | Time 41.3464(41.4080) | Bit/dim 4.3839(4.4102) | Xent 1.5197(1.5844) | Loss 11.7870(13.9963) | Error 0.5497(0.5696) Steps 586(615.56) | Grad Norm 23.4183(19.0825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 82.3633, Epoch Time 638.2231(585.2880), Bit/dim 4.3554(best: 4.3872), Xent 1.4816, Loss 5.0962, Error 0.5366(best: 0.5371)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0940 | Time 42.4748(41.6353) | Bit/dim 4.3573(4.3977) | Xent 1.5475(1.5767) | Loss 12.1837(14.1728) | Error 0.5689(0.5681) Steps 634(618.46) | Grad Norm 12.4455(18.7476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 85.2817, Epoch Time 655.1072(587.3826), Bit/dim 4.3853(best: 4.3554), Xent 1.6224, Loss 5.1965, Error 0.5800(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0950 | Time 43.6800(41.9514) | Bit/dim 4.3854(4.3839) | Xent 1.6766(1.5712) | Loss 37.3144(14.3404) | Error 0.5989(0.5649) Steps 616(621.33) | Grad Norm 66.4723(20.3557) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 41.1588(41.9739) | Bit/dim 4.3716(4.3893) | Xent 1.6096(1.5987) | Loss 11.9812(13.7734) | Error 0.5894(0.5747) Steps 634(619.61) | Grad Norm 11.2457(22.7345) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 82.4476, Epoch Time 646.3633(589.1520), Bit/dim 4.3576(best: 4.3554), Xent 1.5240, Loss 5.1196, Error 0.5561(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0970 | Time 44.0778(42.5491) | Bit/dim 4.3222(4.3775) | Xent 1.5567(1.5904) | Loss 11.9250(13.8691) | Error 0.5453(0.5721) Steps 610(619.02) | Grad Norm 6.6359(19.7463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 81.9179, Epoch Time 670.9081(591.6047), Bit/dim 4.3077(best: 4.3554), Xent 1.4746, Loss 5.0450, Error 0.5248(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0980 | Time 41.9793(42.5597) | Bit/dim 4.3234(4.3610) | Xent 1.5010(1.5738) | Loss 11.8145(14.0137) | Error 0.5475(0.5676) Steps 628(620.35) | Grad Norm 14.6794(17.0183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 83.1331, Epoch Time 649.8468(593.3519), Bit/dim 4.2859(best: 4.3077), Xent 1.4464, Loss 5.0091, Error 0.5165(best: 0.5248)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0990 | Time 44.0010(42.6661) | Bit/dim 4.2948(4.3447) | Xent 1.5010(1.5581) | Loss 11.7613(14.1536) | Error 0.5458(0.5626) Steps 586(618.85) | Grad Norm 12.2108(15.7216) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 41.2119(42.7768) | Bit/dim 4.2863(4.3345) | Xent 1.5148(1.5463) | Loss 11.8047(13.5421) | Error 0.5558(0.5595) Steps 622(620.51) | Grad Norm 9.8928(16.9174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 82.4146, Epoch Time 658.5777(595.3087), Bit/dim 4.3117(best: 4.2859), Xent 1.4577, Loss 5.0406, Error 0.5155(best: 0.5165)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1010 | Time 46.5970(43.1846) | Bit/dim 4.2563(4.3210) | Xent 1.4796(1.5337) | Loss 11.6861(13.6649) | Error 0.5436(0.5547) Steps 622(619.22) | Grad Norm 11.5878(16.7171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 82.2773, Epoch Time 666.4644(597.4434), Bit/dim 4.2652(best: 4.2859), Xent 1.4242, Loss 4.9773, Error 0.5119(best: 0.5155)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1020 | Time 41.8305(43.1382) | Bit/dim 4.2686(4.3071) | Xent 1.5886(1.5234) | Loss 11.8339(13.8094) | Error 0.5708(0.5506) Steps 610(618.20) | Grad Norm 34.3969(17.6437) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 81.7749, Epoch Time 660.4794(599.3345), Bit/dim 4.7411(best: 4.2652), Xent 1.8490, Loss 5.6656, Error 0.6711(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1030 | Time 46.5336(43.1452) | Bit/dim 4.4154(4.3523) | Xent 1.8869(1.5745) | Loss 12.3033(14.1475) | Error 0.6586(0.5672) Steps 598(617.35) | Grad Norm 21.0652(23.0144) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 42.9180(43.1740) | Bit/dim 4.3812(4.3706) | Xent 1.6615(1.6029) | Loss 12.0619(13.6384) | Error 0.6086(0.5795) Steps 640(618.97) | Grad Norm 8.6910(20.9075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 80.8869, Epoch Time 658.3328(601.1044), Bit/dim 4.3542(best: 4.2652), Xent 1.5560, Loss 5.1322, Error 0.5649(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1050 | Time 42.4300(42.9397) | Bit/dim 4.2908(4.3558) | Xent 1.5730(1.5976) | Loss 11.8355(13.7415) | Error 0.5761(0.5778) Steps 640(616.44) | Grad Norm 6.0570(17.3820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 77.2084, Epoch Time 640.6425(602.2906), Bit/dim 4.2860(best: 4.2652), Xent 1.4703, Loss 5.0212, Error 0.5303(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1060 | Time 44.8207(43.0945) | Bit/dim 4.2496(4.3337) | Xent 1.5052(1.5786) | Loss 11.7107(13.8147) | Error 0.5500(0.5716) Steps 592(614.52) | Grad Norm 6.4432(14.8640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 80.5943, Epoch Time 664.0946(604.1447), Bit/dim 4.2465(best: 4.2652), Xent 1.4342, Loss 4.9636, Error 0.5137(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1070 | Time 42.3528(43.0437) | Bit/dim 4.2455(4.3114) | Xent 1.4850(1.5573) | Loss 11.6753(13.9368) | Error 0.5289(0.5632) Steps 628(614.76) | Grad Norm 5.5824(12.6892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 81.8649, Epoch Time 666.0382(606.0015), Bit/dim 4.2203(best: 4.2465), Xent 1.3988, Loss 4.9197, Error 0.5080(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1080 | Time 43.4745(43.3277) | Bit/dim 4.2160(4.2883) | Xent 1.4726(1.5318) | Loss 36.2509(14.0770) | Error 0.5369(0.5545) Steps 610(615.09) | Grad Norm 9.1253(10.5777) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 45.8513(43.5101) | Bit/dim 4.2140(4.2682) | Xent 1.4551(1.5074) | Loss 11.6863(13.4208) | Error 0.5292(0.5462) Steps 628(617.80) | Grad Norm 4.4939(9.8514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 84.4642, Epoch Time 671.6502(607.9709), Bit/dim 4.2055(best: 4.2203), Xent 1.3709, Loss 4.8909, Error 0.4974(best: 0.5080)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1100 | Time 42.5050(43.5508) | Bit/dim 4.2036(4.2518) | Xent 1.4453(1.4881) | Loss 11.3246(13.5224) | Error 0.5211(0.5391) Steps 592(619.24) | Grad Norm 22.7595(9.8550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 81.5834, Epoch Time 662.8725(609.6180), Bit/dim 4.1904(best: 4.2055), Xent 1.3519, Loss 4.8663, Error 0.4886(best: 0.4974)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1110 | Time 46.9030(43.6154) | Bit/dim 4.1821(4.2371) | Xent 1.4267(1.4676) | Loss 11.5344(13.6615) | Error 0.5325(0.5316) Steps 652(620.58) | Grad Norm 16.7766(10.3160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 83.1715, Epoch Time 659.9234(611.1272), Bit/dim 4.1789(best: 4.1904), Xent 1.3523, Loss 4.8551, Error 0.4895(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1120 | Time 44.0798(43.3923) | Bit/dim 4.1853(4.2221) | Xent 1.4111(1.4563) | Loss 11.5802(13.8100) | Error 0.5108(0.5274) Steps 634(623.85) | Grad Norm 11.4940(11.0060) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 45.8714(43.4213) | Bit/dim 4.1827(4.2150) | Xent 1.4169(1.4579) | Loss 11.5972(13.2058) | Error 0.5072(0.5267) Steps 640(623.58) | Grad Norm 10.3652(12.9543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 83.7054, Epoch Time 663.4734(612.6975), Bit/dim 4.1822(best: 4.1789), Xent 1.3766, Loss 4.8704, Error 0.5001(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1140 | Time 44.4860(43.3348) | Bit/dim 4.1651(4.2034) | Xent 1.4018(1.4468) | Loss 11.4998(13.3477) | Error 0.5056(0.5230) Steps 628(625.79) | Grad Norm 7.6105(12.3861) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 81.5484, Epoch Time 663.3087(614.2159), Bit/dim 4.1632(best: 4.1789), Xent 1.3240, Loss 4.8252, Error 0.4764(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1150 | Time 43.6495(43.2553) | Bit/dim 4.1864(4.1931) | Xent 1.3857(1.4313) | Loss 11.5339(13.5043) | Error 0.5053(0.5181) Steps 634(626.29) | Grad Norm 12.8756(11.9731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 82.7489, Epoch Time 655.0064(615.4396), Bit/dim 4.1588(best: 4.1632), Xent 1.3570, Loss 4.8373, Error 0.4912(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1160 | Time 42.7958(43.1942) | Bit/dim 4.1815(4.1874) | Xent 1.3779(1.4244) | Loss 11.4797(13.6821) | Error 0.5056(0.5157) Steps 634(628.19) | Grad Norm 16.5661(14.1036) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 40.6042(42.9532) | Bit/dim 4.1332(4.1773) | Xent 1.3771(1.4189) | Loss 11.2435(13.0878) | Error 0.4958(0.5139) Steps 604(628.38) | Grad Norm 8.6728(14.6582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 82.5466, Epoch Time 649.2600(616.4542), Bit/dim 4.1488(best: 4.1588), Xent 1.3420, Loss 4.8198, Error 0.4807(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1180 | Time 41.9710(42.8824) | Bit/dim 4.1571(4.1691) | Xent 1.4095(1.4106) | Loss 11.2913(13.2088) | Error 0.4944(0.5115) Steps 622(626.57) | Grad Norm 23.1013(15.0181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 81.8793, Epoch Time 652.2901(617.5293), Bit/dim 4.1496(best: 4.1488), Xent 1.3753, Loss 4.8373, Error 0.5034(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1190 | Time 41.3263(42.6112) | Bit/dim 4.1126(4.1605) | Xent 1.3718(1.4065) | Loss 11.2866(13.3687) | Error 0.5025(0.5101) Steps 652(626.62) | Grad Norm 11.1123(15.5360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 81.8931, Epoch Time 648.6265(618.4622), Bit/dim 4.1297(best: 4.1488), Xent 1.3019, Loss 4.7807, Error 0.4732(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1200 | Time 41.9128(42.7324) | Bit/dim 4.1113(4.1525) | Xent 1.3392(1.3932) | Loss 11.3491(13.5101) | Error 0.4739(0.5050) Steps 634(626.97) | Grad Norm 5.0813(13.4218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 81.3472, Epoch Time 644.6041(619.2465), Bit/dim 4.1143(best: 4.1297), Xent 1.2798, Loss 4.7542, Error 0.4633(best: 0.4732)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1210 | Time 41.5777(42.5002) | Bit/dim 4.1077(4.1423) | Xent 1.3536(1.3799) | Loss 35.7452(13.6380) | Error 0.4814(0.4996) Steps 574(625.34) | Grad Norm 8.0266(11.8620) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 42.1216(42.4193) | Bit/dim 4.1442(4.1391) | Xent 1.4655(1.3867) | Loss 11.5378(13.0359) | Error 0.5347(0.5016) Steps 628(623.75) | Grad Norm 29.6375(14.9737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 82.7445, Epoch Time 649.1945(620.1449), Bit/dim 4.1324(best: 4.1143), Xent 1.3360, Loss 4.8004, Error 0.4773(best: 0.4633)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1230 | Time 45.3036(42.5880) | Bit/dim 4.1147(4.1373) | Xent 1.3196(1.3820) | Loss 11.2995(13.1773) | Error 0.4761(0.5000) Steps 640(624.57) | Grad Norm 8.4955(14.7175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 82.0604, Epoch Time 653.6310(621.1495), Bit/dim 4.1042(best: 4.1143), Xent 1.2901, Loss 4.7492, Error 0.4672(best: 0.4633)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1240 | Time 43.3837(42.6481) | Bit/dim 4.0857(4.1283) | Xent 1.3218(1.3716) | Loss 11.1667(13.3355) | Error 0.4825(0.4961) Steps 628(622.89) | Grad Norm 7.3288(12.6892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 80.6209, Epoch Time 646.6788(621.9154), Bit/dim 4.0886(best: 4.1042), Xent 1.2766, Loss 4.7269, Error 0.4624(best: 0.4633)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1250 | Time 43.2252(42.5802) | Bit/dim 4.0780(4.1194) | Xent 1.2989(1.3580) | Loss 11.0828(13.4892) | Error 0.4764(0.4916) Steps 598(621.24) | Grad Norm 11.9088(12.2445) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 42.8361(42.5174) | Bit/dim 4.1018(4.1105) | Xent 1.3323(1.3472) | Loss 11.0555(12.8705) | Error 0.4850(0.4874) Steps 598(618.20) | Grad Norm 20.5555(11.7929) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 81.9295, Epoch Time 649.9788(622.7573), Bit/dim 4.1050(best: 4.0886), Xent 1.4014, Loss 4.8057, Error 0.5096(best: 0.4624)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1270 | Time 42.5827(42.5965) | Bit/dim 4.0999(4.1077) | Xent 1.4019(1.3564) | Loss 11.3181(13.0329) | Error 0.4964(0.4899) Steps 604(616.61) | Grad Norm 17.6786(14.3482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 81.5162, Epoch Time 650.9545(623.6032), Bit/dim 4.0824(best: 4.0886), Xent 1.2596, Loss 4.7122, Error 0.4572(best: 0.4624)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1280 | Time 42.9033(42.4588) | Bit/dim 4.0855(4.1010) | Xent 1.2902(1.3520) | Loss 11.1119(13.1772) | Error 0.4686(0.4889) Steps 628(617.47) | Grad Norm 5.0009(13.2018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 79.6691, Epoch Time 646.0862(624.2777), Bit/dim 4.0736(best: 4.0824), Xent 1.2526, Loss 4.6999, Error 0.4517(best: 0.4572)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1290 | Time 43.1441(42.4640) | Bit/dim 4.0611(4.0924) | Xent 1.3026(1.3410) | Loss 11.0434(13.3027) | Error 0.4708(0.4845) Steps 604(616.85) | Grad Norm 5.8097(12.1413) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 43.0898(42.6247) | Bit/dim 4.0658(4.0857) | Xent 1.3350(1.3296) | Loss 11.1638(12.7301) | Error 0.4922(0.4806) Steps 652(617.03) | Grad Norm 19.2472(11.3888) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 81.3660, Epoch Time 657.1415(625.2636), Bit/dim 4.0732(best: 4.0736), Xent 1.2857, Loss 4.7160, Error 0.4594(best: 0.4517)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1310 | Time 41.5746(42.7475) | Bit/dim 4.0519(4.0795) | Xent 1.2927(1.3240) | Loss 10.9020(12.8818) | Error 0.4731(0.4786) Steps 610(616.74) | Grad Norm 6.7098(12.6074) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 84.0184, Epoch Time 661.4714(626.3498), Bit/dim 4.0549(best: 4.0732), Xent 1.2375, Loss 4.6737, Error 0.4460(best: 0.4517)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1320 | Time 40.9628(42.5237) | Bit/dim 4.0658(4.0717) | Xent 1.3023(1.3160) | Loss 11.1308(13.0168) | Error 0.4750(0.4761) Steps 634(618.54) | Grad Norm 11.5943(11.7231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 83.1538, Epoch Time 642.9855(626.8489), Bit/dim 4.0428(best: 4.0549), Xent 1.2258, Loss 4.6558, Error 0.4418(best: 0.4460)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1330 | Time 40.3218(42.3772) | Bit/dim 4.0458(4.0644) | Xent 1.3321(1.3109) | Loss 11.1728(13.1522) | Error 0.4708(0.4735) Steps 646(618.46) | Grad Norm 13.4150(11.5988) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 82.0652, Epoch Time 646.3331(627.4334), Bit/dim 4.0460(best: 4.0428), Xent 1.2872, Loss 4.6895, Error 0.4622(best: 0.4418)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1340 | Time 42.0651(42.3414) | Bit/dim 4.0488(4.0599) | Xent 1.3419(1.3144) | Loss 36.3021(13.3731) | Error 0.4850(0.4752) Steps 622(619.72) | Grad Norm 18.2662(13.9704) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 39.6968(42.2002) | Bit/dim 4.0485(4.0593) | Xent 1.2826(1.3264) | Loss 10.9050(12.7910) | Error 0.4681(0.4798) Steps 622(620.24) | Grad Norm 13.0501(15.2139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 84.9331, Epoch Time 644.1933(627.9362), Bit/dim 4.0418(best: 4.0428), Xent 1.2555, Loss 4.6696, Error 0.4562(best: 0.4418)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1360 | Time 43.7508(42.2193) | Bit/dim 4.0251(4.0543) | Xent 1.2718(1.3150) | Loss 11.0097(12.9383) | Error 0.4578(0.4753) Steps 640(620.63) | Grad Norm 7.0524(13.5749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 81.4400, Epoch Time 645.5376(628.4643), Bit/dim 4.0273(best: 4.0418), Xent 1.2360, Loss 4.6453, Error 0.4473(best: 0.4418)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1370 | Time 42.8423(42.2395) | Bit/dim 4.0259(4.0471) | Xent 1.2678(1.3039) | Loss 10.9611(13.0759) | Error 0.4536(0.4713) Steps 640(617.80) | Grad Norm 8.4810(12.7498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 81.7824, Epoch Time 653.4776(629.2147), Bit/dim 4.0287(best: 4.0273), Xent 1.2278, Loss 4.6426, Error 0.4454(best: 0.4418)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1380 | Time 43.0922(42.2938) | Bit/dim 4.0246(4.0404) | Xent 1.3023(1.2976) | Loss 10.9738(13.2526) | Error 0.4681(0.4693) Steps 592(617.05) | Grad Norm 14.3254(12.8971) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 43.0812(42.2792) | Bit/dim 4.0011(4.0349) | Xent 1.2833(1.2903) | Loss 10.9763(12.6441) | Error 0.4628(0.4668) Steps 634(619.82) | Grad Norm 9.1860(12.5585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 79.9984, Epoch Time 645.2397(629.6954), Bit/dim 4.0160(best: 4.0273), Xent 1.2203, Loss 4.6261, Error 0.4410(best: 0.4418)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1400 | Time 41.4793(42.1800) | Bit/dim 4.0174(4.0282) | Xent 1.2784(1.2827) | Loss 10.8278(12.7647) | Error 0.4597(0.4636) Steps 622(622.10) | Grad Norm 9.3741(11.9287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 82.0605, Epoch Time 645.1240(630.1583), Bit/dim 4.0106(best: 4.0160), Xent 1.2238, Loss 4.6225, Error 0.4464(best: 0.4410)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1410 | Time 40.7259(42.2258) | Bit/dim 4.0138(4.0232) | Xent 1.2708(1.2780) | Loss 10.7381(12.9118) | Error 0.4575(0.4614) Steps 622(620.27) | Grad Norm 13.5015(12.7798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 82.4684, Epoch Time 653.9587(630.8723), Bit/dim 4.0040(best: 4.0106), Xent 1.1965, Loss 4.6023, Error 0.4352(best: 0.4410)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1420 | Time 41.8594(42.3730) | Bit/dim 4.0108(4.0185) | Xent 1.2576(1.2756) | Loss 10.9292(13.0763) | Error 0.4550(0.4599) Steps 640(617.68) | Grad Norm 8.6404(12.6491) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 41.7639(42.7468) | Bit/dim 3.9979(4.0136) | Xent 1.2887(1.2684) | Loss 10.9841(12.4978) | Error 0.4569(0.4570) Steps 622(615.90) | Grad Norm 10.5234(12.5078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 81.2829, Epoch Time 665.4212(631.9087), Bit/dim 4.0042(best: 4.0040), Xent 1.1953, Loss 4.6018, Error 0.4365(best: 0.4352)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1440 | Time 41.0849(42.7840) | Bit/dim 4.0019(4.0086) | Xent 1.2429(1.2620) | Loss 10.8173(12.6361) | Error 0.4511(0.4545) Steps 598(617.46) | Grad Norm 16.3794(12.5933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 82.9597, Epoch Time 659.3154(632.7309), Bit/dim 4.0009(best: 4.0040), Xent 1.2199, Loss 4.6108, Error 0.4404(best: 0.4352)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1450 | Time 40.1995(42.7281) | Bit/dim 3.9823(4.0038) | Xent 1.2460(1.2572) | Loss 10.7703(12.7828) | Error 0.4428(0.4526) Steps 610(613.49) | Grad Norm 19.4174(13.0349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 84.7698, Epoch Time 647.1589(633.1638), Bit/dim 3.9830(best: 4.0009), Xent 1.1981, Loss 4.5821, Error 0.4303(best: 0.4352)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1460 | Time 41.1784(42.5378) | Bit/dim 4.0088(4.0010) | Xent 1.2550(1.2559) | Loss 10.8781(12.9449) | Error 0.4489(0.4521) Steps 616(612.66) | Grad Norm 16.8544(12.7948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 82.0573, Epoch Time 647.1915(633.5846), Bit/dim 3.9898(best: 3.9830), Xent 1.2817, Loss 4.6306, Error 0.4658(best: 0.4303)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1470 | Time 40.5183(42.3910) | Bit/dim 3.9946(3.9973) | Xent 1.3049(1.2551) | Loss 35.1221(13.1171) | Error 0.4597(0.4516) Steps 592(611.64) | Grad Norm 30.2537(13.6479) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 44.9983(42.5592) | Bit/dim 3.9787(3.9937) | Xent 1.2675(1.2651) | Loss 10.8406(12.5347) | Error 0.4672(0.4557) Steps 562(610.84) | Grad Norm 19.3608(14.3032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 79.9365, Epoch Time 652.1495(634.1416), Bit/dim 3.9824(best: 3.9830), Xent 1.2483, Loss 4.6066, Error 0.4501(best: 0.4303)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1490 | Time 40.1427(42.3720) | Bit/dim 3.9950(3.9919) | Xent 1.2690(1.2665) | Loss 10.9547(12.6933) | Error 0.4628(0.4571) Steps 628(609.22) | Grad Norm 17.6014(15.0424) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 81.2520, Epoch Time 643.8954(634.4342), Bit/dim 3.9759(best: 3.9824), Xent 1.1769, Loss 4.5644, Error 0.4256(best: 0.4303)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1500 | Time 39.6566(42.1222) | Bit/dim 3.9794(3.9879) | Xent 1.2322(1.2593) | Loss 10.8976(12.8498) | Error 0.4489(0.4544) Steps 592(609.13) | Grad Norm 5.1265(14.0349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 85.5958, Epoch Time 638.9458(634.5695), Bit/dim 3.9682(best: 3.9759), Xent 1.1648, Loss 4.5506, Error 0.4232(best: 0.4256)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1510 | Time 40.4292(41.9616) | Bit/dim 3.9789(3.9838) | Xent 1.2076(1.2470) | Loss 10.5741(13.0003) | Error 0.4333(0.4486) Steps 586(608.29) | Grad Norm 7.6505(13.3196) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 42.5782(42.1515) | Bit/dim 3.9640(3.9784) | Xent 1.2383(1.2423) | Loss 10.7798(12.4162) | Error 0.4511(0.4468) Steps 604(607.38) | Grad Norm 15.7165(12.8293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 80.9474, Epoch Time 647.0180(634.9430), Bit/dim 3.9607(best: 3.9682), Xent 1.1625, Loss 4.5420, Error 0.4222(best: 0.4232)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1530 | Time 40.4466(42.1851) | Bit/dim 3.9579(3.9740) | Xent 1.2008(1.2408) | Loss 10.7105(12.5391) | Error 0.4261(0.4461) Steps 610(605.93) | Grad Norm 7.0152(13.7768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 81.0712, Epoch Time 657.6562(635.6244), Bit/dim 3.9611(best: 3.9607), Xent 1.1999, Loss 4.5610, Error 0.4368(best: 0.4222)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1540 | Time 42.8250(42.5553) | Bit/dim 3.9711(3.9708) | Xent 1.1928(1.2372) | Loss 10.6023(12.6858) | Error 0.4292(0.4448) Steps 598(604.91) | Grad Norm 14.4692(13.5001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 83.3937, Epoch Time 659.3679(636.3367), Bit/dim 3.9567(best: 3.9607), Xent 1.1654, Loss 4.5394, Error 0.4260(best: 0.4222)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1550 | Time 41.3160(42.5203) | Bit/dim 3.9576(3.9660) | Xent 1.2397(1.2286) | Loss 10.8982(12.8653) | Error 0.4383(0.4424) Steps 634(608.41) | Grad Norm 17.7601(12.8615) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 43.6846(42.5288) | Bit/dim 3.9520(3.9638) | Xent 1.2139(1.2297) | Loss 10.8027(12.3137) | Error 0.4367(0.4427) Steps 592(604.55) | Grad Norm 13.4857(13.4603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 82.6836, Epoch Time 650.7859(636.7702), Bit/dim 3.9627(best: 3.9567), Xent 1.1777, Loss 4.5516, Error 0.4208(best: 0.4222)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1570 | Time 39.9150(42.3440) | Bit/dim 3.9551(3.9609) | Xent 1.2347(1.2276) | Loss 10.6620(12.4667) | Error 0.4500(0.4416) Steps 604(609.22) | Grad Norm 18.9160(13.9318) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 81.2916, Epoch Time 639.6545(636.8567), Bit/dim 3.9441(best: 3.9567), Xent 1.1469, Loss 4.5175, Error 0.4181(best: 0.4208)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1580 | Time 43.9681(42.3140) | Bit/dim 3.9642(3.9565) | Xent 1.1615(1.2189) | Loss 10.7987(12.6327) | Error 0.4158(0.4384) Steps 628(612.86) | Grad Norm 14.5651(13.1495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 80.5505, Epoch Time 649.0388(637.2222), Bit/dim 3.9373(best: 3.9441), Xent 1.1749, Loss 4.5247, Error 0.4290(best: 0.4181)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1590 | Time 43.1362(42.2682) | Bit/dim 3.9312(3.9514) | Xent 1.1952(1.2145) | Loss 10.7068(12.7662) | Error 0.4375(0.4378) Steps 598(610.04) | Grad Norm 7.2379(12.6142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 79.9267, Epoch Time 653.1599(637.7003), Bit/dim 3.9365(best: 3.9373), Xent 1.1661, Loss 4.5195, Error 0.4213(best: 0.4181)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1600 | Time 40.0878(42.3892) | Bit/dim 3.9325(3.9477) | Xent 1.1907(1.2041) | Loss 34.6678(12.9363) | Error 0.4133(0.4336) Steps 622(610.65) | Grad Norm 13.7703(11.9234) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 45.9435(42.6726) | Bit/dim 3.9269(3.9430) | Xent 1.1587(1.1972) | Loss 10.5571(12.3240) | Error 0.4197(0.4312) Steps 640(610.81) | Grad Norm 10.4022(11.2942) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 80.4926, Epoch Time 657.8522(638.3048), Bit/dim 3.9278(best: 3.9365), Xent 1.1255, Loss 4.4906, Error 0.4094(best: 0.4181)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1620 | Time 44.3285(42.6520) | Bit/dim 3.9082(3.9397) | Xent 1.2288(1.1970) | Loss 10.6824(12.4837) | Error 0.4422(0.4312) Steps 640(613.77) | Grad Norm 12.0156(12.0114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 80.9247, Epoch Time 647.7879(638.5893), Bit/dim 3.9257(best: 3.9278), Xent 1.1487, Loss 4.5001, Error 0.4166(best: 0.4094)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1630 | Time 40.7011(42.4611) | Bit/dim 3.9367(3.9377) | Xent 1.1978(1.1948) | Loss 10.6159(12.6389) | Error 0.4383(0.4306) Steps 598(610.61) | Grad Norm 19.2100(12.6852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 84.4187, Epoch Time 659.4529(639.2152), Bit/dim 3.9237(best: 3.9257), Xent 1.1196, Loss 4.4835, Error 0.4033(best: 0.4094)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1640 | Time 42.0104(42.7349) | Bit/dim 3.9204(3.9337) | Xent 1.1558(1.1896) | Loss 10.6124(12.8006) | Error 0.4117(0.4281) Steps 622(609.89) | Grad Norm 15.1952(12.2727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 80.7395, Epoch Time 654.1841(639.6643), Bit/dim 3.9258(best: 3.9237), Xent 1.1723, Loss 4.5119, Error 0.4230(best: 0.4033)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1660 | Time 42.5205(42.8835) | Bit/dim 3.9244(3.9278) | Xent 1.1614(1.1878) | Loss 10.5419(12.3595) | Error 0.4322(0.4271) Steps 598(610.14) | Grad Norm 8.8901(12.6938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 78.9002, Epoch Time 652.2381(640.0415), Bit/dim 3.9165(best: 3.9237), Xent 1.1142, Loss 4.4736, Error 0.4025(best: 0.4033)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1670 | Time 42.3662(42.7114) | Bit/dim 3.9170(3.9243) | Xent 1.1588(1.1799) | Loss 10.5977(12.4930) | Error 0.4175(0.4242) Steps 592(608.87) | Grad Norm 10.9552(11.8078) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 80.3853, Epoch Time 645.2853(640.1988), Bit/dim 3.9070(best: 3.9165), Xent 1.1138, Loss 4.4639, Error 0.4040(best: 0.4025)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1680 | Time 41.9247(42.5068) | Bit/dim 3.9134(3.9202) | Xent 1.1753(1.1708) | Loss 10.5300(12.6435) | Error 0.4231(0.4213) Steps 616(610.07) | Grad Norm 16.0889(11.4652) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 42.2983(42.8321) | Bit/dim 3.9136(3.9184) | Xent 1.2369(1.1710) | Loss 10.4760(12.0955) | Error 0.4392(0.4205) Steps 586(609.03) | Grad Norm 33.3534(12.7026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 81.3405, Epoch Time 662.4733(640.8671), Bit/dim 3.9179(best: 3.9070), Xent 1.2533, Loss 4.5446, Error 0.4387(best: 0.4025)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1700 | Time 40.8362(42.8415) | Bit/dim 3.9284(3.9173) | Xent 1.1547(1.1758) | Loss 10.5199(12.2516) | Error 0.4114(0.4220) Steps 628(608.70) | Grad Norm 7.9396(12.9101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 82.9669, Epoch Time 653.7265(641.2529), Bit/dim 3.9057(best: 3.9070), Xent 1.0953, Loss 4.4534, Error 0.3945(best: 0.4025)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1710 | Time 41.1373(42.6455) | Bit/dim 3.9010(3.9147) | Xent 1.1549(1.1671) | Loss 10.4479(12.3709) | Error 0.4197(0.4187) Steps 598(608.11) | Grad Norm 7.9565(11.8616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 80.8227, Epoch Time 637.7815(641.1487), Bit/dim 3.9049(best: 3.9057), Xent 1.0842, Loss 4.4470, Error 0.3889(best: 0.3945)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1720 | Time 44.2090(42.2852) | Bit/dim 3.9036(3.9115) | Xent 1.1276(1.1565) | Loss 10.6397(12.5370) | Error 0.3969(0.4148) Steps 598(608.03) | Grad Norm 10.6149(11.6073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 81.1925, Epoch Time 648.4423(641.3675), Bit/dim 3.9140(best: 3.9049), Xent 1.1459, Loss 4.4870, Error 0.4063(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1730 | Time 40.9227(42.2987) | Bit/dim 3.9297(3.9131) | Xent 1.2408(1.1743) | Loss 34.4325(12.7503) | Error 0.4400(0.4199) Steps 616(606.92) | Grad Norm 17.8022(14.1333) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 44.1472(42.3024) | Bit/dim 3.9026(3.9132) | Xent 1.1323(1.1726) | Loss 10.4807(12.1742) | Error 0.4142(0.4199) Steps 598(607.74) | Grad Norm 13.6545(13.9952) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 78.8724, Epoch Time 644.3398(641.4567), Bit/dim 3.9064(best: 3.9049), Xent 1.0959, Loss 4.4543, Error 0.3964(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1750 | Time 42.9737(42.4472) | Bit/dim 3.9144(3.9108) | Xent 1.1299(1.1641) | Loss 10.4521(12.3081) | Error 0.4131(0.4174) Steps 610(607.94) | Grad Norm 8.6444(13.2855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 82.6769, Epoch Time 652.5792(641.7904), Bit/dim 3.9046(best: 3.9049), Xent 1.0871, Loss 4.4481, Error 0.3898(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1760 | Time 40.4008(42.2434) | Bit/dim 3.9041(3.9088) | Xent 1.1222(1.1530) | Loss 10.4511(12.4636) | Error 0.4033(0.4132) Steps 574(604.72) | Grad Norm 11.2453(13.0666) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 80.3423, Epoch Time 640.2910(641.7454), Bit/dim 3.8923(best: 3.9046), Xent 1.1146, Loss 4.4496, Error 0.3910(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1770 | Time 43.5570(42.2785) | Bit/dim 3.8728(3.9046) | Xent 1.0751(1.1439) | Loss 10.3505(12.6222) | Error 0.3897(0.4098) Steps 580(605.29) | Grad Norm 8.2364(13.0624) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 44.2662(42.4873) | Bit/dim 3.9146(3.9062) | Xent 1.3967(1.1856) | Loss 10.8364(12.1230) | Error 0.4892(0.4225) Steps 604(605.47) | Grad Norm 22.9451(16.3740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 81.1411, Epoch Time 657.1026(642.2061), Bit/dim 3.9335(best: 3.8923), Xent 1.1836, Loss 4.5253, Error 0.4262(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1790 | Time 44.0742(42.3287) | Bit/dim 3.9215(3.9120) | Xent 1.1745(1.1956) | Loss 10.5572(12.3066) | Error 0.4236(0.4257) Steps 604(606.67) | Grad Norm 10.2925(15.1663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 81.5708, Epoch Time 645.0144(642.2903), Bit/dim 3.9039(best: 3.8923), Xent 1.0975, Loss 4.4527, Error 0.3940(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1800 | Time 41.6410(42.3136) | Bit/dim 3.8756(3.9092) | Xent 1.1323(1.1812) | Loss 10.4613(12.4720) | Error 0.4097(0.4216) Steps 634(605.25) | Grad Norm 4.1750(12.9376) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 79.5293, Epoch Time 645.4350(642.3847), Bit/dim 3.8894(best: 3.8923), Xent 1.0697, Loss 4.4243, Error 0.3858(best: 0.3889)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1810 | Time 42.2172(42.3848) | Bit/dim 3.8703(3.9035) | Xent 1.0841(1.1630) | Loss 10.3326(12.6102) | Error 0.3864(0.4155) Steps 580(602.23) | Grad Norm 5.1999(11.8893) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 45.0048(42.3288) | Bit/dim 3.8856(3.8981) | Xent 1.0647(1.1412) | Loss 10.4061(12.0284) | Error 0.3800(0.4084) Steps 628(604.32) | Grad Norm 2.7516(10.2881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 79.0835, Epoch Time 645.0833(642.4656), Bit/dim 3.8773(best: 3.8894), Xent 1.0457, Loss 4.4002, Error 0.3739(best: 0.3858)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1830 | Time 43.3892(42.3058) | Bit/dim 3.8805(3.8944) | Xent 1.1076(1.1338) | Loss 10.5508(12.1589) | Error 0.3981(0.4061) Steps 616(605.41) | Grad Norm 12.9810(11.0422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 79.3635, Epoch Time 641.6900(642.4424), Bit/dim 3.8753(best: 3.8773), Xent 1.0705, Loss 4.4105, Error 0.3843(best: 0.3739)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1840 | Time 42.3004(42.3628) | Bit/dim 3.8909(3.8888) | Xent 1.0471(1.1250) | Loss 10.4006(12.3102) | Error 0.3756(0.4036) Steps 586(606.63) | Grad Norm 12.8817(11.1529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 83.3649, Epoch Time 655.9144(642.8465), Bit/dim 3.8860(best: 3.8753), Xent 1.0502, Loss 4.4111, Error 0.3759(best: 0.3739)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1850 | Time 41.3226(42.3715) | Bit/dim 3.8664(3.8855) | Xent 1.0797(1.1133) | Loss 10.3816(12.4759) | Error 0.3800(0.3989) Steps 628(608.45) | Grad Norm 18.0595(11.5569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 80.6464, Epoch Time 650.4157(643.0736), Bit/dim 3.8828(best: 3.8753), Xent 1.1476, Loss 4.4566, Error 0.4174(best: 0.3739)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1860 | Time 42.5336(42.4532) | Bit/dim 3.8956(3.8828) | Xent 1.1691(1.1091) | Loss 34.7906(12.6554) | Error 0.4200(0.3971) Steps 598(604.02) | Grad Norm 23.6525(11.2992) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 41.9868(42.4483) | Bit/dim 3.8727(3.8822) | Xent 1.1300(1.1293) | Loss 10.4973(12.1019) | Error 0.4136(0.4049) Steps 592(602.20) | Grad Norm 11.5384(12.7571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 81.2831, Epoch Time 649.7134(643.2728), Bit/dim 3.8722(best: 3.8753), Xent 1.0655, Loss 4.4050, Error 0.3841(best: 0.3739)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1880 | Time 47.3040(42.6404) | Bit/dim 3.8738(3.8816) | Xent 1.1322(1.1230) | Loss 10.4578(12.2448) | Error 0.4075(0.4027) Steps 574(602.62) | Grad Norm 9.4070(11.5193) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 78.7020, Epoch Time 652.5475(643.5510), Bit/dim 3.8648(best: 3.8722), Xent 1.0313, Loss 4.3805, Error 0.3694(best: 0.3739)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1890 | Time 43.6339(42.6568) | Bit/dim 3.8556(3.8773) | Xent 1.0529(1.1087) | Loss 10.3638(12.4042) | Error 0.3808(0.3979) Steps 592(607.28) | Grad Norm 8.9562(10.3910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 81.6771, Epoch Time 654.3321(643.8745), Bit/dim 3.8625(best: 3.8648), Xent 1.0316, Loss 4.3783, Error 0.3624(best: 0.3694)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1900 | Time 43.5278(42.6774) | Bit/dim 3.8644(3.8747) | Xent 1.0454(1.0968) | Loss 10.4889(12.5712) | Error 0.3750(0.3933) Steps 604(605.44) | Grad Norm 6.6899(10.1138) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 44.6351(42.9244) | Bit/dim 3.8927(3.8726) | Xent 1.1675(1.0957) | Loss 10.3016(11.9907) | Error 0.4136(0.3935) Steps 580(603.59) | Grad Norm 17.0651(10.3985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 78.8271, Epoch Time 660.9814(644.3877), Bit/dim 3.8692(best: 3.8625), Xent 1.0812, Loss 4.4098, Error 0.3940(best: 0.3624)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1920 | Time 41.7841(42.9298) | Bit/dim 3.8906(3.8719) | Xent 1.0955(1.0922) | Loss 10.5558(12.0994) | Error 0.3878(0.3919) Steps 592(603.77) | Grad Norm 15.8818(11.9115) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 82.4757, Epoch Time 660.8779(644.8824), Bit/dim 3.8656(best: 3.8625), Xent 1.0519, Loss 4.3916, Error 0.3823(best: 0.3624)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1930 | Time 42.3431(43.0262) | Bit/dim 3.8683(3.8711) | Xent 1.0475(1.0840) | Loss 10.3048(12.2558) | Error 0.3708(0.3889) Steps 622(604.21) | Grad Norm 8.2703(11.6427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 84.4949, Epoch Time 656.2177(645.2225), Bit/dim 3.8585(best: 3.8625), Xent 0.9967, Loss 4.3568, Error 0.3535(best: 0.3624)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1940 | Time 41.7859(42.7250) | Bit/dim 3.8575(3.8672) | Xent 1.0364(1.0735) | Loss 10.3295(12.4181) | Error 0.3764(0.3850) Steps 610(604.14) | Grad Norm 9.4797(11.3334) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 42.3218(42.6220) | Bit/dim 3.8566(3.8639) | Xent 1.0688(1.0686) | Loss 10.3160(11.8685) | Error 0.3872(0.3832) Steps 592(602.70) | Grad Norm 11.8195(11.3885) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 79.6599, Epoch Time 643.6552(645.1754), Bit/dim 3.8616(best: 3.8585), Xent 1.0076, Loss 4.3654, Error 0.3612(best: 0.3535)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1960 | Time 39.6423(42.4654) | Bit/dim 3.8362(3.8608) | Xent 1.0120(1.0620) | Loss 10.3061(12.0030) | Error 0.3606(0.3805) Steps 622(603.47) | Grad Norm 9.9563(11.5467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 81.3549, Epoch Time 651.3248(645.3599), Bit/dim 3.8535(best: 3.8585), Xent 1.0063, Loss 4.3566, Error 0.3584(best: 0.3535)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1970 | Time 43.2369(42.7944) | Bit/dim 3.8219(3.8572) | Xent 1.0459(1.0539) | Loss 10.3546(12.1635) | Error 0.3644(0.3770) Steps 640(603.06) | Grad Norm 16.2081(11.4771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 80.9983, Epoch Time 658.6417(645.7584), Bit/dim 3.8535(best: 3.8535), Xent 1.0268, Loss 4.3669, Error 0.3648(best: 0.3535)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1980 | Time 43.0770(42.6445) | Bit/dim 3.8336(3.8565) | Xent 1.0407(1.0584) | Loss 10.3034(12.3264) | Error 0.3742(0.3776) Steps 622(604.38) | Grad Norm 11.0865(12.0269) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 81.0475, Epoch Time 647.6751(645.8159), Bit/dim 3.8451(best: 3.8535), Xent 0.9946, Loss 4.3424, Error 0.3496(best: 0.3535)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1990 | Time 39.8097(42.5535) | Bit/dim 3.8598(3.8559) | Xent 1.0433(1.0563) | Loss 33.8924(12.5149) | Error 0.3722(0.3763) Steps 604(607.71) | Grad Norm 6.6257(12.2309) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 42.3852(42.4711) | Bit/dim 3.8656(3.8555) | Xent 1.0470(1.0521) | Loss 10.2770(11.9312) | Error 0.3703(0.3750) Steps 622(608.95) | Grad Norm 15.0494(13.2825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 80.9781, Epoch Time 643.0643(645.7333), Bit/dim 3.8471(best: 3.8451), Xent 0.9871, Loss 4.3407, Error 0.3507(best: 0.3496)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2010 | Time 42.9856(42.4959) | Bit/dim 3.8535(3.8545) | Xent 1.0607(1.0593) | Loss 10.3650(12.1005) | Error 0.3814(0.3765) Steps 598(608.58) | Grad Norm 11.9507(13.6798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 82.2924, Epoch Time 649.5376(645.8475), Bit/dim 3.8674(best: 3.8451), Xent 1.0502, Loss 4.3925, Error 0.3696(best: 0.3496)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2020 | Time 42.5160(42.3196) | Bit/dim 3.8536(3.8554) | Xent 1.0848(1.0625) | Loss 10.3745(12.2917) | Error 0.3811(0.3774) Steps 598(606.81) | Grad Norm 15.0368(15.0086) | Total Time 0.00(0.00)\n",
      "Epoch 0156 | Time 79.8013, Epoch Time 644.5606(645.8088), Bit/dim 3.8435(best: 3.8451), Xent 0.9918, Loss 4.3394, Error 0.3557(best: 0.3496)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2030 | Time 41.4542(42.2432) | Bit/dim 3.8557(3.8531) | Xent 1.0148(1.0540) | Loss 10.3786(12.4737) | Error 0.3589(0.3754) Steps 604(606.25) | Grad Norm 8.5787(13.8124) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 38.7369(42.1645) | Bit/dim 3.8284(3.8501) | Xent 0.9986(1.0408) | Loss 10.1975(11.8854) | Error 0.3647(0.3711) Steps 592(602.12) | Grad Norm 4.9008(12.0007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 83.5283, Epoch Time 645.2021(645.7906), Bit/dim 3.8393(best: 3.8435), Xent 0.9745, Loss 4.3265, Error 0.3475(best: 0.3496)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2050 | Time 39.6132(41.9902) | Bit/dim 3.8418(3.8473) | Xent 1.0090(1.0334) | Loss 10.2420(12.0251) | Error 0.3639(0.3685) Steps 592(602.77) | Grad Norm 11.8967(11.3874) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 82.8631, Epoch Time 642.1433(645.6812), Bit/dim 3.8356(best: 3.8393), Xent 0.9761, Loss 4.3236, Error 0.3488(best: 0.3475)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2060 | Time 43.2111(42.3328) | Bit/dim 3.8279(3.8439) | Xent 1.0365(1.0251) | Loss 10.4006(12.1787) | Error 0.3717(0.3655) Steps 610(604.60) | Grad Norm 9.0661(11.1453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 79.6690, Epoch Time 654.9870(645.9604), Bit/dim 3.8471(best: 3.8356), Xent 1.0411, Loss 4.3676, Error 0.3741(best: 0.3475)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2070 | Time 41.2045(42.2296) | Bit/dim 3.8481(3.8435) | Xent 1.0123(1.0309) | Loss 10.2697(12.3603) | Error 0.3617(0.3679) Steps 598(603.96) | Grad Norm 10.6938(12.4311) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 41.4262(42.2931) | Bit/dim 3.8653(3.8427) | Xent 1.0083(1.0285) | Loss 10.3847(11.8206) | Error 0.3675(0.3684) Steps 652(608.82) | Grad Norm 6.0123(12.1626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 79.6967, Epoch Time 646.0858(645.9642), Bit/dim 3.8380(best: 3.8356), Xent 0.9677, Loss 4.3219, Error 0.3480(best: 0.3475)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2090 | Time 43.6772(42.4668) | Bit/dim 3.8190(3.8401) | Xent 0.9728(1.0186) | Loss 10.3352(11.9480) | Error 0.3456(0.3652) Steps 610(608.76) | Grad Norm 6.1745(10.9646) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 80.6818, Epoch Time 651.9328(646.1432), Bit/dim 3.8287(best: 3.8356), Xent 0.9672, Loss 4.3123, Error 0.3465(best: 0.3475)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2100 | Time 41.8048(42.3152) | Bit/dim 3.8402(3.8383) | Xent 1.0153(1.0129) | Loss 10.2626(12.0850) | Error 0.3667(0.3628) Steps 592(607.31) | Grad Norm 12.9585(11.6019) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 80.7744, Epoch Time 642.5843(646.0365), Bit/dim 3.8320(best: 3.8287), Xent 0.9617, Loss 4.3128, Error 0.3433(best: 0.3465)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2110 | Time 40.2685(42.3096) | Bit/dim 3.8445(3.8368) | Xent 1.2000(1.0161) | Loss 10.4686(12.2863) | Error 0.4136(0.3637) Steps 586(604.99) | Grad Norm 19.4414(11.9701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 79.2572, Epoch Time 650.9957(646.1852), Bit/dim 3.8571(best: 3.8287), Xent 1.0240, Loss 4.3691, Error 0.3651(best: 0.3433)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2120 | Time 41.2104(42.3423) | Bit/dim 3.8474(3.8427) | Xent 1.0768(1.0431) | Loss 34.0801(12.4989) | Error 0.3886(0.3735) Steps 598(606.08) | Grad Norm 14.1156(14.2677) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 42.8400(42.5083) | Bit/dim 3.8295(3.8434) | Xent 0.9962(1.0434) | Loss 10.3718(11.9361) | Error 0.3578(0.3734) Steps 610(608.36) | Grad Norm 3.9234(12.4148) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 80.3059, Epoch Time 655.6279(646.4685), Bit/dim 3.8368(best: 3.8287), Xent 0.9744, Loss 4.3240, Error 0.3470(best: 0.3433)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2140 | Time 41.9777(42.7207) | Bit/dim 3.8282(3.8411) | Xent 0.9884(1.0304) | Loss 10.1914(12.0732) | Error 0.3517(0.3688) Steps 586(608.08) | Grad Norm 6.5467(10.4660) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 80.2289, Epoch Time 655.7549(646.7471), Bit/dim 3.8253(best: 3.8287), Xent 0.9601, Loss 4.3054, Error 0.3428(best: 0.3433)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2150 | Time 41.3896(42.6360) | Bit/dim 3.8356(3.8373) | Xent 1.0255(1.0202) | Loss 10.2403(12.2167) | Error 0.3603(0.3649) Steps 592(608.29) | Grad Norm 14.6370(9.3919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 81.9076, Epoch Time 649.4222(646.8274), Bit/dim 3.8224(best: 3.8253), Xent 0.9592, Loss 4.3020, Error 0.3414(best: 0.3428)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2160 | Time 40.5939(42.4895) | Bit/dim 3.8151(3.8328) | Xent 0.9924(1.0126) | Loss 10.1128(12.3728) | Error 0.3558(0.3629) Steps 604(608.18) | Grad Norm 10.0837(9.6446) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 42.7868(42.3284) | Bit/dim 3.8161(3.8304) | Xent 1.0054(1.0048) | Loss 10.2255(11.7924) | Error 0.3544(0.3594) Steps 640(607.01) | Grad Norm 11.0920(9.6302) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 83.4131, Epoch Time 648.6736(646.8827), Bit/dim 3.8170(best: 3.8224), Xent 0.9605, Loss 4.2972, Error 0.3400(best: 0.3414)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2180 | Time 42.1470(42.3149) | Bit/dim 3.8001(3.8273) | Xent 0.9875(0.9942) | Loss 10.1846(11.9329) | Error 0.3489(0.3554) Steps 604(606.54) | Grad Norm 5.2940(9.2923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 84.6963, Epoch Time 647.0386(646.8874), Bit/dim 3.8219(best: 3.8170), Xent 0.9903, Loss 4.3171, Error 0.3518(best: 0.3400)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2190 | Time 44.0374(42.3356) | Bit/dim 3.8209(3.8263) | Xent 0.9917(1.0061) | Loss 10.1009(12.1264) | Error 0.3606(0.3595) Steps 574(605.75) | Grad Norm 11.5336(11.4311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 81.3433, Epoch Time 652.3808(647.0522), Bit/dim 3.8399(best: 3.8170), Xent 0.9997, Loss 4.3397, Error 0.3595(best: 0.3400)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2200 | Time 40.7792(42.3426) | Bit/dim 3.8339(3.8268) | Xent 0.9827(1.0156) | Loss 10.2814(12.3148) | Error 0.3586(0.3628) Steps 616(606.80) | Grad Norm 12.1436(12.0474) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 45.3218(42.5234) | Bit/dim 3.8361(3.8261) | Xent 0.9838(1.0079) | Loss 10.2037(11.7611) | Error 0.3481(0.3593) Steps 604(608.75) | Grad Norm 10.8567(11.7130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 81.9758, Epoch Time 654.8556(647.2863), Bit/dim 3.8251(best: 3.8170), Xent 0.9473, Loss 4.2987, Error 0.3350(best: 0.3400)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2220 | Time 42.0515(42.5317) | Bit/dim 3.8173(3.8236) | Xent 0.9669(0.9949) | Loss 10.0866(11.8764) | Error 0.3444(0.3549) Steps 580(607.05) | Grad Norm 7.0852(10.7256) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 81.4443, Epoch Time 646.9247(647.2755), Bit/dim 3.8110(best: 3.8170), Xent 0.9497, Loss 4.2859, Error 0.3385(best: 0.3350)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2230 | Time 43.9823(42.4603) | Bit/dim 3.7980(3.8215) | Xent 0.9401(0.9866) | Loss 10.0669(12.0402) | Error 0.3356(0.3523) Steps 592(607.77) | Grad Norm 6.7045(10.6186) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 81.7323, Epoch Time 646.5399(647.2534), Bit/dim 3.8327(best: 3.8110), Xent 1.0966, Loss 4.3810, Error 0.3840(best: 0.3350)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2240 | Time 44.1698(42.4030) | Bit/dim 3.8330(3.8203) | Xent 1.1331(1.0070) | Loss 10.2787(12.2420) | Error 0.4050(0.3593) Steps 604(608.54) | Grad Norm 14.6677(13.0980) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 80.9580, Epoch Time 641.5633(647.0827), Bit/dim 3.8315(best: 3.8110), Xent 0.9861, Loss 4.3246, Error 0.3489(best: 0.3350)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2250 | Time 42.7743(42.1319) | Bit/dim 3.8372(3.8241) | Xent 1.0481(1.0262) | Loss 34.6027(12.4648) | Error 0.3778(0.3667) Steps 574(607.07) | Grad Norm 9.9707(13.3886) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 42.3980(41.8897) | Bit/dim 3.8159(3.8232) | Xent 0.9858(1.0141) | Loss 10.1928(11.8684) | Error 0.3456(0.3626) Steps 628(607.00) | Grad Norm 5.6455(11.6081) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 79.9070, Epoch Time 638.9323(646.8382), Bit/dim 3.8185(best: 3.8110), Xent 0.9393, Loss 4.2881, Error 0.3340(best: 0.3350)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2270 | Time 43.1115(42.1177) | Bit/dim 3.8076(3.8210) | Xent 0.9754(0.9999) | Loss 10.2183(11.9896) | Error 0.3569(0.3571) Steps 628(605.92) | Grad Norm 6.1476(10.5845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 81.8740, Epoch Time 643.9611(646.7519), Bit/dim 3.8046(best: 3.8110), Xent 0.9283, Loss 4.2687, Error 0.3271(best: 0.3340)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2280 | Time 42.3445(42.0219) | Bit/dim 3.8054(3.8166) | Xent 0.9598(0.9860) | Loss 10.1428(12.1304) | Error 0.3447(0.3522) Steps 598(603.00) | Grad Norm 5.6055(9.6001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 79.7145, Epoch Time 648.5306(646.8052), Bit/dim 3.8016(best: 3.8046), Xent 0.9328, Loss 4.2680, Error 0.3339(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2290 | Time 44.6891(42.2013) | Bit/dim 3.7830(3.8127) | Xent 0.9334(0.9791) | Loss 10.2023(12.3104) | Error 0.3358(0.3503) Steps 628(604.29) | Grad Norm 6.7295(9.8114) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 42.0792(42.1804) | Bit/dim 3.8135(3.8103) | Xent 0.9183(0.9689) | Loss 10.1562(11.7384) | Error 0.3381(0.3470) Steps 586(603.48) | Grad Norm 5.2203(9.2653) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 80.8531, Epoch Time 647.9315(646.8390), Bit/dim 3.8103(best: 3.8016), Xent 1.0085, Loss 4.3145, Error 0.3554(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2310 | Time 41.1876(42.1095) | Bit/dim 3.8194(3.8094) | Xent 0.9560(0.9743) | Loss 10.1147(11.8720) | Error 0.3378(0.3479) Steps 592(603.78) | Grad Norm 12.3525(10.3978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 79.3353, Epoch Time 649.6302(646.9228), Bit/dim 3.8046(best: 3.8016), Xent 0.9196, Loss 4.2643, Error 0.3289(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2320 | Time 40.0754(42.2575) | Bit/dim 3.8134(3.8084) | Xent 0.9007(0.9670) | Loss 10.0622(12.0351) | Error 0.3211(0.3453) Steps 610(603.32) | Grad Norm 8.4380(10.4122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 80.9288, Epoch Time 647.3033(646.9342), Bit/dim 3.8024(best: 3.8016), Xent 0.9689, Loss 4.2868, Error 0.3449(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2330 | Time 42.2874(42.2567) | Bit/dim 3.7998(3.8063) | Xent 0.9126(0.9616) | Loss 10.0039(12.2158) | Error 0.3183(0.3429) Steps 586(606.30) | Grad Norm 6.9715(10.2419) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 39.6384(42.1909) | Bit/dim 3.8277(3.8049) | Xent 0.9490(0.9602) | Loss 10.0343(11.6671) | Error 0.3314(0.3420) Steps 586(606.32) | Grad Norm 12.8939(10.3488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 79.9019, Epoch Time 642.2082(646.7924), Bit/dim 3.7964(best: 3.8016), Xent 0.9365, Loss 4.2647, Error 0.3304(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2350 | Time 44.4935(42.3105) | Bit/dim 3.8187(3.8042) | Xent 0.9484(0.9609) | Loss 10.1647(11.8258) | Error 0.3392(0.3427) Steps 628(605.77) | Grad Norm 14.2617(11.2864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 82.0086, Epoch Time 651.9399(646.9468), Bit/dim 3.8037(best: 3.7964), Xent 0.9615, Loss 4.2844, Error 0.3431(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2360 | Time 42.6798(42.4381) | Bit/dim 3.7993(3.8039) | Xent 0.9930(0.9635) | Loss 10.1503(11.9753) | Error 0.3500(0.3434) Steps 616(605.66) | Grad Norm 13.2541(11.7341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 81.7277, Epoch Time 659.8407(647.3336), Bit/dim 3.7965(best: 3.7964), Xent 0.9124, Loss 4.2527, Error 0.3198(best: 0.3271)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2370 | Time 41.9855(42.5518) | Bit/dim 3.7805(3.8014) | Xent 0.8983(0.9545) | Loss 10.1357(12.1359) | Error 0.3181(0.3406) Steps 610(608.14) | Grad Norm 6.8884(11.7360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 82.8468, Epoch Time 658.9309(647.6816), Bit/dim 3.7916(best: 3.7964), Xent 0.9136, Loss 4.2484, Error 0.3242(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2380 | Time 42.8220(42.7444) | Bit/dim 3.8058(3.8005) | Xent 0.8866(0.9478) | Loss 34.5084(12.3309) | Error 0.3239(0.3383) Steps 616(605.51) | Grad Norm 8.7497(11.3748) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 42.3293(42.8060) | Bit/dim 3.8011(3.7994) | Xent 0.9769(0.9493) | Loss 10.1626(11.7582) | Error 0.3517(0.3388) Steps 604(606.59) | Grad Norm 24.8825(12.3258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 82.2836, Epoch Time 657.7743(647.9843), Bit/dim 3.7941(best: 3.7916), Xent 0.9619, Loss 4.2750, Error 0.3452(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2400 | Time 42.0278(42.6302) | Bit/dim 3.7972(3.7995) | Xent 0.9542(0.9530) | Loss 10.1069(11.9281) | Error 0.3467(0.3401) Steps 604(608.74) | Grad Norm 20.1089(13.1438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 81.1642, Epoch Time 643.2486(647.8423), Bit/dim 3.7954(best: 3.7916), Xent 0.9078, Loss 4.2493, Error 0.3221(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2410 | Time 40.4295(42.5738) | Bit/dim 3.8029(3.7994) | Xent 0.9292(0.9475) | Loss 10.1113(12.0865) | Error 0.3261(0.3381) Steps 616(607.98) | Grad Norm 9.9489(12.8614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 82.2649, Epoch Time 649.1834(647.8825), Bit/dim 3.7935(best: 3.7916), Xent 0.9109, Loss 4.2489, Error 0.3234(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2420 | Time 40.0647(42.4027) | Bit/dim 3.7924(3.7989) | Xent 0.8586(0.9433) | Loss 10.1390(12.2845) | Error 0.3069(0.3369) Steps 616(609.49) | Grad Norm 7.7589(12.5932) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 41.6315(42.4495) | Bit/dim 3.8225(3.7995) | Xent 0.9907(0.9508) | Loss 10.3151(11.7369) | Error 0.3742(0.3402) Steps 622(613.39) | Grad Norm 19.5202(13.6924) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 82.2381, Epoch Time 651.1303(647.9799), Bit/dim 3.8123(best: 3.7916), Xent 0.9287, Loss 4.2767, Error 0.3294(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2440 | Time 43.5881(42.4250) | Bit/dim 3.8010(3.7996) | Xent 0.9188(0.9467) | Loss 10.0973(11.8708) | Error 0.3211(0.3387) Steps 622(611.55) | Grad Norm 7.9341(12.6824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 81.8840, Epoch Time 649.8211(648.0352), Bit/dim 3.7946(best: 3.7916), Xent 0.9202, Loss 4.2547, Error 0.3215(best: 0.3198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 2450 | Time 40.2122(42.3900) | Bit/dim 3.8010(3.7977) | Xent 0.9392(0.9405) | Loss 10.0323(12.0379) | Error 0.3339(0.3356) Steps 580(610.96) | Grad Norm 7.2332(11.9133) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 3600 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
