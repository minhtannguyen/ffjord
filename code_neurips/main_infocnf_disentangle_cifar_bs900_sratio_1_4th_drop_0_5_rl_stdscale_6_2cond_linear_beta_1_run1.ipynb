{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_beta.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "# for disentanglement\n",
      "parser.add_argument('--beta', default=0.01, type=float, help='disentanglement weight')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "    \n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    beta_logpz_sup = logpz_sup * (1.0 - args.beta * torch.exp(logpz_sup) / torch.tensor(model.module.y_class).to(logpz_sup))\n",
      "    \n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    beta_logpz_color_sup = logpz_color_sup * (1.0 - args.beta * torch.exp(logpz_color_sup) / torch.tensor(model.module.y_color).to(logpz_color_sup))\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = beta_logpz_sup + beta_logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color,\n",
      "            cond_nn=args.cond_nn)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, beta=1.0, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/infocnf_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_beta_1_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.7513(19.1881) | Bit/dim 25.4501(27.3296) | Xent 2.2876(2.3009) | Xent Color 2.3003(2.3023) | Loss 48.6036(51.8006) | Error 0.8744(0.8883) | Error Color 0.9067(0.8900) |Steps 296(297.56) | Grad Norm 260.8715(274.9994) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.0816(16.0184) | Bit/dim 20.0892(26.0732) | Xent 2.2541(2.2933) | Xent Color 2.2916(2.3002) | Loss 39.1985(49.5896) | Error 0.8656(0.8873) | Error Color 0.8900(0.8912) |Steps 302(300.38) | Grad Norm 216.4227(264.8247) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.3144(13.6725) | Bit/dim 13.8624(23.5605) | Xent 2.2035(2.2753) | Xent Color 2.2717(2.2949) | Loss 27.6638(45.0977) | Error 0.5922(0.8397) | Error Color 0.8156(0.8796) |Steps 314(301.94) | Grad Norm 159.1450(243.6242) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 7.9736(12.1279) | Bit/dim 8.9816(20.2251) | Xent 2.1328(2.2453) | Xent Color 2.2448(2.2853) | Loss 18.9598(39.0966) | Error 0.3878(0.7412) | Error Color 0.8078(0.8611) |Steps 350(311.54) | Grad Norm 95.3419(211.8539) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 8.9024(11.1967) | Bit/dim 6.6902(16.8746) | Xent 2.0629(2.2039) | Xent Color 2.2402(2.2729) | Loss 14.8310(33.0833) | Error 0.3267(0.6288) | Error Color 0.8700(0.8521) |Steps 374(327.06) | Grad Norm 33.4688(171.5146) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.3866(10.6377) | Bit/dim 6.0912(14.1045) | Xent 2.0062(2.1562) | Xent Color 2.2420(2.2657) | Loss 13.8532(28.1207) | Error 0.3844(0.5535) | Error Color 0.9133(0.8631) |Steps 392(343.65) | Grad Norm 25.6622(132.5681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 60.5577, Epoch Time 627.1124(627.1124), Bit/dim 5.7305(best: inf), Xent 1.9353, Xent Color 2.2140. Loss 6.7678, Error 0.2633(best: inf), Error Color 0.8963(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 9.0794(10.1854) | Bit/dim 5.5492(11.9129) | Xent 1.9307(2.1040) | Xent Color 2.1971(2.2552) | Loss 12.7109(24.6393) | Error 0.2989(0.4919) | Error Color 0.8344(0.8703) |Steps 380(353.60) | Grad Norm 17.5834(103.7816) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.6193(9.8005) | Bit/dim 4.9716(10.1621) | Xent 1.8534(2.0447) | Xent Color 2.1751(2.2377) | Loss 11.5543(21.3449) | Error 0.3178(0.4411) | Error Color 0.7600(0.8492) |Steps 356(359.06) | Grad Norm 15.1308(80.2929) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.0372(9.4982) | Bit/dim 4.5770(8.7562) | Xent 1.8010(1.9869) | Xent Color 2.1315(2.2164) | Loss 10.9812(18.6939) | Error 0.2700(0.4034) | Error Color 0.7956(0.8347) |Steps 392(363.86) | Grad Norm 12.7767(63.0845) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.3156(9.2252) | Bit/dim 4.1608(7.6039) | Xent 1.7628(1.9320) | Xent Color 2.1163(2.1916) | Loss 9.9432(16.5175) | Error 0.2778(0.3741) | Error Color 0.7656(0.8211) |Steps 362(367.45) | Grad Norm 9.1396(49.1008) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.8102(9.0945) | Bit/dim 3.7548(6.6410) | Xent 1.7410(1.8848) | Xent Color 2.0682(2.1617) | Loss 9.2797(14.7170) | Error 0.2822(0.3520) | Error Color 0.7044(0.7937) |Steps 350(371.02) | Grad Norm 8.5411(38.4872) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.5334(8.9452) | Bit/dim 3.3577(5.8233) | Xent 1.7650(1.8490) | Xent Color 2.0284(2.1293) | Loss 8.6143(13.1762) | Error 0.3433(0.3404) | Error Color 0.7233(0.7737) |Steps 386(372.33) | Grad Norm 8.2153(30.6802) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.6666(8.8740) | Bit/dim 3.0479(5.1297) | Xent 1.7726(1.8254) | Xent Color 1.9745(2.0951) | Loss 7.8458(11.8822) | Error 0.3378(0.3391) | Error Color 0.7178(0.7614) |Steps 362(374.00) | Grad Norm 6.3500(24.4798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 54.8605, Epoch Time 649.7849(627.7926), Bit/dim 2.9648(best: 5.7305), Xent 1.7667, Xent Color 1.9513. Loss 3.8943, Error 0.2898(best: 0.2633), Error Color 0.7012(best: 0.8963)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.8363(8.9367) | Bit/dim 2.7950(4.5429) | Xent 1.8155(1.8201) | Xent Color 1.9269(2.0567) | Loss 7.5895(11.2260) | Error 0.3667(0.3459) | Error Color 0.7256(0.7511) |Steps 392(380.32) | Grad Norm 5.3275(19.6042) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.3446(9.0048) | Bit/dim 2.6163(4.0555) | Xent 1.8827(1.8273) | Xent Color 1.8894(2.0175) | Loss 7.3178(10.2171) | Error 0.4189(0.3588) | Error Color 0.7222(0.7420) |Steps 410(384.82) | Grad Norm 4.3248(15.6716) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.2703(9.0249) | Bit/dim 2.4745(3.6553) | Xent 1.9184(1.8486) | Xent Color 1.8401(1.9780) | Loss 7.0770(9.4138) | Error 0.4522(0.3804) | Error Color 0.7044(0.7361) |Steps 398(389.46) | Grad Norm 3.2104(12.4930) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.5381(9.0418) | Bit/dim 2.4011(3.3318) | Xent 1.9485(1.8730) | Xent Color 1.7948(1.9384) | Loss 6.8558(8.7640) | Error 0.4867(0.4045) | Error Color 0.6822(0.7279) |Steps 392(392.33) | Grad Norm 2.8824(9.9957) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.4980(9.1000) | Bit/dim 2.3302(3.0737) | Xent 1.9092(1.8890) | Xent Color 1.7796(1.8993) | Loss 6.8160(8.2467) | Error 0.4589(0.4200) | Error Color 0.6833(0.7148) |Steps 386(395.68) | Grad Norm 3.2612(8.0891) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.1954(9.1317) | Bit/dim 2.2708(2.8673) | Xent 1.9079(1.8958) | Xent Color 1.7490(1.8615) | Loss 6.6353(7.8214) | Error 0.4622(0.4319) | Error Color 0.6756(0.7042) |Steps 416(396.75) | Grad Norm 2.7823(6.6712) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 57.4063, Epoch Time 687.3775(629.5801), Bit/dim 2.2422(best: 2.9648), Xent 1.8425, Xent Color 1.7091. Loss 3.1301, Error 0.3568(best: 0.2633), Error Color 0.6810(best: 0.7012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.6048(9.1824) | Bit/dim 2.2408(2.7033) | Xent 1.8508(1.8901) | Xent Color 1.7125(1.8276) | Loss 6.5542(8.0135) | Error 0.4367(0.4340) | Error Color 0.6933(0.6995) |Steps 398(396.96) | Grad Norm 1.8153(5.5547) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.2917(9.1657) | Bit/dim 2.1944(2.5752) | Xent 1.8201(1.8745) | Xent Color 1.6884(1.7972) | Loss 6.3594(7.5947) | Error 0.4067(0.4295) | Error Color 0.6656(0.6967) |Steps 404(395.09) | Grad Norm 2.0686(4.9863) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.3952(9.1713) | Bit/dim 2.1962(2.4763) | Xent 1.7392(1.8494) | Xent Color 1.6728(1.7686) | Loss 6.2157(7.2717) | Error 0.3756(0.4204) | Error Color 0.6744(0.6933) |Steps 374(395.14) | Grad Norm 4.0522(4.5883) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 8.7270(9.0966) | Bit/dim 2.1665(2.4000) | Xent 1.6890(1.8142) | Xent Color 1.6717(1.7392) | Loss 6.3160(7.0202) | Error 0.3433(0.4068) | Error Color 0.7033(0.6911) |Steps 404(395.43) | Grad Norm 6.0301(4.4440) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 8.9394(9.0659) | Bit/dim 2.1852(2.3414) | Xent 1.6096(1.7703) | Xent Color 1.6346(1.7136) | Loss 6.3102(6.8223) | Error 0.3322(0.3912) | Error Color 0.6778(0.6900) |Steps 410(395.72) | Grad Norm 2.3036(4.2815) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 8.7972(9.0321) | Bit/dim 2.1576(2.2952) | Xent 1.5398(1.7132) | Xent Color 1.6177(1.6901) | Loss 6.1680(6.6574) | Error 0.3133(0.3708) | Error Color 0.6356(0.6878) |Steps 404(397.10) | Grad Norm 9.2122(4.6351) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 8.7480(9.0355) | Bit/dim 2.1512(2.2569) | Xent 1.4343(1.6509) | Xent Color 1.5942(1.6698) | Loss 6.0352(6.5137) | Error 0.2933(0.3524) | Error Color 0.6500(0.6818) |Steps 380(395.00) | Grad Norm 5.7244(5.3170) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 56.3644, Epoch Time 675.9919(630.9725), Bit/dim 2.1705(best: 2.2422), Xent 1.3302, Xent Color 1.5706. Loss 2.8956, Error 0.2218(best: 0.2633), Error Color 0.6153(best: 0.6810)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.3935(9.0346) | Bit/dim 2.1800(2.2328) | Xent 1.3041(1.5785) | Xent Color 1.5901(1.6483) | Loss 6.0383(6.8575) | Error 0.2522(0.3336) | Error Color 0.6467(0.6724) |Steps 386(395.59) | Grad Norm 4.4372(5.1501) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 8.8713(9.0096) | Bit/dim 2.1545(2.2144) | Xent 1.2388(1.5008) | Xent Color 1.5755(1.6328) | Loss 6.0992(6.6479) | Error 0.2489(0.3166) | Error Color 0.6267(0.6691) |Steps 398(395.31) | Grad Norm 24.5554(10.2615) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 8.7287(9.0045) | Bit/dim 2.1451(2.1983) | Xent 1.1768(1.4207) | Xent Color 1.5570(1.6114) | Loss 5.9472(6.4650) | Error 0.2556(0.3007) | Error Color 0.6433(0.6563) |Steps 398(395.05) | Grad Norm 30.4450(12.3932) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 8.6605(8.9600) | Bit/dim 2.1637(2.1853) | Xent 1.0525(1.3434) | Xent Color 1.5408(1.5917) | Loss 5.8687(6.3111) | Error 0.2344(0.2869) | Error Color 0.6411(0.6429) |Steps 398(393.53) | Grad Norm 26.4193(14.6554) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.1063(8.9757) | Bit/dim 2.1461(2.1772) | Xent 1.0120(1.2624) | Xent Color 1.4789(1.5650) | Loss 5.8069(6.1965) | Error 0.2222(0.2727) | Error Color 0.5422(0.6215) |Steps 398(394.09) | Grad Norm 9.8537(12.6334) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 8.8463(8.9404) | Bit/dim 2.1328(2.1735) | Xent 0.9625(1.1860) | Xent Color 1.5453(1.5956) | Loss 5.8360(6.1314) | Error 0.2211(0.2591) | Error Color 0.6556(0.6445) |Steps 410(395.17) | Grad Norm 47.5488(26.0262) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.0179(8.9113) | Bit/dim 2.1462(2.1663) | Xent 0.9485(1.1254) | Xent Color 1.5418(1.5827) | Loss 5.7446(6.0525) | Error 0.2378(0.2502) | Error Color 0.7011(0.6404) |Steps 392(394.44) | Grad Norm 42.8748(28.7071) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 56.2782, Epoch Time 668.8991(632.1103), Bit/dim 2.1401(best: 2.1705), Xent 0.8602, Xent Color 1.4343. Loss 2.7137, Error 0.1690(best: 0.2218), Error Color 0.3689(best: 0.6153)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.5710(8.9507) | Bit/dim 2.1239(2.1587) | Xent 0.9197(1.0691) | Xent Color 1.4402(1.5575) | Loss 5.8359(6.4071) | Error 0.2278(0.2425) | Error Color 0.5167(0.6204) |Steps 410(395.89) | Grad Norm 4.8365(27.0247) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 8.7926(8.9544) | Bit/dim 2.1267(2.1509) | Xent 0.8485(1.0161) | Xent Color 1.4366(1.5245) | Loss 5.7672(6.2332) | Error 0.2089(0.2361) | Error Color 0.5000(0.5921) |Steps 410(395.73) | Grad Norm 1.4368(22.9543) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 8.8178(8.9479) | Bit/dim 2.1080(2.1443) | Xent 0.8294(0.9680) | Xent Color 1.3682(1.4904) | Loss 5.5543(6.0924) | Error 0.2111(0.2318) | Error Color 0.4500(0.5630) |Steps 386(396.38) | Grad Norm 5.3626(18.8828) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.1741(8.9739) | Bit/dim 2.1153(2.1363) | Xent 0.7719(0.9202) | Xent Color 1.3389(1.4561) | Loss 5.6442(5.9779) | Error 0.2011(0.2245) | Error Color 0.4544(0.5376) |Steps 410(397.50) | Grad Norm 1.3819(16.3824) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 8.6714(8.9576) | Bit/dim 2.1283(2.1323) | Xent 0.7816(0.8797) | Xent Color 1.7027(1.4411) | Loss 5.6800(5.8865) | Error 0.2144(0.2185) | Error Color 0.7544(0.5339) |Steps 386(395.41) | Grad Norm 102.4462(21.8601) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.0850(8.9202) | Bit/dim 2.1090(2.1276) | Xent 0.8045(0.8528) | Xent Color 1.3613(1.4391) | Loss 5.6207(5.8255) | Error 0.2367(0.2155) | Error Color 0.5344(0.5425) |Steps 398(393.89) | Grad Norm 41.4902(29.7323) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 56.5433, Epoch Time 670.9088(633.2742), Bit/dim 2.1038(best: 2.1401), Xent 0.6603, Xent Color 1.2062. Loss 2.5705, Error 0.1461(best: 0.1690), Error Color 0.3102(best: 0.3689)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 8.8477(8.9177) | Bit/dim 2.1202(2.1194) | Xent 0.7323(0.8300) | Xent Color 1.1983(1.3982) | Loss 5.5480(6.2448) | Error 0.1856(0.2117) | Error Color 0.3489(0.5162) |Steps 398(393.72) | Grad Norm 10.4168(29.7149) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.4281(8.9608) | Bit/dim 2.1105(2.1134) | Xent 0.6904(0.7999) | Xent Color 1.1105(1.3329) | Loss 5.5165(6.0477) | Error 0.1944(0.2070) | Error Color 0.3556(0.4761) |Steps 410(394.39) | Grad Norm 4.4705(25.7857) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 8.8289(8.9851) | Bit/dim 2.1336(2.1152) | Xent 0.6614(0.7677) | Xent Color 1.0748(1.3178) | Loss 5.4037(5.9210) | Error 0.1800(0.2019) | Error Color 0.4378(0.4857) |Steps 380(394.32) | Grad Norm 37.1870(34.3706) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.1840(8.9613) | Bit/dim 2.0719(2.1092) | Xent 0.7596(0.7520) | Xent Color 1.0101(1.2619) | Loss 5.4881(5.7992) | Error 0.2000(0.1984) | Error Color 0.3122(0.4682) |Steps 410(393.83) | Grad Norm 22.4488(34.6817) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 8.8606(8.9351) | Bit/dim 2.0858(2.1010) | Xent 0.6860(0.7496) | Xent Color 0.9374(1.1858) | Loss 5.3166(5.6892) | Error 0.2067(0.1996) | Error Color 0.3078(0.4253) |Steps 374(393.70) | Grad Norm 30.4537(31.5598) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.0278(8.9437) | Bit/dim 2.1111(2.0965) | Xent 0.6544(0.7258) | Xent Color 0.7878(1.0961) | Loss 5.3005(5.5943) | Error 0.1889(0.1955) | Error Color 0.2333(0.3767) |Steps 386(394.90) | Grad Norm 19.0763(28.7540) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.2685(8.9767) | Bit/dim 2.2439(2.1174) | Xent 0.7170(0.7068) | Xent Color 2.5790(1.3207) | Loss 6.4456(5.6966) | Error 0.2111(0.1937) | Error Color 0.7711(0.3879) |Steps 410(397.05) | Grad Norm 122.6100(47.0878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 54.8333, Epoch Time 670.2246(634.3828), Bit/dim 2.2881(best: 2.1038), Xent 0.5329, Xent Color 1.8822. Loss 2.8919, Error 0.1329(best: 0.1461), Error Color 0.7740(best: 0.3102)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.1052(8.9691) | Bit/dim 2.1053(2.1508) | Xent 0.8875(0.7252) | Xent Color 1.4883(1.4139) | Loss 5.8014(6.2119) | Error 0.2544(0.2029) | Error Color 0.6878(0.4603) |Steps 404(396.51) | Grad Norm 54.7149(53.3225) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.1697(9.0678) | Bit/dim 2.0848(2.1419) | Xent 0.8270(0.7557) | Xent Color 1.2253(1.3680) | Loss 5.5520(6.0473) | Error 0.2367(0.2121) | Error Color 0.4978(0.4709) |Steps 404(398.51) | Grad Norm 26.2136(45.4769) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.2484(9.0810) | Bit/dim 2.0827(2.1275) | Xent 0.6687(0.7474) | Xent Color 1.1719(1.3212) | Loss 5.3155(5.9010) | Error 0.1800(0.2117) | Error Color 0.4600(0.4644) |Steps 380(398.49) | Grad Norm 10.9115(37.0566) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.3197(9.1932) | Bit/dim 2.0332(2.1079) | Xent 0.6575(0.7268) | Xent Color 1.1254(1.2709) | Loss 5.3273(5.7660) | Error 0.1867(0.2063) | Error Color 0.3911(0.4511) |Steps 416(401.15) | Grad Norm 3.9000(29.4050) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.6039(9.2319) | Bit/dim 2.0450(2.0911) | Xent 0.6150(0.7048) | Xent Color 1.0391(1.2170) | Loss 5.3226(5.6405) | Error 0.1733(0.2011) | Error Color 0.3322(0.4230) |Steps 440(403.43) | Grad Norm 7.2499(23.7503) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 8.9169(9.2804) | Bit/dim 2.0361(2.0768) | Xent 0.6295(0.6854) | Xent Color 0.9377(1.1556) | Loss 5.0892(5.5394) | Error 0.1911(0.1971) | Error Color 0.2811(0.3924) |Steps 386(403.53) | Grad Norm 5.9297(19.2344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 58.0159, Epoch Time 694.8286(636.1961), Bit/dim 2.0358(best: 2.1038), Xent 0.4837, Xent Color 0.8062. Loss 2.3582, Error 0.1261(best: 0.1329), Error Color 0.1576(best: 0.3102)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.2350(9.3041) | Bit/dim 2.0095(2.0630) | Xent 0.5890(0.6712) | Xent Color 0.8768(1.0895) | Loss 4.9999(5.9851) | Error 0.1689(0.1949) | Error Color 0.2356(0.3595) |Steps 404(406.79) | Grad Norm 6.3949(16.5600) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.3530(9.3384) | Bit/dim 2.0040(2.0532) | Xent 0.5637(0.6511) | Xent Color 0.8092(1.0191) | Loss 5.1496(5.7617) | Error 0.1600(0.1908) | Error Color 0.2178(0.3239) |Steps 416(408.35) | Grad Norm 13.4379(14.8063) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.3765(9.3602) | Bit/dim 2.0166(2.0436) | Xent 0.6069(0.6328) | Xent Color 0.7240(0.9449) | Loss 5.0847(5.5791) | Error 0.1700(0.1858) | Error Color 0.1667(0.2881) |Steps 416(406.39) | Grad Norm 4.5584(13.0028) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.0479(9.3327) | Bit/dim 2.0155(2.0364) | Xent 0.6066(0.6208) | Xent Color 0.6135(0.8694) | Loss 5.0648(5.4480) | Error 0.1789(0.1833) | Error Color 0.1456(0.2513) |Steps 398(406.04) | Grad Norm 4.9034(11.0907) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.2952(9.2813) | Bit/dim 1.9964(2.0292) | Xent 0.5874(0.6115) | Xent Color 0.6334(0.8079) | Loss 5.0924(5.3382) | Error 0.1756(0.1817) | Error Color 0.1811(0.2333) |Steps 416(406.46) | Grad Norm 33.9664(15.4977) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 8.8180(9.2373) | Bit/dim 2.0192(2.0249) | Xent 0.5319(0.6038) | Xent Color 0.5664(0.8107) | Loss 4.9281(5.2776) | Error 0.1433(0.1797) | Error Color 0.1122(0.2487) |Steps 404(405.75) | Grad Norm 7.4544(26.5077) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.4823(9.2323) | Bit/dim 1.9831(2.0181) | Xent 0.6046(0.6044) | Xent Color 0.6629(0.7735) | Loss 5.0317(5.2150) | Error 0.1767(0.1806) | Error Color 0.2456(0.2444) |Steps 386(405.66) | Grad Norm 39.2615(29.2320) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 60.2462, Epoch Time 695.5752(637.9775), Bit/dim 2.0029(best: 2.0358), Xent 0.4413, Xent Color 0.4480. Loss 2.2253, Error 0.1195(best: 0.1261), Error Color 0.0278(best: 0.1576)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.3020(9.2569) | Bit/dim 1.9996(2.0111) | Xent 0.5927(0.5959) | Xent Color 0.5361(0.7166) | Loss 4.9963(5.6304) | Error 0.1822(0.1799) | Error Color 0.1656(0.2201) |Steps 422(408.89) | Grad Norm 25.6062(26.6331) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.2248(9.2844) | Bit/dim 1.9969(2.0067) | Xent 0.5158(0.5825) | Xent Color 0.4674(0.6512) | Loss 4.8737(5.4444) | Error 0.1589(0.1780) | Error Color 0.0856(0.1871) |Steps 428(408.78) | Grad Norm 16.0112(22.7424) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.5973(9.2833) | Bit/dim 1.9638(1.9983) | Xent 0.5992(0.5790) | Xent Color 0.4020(0.5914) | Loss 4.8708(5.2995) | Error 0.1833(0.1777) | Error Color 0.0656(0.1580) |Steps 398(406.38) | Grad Norm 9.6884(19.3005) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.4616(9.2786) | Bit/dim 1.9915(1.9931) | Xent 0.5229(0.5730) | Xent Color 0.3685(0.5348) | Loss 4.9134(5.1822) | Error 0.1611(0.1748) | Error Color 0.0600(0.1323) |Steps 428(406.74) | Grad Norm 5.3911(16.3774) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.1451(9.3100) | Bit/dim 1.9572(1.9875) | Xent 0.5615(0.5704) | Xent Color 0.3578(0.4963) | Loss 4.8309(5.0918) | Error 0.1622(0.1733) | Error Color 0.0733(0.1223) |Steps 416(407.61) | Grad Norm 11.7814(18.5775) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.3986(9.3253) | Bit/dim 1.9855(1.9810) | Xent 0.5005(0.5643) | Xent Color 0.3202(0.4526) | Loss 4.8402(5.0174) | Error 0.1544(0.1720) | Error Color 0.0644(0.1060) |Steps 386(407.67) | Grad Norm 18.0202(16.9747) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.3253(9.3518) | Bit/dim 1.9525(1.9770) | Xent 0.5906(0.5578) | Xent Color 0.4299(0.4806) | Loss 4.9399(4.9974) | Error 0.1700(0.1695) | Error Color 0.1356(0.1374) |Steps 410(408.20) | Grad Norm 30.9252(27.4621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 58.3183, Epoch Time 697.4636(639.7621), Bit/dim 1.9627(best: 2.0029), Xent 0.4322, Xent Color 0.3993. Loss 2.1706, Error 0.1175(best: 0.1195), Error Color 0.1174(best: 0.0278)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.5771(9.3848) | Bit/dim 1.9514(1.9698) | Xent 0.5744(0.5628) | Xent Color 0.4053(0.4727) | Loss 4.8289(5.3777) | Error 0.1800(0.1720) | Error Color 0.1244(0.1418) |Steps 416(409.46) | Grad Norm 32.5403(29.6396) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.3997(9.4277) | Bit/dim 1.9399(1.9625) | Xent 0.4826(0.5541) | Xent Color 0.3177(0.4376) | Loss 4.7000(5.2191) | Error 0.1478(0.1700) | Error Color 0.0722(0.1268) |Steps 404(409.88) | Grad Norm 23.0276(28.3491) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.4250(9.4393) | Bit/dim 1.9560(1.9573) | Xent 0.5205(0.5521) | Xent Color 0.4161(0.4064) | Loss 4.8384(5.1000) | Error 0.1689(0.1688) | Error Color 0.1622(0.1147) |Steps 416(410.22) | Grad Norm 58.4457(28.3116) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.8837(9.4088) | Bit/dim 1.9450(1.9511) | Xent 0.5055(0.5432) | Xent Color 0.2350(0.3766) | Loss 4.7759(4.9950) | Error 0.1600(0.1669) | Error Color 0.0444(0.1033) |Steps 434(410.21) | Grad Norm 13.6472(28.0991) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.5581(9.4120) | Bit/dim 1.9206(1.9423) | Xent 0.4869(0.5384) | Xent Color 0.2465(0.3415) | Loss 4.6396(4.9155) | Error 0.1456(0.1658) | Error Color 0.0556(0.0888) |Steps 422(410.18) | Grad Norm 29.0513(25.6702) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.3440(9.4080) | Bit/dim 1.9233(1.9360) | Xent 0.5270(0.5376) | Xent Color 0.2441(0.3168) | Loss 4.6224(4.8521) | Error 0.1544(0.1657) | Error Color 0.0533(0.0809) |Steps 434(413.05) | Grad Norm 26.6590(25.7402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 57.7062, Epoch Time 703.7885(641.6829), Bit/dim 1.9182(best: 1.9627), Xent 0.3829, Xent Color 0.1555. Loss 2.0528, Error 0.1098(best: 0.1175), Error Color 0.0154(best: 0.0278)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.6827(9.4559) | Bit/dim 1.9282(1.9287) | Xent 0.5299(0.5320) | Xent Color 0.3225(0.3056) | Loss 4.7526(5.3069) | Error 0.1667(0.1642) | Error Color 0.1189(0.0807) |Steps 452(416.08) | Grad Norm 50.4631(29.3040) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.6166(9.4755) | Bit/dim 1.9055(1.9230) | Xent 0.5038(0.5332) | Xent Color 0.2220(0.2974) | Loss 4.6731(5.1362) | Error 0.1611(0.1642) | Error Color 0.0500(0.0825) |Steps 434(415.08) | Grad Norm 27.1109(31.0872) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.5007(9.4970) | Bit/dim 1.9166(1.9173) | Xent 0.5008(0.5206) | Xent Color 0.1472(0.2656) | Loss 4.5872(4.9930) | Error 0.1556(0.1608) | Error Color 0.0189(0.0696) |Steps 416(415.06) | Grad Norm 15.8712(27.3500) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.5484(9.5032) | Bit/dim 2.2846(1.9428) | Xent 0.5850(0.5248) | Xent Color 2.5061(0.6559) | Loss 6.4586(5.1333) | Error 0.1822(0.1614) | Error Color 0.5489(0.1311) |Steps 434(414.02) | Grad Norm 112.1627(45.1299) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 10.2102(9.5434) | Bit/dim 2.0891(1.9850) | Xent 0.6734(0.5779) | Xent Color 1.1064(0.8607) | Loss 5.5243(5.2728) | Error 0.2167(0.1789) | Error Color 0.4789(0.2255) |Steps 428(416.71) | Grad Norm 50.0214(43.5960) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 10.0451(9.6060) | Bit/dim 2.0173(1.9965) | Xent 0.6605(0.6008) | Xent Color 0.7693(0.8728) | Loss 5.1786(5.2617) | Error 0.2011(0.1874) | Error Color 0.3044(0.2607) |Steps 398(416.45) | Grad Norm 33.1942(40.8742) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.8532(9.5520) | Bit/dim 1.9386(1.9897) | Xent 0.5740(0.5972) | Xent Color 0.5466(0.8094) | Loss 4.8361(5.1760) | Error 0.1789(0.1870) | Error Color 0.1756(0.2542) |Steps 434(417.25) | Grad Norm 7.3424(33.3473) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 59.3914, Epoch Time 714.7863(643.8760), Bit/dim 1.9582(best: 1.9182), Xent 0.4119, Xent Color 0.4180. Loss 2.1657, Error 0.1180(best: 0.1098), Error Color 0.0940(best: 0.0154)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.9460(9.6102) | Bit/dim 1.9582(1.9777) | Xent 0.6093(0.5900) | Xent Color 0.4372(0.7254) | Loss 4.7840(5.5099) | Error 0.1978(0.1849) | Error Color 0.1344(0.2300) |Steps 374(416.40) | Grad Norm 7.4871(26.3196) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.9926(9.6413) | Bit/dim 1.9174(1.9663) | Xent 0.4470(0.5761) | Xent Color 0.3402(0.6349) | Loss 4.6491(5.3072) | Error 0.1478(0.1809) | Error Color 0.1022(0.2002) |Steps 392(414.37) | Grad Norm 5.0789(20.8708) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.4888(9.6118) | Bit/dim 1.9372(1.9533) | Xent 0.5089(0.5679) | Xent Color 0.2918(0.5485) | Loss 4.6733(5.1403) | Error 0.1578(0.1770) | Error Color 0.0833(0.1693) |Steps 422(411.31) | Grad Norm 1.8303(16.6118) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.0001(9.5946) | Bit/dim 1.9108(1.9411) | Xent 0.5050(0.5584) | Xent Color 0.2254(0.4714) | Loss 4.5456(5.0047) | Error 0.1589(0.1738) | Error Color 0.0511(0.1414) |Steps 398(411.32) | Grad Norm 6.0017(13.7631) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.5934(9.5937) | Bit/dim 1.8948(1.9301) | Xent 0.5085(0.5434) | Xent Color 0.1880(0.4043) | Loss 4.5490(4.8970) | Error 0.1433(0.1689) | Error Color 0.0367(0.1173) |Steps 404(411.67) | Grad Norm 5.5177(11.4835) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.3751(9.5447) | Bit/dim 1.8880(1.9176) | Xent 0.5089(0.5330) | Xent Color 0.1877(0.3466) | Loss 4.5158(4.8013) | Error 0.1611(0.1662) | Error Color 0.0467(0.0970) |Steps 392(409.88) | Grad Norm 4.8265(9.2102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 59.7795, Epoch Time 716.5286(646.0556), Bit/dim 1.8799(best: 1.9182), Xent 0.3693, Xent Color 0.1009. Loss 1.9975, Error 0.1061(best: 0.1098), Error Color 0.0071(best: 0.0154)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.5537(9.5715) | Bit/dim 1.8630(1.9064) | Xent 0.4972(0.5297) | Xent Color 0.1371(0.2973) | Loss 4.4452(5.2521) | Error 0.1444(0.1649) | Error Color 0.0189(0.0800) |Steps 404(411.65) | Grad Norm 4.3836(7.5611) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 10.3558(9.5918) | Bit/dim 1.8879(1.8946) | Xent 0.4924(0.5232) | Xent Color 0.1427(0.2569) | Loss 4.5409(5.0534) | Error 0.1478(0.1623) | Error Color 0.0278(0.0660) |Steps 392(414.13) | Grad Norm 16.1190(7.1809) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.5677(9.6207) | Bit/dim 1.8455(1.8845) | Xent 0.5297(0.5166) | Xent Color 0.1265(0.2240) | Loss 4.4470(4.9091) | Error 0.1822(0.1609) | Error Color 0.0233(0.0552) |Steps 428(415.29) | Grad Norm 7.4501(6.8862) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.6498(9.6512) | Bit/dim 1.8365(1.8752) | Xent 0.4744(0.5142) | Xent Color 0.1808(0.2002) | Loss 4.4717(4.7967) | Error 0.1467(0.1606) | Error Color 0.0444(0.0476) |Steps 422(417.18) | Grad Norm 35.4277(8.9188) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 10.0418(9.6136) | Bit/dim 2.3367(1.9997) | Xent 1.3924(0.6021) | Xent Color 1.9542(1.3894) | Loss 6.6721(5.5293) | Error 0.4722(0.1894) | Error Color 0.6767(0.1929) |Steps 446(420.16) | Grad Norm 46.0289(43.1964) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.3102(9.7674) | Bit/dim 2.1473(2.0466) | Xent 0.7007(0.7034) | Xent Color 0.7595(1.2750) | Loss 5.3165(5.5698) | Error 0.2122(0.2250) | Error Color 0.2889(0.2467) |Steps 368(424.10) | Grad Norm 9.2638(36.7381) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.7504(9.7538) | Bit/dim 2.0267(2.0533) | Xent 0.6119(0.6852) | Xent Color 0.5970(1.1117) | Loss 5.0456(5.4761) | Error 0.1933(0.2169) | Error Color 0.2244(0.2474) |Steps 452(423.12) | Grad Norm 6.1336(28.8682) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 63.4144, Epoch Time 729.1289(648.5478), Bit/dim 1.9859(best: 1.8799), Xent 0.4237, Xent Color 0.3645. Loss 2.1829, Error 0.1190(best: 0.1061), Error Color 0.0949(best: 0.0071)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.5108(9.7131) | Bit/dim 1.9583(2.0327) | Xent 0.5220(0.6647) | Xent Color 0.4258(0.9468) | Loss 4.8036(5.8587) | Error 0.1600(0.2091) | Error Color 0.1378(0.2264) |Steps 428(423.15) | Grad Norm 7.4170(22.8507) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.5242(9.6740) | Bit/dim 1.9191(2.0082) | Xent 0.5153(0.6318) | Xent Color 0.3429(0.7971) | Loss 4.6998(5.5733) | Error 0.1589(0.1980) | Error Color 0.0944(0.1973) |Steps 410(417.59) | Grad Norm 3.8996(17.8856) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.2526(9.5505) | Bit/dim 1.8869(1.9827) | Xent 0.5633(0.6141) | Xent Color 0.2862(0.6665) | Loss 4.6802(5.3423) | Error 0.1733(0.1912) | Error Color 0.0644(0.1666) |Steps 374(410.05) | Grad Norm 4.4036(14.3475) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 8.5704(9.4455) | Bit/dim 1.8973(1.9574) | Xent 0.5796(0.5906) | Xent Color 0.2275(0.5565) | Loss 4.5777(5.1422) | Error 0.1778(0.1846) | Error Color 0.0500(0.1394) |Steps 410(407.72) | Grad Norm 9.1084(12.1413) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 8.8569(9.3655) | Bit/dim 1.8735(1.9351) | Xent 0.5287(0.5756) | Xent Color 0.2050(0.4653) | Loss 4.5018(4.9832) | Error 0.1811(0.1805) | Error Color 0.0556(0.1162) |Steps 392(404.62) | Grad Norm 3.6694(9.6754) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.3408(9.4122) | Bit/dim 1.8580(1.9135) | Xent 0.4993(0.5629) | Xent Color 0.1748(0.3900) | Loss 4.4978(4.8602) | Error 0.1622(0.1756) | Error Color 0.0356(0.0952) |Steps 410(403.73) | Grad Norm 6.8654(8.0770) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.3269(9.3868) | Bit/dim 1.8247(1.8921) | Xent 0.4746(0.5521) | Xent Color 0.1476(0.3312) | Loss 4.3624(4.7508) | Error 0.1478(0.1720) | Error Color 0.0267(0.0795) |Steps 398(404.03) | Grad Norm 1.6625(7.2882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 58.5237, Epoch Time 698.6692(650.0514), Bit/dim 1.8359(best: 1.8799), Xent 0.3583, Xent Color 0.0869. Loss 1.9472, Error 0.1028(best: 0.1061), Error Color 0.0076(best: 0.0071)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.0204(9.3613) | Bit/dim 1.8071(1.8741) | Xent 0.4936(0.5423) | Xent Color 0.1328(0.2811) | Loss 4.3693(5.0662) | Error 0.1589(0.1684) | Error Color 0.0267(0.0660) |Steps 422(405.23) | Grad Norm 2.1005(6.3295) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.8136(9.3440) | Bit/dim 1.7990(1.8574) | Xent 0.5071(0.5359) | Xent Color 0.1240(0.2428) | Loss 4.2752(4.8824) | Error 0.1500(0.1662) | Error Color 0.0167(0.0560) |Steps 374(402.04) | Grad Norm 1.6713(6.3626) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.8097(9.3174) | Bit/dim 1.7980(1.8421) | Xent 0.4826(0.5238) | Xent Color 0.1030(0.2074) | Loss 4.3631(4.7357) | Error 0.1411(0.1623) | Error Color 0.0178(0.0468) |Steps 392(400.68) | Grad Norm 5.0625(5.7442) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.8707(9.3797) | Bit/dim 1.7754(1.8247) | Xent 0.4637(0.5199) | Xent Color 0.0928(0.1800) | Loss 4.2981(4.6304) | Error 0.1344(0.1616) | Error Color 0.0200(0.0397) |Steps 386(402.85) | Grad Norm 3.8496(5.0460) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.1861(9.3867) | Bit/dim 1.7553(1.8105) | Xent 0.5014(0.5113) | Xent Color 0.0934(0.1591) | Loss 4.2880(4.5411) | Error 0.1600(0.1593) | Error Color 0.0189(0.0340) |Steps 416(404.03) | Grad Norm 7.7121(5.0027) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.3483(9.3685) | Bit/dim 1.7703(1.7966) | Xent 0.5040(0.5087) | Xent Color 0.0757(0.1407) | Loss 4.2808(4.4678) | Error 0.1633(0.1585) | Error Color 0.0133(0.0292) |Steps 380(405.42) | Grad Norm 4.0446(4.6587) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 58.0614, Epoch Time 698.9385(651.5180), Bit/dim 1.7467(best: 1.8359), Xent 0.3480, Xent Color 0.0389. Loss 1.8434, Error 0.1024(best: 0.1028), Error Color 0.0025(best: 0.0071)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.7101(9.4335) | Bit/dim 1.7450(1.7830) | Xent 0.5283(0.5113) | Xent Color 0.0797(0.1252) | Loss 4.1731(4.8833) | Error 0.1633(0.1588) | Error Color 0.0100(0.0252) |Steps 362(404.75) | Grad Norm 3.6726(4.0983) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.4696(9.4664) | Bit/dim 1.7054(1.7666) | Xent 0.4731(0.5074) | Xent Color 0.0785(0.1153) | Loss 4.1740(4.7027) | Error 0.1467(0.1574) | Error Color 0.0089(0.0230) |Steps 416(406.72) | Grad Norm 11.1928(5.3113) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.1189(9.5322) | Bit/dim 1.7194(1.7523) | Xent 0.4665(0.5090) | Xent Color 0.0588(0.1045) | Loss 4.2106(4.5671) | Error 0.1400(0.1573) | Error Color 0.0111(0.0206) |Steps 398(408.53) | Grad Norm 3.6321(5.5513) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.1510(9.5473) | Bit/dim 1.6796(1.7380) | Xent 0.5333(0.5107) | Xent Color 0.0771(0.0978) | Loss 4.0701(4.4578) | Error 0.1644(0.1583) | Error Color 0.0144(0.0193) |Steps 416(408.93) | Grad Norm 10.3321(6.5453) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.7852(9.5516) | Bit/dim 1.6633(1.7251) | Xent 0.4503(0.5025) | Xent Color 0.1146(0.0983) | Loss 4.1800(4.3724) | Error 0.1322(0.1562) | Error Color 0.0322(0.0209) |Steps 440(412.32) | Grad Norm 25.3172(9.4953) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.3180(9.5612) | Bit/dim 2.1779(1.7473) | Xent 0.6281(0.5074) | Xent Color 2.2043(0.6495) | Loss 6.0495(4.6226) | Error 0.1867(0.1575) | Error Color 0.5689(0.0858) |Steps 410(412.84) | Grad Norm 69.1223(30.9301) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.8354(9.5957) | Bit/dim 1.9949(1.8313) | Xent 0.6370(0.5807) | Xent Color 1.0120(0.9906) | Loss 5.2210(4.9440) | Error 0.1867(0.1827) | Error Color 0.4300(0.2080) |Steps 452(416.11) | Grad Norm 19.6192(32.2196) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 62.7534, Epoch Time 721.2145(653.6089), Bit/dim 1.9560(best: 1.7467), Xent 0.4013, Xent Color 0.8898. Loss 2.2788, Error 0.1168(best: 0.1024), Error Color 0.3971(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.5433(9.6127) | Bit/dim 1.8343(1.8501) | Xent 0.6953(0.5906) | Xent Color 0.7830(0.9582) | Loss 5.0148(5.4847) | Error 0.2167(0.1867) | Error Color 0.3111(0.2450) |Steps 440(421.71) | Grad Norm 13.7987(27.1676) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.8270(9.7041) | Bit/dim 1.7902(1.8349) | Xent 0.6716(0.6011) | Xent Color 0.4423(0.8535) | Loss 4.7022(5.2955) | Error 0.1989(0.1889) | Error Color 0.1700(0.2379) |Steps 452(426.48) | Grad Norm 7.1146(22.2072) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 9.9125(9.7283) | Bit/dim 1.7195(1.8108) | Xent 0.5034(0.5858) | Xent Color 0.3357(0.7265) | Loss 4.3604(5.0694) | Error 0.1522(0.1850) | Error Color 0.1144(0.2073) |Steps 386(424.54) | Grad Norm 4.9361(17.6016) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.7344(9.7537) | Bit/dim 1.6988(1.7860) | Xent 0.4714(0.5694) | Xent Color 0.2620(0.6139) | Loss 4.2494(4.8822) | Error 0.1411(0.1791) | Error Color 0.0711(0.1761) |Steps 416(423.62) | Grad Norm 2.6800(13.7067) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 9.4855(9.7321) | Bit/dim 1.6896(1.7596) | Xent 0.5161(0.5632) | Xent Color 0.2025(0.5127) | Loss 4.2858(4.7201) | Error 0.1556(0.1760) | Error Color 0.0544(0.1464) |Steps 440(424.16) | Grad Norm 1.5382(10.6602) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 9.9424(9.8404) | Bit/dim 1.6566(1.7346) | Xent 0.4502(0.5497) | Xent Color 0.1788(0.4304) | Loss 4.1604(4.5896) | Error 0.1467(0.1715) | Error Color 0.0478(0.1216) |Steps 428(423.34) | Grad Norm 1.2976(8.6958) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 61.1605, Epoch Time 737.9147(656.1381), Bit/dim 1.6480(best: 1.7467), Xent 0.3512, Xent Color 0.0834. Loss 1.7567, Error 0.1036(best: 0.1024), Error Color 0.0054(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.6734(9.9009) | Bit/dim 1.6459(1.7125) | Xent 0.4844(0.5413) | Xent Color 0.1390(0.3589) | Loss 4.1574(5.0148) | Error 0.1622(0.1685) | Error Color 0.0289(0.0988) |Steps 422(423.74) | Grad Norm 1.9316(7.1013) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 10.0474(9.9069) | Bit/dim 1.6371(1.6915) | Xent 0.4422(0.5306) | Xent Color 0.1230(0.2983) | Loss 4.1035(4.7872) | Error 0.1522(0.1652) | Error Color 0.0211(0.0792) |Steps 440(426.38) | Grad Norm 1.1462(5.9087) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.5771(9.8980) | Bit/dim 1.6052(1.6720) | Xent 0.4136(0.5219) | Xent Color 0.1076(0.2503) | Loss 3.9749(4.5989) | Error 0.1444(0.1634) | Error Color 0.0167(0.0645) |Steps 404(427.02) | Grad Norm 1.7083(5.1646) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 10.2950(9.9092) | Bit/dim 1.5867(1.6529) | Xent 0.5037(0.5196) | Xent Color 0.1183(0.2113) | Loss 4.1121(4.4565) | Error 0.1656(0.1637) | Error Color 0.0344(0.0532) |Steps 410(426.98) | Grad Norm 5.5060(4.9292) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 9.9835(9.8983) | Bit/dim 1.5923(1.6364) | Xent 0.5456(0.5175) | Xent Color 0.0805(0.1782) | Loss 4.0845(4.3404) | Error 0.1600(0.1632) | Error Color 0.0133(0.0430) |Steps 446(427.21) | Grad Norm 3.4348(4.8657) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 10.0029(9.9436) | Bit/dim 1.5796(1.6196) | Xent 0.5115(0.5162) | Xent Color 0.0639(0.1518) | Loss 3.9525(4.2501) | Error 0.1678(0.1621) | Error Color 0.0089(0.0352) |Steps 422(427.29) | Grad Norm 3.8415(4.5944) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 10.5320(9.9665) | Bit/dim 1.5699(1.6034) | Xent 0.5105(0.5097) | Xent Color 0.0735(0.1308) | Loss 4.0185(4.1793) | Error 0.1589(0.1597) | Error Color 0.0200(0.0297) |Steps 452(428.82) | Grad Norm 3.2532(4.6092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 61.6178, Epoch Time 741.3226(658.6936), Bit/dim 1.5538(best: 1.6480), Xent 0.3395, Xent Color 0.0327. Loss 1.6468, Error 0.0988(best: 0.1024), Error Color 0.0013(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.6836(9.9278) | Bit/dim 1.5267(1.5879) | Xent 0.5052(0.5069) | Xent Color 0.0727(0.1144) | Loss 3.9035(4.5951) | Error 0.1522(0.1581) | Error Color 0.0156(0.0252) |Steps 434(428.01) | Grad Norm 7.8320(5.1124) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 10.5577(10.0317) | Bit/dim 1.5374(1.5756) | Xent 0.4560(0.4990) | Xent Color 0.0621(0.1004) | Loss 3.9170(4.4167) | Error 0.1456(0.1569) | Error Color 0.0122(0.0214) |Steps 458(430.73) | Grad Norm 9.5341(5.3410) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 9.8526(10.0818) | Bit/dim 1.5134(1.5624) | Xent 0.4343(0.4972) | Xent Color 0.0555(0.0884) | Loss 3.8347(4.2870) | Error 0.1556(0.1569) | Error Color 0.0111(0.0183) |Steps 446(434.52) | Grad Norm 2.6657(5.1785) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.7090(10.1416) | Bit/dim 1.5149(1.5498) | Xent 0.3820(0.4885) | Xent Color 0.0454(0.0790) | Loss 3.7621(4.1763) | Error 0.1178(0.1551) | Error Color 0.0067(0.0159) |Steps 440(436.00) | Grad Norm 5.1920(4.9959) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 10.1982(10.1300) | Bit/dim 1.4994(1.5364) | Xent 0.5127(0.4847) | Xent Color 0.0470(0.0711) | Loss 3.8763(4.0930) | Error 0.1533(0.1540) | Error Color 0.0078(0.0142) |Steps 446(438.48) | Grad Norm 2.7107(4.8420) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 10.3002(10.1237) | Bit/dim 1.4841(1.5262) | Xent 0.4655(0.4866) | Xent Color 0.0462(0.0643) | Loss 3.7864(4.0267) | Error 0.1344(0.1526) | Error Color 0.0100(0.0125) |Steps 452(441.55) | Grad Norm 3.7245(4.7838) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.9204(10.1110) | Bit/dim 1.4834(1.5138) | Xent 0.4408(0.4834) | Xent Color 0.0439(0.0602) | Loss 3.7892(3.9650) | Error 0.1544(0.1518) | Error Color 0.0100(0.0116) |Steps 452(438.93) | Grad Norm 2.1697(4.5968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 62.8423, Epoch Time 754.8068(661.5770), Bit/dim 1.4821(best: 1.5538), Xent 0.3254, Xent Color 0.0188. Loss 1.5681, Error 0.0948(best: 0.0988), Error Color 0.0007(best: 0.0013)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 10.5075(10.1741) | Bit/dim 1.4602(1.5035) | Xent 0.3914(0.4781) | Xent Color 0.0427(0.0567) | Loss 3.8352(4.3704) | Error 0.1356(0.1500) | Error Color 0.0111(0.0110) |Steps 464(439.97) | Grad Norm 2.3173(4.9115) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 9.5830(10.2029) | Bit/dim 1.4666(1.4926) | Xent 0.5041(0.4768) | Xent Color 0.0379(0.0531) | Loss 3.7462(4.2137) | Error 0.1622(0.1494) | Error Color 0.0056(0.0103) |Steps 404(440.21) | Grad Norm 4.5717(4.4802) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 10.0516(10.1569) | Bit/dim 1.6990(1.4963) | Xent 0.5075(0.4729) | Xent Color 6.1599(0.2996) | Loss 6.9426(4.2308) | Error 0.1567(0.1483) | Error Color 0.7389(0.0485) |Steps 404(435.45) | Grad Norm 344.0457(21.5849) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 10.8477(10.1109) | Bit/dim 1.9455(1.6472) | Xent 0.7375(0.5490) | Xent Color 0.9542(0.9727) | Loss 5.3354(4.7806) | Error 0.2422(0.1739) | Error Color 0.3778(0.1953) |Steps 476(436.04) | Grad Norm 15.6644(31.5322) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 10.9337(10.3005) | Bit/dim 1.7285(1.6925) | Xent 0.5299(0.5798) | Xent Color 0.7006(0.9482) | Loss 4.7883(4.8587) | Error 0.1667(0.1836) | Error Color 0.2600(0.2331) |Steps 488(449.18) | Grad Norm 4.4193(26.1786) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 10.4157(10.3348) | Bit/dim 1.6760(1.6953) | Xent 0.5050(0.5686) | Xent Color 0.5610(0.8633) | Loss 4.4498(4.7800) | Error 0.1678(0.1807) | Error Color 0.2033(0.2342) |Steps 470(453.05) | Grad Norm 12.4383(21.9250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 63.3539, Epoch Time 768.5340(664.7857), Bit/dim 1.6281(best: 1.4821), Xent 0.3837, Xent Color 0.2576. Loss 1.7884, Error 0.1139(best: 0.0948), Error Color 0.0436(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1400 | Time 10.9988(10.3140) | Bit/dim 1.5808(1.6553) | Xent 0.4961(0.5523) | Xent Color 0.2336(0.6241) | Loss 4.0914(4.9161) | Error 0.1756(0.1752) | Error Color 0.0744(0.1781) |Steps 428(446.73) | Grad Norm 1.8177(15.0001) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 9.7453(10.2241) | Bit/dim 1.5547(1.6318) | Xent 0.4955(0.5366) | Xent Color 0.1608(0.5097) | Loss 4.0985(4.6935) | Error 0.1600(0.1707) | Error Color 0.0400(0.1442) |Steps 440(444.77) | Grad Norm 5.5369(12.3147) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.8833(10.1666) | Bit/dim 1.5305(1.6068) | Xent 0.4759(0.5249) | Xent Color 0.1267(0.4153) | Loss 4.0097(4.5153) | Error 0.1644(0.1668) | Error Color 0.0289(0.1156) |Steps 440(443.76) | Grad Norm 1.6148(10.5720) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 10.2971(10.1227) | Bit/dim 1.4909(1.5823) | Xent 0.4552(0.5153) | Xent Color 0.1426(0.3431) | Loss 3.9879(4.3746) | Error 0.1500(0.1642) | Error Color 0.0356(0.0951) |Steps 440(441.43) | Grad Norm 5.8198(9.9192) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 10.1095(10.1275) | Bit/dim 1.4970(1.5598) | Xent 0.4102(0.5044) | Xent Color 0.1121(0.2850) | Loss 3.9344(4.2544) | Error 0.1322(0.1607) | Error Color 0.0289(0.0778) |Steps 446(440.79) | Grad Norm 6.9435(9.4279) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.8377(10.1284) | Bit/dim 1.4653(1.5385) | Xent 0.4584(0.4956) | Xent Color 0.0940(0.2375) | Loss 3.7208(4.1535) | Error 0.1378(0.1565) | Error Color 0.0189(0.0635) |Steps 452(442.02) | Grad Norm 1.2431(8.5365) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 62.6921, Epoch Time 753.0454(667.4335), Bit/dim 1.4776(best: 1.4821), Xent 0.3227, Xent Color 0.0371. Loss 1.5676, Error 0.0933(best: 0.0948), Error Color 0.0021(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 10.5107(10.1492) | Bit/dim 1.4572(1.5212) | Xent 0.4415(0.4837) | Xent Color 0.0769(0.1970) | Loss 3.8029(4.5507) | Error 0.1444(0.1539) | Error Color 0.0167(0.0514) |Steps 458(440.63) | Grad Norm 1.3808(6.9012) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 9.9867(10.1912) | Bit/dim 1.4572(1.5050) | Xent 0.4752(0.4767) | Xent Color 0.0759(0.1671) | Loss 3.7748(4.3544) | Error 0.1522(0.1518) | Error Color 0.0133(0.0430) |Steps 446(439.97) | Grad Norm 1.7503(6.1703) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.6748(10.1492) | Bit/dim 1.4505(1.4907) | Xent 0.4847(0.4757) | Xent Color 0.0955(0.1432) | Loss 3.7877(4.2020) | Error 0.1522(0.1511) | Error Color 0.0278(0.0361) |Steps 440(438.50) | Grad Norm 14.5859(5.9547) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 10.2562(10.1743) | Bit/dim 1.4570(1.4788) | Xent 0.4500(0.4665) | Xent Color 0.1105(0.1275) | Loss 3.8428(4.0825) | Error 0.1456(0.1486) | Error Color 0.0300(0.0320) |Steps 440(439.36) | Grad Norm 20.8417(7.1590) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 10.1209(10.1499) | Bit/dim 1.5858(1.4976) | Xent 0.8313(0.4833) | Xent Color 1.0522(0.4897) | Loss 4.4890(4.2228) | Error 0.2722(0.1544) | Error Color 0.3556(0.1131) |Steps 416(437.16) | Grad Norm 33.6095(22.6516) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 9.5172(10.0376) | Bit/dim 1.5110(1.5156) | Xent 0.4888(0.4948) | Xent Color 0.4918(0.5304) | Loss 4.0454(4.2213) | Error 0.1556(0.1587) | Error Color 0.1900(0.1496) |Steps 434(433.55) | Grad Norm 8.5483(20.3667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 61.6469, Epoch Time 748.9648(669.8795), Bit/dim 1.5025(best: 1.4776), Xent 0.3236, Xent Color 0.2034. Loss 1.6343, Error 0.0968(best: 0.0933), Error Color 0.0318(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.9663(9.9802) | Bit/dim 1.5005(1.5139) | Xent 0.4273(0.4908) | Xent Color 0.2895(0.4946) | Loss 3.9169(4.6812) | Error 0.1411(0.1565) | Error Color 0.1000(0.1471) |Steps 428(429.93) | Grad Norm 5.9701(17.4165) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 10.0435(9.9723) | Bit/dim 1.4745(1.5044) | Xent 0.5080(0.4836) | Xent Color 0.1723(0.4221) | Loss 3.8879(4.4607) | Error 0.1567(0.1531) | Error Color 0.0511(0.1246) |Steps 428(428.39) | Grad Norm 6.3331(14.3843) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 10.0528(9.9253) | Bit/dim 1.4325(1.4916) | Xent 0.5283(0.4799) | Xent Color 0.1062(0.3478) | Loss 3.7347(4.2818) | Error 0.1678(0.1515) | Error Color 0.0200(0.1008) |Steps 404(426.35) | Grad Norm 2.3617(11.6013) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 9.3701(9.9141) | Bit/dim 1.4320(1.4777) | Xent 0.4311(0.4725) | Xent Color 0.0978(0.2846) | Loss 3.6775(4.1295) | Error 0.1378(0.1486) | Error Color 0.0189(0.0800) |Steps 416(423.70) | Grad Norm 4.8730(9.3691) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.9070(9.9033) | Bit/dim 1.4052(1.4638) | Xent 0.4830(0.4693) | Xent Color 0.0820(0.2334) | Loss 3.6838(4.0108) | Error 0.1444(0.1464) | Error Color 0.0133(0.0638) |Steps 404(420.76) | Grad Norm 1.9936(7.7898) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.2974(9.9147) | Bit/dim 1.4166(1.4514) | Xent 0.4290(0.4604) | Xent Color 0.0728(0.1936) | Loss 3.6926(3.9219) | Error 0.1278(0.1451) | Error Color 0.0122(0.0512) |Steps 428(421.82) | Grad Norm 1.5152(6.4192) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 10.5969(9.9686) | Bit/dim 1.4145(1.4401) | Xent 0.4419(0.4516) | Xent Color 0.0572(0.1612) | Loss 3.7084(3.8503) | Error 0.1422(0.1427) | Error Color 0.0078(0.0410) |Steps 446(422.12) | Grad Norm 1.2243(5.3725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 61.5198, Epoch Time 740.6504(672.0026), Bit/dim 1.4074(best: 1.4776), Xent 0.2881, Xent Color 0.0294. Loss 1.4868, Error 0.0894(best: 0.0933), Error Color 0.0013(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.1733(10.0153) | Bit/dim 1.4009(1.4304) | Xent 0.4319(0.4468) | Xent Color 0.0720(0.1369) | Loss 3.6747(4.2611) | Error 0.1300(0.1411) | Error Color 0.0122(0.0339) |Steps 434(424.48) | Grad Norm 1.5469(4.4359) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.7289(10.0431) | Bit/dim 1.3863(1.4230) | Xent 0.4577(0.4405) | Xent Color 0.0669(0.1173) | Loss 3.6140(4.1009) | Error 0.1322(0.1388) | Error Color 0.0133(0.0279) |Steps 416(426.03) | Grad Norm 4.2897(4.1315) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 10.1043(10.0877) | Bit/dim 1.3731(1.4137) | Xent 0.4243(0.4386) | Xent Color 0.0599(0.1022) | Loss 3.5718(3.9705) | Error 0.1278(0.1380) | Error Color 0.0111(0.0236) |Steps 452(425.31) | Grad Norm 3.8299(3.7297) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 10.7994(10.0985) | Bit/dim 1.3859(1.4051) | Xent 0.3740(0.4315) | Xent Color 0.0573(0.0899) | Loss 3.5720(3.8668) | Error 0.1322(0.1353) | Error Color 0.0111(0.0199) |Steps 410(425.94) | Grad Norm 3.6846(3.4037) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 10.2162(10.0733) | Bit/dim 1.3841(1.3976) | Xent 0.4012(0.4240) | Xent Color 0.0576(0.0798) | Loss 3.5563(3.7813) | Error 0.1378(0.1348) | Error Color 0.0111(0.0170) |Steps 410(423.90) | Grad Norm 5.4266(3.4411) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 9.9404(10.0556) | Bit/dim 1.3698(1.3903) | Xent 0.4183(0.4214) | Xent Color 0.0407(0.0711) | Loss 3.5394(3.7200) | Error 0.1356(0.1342) | Error Color 0.0067(0.0148) |Steps 440(424.65) | Grad Norm 3.8094(3.4309) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 10.2632(10.0981) | Bit/dim 1.3598(1.3839) | Xent 0.4078(0.4185) | Xent Color 0.0447(0.0648) | Loss 3.4715(3.6764) | Error 0.1289(0.1327) | Error Color 0.0100(0.0133) |Steps 434(428.27) | Grad Norm 2.9185(3.4966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 61.3828, Epoch Time 751.6764(674.3928), Bit/dim 1.3662(best: 1.4074), Xent 0.2726, Xent Color 0.0171. Loss 1.4387, Error 0.0831(best: 0.0894), Error Color 0.0006(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.4857(10.1073) | Bit/dim 1.3582(1.3780) | Xent 0.4084(0.4199) | Xent Color 0.0457(0.0592) | Loss 3.5036(4.0588) | Error 0.1200(0.1325) | Error Color 0.0111(0.0122) |Steps 428(429.40) | Grad Norm 3.0536(3.2953) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 10.2316(10.0908) | Bit/dim 1.3480(1.3725) | Xent 0.3813(0.4096) | Xent Color 0.0458(0.0552) | Loss 3.5014(3.9126) | Error 0.1244(0.1295) | Error Color 0.0067(0.0107) |Steps 410(430.26) | Grad Norm 3.5424(3.1276) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.9799(10.0804) | Bit/dim 1.3367(1.3656) | Xent 0.3588(0.3999) | Xent Color 0.0367(0.0513) | Loss 3.4411(3.8035) | Error 0.1222(0.1279) | Error Color 0.0067(0.0098) |Steps 446(430.17) | Grad Norm 2.6944(3.1258) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 10.1266(10.0534) | Bit/dim 1.3402(1.3613) | Xent 0.3501(0.3932) | Xent Color 0.0474(0.0484) | Loss 3.4229(3.7193) | Error 0.1033(0.1258) | Error Color 0.0111(0.0093) |Steps 434(428.13) | Grad Norm 6.8968(3.2451) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 10.1325(10.0414) | Bit/dim 1.3395(1.3558) | Xent 0.4596(0.3947) | Xent Color 0.0341(0.0459) | Loss 3.4704(3.6541) | Error 0.1567(0.1269) | Error Color 0.0078(0.0088) |Steps 416(428.07) | Grad Norm 3.8064(3.7326) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 10.6227(10.0311) | Bit/dim 1.3264(1.3513) | Xent 0.4095(0.3902) | Xent Color 0.0369(0.0439) | Loss 3.3826(3.5991) | Error 0.1333(0.1241) | Error Color 0.0056(0.0085) |Steps 416(426.53) | Grad Norm 4.5924(4.2821) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 61.6248, Epoch Time 746.7744(676.5642), Bit/dim 1.3331(best: 1.3662), Xent 0.2491, Xent Color 0.0136. Loss 1.3988, Error 0.0757(best: 0.0831), Error Color 0.0006(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.9410(10.0196) | Bit/dim 1.3349(1.3467) | Xent 0.3374(0.3868) | Xent Color 0.0482(0.0428) | Loss 3.4410(4.0611) | Error 0.1222(0.1230) | Error Color 0.0133(0.0085) |Steps 434(427.92) | Grad Norm 8.1936(5.0608) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.5534(9.9996) | Bit/dim 1.3167(1.3416) | Xent 0.3444(0.3852) | Xent Color 0.0310(0.0420) | Loss 3.3662(3.8923) | Error 0.1144(0.1238) | Error Color 0.0089(0.0088) |Steps 416(426.42) | Grad Norm 3.9297(5.6332) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 10.2286(10.0117) | Bit/dim 1.3286(1.3370) | Xent 0.3720(0.3804) | Xent Color 0.0335(0.0393) | Loss 3.4071(3.7712) | Error 0.1100(0.1218) | Error Color 0.0100(0.0080) |Steps 422(425.80) | Grad Norm 1.4946(5.1755) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 10.0829(10.0290) | Bit/dim 1.3079(1.3316) | Xent 0.3902(0.3768) | Xent Color 0.0347(0.0375) | Loss 3.4243(3.6716) | Error 0.1333(0.1207) | Error Color 0.0067(0.0076) |Steps 434(426.19) | Grad Norm 6.5860(4.9810) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 10.2043(10.0056) | Bit/dim 1.3185(1.3257) | Xent 0.3224(0.3651) | Xent Color 0.0354(0.0365) | Loss 3.3305(3.5907) | Error 0.1100(0.1180) | Error Color 0.0078(0.0071) |Steps 422(425.08) | Grad Norm 1.6432(4.9514) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 10.0194(9.9643) | Bit/dim 1.3053(1.3215) | Xent 0.3695(0.3588) | Xent Color 0.0316(0.0346) | Loss 3.3822(3.5279) | Error 0.1067(0.1151) | Error Color 0.0044(0.0064) |Steps 422(422.71) | Grad Norm 2.3084(4.4069) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 9.7202(9.9566) | Bit/dim 1.2914(1.3155) | Xent 0.3360(0.3549) | Xent Color 0.0318(0.0335) | Loss 3.3349(3.4818) | Error 0.1122(0.1144) | Error Color 0.0089(0.0065) |Steps 434(421.61) | Grad Norm 3.1966(4.6292) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 59.9111, Epoch Time 741.2506(678.5048), Bit/dim 1.3006(best: 1.3331), Xent 0.2133, Xent Color 0.0083. Loss 1.3560, Error 0.0650(best: 0.0757), Error Color 0.0002(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 9.9430(9.9749) | Bit/dim 1.2942(1.3124) | Xent 0.3255(0.3545) | Xent Color 0.0243(0.0313) | Loss 3.3302(3.8820) | Error 0.0944(0.1142) | Error Color 0.0056(0.0057) |Steps 416(421.54) | Grad Norm 1.9771(4.3412) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 9.9660(9.9842) | Bit/dim 1.2962(1.3066) | Xent 0.3187(0.3518) | Xent Color 0.0443(0.0315) | Loss 3.3231(3.7362) | Error 0.1000(0.1127) | Error Color 0.0089(0.0057) |Steps 416(421.23) | Grad Norm 13.3656(4.7217) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.6889(9.9471) | Bit/dim 1.2821(1.3004) | Xent 0.2902(0.3412) | Xent Color 0.0237(0.0306) | Loss 3.2714(3.6229) | Error 0.1089(0.1105) | Error Color 0.0033(0.0058) |Steps 416(421.59) | Grad Norm 4.8943(4.8800) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 10.1234(9.9433) | Bit/dim 1.2773(1.2953) | Xent 0.2911(0.3347) | Xent Color 0.0265(0.0299) | Loss 3.3400(3.5426) | Error 0.0967(0.1078) | Error Color 0.0089(0.0059) |Steps 398(421.03) | Grad Norm 6.2552(5.2707) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 10.2249(9.9345) | Bit/dim 1.5510(1.3040) | Xent 0.3670(0.3307) | Xent Color 6.1676(0.2324) | Loss 6.5858(3.5968) | Error 0.1156(0.1058) | Error Color 0.6533(0.0326) |Steps 434(421.42) | Grad Norm 322.8094(20.3865) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.4517(9.8973) | Bit/dim 1.9115(1.4956) | Xent 0.5223(0.4257) | Xent Color 1.3609(1.3822) | Loss 5.1298(4.4660) | Error 0.1844(0.1376) | Error Color 0.5044(0.1763) |Steps 392(419.61) | Grad Norm 19.0419(36.3770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 59.1330, Epoch Time 732.3309(680.1196), Bit/dim 1.7557(best: 1.3006), Xent 0.3413, Xent Color 0.6764. Loss 2.0101, Error 0.1069(best: 0.0650), Error Color 0.2886(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.8608(9.7938) | Bit/dim 1.7384(1.5738) | Xent 0.5072(0.4702) | Xent Color 0.8865(1.3190) | Loss 4.5684(5.0936) | Error 0.1744(0.1531) | Error Color 0.3411(0.2457) |Steps 422(416.63) | Grad Norm 9.9562(30.1828) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 9.2926(9.6722) | Bit/dim 1.6198(1.5959) | Xent 0.4433(0.4644) | Xent Color 0.5911(1.1579) | Loss 4.2234(4.8945) | Error 0.1467(0.1518) | Error Color 0.2378(0.2542) |Steps 404(410.98) | Grad Norm 8.9975(24.3366) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.5318(9.5801) | Bit/dim 1.4976(1.5718) | Xent 0.4072(0.4407) | Xent Color 0.3307(0.8307) | Loss 3.7614(4.4824) | Error 0.1389(0.1428) | Error Color 0.1144(0.2130) |Steps 416(405.75) | Grad Norm 4.4410(15.4909) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.5621(9.5581) | Bit/dim 1.4331(1.5425) | Xent 0.3497(0.4177) | Xent Color 0.2661(0.6874) | Loss 3.6842(4.2887) | Error 0.1167(0.1356) | Error Color 0.0844(0.1810) |Steps 410(404.50) | Grad Norm 5.9868(12.6492) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 9.2369(9.5677) | Bit/dim 1.4109(1.5116) | Xent 0.3899(0.4050) | Xent Color 0.1892(0.5621) | Loss 3.6013(4.1211) | Error 0.1156(0.1313) | Error Color 0.0578(0.1490) |Steps 410(405.28) | Grad Norm 6.2699(10.4432) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 10.2229(9.6907) | Bit/dim 1.3866(1.4796) | Xent 0.3313(0.3934) | Xent Color 0.1343(0.4620) | Loss 3.6250(3.9836) | Error 0.1278(0.1276) | Error Color 0.0311(0.1225) |Steps 440(409.16) | Grad Norm 2.3024(8.9208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 60.6328, Epoch Time 715.3255(681.1758), Bit/dim 1.3778(best: 1.3006), Xent 0.2314, Xent Color 0.0628. Loss 1.4513, Error 0.0690(best: 0.0650), Error Color 0.0049(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 9.7450(9.7318) | Bit/dim 1.3779(1.4515) | Xent 0.3508(0.3832) | Xent Color 0.1169(0.3771) | Loss 3.5415(4.3132) | Error 0.1233(0.1241) | Error Color 0.0267(0.0999) |Steps 422(410.51) | Grad Norm 3.9455(7.5379) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 9.8931(9.7992) | Bit/dim 1.3644(1.4266) | Xent 0.3105(0.3711) | Xent Color 0.1129(0.3085) | Loss 3.4957(4.0944) | Error 0.1011(0.1192) | Error Color 0.0322(0.0814) |Steps 410(410.41) | Grad Norm 1.7679(6.5006) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 9.8169(9.8418) | Bit/dim 1.3345(1.4034) | Xent 0.2929(0.3609) | Xent Color 0.0853(0.2537) | Loss 3.4204(3.9220) | Error 0.1000(0.1164) | Error Color 0.0178(0.0665) |Steps 422(412.21) | Grad Norm 3.7212(5.6188) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 10.3211(9.9113) | Bit/dim 1.3264(1.3831) | Xent 0.3359(0.3512) | Xent Color 0.1037(0.2116) | Loss 3.4017(3.7890) | Error 0.1178(0.1130) | Error Color 0.0233(0.0549) |Steps 422(415.09) | Grad Norm 5.4404(5.3244) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 9.8381(9.9709) | Bit/dim 1.3087(1.3657) | Xent 0.3447(0.3499) | Xent Color 0.0771(0.1777) | Loss 3.3719(3.6897) | Error 0.1167(0.1120) | Error Color 0.0167(0.0454) |Steps 404(417.16) | Grad Norm 2.7858(4.9082) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 10.0373(9.9451) | Bit/dim 1.3059(1.3491) | Xent 0.3381(0.3408) | Xent Color 0.0801(0.1501) | Loss 3.4051(3.6032) | Error 0.1000(0.1089) | Error Color 0.0178(0.0376) |Steps 428(418.12) | Grad Norm 3.2757(4.6023) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 10.5235(10.0186) | Bit/dim 1.3070(1.3349) | Xent 0.3021(0.3341) | Xent Color 0.0570(0.1277) | Loss 3.3426(3.5350) | Error 0.0933(0.1078) | Error Color 0.0122(0.0312) |Steps 440(419.07) | Grad Norm 1.8274(3.9633) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 60.5233, Epoch Time 745.5142(683.1059), Bit/dim 1.2969(best: 1.3006), Xent 0.1956, Xent Color 0.0215. Loss 1.3512, Error 0.0616(best: 0.0650), Error Color 0.0012(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 9.7457(10.0091) | Bit/dim 1.2960(1.3233) | Xent 0.2879(0.3266) | Xent Color 0.0688(0.1107) | Loss 3.3617(3.8829) | Error 0.0944(0.1057) | Error Color 0.0189(0.0273) |Steps 446(420.87) | Grad Norm 4.5302(3.8426) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 10.0936(10.0580) | Bit/dim 1.2685(1.3127) | Xent 0.2917(0.3228) | Xent Color 0.0654(0.0966) | Loss 3.2929(3.7366) | Error 0.0944(0.1045) | Error Color 0.0167(0.0234) |Steps 434(423.12) | Grad Norm 3.6469(4.0663) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 10.0334(10.1008) | Bit/dim 1.2553(1.3009) | Xent 0.2641(0.3174) | Xent Color 0.0532(0.0854) | Loss 3.2155(3.6172) | Error 0.0844(0.1031) | Error Color 0.0133(0.0207) |Steps 422(424.96) | Grad Norm 4.6168(4.0790) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 10.2280(10.1341) | Bit/dim 1.2515(1.2913) | Xent 0.2758(0.3125) | Xent Color 0.0503(0.0764) | Loss 3.2122(3.5300) | Error 0.1022(0.1021) | Error Color 0.0078(0.0179) |Steps 446(427.58) | Grad Norm 2.9870(3.7823) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 10.1743(10.1190) | Bit/dim 1.2561(1.2804) | Xent 0.2845(0.3070) | Xent Color 0.0383(0.0678) | Loss 3.2256(3.4529) | Error 0.0978(0.1004) | Error Color 0.0067(0.0151) |Steps 434(427.53) | Grad Norm 2.1186(3.4929) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 10.3223(10.1330) | Bit/dim 1.2515(1.2734) | Xent 0.3402(0.3014) | Xent Color 0.0430(0.0619) | Loss 3.2832(3.4022) | Error 0.1056(0.0974) | Error Color 0.0122(0.0137) |Steps 422(428.65) | Grad Norm 2.7313(3.6874) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 61.3905, Epoch Time 755.4656(685.2767), Bit/dim 1.2453(best: 1.2969), Xent 0.1776, Xent Color 0.0126. Loss 1.2928, Error 0.0566(best: 0.0616), Error Color 0.0002(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 9.9490(10.1706) | Bit/dim 1.2341(1.2655) | Xent 0.2607(0.2986) | Xent Color 0.0464(0.0564) | Loss 3.2148(3.8364) | Error 0.0811(0.0971) | Error Color 0.0133(0.0123) |Steps 446(429.53) | Grad Norm 2.7180(3.4490) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 10.0226(10.1708) | Bit/dim 1.2363(1.2581) | Xent 0.2952(0.2955) | Xent Color 0.0355(0.0516) | Loss 3.1981(3.6751) | Error 0.0911(0.0958) | Error Color 0.0056(0.0110) |Steps 416(428.59) | Grad Norm 2.3954(3.3505) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.9709(10.1641) | Bit/dim 1.2313(1.2513) | Xent 0.2883(0.2907) | Xent Color 0.0318(0.0489) | Loss 3.1362(3.5495) | Error 0.0856(0.0939) | Error Color 0.0044(0.0104) |Steps 416(428.70) | Grad Norm 5.1388(4.1467) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 10.1720(10.1237) | Bit/dim 1.2147(1.2472) | Xent 0.2802(0.2890) | Xent Color 0.0484(0.0522) | Loss 3.1351(3.4646) | Error 0.0978(0.0941) | Error Color 0.0122(0.0120) |Steps 428(427.64) | Grad Norm 8.7645(7.0179) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 10.4208(10.1257) | Bit/dim 1.2091(1.2401) | Xent 0.2737(0.2859) | Xent Color 0.0391(0.0495) | Loss 3.2054(3.3951) | Error 0.0911(0.0934) | Error Color 0.0122(0.0116) |Steps 440(428.51) | Grad Norm 3.9256(7.0598) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 10.0040(10.1218) | Bit/dim 1.2133(1.2337) | Xent 0.3124(0.2807) | Xent Color 0.0268(0.0448) | Loss 3.1864(3.3413) | Error 0.0944(0.0916) | Error Color 0.0033(0.0101) |Steps 428(428.87) | Grad Norm 4.7062(6.3112) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 9.8365(10.0796) | Bit/dim 1.1888(1.2256) | Xent 0.2230(0.2779) | Xent Color 0.0304(0.0413) | Loss 3.1338(3.2968) | Error 0.0800(0.0917) | Error Color 0.0056(0.0088) |Steps 428(429.61) | Grad Norm 5.0587(5.4300) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 62.0065, Epoch Time 750.8398(687.2436), Bit/dim 1.2039(best: 1.2453), Xent 0.1671, Xent Color 0.0084. Loss 1.2478, Error 0.0526(best: 0.0566), Error Color 0.0006(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 10.0955(10.0794) | Bit/dim 1.1987(1.2199) | Xent 0.2994(0.2744) | Xent Color 0.0356(0.0393) | Loss 3.2084(3.7129) | Error 0.1044(0.0904) | Error Color 0.0089(0.0084) |Steps 440(430.04) | Grad Norm 6.1274(5.6354) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 9.7984(10.1138) | Bit/dim 1.1920(1.2147) | Xent 0.2607(0.2718) | Xent Color 0.0234(0.0366) | Loss 3.1456(3.5690) | Error 0.0844(0.0888) | Error Color 0.0044(0.0077) |Steps 440(430.11) | Grad Norm 2.4103(4.9998) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 10.4679(10.1458) | Bit/dim 1.1859(1.2078) | Xent 0.2618(0.2661) | Xent Color 0.0369(0.0348) | Loss 3.1572(3.4586) | Error 0.0978(0.0871) | Error Color 0.0089(0.0073) |Steps 434(431.64) | Grad Norm 5.9526(4.6435) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 10.8503(10.2767) | Bit/dim 1.8445(1.3397) | Xent 0.9449(0.3426) | Xent Color 2.4519(0.9619) | Loss 5.9351(4.1032) | Error 0.3089(0.1069) | Error Color 0.5567(0.1129) |Steps 542(441.50) | Grad Norm 20.4505(22.5441) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.9883(10.2944) | Bit/dim 1.6342(1.4415) | Xent 0.3752(0.3903) | Xent Color 1.0472(1.0840) | Loss 4.4661(4.3131) | Error 0.1222(0.1238) | Error Color 0.4422(0.2053) |Steps 434(446.73) | Grad Norm 15.0067(21.3674) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 9.5341(10.1479) | Bit/dim 1.4910(1.4674) | Xent 0.3571(0.3904) | Xent Color 0.5390(0.9868) | Loss 3.8545(4.2524) | Error 0.1156(0.1235) | Error Color 0.2011(0.2283) |Steps 428(439.61) | Grad Norm 6.6568(18.5833) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 60.0428, Epoch Time 752.7186(689.2079), Bit/dim 1.4065(best: 1.2039), Xent 0.1993, Xent Color 0.2680. Loss 1.5233, Error 0.0647(best: 0.0526), Error Color 0.0684(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 9.7721(10.0089) | Bit/dim 1.4083(1.4588) | Xent 0.3037(0.3739) | Xent Color 0.4013(0.8432) | Loss 3.6435(4.5949) | Error 0.1033(0.1195) | Error Color 0.1500(0.2128) |Steps 404(432.36) | Grad Norm 6.9108(15.5983) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 10.1249(9.9992) | Bit/dim 1.3282(1.4315) | Xent 0.3345(0.3563) | Xent Color 0.2197(0.6961) | Loss 3.4379(4.3029) | Error 0.1089(0.1146) | Error Color 0.0622(0.1827) |Steps 392(426.78) | Grad Norm 3.3722(12.6674) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 9.7089(9.9285) | Bit/dim 1.2792(1.3970) | Xent 0.2882(0.3429) | Xent Color 0.1525(0.5609) | Loss 3.2939(4.0562) | Error 0.0978(0.1105) | Error Color 0.0456(0.1479) |Steps 410(421.90) | Grad Norm 3.4695(10.3637) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 9.4270(9.9012) | Bit/dim 1.2755(1.3642) | Xent 0.3401(0.3313) | Xent Color 0.1155(0.4469) | Loss 3.3248(3.8558) | Error 0.1111(0.1067) | Error Color 0.0256(0.1173) |Steps 416(419.31) | Grad Norm 2.2524(8.4598) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 9.6474(9.8467) | Bit/dim 1.2420(1.3326) | Xent 0.3005(0.3219) | Xent Color 0.1060(0.3580) | Loss 3.2208(3.6957) | Error 0.0922(0.1032) | Error Color 0.0244(0.0928) |Steps 422(419.28) | Grad Norm 2.8434(6.9668) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 9.6988(9.8419) | Bit/dim 1.2223(1.3048) | Xent 0.2810(0.3084) | Xent Color 0.0788(0.2883) | Loss 3.1901(3.5597) | Error 0.0889(0.0990) | Error Color 0.0156(0.0736) |Steps 434(417.60) | Grad Norm 2.5468(5.7818) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 9.6057(9.8253) | Bit/dim 1.1997(1.2812) | Xent 0.2515(0.2952) | Xent Color 0.0679(0.2323) | Loss 3.0699(3.4450) | Error 0.0800(0.0954) | Error Color 0.0122(0.0574) |Steps 410(417.25) | Grad Norm 1.9779(4.8772) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 59.2262, Epoch Time 730.6197(690.4502), Bit/dim 1.2042(best: 1.2039), Xent 0.1677, Xent Color 0.0303. Loss 1.2537, Error 0.0534(best: 0.0526), Error Color 0.0016(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 9.9096(9.8223) | Bit/dim 1.2023(1.2596) | Xent 0.2318(0.2887) | Xent Color 0.0660(0.1906) | Loss 3.0323(3.8088) | Error 0.0733(0.0930) | Error Color 0.0100(0.0461) |Steps 416(416.34) | Grad Norm 1.1878(4.0664) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 10.2492(9.8171) | Bit/dim 1.1910(1.2425) | Xent 0.2667(0.2804) | Xent Color 0.0603(0.1586) | Loss 3.1734(3.6263) | Error 0.0922(0.0911) | Error Color 0.0100(0.0375) |Steps 422(416.47) | Grad Norm 3.3311(3.6233) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.7531(9.8183) | Bit/dim 1.1747(1.2271) | Xent 0.2906(0.2779) | Xent Color 0.0590(0.1335) | Loss 3.0918(3.4890) | Error 0.0967(0.0897) | Error Color 0.0122(0.0307) |Steps 428(414.95) | Grad Norm 1.5203(3.2035) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.9916(9.8587) | Bit/dim 1.1659(1.2144) | Xent 0.2969(0.2760) | Xent Color 0.0525(0.1130) | Loss 3.0998(3.3877) | Error 0.0922(0.0895) | Error Color 0.0067(0.0251) |Steps 416(415.25) | Grad Norm 1.3724(2.7804) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.3629(9.8238) | Bit/dim 1.1538(1.2008) | Xent 0.2944(0.2739) | Xent Color 0.0541(0.0976) | Loss 2.9865(3.2970) | Error 0.0889(0.0884) | Error Color 0.0089(0.0210) |Steps 404(414.57) | Grad Norm 6.1605(2.9219) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 9.9324(9.8423) | Bit/dim 1.1669(1.1906) | Xent 0.2920(0.2667) | Xent Color 0.0500(0.0861) | Loss 3.0618(3.2316) | Error 0.0900(0.0857) | Error Color 0.0056(0.0181) |Steps 392(414.78) | Grad Norm 1.8149(3.4271) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.6995(9.8578) | Bit/dim 1.1491(1.1807) | Xent 0.2410(0.2643) | Xent Color 0.0413(0.0759) | Loss 2.9999(3.1801) | Error 0.0822(0.0847) | Error Color 0.0044(0.0152) |Steps 434(415.64) | Grad Norm 1.5251(3.1258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 59.6906, Epoch Time 732.4622(691.7106), Bit/dim 1.1531(best: 1.2039), Xent 0.1573, Xent Color 0.0157. Loss 1.1964, Error 0.0524(best: 0.0526), Error Color 0.0001(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.0532(9.8785) | Bit/dim 1.1599(1.1731) | Xent 0.2357(0.2620) | Xent Color 0.0560(0.0683) | Loss 3.0935(3.5299) | Error 0.0722(0.0841) | Error Color 0.0144(0.0135) |Steps 428(416.41) | Grad Norm 4.0230(3.0925) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 9.6694(9.9024) | Bit/dim 1.1352(1.1649) | Xent 0.2297(0.2573) | Xent Color 0.0474(0.0615) | Loss 2.9888(3.3920) | Error 0.0722(0.0828) | Error Color 0.0100(0.0118) |Steps 416(416.72) | Grad Norm 3.0838(3.0416) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 10.2406(9.8980) | Bit/dim 1.1371(1.1577) | Xent 0.2260(0.2561) | Xent Color 0.0398(0.0563) | Loss 2.9695(3.2901) | Error 0.0822(0.0826) | Error Color 0.0078(0.0109) |Steps 428(415.70) | Grad Norm 3.1287(3.3833) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.9015(9.9066) | Bit/dim 1.1344(1.1510) | Xent 0.2383(0.2551) | Xent Color 0.0430(0.0521) | Loss 3.0056(3.2100) | Error 0.0844(0.0825) | Error Color 0.0100(0.0099) |Steps 410(414.42) | Grad Norm 6.7800(3.5975) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.2501(9.8901) | Bit/dim 1.1281(1.1454) | Xent 0.2359(0.2524) | Xent Color 0.0391(0.0475) | Loss 2.9586(3.1463) | Error 0.0767(0.0813) | Error Color 0.0111(0.0087) |Steps 416(416.84) | Grad Norm 3.7461(3.5970) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 10.1603(9.9043) | Bit/dim 1.1191(1.1398) | Xent 0.2631(0.2505) | Xent Color 0.0369(0.0443) | Loss 2.9882(3.1066) | Error 0.0944(0.0820) | Error Color 0.0044(0.0078) |Steps 434(417.02) | Grad Norm 5.4709(3.7896) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 58.8582, Epoch Time 736.1209(693.0429), Bit/dim 1.1283(best: 1.1531), Xent 0.1497, Xent Color 0.0315. Loss 1.1736, Error 0.0511(best: 0.0524), Error Color 0.0055(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 9.7133(9.8949) | Bit/dim 1.1132(1.1351) | Xent 0.3135(0.2509) | Xent Color 0.0329(0.0448) | Loss 2.9897(3.5348) | Error 0.0989(0.0812) | Error Color 0.0056(0.0087) |Steps 422(416.72) | Grad Norm 5.1604(5.3030) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 9.7658(9.9128) | Bit/dim 1.1112(1.1292) | Xent 0.2722(0.2473) | Xent Color 0.0324(0.0434) | Loss 2.9511(3.3807) | Error 0.0733(0.0799) | Error Color 0.0056(0.0084) |Steps 416(416.95) | Grad Norm 5.2589(5.8939) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 9.9584(9.9357) | Bit/dim 1.1097(1.1252) | Xent 0.2764(0.2452) | Xent Color 0.0307(0.0406) | Loss 2.9251(3.2662) | Error 0.0856(0.0789) | Error Color 0.0033(0.0077) |Steps 416(417.07) | Grad Norm 5.8244(5.7504) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 9.8034(9.9310) | Bit/dim 1.1148(1.1209) | Xent 0.2478(0.2459) | Xent Color 0.0321(0.0379) | Loss 2.9102(3.1789) | Error 0.0811(0.0788) | Error Color 0.0056(0.0071) |Steps 428(418.36) | Grad Norm 4.3091(5.3002) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 9.7580(9.9493) | Bit/dim 1.1191(1.1151) | Xent 0.2397(0.2476) | Xent Color 0.0270(0.0361) | Loss 2.9838(3.1204) | Error 0.0778(0.0800) | Error Color 0.0011(0.0064) |Steps 422(420.97) | Grad Norm 8.3524(5.3479) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 10.0085(9.9767) | Bit/dim 1.1215(1.1118) | Xent 0.2814(0.2489) | Xent Color 0.0862(0.0370) | Loss 2.9789(3.0725) | Error 0.0911(0.0801) | Error Color 0.0311(0.0071) |Steps 416(419.72) | Grad Norm 29.3317(6.7766) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 9.7987(10.0002) | Bit/dim 1.2050(1.1434) | Xent 0.2360(0.2532) | Xent Color 0.3082(0.2167) | Loss 3.2456(3.1861) | Error 0.0678(0.0817) | Error Color 0.1100(0.0609) |Steps 416(420.32) | Grad Norm 25.2006(19.2172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 60.4317, Epoch Time 743.3729(694.5528), Bit/dim 1.2145(best: 1.1283), Xent 0.1603, Xent Color 0.1373. Loss 1.2889, Error 0.0497(best: 0.0511), Error Color 0.0321(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 9.8942(9.9518) | Bit/dim 1.1612(1.1579) | Xent 0.2134(0.2531) | Xent Color 0.1025(0.2006) | Loss 2.9815(3.5802) | Error 0.0633(0.0815) | Error Color 0.0289(0.0579) |Steps 428(417.60) | Grad Norm 5.7713(17.6156) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 10.2116(9.9621) | Bit/dim 1.1125(1.1542) | Xent 0.2410(0.2582) | Xent Color 0.0754(0.1675) | Loss 3.0591(3.4376) | Error 0.0733(0.0833) | Error Color 0.0167(0.0469) |Steps 434(417.94) | Grad Norm 8.0318(14.9222) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 9.7696(9.9342) | Bit/dim 1.0970(1.1438) | Xent 0.2103(0.2533) | Xent Color 0.0482(0.1383) | Loss 2.8943(3.3174) | Error 0.0744(0.0825) | Error Color 0.0078(0.0378) |Steps 380(417.23) | Grad Norm 1.9968(12.0507) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 10.1685(9.9585) | Bit/dim 1.1061(1.1344) | Xent 0.2421(0.2492) | Xent Color 0.0451(0.1147) | Loss 2.9471(3.2319) | Error 0.0756(0.0813) | Error Color 0.0100(0.0300) |Steps 386(415.97) | Grad Norm 1.4030(9.7884) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 9.9869(9.9592) | Bit/dim 1.0983(1.1254) | Xent 0.2407(0.2455) | Xent Color 0.0420(0.0965) | Loss 2.9842(3.1557) | Error 0.0678(0.0798) | Error Color 0.0044(0.0238) |Steps 404(417.01) | Grad Norm 1.7519(7.9338) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 10.1936(9.9717) | Bit/dim 1.0817(1.1172) | Xent 0.2257(0.2424) | Xent Color 0.0347(0.0818) | Loss 2.9598(3.1004) | Error 0.0656(0.0785) | Error Color 0.0033(0.0192) |Steps 416(416.31) | Grad Norm 2.1869(6.3750) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 61.5596, Epoch Time 740.9253(695.9440), Bit/dim 1.0841(best: 1.1283), Xent 0.1347, Xent Color 0.0126. Loss 1.1209, Error 0.0428(best: 0.0497), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 9.7901(9.9660) | Bit/dim 1.0824(1.1084) | Xent 0.2865(0.2435) | Xent Color 0.0390(0.0698) | Loss 2.9486(3.5640) | Error 0.0811(0.0787) | Error Color 0.0044(0.0154) |Steps 422(419.97) | Grad Norm 1.5701(5.1896) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 10.1396(10.0159) | Bit/dim 1.0727(1.1007) | Xent 0.2781(0.2429) | Xent Color 0.0330(0.0609) | Loss 2.8799(3.3891) | Error 0.0956(0.0784) | Error Color 0.0044(0.0128) |Steps 428(418.37) | Grad Norm 2.4037(4.3921) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 10.1270(10.0300) | Bit/dim 1.0632(1.0940) | Xent 0.2136(0.2398) | Xent Color 0.0385(0.0543) | Loss 2.8993(3.2655) | Error 0.0756(0.0776) | Error Color 0.0056(0.0108) |Steps 404(419.56) | Grad Norm 1.9015(3.8557) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 10.3112(9.9721) | Bit/dim 1.0738(1.0887) | Xent 0.2196(0.2342) | Xent Color 0.0391(0.0485) | Loss 2.9600(3.1679) | Error 0.0689(0.0761) | Error Color 0.0078(0.0092) |Steps 428(420.08) | Grad Norm 1.4234(3.3690) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 10.0702(9.9707) | Bit/dim 1.0790(1.0838) | Xent 0.2278(0.2320) | Xent Color 0.0276(0.0439) | Loss 2.9681(3.0973) | Error 0.0811(0.0755) | Error Color 0.0000(0.0078) |Steps 416(419.44) | Grad Norm 2.0712(3.1518) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 10.7917(10.0011) | Bit/dim 1.0740(1.0792) | Xent 0.2184(0.2304) | Xent Color 0.0246(0.0401) | Loss 2.8859(3.0401) | Error 0.0633(0.0743) | Error Color 0.0011(0.0068) |Steps 416(419.62) | Grad Norm 2.6598(3.0097) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 10.0838(9.9922) | Bit/dim 1.0407(1.0748) | Xent 0.2032(0.2301) | Xent Color 0.0314(0.0374) | Loss 2.8912(3.0016) | Error 0.0544(0.0740) | Error Color 0.0089(0.0062) |Steps 452(420.31) | Grad Norm 3.0288(2.7945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 61.2242, Epoch Time 744.8387(697.4108), Bit/dim 1.0596(best: 1.0841), Xent 0.1329, Xent Color 0.0082. Loss 1.0949, Error 0.0418(best: 0.0428), Error Color 0.0000(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.8658(9.9596) | Bit/dim 1.0564(1.0703) | Xent 0.2252(0.2310) | Xent Color 0.0298(0.0348) | Loss 2.8911(3.4287) | Error 0.0644(0.0742) | Error Color 0.0044(0.0055) |Steps 410(419.54) | Grad Norm 2.9251(2.9231) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.8224(9.9614) | Bit/dim 1.0545(1.0666) | Xent 0.1622(0.2290) | Xent Color 0.0259(0.0326) | Loss 2.7723(3.2777) | Error 0.0511(0.0734) | Error Color 0.0022(0.0050) |Steps 434(420.18) | Grad Norm 1.7160(2.9518) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 9.8698(9.9811) | Bit/dim 1.0643(1.0640) | Xent 0.2720(0.2261) | Xent Color 0.0232(0.0307) | Loss 2.8905(3.1678) | Error 0.0733(0.0722) | Error Color 0.0022(0.0047) |Steps 416(419.53) | Grad Norm 2.8079(2.8029) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 9.6367(9.9873) | Bit/dim 1.0525(1.0598) | Xent 0.2062(0.2215) | Xent Color 0.0249(0.0292) | Loss 2.8156(3.0885) | Error 0.0689(0.0710) | Error Color 0.0033(0.0044) |Steps 410(418.75) | Grad Norm 1.8780(2.9170) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 9.9667(9.9706) | Bit/dim 1.0257(1.0560) | Xent 0.2116(0.2184) | Xent Color 0.0213(0.0278) | Loss 2.8210(3.0251) | Error 0.0767(0.0705) | Error Color 0.0022(0.0039) |Steps 434(421.24) | Grad Norm 3.1927(2.8188) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 10.0821(9.9548) | Bit/dim 1.0305(1.0521) | Xent 0.2085(0.2183) | Xent Color 0.0222(0.0270) | Loss 2.8289(2.9727) | Error 0.0689(0.0705) | Error Color 0.0022(0.0036) |Steps 416(422.33) | Grad Norm 4.7146(3.2791) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 9.8994(9.9398) | Bit/dim 1.0236(1.0486) | Xent 0.2279(0.2208) | Xent Color 0.0244(0.0258) | Loss 2.8363(2.9361) | Error 0.0722(0.0716) | Error Color 0.0033(0.0033) |Steps 446(421.80) | Grad Norm 4.8054(3.6468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 60.8256, Epoch Time 740.6714(698.7086), Bit/dim 1.0452(best: 1.0596), Xent 0.1195, Xent Color 0.0067. Loss 1.0768, Error 0.0379(best: 0.0418), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 10.4780(9.9525) | Bit/dim 1.0471(1.0458) | Xent 0.2120(0.2209) | Xent Color 0.0223(0.0258) | Loss 2.9276(3.3194) | Error 0.0700(0.0712) | Error Color 0.0011(0.0033) |Steps 434(422.41) | Grad Norm 4.8969(4.1732) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 10.0641(9.9333) | Bit/dim 1.0385(1.0433) | Xent 0.1822(0.2132) | Xent Color 0.0200(0.0250) | Loss 2.8072(3.1907) | Error 0.0678(0.0696) | Error Color 0.0022(0.0033) |Steps 404(421.88) | Grad Norm 4.6398(4.1396) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 9.7274(9.9416) | Bit/dim 1.0351(1.0403) | Xent 0.2751(0.2149) | Xent Color 0.0209(0.0241) | Loss 2.8048(3.0909) | Error 0.0889(0.0704) | Error Color 0.0033(0.0033) |Steps 404(420.71) | Grad Norm 11.4011(4.8126) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 9.9227(9.9447) | Bit/dim 1.0284(1.0378) | Xent 0.2142(0.2173) | Xent Color 0.0193(0.0233) | Loss 2.7758(3.0179) | Error 0.0689(0.0710) | Error Color 0.0033(0.0033) |Steps 398(419.68) | Grad Norm 6.6961(5.3199) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 9.5276(9.8872) | Bit/dim 1.0340(1.0348) | Xent 0.1981(0.2160) | Xent Color 0.0189(0.0226) | Loss 2.8111(2.9596) | Error 0.0600(0.0706) | Error Color 0.0022(0.0030) |Steps 404(417.39) | Grad Norm 3.4899(5.0287) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 10.1238(9.8776) | Bit/dim 1.0178(1.0319) | Xent 0.1969(0.2111) | Xent Color 0.0175(0.0223) | Loss 2.7878(2.9135) | Error 0.0678(0.0682) | Error Color 0.0022(0.0030) |Steps 458(419.36) | Grad Norm 4.4824(5.3165) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 61.0230, Epoch Time 735.4550(699.8110), Bit/dim 1.0207(best: 1.0452), Xent 0.1169, Xent Color 0.0062. Loss 1.0515, Error 0.0387(best: 0.0379), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.3210(9.8833) | Bit/dim 1.0231(1.0290) | Xent 0.1779(0.2055) | Xent Color 0.0198(0.0219) | Loss 2.8150(3.3753) | Error 0.0511(0.0662) | Error Color 0.0044(0.0030) |Steps 404(419.68) | Grad Norm 4.8185(5.4841) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.7552(9.8652) | Bit/dim 1.0136(1.0267) | Xent 0.2066(0.2064) | Xent Color 0.0193(0.0212) | Loss 2.8099(3.2212) | Error 0.0733(0.0659) | Error Color 0.0022(0.0027) |Steps 422(417.22) | Grad Norm 3.3013(5.8447) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 9.4911(9.8247) | Bit/dim 1.0091(1.0231) | Xent 0.1859(0.2056) | Xent Color 0.0175(0.0204) | Loss 2.7191(3.1022) | Error 0.0544(0.0654) | Error Color 0.0022(0.0025) |Steps 386(416.85) | Grad Norm 6.9730(6.0276) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.6941(9.7882) | Bit/dim 0.9983(1.0202) | Xent 0.2253(0.2038) | Xent Color 0.0278(0.0206) | Loss 2.6474(3.0101) | Error 0.0689(0.0654) | Error Color 0.0067(0.0027) |Steps 422(416.58) | Grad Norm 6.6546(6.8636) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 10.0330(9.8798) | Bit/dim 1.0115(1.0174) | Xent 0.1964(0.2071) | Xent Color 0.0173(0.0200) | Loss 2.7533(2.9523) | Error 0.0578(0.0664) | Error Color 0.0022(0.0027) |Steps 416(417.32) | Grad Norm 5.2581(6.6036) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 9.8640(9.9051) | Bit/dim 1.0173(1.0156) | Xent 0.2371(0.2051) | Xent Color 0.0196(0.0195) | Loss 2.8204(2.9030) | Error 0.0622(0.0651) | Error Color 0.0033(0.0027) |Steps 416(417.82) | Grad Norm 2.7231(6.0577) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 9.5985(9.8891) | Bit/dim 1.0000(1.0135) | Xent 0.2041(0.2050) | Xent Color 0.0166(0.0190) | Loss 2.7102(2.8618) | Error 0.0678(0.0658) | Error Color 0.0011(0.0023) |Steps 422(418.06) | Grad Norm 3.6027(6.7696) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 61.4368, Epoch Time 736.0917(700.8995), Bit/dim 1.0045(best: 1.0207), Xent 0.1115, Xent Color 0.0048. Loss 1.0335, Error 0.0361(best: 0.0379), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 9.8212(9.8912) | Bit/dim 1.0040(1.0113) | Xent 0.1948(0.2015) | Xent Color 0.0180(0.0195) | Loss 2.7501(3.2702) | Error 0.0767(0.0649) | Error Color 0.0022(0.0025) |Steps 446(419.05) | Grad Norm 11.6299(7.1288) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 10.0064(9.9169) | Bit/dim 0.9862(1.0077) | Xent 0.1760(0.1980) | Xent Color 0.0173(0.0188) | Loss 2.6641(3.1299) | Error 0.0656(0.0641) | Error Color 0.0011(0.0023) |Steps 410(418.30) | Grad Norm 4.2466(7.1971) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 9.8958(9.9053) | Bit/dim 0.9969(1.0055) | Xent 0.2078(0.2012) | Xent Color 0.0170(0.0186) | Loss 2.7227(3.0296) | Error 0.0644(0.0643) | Error Color 0.0000(0.0022) |Steps 422(418.79) | Grad Norm 10.2391(7.6454) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 10.2271(9.9073) | Bit/dim 1.0097(1.0035) | Xent 0.1765(0.2017) | Xent Color 0.0144(0.0182) | Loss 2.8082(2.9594) | Error 0.0578(0.0647) | Error Color 0.0000(0.0020) |Steps 446(420.59) | Grad Norm 11.7598(7.9729) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 10.0791(9.8907) | Bit/dim 0.9878(1.0006) | Xent 0.2155(0.2028) | Xent Color 0.0214(0.0187) | Loss 2.7526(2.9030) | Error 0.0589(0.0646) | Error Color 0.0033(0.0022) |Steps 422(419.92) | Grad Norm 11.0581(8.4009) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 10.0354(9.8967) | Bit/dim 0.9830(0.9977) | Xent 0.2406(0.2041) | Xent Color 0.0187(0.0185) | Loss 2.7296(2.8607) | Error 0.0744(0.0644) | Error Color 0.0033(0.0021) |Steps 428(420.53) | Grad Norm 5.7774(8.5664) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 61.2126, Epoch Time 737.6936(702.0033), Bit/dim 0.9868(best: 1.0045), Xent 0.1076, Xent Color 0.0037. Loss 1.0146, Error 0.0361(best: 0.0361), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.5990(9.9034) | Bit/dim 0.9895(0.9946) | Xent 0.1828(0.2021) | Xent Color 0.0160(0.0180) | Loss 2.7366(3.3351) | Error 0.0622(0.0642) | Error Color 0.0011(0.0022) |Steps 428(422.12) | Grad Norm 10.3292(8.1781) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 10.0830(9.8576) | Bit/dim 0.9871(0.9941) | Xent 0.1697(0.1961) | Xent Color 0.0224(0.0186) | Loss 2.6868(3.1763) | Error 0.0544(0.0631) | Error Color 0.0044(0.0024) |Steps 428(420.38) | Grad Norm 8.4749(9.6077) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 9.7258(9.8484) | Bit/dim 0.9942(0.9930) | Xent 0.2034(0.1985) | Xent Color 0.0145(0.0187) | Loss 2.7552(3.0603) | Error 0.0622(0.0641) | Error Color 0.0011(0.0026) |Steps 428(420.43) | Grad Norm 8.7037(9.7378) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 9.9190(9.8302) | Bit/dim 0.9804(0.9904) | Xent 0.1751(0.1960) | Xent Color 0.0228(0.0177) | Loss 2.7212(2.9687) | Error 0.0544(0.0631) | Error Color 0.0033(0.0023) |Steps 404(421.06) | Grad Norm 9.3443(9.8019) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 9.5103(9.8303) | Bit/dim 0.9749(0.9870) | Xent 0.2027(0.1990) | Xent Color 0.0179(0.0174) | Loss 2.6320(2.8965) | Error 0.0689(0.0641) | Error Color 0.0011(0.0021) |Steps 398(418.32) | Grad Norm 9.5847(9.8344) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 9.7369(9.8332) | Bit/dim 0.9736(0.9837) | Xent 0.1581(0.1950) | Xent Color 0.0144(0.0167) | Loss 2.6155(2.8398) | Error 0.0456(0.0624) | Error Color 0.0022(0.0021) |Steps 416(417.80) | Grad Norm 5.8698(9.2183) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 9.9472(9.8363) | Bit/dim 0.9831(0.9822) | Xent 0.1959(0.1935) | Xent Color 0.0142(0.0165) | Loss 2.6353(2.8003) | Error 0.0700(0.0625) | Error Color 0.0011(0.0021) |Steps 422(416.11) | Grad Norm 12.8073(10.1938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 61.8641, Epoch Time 731.8608(702.8990), Bit/dim 0.9822(best: 0.9868), Xent 0.0978, Xent Color 0.0056. Loss 1.0080, Error 0.0318(best: 0.0361), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 9.7744(9.8784) | Bit/dim 0.9643(0.9809) | Xent 0.2018(0.1920) | Xent Color 0.0189(0.0169) | Loss 2.7433(3.2419) | Error 0.0689(0.0615) | Error Color 0.0056(0.0024) |Steps 428(416.14) | Grad Norm 10.8841(10.7673) | Total Time 0.00(0.00)\n",
      "Iter 2920 | Time 9.6901(9.8692) | Bit/dim 0.9629(0.9786) | Xent 0.1816(0.1894) | Xent Color 0.0231(0.0164) | Loss 2.6356(3.0976) | Error 0.0567(0.0609) | Error Color 0.0067(0.0023) |Steps 404(417.05) | Grad Norm 13.4832(10.7480) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 9.7307(9.8459) | Bit/dim 0.9550(0.9759) | Xent 0.2014(0.1888) | Xent Color 0.0167(0.0168) | Loss 2.6718(2.9885) | Error 0.0600(0.0601) | Error Color 0.0022(0.0025) |Steps 434(416.02) | Grad Norm 7.1154(10.5601) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 10.1756(9.8829) | Bit/dim 0.9895(0.9753) | Xent 0.1754(0.1882) | Xent Color 0.0365(0.0171) | Loss 2.7288(2.9113) | Error 0.0556(0.0597) | Error Color 0.0122(0.0028) |Steps 434(417.28) | Grad Norm 29.8269(11.4590) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 9.5560(9.8697) | Bit/dim 1.8925(1.1560) | Xent 0.6666(0.2602) | Xent Color 2.3241(1.1115) | Loss 5.6555(3.7107) | Error 0.1689(0.0810) | Error Color 0.6011(0.1389) |Steps 434(418.15) | Grad Norm 36.6453(38.6819) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 9.1887(9.7529) | Bit/dim 1.5811(1.2993) | Xent 0.3464(0.3126) | Xent Color 0.7973(1.1005) | Loss 4.0793(3.9482) | Error 0.1156(0.0961) | Error Color 0.3222(0.2063) |Steps 404(415.14) | Grad Norm 14.7593(32.1025) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 9.8758(9.6515) | Bit/dim 1.4598(1.3612) | Xent 0.2997(0.3150) | Xent Color 0.4700(0.9651) | Loss 3.8279(3.9573) | Error 0.0900(0.0975) | Error Color 0.1944(0.2137) |Steps 434(411.93) | Grad Norm 3.2337(25.8451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 60.5276, Epoch Time 725.3926(703.5738), Bit/dim 1.4581(best: 0.9822), Xent 0.1592, Xent Color 0.2844. Loss 1.5690, Error 0.0475(best: 0.0318), Error Color 0.0920(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 9.7499(9.6321) | Bit/dim 1.3328(1.3685) | Xent 0.2570(0.3036) | Xent Color 0.3690(0.8274) | Loss 3.5269(4.2707) | Error 0.0878(0.0940) | Error Color 0.1333(0.2034) |Steps 416(412.64) | Grad Norm 4.9334(20.8522) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 9.7974(9.6095) | Bit/dim 1.2576(1.3478) | Xent 0.2236(0.2810) | Xent Color 0.2548(0.6966) | Loss 3.2734(4.0297) | Error 0.0744(0.0884) | Error Color 0.0944(0.1830) |Steps 428(413.11) | Grad Norm 3.4790(16.4897) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 9.6265(9.6120) | Bit/dim 1.1831(1.3116) | Xent 0.1873(0.2666) | Xent Color 0.1802(0.5747) | Loss 3.1157(3.8095) | Error 0.0556(0.0837) | Error Color 0.0622(0.1566) |Steps 428(412.96) | Grad Norm 2.6165(13.1178) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 10.0238(9.6581) | Bit/dim 1.1422(1.2714) | Xent 0.2227(0.2527) | Xent Color 0.1259(0.4666) | Loss 3.0887(3.6189) | Error 0.0667(0.0796) | Error Color 0.0389(0.1287) |Steps 410(414.07) | Grad Norm 2.7604(10.3255) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 9.8229(9.6902) | Bit/dim 1.1079(1.2345) | Xent 0.1917(0.2420) | Xent Color 0.1051(0.3736) | Loss 2.9562(3.4526) | Error 0.0589(0.0767) | Error Color 0.0311(0.1030) |Steps 422(413.20) | Grad Norm 2.3411(8.2415) | Total Time 0.00(0.00)\n",
      "Iter 3030 | Time 9.9208(9.7317) | Bit/dim 1.1018(1.1992) | Xent 0.1822(0.2316) | Xent Color 0.0914(0.2996) | Loss 2.9546(3.3136) | Error 0.0556(0.0741) | Error Color 0.0233(0.0825) |Steps 428(414.33) | Grad Norm 2.3004(6.7095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 62.4498, Epoch Time 726.4102(704.2589), Bit/dim 1.0776(best: 0.9822), Xent 0.1155, Xent Color 0.0256. Loss 1.1129, Error 0.0371(best: 0.0318), Error Color 0.0015(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 9.5521(9.7837) | Bit/dim 1.0807(1.1663) | Xent 0.2116(0.2246) | Xent Color 0.0574(0.2399) | Loss 2.9128(3.6956) | Error 0.0700(0.0713) | Error Color 0.0111(0.0649) |Steps 428(416.07) | Grad Norm 2.9526(5.6729) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 9.7871(9.8194) | Bit/dim 1.0643(1.1391) | Xent 0.2112(0.2200) | Xent Color 0.0615(0.1937) | Loss 2.8768(3.4828) | Error 0.0578(0.0699) | Error Color 0.0144(0.0517) |Steps 422(416.53) | Grad Norm 3.7492(4.8694) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 10.3064(9.8695) | Bit/dim 1.0393(1.1159) | Xent 0.2330(0.2138) | Xent Color 0.0549(0.1575) | Loss 2.8120(3.3189) | Error 0.0756(0.0676) | Error Color 0.0122(0.0414) |Steps 410(419.25) | Grad Norm 3.1379(4.2239) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 10.0049(9.8882) | Bit/dim 1.0360(1.0966) | Xent 0.2046(0.2121) | Xent Color 0.0498(0.1294) | Loss 2.7455(3.1864) | Error 0.0600(0.0676) | Error Color 0.0122(0.0332) |Steps 410(418.78) | Grad Norm 2.1981(3.9229) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 10.0056(9.9186) | Bit/dim 1.0197(1.0780) | Xent 0.2208(0.2124) | Xent Color 0.0492(0.1076) | Loss 2.7759(3.0845) | Error 0.0633(0.0672) | Error Color 0.0067(0.0268) |Steps 422(421.64) | Grad Norm 3.7349(3.7423) | Total Time 0.00(0.00)\n",
      "Iter 3090 | Time 9.6199(9.9408) | Bit/dim 0.9914(1.0627) | Xent 0.1925(0.2088) | Xent Color 0.0440(0.0901) | Loss 2.7103(3.0043) | Error 0.0700(0.0664) | Error Color 0.0056(0.0216) |Steps 410(422.29) | Grad Norm 4.4276(3.6702) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 9.8126(9.9676) | Bit/dim 1.0062(1.0511) | Xent 0.2337(0.2053) | Xent Color 0.0403(0.0769) | Loss 2.7738(2.9477) | Error 0.0722(0.0655) | Error Color 0.0100(0.0181) |Steps 416(423.84) | Grad Norm 3.2413(3.7384) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 62.6539, Epoch Time 743.9655(705.4501), Bit/dim 1.0139(best: 0.9822), Xent 0.1045, Xent Color 0.0095. Loss 1.0424, Error 0.0337(best: 0.0318), Error Color 0.0002(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 9.8628(9.9930) | Bit/dim 1.0198(1.0406) | Xent 0.1836(0.1989) | Xent Color 0.0367(0.0663) | Loss 2.7799(3.3477) | Error 0.0511(0.0637) | Error Color 0.0067(0.0151) |Steps 422(424.70) | Grad Norm 7.2691(3.8474) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 9.8704(10.0217) | Bit/dim 0.9900(1.0313) | Xent 0.1629(0.1953) | Xent Color 0.0358(0.0578) | Loss 2.7388(3.2015) | Error 0.0522(0.0624) | Error Color 0.0067(0.0129) |Steps 434(426.58) | Grad Norm 2.6916(4.0703) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 9.7771(10.0285) | Bit/dim 1.0028(1.0238) | Xent 0.1755(0.1958) | Xent Color 0.0407(0.0514) | Loss 2.7679(3.0911) | Error 0.0578(0.0626) | Error Color 0.0089(0.0111) |Steps 428(428.13) | Grad Norm 3.7804(4.2367) | Total Time 0.00(0.00)\n",
      "Iter 3140 | Time 10.0632(10.0255) | Bit/dim 0.9931(1.0170) | Xent 0.1986(0.1996) | Xent Color 0.0380(0.0466) | Loss 2.7352(3.0074) | Error 0.0567(0.0637) | Error Color 0.0100(0.0097) |Steps 428(428.59) | Grad Norm 7.9253(4.4239) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 10.0166(10.0311) | Bit/dim 0.9929(1.0094) | Xent 0.2134(0.1984) | Xent Color 0.0288(0.0417) | Loss 2.7586(2.9455) | Error 0.0644(0.0636) | Error Color 0.0056(0.0083) |Steps 446(430.32) | Grad Norm 3.8569(4.4782) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 10.0257(9.9792) | Bit/dim 0.9940(1.0039) | Xent 0.2027(0.1995) | Xent Color 0.0329(0.0385) | Loss 2.7734(2.8996) | Error 0.0722(0.0633) | Error Color 0.0056(0.0074) |Steps 434(431.95) | Grad Norm 6.7779(4.4213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 62.1498, Epoch Time 746.6255(706.6854), Bit/dim 0.9851(best: 0.9822), Xent 0.1040, Xent Color 0.0062. Loss 1.0127, Error 0.0337(best: 0.0318), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 10.1376(10.0379) | Bit/dim 0.9868(0.9993) | Xent 0.1587(0.1921) | Xent Color 0.0184(0.0349) | Loss 2.7682(3.4149) | Error 0.0522(0.0621) | Error Color 0.0011(0.0065) |Steps 446(434.56) | Grad Norm 7.0810(4.6450) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 9.8589(10.0629) | Bit/dim 0.9809(0.9960) | Xent 0.1716(0.1904) | Xent Color 0.0326(0.0336) | Loss 2.7354(3.2417) | Error 0.0467(0.0613) | Error Color 0.0056(0.0062) |Steps 434(433.68) | Grad Norm 10.8424(5.8701) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 10.1516(10.0607) | Bit/dim 0.9915(0.9924) | Xent 0.1668(0.1882) | Xent Color 0.0333(0.0325) | Loss 2.7943(3.1099) | Error 0.0611(0.0606) | Error Color 0.0067(0.0062) |Steps 440(432.99) | Grad Norm 13.1718(7.0629) | Total Time 0.00(0.00)\n",
      "Iter 3200 | Time 9.9143(10.0442) | Bit/dim 0.9769(0.9894) | Xent 0.1953(0.1879) | Xent Color 0.0223(0.0316) | Loss 2.7480(3.0112) | Error 0.0589(0.0599) | Error Color 0.0011(0.0062) |Steps 434(429.91) | Grad Norm 7.9615(7.5648) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 10.0265(10.0245) | Bit/dim 0.9724(0.9845) | Xent 0.1460(0.1848) | Xent Color 0.0160(0.0295) | Loss 2.6732(2.9297) | Error 0.0544(0.0592) | Error Color 0.0011(0.0057) |Steps 428(430.65) | Grad Norm 5.3616(7.0153) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 10.4841(10.0250) | Bit/dim 0.9684(0.9801) | Xent 0.1898(0.1865) | Xent Color 0.0166(0.0273) | Loss 2.7426(2.8721) | Error 0.0600(0.0605) | Error Color 0.0022(0.0052) |Steps 440(429.92) | Grad Norm 1.5210(6.2718) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 9.8801(10.0353) | Bit/dim 0.9609(0.9754) | Xent 0.1998(0.1877) | Xent Color 0.0196(0.0255) | Loss 2.7059(2.8255) | Error 0.0711(0.0614) | Error Color 0.0022(0.0047) |Steps 452(431.19) | Grad Norm 4.9737(5.4840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 63.2802, Epoch Time 749.8070(707.9790), Bit/dim 0.9603(best: 0.9822), Xent 0.0963, Xent Color 0.0044. Loss 0.9855, Error 0.0325(best: 0.0318), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 10.1708(10.0496) | Bit/dim 0.9517(0.9718) | Xent 0.2006(0.1869) | Xent Color 0.0240(0.0241) | Loss 2.7138(3.2813) | Error 0.0622(0.0602) | Error Color 0.0033(0.0041) |Steps 434(432.73) | Grad Norm 3.1337(5.1905) | Total Time 0.00(0.00)\n",
      "Iter 3250 | Time 10.0780(10.0361) | Bit/dim 0.9710(0.9705) | Xent 0.2015(0.1860) | Xent Color 0.0272(0.0237) | Loss 2.7402(3.1320) | Error 0.0578(0.0592) | Error Color 0.0067(0.0042) |Steps 428(432.79) | Grad Norm 13.2411(6.1837) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 9.6251(10.0126) | Bit/dim 0.9520(0.9679) | Xent 0.1843(0.1856) | Xent Color 0.0185(0.0233) | Loss 2.6759(3.0189) | Error 0.0578(0.0593) | Error Color 0.0044(0.0041) |Steps 428(433.05) | Grad Norm 1.7337(6.7352) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 9.9014(9.9980) | Bit/dim 0.9579(0.9640) | Xent 0.1488(0.1838) | Xent Color 0.0146(0.0221) | Loss 2.7370(2.9335) | Error 0.0478(0.0578) | Error Color 0.0033(0.0039) |Steps 440(432.43) | Grad Norm 6.0845(6.5351) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 10.8453(10.0210) | Bit/dim 0.9597(0.9615) | Xent 0.1788(0.1849) | Xent Color 0.0166(0.0213) | Loss 2.6933(2.8668) | Error 0.0522(0.0576) | Error Color 0.0011(0.0034) |Steps 434(429.30) | Grad Norm 5.6263(6.1462) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 10.4642(10.0360) | Bit/dim 0.9650(0.9611) | Xent 0.1567(0.1838) | Xent Color 0.0204(0.0225) | Loss 2.7066(2.8231) | Error 0.0389(0.0562) | Error Color 0.0022(0.0039) |Steps 434(429.36) | Grad Norm 13.6179(8.1149) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 10.0604(10.0337) | Bit/dim 0.9494(0.9602) | Xent 0.1962(0.1800) | Xent Color 0.0157(0.0216) | Loss 2.6698(2.7853) | Error 0.0656(0.0553) | Error Color 0.0000(0.0037) |Steps 422(429.46) | Grad Norm 10.8292(8.6030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 62.4826, Epoch Time 746.1104(709.1230), Bit/dim 0.9511(best: 0.9603), Xent 0.0942, Xent Color 0.0034. Loss 0.9755, Error 0.0332(best: 0.0318), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 10.2140(10.0201) | Bit/dim 0.9435(0.9566) | Xent 0.2105(0.1809) | Xent Color 0.0217(0.0205) | Loss 2.6805(3.1721) | Error 0.0700(0.0557) | Error Color 0.0033(0.0034) |Steps 464(429.66) | Grad Norm 9.0449(8.2364) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 9.9146(9.9932) | Bit/dim 0.9472(0.9539) | Xent 0.1724(0.1793) | Xent Color 0.0181(0.0195) | Loss 2.7116(3.0434) | Error 0.0611(0.0560) | Error Color 0.0000(0.0031) |Steps 440(429.86) | Grad Norm 8.5362(7.8337) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 10.1275(10.0356) | Bit/dim 0.9389(0.9508) | Xent 0.1345(0.1776) | Xent Color 0.0141(0.0186) | Loss 2.6623(2.9494) | Error 0.0433(0.0561) | Error Color 0.0000(0.0028) |Steps 440(430.81) | Grad Norm 3.5640(7.0091) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 10.1806(10.0283) | Bit/dim 0.9784(0.9509) | Xent 0.1516(0.1746) | Xent Color 0.0734(0.0216) | Loss 2.7506(2.8777) | Error 0.0489(0.0553) | Error Color 0.0222(0.0038) |Steps 422(431.02) | Grad Norm 30.6635(8.7465) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 10.1722(10.0191) | Bit/dim 1.4188(1.0335) | Xent 0.2294(0.2021) | Xent Color 0.7527(0.4644) | Loss 3.9932(3.1928) | Error 0.0767(0.0646) | Error Color 0.2744(0.0949) |Steps 440(431.43) | Grad Norm 19.9726(25.8247) | Total Time 0.00(0.00)\n",
      "Iter 3360 | Time 10.1678(10.0760) | Bit/dim 1.2558(1.1061) | Xent 0.2442(0.2210) | Xent Color 0.3548(0.4824) | Loss 3.4827(3.3225) | Error 0.0744(0.0703) | Error Color 0.1244(0.1247) |Steps 440(435.18) | Grad Norm 8.5688(22.4711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 63.2555, Epoch Time 748.8667(710.3153), Bit/dim 1.1470(best: 0.9511), Xent 0.1236, Xent Color 0.1007. Loss 1.2031, Error 0.0425(best: 0.0318), Error Color 0.0158(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 9.5075(10.0330) | Bit/dim 1.1128(1.1210) | Xent 0.2375(0.2267) | Xent Color 0.1775(0.4228) | Loss 3.0024(3.7906) | Error 0.0678(0.0711) | Error Color 0.0578(0.1160) |Steps 410(431.28) | Grad Norm 8.4765(18.8350) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 9.9353(9.9481) | Bit/dim 1.0736(1.1130) | Xent 0.1900(0.2213) | Xent Color 0.1054(0.3466) | Loss 2.8955(3.5770) | Error 0.0644(0.0702) | Error Color 0.0244(0.0958) |Steps 404(427.35) | Grad Norm 4.3125(15.1443) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 9.9532(9.9152) | Bit/dim 1.0314(1.0944) | Xent 0.2090(0.2163) | Xent Color 0.0733(0.2791) | Loss 2.8720(3.3892) | Error 0.0667(0.0686) | Error Color 0.0178(0.0766) |Steps 440(426.86) | Grad Norm 2.9951(12.1196) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 9.7606(9.8522) | Bit/dim 1.0039(1.0729) | Xent 0.1760(0.2086) | Xent Color 0.0629(0.2244) | Loss 2.7870(3.2324) | Error 0.0556(0.0656) | Error Color 0.0133(0.0609) |Steps 446(425.06) | Grad Norm 2.6276(9.5938) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 9.7077(9.9676) | Bit/dim 0.9804(1.0506) | Xent 0.1600(0.2044) | Xent Color 0.0549(0.1808) | Loss 2.7109(3.1069) | Error 0.0467(0.0641) | Error Color 0.0111(0.0484) |Steps 422(425.59) | Grad Norm 1.4441(7.6591) | Total Time 0.00(0.00)\n",
      "Iter 3420 | Time 10.3066(9.9752) | Bit/dim 0.9811(1.0319) | Xent 0.1604(0.1978) | Xent Color 0.0476(0.1458) | Loss 2.7575(3.0080) | Error 0.0544(0.0619) | Error Color 0.0089(0.0383) |Steps 434(425.90) | Grad Norm 2.2799(6.1711) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 10.4871(9.9434) | Bit/dim 0.9766(1.0155) | Xent 0.2029(0.1949) | Xent Color 0.0364(0.1191) | Loss 2.7902(2.9281) | Error 0.0711(0.0612) | Error Color 0.0044(0.0308) |Steps 428(424.15) | Grad Norm 3.4294(5.3222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 62.6018, Epoch Time 737.7118(711.1372), Bit/dim 0.9614(best: 0.9511), Xent 0.1050, Xent Color 0.0118. Loss 0.9906, Error 0.0353(best: 0.0318), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 9.6847(9.9754) | Bit/dim 0.9445(1.0007) | Xent 0.1450(0.1911) | Xent Color 0.0399(0.0979) | Loss 2.6501(3.3113) | Error 0.0489(0.0604) | Error Color 0.0089(0.0245) |Steps 416(425.61) | Grad Norm 2.5553(4.5582) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 9.9086(9.9850) | Bit/dim 0.9609(0.9891) | Xent 0.1808(0.1885) | Xent Color 0.0284(0.0813) | Loss 2.6521(3.1453) | Error 0.0656(0.0602) | Error Color 0.0033(0.0197) |Steps 404(425.79) | Grad Norm 3.0834(4.0302) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 10.5409(10.0416) | Bit/dim 0.9555(0.9785) | Xent 0.1771(0.1856) | Xent Color 0.0270(0.0683) | Loss 2.7571(3.0264) | Error 0.0544(0.0593) | Error Color 0.0033(0.0158) |Steps 428(427.21) | Grad Norm 2.2932(3.6306) | Total Time 0.00(0.00)\n",
      "Iter 3470 | Time 9.9091(10.0162) | Bit/dim 0.9500(0.9706) | Xent 0.1921(0.1838) | Xent Color 0.0286(0.0581) | Loss 2.6596(2.9300) | Error 0.0611(0.0586) | Error Color 0.0056(0.0130) |Steps 416(425.94) | Grad Norm 3.3881(3.5277) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 9.8446(9.9992) | Bit/dim 0.9539(0.9632) | Xent 0.1940(0.1855) | Xent Color 0.0269(0.0501) | Loss 2.6905(2.8577) | Error 0.0722(0.0599) | Error Color 0.0044(0.0105) |Steps 446(426.58) | Grad Norm 3.6363(3.4326) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 10.5021(10.0406) | Bit/dim 0.9381(0.9574) | Xent 0.1705(0.1837) | Xent Color 0.0247(0.0436) | Loss 2.6521(2.8059) | Error 0.0600(0.0597) | Error Color 0.0056(0.0088) |Steps 458(429.38) | Grad Norm 2.7728(3.5679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 63.2518, Epoch Time 750.0069(712.3033), Bit/dim 0.9337(best: 0.9511), Xent 0.0895, Xent Color 0.0059. Loss 0.9576, Error 0.0295(best: 0.0318), Error Color 0.0002(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 10.2165(10.0681) | Bit/dim 0.9303(0.9519) | Xent 0.1693(0.1795) | Xent Color 0.0222(0.0386) | Loss 2.6586(3.3046) | Error 0.0611(0.0580) | Error Color 0.0044(0.0074) |Steps 416(427.76) | Grad Norm 5.3889(3.5530) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 10.0824(10.0708) | Bit/dim 0.9413(0.9473) | Xent 0.1964(0.1798) | Xent Color 0.0237(0.0354) | Loss 2.7014(3.1352) | Error 0.0633(0.0578) | Error Color 0.0044(0.0068) |Steps 446(428.98) | Grad Norm 9.1309(4.1362) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 9.8640(10.0752) | Bit/dim 0.9236(0.9431) | Xent 0.1768(0.1774) | Xent Color 0.0228(0.0323) | Loss 2.6438(3.0093) | Error 0.0567(0.0582) | Error Color 0.0022(0.0061) |Steps 428(431.13) | Grad Norm 4.0582(4.0394) | Total Time 0.00(0.00)\n",
      "Iter 3530 | Time 10.2881(10.0391) | Bit/dim 0.9359(0.9387) | Xent 0.1753(0.1722) | Xent Color 0.0170(0.0295) | Loss 2.6475(2.9099) | Error 0.0544(0.0562) | Error Color 0.0000(0.0053) |Steps 446(431.70) | Grad Norm 3.2994(4.0485) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 9.7013(10.0305) | Bit/dim 0.9283(0.9354) | Xent 0.1518(0.1707) | Xent Color 0.0189(0.0270) | Loss 2.6145(2.8387) | Error 0.0489(0.0551) | Error Color 0.0011(0.0046) |Steps 410(429.32) | Grad Norm 5.0692(4.4916) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 9.9838(10.0278) | Bit/dim 0.9220(0.9334) | Xent 0.1861(0.1691) | Xent Color 0.0265(0.0252) | Loss 2.6897(2.7942) | Error 0.0667(0.0544) | Error Color 0.0044(0.0042) |Steps 440(429.74) | Grad Norm 6.6541(4.9920) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 10.5152(9.9851) | Bit/dim 0.9361(0.9318) | Xent 0.1744(0.1697) | Xent Color 0.0198(0.0235) | Loss 2.7076(2.7484) | Error 0.0522(0.0539) | Error Color 0.0011(0.0036) |Steps 422(428.50) | Grad Norm 10.3236(5.4348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 64.2947, Epoch Time 747.5134(713.3596), Bit/dim 0.9264(best: 0.9337), Xent 0.0894, Xent Color 0.0043. Loss 0.9498, Error 0.0275(best: 0.0295), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 9.8891(10.0135) | Bit/dim 0.9213(0.9292) | Xent 0.1967(0.1692) | Xent Color 0.0206(0.0226) | Loss 2.5953(3.2041) | Error 0.0656(0.0540) | Error Color 0.0044(0.0035) |Steps 404(427.54) | Grad Norm 3.8139(5.5966) | Total Time 0.00(0.00)\n",
      "Iter 3580 | Time 10.3454(10.0092) | Bit/dim 0.9179(0.9258) | Xent 0.1980(0.1709) | Xent Color 0.0209(0.0216) | Loss 2.6869(3.0509) | Error 0.0544(0.0539) | Error Color 0.0056(0.0033) |Steps 434(425.27) | Grad Norm 3.3557(5.7527) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 10.3105(10.0412) | Bit/dim 0.9168(0.9232) | Xent 0.1240(0.1704) | Xent Color 0.0196(0.0207) | Loss 2.6369(2.9417) | Error 0.0356(0.0535) | Error Color 0.0022(0.0030) |Steps 440(425.48) | Grad Norm 3.6044(6.0021) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 9.7958(10.0250) | Bit/dim 0.9133(0.9207) | Xent 0.1796(0.1696) | Xent Color 0.0193(0.0198) | Loss 2.6211(2.8551) | Error 0.0611(0.0535) | Error Color 0.0033(0.0027) |Steps 428(427.04) | Grad Norm 7.3497(6.3560) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 10.1292(10.0175) | Bit/dim 0.9260(0.9202) | Xent 0.1896(0.1669) | Xent Color 0.0147(0.0192) | Loss 2.5874(2.7860) | Error 0.0578(0.0527) | Error Color 0.0011(0.0027) |Steps 422(425.30) | Grad Norm 11.3603(6.9503) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 10.1297(10.0758) | Bit/dim 0.9154(0.9184) | Xent 0.1435(0.1626) | Xent Color 0.0131(0.0189) | Loss 2.6020(2.7452) | Error 0.0444(0.0511) | Error Color 0.0011(0.0026) |Steps 422(426.99) | Grad Norm 4.3880(7.1115) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 10.0571(10.0794) | Bit/dim 0.8962(0.9157) | Xent 0.1934(0.1627) | Xent Color 0.0173(0.0180) | Loss 2.6032(2.7070) | Error 0.0611(0.0513) | Error Color 0.0000(0.0024) |Steps 428(427.58) | Grad Norm 3.8467(6.3217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 63.6213, Epoch Time 750.9729(714.4880), Bit/dim 0.9065(best: 0.9264), Xent 0.0863, Xent Color 0.0031. Loss 0.9289, Error 0.0270(best: 0.0275), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 9.8761(10.0548) | Bit/dim 0.8975(0.9131) | Xent 0.1825(0.1641) | Xent Color 0.0151(0.0174) | Loss 2.6017(3.0882) | Error 0.0478(0.0517) | Error Color 0.0000(0.0024) |Steps 434(427.95) | Grad Norm 9.0999(7.1645) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 10.3680(10.0710) | Bit/dim 0.9098(0.9116) | Xent 0.1666(0.1621) | Xent Color 0.0137(0.0167) | Loss 2.6133(2.9569) | Error 0.0589(0.0516) | Error Color 0.0022(0.0021) |Steps 434(426.37) | Grad Norm 5.8023(6.8627) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 10.3724(10.0440) | Bit/dim 0.9100(0.9099) | Xent 0.1200(0.1642) | Xent Color 0.0186(0.0165) | Loss 2.6135(2.8611) | Error 0.0378(0.0525) | Error Color 0.0056(0.0021) |Steps 446(426.43) | Grad Norm 17.0414(7.4059) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 9.7237(10.0460) | Bit/dim 0.8993(0.9089) | Xent 0.1426(0.1623) | Xent Color 0.0128(0.0164) | Loss 2.5501(2.7900) | Error 0.0500(0.0519) | Error Color 0.0011(0.0024) |Steps 428(426.62) | Grad Norm 8.4435(7.3640) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 9.9108(10.0510) | Bit/dim 0.9120(0.9075) | Xent 0.1483(0.1572) | Xent Color 0.0174(0.0162) | Loss 2.5589(2.7383) | Error 0.0533(0.0506) | Error Color 0.0033(0.0024) |Steps 434(428.70) | Grad Norm 13.2760(8.0941) | Total Time 0.00(0.00)\n",
      "Iter 3690 | Time 10.0150(10.0413) | Bit/dim 0.9076(0.9075) | Xent 0.1563(0.1595) | Xent Color 0.0129(0.0158) | Loss 2.5879(2.6991) | Error 0.0433(0.0512) | Error Color 0.0000(0.0022) |Steps 428(429.12) | Grad Norm 8.3626(9.0441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 64.8589, Epoch Time 750.6301(715.5722), Bit/dim 0.9318(best: 0.9065), Xent 0.0781, Xent Color 0.0060. Loss 0.9528, Error 0.0256(best: 0.0270), Error Color 0.0004(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 9.8596(10.0588) | Bit/dim 0.9059(0.9101) | Xent 0.1709(0.1598) | Xent Color 0.0191(0.0170) | Loss 2.5644(3.1855) | Error 0.0622(0.0509) | Error Color 0.0056(0.0026) |Steps 446(428.90) | Grad Norm 14.7429(10.9519) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 10.8073(10.0864) | Bit/dim 0.9044(0.9095) | Xent 0.1466(0.1604) | Xent Color 0.0123(0.0165) | Loss 2.6204(3.0385) | Error 0.0478(0.0517) | Error Color 0.0000(0.0025) |Steps 404(428.30) | Grad Norm 4.6407(10.7880) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 10.3382(10.0667) | Bit/dim 0.8912(0.9069) | Xent 0.1482(0.1600) | Xent Color 0.0113(0.0157) | Loss 2.6220(2.9181) | Error 0.0422(0.0512) | Error Color 0.0000(0.0022) |Steps 410(428.24) | Grad Norm 11.0589(10.5774) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 10.3755(10.0933) | Bit/dim 0.8932(0.9050) | Xent 0.1707(0.1581) | Xent Color 0.0172(0.0159) | Loss 2.5775(2.8292) | Error 0.0544(0.0502) | Error Color 0.0056(0.0025) |Steps 428(429.54) | Grad Norm 12.2573(10.9644) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 10.6965(10.0892) | Bit/dim 0.8929(0.9034) | Xent 0.1656(0.1574) | Xent Color 0.0126(0.0153) | Loss 2.5941(2.7638) | Error 0.0578(0.0504) | Error Color 0.0011(0.0023) |Steps 434(428.39) | Grad Norm 5.2851(10.5437) | Total Time 0.00(0.00)\n",
      "Iter 3750 | Time 10.1780(10.0999) | Bit/dim 0.8979(0.9025) | Xent 0.1589(0.1585) | Xent Color 0.0105(0.0145) | Loss 2.5551(2.7197) | Error 0.0511(0.0504) | Error Color 0.0000(0.0021) |Steps 428(427.53) | Grad Norm 9.2265(10.4932) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 10.3968(10.1193) | Bit/dim 0.8922(0.9006) | Xent 0.1331(0.1534) | Xent Color 0.0163(0.0149) | Loss 2.5803(2.6811) | Error 0.0433(0.0491) | Error Color 0.0033(0.0021) |Steps 428(428.34) | Grad Norm 5.1197(10.6229) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 64.0174, Epoch Time 753.6984(716.7160), Bit/dim 0.8881(best: 0.9065), Xent 0.0774, Xent Color 0.0027. Loss 0.9081, Error 0.0248(best: 0.0256), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 9.8006(10.1348) | Bit/dim 0.8998(0.8983) | Xent 0.1384(0.1510) | Xent Color 0.0090(0.0142) | Loss 2.5667(3.0935) | Error 0.0478(0.0482) | Error Color 0.0011(0.0019) |Steps 440(430.12) | Grad Norm 6.7558(10.0381) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 10.0806(10.1378) | Bit/dim 0.8765(0.8946) | Xent 0.1530(0.1507) | Xent Color 0.0134(0.0137) | Loss 2.5089(2.9540) | Error 0.0444(0.0476) | Error Color 0.0022(0.0018) |Steps 416(429.65) | Grad Norm 10.2775(9.7988) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 10.5126(10.1426) | Bit/dim 0.8858(0.8920) | Xent 0.1406(0.1512) | Xent Color 0.0094(0.0131) | Loss 2.5356(2.8487) | Error 0.0478(0.0477) | Error Color 0.0000(0.0015) |Steps 422(430.59) | Grad Norm 8.0029(9.0290) | Total Time 0.00(0.00)\n",
      "Iter 3800 | Time 9.9136(10.1337) | Bit/dim 0.8888(0.8898) | Xent 0.1831(0.1536) | Xent Color 0.0128(0.0127) | Loss 2.5770(2.7687) | Error 0.0589(0.0481) | Error Color 0.0011(0.0014) |Steps 434(429.82) | Grad Norm 7.5383(9.1885) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 10.1779(10.1287) | Bit/dim 0.8898(0.8872) | Xent 0.1557(0.1556) | Xent Color 0.0131(0.0125) | Loss 2.5889(2.7096) | Error 0.0500(0.0491) | Error Color 0.0000(0.0013) |Steps 422(429.09) | Grad Norm 16.9565(8.9703) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 10.4754(10.1356) | Bit/dim 0.8740(0.8878) | Xent 0.1381(0.1523) | Xent Color 0.0094(0.0124) | Loss 2.4986(2.6692) | Error 0.0444(0.0481) | Error Color 0.0011(0.0013) |Steps 434(430.86) | Grad Norm 7.3491(9.6072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 64.3473, Epoch Time 756.2056(717.9007), Bit/dim 0.8786(best: 0.8881), Xent 0.0794, Xent Color 0.0020. Loss 0.8990, Error 0.0270(best: 0.0248), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 10.1068(10.1014) | Bit/dim 0.8873(0.8873) | Xent 0.1674(0.1560) | Xent Color 0.0118(0.0125) | Loss 2.6216(3.1976) | Error 0.0533(0.0493) | Error Color 0.0033(0.0014) |Steps 440(431.62) | Grad Norm 4.3987(8.9350) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 9.6732(10.0833) | Bit/dim 0.8700(0.8854) | Xent 0.0967(0.1539) | Xent Color 0.0122(0.0122) | Loss 2.5264(3.0286) | Error 0.0367(0.0484) | Error Color 0.0033(0.0015) |Steps 434(432.23) | Grad Norm 7.8875(8.7840) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 10.3221(10.1108) | Bit/dim 0.8995(0.8871) | Xent 0.1615(0.1564) | Xent Color 0.0105(0.0128) | Loss 2.5500(2.9095) | Error 0.0500(0.0487) | Error Color 0.0022(0.0017) |Steps 398(429.86) | Grad Norm 13.1904(10.4820) | Total Time 0.00(0.00)\n",
      "Iter 3860 | Time 10.1624(10.1049) | Bit/dim 0.8716(0.8856) | Xent 0.1579(0.1561) | Xent Color 0.0107(0.0125) | Loss 2.5365(2.8133) | Error 0.0500(0.0483) | Error Color 0.0011(0.0017) |Steps 434(429.37) | Grad Norm 2.8013(9.9880) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 10.0009(10.1100) | Bit/dim 0.8938(0.8850) | Xent 0.1363(0.1521) | Xent Color 0.0176(0.0122) | Loss 2.5628(2.7413) | Error 0.0511(0.0477) | Error Color 0.0033(0.0016) |Steps 434(430.14) | Grad Norm 20.0425(10.7054) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 9.9578(10.1317) | Bit/dim 0.8809(0.8831) | Xent 0.1205(0.1546) | Xent Color 0.0085(0.0121) | Loss 2.5405(2.6913) | Error 0.0367(0.0485) | Error Color 0.0011(0.0017) |Steps 434(431.60) | Grad Norm 11.3217(11.0035) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 10.0091(10.1244) | Bit/dim 0.8828(0.8820) | Xent 0.1292(0.1517) | Xent Color 0.0107(0.0119) | Loss 2.4911(2.6480) | Error 0.0456(0.0478) | Error Color 0.0022(0.0015) |Steps 428(430.96) | Grad Norm 11.1067(11.1287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 65.7631, Epoch Time 757.0924(719.0764), Bit/dim 0.8751(best: 0.8786), Xent 0.0757, Xent Color 0.0021. Loss 0.8946, Error 0.0241(best: 0.0248), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 9.9007(10.1183) | Bit/dim 0.8644(0.8794) | Xent 0.1582(0.1505) | Xent Color 0.0118(0.0116) | Loss 2.5141(3.0988) | Error 0.0411(0.0470) | Error Color 0.0000(0.0015) |Steps 434(431.89) | Grad Norm 12.6627(10.9178) | Total Time 0.00(0.00)\n",
      "Iter 3910 | Time 9.9118(10.1160) | Bit/dim 0.8885(0.8798) | Xent 0.1410(0.1506) | Xent Color 0.0162(0.0123) | Loss 2.5534(2.9543) | Error 0.0578(0.0476) | Error Color 0.0022(0.0017) |Steps 422(431.78) | Grad Norm 16.5277(12.3042) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 9.7013(10.1024) | Bit/dim 0.8729(0.8788) | Xent 0.1560(0.1498) | Xent Color 0.0102(0.0119) | Loss 2.4827(2.8408) | Error 0.0389(0.0467) | Error Color 0.0000(0.0015) |Steps 440(431.52) | Grad Norm 3.2415(11.8338) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 9.6712(10.1015) | Bit/dim 0.8779(0.8763) | Xent 0.1266(0.1502) | Xent Color 0.0081(0.0118) | Loss 2.5190(2.7601) | Error 0.0356(0.0467) | Error Color 0.0000(0.0015) |Steps 410(429.32) | Grad Norm 14.9385(11.8316) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 10.1142(10.1218) | Bit/dim 0.8649(0.8743) | Xent 0.1602(0.1515) | Xent Color 0.0091(0.0114) | Loss 2.5035(2.6951) | Error 0.0522(0.0476) | Error Color 0.0011(0.0014) |Steps 422(429.72) | Grad Norm 6.6957(11.1489) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 10.1243(10.1117) | Bit/dim 0.8861(0.8723) | Xent 0.1459(0.1491) | Xent Color 0.0149(0.0113) | Loss 2.5027(2.6476) | Error 0.0444(0.0468) | Error Color 0.0011(0.0014) |Steps 428(430.45) | Grad Norm 19.5478(11.0036) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 10.4754(10.1382) | Bit/dim 0.8564(0.8715) | Xent 0.1054(0.1467) | Xent Color 0.0088(0.0117) | Loss 2.5421(2.6172) | Error 0.0344(0.0464) | Error Color 0.0011(0.0016) |Steps 458(432.43) | Grad Norm 3.3017(11.3766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 64.7554, Epoch Time 754.3332(720.1342), Bit/dim 0.8709(best: 0.8751), Xent 0.0695, Xent Color 0.0038. Loss 0.8893, Error 0.0217(best: 0.0241), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 10.2473(10.1074) | Bit/dim 0.8700(0.8705) | Xent 0.1388(0.1447) | Xent Color 0.0107(0.0114) | Loss 2.5274(2.9938) | Error 0.0389(0.0453) | Error Color 0.0011(0.0015) |Steps 434(431.37) | Grad Norm 14.9214(11.7501) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 10.3189(10.0947) | Bit/dim 0.8529(0.8679) | Xent 0.1164(0.1433) | Xent Color 0.0076(0.0108) | Loss 2.4491(2.8613) | Error 0.0389(0.0450) | Error Color 0.0000(0.0014) |Steps 428(428.61) | Grad Norm 12.2095(11.0523) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 9.8536(10.0927) | Bit/dim 0.8804(0.8680) | Xent 0.1308(0.1431) | Xent Color 0.0111(0.0106) | Loss 2.5197(2.7689) | Error 0.0422(0.0453) | Error Color 0.0011(0.0013) |Steps 434(428.76) | Grad Norm 16.9809(11.9251) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 10.0643(10.1433) | Bit/dim 0.8618(0.8658) | Xent 0.1327(0.1439) | Xent Color 0.0068(0.0103) | Loss 2.4850(2.6998) | Error 0.0433(0.0463) | Error Color 0.0000(0.0012) |Steps 446(432.56) | Grad Norm 9.0243(11.2007) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 10.4196(10.1700) | Bit/dim 0.8598(0.8634) | Xent 0.1406(0.1430) | Xent Color 0.0112(0.0099) | Loss 2.4788(2.6444) | Error 0.0456(0.0465) | Error Color 0.0022(0.0011) |Steps 434(431.88) | Grad Norm 9.8158(10.8157) | Total Time 0.00(0.00)\n",
      "Iter 4020 | Time 10.1827(10.1860) | Bit/dim 0.8903(0.8665) | Xent 0.1158(0.1433) | Xent Color 0.0095(0.0102) | Loss 2.5718(2.6150) | Error 0.0400(0.0459) | Error Color 0.0011(0.0012) |Steps 422(431.32) | Grad Norm 18.2276(13.0684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 65.2842, Epoch Time 758.1288(721.2740), Bit/dim 0.8560(best: 0.8709), Xent 0.0721, Xent Color 0.0019. Loss 0.8745, Error 0.0239(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 10.6197(10.2063) | Bit/dim 0.8553(0.8650) | Xent 0.1574(0.1413) | Xent Color 0.0051(0.0102) | Loss 2.4927(3.0754) | Error 0.0511(0.0450) | Error Color 0.0000(0.0014) |Steps 422(431.57) | Grad Norm 9.5375(12.3045) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 10.3596(10.2161) | Bit/dim 0.8650(0.8641) | Xent 0.1327(0.1402) | Xent Color 0.0091(0.0098) | Loss 2.5249(2.9237) | Error 0.0411(0.0448) | Error Color 0.0000(0.0012) |Steps 434(431.29) | Grad Norm 15.6289(12.1296) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 10.2368(10.2071) | Bit/dim 0.8724(0.8635) | Xent 0.1396(0.1417) | Xent Color 0.0082(0.0095) | Loss 2.5542(2.8157) | Error 0.0433(0.0451) | Error Color 0.0011(0.0011) |Steps 428(431.36) | Grad Norm 10.7973(11.6973) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 10.2362(10.1954) | Bit/dim 0.8817(0.8624) | Xent 0.1509(0.1420) | Xent Color 0.0123(0.0099) | Loss 2.5122(2.7306) | Error 0.0467(0.0452) | Error Color 0.0011(0.0013) |Steps 410(432.23) | Grad Norm 29.4331(13.0905) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 10.2106(10.2012) | Bit/dim 0.8691(0.8627) | Xent 0.1252(0.1420) | Xent Color 0.0062(0.0099) | Loss 2.5315(2.6667) | Error 0.0333(0.0446) | Error Color 0.0000(0.0013) |Steps 428(430.53) | Grad Norm 8.5581(13.0552) | Total Time 0.00(0.00)\n",
      "Iter 4080 | Time 10.2115(10.2260) | Bit/dim 0.8555(0.8587) | Xent 0.1187(0.1387) | Xent Color 0.0076(0.0093) | Loss 2.4738(2.6178) | Error 0.0422(0.0439) | Error Color 0.0011(0.0012) |Steps 446(432.78) | Grad Norm 9.1297(11.8367) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 9.9668(10.2447) | Bit/dim 0.8498(0.8586) | Xent 0.1606(0.1398) | Xent Color 0.0057(0.0093) | Loss 2.4856(2.5848) | Error 0.0478(0.0441) | Error Color 0.0000(0.0010) |Steps 440(434.26) | Grad Norm 8.1423(12.9967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 64.3182, Epoch Time 762.2329(722.5028), Bit/dim 0.8571(best: 0.8560), Xent 0.0673, Xent Color 0.0019. Loss 0.8744, Error 0.0222(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 10.4775(10.2375) | Bit/dim 0.8392(0.8598) | Xent 0.1559(0.1388) | Xent Color 0.0082(0.0091) | Loss 2.5305(3.0058) | Error 0.0511(0.0434) | Error Color 0.0011(0.0010) |Steps 446(432.56) | Grad Norm 5.7125(13.2167) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 10.1250(10.2507) | Bit/dim 0.8416(0.8570) | Xent 0.0936(0.1376) | Xent Color 0.0055(0.0095) | Loss 2.4438(2.8679) | Error 0.0300(0.0429) | Error Color 0.0000(0.0010) |Steps 428(433.33) | Grad Norm 5.1952(12.7163) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 9.5612(10.2229) | Bit/dim 0.8473(0.8572) | Xent 0.1318(0.1384) | Xent Color 0.0090(0.0095) | Loss 2.4313(2.7662) | Error 0.0400(0.0431) | Error Color 0.0022(0.0011) |Steps 428(432.88) | Grad Norm 11.9614(13.3291) | Total Time 0.00(0.00)\n",
      "Iter 4130 | Time 10.0492(10.2175) | Bit/dim 0.8361(0.8560) | Xent 0.1275(0.1331) | Xent Color 0.0085(0.0094) | Loss 2.4396(2.6886) | Error 0.0333(0.0421) | Error Color 0.0011(0.0012) |Steps 428(432.48) | Grad Norm 5.0235(13.2221) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 10.4262(10.1855) | Bit/dim 0.8523(0.8594) | Xent 0.1313(0.1354) | Xent Color 0.0082(0.0099) | Loss 2.5350(2.6458) | Error 0.0456(0.0426) | Error Color 0.0011(0.0014) |Steps 452(433.70) | Grad Norm 8.1578(14.3037) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 10.3138(10.1959) | Bit/dim 0.8572(0.8579) | Xent 0.1252(0.1352) | Xent Color 0.0048(0.0101) | Loss 2.4900(2.6012) | Error 0.0389(0.0426) | Error Color 0.0000(0.0015) |Steps 440(434.86) | Grad Norm 8.7037(13.6451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 64.8337, Epoch Time 760.1109(723.6310), Bit/dim 0.8434(best: 0.8560), Xent 0.0702, Xent Color 0.0013. Loss 0.8612, Error 0.0241(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 9.9939(10.1876) | Bit/dim 0.8636(0.8553) | Xent 0.1437(0.1361) | Xent Color 0.0086(0.0099) | Loss 2.4459(3.0963) | Error 0.0456(0.0431) | Error Color 0.0011(0.0014) |Steps 416(432.34) | Grad Norm 16.6717(13.3423) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 11.0327(10.2039) | Bit/dim 0.8410(0.8524) | Xent 0.1334(0.1360) | Xent Color 0.0052(0.0092) | Loss 2.5044(2.9345) | Error 0.0400(0.0432) | Error Color 0.0000(0.0010) |Steps 458(433.71) | Grad Norm 7.6339(12.3203) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 10.0931(10.2289) | Bit/dim 0.8467(0.8479) | Xent 0.1301(0.1373) | Xent Color 0.0085(0.0086) | Loss 2.5028(2.8120) | Error 0.0411(0.0436) | Error Color 0.0000(0.0010) |Steps 428(435.45) | Grad Norm 16.8122(11.8070) | Total Time 0.00(0.00)\n",
      "Iter 4190 | Time 10.1885(10.2362) | Bit/dim 0.8365(0.8475) | Xent 0.1396(0.1339) | Xent Color 0.0065(0.0084) | Loss 2.4296(2.7195) | Error 0.0500(0.0429) | Error Color 0.0000(0.0010) |Steps 452(436.09) | Grad Norm 10.4690(11.9647) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 10.5829(10.2452) | Bit/dim 0.8305(0.8450) | Xent 0.1277(0.1335) | Xent Color 0.0076(0.0081) | Loss 2.5097(2.6458) | Error 0.0378(0.0429) | Error Color 0.0011(0.0010) |Steps 458(436.38) | Grad Norm 12.2756(12.2733) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 10.3828(10.2457) | Bit/dim 0.8388(0.8428) | Xent 0.1599(0.1340) | Xent Color 0.0060(0.0076) | Loss 2.4623(2.5966) | Error 0.0489(0.0428) | Error Color 0.0000(0.0009) |Steps 416(434.26) | Grad Norm 11.3572(11.6413) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 10.5627(10.2611) | Bit/dim 0.8362(0.8428) | Xent 0.0969(0.1326) | Xent Color 0.0092(0.0078) | Loss 2.4576(2.5596) | Error 0.0322(0.0420) | Error Color 0.0011(0.0009) |Steps 440(434.13) | Grad Norm 16.4539(13.0752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 65.8470, Epoch Time 766.2065(724.9083), Bit/dim 0.8439(best: 0.8434), Xent 0.0684, Xent Color 0.0011. Loss 0.8613, Error 0.0233(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4230 | Time 10.2944(10.2585) | Bit/dim 0.8364(0.8423) | Xent 0.1376(0.1301) | Xent Color 0.0061(0.0076) | Loss 2.5133(3.0173) | Error 0.0444(0.0412) | Error Color 0.0000(0.0007) |Steps 434(433.94) | Grad Norm 11.0689(13.3744) | Total Time 0.00(0.00)\n",
      "Iter 4240 | Time 10.1372(10.2684) | Bit/dim 0.8421(0.8406) | Xent 0.0890(0.1306) | Xent Color 0.0062(0.0073) | Loss 2.4246(2.8741) | Error 0.0300(0.0416) | Error Color 0.0000(0.0006) |Steps 440(434.50) | Grad Norm 8.7654(12.5613) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 10.2300(10.2513) | Bit/dim 0.8428(0.8387) | Xent 0.1328(0.1304) | Xent Color 0.0206(0.0077) | Loss 2.4881(2.7631) | Error 0.0500(0.0415) | Error Color 0.0056(0.0007) |Steps 434(434.82) | Grad Norm 26.5835(12.5130) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 10.4611(10.2405) | Bit/dim 0.8916(0.8468) | Xent 0.1065(0.1294) | Xent Color 0.0120(0.0116) | Loss 2.5411(2.6924) | Error 0.0378(0.0415) | Error Color 0.0022(0.0021) |Steps 428(433.27) | Grad Norm 25.0703(16.1064) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 10.7650(10.2764) | Bit/dim 0.8678(0.8508) | Xent 0.1378(0.1297) | Xent Color 0.0072(0.0110) | Loss 2.4670(2.6414) | Error 0.0411(0.0414) | Error Color 0.0011(0.0021) |Steps 416(431.84) | Grad Norm 20.3139(16.4586) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 10.2778(10.2078) | Bit/dim 0.8701(0.8528) | Xent 0.1424(0.1296) | Xent Color 0.0179(0.0108) | Loss 2.5055(2.5952) | Error 0.0467(0.0417) | Error Color 0.0011(0.0019) |Steps 428(430.41) | Grad Norm 21.7875(16.8409) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 9.7636(10.2082) | Bit/dim 0.8299(0.8508) | Xent 0.1348(0.1303) | Xent Color 0.0064(0.0102) | Loss 2.4021(2.5602) | Error 0.0467(0.0419) | Error Color 0.0011(0.0017) |Steps 428(432.64) | Grad Norm 9.6999(16.1341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 64.2538, Epoch Time 760.9506(725.9895), Bit/dim 0.8461(best: 0.8434), Xent 0.0734, Xent Color 0.0018. Loss 0.8649, Error 0.0234(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4300 | Time 9.8820(10.1964) | Bit/dim 0.8447(0.8471) | Xent 0.1506(0.1296) | Xent Color 0.0048(0.0093) | Loss 2.4348(2.9383) | Error 0.0456(0.0414) | Error Color 0.0000(0.0014) |Steps 446(434.39) | Grad Norm 11.4205(14.9391) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 10.4899(10.1842) | Bit/dim 0.8351(0.8415) | Xent 0.1078(0.1306) | Xent Color 0.0079(0.0088) | Loss 2.4456(2.8033) | Error 0.0378(0.0412) | Error Color 0.0022(0.0013) |Steps 446(433.58) | Grad Norm 11.7051(13.6587) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 10.6066(10.2307) | Bit/dim 0.8322(0.8369) | Xent 0.1048(0.1287) | Xent Color 0.0053(0.0082) | Loss 2.4027(2.6998) | Error 0.0400(0.0399) | Error Color 0.0000(0.0011) |Steps 440(434.68) | Grad Norm 9.6132(12.4900) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 10.3061(10.2876) | Bit/dim 0.8373(0.8334) | Xent 0.1150(0.1261) | Xent Color 0.0048(0.0078) | Loss 2.4832(2.6321) | Error 0.0389(0.0394) | Error Color 0.0000(0.0010) |Steps 440(437.57) | Grad Norm 12.1229(12.2062) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 10.1291(10.2993) | Bit/dim 0.8212(0.8307) | Xent 0.1116(0.1252) | Xent Color 0.0049(0.0072) | Loss 2.4124(2.5753) | Error 0.0322(0.0392) | Error Color 0.0000(0.0008) |Steps 434(436.64) | Grad Norm 8.6024(11.7145) | Total Time 0.00(0.00)\n",
      "Iter 4350 | Time 10.4298(10.2920) | Bit/dim 0.8055(0.8272) | Xent 0.0674(0.1233) | Xent Color 0.0054(0.0068) | Loss 2.3692(2.5340) | Error 0.0267(0.0393) | Error Color 0.0000(0.0007) |Steps 404(436.36) | Grad Norm 8.4594(11.0346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 65.8598, Epoch Time 767.2075(727.2261), Bit/dim 0.8273(best: 0.8434), Xent 0.0626, Xent Color 0.0009. Loss 0.8432, Error 0.0215(best: 0.0217), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4360 | Time 10.2064(10.2941) | Bit/dim 0.8597(0.8273) | Xent 0.1037(0.1233) | Xent Color 0.0049(0.0068) | Loss 2.4678(3.0058) | Error 0.0344(0.0390) | Error Color 0.0000(0.0007) |Steps 434(437.53) | Grad Norm 23.3008(12.4953) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 10.1916(10.2841) | Bit/dim 0.8433(0.8311) | Xent 0.1123(0.1256) | Xent Color 0.0067(0.0069) | Loss 2.4221(2.8635) | Error 0.0289(0.0394) | Error Color 0.0011(0.0007) |Steps 434(438.82) | Grad Norm 12.9348(13.6085) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 10.2562(10.2693) | Bit/dim 0.8205(0.8297) | Xent 0.1221(0.1249) | Xent Color 0.0108(0.0087) | Loss 2.4411(2.7499) | Error 0.0333(0.0387) | Error Color 0.0022(0.0012) |Steps 422(437.50) | Grad Norm 9.8795(13.6436) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 10.1517(10.2746) | Bit/dim 0.8085(0.8275) | Xent 0.1135(0.1222) | Xent Color 0.0069(0.0078) | Loss 2.3610(2.6586) | Error 0.0367(0.0381) | Error Color 0.0011(0.0010) |Steps 428(436.92) | Grad Norm 6.7865(12.4943) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_beta.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/infocnf_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_beta_1_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10 --beta 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
