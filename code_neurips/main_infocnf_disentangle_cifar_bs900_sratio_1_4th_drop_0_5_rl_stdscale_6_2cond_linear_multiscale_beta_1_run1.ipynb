{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_multiscale_beta.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "# for disentanglement\n",
      "parser.add_argument('--beta', default=0.01, type=float, help='disentanglement weight')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    z_sup_class = [o[:,:int(np.prod(o.size()[1:])*0.5)] for o in z]\n",
      "    z_sup_class = torch.cat(z_sup_class,1)\n",
      "    \n",
      "    z_sup_color = [o[:,int(np.prod(o.size()[1:])*0.5):] for o in z]\n",
      "    z_sup_color = torch.cat(z_sup_color,1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup_class).view(-1,1)  # logp(z)_sup\n",
      "    beta_logpz_sup = logpz_sup * (1.0 - args.beta * torch.exp(logpz_sup) / torch.tensor(model.module.y_class).to(logpz_sup))\n",
      "    \n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z_sup_color).view(-1,1)  # logp(z)_color_sup\n",
      "    beta_logpz_color_sup = logpz_color_sup * (1.0 - args.beta * torch.exp(logpz_color_sup) / torch.tensor(model.module.y_color).to(logpz_color_sup))\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = beta_logpz_sup + beta_logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup_class = model.module.dropout(z_sup_class)\n",
      "        z_sup_color = model.module.dropout_color(z_sup_color)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup_class)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(z_sup_color)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio * 2.,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_color_sup = fixed_z_color_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_color_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_color_sup = start_color_sup + (2**(ns-2))*a_color_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_color_sup = end_color_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_color_sup = start_color_sup + a_color_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, beta=1.0, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_1_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.8151(21.4206) | Bit/dim 25.4505(27.3296) | Xent 2.2869(2.3009) | Xent Color 2.3007(2.3023) | Loss 48.6140(51.8002) | Error 0.8744(0.8882) | Error Color 0.9067(0.8898) |Steps 296(306.77) | Grad Norm 260.8726(274.9979) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.1320(17.6792) | Bit/dim 20.0909(26.0735) | Xent 2.2513(2.2928) | Xent Color 2.2917(2.3003) | Loss 39.2001(49.5899) | Error 0.8644(0.8869) | Error Color 0.8811(0.8910) |Steps 314(305.49) | Grad Norm 216.4388(264.8256) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.5342(14.9284) | Bit/dim 13.8656(23.5615) | Xent 2.1974(2.2735) | Xent Color 2.2745(2.2954) | Loss 27.6730(45.0992) | Error 0.5411(0.8307) | Error Color 0.7822(0.8687) |Steps 290(305.79) | Grad Norm 159.2156(243.6358) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.2260(13.0870) | Bit/dim 8.9832(20.2265) | Xent 2.1211(2.2412) | Xent Color 2.2517(2.2866) | Loss 18.9609(39.0983) | Error 0.3322(0.7163) | Error Color 0.7511(0.8413) |Steps 338(313.47) | Grad Norm 95.5078(211.8929) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 8.8781(11.9601) | Bit/dim 6.6884(16.8755) | Xent 2.0425(2.1962) | Xent Color 2.2351(2.2744) | Loss 14.8288(33.0830) | Error 0.3333(0.6080) | Error Color 0.7467(0.8130) |Steps 356(328.46) | Grad Norm 33.8635(171.6083) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.3445(11.2160) | Bit/dim 6.0965(14.1054) | Xent 1.9660(2.1429) | Xent Color 2.1978(2.2582) | Loss 13.8245(28.1135) | Error 0.3422(0.5346) | Error Color 0.7800(0.8032) |Steps 410(345.80) | Grad Norm 25.0496(132.5925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 62.5986, Epoch Time 637.1350(637.1350), Bit/dim 5.7409(best: inf), Xent 1.8894, Xent Color 2.1737. Loss 6.7567, Error 0.2505(best: inf), Error Color 0.7276(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.8499(10.6846) | Bit/dim 5.5568(11.9159) | Xent 1.8885(2.0823) | Xent Color 2.1676(2.2383) | Loss 12.6922(24.6278) | Error 0.3100(0.4738) | Error Color 0.6700(0.7873) |Steps 380(356.24) | Grad Norm 18.3093(103.8211) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.6379(10.2070) | Bit/dim 4.9704(10.1646) | Xent 1.8138(2.0166) | Xent Color 2.1314(2.2142) | Loss 11.5142(21.3260) | Error 0.3022(0.4245) | Error Color 0.6000(0.7364) |Steps 380(362.70) | Grad Norm 14.4527(80.3122) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.0999(9.8203) | Bit/dim 4.5811(8.7587) | Xent 1.7558(1.9542) | Xent Color 2.1022(2.1889) | Loss 10.9433(18.6716) | Error 0.2756(0.3892) | Error Color 0.6278(0.7121) |Steps 398(367.12) | Grad Norm 13.4104(63.1010) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.6472(9.5568) | Bit/dim 4.1664(7.6068) | Xent 1.7182(1.8965) | Xent Color 2.0705(2.1608) | Loss 9.9136(16.4922) | Error 0.2700(0.3599) | Error Color 0.5889(0.6806) |Steps 362(367.45) | Grad Norm 9.0484(49.1762) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 9.0130(9.4166) | Bit/dim 3.7622(6.6452) | Xent 1.7104(1.8495) | Xent Color 2.0338(2.1304) | Loss 9.2637(14.6942) | Error 0.2900(0.3392) | Error Color 0.5344(0.6460) |Steps 392(372.42) | Grad Norm 8.0414(38.5381) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 9.0236(9.2721) | Bit/dim 3.3653(5.8283) | Xent 1.7375(1.8180) | Xent Color 2.0034(2.1006) | Loss 8.6042(13.1577) | Error 0.3022(0.3279) | Error Color 0.5533(0.6173) |Steps 398(374.85) | Grad Norm 8.2500(30.6075) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.9757(9.1943) | Bit/dim 3.0526(5.1349) | Xent 1.7686(1.8017) | Xent Color 1.9625(2.0678) | Loss 7.8525(11.8683) | Error 0.3089(0.3252) | Error Color 0.5411(0.5919) |Steps 386(379.52) | Grad Norm 6.3192(24.4516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 58.0137, Epoch Time 670.1128(638.1243), Bit/dim 2.9671(best: 5.7409), Xent 1.7650, Xent Color 1.9373. Loss 3.8927, Error 0.2727(best: 0.2505), Error Color 0.4007(best: 0.7276)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.3730(9.2417) | Bit/dim 2.7925(4.5470) | Xent 1.8212(1.8013) | Xent Color 1.9172(2.0341) | Loss 7.5790(11.2171) | Error 0.3511(0.3309) | Error Color 0.4789(0.5642) |Steps 410(384.23) | Grad Norm 5.6835(19.5808) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.5421(9.2494) | Bit/dim 2.6174(4.0579) | Xent 1.8334(1.8090) | Xent Color 1.8795(1.9970) | Loss 7.2932(10.2055) | Error 0.3667(0.3395) | Error Color 0.4844(0.5414) |Steps 398(386.99) | Grad Norm 4.3351(15.6802) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.4057(9.2839) | Bit/dim 2.4789(3.6578) | Xent 1.8511(1.8204) | Xent Color 1.8173(1.9568) | Loss 7.0444(9.3977) | Error 0.3756(0.3494) | Error Color 0.4633(0.5223) |Steps 416(390.45) | Grad Norm 3.2785(12.5221) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.6793(9.3067) | Bit/dim 2.4090(3.3358) | Xent 1.8710(1.8333) | Xent Color 1.7650(1.9131) | Loss 6.8214(8.7426) | Error 0.4067(0.3620) | Error Color 0.4389(0.5023) |Steps 404(392.45) | Grad Norm 2.7681(10.0175) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.4291(9.3467) | Bit/dim 2.3311(3.0782) | Xent 1.8909(1.8464) | Xent Color 1.7012(1.8651) | Loss 6.7734(8.2204) | Error 0.4244(0.3752) | Error Color 0.3956(0.4827) |Steps 416(394.88) | Grad Norm 2.4631(8.0933) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.8489(9.4360) | Bit/dim 2.2726(2.8714) | Xent 1.8944(1.8565) | Xent Color 1.6574(1.8142) | Loss 6.5911(7.7906) | Error 0.4322(0.3885) | Error Color 0.4000(0.4623) |Steps 386(394.34) | Grad Norm 1.9855(6.5953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 59.2688, Epoch Time 712.3596(640.3514), Bit/dim 2.2406(best: 2.9671), Xent 1.8488, Xent Color 1.5590. Loss 3.0926, Error 0.3303(best: 0.2505), Error Color 0.3205(best: 0.4007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.9572(9.4939) | Bit/dim 2.2369(2.7060) | Xent 1.8804(1.8628) | Xent Color 1.5552(1.7596) | Loss 6.4892(7.9768) | Error 0.4267(0.3986) | Error Color 0.3600(0.4416) |Steps 404(396.44) | Grad Norm 2.9056(5.6167) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.2519(9.4535) | Bit/dim 2.1969(2.5768) | Xent 1.8212(1.8583) | Xent Color 1.4897(1.7008) | Loss 6.3092(7.5507) | Error 0.4022(0.4033) | Error Color 0.3600(0.4234) |Steps 416(396.48) | Grad Norm 1.7886(4.7523) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.0540(9.3729) | Bit/dim 2.1935(2.4774) | Xent 1.7891(1.8448) | Xent Color 1.4049(1.6344) | Loss 6.1355(7.2159) | Error 0.3756(0.4025) | Error Color 0.3744(0.4061) |Steps 404(396.70) | Grad Norm 1.9548(4.2178) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.1388(9.3371) | Bit/dim 2.1736(2.4017) | Xent 1.7253(1.8199) | Xent Color 1.3348(1.5587) | Loss 6.2113(6.9521) | Error 0.3633(0.3966) | Error Color 0.3722(0.3903) |Steps 404(395.92) | Grad Norm 15.3526(4.9756) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.4219(9.3077) | Bit/dim 2.1952(2.3445) | Xent 1.6687(1.7879) | Xent Color 1.2999(1.4933) | Loss 6.2077(6.7452) | Error 0.3622(0.3885) | Error Color 0.3678(0.3875) |Steps 374(396.31) | Grad Norm 38.6800(10.2814) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.2687(9.3080) | Bit/dim 2.1615(2.2988) | Xent 1.6233(1.7444) | Xent Color 1.1964(1.4226) | Loss 6.0418(6.5671) | Error 0.3689(0.3747) | Error Color 0.3122(0.3710) |Steps 416(398.85) | Grad Norm 17.9602(13.0585) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.5137(9.2697) | Bit/dim 2.1560(2.2610) | Xent 1.5278(1.6948) | Xent Color 1.1005(1.3497) | Loss 5.8921(6.4083) | Error 0.3200(0.3614) | Error Color 0.2467(0.3444) |Steps 398(398.36) | Grad Norm 5.7414(13.4258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 59.7222, Epoch Time 693.5453(641.9472), Bit/dim 2.2640(best: 2.2406), Xent 1.4234, Xent Color 1.9068. Loss 3.0966, Error 0.2459(best: 0.2505), Error Color 0.7541(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 8.9681(9.2149) | Bit/dim 2.1901(2.2433) | Xent 1.4021(1.6347) | Xent Color 1.1084(1.3395) | Loss 5.8960(6.7836) | Error 0.2878(0.3472) | Error Color 0.3022(0.3785) |Steps 404(398.30) | Grad Norm 28.9628(27.0119) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.2240(9.2072) | Bit/dim 2.1653(2.2253) | Xent 1.3247(1.5696) | Xent Color 1.0665(1.2791) | Loss 5.9299(6.5562) | Error 0.2567(0.3333) | Error Color 0.2189(0.3611) |Steps 410(398.44) | Grad Norm 21.5742(28.0246) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.0989(9.1688) | Bit/dim 2.1553(2.2092) | Xent 1.2924(1.5015) | Xent Color 0.9835(1.2109) | Loss 5.7693(6.3546) | Error 0.2689(0.3192) | Error Color 0.1867(0.3252) |Steps 410(397.97) | Grad Norm 12.1591(25.2716) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.0635(9.1256) | Bit/dim 2.1806(2.1968) | Xent 1.1452(1.4301) | Xent Color 0.8935(1.1420) | Loss 5.7083(6.1841) | Error 0.2422(0.3059) | Error Color 0.1567(0.2911) |Steps 386(397.11) | Grad Norm 6.1648(21.4239) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.0371(9.1451) | Bit/dim 2.1731(2.1918) | Xent 1.1081(1.3515) | Xent Color 0.8772(1.0706) | Loss 5.6488(6.0563) | Error 0.2400(0.2904) | Error Color 0.2133(0.2650) |Steps 410(398.31) | Grad Norm 17.7399(18.8660) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.1986(9.1290) | Bit/dim 2.3274(2.2100) | Xent 1.0467(1.2709) | Xent Color 3.1437(1.2841) | Loss 6.9288(6.1102) | Error 0.2411(0.2778) | Error Color 0.8378(0.3300) |Steps 398(398.96) | Grad Norm 209.3606(46.2708) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.0218(9.1173) | Bit/dim 2.2236(2.2168) | Xent 1.0159(1.2023) | Xent Color 1.2676(1.3237) | Loss 5.7770(6.0695) | Error 0.2344(0.2676) | Error Color 0.5133(0.3942) |Steps 398(395.71) | Grad Norm 38.9509(50.9431) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 57.3310, Epoch Time 680.5427(643.1050), Bit/dim 2.1886(best: 2.2406), Xent 0.9307, Xent Color 1.1491. Loss 2.7086, Error 0.1824(best: 0.2459), Error Color 0.4119(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.2606(9.1307) | Bit/dim 2.1438(2.2062) | Xent 1.0440(1.1529) | Xent Color 1.1498(1.2807) | Loss 5.7877(6.4007) | Error 0.2456(0.2614) | Error Color 0.3800(0.3941) |Steps 392(393.44) | Grad Norm 25.7287(45.0053) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 8.9855(9.1135) | Bit/dim 2.1350(2.1888) | Xent 0.9888(1.1156) | Xent Color 1.0611(1.2253) | Loss 5.6664(6.2051) | Error 0.2256(0.2572) | Error Color 0.2756(0.3695) |Steps 398(391.53) | Grad Norm 8.2172(36.7923) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 8.8080(9.1167) | Bit/dim 2.1216(2.1748) | Xent 0.8976(1.0688) | Xent Color 0.9569(1.1678) | Loss 5.4105(6.0418) | Error 0.2289(0.2515) | Error Color 0.2244(0.3365) |Steps 386(392.78) | Grad Norm 4.9745(29.4557) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.2613(9.1096) | Bit/dim 2.1365(2.1635) | Xent 0.8298(1.0121) | Xent Color 0.9073(1.1087) | Loss 5.5092(5.9049) | Error 0.2333(0.2427) | Error Color 0.1944(0.3033) |Steps 386(393.81) | Grad Norm 1.7263(23.0325) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 8.7992(9.0644) | Bit/dim 2.1295(2.1569) | Xent 0.8038(0.9565) | Xent Color 0.8856(1.0515) | Loss 5.3300(5.7866) | Error 0.2100(0.2353) | Error Color 0.1733(0.2737) |Steps 380(391.73) | Grad Norm 5.2057(18.3267) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.1283(9.0854) | Bit/dim 2.1334(2.1502) | Xent 0.8238(0.9154) | Xent Color 0.8421(0.9966) | Loss 5.4352(5.6947) | Error 0.2422(0.2293) | Error Color 0.1933(0.2487) |Steps 410(392.87) | Grad Norm 6.8219(15.3215) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 59.9996, Epoch Time 683.5195(644.3175), Bit/dim 2.1333(best: 2.1886), Xent 0.6572, Xent Color 0.7155. Loss 2.4764, Error 0.1495(best: 0.1824), Error Color 0.0692(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.3357(9.1186) | Bit/dim 2.1464(2.1434) | Xent 0.7497(0.8772) | Xent Color 0.7652(0.9413) | Loss 5.4271(6.1077) | Error 0.1978(0.2233) | Error Color 0.1589(0.2264) |Steps 398(393.27) | Grad Norm 6.0807(13.0831) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 8.9910(9.1319) | Bit/dim 2.1304(2.1368) | Xent 0.7190(0.8427) | Xent Color 0.7323(0.8896) | Loss 5.4027(5.9146) | Error 0.2067(0.2187) | Error Color 0.1400(0.2084) |Steps 386(393.02) | Grad Norm 5.4375(12.9595) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.0915(9.1523) | Bit/dim 2.1512(2.1328) | Xent 0.6750(0.8058) | Xent Color 0.9177(0.8485) | Loss 5.3855(5.7652) | Error 0.2000(0.2121) | Error Color 0.3800(0.2007) |Steps 398(395.34) | Grad Norm 86.1296(15.7972) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.3665(9.1682) | Bit/dim 2.1090(2.1354) | Xent 0.6952(0.7764) | Xent Color 0.7431(0.9782) | Loss 5.4134(5.7377) | Error 0.1944(0.2064) | Error Color 0.2111(0.2741) |Steps 410(396.42) | Grad Norm 13.6452(37.9270) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.3591(9.1450) | Bit/dim 2.1040(2.1281) | Xent 0.7369(0.7701) | Xent Color 0.8165(0.9383) | Loss 5.3262(5.6438) | Error 0.2189(0.2072) | Error Color 0.2689(0.2760) |Steps 410(396.55) | Grad Norm 33.1357(36.1539) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.2794(9.1810) | Bit/dim 2.1215(2.1189) | Xent 0.7086(0.7591) | Xent Color 0.6750(0.8803) | Loss 5.3000(5.5600) | Error 0.2089(0.2072) | Error Color 0.1344(0.2475) |Steps 398(397.71) | Grad Norm 12.8076(30.9101) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.0128(9.1652) | Bit/dim 2.0901(2.1123) | Xent 0.7121(0.7367) | Xent Color 0.5999(0.8167) | Loss 5.2660(5.4798) | Error 0.2044(0.2040) | Error Color 0.1000(0.2160) |Steps 398(398.05) | Grad Norm 7.0639(25.6944) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 58.0474, Epoch Time 687.6681(645.6180), Bit/dim 2.1010(best: 2.1333), Xent 0.5433, Xent Color 0.5530. Loss 2.3751, Error 0.1351(best: 0.1495), Error Color 0.0612(best: 0.0692)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.2377(9.2001) | Bit/dim 2.0750(2.1058) | Xent 0.6191(0.7128) | Xent Color 0.5814(0.7583) | Loss 5.2105(5.8630) | Error 0.1800(0.1997) | Error Color 0.1122(0.1897) |Steps 398(399.28) | Grad Norm 6.2362(21.1751) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.1859(9.1613) | Bit/dim 2.0690(2.1001) | Xent 0.6625(0.6956) | Xent Color 0.5186(0.7039) | Loss 5.1520(5.6752) | Error 0.1967(0.1975) | Error Color 0.0911(0.1675) |Steps 386(398.39) | Grad Norm 6.3815(17.4299) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.2932(9.1864) | Bit/dim 2.0772(2.0951) | Xent 0.6306(0.6822) | Xent Color 0.5236(0.6567) | Loss 5.0133(5.5433) | Error 0.1767(0.1954) | Error Color 0.1011(0.1494) |Steps 398(400.10) | Grad Norm 14.8407(15.7174) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.6928(9.2302) | Bit/dim 2.0512(2.0869) | Xent 0.6305(0.6684) | Xent Color 0.4789(0.6107) | Loss 5.0688(5.4309) | Error 0.1933(0.1929) | Error Color 0.0978(0.1330) |Steps 416(400.73) | Grad Norm 15.7424(13.6406) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.1489(9.2513) | Bit/dim 2.0778(2.0819) | Xent 0.5832(0.6535) | Xent Color 0.7011(0.5837) | Loss 5.2239(5.3438) | Error 0.1678(0.1887) | Error Color 0.3200(0.1344) |Steps 404(402.64) | Grad Norm 75.4523(18.2195) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.0689(9.2267) | Bit/dim 2.0655(2.0853) | Xent 0.6436(0.6467) | Xent Color 0.6053(0.7073) | Loss 5.0146(5.3585) | Error 0.1878(0.1875) | Error Color 0.1900(0.2060) |Steps 362(400.16) | Grad Norm 16.8427(32.3390) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 57.9825, Epoch Time 689.2727(646.9276), Bit/dim 2.0453(best: 2.1010), Xent 0.5296, Xent Color 0.4889. Loss 2.2999, Error 0.1341(best: 0.1351), Error Color 0.0784(best: 0.0612)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.3927(9.2082) | Bit/dim 2.0226(2.0745) | Xent 0.6283(0.6576) | Xent Color 0.6354(0.6866) | Loss 4.9281(5.8226) | Error 0.1878(0.1912) | Error Color 0.2100(0.2037) |Steps 380(398.02) | Grad Norm 18.8471(28.7247) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.0527(9.1954) | Bit/dim 2.0236(2.0663) | Xent 0.5726(0.6479) | Xent Color 0.5232(0.6483) | Loss 5.0555(5.6209) | Error 0.1722(0.1904) | Error Color 0.1433(0.1845) |Steps 410(397.46) | Grad Norm 8.7398(23.9799) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.6964(9.2230) | Bit/dim 2.0330(2.0578) | Xent 0.6284(0.6341) | Xent Color 0.4667(0.6035) | Loss 5.0209(5.4591) | Error 0.1733(0.1862) | Error Color 0.1000(0.1627) |Steps 416(398.70) | Grad Norm 6.6384(20.0031) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.2653(9.2700) | Bit/dim 2.0313(2.0514) | Xent 0.5728(0.6210) | Xent Color 0.3962(0.5571) | Loss 4.9899(5.3430) | Error 0.1756(0.1849) | Error Color 0.0822(0.1430) |Steps 398(400.49) | Grad Norm 2.1899(17.5098) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.4538(9.2991) | Bit/dim 2.0067(2.0436) | Xent 0.6006(0.6159) | Xent Color 0.3815(0.5145) | Loss 5.0301(5.2468) | Error 0.1733(0.1844) | Error Color 0.0611(0.1266) |Steps 410(402.64) | Grad Norm 9.3763(17.4261) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.1535(9.3259) | Bit/dim 2.0872(2.0393) | Xent 0.5677(0.6094) | Xent Color 2.0110(0.5441) | Loss 5.7479(5.1998) | Error 0.1844(0.1845) | Error Color 0.6611(0.1429) |Steps 392(402.11) | Grad Norm 230.8710(28.4561) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.4104(9.3085) | Bit/dim 2.0369(2.0579) | Xent 0.7019(0.6168) | Xent Color 0.6686(0.7820) | Loss 5.1951(5.3089) | Error 0.2178(0.1875) | Error Color 0.2700(0.2176) |Steps 410(402.02) | Grad Norm 16.7864(42.4133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 57.8219, Epoch Time 695.9572(648.3985), Bit/dim 2.0441(best: 2.0453), Xent 0.5023, Xent Color 0.6218. Loss 2.3251, Error 0.1367(best: 0.1341), Error Color 0.1943(best: 0.0612)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 8.8402(9.2703) | Bit/dim 2.0322(2.0530) | Xent 0.6774(0.6273) | Xent Color 0.6699(0.7686) | Loss 5.1153(5.7462) | Error 0.2122(0.1914) | Error Color 0.1933(0.2269) |Steps 398(402.59) | Grad Norm 17.3055(36.6143) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.4417(9.2491) | Bit/dim 2.0226(2.0461) | Xent 0.5204(0.6168) | Xent Color 0.5090(0.7108) | Loss 4.9368(5.5571) | Error 0.1533(0.1882) | Error Color 0.1200(0.2043) |Steps 422(402.05) | Grad Norm 7.3250(29.6106) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.2711(9.2411) | Bit/dim 1.9878(2.0346) | Xent 0.5924(0.6077) | Xent Color 0.4334(0.6449) | Loss 4.9238(5.4012) | Error 0.1711(0.1863) | Error Color 0.1022(0.1798) |Steps 410(401.97) | Grad Norm 7.8519(23.6750) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.3105(9.2245) | Bit/dim 2.0175(2.0263) | Xent 0.5271(0.5978) | Xent Color 0.3671(0.5785) | Loss 4.9428(5.2706) | Error 0.1678(0.1841) | Error Color 0.0711(0.1532) |Steps 398(401.51) | Grad Norm 5.8302(18.9169) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.3506(9.2320) | Bit/dim 1.9827(2.0180) | Xent 0.5604(0.5908) | Xent Color 0.3091(0.5145) | Loss 4.8552(5.1632) | Error 0.1644(0.1816) | Error Color 0.0611(0.1303) |Steps 410(400.91) | Grad Norm 4.3236(15.1063) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.3290(9.2570) | Bit/dim 2.0106(2.0103) | Xent 0.5469(0.5802) | Xent Color 0.2815(0.4553) | Loss 4.8684(5.0805) | Error 0.1678(0.1787) | Error Color 0.0611(0.1102) |Steps 410(403.61) | Grad Norm 14.6968(12.5228) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.6771(9.2802) | Bit/dim 1.9768(2.0025) | Xent 0.5754(0.5700) | Xent Color 0.2453(0.4024) | Loss 4.8887(5.0131) | Error 0.1844(0.1758) | Error Color 0.0511(0.0941) |Steps 422(405.05) | Grad Norm 7.8732(11.5953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 59.8554, Epoch Time 693.2136(649.7430), Bit/dim 1.9859(best: 2.0441), Xent 0.4128, Xent Color 0.1797. Loss 2.1341, Error 0.1174(best: 0.1341), Error Color 0.0094(best: 0.0612)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.4632(9.3178) | Bit/dim 1.9759(1.9945) | Xent 0.5605(0.5680) | Xent Color 0.2530(0.3595) | Loss 4.7865(5.3664) | Error 0.1822(0.1761) | Error Color 0.0767(0.0828) |Steps 392(404.36) | Grad Norm 25.6750(11.6221) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.2741(9.3262) | Bit/dim 1.9819(1.9913) | Xent 0.4507(0.5574) | Xent Color 0.5278(0.4239) | Loss 4.8829(5.2623) | Error 0.1367(0.1718) | Error Color 0.2156(0.1263) |Steps 404(405.72) | Grad Norm 42.0375(25.5027) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.3664(9.3161) | Bit/dim 1.9847(1.9869) | Xent 0.5140(0.5567) | Xent Color 0.2740(0.4069) | Loss 4.8242(5.1544) | Error 0.1756(0.1715) | Error Color 0.0567(0.1208) |Steps 398(405.12) | Grad Norm 8.6340(23.5583) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.5304(9.3336) | Bit/dim 1.9804(1.9817) | Xent 0.5245(0.5501) | Xent Color 0.2280(0.3690) | Loss 4.8461(5.0508) | Error 0.1667(0.1707) | Error Color 0.0533(0.1053) |Steps 428(404.49) | Grad Norm 7.7487(19.9751) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.3056(9.3754) | Bit/dim 1.9500(1.9735) | Xent 0.5069(0.5437) | Xent Color 0.1932(0.3276) | Loss 4.6738(4.9705) | Error 0.1633(0.1704) | Error Color 0.0422(0.0891) |Steps 380(405.77) | Grad Norm 4.5343(16.9873) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.5228(9.3505) | Bit/dim 1.9584(1.9677) | Xent 0.5073(0.5426) | Xent Color 0.1810(0.2901) | Loss 4.6592(4.9019) | Error 0.1544(0.1697) | Error Color 0.0289(0.0746) |Steps 392(405.35) | Grad Norm 3.1872(13.9921) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 59.8365, Epoch Time 701.6663(651.3007), Bit/dim 1.9518(best: 1.9859), Xent 0.3904, Xent Color 0.1131. Loss 2.0776, Error 0.1114(best: 0.1174), Error Color 0.0074(best: 0.0094)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.5071(9.3638) | Bit/dim 1.9574(1.9600) | Xent 0.5304(0.5366) | Xent Color 0.1588(0.2569) | Loss 4.7292(5.3298) | Error 0.1544(0.1675) | Error Color 0.0311(0.0626) |Steps 404(404.77) | Grad Norm 10.8106(11.7175) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.5764(9.3662) | Bit/dim 1.9366(1.9536) | Xent 0.5073(0.5347) | Xent Color 0.1391(0.2282) | Loss 4.7142(5.1481) | Error 0.1567(0.1664) | Error Color 0.0256(0.0530) |Steps 422(402.91) | Grad Norm 9.4561(10.4612) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.0331(9.3194) | Bit/dim 1.9455(1.9475) | Xent 0.5282(0.5251) | Xent Color 0.1328(0.2058) | Loss 4.6523(5.0118) | Error 0.1600(0.1635) | Error Color 0.0222(0.0467) |Steps 404(402.90) | Grad Norm 9.2160(10.4898) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.5745(9.3252) | Bit/dim 1.9142(1.9408) | Xent 0.4945(0.5201) | Xent Color 0.1193(0.1851) | Loss 4.6458(4.9022) | Error 0.1556(0.1616) | Error Color 0.0200(0.0403) |Steps 416(401.07) | Grad Norm 1.9380(9.7835) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.3331(9.3484) | Bit/dim 1.9282(1.9347) | Xent 0.4604(0.5181) | Xent Color 0.1035(0.1669) | Loss 4.6563(4.8299) | Error 0.1467(0.1599) | Error Color 0.0189(0.0353) |Steps 398(401.87) | Grad Norm 11.4817(10.0348) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.7187(9.3681) | Bit/dim 3.5535(2.0402) | Xent 0.7677(0.5283) | Xent Color 10.3997(1.5388) | Loss 12.4760(5.6024) | Error 0.2211(0.1629) | Error Color 0.8056(0.1196) |Steps 434(402.64) | Grad Norm 193.0586(43.7251) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 10.2347(9.5212) | Bit/dim 2.4649(2.1898) | Xent 1.0106(0.6322) | Xent Color 1.5018(1.9016) | Loss 6.5988(6.0712) | Error 0.3389(0.1980) | Error Color 0.5400(0.2573) |Steps 464(411.94) | Grad Norm 24.7414(50.3558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 63.6454, Epoch Time 710.8698(653.0878), Bit/dim 2.3341(best: 1.9518), Xent 0.6679, Xent Color 1.0436. Loss 2.7620, Error 0.1892(best: 0.1114), Error Color 0.4091(best: 0.0074)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.7801(9.6467) | Bit/dim 2.2180(2.2116) | Xent 0.8456(0.7146) | Xent Color 1.1869(1.7408) | Loss 5.9615(6.6166) | Error 0.2689(0.2272) | Error Color 0.4856(0.3213) |Steps 422(419.87) | Grad Norm 14.2104(40.6155) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.5815(9.7038) | Bit/dim 2.0972(2.1920) | Xent 0.5984(0.7149) | Xent Color 0.9701(1.5543) | Loss 5.5963(6.3866) | Error 0.1856(0.2274) | Error Color 0.3678(0.3391) |Steps 428(422.60) | Grad Norm 6.5297(31.9369) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.4311(9.7082) | Bit/dim 2.0785(2.1583) | Xent 0.6121(0.7078) | Xent Color 0.8816(1.3846) | Loss 5.4551(6.1471) | Error 0.1933(0.2252) | Error Color 0.3400(0.3409) |Steps 404(422.93) | Grad Norm 3.2021(24.7089) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.5346(9.7518) | Bit/dim 2.0310(2.1252) | Xent 0.6106(0.6919) | Xent Color 0.6964(1.2260) | Loss 5.1996(5.9382) | Error 0.1978(0.2197) | Error Color 0.2278(0.3248) |Steps 416(423.71) | Grad Norm 1.9393(19.0673) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 10.0170(9.7396) | Bit/dim 2.0127(2.0969) | Xent 0.6026(0.6696) | Xent Color 0.6180(1.0787) | Loss 5.1843(5.7541) | Error 0.1733(0.2118) | Error Color 0.2144(0.2991) |Steps 392(423.89) | Grad Norm 2.9522(14.7228) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.1943(9.6930) | Bit/dim 2.0105(2.0717) | Xent 0.5924(0.6478) | Xent Color 0.5331(0.9401) | Loss 5.0736(5.5834) | Error 0.1822(0.2045) | Error Color 0.1944(0.2690) |Steps 416(419.68) | Grad Norm 1.4456(11.3953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 65.1717, Epoch Time 732.7091(655.4764), Bit/dim 2.0064(best: 1.9518), Xent 0.4123, Xent Color 0.3317. Loss 2.1924, Error 0.1178(best: 0.1114), Error Color 0.0553(best: 0.0074)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.4374(9.6668) | Bit/dim 1.9879(2.0522) | Xent 0.5450(0.6309) | Xent Color 0.4180(0.8117) | Loss 4.9338(6.0487) | Error 0.1633(0.1993) | Error Color 0.1222(0.2353) |Steps 416(421.11) | Grad Norm 1.4366(8.8339) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.3888(9.6613) | Bit/dim 2.0198(2.0354) | Xent 0.5693(0.6150) | Xent Color 0.3455(0.6983) | Loss 4.9922(5.7670) | Error 0.1711(0.1929) | Error Color 0.1033(0.2024) |Steps 416(421.23) | Grad Norm 2.2545(7.0313) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.6066(9.6773) | Bit/dim 1.9781(2.0223) | Xent 0.5802(0.5996) | Xent Color 0.2802(0.5981) | Loss 4.8985(5.5562) | Error 0.1944(0.1887) | Error Color 0.0711(0.1704) |Steps 398(420.63) | Grad Norm 2.8359(5.7068) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.3803(9.6897) | Bit/dim 1.9652(2.0119) | Xent 0.5600(0.5900) | Xent Color 0.2447(0.5109) | Loss 4.8849(5.3897) | Error 0.1656(0.1854) | Error Color 0.0511(0.1419) |Steps 416(420.22) | Grad Norm 1.2178(4.6398) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.5157(9.6564) | Bit/dim 1.9852(2.0023) | Xent 0.5148(0.5785) | Xent Color 0.2239(0.4374) | Loss 4.9364(5.2493) | Error 0.1678(0.1814) | Error Color 0.0567(0.1192) |Steps 410(420.59) | Grad Norm 2.0387(4.0603) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 8.9373(9.6180) | Bit/dim 1.9641(1.9924) | Xent 0.5538(0.5647) | Xent Color 0.1786(0.3760) | Loss 4.8085(5.1425) | Error 0.1733(0.1767) | Error Color 0.0367(0.1003) |Steps 416(419.28) | Grad Norm 3.3618(3.7947) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.6432(9.6505) | Bit/dim 1.9718(1.9830) | Xent 0.5397(0.5575) | Xent Color 0.1757(0.3243) | Loss 4.7779(5.0572) | Error 0.1622(0.1727) | Error Color 0.0356(0.0839) |Steps 434(418.85) | Grad Norm 8.3783(4.0247) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 63.2928, Epoch Time 723.8122(657.5265), Bit/dim 1.9583(best: 1.9518), Xent 0.3821, Xent Color 0.1052. Loss 2.0801, Error 0.1110(best: 0.1114), Error Color 0.0091(best: 0.0074)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.4899(9.6779) | Bit/dim 1.9488(1.9745) | Xent 0.4772(0.5496) | Xent Color 0.1450(0.2816) | Loss 4.7241(5.4824) | Error 0.1522(0.1719) | Error Color 0.0333(0.0709) |Steps 428(420.37) | Grad Norm 3.6386(4.3212) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.4285(9.6744) | Bit/dim 1.9342(1.9668) | Xent 0.4796(0.5356) | Xent Color 0.1300(0.2446) | Loss 4.6636(5.2859) | Error 0.1456(0.1666) | Error Color 0.0278(0.0600) |Steps 410(418.76) | Grad Norm 2.9540(4.4081) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.8998(9.6895) | Bit/dim 1.9223(1.9593) | Xent 0.5219(0.5355) | Xent Color 0.1189(0.2134) | Loss 4.6980(5.1411) | Error 0.1633(0.1663) | Error Color 0.0222(0.0505) |Steps 434(418.54) | Grad Norm 8.6387(4.4223) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.9906(9.7287) | Bit/dim 1.9499(1.9515) | Xent 0.6148(0.5291) | Xent Color 0.1260(0.1901) | Loss 4.7033(5.0182) | Error 0.2011(0.1648) | Error Color 0.0278(0.0443) |Steps 452(421.30) | Grad Norm 10.5223(5.1062) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.9223(9.7477) | Bit/dim 1.9381(1.9461) | Xent 0.5421(0.5289) | Xent Color 0.1093(0.1700) | Loss 4.7036(4.9309) | Error 0.1822(0.1648) | Error Color 0.0211(0.0387) |Steps 422(421.44) | Grad Norm 10.1344(5.8221) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 10.1970(9.8009) | Bit/dim 1.9353(1.9399) | Xent 0.4426(0.5234) | Xent Color 0.1041(0.1557) | Loss 4.7032(4.8703) | Error 0.1467(0.1630) | Error Color 0.0211(0.0358) |Steps 428(421.97) | Grad Norm 9.9375(7.1924) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.8859(9.8417) | Bit/dim 1.9118(1.9327) | Xent 0.4745(0.5226) | Xent Color 0.0984(0.1406) | Loss 4.6292(4.8117) | Error 0.1500(0.1633) | Error Color 0.0222(0.0318) |Steps 440(423.92) | Grad Norm 10.4816(7.1350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 63.6034, Epoch Time 735.5138(659.8661), Bit/dim 1.9215(best: 1.9518), Xent 0.3666, Xent Color 0.0521. Loss 2.0262, Error 0.1080(best: 0.1110), Error Color 0.0045(best: 0.0074)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 10.0561(9.8318) | Bit/dim 1.8975(1.9272) | Xent 0.4887(0.5192) | Xent Color 0.0842(0.1256) | Loss 4.6071(5.2181) | Error 0.1544(0.1620) | Error Color 0.0133(0.0270) |Steps 446(425.79) | Grad Norm 4.1353(6.5753) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.5892(9.7952) | Bit/dim 1.9827(1.9293) | Xent 0.4758(0.5166) | Xent Color 0.6787(0.2120) | Loss 4.9420(5.1143) | Error 0.1378(0.1614) | Error Color 0.1800(0.0587) |Steps 410(425.82) | Grad Norm 55.3538(16.0501) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.5899(9.8007) | Bit/dim 1.9386(1.9316) | Xent 0.4800(0.5192) | Xent Color 0.1978(0.2304) | Loss 4.8014(5.0374) | Error 0.1500(0.1625) | Error Color 0.0444(0.0691) |Steps 446(429.41) | Grad Norm 7.4046(15.7656) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.8833(9.8528) | Bit/dim 1.9092(1.9255) | Xent 0.4542(0.5219) | Xent Color 0.1707(0.2178) | Loss 4.7961(4.9721) | Error 0.1411(0.1644) | Error Color 0.0556(0.0632) |Steps 428(432.06) | Grad Norm 8.3582(13.6485) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 10.3082(9.9710) | Bit/dim 1.8906(1.9204) | Xent 0.5258(0.5143) | Xent Color 0.0986(0.1908) | Loss 4.6830(4.9038) | Error 0.1722(0.1621) | Error Color 0.0122(0.0529) |Steps 422(432.08) | Grad Norm 5.1246(11.3692) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.8633(9.9858) | Bit/dim 1.9116(1.9144) | Xent 0.5094(0.5104) | Xent Color 0.0960(0.1650) | Loss 4.6581(4.8356) | Error 0.1767(0.1610) | Error Color 0.0211(0.0435) |Steps 428(433.07) | Grad Norm 3.0286(9.2868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 63.3896, Epoch Time 742.6823(662.3506), Bit/dim 1.8961(best: 1.9215), Xent 0.3535, Xent Color 0.0539. Loss 1.9979, Error 0.1009(best: 0.1080), Error Color 0.0065(best: 0.0045)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.9728(9.9936) | Bit/dim 1.8960(1.9082) | Xent 0.5243(0.5122) | Xent Color 0.0841(0.1454) | Loss 4.5946(5.3187) | Error 0.1644(0.1610) | Error Color 0.0156(0.0367) |Steps 416(433.21) | Grad Norm 5.3918(7.8055) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 10.4201(9.9935) | Bit/dim 1.8612(1.8990) | Xent 0.4673(0.5067) | Xent Color 0.0871(0.1288) | Loss 4.5921(5.1266) | Error 0.1567(0.1587) | Error Color 0.0167(0.0307) |Steps 446(432.53) | Grad Norm 8.4730(6.9689) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.7474(9.9839) | Bit/dim 1.8907(1.8932) | Xent 0.4695(0.5052) | Xent Color 0.0617(0.1134) | Loss 4.6434(4.9888) | Error 0.1456(0.1587) | Error Color 0.0078(0.0262) |Steps 440(433.89) | Grad Norm 3.7984(6.1385) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.8757(10.0084) | Bit/dim 1.8557(1.8874) | Xent 0.5198(0.5016) | Xent Color 0.0554(0.1005) | Loss 4.4957(4.8797) | Error 0.1778(0.1575) | Error Color 0.0067(0.0222) |Steps 452(435.28) | Grad Norm 1.6784(5.3257) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.8415(9.9703) | Bit/dim 1.8506(1.8832) | Xent 0.4401(0.4906) | Xent Color 0.0626(0.0900) | Loss 4.6177(4.7942) | Error 0.1422(0.1544) | Error Color 0.0100(0.0188) |Steps 446(434.72) | Grad Norm 4.3655(4.7257) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 10.5292(9.9952) | Bit/dim 1.8479(1.8787) | Xent 0.5203(0.4915) | Xent Color 0.0614(0.0817) | Loss 4.5804(4.7339) | Error 0.1656(0.1544) | Error Color 0.0111(0.0166) |Steps 464(436.22) | Grad Norm 5.5786(4.5799) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 10.7419(10.0238) | Bit/dim 1.8353(1.8711) | Xent 0.4845(0.4890) | Xent Color 0.0587(0.0759) | Loss 4.4402(4.6779) | Error 0.1500(0.1529) | Error Color 0.0122(0.0153) |Steps 422(437.51) | Grad Norm 5.6602(4.5150) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 64.1741, Epoch Time 747.7778(664.9134), Bit/dim 1.8621(best: 1.8961), Xent 0.3449, Xent Color 0.0376. Loss 1.9578, Error 0.1000(best: 0.1009), Error Color 0.0058(best: 0.0045)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.9403(10.0309) | Bit/dim 1.8582(1.8661) | Xent 0.5286(0.4873) | Xent Color 0.0531(0.0720) | Loss 4.5667(5.1107) | Error 0.1622(0.1531) | Error Color 0.0056(0.0143) |Steps 392(436.93) | Grad Norm 7.6370(5.2106) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 10.0417(10.0676) | Bit/dim 1.8626(1.8614) | Xent 0.5685(0.4896) | Xent Color 0.0422(0.0682) | Loss 4.6324(4.9634) | Error 0.1667(0.1520) | Error Color 0.0067(0.0139) |Steps 440(436.81) | Grad Norm 5.3752(5.5125) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 9.6022(10.1181) | Bit/dim 1.8281(1.8555) | Xent 0.4403(0.4829) | Xent Color 0.0506(0.0641) | Loss 4.4517(4.8412) | Error 0.1367(0.1513) | Error Color 0.0111(0.0132) |Steps 428(437.92) | Grad Norm 6.6101(5.4938) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.8144(10.0892) | Bit/dim 1.8286(1.8518) | Xent 0.4281(0.4788) | Xent Color 0.0435(0.0600) | Loss 4.4166(4.7458) | Error 0.1300(0.1495) | Error Color 0.0067(0.0121) |Steps 416(436.10) | Grad Norm 1.7405(5.5560) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 10.1609(10.0772) | Bit/dim 1.8400(1.8457) | Xent 0.4830(0.4831) | Xent Color 0.0419(0.0567) | Loss 4.5130(4.6695) | Error 0.1667(0.1512) | Error Color 0.0089(0.0114) |Steps 440(436.96) | Grad Norm 3.4973(5.5399) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 9.6904(10.0770) | Bit/dim 1.8229(1.8403) | Xent 0.4402(0.4777) | Xent Color 0.0440(0.0568) | Loss 4.4614(4.6125) | Error 0.1389(0.1497) | Error Color 0.0044(0.0118) |Steps 428(436.73) | Grad Norm 9.9508(6.9927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 63.9196, Epoch Time 755.9353(667.6441), Bit/dim 1.8283(best: 1.8621), Xent 0.3325, Xent Color 0.0264. Loss 1.9180, Error 0.0991(best: 0.1000), Error Color 0.0023(best: 0.0045)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.5532(10.1268) | Bit/dim 1.8206(1.8360) | Xent 0.4510(0.4760) | Xent Color 0.0396(0.0557) | Loss 4.3863(5.1042) | Error 0.1367(0.1486) | Error Color 0.0078(0.0117) |Steps 392(433.55) | Grad Norm 3.1840(7.4160) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 10.2488(10.0850) | Bit/dim 1.8299(1.8309) | Xent 0.4161(0.4706) | Xent Color 0.0461(0.0523) | Loss 4.4169(4.9248) | Error 0.1400(0.1463) | Error Color 0.0100(0.0107) |Steps 416(432.72) | Grad Norm 7.6643(7.0933) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.9882(10.0609) | Bit/dim 1.8067(1.8263) | Xent 0.3952(0.4693) | Xent Color 0.0618(0.0515) | Loss 4.3414(4.7846) | Error 0.1322(0.1464) | Error Color 0.0122(0.0105) |Steps 428(432.53) | Grad Norm 25.6622(8.2934) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 9.8731(10.0091) | Bit/dim 2.8442(1.9501) | Xent 0.6004(0.4856) | Xent Color 4.6150(1.0942) | Loss 8.6065(5.3989) | Error 0.1644(0.1490) | Error Color 0.7256(0.1188) |Steps 428(431.87) | Grad Norm 43.9416(39.2975) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 10.2665(9.9597) | Bit/dim 2.1975(2.0462) | Xent 1.0290(0.6393) | Xent Color 0.9408(1.2833) | Loss 5.7898(5.6585) | Error 0.3511(0.2028) | Error Color 0.3378(0.2320) |Steps 422(429.43) | Grad Norm 21.9608(37.8968) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 9.4858(9.9070) | Bit/dim 2.0482(2.0631) | Xent 0.6791(0.6620) | Xent Color 0.6104(1.1339) | Loss 5.1002(5.5698) | Error 0.2311(0.2134) | Error Color 0.2433(0.2461) |Steps 392(425.84) | Grad Norm 6.0235(30.9979) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.1334(9.7647) | Bit/dim 1.9522(2.0379) | Xent 0.5977(0.6455) | Xent Color 0.5193(0.9789) | Loss 4.9012(5.3974) | Error 0.1844(0.2068) | Error Color 0.1911(0.2339) |Steps 392(417.56) | Grad Norm 4.6803(24.8633) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 59.1365, Epoch Time 726.4631(669.4086), Bit/dim 1.9321(best: 1.8283), Xent 0.4138, Xent Color 0.3135. Loss 2.1139, Error 0.1178(best: 0.0991), Error Color 0.0901(best: 0.0023)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.6237(9.6434) | Bit/dim 1.8903(2.0061) | Xent 0.5800(0.6295) | Xent Color 0.4010(0.8379) | Loss 4.6999(5.6986) | Error 0.1789(0.2008) | Error Color 0.1322(0.2139) |Steps 410(413.30) | Grad Norm 3.5629(19.4813) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 8.8429(9.5014) | Bit/dim 1.9020(1.9807) | Xent 0.5034(0.6067) | Xent Color 0.2994(0.7089) | Loss 4.6140(5.4282) | Error 0.1589(0.1925) | Error Color 0.0911(0.1888) |Steps 404(407.15) | Grad Norm 1.7256(15.1932) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 9.2456(9.4477) | Bit/dim 1.8775(1.9564) | Xent 0.4793(0.5896) | Xent Color 0.2712(0.5982) | Loss 4.5596(5.2173) | Error 0.1611(0.1876) | Error Color 0.0811(0.1623) |Steps 380(402.23) | Grad Norm 1.3158(12.0009) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.3810(9.4075) | Bit/dim 1.8863(1.9358) | Xent 0.4040(0.5649) | Xent Color 0.2271(0.5056) | Loss 4.4424(5.0422) | Error 0.1222(0.1788) | Error Color 0.0800(0.1390) |Steps 398(399.89) | Grad Norm 2.3480(9.5899) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.2028(9.3483) | Bit/dim 1.8691(1.9167) | Xent 0.4886(0.5444) | Xent Color 0.2128(0.4260) | Loss 4.5932(4.9070) | Error 0.1444(0.1726) | Error Color 0.0644(0.1177) |Steps 416(397.85) | Grad Norm 2.8454(7.7370) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.3554(9.3643) | Bit/dim 1.8497(1.9032) | Xent 0.4960(0.5380) | Xent Color 0.1719(0.3599) | Loss 4.4996(4.8045) | Error 0.1567(0.1693) | Error Color 0.0478(0.0992) |Steps 392(397.45) | Grad Norm 3.6753(6.5416) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.8169(9.3727) | Bit/dim 1.8516(1.8883) | Xent 0.4749(0.5244) | Xent Color 0.1324(0.3061) | Loss 4.5016(4.7151) | Error 0.1478(0.1637) | Error Color 0.0322(0.0841) |Steps 422(398.37) | Grad Norm 2.5035(5.6699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 59.8841, Epoch Time 695.6100(670.1947), Bit/dim 1.8530(best: 1.8283), Xent 0.3545, Xent Color 0.0914. Loss 1.9645, Error 0.1015(best: 0.0991), Error Color 0.0158(best: 0.0023)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.4703(9.3664) | Bit/dim 1.8269(1.8764) | Xent 0.4176(0.5149) | Xent Color 0.1337(0.2645) | Loss 4.4800(5.0676) | Error 0.1367(0.1620) | Error Color 0.0389(0.0727) |Steps 404(398.48) | Grad Norm 3.8688(5.1787) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 9.1010(9.4065) | Bit/dim 1.8373(1.8642) | Xent 0.5372(0.5089) | Xent Color 0.1036(0.2276) | Loss 4.4235(4.8997) | Error 0.1633(0.1598) | Error Color 0.0233(0.0616) |Steps 380(399.89) | Grad Norm 3.5310(4.7446) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 9.8410(9.4708) | Bit/dim 1.8161(1.8563) | Xent 0.4665(0.5015) | Xent Color 0.1171(0.1961) | Loss 4.3670(4.7799) | Error 0.1478(0.1582) | Error Color 0.0344(0.0522) |Steps 386(400.27) | Grad Norm 4.4878(4.4542) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 9.7615(9.4690) | Bit/dim 1.8182(1.8473) | Xent 0.4608(0.4944) | Xent Color 0.1093(0.1734) | Loss 4.4314(4.6802) | Error 0.1400(0.1559) | Error Color 0.0300(0.0460) |Steps 416(401.78) | Grad Norm 7.7667(4.7039) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 9.8699(9.4767) | Bit/dim 1.7693(1.8369) | Xent 0.4932(0.4883) | Xent Color 0.1011(0.1538) | Loss 4.3264(4.5958) | Error 0.1500(0.1540) | Error Color 0.0233(0.0401) |Steps 368(403.23) | Grad Norm 4.1777(4.4053) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.9927(9.5374) | Bit/dim 1.8087(1.8277) | Xent 0.4214(0.4762) | Xent Color 0.0927(0.1373) | Loss 4.4186(4.5322) | Error 0.1367(0.1511) | Error Color 0.0222(0.0348) |Steps 410(402.81) | Grad Norm 5.5355(4.1915) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 60.1612, Epoch Time 712.5636(671.4657), Bit/dim 1.8011(best: 1.8283), Xent 0.3275, Xent Color 0.0436. Loss 1.8938, Error 0.0973(best: 0.0991), Error Color 0.0058(best: 0.0023)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 9.0225(9.5431) | Bit/dim 1.7994(1.8195) | Xent 0.5271(0.4774) | Xent Color 0.0880(0.1234) | Loss 4.3917(4.9848) | Error 0.1589(0.1519) | Error Color 0.0267(0.0312) |Steps 398(404.89) | Grad Norm 5.6863(4.0963) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.8200(9.5644) | Bit/dim 1.7850(1.8099) | Xent 0.4274(0.4757) | Xent Color 0.0886(0.1123) | Loss 4.2841(4.8123) | Error 0.1533(0.1507) | Error Color 0.0211(0.0281) |Steps 410(405.59) | Grad Norm 6.6423(3.9921) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 9.6620(9.5435) | Bit/dim 1.7827(1.8028) | Xent 0.4222(0.4676) | Xent Color 0.0698(0.1021) | Loss 4.3854(4.6821) | Error 0.1333(0.1483) | Error Color 0.0189(0.0251) |Steps 410(406.36) | Grad Norm 6.2468(4.1829) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.7274(9.6030) | Bit/dim 1.7769(1.7951) | Xent 0.4456(0.4632) | Xent Color 0.0589(0.0934) | Loss 4.3457(4.5875) | Error 0.1500(0.1471) | Error Color 0.0111(0.0226) |Steps 434(409.28) | Grad Norm 1.6327(4.1732) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 9.6774(9.6256) | Bit/dim 1.7405(1.7862) | Xent 0.4298(0.4647) | Xent Color 0.0729(0.0872) | Loss 4.3275(4.5165) | Error 0.1367(0.1468) | Error Color 0.0144(0.0209) |Steps 422(408.99) | Grad Norm 7.5896(3.9593) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 9.4523(9.6454) | Bit/dim 1.7573(1.7774) | Xent 0.4249(0.4584) | Xent Color 0.1011(0.0830) | Loss 4.2997(4.4506) | Error 0.1311(0.1451) | Error Color 0.0289(0.0199) |Steps 428(409.17) | Grad Norm 12.0830(4.7174) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.1634(9.6460) | Bit/dim 1.7242(1.7675) | Xent 0.4185(0.4524) | Xent Color 0.0584(0.0784) | Loss 4.0999(4.3978) | Error 0.1467(0.1444) | Error Color 0.0111(0.0188) |Steps 380(411.48) | Grad Norm 2.0799(5.3422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 60.1832, Epoch Time 719.3254(672.9015), Bit/dim 1.7433(best: 1.8011), Xent 0.3041, Xent Color 0.0313. Loss 1.8272, Error 0.0909(best: 0.0973), Error Color 0.0040(best: 0.0023)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 9.9482(9.6689) | Bit/dim 1.7240(1.7593) | Xent 0.3943(0.4418) | Xent Color 0.0557(0.0737) | Loss 4.2108(4.8081) | Error 0.1344(0.1416) | Error Color 0.0100(0.0175) |Steps 422(410.66) | Grad Norm 2.6215(5.1983) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 9.6086(9.7202) | Bit/dim 1.7254(1.7505) | Xent 0.4438(0.4387) | Xent Color 0.0448(0.0698) | Loss 4.2087(4.6532) | Error 0.1333(0.1405) | Error Color 0.0056(0.0162) |Steps 398(409.76) | Grad Norm 3.1411(5.2376) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.8304(9.7377) | Bit/dim 1.7109(1.7414) | Xent 0.4626(0.4404) | Xent Color 0.0431(0.0667) | Loss 4.1815(4.5331) | Error 0.1556(0.1399) | Error Color 0.0044(0.0152) |Steps 404(408.19) | Grad Norm 3.0371(5.9257) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 9.9219(9.7728) | Bit/dim 1.7187(1.7337) | Xent 0.4550(0.4352) | Xent Color 0.0446(0.0628) | Loss 4.2413(4.4381) | Error 0.1478(0.1384) | Error Color 0.0067(0.0141) |Steps 416(411.48) | Grad Norm 5.0504(6.0337) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 9.8108(9.7798) | Bit/dim 1.7085(1.7246) | Xent 0.4208(0.4319) | Xent Color 0.0471(0.0610) | Loss 4.1142(4.3600) | Error 0.1356(0.1358) | Error Color 0.0089(0.0140) |Steps 392(411.92) | Grad Norm 3.3368(6.4857) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 9.8919(9.7968) | Bit/dim 1.6744(1.7133) | Xent 0.4279(0.4279) | Xent Color 0.0414(0.0587) | Loss 4.0900(4.2984) | Error 0.1222(0.1353) | Error Color 0.0078(0.0136) |Steps 398(412.95) | Grad Norm 5.6649(6.6559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 60.4298, Epoch Time 733.7865(674.7281), Bit/dim 1.6802(best: 1.7433), Xent 0.2695, Xent Color 0.0188. Loss 1.7523, Error 0.0813(best: 0.0909), Error Color 0.0016(best: 0.0023)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 10.2082(9.8646) | Bit/dim 1.6829(1.7037) | Xent 0.3864(0.4271) | Xent Color 0.0911(0.0607) | Loss 4.1677(4.7937) | Error 0.1256(0.1350) | Error Color 0.0311(0.0148) |Steps 386(411.50) | Grad Norm 20.3281(8.3567) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 9.9594(9.9198) | Bit/dim 1.6791(1.6954) | Xent 0.4389(0.4194) | Xent Color 0.0516(0.0623) | Loss 4.1505(4.6166) | Error 0.1333(0.1332) | Error Color 0.0111(0.0158) |Steps 392(412.03) | Grad Norm 5.9710(9.6952) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 9.7465(9.9087) | Bit/dim 1.6365(1.6863) | Xent 0.4879(0.4153) | Xent Color 0.0413(0.0582) | Loss 4.0554(4.4724) | Error 0.1511(0.1313) | Error Color 0.0044(0.0143) |Steps 398(409.67) | Grad Norm 10.2603(9.4937) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 9.6627(9.9259) | Bit/dim 1.6420(1.6759) | Xent 0.4077(0.4098) | Xent Color 0.0395(0.0548) | Loss 4.0356(4.3648) | Error 0.1322(0.1299) | Error Color 0.0056(0.0132) |Steps 404(410.55) | Grad Norm 4.7376(8.6313) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.9097(9.9369) | Bit/dim 1.6158(1.6651) | Xent 0.4128(0.4062) | Xent Color 0.0421(0.0515) | Loss 4.0091(4.2726) | Error 0.1156(0.1278) | Error Color 0.0100(0.0122) |Steps 392(408.76) | Grad Norm 8.1705(8.6451) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.1059(9.9468) | Bit/dim 1.6241(1.6548) | Xent 0.3790(0.4000) | Xent Color 0.0396(0.0486) | Loss 4.0522(4.2072) | Error 0.1156(0.1248) | Error Color 0.0089(0.0116) |Steps 422(411.70) | Grad Norm 9.3187(7.9614) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 9.7501(9.9518) | Bit/dim 1.6183(1.6445) | Xent 0.3768(0.3903) | Xent Color 0.0286(0.0483) | Loss 4.0162(4.1484) | Error 0.1089(0.1224) | Error Color 0.0033(0.0117) |Steps 416(411.46) | Grad Norm 5.4375(8.5588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 60.0163, Epoch Time 741.5100(676.7315), Bit/dim 1.6144(best: 1.6802), Xent 0.2358, Xent Color 0.0155. Loss 1.6772, Error 0.0714(best: 0.0813), Error Color 0.0012(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.1355(9.9404) | Bit/dim 1.6085(1.6351) | Xent 0.3394(0.3847) | Xent Color 0.0311(0.0451) | Loss 4.0157(4.5692) | Error 0.1122(0.1218) | Error Color 0.0033(0.0103) |Steps 416(412.35) | Grad Norm 5.2358(8.6087) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 10.5342(9.9666) | Bit/dim 1.5827(1.6263) | Xent 0.3567(0.3775) | Xent Color 0.0380(0.0440) | Loss 3.9060(4.4123) | Error 0.1122(0.1195) | Error Color 0.0111(0.0105) |Steps 446(414.96) | Grad Norm 5.9668(8.7298) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 9.6023(9.9947) | Bit/dim 1.5639(1.6149) | Xent 0.3447(0.3760) | Xent Color 0.0402(0.0422) | Loss 3.8250(4.2825) | Error 0.1067(0.1190) | Error Color 0.0100(0.0100) |Steps 392(416.49) | Grad Norm 4.4639(8.4286) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.8150(9.9994) | Bit/dim 1.5719(1.6031) | Xent 0.3385(0.3697) | Xent Color 0.0285(0.0390) | Loss 3.8457(4.1744) | Error 0.1156(0.1175) | Error Color 0.0044(0.0089) |Steps 386(414.31) | Grad Norm 1.8576(7.5214) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 9.9055(9.9655) | Bit/dim 1.5636(1.5918) | Xent 0.3483(0.3629) | Xent Color 0.0269(0.0370) | Loss 3.8565(4.0903) | Error 0.1167(0.1167) | Error Color 0.0078(0.0085) |Steps 410(415.60) | Grad Norm 3.3577(7.4797) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 10.3700(9.9565) | Bit/dim 1.5493(1.5804) | Xent 0.3499(0.3582) | Xent Color 0.0347(0.0360) | Loss 3.8685(4.0238) | Error 0.1189(0.1147) | Error Color 0.0078(0.0084) |Steps 428(414.90) | Grad Norm 12.8660(7.8440) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 10.2832(9.9974) | Bit/dim 2.2956(1.7406) | Xent 0.6901(0.6220) | Xent Color 4.3895(1.8496) | Loss 7.2528(5.2327) | Error 0.2167(0.1557) | Error Color 0.7678(0.1458) |Steps 410(419.68) | Grad Norm 36.3873(40.2375) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 62.7101, Epoch Time 746.3096(678.8189), Bit/dim 2.4191(best: 1.6144), Xent 0.5923, Xent Color 3.3372. Loss 3.4014, Error 0.2007(best: 0.0714), Error Color 0.6566(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 10.1333(10.1293) | Bit/dim 2.1271(1.8799) | Xent 0.9841(0.7846) | Xent Color 1.0613(1.7780) | Loss 5.6371(5.9599) | Error 0.3233(0.2223) | Error Color 0.4256(0.2387) |Steps 452(427.34) | Grad Norm 7.7708(35.6025) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 11.2016(10.2031) | Bit/dim 2.0224(1.9345) | Xent 0.6565(0.7648) | Xent Color 0.7359(1.5362) | Loss 5.2270(5.8201) | Error 0.2111(0.2230) | Error Color 0.2856(0.2668) |Steps 464(433.34) | Grad Norm 8.9987(28.4833) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 10.5970(10.2935) | Bit/dim 1.9295(1.9437) | Xent 0.5334(0.7276) | Xent Color 0.5174(1.2861) | Loss 4.8690(5.6152) | Error 0.1756(0.2174) | Error Color 0.1800(0.2525) |Steps 458(437.84) | Grad Norm 3.1824(22.2492) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 10.2327(10.3003) | Bit/dim 1.9007(1.9398) | Xent 0.5167(0.6828) | Xent Color 0.3789(1.0609) | Loss 4.7422(5.4170) | Error 0.1778(0.2081) | Error Color 0.1078(0.2227) |Steps 458(440.80) | Grad Norm 2.8235(17.2816) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 10.1820(10.3047) | Bit/dim 1.8648(1.9237) | Xent 0.5484(0.6456) | Xent Color 0.2979(0.8734) | Loss 4.7098(5.2378) | Error 0.1722(0.1982) | Error Color 0.0811(0.1923) |Steps 446(443.57) | Grad Norm 2.9653(13.7619) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 10.1602(10.2993) | Bit/dim 1.8198(1.9020) | Xent 0.4934(0.6022) | Xent Color 0.2423(0.7184) | Loss 4.4867(5.0699) | Error 0.1489(0.1855) | Error Color 0.0689(0.1644) |Steps 452(444.48) | Grad Norm 1.7931(11.5363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 64.7966, Epoch Time 773.3804(681.6557), Bit/dim 1.8159(best: 1.6144), Xent 0.3157, Xent Color 0.1315. Loss 1.9277, Error 0.0932(best: 0.0714), Error Color 0.0150(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 10.6263(10.2966) | Bit/dim 1.8000(1.8783) | Xent 0.4601(0.5695) | Xent Color 0.1990(0.5878) | Loss 4.4658(5.4611) | Error 0.1411(0.1759) | Error Color 0.0500(0.1371) |Steps 452(445.45) | Grad Norm 2.4363(9.5126) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 10.0028(10.2558) | Bit/dim 1.7604(1.8530) | Xent 0.4151(0.5427) | Xent Color 0.2021(0.4838) | Loss 4.3342(5.1840) | Error 0.1400(0.1681) | Error Color 0.0611(0.1147) |Steps 434(444.01) | Grad Norm 3.2521(7.8806) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 10.2177(10.2475) | Bit/dim 1.7469(1.8266) | Xent 0.4738(0.5156) | Xent Color 0.1613(0.4023) | Loss 4.2895(4.9681) | Error 0.1378(0.1590) | Error Color 0.0467(0.0963) |Steps 416(441.86) | Grad Norm 6.0112(6.8028) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 9.8435(10.2070) | Bit/dim 1.6885(1.7971) | Xent 0.4623(0.4946) | Xent Color 0.1590(0.3432) | Loss 4.1952(4.7753) | Error 0.1533(0.1533) | Error Color 0.0378(0.0845) |Steps 422(440.01) | Grad Norm 10.5106(8.1174) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 9.8037(10.1314) | Bit/dim 1.6798(1.7661) | Xent 0.4082(0.4690) | Xent Color 0.1353(0.2998) | Loss 4.0664(4.6094) | Error 0.1367(0.1469) | Error Color 0.0311(0.0764) |Steps 410(434.63) | Grad Norm 5.8663(9.5962) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 9.7867(10.0477) | Bit/dim 1.6534(1.7371) | Xent 0.4081(0.4519) | Xent Color 0.4217(0.2763) | Loss 4.2386(4.4733) | Error 0.1256(0.1416) | Error Color 0.1833(0.0757) |Steps 416(429.69) | Grad Norm 61.7085(13.2608) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 10.0131(10.0421) | Bit/dim 1.6232(1.7098) | Xent 0.3952(0.4391) | Xent Color 0.2168(0.2845) | Loss 4.0239(4.3801) | Error 0.1344(0.1376) | Error Color 0.0667(0.0844) |Steps 398(425.48) | Grad Norm 17.8403(16.5872) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 61.8002, Epoch Time 749.0091(683.6763), Bit/dim 1.6221(best: 1.6144), Xent 0.2556, Xent Color 0.1093. Loss 1.7133, Error 0.0769(best: 0.0714), Error Color 0.0179(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 10.3150(10.0595) | Bit/dim 1.5972(1.6855) | Xent 0.3539(0.4315) | Xent Color 0.1322(0.2533) | Loss 3.9631(4.7336) | Error 0.1100(0.1371) | Error Color 0.0400(0.0745) |Steps 416(422.28) | Grad Norm 8.0563(14.7845) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 9.8078(10.0606) | Bit/dim 1.5882(1.6609) | Xent 0.3766(0.4212) | Xent Color 0.1122(0.2186) | Loss 3.9265(4.5286) | Error 0.1267(0.1335) | Error Color 0.0267(0.0631) |Steps 428(422.82) | Grad Norm 7.1510(13.2159) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.9103(10.0174) | Bit/dim 1.5718(1.6377) | Xent 0.3272(0.4045) | Xent Color 0.0913(0.1887) | Loss 3.8428(4.3635) | Error 0.1122(0.1282) | Error Color 0.0233(0.0531) |Steps 380(420.23) | Grad Norm 2.5643(11.5978) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 9.7013(10.0046) | Bit/dim 1.5517(1.6179) | Xent 0.3483(0.3955) | Xent Color 0.0892(0.1623) | Loss 3.8932(4.2379) | Error 0.1133(0.1260) | Error Color 0.0211(0.0440) |Steps 422(422.36) | Grad Norm 4.7184(9.4968) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 9.8189(9.9827) | Bit/dim 1.5497(1.6017) | Xent 0.3628(0.3861) | Xent Color 0.0878(0.1410) | Loss 3.8386(4.1395) | Error 0.1078(0.1227) | Error Color 0.0222(0.0372) |Steps 416(422.84) | Grad Norm 2.8729(7.8201) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.9678(9.9750) | Bit/dim 1.5469(1.5855) | Xent 0.3455(0.3780) | Xent Color 0.0664(0.1230) | Loss 3.8148(4.0558) | Error 0.1022(0.1196) | Error Color 0.0144(0.0318) |Steps 392(420.11) | Grad Norm 1.8629(6.7844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 61.0764, Epoch Time 742.2687(685.4341), Bit/dim 1.5338(best: 1.6144), Xent 0.2247, Xent Color 0.0305. Loss 1.5976, Error 0.0691(best: 0.0714), Error Color 0.0018(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.7488(9.9585) | Bit/dim 1.5333(1.5729) | Xent 0.4143(0.3762) | Xent Color 0.0737(0.1092) | Loss 3.8108(4.5166) | Error 0.1256(0.1189) | Error Color 0.0144(0.0274) |Steps 404(420.15) | Grad Norm 4.8420(6.4507) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 10.2528(9.9654) | Bit/dim 1.5184(1.5597) | Xent 0.3267(0.3692) | Xent Color 0.0613(0.0965) | Loss 3.7921(4.3211) | Error 0.1078(0.1173) | Error Color 0.0100(0.0234) |Steps 428(421.03) | Grad Norm 4.4821(5.5754) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 9.9497(9.9489) | Bit/dim 1.5202(1.5475) | Xent 0.3288(0.3660) | Xent Color 0.0692(0.0872) | Loss 3.8253(4.1726) | Error 0.1022(0.1160) | Error Color 0.0189(0.0211) |Steps 434(419.96) | Grad Norm 3.6234(5.9174) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.6112(9.9566) | Bit/dim 1.5096(1.5368) | Xent 0.3253(0.3565) | Xent Color 0.0798(0.0802) | Loss 3.6597(4.0584) | Error 0.1089(0.1132) | Error Color 0.0222(0.0193) |Steps 422(420.37) | Grad Norm 22.5290(6.4323) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.6157(9.8823) | Bit/dim 1.7681(1.5865) | Xent 0.4696(0.3674) | Xent Color 0.7774(0.5765) | Loss 4.6191(4.3171) | Error 0.1578(0.1169) | Error Color 0.3144(0.1193) |Steps 416(416.49) | Grad Norm 16.5597(24.5834) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 10.2017(9.9220) | Bit/dim 1.6603(1.6182) | Xent 0.4742(0.4069) | Xent Color 0.3438(0.5597) | Loss 4.2773(4.3590) | Error 0.1500(0.1296) | Error Color 0.1122(0.1373) |Steps 452(421.86) | Grad Norm 6.5651(19.9757) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 10.3754(10.0105) | Bit/dim 1.5950(1.6183) | Xent 0.3821(0.4118) | Xent Color 0.2228(0.4881) | Loss 4.1956(4.3235) | Error 0.1222(0.1304) | Error Color 0.0633(0.1269) |Steps 428(427.85) | Grad Norm 2.6376(15.9250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 65.9392, Epoch Time 748.3303(687.3210), Bit/dim 1.5683(best: 1.5338), Xent 0.2642, Xent Color 0.1272. Loss 1.6662, Error 0.0792(best: 0.0691), Error Color 0.0142(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 10.4575(10.0887) | Bit/dim 1.5612(1.6036) | Xent 0.3712(0.4048) | Xent Color 0.2165(0.4208) | Loss 4.0470(4.7748) | Error 0.1111(0.1281) | Error Color 0.0678(0.1129) |Steps 470(433.38) | Grad Norm 2.8754(12.5576) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 10.2646(10.1814) | Bit/dim 1.5369(1.5848) | Xent 0.3556(0.3942) | Xent Color 0.1820(0.3595) | Loss 3.9042(4.5500) | Error 0.1067(0.1254) | Error Color 0.0578(0.0977) |Steps 428(435.50) | Grad Norm 1.5059(9.9785) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 10.5947(10.1948) | Bit/dim 1.5051(1.5644) | Xent 0.3358(0.3845) | Xent Color 0.1483(0.3052) | Loss 3.8252(4.3628) | Error 0.1089(0.1219) | Error Color 0.0378(0.0827) |Steps 458(433.51) | Grad Norm 4.5823(8.2657) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 10.6993(10.2009) | Bit/dim 1.4958(1.5461) | Xent 0.3320(0.3730) | Xent Color 0.1500(0.2585) | Loss 3.7427(4.2113) | Error 0.1044(0.1185) | Error Color 0.0367(0.0692) |Steps 428(430.68) | Grad Norm 8.9834(7.1574) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 10.4745(10.2171) | Bit/dim 1.4793(1.5307) | Xent 0.3432(0.3684) | Xent Color 0.1039(0.2188) | Loss 3.6944(4.0924) | Error 0.1211(0.1179) | Error Color 0.0189(0.0582) |Steps 386(425.49) | Grad Norm 5.3246(6.4619) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 10.1869(10.1557) | Bit/dim 1.4725(1.5144) | Xent 0.3641(0.3598) | Xent Color 0.0789(0.1864) | Loss 3.7258(3.9908) | Error 0.1100(0.1164) | Error Color 0.0100(0.0488) |Steps 434(424.35) | Grad Norm 4.0524(5.6675) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 10.7535(10.1654) | Bit/dim 1.4709(1.5001) | Xent 0.3301(0.3516) | Xent Color 0.0816(0.1599) | Loss 3.6847(3.9075) | Error 0.1100(0.1141) | Error Color 0.0200(0.0413) |Steps 452(422.82) | Grad Norm 2.6933(5.0555) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 61.0744, Epoch Time 758.9381(689.4695), Bit/dim 1.4612(best: 1.5338), Xent 0.2039, Xent Color 0.0342. Loss 1.5207, Error 0.0643(best: 0.0691), Error Color 0.0023(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 9.9066(10.1055) | Bit/dim 1.4593(1.4880) | Xent 0.3381(0.3432) | Xent Color 0.0730(0.1385) | Loss 3.6785(4.2486) | Error 0.1144(0.1122) | Error Color 0.0189(0.0348) |Steps 410(417.53) | Grad Norm 3.1231(4.5726) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 9.7961(10.0776) | Bit/dim 1.4289(1.4776) | Xent 0.3100(0.3363) | Xent Color 0.0724(0.1207) | Loss 3.6307(4.0884) | Error 0.0944(0.1104) | Error Color 0.0156(0.0299) |Steps 410(418.52) | Grad Norm 3.6048(4.7244) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 9.7378(9.9942) | Bit/dim 1.4205(1.4652) | Xent 0.2662(0.3262) | Xent Color 0.0747(0.1064) | Loss 3.5320(3.9548) | Error 0.0911(0.1069) | Error Color 0.0178(0.0254) |Steps 386(417.10) | Grad Norm 3.6357(4.2391) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 9.2524(9.9604) | Bit/dim 1.4113(1.4552) | Xent 0.3094(0.3227) | Xent Color 0.0690(0.0955) | Loss 3.4822(3.8596) | Error 0.0978(0.1045) | Error Color 0.0144(0.0226) |Steps 398(415.50) | Grad Norm 5.9700(4.7057) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.7771(9.9602) | Bit/dim 1.4181(1.4437) | Xent 0.2923(0.3161) | Xent Color 0.0633(0.0888) | Loss 3.5182(3.7758) | Error 0.0967(0.1013) | Error Color 0.0144(0.0208) |Steps 410(414.58) | Grad Norm 3.7392(5.2221) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 9.6720(9.9191) | Bit/dim 1.4140(1.4366) | Xent 0.3101(0.3137) | Xent Color 0.0501(0.0799) | Loss 3.5651(3.7188) | Error 0.1000(0.0998) | Error Color 0.0100(0.0183) |Steps 392(413.76) | Grad Norm 3.9273(5.5881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 61.5722, Epoch Time 738.6470(690.9448), Bit/dim 1.4057(best: 1.4612), Xent 0.1830, Xent Color 0.0245. Loss 1.4575, Error 0.0562(best: 0.0643), Error Color 0.0014(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.1537(9.9846) | Bit/dim 1.3971(1.4283) | Xent 0.3006(0.3084) | Xent Color 0.0539(0.0741) | Loss 3.5099(4.1474) | Error 0.0978(0.0988) | Error Color 0.0133(0.0170) |Steps 404(414.50) | Grad Norm 6.8253(6.0266) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 9.8622(9.9373) | Bit/dim 1.4008(1.4207) | Xent 0.3197(0.3059) | Xent Color 0.0554(0.0723) | Loss 3.5021(3.9826) | Error 0.1089(0.0980) | Error Color 0.0144(0.0173) |Steps 416(411.99) | Grad Norm 12.0922(7.6036) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.6527(9.8944) | Bit/dim 1.3871(1.4129) | Xent 0.3088(0.3045) | Xent Color 0.0493(0.0683) | Loss 3.4190(3.8502) | Error 0.0900(0.0978) | Error Color 0.0133(0.0162) |Steps 374(411.17) | Grad Norm 6.9953(7.7613) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 9.7119(9.8949) | Bit/dim 1.3701(1.4061) | Xent 0.2936(0.3005) | Xent Color 0.0534(0.0621) | Loss 3.4259(3.7509) | Error 0.1078(0.0980) | Error Color 0.0111(0.0142) |Steps 404(409.63) | Grad Norm 6.0237(6.9413) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 10.1863(9.8957) | Bit/dim 1.3693(1.3986) | Xent 0.2706(0.2949) | Xent Color 0.0582(0.0588) | Loss 3.4579(3.6744) | Error 0.0900(0.0961) | Error Color 0.0122(0.0132) |Steps 416(409.82) | Grad Norm 13.6630(7.4091) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.6462(9.8998) | Bit/dim 1.3721(1.3918) | Xent 0.3300(0.2912) | Xent Color 0.0585(0.0563) | Loss 3.4812(3.6208) | Error 0.1100(0.0939) | Error Color 0.0133(0.0126) |Steps 398(412.18) | Grad Norm 15.0893(7.9719) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 10.0334(9.8679) | Bit/dim 1.3435(1.3837) | Xent 0.2382(0.2845) | Xent Color 0.0369(0.0552) | Loss 3.4450(3.5727) | Error 0.0711(0.0917) | Error Color 0.0056(0.0129) |Steps 422(413.60) | Grad Norm 4.3161(8.5931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 60.3829, Epoch Time 735.2182(692.2730), Bit/dim 1.3621(best: 1.4057), Xent 0.1692, Xent Color 0.0136. Loss 1.4078, Error 0.0528(best: 0.0562), Error Color 0.0009(best: 0.0012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 10.2754(9.8937) | Bit/dim 1.3546(1.3770) | Xent 0.2761(0.2809) | Xent Color 0.0358(0.0517) | Loss 3.4598(3.9616) | Error 0.0944(0.0910) | Error Color 0.0056(0.0121) |Steps 446(412.98) | Grad Norm 5.3003(7.9456) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 9.9697(9.8952) | Bit/dim 1.3508(1.3720) | Xent 0.2352(0.2751) | Xent Color 0.0376(0.0479) | Loss 3.4107(3.8168) | Error 0.0733(0.0888) | Error Color 0.0067(0.0109) |Steps 416(412.89) | Grad Norm 9.4054(7.6954) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 9.8462(9.9162) | Bit/dim 1.3414(1.3648) | Xent 0.2660(0.2711) | Xent Color 0.0467(0.0449) | Loss 3.4174(3.7091) | Error 0.0989(0.0877) | Error Color 0.0100(0.0098) |Steps 422(414.38) | Grad Norm 4.9217(6.9250) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 10.2171(9.9389) | Bit/dim 1.3305(1.3575) | Xent 0.2897(0.2741) | Xent Color 0.0279(0.0423) | Loss 3.3878(3.6251) | Error 0.0956(0.0882) | Error Color 0.0056(0.0092) |Steps 392(414.15) | Grad Norm 4.9393(6.8379) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 10.0205(9.9354) | Bit/dim 1.3202(1.3499) | Xent 0.2432(0.2684) | Xent Color 0.0260(0.0405) | Loss 3.2995(3.5503) | Error 0.0756(0.0865) | Error Color 0.0033(0.0087) |Steps 416(414.27) | Grad Norm 3.7511(6.7940) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 10.0323(9.9320) | Bit/dim 1.3144(1.3424) | Xent 0.2907(0.2705) | Xent Color 0.0287(0.0385) | Loss 3.3233(3.4988) | Error 0.0867(0.0860) | Error Color 0.0033(0.0079) |Steps 392(415.92) | Grad Norm 5.7836(6.7359) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 61.7127, Epoch Time 741.8485(693.7603), Bit/dim 1.4725(best: 1.3621), Xent 0.1826, Xent Color 1.4504. Loss 1.8807, Error 0.0601(best: 0.0528), Error Color 0.3005(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 9.8299(9.9416) | Bit/dim 2.3891(1.3756) | Xent 0.2241(0.2705) | Xent Color 10.4625(0.4077) | Loss 9.9173(4.2004) | Error 0.0722(0.0866) | Error Color 0.7122(0.0425) |Steps 398(418.02) | Grad Norm 188.4707(20.3424) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 10.6232(10.0766) | Bit/dim 1.6781(1.4648) | Xent 0.4328(0.3105) | Xent Color 0.7547(0.6265) | Loss 4.5534(4.3492) | Error 0.1333(0.0997) | Error Color 0.3067(0.1294) |Steps 488(428.36) | Grad Norm 12.1369(22.6753) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 10.9491(10.2680) | Bit/dim 1.5429(1.4987) | Xent 0.2612(0.3178) | Xent Color 0.4172(0.6091) | Loss 4.0207(4.3199) | Error 0.0811(0.1022) | Error Color 0.1456(0.1514) |Steps 488(437.03) | Grad Norm 5.9660(18.6091) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 10.2930(10.2832) | Bit/dim 1.4647(1.4962) | Xent 0.3546(0.3189) | Xent Color 0.2825(0.5342) | Loss 3.8449(4.2049) | Error 0.1133(0.1025) | Error Color 0.0867(0.1388) |Steps 434(436.99) | Grad Norm 5.4558(15.0186) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 10.6725(10.3227) | Bit/dim 1.3706(1.4484) | Xent 0.2488(0.3010) | Xent Color 0.1552(0.3840) | Loss 3.6079(3.9453) | Error 0.0867(0.0968) | Error Color 0.0422(0.1036) |Steps 464(440.56) | Grad Norm 1.3713(9.9794) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 10.0897(10.3271) | Bit/dim 1.3418(1.4249) | Xent 0.2523(0.2880) | Xent Color 0.1065(0.3159) | Loss 3.4480(3.8281) | Error 0.0811(0.0928) | Error Color 0.0256(0.0845) |Steps 434(440.91) | Grad Norm 4.1008(8.0573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 63.4396, Epoch Time 774.4560(696.1812), Bit/dim 1.3461(best: 1.3621), Xent 0.1601, Xent Color 0.0530. Loss 1.3994, Error 0.0501(best: 0.0528), Error Color 0.0051(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 9.7159(10.3407) | Bit/dim 1.3411(1.4025) | Xent 0.2356(0.2824) | Xent Color 0.0936(0.2620) | Loss 3.3885(4.2266) | Error 0.0722(0.0911) | Error Color 0.0167(0.0691) |Steps 416(439.76) | Grad Norm 1.3489(6.4721) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 10.7259(10.3709) | Bit/dim 1.3289(1.3846) | Xent 0.2311(0.2720) | Xent Color 0.0984(0.2188) | Loss 3.4900(4.0224) | Error 0.0778(0.0880) | Error Color 0.0244(0.0569) |Steps 452(439.05) | Grad Norm 1.6449(5.2228) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 10.2679(10.3406) | Bit/dim 1.3108(1.3679) | Xent 0.2810(0.2708) | Xent Color 0.0866(0.1849) | Loss 3.4103(3.8657) | Error 0.0911(0.0876) | Error Color 0.0278(0.0479) |Steps 446(437.57) | Grad Norm 3.2362(4.5234) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.9474(10.2756) | Bit/dim 1.3077(1.3544) | Xent 0.2806(0.2669) | Xent Color 0.0666(0.1554) | Loss 3.4108(3.7414) | Error 0.0878(0.0852) | Error Color 0.0167(0.0391) |Steps 422(431.35) | Grad Norm 2.0826(3.7961) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.9193(10.1987) | Bit/dim 1.2884(1.3403) | Xent 0.2661(0.2618) | Xent Color 0.0774(0.1327) | Loss 3.2406(3.6347) | Error 0.0778(0.0834) | Error Color 0.0189(0.0324) |Steps 428(429.01) | Grad Norm 4.7801(3.4925) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 10.5339(10.1550) | Bit/dim 1.3012(1.3291) | Xent 0.2460(0.2562) | Xent Color 0.0659(0.1149) | Loss 3.3564(3.5567) | Error 0.0722(0.0817) | Error Color 0.0156(0.0273) |Steps 428(426.83) | Grad Norm 2.6699(3.4167) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 10.2700(10.1706) | Bit/dim 1.2855(1.3184) | Xent 0.2501(0.2521) | Xent Color 0.0554(0.1004) | Loss 3.2825(3.4932) | Error 0.0844(0.0805) | Error Color 0.0122(0.0231) |Steps 452(425.56) | Grad Norm 2.0842(3.0904) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 62.1158, Epoch Time 757.3151(698.0152), Bit/dim 1.2880(best: 1.3461), Xent 0.1471, Xent Color 0.0230. Loss 1.3305, Error 0.0488(best: 0.0501), Error Color 0.0012(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.3481(10.2046) | Bit/dim 1.2964(1.3100) | Xent 0.2050(0.2503) | Xent Color 0.0609(0.0898) | Loss 3.3399(3.8531) | Error 0.0700(0.0810) | Error Color 0.0122(0.0201) |Steps 440(426.43) | Grad Norm 1.0945(3.2875) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 10.5683(10.1811) | Bit/dim 1.2687(1.3011) | Xent 0.2423(0.2489) | Xent Color 0.0457(0.0804) | Loss 3.2782(3.7047) | Error 0.0833(0.0807) | Error Color 0.0078(0.0173) |Steps 410(424.86) | Grad Norm 1.5369(3.3384) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 10.0126(10.1552) | Bit/dim 1.2698(1.2930) | Xent 0.2177(0.2466) | Xent Color 0.0460(0.0732) | Loss 3.2510(3.5903) | Error 0.0700(0.0797) | Error Color 0.0089(0.0156) |Steps 452(424.84) | Grad Norm 2.1821(3.2811) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.8742(10.0474) | Bit/dim 1.2575(1.2848) | Xent 0.2559(0.2453) | Xent Color 0.0509(0.0670) | Loss 3.2785(3.4988) | Error 0.0789(0.0788) | Error Color 0.0089(0.0143) |Steps 422(422.13) | Grad Norm 2.7179(3.2238) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.6654(10.0269) | Bit/dim 1.2551(1.2776) | Xent 0.2248(0.2391) | Xent Color 0.0446(0.0613) | Loss 3.2128(3.4277) | Error 0.0644(0.0767) | Error Color 0.0089(0.0128) |Steps 416(419.85) | Grad Norm 2.8922(2.9633) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.9303(10.0114) | Bit/dim 1.2456(1.2706) | Xent 0.2225(0.2383) | Xent Color 0.0398(0.0561) | Loss 3.2230(3.3817) | Error 0.0811(0.0779) | Error Color 0.0078(0.0116) |Steps 428(420.58) | Grad Norm 2.0103(3.0159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 59.3812, Epoch Time 744.4897(699.4094), Bit/dim 1.2445(best: 1.2880), Xent 0.1312, Xent Color 0.0131. Loss 1.2806, Error 0.0431(best: 0.0488), Error Color 0.0005(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 9.5766(9.9980) | Bit/dim 1.2397(1.2633) | Xent 0.2378(0.2387) | Xent Color 0.0405(0.0527) | Loss 3.2008(3.8170) | Error 0.0789(0.0785) | Error Color 0.0100(0.0107) |Steps 398(418.95) | Grad Norm 2.4164(3.0276) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 9.9126(9.9668) | Bit/dim 1.2242(1.2546) | Xent 0.2485(0.2351) | Xent Color 0.0415(0.0493) | Loss 3.2043(3.6529) | Error 0.0733(0.0766) | Error Color 0.0089(0.0098) |Steps 434(420.18) | Grad Norm 4.5084(3.0002) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 9.9058(9.9610) | Bit/dim 1.2266(1.2488) | Xent 0.2218(0.2311) | Xent Color 0.0346(0.0458) | Loss 3.1562(3.5310) | Error 0.0667(0.0746) | Error Color 0.0078(0.0091) |Steps 410(420.69) | Grad Norm 2.8675(3.3773) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 9.9615(9.9834) | Bit/dim 1.2264(1.2420) | Xent 0.2141(0.2303) | Xent Color 0.0332(0.0441) | Loss 3.1471(3.4358) | Error 0.0800(0.0743) | Error Color 0.0056(0.0088) |Steps 410(419.12) | Grad Norm 3.3604(3.9531) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 9.6636(9.9534) | Bit/dim 1.2221(1.2340) | Xent 0.2560(0.2322) | Xent Color 0.0288(0.0420) | Loss 3.1819(3.3652) | Error 0.0800(0.0751) | Error Color 0.0044(0.0085) |Steps 416(417.97) | Grad Norm 3.6656(4.0233) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 9.6725(9.9122) | Bit/dim 1.2122(1.2268) | Xent 0.1864(0.2301) | Xent Color 0.0325(0.0396) | Loss 3.0924(3.3023) | Error 0.0622(0.0738) | Error Color 0.0056(0.0077) |Steps 416(415.09) | Grad Norm 3.6302(3.8858) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 9.7304(9.9089) | Bit/dim 1.2019(1.2212) | Xent 0.2522(0.2265) | Xent Color 0.0294(0.0368) | Loss 3.1730(3.2596) | Error 0.0767(0.0731) | Error Color 0.0022(0.0069) |Steps 422(415.57) | Grad Norm 1.9445(3.6551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 60.6211, Epoch Time 738.3613(700.5780), Bit/dim 1.1956(best: 1.2445), Xent 0.1234, Xent Color 0.0093. Loss 1.2288, Error 0.0407(best: 0.0431), Error Color 0.0001(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 9.7204(9.9258) | Bit/dim 1.1872(1.2142) | Xent 0.2120(0.2239) | Xent Color 0.0326(0.0351) | Loss 3.0407(3.6477) | Error 0.0678(0.0722) | Error Color 0.0056(0.0065) |Steps 398(415.42) | Grad Norm 4.2746(3.7200) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 10.0553(9.9403) | Bit/dim 1.1580(1.2064) | Xent 0.2014(0.2179) | Xent Color 0.0278(0.0332) | Loss 3.0882(3.5023) | Error 0.0644(0.0704) | Error Color 0.0033(0.0058) |Steps 434(417.18) | Grad Norm 3.5089(4.0208) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 10.1031(9.9683) | Bit/dim 1.6397(1.2376) | Xent 0.2170(0.2234) | Xent Color 1.4138(0.3168) | Loss 4.5746(3.5989) | Error 0.0556(0.0714) | Error Color 0.4778(0.0668) |Steps 458(418.84) | Grad Norm 50.5205(19.7900) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 10.4270(10.0730) | Bit/dim 1.4160(1.2987) | Xent 0.3723(0.2614) | Xent Color 0.2175(0.3438) | Loss 3.7333(3.6974) | Error 0.1356(0.0851) | Error Color 0.0600(0.0908) |Steps 446(429.70) | Grad Norm 12.0169(19.4881) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 9.5405(10.0275) | Bit/dim 1.2889(1.3072) | Xent 0.3329(0.2672) | Xent Color 0.1208(0.2996) | Loss 3.4331(3.6356) | Error 0.1122(0.0873) | Error Color 0.0344(0.0815) |Steps 416(428.28) | Grad Norm 5.1911(15.9861) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 10.3264(9.9707) | Bit/dim 1.2231(1.2937) | Xent 0.2282(0.2598) | Xent Color 0.1040(0.2518) | Loss 3.2523(3.5413) | Error 0.0778(0.0847) | Error Color 0.0244(0.0683) |Steps 440(421.97) | Grad Norm 3.3331(12.8271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 60.0290, Epoch Time 742.8292(701.8455), Bit/dim 1.2059(best: 1.1956), Xent 0.1324, Xent Color 0.0389. Loss 1.2487, Error 0.0427(best: 0.0407), Error Color 0.0036(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 9.8682(9.9647) | Bit/dim 1.2020(1.2719) | Xent 0.2483(0.2552) | Xent Color 0.0832(0.2088) | Loss 3.1677(3.9558) | Error 0.0767(0.0821) | Error Color 0.0211(0.0554) |Steps 398(417.29) | Grad Norm 2.4677(10.2172) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 9.4045(9.9242) | Bit/dim 1.1792(1.2504) | Xent 0.2112(0.2442) | Xent Color 0.0765(0.1731) | Loss 3.0438(3.7310) | Error 0.0611(0.0780) | Error Color 0.0144(0.0445) |Steps 410(413.48) | Grad Norm 2.2190(8.1877) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 9.5675(9.8787) | Bit/dim 1.1608(1.2304) | Xent 0.2292(0.2379) | Xent Color 0.0723(0.1455) | Loss 3.1357(3.5685) | Error 0.0711(0.0759) | Error Color 0.0167(0.0369) |Steps 422(412.98) | Grad Norm 2.8745(6.5899) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 9.8947(9.8506) | Bit/dim 1.1609(1.2135) | Xent 0.2013(0.2319) | Xent Color 0.0560(0.1222) | Loss 3.1140(3.4367) | Error 0.0722(0.0739) | Error Color 0.0178(0.0306) |Steps 410(410.57) | Grad Norm 2.6539(5.4892) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 10.0327(9.8678) | Bit/dim 1.1636(1.1987) | Xent 0.2056(0.2274) | Xent Color 0.0575(0.1048) | Loss 3.1203(3.3383) | Error 0.0611(0.0729) | Error Color 0.0144(0.0255) |Steps 416(411.41) | Grad Norm 1.2625(4.7952) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 9.9663(9.8379) | Bit/dim 1.1523(1.1852) | Xent 0.1678(0.2234) | Xent Color 0.0485(0.0897) | Loss 3.0224(3.2552) | Error 0.0600(0.0715) | Error Color 0.0089(0.0215) |Steps 410(410.93) | Grad Norm 1.9498(4.1196) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 9.6329(9.8198) | Bit/dim 1.1185(1.1732) | Xent 0.2176(0.2204) | Xent Color 0.0430(0.0785) | Loss 3.0220(3.1950) | Error 0.0656(0.0703) | Error Color 0.0089(0.0181) |Steps 410(409.74) | Grad Norm 1.0929(3.5550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 60.4854, Epoch Time 728.9082(702.6574), Bit/dim 1.1331(best: 1.1956), Xent 0.1130, Xent Color 0.0130. Loss 1.1646, Error 0.0357(best: 0.0407), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 10.2793(9.8083) | Bit/dim 1.1277(1.1622) | Xent 0.1958(0.2178) | Xent Color 0.0445(0.0682) | Loss 3.0019(3.6000) | Error 0.0667(0.0689) | Error Color 0.0089(0.0151) |Steps 434(411.13) | Grad Norm 2.4800(3.2271) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.4233(9.7646) | Bit/dim 1.1223(1.1526) | Xent 0.1782(0.2175) | Xent Color 0.0314(0.0601) | Loss 2.9008(3.4344) | Error 0.0500(0.0681) | Error Color 0.0022(0.0128) |Steps 410(410.61) | Grad Norm 2.2470(2.8930) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 9.9023(9.7863) | Bit/dim 1.1271(1.1453) | Xent 0.2273(0.2116) | Xent Color 0.0316(0.0533) | Loss 3.0086(3.3112) | Error 0.0744(0.0675) | Error Color 0.0044(0.0107) |Steps 410(409.60) | Grad Norm 2.9204(2.7780) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 9.7464(9.7987) | Bit/dim 1.1173(1.1368) | Xent 0.1660(0.2055) | Xent Color 0.0313(0.0485) | Loss 2.9301(3.2174) | Error 0.0500(0.0654) | Error Color 0.0056(0.0095) |Steps 398(409.41) | Grad Norm 2.8628(2.7890) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 10.0182(9.8026) | Bit/dim 1.0839(1.1288) | Xent 0.1851(0.2011) | Xent Color 0.0340(0.0441) | Loss 2.8659(3.1372) | Error 0.0578(0.0644) | Error Color 0.0033(0.0084) |Steps 422(409.19) | Grad Norm 2.8233(2.8109) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 9.7388(9.8093) | Bit/dim 1.0875(1.1210) | Xent 0.1720(0.1973) | Xent Color 0.0255(0.0402) | Loss 2.9137(3.0799) | Error 0.0667(0.0635) | Error Color 0.0022(0.0073) |Steps 416(408.28) | Grad Norm 2.6422(2.7716) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 9.5583(9.7675) | Bit/dim 1.0812(1.1140) | Xent 0.2186(0.2007) | Xent Color 0.0309(0.0368) | Loss 2.9005(3.0340) | Error 0.0722(0.0635) | Error Color 0.0078(0.0066) |Steps 404(405.68) | Grad Norm 3.7161(2.7127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 60.6074, Epoch Time 729.1772(703.4530), Bit/dim 1.0930(best: 1.1331), Xent 0.1063, Xent Color 0.0090. Loss 1.1218, Error 0.0335(best: 0.0357), Error Color 0.0003(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 9.6795(9.7978) | Bit/dim 1.0947(1.1074) | Xent 0.1727(0.2002) | Xent Color 0.0257(0.0349) | Loss 2.9210(3.4026) | Error 0.0589(0.0628) | Error Color 0.0044(0.0063) |Steps 416(407.55) | Grad Norm 2.6353(2.9083) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 9.6339(9.7989) | Bit/dim 1.0868(1.1017) | Xent 0.1790(0.1950) | Xent Color 0.0202(0.0323) | Loss 2.8965(3.2717) | Error 0.0578(0.0621) | Error Color 0.0000(0.0054) |Steps 416(408.76) | Grad Norm 3.9055(2.8635) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 10.0297(9.8165) | Bit/dim 1.0697(1.0950) | Xent 0.2053(0.1928) | Xent Color 0.0313(0.0309) | Loss 2.7857(3.1631) | Error 0.0622(0.0612) | Error Color 0.0078(0.0051) |Steps 386(407.59) | Grad Norm 3.0988(3.0293) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 9.5177(9.7592) | Bit/dim 1.0690(1.0893) | Xent 0.1861(0.1938) | Xent Color 0.0214(0.0293) | Loss 2.8589(3.0828) | Error 0.0556(0.0617) | Error Color 0.0011(0.0049) |Steps 422(406.22) | Grad Norm 3.0155(3.1725) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 9.9661(9.7322) | Bit/dim 1.0572(1.0783) | Xent 0.1862(0.1886) | Xent Color 0.0213(0.0269) | Loss 2.8169(2.9692) | Error 0.0567(0.0591) | Error Color 0.0022(0.0043) |Steps 380(405.72) | Grad Norm 5.8561(3.2993) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 60.1992, Epoch Time 726.4701(704.1435), Bit/dim 1.0592(best: 1.0930), Xent 0.0968, Xent Color 0.0058. Loss 1.0848, Error 0.0308(best: 0.0335), Error Color 0.0000(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.0564(9.7481) | Bit/dim 1.0625(1.0732) | Xent 0.1554(0.1836) | Xent Color 0.0164(0.0253) | Loss 2.8450(3.4140) | Error 0.0589(0.0580) | Error Color 0.0011(0.0038) |Steps 404(404.81) | Grad Norm 2.1961(3.1813) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.8209(9.7341) | Bit/dim 1.0489(1.0680) | Xent 0.1874(0.1818) | Xent Color 0.0215(0.0244) | Loss 2.8297(3.2583) | Error 0.0622(0.0571) | Error Color 0.0044(0.0038) |Steps 398(404.05) | Grad Norm 2.9845(3.1496) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 9.2673(9.7179) | Bit/dim 1.0413(1.0624) | Xent 0.1773(0.1786) | Xent Color 0.0255(0.0233) | Loss 2.7401(3.1385) | Error 0.0533(0.0559) | Error Color 0.0056(0.0035) |Steps 386(403.49) | Grad Norm 3.4807(3.3695) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.3950(9.7485) | Bit/dim 1.0309(1.0566) | Xent 0.1881(0.1777) | Xent Color 0.0233(0.0221) | Loss 2.7070(3.0461) | Error 0.0611(0.0561) | Error Color 0.0044(0.0032) |Steps 398(405.09) | Grad Norm 2.9913(3.8142) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 9.7153(9.7273) | Bit/dim 1.0442(1.0531) | Xent 0.1651(0.1770) | Xent Color 0.0152(0.0219) | Loss 2.7944(2.9862) | Error 0.0522(0.0559) | Error Color 0.0011(0.0034) |Steps 392(404.74) | Grad Norm 6.5484(4.7858) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 9.6610(9.6880) | Bit/dim 1.0487(1.0504) | Xent 0.1959(0.1763) | Xent Color 0.0195(0.0213) | Loss 2.8091(2.9350) | Error 0.0589(0.0562) | Error Color 0.0033(0.0031) |Steps 416(405.04) | Grad Norm 6.3784(5.3741) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 10.3152(9.7253) | Bit/dim 1.0274(1.0458) | Xent 0.1998(0.1792) | Xent Color 0.0186(0.0202) | Loss 2.7722(2.8933) | Error 0.0578(0.0569) | Error Color 0.0011(0.0030) |Steps 392(406.36) | Grad Norm 4.9069(5.2198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 60.6625, Epoch Time 725.1768(704.7745), Bit/dim 1.0303(best: 1.0592), Xent 0.0928, Xent Color 0.0049. Loss 1.0548, Error 0.0299(best: 0.0308), Error Color 0.0002(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 9.9873(9.7308) | Bit/dim 1.0206(1.0421) | Xent 0.1665(0.1752) | Xent Color 0.0140(0.0200) | Loss 2.7541(3.2847) | Error 0.0589(0.0563) | Error Color 0.0011(0.0029) |Steps 416(406.85) | Grad Norm 6.4248(5.5196) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 9.5199(9.7052) | Bit/dim 1.0145(1.0369) | Xent 0.1642(0.1737) | Xent Color 0.0178(0.0194) | Loss 2.7050(3.1470) | Error 0.0522(0.0559) | Error Color 0.0011(0.0028) |Steps 380(404.21) | Grad Norm 4.8231(4.9968) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 10.0997(9.7083) | Bit/dim 1.0157(1.0325) | Xent 0.1326(0.1733) | Xent Color 0.0157(0.0187) | Loss 2.7062(3.0439) | Error 0.0433(0.0546) | Error Color 0.0022(0.0026) |Steps 374(405.33) | Grad Norm 3.8283(4.8060) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 9.6549(9.7271) | Bit/dim 1.0376(1.0318) | Xent 0.1481(0.1739) | Xent Color 0.0184(0.0192) | Loss 2.8040(2.9753) | Error 0.0422(0.0550) | Error Color 0.0011(0.0028) |Steps 398(407.27) | Grad Norm 12.9841(6.7282) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 9.6877(9.7161) | Bit/dim 1.0054(1.0278) | Xent 0.1855(0.1704) | Xent Color 0.0204(0.0192) | Loss 2.7671(2.9173) | Error 0.0567(0.0536) | Error Color 0.0044(0.0030) |Steps 410(406.50) | Grad Norm 4.8645(7.1982) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 9.7106(9.7058) | Bit/dim 1.0020(1.0230) | Xent 0.1841(0.1678) | Xent Color 0.0256(0.0183) | Loss 2.7694(2.8706) | Error 0.0533(0.0529) | Error Color 0.0033(0.0026) |Steps 416(405.13) | Grad Norm 8.9561(7.1759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 60.2980, Epoch Time 724.6101(705.3696), Bit/dim 1.0053(best: 1.0303), Xent 0.0863, Xent Color 0.0035. Loss 1.0278, Error 0.0281(best: 0.0299), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.7250(9.7202) | Bit/dim 1.0040(1.0185) | Xent 0.1429(0.1649) | Xent Color 0.0151(0.0180) | Loss 2.7801(3.3523) | Error 0.0456(0.0519) | Error Color 0.0022(0.0025) |Steps 398(404.07) | Grad Norm 4.5289(7.1084) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 9.5172(9.6911) | Bit/dim 1.0000(1.0132) | Xent 0.1536(0.1629) | Xent Color 0.0139(0.0175) | Loss 2.6734(3.1857) | Error 0.0400(0.0511) | Error Color 0.0011(0.0026) |Steps 392(404.94) | Grad Norm 6.8191(6.7340) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 9.6764(9.6812) | Bit/dim 1.0054(1.0103) | Xent 0.1938(0.1634) | Xent Color 0.0192(0.0169) | Loss 2.7727(3.0677) | Error 0.0622(0.0517) | Error Color 0.0033(0.0026) |Steps 398(403.92) | Grad Norm 9.1928(6.7422) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 10.1074(9.7342) | Bit/dim 1.0228(1.0143) | Xent 0.1505(0.1632) | Xent Color 0.0202(0.0201) | Loss 2.7944(2.9904) | Error 0.0478(0.0513) | Error Color 0.0044(0.0036) |Steps 398(404.52) | Grad Norm 18.3633(10.4007) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 9.6456(9.7436) | Bit/dim 1.0079(1.0143) | Xent 0.1373(0.1649) | Xent Color 0.0221(0.0233) | Loss 2.7178(2.9311) | Error 0.0467(0.0525) | Error Color 0.0022(0.0045) |Steps 404(405.16) | Grad Norm 10.8944(12.4729) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 9.5161(9.7174) | Bit/dim 0.9998(1.0128) | Xent 0.1785(0.1618) | Xent Color 0.0148(0.0232) | Loss 2.6706(2.8769) | Error 0.0478(0.0515) | Error Color 0.0011(0.0044) |Steps 380(405.29) | Grad Norm 10.9710(12.9850) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 9.7008(9.7144) | Bit/dim 0.9951(1.0077) | Xent 0.1350(0.1592) | Xent Color 0.0212(0.0215) | Loss 2.6034(2.8340) | Error 0.0433(0.0509) | Error Color 0.0056(0.0040) |Steps 398(405.11) | Grad Norm 9.9835(12.1232) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 60.4510, Epoch Time 724.8073(705.9527), Bit/dim 0.9901(best: 1.0053), Xent 0.0911, Xent Color 0.0030. Loss 1.0136, Error 0.0292(best: 0.0281), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.2554(9.7622) | Bit/dim 0.9736(1.0027) | Xent 0.1580(0.1577) | Xent Color 0.0129(0.0195) | Loss 2.7160(3.2552) | Error 0.0511(0.0506) | Error Color 0.0022(0.0033) |Steps 404(406.31) | Grad Norm 8.0511(11.3610) | Total Time 0.00(0.00)\n",
      "Iter 2920 | Time 9.4778(9.7801) | Bit/dim 0.9660(0.9968) | Xent 0.1303(0.1563) | Xent Color 0.0159(0.0182) | Loss 2.6160(3.1066) | Error 0.0367(0.0501) | Error Color 0.0011(0.0028) |Steps 398(407.80) | Grad Norm 2.8750(10.1196) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 10.1060(9.8163) | Bit/dim 0.9664(0.9913) | Xent 0.1678(0.1539) | Xent Color 0.0143(0.0171) | Loss 2.7054(2.9941) | Error 0.0611(0.0494) | Error Color 0.0011(0.0025) |Steps 410(407.47) | Grad Norm 9.8965(9.3279) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 9.7606(9.7970) | Bit/dim 0.9747(0.9878) | Xent 0.1277(0.1526) | Xent Color 0.0153(0.0162) | Loss 2.6950(2.9130) | Error 0.0444(0.0484) | Error Color 0.0033(0.0023) |Steps 416(406.56) | Grad Norm 10.8121(9.1298) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 9.9759(9.8013) | Bit/dim 0.9700(0.9842) | Xent 0.1598(0.1530) | Xent Color 0.0139(0.0159) | Loss 2.6860(2.8524) | Error 0.0511(0.0485) | Error Color 0.0011(0.0022) |Steps 428(408.15) | Grad Norm 9.9563(9.3569) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 9.1880(9.7648) | Bit/dim 0.9666(0.9801) | Xent 0.1464(0.1535) | Xent Color 0.0164(0.0153) | Loss 2.5767(2.8003) | Error 0.0456(0.0483) | Error Color 0.0033(0.0020) |Steps 386(408.35) | Grad Norm 9.8846(9.4353) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 10.2707(9.7831) | Bit/dim 0.9497(0.9747) | Xent 0.1191(0.1499) | Xent Color 0.0133(0.0144) | Loss 2.6388(2.7645) | Error 0.0400(0.0473) | Error Color 0.0022(0.0018) |Steps 404(408.96) | Grad Norm 4.2206(8.5635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 61.1546, Epoch Time 732.6315(706.7531), Bit/dim 0.9649(best: 0.9901), Xent 0.0757, Xent Color 0.0023. Loss 0.9844, Error 0.0261(best: 0.0281), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 9.6025(9.7711) | Bit/dim 0.9588(0.9708) | Xent 0.1770(0.1503) | Xent Color 0.0135(0.0140) | Loss 2.7139(3.1509) | Error 0.0511(0.0476) | Error Color 0.0022(0.0017) |Steps 404(408.72) | Grad Norm 16.7155(8.7138) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 9.9026(9.7897) | Bit/dim 0.9584(0.9689) | Xent 0.1524(0.1487) | Xent Color 0.0146(0.0140) | Loss 2.6557(3.0169) | Error 0.0511(0.0472) | Error Color 0.0011(0.0017) |Steps 410(406.99) | Grad Norm 6.5567(9.1168) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 9.8206(9.8304) | Bit/dim 0.9755(0.9670) | Xent 0.0937(0.1468) | Xent Color 0.0205(0.0140) | Loss 2.6929(2.9268) | Error 0.0278(0.0463) | Error Color 0.0056(0.0018) |Steps 398(408.30) | Grad Norm 18.8473(10.2603) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 9.8388(9.8219) | Bit/dim 0.9464(0.9643) | Xent 0.1498(0.1452) | Xent Color 0.0136(0.0147) | Loss 2.6525(2.8571) | Error 0.0478(0.0467) | Error Color 0.0011(0.0021) |Steps 398(408.65) | Grad Norm 7.6565(11.1921) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 9.5190(9.8098) | Bit/dim 0.9418(0.9619) | Xent 0.1694(0.1459) | Xent Color 0.0131(0.0138) | Loss 2.6301(2.7968) | Error 0.0567(0.0465) | Error Color 0.0011(0.0020) |Steps 410(407.14) | Grad Norm 11.8093(10.9818) | Total Time 0.00(0.00)\n",
      "Iter 3030 | Time 9.4358(9.8175) | Bit/dim 0.9563(0.9581) | Xent 0.1270(0.1406) | Xent Color 0.0109(0.0130) | Loss 2.6539(2.7493) | Error 0.0389(0.0449) | Error Color 0.0011(0.0017) |Steps 392(406.86) | Grad Norm 9.0740(10.3954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 61.5796, Epoch Time 731.7653(707.5034), Bit/dim 0.9570(best: 0.9649), Xent 0.0710, Xent Color 0.0030. Loss 0.9755, Error 0.0229(best: 0.0261), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 9.7319(9.7930) | Bit/dim 0.9594(0.9558) | Xent 0.1087(0.1392) | Xent Color 0.0076(0.0127) | Loss 2.6473(3.2215) | Error 0.0389(0.0443) | Error Color 0.0000(0.0015) |Steps 422(407.37) | Grad Norm 14.6443(11.5121) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 9.2859(9.7810) | Bit/dim 2.3261(1.0485) | Xent 0.8017(0.2083) | Xent Color 0.9126(0.2705) | Loss 5.8586(3.3867) | Error 0.1667(0.0622) | Error Color 0.2789(0.0543) |Steps 422(409.98) | Grad Norm 59.4948(28.7409) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 9.1227(9.6420) | Bit/dim 1.5825(1.2234) | Xent 0.3943(0.2563) | Xent Color 0.1757(0.3097) | Loss 3.8887(3.6306) | Error 0.1378(0.0782) | Error Color 0.0600(0.0794) |Steps 392(408.02) | Grad Norm 4.5756(25.0188) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 9.3354(9.5387) | Bit/dim 1.3585(1.2829) | Xent 0.2444(0.2691) | Xent Color 0.1368(0.2763) | Loss 3.4241(3.6415) | Error 0.0689(0.0839) | Error Color 0.0400(0.0760) |Steps 410(406.04) | Grad Norm 3.0328(20.0309) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 10.1346(9.5834) | Bit/dim 1.2011(1.2772) | Xent 0.2768(0.2634) | Xent Color 0.0878(0.2337) | Loss 3.1887(3.5507) | Error 0.0756(0.0823) | Error Color 0.0233(0.0654) |Steps 410(406.24) | Grad Norm 2.8397(15.8413) | Total Time 0.00(0.00)\n",
      "Iter 3090 | Time 9.8426(9.6240) | Bit/dim 1.0978(1.2457) | Xent 0.1873(0.2522) | Xent Color 0.0711(0.1926) | Loss 2.9194(3.4255) | Error 0.0633(0.0798) | Error Color 0.0211(0.0541) |Steps 422(409.16) | Grad Norm 2.4150(12.4982) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 9.5697(9.6825) | Bit/dim 1.0667(1.2058) | Xent 0.1831(0.2385) | Xent Color 0.0559(0.1584) | Loss 2.8681(3.2974) | Error 0.0622(0.0758) | Error Color 0.0133(0.0439) |Steps 404(409.12) | Grad Norm 1.5293(9.8784) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 60.8955, Epoch Time 718.0884(707.8210), Bit/dim 1.0629(best: 0.9570), Xent 0.0966, Xent Color 0.0184. Loss 1.0917, Error 0.0320(best: 0.0229), Error Color 0.0011(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 9.7745(9.7094) | Bit/dim 1.0494(1.1654) | Xent 0.1373(0.2192) | Xent Color 0.0312(0.1287) | Loss 2.8133(3.6198) | Error 0.0389(0.0700) | Error Color 0.0033(0.0352) |Steps 404(408.73) | Grad Norm 1.9557(7.8130) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 9.9473(9.7674) | Bit/dim 1.0050(1.1286) | Xent 0.1465(0.2052) | Xent Color 0.0369(0.1059) | Loss 2.6928(3.4016) | Error 0.0456(0.0653) | Error Color 0.0056(0.0282) |Steps 416(411.41) | Grad Norm 1.8411(6.2627) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 9.8729(9.8099) | Bit/dim 1.0094(1.0979) | Xent 0.1381(0.1959) | Xent Color 0.0334(0.0875) | Loss 2.7174(3.2290) | Error 0.0500(0.0620) | Error Color 0.0056(0.0227) |Steps 410(410.85) | Grad Norm 1.8476(5.1390) | Total Time 0.00(0.00)\n",
      "Iter 3140 | Time 9.8405(9.8287) | Bit/dim 0.9872(1.0715) | Xent 0.1773(0.1911) | Xent Color 0.0338(0.0735) | Loss 2.6876(3.0945) | Error 0.0444(0.0599) | Error Color 0.0100(0.0188) |Steps 404(411.89) | Grad Norm 2.2652(4.3067) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 10.2577(9.8240) | Bit/dim 0.9871(1.0483) | Xent 0.1712(0.1844) | Xent Color 0.0324(0.0626) | Loss 2.7096(2.9917) | Error 0.0511(0.0576) | Error Color 0.0056(0.0155) |Steps 404(409.98) | Grad Norm 1.9723(3.8514) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 10.0904(9.8541) | Bit/dim 0.9821(1.0303) | Xent 0.1828(0.1796) | Xent Color 0.0258(0.0537) | Loss 2.6629(2.9106) | Error 0.0544(0.0564) | Error Color 0.0022(0.0127) |Steps 404(407.93) | Grad Norm 3.6854(3.6974) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 60.9544, Epoch Time 735.7694(708.6594), Bit/dim 0.9700(best: 0.9570), Xent 0.0843, Xent Color 0.0062. Loss 0.9926, Error 0.0294(best: 0.0229), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 10.0996(9.8543) | Bit/dim 0.9727(1.0152) | Xent 0.1604(0.1678) | Xent Color 0.0169(0.0466) | Loss 2.7081(3.3788) | Error 0.0500(0.0530) | Error Color 0.0033(0.0103) |Steps 410(409.54) | Grad Norm 3.0682(3.4890) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 9.6625(9.8431) | Bit/dim 0.9553(1.0019) | Xent 0.1419(0.1653) | Xent Color 0.0247(0.0413) | Loss 2.6290(3.1910) | Error 0.0422(0.0516) | Error Color 0.0022(0.0090) |Steps 404(409.90) | Grad Norm 4.3990(3.5155) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 10.2299(9.8261) | Bit/dim 0.9530(0.9904) | Xent 0.1417(0.1607) | Xent Color 0.0253(0.0363) | Loss 2.6620(3.0443) | Error 0.0344(0.0503) | Error Color 0.0056(0.0074) |Steps 404(409.47) | Grad Norm 3.1601(3.5922) | Total Time 0.00(0.00)\n",
      "Iter 3200 | Time 10.4621(9.8493) | Bit/dim 0.9514(0.9809) | Xent 0.1274(0.1560) | Xent Color 0.0236(0.0323) | Loss 2.6743(2.9362) | Error 0.0367(0.0490) | Error Color 0.0044(0.0063) |Steps 416(410.74) | Grad Norm 3.1175(3.4713) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 9.6737(9.8714) | Bit/dim 0.9465(0.9723) | Xent 0.1272(0.1515) | Xent Color 0.0200(0.0296) | Loss 2.5574(2.8521) | Error 0.0333(0.0479) | Error Color 0.0033(0.0057) |Steps 404(412.36) | Grad Norm 3.7537(3.4478) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 10.6446(9.8671) | Bit/dim 0.9402(0.9650) | Xent 0.1504(0.1503) | Xent Color 0.0144(0.0271) | Loss 2.6683(2.7896) | Error 0.0533(0.0481) | Error Color 0.0022(0.0051) |Steps 410(410.89) | Grad Norm 2.8321(3.6936) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 10.4725(9.9299) | Bit/dim 0.9347(0.9578) | Xent 0.1355(0.1495) | Xent Color 0.0175(0.0249) | Loss 2.6065(2.7421) | Error 0.0389(0.0477) | Error Color 0.0022(0.0045) |Steps 440(413.35) | Grad Norm 5.1581(3.7511) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 58.8167, Epoch Time 735.3206(709.4593), Bit/dim 0.9360(best: 0.9570), Xent 0.0742, Xent Color 0.0047. Loss 0.9558, Error 0.0241(best: 0.0229), Error Color 0.0002(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 9.5387(9.9560) | Bit/dim 0.9256(0.9518) | Xent 0.1341(0.1457) | Xent Color 0.0157(0.0233) | Loss 2.5886(3.1754) | Error 0.0433(0.0463) | Error Color 0.0011(0.0040) |Steps 410(415.40) | Grad Norm 3.0214(3.6206) | Total Time 0.00(0.00)\n",
      "Iter 3250 | Time 10.0213(9.9947) | Bit/dim 0.9330(0.9473) | Xent 0.1658(0.1451) | Xent Color 0.0157(0.0218) | Loss 2.6020(3.0250) | Error 0.0489(0.0456) | Error Color 0.0011(0.0035) |Steps 392(411.91) | Grad Norm 4.8711(3.7794) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 9.5230(9.9909) | Bit/dim 0.9257(0.9422) | Xent 0.1225(0.1434) | Xent Color 0.0155(0.0204) | Loss 2.5517(2.9081) | Error 0.0356(0.0451) | Error Color 0.0022(0.0032) |Steps 410(411.30) | Grad Norm 6.6686(4.1980) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 10.0473(9.9606) | Bit/dim 0.9254(0.9371) | Xent 0.1297(0.1413) | Xent Color 0.0096(0.0194) | Loss 2.6486(2.8180) | Error 0.0411(0.0445) | Error Color 0.0011(0.0030) |Steps 416(411.44) | Grad Norm 4.2083(4.5676) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 9.9281(9.9692) | Bit/dim 0.9282(0.9342) | Xent 0.1317(0.1415) | Xent Color 0.0160(0.0188) | Loss 2.5703(2.7540) | Error 0.0422(0.0444) | Error Color 0.0022(0.0030) |Steps 398(413.16) | Grad Norm 6.0357(5.1207) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 9.9810(9.9432) | Bit/dim 0.9219(0.9293) | Xent 0.0927(0.1385) | Xent Color 0.0190(0.0180) | Loss 2.5648(2.6998) | Error 0.0278(0.0439) | Error Color 0.0022(0.0027) |Steps 404(412.06) | Grad Norm 7.5980(5.0307) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 9.9008(9.9394) | Bit/dim 0.9118(0.9266) | Xent 0.1006(0.1326) | Xent Color 0.0130(0.0171) | Loss 2.5530(2.6603) | Error 0.0289(0.0421) | Error Color 0.0022(0.0026) |Steps 410(410.61) | Grad Norm 7.3168(5.5481) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 60.8969, Epoch Time 741.8707(710.4316), Bit/dim 0.9127(best: 0.9360), Xent 0.0701, Xent Color 0.0035. Loss 0.9311, Error 0.0232(best: 0.0229), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 10.2198(9.9417) | Bit/dim 0.9069(0.9230) | Xent 0.1669(0.1317) | Xent Color 0.0186(0.0171) | Loss 2.5654(3.0339) | Error 0.0456(0.0412) | Error Color 0.0033(0.0027) |Steps 428(410.77) | Grad Norm 6.3800(6.4539) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 9.9720(9.9422) | Bit/dim 0.9109(0.9196) | Xent 0.1366(0.1293) | Xent Color 0.0114(0.0162) | Loss 2.5913(2.9088) | Error 0.0444(0.0402) | Error Color 0.0000(0.0024) |Steps 428(410.50) | Grad Norm 8.4636(6.2867) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 9.9120(9.9398) | Bit/dim 0.9105(0.9176) | Xent 0.1317(0.1305) | Xent Color 0.0147(0.0161) | Loss 2.5676(2.8192) | Error 0.0478(0.0408) | Error Color 0.0022(0.0025) |Steps 422(409.72) | Grad Norm 11.6157(6.8676) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 9.9236(9.9510) | Bit/dim 0.9183(0.9151) | Xent 0.1114(0.1286) | Xent Color 0.0149(0.0155) | Loss 2.5563(2.7503) | Error 0.0389(0.0405) | Error Color 0.0044(0.0023) |Steps 398(408.38) | Grad Norm 8.1468(6.9534) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 10.1076(9.9734) | Bit/dim 0.8844(0.9108) | Xent 0.1497(0.1261) | Xent Color 0.0119(0.0153) | Loss 2.5387(2.6923) | Error 0.0567(0.0401) | Error Color 0.0000(0.0022) |Steps 434(407.51) | Grad Norm 7.3399(7.4664) | Total Time 0.00(0.00)\n",
      "Iter 3360 | Time 10.4331(10.0153) | Bit/dim 0.9586(0.9128) | Xent 0.1364(0.1276) | Xent Color 0.0439(0.0169) | Loss 2.6958(2.6630) | Error 0.0400(0.0401) | Error Color 0.0156(0.0030) |Steps 428(409.06) | Grad Norm 36.7443(9.7551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 61.4189, Epoch Time 744.0575(711.4404), Bit/dim 0.9684(best: 0.9127), Xent 0.0771, Xent Color 0.0317. Loss 0.9956, Error 0.0250(best: 0.0229), Error Color 0.0069(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 9.4551(9.9806) | Bit/dim 1.0053(0.9232) | Xent 0.1437(0.1311) | Xent Color 0.4307(0.0366) | Loss 2.8692(3.1416) | Error 0.0422(0.0411) | Error Color 0.1367(0.0101) |Steps 386(408.13) | Grad Norm 69.4633(15.7669) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 9.4396(9.8793) | Bit/dim 1.8173(1.1701) | Xent 0.6985(0.2631) | Xent Color 0.7425(0.8855) | Loss 4.7561(3.8914) | Error 0.2322(0.0824) | Error Color 0.2911(0.1296) |Steps 410(409.16) | Grad Norm 17.2386(32.4518) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 9.2079(9.7742) | Bit/dim 1.6016(1.3053) | Xent 0.3794(0.3088) | Xent Color 0.4743(0.8038) | Loss 4.1543(4.0094) | Error 0.1200(0.0976) | Error Color 0.1922(0.1566) |Steps 398(408.44) | Grad Norm 9.6952(27.4969) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 9.4169(9.6349) | Bit/dim 1.4174(1.3553) | Xent 0.2264(0.3052) | Xent Color 0.2339(0.6690) | Loss 3.6591(3.9625) | Error 0.0800(0.0972) | Error Color 0.0822(0.1432) |Steps 398(404.71) | Grad Norm 4.0428(21.8649) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 9.4422(9.5480) | Bit/dim 1.2698(1.3455) | Xent 0.1687(0.2900) | Xent Color 0.1710(0.5479) | Loss 3.2775(3.8302) | Error 0.0567(0.0920) | Error Color 0.0589(0.1242) |Steps 392(404.01) | Grad Norm 2.7406(17.2690) | Total Time 0.00(0.00)\n",
      "Iter 3420 | Time 9.2848(9.5454) | Bit/dim 1.1913(1.3127) | Xent 0.2252(0.2715) | Xent Color 0.1043(0.4371) | Loss 3.2015(3.6775) | Error 0.0633(0.0856) | Error Color 0.0178(0.1012) |Steps 416(404.49) | Grad Norm 3.2826(13.5406) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 9.4981(9.5628) | Bit/dim 1.1308(1.2690) | Xent 0.2129(0.2536) | Xent Color 0.0737(0.3463) | Loss 3.1092(3.5218) | Error 0.0600(0.0793) | Error Color 0.0222(0.0809) |Steps 428(406.20) | Grad Norm 2.0042(10.5127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 62.6557, Epoch Time 712.4660(711.4711), Bit/dim 1.1056(best: 0.9127), Xent 0.1081, Xent Color 0.0298. Loss 1.1401, Error 0.0344(best: 0.0229), Error Color 0.0013(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 9.7214(9.6154) | Bit/dim 1.0558(1.2215) | Xent 0.1615(0.2367) | Xent Color 0.0761(0.2762) | Loss 2.9378(3.8377) | Error 0.0489(0.0740) | Error Color 0.0167(0.0643) |Steps 404(406.99) | Grad Norm 3.9090(8.5430) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 10.1902(9.6908) | Bit/dim 1.0405(1.1778) | Xent 0.1839(0.2221) | Xent Color 0.0599(0.2201) | Loss 2.8403(3.5913) | Error 0.0556(0.0700) | Error Color 0.0144(0.0508) |Steps 428(409.07) | Grad Norm 2.0780(7.1249) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 9.8962(9.6786) | Bit/dim 1.0250(1.1374) | Xent 0.1404(0.2081) | Xent Color 0.0637(0.1786) | Loss 2.8705(3.3964) | Error 0.0511(0.0663) | Error Color 0.0178(0.0415) |Steps 410(408.33) | Grad Norm 5.7447(6.3144) | Total Time 0.00(0.00)\n",
      "Iter 3470 | Time 9.6025(9.7316) | Bit/dim 1.0065(1.1032) | Xent 0.1528(0.1973) | Xent Color 0.0441(0.1449) | Loss 2.7594(3.2367) | Error 0.0600(0.0632) | Error Color 0.0111(0.0333) |Steps 410(407.85) | Grad Norm 3.7555(5.5310) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 9.6182(9.7607) | Bit/dim 0.9914(1.0727) | Xent 0.1444(0.1891) | Xent Color 0.0464(0.1191) | Loss 2.7401(3.1089) | Error 0.0444(0.0600) | Error Color 0.0111(0.0274) |Steps 416(407.95) | Grad Norm 2.0145(4.8025) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 10.1966(9.8119) | Bit/dim 0.9705(1.0473) | Xent 0.1773(0.1800) | Xent Color 0.0336(0.0985) | Loss 2.7564(3.0131) | Error 0.0511(0.0569) | Error Color 0.0033(0.0224) |Steps 428(410.08) | Grad Norm 3.3196(4.5650) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 62.2507, Epoch Time 733.7039(712.1381), Bit/dim 0.9603(best: 0.9127), Xent 0.0846, Xent Color 0.0106. Loss 0.9841, Error 0.0285(best: 0.0229), Error Color 0.0006(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 9.6964(9.8038) | Bit/dim 0.9527(1.0255) | Xent 0.1359(0.1731) | Xent Color 0.0374(0.0823) | Loss 2.7195(3.4905) | Error 0.0511(0.0544) | Error Color 0.0056(0.0183) |Steps 416(412.08) | Grad Norm 2.2052(4.3033) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 10.0911(9.8199) | Bit/dim 0.9555(1.0072) | Xent 0.1617(0.1673) | Xent Color 0.0368(0.0705) | Loss 2.7365(3.2854) | Error 0.0456(0.0524) | Error Color 0.0067(0.0158) |Steps 422(412.58) | Grad Norm 4.8051(4.2706) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 10.0018(9.8586) | Bit/dim 0.9384(0.9920) | Xent 0.1540(0.1608) | Xent Color 0.0295(0.0606) | Loss 2.6503(3.1244) | Error 0.0478(0.0503) | Error Color 0.0022(0.0134) |Steps 422(412.69) | Grad Norm 5.4979(4.4138) | Total Time 0.00(0.00)\n",
      "Iter 3530 | Time 9.6936(9.8834) | Bit/dim 0.9496(0.9783) | Xent 0.1100(0.1530) | Xent Color 0.0248(0.0528) | Loss 2.6429(2.9970) | Error 0.0267(0.0481) | Error Color 0.0022(0.0112) |Steps 422(412.77) | Grad Norm 5.9139(4.5625) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 9.9056(9.9225) | Bit/dim 0.9339(0.9664) | Xent 0.1372(0.1493) | Xent Color 0.0285(0.0459) | Loss 2.6468(2.9036) | Error 0.0422(0.0472) | Error Color 0.0078(0.0096) |Steps 416(412.99) | Grad Norm 5.0781(4.3774) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 10.0693(9.9198) | Bit/dim 0.9305(0.9576) | Xent 0.1375(0.1451) | Xent Color 0.0301(0.0408) | Loss 2.6899(2.8391) | Error 0.0444(0.0459) | Error Color 0.0067(0.0082) |Steps 404(413.64) | Grad Norm 5.8139(4.5505) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 9.9177(9.9190) | Bit/dim 0.9265(0.9495) | Xent 0.1351(0.1462) | Xent Color 0.0226(0.0362) | Loss 2.6994(2.7802) | Error 0.0433(0.0460) | Error Color 0.0022(0.0068) |Steps 422(412.96) | Grad Norm 4.2200(4.6727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 62.3004, Epoch Time 742.4748(713.0482), Bit/dim 0.9193(best: 0.9127), Xent 0.0750, Xent Color 0.0061. Loss 0.9396, Error 0.0249(best: 0.0229), Error Color 0.0004(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 10.3971(9.9625) | Bit/dim 0.9176(0.9416) | Xent 0.1446(0.1429) | Xent Color 0.0227(0.0328) | Loss 2.5691(3.2329) | Error 0.0411(0.0446) | Error Color 0.0033(0.0060) |Steps 434(416.05) | Grad Norm 7.7082(5.0322) | Total Time 0.00(0.00)\n",
      "Iter 3580 | Time 9.9997(9.9932) | Bit/dim 0.9091(0.9338) | Xent 0.1810(0.1410) | Xent Color 0.0191(0.0301) | Loss 2.6748(3.0699) | Error 0.0500(0.0438) | Error Color 0.0022(0.0053) |Steps 428(417.54) | Grad Norm 6.9476(5.3243) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 9.9310(9.9796) | Bit/dim 0.9109(0.9272) | Xent 0.1073(0.1396) | Xent Color 0.0199(0.0274) | Loss 2.6168(2.9462) | Error 0.0344(0.0433) | Error Color 0.0011(0.0046) |Steps 416(417.65) | Grad Norm 3.0523(5.3305) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 9.9928(10.0150) | Bit/dim 0.9038(0.9208) | Xent 0.1354(0.1384) | Xent Color 0.0227(0.0260) | Loss 2.5785(2.8510) | Error 0.0400(0.0433) | Error Color 0.0033(0.0043) |Steps 422(419.02) | Grad Norm 5.9275(5.1829) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 10.0398(10.0548) | Bit/dim 0.8998(0.9174) | Xent 0.1455(0.1363) | Xent Color 0.0184(0.0247) | Loss 2.5341(2.7787) | Error 0.0500(0.0430) | Error Color 0.0033(0.0042) |Steps 422(419.48) | Grad Norm 5.3282(6.5240) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 9.8610(10.0039) | Bit/dim 0.9025(0.9132) | Xent 0.0990(0.1324) | Xent Color 0.0149(0.0232) | Loss 2.5858(2.7301) | Error 0.0311(0.0418) | Error Color 0.0022(0.0040) |Steps 428(420.15) | Grad Norm 6.2555(6.7249) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 9.8260(10.0012) | Bit/dim 0.8867(0.9092) | Xent 0.1367(0.1315) | Xent Color 0.0164(0.0217) | Loss 2.5718(2.6883) | Error 0.0467(0.0419) | Error Color 0.0022(0.0035) |Steps 410(419.87) | Grad Norm 6.8828(6.3934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 62.5146, Epoch Time 747.6307(714.0857), Bit/dim 0.8939(best: 0.9127), Xent 0.0655, Xent Color 0.0034. Loss 0.9111, Error 0.0219(best: 0.0229), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 9.9016(9.9733) | Bit/dim 0.8815(0.9035) | Xent 0.1323(0.1323) | Xent Color 0.0169(0.0206) | Loss 2.5570(3.0708) | Error 0.0411(0.0419) | Error Color 0.0000(0.0031) |Steps 398(418.78) | Grad Norm 4.8940(6.0187) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 9.6618(9.9945) | Bit/dim 1.3786(0.9242) | Xent 0.3009(0.1376) | Xent Color 9.7137(0.3216) | Loss 7.9027(3.1197) | Error 0.0900(0.0432) | Error Color 0.7678(0.0306) |Steps 422(419.37) | Grad Norm 265.6267(18.5203) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 11.9074(10.3614) | Bit/dim 2.0105(1.2599) | Xent 0.6261(0.4482) | Xent Color 1.1425(1.1435) | Loss 5.7254(4.2020) | Error 0.2011(0.1184) | Error Color 0.4044(0.1352) |Steps 566(444.46) | Grad Norm 32.6074(27.0188) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 12.3195(11.0626) | Bit/dim 1.8827(1.4450) | Xent 0.2952(0.4460) | Xent Color 0.4015(0.9751) | Loss 5.0039(4.4872) | Error 0.1022(0.1254) | Error Color 0.1389(0.1503) |Steps 578(483.98) | Grad Norm 4.2610(50.9884) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 10.4448(11.1449) | Bit/dim 1.7276(1.5359) | Xent 0.2601(0.3970) | Xent Color 0.1599(0.7901) | Loss 4.4299(4.5328) | Error 0.0833(0.1145) | Error Color 0.0444(0.1341) |Steps 500(493.10) | Grad Norm 3.4863(38.7810) | Total Time 0.00(0.00)\n",
      "Iter 3690 | Time 10.8452(11.0674) | Bit/dim 1.5954(1.5632) | Xent 0.2504(0.3606) | Xent Color 0.1635(0.6269) | Loss 4.0854(4.4448) | Error 0.0778(0.1058) | Error Color 0.0489(0.1124) |Steps 482(491.19) | Grad Norm 5.3958(29.6561) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 68.3871, Epoch Time 823.3752(717.3644), Bit/dim 1.5268(best: 0.8939), Xent 0.1210, Xent Color 0.0544. Loss 1.5706, Error 0.0391(best: 0.0219), Error Color 0.0085(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 10.4357(10.9899) | Bit/dim 1.4854(1.5553) | Xent 0.2261(0.3271) | Xent Color 0.1389(0.4967) | Loss 3.7424(4.8978) | Error 0.0733(0.0978) | Error Color 0.0344(0.0924) |Steps 464(486.67) | Grad Norm 3.2939(23.0015) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 10.9919(10.8966) | Bit/dim 1.4079(1.5242) | Xent 0.2168(0.3010) | Xent Color 0.1031(0.3935) | Loss 3.6806(4.5938) | Error 0.0733(0.0913) | Error Color 0.0244(0.0757) |Steps 470(481.47) | Grad Norm 4.4593(18.1353) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 10.2494(10.7496) | Bit/dim 1.3060(1.4781) | Xent 0.1832(0.2801) | Xent Color 0.0830(0.3146) | Loss 3.5283(4.3211) | Error 0.0567(0.0851) | Error Color 0.0211(0.0625) |Steps 434(471.36) | Grad Norm 3.7297(14.5145) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 10.8819(10.7174) | Bit/dim 1.2376(1.4248) | Xent 0.1822(0.2575) | Xent Color 0.0772(0.2537) | Loss 3.3472(4.0824) | Error 0.0578(0.0791) | Error Color 0.0133(0.0517) |Steps 446(467.14) | Grad Norm 2.6252(11.5643) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 10.6653(10.7089) | Bit/dim 1.1957(1.3708) | Xent 0.2146(0.2416) | Xent Color 0.0606(0.2033) | Loss 3.2548(3.8771) | Error 0.0700(0.0746) | Error Color 0.0167(0.0424) |Steps 446(463.75) | Grad Norm 3.6524(9.3765) | Total Time 0.00(0.00)\n",
      "Iter 3750 | Time 11.1545(10.6580) | Bit/dim 1.1354(1.3148) | Xent 0.2231(0.2260) | Xent Color 0.0601(0.1658) | Loss 3.1409(3.6920) | Error 0.0700(0.0705) | Error Color 0.0156(0.0353) |Steps 476(463.09) | Grad Norm 5.7203(8.5375) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 11.4308(10.6611) | Bit/dim 1.0948(1.2604) | Xent 0.1879(0.2141) | Xent Color 0.0792(0.1366) | Loss 3.0873(3.5299) | Error 0.0589(0.0671) | Error Color 0.0233(0.0298) |Steps 446(460.54) | Grad Norm 15.2837(8.3432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 66.7757, Epoch Time 787.8214(719.4781), Bit/dim 1.0879(best: 0.8939), Xent 0.0969, Xent Color 0.0188. Loss 1.1169, Error 0.0328(best: 0.0219), Error Color 0.0027(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 10.4137(10.6665) | Bit/dim 1.0775(1.2123) | Xent 0.1598(0.2001) | Xent Color 0.0338(0.1119) | Loss 3.0158(3.8805) | Error 0.0578(0.0631) | Error Color 0.0089(0.0244) |Steps 458(460.57) | Grad Norm 2.7303(7.6666) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 11.0964(10.7010) | Bit/dim 1.0327(1.1689) | Xent 0.1518(0.1927) | Xent Color 0.0377(0.0933) | Loss 2.8496(3.6365) | Error 0.0456(0.0603) | Error Color 0.0089(0.0205) |Steps 428(458.22) | Grad Norm 4.0741(7.4325) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 10.9359(10.7230) | Bit/dim 1.0262(1.1327) | Xent 0.1468(0.1883) | Xent Color 0.0449(0.0783) | Loss 2.8606(3.4464) | Error 0.0533(0.0587) | Error Color 0.0100(0.0171) |Steps 446(456.53) | Grad Norm 4.6098(6.5401) | Total Time 0.00(0.00)\n",
      "Iter 3800 | Time 10.4634(10.7432) | Bit/dim 1.0125(1.1014) | Xent 0.1954(0.1824) | Xent Color 0.0302(0.0660) | Loss 2.8858(3.2910) | Error 0.0589(0.0567) | Error Color 0.0067(0.0142) |Steps 458(456.61) | Grad Norm 3.9229(6.0249) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 11.1399(10.8163) | Bit/dim 0.9945(1.0746) | Xent 0.1644(0.1754) | Xent Color 0.0276(0.0562) | Loss 2.8648(3.1751) | Error 0.0444(0.0542) | Error Color 0.0044(0.0118) |Steps 458(458.08) | Grad Norm 3.2793(5.5637) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 10.7364(10.8016) | Bit/dim 0.9788(1.0521) | Xent 0.1327(0.1694) | Xent Color 0.0289(0.0492) | Loss 2.7521(3.0849) | Error 0.0356(0.0520) | Error Color 0.0089(0.0105) |Steps 458(458.48) | Grad Norm 2.9874(5.0840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 66.1820, Epoch Time 802.7613(721.9766), Bit/dim 0.9805(best: 0.8939), Xent 0.0847, Xent Color 0.0081. Loss 1.0037, Error 0.0279(best: 0.0219), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 10.7226(10.8005) | Bit/dim 0.9907(1.0349) | Xent 0.1549(0.1656) | Xent Color 0.0333(0.0434) | Loss 2.9077(3.6022) | Error 0.0578(0.0522) | Error Color 0.0100(0.0090) |Steps 458(458.35) | Grad Norm 8.9188(5.2168) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 11.3691(10.8284) | Bit/dim 0.9615(1.0181) | Xent 0.1053(0.1586) | Xent Color 0.0245(0.0390) | Loss 2.7483(3.3855) | Error 0.0356(0.0492) | Error Color 0.0033(0.0081) |Steps 452(456.98) | Grad Norm 7.7363(5.3412) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 10.9260(10.8244) | Bit/dim 0.9666(1.0042) | Xent 0.1363(0.1563) | Xent Color 0.0265(0.0360) | Loss 2.7339(3.2285) | Error 0.0467(0.0485) | Error Color 0.0033(0.0074) |Steps 464(458.59) | Grad Norm 6.2260(5.9010) | Total Time 0.00(0.00)\n",
      "Iter 3860 | Time 11.3561(10.8015) | Bit/dim 0.9598(0.9933) | Xent 0.1385(0.1562) | Xent Color 0.0196(0.0328) | Loss 2.7738(3.1071) | Error 0.0478(0.0490) | Error Color 0.0011(0.0065) |Steps 464(459.01) | Grad Norm 8.3201(6.5472) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 10.7204(10.8433) | Bit/dim 0.9573(0.9826) | Xent 0.0808(0.1503) | Xent Color 0.0182(0.0307) | Loss 2.6852(3.0081) | Error 0.0289(0.0473) | Error Color 0.0022(0.0060) |Steps 434(456.92) | Grad Norm 5.5011(6.0948) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 10.5896(10.8213) | Bit/dim 0.9417(0.9728) | Xent 0.1241(0.1513) | Xent Color 0.0197(0.0292) | Loss 2.6602(2.9332) | Error 0.0378(0.0475) | Error Color 0.0033(0.0056) |Steps 428(453.53) | Grad Norm 6.7478(6.6992) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 10.0491(10.7821) | Bit/dim 0.9555(0.9688) | Xent 0.1383(0.1497) | Xent Color 0.0410(0.0310) | Loss 2.6869(2.8818) | Error 0.0444(0.0475) | Error Color 0.0133(0.0067) |Steps 464(452.69) | Grad Norm 22.1544(9.7617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 64.9254, Epoch Time 800.5184(724.3328), Bit/dim 0.9743(best: 0.8939), Xent 0.0734, Xent Color 0.0032. Loss 0.9934, Error 0.0229(best: 0.0219), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 10.6846(10.7648) | Bit/dim 0.9702(0.9700) | Xent 0.1446(0.1490) | Xent Color 0.0713(0.0413) | Loss 2.8020(3.3796) | Error 0.0356(0.0477) | Error Color 0.0244(0.0109) |Steps 458(451.10) | Grad Norm 24.7153(13.9867) | Total Time 0.00(0.00)\n",
      "Iter 3910 | Time 10.5958(10.7614) | Bit/dim 0.9595(0.9672) | Xent 0.1331(0.1482) | Xent Color 0.0279(0.0377) | Loss 2.7655(3.2118) | Error 0.0556(0.0475) | Error Color 0.0089(0.0096) |Steps 458(449.94) | Grad Norm 12.2272(13.5245) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 10.9652(10.7494) | Bit/dim 0.9459(0.9609) | Xent 0.1591(0.1439) | Xent Color 0.0262(0.0332) | Loss 2.6916(3.0773) | Error 0.0478(0.0459) | Error Color 0.0056(0.0082) |Steps 464(448.98) | Grad Norm 9.1980(11.9281) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 10.2014(10.7281) | Bit/dim 0.9329(0.9520) | Xent 0.1299(0.1425) | Xent Color 0.0208(0.0289) | Loss 2.7242(2.9723) | Error 0.0422(0.0455) | Error Color 0.0056(0.0067) |Steps 452(449.33) | Grad Norm 4.9556(10.3319) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 10.8148(10.6867) | Bit/dim 0.9247(0.9445) | Xent 0.1313(0.1404) | Xent Color 0.0170(0.0256) | Loss 2.6796(2.8899) | Error 0.0422(0.0445) | Error Color 0.0033(0.0056) |Steps 452(448.91) | Grad Norm 2.9640(8.6713) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 11.0071(10.6608) | Bit/dim 0.9238(0.9372) | Xent 0.1045(0.1376) | Xent Color 0.0173(0.0229) | Loss 2.6048(2.8239) | Error 0.0289(0.0432) | Error Color 0.0033(0.0046) |Steps 422(447.89) | Grad Norm 3.0851(7.2591) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 10.6702(10.6545) | Bit/dim 0.9080(0.9301) | Xent 0.0918(0.1334) | Xent Color 0.0154(0.0209) | Loss 2.6489(2.7742) | Error 0.0289(0.0420) | Error Color 0.0033(0.0040) |Steps 434(447.76) | Grad Norm 3.5621(6.0715) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 65.1992, Epoch Time 792.2742(726.3711), Bit/dim 0.9082(best: 0.8939), Xent 0.0693, Xent Color 0.0029. Loss 0.9263, Error 0.0222(best: 0.0219), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 10.7203(10.6303) | Bit/dim 0.9034(0.9240) | Xent 0.1408(0.1309) | Xent Color 0.0168(0.0196) | Loss 2.6479(3.1663) | Error 0.0467(0.0415) | Error Color 0.0033(0.0036) |Steps 440(445.49) | Grad Norm 3.6103(5.3636) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 10.2752(10.5885) | Bit/dim 0.8908(0.9186) | Xent 0.1461(0.1303) | Xent Color 0.0293(0.0185) | Loss 2.6076(3.0224) | Error 0.0344(0.0406) | Error Color 0.0089(0.0033) |Steps 434(443.83) | Grad Norm 5.0771(4.8593) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 10.5105(10.5444) | Bit/dim 0.9033(0.9135) | Xent 0.1023(0.1296) | Xent Color 0.0186(0.0175) | Loss 2.6129(2.9140) | Error 0.0289(0.0404) | Error Color 0.0022(0.0031) |Steps 416(441.23) | Grad Norm 4.4737(4.6388) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 10.3440(10.5534) | Bit/dim 0.9001(0.9096) | Xent 0.1640(0.1319) | Xent Color 0.0123(0.0164) | Loss 2.6112(2.8366) | Error 0.0533(0.0411) | Error Color 0.0011(0.0026) |Steps 440(442.47) | Grad Norm 8.4819(5.1092) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 10.5310(10.5339) | Bit/dim 0.9041(0.9064) | Xent 0.1296(0.1301) | Xent Color 0.0164(0.0164) | Loss 2.5820(2.7728) | Error 0.0378(0.0409) | Error Color 0.0022(0.0028) |Steps 446(441.75) | Grad Norm 8.1166(6.0296) | Total Time 0.00(0.00)\n",
      "Iter 4020 | Time 10.1578(10.5278) | Bit/dim 0.9031(0.9042) | Xent 0.1172(0.1293) | Xent Color 0.0201(0.0172) | Loss 2.6113(2.7296) | Error 0.0411(0.0404) | Error Color 0.0044(0.0031) |Steps 434(439.01) | Grad Norm 14.3608(7.5362) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 64.6005, Epoch Time 781.0556(728.0116), Bit/dim 0.9011(best: 0.8939), Xent 0.0696, Xent Color 0.0023. Loss 0.9191, Error 0.0212(best: 0.0219), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 10.9280(10.5255) | Bit/dim 0.8942(0.9025) | Xent 0.1255(0.1272) | Xent Color 0.0186(0.0176) | Loss 2.6028(3.2187) | Error 0.0378(0.0403) | Error Color 0.0044(0.0033) |Steps 464(440.71) | Grad Norm 10.6120(8.7282) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 10.6000(10.5264) | Bit/dim 0.8868(0.9002) | Xent 0.1034(0.1245) | Xent Color 0.0112(0.0172) | Loss 2.5900(3.0540) | Error 0.0289(0.0392) | Error Color 0.0000(0.0031) |Steps 458(440.70) | Grad Norm 6.5917(9.1862) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 10.7330(10.5539) | Bit/dim 0.8949(0.8985) | Xent 0.1098(0.1255) | Xent Color 0.0143(0.0162) | Loss 2.6088(2.9354) | Error 0.0389(0.0395) | Error Color 0.0022(0.0028) |Steps 416(440.29) | Grad Norm 7.2456(9.3920) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 10.4051(10.5304) | Bit/dim 0.8812(0.8932) | Xent 0.1503(0.1235) | Xent Color 0.0088(0.0156) | Loss 2.5862(2.8325) | Error 0.0511(0.0389) | Error Color 0.0000(0.0028) |Steps 434(439.15) | Grad Norm 9.0460(8.9098) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 10.3207(10.5020) | Bit/dim 0.9134(0.8917) | Xent 0.1351(0.1257) | Xent Color 0.0607(0.0169) | Loss 2.6624(2.7656) | Error 0.0500(0.0400) | Error Color 0.0222(0.0033) |Steps 446(438.81) | Grad Norm 39.2468(10.0434) | Total Time 0.00(0.00)\n",
      "Iter 4080 | Time 11.5333(10.5455) | Bit/dim 2.3965(1.2496) | Xent 0.6270(0.4006) | Xent Color 2.0572(1.2791) | Loss 6.7702(4.1021) | Error 0.2033(0.0951) | Error Color 0.6167(0.1751) |Steps 536(446.81) | Grad Norm 19.9290(37.3830) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 12.3397(11.1323) | Bit/dim 2.0743(1.4937) | Xent 0.4439(0.4550) | Xent Color 0.9400(1.2905) | Loss 5.6791(4.6356) | Error 0.1411(0.1222) | Error Color 0.3367(0.2551) |Steps 584(484.41) | Grad Norm 5.4408(29.8128) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 87.2175, Epoch Time 833.2723(731.1694), Bit/dim 2.0154(best: 0.8939), Xent 0.2077, Xent Color 0.5675. Loss 2.2092, Error 0.0632(best: 0.0212), Error Color 0.2044(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 13.1615(11.6401) | Bit/dim 1.8672(1.6154) | Xent 0.3168(0.4222) | Xent Color 0.4872(1.1306) | Loss 5.2099(5.6357) | Error 0.1011(0.1183) | Error Color 0.1900(0.2571) |Steps 614(514.31) | Grad Norm 4.6239(23.2856) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 13.0228(11.9569) | Bit/dim 1.7278(1.6579) | Xent 0.2103(0.3831) | Xent Color 0.3538(0.9443) | Loss 4.6900(5.4217) | Error 0.0611(0.1101) | Error Color 0.1278(0.2314) |Steps 578(532.93) | Grad Norm 1.9139(18.1840) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 11.8818(12.0988) | Bit/dim 1.5896(1.6574) | Xent 0.2511(0.3435) | Xent Color 0.3080(0.7798) | Loss 4.2678(5.1727) | Error 0.0922(0.1004) | Error Color 0.1111(0.2015) |Steps 560(542.88) | Grad Norm 6.4413(14.7840) | Total Time 0.00(0.00)\n",
      "Iter 4130 | Time 13.0484(12.2065) | Bit/dim 1.4986(1.6283) | Xent 0.2135(0.3043) | Xent Color 0.2253(0.6418) | Loss 4.1966(4.9232) | Error 0.0744(0.0913) | Error Color 0.0756(0.1724) |Steps 566(548.81) | Grad Norm 5.4561(12.4180) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 11.8067(12.0610) | Bit/dim 1.3744(1.5369) | Xent 0.1964(0.2537) | Xent Color 0.1350(0.4229) | Loss 3.8874(4.4658) | Error 0.0611(0.0772) | Error Color 0.0467(0.1181) |Steps 530(539.47) | Grad Norm 12.1339(9.9616) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 73.6890, Epoch Time 910.6316(736.5533), Bit/dim 1.5783(best: 0.8939), Xent 0.5386, Xent Color 0.5919. Loss 1.8609, Error 0.1827(best: 0.0212), Error Color 0.2629(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 11.2866(11.8764) | Bit/dim 1.5353(1.5154) | Xent 0.4227(0.2637) | Xent Color 0.6687(0.5929) | Loss 4.3634(5.1065) | Error 0.1389(0.0820) | Error Color 0.2556(0.1626) |Steps 530(530.87) | Grad Norm 17.3036(20.3024) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 11.3465(11.7164) | Bit/dim 1.5144(1.5276) | Xent 0.2522(0.2617) | Xent Color 0.3560(0.5634) | Loss 4.0823(4.8868) | Error 0.0789(0.0817) | Error Color 0.1233(0.1711) |Steps 512(524.61) | Grad Norm 4.5920(17.3370) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 12.1818(11.6489) | Bit/dim 1.4173(1.5066) | Xent 0.1839(0.2423) | Xent Color 0.2662(0.4952) | Loss 3.9142(4.6395) | Error 0.0556(0.0758) | Error Color 0.0878(0.1537) |Steps 482(516.39) | Grad Norm 2.8274(13.6709) | Total Time 0.00(0.00)\n",
      "Iter 4190 | Time 12.0909(11.5958) | Bit/dim 1.3325(1.4719) | Xent 0.1429(0.2246) | Xent Color 0.1918(0.4208) | Loss 3.5976(4.3999) | Error 0.0433(0.0707) | Error Color 0.0689(0.1316) |Steps 506(513.34) | Grad Norm 2.4628(10.7540) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 11.8205(11.5174) | Bit/dim 1.2888(1.4295) | Xent 0.1944(0.2113) | Xent Color 0.1398(0.3519) | Loss 3.6232(4.1847) | Error 0.0522(0.0657) | Error Color 0.0456(0.1102) |Steps 512(509.31) | Grad Norm 1.8028(8.4539) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 11.2824(11.4592) | Bit/dim 1.2652(1.3887) | Xent 0.1979(0.2004) | Xent Color 0.1126(0.2906) | Loss 3.4866(4.0005) | Error 0.0611(0.0627) | Error Color 0.0300(0.0891) |Steps 494(505.39) | Grad Norm 1.7735(6.6440) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 10.3650(11.3318) | Bit/dim 1.2052(1.3472) | Xent 0.1444(0.1886) | Xent Color 0.1055(0.2407) | Loss 3.3040(3.8298) | Error 0.0500(0.0590) | Error Color 0.0267(0.0720) |Steps 482(501.36) | Grad Norm 2.5188(5.4021) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 69.2345, Epoch Time 835.9076(739.5339), Bit/dim 1.2084(best: 0.8939), Xent 0.0891, Xent Color 0.0348. Loss 1.2394, Error 0.0280(best: 0.0212), Error Color 0.0020(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4230 | Time 11.1108(11.2007) | Bit/dim 1.1999(1.3086) | Xent 0.1646(0.1794) | Xent Color 0.0734(0.2002) | Loss 3.3122(4.2374) | Error 0.0478(0.0562) | Error Color 0.0211(0.0587) |Steps 464(492.02) | Grad Norm 2.6281(4.5099) | Total Time 0.00(0.00)\n",
      "Iter 4240 | Time 10.6719(11.0628) | Bit/dim 1.1677(1.2737) | Xent 0.1054(0.1690) | Xent Color 0.0709(0.1667) | Loss 3.1849(3.9723) | Error 0.0367(0.0538) | Error Color 0.0133(0.0475) |Steps 476(485.36) | Grad Norm 2.0868(3.8997) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 10.5344(10.9041) | Bit/dim 1.1216(1.2403) | Xent 0.1098(0.1636) | Xent Color 0.0594(0.1412) | Loss 3.1200(3.7610) | Error 0.0344(0.0516) | Error Color 0.0100(0.0391) |Steps 470(476.98) | Grad Norm 2.1511(3.5259) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 10.1717(10.8364) | Bit/dim 1.1145(1.2087) | Xent 0.1128(0.1575) | Xent Color 0.0663(0.1201) | Loss 3.0878(3.5862) | Error 0.0356(0.0490) | Error Color 0.0156(0.0320) |Steps 464(472.17) | Grad Norm 3.4130(3.4412) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 10.7726(10.7035) | Bit/dim 1.1041(1.1822) | Xent 0.1698(0.1540) | Xent Color 0.0496(0.1036) | Loss 3.0119(3.4511) | Error 0.0556(0.0481) | Error Color 0.0100(0.0272) |Steps 452(466.96) | Grad Norm 4.1513(3.4309) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 11.0647(10.6579) | Bit/dim 1.0835(1.1582) | Xent 0.1443(0.1521) | Xent Color 0.0612(0.0907) | Loss 3.0378(3.3411) | Error 0.0456(0.0473) | Error Color 0.0167(0.0230) |Steps 446(462.19) | Grad Norm 2.5575(3.5742) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 10.3931(10.6413) | Bit/dim 1.0639(1.1362) | Xent 0.1581(0.1497) | Xent Color 0.0532(0.0811) | Loss 2.9535(3.2499) | Error 0.0422(0.0464) | Error Color 0.0133(0.0204) |Steps 434(460.98) | Grad Norm 4.1354(3.9446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 67.4543, Epoch Time 786.7444(740.9503), Bit/dim 1.0681(best: 0.8939), Xent 0.0776, Xent Color 0.0149. Loss 1.0912, Error 0.0255(best: 0.0212), Error Color 0.0009(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4300 | Time 10.6478(10.5949) | Bit/dim 1.0691(1.1173) | Xent 0.1554(0.1488) | Xent Color 0.0501(0.0716) | Loss 2.9698(3.6291) | Error 0.0511(0.0465) | Error Color 0.0122(0.0174) |Steps 458(460.36) | Grad Norm 3.8554(3.6279) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 9.6706(10.5196) | Bit/dim 1.0486(1.0987) | Xent 0.1160(0.1466) | Xent Color 0.0412(0.0649) | Loss 2.9553(3.4457) | Error 0.0389(0.0460) | Error Color 0.0089(0.0155) |Steps 446(459.03) | Grad Norm 4.7461(3.7651) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 10.3819(10.5338) | Bit/dim 1.0450(1.0829) | Xent 0.1334(0.1439) | Xent Color 0.0379(0.0596) | Loss 2.8454(3.2988) | Error 0.0444(0.0446) | Error Color 0.0078(0.0139) |Steps 446(456.25) | Grad Norm 4.3872(4.1629) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 10.9180(10.5968) | Bit/dim 1.0375(1.0689) | Xent 0.1219(0.1397) | Xent Color 0.0452(0.0549) | Loss 2.9540(3.1911) | Error 0.0356(0.0432) | Error Color 0.0100(0.0126) |Steps 482(457.18) | Grad Norm 12.3784(4.8666) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 10.5942(10.6341) | Bit/dim 1.0270(1.0582) | Xent 0.1329(0.1374) | Xent Color 0.0472(0.0532) | Loss 2.8591(3.1066) | Error 0.0389(0.0425) | Error Color 0.0100(0.0121) |Steps 452(455.70) | Grad Norm 14.8598(6.5382) | Total Time 0.00(0.00)\n",
      "Iter 4350 | Time 10.5105(10.6300) | Bit/dim 0.9975(1.0479) | Xent 0.1054(0.1343) | Xent Color 0.0376(0.0506) | Loss 2.8289(3.0391) | Error 0.0378(0.0419) | Error Color 0.0056(0.0113) |Steps 476(457.41) | Grad Norm 11.8049(7.6671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 66.7108, Epoch Time 788.1620(742.3666), Bit/dim 1.0033(best: 0.8939), Xent 0.0747, Xent Color 0.0098. Loss 1.0244, Error 0.0235(best: 0.0212), Error Color 0.0003(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4360 | Time 10.7492(10.6351) | Bit/dim 0.9968(1.0368) | Xent 0.1527(0.1344) | Xent Color 0.0451(0.0466) | Loss 2.8091(3.5173) | Error 0.0467(0.0418) | Error Color 0.0078(0.0101) |Steps 440(455.11) | Grad Norm 5.5162(7.5115) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 10.2107(10.6258) | Bit/dim 1.0099(1.0270) | Xent 0.1219(0.1323) | Xent Color 0.0232(0.0427) | Loss 2.7814(3.3311) | Error 0.0378(0.0412) | Error Color 0.0022(0.0088) |Steps 428(453.59) | Grad Norm 4.8588(6.9670) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 10.3163(10.6271) | Bit/dim 0.9809(1.0170) | Xent 0.1158(0.1304) | Xent Color 0.0378(0.0395) | Loss 2.8524(3.1907) | Error 0.0356(0.0410) | Error Color 0.0067(0.0079) |Steps 464(455.34) | Grad Norm 7.3118(6.6354) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 10.5677(10.5934) | Bit/dim 0.9869(1.0100) | Xent 0.0826(0.1270) | Xent Color 0.0322(0.0397) | Loss 2.7832(3.0846) | Error 0.0222(0.0399) | Error Color 0.0056(0.0085) |Steps 446(455.26) | Grad Norm 8.2929(7.4558) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 11.0397(10.6268) | Bit/dim 0.9965(1.0044) | Xent 0.1424(0.1251) | Xent Color 0.0326(0.0385) | Loss 2.8232(3.0075) | Error 0.0389(0.0394) | Error Color 0.0067(0.0083) |Steps 458(453.78) | Grad Norm 11.4059(8.6287) | Total Time 0.00(0.00)\n",
      "Iter 4410 | Time 10.1103(10.6253) | Bit/dim 0.9707(0.9977) | Xent 0.1362(0.1252) | Xent Color 0.0205(0.0358) | Loss 2.7440(2.9413) | Error 0.0478(0.0396) | Error Color 0.0011(0.0074) |Steps 434(451.26) | Grad Norm 3.4019(8.3750) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 11.1385(10.6945) | Bit/dim 0.9522(0.9894) | Xent 0.1419(0.1280) | Xent Color 0.0367(0.0339) | Loss 2.7436(2.8944) | Error 0.0356(0.0403) | Error Color 0.0089(0.0070) |Steps 452(454.36) | Grad Norm 3.3307(7.4913) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 66.7629, Epoch Time 794.3708(743.9267), Bit/dim 0.9630(best: 0.8939), Xent 0.0666, Xent Color 0.0073. Loss 0.9815, Error 0.0234(best: 0.0212), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4430 | Time 10.7020(10.7294) | Bit/dim 0.9678(0.9825) | Xent 0.1168(0.1254) | Xent Color 0.0299(0.0327) | Loss 2.7320(3.3355) | Error 0.0344(0.0395) | Error Color 0.0033(0.0067) |Steps 428(453.22) | Grad Norm 7.8877(7.8382) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 11.0088(10.7580) | Bit/dim 0.9592(0.9801) | Xent 0.1065(0.1254) | Xent Color 0.0470(0.0384) | Loss 2.7514(3.1850) | Error 0.0367(0.0397) | Error Color 0.0111(0.0090) |Steps 470(452.13) | Grad Norm 10.7294(10.2449) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 10.8887(10.7664) | Bit/dim 0.9528(0.9755) | Xent 0.1475(0.1231) | Xent Color 0.0306(0.0417) | Loss 2.7700(3.0687) | Error 0.0433(0.0394) | Error Color 0.0067(0.0102) |Steps 440(451.23) | Grad Norm 3.9268(10.5927) | Total Time 0.00(0.00)\n",
      "Iter 4460 | Time 10.8169(10.7876) | Bit/dim 0.9609(0.9705) | Xent 0.1162(0.1202) | Xent Color 0.0268(0.0385) | Loss 2.6756(2.9739) | Error 0.0344(0.0382) | Error Color 0.0056(0.0092) |Steps 440(452.51) | Grad Norm 4.4976(9.5030) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 10.9684(10.8109) | Bit/dim 0.9468(0.9641) | Xent 0.1056(0.1186) | Xent Color 0.0260(0.0341) | Loss 2.7283(2.9052) | Error 0.0322(0.0375) | Error Color 0.0044(0.0078) |Steps 482(456.60) | Grad Norm 7.6732(8.5915) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 11.0479(10.8015) | Bit/dim 0.9292(0.9578) | Xent 0.1151(0.1190) | Xent Color 0.0201(0.0305) | Loss 2.6470(2.8490) | Error 0.0344(0.0372) | Error Color 0.0033(0.0066) |Steps 452(457.42) | Grad Norm 6.0082(8.2137) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 67.5887, Epoch Time 804.4642(745.7429), Bit/dim 0.9328(best: 0.8939), Xent 0.0647, Xent Color 0.0056. Loss 0.9504, Error 0.0220(best: 0.0212), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4490 | Time 10.6608(10.7737) | Bit/dim 0.9377(0.9527) | Xent 0.1210(0.1188) | Xent Color 0.0231(0.0282) | Loss 2.6657(3.3857) | Error 0.0356(0.0371) | Error Color 0.0022(0.0057) |Steps 452(456.02) | Grad Norm 6.4927(7.8912) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 10.1754(10.7322) | Bit/dim 0.9470(0.9481) | Xent 0.1175(0.1155) | Xent Color 0.0259(0.0270) | Loss 2.6477(3.1995) | Error 0.0333(0.0355) | Error Color 0.0067(0.0053) |Steps 428(453.15) | Grad Norm 7.9923(7.6782) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 10.2042(10.7150) | Bit/dim 0.9347(0.9428) | Xent 0.1258(0.1168) | Xent Color 0.0238(0.0256) | Loss 2.6785(3.0636) | Error 0.0278(0.0361) | Error Color 0.0044(0.0051) |Steps 440(451.05) | Grad Norm 5.6145(7.0110) | Total Time 0.00(0.00)\n",
      "Iter 4520 | Time 10.8289(10.7507) | Bit/dim 0.9213(0.9373) | Xent 0.1487(0.1150) | Xent Color 0.0160(0.0234) | Loss 2.6764(2.9566) | Error 0.0378(0.0356) | Error Color 0.0011(0.0042) |Steps 458(449.88) | Grad Norm 6.5108(6.7055) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 10.8993(10.7410) | Bit/dim 0.9137(0.9311) | Xent 0.0885(0.1131) | Xent Color 0.0213(0.0224) | Loss 2.6755(2.8736) | Error 0.0233(0.0348) | Error Color 0.0089(0.0041) |Steps 440(450.34) | Grad Norm 6.2477(6.3085) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 10.0556(10.7100) | Bit/dim 1.2456(0.9491) | Xent 0.1172(0.1159) | Xent Color 0.2283(0.0355) | Loss 3.3455(2.8649) | Error 0.0356(0.0357) | Error Color 0.0878(0.0095) |Steps 434(451.06) | Grad Norm 40.9300(11.3033) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_multiscale_beta.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_1_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10 --beta 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
