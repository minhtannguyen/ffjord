{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_multiscale_beta.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "# for disentanglement\n",
      "parser.add_argument('--beta', default=0.01, type=float, help='disentanglement weight')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    z_sup_class = [o[:,:int(np.prod(o.size()[1:])*0.5)] for o in z]\n",
      "    z_sup_class = torch.cat(z_sup_class,1)\n",
      "    \n",
      "    z_sup_color = [o[:,int(np.prod(o.size()[1:])*0.5):] for o in z]\n",
      "    z_sup_color = torch.cat(z_sup_color,1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup_class).view(-1,1)  # logp(z)_sup\n",
      "    beta_logpz_sup = logpz_sup * (1.0 - args.beta * torch.exp(logpz_sup) / torch.tensor(model.module.y_class).to(logpz_sup))\n",
      "    \n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z_sup_color).view(-1,1)  # logp(z)_color_sup\n",
      "    beta_logpz_color_sup = logpz_color_sup * (1.0 - args.beta * torch.exp(logpz_color_sup) / torch.tensor(model.module.y_color).to(logpz_color_sup))\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = beta_logpz_sup + beta_logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup_class = model.module.dropout(z_sup_class)\n",
      "        z_sup_color = model.module.dropout_color(z_sup_color)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup_class)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(z_sup_color)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio * 2.,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_color_sup = fixed_z_color_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_color_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_color_sup = start_color_sup + (2**(ns-2))*a_color_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_color_sup = end_color_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_color_sup = start_color_sup + a_color_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, beta=0.001, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_0_001_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 7.1357(33.1041) | Bit/dim 25.4505(27.3297) | Xent 2.2869(2.3009) | Xent Color 2.3007(2.3023) | Loss 48.6142(51.7971) | Error 0.8744(0.8882) | Error Color 0.9067(0.8898) |Steps 290(297.51) | Grad Norm 260.8726(275.0042) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.4531(26.3283) | Bit/dim 20.0909(26.0735) | Xent 2.2513(2.2928) | Xent Color 2.2917(2.3003) | Loss 39.2005(49.5875) | Error 0.8644(0.8869) | Error Color 0.8811(0.8910) |Steps 314(300.83) | Grad Norm 216.4389(264.8302) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.6809(21.3553) | Bit/dim 13.8656(23.5615) | Xent 2.1974(2.2735) | Xent Color 2.2745(2.2954) | Loss 27.6681(45.0966) | Error 0.5411(0.8307) | Error Color 0.7822(0.8687) |Steps 308(303.32) | Grad Norm 159.2129(243.6392) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.4738(17.8921) | Bit/dim 8.9832(20.2265) | Xent 2.1211(2.2412) | Xent Color 2.2517(2.2866) | Loss 18.9596(39.0964) | Error 0.3322(0.7163) | Error Color 0.7511(0.8413) |Steps 338(313.78) | Grad Norm 95.5056(211.8953) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.3784(15.5277) | Bit/dim 6.6884(16.8755) | Xent 2.0425(2.1962) | Xent Color 2.2351(2.2744) | Loss 14.8279(33.0816) | Error 0.3333(0.6080) | Error Color 0.7467(0.8130) |Steps 404(332.18) | Grad Norm 33.8641(171.6101) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.4919(13.9046) | Bit/dim 6.0965(14.1054) | Xent 1.9660(2.1429) | Xent Color 2.1978(2.2582) | Loss 13.8246(28.1125) | Error 0.3422(0.5346) | Error Color 0.7800(0.8032) |Steps 398(347.42) | Grad Norm 25.0490(132.5938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 70.3632, Epoch Time 664.1834(664.1834), Bit/dim 5.7409(best: inf), Xent 1.8894, Xent Color 2.1737. Loss 6.7567, Error 0.2505(best: inf), Error Color 0.7276(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.8113(12.6809) | Bit/dim 5.5568(11.9159) | Xent 1.8885(2.0823) | Xent Color 2.1676(2.2383) | Loss 12.6816(24.6171) | Error 0.3100(0.4738) | Error Color 0.6700(0.7873) |Steps 374(358.22) | Grad Norm 18.3095(103.8221) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.3860(11.6622) | Bit/dim 4.9704(10.1646) | Xent 1.8138(2.0166) | Xent Color 2.1314(2.2142) | Loss 11.5146(21.3182) | Error 0.3022(0.4245) | Error Color 0.6000(0.7365) |Steps 386(362.97) | Grad Norm 14.4525(80.3129) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 8.9235(10.9258) | Bit/dim 4.5811(8.7587) | Xent 1.7558(1.9542) | Xent Color 2.1022(2.1889) | Loss 10.9352(18.6653) | Error 0.2756(0.3892) | Error Color 0.6278(0.7121) |Steps 386(365.24) | Grad Norm 13.4106(63.1016) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.6613(10.3867) | Bit/dim 4.1664(7.6068) | Xent 1.7182(1.8965) | Xent Color 2.0705(2.1608) | Loss 9.9130(16.4880) | Error 0.2700(0.3599) | Error Color 0.5889(0.6806) |Steps 350(367.51) | Grad Norm 9.0480(49.1766) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 9.0774(10.0469) | Bit/dim 3.7622(6.6452) | Xent 1.7104(1.8495) | Xent Color 2.0338(2.1304) | Loss 9.2636(14.6908) | Error 0.2900(0.3392) | Error Color 0.5344(0.6460) |Steps 392(370.90) | Grad Norm 8.0414(38.5384) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.9859(9.7528) | Bit/dim 3.3653(5.8283) | Xent 1.7375(1.8180) | Xent Color 2.0034(2.1006) | Loss 8.6044(13.1554) | Error 0.3022(0.3279) | Error Color 0.5533(0.6173) |Steps 374(372.31) | Grad Norm 8.2500(30.6077) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.7664(9.5767) | Bit/dim 3.0526(5.1349) | Xent 1.7686(1.8017) | Xent Color 1.9625(2.0678) | Loss 7.8460(11.8665) | Error 0.3089(0.3252) | Error Color 0.5411(0.5919) |Steps 374(375.22) | Grad Norm 6.3185(24.4518) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 56.5668, Epoch Time 674.0089(664.4781), Bit/dim 2.9671(best: 5.7409), Xent 1.7650, Xent Color 1.9373. Loss 3.8927, Error 0.2727(best: 0.2505), Error Color 0.4008(best: 0.7276)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.2012(9.5202) | Bit/dim 2.7925(4.5470) | Xent 1.8212(1.8013) | Xent Color 1.9172(2.0341) | Loss 7.5863(11.2146) | Error 0.3511(0.3308) | Error Color 0.4789(0.5643) |Steps 386(380.63) | Grad Norm 5.6834(19.5810) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.3533(9.4868) | Bit/dim 2.6174(4.0579) | Xent 1.8334(1.8090) | Xent Color 1.8795(1.9970) | Loss 7.2932(10.2037) | Error 0.3667(0.3395) | Error Color 0.4844(0.5414) |Steps 398(384.99) | Grad Norm 4.3352(15.6803) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.5245(9.4644) | Bit/dim 2.4090(3.3358) | Xent 1.8710(1.8333) | Xent Color 1.7650(1.9131) | Loss 6.8277(8.7417) | Error 0.4067(0.3620) | Error Color 0.4389(0.5023) |Steps 392(389.55) | Grad Norm 2.7683(10.0176) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.9069(9.5141) | Bit/dim 2.3311(3.0782) | Xent 1.8909(1.8464) | Xent Color 1.7012(1.8651) | Loss 6.7732(8.2203) | Error 0.4244(0.3752) | Error Color 0.3956(0.4827) |Steps 404(392.43) | Grad Norm 2.4629(8.0934) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.5304(9.5412) | Bit/dim 2.2726(2.8714) | Xent 1.8944(1.8565) | Xent Color 1.6574(1.8142) | Loss 6.5911(7.7906) | Error 0.4322(0.3885) | Error Color 0.4000(0.4624) |Steps 392(395.52) | Grad Norm 1.9854(6.5954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 60.1944, Epoch Time 710.5759(665.8611), Bit/dim 2.2406(best: 2.9671), Xent 1.8488, Xent Color 1.5590. Loss 3.0926, Error 0.3303(best: 0.2505), Error Color 0.3205(best: 0.4008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.7129(9.5584) | Bit/dim 2.2369(2.7060) | Xent 1.8804(1.8628) | Xent Color 1.5552(1.7596) | Loss 6.4900(7.9707) | Error 0.4267(0.3986) | Error Color 0.3611(0.4416) |Steps 422(396.57) | Grad Norm 2.9057(5.6169) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.3468(9.5322) | Bit/dim 2.1969(2.5768) | Xent 1.8212(1.8583) | Xent Color 1.4897(1.7008) | Loss 6.2961(7.5458) | Error 0.4022(0.4033) | Error Color 0.3600(0.4235) |Steps 392(396.54) | Grad Norm 1.7889(4.7525) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.3083(9.4806) | Bit/dim 2.1935(2.4774) | Xent 1.7891(1.8448) | Xent Color 1.4049(1.6344) | Loss 6.1324(7.2122) | Error 0.3756(0.4026) | Error Color 0.3744(0.4062) |Steps 380(397.55) | Grad Norm 1.9539(4.2180) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.5240(9.4532) | Bit/dim 2.1735(2.4017) | Xent 1.7253(1.8199) | Xent Color 1.3348(1.5587) | Loss 6.2112(6.9498) | Error 0.3633(0.3966) | Error Color 0.3722(0.3903) |Steps 404(398.61) | Grad Norm 15.3458(4.9752) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.6972(9.4404) | Bit/dim 2.1952(2.3445) | Xent 1.6687(1.7879) | Xent Color 1.2998(1.4933) | Loss 6.2081(6.7438) | Error 0.3622(0.3885) | Error Color 0.3678(0.3875) |Steps 374(397.12) | Grad Norm 38.6720(10.2803) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.2626(9.4042) | Bit/dim 2.1615(2.2988) | Xent 1.6233(1.7444) | Xent Color 1.1963(1.4226) | Loss 6.0415(6.5661) | Error 0.3689(0.3747) | Error Color 0.3122(0.3711) |Steps 404(399.17) | Grad Norm 17.9276(13.0586) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.1795(9.3811) | Bit/dim 2.1560(2.2610) | Xent 1.5278(1.6948) | Xent Color 1.1004(1.3497) | Loss 5.8918(6.4073) | Error 0.3200(0.3613) | Error Color 0.2456(0.3444) |Steps 386(398.62) | Grad Norm 5.6386(13.4226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 59.6266, Epoch Time 701.1842(666.9208), Bit/dim 2.2641(best: 2.2406), Xent 1.4234, Xent Color 1.9078. Loss 3.0969, Error 0.2460(best: 0.2505), Error Color 0.7542(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.2742(9.3437) | Bit/dim 2.1900(2.2434) | Xent 1.4020(1.6347) | Xent Color 1.1056(1.3397) | Loss 5.8832(6.7799) | Error 0.2878(0.3471) | Error Color 0.2944(0.3785) |Steps 386(398.97) | Grad Norm 27.9495(27.0439) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.2584(9.3246) | Bit/dim 2.1653(2.2253) | Xent 1.3247(1.5696) | Xent Color 1.0667(1.2793) | Loss 5.9302(6.5538) | Error 0.2567(0.3333) | Error Color 0.2189(0.3611) |Steps 398(398.51) | Grad Norm 21.5211(28.0622) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.4229(9.2856) | Bit/dim 2.1553(2.2092) | Xent 1.2924(1.5015) | Xent Color 0.9835(1.2111) | Loss 5.7690(6.3534) | Error 0.2689(0.3191) | Error Color 0.1889(0.3251) |Steps 398(395.93) | Grad Norm 11.9963(25.2892) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.0797(9.2562) | Bit/dim 2.1805(2.1968) | Xent 1.1452(1.4301) | Xent Color 0.8938(1.1423) | Loss 5.7077(6.1834) | Error 0.2422(0.3059) | Error Color 0.1589(0.2914) |Steps 410(396.85) | Grad Norm 5.8301(21.4044) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.4669(9.2625) | Bit/dim 2.1732(2.1917) | Xent 1.1081(1.3516) | Xent Color 0.8777(1.0709) | Loss 5.6500(6.0557) | Error 0.2400(0.2904) | Error Color 0.2133(0.2654) |Steps 392(398.84) | Grad Norm 17.6952(18.8218) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.3931(9.3055) | Bit/dim 2.1751(2.2039) | Xent 1.0328(1.2704) | Xent Color 1.0448(1.2066) | Loss 5.7106(6.0636) | Error 0.2311(0.2773) | Error Color 0.3567(0.3188) |Steps 398(398.11) | Grad Norm 59.3427(42.3658) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.0806(9.2796) | Bit/dim 2.1782(2.2016) | Xent 1.0207(1.2025) | Xent Color 1.1048(1.1969) | Loss 5.6251(5.9859) | Error 0.2389(0.2669) | Error Color 0.4022(0.3557) |Steps 368(395.31) | Grad Norm 28.9706(42.9923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 58.4088, Epoch Time 691.3301(667.6531), Bit/dim 2.1697(best: 2.2406), Xent 0.9303, Xent Color 0.9511. Loss 2.6401, Error 0.1805(best: 0.2460), Error Color 0.2047(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.0950(9.2285) | Bit/dim 2.1403(2.1905) | Xent 1.0252(1.1500) | Xent Color 1.0362(1.1544) | Loss 5.7249(6.3290) | Error 0.2389(0.2596) | Error Color 0.3011(0.3439) |Steps 398(395.67) | Grad Norm 22.9256(36.8145) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 9.4353(9.2277) | Bit/dim 2.1393(2.1768) | Xent 0.9472(1.1036) | Xent Color 0.9763(1.1062) | Loss 5.6324(6.1392) | Error 0.2156(0.2537) | Error Color 0.2767(0.3168) |Steps 410(395.55) | Grad Norm 15.9155(31.0167) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 9.4444(9.2383) | Bit/dim 2.1281(2.1675) | Xent 0.8661(1.0494) | Xent Color 0.8786(1.0548) | Loss 5.3907(5.9828) | Error 0.2267(0.2467) | Error Color 0.2133(0.2881) |Steps 386(396.10) | Grad Norm 21.7532(27.2743) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.1749(9.2460) | Bit/dim 2.1399(2.1591) | Xent 0.8178(0.9925) | Xent Color 0.8237(1.0000) | Loss 5.4822(5.8521) | Error 0.2289(0.2376) | Error Color 0.1767(0.2598) |Steps 398(395.77) | Grad Norm 21.7653(23.4128) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 9.1976(9.2350) | Bit/dim 2.1300(2.1540) | Xent 0.7970(0.9401) | Xent Color 0.7852(0.9457) | Loss 5.2830(5.7368) | Error 0.2067(0.2305) | Error Color 0.1500(0.2348) |Steps 386(394.89) | Grad Norm 6.5011(20.2197) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.4337(9.2442) | Bit/dim 2.1496(2.1534) | Xent 0.8202(0.9019) | Xent Color 1.0027(0.9900) | Loss 5.5346(5.7000) | Error 0.2356(0.2252) | Error Color 0.3811(0.2736) |Steps 374(395.47) | Grad Norm 77.7460(36.5172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 59.2250, Epoch Time 691.2582(668.3612), Bit/dim 2.1496(best: 2.1697), Xent 0.6684, Xent Color 0.9044. Loss 2.5428, Error 0.1498(best: 0.1805), Error Color 0.3543(best: 0.2047)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.3246(9.2374) | Bit/dim 2.1546(2.1493) | Xent 0.7654(0.8680) | Xent Color 0.8307(0.9778) | Loss 5.4729(6.1355) | Error 0.2000(0.2208) | Error Color 0.2722(0.2892) |Steps 410(394.60) | Grad Norm 35.8288(39.8169) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.5675(9.2343) | Bit/dim 2.1258(2.1409) | Xent 0.7509(0.8439) | Xent Color 0.7465(0.9271) | Loss 5.4153(5.9426) | Error 0.2111(0.2182) | Error Color 0.1678(0.2699) |Steps 386(394.39) | Grad Norm 13.2925(36.3555) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.2338(9.2604) | Bit/dim 2.1400(2.1351) | Xent 0.6815(0.8108) | Xent Color 0.6477(0.8682) | Loss 5.2488(5.7815) | Error 0.1989(0.2124) | Error Color 0.1222(0.2375) |Steps 374(394.66) | Grad Norm 6.6574(30.6717) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.3667(9.2838) | Bit/dim 2.1071(2.1223) | Xent 0.6851(0.7612) | Xent Color 0.6211(0.7567) | Loss 5.2394(5.5511) | Error 0.2044(0.2044) | Error Color 0.1367(0.1863) |Steps 398(397.13) | Grad Norm 25.8071(22.8707) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.3378(9.2833) | Bit/dim 2.1243(2.1159) | Xent 0.6817(0.7415) | Xent Color 0.5583(0.7112) | Loss 5.2457(5.4739) | Error 0.2100(0.2023) | Error Color 0.1078(0.1690) |Steps 404(398.58) | Grad Norm 15.4489(21.0904) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.2652(9.2682) | Bit/dim 2.0875(2.1096) | Xent 0.7100(0.7222) | Xent Color 0.5039(0.6648) | Loss 5.2217(5.4042) | Error 0.2033(0.1997) | Error Color 0.0878(0.1511) |Steps 398(399.91) | Grad Norm 8.5882(19.5459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 59.1651, Epoch Time 694.7810(669.1538), Bit/dim 2.0985(best: 2.1496), Xent 0.5447, Xent Color 0.4565. Loss 2.3487, Error 0.1356(best: 0.1498), Error Color 0.0350(best: 0.2047)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.4429(9.2735) | Bit/dim 2.1203(2.1155) | Xent 0.6004(0.7015) | Xent Color 0.8465(0.8296) | Loss 5.4069(5.9127) | Error 0.1822(0.1968) | Error Color 0.3167(0.2135) |Steps 416(401.41) | Grad Norm 53.7975(38.3341) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 8.7778(9.2247) | Bit/dim 2.0959(2.1185) | Xent 0.7520(0.6979) | Xent Color 0.8800(0.8629) | Loss 5.3872(5.7763) | Error 0.2300(0.1985) | Error Color 0.3967(0.2606) |Steps 392(397.96) | Grad Norm 32.9937(39.1592) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 8.8381(9.2229) | Bit/dim 2.0666(2.1075) | Xent 0.6849(0.7076) | Xent Color 0.7083(0.8318) | Loss 5.0539(5.6449) | Error 0.1900(0.2014) | Error Color 0.1722(0.2531) |Steps 368(396.12) | Grad Norm 14.3302(32.8509) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.5169(9.2188) | Bit/dim 2.0488(2.0951) | Xent 0.6476(0.6943) | Xent Color 0.6067(0.7839) | Loss 5.1186(5.5198) | Error 0.2000(0.1991) | Error Color 0.1367(0.2296) |Steps 410(396.03) | Grad Norm 5.8446(26.6789) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.1376(9.2068) | Bit/dim 2.0625(2.0867) | Xent 0.5837(0.6741) | Xent Color 0.5687(0.7302) | Loss 5.1336(5.4109) | Error 0.1711(0.1944) | Error Color 0.1267(0.2050) |Steps 404(396.06) | Grad Norm 15.2739(21.5064) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 8.9100(9.2292) | Bit/dim 2.0675(2.0794) | Xent 0.5831(0.6570) | Xent Color 0.7552(0.7006) | Loss 5.0863(5.3405) | Error 0.1722(0.1906) | Error Color 0.2722(0.1983) |Steps 398(397.83) | Grad Norm 81.5947(26.8092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 57.9303, Epoch Time 689.3220(669.7588), Bit/dim 2.0555(best: 2.0985), Xent 0.4946, Xent Color 0.4931. Loss 2.3024, Error 0.1310(best: 0.1356), Error Color 0.1035(best: 0.0350)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.1381(9.2249) | Bit/dim 2.0238(2.0710) | Xent 0.5933(0.6497) | Xent Color 0.4900(0.6762) | Loss 4.8573(5.8191) | Error 0.1789(0.1894) | Error Color 0.1000(0.1951) |Steps 392(396.16) | Grad Norm 10.6811(31.4018) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.6211(9.2392) | Bit/dim 2.0218(2.0631) | Xent 0.5736(0.6404) | Xent Color 0.4642(0.6255) | Loss 5.0321(5.6118) | Error 0.1722(0.1883) | Error Color 0.1089(0.1722) |Steps 392(395.73) | Grad Norm 11.6716(27.5993) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.5597(9.2775) | Bit/dim 2.0325(2.0552) | Xent 0.6300(0.6284) | Xent Color 0.4298(0.5755) | Loss 5.0044(5.4484) | Error 0.1722(0.1844) | Error Color 0.0856(0.1503) |Steps 416(397.50) | Grad Norm 5.8514(23.6204) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.5282(9.3323) | Bit/dim 2.0293(2.0489) | Xent 0.5774(0.6179) | Xent Color 0.3768(0.5271) | Loss 4.9706(5.3301) | Error 0.1800(0.1836) | Error Color 0.0733(0.1301) |Steps 392(399.31) | Grad Norm 13.3438(19.7321) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.4275(9.3618) | Bit/dim 2.0233(2.0423) | Xent 0.5958(0.6134) | Xent Color 0.7368(0.5062) | Loss 5.2202(5.2443) | Error 0.1733(0.1834) | Error Color 0.2878(0.1277) |Steps 410(400.83) | Grad Norm 110.1835(25.4594) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 8.9523(9.3138) | Bit/dim 2.0579(2.0561) | Xent 0.6403(0.6126) | Xent Color 0.7970(0.7596) | Loss 5.1677(5.3299) | Error 0.2022(0.1854) | Error Color 0.3278(0.2134) |Steps 398(399.89) | Grad Norm 22.8388(45.4484) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.3290(9.3087) | Bit/dim 2.0153(2.0544) | Xent 0.6347(0.6279) | Xent Color 0.5902(0.7258) | Loss 5.0803(5.2830) | Error 0.1833(0.1907) | Error Color 0.1889(0.2133) |Steps 410(397.89) | Grad Norm 13.5436(38.3168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 58.6782, Epoch Time 698.1983(670.6120), Bit/dim 2.0334(best: 2.0555), Xent 0.4650, Xent Color 0.4484. Loss 2.2617, Error 0.1264(best: 0.1310), Error Color 0.0565(best: 0.0350)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.1209(9.3383) | Bit/dim 2.0217(2.0454) | Xent 0.6104(0.6200) | Xent Color 0.4871(0.6739) | Loss 4.9793(5.6898) | Error 0.2000(0.1891) | Error Color 0.1111(0.1926) |Steps 380(398.70) | Grad Norm 6.7198(30.5132) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.5783(9.3146) | Bit/dim 2.0194(2.0375) | Xent 0.5048(0.6048) | Xent Color 0.4086(0.6089) | Loss 4.8793(5.4936) | Error 0.1544(0.1848) | Error Color 0.0922(0.1655) |Steps 410(397.24) | Grad Norm 5.3677(23.7556) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.1987(9.3393) | Bit/dim 1.9836(2.0266) | Xent 0.5843(0.5955) | Xent Color 0.3564(0.5462) | Loss 4.8877(5.3402) | Error 0.1667(0.1824) | Error Color 0.0811(0.1427) |Steps 410(398.51) | Grad Norm 8.1378(18.5620) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.2794(9.3584) | Bit/dim 2.0138(2.0191) | Xent 0.5206(0.5886) | Xent Color 0.3093(0.4881) | Loss 4.9066(5.2155) | Error 0.1711(0.1814) | Error Color 0.0600(0.1212) |Steps 410(400.17) | Grad Norm 5.9478(15.0929) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.2944(9.3334) | Bit/dim 1.9772(2.0115) | Xent 0.5658(0.5834) | Xent Color 0.2629(0.4355) | Loss 4.8166(5.1150) | Error 0.1600(0.1791) | Error Color 0.0478(0.1036) |Steps 398(400.99) | Grad Norm 2.8531(12.6481) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.1969(9.3457) | Bit/dim 2.0054(2.0043) | Xent 0.5456(0.5747) | Xent Color 0.2523(0.3885) | Loss 4.8510(5.0390) | Error 0.1678(0.1763) | Error Color 0.0611(0.0884) |Steps 380(401.47) | Grad Norm 14.0031(11.0095) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.8674(9.3632) | Bit/dim 1.9707(1.9966) | Xent 0.5732(0.5655) | Xent Color 0.2296(0.3463) | Loss 4.8632(4.9761) | Error 0.1778(0.1736) | Error Color 0.0378(0.0761) |Steps 404(403.53) | Grad Norm 4.8517(9.6926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.3404(9.3869) | Bit/dim 2.0632(2.0040) | Xent 0.5505(0.5651) | Xent Color 1.2018(0.6290) | Loss 5.3796(5.5076) | Error 0.1822(0.1751) | Error Color 0.4667(0.1386) |Steps 398(403.68) | Grad Norm 41.5453(29.0185) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 8.9267(9.3021) | Bit/dim 2.0835(2.0257) | Xent 0.5949(0.5813) | Xent Color 0.6864(0.7536) | Loss 5.1379(5.4781) | Error 0.1867(0.1793) | Error Color 0.2433(0.2016) |Steps 392(400.65) | Grad Norm 18.2130(30.3052) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.0619(9.2465) | Bit/dim 2.0252(2.0270) | Xent 0.5543(0.6033) | Xent Color 0.5817(0.7270) | Loss 5.0070(5.3745) | Error 0.1811(0.1864) | Error Color 0.1789(0.2074) |Steps 386(396.98) | Grad Norm 6.4512(25.4576) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.2856(9.2186) | Bit/dim 2.0061(2.0193) | Xent 0.5778(0.5986) | Xent Color 0.4423(0.6671) | Loss 4.9771(5.2511) | Error 0.1833(0.1857) | Error Color 0.1167(0.1874) |Steps 356(394.15) | Grad Norm 6.0610(20.4246) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.3714(9.2374) | Bit/dim 1.9739(2.0077) | Xent 0.5291(0.5859) | Xent Color 0.3460(0.5948) | Loss 4.7824(5.1476) | Error 0.1711(0.1830) | Error Color 0.0789(0.1639) |Steps 398(397.21) | Grad Norm 2.3481(16.1689) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.4224(9.2716) | Bit/dim 1.9801(1.9988) | Xent 0.5211(0.5784) | Xent Color 0.3129(0.5229) | Loss 4.7462(5.0573) | Error 0.1633(0.1807) | Error Color 0.0700(0.1407) |Steps 344(397.69) | Grad Norm 1.4688(12.6130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 60.0012, Epoch Time 691.9718(672.1102), Bit/dim 1.9743(best: 1.9803), Xent 0.4037, Xent Color 0.1928. Loss 2.1234, Error 0.1147(best: 0.1183), Error Color 0.0160(best: 0.0096)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.3902(9.2726) | Bit/dim 1.9799(1.9889) | Xent 0.5451(0.5674) | Xent Color 0.2439(0.4549) | Loss 4.7911(5.4702) | Error 0.1544(0.1766) | Error Color 0.0500(0.1187) |Steps 422(397.75) | Grad Norm 3.9986(9.8732) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.4167(9.2293) | Bit/dim 1.9609(1.9810) | Xent 0.5265(0.5625) | Xent Color 0.2212(0.3968) | Loss 4.7705(5.2706) | Error 0.1633(0.1747) | Error Color 0.0467(0.1000) |Steps 392(397.48) | Grad Norm 1.3108(8.0643) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.5328(9.2560) | Bit/dim 1.9689(1.9740) | Xent 0.5446(0.5498) | Xent Color 0.2055(0.3482) | Loss 4.7233(5.1198) | Error 0.1644(0.1711) | Error Color 0.0433(0.0854) |Steps 350(396.89) | Grad Norm 6.9194(7.2082) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.3429(9.2499) | Bit/dim 1.9385(1.9663) | Xent 0.5057(0.5426) | Xent Color 0.1913(0.3085) | Loss 4.6916(4.9981) | Error 0.1656(0.1692) | Error Color 0.0411(0.0728) |Steps 410(396.24) | Grad Norm 2.7846(6.8746) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.7152(9.2733) | Bit/dim 1.9517(1.9597) | Xent 0.4705(0.5383) | Xent Color 0.1720(0.2745) | Loss 4.7201(4.9144) | Error 0.1567(0.1669) | Error Color 0.0300(0.0630) |Steps 374(395.13) | Grad Norm 14.1275(7.0830) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 8.9974(9.2706) | Bit/dim 1.9408(1.9531) | Xent 0.5904(0.5368) | Xent Color 0.1886(0.2479) | Loss 4.7171(4.8448) | Error 0.1956(0.1668) | Error Color 0.0467(0.0563) |Steps 386(396.76) | Grad Norm 22.1936(8.8535) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.6246(9.2659) | Bit/dim 1.9134(1.9460) | Xent 0.4795(0.5288) | Xent Color 0.1670(0.2255) | Loss 4.5852(4.7807) | Error 0.1456(0.1641) | Error Color 0.0344(0.0509) |Steps 386(395.75) | Grad Norm 15.1882(9.9591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 59.6858, Epoch Time 693.8523(672.7625), Bit/dim 1.9318(best: 1.9743), Xent 0.3749, Xent Color 0.0833. Loss 2.0464, Error 0.1054(best: 0.1147), Error Color 0.0053(best: 0.0096)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 8.8717(9.2686) | Bit/dim 1.9366(1.9394) | Xent 0.5505(0.5250) | Xent Color 0.1219(0.2022) | Loss 4.6052(5.1628) | Error 0.1789(0.1620) | Error Color 0.0244(0.0439) |Steps 398(396.15) | Grad Norm 3.7713(9.3201) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.5555(9.2767) | Bit/dim 1.9061(1.9342) | Xent 0.3862(0.5174) | Xent Color 0.1330(0.1834) | Loss 4.5032(5.0093) | Error 0.1233(0.1599) | Error Color 0.0278(0.0391) |Steps 374(396.11) | Grad Norm 11.6584(9.7392) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.0736(9.2681) | Bit/dim 1.9313(1.9274) | Xent 0.4700(0.5167) | Xent Color 0.1073(0.1663) | Loss 4.5611(4.8879) | Error 0.1444(0.1602) | Error Color 0.0178(0.0346) |Steps 398(395.07) | Grad Norm 10.5153(9.6495) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 9.0497(9.2291) | Bit/dim 1.9061(1.9202) | Xent 0.4726(0.5137) | Xent Color 0.1029(0.1516) | Loss 4.4570(4.7950) | Error 0.1489(0.1587) | Error Color 0.0267(0.0313) |Steps 392(395.05) | Grad Norm 10.7362(8.7350) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.3454(9.2626) | Bit/dim 1.8950(1.9143) | Xent 0.4979(0.5077) | Xent Color 0.1116(0.1378) | Loss 4.4987(4.7248) | Error 0.1467(0.1572) | Error Color 0.0178(0.0275) |Steps 410(395.34) | Grad Norm 19.8595(9.2045) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.5621(9.2554) | Bit/dim 2.5003(2.0806) | Xent 0.9386(0.5369) | Xent Color 3.9085(1.5108) | Loss 7.4840(5.6173) | Error 0.3022(0.1664) | Error Color 0.6800(0.1568) |Steps 434(397.11) | Grad Norm 43.6034(46.1156) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 56.6887, Epoch Time 692.9623(673.3685), Bit/dim 2.1494(best: 1.9318), Xent 0.5996, Xent Color 0.6927. Loss 2.4725, Error 0.1782(best: 0.1054), Error Color 0.2422(best: 0.0053)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 8.9077(9.3001) | Bit/dim 2.1439(2.1198) | Xent 0.7506(0.6827) | Xent Color 0.9502(1.4571) | Loss 5.3839(6.1827) | Error 0.2400(0.2178) | Error Color 0.3811(0.2396) |Steps 356(395.65) | Grad Norm 12.4690(39.2802) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 8.7020(9.2015) | Bit/dim 2.0852(2.1198) | Xent 0.7126(0.6884) | Xent Color 0.7802(1.2974) | Loss 5.2050(5.9485) | Error 0.2244(0.2188) | Error Color 0.2967(0.2623) |Steps 374(389.70) | Grad Norm 8.9793(31.5090) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.1822(9.1662) | Bit/dim 1.9922(2.0930) | Xent 0.6724(0.6819) | Xent Color 0.6078(1.1336) | Loss 4.9569(5.7164) | Error 0.2211(0.2178) | Error Color 0.2067(0.2572) |Steps 398(389.12) | Grad Norm 5.3106(25.1134) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 8.9786(9.1784) | Bit/dim 1.9675(2.0653) | Xent 0.5942(0.6659) | Xent Color 0.4713(0.9742) | Loss 4.8587(5.5056) | Error 0.1767(0.2122) | Error Color 0.1544(0.2384) |Steps 398(388.85) | Grad Norm 9.9174(20.3887) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.1922(9.1530) | Bit/dim 1.9789(2.0407) | Xent 0.5388(0.6430) | Xent Color 0.3610(0.8221) | Loss 4.8277(5.3159) | Error 0.1756(0.2044) | Error Color 0.1189(0.2094) |Steps 392(389.99) | Grad Norm 6.5297(16.1644) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 8.6623(9.1566) | Bit/dim 1.9539(2.0183) | Xent 0.5693(0.6175) | Xent Color 0.2465(0.6827) | Loss 4.6542(5.1621) | Error 0.1878(0.1953) | Error Color 0.0711(0.1749) |Steps 386(389.44) | Grad Norm 2.1555(12.7288) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 8.8257(9.1699) | Bit/dim 1.9547(1.9988) | Xent 0.5554(0.5997) | Xent Color 0.2141(0.5613) | Loss 4.6412(5.0379) | Error 0.1711(0.1881) | Error Color 0.0522(0.1438) |Steps 374(391.31) | Grad Norm 3.6423(10.3778) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 59.4104, Epoch Time 684.9581(673.7162), Bit/dim 1.9474(best: 1.9318), Xent 0.3801, Xent Color 0.1017. Loss 2.0679, Error 0.1116(best: 0.1054), Error Color 0.0088(best: 0.0053)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.4369(9.1903) | Bit/dim 1.9342(1.9825) | Xent 0.4853(0.5820) | Xent Color 0.1437(0.4583) | Loss 4.6109(5.3858) | Error 0.1589(0.1827) | Error Color 0.0311(0.1164) |Steps 416(392.60) | Grad Norm 5.9232(9.2080) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.3221(9.1994) | Bit/dim 1.9161(1.9682) | Xent 0.4801(0.5601) | Xent Color 0.1332(0.3748) | Loss 4.5570(5.1810) | Error 0.1500(0.1752) | Error Color 0.0256(0.0940) |Steps 398(394.33) | Grad Norm 4.0390(8.1680) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.3723(9.2000) | Bit/dim 1.8991(1.9548) | Xent 0.5118(0.5532) | Xent Color 0.1057(0.3113) | Loss 4.6025(5.0319) | Error 0.1567(0.1724) | Error Color 0.0189(0.0771) |Steps 404(394.66) | Grad Norm 4.0105(7.8127) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.1056(9.1851) | Bit/dim 1.9243(1.9419) | Xent 0.5981(0.5403) | Xent Color 0.1228(0.2614) | Loss 4.5811(4.9033) | Error 0.1800(0.1684) | Error Color 0.0233(0.0636) |Steps 386(393.77) | Grad Norm 5.9394(6.9569) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.5120(9.2157) | Bit/dim 1.9111(1.9323) | Xent 0.5283(0.5344) | Xent Color 0.1351(0.2225) | Loss 4.5416(4.8093) | Error 0.1756(0.1666) | Error Color 0.0344(0.0538) |Steps 386(394.28) | Grad Norm 14.5510(7.2083) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.0858(9.2164) | Bit/dim 1.9097(1.9228) | Xent 0.4253(0.5245) | Xent Color 0.1218(0.1948) | Loss 4.5283(4.7428) | Error 0.1489(0.1632) | Error Color 0.0311(0.0478) |Steps 386(395.52) | Grad Norm 15.9956(8.7785) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.0924(9.2088) | Bit/dim 1.8832(1.9128) | Xent 0.4691(0.5199) | Xent Color 0.1451(0.1768) | Loss 4.4730(4.6811) | Error 0.1500(0.1619) | Error Color 0.0422(0.0442) |Steps 410(394.17) | Grad Norm 20.5874(10.5220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 59.2079, Epoch Time 690.0854(674.2073), Bit/dim 1.8913(best: 1.9318), Xent 0.3558, Xent Color 0.0458. Loss 1.9917, Error 0.1038(best: 0.1054), Error Color 0.0031(best: 0.0053)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.3148(9.1916) | Bit/dim 1.8665(1.9049) | Xent 0.4851(0.5136) | Xent Color 0.1076(0.1596) | Loss 4.4562(5.0422) | Error 0.1478(0.1601) | Error Color 0.0278(0.0397) |Steps 392(394.25) | Grad Norm 13.5154(11.7159) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.0659(9.1882) | Bit/dim 1.8683(1.8974) | Xent 0.4704(0.5094) | Xent Color 0.0855(0.1439) | Loss 4.3888(4.8910) | Error 0.1356(0.1582) | Error Color 0.0167(0.0358) |Steps 392(393.88) | Grad Norm 4.7453(11.8500) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.3743(9.1653) | Bit/dim 1.8700(1.8902) | Xent 0.4504(0.5004) | Xent Color 0.0556(0.1267) | Loss 4.4415(4.7728) | Error 0.1389(0.1553) | Error Color 0.0078(0.0308) |Steps 404(393.08) | Grad Norm 1.8966(10.8664) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 8.9361(9.1785) | Bit/dim 1.8582(1.8808) | Xent 0.4293(0.4979) | Xent Color 0.0955(0.1154) | Loss 4.4463(4.6910) | Error 0.1344(0.1546) | Error Color 0.0256(0.0282) |Steps 404(392.70) | Grad Norm 17.6887(11.3780) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.4501(9.1960) | Bit/dim 1.8392(1.8734) | Xent 0.5049(0.4905) | Xent Color 0.0583(0.1035) | Loss 4.4429(4.6223) | Error 0.1622(0.1531) | Error Color 0.0078(0.0244) |Steps 386(393.39) | Grad Norm 5.2317(10.4926) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.1247(9.2390) | Bit/dim 1.8623(1.8664) | Xent 0.4918(0.4871) | Xent Color 0.0654(0.0939) | Loss 4.4272(4.5666) | Error 0.1689(0.1522) | Error Color 0.0156(0.0218) |Steps 362(393.24) | Grad Norm 11.7997(9.9978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 60.0656, Epoch Time 690.3591(674.6918), Bit/dim 1.8473(best: 1.8913), Xent 0.3353, Xent Color 0.0292. Loss 1.9384, Error 0.0946(best: 0.1038), Error Color 0.0021(best: 0.0031)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.7271(9.2547) | Bit/dim 1.8469(1.8603) | Xent 0.4954(0.4887) | Xent Color 0.0747(0.0890) | Loss 4.3805(5.0033) | Error 0.1544(0.1524) | Error Color 0.0167(0.0209) |Steps 386(393.44) | Grad Norm 14.8680(10.9227) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.6496(9.2789) | Bit/dim 1.8186(1.8514) | Xent 0.4546(0.4845) | Xent Color 0.1787(0.0904) | Loss 4.4089(4.8389) | Error 0.1456(0.1501) | Error Color 0.0533(0.0221) |Steps 380(394.36) | Grad Norm 43.3125(13.3811) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.2287(9.3343) | Bit/dim 2.1876(1.9326) | Xent 0.6155(0.5124) | Xent Color 1.4074(0.9008) | Loss 5.7418(5.2588) | Error 0.2067(0.1598) | Error Color 0.4978(0.1554) |Steps 404(395.31) | Grad Norm 40.1002(35.4105) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.2665(9.3983) | Bit/dim 1.9854(1.9701) | Xent 0.7032(0.5727) | Xent Color 0.8128(0.9328) | Loss 5.0802(5.3021) | Error 0.2144(0.1807) | Error Color 0.3378(0.2183) |Steps 404(398.46) | Grad Norm 9.9848(31.0737) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.4845(9.3837) | Bit/dim 1.9049(1.9652) | Xent 0.5410(0.5769) | Xent Color 0.4910(0.8398) | Loss 4.8581(5.2033) | Error 0.1756(0.1836) | Error Color 0.1822(0.2204) |Steps 404(399.60) | Grad Norm 6.8456(24.5912) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.1235(9.2788) | Bit/dim 1.8825(1.9497) | Xent 0.5582(0.5725) | Xent Color 0.3622(0.7291) | Loss 4.6272(5.0716) | Error 0.1789(0.1824) | Error Color 0.1200(0.2005) |Steps 398(396.29) | Grad Norm 3.9607(19.4666) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.6824(9.2196) | Bit/dim 1.8525(1.9300) | Xent 0.5114(0.5605) | Xent Color 0.2945(0.6243) | Loss 4.4953(4.9379) | Error 0.1589(0.1777) | Error Color 0.1022(0.1763) |Steps 392(392.73) | Grad Norm 3.3666(15.4610) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 57.9335, Epoch Time 695.2184(675.3076), Bit/dim 1.8760(best: 1.8473), Xent 0.3780, Xent Color 0.1806. Loss 2.0156, Error 0.1112(best: 0.0946), Error Color 0.0255(best: 0.0021)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.3696(9.2288) | Bit/dim 1.8671(1.9126) | Xent 0.5471(0.5488) | Xent Color 0.2239(0.5282) | Loss 4.5229(5.2376) | Error 0.1656(0.1741) | Error Color 0.0644(0.1500) |Steps 362(390.86) | Grad Norm 3.2526(11.9566) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.4616(9.1913) | Bit/dim 1.8636(1.8971) | Xent 0.6154(0.5429) | Xent Color 0.1855(0.4447) | Loss 4.5537(5.0442) | Error 0.1856(0.1701) | Error Color 0.0544(0.1255) |Steps 374(389.77) | Grad Norm 1.6434(9.3403) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 9.1683(9.1600) | Bit/dim 1.8263(1.8816) | Xent 0.4523(0.5279) | Xent Color 0.1728(0.3762) | Loss 4.3812(4.8817) | Error 0.1322(0.1657) | Error Color 0.0411(0.1053) |Steps 398(388.84) | Grad Norm 2.1435(7.8548) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 8.9851(9.1114) | Bit/dim 1.8220(1.8697) | Xent 0.4566(0.5162) | Xent Color 0.1479(0.3177) | Loss 4.3475(4.7578) | Error 0.1356(0.1618) | Error Color 0.0367(0.0876) |Steps 386(388.28) | Grad Norm 2.0973(6.6641) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 8.7169(9.0875) | Bit/dim 1.8309(1.8566) | Xent 0.5033(0.5147) | Xent Color 0.1266(0.2690) | Loss 4.4565(4.6574) | Error 0.1700(0.1619) | Error Color 0.0289(0.0732) |Steps 386(386.80) | Grad Norm 4.3709(5.5789) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 8.7923(9.0895) | Bit/dim 1.8084(1.8448) | Xent 0.4462(0.5053) | Xent Color 0.1129(0.2290) | Loss 4.3419(4.5798) | Error 0.1389(0.1582) | Error Color 0.0278(0.0606) |Steps 386(388.03) | Grad Norm 4.5829(4.7692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 59.8904, Epoch Time 683.8812(675.5648), Bit/dim 1.8086(best: 1.8473), Xent 0.3447, Xent Color 0.0519. Loss 1.9077, Error 0.1005(best: 0.0946), Error Color 0.0055(best: 0.0021)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.0370(9.1235) | Bit/dim 1.8039(1.8347) | Xent 0.4638(0.5008) | Xent Color 0.1087(0.1984) | Loss 4.3489(5.0045) | Error 0.1411(0.1564) | Error Color 0.0278(0.0521) |Steps 386(387.24) | Grad Norm 5.0284(4.8385) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 8.7139(9.1081) | Bit/dim 1.8072(1.8243) | Xent 0.4201(0.4927) | Xent Color 0.0855(0.1718) | Loss 4.2878(4.8235) | Error 0.1456(0.1536) | Error Color 0.0189(0.0443) |Steps 386(384.68) | Grad Norm 7.1636(4.9535) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.4202(9.1145) | Bit/dim 1.7767(1.8145) | Xent 0.3939(0.4881) | Xent Color 0.0737(0.1519) | Loss 4.1805(4.6768) | Error 0.1244(0.1521) | Error Color 0.0111(0.0385) |Steps 368(383.56) | Grad Norm 3.1978(5.4353) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 9.5400(9.1244) | Bit/dim 1.7595(1.8033) | Xent 0.4745(0.4885) | Xent Color 0.0827(0.1338) | Loss 4.3465(4.5696) | Error 0.1544(0.1524) | Error Color 0.0144(0.0330) |Steps 404(384.30) | Grad Norm 1.3986(4.9744) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 9.4257(9.1346) | Bit/dim 1.7706(1.7941) | Xent 0.5088(0.4863) | Xent Color 0.0754(0.1196) | Loss 4.3147(4.4780) | Error 0.1567(0.1518) | Error Color 0.0144(0.0287) |Steps 386(382.58) | Grad Norm 3.0902(4.6922) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 9.0686(9.1465) | Bit/dim 1.7652(1.7836) | Xent 0.5063(0.4854) | Xent Color 0.0653(0.1076) | Loss 4.2070(4.4081) | Error 0.1678(0.1524) | Error Color 0.0133(0.0259) |Steps 380(385.14) | Grad Norm 4.4338(4.5830) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 8.9996(9.2017) | Bit/dim 1.7561(1.7726) | Xent 0.4740(0.4825) | Xent Color 0.0632(0.0974) | Loss 4.2151(4.3560) | Error 0.1489(0.1514) | Error Color 0.0144(0.0232) |Steps 374(386.91) | Grad Norm 3.6682(4.4492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 58.3358, Epoch Time 687.6859(675.9285), Bit/dim 1.7398(best: 1.8086), Xent 0.3303, Xent Color 0.0267. Loss 1.8290, Error 0.0975(best: 0.0946), Error Color 0.0017(best: 0.0021)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.2578(9.2292) | Bit/dim 1.7121(1.7613) | Xent 0.5029(0.4819) | Xent Color 0.0700(0.0887) | Loss 4.1787(4.7502) | Error 0.1678(0.1517) | Error Color 0.0178(0.0209) |Steps 392(386.99) | Grad Norm 6.0512(4.0847) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 9.3168(9.2564) | Bit/dim 1.7244(1.7529) | Xent 0.4669(0.4800) | Xent Color 0.0544(0.0820) | Loss 4.1591(4.5933) | Error 0.1544(0.1509) | Error Color 0.0100(0.0193) |Steps 404(387.79) | Grad Norm 2.2610(4.3826) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 9.2532(9.2760) | Bit/dim 1.7069(1.7432) | Xent 0.4092(0.4804) | Xent Color 0.0669(0.0786) | Loss 4.1278(4.4810) | Error 0.1389(0.1518) | Error Color 0.0211(0.0191) |Steps 398(389.96) | Grad Norm 12.5256(5.9223) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 8.6339(9.2812) | Bit/dim 1.7104(1.7334) | Xent 0.3635(0.4732) | Xent Color 0.0475(0.0733) | Loss 3.9771(4.3822) | Error 0.1189(0.1506) | Error Color 0.0078(0.0176) |Steps 362(387.73) | Grad Norm 3.4975(6.3475) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.5122(9.3129) | Bit/dim 1.6949(1.7222) | Xent 0.4530(0.4665) | Xent Color 0.0558(0.0687) | Loss 4.1617(4.3094) | Error 0.1267(0.1488) | Error Color 0.0156(0.0167) |Steps 398(389.62) | Grad Norm 5.9774(6.9532) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.2897(9.3258) | Bit/dim 1.6745(1.7135) | Xent 0.4593(0.4709) | Xent Color 0.0677(0.0652) | Loss 4.0871(4.2547) | Error 0.1522(0.1488) | Error Color 0.0144(0.0156) |Steps 380(390.49) | Grad Norm 7.9250(7.8540) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.2104(9.3387) | Bit/dim 1.6715(1.7019) | Xent 0.4542(0.4658) | Xent Color 0.0492(0.0638) | Loss 4.0946(4.2045) | Error 0.1444(0.1463) | Error Color 0.0100(0.0154) |Steps 398(389.26) | Grad Norm 10.3933(9.3844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 58.0227, Epoch Time 697.0231(676.5613), Bit/dim 1.6701(best: 1.7398), Xent 0.3186, Xent Color 0.0212. Loss 1.7551, Error 0.0930(best: 0.0946), Error Color 0.0015(best: 0.0017)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.3569(9.3292) | Bit/dim 1.6450(1.6912) | Xent 0.3885(0.4639) | Xent Color 0.0323(0.0585) | Loss 4.0406(4.5642) | Error 0.1311(0.1465) | Error Color 0.0067(0.0139) |Steps 404(389.74) | Grad Norm 3.9715(8.4163) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 8.9347(9.3392) | Bit/dim 1.6512(1.6793) | Xent 0.4929(0.4607) | Xent Color 0.0450(0.0549) | Loss 4.0093(4.4180) | Error 0.1411(0.1449) | Error Color 0.0100(0.0130) |Steps 392(390.61) | Grad Norm 13.9850(8.5305) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 11.6788(9.4890) | Bit/dim 2.2782(1.8574) | Xent 0.7797(0.5242) | Xent Color 1.7395(1.3970) | Loss 6.2970(5.3217) | Error 0.2667(0.1607) | Error Color 0.6100(0.1457) |Steps 512(398.67) | Grad Norm 25.8124(37.3279) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 15.2193(10.9610) | Bit/dim 2.2763(1.9515) | Xent 0.9214(0.6427) | Xent Color 1.0419(1.3384) | Loss 6.4468(5.6034) | Error 0.2944(0.2031) | Error Color 0.4100(0.2217) |Steps 560(463.60) | Grad Norm 168800101177.8485(18278007599.0809) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 23.9295(13.4518) | Bit/dim 5.7622(2.3617) | Xent 2.1399(0.8383) | Xent Color 6.8719(1.7746) | Loss 16.3403(6.7560) | Error 0.4711(0.2447) | Error Color 0.7622(0.3224) |Steps 1040(573.19) | Grad Norm inf(inf) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_multiscale_beta.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_0_001_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10 --beta 0.001\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
