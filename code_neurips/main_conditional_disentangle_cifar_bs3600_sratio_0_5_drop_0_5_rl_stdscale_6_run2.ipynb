{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=3600, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run2', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 25.4115(50.7513) | Bit/dim 8.7760(8.9359) | Xent 2.2777(2.2998) | Loss 21.9511(22.2997) | Error 0.7422(0.8627) Steps 502(479.28) | Grad Norm 16.9686(24.1808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 71.6935, Epoch Time 482.5860(482.5860), Bit/dim 8.6426(best: inf), Xent 2.2557, Loss 9.7704, Error 0.7391(best: inf)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0020 | Time 28.9280(45.0798) | Bit/dim 8.5447(8.8520) | Xent 2.2228(2.2858) | Loss 21.5624(22.6712) | Error 0.7369(0.8320) Steps 514(483.61) | Grad Norm 7.4119(20.5403) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 69.2877, Epoch Time 461.8656(481.9644), Bit/dim 8.3920(best: 8.6426), Xent 2.1767, Loss 9.4803, Error 0.7330(best: 0.7391)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0030 | Time 27.7395(40.7775) | Bit/dim 8.3662(8.7383) | Xent 2.1668(2.2607) | Loss 20.9765(22.8462) | Error 0.7417(0.8088) Steps 472(487.03) | Grad Norm 6.7260(16.9791) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 68.2335, Epoch Time 462.6931(481.3863), Bit/dim 8.1324(best: 8.3920), Xent 2.1147, Loss 9.1898, Error 0.7146(best: 0.7330)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0040 | Time 26.8677(37.6568) | Bit/dim 8.1209(8.6048) | Xent 2.1142(2.2284) | Loss 42.1069(22.9161) | Error 0.7153(0.7876) Steps 502(491.62) | Grad Norm 5.3803(13.9671) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 29.6040(35.3955) | Bit/dim 7.8759(8.4445) | Xent 2.1015(2.1954) | Loss 19.7593(22.1611) | Error 0.7197(0.7688) Steps 526(494.53) | Grad Norm 5.1999(11.6849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 69.6368, Epoch Time 472.9150(481.1321), Bit/dim 7.7757(best: 8.1324), Xent 2.0747, Loss 8.8130, Error 0.6962(best: 0.7146)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0060 | Time 28.2901(33.7675) | Bit/dim 7.5462(8.2468) | Xent 2.0670(2.1639) | Loss 19.1715(21.9802) | Error 0.6858(0.7495) Steps 484(495.66) | Grad Norm 4.6354(9.9391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 70.3496, Epoch Time 472.2557(480.8659), Bit/dim 7.3609(best: 7.7757), Xent 2.0663, Loss 8.3941, Error 0.6717(best: 0.6962)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0070 | Time 30.2566(32.8375) | Bit/dim 7.2551(8.0176) | Xent 2.0781(2.1405) | Loss 18.4496(21.7100) | Error 0.6772(0.7329) Steps 502(498.87) | Grad Norm 3.5149(8.3770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 70.9879, Epoch Time 477.3256(480.7596), Bit/dim 7.1152(best: 7.3609), Xent 2.0874, Loss 8.1589, Error 0.6850(best: 0.6717)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0080 | Time 30.6245(32.1227) | Bit/dim 7.0982(7.7924) | Xent 2.0905(2.1261) | Loss 18.5082(21.4860) | Error 0.6919(0.7206) Steps 544(504.14) | Grad Norm 2.3368(6.8821) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 29.0283(31.4559) | Bit/dim 7.0394(7.6005) | Xent 2.0817(2.1179) | Loss 18.0865(20.6303) | Error 0.6989(0.7174) Steps 532(511.66) | Grad Norm 2.0573(5.6612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 73.3374, Epoch Time 477.4879(480.6615), Bit/dim 7.0272(best: 7.1152), Xent 2.0797, Loss 8.0670, Error 0.6924(best: 0.6717)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0100 | Time 32.0757(31.3449) | Bit/dim 6.9955(7.4453) | Xent 2.0784(2.1072) | Loss 18.1258(20.5050) | Error 0.7083(0.7139) Steps 562(518.21) | Grad Norm 1.8665(4.7042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 74.1909, Epoch Time 491.7423(480.9939), Bit/dim 6.9831(best: 7.0272), Xent 2.0505, Loss 8.0084, Error 0.6847(best: 0.6717)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0110 | Time 30.1841(31.1955) | Bit/dim 6.9600(7.3219) | Xent 2.0620(2.0952) | Loss 17.5729(20.4101) | Error 0.6975(0.7101) Steps 544(523.02) | Grad Norm 1.2913(4.0680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 74.7458, Epoch Time 499.6466(481.5535), Bit/dim 6.9380(best: 6.9831), Xent 2.0317, Loss 7.9538, Error 0.6789(best: 0.6717)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0120 | Time 32.0833(31.3609) | Bit/dim 6.9295(7.2229) | Xent 2.0353(2.0819) | Loss 17.8224(20.3734) | Error 0.6856(0.7043) Steps 514(525.32) | Grad Norm 8.1620(4.5331) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 32.4264(31.7001) | Bit/dim 6.8903(7.1391) | Xent 2.0371(2.0692) | Loss 17.6532(19.6789) | Error 0.6858(0.6992) Steps 520(527.89) | Grad Norm 4.1740(4.7046) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 73.6170, Epoch Time 512.6199(482.4855), Bit/dim 6.8826(best: 6.9380), Xent 2.0170, Loss 7.8911, Error 0.6696(best: 0.6717)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0140 | Time 33.1776(31.8046) | Bit/dim 6.8324(7.0647) | Xent 2.0217(2.0575) | Loss 17.6753(19.6714) | Error 0.6744(0.6949) Steps 532(531.99) | Grad Norm 4.1785(5.1635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 74.9519, Epoch Time 512.1664(483.3759), Bit/dim 6.8029(best: 6.8826), Xent 2.0042, Loss 7.8050, Error 0.6716(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0150 | Time 32.4838(32.0669) | Bit/dim 6.7824(6.9933) | Xent 2.2059(2.0527) | Loss 17.6688(19.6791) | Error 0.7786(0.6947) Steps 520(534.73) | Grad Norm 74.1629(8.7882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 71.5944, Epoch Time 514.7106(484.3160), Bit/dim 6.7192(best: 6.8029), Xent 2.0843, Loss 7.7613, Error 0.7543(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0160 | Time 32.8993(32.1986) | Bit/dim 6.6824(6.9217) | Xent 2.0226(2.0545) | Loss 17.0941(19.6979) | Error 0.6906(0.7029) Steps 538(535.64) | Grad Norm 13.5470(15.0395) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 74.0156, Epoch Time 514.2863(485.2151), Bit/dim 6.5982(best: 6.7192), Xent 2.0074, Loss 7.6019, Error 0.6983(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0170 | Time 30.9983(32.3113) | Bit/dim 6.5935(6.8457) | Xent 2.0222(2.0443) | Loss 40.4263(19.7366) | Error 0.7092(0.7007) Steps 538(538.98) | Grad Norm 26.0146(16.3045) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 30.0360(32.2836) | Bit/dim 6.4711(6.7615) | Xent 2.0055(2.0307) | Loss 16.7309(18.9928) | Error 0.6864(0.6929) Steps 496(539.72) | Grad Norm 40.7333(16.2845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 74.1216, Epoch Time 507.5852(485.8862), Bit/dim 6.4520(best: 6.5982), Xent 2.0367, Loss 7.4704, Error 0.7366(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0190 | Time 32.7117(32.2506) | Bit/dim 6.3325(6.6672) | Xent 1.9926(2.0275) | Loss 16.4812(18.9636) | Error 0.6858(0.6963) Steps 538(541.93) | Grad Norm 29.2004(24.2686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 74.6962, Epoch Time 511.8223(486.6643), Bit/dim 6.2311(best: 6.4520), Xent 1.9787, Loss 7.2205, Error 0.6927(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0200 | Time 33.1272(32.2672) | Bit/dim 6.2216(6.5549) | Xent 2.5203(2.0385) | Loss 16.8970(18.8918) | Error 0.8461(0.7019) Steps 544(542.88) | Grad Norm 177.3059(33.7406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 74.6559, Epoch Time 517.3222(487.5840), Bit/dim 6.0484(best: 6.2311), Xent 2.0536, Loss 7.0752, Error 0.7481(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0210 | Time 34.7920(32.6126) | Bit/dim 6.0059(6.4318) | Xent 1.9795(2.0444) | Loss 15.8501(18.8140) | Error 0.6617(0.7069) Steps 580(545.23) | Grad Norm 20.4672(39.3918) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 34.8467(33.1036) | Bit/dim 5.9804(6.3110) | Xent 2.2638(2.0426) | Loss 15.9514(18.0162) | Error 0.7939(0.7046) Steps 574(546.11) | Grad Norm 155.9217(41.7971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 75.0603, Epoch Time 541.6142(489.2049), Bit/dim 6.2904(best: 6.0484), Xent 2.4203, Loss 7.5006, Error 0.7901(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0230 | Time 31.8667(33.3014) | Bit/dim 5.9115(6.2480) | Xent 2.0428(2.0763) | Loss 15.7929(18.0763) | Error 0.7258(0.7142) Steps 550(547.49) | Grad Norm 27.4352(59.8598) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 74.4663, Epoch Time 527.3958(490.3506), Bit/dim 5.8597(best: 6.0484), Xent 2.0364, Loss 6.8779, Error 0.7102(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0240 | Time 31.7735(32.9629) | Bit/dim 5.8394(6.1501) | Xent 2.0416(2.0708) | Loss 15.5680(18.0534) | Error 0.7025(0.7166) Steps 574(549.22) | Grad Norm 22.2487(51.8474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 73.8434, Epoch Time 500.4214(490.6527), Bit/dim 5.7813(best: 5.8597), Xent 2.0069, Loss 6.7847, Error 0.6800(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0250 | Time 31.7821(32.5778) | Bit/dim 5.7595(6.0567) | Xent 2.0261(2.0590) | Loss 15.4586(18.0385) | Error 0.6942(0.7120) Steps 526(547.42) | Grad Norm 17.1100(42.6506) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 31.2223(32.2900) | Bit/dim 5.7251(5.9757) | Xent 1.9889(2.0439) | Loss 15.3162(17.3130) | Error 0.6608(0.7036) Steps 544(545.43) | Grad Norm 4.4284(33.6944) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 74.5436, Epoch Time 499.4728(490.9173), Bit/dim 5.7324(best: 5.7813), Xent 1.9723, Loss 6.7185, Error 0.6618(best: 0.6696)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0270 | Time 30.4046(32.0445) | Bit/dim 5.7170(5.9081) | Xent 1.9735(2.0268) | Loss 15.1786(17.2898) | Error 0.6600(0.6951) Steps 532(545.31) | Grad Norm 7.6739(26.3133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 72.5740, Epoch Time 495.1806(491.0452), Bit/dim 5.6931(best: 5.7324), Xent 1.9538, Loss 6.6700, Error 0.6583(best: 0.6618)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0280 | Time 32.2495(31.6884) | Bit/dim 5.6680(5.8506) | Xent 1.9394(2.0131) | Loss 14.9925(17.2882) | Error 0.6694(0.6901) Steps 568(544.83) | Grad Norm 22.4381(26.4900) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 73.2112, Epoch Time 488.0252(490.9546), Bit/dim 5.6668(best: 5.6931), Xent 1.9381, Loss 6.6359, Error 0.6539(best: 0.6583)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0290 | Time 29.5245(31.3825) | Bit/dim 5.6578(5.8035) | Xent 1.9654(2.0026) | Loss 14.9456(17.3305) | Error 0.6886(0.6872) Steps 520(540.08) | Grad Norm 44.1273(28.7143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0300 | Time 28.5742(31.4042) | Bit/dim 5.6396(5.7627) | Xent 1.9238(1.9894) | Loss 38.1448(17.4146) | Error 0.6456(0.6814) Steps 532(538.48) | Grad Norm 10.3626(27.7453) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 28.9451(31.1170) | Bit/dim 5.6140(5.7285) | Xent 1.9276(1.9766) | Loss 14.9168(16.7635) | Error 0.6622(0.6758) Steps 550(535.66) | Grad Norm 31.6811(27.4903) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 71.6156, Epoch Time 484.0637(490.9817), Bit/dim 5.6128(best: 5.6401), Xent 1.9203, Loss 6.5729, Error 0.6378(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0320 | Time 31.6534(31.1501) | Bit/dim 5.5955(5.6961) | Xent 1.9175(1.9634) | Loss 14.7517(16.8330) | Error 0.6497(0.6703) Steps 544(537.11) | Grad Norm 6.2721(23.9248) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 72.2049, Epoch Time 495.3434(491.1126), Bit/dim 5.5849(best: 5.6128), Xent 1.9097, Loss 6.5398, Error 0.6401(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0330 | Time 31.8774(31.2948) | Bit/dim 5.6173(5.6681) | Xent 2.1456(1.9644) | Loss 14.9982(16.9547) | Error 0.7697(0.6719) Steps 556(538.03) | Grad Norm 136.2434(31.6605) | Total Time 0.00(0.00)\n",
      "Epoch 0026 | Time 73.3422, Epoch Time 491.9449(491.1375), Bit/dim 5.5433(best: 5.5849), Xent 1.9267, Loss 6.5066, Error 0.6477(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0340 | Time 28.8405(31.0414) | Bit/dim 5.5551(5.6414) | Xent 1.9388(1.9623) | Loss 14.7420(17.0982) | Error 0.6683(0.6722) Steps 532(537.93) | Grad Norm 19.9573(32.2252) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 33.2697(31.0739) | Bit/dim 5.5183(5.6117) | Xent 1.9907(1.9696) | Loss 14.5912(16.4773) | Error 0.6994(0.6768) Steps 538(536.31) | Grad Norm 42.5287(35.3718) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 74.2939, Epoch Time 493.2125(491.1998), Bit/dim 5.5068(best: 5.5433), Xent 1.9608, Loss 6.4872, Error 0.6742(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0360 | Time 32.3012(31.2848) | Bit/dim 5.4622(5.5801) | Xent 1.9288(1.9634) | Loss 14.4131(16.5573) | Error 0.6594(0.6736) Steps 538(538.63) | Grad Norm 9.6910(30.4811) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 74.6287, Epoch Time 504.3179(491.5933), Bit/dim 5.4576(best: 5.5068), Xent 1.9135, Loss 6.4143, Error 0.6407(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0370 | Time 35.6143(31.5772) | Bit/dim 5.4254(5.5468) | Xent 1.9173(1.9518) | Loss 14.5558(16.6627) | Error 0.6528(0.6685) Steps 568(542.28) | Grad Norm 17.0996(26.9104) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 74.8686, Epoch Time 522.8250(492.5303), Bit/dim 5.3991(best: 5.4576), Xent 1.8965, Loss 6.3473, Error 0.6419(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0380 | Time 32.6524(32.0463) | Bit/dim 5.4097(5.5109) | Xent 1.9373(1.9434) | Loss 14.5695(16.7496) | Error 0.6753(0.6657) Steps 580(541.68) | Grad Norm 49.1429(26.5212) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 36.4531(33.0714) | Bit/dim 5.3460(5.4745) | Xent 1.9115(1.9362) | Loss 14.3503(16.1465) | Error 0.6586(0.6646) Steps 586(548.49) | Grad Norm 38.4425(28.2631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 75.9356, Epoch Time 551.0893(494.2870), Bit/dim 5.3436(best: 5.3991), Xent 1.8757, Loss 6.2814, Error 0.6274(best: 0.6376)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0400 | Time 38.4211(34.3528) | Bit/dim 5.3082(5.4360) | Xent 1.9039(1.9295) | Loss 14.4027(16.2088) | Error 0.6603(0.6625) Steps 562(557.12) | Grad Norm 37.1841(28.3894) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 77.6764, Epoch Time 584.0474(496.9799), Bit/dim 5.2948(best: 5.3436), Xent 1.8863, Loss 6.2380, Error 0.6433(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0410 | Time 38.5957(35.2978) | Bit/dim 5.2565(5.3971) | Xent 1.8846(1.9204) | Loss 14.3806(16.2985) | Error 0.6536(0.6603) Steps 586(561.38) | Grad Norm 28.6778(26.6786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 78.3921, Epoch Time 596.2206(499.9571), Bit/dim 5.2516(best: 5.2948), Xent 1.8774, Loss 6.1903, Error 0.6428(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0420 | Time 38.8617(36.1166) | Bit/dim 5.2531(5.3611) | Xent 1.8951(1.9138) | Loss 14.2599(16.4057) | Error 0.6492(0.6590) Steps 550(566.56) | Grad Norm 44.7373(27.7270) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 78.2206, Epoch Time 588.4649(502.6123), Bit/dim 5.2065(best: 5.2516), Xent 1.8742, Loss 6.1436, Error 0.6438(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0430 | Time 34.9886(36.5115) | Bit/dim 5.1878(5.3240) | Xent 1.8814(1.9099) | Loss 38.3288(16.5414) | Error 0.6511(0.6585) Steps 586(572.66) | Grad Norm 13.4492(26.8681) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 35.3043(36.5017) | Bit/dim 5.1634(5.2892) | Xent 1.8588(1.9001) | Loss 13.9660(15.8887) | Error 0.6444(0.6550) Steps 580(574.85) | Grad Norm 10.6394(23.1727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 76.8329, Epoch Time 564.9620(504.4828), Bit/dim 5.1709(best: 5.2065), Xent 1.8625, Loss 6.1022, Error 0.6424(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0450 | Time 36.5536(36.3143) | Bit/dim 5.1645(5.2615) | Xent 1.9951(1.9219) | Loss 14.1446(16.0151) | Error 0.7011(0.6642) Steps 574(574.70) | Grad Norm 44.9185(32.3203) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 79.4953, Epoch Time 559.4947(506.1332), Bit/dim 5.1484(best: 5.1709), Xent 1.9480, Loss 6.1224, Error 0.6717(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0460 | Time 35.9003(36.1612) | Bit/dim 5.1288(5.2312) | Xent 1.9312(1.9303) | Loss 14.0941(16.1403) | Error 0.6783(0.6703) Steps 592(574.64) | Grad Norm 18.6535(30.3032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 75.9423, Epoch Time 559.4008(507.7312), Bit/dim 5.1073(best: 5.1484), Xent 1.8880, Loss 6.0513, Error 0.6388(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0470 | Time 35.3913(36.0066) | Bit/dim 5.0904(5.2024) | Xent 1.9011(1.9258) | Loss 13.7614(16.2699) | Error 0.6514(0.6673) Steps 586(576.64) | Grad Norm 6.7278(25.0846) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 35.0786(35.6149) | Bit/dim 5.0640(5.1713) | Xent 1.8678(1.9152) | Loss 13.7976(15.6116) | Error 0.6397(0.6627) Steps 568(572.45) | Grad Norm 9.1904(20.5993) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 75.5996, Epoch Time 540.7789(508.7226), Bit/dim 5.0653(best: 5.1073), Xent 1.8573, Loss 5.9939, Error 0.6417(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0490 | Time 33.4088(35.3876) | Bit/dim 5.0557(5.1414) | Xent 1.8607(1.9045) | Loss 13.8036(15.6950) | Error 0.6464(0.6590) Steps 580(570.39) | Grad Norm 5.8959(17.9591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 75.9135, Epoch Time 544.2085(509.7872), Bit/dim 5.0424(best: 5.0653), Xent 1.8320, Loss 5.9583, Error 0.6198(best: 0.6274)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0500 | Time 36.1859(35.0296) | Bit/dim 5.0200(5.1131) | Xent 1.8502(1.8918) | Loss 13.7550(15.8015) | Error 0.6400(0.6554) Steps 562(570.57) | Grad Norm 19.6494(18.8612) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 76.2488, Epoch Time 536.1621(510.5784), Bit/dim 5.0637(best: 5.0424), Xent 2.1896, Loss 6.1585, Error 0.7326(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0510 | Time 34.0831(34.8843) | Bit/dim 5.0767(5.0951) | Xent 1.9464(1.9095) | Loss 13.9894(15.9728) | Error 0.6889(0.6612) Steps 580(570.40) | Grad Norm 50.4431(28.6725) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 36.0885(34.7087) | Bit/dim 5.0156(5.0834) | Xent 1.9550(1.9287) | Loss 13.8147(15.4086) | Error 0.6797(0.6712) Steps 574(571.30) | Grad Norm 14.4181(29.3474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 74.3260, Epoch Time 536.2589(511.3489), Bit/dim 5.0050(best: 5.0424), Xent 1.9271, Loss 5.9686, Error 0.6731(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0530 | Time 33.6022(34.4972) | Bit/dim 4.9627(5.0587) | Xent 1.9453(1.9317) | Loss 13.6656(15.4734) | Error 0.6867(0.6734) Steps 562(566.94) | Grad Norm 10.9191(25.1362) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 75.2909, Epoch Time 536.2858(512.0970), Bit/dim 4.9574(best: 5.0050), Xent 1.9257, Loss 5.9203, Error 0.6675(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0540 | Time 34.9467(34.5312) | Bit/dim 5.0240(5.0468) | Xent 2.0225(1.9568) | Loss 13.6412(15.6170) | Error 0.7158(0.6855) Steps 574(564.33) | Grad Norm 32.2485(26.3080) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 75.2380, Epoch Time 539.8063(512.9282), Bit/dim 4.9860(best: 4.9574), Xent 1.9238, Loss 5.9480, Error 0.6706(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0550 | Time 34.9227(34.4183) | Bit/dim 4.9890(5.0273) | Xent 1.9392(1.9571) | Loss 13.8011(15.7282) | Error 0.6856(0.6867) Steps 568(561.96) | Grad Norm 15.0487(23.8812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 73.0084, Epoch Time 532.2419(513.5077), Bit/dim 4.8961(best: 4.9574), Xent 1.8498, Loss 5.8210, Error 0.6356(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0560 | Time 34.4932(34.3762) | Bit/dim 4.8967(4.9962) | Xent 1.8859(1.9430) | Loss 36.9542(15.8462) | Error 0.6558(0.6808) Steps 532(561.08) | Grad Norm 5.8698(19.9896) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 34.4459(34.1533) | Bit/dim 4.8626(4.9645) | Xent 1.8383(1.9189) | Loss 13.1529(15.1546) | Error 0.6383(0.6713) Steps 532(555.20) | Grad Norm 3.3224(16.0480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 74.6253, Epoch Time 529.8946(513.9993), Bit/dim 4.8559(best: 4.8961), Xent 1.8088, Loss 5.7603, Error 0.6204(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0580 | Time 35.9420(34.2585) | Bit/dim 4.8340(4.9344) | Xent 1.8171(1.8937) | Loss 13.2398(15.2204) | Error 0.6306(0.6621) Steps 556(551.07) | Grad Norm 6.6196(13.2962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 74.6871, Epoch Time 540.8659(514.8053), Bit/dim 4.8587(best: 4.8559), Xent 1.9196, Loss 5.8185, Error 0.6905(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0590 | Time 33.8122(34.2966) | Bit/dim 5.0059(4.9344) | Xent 2.0382(1.9132) | Loss 13.6620(15.3818) | Error 0.7589(0.6708) Steps 544(552.30) | Grad Norm 40.7735(20.6740) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 76.2196, Epoch Time 545.2540(515.7187), Bit/dim 4.8742(best: 4.8559), Xent 1.8374, Loss 5.7928, Error 0.6332(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0600 | Time 35.5669(34.4384) | Bit/dim 4.9065(4.9218) | Xent 1.9030(1.9118) | Loss 13.3910(15.5477) | Error 0.6681(0.6706) Steps 562(553.85) | Grad Norm 20.3030(21.2350) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 34.4821(34.3012) | Bit/dim 4.7885(4.8956) | Xent 1.8405(1.8973) | Loss 13.1377(14.9275) | Error 0.6306(0.6644) Steps 532(548.70) | Grad Norm 12.7472(18.8236) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 72.4461, Epoch Time 532.6290(516.2260), Bit/dim 4.7940(best: 4.8559), Xent 1.7975, Loss 5.6927, Error 0.6182(best: 0.6198)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0620 | Time 37.6095(34.5119) | Bit/dim 4.7772(4.8658) | Xent 1.7747(1.8744) | Loss 13.1534(14.9901) | Error 0.6122(0.6560) Steps 562(545.00) | Grad Norm 2.8976(15.8088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 72.7935, Epoch Time 541.1406(516.9735), Bit/dim 4.7564(best: 4.7940), Xent 1.7494, Loss 5.6311, Error 0.6024(best: 0.6182)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0630 | Time 34.8756(34.4730) | Bit/dim 4.7568(4.8374) | Xent 1.7846(1.8501) | Loss 12.9817(15.0451) | Error 0.6283(0.6469) Steps 568(544.91) | Grad Norm 18.5043(13.9601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 74.8791, Epoch Time 552.4686(518.0383), Bit/dim 4.7260(best: 4.7564), Xent 1.7409, Loss 5.5965, Error 0.6111(best: 0.6024)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0640 | Time 33.3924(34.8189) | Bit/dim 4.7407(4.8094) | Xent 1.9037(1.8366) | Loss 13.1060(15.1515) | Error 0.6653(0.6435) Steps 568(546.46) | Grad Norm 44.3048(16.3039) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 37.7602(35.4385) | Bit/dim 4.7286(4.7848) | Xent 1.7832(1.8211) | Loss 12.9188(14.5590) | Error 0.6278(0.6388) Steps 550(549.39) | Grad Norm 17.6050(18.2284) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 74.5921, Epoch Time 566.8842(519.5037), Bit/dim 4.7107(best: 4.7260), Xent 1.7391, Loss 5.5803, Error 0.6003(best: 0.6024)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0660 | Time 37.1834(35.7094) | Bit/dim 4.7031(4.7626) | Xent 1.7777(1.8092) | Loss 13.0235(14.6429) | Error 0.6236(0.6352) Steps 574(551.27) | Grad Norm 18.1508(20.0538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 74.8323, Epoch Time 566.4965(520.9135), Bit/dim 4.6785(best: 4.7107), Xent 1.7055, Loss 5.5313, Error 0.5875(best: 0.6003)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0670 | Time 34.8343(35.8422) | Bit/dim 4.6826(4.7394) | Xent 1.7353(1.7927) | Loss 12.6287(14.6982) | Error 0.6019(0.6301) Steps 550(551.95) | Grad Norm 18.3902(19.1208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 78.4468, Epoch Time 567.4788(522.3104), Bit/dim 4.6876(best: 4.6785), Xent 1.7428, Loss 5.5590, Error 0.6055(best: 0.5875)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0680 | Time 35.0588(36.0416) | Bit/dim 4.7194(4.7239) | Xent 1.8694(1.7905) | Loss 13.0048(14.8637) | Error 0.6489(0.6300) Steps 562(557.27) | Grad Norm 62.6446(22.8844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 76.2978, Epoch Time 568.1658(523.6861), Bit/dim 4.6556(best: 4.6785), Xent 1.7313, Loss 5.5213, Error 0.6053(best: 0.5875)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0690 | Time 39.0092(36.2424) | Bit/dim 4.6542(4.7116) | Xent 1.7702(1.7853) | Loss 36.2702(15.0349) | Error 0.6094(0.6283) Steps 550(561.94) | Grad Norm 19.1595(24.2504) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 35.5832(36.2264) | Bit/dim 4.6207(4.6946) | Xent 1.7108(1.7718) | Loss 12.6993(14.4401) | Error 0.6172(0.6244) Steps 574(563.67) | Grad Norm 7.2989(21.9404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 77.9850, Epoch Time 571.1354(525.1096), Bit/dim 4.6302(best: 4.6556), Xent 1.6631, Loss 5.4618, Error 0.5831(best: 0.5875)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0710 | Time 38.9805(36.4377) | Bit/dim 4.7620(4.6839) | Xent 1.8095(1.7655) | Loss 13.1069(14.5655) | Error 0.6461(0.6225) Steps 556(564.42) | Grad Norm 52.3877(22.9734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 77.3851, Epoch Time 567.8678(526.3923), Bit/dim 4.8546(best: 4.6302), Xent 1.8593, Loss 5.7843, Error 0.6609(best: 0.5831)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0720 | Time 37.2123(36.2624) | Bit/dim 4.7584(4.7138) | Xent 2.0223(1.8177) | Loss 13.3070(14.8157) | Error 0.7153(0.6419) Steps 556(566.71) | Grad Norm 35.1693(28.0076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 76.4404, Epoch Time 553.1215(527.1942), Bit/dim 4.6731(best: 4.6302), Xent 1.7825, Loss 5.5643, Error 0.6374(best: 0.5831)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0730 | Time 33.3334(35.9153) | Bit/dim 4.6568(4.7113) | Xent 1.7915(1.8227) | Loss 12.7460(15.0197) | Error 0.6317(0.6445) Steps 580(564.45) | Grad Norm 7.2518(24.2729) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 37.1361(35.8760) | Bit/dim 4.6240(4.6908) | Xent 1.7205(1.8063) | Loss 12.6862(14.4020) | Error 0.6058(0.6388) Steps 592(565.20) | Grad Norm 6.3543(20.6098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 74.5051, Epoch Time 553.2113(527.9747), Bit/dim 4.6136(best: 4.6302), Xent 1.6664, Loss 5.4468, Error 0.5910(best: 0.5831)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0750 | Time 35.3507(35.8382) | Bit/dim 4.6120(4.6682) | Xent 1.8196(1.7878) | Loss 12.6180(14.4819) | Error 0.6378(0.6325) Steps 556(562.89) | Grad Norm 41.2490(20.6290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 75.4411, Epoch Time 554.3940(528.7673), Bit/dim 4.5887(best: 4.6136), Xent 1.7821, Loss 5.4797, Error 0.6302(best: 0.5831)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0760 | Time 35.3176(35.9206) | Bit/dim 4.5731(4.6444) | Xent 1.7435(1.7806) | Loss 12.3320(14.6026) | Error 0.6167(0.6307) Steps 568(565.27) | Grad Norm 15.7110(20.8807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 77.3848, Epoch Time 570.7520(530.0268), Bit/dim 4.5562(best: 4.5887), Xent 1.6359, Loss 5.3741, Error 0.5760(best: 0.5831)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0770 | Time 37.8729(36.1342) | Bit/dim 4.5477(4.6221) | Xent 1.6354(1.7584) | Loss 12.3565(14.7281) | Error 0.5844(0.6232) Steps 538(565.48) | Grad Norm 8.5075(18.1236) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 36.8996(36.3893) | Bit/dim 4.6246(4.6039) | Xent 1.6580(1.7354) | Loss 12.6850(14.1453) | Error 0.5931(0.6155) Steps 574(567.55) | Grad Norm 24.3070(16.1526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 80.0108, Epoch Time 577.5631(531.4529), Bit/dim 4.6595(best: 4.5562), Xent 1.5984, Loss 5.4587, Error 0.5695(best: 0.5760)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0790 | Time 37.7899(36.6176) | Bit/dim 4.5315(4.5906) | Xent 1.6185(1.7117) | Loss 12.3858(14.2511) | Error 0.5756(0.6082) Steps 592(568.59) | Grad Norm 7.4469(14.6457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 79.5339, Epoch Time 580.3190(532.9189), Bit/dim 4.5271(best: 4.5562), Xent 1.5819, Loss 5.3180, Error 0.5622(best: 0.5695)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0800 | Time 35.1776(36.7394) | Bit/dim 4.5245(4.5734) | Xent 1.6391(1.6917) | Loss 12.2975(14.3827) | Error 0.5750(0.6022) Steps 562(569.21) | Grad Norm 21.2045(13.0188) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 76.4957, Epoch Time 573.2818(534.1298), Bit/dim 4.5067(best: 4.5271), Xent 1.5850, Loss 5.2992, Error 0.5693(best: 0.5622)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0810 | Time 36.6773(36.7789) | Bit/dim 4.4888(4.5561) | Xent 1.6574(1.6871) | Loss 12.1864(14.4631) | Error 0.5931(0.6014) Steps 562(567.30) | Grad Norm 16.1155(15.0102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 78.5589, Epoch Time 577.5809(535.4333), Bit/dim 4.5018(best: 4.5067), Xent 1.6158, Loss 5.3096, Error 0.5698(best: 0.5622)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0820 | Time 36.3042(36.8772) | Bit/dim 4.5091(4.5412) | Xent 1.6558(1.6865) | Loss 36.2477(14.6117) | Error 0.5825(0.6019) Steps 574(566.52) | Grad Norm 24.5938(18.0388) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 40.5701(37.0129) | Bit/dim 4.4909(4.5246) | Xent 1.6268(1.6764) | Loss 12.3162(14.0042) | Error 0.5831(0.5987) Steps 628(572.08) | Grad Norm 19.1551(18.8204) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 75.2474, Epoch Time 575.3242(536.6301), Bit/dim 4.4969(best: 4.5018), Xent 1.5547, Loss 5.2742, Error 0.5516(best: 0.5622)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0840 | Time 38.3483(37.1160) | Bit/dim 4.4585(4.5123) | Xent 1.6635(1.6693) | Loss 12.1856(14.1153) | Error 0.5881(0.5959) Steps 586(573.73) | Grad Norm 34.3277(21.0538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 76.3158, Epoch Time 578.9343(537.8992), Bit/dim 4.4680(best: 4.4969), Xent 1.5776, Loss 5.2568, Error 0.5595(best: 0.5516)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0850 | Time 35.1257(37.0973) | Bit/dim 4.4450(4.4985) | Xent 1.6397(1.6592) | Loss 12.2399(14.2705) | Error 0.5789(0.5927) Steps 610(575.98) | Grad Norm 19.3450(20.0187) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 76.9012, Epoch Time 579.5902(539.1499), Bit/dim 4.4370(best: 4.4680), Xent 1.5392, Loss 5.2066, Error 0.5495(best: 0.5516)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0860 | Time 37.8712(37.2607) | Bit/dim 4.4338(4.4842) | Xent 1.5894(1.6455) | Loss 12.0830(14.4474) | Error 0.5792(0.5882) Steps 598(579.65) | Grad Norm 18.2714(19.0083) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 35.5968(37.3660) | Bit/dim 4.4164(4.4704) | Xent 1.6294(1.6337) | Loss 12.1894(13.8499) | Error 0.5803(0.5844) Steps 592(584.84) | Grad Norm 20.1011(19.0650) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 80.2415, Epoch Time 587.1716(540.5906), Bit/dim 4.4201(best: 4.4370), Xent 1.5421, Loss 5.1911, Error 0.5516(best: 0.5495)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0880 | Time 39.6449(37.6991) | Bit/dim 4.4068(4.4556) | Xent 1.6166(1.6230) | Loss 12.0896(13.9614) | Error 0.5828(0.5808) Steps 610(587.88) | Grad Norm 28.0188(19.1268) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 77.7516, Epoch Time 597.2059(542.2890), Bit/dim 4.4238(best: 4.4201), Xent 1.5795, Loss 5.2136, Error 0.5685(best: 0.5495)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0890 | Time 36.8852(37.8829) | Bit/dim 4.4260(4.4465) | Xent 1.5588(1.6218) | Loss 11.9138(14.1210) | Error 0.5592(0.5806) Steps 580(590.51) | Grad Norm 18.7140(19.3918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 76.8454, Epoch Time 593.8156(543.8348), Bit/dim 4.3892(best: 4.4201), Xent 1.5342, Loss 5.1563, Error 0.5498(best: 0.5495)\n",
      "Iter 0900 | Time 39.0903(38.0877) | Bit/dim 4.3723(4.4323) | Xent 1.6276(1.6136) | Loss 12.0780(14.2422) | Error 0.5872(0.5783) Steps 610(594.46) | Grad Norm 30.2498(18.5994) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 38.8853(38.3651) | Bit/dim 4.4483(4.4272) | Xent 1.6416(1.6197) | Loss 12.1879(13.6876) | Error 0.5897(0.5798) Steps 616(594.86) | Grad Norm 34.5979(21.6709) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 78.4178, Epoch Time 602.7537(545.6024), Bit/dim 4.3787(best: 4.3892), Xent 1.5173, Loss 5.1374, Error 0.5409(best: 0.5495)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0920 | Time 39.3363(38.2578) | Bit/dim 4.3640(4.4158) | Xent 1.6592(1.6129) | Loss 11.8808(13.8074) | Error 0.5861(0.5783) Steps 502(592.68) | Grad Norm 27.8786(20.4551) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 79.6478, Epoch Time 587.9020(546.8714), Bit/dim 4.9036(best: 4.3787), Xent 2.2035, Loss 6.0053, Error 0.7710(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0930 | Time 38.3669(38.1685) | Bit/dim 4.8881(4.5380) | Xent 2.2311(1.8017) | Loss 13.7178(14.4287) | Error 0.7897(0.6251) Steps 604(594.59) | Grad Norm 20.4963(30.0187) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 78.9900, Epoch Time 591.6618(548.2151), Bit/dim 4.7896(best: 4.3787), Xent 1.9704, Loss 5.7748, Error 0.6751(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0940 | Time 38.4453(38.3056) | Bit/dim 4.6845(4.5992) | Xent 2.0204(1.8668) | Loss 13.1250(14.8372) | Error 0.7069(0.6509) Steps 604(599.08) | Grad Norm 14.1350(26.7882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 72.3180, Epoch Time 590.7992(549.4926), Bit/dim 4.5292(best: 4.3787), Xent 1.8644, Loss 5.4614, Error 0.6662(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0950 | Time 37.6387(38.3019) | Bit/dim 4.5339(4.5936) | Xent 1.9206(1.8951) | Loss 34.7888(14.9546) | Error 0.6811(0.6637) Steps 574(589.45) | Grad Norm 7.7259(22.5690) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 37.8793(37.9370) | Bit/dim 4.4594(4.5632) | Xent 1.8243(1.8865) | Loss 12.3184(14.2961) | Error 0.6517(0.6632) Steps 562(584.10) | Grad Norm 4.6589(18.3534) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 73.5330, Epoch Time 572.9363(550.1959), Bit/dim 4.4447(best: 4.3787), Xent 1.7655, Loss 5.3274, Error 0.6217(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0970 | Time 38.9893(37.9944) | Bit/dim 4.4062(4.5280) | Xent 1.7796(1.8627) | Loss 12.2723(14.3026) | Error 0.6358(0.6556) Steps 526(571.71) | Grad Norm 7.1445(14.9667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 73.4864, Epoch Time 577.7775(551.0234), Bit/dim 4.3946(best: 4.3787), Xent 1.6899, Loss 5.2395, Error 0.5991(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0980 | Time 37.5831(37.8459) | Bit/dim 4.3794(4.4926) | Xent 1.7225(1.8299) | Loss 12.1214(14.3557) | Error 0.6092(0.6446) Steps 562(565.61) | Grad Norm 6.5214(12.5663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 73.8620, Epoch Time 577.5604(551.8195), Bit/dim 4.3699(best: 4.3787), Xent 1.6648, Loss 5.2022, Error 0.5934(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0990 | Time 37.4502(37.7497) | Bit/dim 4.3652(4.4605) | Xent 1.7829(1.7992) | Loss 12.1730(14.4350) | Error 0.6289(0.6360) Steps 568(563.54) | Grad Norm 34.2159(12.4351) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 36.5001(37.6022) | Bit/dim 4.3494(4.4322) | Xent 1.6849(1.7704) | Loss 11.9388(13.7940) | Error 0.6036(0.6271) Steps 556(560.01) | Grad Norm 12.1846(12.6330) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 76.0800, Epoch Time 578.8203(552.6295), Bit/dim 4.3429(best: 4.3699), Xent 1.6173, Loss 5.1515, Error 0.5747(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1010 | Time 38.0737(37.5334) | Bit/dim 4.3082(4.4046) | Xent 1.6245(1.7375) | Loss 11.7249(13.8615) | Error 0.5806(0.6162) Steps 574(559.44) | Grad Norm 5.7829(11.5404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 78.6072, Epoch Time 585.6432(553.6199), Bit/dim 4.3135(best: 4.3429), Xent 1.5767, Loss 5.1018, Error 0.5658(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1020 | Time 38.0332(38.0177) | Bit/dim 4.3020(4.3794) | Xent 1.5942(1.7094) | Loss 11.9722(13.9645) | Error 0.5769(0.6074) Steps 592(567.93) | Grad Norm 8.6709(11.5787) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 80.0803, Epoch Time 620.4232(555.6240), Bit/dim 4.2974(best: 4.3135), Xent 1.5785, Loss 5.0866, Error 0.5670(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1030 | Time 37.4482(38.5351) | Bit/dim 4.3049(4.3579) | Xent 1.5949(1.6847) | Loss 11.8250(14.1158) | Error 0.5625(0.5986) Steps 580(579.23) | Grad Norm 16.1225(11.9014) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 42.5155(39.0157) | Bit/dim 4.2868(4.3389) | Xent 1.7133(1.6712) | Loss 11.9956(13.5297) | Error 0.6153(0.5945) Steps 604(586.08) | Grad Norm 31.4284(13.5528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1050 | Time 40.3275(39.0595) | Bit/dim 4.2658(4.3209) | Xent 1.5845(1.6534) | Loss 11.7115(13.6226) | Error 0.5781(0.5897) Steps 556(588.70) | Grad Norm 11.4356(13.7524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 80.9502, Epoch Time 603.6991(558.8228), Bit/dim 4.2702(best: 4.2771), Xent 1.5321, Loss 5.0363, Error 0.5487(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1060 | Time 40.1194(38.9820) | Bit/dim 4.2543(4.3045) | Xent 1.5763(1.6379) | Loss 11.8139(13.7523) | Error 0.5717(0.5855) Steps 604(588.95) | Grad Norm 13.5180(13.4419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 82.5252, Epoch Time 613.1166(560.4517), Bit/dim 4.2455(best: 4.2702), Xent 1.5176, Loss 5.0044, Error 0.5476(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1070 | Time 39.1088(39.2272) | Bit/dim 4.2483(4.2879) | Xent 1.6091(1.6210) | Loss 11.8374(13.9055) | Error 0.5789(0.5798) Steps 592(591.27) | Grad Norm 14.6208(13.7492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 83.0484, Epoch Time 617.4747(562.1624), Bit/dim 4.2266(best: 4.2455), Xent 1.5165, Loss 4.9849, Error 0.5461(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1080 | Time 41.8157(39.4667) | Bit/dim 4.2076(4.2711) | Xent 1.5208(1.6053) | Loss 36.2600(14.0598) | Error 0.5617(0.5753) Steps 646(594.52) | Grad Norm 20.2071(13.7763) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 39.7448(39.6416) | Bit/dim 4.2641(4.2648) | Xent 1.5974(1.6010) | Loss 11.6728(13.4510) | Error 0.5725(0.5734) Steps 592(596.60) | Grad Norm 20.5644(16.0503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 79.7263, Epoch Time 619.3338(563.8775), Bit/dim 4.2247(best: 4.2266), Xent 1.5313, Loss 4.9903, Error 0.5520(best: 0.5409)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1100 | Time 40.4272(39.7404) | Bit/dim 4.2197(4.2541) | Xent 1.5410(1.5945) | Loss 11.4817(13.5430) | Error 0.5528(0.5718) Steps 592(595.90) | Grad Norm 8.9911(16.0080) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1110 | Time 38.3885(39.7462) | Bit/dim 4.2019(4.2422) | Xent 1.5109(1.5767) | Loss 11.6219(13.6819) | Error 0.5467(0.5658) Steps 586(597.29) | Grad Norm 16.6356(14.9843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 80.3092, Epoch Time 609.6543(566.6411), Bit/dim 4.1951(best: 4.2087), Xent 1.6148, Loss 5.0025, Error 0.5765(best: 0.5360)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1120 | Time 40.2161(39.6626) | Bit/dim 4.2084(4.2292) | Xent 1.6260(1.5718) | Loss 11.7241(13.8453) | Error 0.5700(0.5645) Steps 580(595.81) | Grad Norm 39.3421(16.0375) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 39.1998(39.4737) | Bit/dim 4.1691(4.2163) | Xent 1.5426(1.5671) | Loss 11.5660(13.2560) | Error 0.5664(0.5629) Steps 556(594.93) | Grad Norm 6.3151(15.5764) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 79.6740, Epoch Time 606.1032(567.8249), Bit/dim 4.1771(best: 4.1951), Xent 1.4685, Loss 4.9114, Error 0.5326(best: 0.5360)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1140 | Time 39.5556(39.5538) | Bit/dim 4.1809(4.2041) | Xent 1.5280(1.5528) | Loss 11.4923(13.3696) | Error 0.5519(0.5585) Steps 580(592.69) | Grad Norm 13.0848(14.0655) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 80.3598, Epoch Time 617.5397(569.3164), Bit/dim 4.1602(best: 4.1771), Xent 1.5023, Loss 4.9114, Error 0.5491(best: 0.5326)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1150 | Time 41.5511(39.7216) | Bit/dim 4.1623(4.1929) | Xent 1.4794(1.5438) | Loss 11.3356(13.4706) | Error 0.5461(0.5554) Steps 562(590.24) | Grad Norm 17.0450(14.6496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 79.6428, Epoch Time 614.9578(570.6856), Bit/dim 4.1599(best: 4.1602), Xent 1.6655, Loss 4.9926, Error 0.5865(best: 0.5326)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1160 | Time 41.5770(39.7606) | Bit/dim 4.1642(4.1825) | Xent 1.5265(1.5501) | Loss 11.5077(13.6645) | Error 0.5525(0.5571) Steps 604(594.47) | Grad Norm 17.2795(16.4544) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 38.4538(39.7693) | Bit/dim 4.1347(4.1764) | Xent 1.5056(1.5508) | Loss 11.2601(13.0961) | Error 0.5442(0.5580) Steps 580(594.90) | Grad Norm 9.1963(15.8103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 77.9683, Epoch Time 613.3475(571.9655), Bit/dim 4.1550(best: 4.1599), Xent 1.4691, Loss 4.8896, Error 0.5337(best: 0.5326)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1180 | Time 38.2436(39.8066) | Bit/dim 4.1325(4.1667) | Xent 1.5087(1.5392) | Loss 11.3193(13.1965) | Error 0.5447(0.5547) Steps 598(593.00) | Grad Norm 6.7175(14.1870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 80.0641, Epoch Time 617.2178(573.3230), Bit/dim 4.1279(best: 4.1550), Xent 1.4380, Loss 4.8469, Error 0.5235(best: 0.5326)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1190 | Time 36.8577(39.7613) | Bit/dim 4.1080(4.1558) | Xent 1.4748(1.5239) | Loss 11.2584(13.3051) | Error 0.5292(0.5497) Steps 580(591.56) | Grad Norm 12.4152(13.5935) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 77.2964, Epoch Time 607.8422(574.3586), Bit/dim 4.1122(best: 4.1279), Xent 1.3961, Loss 4.8102, Error 0.5076(best: 0.5235)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1200 | Time 38.3029(39.7819) | Bit/dim 4.1206(4.1457) | Xent 1.4485(1.5076) | Loss 11.2936(13.4374) | Error 0.5183(0.5441) Steps 580(590.66) | Grad Norm 7.1617(13.1745) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 77.6572, Epoch Time 607.6372(575.3570), Bit/dim 4.1219(best: 4.1122), Xent 1.4305, Loss 4.8372, Error 0.5151(best: 0.5076)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1210 | Time 39.9010(39.6597) | Bit/dim 4.1175(4.1353) | Xent 1.4954(1.4966) | Loss 34.8021(13.5804) | Error 0.5414(0.5405) Steps 586(590.33) | Grad Norm 26.7102(13.5959) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 41.3054(39.8109) | Bit/dim 4.1119(4.1313) | Xent 1.4651(1.4858) | Loss 11.2851(12.9780) | Error 0.5311(0.5376) Steps 640(591.75) | Grad Norm 9.7059(13.9139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 77.9778, Epoch Time 613.2092(576.4925), Bit/dim 4.1006(best: 4.1122), Xent 1.3767, Loss 4.7890, Error 0.4988(best: 0.5076)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1230 | Time 40.0028(39.7127) | Bit/dim 4.0840(4.1209) | Xent 1.4808(1.4760) | Loss 11.3444(13.0870) | Error 0.5344(0.5339) Steps 598(592.01) | Grad Norm 24.8363(13.2143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 79.1690, Epoch Time 609.9316(577.4957), Bit/dim 4.1086(best: 4.1006), Xent 1.4980, Loss 4.8576, Error 0.5450(best: 0.4988)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1240 | Time 41.4218(39.8537) | Bit/dim 4.0855(4.1153) | Xent 1.5406(1.4909) | Loss 11.4389(13.2600) | Error 0.5647(0.5378) Steps 592(593.39) | Grad Norm 21.5172(15.8873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 76.9659, Epoch Time 622.6070(578.8490), Bit/dim 4.0862(best: 4.1006), Xent 1.3813, Loss 4.7769, Error 0.5069(best: 0.4988)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1250 | Time 40.8034(40.0745) | Bit/dim 4.0987(4.1092) | Xent 1.4459(1.4818) | Loss 11.1966(13.4485) | Error 0.5181(0.5353) Steps 556(590.20) | Grad Norm 13.6188(15.0849) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 39.5745(39.9883) | Bit/dim 4.0630(4.0998) | Xent 1.4213(1.4664) | Loss 11.0236(12.8576) | Error 0.5197(0.5298) Steps 586(586.78) | Grad Norm 13.2177(13.8706) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 78.4828, Epoch Time 615.9759(579.9628), Bit/dim 4.0673(best: 4.0862), Xent 1.3679, Loss 4.7512, Error 0.5008(best: 0.4988)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1270 | Time 41.1685(39.8890) | Bit/dim 4.0623(4.0912) | Xent 1.3870(1.4491) | Loss 11.2071(12.9799) | Error 0.5092(0.5233) Steps 580(584.06) | Grad Norm 12.3254(12.8986) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 76.1145, Epoch Time 608.4250(580.8167), Bit/dim 4.0736(best: 4.0673), Xent 1.3802, Loss 4.7637, Error 0.4991(best: 0.4988)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1280 | Time 41.4723(40.1692) | Bit/dim 4.0336(4.0834) | Xent 1.3749(1.4420) | Loss 10.9689(13.1106) | Error 0.4858(0.5209) Steps 562(588.75) | Grad Norm 11.8362(13.8268) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 77.8487, Epoch Time 630.4663(582.3062), Bit/dim 4.0504(best: 4.0673), Xent 1.3641, Loss 4.7325, Error 0.4939(best: 0.4988)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1290 | Time 38.8497(40.4163) | Bit/dim 4.0520(4.0750) | Xent 1.4028(1.4403) | Loss 11.2846(13.2768) | Error 0.4972(0.5200) Steps 604(589.29) | Grad Norm 13.0143(14.2032) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 40.9724(40.3686) | Bit/dim 4.0574(4.0671) | Xent 1.3845(1.4272) | Loss 11.0097(12.6929) | Error 0.5033(0.5164) Steps 568(589.19) | Grad Norm 10.0968(12.6225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 78.8086, Epoch Time 621.2197(583.4736), Bit/dim 4.0428(best: 4.0504), Xent 1.3347, Loss 4.7101, Error 0.4836(best: 0.4939)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1310 | Time 39.7139(40.4043) | Bit/dim 4.0444(4.0622) | Xent 1.3919(1.4250) | Loss 11.0084(12.8068) | Error 0.5111(0.5144) Steps 568(589.65) | Grad Norm 5.7348(14.4309) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 80.9837, Epoch Time 624.1626(584.6943), Bit/dim 4.0471(best: 4.0428), Xent 1.3603, Loss 4.7272, Error 0.4909(best: 0.4836)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1320 | Time 41.3131(40.4382) | Bit/dim 4.0466(4.0590) | Xent 1.3808(1.4231) | Loss 11.0113(12.9674) | Error 0.5053(0.5137) Steps 610(590.37) | Grad Norm 11.3126(14.6711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 77.0704, Epoch Time 616.5279(585.6493), Bit/dim 4.0264(best: 4.0428), Xent 1.3219, Loss 4.6873, Error 0.4829(best: 0.4836)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1330 | Time 39.8933(40.2968) | Bit/dim 4.0383(4.0547) | Xent 1.3636(1.4084) | Loss 11.0673(13.1210) | Error 0.4828(0.5081) Steps 580(593.01) | Grad Norm 6.7547(13.6260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 77.1946, Epoch Time 608.5295(586.3357), Bit/dim 4.0232(best: 4.0264), Xent 1.3205, Loss 4.6835, Error 0.4828(best: 0.4829)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1340 | Time 38.5318(40.0471) | Bit/dim 3.9975(4.0461) | Xent 1.3869(1.4011) | Loss 34.0948(13.2428) | Error 0.4964(0.5053) Steps 568(589.31) | Grad Norm 16.4155(12.9809) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 42.8190(40.3035) | Bit/dim 4.0137(4.0405) | Xent 1.3778(1.3916) | Loss 11.0774(12.6504) | Error 0.4969(0.5022) Steps 580(587.66) | Grad Norm 15.4441(13.5242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 79.0640, Epoch Time 621.5639(587.3925), Bit/dim 4.0394(best: 4.0232), Xent 1.3929, Loss 4.7358, Error 0.5027(best: 0.4828)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1360 | Time 40.1764(40.2156) | Bit/dim 4.0288(4.0371) | Xent 1.3483(1.3899) | Loss 11.0970(12.7952) | Error 0.4911(0.5010) Steps 586(585.51) | Grad Norm 16.5025(14.6996) | Total Time 0.00(0.00)\n",
      "Epoch 0105 | Time 77.0911, Epoch Time 619.5084(588.3560), Bit/dim 4.0167(best: 4.0232), Xent 1.3123, Loss 4.6729, Error 0.4734(best: 0.4828)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1370 | Time 39.5223(40.1178) | Bit/dim 4.0098(4.0324) | Xent 1.3486(1.3874) | Loss 11.0006(12.9805) | Error 0.4858(0.5005) Steps 568(586.09) | Grad Norm 8.6787(14.4102) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 80.0236, Epoch Time 611.3193(589.0449), Bit/dim 4.0014(best: 4.0167), Xent 1.2913, Loss 4.6471, Error 0.4684(best: 0.4734)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1380 | Time 38.3000(40.0719) | Bit/dim 4.0047(4.0251) | Xent 1.3132(1.3749) | Loss 10.9005(13.1468) | Error 0.4714(0.4966) Steps 592(590.70) | Grad Norm 6.8875(12.5158) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 37.8091(39.8780) | Bit/dim 3.9923(4.0179) | Xent 1.3677(1.3663) | Loss 10.8702(12.5664) | Error 0.4925(0.4927) Steps 568(588.56) | Grad Norm 14.1635(12.4017) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 78.8517, Epoch Time 606.5619(589.5704), Bit/dim 3.9935(best: 4.0014), Xent 1.2707, Loss 4.6288, Error 0.4591(best: 0.4684)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1400 | Time 42.1489(39.9200) | Bit/dim 3.9966(4.0114) | Xent 1.3067(1.3563) | Loss 10.9038(12.6935) | Error 0.4772(0.4893) Steps 634(590.26) | Grad Norm 6.6438(11.8852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 79.5649, Epoch Time 616.3546(590.3739), Bit/dim 3.9893(best: 3.9935), Xent 1.2870, Loss 4.6328, Error 0.4668(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1410 | Time 38.4299(39.8996) | Bit/dim 4.0578(4.0070) | Xent 1.7149(1.3695) | Loss 11.4319(12.8803) | Error 0.6019(0.4927) Steps 580(588.13) | Grad Norm 38.9348(14.0530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 78.8722, Epoch Time 616.3396(591.1529), Bit/dim 4.0070(best: 3.9893), Xent 1.3805, Loss 4.6972, Error 0.4966(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1420 | Time 38.8425(39.9432) | Bit/dim 3.9980(4.0099) | Xent 1.4412(1.3918) | Loss 11.1118(13.0775) | Error 0.5183(0.5014) Steps 598(590.14) | Grad Norm 10.1064(14.4824) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 39.7252(40.0318) | Bit/dim 3.9764(4.0057) | Xent 1.3753(1.3927) | Loss 11.0160(12.5325) | Error 0.4994(0.5019) Steps 610(591.23) | Grad Norm 7.3906(13.3656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 78.8588, Epoch Time 618.3375(591.9685), Bit/dim 3.9840(best: 3.9893), Xent 1.3152, Loss 4.6416, Error 0.4750(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1440 | Time 38.2128(39.9691) | Bit/dim 3.9723(3.9982) | Xent 1.3453(1.3814) | Loss 10.8616(12.6205) | Error 0.4894(0.4977) Steps 592(589.10) | Grad Norm 2.4417(11.2841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 79.9866, Epoch Time 617.7361(592.7415), Bit/dim 3.9706(best: 3.9840), Xent 1.2788, Loss 4.6100, Error 0.4633(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1450 | Time 40.6857(39.9999) | Bit/dim 3.9717(3.9896) | Xent 1.3105(1.3637) | Loss 10.8699(12.7012) | Error 0.4728(0.4917) Steps 586(586.79) | Grad Norm 5.8761(10.0648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 78.3838, Epoch Time 609.4990(593.2442), Bit/dim 3.9617(best: 3.9706), Xent 1.2606, Loss 4.5920, Error 0.4591(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1460 | Time 38.3186(39.7543) | Bit/dim 3.9502(3.9832) | Xent 1.2978(1.3495) | Loss 10.6968(12.8375) | Error 0.4725(0.4868) Steps 556(584.49) | Grad Norm 7.3993(9.4556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 78.6415, Epoch Time 608.5302(593.7028), Bit/dim 3.9614(best: 3.9617), Xent 1.2839, Loss 4.6034, Error 0.4716(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1470 | Time 38.0715(39.7035) | Bit/dim 3.9669(3.9770) | Xent 1.3243(1.3374) | Loss 33.8005(12.9913) | Error 0.4744(0.4817) Steps 574(584.37) | Grad Norm 16.0096(9.7141) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 38.1251(39.5599) | Bit/dim 3.9993(3.9750) | Xent 1.3431(1.3407) | Loss 10.7424(12.4260) | Error 0.4767(0.4814) Steps 568(584.20) | Grad Norm 20.3085(12.0305) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 79.9527, Epoch Time 606.1488(594.0762), Bit/dim 3.9635(best: 3.9614), Xent 1.2975, Loss 4.6123, Error 0.4734(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1490 | Time 40.3974(39.6270) | Bit/dim 3.9501(3.9699) | Xent 1.3404(1.3399) | Loss 10.8430(12.5797) | Error 0.4722(0.4812) Steps 568(586.20) | Grad Norm 24.9404(12.5045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 77.2952, Epoch Time 614.1069(594.6771), Bit/dim 3.9612(best: 3.9614), Xent 1.2559, Loss 4.5892, Error 0.4538(best: 0.4591)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1500 | Time 43.8796(39.9844) | Bit/dim 3.9646(3.9683) | Xent 1.2801(1.3283) | Loss 10.9021(12.7454) | Error 0.4706(0.4778) Steps 580(584.36) | Grad Norm 15.0588(12.4254) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 79.7076, Epoch Time 623.0877(595.5294), Bit/dim 3.9435(best: 3.9612), Xent 1.2286, Loss 4.5578, Error 0.4442(best: 0.4538)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1510 | Time 39.4735(39.9482) | Bit/dim 3.9604(3.9639) | Xent 1.2449(1.3170) | Loss 10.6802(12.9194) | Error 0.4503(0.4740) Steps 610(585.91) | Grad Norm 7.3893(11.7200) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 38.0806(39.8305) | Bit/dim 3.9500(3.9590) | Xent 1.2789(1.3052) | Loss 10.6919(12.3410) | Error 0.4528(0.4695) Steps 580(584.36) | Grad Norm 17.6047(11.3860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 77.2835, Epoch Time 608.7421(595.9258), Bit/dim 3.9480(best: 3.9435), Xent 1.2866, Loss 4.5912, Error 0.4636(best: 0.4442)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1530 | Time 40.3984(39.9357) | Bit/dim 3.9501(3.9566) | Xent 1.2744(1.3020) | Loss 10.7832(12.4876) | Error 0.4547(0.4684) Steps 580(584.72) | Grad Norm 19.9232(12.8752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 77.6995, Epoch Time 613.0104(596.4383), Bit/dim 3.9872(best: 3.9435), Xent 1.2681, Loss 4.6213, Error 0.4591(best: 0.4442)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1540 | Time 41.6038(39.9086) | Bit/dim 3.9559(3.9569) | Xent 1.3205(1.3045) | Loss 10.8108(12.6665) | Error 0.4792(0.4691) Steps 604(584.20) | Grad Norm 19.0924(14.4218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 76.1609, Epoch Time 613.7591(596.9580), Bit/dim 3.9445(best: 3.9435), Xent 1.2169, Loss 4.5529, Error 0.4410(best: 0.4442)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1550 | Time 41.7247(40.0090) | Bit/dim 3.9471(3.9518) | Xent 1.2677(1.2985) | Loss 10.6161(12.8037) | Error 0.4511(0.4671) Steps 628(585.60) | Grad Norm 7.7328(12.9739) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 39.8758(40.1789) | Bit/dim 3.9333(3.9472) | Xent 1.2661(1.2884) | Loss 10.6052(12.2510) | Error 0.4492(0.4633) Steps 610(586.17) | Grad Norm 10.6577(12.5113) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 76.9415, Epoch Time 622.9462(597.7376), Bit/dim 3.9336(best: 3.9435), Xent 1.2029, Loss 4.5351, Error 0.4331(best: 0.4410)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1570 | Time 39.2172(39.9926) | Bit/dim 3.9152(3.9423) | Xent 1.2791(1.2795) | Loss 10.6931(12.3721) | Error 0.4650(0.4608) Steps 556(584.67) | Grad Norm 11.6272(12.5390) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 77.7667, Epoch Time 609.6093(598.0938), Bit/dim 3.9558(best: 3.9336), Xent 1.2093, Loss 4.5605, Error 0.4354(best: 0.4331)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1580 | Time 39.8640(39.9142) | Bit/dim 3.9427(3.9429) | Xent 1.2452(1.2753) | Loss 10.7170(12.5274) | Error 0.4428(0.4583) Steps 562(584.00) | Grad Norm 14.3395(13.8346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 77.4939, Epoch Time 608.2897(598.3996), Bit/dim 3.9309(best: 3.9336), Xent 1.1881, Loss 4.5249, Error 0.4326(best: 0.4331)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1590 | Time 38.9862(39.7898) | Bit/dim 3.9195(3.9386) | Xent 1.2819(1.2687) | Loss 10.6888(12.6788) | Error 0.4639(0.4561) Steps 580(583.56) | Grad Norm 12.4172(13.3989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 77.5593, Epoch Time 609.1408(598.7219), Bit/dim 3.9196(best: 3.9309), Xent 1.1968, Loss 4.5180, Error 0.4286(best: 0.4326)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1600 | Time 41.5465(39.8274) | Bit/dim 3.9236(3.9340) | Xent 1.2089(1.2631) | Loss 34.1247(12.8464) | Error 0.4392(0.4539) Steps 580(582.44) | Grad Norm 12.0679(13.2959) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 40.3976(39.8164) | Bit/dim 3.9138(3.9294) | Xent 1.2883(1.2580) | Loss 10.7520(12.2700) | Error 0.4611(0.4523) Steps 568(581.11) | Grad Norm 11.5679(13.5516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 76.4826, Epoch Time 610.3401(599.0704), Bit/dim 3.9104(best: 3.9196), Xent 1.1689, Loss 4.4948, Error 0.4231(best: 0.4286)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1620 | Time 39.3277(39.8092) | Bit/dim 3.8927(3.9254) | Xent 1.2014(1.2508) | Loss 10.5717(12.4050) | Error 0.4306(0.4496) Steps 604(584.19) | Grad Norm 14.3239(12.8888) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 77.7929, Epoch Time 610.7367(599.4204), Bit/dim 3.9185(best: 3.9104), Xent 1.1974, Loss 4.5172, Error 0.4335(best: 0.4231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1630 | Time 41.4856(39.9494) | Bit/dim 3.9165(3.9219) | Xent 1.2428(1.2450) | Loss 10.6330(12.5622) | Error 0.4461(0.4476) Steps 550(581.56) | Grad Norm 20.9711(12.8938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 78.7575, Epoch Time 619.8754(600.0341), Bit/dim 3.9074(best: 3.9104), Xent 1.1757, Loss 4.4953, Error 0.4221(best: 0.4231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1640 | Time 39.2251(39.9097) | Bit/dim 3.9076(3.9165) | Xent 1.2168(1.2381) | Loss 10.5041(12.6919) | Error 0.4369(0.4458) Steps 574(579.90) | Grad Norm 14.8439(12.2826) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 41.8924(39.9858) | Bit/dim 3.9012(3.9128) | Xent 1.2015(1.2302) | Loss 10.6076(12.1276) | Error 0.4247(0.4427) Steps 562(580.85) | Grad Norm 15.5265(11.8489) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 79.0005, Epoch Time 615.6253(600.5018), Bit/dim 3.8961(best: 3.9074), Xent 1.1422, Loss 4.4672, Error 0.4127(best: 0.4221)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1660 | Time 37.9634(39.8007) | Bit/dim 3.9076(3.9111) | Xent 1.2640(1.2308) | Loss 10.5745(12.2674) | Error 0.4497(0.4419) Steps 550(579.22) | Grad Norm 21.7852(13.0466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 77.1427, Epoch Time 603.9673(600.6058), Bit/dim 3.9176(best: 3.8961), Xent 1.2286, Loss 4.5319, Error 0.4470(best: 0.4127)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1670 | Time 40.5681(39.7744) | Bit/dim 3.9399(3.9121) | Xent 1.2232(1.2465) | Loss 10.6214(12.4521) | Error 0.4458(0.4474) Steps 556(578.74) | Grad Norm 16.1337(14.6195) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 76.7409, Epoch Time 605.9534(600.7662), Bit/dim 3.9061(best: 3.8961), Xent 1.1869, Loss 4.4995, Error 0.4276(best: 0.4127)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1680 | Time 41.0992(39.6798) | Bit/dim 3.9056(3.9120) | Xent 1.2070(1.2444) | Loss 10.5237(12.6379) | Error 0.4392(0.4467) Steps 562(579.56) | Grad Norm 8.3572(13.9179) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 41.6212(39.8726) | Bit/dim 3.8928(3.9082) | Xent 1.1888(1.2312) | Loss 10.4554(12.0876) | Error 0.4242(0.4429) Steps 556(579.31) | Grad Norm 20.0764(12.9976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 76.9791, Epoch Time 616.9251(601.2510), Bit/dim 3.9021(best: 3.8961), Xent 1.1697, Loss 4.4869, Error 0.4237(best: 0.4127)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1700 | Time 40.2958(39.8630) | Bit/dim 3.8965(3.9044) | Xent 1.1998(1.2189) | Loss 10.5338(12.2151) | Error 0.4336(0.4379) Steps 586(580.35) | Grad Norm 10.1705(12.3868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 77.5419, Epoch Time 614.0516(601.6350), Bit/dim 3.8875(best: 3.8961), Xent 1.1794, Loss 4.4772, Error 0.4168(best: 0.4127)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1710 | Time 39.7556(39.8069) | Bit/dim 3.8773(3.8986) | Xent 1.1725(1.2104) | Loss 10.5975(12.3296) | Error 0.4253(0.4343) Steps 604(581.26) | Grad Norm 10.7806(11.8699) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 3600 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run2 --seed 2 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
