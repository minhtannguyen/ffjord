{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_beta.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "# for disentanglement\n",
      "parser.add_argument('--beta', default=0.01, type=float, help='disentanglement weight')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "    \n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    beta_logpz_sup = logpz_sup * (1.0 - args.beta * torch.exp(logpz_sup) / torch.tensor(model.module.y_class).to(logpz_sup))\n",
      "    \n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    beta_logpz_color_sup = logpz_color_sup * (1.0 - args.beta * torch.exp(logpz_color_sup) / torch.tensor(model.module.y_color).to(logpz_color_sup))\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = beta_logpz_sup + beta_logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color,\n",
      "            cond_nn=args.cond_nn)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, beta=0.1, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/infocnf_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_beta_0_1_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.8125(32.9013) | Bit/dim 25.4501(27.3296) | Xent 2.2876(2.3009) | Xent Color 2.3003(2.3023) | Loss 48.6138(51.7973) | Error 0.8744(0.8883) | Error Color 0.9067(0.8900) |Steps 302(293.24) | Grad Norm 260.8731(275.0054) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.2217(26.1109) | Bit/dim 20.0892(26.0732) | Xent 2.2541(2.2933) | Xent Color 2.2916(2.3002) | Loss 39.1982(49.5870) | Error 0.8656(0.8873) | Error Color 0.8900(0.8912) |Steps 314(297.32) | Grad Norm 216.4227(264.8293) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.3209(21.1498) | Bit/dim 13.8624(23.5605) | Xent 2.2035(2.2753) | Xent Color 2.2717(2.2949) | Loss 27.6638(45.0965) | Error 0.5922(0.8397) | Error Color 0.8156(0.8796) |Steps 290(300.24) | Grad Norm 159.1471(243.6281) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.3816(17.6711) | Bit/dim 8.9816(20.2251) | Xent 2.1328(2.2453) | Xent Color 2.2448(2.2853) | Loss 18.9601(39.0961) | Error 0.3878(0.7412) | Error Color 0.8078(0.8611) |Steps 368(313.12) | Grad Norm 95.3416(211.8567) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 8.9943(15.3064) | Bit/dim 6.6902(16.8746) | Xent 2.0629(2.2039) | Xent Color 2.2402(2.2729) | Loss 14.8324(33.0827) | Error 0.3267(0.6288) | Error Color 0.8700(0.8521) |Steps 392(327.51) | Grad Norm 33.4678(171.5166) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.1169(13.6850) | Bit/dim 6.0912(14.1045) | Xent 2.0062(2.1562) | Xent Color 2.2420(2.2657) | Loss 13.8518(28.1202) | Error 0.3844(0.5535) | Error Color 0.9133(0.8631) |Steps 416(344.30) | Grad Norm 25.6620(132.5696) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 59.4127, Epoch Time 644.9226(644.9226), Bit/dim 5.7305(best: inf), Xent 1.9353, Xent Color 2.2140. Loss 6.7678, Error 0.2633(best: inf), Error Color 0.8963(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 9.1716(12.5250) | Bit/dim 5.5492(11.9129) | Xent 1.9307(2.1040) | Xent Color 2.1971(2.2552) | Loss 12.7106(24.6303) | Error 0.2989(0.4919) | Error Color 0.8344(0.8703) |Steps 386(355.45) | Grad Norm 17.5835(103.7827) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.5610(11.5340) | Bit/dim 4.9716(10.1621) | Xent 1.8534(2.0447) | Xent Color 2.1751(2.2377) | Loss 11.5545(21.3369) | Error 0.3178(0.4411) | Error Color 0.7600(0.8492) |Steps 398(361.10) | Grad Norm 15.1310(80.2937) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 8.8588(10.7922) | Bit/dim 4.5770(8.7562) | Xent 1.8010(1.9869) | Xent Color 2.1315(2.2164) | Loss 10.9697(18.6886) | Error 0.2700(0.4034) | Error Color 0.7956(0.8347) |Steps 392(364.48) | Grad Norm 12.7769(63.0851) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.9213(10.2527) | Bit/dim 4.1608(7.6039) | Xent 1.7628(1.9320) | Xent Color 2.1163(2.1916) | Loss 9.9438(16.5136) | Error 0.2778(0.3741) | Error Color 0.7656(0.8211) |Steps 350(365.45) | Grad Norm 9.1393(49.1012) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.7165(9.8777) | Bit/dim 3.7548(6.6410) | Xent 1.7410(1.8848) | Xent Color 2.0682(2.1617) | Loss 9.2798(14.7144) | Error 0.2822(0.3520) | Error Color 0.7044(0.7937) |Steps 374(369.78) | Grad Norm 8.5411(38.4875) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.6346(9.5841) | Bit/dim 3.3577(5.8233) | Xent 1.7650(1.8490) | Xent Color 2.0284(2.1293) | Loss 8.6152(13.1739) | Error 0.3433(0.3404) | Error Color 0.7233(0.7737) |Steps 398(370.85) | Grad Norm 8.2154(30.6804) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 8.7987(9.4411) | Bit/dim 3.0479(5.1297) | Xent 1.7725(1.8254) | Xent Color 1.9745(2.0951) | Loss 7.8574(11.8811) | Error 0.3378(0.3391) | Error Color 0.7178(0.7613) |Steps 398(374.81) | Grad Norm 6.3515(24.4801) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 58.2932, Epoch Time 665.7141(645.5463), Bit/dim 2.9648(best: 5.7305), Xent 1.7667, Xent Color 1.9513. Loss 3.8944, Error 0.2897(best: 0.2633), Error Color 0.7012(best: 0.8963)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 8.9277(9.3847) | Bit/dim 2.7950(4.5429) | Xent 1.8155(1.8201) | Xent Color 1.9269(2.0567) | Loss 7.5902(11.2258) | Error 0.3667(0.3459) | Error Color 0.7256(0.7511) |Steps 392(381.88) | Grad Norm 5.3281(19.6043) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.2666(9.3602) | Bit/dim 2.6163(4.0555) | Xent 1.8827(1.8273) | Xent Color 1.8894(2.0175) | Loss 7.3182(10.2168) | Error 0.4189(0.3588) | Error Color 0.7211(0.7420) |Steps 416(387.14) | Grad Norm 4.3243(15.6716) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.7828(9.3872) | Bit/dim 2.4745(3.6553) | Xent 1.9184(1.8486) | Xent Color 1.8401(1.9780) | Loss 7.0775(9.4136) | Error 0.4522(0.3804) | Error Color 0.7056(0.7361) |Steps 428(391.49) | Grad Norm 3.2100(12.4931) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.4950(9.3581) | Bit/dim 2.4011(3.3318) | Xent 1.9485(1.8730) | Xent Color 1.7948(1.9384) | Loss 6.8558(8.7637) | Error 0.4867(0.4044) | Error Color 0.6811(0.7279) |Steps 410(395.89) | Grad Norm 2.8826(9.9957) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.4962(9.3829) | Bit/dim 2.3302(3.0737) | Xent 1.9092(1.8890) | Xent Color 1.7796(1.8993) | Loss 6.8158(8.2471) | Error 0.4589(0.4200) | Error Color 0.6833(0.7148) |Steps 416(398.61) | Grad Norm 3.2615(8.0892) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.4561(9.4170) | Bit/dim 2.2708(2.8673) | Xent 1.9079(1.8958) | Xent Color 1.7490(1.8615) | Loss 6.6352(7.8220) | Error 0.4622(0.4319) | Error Color 0.6756(0.7042) |Steps 416(398.99) | Grad Norm 2.7827(6.6713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 58.9516, Epoch Time 704.3374(647.3101), Bit/dim 2.2422(best: 2.9648), Xent 1.8425, Xent Color 1.7091. Loss 3.1301, Error 0.3568(best: 0.2633), Error Color 0.6812(best: 0.7012)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.6992(9.4927) | Bit/dim 2.2408(2.7033) | Xent 1.8508(1.8901) | Xent Color 1.7125(1.8276) | Loss 6.5564(8.0107) | Error 0.4367(0.4339) | Error Color 0.6933(0.6995) |Steps 410(401.73) | Grad Norm 1.8153(5.5548) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.3785(9.4433) | Bit/dim 2.1944(2.5752) | Xent 1.8201(1.8745) | Xent Color 1.6884(1.7972) | Loss 6.3579(7.5930) | Error 0.4067(0.4295) | Error Color 0.6656(0.6967) |Steps 386(399.08) | Grad Norm 2.0682(4.9864) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.0505(9.3943) | Bit/dim 2.1962(2.4763) | Xent 1.7392(1.8494) | Xent Color 1.6728(1.7686) | Loss 6.1888(7.2690) | Error 0.3756(0.4204) | Error Color 0.6744(0.6933) |Steps 380(397.83) | Grad Norm 4.0521(4.5884) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.0998(9.3545) | Bit/dim 2.1665(2.4000) | Xent 1.6890(1.8142) | Xent Color 1.6717(1.7392) | Loss 6.3160(7.0183) | Error 0.3433(0.4068) | Error Color 0.7033(0.6911) |Steps 398(398.40) | Grad Norm 6.0299(4.4440) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.7331(9.2995) | Bit/dim 2.1852(2.3414) | Xent 1.6096(1.7703) | Xent Color 1.6346(1.7136) | Loss 6.3102(6.8214) | Error 0.3322(0.3912) | Error Color 0.6778(0.6900) |Steps 416(398.60) | Grad Norm 2.3037(4.2815) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.0878(9.2610) | Bit/dim 2.1576(2.2952) | Xent 1.5398(1.7132) | Xent Color 1.6177(1.6901) | Loss 6.1679(6.6567) | Error 0.3133(0.3708) | Error Color 0.6356(0.6878) |Steps 380(397.44) | Grad Norm 9.2128(4.6352) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 8.8942(9.2270) | Bit/dim 2.1512(2.2569) | Xent 1.4344(1.6509) | Xent Color 1.5942(1.6698) | Loss 6.0375(6.5134) | Error 0.2933(0.3524) | Error Color 0.6500(0.6818) |Steps 404(396.83) | Grad Norm 5.7256(5.3171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 58.1068, Epoch Time 690.8906(648.6175), Bit/dim 2.1705(best: 2.2422), Xent 1.3302, Xent Color 1.5706. Loss 2.8956, Error 0.2218(best: 0.2633), Error Color 0.6156(best: 0.6812)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.2458(9.2329) | Bit/dim 2.1800(2.2328) | Xent 1.3041(1.5785) | Xent Color 1.5901(1.6483) | Loss 6.0383(6.8554) | Error 0.2522(0.3336) | Error Color 0.6467(0.6723) |Steps 362(396.72) | Grad Norm 4.4552(5.1512) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.5603(9.2045) | Bit/dim 2.1545(2.2144) | Xent 1.2388(1.5008) | Xent Color 1.5755(1.6328) | Loss 6.0973(6.6462) | Error 0.2489(0.3166) | Error Color 0.6267(0.6691) |Steps 374(396.85) | Grad Norm 24.5792(10.2632) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 8.9162(9.1971) | Bit/dim 2.1451(2.1983) | Xent 1.1768(1.4207) | Xent Color 1.5570(1.6114) | Loss 5.9474(6.4639) | Error 0.2556(0.3007) | Error Color 0.6433(0.6563) |Steps 398(396.04) | Grad Norm 30.4484(12.3949) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 8.9025(9.1484) | Bit/dim 2.1637(2.1853) | Xent 1.0525(1.3434) | Xent Color 1.5407(1.5917) | Loss 5.8682(6.3101) | Error 0.2344(0.2869) | Error Color 0.6400(0.6429) |Steps 374(394.59) | Grad Norm 26.3441(14.6561) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.0392(9.1295) | Bit/dim 2.1461(2.1772) | Xent 1.0119(1.2624) | Xent Color 1.4789(1.5650) | Loss 5.8069(6.1961) | Error 0.2222(0.2727) | Error Color 0.5422(0.6214) |Steps 386(394.08) | Grad Norm 9.9242(12.6415) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.2682(9.1212) | Bit/dim 2.1331(2.1735) | Xent 0.9625(1.1860) | Xent Color 1.5497(1.5958) | Loss 5.8386(6.1310) | Error 0.2211(0.2591) | Error Color 0.6622(0.6449) |Steps 410(394.99) | Grad Norm 48.7132(26.0852) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.2648(9.1324) | Bit/dim 2.1463(2.1664) | Xent 0.9486(1.1254) | Xent Color 1.5427(1.5832) | Loss 5.7551(6.0524) | Error 0.2378(0.2502) | Error Color 0.7011(0.6411) |Steps 392(394.96) | Grad Norm 43.0215(28.8669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 58.0039, Epoch Time 682.6915(649.6397), Bit/dim 2.1401(best: 2.1705), Xent 0.8603, Xent Color 1.4338. Loss 2.7136, Error 0.1689(best: 0.2218), Error Color 0.3690(best: 0.6156)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.0357(9.0964) | Bit/dim 2.1238(2.1587) | Xent 0.9200(1.0692) | Xent Color 1.4394(1.5580) | Loss 5.8354(6.4090) | Error 0.2289(0.2425) | Error Color 0.5011(0.6205) |Steps 410(395.44) | Grad Norm 2.1136(27.0702) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 9.0383(9.0707) | Bit/dim 2.1267(2.1509) | Xent 0.8487(1.0162) | Xent Color 1.4370(1.5249) | Loss 5.7675(6.2350) | Error 0.2089(0.2362) | Error Color 0.4978(0.5918) |Steps 398(395.95) | Grad Norm 2.5502(22.8748) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 9.1151(9.0708) | Bit/dim 2.1080(2.1443) | Xent 0.8293(0.9681) | Xent Color 1.3680(1.4905) | Loss 5.5542(6.0934) | Error 0.2111(0.2318) | Error Color 0.4511(0.5625) |Steps 386(393.83) | Grad Norm 4.3253(18.4661) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 8.9833(9.0743) | Bit/dim 2.1153(2.1363) | Xent 0.7718(0.9202) | Xent Color 1.3391(1.4562) | Loss 5.6451(5.9785) | Error 0.2011(0.2245) | Error Color 0.4544(0.5377) |Steps 386(394.84) | Grad Norm 2.0364(16.1174) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 8.8908(9.0664) | Bit/dim 2.1256(2.1321) | Xent 0.7814(0.8797) | Xent Color 1.6543(1.4390) | Loss 5.6535(5.8856) | Error 0.2144(0.2185) | Error Color 0.7367(0.5329) |Steps 398(393.62) | Grad Norm 96.4159(21.5143) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.1315(9.0694) | Bit/dim 2.1058(2.1273) | Xent 0.8039(0.8527) | Xent Color 1.2973(1.4335) | Loss 5.5860(5.8226) | Error 0.2333(0.2153) | Error Color 0.4433(0.5380) |Steps 410(392.87) | Grad Norm 20.2208(28.5397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 57.9907, Epoch Time 677.9600(650.4893), Bit/dim 2.1067(best: 2.1401), Xent 0.6584, Xent Color 1.2357. Loss 2.5802, Error 0.1462(best: 0.1689), Error Color 0.3901(best: 0.3690)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.0464(9.0307) | Bit/dim 2.1242(2.1192) | Xent 0.7305(0.8293) | Xent Color 1.2361(1.3898) | Loss 5.5720(6.2371) | Error 0.1878(0.2118) | Error Color 0.4133(0.5092) |Steps 386(393.42) | Grad Norm 34.5531(28.1701) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 8.8629(9.0298) | Bit/dim 2.1129(2.1136) | Xent 0.6885(0.7988) | Xent Color 1.1556(1.3305) | Loss 5.5403(6.0445) | Error 0.1889(0.2069) | Error Color 0.4233(0.4783) |Steps 410(394.51) | Grad Norm 34.0121(27.2106) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.2109(9.0570) | Bit/dim 2.1322(2.1143) | Xent 0.6720(0.7678) | Xent Color 1.0770(1.3053) | Loss 5.4200(5.9127) | Error 0.1856(0.2021) | Error Color 0.4444(0.4804) |Steps 392(395.95) | Grad Norm 49.4076(34.2906) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 8.9876(9.0311) | Bit/dim 2.0738(2.1091) | Xent 0.7503(0.7508) | Xent Color 1.0278(1.2543) | Loss 5.4951(5.7941) | Error 0.1956(0.1986) | Error Color 0.3533(0.4713) |Steps 410(396.18) | Grad Norm 29.7868(35.7757) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 8.9773(9.0304) | Bit/dim 2.0805(2.1010) | Xent 0.6902(0.7489) | Xent Color 0.8840(1.1789) | Loss 5.2819(5.6848) | Error 0.2078(0.1999) | Error Color 0.2144(0.4265) |Steps 374(396.26) | Grad Norm 5.7221(31.8396) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 8.8995(9.0428) | Bit/dim 2.1100(2.0961) | Xent 0.6550(0.7259) | Xent Color 0.7729(1.0898) | Loss 5.2921(5.5897) | Error 0.1889(0.1958) | Error Color 0.1811(0.3748) |Steps 404(397.66) | Grad Norm 13.5405(27.4699) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 8.7905(9.0653) | Bit/dim 2.1856(2.1118) | Xent 0.7099(0.7067) | Xent Color 1.6899(1.2203) | Loss 5.9283(5.6406) | Error 0.2011(0.1941) | Error Color 0.6311(0.4076) |Steps 398(399.41) | Grad Norm 78.1085(47.7825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 58.2481, Epoch Time 679.2095(651.3509), Bit/dim 2.1656(best: 2.1067), Xent 0.6152, Xent Color 1.4700. Loss 2.6869, Error 0.1488(best: 0.1462), Error Color 0.6789(best: 0.3690)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.2168(9.0827) | Bit/dim 2.0669(2.1110) | Xent 0.7535(0.7106) | Xent Color 1.0092(1.2010) | Loss 5.4586(6.0575) | Error 0.2056(0.1962) | Error Color 0.3167(0.4266) |Steps 404(399.87) | Grad Norm 7.8886(46.4642) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 8.9973(9.1024) | Bit/dim 2.0493(2.0998) | Xent 0.7119(0.7177) | Xent Color 1.0044(1.1566) | Loss 5.3415(5.8757) | Error 0.2044(0.1992) | Error Color 0.3644(0.4108) |Steps 380(399.44) | Grad Norm 28.9443(41.1322) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.1219(9.0891) | Bit/dim 2.0571(2.0899) | Xent 0.6324(0.7018) | Xent Color 0.8957(1.0940) | Loss 5.1144(5.7230) | Error 0.1722(0.1978) | Error Color 0.2922(0.3725) |Steps 392(399.28) | Grad Norm 19.3317(34.5712) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 9.2402(9.1438) | Bit/dim 2.0292(2.0774) | Xent 0.6090(0.6806) | Xent Color 0.7992(1.0198) | Loss 5.1495(5.5864) | Error 0.1811(0.1930) | Error Color 0.2278(0.3343) |Steps 374(399.82) | Grad Norm 12.1366(28.2667) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.4456(9.1827) | Bit/dim 2.0449(2.0687) | Xent 0.5918(0.6586) | Xent Color 0.6777(0.9403) | Loss 5.1667(5.4665) | Error 0.1711(0.1890) | Error Color 0.1278(0.2906) |Steps 422(402.11) | Grad Norm 5.4893(23.7178) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 9.0526(9.1657) | Bit/dim 2.0391(2.0602) | Xent 0.6052(0.6473) | Xent Color 0.6112(0.8632) | Loss 4.9598(5.3718) | Error 0.1800(0.1867) | Error Color 0.1322(0.2530) |Steps 410(402.74) | Grad Norm 18.3649(21.2522) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 61.0746, Epoch Time 691.6266(652.5592), Bit/dim 2.0387(best: 2.1067), Xent 0.4794, Xent Color 0.5279. Loss 2.2905, Error 0.1244(best: 0.1462), Error Color 0.0871(best: 0.3690)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 8.9384(9.1915) | Bit/dim 2.0112(2.0516) | Xent 0.5818(0.6394) | Xent Color 0.5771(0.7914) | Loss 4.8773(5.8213) | Error 0.1633(0.1855) | Error Color 0.1233(0.2214) |Steps 392(403.64) | Grad Norm 23.2224(20.5021) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.3508(9.2014) | Bit/dim 2.0616(2.0481) | Xent 0.5542(0.6262) | Xent Color 1.9447(0.7980) | Loss 5.7648(5.6483) | Error 0.1578(0.1835) | Error Color 0.7222(0.2348) |Steps 392(403.04) | Grad Norm 172.4711(30.0576) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.0431(9.1631) | Bit/dim 2.0167(2.0464) | Xent 0.6678(0.6239) | Xent Color 0.6930(0.8331) | Loss 5.1003(5.5361) | Error 0.1922(0.1830) | Error Color 0.2089(0.2676) |Steps 416(402.87) | Grad Norm 24.8764(36.6352) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.2175(9.2001) | Bit/dim 2.0191(2.0394) | Xent 0.6188(0.6242) | Xent Color 0.6602(0.7886) | Loss 5.1141(5.4279) | Error 0.1844(0.1841) | Error Color 0.2422(0.2526) |Steps 410(404.21) | Grad Norm 26.6676(33.0653) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.3251(9.1966) | Bit/dim 1.9937(2.0316) | Xent 0.5896(0.6141) | Xent Color 0.5622(0.7290) | Loss 5.0614(5.3186) | Error 0.1811(0.1825) | Error Color 0.1278(0.2233) |Steps 410(404.52) | Grad Norm 18.6327(28.4922) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.0523(9.2224) | Bit/dim 2.0110(2.0243) | Xent 0.5395(0.6045) | Xent Color 0.4694(0.6695) | Loss 4.8963(5.2224) | Error 0.1522(0.1801) | Error Color 0.0789(0.1908) |Steps 380(403.61) | Grad Norm 7.4934(24.0825) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.2233(9.2682) | Bit/dim 1.9785(2.0164) | Xent 0.5913(0.5990) | Xent Color 0.4150(0.6103) | Loss 4.9073(5.1438) | Error 0.1711(0.1794) | Error Color 0.0778(0.1622) |Steps 404(404.35) | Grad Norm 12.2948(21.0590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 60.0272, Epoch Time 693.6196(653.7910), Bit/dim 2.0011(best: 2.0387), Xent 0.4348, Xent Color 0.3417. Loss 2.1953, Error 0.1185(best: 0.1244), Error Color 0.0215(best: 0.0871)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.4292(9.3038) | Bit/dim 1.9933(2.0085) | Xent 0.5938(0.5902) | Xent Color 0.4049(0.5601) | Loss 4.9305(5.5602) | Error 0.1833(0.1783) | Error Color 0.0867(0.1410) |Steps 392(405.61) | Grad Norm 26.3907(20.2592) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.4213(9.3340) | Bit/dim 1.9907(2.0033) | Xent 0.5178(0.5781) | Xent Color 0.3621(0.5135) | Loss 4.8194(5.3819) | Error 0.1622(0.1762) | Error Color 0.0611(0.1238) |Steps 404(404.77) | Grad Norm 18.0359(20.9692) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.5123(9.3238) | Bit/dim 1.9579(1.9944) | Xent 0.5948(0.5751) | Xent Color 0.3282(0.4717) | Loss 4.8496(5.2456) | Error 0.1867(0.1768) | Error Color 0.0533(0.1091) |Steps 404(405.52) | Grad Norm 14.8075(20.7310) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.1524(9.2904) | Bit/dim 2.1009(1.9998) | Xent 0.5073(0.5689) | Xent Color 1.0084(0.5932) | Loss 5.3609(5.2245) | Error 0.1556(0.1739) | Error Color 0.4411(0.1538) |Steps 398(403.19) | Grad Norm 86.6171(36.9193) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.5379(9.3491) | Bit/dim 1.9850(2.0014) | Xent 0.6498(0.5900) | Xent Color 0.5640(0.6396) | Loss 5.0308(5.1978) | Error 0.1822(0.1801) | Error Color 0.1622(0.1891) |Steps 434(402.50) | Grad Norm 8.0070(35.5583) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.3380(9.4095) | Bit/dim 1.9930(1.9961) | Xent 0.5417(0.5924) | Xent Color 0.4854(0.6148) | Loss 4.9402(5.1365) | Error 0.1733(0.1803) | Error Color 0.1100(0.1827) |Steps 422(407.51) | Grad Norm 7.5167(29.1174) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.4694(9.4505) | Bit/dim 1.9552(1.9866) | Xent 0.5774(0.5824) | Xent Color 0.4102(0.5683) | Loss 4.9412(5.0669) | Error 0.1656(0.1772) | Error Color 0.1000(0.1635) |Steps 422(411.24) | Grad Norm 3.7543(23.4230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 59.7112, Epoch Time 705.2307(655.3342), Bit/dim 1.9657(best: 2.0011), Xent 0.4070, Xent Color 0.3270. Loss 2.1492, Error 0.1146(best: 0.1185), Error Color 0.0407(best: 0.0215)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.0013(9.4225) | Bit/dim 1.9540(1.9777) | Xent 0.5703(0.5756) | Xent Color 0.3184(0.5151) | Loss 4.7713(5.4156) | Error 0.1733(0.1762) | Error Color 0.0622(0.1421) |Steps 416(411.83) | Grad Norm 9.8014(19.5947) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.4516(9.4924) | Bit/dim 2.0381(1.9724) | Xent 0.4795(0.5566) | Xent Color 0.5863(0.6207) | Loss 5.0539(5.2253) | Error 0.1600(0.1701) | Error Color 0.2589(0.1680) |Steps 434(411.64) | Grad Norm 48.1677(34.5325) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.6121(9.4634) | Bit/dim 1.9979(1.9766) | Xent 0.5229(0.5588) | Xent Color 0.4064(0.6448) | Loss 4.9540(5.1691) | Error 0.1622(0.1716) | Error Color 0.1189(0.1941) |Steps 440(411.03) | Grad Norm 13.3738(37.0614) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.1576(9.4861) | Bit/dim 1.9564(1.9702) | Xent 0.5018(0.5551) | Xent Color 0.3742(0.5786) | Loss 4.7716(5.0789) | Error 0.1511(0.1709) | Error Color 0.0989(0.1745) |Steps 386(412.72) | Grad Norm 26.3439(32.0440) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 10.0940(9.4632) | Bit/dim 1.9447(1.9626) | Xent 0.5240(0.5532) | Xent Color 0.3016(0.5084) | Loss 4.6736(4.9915) | Error 0.1633(0.1707) | Error Color 0.0656(0.1468) |Steps 404(411.62) | Grad Norm 14.0076(26.9807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 59.3464, Epoch Time 707.6824(656.9046), Bit/dim 1.9395(best: 1.9657), Xent 0.3862, Xent Color 0.1790. Loss 2.0808, Error 0.1100(best: 0.1146), Error Color 0.0105(best: 0.0215)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.6085(9.4471) | Bit/dim 1.9424(1.9527) | Xent 0.5345(0.5453) | Xent Color 0.2193(0.4387) | Loss 4.7183(5.4020) | Error 0.1689(0.1684) | Error Color 0.0311(0.1185) |Steps 398(410.39) | Grad Norm 8.6381(21.5960) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.1749(9.4404) | Bit/dim 1.9218(1.9446) | Xent 0.5094(0.5434) | Xent Color 0.1932(0.3777) | Loss 4.6802(5.2020) | Error 0.1633(0.1673) | Error Color 0.0344(0.0961) |Steps 404(407.31) | Grad Norm 5.8515(18.1972) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.2703(9.4166) | Bit/dim 1.9291(1.9370) | Xent 0.5061(0.5296) | Xent Color 0.1803(0.3292) | Loss 4.6221(5.0508) | Error 0.1622(0.1633) | Error Color 0.0244(0.0794) |Steps 428(410.04) | Grad Norm 17.6260(17.1723) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 9.5113(9.4226) | Bit/dim 1.9004(1.9290) | Xent 0.5328(0.5257) | Xent Color 0.1900(0.2911) | Loss 4.6479(4.9334) | Error 0.1600(0.1610) | Error Color 0.0411(0.0678) |Steps 416(408.83) | Grad Norm 25.7254(17.0680) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.7536(9.4955) | Bit/dim 1.9613(1.9345) | Xent 0.5686(0.5352) | Xent Color 0.4190(0.4401) | Loss 4.9309(4.9630) | Error 0.1733(0.1636) | Error Color 0.1489(0.1225) |Steps 416(410.54) | Grad Norm 25.2089(32.4079) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 10.0181(9.5367) | Bit/dim 1.9351(1.9368) | Xent 0.6059(0.5432) | Xent Color 0.2986(0.4283) | Loss 4.8029(4.9229) | Error 0.1822(0.1680) | Error Color 0.0689(0.1235) |Steps 404(409.85) | Grad Norm 8.9995(28.6626) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.3336(9.4871) | Bit/dim 1.8946(1.9297) | Xent 0.4972(0.5384) | Xent Color 0.2437(0.3898) | Loss 4.5907(4.8503) | Error 0.1611(0.1667) | Error Color 0.0444(0.1094) |Steps 404(409.66) | Grad Norm 5.8018(23.2532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 60.1063, Epoch Time 708.5756(658.4548), Bit/dim 1.9049(best: 1.9395), Xent 0.3755, Xent Color 0.1614. Loss 2.0391, Error 0.1044(best: 0.1100), Error Color 0.0164(best: 0.0105)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.5730(9.4678) | Bit/dim 1.9116(1.9211) | Xent 0.5665(0.5333) | Xent Color 0.1806(0.3453) | Loss 4.5822(5.2184) | Error 0.1767(0.1661) | Error Color 0.0367(0.0930) |Steps 386(410.12) | Grad Norm 2.1253(18.8890) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.8605(9.4700) | Bit/dim 1.8793(1.9137) | Xent 0.4249(0.5249) | Xent Color 0.1521(0.3005) | Loss 4.5012(5.0473) | Error 0.1478(0.1640) | Error Color 0.0344(0.0781) |Steps 440(411.33) | Grad Norm 2.5475(15.1301) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.3514(9.4585) | Bit/dim 1.9006(1.9047) | Xent 0.4742(0.5220) | Xent Color 0.1352(0.2588) | Loss 4.5449(4.9117) | Error 0.1444(0.1628) | Error Color 0.0222(0.0640) |Steps 416(412.64) | Grad Norm 2.3332(11.9407) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 8.7864(9.4592) | Bit/dim 1.8751(1.8958) | Xent 0.4917(0.5189) | Xent Color 0.1206(0.2243) | Loss 4.4309(4.8045) | Error 0.1544(0.1609) | Error Color 0.0189(0.0524) |Steps 404(411.68) | Grad Norm 2.2005(9.5500) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.3809(9.5094) | Bit/dim 1.8624(1.8878) | Xent 0.4959(0.5102) | Xent Color 0.1034(0.1950) | Loss 4.4559(4.7226) | Error 0.1456(0.1589) | Error Color 0.0178(0.0434) |Steps 422(411.78) | Grad Norm 4.2960(7.9680) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.3578(9.5404) | Bit/dim 1.8568(1.8780) | Xent 0.4944(0.5042) | Xent Color 0.1164(0.1713) | Loss 4.4306(4.6515) | Error 0.1556(0.1570) | Error Color 0.0211(0.0368) |Steps 422(411.79) | Grad Norm 8.7223(7.0817) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 60.8880, Epoch Time 713.3957(660.1030), Bit/dim 1.8502(best: 1.9049), Xent 0.3590, Xent Color 0.0627. Loss 1.9556, Error 0.1033(best: 0.1044), Error Color 0.0048(best: 0.0105)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.4293(9.5663) | Bit/dim 1.8337(1.8694) | Xent 0.4939(0.5057) | Xent Color 0.0973(0.1528) | Loss 4.3917(5.1311) | Error 0.1511(0.1569) | Error Color 0.0156(0.0322) |Steps 422(411.80) | Grad Norm 13.8057(7.3485) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.2600(9.5641) | Bit/dim 1.8583(1.8599) | Xent 0.4840(0.5022) | Xent Color 0.0927(0.1380) | Loss 4.4657(4.9458) | Error 0.1456(0.1556) | Error Color 0.0178(0.0287) |Steps 404(410.25) | Grad Norm 14.3870(8.6432) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.7358(9.5791) | Bit/dim 1.8159(1.8515) | Xent 0.5321(0.4990) | Xent Color 0.0786(0.1246) | Loss 4.3639(4.8117) | Error 0.1889(0.1551) | Error Color 0.0133(0.0250) |Steps 428(411.90) | Grad Norm 3.7670(8.6558) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.7806(9.6082) | Bit/dim 1.8024(1.8430) | Xent 0.4628(0.4989) | Xent Color 0.0723(0.1116) | Loss 4.3564(4.7048) | Error 0.1467(0.1554) | Error Color 0.0111(0.0215) |Steps 410(413.56) | Grad Norm 6.0485(7.9556) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.7562(9.6143) | Bit/dim 1.8104(1.8336) | Xent 0.4922(0.4992) | Xent Color 0.0611(0.1010) | Loss 4.3887(4.6110) | Error 0.1711(0.1557) | Error Color 0.0044(0.0189) |Steps 398(413.46) | Grad Norm 7.2541(7.9210) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 8.8375(9.6297) | Bit/dim 3.0371(1.8651) | Xent 0.5303(0.4972) | Xent Color 24.5138(0.9650) | Loss 17.5086(5.0072) | Error 0.1544(0.1545) | Error Color 0.8700(0.0719) |Steps 368(412.84) | Grad Norm 477.0198(33.5078) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.7632(9.7873) | Bit/dim 2.3447(2.0835) | Xent 0.9008(0.7047) | Xent Color 1.8853(1.9262) | Loss 6.4345(5.9210) | Error 0.3022(0.2240) | Error Color 0.5756(0.2342) |Steps 416(421.63) | Grad Norm 38.5472(50.8285) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 68.0155, Epoch Time 735.3541(662.3605), Bit/dim 2.3126(best: 1.8502), Xent 0.4563, Xent Color 1.7037. Loss 2.8526, Error 0.1441(best: 0.1033), Error Color 0.6034(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 10.7713(9.9564) | Bit/dim 2.1565(2.1321) | Xent 0.7088(0.7167) | Xent Color 1.3866(1.9446) | Loss 6.0584(6.7040) | Error 0.2378(0.2291) | Error Color 0.4944(0.3326) |Steps 506(436.20) | Grad Norm 15.6490(46.5875) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.9831(10.1182) | Bit/dim 2.0292(2.1176) | Xent 0.6576(0.7123) | Xent Color 1.1221(1.7584) | Loss 5.5404(6.4606) | Error 0.2000(0.2285) | Error Color 0.4467(0.3734) |Steps 476(448.71) | Grad Norm 16.0654(39.6410) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 10.1666(10.1416) | Bit/dim 1.9764(2.0895) | Xent 0.6237(0.6909) | Xent Color 0.8519(1.5416) | Loss 5.3418(6.1895) | Error 0.1878(0.2194) | Error Color 0.3400(0.3702) |Steps 458(452.90) | Grad Norm 11.8406(32.2265) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.8959(10.1359) | Bit/dim 1.9710(2.0575) | Xent 0.6438(0.6607) | Xent Color 0.6357(1.3193) | Loss 5.1112(5.9200) | Error 0.1944(0.2109) | Error Color 0.2433(0.3433) |Steps 458(452.86) | Grad Norm 3.9900(25.7113) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.9223(10.0626) | Bit/dim 1.9494(2.0289) | Xent 0.6071(0.6409) | Xent Color 0.4692(1.1100) | Loss 4.9868(5.6852) | Error 0.2122(0.2041) | Error Color 0.1722(0.3037) |Steps 428(450.56) | Grad Norm 3.3792(19.7010) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.7980(10.0551) | Bit/dim 1.9329(2.0019) | Xent 0.5291(0.6201) | Xent Color 0.3891(0.9233) | Loss 4.9839(5.4914) | Error 0.1644(0.1959) | Error Color 0.1500(0.2604) |Steps 452(450.69) | Grad Norm 4.6935(15.3911) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.7630(10.0587) | Bit/dim 1.8925(1.9758) | Xent 0.5033(0.6012) | Xent Color 0.3023(0.7647) | Loss 4.7849(5.3259) | Error 0.1556(0.1892) | Error Color 0.0911(0.2182) |Steps 440(450.02) | Grad Norm 2.2399(12.1121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 67.4237, Epoch Time 763.0236(665.3804), Bit/dim 1.9031(best: 1.8502), Xent 0.3797, Xent Color 0.1997. Loss 2.0480, Error 0.1103(best: 0.1033), Error Color 0.0319(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.6270(9.9605) | Bit/dim 1.8670(1.9522) | Xent 0.5071(0.5849) | Xent Color 0.2409(0.6320) | Loss 4.7225(5.6776) | Error 0.1644(0.1833) | Error Color 0.0678(0.1801) |Steps 452(448.16) | Grad Norm 5.8202(9.9886) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.8910(9.9116) | Bit/dim 1.8520(1.9297) | Xent 0.5187(0.5724) | Xent Color 0.2071(0.5260) | Loss 4.6739(5.4322) | Error 0.1578(0.1793) | Error Color 0.0478(0.1489) |Steps 446(448.88) | Grad Norm 4.5630(9.5573) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.5338(9.8332) | Bit/dim 1.8455(1.9085) | Xent 0.4946(0.5564) | Xent Color 0.1643(0.4360) | Loss 4.6617(5.2321) | Error 0.1522(0.1747) | Error Color 0.0333(0.1207) |Steps 434(447.18) | Grad Norm 8.5462(8.9509) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 10.0450(9.8249) | Bit/dim 1.8164(1.8858) | Xent 0.4822(0.5483) | Xent Color 0.1525(0.3748) | Loss 4.6434(5.0888) | Error 0.1356(0.1718) | Error Color 0.0400(0.1041) |Steps 446(447.12) | Grad Norm 3.3701(11.5204) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 10.4689(9.8877) | Bit/dim 1.7966(1.8663) | Xent 0.5101(0.5368) | Xent Color 0.1709(0.3254) | Loss 4.6349(4.9720) | Error 0.1556(0.1683) | Error Color 0.0511(0.0897) |Steps 446(447.71) | Grad Norm 19.7811(13.0846) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.8812(9.8966) | Bit/dim 1.8114(1.8486) | Xent 0.5225(0.5322) | Xent Color 0.1428(0.2801) | Loss 4.6106(4.8730) | Error 0.1644(0.1669) | Error Color 0.0378(0.0763) |Steps 470(448.94) | Grad Norm 17.7408(13.6516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 66.2777, Epoch Time 738.2057(667.5652), Bit/dim 1.7895(best: 1.8502), Xent 0.3621, Xent Color 0.1088. Loss 1.9072, Error 0.1053(best: 0.1033), Error Color 0.0230(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 10.1373(9.9356) | Bit/dim 1.7863(1.8326) | Xent 0.5348(0.5333) | Xent Color 0.1564(0.2475) | Loss 4.5307(5.3877) | Error 0.1733(0.1668) | Error Color 0.0411(0.0668) |Steps 458(450.69) | Grad Norm 20.1931(15.8784) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.7876(9.9750) | Bit/dim 1.7473(1.8143) | Xent 0.4923(0.5288) | Xent Color 0.1057(0.2235) | Loss 4.5038(5.1610) | Error 0.1478(0.1651) | Error Color 0.0156(0.0610) |Steps 470(450.90) | Grad Norm 12.3289(17.7060) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 10.0284(10.0253) | Bit/dim 1.7634(1.7994) | Xent 0.4915(0.5296) | Xent Color 0.0966(0.2010) | Loss 4.4895(4.9920) | Error 0.1522(0.1644) | Error Color 0.0189(0.0544) |Steps 440(449.10) | Grad Norm 9.1349(18.6600) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.8684(9.9932) | Bit/dim 1.9260(1.8171) | Xent 0.5756(0.5333) | Xent Color 1.2684(0.5696) | Loss 5.2644(5.0928) | Error 0.1878(0.1649) | Error Color 0.4278(0.1374) |Steps 458(448.50) | Grad Norm 48.3248(34.9237) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 10.0285(9.9770) | Bit/dim 1.8220(1.8312) | Xent 0.5094(0.5534) | Xent Color 0.4934(0.5959) | Loss 4.6926(5.0396) | Error 0.1522(0.1726) | Error Color 0.1911(0.1735) |Steps 446(447.43) | Grad Norm 10.8143(31.5452) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.8565(10.0012) | Bit/dim 1.7637(1.8212) | Xent 0.5128(0.5580) | Xent Color 0.2876(0.5354) | Loss 4.5182(4.9171) | Error 0.1544(0.1737) | Error Color 0.0911(0.1610) |Steps 434(442.37) | Grad Norm 4.5521(25.3093) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 10.6739(10.0432) | Bit/dim 1.7205(1.8014) | Xent 0.5515(0.5583) | Xent Color 0.1851(0.4563) | Loss 4.3987(4.8028) | Error 0.1667(0.1739) | Error Color 0.0344(0.1347) |Steps 404(439.30) | Grad Norm 4.9832(20.2636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 65.2346, Epoch Time 752.1835(670.1037), Bit/dim 1.7458(best: 1.7895), Xent 0.3717, Xent Color 0.1017. Loss 1.8641, Error 0.1099(best: 0.1033), Error Color 0.0054(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 9.9878(10.0470) | Bit/dim 1.7290(1.7834) | Xent 0.6292(0.5535) | Xent Color 0.1318(0.3776) | Loss 4.4355(5.2019) | Error 0.1867(0.1736) | Error Color 0.0244(0.1075) |Steps 422(437.42) | Grad Norm 1.9781(16.0643) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 10.2190(10.1217) | Bit/dim 1.7214(1.7653) | Xent 0.6497(0.5553) | Xent Color 0.1079(0.3130) | Loss 4.4354(4.9840) | Error 0.1822(0.1720) | Error Color 0.0189(0.0860) |Steps 422(438.04) | Grad Norm 3.5050(12.7505) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 10.1620(10.1338) | Bit/dim 1.6787(1.7461) | Xent 0.4862(0.5419) | Xent Color 0.1177(0.2604) | Loss 4.2898(4.8036) | Error 0.1400(0.1686) | Error Color 0.0222(0.0685) |Steps 446(442.16) | Grad Norm 1.8373(10.1199) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.8832(10.1063) | Bit/dim 1.6662(1.7294) | Xent 0.4685(0.5336) | Xent Color 0.1022(0.2195) | Loss 4.1878(4.6664) | Error 0.1389(0.1664) | Error Color 0.0200(0.0554) |Steps 440(441.17) | Grad Norm 3.1292(8.2247) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 10.3073(10.0988) | Bit/dim 1.6632(1.7106) | Xent 0.5119(0.5337) | Xent Color 0.0913(0.1871) | Loss 4.3228(4.5559) | Error 0.1467(0.1656) | Error Color 0.0189(0.0456) |Steps 458(442.33) | Grad Norm 1.9525(6.5965) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 10.0589(10.1662) | Bit/dim 1.6357(1.6928) | Xent 0.4566(0.5267) | Xent Color 0.0822(0.1618) | Loss 4.2191(4.4735) | Error 0.1411(0.1641) | Error Color 0.0167(0.0378) |Steps 452(442.40) | Grad Norm 2.7542(5.7070) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 67.2942, Epoch Time 762.5485(672.8771), Bit/dim 1.6308(best: 1.7458), Xent 0.3518, Xent Color 0.0404. Loss 1.7289, Error 0.1026(best: 0.1033), Error Color 0.0025(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 10.2522(10.1397) | Bit/dim 1.6286(1.6768) | Xent 0.4796(0.5246) | Xent Color 0.0730(0.1413) | Loss 4.1687(4.9896) | Error 0.1678(0.1635) | Error Color 0.0078(0.0315) |Steps 422(441.84) | Grad Norm 4.4918(5.5404) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 9.9081(10.1688) | Bit/dim 1.6200(1.6607) | Xent 0.4542(0.5180) | Xent Color 0.0697(0.1239) | Loss 4.1286(4.7725) | Error 0.1522(0.1615) | Error Color 0.0089(0.0265) |Steps 440(446.06) | Grad Norm 2.3933(5.1365) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 10.2388(10.1659) | Bit/dim 1.5887(1.6447) | Xent 0.4138(0.5131) | Xent Color 0.0829(0.1111) | Loss 3.9902(4.5935) | Error 0.1433(0.1609) | Error Color 0.0200(0.0232) |Steps 422(444.97) | Grad Norm 6.1246(4.5287) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 10.5370(10.1861) | Bit/dim 1.5684(1.6281) | Xent 0.5053(0.5138) | Xent Color 0.0783(0.1003) | Loss 4.1307(4.4621) | Error 0.1711(0.1625) | Error Color 0.0156(0.0204) |Steps 452(447.62) | Grad Norm 1.8937(4.8385) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 10.8442(10.1575) | Bit/dim 1.5709(1.6130) | Xent 0.5411(0.5145) | Xent Color 0.0673(0.0906) | Loss 4.1500(4.3535) | Error 0.1644(0.1624) | Error Color 0.0156(0.0177) |Steps 440(446.27) | Grad Norm 4.1461(4.8809) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 10.3919(10.1781) | Bit/dim 1.5587(1.5971) | Xent 0.5237(0.5157) | Xent Color 0.0465(0.0824) | Loss 4.0012(4.2689) | Error 0.1656(0.1621) | Error Color 0.0056(0.0156) |Steps 446(445.99) | Grad Norm 5.5346(4.8999) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 10.4034(10.1788) | Bit/dim 1.5482(1.5812) | Xent 0.5045(0.5113) | Xent Color 0.0595(0.0759) | Loss 4.0536(4.2044) | Error 0.1611(0.1606) | Error Color 0.0133(0.0140) |Steps 476(448.95) | Grad Norm 4.9628(5.2448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 66.8652, Epoch Time 763.4606(675.5946), Bit/dim 2.9625(best: 1.6308), Xent 0.7709, Xent Color 28.8352. Loss 10.3640, Error 0.2625(best: 0.1026), Error Color 0.9006(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.6310(10.0973) | Bit/dim 2.3362(1.7638) | Xent 0.6612(0.5775) | Xent Color 3.0901(1.5502) | Loss 6.8727(5.7075) | Error 0.2111(0.1745) | Error Color 0.6944(0.1490) |Steps 410(443.68) | Grad Norm 44.5142(41.9765) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 11.2535(10.3178) | Bit/dim 2.0781(1.8567) | Xent 0.8499(0.7728) | Xent Color 1.2330(1.5563) | Loss 5.8658(5.8344) | Error 0.2889(0.2411) | Error Color 0.4067(0.2394) |Steps 500(456.40) | Grad Norm 43.8335(47.2361) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 10.1739(10.3759) | Bit/dim 1.9655(1.9065) | Xent 0.5765(0.7635) | Xent Color 0.8939(1.4329) | Loss 5.2551(5.7859) | Error 0.1933(0.2372) | Error Color 0.3578(0.2789) |Steps 470(461.13) | Grad Norm 37.1747(46.2553) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.2880(10.1894) | Bit/dim 1.8898(1.9076) | Xent 0.5402(0.7314) | Xent Color 0.5116(1.2237) | Loss 4.6592(5.5523) | Error 0.1800(0.2278) | Error Color 0.1900(0.2707) |Steps 380(447.39) | Grad Norm 18.1084(39.7081) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.4487(10.0142) | Bit/dim 1.8394(1.8920) | Xent 0.5715(0.6866) | Xent Color 0.3708(1.0108) | Loss 4.6719(5.3183) | Error 0.1722(0.2149) | Error Color 0.1356(0.2406) |Steps 422(435.13) | Grad Norm 7.9065(31.6626) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.1812(9.8621) | Bit/dim 1.7824(1.8710) | Xent 0.5461(0.6542) | Xent Color 0.2430(0.8205) | Loss 4.4571(5.1138) | Error 0.1667(0.2032) | Error Color 0.0700(0.2008) |Steps 422(428.09) | Grad Norm 2.9155(24.6965) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.4251(9.7753) | Bit/dim 1.7603(1.8429) | Xent 0.5077(0.6252) | Xent Color 0.2141(0.6672) | Loss 4.3904(4.9288) | Error 0.1722(0.1946) | Error Color 0.0644(0.1667) |Steps 398(422.78) | Grad Norm 3.8151(19.3092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 60.6309, Epoch Time 738.2338(677.4737), Bit/dim 1.7583(best: 1.6308), Xent 0.3640, Xent Color 0.1303. Loss 1.8818, Error 0.1069(best: 0.1026), Error Color 0.0151(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 10.0691(9.8063) | Bit/dim 1.7161(1.8155) | Xent 0.4651(0.5965) | Xent Color 0.1832(0.5457) | Loss 4.3434(5.2146) | Error 0.1711(0.1859) | Error Color 0.0389(0.1374) |Steps 422(419.91) | Grad Norm 3.7853(15.1216) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 10.0421(9.8732) | Bit/dim 1.7101(1.7876) | Xent 0.5861(0.5798) | Xent Color 0.1585(0.4500) | Loss 4.2824(4.9724) | Error 0.1856(0.1808) | Error Color 0.0367(0.1135) |Steps 392(419.45) | Grad Norm 7.6205(12.2766) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 10.1229(9.8868) | Bit/dim 1.6798(1.7645) | Xent 0.4938(0.5622) | Xent Color 0.1473(0.3735) | Loss 4.1974(4.7869) | Error 0.1400(0.1744) | Error Color 0.0400(0.0940) |Steps 452(420.13) | Grad Norm 1.4257(10.1067) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 10.0202(9.9105) | Bit/dim 1.6692(1.7420) | Xent 0.5239(0.5528) | Xent Color 0.1519(0.3131) | Loss 4.2142(4.6349) | Error 0.1578(0.1717) | Error Color 0.0389(0.0786) |Steps 434(420.54) | Grad Norm 9.4245(8.5888) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 10.2040(9.9276) | Bit/dim 1.6127(1.7187) | Xent 0.5105(0.5439) | Xent Color 0.1437(0.2665) | Loss 4.1137(4.5053) | Error 0.1600(0.1690) | Error Color 0.0400(0.0666) |Steps 416(421.10) | Grad Norm 10.5322(8.3145) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 10.6140(9.9905) | Bit/dim 1.6346(1.6965) | Xent 0.4619(0.5286) | Xent Color 0.1420(0.2299) | Loss 4.1829(4.4044) | Error 0.1533(0.1646) | Error Color 0.0422(0.0577) |Steps 440(423.62) | Grad Norm 12.3176(8.4138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 61.9051, Epoch Time 746.0198(679.5301), Bit/dim 1.6231(best: 1.6308), Xent 0.3451, Xent Color 0.0659. Loss 1.7258, Error 0.1022(best: 0.1026), Error Color 0.0071(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 10.2158(10.0268) | Bit/dim 1.6337(1.6776) | Xent 0.5815(0.5253) | Xent Color 0.5032(0.2155) | Loss 4.3344(4.8460) | Error 0.1656(0.1633) | Error Color 0.1911(0.0570) |Steps 452(425.02) | Grad Norm 74.9839(11.6854) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.2417(9.9642) | Bit/dim 1.7640(1.7080) | Xent 0.6990(0.5526) | Xent Color 1.0422(0.6884) | Loss 4.8074(4.9715) | Error 0.2289(0.1737) | Error Color 0.3611(0.1652) |Steps 428(425.63) | Grad Norm 19.2099(22.9053) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 10.1040(9.8313) | Bit/dim 1.6983(1.7167) | Xent 0.5728(0.5655) | Xent Color 0.6410(0.7204) | Loss 4.4866(4.8659) | Error 0.1844(0.1792) | Error Color 0.2489(0.2016) |Steps 422(421.61) | Grad Norm 11.6372(20.0963) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 10.1932(9.8360) | Bit/dim 1.6465(1.7023) | Xent 0.4972(0.5568) | Xent Color 0.4523(0.6690) | Loss 4.2207(4.7151) | Error 0.1578(0.1759) | Error Color 0.1722(0.2006) |Steps 440(422.11) | Grad Norm 8.4971(17.2947) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 9.9128(9.8654) | Bit/dim 1.6010(1.6821) | Xent 0.4925(0.5512) | Xent Color 0.3382(0.5939) | Loss 4.1441(4.5739) | Error 0.1656(0.1733) | Error Color 0.1078(0.1840) |Steps 440(421.65) | Grad Norm 1.7961(14.0449) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 10.4197(9.9481) | Bit/dim 1.5999(1.6607) | Xent 0.4572(0.5411) | Xent Color 0.2617(0.5177) | Loss 4.0352(4.4385) | Error 0.1467(0.1703) | Error Color 0.0833(0.1630) |Steps 446(424.58) | Grad Norm 1.1746(11.1582) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 10.8670(10.0245) | Bit/dim 1.5659(1.6401) | Xent 0.4915(0.5334) | Xent Color 0.2065(0.4434) | Loss 3.8740(4.3239) | Error 0.1489(0.1672) | Error Color 0.0456(0.1388) |Steps 422(427.74) | Grad Norm 1.7835(8.8389) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 60.4262, Epoch Time 738.5305(681.3001), Bit/dim 1.5790(best: 1.6231), Xent 0.3559, Xent Color 0.1265. Loss 1.6996, Error 0.1018(best: 0.1022), Error Color 0.0150(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 10.0770(10.0736) | Bit/dim 1.5605(1.6227) | Xent 0.4981(0.5234) | Xent Color 0.1764(0.3801) | Loss 3.9473(4.6470) | Error 0.1489(0.1645) | Error Color 0.0422(0.1183) |Steps 434(429.56) | Grad Norm 1.6488(6.9832) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 9.7531(10.0355) | Bit/dim 1.5621(1.6073) | Xent 0.5078(0.5170) | Xent Color 0.1631(0.3267) | Loss 3.8925(4.4606) | Error 0.1778(0.1633) | Error Color 0.0456(0.1001) |Steps 404(428.83) | Grad Norm 5.4487(5.8218) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 10.2810(10.0932) | Bit/dim 1.5531(1.5937) | Xent 0.5340(0.5170) | Xent Color 0.1401(0.2806) | Loss 3.9289(4.3180) | Error 0.1667(0.1633) | Error Color 0.0311(0.0838) |Steps 452(427.80) | Grad Norm 1.6933(4.9352) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 10.5449(10.0770) | Bit/dim 1.5600(1.5823) | Xent 0.4644(0.5066) | Xent Color 0.1298(0.2420) | Loss 3.9214(4.2018) | Error 0.1389(0.1603) | Error Color 0.0389(0.0707) |Steps 446(429.44) | Grad Norm 4.2969(4.3694) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 10.0801(10.1014) | Bit/dim 1.5538(1.5716) | Xent 0.4860(0.5013) | Xent Color 0.1158(0.2097) | Loss 3.8437(4.1110) | Error 0.1600(0.1584) | Error Color 0.0267(0.0588) |Steps 416(428.57) | Grad Norm 5.4327(4.4301) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 10.3309(10.1358) | Bit/dim 1.5183(1.5593) | Xent 0.4792(0.4983) | Xent Color 0.1068(0.1839) | Loss 3.8559(4.0474) | Error 0.1456(0.1579) | Error Color 0.0267(0.0500) |Steps 464(430.95) | Grad Norm 2.1465(4.7410) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 61.2616, Epoch Time 754.0400(683.4823), Bit/dim 1.5265(best: 1.5790), Xent 0.3295, Xent Color 0.0405. Loss 1.6190, Error 0.0984(best: 0.1018), Error Color 0.0024(best: 0.0025)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 10.5534(10.1634) | Bit/dim 1.5280(1.5494) | Xent 0.4504(0.4970) | Xent Color 0.0941(0.1616) | Loss 3.9186(4.4969) | Error 0.1478(0.1575) | Error Color 0.0200(0.0427) |Steps 446(430.01) | Grad Norm 3.8300(4.5928) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 10.1321(10.1725) | Bit/dim 1.5301(1.5418) | Xent 0.5041(0.4917) | Xent Color 0.0743(0.1420) | Loss 3.8862(4.3246) | Error 0.1511(0.1551) | Error Color 0.0100(0.0362) |Steps 422(430.82) | Grad Norm 1.7594(4.2903) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 10.5671(10.1810) | Bit/dim 1.4916(1.5340) | Xent 0.5426(0.4909) | Xent Color 0.0711(0.1251) | Loss 3.7992(4.1855) | Error 0.1611(0.1545) | Error Color 0.0100(0.0308) |Steps 416(429.22) | Grad Norm 2.6509(3.9784) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 10.4621(10.1951) | Bit/dim 1.4998(1.5260) | Xent 0.4666(0.4863) | Xent Color 0.0750(0.1116) | Loss 3.8130(4.0841) | Error 0.1456(0.1532) | Error Color 0.0122(0.0266) |Steps 422(428.90) | Grad Norm 3.8362(3.5929) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 10.0137(10.2081) | Bit/dim 1.4764(1.5178) | Xent 0.5220(0.4853) | Xent Color 0.0611(0.0992) | Loss 3.7764(4.0005) | Error 0.1556(0.1517) | Error Color 0.0133(0.0228) |Steps 428(430.30) | Grad Norm 2.4922(3.4091) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.2160(10.1688) | Bit/dim 1.4914(1.5105) | Xent 0.4451(0.4811) | Xent Color 0.0538(0.0887) | Loss 3.7987(3.9399) | Error 0.1300(0.1504) | Error Color 0.0078(0.0195) |Steps 440(431.74) | Grad Norm 1.7140(3.0935) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 10.3905(10.2061) | Bit/dim 1.4885(1.5035) | Xent 0.4802(0.4735) | Xent Color 0.0522(0.0805) | Loss 3.7937(3.8888) | Error 0.1522(0.1484) | Error Color 0.0133(0.0173) |Steps 470(432.69) | Grad Norm 5.0749(3.3613) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 62.3519, Epoch Time 760.0258(685.7786), Bit/dim 1.4839(best: 1.5265), Xent 0.3155, Xent Color 0.0192. Loss 1.5676, Error 0.0942(best: 0.0984), Error Color 0.0006(best: 0.0024)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.1704(10.2006) | Bit/dim 1.4810(1.4975) | Xent 0.4630(0.4700) | Xent Color 0.0607(0.0735) | Loss 3.8172(4.3032) | Error 0.1367(0.1475) | Error Color 0.0100(0.0151) |Steps 452(432.58) | Grad Norm 6.4001(3.4895) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.9089(10.2237) | Bit/dim 1.4622(1.4926) | Xent 0.4959(0.4660) | Xent Color 0.0604(0.0680) | Loss 3.7200(4.1634) | Error 0.1511(0.1468) | Error Color 0.0111(0.0136) |Steps 422(430.71) | Grad Norm 3.3982(3.4601) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 10.0096(10.2373) | Bit/dim 1.4499(1.4852) | Xent 0.4468(0.4668) | Xent Color 0.0555(0.0629) | Loss 3.6606(4.0477) | Error 0.1344(0.1468) | Error Color 0.0122(0.0125) |Steps 434(432.63) | Grad Norm 6.4159(3.4406) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.9634(10.2178) | Bit/dim 1.4639(1.4782) | Xent 0.4141(0.4604) | Xent Color 0.0478(0.0588) | Loss 3.6626(3.9577) | Error 0.1389(0.1442) | Error Color 0.0089(0.0113) |Steps 422(433.46) | Grad Norm 2.7678(3.4950) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 9.9101(10.1942) | Bit/dim 1.4614(1.4719) | Xent 0.4591(0.4547) | Xent Color 0.0450(0.0550) | Loss 3.7324(3.8868) | Error 0.1511(0.1431) | Error Color 0.0056(0.0106) |Steps 434(433.23) | Grad Norm 5.3041(3.7800) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 10.6099(10.2327) | Bit/dim 1.4469(1.4654) | Xent 0.4568(0.4543) | Xent Color 0.0326(0.0520) | Loss 3.7146(3.8399) | Error 0.1422(0.1431) | Error Color 0.0056(0.0102) |Steps 452(433.50) | Grad Norm 3.0302(3.9310) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 10.4641(10.2766) | Bit/dim 1.4363(1.4595) | Xent 0.4476(0.4521) | Xent Color 0.0540(0.0494) | Loss 3.6928(3.8105) | Error 0.1411(0.1423) | Error Color 0.0100(0.0096) |Steps 428(433.42) | Grad Norm 10.5466(4.1348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 63.6942, Epoch Time 763.9758(688.1246), Bit/dim 1.4434(best: 1.4839), Xent 0.3074, Xent Color 0.0154. Loss 1.5241, Error 0.0921(best: 0.0942), Error Color 0.0008(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.4643(10.2222) | Bit/dim 2.9863(1.5280) | Xent 1.9028(0.5009) | Xent Color 11.5787(1.2497) | Loss 12.2019(4.8940) | Error 0.3722(0.1505) | Error Color 0.8833(0.0774) |Steps 392(433.49) | Grad Norm 152.8161(30.2601) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 11.4754(10.4066) | Bit/dim 2.0463(1.6955) | Xent 1.2396(0.6487) | Xent Color 1.3208(1.7919) | Loss 6.0113(5.4409) | Error 0.3767(0.2006) | Error Color 0.5033(0.2214) |Steps 512(449.63) | Grad Norm 34.0722(33.6168) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 10.5012(10.4798) | Bit/dim 1.8498(1.7666) | Xent 0.7020(0.6869) | Xent Color 0.9109(1.6025) | Loss 5.1740(5.4514) | Error 0.2411(0.2162) | Error Color 0.3833(0.2739) |Steps 494(460.82) | Grad Norm 12.2242(28.7897) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 10.1087(10.5235) | Bit/dim 1.7313(1.7691) | Xent 0.6215(0.6790) | Xent Color 0.6615(1.3745) | Loss 4.6388(5.3060) | Error 0.1733(0.2138) | Error Color 0.2733(0.2808) |Steps 428(465.09) | Grad Norm 7.7570(23.2717) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 9.2162(10.3326) | Bit/dim 1.6823(1.7509) | Xent 0.6467(0.6514) | Xent Color 0.3737(1.1360) | Loss 4.3620(5.0814) | Error 0.2033(0.2059) | Error Color 0.1356(0.2531) |Steps 392(453.99) | Grad Norm 5.7569(18.4293) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 9.7452(10.2092) | Bit/dim 1.6121(1.7230) | Xent 0.5167(0.6161) | Xent Color 0.3212(0.9262) | Loss 4.1022(4.8565) | Error 0.1667(0.1934) | Error Color 0.1167(0.2180) |Steps 410(447.40) | Grad Norm 3.5440(14.5166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 61.8659, Epoch Time 766.3387(690.4710), Bit/dim 1.5954(best: 1.4434), Xent 0.3473, Xent Color 0.1463. Loss 1.7188, Error 0.1006(best: 0.0921), Error Color 0.0128(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 10.3616(10.2031) | Bit/dim 1.5835(1.6900) | Xent 0.4915(0.5882) | Xent Color 0.2514(0.7549) | Loss 4.0976(5.1926) | Error 0.1411(0.1841) | Error Color 0.0789(0.1832) |Steps 458(444.25) | Grad Norm 4.4919(11.5386) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.9578(10.1763) | Bit/dim 1.5404(1.6559) | Xent 0.4693(0.5686) | Xent Color 0.2243(0.6184) | Loss 3.9507(4.8881) | Error 0.1589(0.1787) | Error Color 0.0767(0.1545) |Steps 404(440.94) | Grad Norm 2.6080(9.5127) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 10.6054(10.1989) | Bit/dim 1.5386(1.6251) | Xent 0.5399(0.5491) | Xent Color 0.1658(0.5104) | Loss 4.0143(4.6584) | Error 0.1600(0.1717) | Error Color 0.0489(0.1312) |Steps 476(440.78) | Grad Norm 2.9894(8.0802) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 10.1657(10.1624) | Bit/dim 1.5010(1.5964) | Xent 0.4900(0.5316) | Xent Color 0.1719(0.4215) | Loss 3.9577(4.4651) | Error 0.1633(0.1672) | Error Color 0.0522(0.1097) |Steps 410(436.26) | Grad Norm 5.3131(6.8274) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 10.4703(10.1608) | Bit/dim 1.5103(1.5712) | Xent 0.4713(0.5107) | Xent Color 0.1513(0.3531) | Loss 3.8277(4.3071) | Error 0.1722(0.1614) | Error Color 0.0378(0.0929) |Steps 428(436.67) | Grad Norm 2.5051(6.3591) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 10.2788(10.1012) | Bit/dim 1.4919(1.5515) | Xent 0.4814(0.4989) | Xent Color 0.1561(0.3001) | Loss 3.8972(4.1828) | Error 0.1422(0.1572) | Error Color 0.0422(0.0805) |Steps 422(434.76) | Grad Norm 8.7696(6.6207) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 10.0286(10.1054) | Bit/dim 1.4713(1.5327) | Xent 0.4702(0.4888) | Xent Color 0.1141(0.2517) | Loss 3.7958(4.0870) | Error 0.1500(0.1547) | Error Color 0.0311(0.0673) |Steps 416(433.74) | Grad Norm 7.2945(6.3494) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 63.8517, Epoch Time 754.6951(692.3977), Bit/dim 1.4790(best: 1.4434), Xent 0.3123, Xent Color 0.0473. Loss 1.5689, Error 0.0953(best: 0.0921), Error Color 0.0016(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 10.2052(10.1002) | Bit/dim 1.4679(1.5186) | Xent 0.4447(0.4869) | Xent Color 0.0965(0.2131) | Loss 3.7975(4.4767) | Error 0.1411(0.1543) | Error Color 0.0211(0.0568) |Steps 446(432.60) | Grad Norm 2.3297(5.6478) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 10.3441(10.0709) | Bit/dim 1.4655(1.5038) | Xent 0.4881(0.4825) | Xent Color 0.0916(0.1818) | Loss 3.7798(4.2891) | Error 0.1544(0.1519) | Error Color 0.0222(0.0475) |Steps 404(431.62) | Grad Norm 4.0818(5.0934) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.6021(10.0691) | Bit/dim 1.4520(1.4902) | Xent 0.4167(0.4719) | Xent Color 0.0838(0.1551) | Loss 3.7118(4.1445) | Error 0.1400(0.1492) | Error Color 0.0178(0.0399) |Steps 416(432.67) | Grad Norm 4.4170(4.6005) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 10.0227(10.0505) | Bit/dim 1.4459(1.4795) | Xent 0.4161(0.4658) | Xent Color 0.0777(0.1350) | Loss 3.7665(4.0388) | Error 0.1322(0.1472) | Error Color 0.0178(0.0344) |Steps 428(431.25) | Grad Norm 4.0087(4.4776) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 10.4603(10.1136) | Bit/dim 1.4403(1.4713) | Xent 0.4954(0.4600) | Xent Color 0.0694(0.1183) | Loss 3.7545(3.9589) | Error 0.1556(0.1454) | Error Color 0.0133(0.0294) |Steps 434(429.99) | Grad Norm 2.7920(4.2986) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 9.8362(10.1307) | Bit/dim 1.4481(1.4626) | Xent 0.4383(0.4524) | Xent Color 0.0590(0.1055) | Loss 3.7060(3.8901) | Error 0.1444(0.1437) | Error Color 0.0144(0.0262) |Steps 452(431.81) | Grad Norm 3.0735(4.5707) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 63.7058, Epoch Time 755.3071(694.2850), Bit/dim 1.4357(best: 1.4434), Xent 0.3009, Xent Color 0.0247. Loss 1.5171, Error 0.0902(best: 0.0921), Error Color 0.0005(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 10.2458(10.1767) | Bit/dim 1.4358(1.4565) | Xent 0.4589(0.4536) | Xent Color 0.0662(0.0957) | Loss 3.6733(4.3899) | Error 0.1478(0.1438) | Error Color 0.0156(0.0238) |Steps 422(432.21) | Grad Norm 3.2792(5.1406) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 10.3580(10.1607) | Bit/dim 1.4338(1.4426) | Xent 0.3972(0.4470) | Xent Color 0.0533(0.0777) | Loss 3.7125(4.0564) | Error 0.1289(0.1422) | Error Color 0.0122(0.0186) |Steps 416(429.99) | Grad Norm 5.3021(5.0561) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 10.1326(10.1643) | Bit/dim 1.4222(1.4368) | Xent 0.4290(0.4384) | Xent Color 0.0462(0.0699) | Loss 3.5441(3.9463) | Error 0.1411(0.1397) | Error Color 0.0078(0.0163) |Steps 428(429.85) | Grad Norm 4.3600(4.8273) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 10.3088(10.1407) | Bit/dim 1.4051(1.4302) | Xent 0.3837(0.4258) | Xent Color 0.0365(0.0629) | Loss 3.5810(3.8565) | Error 0.1122(0.1360) | Error Color 0.0056(0.0143) |Steps 422(429.68) | Grad Norm 1.5648(4.2317) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 10.0337(10.1784) | Bit/dim 1.4109(1.4258) | Xent 0.4331(0.4227) | Xent Color 0.0354(0.0569) | Loss 3.5947(3.7910) | Error 0.1300(0.1354) | Error Color 0.0078(0.0127) |Steps 422(428.86) | Grad Norm 2.1306(4.2571) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 10.5226(10.1352) | Bit/dim 1.4057(1.4197) | Xent 0.3752(0.4202) | Xent Color 0.0320(0.0540) | Loss 3.6259(3.7379) | Error 0.1344(0.1348) | Error Color 0.0011(0.0119) |Steps 452(427.72) | Grad Norm 1.9922(4.4933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 60.3943, Epoch Time 753.2246(696.0532), Bit/dim 1.4076(best: 1.4357), Xent 0.2800, Xent Color 0.0224. Loss 1.4832, Error 0.0839(best: 0.0902), Error Color 0.0020(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 10.2711(10.1415) | Bit/dim 1.4221(1.4160) | Xent 0.3999(0.4164) | Xent Color 0.1963(0.0636) | Loss 3.6920(4.1538) | Error 0.1322(0.1330) | Error Color 0.0767(0.0169) |Steps 458(429.60) | Grad Norm 43.3531(8.4539) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 12.4735(10.3195) | Bit/dim 1.9091(1.5286) | Xent 1.0391(0.4558) | Xent Color 1.1788(1.0748) | Loss 5.5645(4.7224) | Error 0.3611(0.1462) | Error Color 0.4089(0.1454) |Steps 542(441.28) | Grad Norm 24.6411(29.7905) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 10.3164(10.5573) | Bit/dim 1.7252(1.5990) | Xent 0.4910(0.5207) | Xent Color 0.6503(1.0032) | Loss 4.5666(4.7877) | Error 0.1622(0.1684) | Error Color 0.2511(0.1881) |Steps 452(453.85) | Grad Norm 6.7244(25.1645) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 10.9256(10.5078) | Bit/dim 1.5681(1.6041) | Xent 0.5192(0.5092) | Xent Color 0.4581(0.8866) | Loss 4.2039(4.6687) | Error 0.1811(0.1632) | Error Color 0.1600(0.1944) |Steps 488(455.59) | Grad Norm 11.0872(21.1265) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 10.1768(10.5216) | Bit/dim 1.5141(1.5872) | Xent 0.4896(0.5061) | Xent Color 0.2947(0.7381) | Loss 3.9658(4.5108) | Error 0.1533(0.1610) | Error Color 0.0978(0.1722) |Steps 428(455.29) | Grad Norm 5.0799(16.9519) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 10.2620(10.3801) | Bit/dim 1.4919(1.5645) | Xent 0.4741(0.4916) | Xent Color 0.2166(0.6033) | Loss 3.8994(4.3444) | Error 0.1511(0.1560) | Error Color 0.0644(0.1460) |Steps 446(446.08) | Grad Norm 2.2662(13.4533) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 10.0220(10.2970) | Bit/dim 1.4768(1.5397) | Xent 0.4169(0.4776) | Xent Color 0.1554(0.4924) | Loss 3.7442(4.1975) | Error 0.1389(0.1515) | Error Color 0.0344(0.1206) |Steps 446(442.93) | Grad Norm 2.8653(10.7346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 62.9800, Epoch Time 776.8919(698.4783), Bit/dim 1.4654(best: 1.4076), Xent 0.2968, Xent Color 0.0721. Loss 1.5576, Error 0.0890(best: 0.0839), Error Color 0.0050(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 10.0383(10.2197) | Bit/dim 1.4634(1.5181) | Xent 0.4235(0.4644) | Xent Color 0.1451(0.4002) | Loss 3.7325(4.4802) | Error 0.1367(0.1481) | Error Color 0.0367(0.0983) |Steps 428(436.17) | Grad Norm 1.6584(8.4627) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 10.3558(10.2032) | Bit/dim 1.4262(1.4988) | Xent 0.4264(0.4560) | Xent Color 0.1109(0.3236) | Loss 3.6737(4.2714) | Error 0.1311(0.1452) | Error Color 0.0200(0.0785) |Steps 416(434.38) | Grad Norm 2.1265(6.7950) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 9.5817(10.1322) | Bit/dim 1.4133(1.4797) | Xent 0.3523(0.4436) | Xent Color 0.0947(0.2629) | Loss 3.5436(4.1014) | Error 0.1133(0.1414) | Error Color 0.0211(0.0630) |Steps 434(431.49) | Grad Norm 1.8137(5.6859) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 9.3652(10.0561) | Bit/dim 1.4093(1.4645) | Xent 0.3982(0.4359) | Xent Color 0.0835(0.2158) | Loss 3.4988(3.9723) | Error 0.1300(0.1388) | Error Color 0.0167(0.0509) |Steps 404(428.45) | Grad Norm 4.2384(4.8887) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.9363(10.0386) | Bit/dim 1.4162(1.4496) | Xent 0.3891(0.4268) | Xent Color 0.0667(0.1782) | Loss 3.5698(3.8642) | Error 0.1356(0.1360) | Error Color 0.0111(0.0413) |Steps 428(423.38) | Grad Norm 1.5945(4.1107) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 10.1038(9.9873) | Bit/dim 1.4126(1.4403) | Xent 0.4280(0.4166) | Xent Color 0.0618(0.1487) | Loss 3.5785(3.7858) | Error 0.1433(0.1330) | Error Color 0.0111(0.0337) |Steps 440(421.89) | Grad Norm 1.1505(3.6543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 61.6778, Epoch Time 743.9271(699.8418), Bit/dim 1.4056(best: 1.4076), Xent 0.2660, Xent Color 0.0224. Loss 1.4777, Error 0.0799(best: 0.0839), Error Color 0.0008(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.2297(10.0035) | Bit/dim 1.3961(1.4307) | Xent 0.3461(0.4105) | Xent Color 0.0675(0.1263) | Loss 3.5083(4.1864) | Error 0.1122(0.1301) | Error Color 0.0167(0.0282) |Steps 440(423.49) | Grad Norm 4.4460(3.4759) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 10.0557(9.9560) | Bit/dim 1.4014(1.4224) | Xent 0.3695(0.4060) | Xent Color 0.0562(0.1086) | Loss 3.5091(4.0133) | Error 0.1167(0.1285) | Error Color 0.0133(0.0238) |Steps 404(419.24) | Grad Norm 3.1582(3.2508) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.5603(9.9254) | Bit/dim 1.3910(1.4147) | Xent 0.3811(0.3996) | Xent Color 0.0551(0.0937) | Loss 3.4474(3.8762) | Error 0.1100(0.1258) | Error Color 0.0156(0.0201) |Steps 422(415.33) | Grad Norm 0.9212(2.7337) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 10.0665(9.8922) | Bit/dim 1.3752(1.4022) | Xent 0.3910(0.3893) | Xent Color 0.0467(0.0738) | Loss 3.4797(3.6998) | Error 0.1389(0.1250) | Error Color 0.0056(0.0151) |Steps 440(415.69) | Grad Norm 2.5721(2.4321) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.6534(9.8869) | Bit/dim 1.3816(1.3969) | Xent 0.4146(0.3829) | Xent Color 0.0500(0.0668) | Loss 3.5184(3.6452) | Error 0.1344(0.1232) | Error Color 0.0089(0.0136) |Steps 404(414.92) | Grad Norm 3.1651(2.4788) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 10.0303(9.9316) | Bit/dim 1.3587(1.3902) | Xent 0.3044(0.3797) | Xent Color 0.0446(0.0606) | Loss 3.4433(3.5994) | Error 0.1011(0.1233) | Error Color 0.0100(0.0121) |Steps 428(417.71) | Grad Norm 1.7360(2.3754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 61.9504, Epoch Time 738.2884(700.9952), Bit/dim 1.3762(best: 1.4056), Xent 0.2345, Xent Color 0.0142. Loss 1.4384, Error 0.0723(best: 0.0799), Error Color 0.0007(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 10.1434(9.9489) | Bit/dim 1.3681(1.3857) | Xent 0.3994(0.3754) | Xent Color 0.0502(0.0560) | Loss 3.5085(3.9902) | Error 0.1311(0.1222) | Error Color 0.0122(0.0109) |Steps 404(416.15) | Grad Norm 2.8043(2.4486) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 10.2813(9.9615) | Bit/dim 1.3665(1.3825) | Xent 0.3364(0.3707) | Xent Color 0.0340(0.0516) | Loss 3.4635(3.8512) | Error 0.1067(0.1191) | Error Color 0.0044(0.0096) |Steps 404(416.46) | Grad Norm 2.8840(2.4851) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 10.2202(10.0213) | Bit/dim 1.3616(1.3775) | Xent 0.3380(0.3599) | Xent Color 0.0382(0.0480) | Loss 3.4216(3.7423) | Error 0.1222(0.1158) | Error Color 0.0056(0.0088) |Steps 404(418.17) | Grad Norm 1.2282(2.2809) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 10.2882(9.9868) | Bit/dim 1.3531(1.3725) | Xent 0.3620(0.3589) | Xent Color 0.0343(0.0451) | Loss 3.4543(3.6631) | Error 0.1244(0.1164) | Error Color 0.0067(0.0083) |Steps 422(417.85) | Grad Norm 1.8181(2.1539) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.7616(9.9234) | Bit/dim 1.3435(1.3670) | Xent 0.3362(0.3519) | Xent Color 0.0297(0.0423) | Loss 3.3491(3.5907) | Error 0.1056(0.1141) | Error Color 0.0033(0.0077) |Steps 422(416.22) | Grad Norm 1.3816(2.0516) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 10.4540(9.9384) | Bit/dim 1.3447(1.3621) | Xent 0.3587(0.3496) | Xent Color 0.0301(0.0411) | Loss 3.3641(3.5409) | Error 0.1089(0.1134) | Error Color 0.0033(0.0077) |Steps 428(417.17) | Grad Norm 1.4577(2.0233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 62.1343, Epoch Time 742.9342(702.2534), Bit/dim 1.3483(best: 1.3762), Xent 0.2110, Xent Color 0.0096. Loss 1.4035, Error 0.0660(best: 0.0723), Error Color 0.0001(best: 0.0005)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 10.0572(9.9650) | Bit/dim 1.3583(1.3595) | Xent 0.3007(0.3431) | Xent Color 0.0405(0.0394) | Loss 3.3857(3.9878) | Error 0.1011(0.1110) | Error Color 0.0078(0.0074) |Steps 416(417.15) | Grad Norm 3.8469(2.2837) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 9.7290(9.9681) | Bit/dim 1.3398(1.3560) | Xent 0.3771(0.3361) | Xent Color 0.0244(0.0377) | Loss 3.4038(3.8278) | Error 0.1211(0.1075) | Error Color 0.0022(0.0068) |Steps 404(417.33) | Grad Norm 1.9670(2.2682) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 9.4219(9.9215) | Bit/dim 1.3334(1.3508) | Xent 0.2810(0.3319) | Xent Color 0.0343(0.0366) | Loss 3.3303(3.7108) | Error 0.1000(0.1065) | Error Color 0.0067(0.0065) |Steps 416(417.98) | Grad Norm 1.1222(2.0481) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 10.0271(9.9377) | Bit/dim 1.3559(1.3477) | Xent 0.3832(0.3281) | Xent Color 0.0213(0.0341) | Loss 3.4411(3.6264) | Error 0.1211(0.1058) | Error Color 0.0022(0.0059) |Steps 416(417.82) | Grad Norm 2.8126(2.0019) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 10.1367(9.9611) | Bit/dim 1.3373(1.3435) | Xent 0.3371(0.3255) | Xent Color 0.0279(0.0336) | Loss 3.4000(3.5584) | Error 0.1167(0.1050) | Error Color 0.0044(0.0059) |Steps 404(418.84) | Grad Norm 2.7960(2.1518) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 10.2054(9.9568) | Bit/dim 1.3283(1.3397) | Xent 0.3178(0.3181) | Xent Color 0.0207(0.0325) | Loss 3.4142(3.5031) | Error 0.1011(0.1025) | Error Color 0.0033(0.0056) |Steps 422(418.49) | Grad Norm 1.7150(2.3731) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 9.8727(10.0056) | Bit/dim 1.3152(1.3363) | Xent 0.2640(0.3077) | Xent Color 0.0264(0.0314) | Loss 3.3012(3.4567) | Error 0.0867(0.0992) | Error Color 0.0067(0.0054) |Steps 416(419.84) | Grad Norm 2.4925(2.5566) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 61.0468, Epoch Time 744.0093(703.5060), Bit/dim 1.3240(best: 1.3483), Xent 0.1967, Xent Color 0.0085. Loss 1.3754, Error 0.0612(best: 0.0660), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 10.1045(10.0583) | Bit/dim 1.3233(1.3316) | Xent 0.2616(0.3062) | Xent Color 0.0264(0.0314) | Loss 3.2602(3.8783) | Error 0.0856(0.0983) | Error Color 0.0033(0.0056) |Steps 428(421.22) | Grad Norm 2.1167(2.7254) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 10.7800(10.0730) | Bit/dim 1.3170(1.3286) | Xent 0.2873(0.2998) | Xent Color 0.0194(0.0298) | Loss 3.3788(3.7357) | Error 0.0933(0.0969) | Error Color 0.0022(0.0051) |Steps 434(421.95) | Grad Norm 1.8594(2.7252) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.7694(10.0676) | Bit/dim 1.3067(1.3248) | Xent 0.3156(0.3000) | Xent Color 0.0327(0.0300) | Loss 3.3297(3.6333) | Error 0.1000(0.0967) | Error Color 0.0044(0.0054) |Steps 422(422.88) | Grad Norm 6.2202(3.1565) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.8256(10.0573) | Bit/dim 1.3039(1.3218) | Xent 0.3245(0.2984) | Xent Color 0.0293(0.0288) | Loss 3.3539(3.5564) | Error 0.0978(0.0962) | Error Color 0.0067(0.0050) |Steps 422(423.33) | Grad Norm 4.5489(3.2932) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.6829(9.9962) | Bit/dim 1.2909(1.3165) | Xent 0.3142(0.2958) | Xent Color 0.0217(0.0278) | Loss 3.2415(3.4877) | Error 0.0900(0.0947) | Error Color 0.0022(0.0049) |Steps 416(421.72) | Grad Norm 2.7694(3.2531) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 10.2648(10.0281) | Bit/dim 1.3077(1.3128) | Xent 0.3180(0.2892) | Xent Color 0.0239(0.0275) | Loss 3.3008(3.4392) | Error 0.0989(0.0934) | Error Color 0.0067(0.0050) |Steps 368(418.92) | Grad Norm 1.8086(3.1581) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.9775(9.9744) | Bit/dim 1.2958(1.3086) | Xent 0.2903(0.2857) | Xent Color 0.0213(0.0263) | Loss 3.2706(3.3987) | Error 0.0978(0.0923) | Error Color 0.0033(0.0046) |Steps 422(419.02) | Grad Norm 3.0684(3.0571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 62.8287, Epoch Time 747.1075(704.8141), Bit/dim 1.2969(best: 1.3240), Xent 0.1726, Xent Color 0.0059. Loss 1.3415, Error 0.0569(best: 0.0612), Error Color 0.0001(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.1975(10.0297) | Bit/dim 1.3069(1.3056) | Xent 0.2487(0.2817) | Xent Color 0.0280(0.0257) | Loss 3.3416(3.7597) | Error 0.0811(0.0915) | Error Color 0.0122(0.0047) |Steps 434(420.94) | Grad Norm 4.4243(2.9681) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 10.1103(10.0145) | Bit/dim 1.2799(1.3011) | Xent 0.2399(0.2785) | Xent Color 0.0247(0.0254) | Loss 3.2342(3.6287) | Error 0.0833(0.0902) | Error Color 0.0044(0.0047) |Steps 440(421.68) | Grad Norm 2.6291(2.9229) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 10.4944(10.0395) | Bit/dim 1.2865(1.2972) | Xent 0.2558(0.2773) | Xent Color 0.0281(0.0261) | Loss 3.2490(3.5328) | Error 0.0800(0.0901) | Error Color 0.0078(0.0052) |Steps 428(420.18) | Grad Norm 4.9942(3.7551) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 10.1052(10.0460) | Bit/dim 1.2805(1.2926) | Xent 0.2418(0.2752) | Xent Color 0.0181(0.0249) | Loss 3.2473(3.4560) | Error 0.0767(0.0886) | Error Color 0.0022(0.0047) |Steps 434(421.23) | Grad Norm 2.7624(3.8350) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 10.0089(10.0175) | Bit/dim 1.2772(1.2892) | Xent 0.2576(0.2721) | Xent Color 0.0257(0.0248) | Loss 3.2153(3.3972) | Error 0.0822(0.0875) | Error Color 0.0067(0.0044) |Steps 410(420.51) | Grad Norm 4.6222(4.3837) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.4467(10.0306) | Bit/dim 1.2718(1.2862) | Xent 0.2780(0.2699) | Xent Color 0.0236(0.0261) | Loss 3.2313(3.3613) | Error 0.0900(0.0876) | Error Color 0.0033(0.0048) |Steps 404(418.89) | Grad Norm 5.6309(5.5845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 62.6429, Epoch Time 749.9527(706.1682), Bit/dim 1.2702(best: 1.2969), Xent 0.1534, Xent Color 0.0060. Loss 1.3100, Error 0.0490(best: 0.0569), Error Color 0.0000(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 9.7311(10.0424) | Bit/dim 1.2620(1.2818) | Xent 0.3084(0.2674) | Xent Color 0.0238(0.0256) | Loss 3.2423(3.7884) | Error 0.1011(0.0867) | Error Color 0.0044(0.0048) |Steps 398(418.31) | Grad Norm 4.3040(5.5561) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 9.9870(10.0026) | Bit/dim 1.2559(1.2762) | Xent 0.2949(0.2625) | Xent Color 0.0171(0.0245) | Loss 3.2438(3.6344) | Error 0.0844(0.0850) | Error Color 0.0022(0.0047) |Steps 404(416.98) | Grad Norm 4.1416(5.0806) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 10.3617(10.0311) | Bit/dim 1.2637(1.2729) | Xent 0.2686(0.2586) | Xent Color 0.0204(0.0232) | Loss 3.2095(3.5206) | Error 0.0856(0.0836) | Error Color 0.0011(0.0043) |Steps 410(417.88) | Grad Norm 6.1645(4.9489) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 9.8325(10.0333) | Bit/dim 1.2628(1.2690) | Xent 0.2565(0.2572) | Xent Color 0.0176(0.0223) | Loss 3.1952(3.4334) | Error 0.0911(0.0836) | Error Color 0.0033(0.0041) |Steps 404(418.05) | Grad Norm 2.9620(4.9843) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 10.5599(10.0277) | Bit/dim 1.2629(1.2636) | Xent 0.2439(0.2574) | Xent Color 0.0145(0.0219) | Loss 3.2189(3.3709) | Error 0.0667(0.0836) | Error Color 0.0033(0.0041) |Steps 422(417.59) | Grad Norm 3.6843(4.9628) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 10.1264(10.0361) | Bit/dim 1.2533(1.2595) | Xent 0.2547(0.2557) | Xent Color 0.0226(0.0211) | Loss 3.1567(3.3211) | Error 0.0844(0.0826) | Error Color 0.0033(0.0039) |Steps 422(417.90) | Grad Norm 7.4378(5.0541) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 10.0305(10.0415) | Bit/dim 1.2508(1.2575) | Xent 0.2329(0.2509) | Xent Color 0.0315(0.0218) | Loss 3.2308(3.2894) | Error 0.0578(0.0807) | Error Color 0.0089(0.0040) |Steps 422(418.71) | Grad Norm 9.9161(5.5619) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 61.9588, Epoch Time 747.6013(707.4112), Bit/dim 1.2415(best: 1.2702), Xent 0.1444, Xent Color 0.0038. Loss 1.2786, Error 0.0454(best: 0.0490), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 9.6919(10.0061) | Bit/dim 1.2373(1.2543) | Xent 0.2059(0.2470) | Xent Color 0.0193(0.0222) | Loss 3.0914(3.6713) | Error 0.0600(0.0790) | Error Color 0.0056(0.0046) |Steps 410(418.69) | Grad Norm 2.7242(5.6964) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 9.7051(9.9915) | Bit/dim 1.2100(1.2491) | Xent 0.2329(0.2462) | Xent Color 0.0175(0.0205) | Loss 3.1510(3.5382) | Error 0.0656(0.0788) | Error Color 0.0033(0.0041) |Steps 422(418.40) | Grad Norm 2.4558(5.1079) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 10.3315(10.0103) | Bit/dim 1.2130(1.2423) | Xent 0.2234(0.2439) | Xent Color 0.0189(0.0203) | Loss 3.1054(3.4335) | Error 0.0733(0.0787) | Error Color 0.0033(0.0040) |Steps 422(418.57) | Grad Norm 2.6704(4.8118) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 9.6604(9.9752) | Bit/dim 1.2259(1.2375) | Xent 0.2372(0.2411) | Xent Color 0.0133(0.0186) | Loss 3.1420(3.3652) | Error 0.0833(0.0780) | Error Color 0.0011(0.0033) |Steps 398(419.30) | Grad Norm 2.3597(4.1828) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 9.6826(9.9472) | Bit/dim 1.2209(1.2334) | Xent 0.2359(0.2397) | Xent Color 0.0178(0.0184) | Loss 3.1704(3.3038) | Error 0.0722(0.0767) | Error Color 0.0033(0.0032) |Steps 422(420.47) | Grad Norm 5.8270(4.3340) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 10.2752(9.9859) | Bit/dim 1.2054(1.2297) | Xent 0.2156(0.2370) | Xent Color 0.0171(0.0179) | Loss 3.1416(3.2614) | Error 0.0700(0.0756) | Error Color 0.0033(0.0031) |Steps 422(420.40) | Grad Norm 3.3210(4.3083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 62.3700, Epoch Time 743.5846(708.4964), Bit/dim 1.2082(best: 1.2415), Xent 0.1310, Xent Color 0.0033. Loss 1.2418, Error 0.0415(best: 0.0454), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 10.0684(9.9779) | Bit/dim 1.2055(1.2241) | Xent 0.2635(0.2371) | Xent Color 0.0185(0.0175) | Loss 3.1503(3.7248) | Error 0.0667(0.0748) | Error Color 0.0033(0.0030) |Steps 416(419.38) | Grad Norm 4.9567(4.5162) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 9.9206(9.9237) | Bit/dim 2.5190(1.4749) | Xent 1.3220(0.3819) | Xent Color 16.3552(3.5720) | Loss 13.2782(5.6877) | Error 0.4456(0.1056) | Error Color 0.8411(0.1304) |Steps 440(419.51) | Grad Norm 146.3088(47.2965) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 12.1906(10.3563) | Bit/dim 2.2237(1.6830) | Xent 0.6858(0.5327) | Xent Color 1.6420(3.5716) | Loss 6.2260(6.0845) | Error 0.2133(0.1519) | Error Color 0.5544(0.2716) |Steps 566(439.58) | Grad Norm 17.1967(42.9415) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 13.6337(11.0067) | Bit/dim 2.0843(1.8083) | Xent 0.5037(0.5574) | Xent Color 1.0597(2.9509) | Loss 5.7499(6.0247) | Error 0.1656(0.1667) | Error Color 0.4067(0.3164) |Steps 608(479.86) | Grad Norm 13.9765(36.7100) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 12.6043(11.4620) | Bit/dim 2.0048(1.8662) | Xent 0.5142(0.5532) | Xent Color 0.8193(2.4077) | Loss 5.3105(5.8641) | Error 0.1733(0.1693) | Error Color 0.3033(0.3222) |Steps 572(507.71) | Grad Norm 16.1598(452052526.4730) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 13.4889(11.8710) | Bit/dim 1.9423(1.8896) | Xent 0.4255(0.5355) | Xent Color 0.6280(1.9534) | Loss 5.0277(5.6552) | Error 0.1356(0.1643) | Error Color 0.2544(0.3082) |Steps 572(523.89) | Grad Norm 20.8449(333354443.7642) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 13.7357(12.2384) | Bit/dim 1.8501(1.8911) | Xent 0.4421(0.5144) | Xent Color 0.4179(1.5682) | Loss 4.8172(5.4659) | Error 0.1411(0.1595) | Error Color 0.1478(0.2747) |Steps 608(544.67) | Grad Norm 10.9137(245823612.9836) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 80.2098, Epoch Time 907.3125(714.4609), Bit/dim 1.8510(best: 1.2082), Xent 0.2765, Xent Color 0.2812. Loss 1.9904, Error 0.0811(best: 0.0415), Error Color 0.0839(best: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_beta.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/infocnf_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_beta_0_1_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10 --beta 0.1\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
