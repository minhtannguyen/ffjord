{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_multiscale.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    z_sup_class = [o[:,:int(np.prod(o.size()[1:])*0.5)] for o in z]\n",
      "    z_sup_class = torch.cat(z_sup_class,1)\n",
      "    \n",
      "    z_sup_color = [o[:,int(np.prod(o.size()[1:])*0.5):] for o in z]\n",
      "    z_sup_color = torch.cat(z_sup_color,1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup_class).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z_sup_color).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup_class = model.module.dropout(z_sup_class)\n",
      "        z_sup_color = model.module.dropout_color(z_sup_color)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup_class)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(z_sup_color)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio * 2.,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_color_sup = fixed_z_color_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_color_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_color_sup = start_color_sup + (2**(ns-2))*a_color_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_color_sup = end_color_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_color_sup = start_color_sup + a_color_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 19.5547(33.7215) | Bit/dim 25.4505(27.3296) | Xent 2.2869(2.3009) | Xent Color 2.3007(2.3023) | Loss 48.6135(51.8011) | Error 0.8744(0.8882) | Error Color 0.9067(0.8898) |Steps 296(297.28) | Grad Norm 260.8693(274.9979) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 19.6541(30.1838) | Bit/dim 20.0909(26.0735) | Xent 2.2513(2.2928) | Xent Color 2.2917(2.3003) | Loss 39.2005(49.5902) | Error 0.8644(0.8869) | Error Color 0.8811(0.8910) |Steps 308(299.42) | Grad Norm 216.4387(264.8258) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 22.3404(27.6342) | Bit/dim 13.8656(23.5615) | Xent 2.1974(2.2735) | Xent Color 2.2745(2.2954) | Loss 27.6680(45.0994) | Error 0.5411(0.8307) | Error Color 0.7822(0.8687) |Steps 332(304.30) | Grad Norm 159.2129(243.6358) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 22.7694(26.4002) | Bit/dim 8.9832(20.2265) | Xent 2.1211(2.2412) | Xent Color 2.2517(2.2866) | Loss 18.9604(39.0992) | Error 0.3322(0.7163) | Error Color 0.7511(0.8413) |Steps 350(313.52) | Grad Norm 95.5079(211.8928) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 26.3617(25.9724) | Bit/dim 6.6884(16.8755) | Xent 2.0425(2.1962) | Xent Color 2.2351(2.2744) | Loss 14.8290(33.0831) | Error 0.3333(0.6080) | Error Color 0.7467(0.8130) |Steps 356(329.43) | Grad Norm 33.8641(171.6084) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 26.6455(25.9976) | Bit/dim 6.0965(14.1054) | Xent 1.9660(2.1429) | Xent Color 2.1978(2.2582) | Loss 13.8238(28.1133) | Error 0.3422(0.5346) | Error Color 0.7800(0.8032) |Steps 404(348.53) | Grad Norm 25.0497(132.5926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 67.1738, Epoch Time 1609.9125(1609.9125), Bit/dim 5.7409(best: inf), Xent 1.8894, Xent Color 2.1737. Loss 6.7567, Error 0.2505(best: inf), Error Color 0.7276(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 24.6794(25.8931) | Bit/dim 5.5568(11.9159) | Xent 1.8885(2.0823) | Xent Color 2.1676(2.2383) | Loss 12.6935(24.6227) | Error 0.3100(0.4738) | Error Color 0.6700(0.7873) |Steps 374(356.79) | Grad Norm 18.3100(103.8212) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 25.1401(25.7836) | Bit/dim 4.9704(10.1646) | Xent 1.8138(2.0166) | Xent Color 2.1314(2.2142) | Loss 11.5145(21.3213) | Error 0.3022(0.4245) | Error Color 0.6000(0.7364) |Steps 368(361.46) | Grad Norm 14.4526(80.3123) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 24.3621(25.6165) | Bit/dim 4.5811(8.7587) | Xent 1.7558(1.9542) | Xent Color 2.1022(2.1889) | Loss 10.9425(18.6679) | Error 0.2756(0.3892) | Error Color 0.6278(0.7121) |Steps 380(364.76) | Grad Norm 13.4107(63.1011) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 24.7903(25.4024) | Bit/dim 4.1664(7.6068) | Xent 1.7182(1.8965) | Xent Color 2.0705(2.1608) | Loss 9.9140(16.4895) | Error 0.2700(0.3599) | Error Color 0.5889(0.6806) |Steps 374(368.00) | Grad Norm 9.0484(49.1762) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 25.3996(25.4263) | Bit/dim 3.7622(6.6452) | Xent 1.7104(1.8495) | Xent Color 2.0338(2.1304) | Loss 9.2628(14.6919) | Error 0.2900(0.3392) | Error Color 0.5344(0.6460) |Steps 374(370.31) | Grad Norm 8.0413(38.5382) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 24.2925(25.2253) | Bit/dim 3.3653(5.8283) | Xent 1.7375(1.8180) | Xent Color 2.0034(2.1006) | Loss 8.6036(13.1562) | Error 0.3022(0.3279) | Error Color 0.5533(0.6173) |Steps 386(372.59) | Grad Norm 8.2500(30.6075) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 24.9260(25.1992) | Bit/dim 3.0526(5.1349) | Xent 1.7686(1.8017) | Xent Color 1.9625(2.0678) | Loss 7.8603(11.8673) | Error 0.3089(0.3252) | Error Color 0.5411(0.5919) |Steps 386(375.83) | Grad Norm 6.3195(24.4516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 66.2119, Epoch Time 1743.0038(1613.9052), Bit/dim 2.9671(best: 5.7409), Xent 1.7650, Xent Color 1.9373. Loss 3.8927, Error 0.2728(best: 0.2505), Error Color 0.4007(best: 0.7276)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 26.1123(25.3877) | Bit/dim 2.7925(4.5470) | Xent 1.8212(1.8013) | Xent Color 1.9172(2.0341) | Loss 7.5883(11.2088) | Error 0.3511(0.3309) | Error Color 0.4789(0.5642) |Steps 410(382.52) | Grad Norm 5.6833(19.5809) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 27.4009(25.7090) | Bit/dim 2.6174(4.0578) | Xent 1.8334(1.8090) | Xent Color 1.8795(1.9970) | Loss 7.2931(10.1992) | Error 0.3656(0.3394) | Error Color 0.4844(0.5413) |Steps 416(386.87) | Grad Norm 4.3351(15.6802) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 28.3572(25.8513) | Bit/dim 2.4789(3.6578) | Xent 1.8511(1.8204) | Xent Color 1.8173(1.9568) | Loss 7.0442(9.3927) | Error 0.3756(0.3494) | Error Color 0.4633(0.5223) |Steps 428(391.10) | Grad Norm 3.2785(12.5221) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 28.9337(26.0381) | Bit/dim 2.4090(3.3358) | Xent 1.8710(1.8333) | Xent Color 1.7650(1.9131) | Loss 6.8181(8.7392) | Error 0.4067(0.3620) | Error Color 0.4389(0.5023) |Steps 386(393.08) | Grad Norm 2.7682(10.0175) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 25.8742(26.5165) | Bit/dim 2.3311(3.0782) | Xent 1.8909(1.8464) | Xent Color 1.7012(1.8651) | Loss 6.7734(8.2183) | Error 0.4244(0.3752) | Error Color 0.3956(0.4827) |Steps 404(398.10) | Grad Norm 2.4629(8.0933) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 28.2666(26.9474) | Bit/dim 2.2726(2.8714) | Xent 1.8944(1.8565) | Xent Color 1.6574(1.8142) | Loss 6.5899(7.7882) | Error 0.4322(0.3885) | Error Color 0.4000(0.4624) |Steps 416(397.00) | Grad Norm 1.9854(6.5953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 70.3937, Epoch Time 1877.1228(1621.8017), Bit/dim 2.2406(best: 2.9671), Xent 1.8488, Xent Color 1.5590. Loss 3.0926, Error 0.3303(best: 0.2505), Error Color 0.3205(best: 0.4007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 28.7556(27.1013) | Bit/dim 2.2369(2.7060) | Xent 1.8804(1.8628) | Xent Color 1.5552(1.7596) | Loss 6.4901(7.9666) | Error 0.4267(0.3986) | Error Color 0.3611(0.4416) |Steps 392(396.25) | Grad Norm 2.9062(5.6168) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 25.8024(27.0836) | Bit/dim 2.1969(2.5768) | Xent 1.8212(1.8583) | Xent Color 1.4897(1.7008) | Loss 6.3079(7.5435) | Error 0.4022(0.4033) | Error Color 0.3600(0.4235) |Steps 404(395.69) | Grad Norm 1.7888(4.7525) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 25.9849(26.8270) | Bit/dim 2.1935(2.4774) | Xent 1.7891(1.8448) | Xent Color 1.4049(1.6344) | Loss 6.1331(7.2105) | Error 0.3756(0.4025) | Error Color 0.3744(0.4062) |Steps 392(395.35) | Grad Norm 1.9547(4.2179) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 26.5763(26.7250) | Bit/dim 2.1735(2.4017) | Xent 1.7253(1.8199) | Xent Color 1.3348(1.5587) | Loss 6.2113(6.9483) | Error 0.3633(0.3966) | Error Color 0.3722(0.3903) |Steps 416(396.50) | Grad Norm 15.3492(4.9751) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 24.2252(26.4594) | Bit/dim 2.1952(2.3445) | Xent 1.6687(1.7879) | Xent Color 1.2999(1.4933) | Loss 6.2196(6.7427) | Error 0.3622(0.3885) | Error Color 0.3678(0.3875) |Steps 404(395.81) | Grad Norm 38.6858(10.2811) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 26.3945(26.3002) | Bit/dim 2.1615(2.2988) | Xent 1.6233(1.7444) | Xent Color 1.1964(1.4226) | Loss 6.0424(6.5659) | Error 0.3689(0.3747) | Error Color 0.3122(0.3711) |Steps 410(397.63) | Grad Norm 17.9875(13.0590) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 24.4895(26.0599) | Bit/dim 2.1560(2.2610) | Xent 1.5278(1.6948) | Xent Color 1.1006(1.3497) | Loss 5.8924(6.4068) | Error 0.3200(0.3614) | Error Color 0.2478(0.3444) |Steps 398(397.54) | Grad Norm 5.8862(13.4282) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 68.7200, Epoch Time 1810.6381(1627.4668), Bit/dim 2.2639(best: 2.2406), Xent 1.4234, Xent Color 1.9053. Loss 3.0961, Error 0.2461(best: 0.2505), Error Color 0.7535(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 26.5104(25.9264) | Bit/dim 2.1904(2.2433) | Xent 1.4021(1.6347) | Xent Color 1.1123(1.3393) | Loss 5.8864(6.7806) | Error 0.2889(0.3472) | Error Color 0.3133(0.3787) |Steps 398(397.51) | Grad Norm 30.3235(26.9723) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 24.5235(25.7496) | Bit/dim 2.1653(2.2253) | Xent 1.3247(1.5696) | Xent Color 1.0662(1.2788) | Loss 5.9299(6.5542) | Error 0.2567(0.3333) | Error Color 0.2189(0.3611) |Steps 410(399.05) | Grad Norm 21.5928(27.9804) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 24.2071(25.7126) | Bit/dim 2.1553(2.2092) | Xent 1.2923(1.5015) | Xent Color 0.9834(1.2106) | Loss 5.7693(6.3527) | Error 0.2689(0.3192) | Error Color 0.1856(0.3251) |Steps 386(397.16) | Grad Norm 12.2807(25.2552) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 24.9803(25.5555) | Bit/dim 2.1806(2.1969) | Xent 1.1451(1.4300) | Xent Color 0.8933(1.1418) | Loss 5.7083(6.1826) | Error 0.2422(0.3060) | Error Color 0.1578(0.2910) |Steps 410(396.43) | Grad Norm 6.5056(21.4481) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 24.7912(25.5293) | Bit/dim 2.1730(2.1918) | Xent 1.1081(1.3515) | Xent Color 0.8771(1.0704) | Loss 5.6493(6.0556) | Error 0.2400(0.2904) | Error Color 0.2122(0.2648) |Steps 410(397.42) | Grad Norm 18.2041(18.9293) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 24.0286(25.2898) | Bit/dim 2.1936(2.1905) | Xent 1.0335(1.2700) | Xent Color 1.5295(1.0298) | Loss 5.9736(5.9607) | Error 0.2311(0.2767) | Error Color 0.7022(0.2616) |Steps 398(397.84) | Grad Norm 166.1099(24.6717) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 26.2065(25.3637) | Bit/dim 2.2257(2.2125) | Xent 0.9926(1.1980) | Xent Color 1.4506(1.2853) | Loss 5.8502(6.0467) | Error 0.2333(0.2663) | Error Color 0.6333(0.3624) |Steps 398(398.11) | Grad Norm 71.2681(48.8374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 66.6265, Epoch Time 1758.6807(1631.4032), Bit/dim 2.1850(best: 2.2406), Xent 0.9086, Xent Color 1.0069. Loss 2.6639, Error 0.1785(best: 0.2461), Error Color 0.3289(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 25.0387(25.3565) | Bit/dim 2.1565(2.2050) | Xent 1.0363(1.1441) | Xent Color 1.0932(1.2487) | Loss 5.7592(6.3832) | Error 0.2389(0.2591) | Error Color 0.3689(0.3815) |Steps 410(396.48) | Grad Norm 22.1396(44.4959) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 26.2933(25.2970) | Bit/dim 2.1357(2.1883) | Xent 0.9979(1.1078) | Xent Color 1.0593(1.2012) | Loss 5.6675(6.1920) | Error 0.2278(0.2547) | Error Color 0.2833(0.3631) |Steps 410(395.99) | Grad Norm 22.5721(38.5361) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 25.6800(25.2683) | Bit/dim 2.1190(2.1739) | Xent 0.9053(1.0652) | Xent Color 0.9431(1.1450) | Loss 5.4125(6.0310) | Error 0.2267(0.2501) | Error Color 0.2233(0.3293) |Steps 410(396.09) | Grad Norm 10.6500(31.8108) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 25.4455(25.2261) | Bit/dim 2.1376(2.1627) | Xent 0.8286(1.0100) | Xent Color 0.8927(1.0874) | Loss 5.5032(5.8945) | Error 0.2356(0.2416) | Error Color 0.1989(0.2992) |Steps 410(395.77) | Grad Norm 7.7628(25.5422) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 25.0979(25.1359) | Bit/dim 2.1300(2.1565) | Xent 0.8002(0.9543) | Xent Color 0.8654(1.0294) | Loss 5.3198(5.7748) | Error 0.2100(0.2342) | Error Color 0.1744(0.2689) |Steps 374(393.09) | Grad Norm 9.7444(20.4055) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 26.0042(25.2036) | Bit/dim 2.1332(2.1501) | Xent 0.8225(0.9134) | Xent Color 0.8249(0.9767) | Loss 5.4193(5.6840) | Error 0.2411(0.2285) | Error Color 0.1867(0.2457) |Steps 410(394.24) | Grad Norm 11.7521(18.8260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 67.7666, Epoch Time 1748.7122(1634.9225), Bit/dim 2.1355(best: 2.1850), Xent 0.6575, Xent Color 0.7503. Loss 2.4874, Error 0.1485(best: 0.1785), Error Color 0.1638(best: 0.3205)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 25.4840(25.1607) | Bit/dim 2.1478(2.1437) | Xent 0.7471(0.8752) | Xent Color 0.7700(0.9302) | Loss 5.4199(6.0951) | Error 0.1944(0.2227) | Error Color 0.1856(0.2323) |Steps 410(394.24) | Grad Norm 24.8557(21.1592) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 23.9022(25.0884) | Bit/dim 2.1314(2.1378) | Xent 0.7181(0.8411) | Xent Color 0.7399(0.8948) | Loss 5.4076(5.9114) | Error 0.2044(0.2182) | Error Color 0.1578(0.2291) |Steps 398(395.83) | Grad Norm 20.7547(25.6689) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 25.0356(25.0632) | Bit/dim 2.1362(2.1327) | Xent 0.6849(0.8056) | Xent Color 0.6494(0.8441) | Loss 5.2435(5.7564) | Error 0.2011(0.2117) | Error Color 0.1289(0.2074) |Steps 398(395.99) | Grad Norm 5.7569(23.4609) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 24.5172(25.0324) | Bit/dim 2.1035(2.1252) | Xent 0.6864(0.7773) | Xent Color 0.6934(0.7937) | Loss 5.3768(5.6317) | Error 0.1867(0.2059) | Error Color 0.2089(0.1875) |Steps 404(396.95) | Grad Norm 47.5229(20.4884) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 26.7565(24.9346) | Bit/dim 2.1420(2.1317) | Xent 0.7038(0.7598) | Xent Color 1.0538(0.9547) | Loss 5.4864(5.6502) | Error 0.2144(0.2047) | Error Color 0.3778(0.2596) |Steps 410(397.42) | Grad Norm 62.2525(42.4595) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 25.8330(24.9779) | Bit/dim 2.1144(2.1243) | Xent 0.7458(0.7528) | Xent Color 0.7656(0.9216) | Loss 5.3447(5.5828) | Error 0.2189(0.2054) | Error Color 0.2056(0.2646) |Steps 392(398.49) | Grad Norm 18.9626(39.0957) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 24.6768(24.9833) | Bit/dim 2.0849(2.1144) | Xent 0.7329(0.7428) | Xent Color 0.6539(0.8632) | Loss 5.2907(5.5043) | Error 0.2078(0.2048) | Error Color 0.1367(0.2371) |Steps 404(399.13) | Grad Norm 11.3656(32.5252) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 67.4736, Epoch Time 1733.6910(1637.8856), Bit/dim 2.1015(best: 2.1355), Xent 0.5469, Xent Color 0.5958. Loss 2.3872, Error 0.1351(best: 0.1485), Error Color 0.0645(best: 0.1638)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 24.4193(24.9947) | Bit/dim 2.0758(2.1073) | Xent 0.6107(0.7172) | Xent Color 0.6225(0.8031) | Loss 5.2267(5.8789) | Error 0.1756(0.1998) | Error Color 0.1278(0.2098) |Steps 404(400.10) | Grad Norm 10.5123(26.4830) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 24.4274(24.9875) | Bit/dim 2.0681(2.1008) | Xent 0.6647(0.6992) | Xent Color 0.5511(0.7455) | Loss 5.1631(5.6894) | Error 0.2000(0.1980) | Error Color 0.1022(0.1865) |Steps 398(399.81) | Grad Norm 12.4724(21.9079) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 26.1344(25.1571) | Bit/dim 2.0768(2.0955) | Xent 0.6276(0.6839) | Xent Color 0.5656(0.6962) | Loss 5.0386(5.5562) | Error 0.1789(0.1958) | Error Color 0.1189(0.1675) |Steps 386(401.34) | Grad Norm 21.9478(19.7757) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 27.7834(25.4911) | Bit/dim 2.0492(2.0868) | Xent 0.6319(0.6697) | Xent Color 0.5049(0.6473) | Loss 5.0775(5.4425) | Error 0.1911(0.1930) | Error Color 0.1078(0.1491) |Steps 404(403.18) | Grad Norm 18.3382(17.2150) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 25.9464(25.5485) | Bit/dim 2.0646(2.0810) | Xent 0.5794(0.6542) | Xent Color 0.4690(0.6038) | Loss 5.0931(5.3465) | Error 0.1678(0.1889) | Error Color 0.0933(0.1353) |Steps 404(402.49) | Grad Norm 15.5149(17.1376) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 24.4581(25.5111) | Bit/dim 2.1399(2.0918) | Xent 0.6079(0.6440) | Xent Color 1.0369(0.8218) | Loss 5.3304(5.4193) | Error 0.1900(0.1875) | Error Color 0.4533(0.2201) |Steps 380(402.40) | Grad Norm 65.1355(37.9049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 67.2193, Epoch Time 1780.0465(1642.1504), Bit/dim 2.0633(best: 2.1015), Xent 0.5523, Xent Color 0.5815. Loss 2.3468, Error 0.1355(best: 0.1351), Error Color 0.1190(best: 0.0645)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 25.0891(25.6540) | Bit/dim 2.0504(2.0854) | Xent 0.6639(0.6579) | Xent Color 0.7176(0.8057) | Loss 5.0181(5.8810) | Error 0.1978(0.1929) | Error Color 0.2433(0.2365) |Steps 368(399.45) | Grad Norm 21.5423(34.7495) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 26.0663(25.5906) | Bit/dim 2.0224(2.0743) | Xent 0.6008(0.6604) | Xent Color 0.6097(0.7629) | Loss 5.0937(5.6773) | Error 0.1756(0.1951) | Error Color 0.1578(0.2216) |Steps 410(397.18) | Grad Norm 10.6293(28.5270) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 27.2785(25.6802) | Bit/dim 2.0348(2.0644) | Xent 0.6391(0.6465) | Xent Color 0.5656(0.7109) | Loss 5.0606(5.5071) | Error 0.1800(0.1909) | Error Color 0.1322(0.1993) |Steps 380(393.87) | Grad Norm 9.1791(23.4043) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 27.3552(25.8204) | Bit/dim 2.0348(2.0566) | Xent 0.5737(0.6319) | Xent Color 0.4635(0.6538) | Loss 5.0178(5.3862) | Error 0.1744(0.1887) | Error Color 0.1067(0.1758) |Steps 422(396.18) | Grad Norm 11.2564(19.0194) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 24.9233(25.9911) | Bit/dim 2.0089(2.0482) | Xent 0.6014(0.6236) | Xent Color 0.4392(0.5987) | Loss 5.0521(5.2845) | Error 0.1733(0.1875) | Error Color 0.0733(0.1533) |Steps 410(399.16) | Grad Norm 5.1701(16.0818) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 26.6195(25.9841) | Bit/dim 2.0274(2.0409) | Xent 0.5755(0.6159) | Xent Color 0.3961(0.5515) | Loss 4.9205(5.1983) | Error 0.1822(0.1872) | Error Color 0.0667(0.1352) |Steps 404(400.14) | Grad Norm 6.8280(14.7221) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 25.6111(26.0312) | Bit/dim 1.9951(2.0331) | Xent 0.5950(0.6126) | Xent Color 0.3458(0.5062) | Loss 4.9167(5.1299) | Error 0.1756(0.1869) | Error Color 0.0567(0.1192) |Steps 410(402.14) | Grad Norm 2.6453(13.9178) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 72.9474, Epoch Time 1808.6953(1647.1467), Bit/dim 2.0175(best: 2.0633), Xent 0.4457, Xent Color 0.2804. Loss 2.1991, Error 0.1214(best: 0.1351), Error Color 0.0224(best: 0.0645)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 27.8010(26.1094) | Bit/dim 2.0135(2.0253) | Xent 0.6096(0.6028) | Xent Color 0.5615(0.4761) | Loss 5.0131(5.5508) | Error 0.2000(0.1849) | Error Color 0.1933(0.1115) |Steps 410(403.92) | Grad Norm 81.9275(16.7765) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 27.9841(26.2309) | Bit/dim 2.0429(2.0371) | Xent 0.5909(0.5967) | Xent Color 0.8381(0.6682) | Loss 5.1442(5.5139) | Error 0.1744(0.1840) | Error Color 0.3300(0.1818) |Steps 398(404.17) | Grad Norm 42.1287(36.4028) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 26.1884(26.1034) | Bit/dim 1.9955(2.0325) | Xent 0.6221(0.6043) | Xent Color 0.5007(0.6450) | Loss 4.9821(5.3979) | Error 0.1789(0.1871) | Error Color 0.1367(0.1822) |Steps 380(403.19) | Grad Norm 8.8000(31.7717) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 25.4415(25.9437) | Bit/dim 2.0227(2.0259) | Xent 0.5302(0.6003) | Xent Color 0.4187(0.5959) | Loss 4.9669(5.2792) | Error 0.1744(0.1866) | Error Color 0.0922(0.1628) |Steps 398(401.68) | Grad Norm 9.3837(26.0106) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 26.7132(25.9315) | Bit/dim 1.9855(2.0184) | Xent 0.5657(0.5930) | Xent Color 0.3419(0.5386) | Loss 4.8780(5.1761) | Error 0.1600(0.1833) | Error Color 0.0711(0.1415) |Steps 404(401.63) | Grad Norm 5.0837(20.9574) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 26.1828(26.0910) | Bit/dim 2.0108(2.0111) | Xent 0.5493(0.5817) | Xent Color 0.2965(0.4819) | Loss 4.8817(5.0947) | Error 0.1633(0.1795) | Error Color 0.0589(0.1202) |Steps 410(402.82) | Grad Norm 4.2616(16.8976) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 26.6785(26.1116) | Bit/dim 1.9785(2.0034) | Xent 0.5776(0.5717) | Xent Color 0.2711(0.4279) | Loss 4.8954(5.0268) | Error 0.1844(0.1767) | Error Color 0.0500(0.1018) |Steps 422(402.35) | Grad Norm 3.7991(13.6358) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 73.7219, Epoch Time 1817.9603(1652.2711), Bit/dim 1.9876(best: 2.0175), Xent 0.4141, Xent Color 0.2041. Loss 2.1422, Error 0.1178(best: 0.1214), Error Color 0.0133(best: 0.0224)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 25.8731(26.1230) | Bit/dim 1.9757(1.9954) | Xent 0.5613(0.5698) | Xent Color 0.2282(0.3822) | Loss 4.7754(5.3828) | Error 0.1844(0.1770) | Error Color 0.0333(0.0869) |Steps 410(402.35) | Grad Norm 4.7570(11.2800) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 25.9439(26.0664) | Bit/dim 1.9632(1.9871) | Xent 0.4517(0.5596) | Xent Color 0.2250(0.3419) | Loss 4.6877(5.2198) | Error 0.1333(0.1730) | Error Color 0.0411(0.0750) |Steps 410(402.05) | Grad Norm 9.4546(9.9764) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 26.5327(26.1694) | Bit/dim 1.9778(1.9816) | Xent 0.4946(0.5533) | Xent Color 0.1916(0.3063) | Loss 4.7582(5.0968) | Error 0.1678(0.1710) | Error Color 0.0344(0.0648) |Steps 422(404.61) | Grad Norm 4.4522(8.8483) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 27.6453(26.1569) | Bit/dim 1.9685(1.9751) | Xent 0.5210(0.5453) | Xent Color 0.1681(0.2748) | Loss 4.7867(4.9908) | Error 0.1678(0.1700) | Error Color 0.0278(0.0557) |Steps 422(404.20) | Grad Norm 5.6118(8.6884) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 25.7502(26.2843) | Bit/dim 2.0470(1.9849) | Xent 0.5055(0.5399) | Xent Color 0.9357(0.5874) | Loss 5.1926(5.1035) | Error 0.1533(0.1689) | Error Color 0.3589(0.1296) |Steps 422(405.03) | Grad Norm 33.1911(28.5107) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 28.1834(26.2383) | Bit/dim 2.0517(2.0078) | Xent 0.6348(0.5720) | Xent Color 0.6753(0.6439) | Loss 5.0681(5.1406) | Error 0.1878(0.1782) | Error Color 0.2422(0.1777) |Steps 380(401.48) | Grad Norm 17.3314(27.6350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 68.9715, Epoch Time 1821.0824(1657.3355), Bit/dim 2.0176(best: 1.9876), Xent 0.4376, Xent Color 0.4241. Loss 2.2330, Error 0.1218(best: 0.1178), Error Color 0.0723(best: 0.0133)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 27.4772(26.2956) | Bit/dim 2.0069(2.0068) | Xent 0.5802(0.5842) | Xent Color 0.4838(0.6200) | Loss 4.9684(5.5829) | Error 0.1778(0.1815) | Error Color 0.1278(0.1758) |Steps 392(399.62) | Grad Norm 8.7495(22.5021) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 25.2034(26.2299) | Bit/dim 1.9766(1.9988) | Xent 0.5575(0.5851) | Xent Color 0.3801(0.5683) | Loss 4.8996(5.3917) | Error 0.1767(0.1827) | Error Color 0.0989(0.1591) |Steps 404(395.98) | Grad Norm 7.3922(18.1106) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 27.0387(26.2191) | Bit/dim 1.9811(1.9904) | Xent 0.5572(0.5713) | Xent Color 0.2895(0.5024) | Loss 4.8092(5.2302) | Error 0.1689(0.1790) | Error Color 0.0622(0.1365) |Steps 404(397.03) | Grad Norm 2.1817(14.2661) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 27.7327(26.2698) | Bit/dim 1.9499(1.9812) | Xent 0.5188(0.5618) | Xent Color 0.2341(0.4369) | Loss 4.7238(5.0932) | Error 0.1711(0.1765) | Error Color 0.0467(0.1148) |Steps 404(395.66) | Grad Norm 3.5870(11.4737) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 26.6118(26.1876) | Bit/dim 1.9607(1.9733) | Xent 0.4762(0.5546) | Xent Color 0.1960(0.3775) | Loss 4.7500(4.9947) | Error 0.1567(0.1734) | Error Color 0.0367(0.0958) |Steps 392(395.66) | Grad Norm 10.5655(10.1472) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 24.8657(26.1587) | Bit/dim 1.9485(1.9653) | Xent 0.5967(0.5506) | Xent Color 0.1902(0.3285) | Loss 4.7318(4.9092) | Error 0.1967(0.1722) | Error Color 0.0444(0.0809) |Steps 386(394.08) | Grad Norm 15.2493(9.9878) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 25.7929(26.1359) | Bit/dim 1.9192(1.9568) | Xent 0.4870(0.5406) | Xent Color 0.1701(0.2880) | Loss 4.5964(4.8337) | Error 0.1411(0.1684) | Error Color 0.0344(0.0695) |Steps 386(394.46) | Grad Norm 3.6498(10.0037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 73.4054, Epoch Time 1823.2114(1662.3117), Bit/dim 1.9397(best: 1.9876), Xent 0.3785, Xent Color 0.1035. Loss 2.0602, Error 0.1067(best: 0.1178), Error Color 0.0096(best: 0.0133)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 25.2889(26.0098) | Bit/dim 1.9436(1.9492) | Xent 0.5519(0.5346) | Xent Color 0.1378(0.2532) | Loss 4.6148(5.2022) | Error 0.1778(0.1654) | Error Color 0.0289(0.0590) |Steps 374(393.61) | Grad Norm 6.8849(9.9546) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 26.4471(26.0139) | Bit/dim 1.9126(1.9432) | Xent 0.3903(0.5258) | Xent Color 0.1424(0.2258) | Loss 4.5181(5.0432) | Error 0.1244(0.1625) | Error Color 0.0322(0.0515) |Steps 380(391.94) | Grad Norm 8.8691(10.7488) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 24.7977(26.0382) | Bit/dim 1.9393(1.9358) | Xent 0.4725(0.5242) | Xent Color 0.1293(0.2029) | Loss 4.5635(4.9177) | Error 0.1467(0.1628) | Error Color 0.0311(0.0459) |Steps 386(391.17) | Grad Norm 16.8493(11.6110) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 26.6605(26.1418) | Bit/dim 1.9113(1.9281) | Xent 0.4768(0.5202) | Xent Color 0.1075(0.1819) | Loss 4.4659(4.8214) | Error 0.1500(0.1609) | Error Color 0.0233(0.0402) |Steps 398(391.15) | Grad Norm 4.7171(10.7050) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 25.6636(25.9769) | Bit/dim 1.9008(1.9219) | Xent 0.5011(0.5135) | Xent Color 0.1040(0.1626) | Loss 4.4836(4.7479) | Error 0.1478(0.1588) | Error Color 0.0144(0.0348) |Steps 380(391.85) | Grad Norm 11.2191(10.9418) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 27.2483(25.9972) | Bit/dim 1.8983(1.9140) | Xent 0.4917(0.5067) | Xent Color 0.0895(0.1469) | Loss 4.4883(4.6822) | Error 0.1489(0.1563) | Error Color 0.0178(0.0310) |Steps 410(394.40) | Grad Norm 4.0756(10.7074) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 73.5708, Epoch Time 1806.4191(1666.6350), Bit/dim 1.8948(best: 1.9397), Xent 0.3579, Xent Color 0.0503. Loss 1.9968, Error 0.1047(best: 0.1067), Error Color 0.0016(best: 0.0096)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 26.9015(26.0120) | Bit/dim 1.8788(1.9072) | Xent 0.4806(0.5049) | Xent Color 0.1055(0.1327) | Loss 4.4340(5.1578) | Error 0.1411(0.1563) | Error Color 0.0178(0.0271) |Steps 386(395.27) | Grad Norm 20.7236(10.0492) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 25.1817(26.0859) | Bit/dim 1.9067(1.9000) | Xent 0.4878(0.5017) | Xent Color 0.0673(0.1281) | Loss 4.5026(4.9826) | Error 0.1422(0.1549) | Error Color 0.0111(0.0283) |Steps 386(396.54) | Grad Norm 6.5778(13.4152) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 27.8747(26.1340) | Bit/dim 2.6603(2.0817) | Xent 0.7653(0.5192) | Xent Color 6.5386(1.4677) | Loss 8.9168(5.8205) | Error 0.2533(0.1609) | Error Color 0.7956(0.1509) |Steps 380(397.74) | Grad Norm 80.8839(54.4732) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 24.4029(26.6632) | Bit/dim 2.2461(2.1540) | Xent 0.8240(0.6650) | Xent Color 1.4554(1.5923) | Loss 5.9350(5.9932) | Error 0.2789(0.2131) | Error Color 0.5167(0.2582) |Steps 392(403.92) | Grad Norm 30.2414(49.4495) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 25.7212(26.4354) | Bit/dim 2.1214(2.1628) | Xent 0.6639(0.6809) | Xent Color 0.8958(1.4330) | Loss 5.3555(5.8610) | Error 0.2222(0.2182) | Error Color 0.3478(0.2914) |Steps 392(397.65) | Grad Norm 11.6831(40.7197) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 27.3817(26.2867) | Bit/dim 2.0160(2.1296) | Xent 0.6661(0.6821) | Xent Color 0.6803(1.2646) | Loss 5.0028(5.6690) | Error 0.1978(0.2173) | Error Color 0.2567(0.2948) |Steps 362(392.46) | Grad Norm 9.9301(32.6593) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 25.0154(26.3364) | Bit/dim 2.0010(2.0947) | Xent 0.5843(0.6624) | Xent Color 0.5900(1.0944) | Loss 4.9059(5.4881) | Error 0.1789(0.2094) | Error Color 0.2011(0.2765) |Steps 392(392.68) | Grad Norm 4.5541(26.0356) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 73.0258, Epoch Time 1840.7319(1671.8579), Bit/dim 1.9890(best: 1.8948), Xent 0.3985, Xent Color 0.3505. Loss 2.1762, Error 0.1174(best: 0.1047), Error Color 0.0819(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 27.2939(26.2341) | Bit/dim 1.9771(2.0640) | Xent 0.4955(0.6378) | Xent Color 0.4138(0.9309) | Loss 4.8120(5.7814) | Error 0.1644(0.2022) | Error Color 0.1256(0.2452) |Steps 398(391.52) | Grad Norm 8.0819(21.1208) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 24.9533(26.2190) | Bit/dim 1.9541(2.0380) | Xent 0.4897(0.6073) | Xent Color 0.2960(0.7752) | Loss 4.6906(5.5154) | Error 0.1511(0.1917) | Error Color 0.0833(0.2064) |Steps 374(391.82) | Grad Norm 5.1300(16.6944) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 26.4811(26.2987) | Bit/dim 1.9363(2.0161) | Xent 0.5310(0.5919) | Xent Color 0.1991(0.6383) | Loss 4.7186(5.3157) | Error 0.1667(0.1860) | Error Color 0.0433(0.1700) |Steps 404(393.68) | Grad Norm 2.7160(13.1316) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 25.9814(26.2053) | Bit/dim 1.9579(1.9963) | Xent 0.6164(0.5738) | Xent Color 0.1903(0.5240) | Loss 4.6951(5.1441) | Error 0.2000(0.1803) | Error Color 0.0478(0.1382) |Steps 386(394.61) | Grad Norm 2.7522(10.4658) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 25.5742(26.1435) | Bit/dim 1.9405(1.9808) | Xent 0.5458(0.5640) | Xent Color 0.1572(0.4294) | Loss 4.6361(5.0169) | Error 0.1822(0.1769) | Error Color 0.0356(0.1117) |Steps 386(394.62) | Grad Norm 5.2529(8.6331) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 25.7231(26.1676) | Bit/dim 1.9358(1.9658) | Xent 0.4446(0.5509) | Xent Color 0.1338(0.3533) | Loss 4.6221(4.9198) | Error 0.1578(0.1728) | Error Color 0.0311(0.0905) |Steps 392(396.46) | Grad Norm 4.5163(7.4356) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 26.7996(26.1613) | Bit/dim 1.9061(1.9512) | Xent 0.4905(0.5436) | Xent Color 0.1274(0.2945) | Loss 4.5306(4.8327) | Error 0.1578(0.1705) | Error Color 0.0233(0.0737) |Steps 368(396.76) | Grad Norm 3.1754(6.1258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 74.6978, Epoch Time 1822.0488(1676.3636), Bit/dim 1.9173(best: 1.8948), Xent 0.3656, Xent Color 0.0660. Loss 2.0252, Error 0.1079(best: 0.1047), Error Color 0.0061(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 25.5196(26.1770) | Bit/dim 1.8899(1.9394) | Xent 0.4979(0.5347) | Xent Color 0.1106(0.2462) | Loss 4.5118(5.1802) | Error 0.1556(0.1676) | Error Color 0.0244(0.0602) |Steps 404(397.42) | Grad Norm 4.4544(5.5499) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 26.0844(26.2879) | Bit/dim 1.8898(1.9287) | Xent 0.4812(0.5277) | Xent Color 0.1116(0.2089) | Loss 4.4899(5.0120) | Error 0.1400(0.1655) | Error Color 0.0300(0.0506) |Steps 404(396.61) | Grad Norm 5.8209(5.2325) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 26.0096(26.2363) | Bit/dim 1.8945(1.9191) | Xent 0.4601(0.5162) | Xent Color 0.0864(0.1789) | Loss 4.5477(4.8832) | Error 0.1389(0.1611) | Error Color 0.0133(0.0423) |Steps 404(396.87) | Grad Norm 10.4345(5.4804) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 28.3792(26.3019) | Bit/dim 1.8753(1.9072) | Xent 0.4404(0.5120) | Xent Color 0.0845(0.1557) | Loss 4.4861(4.7896) | Error 0.1400(0.1587) | Error Color 0.0189(0.0364) |Steps 404(398.58) | Grad Norm 2.4230(5.5504) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 26.6761(26.3466) | Bit/dim 1.8587(1.8987) | Xent 0.5096(0.5022) | Xent Color 0.0738(0.1421) | Loss 4.5080(4.7152) | Error 0.1667(0.1566) | Error Color 0.0122(0.0333) |Steps 428(399.10) | Grad Norm 4.2528(8.1060) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 27.2887(26.3725) | Bit/dim 1.8806(1.8904) | Xent 0.4971(0.4972) | Xent Color 0.0689(0.1271) | Loss 4.5010(4.6525) | Error 0.1711(0.1557) | Error Color 0.0089(0.0296) |Steps 422(399.52) | Grad Norm 6.0216(8.3586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 75.2558, Epoch Time 1837.5819(1681.2002), Bit/dim 1.8679(best: 1.8948), Xent 0.3406, Xent Color 0.0406. Loss 1.9632, Error 0.0958(best: 0.1047), Error Color 0.0067(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 25.7349(26.4943) | Bit/dim 1.8664(1.8828) | Xent 0.4975(0.4973) | Xent Color 0.0686(0.1127) | Loss 4.3928(5.0933) | Error 0.1544(0.1553) | Error Color 0.0133(0.0254) |Steps 392(399.93) | Grad Norm 7.1614(7.9398) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 27.5577(26.6080) | Bit/dim 1.8292(1.8721) | Xent 0.4458(0.4920) | Xent Color 0.0717(0.1009) | Loss 4.4191(4.9171) | Error 0.1467(0.1532) | Error Color 0.0178(0.0222) |Steps 410(401.53) | Grad Norm 9.7783(7.2678) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 29.7962(27.0934) | Bit/dim 2.0834(1.9590) | Xent 0.8662(0.5788) | Xent Color 1.9803(1.0385) | Loss 5.9877(5.4320) | Error 0.3044(0.1836) | Error Color 0.6867(0.1695) |Steps 452(406.85) | Grad Norm 23.8227(30.8435) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 28.5795(27.7674) | Bit/dim 2.0113(2.0040) | Xent 0.7509(0.6250) | Xent Color 0.7739(1.0482) | Loss 5.1967(5.4753) | Error 0.2522(0.2005) | Error Color 0.3022(0.2366) |Steps 440(416.78) | Grad Norm 7.5601(26.7287) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 31.9421(28.4054) | Bit/dim 1.9592(2.0041) | Xent 0.5497(0.6189) | Xent Color 0.6128(0.9643) | Loss 5.1055(5.3928) | Error 0.1789(0.1985) | Error Color 0.2222(0.2492) |Steps 488(427.35) | Grad Norm 4.7199(21.5391) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 30.9888(28.8788) | Bit/dim 1.9245(1.9907) | Xent 0.6124(0.6100) | Xent Color 0.4725(0.8509) | Loss 4.9212(5.2865) | Error 0.1911(0.1950) | Error Color 0.1611(0.2320) |Steps 464(438.12) | Grad Norm 3.6454(16.8643) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 30.6308(29.1798) | Bit/dim 1.8881(1.9700) | Xent 0.5412(0.5952) | Xent Color 0.3862(0.7405) | Loss 4.7047(5.1611) | Error 0.1656(0.1890) | Error Color 0.1256(0.2073) |Steps 458(441.47) | Grad Norm 3.1353(13.3824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 78.3320, Epoch Time 2019.4767(1691.3484), Bit/dim 1.9137(best: 1.8679), Xent 0.3885, Xent Color 0.2516. Loss 2.0737, Error 0.1137(best: 0.0958), Error Color 0.0312(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 27.2697(29.2429) | Bit/dim 1.8998(1.9518) | Xent 0.5722(0.5797) | Xent Color 0.3107(0.6384) | Loss 4.7134(5.5102) | Error 0.1733(0.1840) | Error Color 0.0889(0.1802) |Steps 428(439.94) | Grad Norm 1.3371(10.4800) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 30.3588(29.2204) | Bit/dim 1.9009(1.9353) | Xent 0.6158(0.5693) | Xent Color 0.2398(0.5437) | Loss 4.7417(5.2907) | Error 0.1900(0.1786) | Error Color 0.0522(0.1515) |Steps 446(438.53) | Grad Norm 1.5725(8.1700) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 28.9760(29.0656) | Bit/dim 1.8645(1.9198) | Xent 0.4665(0.5504) | Xent Color 0.2018(0.4591) | Loss 4.5256(5.0989) | Error 0.1367(0.1731) | Error Color 0.0422(0.1259) |Steps 404(435.56) | Grad Norm 1.1176(6.3574) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 27.3362(28.8465) | Bit/dim 1.8640(1.9085) | Xent 0.4568(0.5354) | Xent Color 0.1689(0.3852) | Loss 4.4496(4.9498) | Error 0.1400(0.1670) | Error Color 0.0333(0.1035) |Steps 410(430.24) | Grad Norm 1.7745(5.1389) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 27.5588(28.4685) | Bit/dim 1.8729(1.8961) | Xent 0.5104(0.5312) | Xent Color 0.1344(0.3216) | Loss 4.5496(4.8298) | Error 0.1689(0.1660) | Error Color 0.0267(0.0836) |Steps 392(422.49) | Grad Norm 0.9448(4.1789) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 27.4768(28.2604) | Bit/dim 1.8528(1.8853) | Xent 0.4627(0.5193) | Xent Color 0.1129(0.2684) | Loss 4.4682(4.7350) | Error 0.1456(0.1616) | Error Color 0.0189(0.0672) |Steps 410(418.45) | Grad Norm 2.1001(3.4911) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 74.7810, Epoch Time 1969.5755(1699.6953), Bit/dim 1.8563(best: 1.8679), Xent 0.3525, Xent Color 0.0569. Loss 1.9587, Error 0.1040(best: 0.0958), Error Color 0.0042(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 28.6571(28.1885) | Bit/dim 1.8494(1.8766) | Xent 0.4660(0.5119) | Xent Color 0.1108(0.2269) | Loss 4.4452(5.1706) | Error 0.1389(0.1594) | Error Color 0.0178(0.0551) |Steps 380(415.32) | Grad Norm 4.0276(3.3898) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 26.3672(28.0787) | Bit/dim 1.8548(1.8678) | Xent 0.4259(0.5022) | Xent Color 0.0895(0.1925) | Loss 4.4251(4.9823) | Error 0.1422(0.1563) | Error Color 0.0122(0.0446) |Steps 410(413.24) | Grad Norm 1.7693(3.1861) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 26.5616(27.9746) | Bit/dim 1.8276(1.8598) | Xent 0.4036(0.4963) | Xent Color 0.0793(0.1652) | Loss 4.2962(4.8259) | Error 0.1278(0.1546) | Error Color 0.0078(0.0371) |Steps 428(409.93) | Grad Norm 2.6091(2.9117) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 28.6263(27.9476) | Bit/dim 1.8141(1.8507) | Xent 0.4825(0.4954) | Xent Color 0.0851(0.1435) | Loss 4.4534(4.7113) | Error 0.1500(0.1539) | Error Color 0.0111(0.0311) |Steps 434(410.17) | Grad Norm 3.9420(2.7874) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 27.5970(28.0053) | Bit/dim 1.8291(1.8441) | Xent 0.5155(0.4930) | Xent Color 0.0822(0.1262) | Loss 4.4315(4.6180) | Error 0.1589(0.1543) | Error Color 0.0178(0.0266) |Steps 416(410.38) | Grad Norm 4.7518(2.9559) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 27.9094(28.1091) | Bit/dim 1.8280(1.8363) | Xent 0.5139(0.4911) | Xent Color 0.0640(0.1116) | Loss 4.3261(4.5483) | Error 0.1811(0.1544) | Error Color 0.0100(0.0229) |Steps 404(408.60) | Grad Norm 2.7461(3.0007) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 28.3368(28.0811) | Bit/dim 1.8228(1.8282) | Xent 0.4769(0.4873) | Xent Color 0.0757(0.1001) | Loss 4.3948(4.4968) | Error 0.1511(0.1532) | Error Color 0.0133(0.0201) |Steps 416(409.78) | Grad Norm 6.7253(3.0760) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 75.6361, Epoch Time 1945.0395(1707.0556), Bit/dim 1.8069(best: 1.8563), Xent 0.3359, Xent Color 0.0312. Loss 1.8986, Error 0.0993(best: 0.0958), Error Color 0.0018(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 29.0473(28.2411) | Bit/dim 1.7764(1.8195) | Xent 0.5149(0.4873) | Xent Color 0.0700(0.0910) | Loss 4.3264(4.9145) | Error 0.1711(0.1533) | Error Color 0.0178(0.0185) |Steps 416(409.20) | Grad Norm 2.4987(3.3535) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 29.7233(28.2199) | Bit/dim 1.7968(1.8141) | Xent 0.4703(0.4846) | Xent Color 0.0558(0.0831) | Loss 4.3140(4.7529) | Error 0.1556(0.1517) | Error Color 0.0078(0.0168) |Steps 434(407.29) | Grad Norm 4.7549(3.3851) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 26.9047(28.2240) | Bit/dim 1.7757(1.8067) | Xent 0.4151(0.4851) | Xent Color 0.0528(0.0761) | Loss 4.2600(4.6381) | Error 0.1344(0.1526) | Error Color 0.0067(0.0150) |Steps 410(408.62) | Grad Norm 2.1533(3.4509) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 27.2686(28.2334) | Bit/dim 1.7872(1.7995) | Xent 0.3708(0.4771) | Xent Color 0.0497(0.0695) | Loss 4.1475(4.5401) | Error 0.1267(0.1511) | Error Color 0.0078(0.0133) |Steps 398(409.15) | Grad Norm 3.8678(3.2247) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 29.7421(28.4633) | Bit/dim 1.7719(1.7907) | Xent 0.4613(0.4710) | Xent Color 0.0559(0.0652) | Loss 4.3066(4.4670) | Error 0.1278(0.1495) | Error Color 0.0156(0.0126) |Steps 416(409.84) | Grad Norm 5.8604(3.7550) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 28.1614(28.4226) | Bit/dim 1.7534(1.7845) | Xent 0.4643(0.4753) | Xent Color 0.0570(0.0609) | Loss 4.2440(4.4153) | Error 0.1544(0.1491) | Error Color 0.0144(0.0116) |Steps 440(409.23) | Grad Norm 4.1626(4.1516) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 26.9483(28.2789) | Bit/dim 1.7552(1.7752) | Xent 0.4590(0.4700) | Xent Color 0.0424(0.0572) | Loss 4.2564(4.3636) | Error 0.1456(0.1467) | Error Color 0.0089(0.0111) |Steps 434(406.91) | Grad Norm 4.9474(4.1038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 76.2090, Epoch Time 1967.7782(1714.8773), Bit/dim 1.7544(best: 1.8069), Xent 0.3233, Xent Color 0.0204. Loss 1.8403, Error 0.0969(best: 0.0958), Error Color 0.0018(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 30.2430(28.3652) | Bit/dim 1.7294(1.7671) | Xent 0.3856(0.4676) | Xent Color 0.0410(0.0547) | Loss 4.2160(4.7346) | Error 0.1267(0.1461) | Error Color 0.0078(0.0107) |Steps 410(407.16) | Grad Norm 8.0540(4.6239) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 26.4791(28.4483) | Bit/dim 1.7382(1.7579) | Xent 0.4966(0.4647) | Xent Color 0.0532(0.0526) | Loss 4.1960(4.5938) | Error 0.1600(0.1458) | Error Color 0.0133(0.0101) |Steps 392(405.35) | Grad Norm 13.9003(5.1916) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 29.8075(28.6553) | Bit/dim 1.7152(1.7514) | Xent 0.4456(0.4622) | Xent Color 0.0546(0.0501) | Loss 4.1321(4.4926) | Error 0.1433(0.1459) | Error Color 0.0111(0.0096) |Steps 410(408.35) | Grad Norm 9.1371(6.0200) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 31.6267(28.7166) | Bit/dim 1.7118(1.7426) | Xent 0.4150(0.4582) | Xent Color 0.0342(0.0472) | Loss 4.1632(4.4041) | Error 0.1278(0.1447) | Error Color 0.0033(0.0086) |Steps 440(407.72) | Grad Norm 3.8932(5.7861) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 29.8571(28.6358) | Bit/dim 1.6672(1.7323) | Xent 0.4708(0.4546) | Xent Color 0.0364(0.0446) | Loss 4.0856(4.3259) | Error 0.1444(0.1442) | Error Color 0.0044(0.0079) |Steps 398(407.31) | Grad Norm 3.8330(5.3591) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 28.7482(28.7520) | Bit/dim 1.6990(1.7225) | Xent 0.4014(0.4443) | Xent Color 0.0329(0.0430) | Loss 4.1826(4.2659) | Error 0.1322(0.1413) | Error Color 0.0056(0.0075) |Steps 422(406.59) | Grad Norm 2.4450(5.5614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 76.4343, Epoch Time 1993.9696(1723.2500), Bit/dim 1.6883(best: 1.7544), Xent 0.2976, Xent Color 0.0129. Loss 1.7659, Error 0.0917(best: 0.0958), Error Color 0.0009(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 28.8209(28.6809) | Bit/dim 1.6889(1.7133) | Xent 0.4994(0.4454) | Xent Color 0.0314(0.0406) | Loss 4.1265(4.7071) | Error 0.1511(0.1410) | Error Color 0.0033(0.0073) |Steps 404(406.08) | Grad Norm 6.6515(5.1953) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 27.8110(28.7316) | Bit/dim 1.6731(1.7023) | Xent 0.3972(0.4443) | Xent Color 0.0307(0.0386) | Loss 3.9983(4.5364) | Error 0.1300(0.1401) | Error Color 0.0056(0.0070) |Steps 392(404.57) | Grad Norm 3.6291(4.7230) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 29.0462(28.8186) | Bit/dim 1.8129(1.6981) | Xent 0.4145(0.4381) | Xent Color 3.7139(0.1546) | Loss 6.0672(4.4725) | Error 0.1322(0.1389) | Error Color 0.4267(0.0223) |Steps 428(405.09) | Grad Norm 198.0137(13.5352) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 26.6311(28.9491) | Bit/dim 1.9008(1.7464) | Xent 0.4915(0.4865) | Xent Color 0.6666(0.5252) | Loss 4.8424(4.6750) | Error 0.1633(0.1561) | Error Color 0.2789(0.1191) |Steps 410(407.43) | Grad Norm 14.9144(20.6468) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 25.4784(28.4015) | Bit/dim 1.7500(1.7649) | Xent 0.4911(0.4965) | Xent Color 0.4147(0.5184) | Loss 4.4632(4.6622) | Error 0.1567(0.1580) | Error Color 0.1633(0.1379) |Steps 404(407.57) | Grad Norm 7.8403(17.6900) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 26.5480(28.2031) | Bit/dim 1.7349(1.7595) | Xent 0.4664(0.4897) | Xent Color 0.2632(0.4671) | Loss 4.3022(4.5791) | Error 0.1511(0.1553) | Error Color 0.0900(0.1317) |Steps 404(406.08) | Grad Norm 3.2795(14.2859) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 26.4431(28.0673) | Bit/dim 1.6713(1.7418) | Xent 0.4449(0.4825) | Xent Color 0.2000(0.4040) | Loss 4.0751(4.4833) | Error 0.1544(0.1543) | Error Color 0.0533(0.1144) |Steps 386(405.47) | Grad Norm 2.4698(11.3389) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 73.3154, Epoch Time 1960.3987(1730.3645), Bit/dim 1.6850(best: 1.6883), Xent 0.3117, Xent Color 0.1179. Loss 1.7924, Error 0.0925(best: 0.0917), Error Color 0.0147(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 29.6171(28.0919) | Bit/dim 1.6567(1.7241) | Xent 0.4163(0.4669) | Xent Color 0.1665(0.3445) | Loss 4.1263(4.8303) | Error 0.1289(0.1498) | Error Color 0.0422(0.0971) |Steps 392(406.63) | Grad Norm 3.1376(9.1209) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 27.0017(28.3324) | Bit/dim 1.6490(1.7054) | Xent 0.4455(0.4588) | Xent Color 0.1183(0.2900) | Loss 4.0474(4.6362) | Error 0.1400(0.1467) | Error Color 0.0256(0.0802) |Steps 374(405.07) | Grad Norm 1.8921(7.5666) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 28.8880(28.3087) | Bit/dim 1.6302(1.6873) | Xent 0.4561(0.4552) | Xent Color 0.0878(0.2436) | Loss 4.0271(4.4798) | Error 0.1389(0.1451) | Error Color 0.0111(0.0663) |Steps 410(404.50) | Grad Norm 1.9550(6.3601) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 30.0229(28.3726) | Bit/dim 1.6312(1.6711) | Xent 0.4510(0.4472) | Xent Color 0.0800(0.2029) | Loss 4.0542(4.3519) | Error 0.1467(0.1424) | Error Color 0.0189(0.0545) |Steps 422(404.18) | Grad Norm 2.1385(5.2435) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 31.3391(28.5441) | Bit/dim 1.6121(1.6539) | Xent 0.4051(0.4400) | Xent Color 0.0837(0.1716) | Loss 3.9319(4.2462) | Error 0.1411(0.1393) | Error Color 0.0200(0.0444) |Steps 422(406.21) | Grad Norm 1.9564(4.3836) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 27.8203(28.5562) | Bit/dim 1.5746(1.6353) | Xent 0.4335(0.4313) | Xent Color 0.0610(0.1466) | Loss 3.8982(4.1596) | Error 0.1389(0.1373) | Error Color 0.0156(0.0373) |Steps 410(408.10) | Grad Norm 3.1294(3.9067) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 74.3694, Epoch Time 1985.6788(1738.0239), Bit/dim 1.5745(best: 1.6850), Xent 0.2640, Xent Color 0.0265. Loss 1.6471, Error 0.0787(best: 0.0917), Error Color 0.0012(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 29.3631(28.6518) | Bit/dim 1.5749(1.6186) | Xent 0.3770(0.4279) | Xent Color 0.0559(0.1246) | Loss 3.8958(4.6149) | Error 0.1133(0.1358) | Error Color 0.0133(0.0306) |Steps 410(410.55) | Grad Norm 3.0963(3.7451) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 27.5236(28.5189) | Bit/dim 1.5647(1.6035) | Xent 0.4360(0.4178) | Xent Color 0.0571(0.1073) | Loss 3.9290(4.4216) | Error 0.1422(0.1328) | Error Color 0.0122(0.0255) |Steps 404(410.61) | Grad Norm 3.4142(3.8201) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 27.5048(28.4744) | Bit/dim 1.5198(1.5882) | Xent 0.4865(0.4128) | Xent Color 0.0473(0.0928) | Loss 3.8450(4.2662) | Error 0.1511(0.1308) | Error Color 0.0111(0.0213) |Steps 404(411.77) | Grad Norm 3.4437(3.9648) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 28.2040(28.2772) | Bit/dim 1.5207(1.5720) | Xent 0.4028(0.4068) | Xent Color 0.0385(0.0818) | Loss 3.8016(4.1473) | Error 0.1300(0.1291) | Error Color 0.0056(0.0184) |Steps 410(410.67) | Grad Norm 2.5576(4.0889) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 28.4266(28.3770) | Bit/dim 1.4852(1.5553) | Xent 0.3952(0.4019) | Xent Color 0.0390(0.0714) | Loss 3.7386(4.0436) | Error 0.1189(0.1268) | Error Color 0.0056(0.0155) |Steps 422(410.25) | Grad Norm 3.4847(4.2832) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 26.6274(28.4260) | Bit/dim 1.4884(1.5390) | Xent 0.3877(0.3952) | Xent Color 0.0357(0.0632) | Loss 3.7665(3.9660) | Error 0.1200(0.1244) | Error Color 0.0044(0.0133) |Steps 416(412.38) | Grad Norm 2.3030(4.1694) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 29.6631(28.5731) | Bit/dim 1.4797(1.5231) | Xent 0.3819(0.3859) | Xent Color 0.0413(0.0574) | Loss 3.7652(3.8968) | Error 0.1056(0.1212) | Error Color 0.0100(0.0120) |Steps 422(412.85) | Grad Norm 8.5379(4.9386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 74.9089, Epoch Time 1968.4494(1744.9367), Bit/dim 1.4739(best: 1.5745), Xent 0.2303, Xent Color 0.0153. Loss 1.5353, Error 0.0713(best: 0.0787), Error Color 0.0009(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 28.9782(28.5023) | Bit/dim 1.4612(1.5085) | Xent 0.3531(0.3785) | Xent Color 0.0343(0.0534) | Loss 3.7260(4.2915) | Error 0.1178(0.1194) | Error Color 0.0033(0.0111) |Steps 404(412.28) | Grad Norm 7.3549(6.2424) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 28.9049(28.6589) | Bit/dim 1.4346(1.4948) | Xent 0.3668(0.3677) | Xent Color 0.0445(0.0513) | Loss 3.5980(4.1280) | Error 0.1067(0.1162) | Error Color 0.0078(0.0109) |Steps 398(413.02) | Grad Norm 7.8186(7.4343) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 27.9439(28.8713) | Bit/dim 1.4196(1.4792) | Xent 0.3248(0.3654) | Xent Color 0.0517(0.0476) | Loss 3.5523(3.9948) | Error 0.1011(0.1147) | Error Color 0.0156(0.0101) |Steps 422(416.10) | Grad Norm 10.7874(7.5874) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 30.2090(29.0229) | Bit/dim 1.4183(1.4632) | Xent 0.3241(0.3583) | Xent Color 0.0299(0.0433) | Loss 3.5277(3.8808) | Error 0.1111(0.1130) | Error Color 0.0044(0.0088) |Steps 428(416.01) | Grad Norm 5.5876(7.3003) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 31.0445(29.1231) | Bit/dim 1.4112(1.4495) | Xent 0.3683(0.3533) | Xent Color 0.0362(0.0427) | Loss 3.5389(3.7955) | Error 0.1322(0.1128) | Error Color 0.0078(0.0090) |Steps 428(416.31) | Grad Norm 12.8972(8.8481) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 30.1200(29.1239) | Bit/dim 1.3951(1.4360) | Xent 0.3358(0.3490) | Xent Color 0.0269(0.0398) | Loss 3.5268(3.7260) | Error 0.1156(0.1122) | Error Color 0.0033(0.0083) |Steps 410(416.66) | Grad Norm 8.4720(9.3603) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 29.6929(28.9658) | Bit/dim 1.3800(1.4229) | Xent 0.2983(0.3430) | Xent Color 0.0557(0.0381) | Loss 3.4819(3.6742) | Error 0.0933(0.1103) | Error Color 0.0167(0.0080) |Steps 416(417.51) | Grad Norm 17.2808(9.8179) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 75.6545, Epoch Time 2024.4065(1753.3208), Bit/dim 1.3840(best: 1.4739), Xent 0.2049, Xent Color 0.0147. Loss 1.4389, Error 0.0635(best: 0.0713), Error Color 0.0018(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 26.5363(29.0288) | Bit/dim 1.3665(1.4102) | Xent 0.3193(0.3425) | Xent Color 0.0255(0.0359) | Loss 3.4207(4.0438) | Error 0.1089(0.1087) | Error Color 0.0044(0.0072) |Steps 404(415.94) | Grad Norm 4.2849(9.7077) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 26.0843(28.9718) | Bit/dim 1.7086(1.4109) | Xent 0.3500(0.3359) | Xent Color 5.7917(0.2219) | Loss 6.6968(4.0014) | Error 0.1222(0.1063) | Error Color 0.6667(0.0321) |Steps 392(413.66) | Grad Norm 249.2092(19.5950) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 29.6293(29.3342) | Bit/dim 1.9310(1.5869) | Xent 0.9029(0.5190) | Xent Color 1.5919(1.2730) | Loss 5.6718(4.8122) | Error 0.2856(0.1707) | Error Color 0.5267(0.1889) |Steps 458(420.76) | Grad Norm 25.6536(37.1708) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 30.1884(28.9923) | Bit/dim 1.7511(1.6638) | Xent 0.6429(0.5520) | Xent Color 0.7068(1.1826) | Loss 4.6799(4.8750) | Error 0.2222(0.1799) | Error Color 0.2767(0.2331) |Steps 434(423.49) | Grad Norm 11.7116(31.0679) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 26.5818(28.9060) | Bit/dim 1.6807(1.6752) | Xent 0.6102(0.5710) | Xent Color 0.4545(1.0033) | Loss 4.4465(4.7945) | Error 0.1944(0.1859) | Error Color 0.1733(0.2219) |Steps 428(428.58) | Grad Norm 7.5897(24.9666) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 29.3369(28.8350) | Bit/dim 1.5864(1.6623) | Xent 0.5227(0.5547) | Xent Color 0.2837(0.8262) | Loss 4.0797(4.6496) | Error 0.1644(0.1799) | Error Color 0.1067(0.1957) |Steps 392(426.74) | Grad Norm 2.3920(19.4357) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 75.6255, Epoch Time 2002.8777(1760.8075), Bit/dim 1.5512(best: 1.3840), Xent 0.3121, Xent Color 0.1400. Loss 1.6642, Error 0.0912(best: 0.0635), Error Color 0.0221(best: 0.0009)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 31.2667(28.9117) | Bit/dim 1.5319(1.6347) | Xent 0.4689(0.5371) | Xent Color 0.2033(0.6735) | Loss 3.9760(5.0367) | Error 0.1467(0.1741) | Error Color 0.0644(0.1663) |Steps 464(427.58) | Grad Norm 1.8173(15.1832) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_multiscale.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
