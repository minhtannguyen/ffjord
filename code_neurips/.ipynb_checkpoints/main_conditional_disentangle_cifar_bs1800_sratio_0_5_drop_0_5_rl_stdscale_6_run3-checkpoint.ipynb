{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=1800, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs1800_sratio_0_5_drop_0_5_rl_stdscale_6_run2', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 18.0568(36.0828) | Bit/dim 8.8294(8.9051) | Xent 2.2769(2.2998) | Loss 22.1132(22.2308) | Error 0.7339(0.8633) Steps 538(491.19) | Grad Norm 17.4122(24.3010) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 18.0233(31.2347) | Bit/dim 8.5476(8.8273) | Xent 2.2229(2.2864) | Loss 21.5214(22.0609) | Error 0.7350(0.8329) Steps 514(493.81) | Grad Norm 7.5690(20.6413) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 75.0811, Epoch Time 586.7644(586.7644), Bit/dim 8.3762(best: inf), Xent 2.1752, Loss 9.4639, Error 0.7351(best: inf)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0030 | Time 16.9716(27.5612) | Bit/dim 8.3616(8.7193) | Xent 2.1779(2.2620) | Loss 21.1059(22.4447) | Error 0.7422(0.8087) Steps 544(497.83) | Grad Norm 6.8387(17.0806) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 17.3913(24.8367) | Bit/dim 8.1298(8.5928) | Xent 2.1264(2.2305) | Loss 20.4853(21.9780) | Error 0.7333(0.7903) Steps 472(497.80) | Grad Norm 5.4536(14.1153) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 15.7015(22.8483) | Bit/dim 7.8874(8.4365) | Xent 2.0914(2.1985) | Loss 19.6728(21.4748) | Error 0.7017(0.7707) Steps 490(496.46) | Grad Norm 5.2852(11.8386) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 68.6821, Epoch Time 552.3785(585.7328), Bit/dim 7.7134(best: 8.3762), Xent 2.0781, Loss 8.7525, Error 0.6960(best: 0.7351)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0060 | Time 16.2436(21.3973) | Bit/dim 7.5499(8.2400) | Xent 2.0733(2.1679) | Loss 19.0041(21.5285) | Error 0.6828(0.7498) Steps 490(499.59) | Grad Norm 4.6518(10.0814) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 17.4202(20.5116) | Bit/dim 7.2651(8.0134) | Xent 2.0856(2.1447) | Loss 18.5070(20.8030) | Error 0.6900(0.7342) Steps 520(504.32) | Grad Norm 3.5109(8.5257) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 18.2616(19.8242) | Bit/dim 7.0913(7.7893) | Xent 2.0849(2.1316) | Loss 18.0007(20.1501) | Error 0.6828(0.7234) Steps 514(508.23) | Grad Norm 2.5864(7.0116) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 71.2698, Epoch Time 571.8294(585.3157), Bit/dim 7.0872(best: 7.7134), Xent 2.0970, Loss 8.1357, Error 0.6966(best: 0.6960)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0090 | Time 17.3909(19.4684) | Bit/dim 7.0377(7.5985) | Xent 2.0956(2.1233) | Loss 17.9214(20.1404) | Error 0.7156(0.7198) Steps 508(509.23) | Grad Norm 4.4518(5.9069) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 18.1792(19.2443) | Bit/dim 6.9973(7.4427) | Xent 2.0828(2.1147) | Loss 17.9151(19.5760) | Error 0.6961(0.7175) Steps 502(513.17) | Grad Norm 1.4686(4.9255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 74.4465, Epoch Time 590.4564(585.4699), Bit/dim 6.9718(best: 7.0872), Xent 2.0601, Loss 8.0019, Error 0.7116(best: 0.6960)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0110 | Time 19.7840(19.0755) | Bit/dim 6.9585(7.3219) | Xent 2.0634(2.1030) | Loss 17.8524(19.8366) | Error 0.7061(0.7141) Steps 472(516.58) | Grad Norm 3.2678(4.6284) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 20.2795(19.0543) | Bit/dim 6.9269(7.2229) | Xent 2.0341(2.0889) | Loss 17.7378(19.3310) | Error 0.6911(0.7094) Steps 562(522.84) | Grad Norm 2.3132(4.7301) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 18.4008(19.0544) | Bit/dim 6.8790(7.1397) | Xent 2.0469(2.0782) | Loss 17.9300(18.9262) | Error 0.6839(0.7052) Steps 556(527.32) | Grad Norm 2.4737(5.1043) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 74.9684, Epoch Time 607.6805(586.1363), Bit/dim 6.8600(best: 6.9718), Xent 2.0249, Loss 7.8724, Error 0.6773(best: 0.6960)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0140 | Time 19.6863(19.1635) | Bit/dim 6.8308(7.0660) | Xent 2.0105(2.0670) | Loss 17.6024(19.2164) | Error 0.6689(0.7004) Steps 550(532.89) | Grad Norm 6.5308(5.9546) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 18.9837(19.2664) | Bit/dim 6.7611(6.9954) | Xent 2.0433(2.0576) | Loss 17.5857(18.7814) | Error 0.6961(0.6982) Steps 550(538.30) | Grad Norm 22.0314(7.1304) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 19.8123(19.3193) | Bit/dim 6.6843(6.9224) | Xent 2.0274(2.0500) | Loss 17.1546(18.4141) | Error 0.7100(0.6976) Steps 520(540.00) | Grad Norm 40.5620(11.2796) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 75.1730, Epoch Time 618.8983(587.1191), Bit/dim 6.6510(best: 6.8600), Xent 2.0231, Loss 7.6626, Error 0.6957(best: 0.6773)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0170 | Time 18.6407(19.4071) | Bit/dim 6.5786(6.8418) | Xent 2.0514(2.0460) | Loss 17.2339(18.6626) | Error 0.7194(0.6982) Steps 532(542.90) | Grad Norm 46.5080(17.6249) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 18.8543(19.3822) | Bit/dim 6.4157(6.7485) | Xent 1.9788(2.0362) | Loss 16.7411(18.2104) | Error 0.6567(0.6961) Steps 526(542.12) | Grad Norm 6.3800(19.3630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 75.2049, Epoch Time 620.8741(588.1318), Bit/dim 6.3100(best: 6.6510), Xent 2.0273, Loss 7.3237, Error 0.7351(best: 0.6773)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0190 | Time 18.7701(19.4293) | Bit/dim 6.3125(6.6530) | Xent 2.0266(2.0687) | Loss 39.7990(18.5570) | Error 0.7478(0.7098) Steps 538(541.79) | Grad Norm 57.4158(35.9448) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 19.8533(19.3818) | Bit/dim 6.1580(6.5414) | Xent 2.0101(2.0676) | Loss 16.2549(17.9787) | Error 0.6772(0.7145) Steps 550(541.71) | Grad Norm 29.5193(38.1803) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 20.3044(19.4646) | Bit/dim 6.0007(6.4187) | Xent 2.0160(2.0565) | Loss 15.9521(17.4695) | Error 0.6644(0.7075) Steps 550(543.20) | Grad Norm 26.0107(34.1158) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 76.2387, Epoch Time 621.9199(589.1454), Bit/dim 5.9169(best: 6.3100), Xent 1.9869, Loss 6.9104, Error 0.6568(best: 0.6773)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0220 | Time 20.8985(19.6869) | Bit/dim 5.9221(6.2915) | Xent 2.0079(2.0441) | Loss 15.9207(17.6479) | Error 0.7033(0.7019) Steps 556(547.15) | Grad Norm 67.4711(34.0417) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 19.8892(19.8507) | Bit/dim 5.8778(6.2084) | Xent 2.0533(2.0754) | Loss 15.6127(17.2014) | Error 0.7161(0.7124) Steps 580(550.76) | Grad Norm 35.8354(54.3069) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 19.9214(19.7073) | Bit/dim 5.8125(6.1111) | Xent 2.0423(2.0677) | Loss 15.5394(16.7544) | Error 0.7194(0.7138) Steps 550(549.63) | Grad Norm 26.6235(47.1401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 75.3485, Epoch Time 628.4012(590.3231), Bit/dim 5.7841(best: 5.9169), Xent 2.0276, Loss 6.7979, Error 0.6912(best: 0.6568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0250 | Time 19.7756(19.5311) | Bit/dim 5.7546(6.0227) | Xent 1.9991(2.0565) | Loss 15.3199(16.9676) | Error 0.6739(0.7089) Steps 532(547.26) | Grad Norm 9.1151(39.8844) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 18.0739(19.4620) | Bit/dim 5.7337(5.9467) | Xent 1.9852(2.0415) | Loss 15.2548(16.5333) | Error 0.6756(0.7019) Steps 550(547.80) | Grad Norm 4.5465(32.5626) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 18.9883(19.4339) | Bit/dim 5.6547(5.8802) | Xent 1.9767(2.0237) | Loss 15.1950(16.1900) | Error 0.6572(0.6916) Steps 550(546.76) | Grad Norm 5.0113(26.6554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 73.2294, Epoch Time 610.0728(590.9156), Bit/dim 5.6886(best: 5.7841), Xent 1.9493, Loss 6.6632, Error 0.6413(best: 0.6568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0280 | Time 19.6998(19.3966) | Bit/dim 5.6716(5.8295) | Xent 1.9279(2.0055) | Loss 15.1060(16.4435) | Error 0.6589(0.6839) Steps 562(548.65) | Grad Norm 7.3790(21.9837) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 19.2716(19.2024) | Bit/dim 5.6258(5.7813) | Xent 1.9589(1.9991) | Loss 14.9821(16.0601) | Error 0.6656(0.6847) Steps 526(544.73) | Grad Norm 23.1021(26.2907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 72.8086, Epoch Time 599.9107(591.1854), Bit/dim 5.6273(best: 5.6886), Xent 1.9429, Loss 6.5988, Error 0.6620(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0300 | Time 18.1209(19.0773) | Bit/dim 5.5810(5.7400) | Xent 1.9766(1.9883) | Loss 14.8571(16.4447) | Error 0.6883(0.6829) Steps 562(544.54) | Grad Norm 24.7829(25.4049) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 19.0772(18.9931) | Bit/dim 5.5884(5.7061) | Xent 1.9444(1.9737) | Loss 14.9871(16.0512) | Error 0.6506(0.6763) Steps 556(543.24) | Grad Norm 21.8455(23.7389) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 17.8072(18.8725) | Bit/dim 5.6009(5.6791) | Xent 2.0779(1.9753) | Loss 15.1562(15.7662) | Error 0.7528(0.6782) Steps 544(541.28) | Grad Norm 114.5646(32.8665) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 74.9649, Epoch Time 597.6456(591.3792), Bit/dim 5.5672(best: 5.6273), Xent 1.9407, Loss 6.5375, Error 0.6626(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0330 | Time 18.8881(18.8854) | Bit/dim 5.5726(5.6504) | Xent 1.9504(1.9665) | Loss 14.9198(16.1116) | Error 0.6806(0.6736) Steps 562(542.92) | Grad Norm 19.6590(31.1887) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 18.7994(18.9112) | Bit/dim 5.5178(5.6185) | Xent 1.9110(1.9585) | Loss 14.6382(15.7460) | Error 0.6478(0.6711) Steps 520(539.14) | Grad Norm 12.7532(26.7842) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 18.3047(18.8769) | Bit/dim 5.5032(5.5870) | Xent 2.1503(1.9664) | Loss 15.0585(15.4873) | Error 0.7483(0.6754) Steps 562(541.69) | Grad Norm 84.4603(29.2968) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 75.2966, Epoch Time 602.8014(591.7219), Bit/dim 5.4848(best: 5.5672), Xent 1.9963, Loss 6.4829, Error 0.7146(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0360 | Time 20.0207(18.9756) | Bit/dim 5.4622(5.5542) | Xent 1.9461(1.9662) | Loss 14.6198(15.7916) | Error 0.6572(0.6768) Steps 568(543.66) | Grad Norm 9.3060(28.9442) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 18.9059(19.2281) | Bit/dim 5.4051(5.5255) | Xent 1.9463(1.9770) | Loss 14.4609(15.4866) | Error 0.6700(0.6827) Steps 544(545.71) | Grad Norm 16.5612(39.0083) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 76.4221, Epoch Time 630.5281(592.8861), Bit/dim 5.3688(best: 5.4848), Xent 1.9477, Loss 6.3427, Error 0.6601(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0380 | Time 21.0637(19.6130) | Bit/dim 5.3718(5.4886) | Xent 1.9482(1.9722) | Loss 14.4758(15.9329) | Error 0.6767(0.6815) Steps 544(548.94) | Grad Norm 5.3426(33.2459) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 21.1522(20.1043) | Bit/dim 5.3193(5.4478) | Xent 1.9387(1.9665) | Loss 14.3522(15.5283) | Error 0.6661(0.6787) Steps 592(556.05) | Grad Norm 5.6371(27.0860) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 21.7660(20.4986) | Bit/dim 5.2604(5.4063) | Xent 1.8992(1.9552) | Loss 14.2489(15.2056) | Error 0.6428(0.6730) Steps 556(561.30) | Grad Norm 9.2251(22.0591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 77.8011, Epoch Time 679.3610(595.4803), Bit/dim 5.2545(best: 5.3688), Xent 1.9054, Loss 6.2072, Error 0.6486(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0410 | Time 22.4242(20.9503) | Bit/dim 5.2542(5.3646) | Xent 1.8983(1.9444) | Loss 14.1281(15.5791) | Error 0.6561(0.6695) Steps 598(565.95) | Grad Norm 8.4176(18.9056) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 21.2705(21.2287) | Bit/dim 5.1966(5.3256) | Xent 1.9438(1.9359) | Loss 14.0504(15.1941) | Error 0.6806(0.6672) Steps 574(569.37) | Grad Norm 14.0273(16.1494) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 22.2381(21.2809) | Bit/dim 5.1882(5.2910) | Xent 1.8402(1.9248) | Loss 14.0077(14.9025) | Error 0.6394(0.6639) Steps 538(571.56) | Grad Norm 7.7635(15.3624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 77.3857, Epoch Time 682.2249(598.0827), Bit/dim 5.1689(best: 5.2545), Xent 1.8606, Loss 6.0992, Error 0.6404(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0440 | Time 20.4256(21.1078) | Bit/dim 5.1778(5.2591) | Xent 1.9365(1.9198) | Loss 14.0322(15.2473) | Error 0.6756(0.6637) Steps 538(569.10) | Grad Norm 38.1347(20.9653) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 21.2463(20.9508) | Bit/dim 5.1222(5.2281) | Xent 1.9098(1.9223) | Loss 14.0382(14.9151) | Error 0.6656(0.6671) Steps 580(569.64) | Grad Norm 34.7429(24.2108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 76.6816, Epoch Time 647.9611(599.5790), Bit/dim 5.0926(best: 5.1689), Xent 1.8494, Loss 6.0173, Error 0.6332(best: 0.6404)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0460 | Time 20.7412(20.8487) | Bit/dim 5.0686(5.1954) | Xent 1.8768(1.9173) | Loss 37.3566(15.3564) | Error 0.6539(0.6668) Steps 562(568.73) | Grad Norm 7.7329(22.3408) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 20.1454(20.7980) | Bit/dim 5.0671(5.1672) | Xent 1.8781(1.9058) | Loss 13.7852(14.9509) | Error 0.6594(0.6625) Steps 556(566.32) | Grad Norm 26.6555(21.3370) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 19.9003(20.6949) | Bit/dim 5.0412(5.1366) | Xent 1.9032(1.9034) | Loss 13.6099(14.6370) | Error 0.6628(0.6625) Steps 520(563.43) | Grad Norm 15.8260(23.3208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 76.9446, Epoch Time 648.8382(601.0568), Bit/dim 5.0356(best: 5.0926), Xent 1.8492, Loss 5.9602, Error 0.6352(best: 0.6332)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0490 | Time 21.6456(20.6556) | Bit/dim 5.0405(5.1130) | Xent 1.8744(1.9009) | Loss 13.8303(15.0622) | Error 0.6517(0.6620) Steps 562(565.68) | Grad Norm 13.6162(23.6263) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 21.2070(20.7038) | Bit/dim 5.0125(5.0844) | Xent 1.8469(1.8944) | Loss 13.5484(14.6861) | Error 0.6378(0.6586) Steps 520(564.21) | Grad Norm 12.6803(23.2281) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 19.9262(20.6776) | Bit/dim 4.9644(5.0612) | Xent 1.8918(1.8962) | Loss 13.4661(14.4112) | Error 0.6622(0.6610) Steps 562(565.20) | Grad Norm 28.2105(27.0208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 77.4968, Epoch Time 651.9753(602.5844), Bit/dim 4.9780(best: 5.0356), Xent 1.8617, Loss 5.9089, Error 0.6464(best: 0.6332)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0520 | Time 21.1889(20.5553) | Bit/dim 4.9194(5.0347) | Xent 1.8313(1.8935) | Loss 13.5061(14.7759) | Error 0.6489(0.6612) Steps 586(566.10) | Grad Norm 14.0955(27.6776) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 20.9652(20.4532) | Bit/dim 4.9128(5.0036) | Xent 1.8118(1.8822) | Loss 13.4621(14.4142) | Error 0.6372(0.6582) Steps 562(565.19) | Grad Norm 10.3489(23.9435) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 20.3998(20.5626) | Bit/dim 4.9129(4.9792) | Xent 1.8815(1.8813) | Loss 13.3658(14.1707) | Error 0.6683(0.6588) Steps 580(567.61) | Grad Norm 31.7626(24.3324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 77.2978, Epoch Time 646.0818(603.8893), Bit/dim 4.9283(best: 4.9780), Xent 1.8941, Loss 5.8754, Error 0.6574(best: 0.6332)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0550 | Time 21.9877(20.6208) | Bit/dim 4.8640(4.9563) | Xent 1.8178(1.8767) | Loss 13.4513(14.5362) | Error 0.6417(0.6575) Steps 550(568.62) | Grad Norm 20.5497(24.9881) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 20.4629(20.6331) | Bit/dim 4.8739(4.9330) | Xent 1.8589(1.8710) | Loss 13.3277(14.2258) | Error 0.6567(0.6554) Steps 532(570.53) | Grad Norm 31.8235(26.0139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 77.2178, Epoch Time 655.7154(605.4441), Bit/dim 4.8809(best: 4.9283), Xent 1.8433, Loss 5.8025, Error 0.6384(best: 0.6332)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0570 | Time 19.9363(20.6847) | Bit/dim 4.8695(4.9138) | Xent 1.8961(1.8690) | Loss 13.5166(14.6997) | Error 0.6783(0.6556) Steps 580(571.62) | Grad Norm 31.1973(27.5880) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 20.6699(20.7343) | Bit/dim 4.7926(4.8905) | Xent 1.8155(1.8595) | Loss 13.2695(14.3164) | Error 0.6217(0.6524) Steps 592(570.66) | Grad Norm 10.3522(26.2742) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 20.2905(20.7974) | Bit/dim 4.7963(4.8651) | Xent 1.8062(1.8454) | Loss 13.1424(13.9972) | Error 0.6300(0.6470) Steps 556(568.41) | Grad Norm 11.2571(23.1774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 77.3408, Epoch Time 658.4795(607.0351), Bit/dim 4.7666(best: 4.8809), Xent 1.7317, Loss 5.6324, Error 0.6053(best: 0.6332)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0600 | Time 21.6093(20.9303) | Bit/dim 4.7543(4.8386) | Xent 1.7922(1.8265) | Loss 13.0541(14.3712) | Error 0.6406(0.6400) Steps 592(569.81) | Grad Norm 18.9848(21.1526) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 21.4856(20.9466) | Bit/dim 4.9481(4.8797) | Xent 1.9486(1.8651) | Loss 13.7214(14.2127) | Error 0.6889(0.6553) Steps 586(571.50) | Grad Norm 52.5544(29.8440) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 20.6397(20.8156) | Bit/dim 4.8640(4.8943) | Xent 1.9114(1.9046) | Loss 13.2850(14.0448) | Error 0.6861(0.6720) Steps 532(565.03) | Grad Norm 22.9661(31.4023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 75.1592, Epoch Time 656.2440(608.5114), Bit/dim 4.8399(best: 4.7666), Xent 1.8608, Loss 5.7703, Error 0.6500(best: 0.6053)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0630 | Time 19.3462(20.6037) | Bit/dim 4.7871(4.8717) | Xent 1.8650(1.9079) | Loss 13.0500(14.3722) | Error 0.6633(0.6738) Steps 544(560.75) | Grad Norm 8.5536(26.5024) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 20.4479(20.5144) | Bit/dim 4.7259(4.8401) | Xent 1.7911(1.8901) | Loss 12.9042(14.0235) | Error 0.6300(0.6669) Steps 532(555.07) | Grad Norm 8.3416(21.6455) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 76.0986, Epoch Time 645.3724(609.6172), Bit/dim 4.7067(best: 4.7666), Xent 1.7510, Loss 5.5822, Error 0.6132(best: 0.6053)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0650 | Time 20.9943(20.7430) | Bit/dim 4.7087(4.8079) | Xent 1.7676(1.8672) | Loss 12.8193(14.4199) | Error 0.6194(0.6571) Steps 592(553.36) | Grad Norm 9.6990(18.6020) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 22.6108(20.8766) | Bit/dim 4.6590(4.7756) | Xent 1.7308(1.8450) | Loss 12.7122(14.0022) | Error 0.6017(0.6491) Steps 562(555.35) | Grad Norm 4.8950(17.5999) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 20.6924(20.8913) | Bit/dim 4.6695(4.7470) | Xent 1.7301(1.8236) | Loss 12.7138(13.6829) | Error 0.6094(0.6439) Steps 574(558.83) | Grad Norm 18.0786(17.6729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 76.5978, Epoch Time 664.1311(611.2526), Bit/dim 4.6591(best: 4.7067), Xent 1.6819, Loss 5.5001, Error 0.5895(best: 0.6053)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0680 | Time 21.6155(21.0367) | Bit/dim 4.6508(4.7226) | Xent 1.8139(1.8047) | Loss 12.6480(14.0622) | Error 0.6206(0.6370) Steps 556(562.88) | Grad Norm 45.3874(17.9522) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 23.2619(21.1340) | Bit/dim 4.6262(4.7028) | Xent 1.7154(1.7952) | Loss 12.7995(13.7300) | Error 0.6067(0.6344) Steps 610(564.94) | Grad Norm 10.5326(19.7037) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 19.6541(21.1426) | Bit/dim 4.6479(4.6932) | Xent 1.9264(1.8095) | Loss 13.0396(13.5145) | Error 0.6906(0.6400) Steps 592(566.17) | Grad Norm 48.4509(23.8818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 75.8928, Epoch Time 669.3190(612.9946), Bit/dim 4.6413(best: 4.6591), Xent 1.7610, Loss 5.5218, Error 0.6195(best: 0.5895)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0710 | Time 21.3434(21.1223) | Bit/dim 4.6443(4.6801) | Xent 1.7863(1.8064) | Loss 12.8272(13.8950) | Error 0.6339(0.6397) Steps 550(567.50) | Grad Norm 25.5521(23.2008) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 20.8585(21.3569) | Bit/dim 4.6070(4.6596) | Xent 1.7449(1.7949) | Loss 12.5977(13.5738) | Error 0.6189(0.6369) Steps 544(569.74) | Grad Norm 18.7601(21.8087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 77.7381, Epoch Time 678.4478(614.9582), Bit/dim 4.5799(best: 4.6413), Xent 1.6558, Loss 5.4078, Error 0.5834(best: 0.5895)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0730 | Time 19.4371(21.3356) | Bit/dim 4.5670(4.6413) | Xent 1.6591(1.7773) | Loss 36.1228(14.0217) | Error 0.5994(0.6311) Steps 550(568.61) | Grad Norm 10.6990(21.0157) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 21.8835(21.3376) | Bit/dim 4.5376(4.6227) | Xent 1.7028(1.7539) | Loss 12.5561(13.6297) | Error 0.6094(0.6226) Steps 592(571.87) | Grad Norm 8.5551(18.5363) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 22.4033(21.4836) | Bit/dim 4.5529(4.6086) | Xent 1.7195(1.7392) | Loss 12.5532(13.3483) | Error 0.5906(0.6175) Steps 574(571.40) | Grad Norm 27.9930(18.7901) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 78.7800, Epoch Time 680.5818(616.9269), Bit/dim 4.5407(best: 4.5799), Xent 1.6156, Loss 5.3485, Error 0.5667(best: 0.5834)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0760 | Time 21.3490(21.6277) | Bit/dim 4.5121(4.5886) | Xent 1.6214(1.7214) | Loss 12.2856(13.7728) | Error 0.5689(0.6105) Steps 568(572.65) | Grad Norm 19.2889(17.5565) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 21.8572(21.6038) | Bit/dim 4.5174(4.5741) | Xent 1.6390(1.7090) | Loss 12.4680(13.4420) | Error 0.5917(0.6069) Steps 574(575.18) | Grad Norm 13.1009(18.0321) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 22.1693(21.5658) | Bit/dim 4.5240(4.5621) | Xent 1.6557(1.7051) | Loss 12.4799(13.1930) | Error 0.6050(0.6062) Steps 574(577.53) | Grad Norm 24.0744(19.5406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 80.9305, Epoch Time 682.0521(618.8807), Bit/dim 4.5264(best: 4.5407), Xent 1.5953, Loss 5.3240, Error 0.5684(best: 0.5667)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0790 | Time 22.4593(21.5877) | Bit/dim 4.5248(4.5545) | Xent 1.6828(1.7027) | Loss 12.5115(13.6242) | Error 0.6011(0.6066) Steps 616(579.08) | Grad Norm 11.8362(19.7552) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 22.8154(21.6333) | Bit/dim 4.5610(4.5539) | Xent 1.7489(1.7016) | Loss 12.7072(13.3377) | Error 0.6200(0.6049) Steps 622(582.38) | Grad Norm 23.6303(22.6603) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 24.2506(21.8398) | Bit/dim 4.5196(4.5463) | Xent 1.6572(1.7011) | Loss 12.5140(13.1128) | Error 0.5744(0.6052) Steps 562(584.09) | Grad Norm 29.7987(24.0474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 80.8464, Epoch Time 689.5268(621.0001), Bit/dim 4.5111(best: 4.5264), Xent 1.5947, Loss 5.3085, Error 0.5653(best: 0.5667)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0820 | Time 21.5360(21.9089) | Bit/dim 4.4764(4.5306) | Xent 1.6358(1.6869) | Loss 12.3609(13.4603) | Error 0.5806(0.6018) Steps 598(586.91) | Grad Norm 13.0015(21.0632) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 23.6107(22.0436) | Bit/dim 4.4517(4.5119) | Xent 1.6331(1.6712) | Loss 12.1752(13.1469) | Error 0.6011(0.5975) Steps 616(586.22) | Grad Norm 30.4651(19.6246) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 80.9727, Epoch Time 696.4298(623.2630), Bit/dim 4.4453(best: 4.5111), Xent 1.5547, Loss 5.2227, Error 0.5580(best: 0.5653)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0840 | Time 20.9323(21.9900) | Bit/dim 4.4229(4.4954) | Xent 1.6741(1.6644) | Loss 12.2295(13.6257) | Error 0.6017(0.5949) Steps 586(588.42) | Grad Norm 30.8986(20.2846) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 22.0323(22.0417) | Bit/dim 4.4152(4.4822) | Xent 1.6334(1.6589) | Loss 12.1526(13.2475) | Error 0.5933(0.5928) Steps 616(592.77) | Grad Norm 11.5090(20.5719) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 21.0976(22.0969) | Bit/dim 4.4004(4.4679) | Xent 1.5810(1.6506) | Loss 12.1228(12.9760) | Error 0.5683(0.5918) Steps 580(594.82) | Grad Norm 17.3282(20.3324) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 79.8703, Epoch Time 695.5001(625.4301), Bit/dim 4.4240(best: 4.4453), Xent 1.5450, Loss 5.1965, Error 0.5498(best: 0.5580)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0870 | Time 23.0802(22.1790) | Bit/dim 4.4324(4.4560) | Xent 1.6622(1.6380) | Loss 12.3432(13.3872) | Error 0.5956(0.5868) Steps 598(595.12) | Grad Norm 29.8425(20.9077) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 23.3759(22.3266) | Bit/dim 4.4247(4.4437) | Xent 1.6869(1.6328) | Loss 12.2548(13.0667) | Error 0.6000(0.5851) Steps 580(597.37) | Grad Norm 40.3285(22.5097) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 23.7986(22.5332) | Bit/dim 4.4036(4.4355) | Xent 1.6296(1.6380) | Loss 12.0774(12.8377) | Error 0.5733(0.5858) Steps 616(601.55) | Grad Norm 18.9840(24.4595) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 78.8809, Epoch Time 713.1477(628.0616), Bit/dim 4.4047(best: 4.4240), Xent 1.5457, Loss 5.1776, Error 0.5610(best: 0.5498)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0900 | Time 21.7869(22.6291) | Bit/dim 4.3827(4.4231) | Xent 1.5891(1.6328) | Loss 11.9513(13.2306) | Error 0.5750(0.5855) Steps 592(603.59) | Grad Norm 16.0869(22.5984) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 24.0592(22.8704) | Bit/dim 4.3520(4.4063) | Xent 1.6543(1.6196) | Loss 12.1484(12.9106) | Error 0.5956(0.5815) Steps 640(605.61) | Grad Norm 8.2887(19.7163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 81.1466, Epoch Time 725.8396(630.9949), Bit/dim 4.3713(best: 4.4047), Xent 1.5300, Loss 5.1363, Error 0.5499(best: 0.5498)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0920 | Time 23.2974(23.0059) | Bit/dim 4.3433(4.3963) | Xent 1.6346(1.6166) | Loss 12.1932(13.3648) | Error 0.5906(0.5805) Steps 640(610.23) | Grad Norm 26.7793(21.4617) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 24.2019(23.2832) | Bit/dim 4.3365(4.3860) | Xent 1.5357(1.6080) | Loss 11.8321(13.0020) | Error 0.5678(0.5785) Steps 592(614.64) | Grad Norm 16.9346(21.1212) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 23.8112(23.3684) | Bit/dim 4.3249(4.3725) | Xent 1.7382(1.6047) | Loss 12.1329(12.7351) | Error 0.6206(0.5769) Steps 652(614.98) | Grad Norm 33.4411(22.1038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 81.6876, Epoch Time 742.6607(634.3449), Bit/dim 4.3210(best: 4.3713), Xent 1.4948, Loss 5.0683, Error 0.5377(best: 0.5498)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0950 | Time 24.3129(23.5156) | Bit/dim 4.3359(4.3626) | Xent 1.5346(1.6025) | Loss 11.8560(13.2222) | Error 0.5533(0.5761) Steps 586(618.29) | Grad Norm 11.9095(22.1718) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 24.9258(23.5903) | Bit/dim 4.3356(4.3547) | Xent 1.5637(1.5984) | Loss 12.0178(12.8930) | Error 0.5517(0.5737) Steps 598(618.78) | Grad Norm 20.3928(21.7765) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 21.7585(23.4898) | Bit/dim 4.3217(4.3469) | Xent 1.5470(1.5908) | Loss 11.8361(12.6478) | Error 0.5556(0.5703) Steps 604(618.07) | Grad Norm 20.0763(22.1877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 81.0400, Epoch Time 734.2690(637.3426), Bit/dim 4.2955(best: 4.3210), Xent 1.4360, Loss 5.0135, Error 0.5214(best: 0.5377)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0980 | Time 23.3698(23.4616) | Bit/dim 4.2722(4.3331) | Xent 1.5189(1.5681) | Loss 11.7294(13.0332) | Error 0.5644(0.5626) Steps 628(616.53) | Grad Norm 14.7441(19.8345) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 25.6907(23.5759) | Bit/dim 4.2527(4.3183) | Xent 1.5235(1.5565) | Loss 11.8375(12.7127) | Error 0.5528(0.5596) Steps 658(619.91) | Grad Norm 18.6885(19.2157) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 83.8569, Epoch Time 737.4166(640.3449), Bit/dim 4.2671(best: 4.2955), Xent 1.4643, Loss 4.9992, Error 0.5283(best: 0.5214)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1000 | Time 25.3572(23.5634) | Bit/dim 4.2791(4.3088) | Xent 1.5258(1.5560) | Loss 37.9088(13.2574) | Error 0.5483(0.5590) Steps 634(619.93) | Grad Norm 13.0166(20.7424) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 23.5259(23.7476) | Bit/dim 4.2485(4.2964) | Xent 1.5210(1.5511) | Loss 11.6797(12.8646) | Error 0.5383(0.5576) Steps 634(623.81) | Grad Norm 10.5321(20.5310) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 23.3288(23.6648) | Bit/dim 4.2355(4.2835) | Xent 1.4856(1.5370) | Loss 11.5156(12.5551) | Error 0.5256(0.5529) Steps 628(621.12) | Grad Norm 15.2309(18.5221) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 83.1941, Epoch Time 747.3858(643.5561), Bit/dim 4.2265(best: 4.2671), Xent 1.4251, Loss 4.9391, Error 0.5131(best: 0.5214)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1030 | Time 24.9796(23.7924) | Bit/dim 4.2326(4.2707) | Xent 1.4340(1.5244) | Loss 11.5438(13.0391) | Error 0.5189(0.5491) Steps 658(622.31) | Grad Norm 15.7140(17.9231) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 23.2585(23.6385) | Bit/dim 4.2685(4.2655) | Xent 1.5534(1.5300) | Loss 11.8751(12.7066) | Error 0.5594(0.5488) Steps 634(622.62) | Grad Norm 24.5373(20.9099) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 24.1711(23.7538) | Bit/dim 4.2388(4.2575) | Xent 1.4970(1.5318) | Loss 11.8574(12.4597) | Error 0.5428(0.5504) Steps 610(622.89) | Grad Norm 10.2175(19.1423) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 81.4222, Epoch Time 739.2952(646.4283), Bit/dim 4.2090(best: 4.2265), Xent 1.4088, Loss 4.9134, Error 0.5160(best: 0.5131)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1060 | Time 23.9083(23.6315) | Bit/dim 4.2616(4.2521) | Xent 1.6225(1.5338) | Loss 11.7930(12.8761) | Error 0.5800(0.5524) Steps 562(619.86) | Grad Norm 41.6141(20.4355) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 22.2106(23.5332) | Bit/dim 4.2137(4.2477) | Xent 1.5123(1.5202) | Loss 11.6139(12.5313) | Error 0.5356(0.5477) Steps 604(617.18) | Grad Norm 10.5396(18.2938) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 24.4598(23.4850) | Bit/dim 4.2211(4.2367) | Xent 1.6360(1.5126) | Loss 11.9241(12.2805) | Error 0.5794(0.5435) Steps 652(617.50) | Grad Norm 45.2182(19.2545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 80.7004, Epoch Time 725.2606(648.7932), Bit/dim 4.2098(best: 4.2090), Xent 1.3858, Loss 4.9027, Error 0.5026(best: 0.5131)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1090 | Time 24.9064(23.6032) | Bit/dim 4.2022(4.2265) | Xent 1.4941(1.4982) | Loss 11.5003(12.6408) | Error 0.5272(0.5385) Steps 634(620.60) | Grad Norm 23.8639(18.2524) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 23.8283(23.5099) | Bit/dim 4.1886(4.2139) | Xent 1.4656(1.4832) | Loss 11.5810(12.3269) | Error 0.5294(0.5348) Steps 616(618.06) | Grad Norm 18.4293(17.3647) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1110 | Time 24.2050(23.5468) | Bit/dim 4.1394(4.1999) | Xent 1.3862(1.4669) | Loss 11.2857(12.7765) | Error 0.5161(0.5292) Steps 628(617.38) | Grad Norm 9.4619(15.9043) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 22.0419(23.4830) | Bit/dim 4.1584(4.1892) | Xent 1.4174(1.4467) | Loss 11.2569(12.3994) | Error 0.5117(0.5225) Steps 586(614.21) | Grad Norm 16.5091(14.3399) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 23.4041(23.4373) | Bit/dim 4.1490(4.1817) | Xent 1.4170(1.4454) | Loss 11.4870(12.1473) | Error 0.5206(0.5216) Steps 610(612.97) | Grad Norm 12.9151(15.0307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 82.6580, Epoch Time 733.9304(653.8143), Bit/dim 4.1426(best: 4.1604), Xent 1.3241, Loss 4.8047, Error 0.4778(best: 0.4921)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1140 | Time 24.2820(23.4783) | Bit/dim 4.1263(4.1708) | Xent 1.3889(1.4343) | Loss 11.3059(12.5740) | Error 0.4817(0.5181) Steps 580(610.70) | Grad Norm 11.3215(14.3588) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 24.3737(23.4504) | Bit/dim 4.1549(4.1635) | Xent 1.3647(1.4321) | Loss 11.4554(12.2546) | Error 0.4961(0.5153) Steps 598(609.58) | Grad Norm 13.0477(15.6717) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 24.0256(23.4830) | Bit/dim 4.1271(4.1542) | Xent 1.4293(1.4262) | Loss 11.3756(12.0154) | Error 0.5056(0.5124) Steps 622(609.65) | Grad Norm 9.8169(15.4919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 81.1773, Epoch Time 730.9053(656.1270), Bit/dim 4.1272(best: 4.1426), Xent 1.3129, Loss 4.7836, Error 0.4763(best: 0.4778)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1170 | Time 23.3528(23.4510) | Bit/dim 4.1875(4.1492) | Xent 1.5750(1.4263) | Loss 11.5646(12.4192) | Error 0.5567(0.5117) Steps 598(610.65) | Grad Norm 34.0821(17.0148) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 22.7386(23.5245) | Bit/dim 4.1337(4.1485) | Xent 1.3450(1.4287) | Loss 11.1551(12.1400) | Error 0.4817(0.5128) Steps 604(611.57) | Grad Norm 10.9680(17.3263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 79.5977, Epoch Time 733.2796(658.4416), Bit/dim 4.1141(best: 4.1272), Xent 1.3001, Loss 4.7641, Error 0.4729(best: 0.4763)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1190 | Time 22.6088(23.4808) | Bit/dim 4.1152(4.1422) | Xent 1.3594(1.4178) | Loss 11.3816(12.6479) | Error 0.4817(0.5084) Steps 610(612.10) | Grad Norm 14.7822(16.5870) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 24.0160(23.5104) | Bit/dim 4.0949(4.1346) | Xent 1.3557(1.3980) | Loss 11.2584(12.2764) | Error 0.4894(0.5025) Steps 616(612.09) | Grad Norm 10.8361(15.4341) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 24.0918(23.4073) | Bit/dim 4.0914(4.1252) | Xent 1.3619(1.3978) | Loss 11.1671(12.0035) | Error 0.4889(0.5026) Steps 592(610.89) | Grad Norm 8.9177(15.7368) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 80.9590, Epoch Time 728.0058(660.5285), Bit/dim 4.0903(best: 4.1141), Xent 1.3056, Loss 4.7430, Error 0.4740(best: 0.4729)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1220 | Time 23.2367(23.3919) | Bit/dim 4.0750(4.1148) | Xent 1.3767(1.3851) | Loss 11.1082(12.4370) | Error 0.4917(0.4990) Steps 598(606.81) | Grad Norm 23.3965(14.8487) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 24.3335(23.4265) | Bit/dim 4.1282(4.1112) | Xent 1.5463(1.3908) | Loss 11.2872(12.1179) | Error 0.5339(0.4987) Steps 634(605.90) | Grad Norm 31.2077(16.5144) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 20.8344(23.3406) | Bit/dim 4.0589(4.1048) | Xent 1.3513(1.3980) | Loss 11.0594(11.8786) | Error 0.4933(0.5017) Steps 598(604.69) | Grad Norm 11.8152(16.8490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 78.9821, Epoch Time 726.5368(662.5088), Bit/dim 4.0802(best: 4.0903), Xent 1.2897, Loss 4.7251, Error 0.4668(best: 0.4729)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1250 | Time 22.4190(23.2976) | Bit/dim 4.0976(4.0972) | Xent 1.3646(1.3820) | Loss 11.1206(12.2581) | Error 0.4883(0.4968) Steps 610(603.44) | Grad Norm 15.7570(15.3025) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 23.0327(23.2729) | Bit/dim 4.0685(4.0907) | Xent 1.3642(1.3666) | Loss 11.1919(11.9473) | Error 0.4878(0.4908) Steps 598(602.41) | Grad Norm 15.3491(14.1880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 77.5996, Epoch Time 716.2368(664.1206), Bit/dim 4.0528(best: 4.0802), Xent 1.2442, Loss 4.6749, Error 0.4528(best: 0.4668)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1270 | Time 21.2680(23.1080) | Bit/dim 4.0415(4.0799) | Xent 1.2866(1.3515) | Loss 34.8086(12.4172) | Error 0.4600(0.4856) Steps 580(598.53) | Grad Norm 4.8187(12.4492) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 22.4575(23.1307) | Bit/dim 4.0821(4.0754) | Xent 1.3428(1.3547) | Loss 11.1713(12.0783) | Error 0.4700(0.4856) Steps 598(597.93) | Grad Norm 29.9131(14.7393) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 22.7099(23.2561) | Bit/dim 4.0502(4.0726) | Xent 1.3898(1.3573) | Loss 11.0174(11.8295) | Error 0.4994(0.4869) Steps 574(598.91) | Grad Norm 15.7393(15.4526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 79.2871, Epoch Time 723.1953(665.8929), Bit/dim 4.0482(best: 4.0528), Xent 1.2505, Loss 4.6735, Error 0.4530(best: 0.4528)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1300 | Time 24.4822(23.2111) | Bit/dim 4.0601(4.0667) | Xent 1.2489(1.3455) | Loss 11.0894(12.2633) | Error 0.4422(0.4841) Steps 574(595.04) | Grad Norm 7.3705(13.9797) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 22.7697(23.1867) | Bit/dim 4.0187(4.0580) | Xent 1.3092(1.3326) | Loss 10.8647(11.9316) | Error 0.4544(0.4787) Steps 562(595.86) | Grad Norm 4.4651(12.2626) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 24.4671(23.2150) | Bit/dim 4.0379(4.0508) | Xent 1.3132(1.3236) | Loss 11.0589(11.6812) | Error 0.4600(0.4750) Steps 640(596.00) | Grad Norm 13.7903(12.9597) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 78.2622, Epoch Time 719.9586(667.5148), Bit/dim 4.0254(best: 4.0482), Xent 1.2343, Loss 4.6426, Error 0.4433(best: 0.4528)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1330 | Time 22.6106(23.0771) | Bit/dim 4.0397(4.0446) | Xent 1.2867(1.3157) | Loss 10.8904(12.0644) | Error 0.4539(0.4730) Steps 598(594.19) | Grad Norm 14.8732(12.5480) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 22.8805(23.0395) | Bit/dim 4.0067(4.0367) | Xent 1.2737(1.3056) | Loss 10.9083(11.7702) | Error 0.4628(0.4687) Steps 592(590.64) | Grad Norm 14.2885(12.1352) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 22.7038(23.1544) | Bit/dim 4.0249(4.0317) | Xent 1.3014(1.3035) | Loss 10.9457(11.5528) | Error 0.4739(0.4684) Steps 568(592.55) | Grad Norm 14.6484(12.0579) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 78.6086, Epoch Time 720.4033(669.1015), Bit/dim 4.0229(best: 4.0254), Xent 1.2738, Loss 4.6598, Error 0.4584(best: 0.4433)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1360 | Time 22.3953(23.1032) | Bit/dim 4.0122(4.0276) | Xent 1.2618(1.2954) | Loss 10.8419(11.9255) | Error 0.4717(0.4655) Steps 574(591.94) | Grad Norm 16.8362(13.0228) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 22.0752(23.0324) | Bit/dim 3.9953(4.0249) | Xent 1.3069(1.3002) | Loss 10.9222(11.6687) | Error 0.4717(0.4676) Steps 574(592.19) | Grad Norm 14.8423(14.2353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 79.4010, Epoch Time 717.5965(670.5563), Bit/dim 4.0022(best: 4.0229), Xent 1.2726, Loss 4.6385, Error 0.4531(best: 0.4433)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1380 | Time 22.0961(23.0465) | Bit/dim 4.0249(4.0193) | Xent 1.2286(1.2992) | Loss 10.8041(12.1642) | Error 0.4422(0.4679) Steps 556(590.44) | Grad Norm 19.0177(14.6885) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 24.0040(23.1136) | Bit/dim 4.0135(4.0191) | Xent 1.2598(1.2909) | Loss 10.9642(11.8308) | Error 0.4500(0.4638) Steps 598(592.01) | Grad Norm 10.4550(14.5328) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 23.4223(23.0991) | Bit/dim 3.9539(4.0125) | Xent 1.2855(1.2861) | Loss 10.8176(11.5831) | Error 0.4661(0.4627) Steps 604(592.14) | Grad Norm 10.1234(14.1433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 83.1248, Epoch Time 726.1055(672.2228), Bit/dim 3.9921(best: 4.0022), Xent 1.1987, Loss 4.5915, Error 0.4358(best: 0.4433)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1410 | Time 23.9786(23.2552) | Bit/dim 3.9968(4.0086) | Xent 1.2445(1.2757) | Loss 10.7552(11.9956) | Error 0.4450(0.4584) Steps 586(591.29) | Grad Norm 9.2949(14.0043) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 22.6191(23.4027) | Bit/dim 3.9778(4.0024) | Xent 1.2856(1.2657) | Loss 10.9235(11.6852) | Error 0.4511(0.4554) Steps 592(591.93) | Grad Norm 16.2154(13.3870) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 22.2720(23.4123) | Bit/dim 3.9803(3.9950) | Xent 1.2724(1.2641) | Loss 10.8836(11.4599) | Error 0.4378(0.4535) Steps 556(594.86) | Grad Norm 17.9043(12.9947) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 83.1489, Epoch Time 740.1268(674.2599), Bit/dim 3.9742(best: 3.9921), Xent 1.2047, Loss 4.5765, Error 0.4308(best: 0.4358)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1440 | Time 23.0500(23.3347) | Bit/dim 3.9772(3.9889) | Xent 1.2419(1.2589) | Loss 10.7018(11.8472) | Error 0.4428(0.4514) Steps 592(595.18) | Grad Norm 13.4892(13.0144) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 21.9379(23.2954) | Bit/dim 3.9577(3.9847) | Xent 1.2506(1.2622) | Loss 10.7088(11.5856) | Error 0.4539(0.4523) Steps 568(593.34) | Grad Norm 13.2744(13.2106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 80.8557, Epoch Time 725.4061(675.7943), Bit/dim 3.9637(best: 3.9742), Xent 1.1903, Loss 4.5588, Error 0.4297(best: 0.4308)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1460 | Time 23.0398(23.2815) | Bit/dim 3.9577(3.9782) | Xent 1.2226(1.2566) | Loss 10.5884(12.0534) | Error 0.4333(0.4509) Steps 604(592.21) | Grad Norm 10.9231(12.2315) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 24.6964(23.2146) | Bit/dim 3.9540(3.9750) | Xent 1.2667(1.2535) | Loss 10.7643(11.7124) | Error 0.4489(0.4492) Steps 598(592.70) | Grad Norm 18.7245(12.6690) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 23.3053(23.1572) | Bit/dim 3.9640(3.9727) | Xent 1.2320(1.2482) | Loss 10.5524(11.4563) | Error 0.4361(0.4467) Steps 562(592.15) | Grad Norm 9.3769(13.6878) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 78.3192, Epoch Time 714.8538(676.9661), Bit/dim 3.9614(best: 3.9637), Xent 1.1904, Loss 4.5566, Error 0.4277(best: 0.4297)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1490 | Time 23.7481(23.1435) | Bit/dim 3.9437(3.9686) | Xent 1.1779(1.2431) | Loss 10.7584(11.9211) | Error 0.4267(0.4444) Steps 628(593.82) | Grad Norm 18.1545(14.6443) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 24.9486(23.2365) | Bit/dim 3.9416(3.9643) | Xent 1.1788(1.2384) | Loss 10.6115(11.6090) | Error 0.4217(0.4425) Steps 568(592.93) | Grad Norm 6.3931(13.4375) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 22.3637(23.1400) | Bit/dim 3.9291(3.9586) | Xent 1.2006(1.2359) | Loss 10.6183(11.3775) | Error 0.4233(0.4421) Steps 574(593.94) | Grad Norm 14.8157(12.9594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 80.2543, Epoch Time 720.9106(678.2844), Bit/dim 3.9448(best: 3.9614), Xent 1.1663, Loss 4.5280, Error 0.4214(best: 0.4277)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1520 | Time 22.2728(23.0226) | Bit/dim 3.9528(3.9545) | Xent 1.2808(1.2334) | Loss 10.7058(11.7868) | Error 0.4600(0.4423) Steps 586(594.54) | Grad Norm 20.4843(13.0083) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 21.1854(22.9053) | Bit/dim 3.9851(3.9523) | Xent 1.2106(1.2423) | Loss 10.8105(11.5152) | Error 0.4294(0.4444) Steps 574(592.88) | Grad Norm 14.1696(14.4290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 79.5966, Epoch Time 718.6330(679.4949), Bit/dim 3.9431(best: 3.9448), Xent 1.1415, Loss 4.5138, Error 0.4100(best: 0.4214)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1540 | Time 23.2245(23.1101) | Bit/dim 3.9308(3.9471) | Xent 1.2011(1.2333) | Loss 35.7955(12.0437) | Error 0.4228(0.4411) Steps 598(592.51) | Grad Norm 10.2551(13.4032) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 22.7104(23.1883) | Bit/dim 3.9198(3.9427) | Xent 1.2482(1.2242) | Loss 10.5787(11.6665) | Error 0.4372(0.4368) Steps 598(593.46) | Grad Norm 17.1406(12.5876) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 22.4086(23.1208) | Bit/dim 3.9555(3.9401) | Xent 1.1693(1.2201) | Loss 10.6682(11.3997) | Error 0.4061(0.4353) Steps 574(590.91) | Grad Norm 7.3463(11.4969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1570 | Time 21.6840(23.1548) | Bit/dim 3.9513(3.9357) | Xent 1.1457(1.2119) | Loss 10.6038(11.8537) | Error 0.4011(0.4323) Steps 586(590.95) | Grad Norm 15.1360(11.3269) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 23.3493(23.1639) | Bit/dim 3.8842(3.9296) | Xent 1.1746(1.2026) | Loss 10.4394(11.5151) | Error 0.4278(0.4303) Steps 604(593.18) | Grad Norm 15.8307(11.6445) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 23.4244(23.2213) | Bit/dim 3.9292(3.9268) | Xent 1.2275(1.2040) | Loss 10.6464(11.2829) | Error 0.4472(0.4306) Steps 616(594.30) | Grad Norm 7.8872(12.1474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 78.7787, Epoch Time 723.6821(682.1604), Bit/dim 3.9128(best: 3.9212), Xent 1.1562, Loss 4.4909, Error 0.4191(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1600 | Time 23.2361(23.2328) | Bit/dim 3.9174(3.9229) | Xent 1.1565(1.1991) | Loss 10.5023(11.6828) | Error 0.4194(0.4288) Steps 592(591.03) | Grad Norm 8.0534(12.2826) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 22.9964(23.2279) | Bit/dim 3.9155(3.9208) | Xent 1.1771(1.2005) | Loss 10.6069(11.4049) | Error 0.4350(0.4302) Steps 568(590.89) | Grad Norm 12.8889(13.1321) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 23.3018(23.2000) | Bit/dim 3.9008(3.9185) | Xent 1.2015(1.1967) | Loss 10.6017(11.1831) | Error 0.4206(0.4287) Steps 556(589.69) | Grad Norm 12.7559(12.5166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 79.0516, Epoch Time 719.7348(683.2877), Bit/dim 3.9073(best: 3.9128), Xent 1.1433, Loss 4.4789, Error 0.4123(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1630 | Time 22.2300(23.2418) | Bit/dim 3.8853(3.9144) | Xent 1.2163(1.1949) | Loss 10.5815(11.5657) | Error 0.4383(0.4279) Steps 604(588.20) | Grad Norm 10.8328(12.2419) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 23.0167(23.1175) | Bit/dim 3.9021(3.9124) | Xent 1.1539(1.1998) | Loss 10.5328(11.3081) | Error 0.4133(0.4286) Steps 568(587.18) | Grad Norm 12.5700(13.5806) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 79.4984, Epoch Time 722.7809(684.4725), Bit/dim 3.9062(best: 3.9073), Xent 1.1454, Loss 4.4789, Error 0.4105(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1650 | Time 23.2455(23.1906) | Bit/dim 3.9117(3.9104) | Xent 1.1645(1.1991) | Loss 10.6863(11.7904) | Error 0.4239(0.4292) Steps 622(589.16) | Grad Norm 9.9166(13.0034) | Total Time 0.00(0.00)\n",
      "Iter 1660 | Time 22.6090(23.2540) | Bit/dim 3.8920(3.9070) | Xent 1.1857(1.1903) | Loss 10.5654(11.4568) | Error 0.4400(0.4270) Steps 568(584.68) | Grad Norm 10.9364(13.2074) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 23.4504(23.1330) | Bit/dim 3.9278(3.9078) | Xent 1.1353(1.1801) | Loss 10.5619(11.2111) | Error 0.4039(0.4230) Steps 616(583.32) | Grad Norm 18.5683(13.5451) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 79.0586, Epoch Time 722.8123(685.6227), Bit/dim 3.8943(best: 3.9062), Xent 1.1511, Loss 4.4698, Error 0.4168(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1680 | Time 23.5381(23.1837) | Bit/dim 3.9314(3.9046) | Xent 1.0996(1.1757) | Loss 10.5016(11.6633) | Error 0.4044(0.4215) Steps 628(586.12) | Grad Norm 13.0690(13.4543) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 23.2537(23.2122) | Bit/dim 3.9176(3.9026) | Xent 1.2289(1.1828) | Loss 10.6429(11.3792) | Error 0.4417(0.4230) Steps 580(583.41) | Grad Norm 25.7884(14.4929) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 23.6537(23.2618) | Bit/dim 3.8767(3.9006) | Xent 1.2459(1.1880) | Loss 10.6179(11.1788) | Error 0.4472(0.4254) Steps 586(586.83) | Grad Norm 14.0607(14.3998) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 79.9939, Epoch Time 725.6619(686.8238), Bit/dim 3.8986(best: 3.8943), Xent 1.1356, Loss 4.4664, Error 0.4108(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1710 | Time 22.0287(23.2540) | Bit/dim 3.8894(3.8968) | Xent 1.1426(1.1827) | Loss 10.0980(11.5684) | Error 0.4150(0.4240) Steps 568(586.68) | Grad Norm 10.5288(13.6973) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 22.5564(23.2350) | Bit/dim 3.8672(3.8942) | Xent 1.1109(1.1705) | Loss 10.2973(11.2671) | Error 0.4000(0.4203) Steps 586(585.96) | Grad Norm 7.0824(13.0707) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 78.4892, Epoch Time 719.6734(687.8093), Bit/dim 3.8781(best: 3.8943), Xent 1.0917, Loss 4.4240, Error 0.3905(best: 0.4100)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1730 | Time 22.4144(23.1663) | Bit/dim 3.8915(3.8918) | Xent 1.1271(1.1575) | Loss 10.4125(11.7461) | Error 0.3961(0.4148) Steps 568(583.42) | Grad Norm 9.7151(12.2318) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 22.6103(23.1469) | Bit/dim 3.8488(3.8883) | Xent 1.1072(1.1483) | Loss 10.4176(11.4056) | Error 0.4028(0.4105) Steps 586(585.61) | Grad Norm 11.8454(11.8670) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 22.4246(23.0105) | Bit/dim 3.8785(3.8831) | Xent 1.1790(1.1455) | Loss 10.5303(11.1519) | Error 0.4128(0.4091) Steps 604(586.43) | Grad Norm 14.6629(11.8860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 78.7749, Epoch Time 714.1677(688.6001), Bit/dim 3.8851(best: 3.8781), Xent 1.1200, Loss 4.4451, Error 0.4011(best: 0.3905)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1760 | Time 22.9035(23.0476) | Bit/dim 3.8751(3.8834) | Xent 1.1572(1.1467) | Loss 10.5906(11.6171) | Error 0.4111(0.4086) Steps 610(588.59) | Grad Norm 16.6646(13.2882) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 22.3604(23.1111) | Bit/dim 3.8686(3.8795) | Xent 1.0936(1.1395) | Loss 10.4371(11.3035) | Error 0.3883(0.4056) Steps 580(587.25) | Grad Norm 17.0187(12.6838) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 22.7980(23.1495) | Bit/dim 3.8750(3.8777) | Xent 1.0949(1.1379) | Loss 10.4149(11.0781) | Error 0.3850(0.4061) Steps 592(582.28) | Grad Norm 9.9951(13.1337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 78.4540, Epoch Time 724.7525(689.6846), Bit/dim 3.8799(best: 3.8781), Xent 1.0932, Loss 4.4265, Error 0.3883(best: 0.3905)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1790 | Time 22.0860(23.0929) | Bit/dim 3.8492(3.8739) | Xent 1.1218(1.1357) | Loss 10.3881(11.5012) | Error 0.3989(0.4052) Steps 592(584.46) | Grad Norm 10.4184(13.6810) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 21.6420(23.0389) | Bit/dim 3.8868(3.8728) | Xent 1.1095(1.1354) | Loss 10.4379(11.2265) | Error 0.3922(0.4046) Steps 580(586.03) | Grad Norm 17.0471(13.7034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 79.2507, Epoch Time 717.5231(690.5198), Bit/dim 3.8607(best: 3.8781), Xent 1.0617, Loss 4.3915, Error 0.3763(best: 0.3883)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1810 | Time 23.7021(23.1152) | Bit/dim 3.8808(3.8717) | Xent 1.0338(1.1290) | Loss 34.5682(11.7368) | Error 0.3761(0.4029) Steps 574(585.86) | Grad Norm 6.9007(13.0095) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 22.4008(23.2310) | Bit/dim 3.8586(3.8674) | Xent 1.0998(1.1222) | Loss 10.4382(11.3830) | Error 0.3883(0.4002) Steps 604(587.90) | Grad Norm 13.6607(12.5891) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 22.2189(23.0880) | Bit/dim 3.8665(3.8681) | Xent 1.0433(1.1231) | Loss 10.3143(11.1333) | Error 0.3811(0.4006) Steps 586(586.08) | Grad Norm 11.0029(13.4681) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 81.8957, Epoch Time 721.6707(691.4543), Bit/dim 3.8681(best: 3.8607), Xent 1.0761, Loss 4.4062, Error 0.3829(best: 0.3763)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1840 | Time 23.0942(23.0925) | Bit/dim 3.8791(3.8651) | Xent 1.1148(1.1213) | Loss 10.3903(11.6202) | Error 0.3972(0.3992) Steps 586(587.20) | Grad Norm 5.6957(13.3793) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 22.3470(23.1859) | Bit/dim 3.8593(3.8634) | Xent 1.1244(1.1277) | Loss 10.3515(11.3075) | Error 0.4050(0.4013) Steps 586(590.25) | Grad Norm 15.5075(14.1348) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 22.1626(23.3232) | Bit/dim 3.8497(3.8635) | Xent 1.1132(1.1254) | Loss 10.4691(11.0875) | Error 0.3867(0.4005) Steps 616(595.41) | Grad Norm 9.8239(14.3054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 79.8041, Epoch Time 731.9312(692.6686), Bit/dim 3.8570(best: 3.8607), Xent 1.0559, Loss 4.3850, Error 0.3720(best: 0.3763)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1870 | Time 23.3270(23.3402) | Bit/dim 3.8407(3.8613) | Xent 1.1442(1.1318) | Loss 10.3988(11.5329) | Error 0.3978(0.4017) Steps 586(593.58) | Grad Norm 9.3759(14.5742) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 22.1078(23.1912) | Bit/dim 3.8514(3.8608) | Xent 1.1306(1.1275) | Loss 10.4345(11.2383) | Error 0.4017(0.4006) Steps 574(591.28) | Grad Norm 5.4992(13.1618) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 23.2049(23.2381) | Bit/dim 3.8497(3.8590) | Xent 1.1074(1.1255) | Loss 10.4135(11.0258) | Error 0.3872(0.4000) Steps 616(590.74) | Grad Norm 6.1683(13.1461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 80.2292, Epoch Time 723.2052(693.5847), Bit/dim 3.8510(best: 3.8570), Xent 1.0835, Loss 4.3927, Error 0.3831(best: 0.3720)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1900 | Time 22.4477(23.3872) | Bit/dim 3.8332(3.8555) | Xent 1.1267(1.1135) | Loss 10.3801(11.3958) | Error 0.3961(0.3961) Steps 592(590.12) | Grad Norm 7.7316(12.9063) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 23.3087(23.3608) | Bit/dim 3.8341(3.8531) | Xent 1.1018(1.1097) | Loss 10.3228(11.1176) | Error 0.3922(0.3953) Steps 544(587.96) | Grad Norm 13.0203(13.0751) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 81.5884, Epoch Time 731.6364(694.7263), Bit/dim 3.8527(best: 3.8510), Xent 1.0343, Loss 4.3698, Error 0.3662(best: 0.3720)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1920 | Time 24.0858(23.3081) | Bit/dim 3.8371(3.8517) | Xent 1.0143(1.1026) | Loss 10.2343(11.5969) | Error 0.3717(0.3928) Steps 574(586.81) | Grad Norm 10.7755(12.8846) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 23.4279(23.2993) | Bit/dim 3.8595(3.8496) | Xent 1.0844(1.0993) | Loss 10.5189(11.2681) | Error 0.3900(0.3911) Steps 592(588.75) | Grad Norm 9.2320(12.4380) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 23.2773(23.2520) | Bit/dim 3.8513(3.8469) | Xent 1.0621(1.0969) | Loss 10.3215(11.0241) | Error 0.3739(0.3903) Steps 556(586.32) | Grad Norm 8.4098(12.6532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 80.7847, Epoch Time 723.4885(695.5891), Bit/dim 3.8334(best: 3.8510), Xent 1.0479, Loss 4.3574, Error 0.3699(best: 0.3662)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1950 | Time 23.9562(23.2274) | Bit/dim 3.8221(3.8455) | Xent 1.0597(1.0864) | Loss 10.3633(11.4532) | Error 0.3667(0.3863) Steps 610(583.69) | Grad Norm 9.8974(12.0887) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 22.7131(23.1734) | Bit/dim 3.8147(3.8404) | Xent 1.0743(1.0839) | Loss 10.2546(11.1452) | Error 0.3850(0.3863) Steps 562(584.75) | Grad Norm 14.5447(12.0223) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 23.4058(23.1875) | Bit/dim 3.8211(3.8376) | Xent 1.1140(1.0831) | Loss 10.2809(10.9269) | Error 0.4133(0.3865) Steps 586(584.04) | Grad Norm 11.0522(11.6202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 80.7959, Epoch Time 723.1977(696.4174), Bit/dim 3.8394(best: 3.8334), Xent 1.0564, Loss 4.3676, Error 0.3713(best: 0.3662)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1980 | Time 23.6270(23.2419) | Bit/dim 3.8346(3.8368) | Xent 1.0010(1.0737) | Loss 10.1631(11.2798) | Error 0.3472(0.3824) Steps 592(584.05) | Grad Norm 10.2810(11.7181) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 24.6120(23.2500) | Bit/dim 3.8173(3.8345) | Xent 1.0374(1.0681) | Loss 10.4078(11.0183) | Error 0.3650(0.3798) Steps 592(585.62) | Grad Norm 8.5048(11.5226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 78.9614, Epoch Time 726.0411(697.3061), Bit/dim 3.8577(best: 3.8334), Xent 1.0952, Loss 4.4053, Error 0.3852(best: 0.3662)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2000 | Time 23.8161(23.2587) | Bit/dim 3.8442(3.8354) | Xent 1.0727(1.0785) | Loss 10.3413(11.5530) | Error 0.3928(0.3836) Steps 610(588.25) | Grad Norm 11.7730(12.9483) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 22.5345(23.1922) | Bit/dim 3.8346(3.8358) | Xent 1.0165(1.0812) | Loss 10.4064(11.2310) | Error 0.3722(0.3851) Steps 580(585.86) | Grad Norm 11.7355(13.4890) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 23.7741(23.3795) | Bit/dim 3.8387(3.8343) | Xent 1.0150(1.0748) | Loss 10.1760(10.9829) | Error 0.3689(0.3836) Steps 598(588.14) | Grad Norm 10.3661(13.0825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 78.2137, Epoch Time 728.7232(698.2486), Bit/dim 3.8282(best: 3.8334), Xent 1.0163, Loss 4.3364, Error 0.3651(best: 0.3662)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2030 | Time 23.6023(23.3426) | Bit/dim 3.8177(3.8322) | Xent 1.0012(1.0658) | Loss 10.2738(11.4108) | Error 0.3483(0.3801) Steps 604(584.25) | Grad Norm 14.5987(11.9625) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 23.0901(23.4449) | Bit/dim 3.8143(3.8276) | Xent 1.0239(1.0551) | Loss 10.0609(11.0843) | Error 0.3589(0.3758) Steps 580(584.16) | Grad Norm 6.4032(11.1615) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 22.4489(23.3400) | Bit/dim 3.8317(3.8255) | Xent 1.0304(1.0536) | Loss 10.2745(10.8693) | Error 0.3628(0.3745) Steps 628(586.54) | Grad Norm 16.4378(11.5438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2060 | Time 22.9807(23.2480) | Bit/dim 3.8165(3.8244) | Xent 1.0090(1.0512) | Loss 10.1621(11.2746) | Error 0.3472(0.3733) Steps 598(584.90) | Grad Norm 9.5128(11.4684) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 22.8365(23.1921) | Bit/dim 3.8259(3.8223) | Xent 1.0124(1.0441) | Loss 10.3182(10.9923) | Error 0.3600(0.3703) Steps 604(583.78) | Grad Norm 17.3085(11.9707) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 81.4016, Epoch Time 719.8303(699.7371), Bit/dim 3.8188(best: 3.8202), Xent 1.0288, Loss 4.3332, Error 0.3651(best: 0.3651)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2080 | Time 24.3956(23.1933) | Bit/dim 3.8235(3.8206) | Xent 1.0328(1.0427) | Loss 34.0867(11.4989) | Error 0.3678(0.3715) Steps 580(583.15) | Grad Norm 13.9416(12.1143) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 23.5241(23.2282) | Bit/dim 3.8116(3.8191) | Xent 1.0081(1.0401) | Loss 10.1326(11.1647) | Error 0.3667(0.3707) Steps 568(582.55) | Grad Norm 16.2549(12.3468) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 22.3928(23.2954) | Bit/dim 3.8348(3.8182) | Xent 1.0406(1.0382) | Loss 10.2181(10.9096) | Error 0.3678(0.3699) Steps 562(581.47) | Grad Norm 11.1687(12.3744) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 79.3944, Epoch Time 729.4891(700.6296), Bit/dim 3.8076(best: 3.8188), Xent 0.9802, Loss 4.2977, Error 0.3449(best: 0.3651)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2110 | Time 22.0903(23.3042) | Bit/dim 3.8184(3.8141) | Xent 0.9812(1.0290) | Loss 10.1053(11.3579) | Error 0.3417(0.3660) Steps 586(580.49) | Grad Norm 11.6876(12.1018) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 22.3615(23.2955) | Bit/dim 3.7790(3.8104) | Xent 1.0576(1.0266) | Loss 10.2419(11.0456) | Error 0.3728(0.3652) Steps 580(579.71) | Grad Norm 11.7516(11.3504) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 23.3293(23.3128) | Bit/dim 3.8141(3.8094) | Xent 1.0135(1.0264) | Loss 10.2411(10.8220) | Error 0.3628(0.3649) Steps 580(580.14) | Grad Norm 10.3084(11.2771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 80.3138, Epoch Time 724.8555(701.3564), Bit/dim 3.8033(best: 3.8076), Xent 0.9904, Loss 4.2984, Error 0.3440(best: 0.3449)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2140 | Time 22.4639(23.3031) | Bit/dim 3.8250(3.8074) | Xent 1.0626(1.0247) | Loss 10.1488(11.2490) | Error 0.3728(0.3638) Steps 562(580.66) | Grad Norm 12.9629(11.2962) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 22.2500(23.3105) | Bit/dim 3.7811(3.8075) | Xent 0.9871(1.0197) | Loss 9.9645(10.9728) | Error 0.3589(0.3633) Steps 550(583.72) | Grad Norm 12.6152(11.8712) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 22.2471(23.3167) | Bit/dim 3.7965(3.8059) | Xent 1.0485(1.0210) | Loss 10.1604(10.7635) | Error 0.3672(0.3638) Steps 592(584.93) | Grad Norm 13.8354(12.1002) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 82.2341, Epoch Time 730.8442(702.2410), Bit/dim 3.8047(best: 3.8033), Xent 1.0073, Loss 4.3083, Error 0.3572(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2170 | Time 24.1330(23.2266) | Bit/dim 3.7942(3.8053) | Xent 1.0237(1.0169) | Loss 10.0988(11.1467) | Error 0.3539(0.3623) Steps 592(585.31) | Grad Norm 8.6135(11.5240) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 24.0833(23.1832) | Bit/dim 3.7965(3.8046) | Xent 1.0077(1.0131) | Loss 10.2653(10.8866) | Error 0.3517(0.3608) Steps 598(585.93) | Grad Norm 6.6638(11.7233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 84.0080, Epoch Time 727.0767(702.9861), Bit/dim 3.8041(best: 3.8033), Xent 0.9871, Loss 4.2977, Error 0.3478(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2190 | Time 22.0610(23.2231) | Bit/dim 3.8258(3.8055) | Xent 1.0157(1.0133) | Loss 10.2417(11.3727) | Error 0.3600(0.3604) Steps 592(586.88) | Grad Norm 19.2105(12.2856) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 24.1343(23.2794) | Bit/dim 3.7785(3.8044) | Xent 0.9549(1.0133) | Loss 10.1162(11.0541) | Error 0.3306(0.3592) Steps 592(587.09) | Grad Norm 4.1132(12.1753) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 23.3974(23.3749) | Bit/dim 3.7821(3.8028) | Xent 1.0299(1.0075) | Loss 10.1455(10.8180) | Error 0.3583(0.3563) Steps 604(587.93) | Grad Norm 15.7761(12.0140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 83.3142, Epoch Time 734.1026(703.9196), Bit/dim 3.7937(best: 3.8033), Xent 1.0128, Loss 4.3001, Error 0.3580(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2230 | Time 22.3351(23.3768) | Bit/dim 3.8300(3.7996) | Xent 0.9612(1.0043) | Loss 10.0961(10.9457) | Error 0.3500(0.3558) Steps 574(587.53) | Grad Norm 12.5687(11.8175) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 24.9397(23.3940) | Bit/dim 3.7825(3.7979) | Xent 1.0569(1.0010) | Loss 10.2312(10.7203) | Error 0.3789(0.3550) Steps 640(589.45) | Grad Norm 19.9870(11.6891) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 80.6810, Epoch Time 728.7538(704.6646), Bit/dim 3.8034(best: 3.7937), Xent 0.9833, Loss 4.2951, Error 0.3482(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2250 | Time 22.1160(23.3961) | Bit/dim 3.7802(3.7975) | Xent 1.0495(0.9999) | Loss 10.0852(11.1304) | Error 0.3717(0.3547) Steps 598(588.72) | Grad Norm 25.1711(12.6206) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 23.4401(23.3787) | Bit/dim 3.8105(3.7994) | Xent 1.1090(1.0207) | Loss 10.4307(10.9055) | Error 0.3867(0.3621) Steps 616(591.47) | Grad Norm 11.1609(13.7184) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 83.9759, Epoch Time 734.6872(705.5653), Bit/dim 3.8127(best: 3.7937), Xent 1.0562, Loss 4.3408, Error 0.3799(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2270 | Time 23.3461(23.4234) | Bit/dim 3.7916(3.8029) | Xent 0.9681(1.0408) | Loss 10.1494(11.4517) | Error 0.3361(0.3693) Steps 586(589.01) | Grad Norm 7.2087(14.6348) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 23.2735(23.3079) | Bit/dim 3.8029(3.8025) | Xent 0.9863(1.0305) | Loss 10.1572(11.1105) | Error 0.3578(0.3660) Steps 562(588.74) | Grad Norm 9.9523(13.4930) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 22.8025(23.2280) | Bit/dim 3.7570(3.7990) | Xent 1.0050(1.0182) | Loss 10.0716(10.8379) | Error 0.3544(0.3611) Steps 586(586.52) | Grad Norm 8.1215(11.8680) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 80.3802, Epoch Time 716.8813(705.9048), Bit/dim 3.7994(best: 3.7937), Xent 0.9576, Loss 4.2782, Error 0.3325(best: 0.3440)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2300 | Time 23.3643(23.2171) | Bit/dim 3.7885(3.7968) | Xent 1.0244(1.0076) | Loss 10.2131(11.2631) | Error 0.3689(0.3574) Steps 556(583.61) | Grad Norm 11.0548(11.2064) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 23.1930(23.2112) | Bit/dim 3.7774(3.7927) | Xent 0.9454(0.9941) | Loss 10.0561(10.9498) | Error 0.3339(0.3533) Steps 574(581.83) | Grad Norm 6.5590(10.8977) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 1800 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs1800_sratio_0_5_drop_0_5_rl_stdscale_6_run2 --seed 2 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
