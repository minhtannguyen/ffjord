{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=3600, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 27.2426(51.7013) | Bit/dim 8.8051(8.9212) | Xent 2.2775(2.2998) | Loss 21.7621(22.2959) | Error 0.7583(0.8664) Steps 496(499.89) | Grad Norm 22.1319(29.2520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 71.6590, Epoch Time 496.6592(496.6592), Bit/dim 8.6534(best: inf), Xent 2.2557, Loss 9.7813, Error 0.7508(best: inf)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0020 | Time 28.7072(45.8208) | Bit/dim 8.5424(8.8437) | Xent 2.2206(2.2859) | Loss 21.0277(22.6259) | Error 0.7447(0.8378) Steps 478(498.07) | Grad Norm 8.5057(25.2293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 68.8802, Epoch Time 463.8840(495.6759), Bit/dim 8.4044(best: 8.6534), Xent 2.1773, Loss 9.4930, Error 0.7378(best: 0.7508)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0030 | Time 29.2391(41.4427) | Bit/dim 8.3341(8.7352) | Xent 2.1764(2.2605) | Loss 20.5569(22.8167) | Error 0.7592(0.8141) Steps 484(494.73) | Grad Norm 7.1734(20.5145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 66.9798, Epoch Time 458.4627(494.5595), Bit/dim 8.1645(best: 8.4044), Xent 2.1135, Loss 9.2213, Error 0.7156(best: 0.7378)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0040 | Time 30.1401(38.1961) | Bit/dim 8.1865(8.6093) | Xent 2.1177(2.2287) | Loss 42.0313(22.8816) | Error 0.7289(0.7935) Steps 526(496.44) | Grad Norm 5.1248(16.7721) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 28.6472(35.8005) | Bit/dim 7.9441(8.4605) | Xent 2.0864(2.1945) | Loss 19.7199(22.1599) | Error 0.7067(0.7729) Steps 478(495.87) | Grad Norm 5.1128(13.7951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 67.6305, Epoch Time 464.4757(493.6570), Bit/dim 7.8499(best: 8.1645), Xent 2.0683, Loss 8.8840, Error 0.6953(best: 0.7156)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0060 | Time 33.3166(34.2828) | Bit/dim 7.6553(8.2810) | Xent 2.0605(2.1627) | Loss 19.4255(22.0399) | Error 0.6875(0.7539) Steps 514(500.24) | Grad Norm 5.2858(11.5139) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 68.8334, Epoch Time 474.9259(493.0951), Bit/dim 7.4726(best: 7.8499), Xent 2.0448, Loss 8.4950, Error 0.6744(best: 0.6953)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0070 | Time 29.2690(33.1530) | Bit/dim 7.3709(8.0716) | Xent 2.0425(2.1331) | Loss 18.6854(21.8124) | Error 0.6714(0.7357) Steps 532(503.75) | Grad Norm 4.1200(9.6684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 71.6567, Epoch Time 479.8228(492.6969), Bit/dim 7.1941(best: 7.4726), Xent 2.0465, Loss 8.2174, Error 0.6677(best: 0.6744)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0080 | Time 29.4156(32.5071) | Bit/dim 7.1864(7.8564) | Xent 2.0474(2.1113) | Loss 18.3463(21.6058) | Error 0.6767(0.7208) Steps 526(505.98) | Grad Norm 2.7458(8.0207) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 30.2550(32.1343) | Bit/dim 7.0725(7.6618) | Xent 2.0551(2.0977) | Loss 17.7838(20.7067) | Error 0.6956(0.7122) Steps 532(510.94) | Grad Norm 1.8664(6.4895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 71.1918, Epoch Time 489.5358(492.6021), Bit/dim 7.0689(best: 7.1941), Xent 2.0537, Loss 8.0957, Error 0.6818(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0100 | Time 31.8641(32.0011) | Bit/dim 7.0295(7.5001) | Xent 2.0628(2.0878) | Loss 18.1230(20.5669) | Error 0.7125(0.7086) Steps 550(517.51) | Grad Norm 1.8171(5.2255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 71.5796, Epoch Time 502.4217(492.8967), Bit/dim 7.0105(best: 7.0689), Xent 2.0456, Loss 8.0333, Error 0.6855(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0110 | Time 30.5313(31.9732) | Bit/dim 6.9893(7.3703) | Xent 2.0584(2.0784) | Loss 17.8570(20.4785) | Error 0.6939(0.7050) Steps 526(523.13) | Grad Norm 1.3747(4.3091) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 72.4154, Epoch Time 513.8064(493.5240), Bit/dim 6.9661(best: 7.0105), Xent 2.0270, Loss 7.9796, Error 0.6739(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0120 | Time 34.0771(32.5617) | Bit/dim 6.9706(7.2663) | Xent 2.0472(2.0687) | Loss 18.0302(20.4791) | Error 0.6867(0.7009) Steps 538(526.31) | Grad Norm 1.1636(3.8328) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 35.2282(33.2496) | Bit/dim 6.9172(7.1782) | Xent 2.0185(2.0577) | Loss 17.9069(19.7981) | Error 0.6783(0.6962) Steps 550(528.86) | Grad Norm 2.3784(3.5763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 73.0020, Epoch Time 545.0887(495.0709), Bit/dim 6.9140(best: 6.9661), Xent 2.0130, Loss 7.9205, Error 0.6724(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0140 | Time 34.1860(33.6203) | Bit/dim 6.8679(7.1021) | Xent 2.0047(2.0468) | Loss 17.7026(19.7457) | Error 0.6733(0.6915) Steps 550(533.33) | Grad Norm 2.5458(3.8088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 73.6078, Epoch Time 540.1386(496.4229), Bit/dim 6.8423(best: 6.9140), Xent 2.0056, Loss 7.8451, Error 0.6794(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0150 | Time 34.4575(33.8970) | Bit/dim 6.7966(7.0296) | Xent 1.9923(2.0374) | Loss 17.5252(19.7501) | Error 0.6658(0.6878) Steps 526(535.76) | Grad Norm 10.7368(5.1677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 74.2786, Epoch Time 543.3952(497.8321), Bit/dim 6.7278(best: 6.8423), Xent 1.9898, Loss 7.7227, Error 0.6756(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0160 | Time 36.6904(34.1779) | Bit/dim 6.7007(6.9538) | Xent 2.0467(2.0297) | Loss 17.4866(19.8057) | Error 0.7258(0.6864) Steps 574(540.45) | Grad Norm 47.8234(8.1059) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 74.3277, Epoch Time 544.6416(499.2364), Bit/dim 6.5771(best: 6.7278), Xent 1.9953, Loss 7.5747, Error 0.6769(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0170 | Time 35.6729(34.4218) | Bit/dim 6.5776(6.8705) | Xent 2.0009(2.0301) | Loss 40.4946(19.8452) | Error 0.6847(0.6919) Steps 556(546.89) | Grad Norm 36.6833(16.6281) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 33.6869(34.5234) | Bit/dim 6.3893(6.7678) | Xent 2.0016(2.0211) | Loss 16.7931(19.0648) | Error 0.6950(0.6897) Steps 562(545.36) | Grad Norm 36.3865(18.6010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 71.7957, Epoch Time 537.4019(500.3814), Bit/dim 6.3891(best: 6.5771), Xent 2.0920, Loss 7.4351, Error 0.7493(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0190 | Time 37.4676(34.3525) | Bit/dim 6.2914(6.6609) | Xent 2.2273(2.0551) | Loss 16.6307(19.0134) | Error 0.8114(0.7090) Steps 532(545.48) | Grad Norm 93.2352(39.3530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 73.7532, Epoch Time 534.2540(501.3975), Bit/dim 6.1933(best: 6.3891), Xent 2.0249, Loss 7.2057, Error 0.6739(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0200 | Time 31.2936(34.1301) | Bit/dim 6.1057(6.5389) | Xent 2.0385(2.0491) | Loss 16.1662(18.9182) | Error 0.6878(0.7069) Steps 550(545.24) | Grad Norm 48.0350(39.2548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 75.2660, Epoch Time 519.3909(501.9373), Bit/dim 5.9611(best: 6.1933), Xent 1.9955, Loss 6.9589, Error 0.6667(best: 0.6677)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0210 | Time 32.9832(33.9153) | Bit/dim 5.9405(6.4011) | Xent 1.9992(2.0421) | Loss 15.5111(18.7945) | Error 0.6881(0.7043) Steps 544(546.46) | Grad Norm 28.3415(38.0824) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 35.6904(34.0444) | Bit/dim 5.9728(6.3067) | Xent 2.0480(2.0680) | Loss 15.9391(18.0837) | Error 0.6933(0.7141) Steps 568(549.24) | Grad Norm 26.4125(59.0768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 74.0281, Epoch Time 535.7483(502.9517), Bit/dim 5.9973(best: 5.9611), Xent 2.0285, Loss 7.0116, Error 0.6794(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0230 | Time 33.1885(33.7922) | Bit/dim 5.8372(6.1994) | Xent 2.0787(2.0685) | Loss 15.5174(17.9937) | Error 0.7211(0.7174) Steps 502(546.32) | Grad Norm 22.9345(51.5989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 73.1975, Epoch Time 514.5186(503.2987), Bit/dim 5.7894(best: 5.9611), Xent 2.0330, Loss 6.8059, Error 0.6844(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0240 | Time 31.3791(33.3226) | Bit/dim 5.7584(6.0919) | Xent 2.0069(2.0601) | Loss 15.3381(17.9045) | Error 0.6814(0.7113) Steps 538(544.34) | Grad Norm 23.4009(43.1939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 72.7126, Epoch Time 498.1416(503.1440), Bit/dim 5.7147(best: 5.7894), Xent 1.9940, Loss 6.7118, Error 0.6752(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0250 | Time 33.6194(32.9385) | Bit/dim 5.7060(5.9959) | Xent 1.9743(2.0453) | Loss 15.1556(17.8639) | Error 0.6594(0.7032) Steps 532(543.70) | Grad Norm 13.2693(35.0182) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 32.2357(32.7372) | Bit/dim 5.6579(5.9138) | Xent 1.9474(2.0264) | Loss 15.1568(17.1538) | Error 0.6528(0.6929) Steps 556(545.88) | Grad Norm 9.2074(28.7490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 72.2008, Epoch Time 509.3202(503.3292), Bit/dim 5.6660(best: 5.7147), Xent 1.9516, Loss 6.6418, Error 0.6437(best: 0.6667)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0270 | Time 32.3432(32.5861) | Bit/dim 5.6497(5.8456) | Xent 1.9371(2.0058) | Loss 15.1101(17.1012) | Error 0.6539(0.6829) Steps 550(544.61) | Grad Norm 11.7883(23.6029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 73.1130, Epoch Time 510.8232(503.5541), Bit/dim 5.6290(best: 5.6660), Xent 1.9253, Loss 6.5917, Error 0.6364(best: 0.6437)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0280 | Time 32.7989(32.6570) | Bit/dim 5.6113(5.7875) | Xent 1.9286(1.9885) | Loss 14.9791(17.1144) | Error 0.6422(0.6761) Steps 580(543.15) | Grad Norm 10.8075(21.9734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 73.2759, Epoch Time 512.3115(503.8168), Bit/dim 5.6096(best: 5.6290), Xent 1.9695, Loss 6.5944, Error 0.7030(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0290 | Time 32.1750(32.5279) | Bit/dim 5.6284(5.7453) | Xent 2.0029(1.9888) | Loss 14.9390(17.1816) | Error 0.7058(0.6793) Steps 520(540.16) | Grad Norm 61.2657(28.6162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 73.4471, Epoch Time 504.5731(503.8395), Bit/dim 5.5743(best: 5.6096), Xent 1.9583, Loss 6.5535, Error 0.6578(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0300 | Time 30.8914(32.3652) | Bit/dim 5.5673(5.7067) | Xent 1.9638(1.9868) | Loss 38.3873(17.3100) | Error 0.6700(0.6823) Steps 532(540.28) | Grad Norm 10.3395(30.2533) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 31.1028(32.2519) | Bit/dim 5.5791(5.6747) | Xent 2.0021(2.0046) | Loss 14.7330(16.6948) | Error 0.6908(0.6923) Steps 502(541.01) | Grad Norm 39.6451(42.6657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 74.3195, Epoch Time 505.6542(503.8939), Bit/dim 5.5474(best: 5.5743), Xent 1.9614, Loss 6.5281, Error 0.6530(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0320 | Time 34.5109(32.3210) | Bit/dim 5.5574(5.6401) | Xent 1.9663(1.9968) | Loss 14.8695(16.7521) | Error 0.6558(0.6873) Steps 550(538.22) | Grad Norm 21.3338(37.5742) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 73.1757, Epoch Time 510.1973(504.0830), Bit/dim 5.5135(best: 5.5474), Xent 1.9385, Loss 6.4828, Error 0.6402(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0330 | Time 31.9810(32.2636) | Bit/dim 5.4978(5.6052) | Xent 1.9265(1.9840) | Loss 14.6814(16.8574) | Error 0.6489(0.6794) Steps 550(541.59) | Grad Norm 14.8735(30.9443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 73.8382, Epoch Time 502.7492(504.0430), Bit/dim 5.4677(best: 5.5135), Xent 1.9034, Loss 6.4194, Error 0.6290(best: 0.6364)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0340 | Time 33.9417(32.2099) | Bit/dim 5.4552(5.5723) | Xent 1.9110(1.9690) | Loss 14.5430(16.9873) | Error 0.6533(0.6730) Steps 538(542.35) | Grad Norm 8.8196(25.3267) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 34.2263(32.2501) | Bit/dim 5.4372(5.5388) | Xent 1.8855(1.9511) | Loss 14.5592(16.3492) | Error 0.6461(0.6665) Steps 550(542.84) | Grad Norm 8.3137(20.5240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 74.7147, Epoch Time 509.9703(504.2208), Bit/dim 5.4183(best: 5.4677), Xent 1.8787, Loss 6.3577, Error 0.6301(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0360 | Time 32.6903(32.4597) | Bit/dim 5.3834(5.5039) | Xent 1.9281(1.9364) | Loss 14.4630(16.4054) | Error 0.6694(0.6624) Steps 586(545.16) | Grad Norm 24.0897(19.5277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 74.7358, Epoch Time 522.7453(504.7766), Bit/dim 5.3780(best: 5.4183), Xent 1.9337, Loss 6.3449, Error 0.6884(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0370 | Time 32.2710(32.6559) | Bit/dim 5.4033(5.4711) | Xent 2.0096(1.9373) | Loss 14.8017(16.5207) | Error 0.7031(0.6661) Steps 568(547.63) | Grad Norm 45.2489(24.7288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 76.3955, Epoch Time 526.6949(505.4341), Bit/dim 5.3267(best: 5.3780), Xent 1.9251, Loss 6.2892, Error 0.6643(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0380 | Time 34.3306(33.0078) | Bit/dim 5.3174(5.4342) | Xent 1.9628(1.9399) | Loss 14.4552(16.6569) | Error 0.6772(0.6699) Steps 586(552.22) | Grad Norm 76.6961(28.8987) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 33.9765(33.2266) | Bit/dim 5.3256(5.4055) | Xent 1.8731(1.9303) | Loss 14.1726(16.0393) | Error 0.6433(0.6670) Steps 508(552.91) | Grad Norm 27.5192(32.2525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 74.5129, Epoch Time 532.6941(506.2519), Bit/dim 5.2963(best: 5.3267), Xent 1.8719, Loss 6.2322, Error 0.6305(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0400 | Time 33.3948(33.5022) | Bit/dim 5.2154(5.3693) | Xent 1.8778(1.9186) | Loss 14.0972(16.0946) | Error 0.6583(0.6627) Steps 580(556.42) | Grad Norm 18.9521(29.1803) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 74.0089, Epoch Time 539.2892(507.2430), Bit/dim 5.2273(best: 5.2963), Xent 1.8583, Loss 6.1564, Error 0.6277(best: 0.6290)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0410 | Time 33.3703(33.9005) | Bit/dim 5.2197(5.3354) | Xent 1.8883(1.9069) | Loss 14.0303(16.1693) | Error 0.6486(0.6586) Steps 568(559.94) | Grad Norm 10.0275(25.7785) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 72.8083, Epoch Time 537.3941(508.1476), Bit/dim 5.1901(best: 5.2273), Xent 1.8497, Loss 6.1149, Error 0.6231(best: 0.6277)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0420 | Time 33.6547(34.0379) | Bit/dim 5.2343(5.3012) | Xent 1.8918(1.8956) | Loss 14.1995(16.2393) | Error 0.6536(0.6547) Steps 574(558.45) | Grad Norm 41.5977(24.3625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 74.1674, Epoch Time 538.4800(509.0575), Bit/dim 5.1635(best: 5.1901), Xent 1.8589, Loss 6.0930, Error 0.6431(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0430 | Time 35.6206(34.2006) | Bit/dim 5.1386(5.2654) | Xent 1.8698(1.8874) | Loss 37.1524(16.3356) | Error 0.6531(0.6527) Steps 520(557.57) | Grad Norm 38.2129(25.4557) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 32.4807(34.2459) | Bit/dim 5.4469(5.2491) | Xent 2.2093(1.9133) | Loss 14.8471(15.7502) | Error 0.8006(0.6638) Steps 556(555.61) | Grad Norm 85.4048(33.8205) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 75.8729, Epoch Time 534.3033(509.8149), Bit/dim 5.1870(best: 5.1635), Xent 2.0461, Loss 6.2100, Error 0.7254(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0450 | Time 36.4698(34.1233) | Bit/dim 5.1909(5.2570) | Xent 2.0301(1.9610) | Loss 14.2682(15.9824) | Error 0.7325(0.6828) Steps 574(559.10) | Grad Norm 12.2163(32.9375) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 73.9106, Epoch Time 540.9739(510.7497), Bit/dim 5.1498(best: 5.1635), Xent 1.9539, Loss 6.1268, Error 0.6645(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0460 | Time 35.7500(34.4219) | Bit/dim 5.1275(5.2308) | Xent 1.9581(1.9694) | Loss 13.9316(16.1163) | Error 0.6922(0.6868) Steps 514(558.73) | Grad Norm 14.3033(27.9549) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 73.5670, Epoch Time 552.3936(511.9990), Bit/dim 5.0754(best: 5.1498), Xent 1.8945, Loss 6.0227, Error 0.6477(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0470 | Time 35.1920(34.8218) | Bit/dim 5.0886(5.1972) | Xent 1.9200(1.9575) | Loss 13.7461(16.1743) | Error 0.6622(0.6821) Steps 532(555.54) | Grad Norm 7.7701(23.1433) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 37.7916(35.0534) | Bit/dim 5.0427(5.1594) | Xent 1.8787(1.9383) | Loss 13.6901(15.5217) | Error 0.6556(0.6742) Steps 562(551.94) | Grad Norm 5.1650(18.4446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 75.5866, Epoch Time 554.7393(513.2812), Bit/dim 5.0318(best: 5.0754), Xent 1.8482, Loss 5.9559, Error 0.6270(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0490 | Time 38.4546(35.7696) | Bit/dim 5.0300(5.1239) | Xent 1.8840(1.9180) | Loss 13.8031(15.6026) | Error 0.6497(0.6661) Steps 532(553.55) | Grad Norm 31.1150(15.4497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 75.5887, Epoch Time 573.8211(515.0974), Bit/dim 5.0668(best: 5.0318), Xent 2.0196, Loss 6.0766, Error 0.7315(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0500 | Time 36.0263(35.9983) | Bit/dim 4.9990(5.1071) | Xent 1.9319(1.9579) | Loss 13.5597(15.7565) | Error 0.6742(0.6791) Steps 538(553.26) | Grad Norm 7.0278(27.2630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 76.7272, Epoch Time 583.1850(517.1400), Bit/dim 4.9767(best: 5.0318), Xent 1.9224, Loss 5.9379, Error 0.6674(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0510 | Time 39.2228(36.6409) | Bit/dim 4.9822(5.0770) | Xent 1.9424(1.9567) | Loss 13.6904(15.8557) | Error 0.6736(0.6805) Steps 598(557.94) | Grad Norm 7.3443(24.1613) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 36.6121(37.1694) | Bit/dim 4.9311(5.0425) | Xent 1.9280(1.9487) | Loss 13.5590(15.2492) | Error 0.6778(0.6776) Steps 580(563.45) | Grad Norm 6.3279(20.0172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 77.2315, Epoch Time 597.8127(519.5602), Bit/dim 4.9227(best: 4.9767), Xent 1.8825, Loss 5.8639, Error 0.6547(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0530 | Time 38.2642(37.7651) | Bit/dim 4.8900(5.0077) | Xent 1.8776(1.9338) | Loss 13.4784(15.3233) | Error 0.6544(0.6738) Steps 550(562.80) | Grad Norm 9.6112(17.1346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 78.8499, Epoch Time 604.9933(522.1232), Bit/dim 4.8925(best: 4.9227), Xent 1.8271, Loss 5.8060, Error 0.6271(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0540 | Time 38.7879(38.1144) | Bit/dim 4.8812(4.9754) | Xent 1.8365(1.9116) | Loss 13.3894(15.4158) | Error 0.6447(0.6669) Steps 574(565.86) | Grad Norm 25.1436(16.6391) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 77.5279, Epoch Time 600.9388(524.4877), Bit/dim 4.8494(best: 4.8925), Xent 1.7846, Loss 5.7417, Error 0.6113(best: 0.6231)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0550 | Time 39.7121(38.1773) | Bit/dim 4.8242(4.9455) | Xent 1.7772(1.8868) | Loss 13.0224(15.4829) | Error 0.6264(0.6586) Steps 568(568.39) | Grad Norm 5.4559(16.4440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 78.8181, Epoch Time 588.0461(526.3944), Bit/dim 5.0560(best: 4.8494), Xent 1.8833, Loss 5.9976, Error 0.6697(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0560 | Time 37.7109(38.1217) | Bit/dim 5.0537(4.9873) | Xent 1.8804(1.9037) | Loss 39.0678(15.8357) | Error 0.6617(0.6639) Steps 604(570.59) | Grad Norm 19.2082(27.6571) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 41.6976(38.3624) | Bit/dim 4.9686(4.9957) | Xent 1.9210(1.9099) | Loss 13.6754(15.2856) | Error 0.6706(0.6674) Steps 544(567.17) | Grad Norm 13.2504(25.3961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 75.8773, Epoch Time 594.9308(528.4505), Bit/dim 4.9171(best: 4.8494), Xent 1.8760, Loss 5.8551, Error 0.6512(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0580 | Time 39.8691(38.3594) | Bit/dim 4.8695(4.9703) | Xent 1.9079(1.9074) | Loss 13.4655(15.3516) | Error 0.6792(0.6670) Steps 598(566.03) | Grad Norm 13.5502(21.5635) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 75.5011, Epoch Time 587.4118(530.2193), Bit/dim 4.8277(best: 4.8494), Xent 1.8366, Loss 5.7461, Error 0.6363(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0590 | Time 38.0773(38.3276) | Bit/dim 4.8029(4.9342) | Xent 1.8447(1.8976) | Loss 13.1236(15.4107) | Error 0.6419(0.6639) Steps 568(564.13) | Grad Norm 5.5896(17.5134) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 76.5661, Epoch Time 588.4126(531.9651), Bit/dim 4.7839(best: 4.8277), Xent 1.8007, Loss 5.6843, Error 0.6291(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0600 | Time 39.0648(38.3423) | Bit/dim 4.7708(4.8968) | Xent 1.8406(1.8824) | Loss 12.9936(15.4785) | Error 0.6503(0.6593) Steps 550(560.03) | Grad Norm 7.5366(15.4142) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 37.5809(38.5854) | Bit/dim 4.7490(4.8659) | Xent 1.8047(1.8722) | Loss 13.0435(14.8524) | Error 0.6286(0.6558) Steps 574(563.25) | Grad Norm 9.6632(17.5106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 76.9275, Epoch Time 606.2415(534.1934), Bit/dim 4.8069(best: 4.7839), Xent 1.8466, Loss 5.7302, Error 0.6466(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0620 | Time 38.8497(38.7654) | Bit/dim 4.7438(4.8478) | Xent 1.7997(1.8624) | Loss 12.9523(14.9503) | Error 0.6206(0.6520) Steps 550(562.98) | Grad Norm 13.2848(21.5460) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 77.7144, Epoch Time 601.5945(536.2155), Bit/dim 4.8164(best: 4.7839), Xent 1.7739, Loss 5.7034, Error 0.6207(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0630 | Time 38.3176(38.7348) | Bit/dim 4.7239(4.8285) | Xent 1.7811(1.8445) | Loss 12.8637(15.0508) | Error 0.6272(0.6461) Steps 574(565.62) | Grad Norm 14.9647(21.2408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 78.7595, Epoch Time 600.5283(538.1449), Bit/dim 4.7166(best: 4.7839), Xent 1.7265, Loss 5.5799, Error 0.5996(best: 0.6113)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0640 | Time 39.6424(38.9759) | Bit/dim 4.6912(4.8019) | Xent 1.7552(1.8253) | Loss 12.7916(15.1769) | Error 0.6111(0.6397) Steps 574(569.48) | Grad Norm 8.2857(20.1723) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 41.4801(39.2420) | Bit/dim 4.7192(4.7780) | Xent 1.7403(1.8044) | Loss 12.6946(14.5712) | Error 0.6336(0.6336) Steps 550(572.15) | Grad Norm 18.0599(18.5072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 79.8249, Epoch Time 615.3866(540.4621), Bit/dim 4.6928(best: 4.7166), Xent 1.6896, Loss 5.5376, Error 0.5845(best: 0.5996)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0660 | Time 39.2212(39.5996) | Bit/dim 4.7801(4.7664) | Xent 1.7670(1.8033) | Loss 13.1471(14.7343) | Error 0.6047(0.6337) Steps 592(579.46) | Grad Norm 33.8666(24.4663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 79.4472, Epoch Time 620.3672(542.8593), Bit/dim 4.7308(best: 4.6928), Xent 1.7476, Loss 5.6046, Error 0.6064(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0670 | Time 40.2201(39.6770) | Bit/dim 4.6881(4.7531) | Xent 1.7604(1.8000) | Loss 12.8873(14.8919) | Error 0.6161(0.6332) Steps 568(582.99) | Grad Norm 10.5155(24.4715) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 79.9140, Epoch Time 617.5274(545.0993), Bit/dim 4.6738(best: 4.6928), Xent 1.7021, Loss 5.5248, Error 0.5968(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0680 | Time 41.9699(39.8887) | Bit/dim 4.6736(4.7324) | Xent 1.7287(1.7881) | Loss 12.7123(15.0247) | Error 0.6192(0.6295) Steps 628(588.03) | Grad Norm 8.9682(21.9576) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 80.6092, Epoch Time 621.2837(547.3848), Bit/dim 4.6399(best: 4.6738), Xent 1.6562, Loss 5.4680, Error 0.5840(best: 0.5845)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0690 | Time 42.2017(40.0740) | Bit/dim 4.6338(4.7113) | Xent 1.6971(1.7685) | Loss 36.8417(15.1555) | Error 0.5950(0.6227) Steps 616(594.58) | Grad Norm 13.6920(20.0919) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 39.3673(40.3932) | Bit/dim 4.7063(4.7027) | Xent 1.9273(1.7754) | Loss 13.0885(14.5704) | Error 0.6725(0.6258) Steps 592(594.18) | Grad Norm 82.1988(27.7504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 80.5972, Epoch Time 632.7182(549.9448), Bit/dim 4.6460(best: 4.6399), Xent 1.7549, Loss 5.5234, Error 0.6250(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0710 | Time 38.7453(40.4409) | Bit/dim 4.6359(4.6893) | Xent 1.7912(1.7773) | Loss 12.7370(14.7089) | Error 0.6394(0.6279) Steps 592(593.96) | Grad Norm 29.1324(26.4708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 78.6955, Epoch Time 616.7053(551.9476), Bit/dim 4.6221(best: 4.6399), Xent 1.6964, Loss 5.4703, Error 0.5949(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0720 | Time 40.9025(40.3062) | Bit/dim 4.6178(4.6725) | Xent 1.7343(1.7653) | Loss 12.6995(14.8087) | Error 0.6253(0.6251) Steps 586(592.04) | Grad Norm 19.7503(24.7892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 79.3771, Epoch Time 619.5175(553.9747), Bit/dim 4.5986(best: 4.6221), Xent 1.6248, Loss 5.4110, Error 0.5772(best: 0.5840)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0730 | Time 41.2319(40.2562) | Bit/dim 4.5704(4.6552) | Xent 1.6655(1.7473) | Loss 12.5550(14.9284) | Error 0.5914(0.6190) Steps 586(591.03) | Grad Norm 24.3481(25.1754) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 42.5807(40.4015) | Bit/dim 4.5872(4.6438) | Xent 1.6577(1.7371) | Loss 12.3599(14.3177) | Error 0.6011(0.6165) Steps 562(588.25) | Grad Norm 5.1423(26.0316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 79.8947, Epoch Time 621.9646(556.0144), Bit/dim 4.6358(best: 4.5986), Xent 1.6725, Loss 5.4720, Error 0.5848(best: 0.5772)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0750 | Time 40.6407(40.5366) | Bit/dim 4.6102(4.6376) | Xent 1.7290(1.7388) | Loss 12.6231(14.4509) | Error 0.6206(0.6175) Steps 592(589.70) | Grad Norm 34.7996(29.9656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 80.2786, Epoch Time 630.0148(558.2345), Bit/dim 4.5814(best: 4.5986), Xent 1.6241, Loss 5.3935, Error 0.5745(best: 0.5772)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0760 | Time 39.6867(40.6702) | Bit/dim 4.5645(4.6217) | Xent 1.6851(1.7264) | Loss 12.6424(14.5893) | Error 0.5972(0.6131) Steps 634(590.33) | Grad Norm 20.1152(27.2567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 80.5820, Epoch Time 620.7488(560.1099), Bit/dim 4.5483(best: 4.5814), Xent 1.6130, Loss 5.3548, Error 0.5735(best: 0.5745)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0770 | Time 42.2866(40.5664) | Bit/dim 4.5496(4.6030) | Xent 1.6656(1.7110) | Loss 12.7164(14.7460) | Error 0.6017(0.6079) Steps 622(594.17) | Grad Norm 23.1231(24.7177) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 40.2695(40.6088) | Bit/dim 4.5543(4.5879) | Xent 1.6613(1.6920) | Loss 12.4828(14.1616) | Error 0.6044(0.6023) Steps 574(597.88) | Grad Norm 39.3329(23.5489) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 81.3241, Epoch Time 626.8365(562.1117), Bit/dim 4.5353(best: 4.5483), Xent 1.6528, Loss 5.3617, Error 0.5946(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0790 | Time 39.4304(40.7397) | Bit/dim 4.5163(4.5726) | Xent 1.6837(1.6928) | Loss 12.4568(14.3005) | Error 0.6022(0.6028) Steps 622(600.51) | Grad Norm 24.3479(27.1677) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 82.8344, Epoch Time 634.8395(564.2935), Bit/dim 4.5432(best: 4.5353), Xent 1.6451, Loss 5.3657, Error 0.5916(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0800 | Time 41.0617(40.8452) | Bit/dim 4.5164(4.5638) | Xent 1.6493(1.6969) | Loss 12.3498(14.4622) | Error 0.5825(0.6047) Steps 616(602.20) | Grad Norm 6.8612(28.2251) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 81.2626, Epoch Time 631.6558(566.3144), Bit/dim 4.4978(best: 4.5353), Xent 1.5806, Loss 5.2881, Error 0.5658(best: 0.5735)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0810 | Time 39.5316(40.8084) | Bit/dim 4.4891(4.5470) | Xent 1.5984(1.6800) | Loss 12.1575(14.5978) | Error 0.5736(0.5997) Steps 586(604.20) | Grad Norm 6.5636(24.1271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 83.6346, Epoch Time 635.1222(568.3786), Bit/dim 4.6588(best: 4.4978), Xent 1.6515, Loss 5.4845, Error 0.5898(best: 0.5658)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0820 | Time 41.6616(41.0880) | Bit/dim 4.6575(4.5403) | Xent 1.7163(1.6683) | Loss 38.3972(14.8037) | Error 0.5989(0.5949) Steps 610(606.91) | Grad Norm 68.9778(23.8049) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 40.9529(41.1916) | Bit/dim 4.5754(4.5612) | Xent 1.6629(1.6766) | Loss 12.5975(14.2428) | Error 0.5969(0.5974) Steps 586(604.09) | Grad Norm 14.2301(25.5237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 81.5708, Epoch Time 636.4593(570.4210), Bit/dim 4.5360(best: 4.4978), Xent 1.5711, Loss 5.3215, Error 0.5617(best: 0.5658)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0840 | Time 41.0413(41.3632) | Bit/dim 4.4898(4.5501) | Xent 1.5869(1.6628) | Loss 12.2420(14.3530) | Error 0.5578(0.5929) Steps 556(600.95) | Grad Norm 10.2151(22.2487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 79.9973, Epoch Time 642.5546(572.5850), Bit/dim 4.4725(best: 4.4978), Xent 1.5458, Loss 5.2454, Error 0.5520(best: 0.5617)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0850 | Time 40.4247(41.5529) | Bit/dim 4.4659(4.5319) | Xent 1.6024(1.6471) | Loss 12.1663(14.4450) | Error 0.5769(0.5890) Steps 568(601.54) | Grad Norm 19.8602(21.3513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 80.9160, Epoch Time 642.6750(574.6877), Bit/dim 4.4404(best: 4.4725), Xent 1.5322, Loss 5.2065, Error 0.5511(best: 0.5520)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0860 | Time 39.2184(41.6049) | Bit/dim 4.4424(4.5106) | Xent 1.6021(1.6331) | Loss 12.0914(14.5575) | Error 0.5742(0.5846) Steps 604(600.64) | Grad Norm 17.1552(19.9302) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 40.3712(41.4885) | Bit/dim 4.4386(4.4891) | Xent 1.5987(1.6216) | Loss 12.1083(13.9226) | Error 0.5678(0.5806) Steps 580(604.31) | Grad Norm 25.7767(19.8402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 83.2989, Epoch Time 632.3648(576.4181), Bit/dim 4.4455(best: 4.4404), Xent 1.5983, Loss 5.2447, Error 0.5728(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0880 | Time 44.3882(41.8069) | Bit/dim 4.4579(4.4859) | Xent 1.6236(1.6412) | Loss 12.2758(14.1271) | Error 0.5894(0.5862) Steps 628(610.62) | Grad Norm 18.6882(24.8363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 82.1536, Epoch Time 649.0545(578.5971), Bit/dim 4.4224(best: 4.4404), Xent 1.5748, Loss 5.2098, Error 0.5664(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0890 | Time 41.2950(41.7386) | Bit/dim 4.4152(4.4707) | Xent 1.5902(1.6419) | Loss 12.1452(14.2607) | Error 0.5667(0.5880) Steps 610(612.19) | Grad Norm 14.1262(23.1698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 81.4169, Epoch Time 634.7068(580.2804), Bit/dim 4.4070(best: 4.4224), Xent 1.5221, Loss 5.1680, Error 0.5380(best: 0.5511)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0900 | Time 42.1525(41.6360) | Bit/dim 4.3866(4.4524) | Xent 1.5492(1.6247) | Loss 12.1773(14.3528) | Error 0.5547(0.5827) Steps 634(612.56) | Grad Norm 14.5351(19.9922) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 41.9381(41.6427) | Bit/dim 4.3804(4.4404) | Xent 1.5567(1.6149) | Loss 12.0979(13.7681) | Error 0.5658(0.5799) Steps 598(613.12) | Grad Norm 13.4255(19.8723) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 83.7943, Epoch Time 640.6680(582.0921), Bit/dim 4.3872(best: 4.4070), Xent 1.5137, Loss 5.1441, Error 0.5477(best: 0.5380)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0920 | Time 41.7000(41.5268) | Bit/dim 4.3579(4.4231) | Xent 1.5265(1.5947) | Loss 11.9450(13.8650) | Error 0.5425(0.5733) Steps 634(614.15) | Grad Norm 12.3846(17.7586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 83.5486, Epoch Time 634.0506(583.6508), Bit/dim 4.3953(best: 4.3872), Xent 1.5043, Loss 5.1474, Error 0.5371(best: 0.5380)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0930 | Time 41.3464(41.4080) | Bit/dim 4.3839(4.4102) | Xent 1.5197(1.5844) | Loss 11.7870(13.9963) | Error 0.5497(0.5696) Steps 586(615.56) | Grad Norm 23.4183(19.0825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 82.3633, Epoch Time 638.2231(585.2880), Bit/dim 4.3554(best: 4.3872), Xent 1.4816, Loss 5.0962, Error 0.5366(best: 0.5371)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0940 | Time 42.4748(41.6353) | Bit/dim 4.3573(4.3977) | Xent 1.5475(1.5767) | Loss 12.1837(14.1728) | Error 0.5689(0.5681) Steps 634(618.46) | Grad Norm 12.4455(18.7476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 85.2817, Epoch Time 655.1072(587.3826), Bit/dim 4.3853(best: 4.3554), Xent 1.6224, Loss 5.1965, Error 0.5800(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0950 | Time 43.6800(41.9514) | Bit/dim 4.3854(4.3839) | Xent 1.6766(1.5712) | Loss 37.3144(14.3404) | Error 0.5989(0.5649) Steps 616(621.33) | Grad Norm 66.4723(20.3557) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 41.1588(41.9739) | Bit/dim 4.3716(4.3893) | Xent 1.6096(1.5987) | Loss 11.9812(13.7734) | Error 0.5894(0.5747) Steps 634(619.61) | Grad Norm 11.2457(22.7345) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 82.4476, Epoch Time 646.3633(589.1520), Bit/dim 4.3576(best: 4.3554), Xent 1.5240, Loss 5.1196, Error 0.5561(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0970 | Time 44.0778(42.5491) | Bit/dim 4.3222(4.3775) | Xent 1.5567(1.5904) | Loss 11.9250(13.8691) | Error 0.5453(0.5721) Steps 610(619.02) | Grad Norm 6.6359(19.7463) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 81.9179, Epoch Time 670.9081(591.6047), Bit/dim 4.3077(best: 4.3554), Xent 1.4746, Loss 5.0450, Error 0.5248(best: 0.5366)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0980 | Time 41.9793(42.5597) | Bit/dim 4.3234(4.3610) | Xent 1.5010(1.5738) | Loss 11.8145(14.0137) | Error 0.5475(0.5676) Steps 628(620.35) | Grad Norm 14.6794(17.0183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 83.1331, Epoch Time 649.8468(593.3519), Bit/dim 4.2859(best: 4.3077), Xent 1.4464, Loss 5.0091, Error 0.5165(best: 0.5248)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 0990 | Time 44.0010(42.6661) | Bit/dim 4.2948(4.3447) | Xent 1.5010(1.5581) | Loss 11.7613(14.1536) | Error 0.5458(0.5626) Steps 586(618.85) | Grad Norm 12.2108(15.7216) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 41.2119(42.7768) | Bit/dim 4.2863(4.3345) | Xent 1.5148(1.5463) | Loss 11.8047(13.5421) | Error 0.5558(0.5595) Steps 622(620.51) | Grad Norm 9.8928(16.9174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 82.4146, Epoch Time 658.5777(595.3087), Bit/dim 4.3117(best: 4.2859), Xent 1.4577, Loss 5.0406, Error 0.5155(best: 0.5165)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1010 | Time 46.5970(43.1846) | Bit/dim 4.2563(4.3210) | Xent 1.4796(1.5337) | Loss 11.6861(13.6649) | Error 0.5436(0.5547) Steps 622(619.22) | Grad Norm 11.5878(16.7171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 82.2773, Epoch Time 666.4644(597.4434), Bit/dim 4.2652(best: 4.2859), Xent 1.4242, Loss 4.9773, Error 0.5119(best: 0.5155)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1020 | Time 41.8305(43.1382) | Bit/dim 4.2686(4.3071) | Xent 1.5886(1.5234) | Loss 11.8339(13.8094) | Error 0.5708(0.5506) Steps 610(618.20) | Grad Norm 34.3969(17.6437) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 81.7749, Epoch Time 660.4794(599.3345), Bit/dim 4.7411(best: 4.2652), Xent 1.8490, Loss 5.6656, Error 0.6711(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1030 | Time 46.5336(43.1452) | Bit/dim 4.4154(4.3523) | Xent 1.8869(1.5745) | Loss 12.3033(14.1475) | Error 0.6586(0.5672) Steps 598(617.35) | Grad Norm 21.0652(23.0144) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 42.9180(43.1740) | Bit/dim 4.3812(4.3706) | Xent 1.6615(1.6029) | Loss 12.0619(13.6384) | Error 0.6086(0.5795) Steps 640(618.97) | Grad Norm 8.6910(20.9075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 80.8869, Epoch Time 658.3328(601.1044), Bit/dim 4.3542(best: 4.2652), Xent 1.5560, Loss 5.1322, Error 0.5649(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1050 | Time 42.4300(42.9397) | Bit/dim 4.2908(4.3558) | Xent 1.5730(1.5976) | Loss 11.8355(13.7415) | Error 0.5761(0.5778) Steps 640(616.44) | Grad Norm 6.0570(17.3820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 77.2084, Epoch Time 640.6425(602.2906), Bit/dim 4.2860(best: 4.2652), Xent 1.4703, Loss 5.0212, Error 0.5303(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1060 | Time 44.8207(43.0945) | Bit/dim 4.2496(4.3337) | Xent 1.5052(1.5786) | Loss 11.7107(13.8147) | Error 0.5500(0.5716) Steps 592(614.52) | Grad Norm 6.4432(14.8640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 80.5943, Epoch Time 664.0946(604.1447), Bit/dim 4.2465(best: 4.2652), Xent 1.4342, Loss 4.9636, Error 0.5137(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1070 | Time 42.3528(43.0437) | Bit/dim 4.2455(4.3114) | Xent 1.4850(1.5573) | Loss 11.6753(13.9368) | Error 0.5289(0.5632) Steps 628(614.76) | Grad Norm 5.5824(12.6892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 81.8649, Epoch Time 666.0382(606.0015), Bit/dim 4.2203(best: 4.2465), Xent 1.3988, Loss 4.9197, Error 0.5080(best: 0.5119)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1080 | Time 43.4745(43.3277) | Bit/dim 4.2160(4.2883) | Xent 1.4726(1.5318) | Loss 36.2509(14.0770) | Error 0.5369(0.5545) Steps 610(615.09) | Grad Norm 9.1253(10.5777) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 45.8513(43.5101) | Bit/dim 4.2140(4.2682) | Xent 1.4551(1.5074) | Loss 11.6863(13.4208) | Error 0.5292(0.5462) Steps 628(617.80) | Grad Norm 4.4939(9.8514) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 84.4642, Epoch Time 671.6502(607.9709), Bit/dim 4.2055(best: 4.2203), Xent 1.3709, Loss 4.8909, Error 0.4974(best: 0.5080)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1100 | Time 42.5050(43.5508) | Bit/dim 4.2036(4.2518) | Xent 1.4453(1.4881) | Loss 11.3246(13.5224) | Error 0.5211(0.5391) Steps 592(619.24) | Grad Norm 22.7595(9.8550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 81.5834, Epoch Time 662.8725(609.6180), Bit/dim 4.1904(best: 4.2055), Xent 1.3519, Loss 4.8663, Error 0.4886(best: 0.4974)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1110 | Time 46.9030(43.6154) | Bit/dim 4.1821(4.2371) | Xent 1.4267(1.4676) | Loss 11.5344(13.6615) | Error 0.5325(0.5316) Steps 652(620.58) | Grad Norm 16.7766(10.3160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 83.1715, Epoch Time 659.9234(611.1272), Bit/dim 4.1789(best: 4.1904), Xent 1.3523, Loss 4.8551, Error 0.4895(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1120 | Time 44.0798(43.3923) | Bit/dim 4.1853(4.2221) | Xent 1.4111(1.4563) | Loss 11.5802(13.8100) | Error 0.5108(0.5274) Steps 634(623.85) | Grad Norm 11.4940(11.0060) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 45.8714(43.4213) | Bit/dim 4.1827(4.2150) | Xent 1.4169(1.4579) | Loss 11.5972(13.2058) | Error 0.5072(0.5267) Steps 640(623.58) | Grad Norm 10.3652(12.9543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 83.7054, Epoch Time 663.4734(612.6975), Bit/dim 4.1822(best: 4.1789), Xent 1.3766, Loss 4.8704, Error 0.5001(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1140 | Time 44.4860(43.3348) | Bit/dim 4.1651(4.2034) | Xent 1.4018(1.4468) | Loss 11.4998(13.3477) | Error 0.5056(0.5230) Steps 628(625.79) | Grad Norm 7.6105(12.3861) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 81.5484, Epoch Time 663.3087(614.2159), Bit/dim 4.1632(best: 4.1789), Xent 1.3240, Loss 4.8252, Error 0.4764(best: 0.4886)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1150 | Time 43.6495(43.2553) | Bit/dim 4.1864(4.1931) | Xent 1.3857(1.4313) | Loss 11.5339(13.5043) | Error 0.5053(0.5181) Steps 634(626.29) | Grad Norm 12.8756(11.9731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 82.7489, Epoch Time 655.0064(615.4396), Bit/dim 4.1588(best: 4.1632), Xent 1.3570, Loss 4.8373, Error 0.4912(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1160 | Time 42.7958(43.1942) | Bit/dim 4.1815(4.1874) | Xent 1.3779(1.4244) | Loss 11.4797(13.6821) | Error 0.5056(0.5157) Steps 634(628.19) | Grad Norm 16.5661(14.1036) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 40.6042(42.9532) | Bit/dim 4.1332(4.1773) | Xent 1.3771(1.4189) | Loss 11.2435(13.0878) | Error 0.4958(0.5139) Steps 604(628.38) | Grad Norm 8.6728(14.6582) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 82.5466, Epoch Time 649.2600(616.4542), Bit/dim 4.1488(best: 4.1588), Xent 1.3420, Loss 4.8198, Error 0.4807(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1180 | Time 41.9710(42.8824) | Bit/dim 4.1571(4.1691) | Xent 1.4095(1.4106) | Loss 11.2913(13.2088) | Error 0.4944(0.5115) Steps 622(626.57) | Grad Norm 23.1013(15.0181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 81.8793, Epoch Time 652.2901(617.5293), Bit/dim 4.1496(best: 4.1488), Xent 1.3753, Loss 4.8373, Error 0.5034(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1190 | Time 41.3263(42.6112) | Bit/dim 4.1126(4.1605) | Xent 1.3718(1.4065) | Loss 11.2866(13.3687) | Error 0.5025(0.5101) Steps 652(626.62) | Grad Norm 11.1123(15.5360) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 81.8931, Epoch Time 648.6265(618.4622), Bit/dim 4.1297(best: 4.1488), Xent 1.3019, Loss 4.7807, Error 0.4732(best: 0.4764)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1200 | Time 41.9128(42.7324) | Bit/dim 4.1113(4.1525) | Xent 1.3392(1.3932) | Loss 11.3491(13.5101) | Error 0.4739(0.5050) Steps 634(626.97) | Grad Norm 5.0813(13.4218) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 81.3472, Epoch Time 644.6041(619.2465), Bit/dim 4.1143(best: 4.1297), Xent 1.2798, Loss 4.7542, Error 0.4633(best: 0.4732)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1210 | Time 41.5777(42.5002) | Bit/dim 4.1077(4.1423) | Xent 1.3536(1.3799) | Loss 35.7452(13.6380) | Error 0.4814(0.4996) Steps 574(625.34) | Grad Norm 8.0266(11.8620) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 42.1216(42.4193) | Bit/dim 4.1442(4.1391) | Xent 1.4655(1.3867) | Loss 11.5378(13.0359) | Error 0.5347(0.5016) Steps 628(623.75) | Grad Norm 29.6375(14.9737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 82.7445, Epoch Time 649.1945(620.1449), Bit/dim 4.1324(best: 4.1143), Xent 1.3360, Loss 4.8004, Error 0.4773(best: 0.4633)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n",
      "Iter 1230 | Time 45.3036(42.5880) | Bit/dim 4.1147(4.1373) | Xent 1.3196(1.3820) | Loss 11.2995(13.1773) | Error 0.4761(0.5000) Steps 640(624.57) | Grad Norm 8.4955(14.7175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 82.0604, Epoch Time 653.6310(621.1495), Bit/dim 4.1042(best: 4.1143), Xent 1.2901, Loss 4.7492, Error 0.4672(best: 0.4633)\n",
      "===> Using batch size 3600. Total 13 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 3600 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs3600_sratio_0_5_drop_0_5_rl_stdscale_6_run2 --seed 2 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
