{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=5400, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "validating...\n",
      "Epoch 0001 | Time 181.5506, Epoch Time 583.8395(583.8395), Bit/dim 8.7691(best: inf), Xent 2.2765, Loss 9.9073, Error 0.7563(best: inf)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0010 | Time 39.8689(76.0315) | Bit/dim 8.8040(8.9267) | Xent 2.2778(2.2998) | Loss 43.5412(22.8565) | Error 0.7650(0.8670) Steps 478(508.91) | Grad Norm 22.0670(29.0925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 180.8828, Epoch Time 528.8241(582.1891), Bit/dim 8.5359(best: 8.7691), Xent 2.2237, Loss 9.6477, Error 0.7398(best: 0.7563)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0020 | Time 33.9943(65.3640) | Bit/dim 8.5154(8.8469) | Xent 2.2188(2.2855) | Loss 21.0976(23.1646) | Error 0.7419(0.8379) Steps 502(504.41) | Grad Norm 8.6528(25.1310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 179.5365, Epoch Time 526.0483(580.5048), Bit/dim 8.3871(best: 8.5359), Xent 2.1700, Loss 9.4721, Error 0.7376(best: 0.7398)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0030 | Time 36.4616(57.8717) | Bit/dim 8.3471(8.7384) | Xent 2.1670(2.2598) | Loss 20.5633(23.2338) | Error 0.7483(0.8143) Steps 514(503.01) | Grad Norm 7.2673(20.4087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 175.9757, Epoch Time 517.2177(578.6062), Bit/dim 8.2221(best: 8.3871), Xent 2.1252, Loss 9.2847, Error 0.7211(best: 0.7376)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0040 | Time 38.7960(52.2167) | Bit/dim 8.1776(8.6132) | Xent 2.1210(2.2278) | Loss 20.4819(23.1487) | Error 0.7276(0.7929) Steps 502(504.59) | Grad Norm 5.1446(16.6946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 180.9153, Epoch Time 529.4320(577.1310), Bit/dim 8.0336(best: 8.2221), Xent 2.0869, Loss 9.0771, Error 0.7097(best: 0.7211)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0050 | Time 37.2413(48.2708) | Bit/dim 7.9468(8.4626) | Xent 2.0852(2.1927) | Loss 19.8388(22.9361) | Error 0.7044(0.7724) Steps 490(501.85) | Grad Norm 5.0488(13.7165) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 182.3196, Epoch Time 537.8505(575.9526), Bit/dim 7.7925(best: 8.0336), Xent 2.0606, Loss 8.8228, Error 0.6899(best: 0.7097)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0060 | Time 39.4934(45.4985) | Bit/dim 7.6472(8.2821) | Xent 2.0527(2.1596) | Loss 19.2055(22.6222) | Error 0.6907(0.7526) Steps 496(501.26) | Grad Norm 5.0044(11.4452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 182.6287, Epoch Time 542.0583(574.9357), Bit/dim 7.5284(best: 7.7925), Xent 2.0431, Loss 8.5499, Error 0.6727(best: 0.6899)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0070 | Time 39.3518(43.5358) | Bit/dim 7.3626(8.0727) | Xent 2.0550(2.1301) | Loss 18.6388(22.2287) | Error 0.6898(0.7339) Steps 526(502.49) | Grad Norm 3.8400(9.6154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 186.8519, Epoch Time 541.3742(573.9289), Bit/dim 7.2995(best: 7.5284), Xent 2.0393, Loss 8.3192, Error 0.6660(best: 0.6727)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0080 | Time 39.0109(42.1181) | Bit/dim 7.1718(7.8565) | Xent 2.0584(2.1087) | Loss 18.4885(21.8075) | Error 0.6843(0.7190) Steps 502(506.11) | Grad Norm 2.8177(7.9626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 189.6086, Epoch Time 549.2218(573.1877), Bit/dim 7.1539(best: 7.2995), Xent 2.0448, Loss 8.1763, Error 0.6670(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0090 | Time 42.4885(41.2264) | Bit/dim 7.0752(7.6622) | Xent 2.0624(2.0946) | Loss 18.2408(21.4068) | Error 0.6987(0.7099) Steps 502(510.15) | Grad Norm 1.7348(6.4213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 189.8991, Epoch Time 554.9107(572.6394), Bit/dim 7.0750(best: 7.1539), Xent 2.0498, Loss 8.0999, Error 0.6771(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0011 | Time 190.0746, Epoch Time 560.4686(572.2743), Bit/dim 7.0294(best: 7.0750), Xent 2.0465, Loss 8.0526, Error 0.6786(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0100 | Time 37.1832(40.6792) | Bit/dim 7.0325(7.5009) | Xent 2.0458(2.0842) | Loss 40.8475(21.7504) | Error 0.7015(0.7062) Steps 526(517.56) | Grad Norm 1.9562(5.1395) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 189.7228, Epoch Time 564.3853(572.0376), Bit/dim 6.9967(best: 7.0294), Xent 2.0360, Loss 8.0147, Error 0.6789(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0110 | Time 39.6294(40.4455) | Bit/dim 7.0024(7.3706) | Xent 2.0406(2.0750) | Loss 17.9926(21.4297) | Error 0.6954(0.7031) Steps 538(522.44) | Grad Norm 1.0366(4.1595) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 192.1885, Epoch Time 576.6219(572.1751), Bit/dim 6.9660(best: 6.9967), Xent 2.0243, Loss 7.9782, Error 0.6779(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0120 | Time 41.1066(40.7414) | Bit/dim 6.9607(7.2657) | Xent 2.0153(2.0642) | Loss 17.8859(21.1585) | Error 0.6837(0.6991) Steps 568(526.79) | Grad Norm 3.6699(3.6296) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 192.7055, Epoch Time 581.7874(572.4635), Bit/dim 6.9319(best: 6.9660), Xent 2.0122, Loss 7.9380, Error 0.6690(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0130 | Time 42.2962(40.9618) | Bit/dim 6.9140(7.1786) | Xent 2.0112(2.0539) | Loss 17.7827(20.9272) | Error 0.6833(0.6951) Steps 532(531.98) | Grad Norm 4.4703(3.3500) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 194.0519, Epoch Time 587.2136(572.9060), Bit/dim 6.8899(best: 6.9319), Xent 2.0063, Loss 7.8930, Error 0.6671(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0140 | Time 42.8345(41.2149) | Bit/dim 6.8717(7.1023) | Xent 2.0129(2.0439) | Loss 17.6837(20.6935) | Error 0.6691(0.6910) Steps 562(536.40) | Grad Norm 3.6074(4.1774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 192.5969, Epoch Time 584.2262(573.2456), Bit/dim 6.8359(best: 6.8899), Xent 1.9934, Loss 7.8326, Error 0.6641(best: 0.6660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0150 | Time 40.8169(41.2657) | Bit/dim 6.7935(7.0304) | Xent 2.0051(2.0345) | Loss 17.4919(20.4754) | Error 0.6889(0.6880) Steps 550(539.15) | Grad Norm 16.6884(5.8317) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 195.6468, Epoch Time 585.6289(573.6171), Bit/dim 6.7662(best: 6.8359), Xent 2.0031, Loss 7.7677, Error 0.6760(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0160 | Time 42.4546(41.4790) | Bit/dim 6.7040(6.9549) | Xent 1.9914(2.0259) | Loss 17.2148(20.2341) | Error 0.6650(0.6857) Steps 562(544.29) | Grad Norm 6.7811(8.2297) | Total Time 0.00(0.00)\n",
      "Epoch 0018 | Time 198.6373, Epoch Time 593.6147(574.2170), Bit/dim 6.6621(best: 6.7662), Xent 1.9791, Loss 7.6517, Error 0.6685(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0170 | Time 43.7235(41.6784) | Bit/dim 6.5966(6.8707) | Xent 2.0727(2.0289) | Loss 17.0971(20.0003) | Error 0.7354(0.6909) Steps 574(546.99) | Grad Norm 73.5578(17.6871) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 195.9540, Epoch Time 592.3833(574.7620), Bit/dim 6.5510(best: 6.6621), Xent 1.9997, Loss 7.5508, Error 0.6881(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0180 | Time 41.5754(41.6861) | Bit/dim 6.4044(6.7703) | Xent 2.0017(2.0216) | Loss 16.6522(19.7322) | Error 0.6961(0.6904) Steps 532(547.50) | Grad Norm 36.4034(22.5304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 194.5937, Epoch Time 586.6819(575.1196), Bit/dim 6.3841(best: 6.5510), Xent 1.9735, Loss 7.3709, Error 0.6727(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0021 | Time 198.0510, Epoch Time 598.5698(575.8231), Bit/dim 6.2315(best: 6.3841), Xent 1.9921, Loss 7.2276, Error 0.6659(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0190 | Time 41.9577(41.9171) | Bit/dim 6.2347(6.6526) | Xent 1.9947(2.0477) | Loss 39.9184(20.1632) | Error 0.6746(0.7023) Steps 538(548.62) | Grad Norm 43.7432(34.8687) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 197.1576, Epoch Time 593.0099(576.3387), Bit/dim 6.1457(best: 6.2315), Xent 2.0115, Loss 7.1514, Error 0.6823(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0200 | Time 42.4136(41.9151) | Bit/dim 6.0719(6.5223) | Xent 2.0053(2.0437) | Loss 15.9064(19.7883) | Error 0.6774(0.7025) Steps 526(549.91) | Grad Norm 33.4763(35.3484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 200.8171, Epoch Time 588.2330(576.6955), Bit/dim 6.2698(best: 6.1457), Xent 2.1059, Loss 7.3228, Error 0.7508(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0210 | Time 44.5540(42.0074) | Bit/dim 6.1732(6.4496) | Xent 2.0526(2.1185) | Loss 16.3573(19.6387) | Error 0.6952(0.7219) Steps 592(552.00) | Grad Norm 30.0972(64.2487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 196.5776, Epoch Time 600.4316(577.4076), Bit/dim 6.0488(best: 6.1457), Xent 2.0816, Loss 7.0896, Error 0.7082(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0220 | Time 42.4118(42.0487) | Bit/dim 6.0201(6.3532) | Xent 2.0993(2.1110) | Loss 15.9940(19.3378) | Error 0.7239(0.7257) Steps 556(554.58) | Grad Norm 17.2493(53.6810) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 191.8100, Epoch Time 578.2151(577.4319), Bit/dim 5.9179(best: 6.0488), Xent 2.0681, Loss 6.9519, Error 0.7110(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0230 | Time 39.8570(41.6703) | Bit/dim 5.8421(6.2383) | Xent 2.0590(2.1005) | Loss 15.4728(18.9936) | Error 0.6985(0.7224) Steps 526(550.58) | Grad Norm 7.1300(42.5507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 192.7291, Epoch Time 574.4744(577.3431), Bit/dim 5.8181(best: 5.9179), Xent 2.0255, Loss 6.8308, Error 0.6731(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0240 | Time 38.8788(41.3102) | Bit/dim 5.7709(6.1264) | Xent 2.0232(2.0824) | Loss 15.3849(18.6565) | Error 0.6876(0.7129) Steps 544(550.56) | Grad Norm 13.3448(34.2219) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 195.0829, Epoch Time 570.7017(577.1439), Bit/dim 5.7600(best: 5.8181), Xent 1.9977, Loss 6.7588, Error 0.6673(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0250 | Time 40.0872(40.8948) | Bit/dim 5.7057(6.0276) | Xent 1.9972(2.0613) | Loss 15.2418(18.3750) | Error 0.6769(0.7036) Steps 538(549.50) | Grad Norm 12.3610(27.8631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 197.7459, Epoch Time 573.0710(577.0217), Bit/dim 5.7137(best: 5.7600), Xent 1.9767, Loss 6.7020, Error 0.6653(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0260 | Time 40.1986(40.8632) | Bit/dim 5.7018(5.9424) | Xent 1.9625(2.0397) | Loss 15.2200(18.0942) | Error 0.6683(0.6945) Steps 580(548.60) | Grad Norm 5.5133(22.9045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 194.1477, Epoch Time 576.8196(577.0156), Bit/dim 5.6822(best: 5.7137), Xent 1.9571, Loss 6.6607, Error 0.6526(best: 0.6641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0270 | Time 40.9286(40.5635) | Bit/dim 5.7139(5.8797) | Xent 2.1769(2.0466) | Loss 15.4354(17.8744) | Error 0.7928(0.6993) Steps 526(545.01) | Grad Norm 115.1699(33.5535) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 196.2363, Epoch Time 570.2869(576.8138), Bit/dim 5.7193(best: 5.6822), Xent 2.0523, Loss 6.7454, Error 0.7240(best: 0.6526)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0031 | Time 195.9270, Epoch Time 563.7083(576.4206), Bit/dim 5.6495(best: 5.6822), Xent 1.9954, Loss 6.6472, Error 0.6701(best: 0.6526)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0280 | Time 36.7474(40.1137) | Bit/dim 5.6582(5.8285) | Xent 2.0070(2.0408) | Loss 37.6435(18.3721) | Error 0.6833(0.7019) Steps 520(542.57) | Grad Norm 12.9470(33.0777) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 195.8073, Epoch Time 561.3380(575.9681), Bit/dim 5.6218(best: 5.6495), Xent 1.9803, Loss 6.6119, Error 0.6678(best: 0.6526)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0290 | Time 39.3868(39.8405) | Bit/dim 5.6282(5.7776) | Xent 1.9931(2.0290) | Loss 15.0420(18.1739) | Error 0.6880(0.6965) Steps 508(538.37) | Grad Norm 13.7678(27.9191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 195.8273, Epoch Time 569.0276(575.7599), Bit/dim 5.5974(best: 5.6218), Xent 1.9673, Loss 6.5810, Error 0.6599(best: 0.6526)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0300 | Time 39.1989(39.8809) | Bit/dim 5.5833(5.7328) | Xent 1.9776(2.0170) | Loss 14.8227(17.9673) | Error 0.6609(0.6905) Steps 526(537.54) | Grad Norm 9.7761(23.4364) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 195.9919, Epoch Time 575.3014(575.7462), Bit/dim 5.5798(best: 5.5974), Xent 1.9509, Loss 6.5553, Error 0.6512(best: 0.6526)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0310 | Time 39.0931(39.8875) | Bit/dim 5.5845(5.6927) | Xent 1.9463(2.0026) | Loss 15.0266(17.7855) | Error 0.6719(0.6841) Steps 550(538.08) | Grad Norm 11.1194(19.8350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 196.5147, Epoch Time 572.7504(575.6563), Bit/dim 5.5593(best: 5.5798), Xent 1.9305, Loss 6.5246, Error 0.6433(best: 0.6512)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0320 | Time 40.5189(39.9910) | Bit/dim 5.5601(5.6575) | Xent 1.9522(1.9861) | Loss 14.9052(17.6149) | Error 0.6619(0.6768) Steps 544(539.86) | Grad Norm 14.1156(17.2792) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 196.2706, Epoch Time 570.3357(575.4967), Bit/dim 5.5407(best: 5.5593), Xent 1.9210, Loss 6.5012, Error 0.6436(best: 0.6433)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0330 | Time 39.1342(39.8419) | Bit/dim 5.5179(5.6268) | Xent 1.9398(1.9730) | Loss 14.6908(17.4797) | Error 0.6652(0.6716) Steps 550(539.09) | Grad Norm 36.0717(19.1556) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 196.9757, Epoch Time 571.3327(575.3718), Bit/dim 5.5173(best: 5.5407), Xent 1.9040, Loss 6.4693, Error 0.6336(best: 0.6433)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0340 | Time 41.0775(39.9881) | Bit/dim 5.5117(5.5968) | Xent 1.9069(1.9587) | Loss 14.6998(17.3245) | Error 0.6544(0.6671) Steps 556(541.45) | Grad Norm 31.9283(21.3887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 197.4406, Epoch Time 577.4971(575.4355), Bit/dim 5.4971(best: 5.5173), Xent 1.9117, Loss 6.4530, Error 0.6566(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0350 | Time 40.6864(40.2117) | Bit/dim 5.4827(5.5689) | Xent 1.9176(1.9513) | Loss 14.7917(17.2000) | Error 0.6463(0.6662) Steps 550(541.54) | Grad Norm 24.3939(27.9304) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 198.0603, Epoch Time 581.3975(575.6144), Bit/dim 5.4699(best: 5.4971), Xent 1.9047, Loss 6.4223, Error 0.6499(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0360 | Time 39.5302(40.4020) | Bit/dim 5.4513(5.5405) | Xent 1.9491(1.9414) | Loss 14.5421(17.0717) | Error 0.6794(0.6638) Steps 556(540.77) | Grad Norm 64.4561(28.4564) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 196.0809, Epoch Time 581.6649(575.7959), Bit/dim 5.4594(best: 5.4699), Xent 1.9234, Loss 6.4212, Error 0.6650(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0041 | Time 198.6172, Epoch Time 585.0625(576.0739), Bit/dim 5.4179(best: 5.4594), Xent 1.9285, Loss 6.3822, Error 0.6567(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0370 | Time 39.5932(40.5529) | Bit/dim 5.4098(5.5124) | Xent 1.9290(1.9398) | Loss 37.3168(17.6292) | Error 0.6667(0.6658) Steps 550(543.54) | Grad Norm 43.0895(31.8222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 199.0431, Epoch Time 580.9744(576.2209), Bit/dim 5.3787(best: 5.4179), Xent 1.9149, Loss 6.3361, Error 0.6670(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0380 | Time 40.5710(40.6602) | Bit/dim 5.3694(5.4799) | Xent 1.9046(1.9370) | Loss 14.4127(17.5016) | Error 0.6498(0.6664) Steps 556(546.68) | Grad Norm 28.0404(31.3154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 201.4689, Epoch Time 589.5810(576.6217), Bit/dim 5.3393(best: 5.3787), Xent 1.8655, Loss 6.2720, Error 0.6235(best: 0.6336)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0390 | Time 41.5516(40.8461) | Bit/dim 5.3412(5.4472) | Xent 1.8886(1.9247) | Loss 14.2931(17.3149) | Error 0.6483(0.6608) Steps 520(546.90) | Grad Norm 27.5642(28.8224) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 200.4151, Epoch Time 592.1573(577.0878), Bit/dim 5.3145(best: 5.3393), Xent 1.8543, Loss 6.2416, Error 0.6279(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0400 | Time 42.7301(41.1968) | Bit/dim 5.3141(5.4139) | Xent 1.8626(1.9097) | Loss 14.3646(17.1509) | Error 0.6431(0.6554) Steps 604(553.70) | Grad Norm 17.0145(25.9618) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 202.5775, Epoch Time 598.4160(577.7276), Bit/dim 5.2807(best: 5.3145), Xent 1.8765, Loss 6.2190, Error 0.6484(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0410 | Time 41.2405(41.3285) | Bit/dim 5.2894(5.3799) | Xent 1.9046(1.9037) | Loss 14.2940(17.0195) | Error 0.6676(0.6550) Steps 550(552.94) | Grad Norm 62.4540(31.4753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 201.6009, Epoch Time 593.6368(578.2049), Bit/dim 5.2462(best: 5.2807), Xent 1.8447, Loss 6.1686, Error 0.6280(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0420 | Time 41.3110(41.5407) | Bit/dim 5.2381(5.3469) | Xent 1.8601(1.8942) | Loss 14.0632(16.8614) | Error 0.6446(0.6532) Steps 556(552.92) | Grad Norm 16.9032(30.9782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 198.6998, Epoch Time 594.9593(578.7075), Bit/dim 5.2180(best: 5.2462), Xent 1.8493, Loss 6.1426, Error 0.6336(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0430 | Time 40.9974(41.4805) | Bit/dim 5.2236(5.3191) | Xent 1.9560(1.9114) | Loss 14.2094(16.7719) | Error 0.6930(0.6631) Steps 580(553.07) | Grad Norm 30.2780(34.8693) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 200.7873, Epoch Time 590.3452(579.0567), Bit/dim 5.2164(best: 5.2180), Xent 1.9056, Loss 6.1692, Error 0.6693(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0440 | Time 41.3484(41.6734) | Bit/dim 5.2074(5.2902) | Xent 1.8930(1.9125) | Loss 14.0848(16.6566) | Error 0.6524(0.6638) Steps 544(553.92) | Grad Norm 31.6439(34.0781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 199.2060, Epoch Time 591.3951(579.4268), Bit/dim 5.1895(best: 5.2164), Xent 1.8685, Loss 6.1237, Error 0.6438(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0450 | Time 42.3260(41.6151) | Bit/dim 5.1526(5.2592) | Xent 1.8921(1.9031) | Loss 13.9389(16.5215) | Error 0.6644(0.6602) Steps 520(551.35) | Grad Norm 34.7114(31.6907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 198.1181, Epoch Time 588.0676(579.6860), Bit/dim 5.1507(best: 5.1895), Xent 1.8661, Loss 6.0838, Error 0.6439(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0051 | Time 197.8554, Epoch Time 592.2558(580.0631), Bit/dim 5.1178(best: 5.1507), Xent 1.8479, Loss 6.0417, Error 0.6257(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0460 | Time 40.7305(41.6883) | Bit/dim 5.1240(5.2284) | Xent 1.8708(1.8933) | Loss 36.8587(17.0552) | Error 0.6450(0.6568) Steps 526(552.48) | Grad Norm 18.9760(30.1865) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 198.1500, Epoch Time 588.8974(580.3282), Bit/dim 5.0942(best: 5.1178), Xent 1.8365, Loss 6.0125, Error 0.6373(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0470 | Time 42.5206(41.6819) | Bit/dim 5.0923(5.1970) | Xent 1.8300(1.8802) | Loss 13.7167(16.8881) | Error 0.6317(0.6516) Steps 526(551.25) | Grad Norm 17.9042(28.1003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 199.5204, Epoch Time 604.4790(581.0527), Bit/dim 5.1365(best: 5.0942), Xent 2.0325, Loss 6.1527, Error 0.7049(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0480 | Time 42.4545(42.0147) | Bit/dim 5.1176(5.1704) | Xent 1.8874(1.8814) | Loss 13.8846(16.7484) | Error 0.6630(0.6533) Steps 580(555.04) | Grad Norm 43.8838(32.5906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 199.3649, Epoch Time 591.2281(581.3579), Bit/dim 5.0947(best: 5.0942), Xent 1.9726, Loss 6.0810, Error 0.7162(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0490 | Time 41.7796(41.9901) | Bit/dim 5.0466(5.1571) | Xent 1.9647(1.9119) | Loss 13.7754(16.6805) | Error 0.6952(0.6677) Steps 568(556.51) | Grad Norm 25.5038(37.9800) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 198.8565, Epoch Time 593.8985(581.7342), Bit/dim 5.0526(best: 5.0942), Xent 1.9214, Loss 6.0133, Error 0.6668(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0500 | Time 43.4622(42.1328) | Bit/dim 5.0126(5.1278) | Xent 1.9136(1.9176) | Loss 13.5901(16.5341) | Error 0.6765(0.6704) Steps 556(557.56) | Grad Norm 10.6749(31.8766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 201.1586, Epoch Time 603.9855(582.4017), Bit/dim 5.0020(best: 5.0526), Xent 1.8765, Loss 5.9403, Error 0.6418(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0510 | Time 41.5584(42.4025) | Bit/dim 4.9862(5.0965) | Xent 1.8821(1.9115) | Loss 13.5153(16.3598) | Error 0.6481(0.6667) Steps 550(555.23) | Grad Norm 8.6454(26.0569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 200.8856, Epoch Time 604.6883(583.0703), Bit/dim 4.9740(best: 5.0020), Xent 1.8384, Loss 5.8932, Error 0.6274(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0520 | Time 43.5045(42.6912) | Bit/dim 4.9451(5.0617) | Xent 1.8385(1.8951) | Loss 13.4621(16.1934) | Error 0.6346(0.6595) Steps 568(558.12) | Grad Norm 13.3321(21.8488) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 201.6955, Epoch Time 609.1826(583.8537), Bit/dim 4.9407(best: 4.9740), Xent 1.7906, Loss 5.8360, Error 0.6108(best: 0.6235)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0530 | Time 43.4471(43.0211) | Bit/dim 4.9691(5.0301) | Xent 1.9203(1.8766) | Loss 13.5768(16.0285) | Error 0.6815(0.6534) Steps 574(560.04) | Grad Norm 82.7092(21.9313) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 206.8203, Epoch Time 623.1002(585.0311), Bit/dim 5.5408(best: 4.9407), Xent 2.3698, Loss 6.7257, Error 0.7721(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0540 | Time 42.8680(43.1950) | Bit/dim 5.1194(5.0678) | Xent 1.9567(1.9213) | Loss 13.8919(16.1081) | Error 0.6831(0.6686) Steps 604(563.55) | Grad Norm 20.5970(30.8340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 210.5869, Epoch Time 617.3296(586.0000), Bit/dim 5.0489(best: 4.9407), Xent 1.9497, Loss 6.0238, Error 0.6763(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0061 | Time 206.9076, Epoch Time 632.6049(587.3982), Bit/dim 4.9737(best: 4.9407), Xent 1.8877, Loss 5.9175, Error 0.6542(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0550 | Time 42.0712(43.6927) | Bit/dim 4.9739(5.0503) | Xent 1.8931(1.9303) | Loss 36.4838(16.7176) | Error 0.6661(0.6747) Steps 538(566.84) | Grad Norm 13.6967(26.9396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 203.2212, Epoch Time 620.6567(588.3959), Bit/dim 4.9117(best: 4.9407), Xent 1.8610, Loss 5.8422, Error 0.6455(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0560 | Time 45.4406(43.9952) | Bit/dim 4.9023(5.0191) | Xent 1.8611(1.9212) | Loss 13.4256(16.5475) | Error 0.6470(0.6708) Steps 586(566.51) | Grad Norm 10.4001(22.2487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 202.2342, Epoch Time 623.5244(589.4498), Bit/dim 4.8677(best: 4.9117), Xent 1.8323, Loss 5.7839, Error 0.6369(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0570 | Time 45.0113(44.1952) | Bit/dim 4.8643(4.9828) | Xent 1.8673(1.9084) | Loss 13.3252(16.3547) | Error 0.6476(0.6666) Steps 544(563.40) | Grad Norm 4.9925(18.2495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 199.3496, Epoch Time 620.8779(590.3926), Bit/dim 4.8431(best: 4.8677), Xent 1.8183, Loss 5.7522, Error 0.6279(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0580 | Time 45.3082(44.3787) | Bit/dim 4.8370(4.9477) | Xent 1.8290(1.8913) | Loss 13.0631(16.1710) | Error 0.6413(0.6616) Steps 538(563.51) | Grad Norm 5.3491(16.9114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 205.1402, Epoch Time 627.4863(591.5054), Bit/dim 4.8109(best: 4.8431), Xent 1.7826, Loss 5.7022, Error 0.6237(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0590 | Time 46.5201(44.7638) | Bit/dim 4.8158(4.9175) | Xent 1.8330(1.8763) | Loss 13.2042(16.0225) | Error 0.6422(0.6574) Steps 586(566.47) | Grad Norm 20.0480(20.0358) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 207.0007, Epoch Time 640.6082(592.9785), Bit/dim 4.8005(best: 4.8109), Xent 1.7842, Loss 5.6926, Error 0.6154(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0600 | Time 45.9301(45.3046) | Bit/dim 4.7896(4.8873) | Xent 1.8184(1.8591) | Loss 13.1036(15.8906) | Error 0.6391(0.6523) Steps 586(571.29) | Grad Norm 27.9126(21.0288) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 207.8934, Epoch Time 643.4103(594.4915), Bit/dim 4.7759(best: 4.8005), Xent 1.7731, Loss 5.6624, Error 0.6218(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0610 | Time 47.7015(45.7705) | Bit/dim 4.7666(4.8597) | Xent 1.7915(1.8430) | Loss 13.1381(15.7697) | Error 0.6233(0.6471) Steps 586(574.73) | Grad Norm 19.5342(21.8855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 210.1134, Epoch Time 653.7947(596.2706), Bit/dim 4.7578(best: 4.7759), Xent 1.7300, Loss 5.6228, Error 0.6056(best: 0.6108)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0620 | Time 44.3993(46.1464) | Bit/dim 4.8700(4.8355) | Xent 1.9041(1.8260) | Loss 13.3515(15.6538) | Error 0.6713(0.6426) Steps 616(581.28) | Grad Norm 77.2079(22.3685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 214.3890, Epoch Time 657.0130(598.0928), Bit/dim 5.5396(best: 4.7578), Xent 2.2437, Loss 6.6615, Error 0.8092(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0630 | Time 47.8529(46.2616) | Bit/dim 5.0137(4.9423) | Xent 1.9091(1.8813) | Loss 13.7374(15.8932) | Error 0.6706(0.6613) Steps 580(587.63) | Grad Norm 8.8425(29.1810) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 212.9823, Epoch Time 647.2126(599.5664), Bit/dim 5.0658(best: 4.7578), Xent 1.9133, Loss 6.0225, Error 0.6726(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0071 | Time 205.4669, Epoch Time 630.3532(600.4900), Bit/dim 4.8939(best: 4.7578), Xent 1.8535, Loss 5.8206, Error 0.6530(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0640 | Time 48.1340(46.1284) | Bit/dim 4.9001(4.9517) | Xent 1.8906(1.8895) | Loss 36.8757(16.5375) | Error 0.6720(0.6662) Steps 568(584.35) | Grad Norm 7.7528(24.5499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 200.5076, Epoch Time 627.7601(601.3081), Bit/dim 4.7971(best: 4.7578), Xent 1.8474, Loss 5.7208, Error 0.6481(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0650 | Time 43.4983(45.7832) | Bit/dim 4.7900(4.9204) | Xent 1.8668(1.8854) | Loss 12.9946(16.3189) | Error 0.6480(0.6648) Steps 526(579.42) | Grad Norm 3.9985(20.0226) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 202.1982, Epoch Time 616.8848(601.7754), Bit/dim 4.7533(best: 4.7578), Xent 1.8011, Loss 5.6539, Error 0.6286(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0660 | Time 46.9613(45.5628) | Bit/dim 4.7435(4.8788) | Xent 1.8305(1.8742) | Loss 12.9981(16.0924) | Error 0.6461(0.6605) Steps 568(573.57) | Grad Norm 2.8982(15.9495) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 201.1059, Epoch Time 628.6958(602.5830), Bit/dim 4.7150(best: 4.7533), Xent 1.7674, Loss 5.5987, Error 0.6120(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0670 | Time 45.2866(45.4942) | Bit/dim 4.7151(4.8379) | Xent 1.7950(1.8550) | Loss 12.8410(15.8948) | Error 0.6341(0.6537) Steps 568(570.71) | Grad Norm 3.6466(12.6427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 204.0115, Epoch Time 627.3868(603.3272), Bit/dim 4.6916(best: 4.7150), Xent 1.7412, Loss 5.5622, Error 0.6097(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0680 | Time 47.0809(45.3873) | Bit/dim 4.6751(4.7998) | Xent 1.7669(1.8345) | Loss 12.7140(15.6989) | Error 0.6181(0.6459) Steps 556(569.79) | Grad Norm 6.2385(10.8860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 206.0291, Epoch Time 633.8079(604.2416), Bit/dim 4.6732(best: 4.6916), Xent 1.7139, Loss 5.5301, Error 0.5972(best: 0.6056)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0690 | Time 45.4794(45.6827) | Bit/dim 4.6657(4.7659) | Xent 1.7318(1.8111) | Loss 12.7421(15.5301) | Error 0.6085(0.6379) Steps 580(569.85) | Grad Norm 13.2958(10.2247) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 206.5519, Epoch Time 641.5600(605.3611), Bit/dim 4.7623(best: 4.6732), Xent 1.8098, Loss 5.6672, Error 0.6436(best: 0.5972)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0700 | Time 50.9175(45.9654) | Bit/dim 4.6671(4.7498) | Xent 1.9870(1.8221) | Loss 13.1008(15.4598) | Error 0.6996(0.6418) Steps 568(574.88) | Grad Norm 62.1782(19.3014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 206.7731, Epoch Time 644.8058(606.5445), Bit/dim 4.6526(best: 4.6732), Xent 1.7471, Loss 5.5262, Error 0.6116(best: 0.5972)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0710 | Time 48.0198(46.3360) | Bit/dim 4.6368(4.7250) | Xent 1.7659(1.8182) | Loss 12.6913(15.3566) | Error 0.6220(0.6415) Steps 568(576.68) | Grad Norm 5.7795(20.2840) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 209.1131, Epoch Time 651.6782(607.8985), Bit/dim 4.6342(best: 4.6526), Xent 1.7225, Loss 5.4954, Error 0.6098(best: 0.5972)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0720 | Time 46.0936(46.5305) | Bit/dim 4.6067(4.6991) | Xent 1.7331(1.8002) | Loss 12.7449(15.2473) | Error 0.6102(0.6358) Steps 610(580.78) | Grad Norm 7.9388(18.3721) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 207.3410, Epoch Time 646.5791(609.0589), Bit/dim 4.6183(best: 4.6342), Xent 1.6953, Loss 5.4659, Error 0.6027(best: 0.5972)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0081 | Time 212.3988, Epoch Time 655.3452(610.4475), Bit/dim 4.6223(best: 4.6183), Xent 1.6527, Loss 5.4487, Error 0.5860(best: 0.5972)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0730 | Time 47.7606(46.7692) | Bit/dim 4.6244(4.6774) | Xent 1.6979(1.7777) | Loss 37.0075(15.8673) | Error 0.5998(0.6284) Steps 556(582.24) | Grad Norm 23.0358(17.7408) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 212.7794, Epoch Time 655.3005(611.7931), Bit/dim 4.5880(best: 4.6183), Xent 1.6318, Loss 5.4039, Error 0.5729(best: 0.5860)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0740 | Time 47.1946(46.8680) | Bit/dim 4.5978(4.6581) | Xent 1.7045(1.7554) | Loss 12.6089(15.7198) | Error 0.6130(0.6215) Steps 634(587.97) | Grad Norm 25.4736(17.8289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 213.0700, Epoch Time 658.2160(613.1858), Bit/dim 4.5820(best: 4.5880), Xent 1.6289, Loss 5.3965, Error 0.5733(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0750 | Time 48.7442(47.1106) | Bit/dim 4.5752(4.6409) | Xent 1.6762(1.7379) | Loss 12.5478(15.6074) | Error 0.5885(0.6152) Steps 586(591.68) | Grad Norm 32.8570(20.4457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 211.2344, Epoch Time 656.6961(614.4911), Bit/dim 4.5645(best: 4.5820), Xent 1.6327, Loss 5.3809, Error 0.5739(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0760 | Time 47.1357(47.1703) | Bit/dim 4.5926(4.6226) | Xent 1.7308(1.7242) | Loss 12.6757(15.4772) | Error 0.6133(0.6112) Steps 556(593.23) | Grad Norm 44.0472(21.3939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 214.8842, Epoch Time 660.6469(615.8757), Bit/dim 4.5744(best: 4.5645), Xent 1.6830, Loss 5.4159, Error 0.6001(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0770 | Time 46.8158(47.5153) | Bit/dim 4.5714(4.6157) | Xent 1.7154(1.7360) | Loss 12.5979(15.4086) | Error 0.6031(0.6148) Steps 610(597.53) | Grad Norm 23.5620(26.5420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 215.2076, Epoch Time 665.3325(617.3595), Bit/dim 4.5472(best: 4.5645), Xent 1.6639, Loss 5.3791, Error 0.5822(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0780 | Time 45.8856(47.7830) | Bit/dim 4.5382(4.5990) | Xent 1.6559(1.7256) | Loss 12.4263(15.3020) | Error 0.5907(0.6120) Steps 598(600.90) | Grad Norm 14.5447(24.2173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 212.9156, Epoch Time 665.5631(618.8056), Bit/dim 4.5596(best: 4.5472), Xent 1.6267, Loss 5.3730, Error 0.5787(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0790 | Time 45.9701(48.0192) | Bit/dim 4.5447(4.5851) | Xent 1.6262(1.7092) | Loss 12.3260(15.1711) | Error 0.5765(0.6066) Steps 610(598.62) | Grad Norm 13.2948(23.4187) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 213.8104, Epoch Time 671.8420(620.3967), Bit/dim 4.5216(best: 4.5472), Xent 1.5906, Loss 5.3169, Error 0.5660(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0800 | Time 48.2181(47.9689) | Bit/dim 4.9812(4.5876) | Xent 2.1236(1.7093) | Loss 13.9259(15.0854) | Error 0.7359(0.6069) Steps 616(596.68) | Grad Norm 87.4656(25.6895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 220.0458, Epoch Time 664.7661(621.7277), Bit/dim 4.7386(best: 4.5216), Xent 1.8882, Loss 5.6827, Error 0.6561(best: 0.5660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0810 | Time 47.4264(48.1993) | Bit/dim 4.6664(4.6128) | Xent 1.7963(1.7485) | Loss 12.7936(15.1180) | Error 0.6337(0.6197) Steps 622(601.13) | Grad Norm 19.0398(27.3773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 207.7469, Epoch Time 664.7609(623.0187), Bit/dim 4.6600(best: 4.5216), Xent 1.7525, Loss 5.5362, Error 0.6215(best: 0.5660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0091 | Time 204.9523, Epoch Time 637.8598(623.4640), Bit/dim 4.5541(best: 4.5216), Xent 1.6601, Loss 5.3841, Error 0.5809(best: 0.5660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0820 | Time 44.5994(47.6240) | Bit/dim 4.5559(4.6062) | Xent 1.7002(1.7491) | Loss 35.8332(15.6712) | Error 0.5967(0.6198) Steps 616(594.15) | Grad Norm 4.4842(23.5530) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 203.7163, Epoch Time 631.7839(623.7136), Bit/dim 4.5147(best: 4.5216), Xent 1.6195, Loss 5.3244, Error 0.5664(best: 0.5660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0830 | Time 46.7834(47.1390) | Bit/dim 4.5105(4.5863) | Xent 1.6663(1.7320) | Loss 12.3119(15.4923) | Error 0.5887(0.6142) Steps 556(587.31) | Grad Norm 10.8811(19.6382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 209.5800, Epoch Time 634.2234(624.0289), Bit/dim 4.4971(best: 4.5147), Xent 1.6049, Loss 5.2995, Error 0.5641(best: 0.5660)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0840 | Time 48.1899(46.8365) | Bit/dim 4.4894(4.5634) | Xent 1.6206(1.7122) | Loss 12.3848(15.3492) | Error 0.5793(0.6078) Steps 598(586.80) | Grad Norm 12.9884(18.4348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 215.5012, Epoch Time 653.7991(624.9220), Bit/dim 4.4695(best: 4.4971), Xent 1.5683, Loss 5.2536, Error 0.5545(best: 0.5641)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0850 | Time 47.6309(46.7813) | Bit/dim 4.4753(4.5417) | Xent 1.6304(1.6967) | Loss 12.2765(15.2143) | Error 0.5763(0.6031) Steps 628(590.82) | Grad Norm 21.0287(20.0569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 217.1340, Epoch Time 652.3729(625.7455), Bit/dim 4.4872(best: 4.4695), Xent 1.5698, Loss 5.2721, Error 0.5612(best: 0.5545)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0860 | Time 49.7457(46.8893) | Bit/dim 4.4744(4.5244) | Xent 1.7040(1.6872) | Loss 12.4717(15.1264) | Error 0.5991(0.6003) Steps 610(593.75) | Grad Norm 39.9870(22.8687) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 217.4632, Epoch Time 660.6841(626.7937), Bit/dim 4.4417(best: 4.4695), Xent 1.5577, Loss 5.2205, Error 0.5539(best: 0.5545)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0870 | Time 46.9212(46.9662) | Bit/dim 4.4234(4.5033) | Xent 1.6024(1.6728) | Loss 12.2858(15.0408) | Error 0.5820(0.5968) Steps 646(599.10) | Grad Norm 13.7269(20.9946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 217.8642, Epoch Time 655.0559(627.6415), Bit/dim 4.4255(best: 4.4417), Xent 1.5468, Loss 5.1989, Error 0.5550(best: 0.5539)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0880 | Time 49.7381(47.0322) | Bit/dim 4.4109(4.4818) | Xent 1.6089(1.6536) | Loss 12.2719(14.9187) | Error 0.5780(0.5907) Steps 622(602.81) | Grad Norm 11.1754(18.3120) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 218.6356, Epoch Time 662.5567(628.6890), Bit/dim 4.4211(best: 4.4255), Xent 1.5350, Loss 5.1886, Error 0.5429(best: 0.5539)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0890 | Time 48.3822(47.5644) | Bit/dim 4.4762(4.4715) | Xent 1.6565(1.6412) | Loss 12.3956(14.8383) | Error 0.5881(0.5866) Steps 634(608.63) | Grad Norm 47.4764(19.4428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 218.8890, Epoch Time 679.8804(630.2247), Bit/dim 4.4761(best: 4.4211), Xent 1.5952, Loss 5.2737, Error 0.5659(best: 0.5429)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0900 | Time 45.9787(47.9819) | Bit/dim 4.4465(4.4770) | Xent 1.5967(1.6387) | Loss 12.1359(14.7855) | Error 0.5650(0.5864) Steps 598(610.09) | Grad Norm 10.9705(20.3029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 221.1786, Epoch Time 679.3014(631.6970), Bit/dim 4.4552(best: 4.4211), Xent 1.5329, Loss 5.2216, Error 0.5536(best: 0.5429)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0101 | Time 213.0454, Epoch Time 662.5905(632.6238), Bit/dim 4.4115(best: 4.4211), Xent 1.5422, Loss 5.1826, Error 0.5517(best: 0.5429)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0910 | Time 49.3205(48.0486) | Bit/dim 4.4016(4.4644) | Xent 1.5961(1.6311) | Loss 35.9199(15.3743) | Error 0.5796(0.5849) Steps 586(607.01) | Grad Norm 17.2087(20.6915) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 218.6183, Epoch Time 672.2558(633.8128), Bit/dim 4.3911(best: 4.4115), Xent 1.5011, Loss 5.1416, Error 0.5350(best: 0.5429)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0920 | Time 47.9170(48.2251) | Bit/dim 4.3977(4.4469) | Xent 1.5856(1.6147) | Loss 12.1395(15.2300) | Error 0.5696(0.5798) Steps 628(609.46) | Grad Norm 20.6917(18.7584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 220.9754, Epoch Time 672.8767(634.9847), Bit/dim 4.3667(best: 4.3911), Xent 1.4782, Loss 5.1058, Error 0.5304(best: 0.5350)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0930 | Time 50.8852(48.2714) | Bit/dim 4.3703(4.4277) | Xent 1.5423(1.5943) | Loss 12.0211(15.0992) | Error 0.5544(0.5734) Steps 622(611.01) | Grad Norm 4.5439(15.6910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 219.6061, Epoch Time 686.4584(636.5289), Bit/dim 4.3634(best: 4.3667), Xent 1.5009, Loss 5.1138, Error 0.5415(best: 0.5304)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0940 | Time 52.0168(48.7816) | Bit/dim 4.3586(4.4153) | Xent 1.6257(1.5906) | Loss 12.1214(14.9878) | Error 0.5809(0.5719) Steps 610(614.70) | Grad Norm 49.4275(18.0550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 225.2356, Epoch Time 685.4742(637.9973), Bit/dim 4.4908(best: 4.3634), Xent 1.7442, Loss 5.3629, Error 0.6241(best: 0.5304)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0950 | Time 45.7563(48.7308) | Bit/dim 4.4707(4.4507) | Xent 1.7455(1.6274) | Loss 12.4269(15.0432) | Error 0.6300(0.5841) Steps 634(620.85) | Grad Norm 17.5229(21.4590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 215.1959, Epoch Time 668.6504(638.9169), Bit/dim 4.4222(best: 4.3634), Xent 1.5939, Loss 5.2192, Error 0.5683(best: 0.5304)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0960 | Time 47.8155(48.7120) | Bit/dim 4.3850(4.4423) | Xent 1.6237(1.6342) | Loss 12.0128(14.9467) | Error 0.5915(0.5877) Steps 562(618.97) | Grad Norm 10.7200(19.4394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 206.5737, Epoch Time 652.4554(639.3230), Bit/dim 4.3691(best: 4.3634), Xent 1.5292, Loss 5.1337, Error 0.5456(best: 0.5304)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0970 | Time 46.9674(48.5315) | Bit/dim 4.3410(4.4226) | Xent 1.5401(1.6177) | Loss 11.8960(14.7519) | Error 0.5591(0.5824) Steps 568(608.35) | Grad Norm 8.0869(16.7049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 212.4531, Epoch Time 661.1318(639.9773), Bit/dim 4.3453(best: 4.3634), Xent 1.4796, Loss 5.0851, Error 0.5273(best: 0.5304)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0980 | Time 48.3064(48.2865) | Bit/dim 4.3257(4.4000) | Xent 1.5396(1.5946) | Loss 11.8651(14.5758) | Error 0.5591(0.5745) Steps 562(606.42) | Grad Norm 8.3552(14.2989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 217.3822, Epoch Time 666.8345(640.7830), Bit/dim 4.3222(best: 4.3453), Xent 1.4543, Loss 5.0494, Error 0.5215(best: 0.5273)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0990 | Time 48.8930(48.4433) | Bit/dim 4.3115(4.3773) | Xent 1.4776(1.5712) | Loss 11.8103(14.4414) | Error 0.5350(0.5661) Steps 628(610.74) | Grad Norm 9.5546(12.4893) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 219.0096, Epoch Time 673.5660(641.7665), Bit/dim 4.3065(best: 4.3222), Xent 1.4516, Loss 5.0323, Error 0.5204(best: 0.5215)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0111 | Time 219.5621, Epoch Time 681.1567(642.9482), Bit/dim 4.2890(best: 4.3065), Xent 1.4790, Loss 5.0285, Error 0.5341(best: 0.5204)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1000 | Time 50.9847(48.7702) | Bit/dim 4.3024(4.3575) | Xent 1.5439(1.5567) | Loss 37.7180(15.0963) | Error 0.5580(0.5605) Steps 664(617.54) | Grad Norm 24.7709(14.5574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 225.2198, Epoch Time 686.9743(644.2690), Bit/dim 4.2806(best: 4.2890), Xent 1.4465, Loss 5.0038, Error 0.5209(best: 0.5204)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1010 | Time 50.7415(48.9437) | Bit/dim 4.2846(4.3384) | Xent 1.5378(1.5444) | Loss 11.8491(14.9882) | Error 0.5543(0.5556) Steps 670(621.15) | Grad Norm 20.9199(14.9971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 224.3119, Epoch Time 684.4231(645.4736), Bit/dim 4.2777(best: 4.2806), Xent 1.4822, Loss 5.0188, Error 0.5395(best: 0.5204)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1020 | Time 47.8619(49.0110) | Bit/dim 4.3037(4.3256) | Xent 1.5273(1.5419) | Loss 11.9178(14.8882) | Error 0.5504(0.5547) Steps 622(625.21) | Grad Norm 18.7037(17.7531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 222.5501, Epoch Time 686.2249(646.6961), Bit/dim 4.2739(best: 4.2777), Xent 1.4511, Loss 4.9995, Error 0.5248(best: 0.5204)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1030 | Time 53.0966(49.2904) | Bit/dim 4.2822(4.3126) | Xent 1.4842(1.5311) | Loss 11.8034(14.7369) | Error 0.5415(0.5512) Steps 586(626.03) | Grad Norm 7.6807(17.1916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 220.6663, Epoch Time 687.0668(647.9073), Bit/dim 4.2514(best: 4.2739), Xent 1.4035, Loss 4.9531, Error 0.5036(best: 0.5204)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1040 | Time 44.6178(49.1348) | Bit/dim 4.3025(4.2991) | Xent 1.4908(1.5172) | Loss 11.7059(14.5991) | Error 0.5398(0.5463) Steps 646(628.98) | Grad Norm 32.0276(16.4676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 215.7032, Epoch Time 671.5602(648.6169), Bit/dim 4.2907(best: 4.2514), Xent 1.4531, Loss 5.0173, Error 0.5268(best: 0.5036)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1050 | Time 49.8445(49.3132) | Bit/dim 4.2630(4.2937) | Xent 1.5074(1.5140) | Loss 11.8205(14.5148) | Error 0.5517(0.5461) Steps 670(629.69) | Grad Norm 22.4195(18.8798) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 214.2909, Epoch Time 672.8998(649.3453), Bit/dim 4.2819(best: 4.2514), Xent 1.4292, Loss 4.9964, Error 0.5219(best: 0.5036)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1060 | Time 49.1949(49.2373) | Bit/dim 4.2363(4.2836) | Xent 1.4644(1.5075) | Loss 11.6143(14.3512) | Error 0.5294(0.5454) Steps 592(627.42) | Grad Norm 8.7560(17.6586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 215.9354, Epoch Time 676.5799(650.1624), Bit/dim 4.2369(best: 4.2514), Xent 1.4128, Loss 4.9434, Error 0.5090(best: 0.5036)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 1070 | Time 52.7828(49.2035) | Bit/dim 4.2154(4.2691) | Xent 1.4193(1.4918) | Loss 11.5269(14.2171) | Error 0.5206(0.5403) Steps 670(626.48) | Grad Norm 6.6333(15.7377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 218.0207, Epoch Time 676.7270(650.9593), Bit/dim 4.2226(best: 4.2369), Xent 1.3751, Loss 4.9102, Error 0.4963(best: 0.5036)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 5400 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
