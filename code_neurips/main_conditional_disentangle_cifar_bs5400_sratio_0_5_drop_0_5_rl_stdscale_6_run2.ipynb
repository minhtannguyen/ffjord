{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3,4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=5400, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run2', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "validating...\n",
      "Epoch 0001 | Time 183.1461, Epoch Time 580.0293(580.0293), Bit/dim 8.7588(best: inf), Xent 2.2765, Loss 9.8971, Error 0.7372(best: inf)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0010 | Time 41.7802(74.9737) | Bit/dim 8.7580(8.9575) | Xent 2.2776(2.2998) | Loss 44.5539(22.9676) | Error 0.7515(0.8631) Steps 496(481.06) | Grad Norm 17.1360(24.0910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 179.8262, Epoch Time 533.7084(578.6397), Bit/dim 8.5265(best: 8.7588), Xent 2.2242, Loss 9.6387, Error 0.7390(best: 0.7372)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0020 | Time 38.3032(65.0345) | Bit/dim 8.5162(8.8671) | Xent 2.2226(2.2856) | Loss 21.2463(23.2379) | Error 0.7480(0.8322) Steps 526(483.80) | Grad Norm 7.0084(20.4465) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 180.2641, Epoch Time 542.9702(577.5696), Bit/dim 8.3745(best: 8.5265), Xent 2.1694, Loss 9.4592, Error 0.7320(best: 0.7372)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0030 | Time 38.8192(57.9686) | Bit/dim 8.3411(8.7499) | Xent 2.1656(2.2597) | Loss 20.9589(23.3059) | Error 0.7431(0.8083) Steps 550(491.51) | Grad Norm 6.4397(16.9061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 181.9797, Epoch Time 532.0094(576.2028), Bit/dim 8.1978(best: 8.3745), Xent 2.1248, Loss 9.2602, Error 0.7169(best: 0.7320)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0040 | Time 37.3020(52.4092) | Bit/dim 8.1150(8.6141) | Xent 2.1157(2.2275) | Loss 20.1478(23.1924) | Error 0.7261(0.7874) Steps 472(492.85) | Grad Norm 5.1196(13.8860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 180.9258, Epoch Time 530.5410(574.8329), Bit/dim 7.9836(best: 8.1978), Xent 2.0907, Loss 9.0290, Error 0.7022(best: 0.7169)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0050 | Time 36.9740(48.4393) | Bit/dim 7.8743(8.4515) | Xent 2.0842(2.1932) | Loss 19.5923(22.9519) | Error 0.7087(0.7676) Steps 502(494.81) | Grad Norm 4.9485(11.6125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 181.4392, Epoch Time 533.0513(573.5795), Bit/dim 7.7109(best: 7.9836), Xent 2.0678, Loss 8.7448, Error 0.6892(best: 0.7022)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0060 | Time 37.0053(45.5998) | Bit/dim 7.5443(8.2517) | Xent 2.0732(2.1621) | Loss 18.9920(22.6120) | Error 0.6869(0.7489) Steps 478(495.53) | Grad Norm 4.6971(9.8718) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 183.0289, Epoch Time 539.4318(572.5550), Bit/dim 7.4190(best: 7.7109), Xent 2.0609, Loss 8.4494, Error 0.6736(best: 0.6892)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0070 | Time 38.4143(43.6731) | Bit/dim 7.2590(8.0212) | Xent 2.0792(2.1379) | Loss 18.5302(22.1515) | Error 0.6898(0.7316) Steps 526(503.21) | Grad Norm 3.4370(8.3198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 187.9494, Epoch Time 551.1855(571.9140), Bit/dim 7.2005(best: 7.4190), Xent 2.0732, Loss 8.2371, Error 0.6724(best: 0.6736)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0080 | Time 38.0473(42.2801) | Bit/dim 7.1019(7.7948) | Xent 2.0885(2.1235) | Loss 18.2128(21.7005) | Error 0.7052(0.7200) Steps 508(507.13) | Grad Norm 2.1066(6.8214) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 190.3599, Epoch Time 552.5439(571.3329), Bit/dim 7.0866(best: 7.2005), Xent 2.0853, Loss 8.1293, Error 0.6880(best: 0.6724)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0090 | Time 38.4169(41.5641) | Bit/dim 7.0348(7.6018) | Xent 2.0871(2.1147) | Loss 18.0724(21.3092) | Error 0.7044(0.7164) Steps 514(508.10) | Grad Norm 1.5650(5.5194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 191.7540, Epoch Time 563.7867(571.1065), Bit/dim 7.0316(best: 7.0866), Xent 2.0780, Loss 8.0706, Error 0.6913(best: 0.6724)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0011 | Time 191.9687, Epoch Time 558.7251(570.7350), Bit/dim 6.9983(best: 7.0316), Xent 2.0578, Loss 8.0272, Error 0.6859(best: 0.6724)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0100 | Time 38.9004(40.8655) | Bit/dim 7.0056(7.4467) | Xent 2.0601(2.1043) | Loss 41.0818(21.6589) | Error 0.6861(0.7127) Steps 532(515.25) | Grad Norm 2.6816(4.6330) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 192.6251, Epoch Time 571.0888(570.7456), Bit/dim 6.9698(best: 6.9983), Xent 2.0393, Loss 7.9895, Error 0.6805(best: 0.6724)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0110 | Time 39.8754(40.7314) | Bit/dim 6.9667(7.3230) | Xent 2.0433(2.0909) | Loss 17.8273(21.3304) | Error 0.6920(0.7081) Steps 514(518.19) | Grad Norm 3.7019(4.1329) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 194.5342, Epoch Time 579.5347(571.0093), Bit/dim 6.9371(best: 6.9698), Xent 2.0273, Loss 7.9507, Error 0.6708(best: 0.6724)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0120 | Time 42.0561(40.8351) | Bit/dim 6.9326(7.2232) | Xent 2.0341(2.0780) | Loss 17.9312(21.0492) | Error 0.6781(0.7033) Steps 532(520.28) | Grad Norm 4.1370(3.8571) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 193.2431, Epoch Time 582.9345(571.3671), Bit/dim 6.8984(best: 6.9371), Xent 2.0197, Loss 7.9083, Error 0.6683(best: 0.6708)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0130 | Time 40.9550(40.9072) | Bit/dim 6.8812(7.1383) | Xent 2.0253(2.0648) | Loss 17.6611(20.8082) | Error 0.6765(0.6981) Steps 538(525.28) | Grad Norm 1.2786(3.9847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 197.1916, Epoch Time 582.6075(571.7043), Bit/dim 6.8521(best: 6.8984), Xent 2.0210, Loss 7.8626, Error 0.6802(best: 0.6683)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0140 | Time 43.9103(41.1599) | Bit/dim 6.8259(7.0624) | Xent 2.0292(2.0545) | Loss 17.7368(20.5820) | Error 0.6891(0.6943) Steps 538(528.70) | Grad Norm 16.0252(5.8621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 194.9510, Epoch Time 583.1696(572.0482), Bit/dim 6.7949(best: 6.8521), Xent 2.0187, Loss 7.8043, Error 0.6943(best: 0.6683)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0150 | Time 42.5474(41.1683) | Bit/dim 6.7539(6.9891) | Xent 2.0144(2.0448) | Loss 17.4578(20.3658) | Error 0.6970(0.6918) Steps 580(533.56) | Grad Norm 18.1938(8.1786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 195.7935, Epoch Time 583.2892(572.3855), Bit/dim 6.7185(best: 6.7949), Xent 1.9877, Loss 7.7123, Error 0.6654(best: 0.6683)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0160 | Time 42.1149(41.1168) | Bit/dim 6.6511(6.9132) | Xent 1.9862(2.0334) | Loss 17.0630(20.1224) | Error 0.6676(0.6878) Steps 532(535.45) | Grad Norm 4.3690(9.8541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 196.5669, Epoch Time 584.6791(572.7543), Bit/dim 6.6331(best: 6.7185), Xent 2.0146, Loss 7.6404, Error 0.7030(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0170 | Time 39.9687(41.2852) | Bit/dim 6.5578(6.8309) | Xent 2.0304(2.0393) | Loss 16.8832(19.8966) | Error 0.7124(0.6977) Steps 562(537.32) | Grad Norm 41.3882(19.2342) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 199.5797, Epoch Time 590.7425(573.2939), Bit/dim 6.5271(best: 6.6331), Xent 1.9884, Loss 7.5213, Error 0.6915(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0180 | Time 44.7446(41.1206) | Bit/dim 6.4310(6.7398) | Xent 1.9853(2.0353) | Loss 16.6693(19.6417) | Error 0.6576(0.7019) Steps 544(537.70) | Grad Norm 14.1458(24.1377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 197.0145, Epoch Time 579.5518(573.4817), Bit/dim 6.4230(best: 6.5271), Xent 2.0145, Loss 7.4303, Error 0.7010(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0021 | Time 197.5690, Epoch Time 578.5158(573.6327), Bit/dim 6.2815(best: 6.4230), Xent 1.9847, Loss 7.2738, Error 0.6855(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0190 | Time 42.9729(41.0289) | Bit/dim 6.2800(6.6368) | Xent 1.9881(2.0265) | Loss 40.5635(20.0882) | Error 0.6920(0.6989) Steps 574(538.77) | Grad Norm 27.5445(26.3845) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 195.9299, Epoch Time 592.3234(574.1934), Bit/dim 6.1288(best: 6.2815), Xent 1.9727, Loss 7.1151, Error 0.6741(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0200 | Time 43.4687(41.2875) | Bit/dim 6.1070(6.5184) | Xent 2.0004(2.0194) | Loss 15.9342(19.7305) | Error 0.7020(0.6983) Steps 532(542.36) | Grad Norm 49.1635(32.0504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 201.8501, Epoch Time 599.5759(574.9549), Bit/dim 5.9985(best: 6.1288), Xent 1.9633, Loss 6.9802, Error 0.6585(best: 0.6654)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0210 | Time 43.9623(41.6642) | Bit/dim 5.9681(6.3917) | Xent 1.9892(2.0202) | Loss 15.8354(19.4138) | Error 0.6815(0.7018) Steps 538(543.78) | Grad Norm 43.7889(43.1924) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 201.0872, Epoch Time 604.9233(575.8539), Bit/dim 5.9136(best: 5.9985), Xent 2.0021, Loss 6.9147, Error 0.7029(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0220 | Time 42.1644(41.9720) | Bit/dim 5.9788(6.2734) | Xent 2.4547(2.0593) | Loss 16.2888(19.1145) | Error 0.7978(0.7078) Steps 556(545.09) | Grad Norm 143.5829(54.4748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 199.0940, Epoch Time 594.8199(576.4229), Bit/dim 5.8942(best: 5.9136), Xent 2.0938, Loss 6.9411, Error 0.7214(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0230 | Time 42.1754(41.9914) | Bit/dim 5.8439(6.1692) | Xent 2.0380(2.0661) | Loss 15.4896(18.8448) | Error 0.6965(0.7135) Steps 538(549.31) | Grad Norm 23.9797(51.7724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 197.6958, Epoch Time 595.2318(576.9872), Bit/dim 5.7935(best: 5.8942), Xent 2.0282, Loss 6.8075, Error 0.7072(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0240 | Time 43.5999(42.1333) | Bit/dim 5.7643(6.0720) | Xent 2.0167(2.0570) | Loss 15.4289(18.5673) | Error 0.6856(0.7097) Steps 580(552.98) | Grad Norm 7.3603(42.7994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 197.9656, Epoch Time 591.3169(577.4171), Bit/dim 5.7609(best: 5.7935), Xent 2.0003, Loss 6.7610, Error 0.6665(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0250 | Time 41.1024(42.0127) | Bit/dim 5.9449(6.0034) | Xent 2.4266(2.0882) | Loss 16.1556(18.3848) | Error 0.8472(0.7183) Steps 544(551.17) | Grad Norm 216.6950(57.3678) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 197.8270, Epoch Time 588.8049(577.7587), Bit/dim 5.8649(best: 5.7609), Xent 2.0987, Loss 6.9142, Error 0.7383(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0260 | Time 38.7164(41.6293) | Bit/dim 5.7597(5.9504) | Xent 2.0679(2.0832) | Loss 15.4001(18.1863) | Error 0.7085(0.7169) Steps 538(550.91) | Grad Norm 29.8431(55.0238) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 194.7049, Epoch Time 578.8745(577.7922), Bit/dim 5.7597(best: 5.7609), Xent 2.0569, Loss 6.7882, Error 0.7181(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0270 | Time 42.4695(41.3357) | Bit/dim 5.7092(5.8911) | Xent 2.0322(2.0764) | Loss 15.3323(17.9802) | Error 0.6963(0.7149) Steps 544(550.34) | Grad Norm 14.3621(44.2529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 194.1671, Epoch Time 573.2208(577.6550), Bit/dim 5.7023(best: 5.7597), Xent 2.0346, Loss 6.7196, Error 0.6936(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0031 | Time 193.2621, Epoch Time 569.6935(577.4162), Bit/dim 5.6746(best: 5.7023), Xent 1.9988, Loss 6.6740, Error 0.6609(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0280 | Time 40.0106(40.9915) | Bit/dim 5.6906(5.8380) | Xent 2.0055(2.0624) | Loss 37.7282(18.4555) | Error 0.6781(0.7078) Steps 520(546.67) | Grad Norm 4.7383(34.5101) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 192.6006, Epoch Time 565.6459(577.0631), Bit/dim 5.6530(best: 5.6746), Xent 1.9706, Loss 6.6383, Error 0.6486(best: 0.6585)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0290 | Time 40.3965(40.6722) | Bit/dim 5.6322(5.7906) | Xent 1.9840(2.0440) | Loss 15.0285(18.2330) | Error 0.6833(0.6988) Steps 544(546.73) | Grad Norm 3.8717(26.5402) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 190.7719, Epoch Time 570.6117(576.8695), Bit/dim 5.6332(best: 5.6530), Xent 1.9511, Loss 6.6088, Error 0.6513(best: 0.6486)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0300 | Time 40.5632(40.6105) | Bit/dim 5.6216(5.7500) | Xent 1.9496(2.0233) | Loss 14.9823(18.0382) | Error 0.6596(0.6889) Steps 544(546.24) | Grad Norm 3.3390(20.5086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 191.4367, Epoch Time 573.8757(576.7797), Bit/dim 5.6114(best: 5.6332), Xent 1.9310, Loss 6.5769, Error 0.6393(best: 0.6486)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0310 | Time 40.1012(40.5899) | Bit/dim 5.5977(5.7144) | Xent 1.9389(2.0035) | Loss 14.8842(17.8411) | Error 0.6565(0.6809) Steps 532(542.57) | Grad Norm 1.5728(16.3088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 191.2059, Epoch Time 568.4773(576.5307), Bit/dim 5.5849(best: 5.6114), Xent 1.9181, Loss 6.5440, Error 0.6434(best: 0.6393)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0320 | Time 38.7526(40.4591) | Bit/dim 5.5906(5.6822) | Xent 1.9425(1.9864) | Loss 14.9284(17.6769) | Error 0.6596(0.6744) Steps 556(540.86) | Grad Norm 11.2419(14.0960) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 191.9403, Epoch Time 571.3301(576.3746), Bit/dim 5.5633(best: 5.5849), Xent 1.9091, Loss 6.5178, Error 0.6398(best: 0.6393)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0330 | Time 40.1753(40.3090) | Bit/dim 5.5489(5.6508) | Xent 1.9019(1.9698) | Loss 14.7671(17.5272) | Error 0.6441(0.6691) Steps 514(539.19) | Grad Norm 12.4045(14.0713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 196.6713, Epoch Time 572.1868(576.2490), Bit/dim 5.5360(best: 5.5633), Xent 1.9037, Loss 6.4878, Error 0.6387(best: 0.6393)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0340 | Time 40.8620(40.4591) | Bit/dim 5.5147(5.6192) | Xent 1.9543(1.9584) | Loss 14.6489(17.3643) | Error 0.6696(0.6654) Steps 526(538.80) | Grad Norm 59.9386(15.8935) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 194.3718, Epoch Time 578.6843(576.3221), Bit/dim 5.6140(best: 5.5360), Xent 2.2232, Loss 6.7256, Error 0.7607(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0350 | Time 41.4983(40.5153) | Bit/dim 5.5232(5.5993) | Xent 1.9694(1.9746) | Loss 14.6764(17.2680) | Error 0.6900(0.6751) Steps 526(538.57) | Grad Norm 35.3214(29.1771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 193.0456, Epoch Time 575.1443(576.2867), Bit/dim 5.4813(best: 5.5360), Xent 1.9495, Loss 6.4561, Error 0.6620(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0360 | Time 40.7853(40.6107) | Bit/dim 5.4343(5.5670) | Xent 1.9827(1.9721) | Loss 14.6315(17.1394) | Error 0.6913(0.6765) Steps 562(541.15) | Grad Norm 11.8582(26.0039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 195.1647, Epoch Time 580.5510(576.4147), Bit/dim 5.4435(best: 5.4813), Xent 1.9474, Loss 6.4172, Error 0.6714(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0041 | Time 199.5558, Epoch Time 593.6540(576.9318), Bit/dim 5.4010(best: 5.4435), Xent 1.9231, Loss 6.3625, Error 0.6467(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0370 | Time 42.8135(40.9957) | Bit/dim 5.3922(5.5291) | Xent 1.9460(1.9649) | Loss 38.1678(17.6947) | Error 0.6650(0.6743) Steps 562(542.17) | Grad Norm 6.3036(22.5314) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 201.1705, Epoch Time 603.7197(577.7355), Bit/dim 5.3578(best: 5.4010), Xent 1.9084, Loss 6.3120, Error 0.6403(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0380 | Time 42.5650(41.5748) | Bit/dim 5.3395(5.4886) | Xent 1.9232(1.9557) | Loss 14.3221(17.5335) | Error 0.6565(0.6709) Steps 568(550.92) | Grad Norm 5.4488(18.9343) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 206.6500, Epoch Time 629.8425(579.2987), Bit/dim 5.3206(best: 5.3578), Xent 1.8906, Loss 6.2659, Error 0.6324(best: 0.6387)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0390 | Time 46.3373(42.6271) | Bit/dim 5.3122(5.4467) | Xent 1.9208(1.9447) | Loss 14.3852(17.3926) | Error 0.6581(0.6668) Steps 592(559.47) | Grad Norm 13.8050(15.8618) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 205.2360, Epoch Time 635.0270(580.9705), Bit/dim 5.2917(best: 5.3206), Xent 1.9325, Loss 6.2580, Error 0.6635(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0400 | Time 43.9586(43.4698) | Bit/dim 5.3010(5.4078) | Xent 2.0712(1.9528) | Loss 14.5174(17.2583) | Error 0.7398(0.6720) Steps 574(564.56) | Grad Norm 76.5088(24.4657) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 203.5069, Epoch Time 625.6519(582.3110), Bit/dim 5.2572(best: 5.2917), Xent 1.9135, Loss 6.2139, Error 0.6524(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0410 | Time 45.1399(44.0449) | Bit/dim 5.2550(5.3708) | Xent 1.9398(1.9513) | Loss 14.1815(17.0977) | Error 0.6728(0.6731) Steps 568(567.96) | Grad Norm 22.5010(24.1841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 204.4855, Epoch Time 633.5851(583.8492), Bit/dim 5.2336(best: 5.2572), Xent 1.9120, Loss 6.1896, Error 0.6526(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0420 | Time 43.8741(44.1540) | Bit/dim 5.2136(5.3336) | Xent 1.9064(1.9419) | Loss 14.1866(16.9396) | Error 0.6544(0.6704) Steps 622(572.15) | Grad Norm 2.7825(20.7250) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 204.8947, Epoch Time 619.0025(584.9038), Bit/dim 5.2042(best: 5.2336), Xent 1.8863, Loss 6.1473, Error 0.6381(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0430 | Time 43.6184(44.2336) | Bit/dim 5.1696(5.2988) | Xent 1.8991(1.9310) | Loss 13.9795(16.7996) | Error 0.6483(0.6660) Steps 580(574.81) | Grad Norm 3.9528(17.0243) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 204.0904, Epoch Time 623.0476(586.0481), Bit/dim 5.1769(best: 5.2042), Xent 1.8761, Loss 6.1149, Error 0.6330(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0440 | Time 42.1935(44.1959) | Bit/dim 5.1572(5.2641) | Xent 1.8635(1.9178) | Loss 13.8212(16.6289) | Error 0.6407(0.6615) Steps 562(573.87) | Grad Norm 3.6158(14.0290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 202.7392, Epoch Time 612.2050(586.8328), Bit/dim 5.1510(best: 5.1769), Xent 1.8586, Loss 6.0803, Error 0.6282(best: 0.6324)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0450 | Time 42.1580(44.0255) | Bit/dim 5.1238(5.2321) | Xent 1.8898(1.9085) | Loss 14.0068(16.4967) | Error 0.6528(0.6588) Steps 592(572.13) | Grad Norm 19.6477(14.0183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 201.1659, Epoch Time 610.0655(587.5298), Bit/dim 5.1353(best: 5.1510), Xent 1.8967, Loss 6.0837, Error 0.6637(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0051 | Time 200.8099, Epoch Time 608.8862(588.1705), Bit/dim 5.1073(best: 5.1353), Xent 1.8683, Loss 6.0415, Error 0.6349(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0460 | Time 44.0117(43.9158) | Bit/dim 5.1111(5.2025) | Xent 1.8746(1.9035) | Loss 37.8650(17.0789) | Error 0.6409(0.6583) Steps 592(570.26) | Grad Norm 21.2774(15.6090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 201.1317, Epoch Time 608.6392(588.7845), Bit/dim 5.0791(best: 5.1073), Xent 1.8365, Loss 5.9973, Error 0.6282(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0470 | Time 43.9540(43.8831) | Bit/dim 5.0938(5.1735) | Xent 1.8571(1.8956) | Loss 13.6630(16.9408) | Error 0.6357(0.6560) Steps 556(571.25) | Grad Norm 23.5040(18.4725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 201.7293, Epoch Time 613.4580(589.5248), Bit/dim 5.0550(best: 5.0791), Xent 1.8378, Loss 5.9739, Error 0.6300(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0480 | Time 45.7299(43.7935) | Bit/dim 5.0402(5.1439) | Xent 1.8598(1.8862) | Loss 13.6540(16.7995) | Error 0.6446(0.6530) Steps 592(567.95) | Grad Norm 26.6808(19.7387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 201.3540, Epoch Time 603.8162(589.9535), Bit/dim 5.0367(best: 5.0550), Xent 1.8860, Loss 5.9797, Error 0.6567(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0490 | Time 44.5860(43.6317) | Bit/dim 5.0275(5.1160) | Xent 1.8753(1.8861) | Loss 13.7450(16.6766) | Error 0.6607(0.6545) Steps 568(568.50) | Grad Norm 27.0034(23.8620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 202.3022, Epoch Time 616.4148(590.7473), Bit/dim 5.0188(best: 5.0367), Xent 1.8684, Loss 5.9530, Error 0.6414(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0500 | Time 42.7041(43.8595) | Bit/dim 4.9855(5.0889) | Xent 1.8761(1.8856) | Loss 13.6923(16.5380) | Error 0.6585(0.6560) Steps 556(567.60) | Grad Norm 19.9030(26.2995) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 202.2488, Epoch Time 617.6565(591.5546), Bit/dim 4.9775(best: 5.0188), Xent 1.8239, Loss 5.8894, Error 0.6229(best: 0.6282)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0510 | Time 43.2118(43.8902) | Bit/dim 4.9708(5.0598) | Xent 1.8522(1.8776) | Loss 13.3679(16.3890) | Error 0.6348(0.6530) Steps 562(566.63) | Grad Norm 13.6705(23.8503) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 203.4270, Epoch Time 613.7275(592.2198), Bit/dim 4.9553(best: 4.9775), Xent 1.8399, Loss 5.8753, Error 0.6487(best: 0.6229)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0520 | Time 43.7719(43.9068) | Bit/dim 5.0055(5.0372) | Xent 2.0102(1.8805) | Loss 13.8381(16.2872) | Error 0.7104(0.6556) Steps 562(568.15) | Grad Norm 59.4608(25.9260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 200.3448, Epoch Time 615.3001(592.9122), Bit/dim 5.0405(best: 4.9553), Xent 1.8872, Loss 5.9841, Error 0.6718(best: 0.6229)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0530 | Time 44.9831(43.9764) | Bit/dim 4.9698(5.0235) | Xent 1.8934(1.8824) | Loss 13.4506(16.1540) | Error 0.6519(0.6571) Steps 526(564.88) | Grad Norm 42.5316(27.6774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 199.6957, Epoch Time 611.5387(593.4710), Bit/dim 4.9470(best: 4.9553), Xent 1.9441, Loss 5.9191, Error 0.6873(best: 0.6229)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0540 | Time 45.3269(44.0405) | Bit/dim 4.9179(4.9989) | Xent 1.8648(1.8860) | Loss 13.4562(16.0408) | Error 0.6511(0.6588) Steps 580(567.98) | Grad Norm 26.1046(30.9980) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 202.9336, Epoch Time 619.0035(594.2370), Bit/dim 4.9045(best: 4.9470), Xent 1.8286, Loss 5.8188, Error 0.6158(best: 0.6229)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0061 | Time 196.1858, Epoch Time 610.6748(594.7301), Bit/dim 4.8752(best: 4.9045), Xent 1.8052, Loss 5.7778, Error 0.6210(best: 0.6158)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0550 | Time 43.9057(44.0694) | Bit/dim 4.8789(4.9711) | Xent 1.8289(1.8729) | Loss 37.9039(16.6311) | Error 0.6428(0.6541) Steps 550(569.53) | Grad Norm 14.3207(27.1940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 201.9663, Epoch Time 616.1163(595.3717), Bit/dim 4.8515(best: 4.8752), Xent 1.7893, Loss 5.7462, Error 0.6147(best: 0.6158)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0560 | Time 43.0056(44.0631) | Bit/dim 4.8847(4.9430) | Xent 1.9005(1.8610) | Loss 13.3510(16.4558) | Error 0.6759(0.6495) Steps 556(567.39) | Grad Norm 40.6073(25.5235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 207.5349, Epoch Time 625.8817(596.2870), Bit/dim 4.8888(best: 4.8515), Xent 1.8249, Loss 5.8013, Error 0.6213(best: 0.6147)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0570 | Time 41.7049(44.2264) | Bit/dim 4.8669(4.9301) | Xent 1.8470(1.8701) | Loss 13.4230(16.3721) | Error 0.6578(0.6550) Steps 592(570.92) | Grad Norm 21.2285(29.1325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 199.7229, Epoch Time 619.9595(596.9972), Bit/dim 4.8518(best: 4.8515), Xent 1.7948, Loss 5.7492, Error 0.6133(best: 0.6147)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0580 | Time 43.0444(44.3822) | Bit/dim 4.8249(4.9081) | Xent 1.8071(1.8619) | Loss 13.1775(16.2420) | Error 0.6289(0.6519) Steps 562(572.02) | Grad Norm 11.7599(27.2144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 202.1342, Epoch Time 615.5601(597.5541), Bit/dim 4.7983(best: 4.8515), Xent 1.7622, Loss 5.6794, Error 0.6033(best: 0.6133)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0590 | Time 44.4632(44.3867) | Bit/dim 4.7781(4.8798) | Xent 1.7577(1.8430) | Loss 13.0583(16.0612) | Error 0.6074(0.6442) Steps 592(571.83) | Grad Norm 3.4830(23.2532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 203.1961, Epoch Time 623.4634(598.3313), Bit/dim 4.7718(best: 4.7983), Xent 1.7411, Loss 5.6424, Error 0.6033(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0600 | Time 45.3248(44.5334) | Bit/dim 4.8036(4.8542) | Xent 1.9847(1.8335) | Loss 13.3210(15.9153) | Error 0.6893(0.6412) Steps 556(569.85) | Grad Norm 77.9184(24.5122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 205.4525, Epoch Time 625.0786(599.1338), Bit/dim 4.8401(best: 4.7718), Xent 1.9551, Loss 5.8176, Error 0.7173(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0610 | Time 45.3951(44.6397) | Bit/dim 4.8397(4.8486) | Xent 1.9463(1.8684) | Loss 13.5968(15.8479) | Error 0.6935(0.6563) Steps 586(574.42) | Grad Norm 51.7356(30.1626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 203.2827, Epoch Time 625.2573(599.9175), Bit/dim 4.8919(best: 4.7718), Xent 1.9611, Loss 5.8725, Error 0.7024(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0620 | Time 46.7211(44.7156) | Bit/dim 4.8377(4.8511) | Xent 1.9662(1.8906) | Loss 13.4318(15.7922) | Error 0.7017(0.6666) Steps 604(575.04) | Grad Norm 20.6474(30.8321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 201.6441, Epoch Time 621.6065(600.5681), Bit/dim 4.8030(best: 4.7718), Xent 1.8402, Loss 5.7231, Error 0.6541(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0630 | Time 44.3430(44.9432) | Bit/dim 4.7498(4.8318) | Xent 1.8217(1.8785) | Loss 13.0474(15.6440) | Error 0.6378(0.6617) Steps 598(574.25) | Grad Norm 7.5406(26.0807) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 204.6326, Epoch Time 631.6458(601.5005), Bit/dim 4.7599(best: 4.7718), Xent 1.7717, Loss 5.6458, Error 0.6110(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0071 | Time 202.3661, Epoch Time 622.0167(602.1160), Bit/dim 4.7225(best: 4.7599), Xent 1.7109, Loss 5.5779, Error 0.5896(best: 0.6033)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0640 | Time 42.5381(44.8556) | Bit/dim 4.7163(4.8071) | Xent 1.7302(1.8478) | Loss 36.4284(16.2247) | Error 0.6080(0.6501) Steps 580(569.49) | Grad Norm 13.2258(22.1591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 202.6775, Epoch Time 613.8864(602.4691), Bit/dim 4.7005(best: 4.7225), Xent 1.6941, Loss 5.5476, Error 0.5922(best: 0.5896)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0650 | Time 43.5640(44.7171) | Bit/dim 4.6898(4.7799) | Xent 1.7074(1.8176) | Loss 12.7801(16.0356) | Error 0.6026(0.6398) Steps 562(567.88) | Grad Norm 3.1709(19.6949) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 202.1645, Epoch Time 620.3390(603.0052), Bit/dim 4.6824(best: 4.7005), Xent 1.7105, Loss 5.5376, Error 0.5952(best: 0.5896)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0660 | Time 44.5108(44.6284) | Bit/dim 4.6638(4.7566) | Xent 1.7245(1.8119) | Loss 12.8627(15.8813) | Error 0.6081(0.6390) Steps 598(570.93) | Grad Norm 3.2512(21.5624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 202.7380, Epoch Time 618.8004(603.4790), Bit/dim 4.6695(best: 4.6824), Xent 1.6861, Loss 5.5126, Error 0.5895(best: 0.5896)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0670 | Time 45.4746(44.7723) | Bit/dim 4.6664(4.7348) | Xent 1.7411(1.7936) | Loss 12.6699(15.7171) | Error 0.6065(0.6334) Steps 562(571.73) | Grad Norm 26.8911(21.2273) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 205.4918, Epoch Time 632.8970(604.3616), Bit/dim 4.6617(best: 4.6695), Xent 1.6945, Loss 5.5089, Error 0.6036(best: 0.5895)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0680 | Time 44.3104(44.7910) | Bit/dim 4.6521(4.7142) | Xent 1.7441(1.7775) | Loss 12.9000(15.5853) | Error 0.6126(0.6281) Steps 568(572.46) | Grad Norm 35.1968(21.9397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 206.7928, Epoch Time 632.5381(605.2069), Bit/dim 4.6558(best: 4.6617), Xent 1.7221, Loss 5.5169, Error 0.6124(best: 0.5895)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0690 | Time 45.4222(45.2958) | Bit/dim 4.7038(4.7063) | Xent 1.7758(1.7814) | Loss 12.8737(15.4978) | Error 0.6261(0.6296) Steps 568(573.87) | Grad Norm 25.3042(27.0686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 205.4132, Epoch Time 636.2461(606.1380), Bit/dim 4.7109(best: 4.6558), Xent 1.7868, Loss 5.6043, Error 0.6421(best: 0.5895)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0700 | Time 44.1097(45.1541) | Bit/dim 4.6619(4.7017) | Xent 1.7337(1.7817) | Loss 12.6616(15.4063) | Error 0.6198(0.6309) Steps 556(572.48) | Grad Norm 14.8299(25.6249) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 203.0551, Epoch Time 623.5533(606.6605), Bit/dim 4.6409(best: 4.6558), Xent 1.6878, Loss 5.4847, Error 0.5852(best: 0.5895)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0710 | Time 44.8333(45.2544) | Bit/dim 4.6250(4.6843) | Xent 1.7284(1.7675) | Loss 12.7017(15.2970) | Error 0.6089(0.6253) Steps 598(571.83) | Grad Norm 29.8845(23.1235) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 205.0337, Epoch Time 630.8488(607.3861), Bit/dim 4.6189(best: 4.6409), Xent 1.6668, Loss 5.4523, Error 0.5902(best: 0.5852)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0720 | Time 47.6980(45.4489) | Bit/dim 4.5841(4.6647) | Xent 1.6744(1.7483) | Loss 12.6676(15.1770) | Error 0.5946(0.6190) Steps 556(574.82) | Grad Norm 13.8434(21.9788) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 205.6320, Epoch Time 635.7456(608.2369), Bit/dim 4.6065(best: 4.6189), Xent 1.6264, Loss 5.4197, Error 0.5729(best: 0.5852)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0081 | Time 208.6467, Epoch Time 629.6607(608.8796), Bit/dim 4.6532(best: 4.6065), Xent 1.7058, Loss 5.5061, Error 0.6053(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0730 | Time 47.8748(45.4039) | Bit/dim 4.6615(4.6584) | Xent 1.7860(1.7627) | Loss 37.4358(15.8711) | Error 0.6296(0.6240) Steps 628(580.92) | Grad Norm 23.2429(26.0577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 207.0453, Epoch Time 637.2387(609.7304), Bit/dim 4.6062(best: 4.6065), Xent 1.6886, Loss 5.4505, Error 0.5910(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0740 | Time 45.1665(45.4691) | Bit/dim 4.5726(4.6446) | Xent 1.6982(1.7563) | Loss 12.5490(15.7579) | Error 0.6028(0.6222) Steps 598(579.91) | Grad Norm 5.5541(23.4912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 203.3266, Epoch Time 627.5601(610.2653), Bit/dim 4.5703(best: 4.6062), Xent 1.6272, Loss 5.3839, Error 0.5711(best: 0.5729)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0750 | Time 45.1779(45.5554) | Bit/dim 4.5636(4.6267) | Xent 1.6717(1.7424) | Loss 12.4625(15.5959) | Error 0.5809(0.6175) Steps 580(581.93) | Grad Norm 12.9136(21.1433) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 205.5854, Epoch Time 636.0579(611.0391), Bit/dim 4.5524(best: 4.5703), Xent 1.5833, Loss 5.3441, Error 0.5600(best: 0.5711)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0760 | Time 45.5918(45.6214) | Bit/dim 4.5423(4.6080) | Xent 1.6420(1.7158) | Loss 12.3297(15.4693) | Error 0.5811(0.6085) Steps 550(582.23) | Grad Norm 15.6656(18.4526) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 208.8781, Epoch Time 640.5674(611.9249), Bit/dim 4.5377(best: 4.5524), Xent 1.5765, Loss 5.3260, Error 0.5572(best: 0.5600)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0770 | Time 46.7903(45.7434) | Bit/dim 4.5211(4.5877) | Xent 1.6171(1.6949) | Loss 12.3300(15.3115) | Error 0.5724(0.6014) Steps 580(584.36) | Grad Norm 10.7306(17.1733) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 212.4216, Epoch Time 645.5636(612.9341), Bit/dim 4.5424(best: 4.5377), Xent 1.5601, Loss 5.3225, Error 0.5544(best: 0.5572)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0780 | Time 47.8049(46.1017) | Bit/dim 4.5672(4.5800) | Xent 1.8588(1.6941) | Loss 12.6739(15.2141) | Error 0.6528(0.6020) Steps 592(589.28) | Grad Norm 51.2209(20.0683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 212.9194, Epoch Time 652.9862(614.1356), Bit/dim 4.5843(best: 4.5377), Xent 1.7618, Loss 5.4653, Error 0.6280(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0790 | Time 47.9264(46.3709) | Bit/dim 4.5494(4.5731) | Xent 1.7039(1.7100) | Loss 12.5723(15.1449) | Error 0.6170(0.6088) Steps 610(590.73) | Grad Norm 14.4339(21.1774) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 211.4800, Epoch Time 649.2844(615.1901), Bit/dim 4.5362(best: 4.5377), Xent 1.6261, Loss 5.3493, Error 0.5772(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0800 | Time 44.3411(46.2333) | Bit/dim 4.5151(4.5603) | Xent 1.6585(1.7006) | Loss 12.2950(15.0390) | Error 0.5900(0.6060) Steps 574(587.26) | Grad Norm 21.3338(19.4080) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 202.5339, Epoch Time 631.9258(615.6922), Bit/dim 4.5080(best: 4.5362), Xent 1.5818, Loss 5.2989, Error 0.5566(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0810 | Time 45.2016(46.1117) | Bit/dim 4.5458(4.5469) | Xent 1.6942(1.6865) | Loss 12.4821(14.8761) | Error 0.5970(0.6004) Steps 556(586.04) | Grad Norm 34.8862(20.3049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 198.2970, Epoch Time 627.8220(616.0561), Bit/dim 4.5002(best: 4.5080), Xent 1.6146, Loss 5.3075, Error 0.5797(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "validating...\n",
      "Epoch 0091 | Time 204.3005, Epoch Time 631.5109(616.5197), Bit/dim 4.4787(best: 4.5002), Xent 1.5514, Loss 5.2544, Error 0.5546(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0820 | Time 47.4597(46.0687) | Bit/dim 4.4813(4.5306) | Xent 1.5922(1.6721) | Loss 35.9402(15.4207) | Error 0.5765(0.5955) Steps 604(582.55) | Grad Norm 10.5758(19.8707) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 208.3741, Epoch Time 651.1854(617.5597), Bit/dim 4.4610(best: 4.4787), Xent 1.5469, Loss 5.2344, Error 0.5495(best: 0.5544)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0830 | Time 47.1193(46.4102) | Bit/dim 4.4576(4.5124) | Xent 1.5893(1.6554) | Loss 12.0934(15.2853) | Error 0.5661(0.5896) Steps 580(583.65) | Grad Norm 19.2910(18.2855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 208.7054, Epoch Time 643.5157(618.3384), Bit/dim 4.4925(best: 4.4610), Xent 1.6454, Loss 5.3153, Error 0.5835(best: 0.5495)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0840 | Time 45.8982(46.3665) | Bit/dim 4.5089(4.5093) | Xent 1.6080(1.6530) | Loss 12.3879(15.1885) | Error 0.5769(0.5887) Steps 604(580.84) | Grad Norm 27.3399(21.6231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 209.8501, Epoch Time 642.3291(619.0581), Bit/dim 4.4611(best: 4.4610), Xent 1.5739, Loss 5.2481, Error 0.5578(best: 0.5495)\n",
      "===> Using batch size 5400. Total 9 iterations/epoch.\n",
      "Iter 0850 | Time 47.5167(46.4681) | Bit/dim 4.4620(4.5003) | Xent 1.6161(1.6517) | Loss 12.2539(15.0858) | Error 0.5741(0.5882) Steps 610(585.39) | Grad Norm 20.1129(21.8050) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 5400 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs5400_sratio_0_5_drop_0_5_rl_stdscale_6_run2 --seed 2 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
