{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond_multiscale_beta.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "parser.add_argument(\"--cond_nn\", choices=[\"linear\", \"mlp\"], type=str, default=\"linear\")\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "# for disentanglement\n",
      "parser.add_argument('--beta', default=0.01, type=float, help='disentanglement weight')\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    z_sup_class = [o[:,:int(np.prod(o.size()[1:])*0.5)] for o in z]\n",
      "    z_sup_class = torch.cat(z_sup_class,1)\n",
      "    \n",
      "    z_sup_color = [o[:,int(np.prod(o.size()[1:])*0.5):] for o in z]\n",
      "    z_sup_color = torch.cat(z_sup_color,1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup_class).view(-1,1)  # logp(z)_sup\n",
      "    beta_logpz_sup = logpz_sup * (1.0 - args.beta * torch.exp(logpz_sup) / torch.tensor(model.module.y_class).to(logpz_sup))\n",
      "    \n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z_sup_color).view(-1,1)  # logp(z)_color_sup\n",
      "    beta_logpz_color_sup = logpz_color_sup * (1.0 - args.beta * torch.exp(logpz_color_sup) / torch.tensor(model.module.y_color).to(logpz_color_sup))\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = beta_logpz_sup + beta_logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup_class = model.module.dropout(z_sup_class)\n",
      "        z_sup_color = model.module.dropout_color(z_sup_color)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup_class)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(z_sup_color)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio * 2.,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            cond_nn=args.cond_nn,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_color_sup = fixed_z_color_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_color_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_color_sup = start_color_sup + (2**(ns-2))*a_color_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_color_sup = end_color_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_color_sup = start_color_sup + a_color_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_color_sup[:,start_color_sup:end_color_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, beta=100.0, cond_nn='linear', condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_100_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 6.9168(22.3517) | Bit/dim 25.4505(27.3296) | Xent 2.2869(2.3009) | Xent Color 2.3007(2.3023) | Loss 48.6140(51.8004) | Error 0.8744(0.8882) | Error Color 0.9067(0.8898) |Steps 302(302.41) | Grad Norm 260.8726(274.9980) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.3612(18.3775) | Bit/dim 20.0909(26.0735) | Xent 2.2513(2.2928) | Xent Color 2.2917(2.3003) | Loss 39.2004(49.5903) | Error 0.8644(0.8869) | Error Color 0.8811(0.8910) |Steps 314(304.49) | Grad Norm 216.4388(264.8254) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.5125(15.4706) | Bit/dim 13.8656(23.5615) | Xent 2.1974(2.2735) | Xent Color 2.2745(2.2954) | Loss 27.6801(45.0999) | Error 0.5411(0.8307) | Error Color 0.7822(0.8687) |Steps 302(307.56) | Grad Norm 159.2173(243.6358) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 7.9933(13.4767) | Bit/dim 8.9832(20.2265) | Xent 2.1211(2.2412) | Xent Color 2.2517(2.2866) | Loss 18.9596(39.0984) | Error 0.3322(0.7163) | Error Color 0.7511(0.8413) |Steps 362(316.14) | Grad Norm 95.5072(211.8927) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 9.2351(12.2533) | Bit/dim 6.6884(16.8755) | Xent 2.0425(2.1962) | Xent Color 2.2351(2.2744) | Loss 14.8156(33.0824) | Error 0.3333(0.6080) | Error Color 0.7467(0.8130) |Steps 356(330.83) | Grad Norm 33.8636(171.6083) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.5931(11.4482) | Bit/dim 6.0965(14.1054) | Xent 1.9660(2.1429) | Xent Color 2.1978(2.2582) | Loss 13.8232(28.1131) | Error 0.3422(0.5346) | Error Color 0.7800(0.8032) |Steps 416(346.46) | Grad Norm 25.0494(132.5926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 60.4878, Epoch Time 641.9139(641.9139), Bit/dim 5.7409(best: inf), Xent 1.8894, Xent Color 2.1737. Loss 6.7567, Error 0.2505(best: inf), Error Color 0.7276(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 8.7464(10.8132) | Bit/dim 5.5568(11.9159) | Xent 1.8885(2.0823) | Xent Color 2.1676(2.2383) | Loss 12.6935(24.6307) | Error 0.3100(0.4738) | Error Color 0.6700(0.7873) |Steps 386(355.76) | Grad Norm 18.3099(103.8212) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.5182(10.2870) | Bit/dim 4.9704(10.1646) | Xent 1.8138(2.0166) | Xent Color 2.1314(2.2142) | Loss 11.5138(21.3281) | Error 0.3022(0.4245) | Error Color 0.6000(0.7364) |Steps 356(360.92) | Grad Norm 14.4525(80.3123) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 8.6082(9.8682) | Bit/dim 4.5811(8.7587) | Xent 1.7558(1.9542) | Xent Color 2.1022(2.1889) | Loss 10.9345(18.6717) | Error 0.2756(0.3892) | Error Color 0.6278(0.7121) |Steps 386(366.19) | Grad Norm 13.4106(63.1011) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.2686(9.5735) | Bit/dim 4.1664(7.6068) | Xent 1.7182(1.8965) | Xent Color 2.0705(2.1608) | Loss 9.9140(16.4932) | Error 0.2700(0.3599) | Error Color 0.5889(0.6806) |Steps 374(368.04) | Grad Norm 9.0484(49.1762) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.8782(9.3918) | Bit/dim 3.7622(6.6452) | Xent 1.7104(1.8495) | Xent Color 2.0338(2.1304) | Loss 9.2631(14.6946) | Error 0.2900(0.3392) | Error Color 0.5344(0.6460) |Steps 398(370.99) | Grad Norm 8.0414(38.5382) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 8.6520(9.2694) | Bit/dim 3.3653(5.8283) | Xent 1.7375(1.8180) | Xent Color 2.0034(2.1006) | Loss 8.6042(13.1580) | Error 0.3022(0.3279) | Error Color 0.5533(0.6173) |Steps 374(371.57) | Grad Norm 8.2501(30.6076) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 9.0581(9.2055) | Bit/dim 3.0526(5.1349) | Xent 1.7686(1.8017) | Xent Color 1.9625(2.0678) | Loss 7.8454(11.8688) | Error 0.3089(0.3252) | Error Color 0.5411(0.5919) |Steps 362(375.01) | Grad Norm 6.3195(24.4516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 56.3205, Epoch Time 665.3748(642.6178), Bit/dim 2.9671(best: 5.7409), Xent 1.7650, Xent Color 1.9373. Loss 3.8927, Error 0.2727(best: 0.2505), Error Color 0.4008(best: 0.7276)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.3029(9.2211) | Bit/dim 2.7925(4.5470) | Xent 1.8212(1.8013) | Xent Color 1.9172(2.0341) | Loss 7.5907(11.2149) | Error 0.3511(0.3308) | Error Color 0.4789(0.5642) |Steps 380(377.39) | Grad Norm 5.6834(19.5809) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.5074(9.2559) | Bit/dim 2.6174(4.0578) | Xent 1.8334(1.8090) | Xent Color 1.8795(1.9970) | Loss 7.2930(10.2041) | Error 0.3656(0.3394) | Error Color 0.4844(0.5413) |Steps 392(383.54) | Grad Norm 4.3351(15.6802) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.5868(9.2571) | Bit/dim 2.4789(3.6578) | Xent 1.8511(1.8204) | Xent Color 1.8173(1.9568) | Loss 7.0447(9.3968) | Error 0.3756(0.3494) | Error Color 0.4633(0.5223) |Steps 398(387.65) | Grad Norm 3.2785(12.5222) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.3981(9.2490) | Bit/dim 2.4090(3.3358) | Xent 1.8710(1.8333) | Xent Color 1.7650(1.9131) | Loss 6.8230(8.7422) | Error 0.4067(0.3619) | Error Color 0.4389(0.5022) |Steps 410(390.55) | Grad Norm 2.7683(10.0176) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.3088(9.2946) | Bit/dim 2.3311(3.0782) | Xent 1.8909(1.8464) | Xent Color 1.7012(1.8651) | Loss 6.7738(8.2203) | Error 0.4244(0.3752) | Error Color 0.3956(0.4827) |Steps 398(393.80) | Grad Norm 2.4628(8.0933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 58.4698, Epoch Time 699.9366(644.3373), Bit/dim 2.2406(best: 2.9671), Xent 1.8488, Xent Color 1.5590. Loss 3.0926, Error 0.3303(best: 0.2505), Error Color 0.3206(best: 0.4008)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.8337(9.4025) | Bit/dim 2.2369(2.7060) | Xent 1.8804(1.8628) | Xent Color 1.5552(1.7596) | Loss 6.4895(7.9724) | Error 0.4267(0.3986) | Error Color 0.3611(0.4416) |Steps 392(396.39) | Grad Norm 2.9063(5.6168) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.3353(9.3904) | Bit/dim 2.1969(2.5768) | Xent 1.8212(1.8583) | Xent Color 1.4897(1.7008) | Loss 6.2961(7.5469) | Error 0.4022(0.4032) | Error Color 0.3600(0.4235) |Steps 392(395.56) | Grad Norm 1.7886(4.7524) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.2241(9.3438) | Bit/dim 2.1935(2.4774) | Xent 1.7891(1.8448) | Xent Color 1.4049(1.6344) | Loss 6.1313(7.2127) | Error 0.3756(0.4026) | Error Color 0.3744(0.4062) |Steps 380(396.84) | Grad Norm 1.9544(4.2179) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 9.3156(9.3408) | Bit/dim 2.1735(2.4017) | Xent 1.7253(1.8199) | Xent Color 1.3348(1.5587) | Loss 6.2110(6.9497) | Error 0.3633(0.3967) | Error Color 0.3722(0.3903) |Steps 404(398.06) | Grad Norm 15.3447(4.9751) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.3503(9.3227) | Bit/dim 2.1952(2.3445) | Xent 1.6687(1.7879) | Xent Color 1.2999(1.4933) | Loss 6.2079(6.7436) | Error 0.3622(0.3885) | Error Color 0.3678(0.3875) |Steps 398(399.64) | Grad Norm 38.6834(10.2811) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.1895(9.2982) | Bit/dim 2.1615(2.2988) | Xent 1.6233(1.7444) | Xent Color 1.1964(1.4226) | Loss 6.0430(6.5661) | Error 0.3689(0.3747) | Error Color 0.3122(0.3710) |Steps 404(399.70) | Grad Norm 17.9625(13.0589) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.2173(9.2546) | Bit/dim 2.1560(2.2610) | Xent 1.5278(1.6948) | Xent Color 1.1005(1.3497) | Loss 5.8919(6.4073) | Error 0.3200(0.3614) | Error Color 0.2467(0.3444) |Steps 404(399.37) | Grad Norm 5.8082(13.4260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 59.5171, Epoch Time 693.2465(645.8046), Bit/dim 2.2640(best: 2.2406), Xent 1.4234, Xent Color 1.9062. Loss 3.0964, Error 0.2461(best: 0.2505), Error Color 0.7538(best: 0.3206)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.2798(9.2123) | Bit/dim 2.1903(2.2433) | Xent 1.4021(1.6347) | Xent Color 1.1102(1.3394) | Loss 5.8860(6.7793) | Error 0.2889(0.3472) | Error Color 0.3078(0.3785) |Steps 410(398.98) | Grad Norm 29.6017(26.9942) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.1948(9.2335) | Bit/dim 2.1653(2.2253) | Xent 1.3247(1.5696) | Xent Color 1.0664(1.2790) | Loss 5.9301(6.5533) | Error 0.2567(0.3333) | Error Color 0.2189(0.3610) |Steps 398(399.01) | Grad Norm 21.5778(28.0045) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.2375(9.2225) | Bit/dim 2.1553(2.2092) | Xent 1.2923(1.5015) | Xent Color 0.9834(1.2108) | Loss 5.7690(6.3528) | Error 0.2689(0.3192) | Error Color 0.1867(0.3251) |Steps 398(397.22) | Grad Norm 12.2022(25.2652) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.2675(9.1997) | Bit/dim 2.1806(2.1968) | Xent 1.1451(1.4300) | Xent Color 0.8934(1.1419) | Loss 5.7083(6.1826) | Error 0.2422(0.3059) | Error Color 0.1556(0.2909) |Steps 410(396.99) | Grad Norm 6.2841(21.4340) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.4365(9.2039) | Bit/dim 2.1731(2.1918) | Xent 1.1081(1.3515) | Xent Color 0.8772(1.0705) | Loss 5.6486(6.0551) | Error 0.2400(0.2904) | Error Color 0.2133(0.2649) |Steps 410(397.32) | Grad Norm 17.9451(18.8911) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.0178(9.1807) | Bit/dim 2.3456(2.2110) | Xent 1.0445(1.2710) | Xent Color 3.5289(1.2831) | Loss 7.1341(6.1107) | Error 0.2411(0.2781) | Error Color 0.8167(0.3112) |Steps 398(397.17) | Grad Norm 221.1555(43.6484) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 8.4240(9.1081) | Bit/dim 2.1851(2.2171) | Xent 1.0213(1.2023) | Xent Color 1.1521(1.3301) | Loss 5.6587(6.0729) | Error 0.2378(0.2680) | Error Color 0.4233(0.3840) |Steps 386(395.51) | Grad Norm 21.1805(50.1923) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 56.2796, Epoch Time 682.7431(646.9127), Bit/dim 2.1923(best: 2.2406), Xent 0.9375, Xent Color 1.1285. Loss 2.7088, Error 0.1819(best: 0.2461), Error Color 0.3963(best: 0.3206)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.0486(9.1137) | Bit/dim 2.1386(2.2066) | Xent 1.0514(1.1528) | Xent Color 1.1122(1.2870) | Loss 5.7646(6.3949) | Error 0.2389(0.2613) | Error Color 0.3322(0.3890) |Steps 386(395.26) | Grad Norm 21.7664(44.6060) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 9.0781(9.0854) | Bit/dim 2.1307(2.1890) | Xent 0.9937(1.1160) | Xent Color 1.0594(1.2317) | Loss 5.6478(6.2004) | Error 0.2256(0.2570) | Error Color 0.2622(0.3634) |Steps 374(392.23) | Grad Norm 6.9840(36.8599) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 8.6903(9.0801) | Bit/dim 2.1219(2.1748) | Xent 0.9027(1.0701) | Xent Color 0.9632(1.1737) | Loss 5.4158(6.0382) | Error 0.2267(0.2516) | Error Color 0.2189(0.3317) |Steps 398(390.38) | Grad Norm 6.9025(29.5238) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 9.0085(9.0729) | Bit/dim 2.1371(2.1634) | Xent 0.8316(1.0136) | Xent Color 0.9147(1.1145) | Loss 5.5145(5.9023) | Error 0.2367(0.2427) | Error Color 0.2033(0.3003) |Steps 398(390.75) | Grad Norm 5.4899(23.4956) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 9.0733(9.0749) | Bit/dim 2.1288(2.1568) | Xent 0.8036(0.9578) | Xent Color 0.8903(1.0569) | Loss 5.3305(5.7842) | Error 0.2100(0.2353) | Error Color 0.1833(0.2720) |Steps 398(392.81) | Grad Norm 1.4496(18.5071) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.1608(9.0696) | Bit/dim 2.1329(2.1500) | Xent 0.8242(0.9165) | Xent Color 0.8480(1.0025) | Loss 5.4312(5.6930) | Error 0.2433(0.2295) | Error Color 0.1978(0.2488) |Steps 398(393.81) | Grad Norm 6.1321(15.7732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 58.4108, Epoch Time 680.3121(647.9147), Bit/dim 2.1327(best: 2.1923), Xent 0.6569, Xent Color 0.7227. Loss 2.4776, Error 0.1491(best: 0.1819), Error Color 0.0673(best: 0.3206)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.3671(9.0828) | Bit/dim 2.1458(2.1432) | Xent 0.7493(0.8779) | Xent Color 0.7733(0.9482) | Loss 5.4188(6.1096) | Error 0.1956(0.2234) | Error Color 0.1644(0.2277) |Steps 380(394.25) | Grad Norm 3.7230(13.7361) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.2532(9.1129) | Bit/dim 2.1303(2.1365) | Xent 0.7185(0.8431) | Xent Color 0.7462(0.8976) | Loss 5.4087(5.9158) | Error 0.2044(0.2186) | Error Color 0.1544(0.2108) |Steps 392(394.72) | Grad Norm 11.8162(13.9106) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.1310(9.1211) | Bit/dim 2.1972(2.1344) | Xent 0.6694(0.8059) | Xent Color 1.8534(0.8961) | Loss 5.8877(5.7863) | Error 0.1989(0.2120) | Error Color 0.7133(0.2229) |Steps 386(395.55) | Grad Norm 181.9593(21.9130) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.1896(9.1423) | Bit/dim 2.1070(2.1342) | Xent 0.6988(0.7774) | Xent Color 0.8498(0.9698) | Loss 5.4514(5.7287) | Error 0.1922(0.2065) | Error Color 0.2900(0.2700) |Steps 410(397.03) | Grad Norm 35.1034(35.2757) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 8.9934(9.1178) | Bit/dim 2.0943(2.1257) | Xent 0.7381(0.7718) | Xent Color 0.7221(0.9234) | Loss 5.2661(5.6310) | Error 0.2178(0.2073) | Error Color 0.1667(0.2635) |Steps 398(397.98) | Grad Norm 7.9409(32.4472) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.1849(9.1456) | Bit/dim 2.1249(2.1173) | Xent 0.6998(0.7576) | Xent Color 0.6852(0.8680) | Loss 5.3074(5.5487) | Error 0.2089(0.2066) | Error Color 0.1322(0.2361) |Steps 386(397.60) | Grad Norm 17.6101(28.0697) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 8.9961(9.1362) | Bit/dim 2.0895(2.1112) | Xent 0.7117(0.7345) | Xent Color 0.5958(0.8069) | Loss 5.2630(5.4709) | Error 0.2000(0.2031) | Error Color 0.1022(0.2067) |Steps 392(397.66) | Grad Norm 7.1306(23.6164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 58.4252, Epoch Time 684.8934(649.0241), Bit/dim 2.1003(best: 2.1327), Xent 0.5442, Xent Color 0.5520. Loss 2.3744, Error 0.1356(best: 0.1491), Error Color 0.0488(best: 0.0673)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.4976(9.1581) | Bit/dim 2.0740(2.1047) | Xent 0.6192(0.7113) | Xent Color 0.5778(0.7510) | Loss 5.2071(5.8594) | Error 0.1789(0.1989) | Error Color 0.1111(0.1831) |Steps 398(397.62) | Grad Norm 4.6220(19.6936) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.1783(9.1470) | Bit/dim 2.0683(2.0991) | Xent 0.6639(0.6943) | Xent Color 0.5334(0.6999) | Loss 5.1579(5.6730) | Error 0.2000(0.1969) | Error Color 0.1033(0.1632) |Steps 386(397.85) | Grad Norm 13.5309(17.1127) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.3854(9.1934) | Bit/dim 2.0770(2.0942) | Xent 0.6297(0.6812) | Xent Color 0.5367(0.6554) | Loss 5.0213(5.5425) | Error 0.1767(0.1951) | Error Color 0.1111(0.1482) |Steps 410(400.68) | Grad Norm 18.6077(16.0300) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 8.9920(9.2145) | Bit/dim 2.0498(2.0860) | Xent 0.6324(0.6677) | Xent Color 0.4898(0.6114) | Loss 5.0710(5.4306) | Error 0.1933(0.1925) | Error Color 0.1011(0.1333) |Steps 380(400.97) | Grad Norm 20.3618(14.5285) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.1303(9.2138) | Bit/dim 2.0643(2.0804) | Xent 0.5816(0.6530) | Xent Color 0.4627(0.5756) | Loss 5.0913(5.3380) | Error 0.1711(0.1886) | Error Color 0.0856(0.1250) |Steps 398(401.53) | Grad Norm 21.1734(16.1868) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 8.7480(9.2018) | Bit/dim 2.1240(2.0865) | Xent 0.6025(0.6435) | Xent Color 1.0194(0.7335) | Loss 5.2803(5.3742) | Error 0.1900(0.1872) | Error Color 0.4511(0.2121) |Steps 386(401.91) | Grad Norm 69.1569(37.0607) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 58.2900, Epoch Time 688.9230(650.2211), Bit/dim 2.0521(best: 2.1003), Xent 0.5416, Xent Color 0.5158. Loss 2.3165, Error 0.1342(best: 0.1356), Error Color 0.0814(best: 0.0488)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.0421(9.1776) | Bit/dim 2.0214(2.0785) | Xent 0.6526(0.6541) | Xent Color 0.6640(0.7260) | Loss 4.9389(5.8474) | Error 0.1967(0.1908) | Error Color 0.2389(0.2259) |Steps 398(399.07) | Grad Norm 18.3728(34.4274) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.1267(9.1758) | Bit/dim 2.0205(2.0686) | Xent 0.5873(0.6525) | Xent Color 0.5537(0.6929) | Loss 5.0619(5.6462) | Error 0.1767(0.1919) | Error Color 0.1367(0.2085) |Steps 410(397.63) | Grad Norm 5.3920(28.9231) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.3458(9.1764) | Bit/dim 2.0315(2.0593) | Xent 0.6325(0.6389) | Xent Color 0.5124(0.6479) | Loss 5.0365(5.4818) | Error 0.1733(0.1874) | Error Color 0.1178(0.1860) |Steps 410(397.13) | Grad Norm 7.4894(23.7651) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.6396(9.2158) | Bit/dim 2.0319(2.0524) | Xent 0.5694(0.6248) | Xent Color 0.4263(0.5993) | Loss 4.9947(5.3627) | Error 0.1711(0.1858) | Error Color 0.0900(0.1637) |Steps 386(398.31) | Grad Norm 4.3979(20.3959) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.6134(9.2644) | Bit/dim 2.0061(2.0445) | Xent 0.6008(0.6181) | Xent Color 0.4070(0.5543) | Loss 5.0409(5.2652) | Error 0.1733(0.1852) | Error Color 0.0700(0.1455) |Steps 416(400.53) | Grad Norm 7.3023(19.8306) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.3987(9.2775) | Bit/dim 2.0716(2.0400) | Xent 0.5659(0.6113) | Xent Color 1.6075(0.5762) | Loss 5.5383(5.2143) | Error 0.1800(0.1852) | Error Color 0.5978(0.1624) |Steps 398(401.45) | Grad Norm 188.7282(31.2213) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.3774(9.2634) | Bit/dim 2.0177(2.0431) | Xent 0.6547(0.6130) | Xent Color 0.6326(0.6700) | Loss 5.1255(5.2305) | Error 0.1956(0.1861) | Error Color 0.2389(0.2069) |Steps 428(402.83) | Grad Norm 34.2234(40.9269) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 59.2338, Epoch Time 693.7955(651.5283), Bit/dim 2.0335(best: 2.0521), Xent 0.4721, Xent Color 0.4293. Loss 2.2588, Error 0.1272(best: 0.1342), Error Color 0.0706(best: 0.0488)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.1175(9.2652) | Bit/dim 2.0118(2.0358) | Xent 0.6207(0.6115) | Xent Color 0.5024(0.6370) | Loss 4.9814(5.6587) | Error 0.2011(0.1864) | Error Color 0.1211(0.1959) |Steps 404(405.21) | Grad Norm 13.5477(35.2810) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.3311(9.2977) | Bit/dim 2.0177(2.0298) | Xent 0.5034(0.5985) | Xent Color 0.4011(0.5832) | Loss 4.8754(5.4750) | Error 0.1511(0.1830) | Error Color 0.0711(0.1701) |Steps 392(404.25) | Grad Norm 9.9744(29.2786) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.3820(9.3033) | Bit/dim 1.9804(2.0203) | Xent 0.5787(0.5908) | Xent Color 0.3481(0.5268) | Loss 4.9062(5.3290) | Error 0.1667(0.1808) | Error Color 0.0711(0.1462) |Steps 398(404.15) | Grad Norm 6.4487(23.9396) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.4016(9.2804) | Bit/dim 2.0109(2.0139) | Xent 0.5300(0.5848) | Xent Color 0.3004(0.4730) | Loss 4.9018(5.2058) | Error 0.1733(0.1800) | Error Color 0.0578(0.1238) |Steps 410(403.90) | Grad Norm 2.9152(19.0616) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.2808(9.2919) | Bit/dim 1.9759(2.0072) | Xent 0.5669(0.5805) | Xent Color 0.2620(0.4231) | Loss 4.8251(5.1098) | Error 0.1611(0.1784) | Error Color 0.0489(0.1049) |Steps 398(404.01) | Grad Norm 4.1595(15.3387) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.1067(9.3131) | Bit/dim 2.0043(2.0008) | Xent 0.5433(0.5724) | Xent Color 0.2412(0.3791) | Loss 4.8398(5.0362) | Error 0.1678(0.1762) | Error Color 0.0500(0.0897) |Steps 392(402.95) | Grad Norm 9.8235(13.2086) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.1863(9.3236) | Bit/dim 1.9692(1.9937) | Xent 0.5733(0.5639) | Xent Color 0.2548(0.3415) | Loss 4.8922(4.9771) | Error 0.1811(0.1738) | Error Color 0.0556(0.0781) |Steps 410(403.73) | Grad Norm 18.1518(12.7245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 60.1504, Epoch Time 698.3476(652.9329), Bit/dim 1.9800(best: 2.0335), Xent 0.4123, Xent Color 0.1987. Loss 2.1327, Error 0.1179(best: 0.1272), Error Color 0.0270(best: 0.0488)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.3365(9.3470) | Bit/dim 1.9685(1.9864) | Xent 0.5593(0.5627) | Xent Color 0.2049(0.3173) | Loss 4.7708(5.3465) | Error 0.1789(0.1743) | Error Color 0.0378(0.0741) |Steps 422(405.01) | Grad Norm 16.5495(14.6701) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.1801(9.3148) | Bit/dim 1.9531(1.9781) | Xent 0.4475(0.5532) | Xent Color 0.1951(0.2891) | Loss 4.6805(5.1884) | Error 0.1344(0.1704) | Error Color 0.0322(0.0653) |Steps 404(404.76) | Grad Norm 2.2999(13.4190) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.0608(9.2959) | Bit/dim 1.9751(1.9729) | Xent 0.4973(0.5477) | Xent Color 0.2947(0.2681) | Loss 4.8043(5.0731) | Error 0.1644(0.1687) | Error Color 0.1022(0.0603) |Steps 398(404.32) | Grad Norm 46.8313(14.6438) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.3397(9.2762) | Bit/dim 2.4493(2.0607) | Xent 0.8559(0.5722) | Xent Color 1.2733(1.0983) | Loss 6.2971(5.5430) | Error 0.2589(0.1783) | Error Color 0.4589(0.1973) |Steps 386(404.71) | Grad Norm 30.9608(38.7005) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.1332(9.2429) | Bit/dim 2.0429(2.0807) | Xent 0.6606(0.6602) | Xent Color 0.8560(1.0120) | Loss 5.1254(5.4752) | Error 0.2144(0.2087) | Error Color 0.3267(0.2658) |Steps 368(395.76) | Grad Norm 5.8565(25.8531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 57.3103, Epoch Time 690.2290(654.0518), Bit/dim 2.0202(best: 1.9800), Xent 0.4494, Xent Color 0.5726. Loss 2.2757, Error 0.1250(best: 0.1179), Error Color 0.1202(best: 0.0270)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 8.9988(9.2193) | Bit/dim 2.0274(2.0630) | Xent 0.6095(0.6498) | Xent Color 0.7046(0.9415) | Loss 5.0855(5.8493) | Error 0.1811(0.2039) | Error Color 0.2278(0.2651) |Steps 386(393.59) | Grad Norm 14.1709(21.2140) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.2155(9.1585) | Bit/dim 2.0052(2.0481) | Xent 0.5716(0.6357) | Xent Color 0.5750(0.8596) | Loss 5.0112(5.6216) | Error 0.1778(0.1994) | Error Color 0.1800(0.2481) |Steps 398(391.73) | Grad Norm 4.6413(17.5561) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.2847(9.1652) | Bit/dim 2.0055(2.0336) | Xent 0.5764(0.6144) | Xent Color 0.5022(0.7775) | Loss 4.9231(5.4338) | Error 0.1767(0.1941) | Error Color 0.1433(0.2268) |Steps 398(390.72) | Grad Norm 4.6544(14.9138) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 8.9926(9.1379) | Bit/dim 1.9735(2.0194) | Xent 0.5519(0.5994) | Xent Color 0.4360(0.6945) | Loss 4.8719(5.2773) | Error 0.1756(0.1894) | Error Color 0.1133(0.2018) |Steps 410(389.80) | Grad Norm 6.7490(12.4926) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.2138(9.1568) | Bit/dim 1.9867(2.0082) | Xent 0.4947(0.5878) | Xent Color 0.3603(0.6147) | Loss 4.8841(5.1662) | Error 0.1700(0.1849) | Error Color 0.0867(0.1770) |Steps 380(390.37) | Grad Norm 8.9762(11.0462) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.3814(9.1591) | Bit/dim 1.9751(1.9981) | Xent 0.6149(0.5799) | Xent Color 0.3022(0.5419) | Loss 4.8244(5.0640) | Error 0.2000(0.1821) | Error Color 0.0833(0.1546) |Steps 380(390.84) | Grad Norm 3.4310(10.7855) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 8.8925(9.1496) | Bit/dim 1.9477(1.9881) | Xent 0.4961(0.5662) | Xent Color 0.2664(0.4722) | Loss 4.6778(4.9705) | Error 0.1533(0.1770) | Error Color 0.0589(0.1313) |Steps 386(389.81) | Grad Norm 2.8006(8.9349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 59.4917, Epoch Time 685.5579(654.9969), Bit/dim 1.9677(best: 1.9800), Xent 0.3906, Xent Color 0.1683. Loss 2.1074, Error 0.1087(best: 0.1179), Error Color 0.0154(best: 0.0270)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 9.0580(9.2071) | Bit/dim 1.9735(1.9798) | Xent 0.5705(0.5584) | Xent Color 0.2107(0.4100) | Loss 4.6944(5.3241) | Error 0.1856(0.1737) | Error Color 0.0456(0.1103) |Steps 398(394.36) | Grad Norm 4.3140(8.4157) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 8.9847(9.2187) | Bit/dim 1.9413(1.9733) | Xent 0.4064(0.5474) | Xent Color 0.1976(0.3564) | Loss 4.6136(5.1557) | Error 0.1333(0.1701) | Error Color 0.0522(0.0936) |Steps 386(394.64) | Grad Norm 5.4178(8.1258) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 8.9939(9.2259) | Bit/dim 1.9678(1.9655) | Xent 0.4883(0.5445) | Xent Color 0.1657(0.3104) | Loss 4.6417(5.0215) | Error 0.1478(0.1689) | Error Color 0.0389(0.0788) |Steps 404(394.58) | Grad Norm 2.5698(7.3316) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 8.9708(9.1732) | Bit/dim 2.4149(2.0343) | Xent 0.5152(0.5407) | Xent Color 4.9209(1.0225) | Loss 7.5635(5.3931) | Error 0.1578(0.1660) | Error Color 0.7000(0.1565) |Steps 380(392.31) | Grad Norm 104.0784(35.9025) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 9.6661(9.2348) | Bit/dim 2.1050(2.0804) | Xent 0.5687(0.5824) | Xent Color 1.0158(1.1107) | Loss 5.3379(5.4929) | Error 0.1567(0.1815) | Error Color 0.3833(0.2363) |Steps 398(395.76) | Grad Norm 9.7473(34.5612) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.2601(9.3183) | Bit/dim 2.0384(2.0753) | Xent 0.6076(0.5765) | Xent Color 0.6444(1.0238) | Loss 5.1255(5.4168) | Error 0.1922(0.1798) | Error Color 0.2122(0.2517) |Steps 416(401.56) | Grad Norm 5.6069(27.6095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 60.5872, Epoch Time 698.4562(656.3007), Bit/dim 1.9956(best: 1.9677), Xent 0.3892, Xent Color 0.3450. Loss 2.1792, Error 0.1137(best: 0.1087), Error Color 0.0581(best: 0.0154)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.1753(9.3409) | Bit/dim 1.9716(2.0550) | Xent 0.5263(0.5775) | Xent Color 0.4385(0.8908) | Loss 4.8244(5.8307) | Error 0.1456(0.1801) | Error Color 0.1333(0.2317) |Steps 410(402.21) | Grad Norm 4.7581(21.6658) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 8.9232(9.3296) | Bit/dim 1.9859(2.0304) | Xent 0.5317(0.5680) | Xent Color 0.3602(0.7647) | Loss 4.8201(5.5599) | Error 0.1600(0.1765) | Error Color 0.1144(0.2042) |Steps 386(399.64) | Grad Norm 2.7839(16.9207) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 9.4381(9.3233) | Bit/dim 1.9419(2.0093) | Xent 0.5425(0.5588) | Xent Color 0.2912(0.6512) | Loss 4.6809(5.3513) | Error 0.1800(0.1741) | Error Color 0.0733(0.1749) |Steps 428(401.39) | Grad Norm 1.9010(13.1341) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.2605(9.3189) | Bit/dim 1.9280(1.9927) | Xent 0.5324(0.5520) | Xent Color 0.2411(0.5508) | Loss 4.6671(5.1820) | Error 0.1622(0.1718) | Error Color 0.0456(0.1462) |Steps 404(401.63) | Grad Norm 1.3347(10.3206) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 9.5083(9.3117) | Bit/dim 1.9459(1.9780) | Xent 0.4881(0.5445) | Xent Color 0.2257(0.4657) | Loss 4.6983(5.0381) | Error 0.1633(0.1694) | Error Color 0.0644(0.1218) |Steps 386(397.39) | Grad Norm 4.7899(8.1990) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 8.7849(9.2839) | Bit/dim 1.9289(1.9650) | Xent 0.5331(0.5355) | Xent Color 0.1667(0.3950) | Loss 4.5468(4.9302) | Error 0.1689(0.1663) | Error Color 0.0378(0.1024) |Steps 392(399.01) | Grad Norm 1.3875(6.6239) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 8.8762(9.2619) | Bit/dim 1.9350(1.9535) | Xent 0.5239(0.5317) | Xent Color 0.1550(0.3347) | Loss 4.5545(4.8494) | Error 0.1567(0.1634) | Error Color 0.0300(0.0854) |Steps 392(400.10) | Grad Norm 1.8388(5.4808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 59.4182, Epoch Time 694.1282(657.4355), Bit/dim 1.9233(best: 1.9677), Xent 0.3703, Xent Color 0.0884. Loss 2.0379, Error 0.1060(best: 0.1087), Error Color 0.0074(best: 0.0154)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.1331(9.2421) | Bit/dim 1.9121(1.9434) | Xent 0.4766(0.5268) | Xent Color 0.1324(0.2859) | Loss 4.5740(5.2395) | Error 0.1544(0.1629) | Error Color 0.0311(0.0716) |Steps 416(398.49) | Grad Norm 2.4211(4.9248) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.2689(9.2267) | Bit/dim 1.8995(1.9345) | Xent 0.4669(0.5155) | Xent Color 0.1264(0.2449) | Loss 4.5161(5.0599) | Error 0.1478(0.1594) | Error Color 0.0267(0.0597) |Steps 386(398.47) | Grad Norm 1.6162(4.2843) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.1528(9.1616) | Bit/dim 1.8860(1.9262) | Xent 0.5041(0.5164) | Xent Color 0.1032(0.2110) | Loss 4.5595(4.9317) | Error 0.1544(0.1595) | Error Color 0.0189(0.0503) |Steps 410(396.88) | Grad Norm 2.2888(3.6987) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.4296(9.1562) | Bit/dim 1.9116(1.9177) | Xent 0.5907(0.5096) | Xent Color 0.0984(0.1842) | Loss 4.5353(4.8190) | Error 0.1867(0.1575) | Error Color 0.0156(0.0426) |Steps 404(394.43) | Grad Norm 2.2592(3.3388) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.3289(9.2100) | Bit/dim 1.9013(1.9116) | Xent 0.5176(0.5101) | Xent Color 0.0892(0.1618) | Loss 4.4920(4.7389) | Error 0.1722(0.1583) | Error Color 0.0178(0.0366) |Steps 410(394.32) | Grad Norm 2.4331(3.2525) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 8.6877(9.2306) | Bit/dim 1.8966(1.9046) | Xent 0.4333(0.5056) | Xent Color 0.0804(0.1425) | Loss 4.4845(4.6830) | Error 0.1544(0.1571) | Error Color 0.0144(0.0316) |Steps 374(393.79) | Grad Norm 2.6922(3.3415) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.6180(9.2365) | Bit/dim 1.8723(1.8966) | Xent 0.4593(0.5059) | Xent Color 0.0791(0.1271) | Loss 4.4568(4.6330) | Error 0.1533(0.1576) | Error Color 0.0133(0.0271) |Steps 422(394.13) | Grad Norm 2.4341(3.2392) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 60.2159, Epoch Time 690.6332(658.4315), Bit/dim 1.8827(best: 1.9233), Xent 0.3528, Xent Color 0.0396. Loss 1.9808, Error 0.1034(best: 0.1060), Error Color 0.0018(best: 0.0074)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 9.1951(9.2351) | Bit/dim 1.8581(1.8903) | Xent 0.4776(0.5030) | Xent Color 0.0797(0.1131) | Loss 4.4296(5.0144) | Error 0.1467(0.1561) | Error Color 0.0178(0.0235) |Steps 374(394.95) | Grad Norm 3.0004(3.0318) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 9.1333(9.2269) | Bit/dim 1.8585(1.8839) | Xent 0.4767(0.5015) | Xent Color 0.0775(0.1023) | Loss 4.3829(4.8677) | Error 0.1422(0.1559) | Error Color 0.0156(0.0213) |Steps 404(395.49) | Grad Norm 1.8587(3.2153) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.3493(9.1932) | Bit/dim 1.8645(1.8780) | Xent 0.4461(0.4942) | Xent Color 0.0722(0.0955) | Loss 4.5036(4.7565) | Error 0.1389(0.1532) | Error Color 0.0189(0.0205) |Steps 404(396.27) | Grad Norm 15.4566(4.9808) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.2186(9.2062) | Bit/dim 1.8496(1.8695) | Xent 0.4310(0.4939) | Xent Color 0.1110(0.0930) | Loss 4.4358(4.6811) | Error 0.1367(0.1533) | Error Color 0.0311(0.0211) |Steps 380(396.24) | Grad Norm 22.5276(7.6563) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.4098(9.2179) | Bit/dim 1.8359(1.8641) | Xent 0.5003(0.4872) | Xent Color 0.1394(0.1013) | Loss 4.5137(4.6263) | Error 0.1578(0.1518) | Error Color 0.0489(0.0256) |Steps 422(398.29) | Grad Norm 32.5783(12.9000) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.5128(9.2618) | Bit/dim 1.8537(1.8580) | Xent 0.4960(0.4851) | Xent Color 0.0626(0.0963) | Loss 4.4380(4.5739) | Error 0.1611(0.1514) | Error Color 0.0111(0.0245) |Steps 356(398.37) | Grad Norm 7.4047(13.4346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 60.3996, Epoch Time 693.5039(659.4836), Bit/dim 1.8391(best: 1.8827), Xent 0.3368, Xent Color 0.0369. Loss 1.9325, Error 0.0956(best: 0.1034), Error Color 0.0049(best: 0.0018)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.5507(9.3023) | Bit/dim 1.8388(1.8522) | Xent 0.5029(0.4878) | Xent Color 0.0545(0.0906) | Loss 4.3691(5.0191) | Error 0.1544(0.1513) | Error Color 0.0133(0.0232) |Steps 386(399.48) | Grad Norm 8.1809(13.2656) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.2415(9.3178) | Bit/dim 1.8029(1.8425) | Xent 0.4454(0.4839) | Xent Color 0.0705(0.0823) | Loss 4.3580(4.8493) | Error 0.1489(0.1496) | Error Color 0.0167(0.0201) |Steps 398(399.67) | Grad Norm 12.4153(12.0153) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.5521(9.3607) | Bit/dim 1.8257(1.8353) | Xent 0.4333(0.4840) | Xent Color 0.0473(0.0750) | Loss 4.4010(4.7247) | Error 0.1367(0.1503) | Error Color 0.0122(0.0181) |Steps 410(399.44) | Grad Norm 5.3205(10.9223) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.1332(9.3680) | Bit/dim 1.7884(1.8275) | Xent 0.5021(0.4813) | Xent Color 0.0474(0.0681) | Loss 4.2326(4.6256) | Error 0.1656(0.1503) | Error Color 0.0078(0.0159) |Steps 380(400.11) | Grad Norm 6.8033(9.8538) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.0878(9.3172) | Bit/dim 1.7757(1.8203) | Xent 0.4223(0.4713) | Xent Color 0.0438(0.0616) | Loss 4.4174(4.5451) | Error 0.1278(0.1472) | Error Color 0.0100(0.0137) |Steps 416(401.53) | Grad Norm 2.7827(8.4118) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.7871(9.3611) | Bit/dim 1.7706(1.8125) | Xent 0.5014(0.4726) | Xent Color 0.0467(0.0570) | Loss 4.3556(4.4922) | Error 0.1600(0.1476) | Error Color 0.0078(0.0123) |Steps 422(401.54) | Grad Norm 3.8672(7.8650) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.4766(9.3576) | Bit/dim 1.7516(1.8011) | Xent 0.4711(0.4714) | Xent Color 0.0476(0.0544) | Loss 4.1922(4.4348) | Error 0.1456(0.1467) | Error Color 0.0111(0.0116) |Steps 392(402.72) | Grad Norm 4.3378(7.8181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 60.5850, Epoch Time 702.2885(660.7678), Bit/dim 1.7732(best: 1.8391), Xent 0.3334, Xent Color 0.0203. Loss 1.8616, Error 0.0967(best: 0.0956), Error Color 0.0016(best: 0.0018)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 8.8706(9.3522) | Bit/dim 2.3940(1.8230) | Xent 0.6367(0.4748) | Xent Color 13.9918(0.8775) | Loss 11.7723(5.2566) | Error 0.2111(0.1494) | Error Color 0.8078(0.0652) |Steps 404(403.46) | Grad Norm 209.8615(31.2137) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.5022(9.5539) | Bit/dim 2.1491(1.9321) | Xent 0.7644(0.5509) | Xent Color 0.9349(1.1235) | Loss 5.6373(5.4928) | Error 0.2589(0.1748) | Error Color 0.3711(0.1733) |Steps 434(414.22) | Grad Norm 11.1003(31.1842) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 10.8993(9.7180) | Bit/dim 2.0039(1.9650) | Xent 0.5867(0.5843) | Xent Color 0.7582(1.0506) | Loss 5.3871(5.4879) | Error 0.1800(0.1870) | Error Color 0.2911(0.2156) |Steps 464(424.22) | Grad Norm 5.7508(24.7882) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 10.4058(9.9655) | Bit/dim 1.9587(1.9712) | Xent 0.5112(0.5794) | Xent Color 0.5466(0.9367) | Loss 4.9545(5.4164) | Error 0.1533(0.1848) | Error Color 0.1878(0.2183) |Steps 410(434.97) | Grad Norm 2.0812(19.5478) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 10.2343(10.0039) | Bit/dim 1.9500(1.9646) | Xent 0.5621(0.5801) | Xent Color 0.3926(0.8072) | Loss 4.9525(5.2940) | Error 0.1789(0.1841) | Error Color 0.1167(0.1985) |Steps 446(437.28) | Grad Norm 2.7927(15.2241) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 10.2409(10.0999) | Bit/dim 1.9209(1.9546) | Xent 0.5024(0.5666) | Xent Color 0.2963(0.6851) | Loss 4.8259(5.1811) | Error 0.1633(0.1791) | Error Color 0.0900(0.1728) |Steps 428(440.03) | Grad Norm 2.3262(11.8623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 64.9116, Epoch Time 760.2350(663.7518), Bit/dim 1.9188(best: 1.7732), Xent 0.3765, Xent Color 0.1533. Loss 2.0513, Error 0.1083(best: 0.0956), Error Color 0.0155(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.9086(10.1474) | Bit/dim 1.9103(1.9444) | Xent 0.4924(0.5553) | Xent Color 0.2291(0.5726) | Loss 4.7811(5.6540) | Error 0.1578(0.1756) | Error Color 0.0544(0.1445) |Steps 458(442.54) | Grad Norm 1.9015(9.3383) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 10.5137(10.2259) | Bit/dim 1.9131(1.9337) | Xent 0.4428(0.5398) | Xent Color 0.1840(0.4741) | Loss 4.7115(5.4136) | Error 0.1500(0.1696) | Error Color 0.0400(0.1181) |Steps 434(443.22) | Grad Norm 2.5506(7.3855) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 10.0365(10.2301) | Bit/dim 1.8825(1.9230) | Xent 0.4224(0.5280) | Xent Color 0.1478(0.3940) | Loss 4.5686(5.2134) | Error 0.1422(0.1662) | Error Color 0.0333(0.0971) |Steps 446(444.72) | Grad Norm 1.4255(5.9839) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 10.5151(10.2494) | Bit/dim 1.8614(1.9105) | Xent 0.4992(0.5225) | Xent Color 0.1499(0.3287) | Loss 4.6884(5.0619) | Error 0.1611(0.1647) | Error Color 0.0344(0.0795) |Steps 446(443.06) | Grad Norm 2.3404(5.2601) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 10.2545(10.1861) | Bit/dim 1.8702(1.8887) | Xent 0.5098(0.5090) | Xent Color 0.1073(0.2347) | Loss 4.5638(4.8391) | Error 0.1644(0.1619) | Error Color 0.0222(0.0553) |Steps 434(436.81) | Grad Norm 5.0375(4.2929) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.9721(10.1727) | Bit/dim 1.8543(1.8761) | Xent 0.4923(0.5019) | Xent Color 0.1096(0.2015) | Loss 4.6139(4.7637) | Error 0.1500(0.1592) | Error Color 0.0256(0.0467) |Steps 434(434.82) | Grad Norm 2.6090(3.8197) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 65.0247, Epoch Time 761.5243(666.6850), Bit/dim 1.8384(best: 1.7732), Xent 0.3397, Xent Color 0.0553. Loss 1.9371, Error 0.1006(best: 0.0956), Error Color 0.0036(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 10.2125(10.1219) | Bit/dim 1.8046(1.8628) | Xent 0.5102(0.4992) | Xent Color 0.1100(0.1756) | Loss 4.4979(5.2161) | Error 0.1700(0.1577) | Error Color 0.0256(0.0399) |Steps 446(432.19) | Grad Norm 5.5475(3.9158) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 9.7030(10.0755) | Bit/dim 1.8201(1.8526) | Xent 0.4638(0.4944) | Xent Color 0.0881(0.1549) | Loss 4.4895(5.0257) | Error 0.1467(0.1556) | Error Color 0.0200(0.0348) |Steps 428(431.18) | Grad Norm 4.0913(3.9991) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 10.0579(10.0372) | Bit/dim 1.7960(1.8405) | Xent 0.4122(0.4927) | Xent Color 0.0995(0.1391) | Loss 4.4575(4.8864) | Error 0.1344(0.1558) | Error Color 0.0267(0.0317) |Steps 440(429.72) | Grad Norm 7.4179(3.9647) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.4293(10.0050) | Bit/dim 1.8021(1.8286) | Xent 0.3720(0.4819) | Xent Color 0.0828(0.1251) | Loss 4.3590(4.7669) | Error 0.1189(0.1530) | Error Color 0.0133(0.0280) |Steps 416(427.34) | Grad Norm 6.6973(3.9358) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 10.0980(9.9768) | Bit/dim 1.7799(1.8152) | Xent 0.4601(0.4738) | Xent Color 0.0915(0.1139) | Loss 4.4507(4.6745) | Error 0.1289(0.1507) | Error Color 0.0256(0.0254) |Steps 434(425.92) | Grad Norm 6.4710(4.4206) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 10.0151(9.9451) | Bit/dim 1.7620(1.8050) | Xent 0.4626(0.4761) | Xent Color 0.1402(0.1107) | Loss 4.4164(4.6080) | Error 0.1544(0.1501) | Error Color 0.0456(0.0263) |Steps 440(427.79) | Grad Norm 22.5196(7.1421) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.8601(9.9091) | Bit/dim 1.7576(1.7915) | Xent 0.4428(0.4694) | Xent Color 0.0633(0.1015) | Loss 4.3560(4.5407) | Error 0.1444(0.1470) | Error Color 0.0122(0.0236) |Steps 428(426.08) | Grad Norm 3.6299(6.9477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 63.4321, Epoch Time 739.3576(668.8652), Bit/dim 1.7572(best: 1.7732), Xent 0.3145, Xent Color 0.0337. Loss 1.8443, Error 0.0922(best: 0.0956), Error Color 0.0031(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 10.1018(9.8566) | Bit/dim 1.7277(1.7792) | Xent 0.3907(0.4656) | Xent Color 0.0512(0.0927) | Loss 4.3635(4.9592) | Error 0.1278(0.1463) | Error Color 0.0089(0.0211) |Steps 440(426.81) | Grad Norm 3.1188(6.4217) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 9.8084(9.8599) | Bit/dim 1.7333(1.7658) | Xent 0.4703(0.4610) | Xent Color 0.0541(0.0845) | Loss 4.3146(4.7884) | Error 0.1400(0.1452) | Error Color 0.0067(0.0189) |Steps 410(425.11) | Grad Norm 3.3470(5.8021) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 10.0215(9.8794) | Bit/dim 1.7061(1.7551) | Xent 0.4409(0.4577) | Xent Color 0.0617(0.0784) | Loss 4.2553(4.6625) | Error 0.1356(0.1443) | Error Color 0.0133(0.0173) |Steps 440(426.39) | Grad Norm 4.1421(6.1634) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 9.9785(9.9062) | Bit/dim 1.6999(1.7428) | Xent 0.4132(0.4536) | Xent Color 0.0538(0.0730) | Loss 4.2504(4.5550) | Error 0.1278(0.1421) | Error Color 0.0078(0.0159) |Steps 434(424.99) | Grad Norm 4.6533(6.1945) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 10.4005(9.9014) | Bit/dim 1.6516(1.7292) | Xent 0.4786(0.4498) | Xent Color 0.0522(0.0693) | Loss 4.2004(4.4650) | Error 0.1456(0.1411) | Error Color 0.0078(0.0152) |Steps 416(424.64) | Grad Norm 5.5342(7.1710) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.5999(9.8819) | Bit/dim 2.8633(1.8622) | Xent 0.5516(0.4744) | Xent Color 2.0808(1.3810) | Loss 7.5009(5.2694) | Error 0.1578(0.1501) | Error Color 0.5811(0.1494) |Steps 434(426.15) | Grad Norm 39.6812(38.3547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 63.8672, Epoch Time 738.8370(670.9643), Bit/dim 2.1225(best: 1.7572), Xent 0.6071, Xent Color 1.1399. Loss 2.5593, Error 0.1720(best: 0.0922), Error Color 0.3536(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 9.9957(9.9020) | Bit/dim 2.0769(1.9558) | Xent 0.8404(0.6093) | Xent Color 1.0976(1.4534) | Loss 5.5782(6.0718) | Error 0.2711(0.1962) | Error Color 0.4367(0.2471) |Steps 446(429.80) | Grad Norm 9.0423(33.6840) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.5051(9.7379) | Bit/dim 1.9360(1.9643) | Xent 0.5639(0.6198) | Xent Color 0.6543(1.2860) | Loss 4.8178(5.8182) | Error 0.1867(0.1999) | Error Color 0.2322(0.2648) |Steps 386(424.21) | Grad Norm 3.7114(26.5692) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 9.9217(9.6781) | Bit/dim 1.8864(1.9474) | Xent 0.5417(0.6138) | Xent Color 0.5070(1.0935) | Loss 4.7948(5.5471) | Error 0.1700(0.1979) | Error Color 0.1833(0.2490) |Steps 446(418.77) | Grad Norm 2.9307(20.6358) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.4004(9.6160) | Bit/dim 1.8528(1.9248) | Xent 0.5441(0.5949) | Xent Color 0.3800(0.9182) | Loss 4.6349(5.3115) | Error 0.1756(0.1914) | Error Color 0.1367(0.2239) |Steps 410(415.80) | Grad Norm 3.2553(16.0200) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 9.9706(9.6277) | Bit/dim 1.7974(1.8992) | Xent 0.4680(0.5768) | Xent Color 0.2790(0.7664) | Loss 4.5024(5.1180) | Error 0.1500(0.1850) | Error Color 0.0833(0.1940) |Steps 392(413.80) | Grad Norm 1.6445(12.4992) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 9.3517(9.5685) | Bit/dim 1.8017(1.8735) | Xent 0.4650(0.5532) | Xent Color 0.2466(0.6356) | Loss 4.4397(4.9415) | Error 0.1456(0.1770) | Error Color 0.0756(0.1643) |Steps 386(411.12) | Grad Norm 1.8923(9.7815) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.8212(9.5825) | Bit/dim 1.7570(1.8478) | Xent 0.4675(0.5313) | Xent Color 0.1810(0.5209) | Loss 4.2440(4.7920) | Error 0.1589(0.1705) | Error Color 0.0500(0.1359) |Steps 398(410.17) | Grad Norm 1.3652(7.7039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 59.1331, Epoch Time 710.1804(672.1408), Bit/dim 1.7718(best: 1.7572), Xent 0.3313, Xent Color 0.1016. Loss 1.8800, Error 0.0995(best: 0.0922), Error Color 0.0110(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 9.5420(9.5190) | Bit/dim 1.7448(1.8251) | Xent 0.4271(0.5089) | Xent Color 0.1633(0.4277) | Loss 4.3083(5.1140) | Error 0.1411(0.1635) | Error Color 0.0411(0.1115) |Steps 404(408.54) | Grad Norm 2.7019(6.4093) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 8.9240(9.5031) | Bit/dim 1.7368(1.8031) | Xent 0.4822(0.4968) | Xent Color 0.1158(0.3510) | Loss 4.2686(4.8976) | Error 0.1522(0.1598) | Error Color 0.0256(0.0908) |Steps 404(409.98) | Grad Norm 1.6229(5.4453) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.7167(9.4905) | Bit/dim 1.7194(1.7824) | Xent 0.4848(0.4902) | Xent Color 0.1047(0.2920) | Loss 4.2361(4.7246) | Error 0.1533(0.1569) | Error Color 0.0156(0.0749) |Steps 410(407.95) | Grad Norm 4.3841(4.9199) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 9.5368(9.5286) | Bit/dim 1.7234(1.7648) | Xent 0.4676(0.4790) | Xent Color 0.0956(0.2424) | Loss 4.2610(4.5834) | Error 0.1444(0.1529) | Error Color 0.0200(0.0615) |Steps 404(408.84) | Grad Norm 2.8683(4.2619) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 9.2896(9.5295) | Bit/dim 1.7079(1.7474) | Xent 0.4348(0.4709) | Xent Color 0.1113(0.2044) | Loss 4.1287(4.4705) | Error 0.1422(0.1487) | Error Color 0.0256(0.0509) |Steps 410(406.87) | Grad Norm 2.6981(3.6780) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 9.9840(9.5512) | Bit/dim 1.6715(1.7295) | Xent 0.4561(0.4619) | Xent Color 0.0719(0.1737) | Loss 4.1131(4.3813) | Error 0.1400(0.1461) | Error Color 0.0167(0.0423) |Steps 386(404.49) | Grad Norm 2.7342(3.2996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 60.0074, Epoch Time 710.8872(673.3032), Bit/dim 1.6773(best: 1.7572), Xent 0.2860, Xent Color 0.0340. Loss 1.7573, Error 0.0849(best: 0.0922), Error Color 0.0029(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.7874(9.5671) | Bit/dim 1.6759(1.7145) | Xent 0.4245(0.4592) | Xent Color 0.0706(0.1490) | Loss 4.1578(4.8493) | Error 0.1378(0.1438) | Error Color 0.0122(0.0359) |Steps 410(404.60) | Grad Norm 3.0427(3.1078) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 10.1349(9.6463) | Bit/dim 1.6724(1.7018) | Xent 0.4523(0.4494) | Xent Color 0.0743(0.1289) | Loss 4.1101(4.6502) | Error 0.1456(0.1415) | Error Color 0.0178(0.0304) |Steps 416(409.11) | Grad Norm 3.4581(3.1717) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 9.9227(9.6891) | Bit/dim 1.6310(1.6896) | Xent 0.5083(0.4444) | Xent Color 0.0648(0.1133) | Loss 4.0659(4.4979) | Error 0.1533(0.1390) | Error Color 0.0144(0.0268) |Steps 410(409.73) | Grad Norm 4.7090(3.5644) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 10.1159(9.7067) | Bit/dim 1.6356(1.6769) | Xent 0.4421(0.4394) | Xent Color 0.0614(0.1022) | Loss 4.0495(4.3825) | Error 0.1444(0.1383) | Error Color 0.0156(0.0244) |Steps 404(410.49) | Grad Norm 1.8368(3.8737) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.6202(9.7573) | Bit/dim 1.6124(1.6644) | Xent 0.4297(0.4347) | Xent Color 0.0616(0.0924) | Loss 3.9730(4.2818) | Error 0.1233(0.1360) | Error Color 0.0089(0.0220) |Steps 410(409.94) | Grad Norm 4.0389(4.5569) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 10.1661(9.8003) | Bit/dim 1.6145(1.6525) | Xent 0.4146(0.4288) | Xent Color 0.0444(0.0822) | Loss 4.0276(4.2099) | Error 0.1144(0.1338) | Error Color 0.0100(0.0193) |Steps 422(412.07) | Grad Norm 2.3872(4.4298) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 9.6637(9.8440) | Bit/dim 1.6099(1.6406) | Xent 0.4267(0.4205) | Xent Color 0.0502(0.0750) | Loss 4.0340(4.1465) | Error 0.1278(0.1323) | Error Color 0.0133(0.0172) |Steps 428(414.68) | Grad Norm 3.6840(4.4492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 60.8023, Epoch Time 733.6580(675.1138), Bit/dim 1.6023(best: 1.6773), Xent 0.2674, Xent Color 0.0179. Loss 1.6736, Error 0.0827(best: 0.0849), Error Color 0.0011(best: 0.0016)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 10.1047(9.8145) | Bit/dim 1.5990(1.6300) | Xent 0.3780(0.4146) | Xent Color 0.0560(0.0696) | Loss 4.0257(4.5565) | Error 0.1178(0.1309) | Error Color 0.0144(0.0160) |Steps 416(414.27) | Grad Norm 9.3154(5.1521) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.7226(9.8396) | Bit/dim 1.5762(1.6203) | Xent 0.3995(0.4054) | Xent Color 0.0546(0.0636) | Loss 3.9037(4.3963) | Error 0.1278(0.1285) | Error Color 0.0144(0.0144) |Steps 416(413.38) | Grad Norm 5.4326(5.0726) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 9.5214(9.8156) | Bit/dim 1.5558(1.6080) | Xent 0.3598(0.4031) | Xent Color 0.0495(0.0585) | Loss 3.7897(4.2638) | Error 0.1156(0.1283) | Error Color 0.0156(0.0129) |Steps 392(413.56) | Grad Norm 1.7998(4.5994) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.7293(9.7837) | Bit/dim 1.5616(1.5956) | Xent 0.3759(0.3962) | Xent Color 0.0434(0.0532) | Loss 3.8057(4.1537) | Error 0.1256(0.1267) | Error Color 0.0067(0.0114) |Steps 410(413.11) | Grad Norm 2.9151(4.3514) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 9.1745(9.7742) | Bit/dim 1.5516(1.5834) | Xent 0.3956(0.3907) | Xent Color 0.0361(0.0483) | Loss 3.8109(4.0662) | Error 0.1311(0.1245) | Error Color 0.0111(0.0102) |Steps 398(412.64) | Grad Norm 2.5494(3.9357) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 9.6657(9.7550) | Bit/dim 1.5357(1.5712) | Xent 0.3739(0.3858) | Xent Color 0.0293(0.0456) | Loss 3.7924(3.9947) | Error 0.1222(0.1226) | Error Color 0.0044(0.0098) |Steps 434(412.63) | Grad Norm 3.0897(4.3922) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 9.6255(9.7704) | Bit/dim 1.5117(1.5593) | Xent 0.3387(0.3797) | Xent Color 0.0362(0.0445) | Loss 3.7628(3.9444) | Error 0.1067(0.1205) | Error Color 0.0056(0.0095) |Steps 410(411.24) | Grad Norm 4.3188(5.0819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 61.0887, Epoch Time 729.0757(676.7327), Bit/dim 1.5287(best: 1.6023), Xent 0.2283, Xent Color 0.0284. Loss 1.5928, Error 0.0696(best: 0.0827), Error Color 0.0085(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 9.0595(9.7120) | Bit/dim 1.5125(1.5486) | Xent 0.3853(0.3781) | Xent Color 0.0597(0.0475) | Loss 3.7217(4.3074) | Error 0.1256(0.1190) | Error Color 0.0178(0.0111) |Steps 404(411.04) | Grad Norm 15.4276(7.4554) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 9.6701(9.7490) | Bit/dim 1.4926(1.5387) | Xent 0.3415(0.3692) | Xent Color 0.0415(0.0515) | Loss 3.7036(4.1652) | Error 0.1133(0.1162) | Error Color 0.0111(0.0126) |Steps 398(410.76) | Grad Norm 8.2247(9.4975) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.6166(9.7753) | Bit/dim 1.4770(1.5259) | Xent 0.3007(0.3612) | Xent Color 0.0350(0.0470) | Loss 3.6461(4.0496) | Error 0.0978(0.1141) | Error Color 0.0078(0.0111) |Steps 428(415.27) | Grad Norm 6.5017(8.6532) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 9.9686(9.8249) | Bit/dim 1.4717(1.5148) | Xent 0.3086(0.3507) | Xent Color 0.0236(0.0429) | Loss 3.6449(3.9551) | Error 0.0956(0.1114) | Error Color 0.0044(0.0099) |Steps 428(415.53) | Grad Norm 1.9791(7.3842) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 9.4642(9.8277) | Bit/dim 1.4642(1.5020) | Xent 0.3861(0.3494) | Xent Color 0.0245(0.0392) | Loss 3.6509(3.8808) | Error 0.1233(0.1120) | Error Color 0.0067(0.0090) |Steps 404(415.77) | Grad Norm 2.4200(6.6123) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 9.3266(9.8333) | Bit/dim 1.4451(1.4900) | Xent 0.3086(0.3410) | Xent Color 0.0333(0.0357) | Loss 3.5398(3.8113) | Error 0.1056(0.1085) | Error Color 0.0089(0.0079) |Steps 410(414.81) | Grad Norm 6.2205(5.8841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 60.3204, Epoch Time 729.5952(678.3186), Bit/dim 1.4441(best: 1.5287), Xent 0.2065, Xent Color 0.0086. Loss 1.4978, Error 0.0657(best: 0.0696), Error Color 0.0006(best: 0.0011)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.9347(9.8065) | Bit/dim 1.4400(1.4779) | Xent 0.3029(0.3386) | Xent Color 0.0329(0.0328) | Loss 3.5934(4.2640) | Error 0.0956(0.1075) | Error Color 0.0067(0.0067) |Steps 404(412.89) | Grad Norm 7.6372(5.7028) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.9953(9.7952) | Bit/dim 1.4182(1.4656) | Xent 0.3138(0.3357) | Xent Color 0.0289(0.0307) | Loss 3.5618(4.0865) | Error 0.1067(0.1079) | Error Color 0.0067(0.0061) |Steps 434(415.14) | Grad Norm 6.3771(5.7905) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 9.6221(9.7952) | Bit/dim 1.4252(1.4540) | Xent 0.3604(0.3289) | Xent Color 0.0272(0.0286) | Loss 3.5465(3.9563) | Error 0.1089(0.1058) | Error Color 0.0056(0.0054) |Steps 410(415.57) | Grad Norm 7.0167(5.4481) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 9.8214(9.7796) | Bit/dim 1.3969(1.4418) | Xent 0.3175(0.3216) | Xent Color 0.0228(0.0270) | Loss 3.5799(3.8451) | Error 0.1033(0.1034) | Error Color 0.0056(0.0049) |Steps 434(417.11) | Grad Norm 5.8919(5.3612) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 10.7677(9.8840) | Bit/dim 2.0131(1.5840) | Xent 0.5076(0.4284) | Xent Color 2.4857(1.0277) | Loss 5.8326(4.5545) | Error 0.1578(0.1298) | Error Color 0.7100(0.1290) |Steps 428(421.64) | Grad Norm 32.4844(30.7762) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 11.1780(10.1007) | Bit/dim 1.8125(1.6663) | Xent 0.5536(0.4579) | Xent Color 0.5916(0.9901) | Loss 4.8178(4.6757) | Error 0.1822(0.1417) | Error Color 0.2356(0.1823) |Steps 488(431.40) | Grad Norm 8.2697(26.5856) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 11.3044(10.3563) | Bit/dim 1.7069(1.6889) | Xent 0.3884(0.4623) | Xent Color 0.3422(0.8524) | Loss 4.3523(4.6517) | Error 0.1378(0.1449) | Error Color 0.1133(0.1809) |Steps 494(446.58) | Grad Norm 4.4879(21.2877) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 68.6497, Epoch Time 764.6409(680.9082), Bit/dim 1.6911(best: 1.4441), Xent 0.2808, Xent Color 0.1980. Loss 1.8108, Error 0.0847(best: 0.0657), Error Color 0.0401(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 11.0572(10.4703) | Bit/dim 1.6135(1.6823) | Xent 0.4104(0.4583) | Xent Color 0.2777(0.7093) | Loss 4.2106(5.0928) | Error 0.1267(0.1449) | Error Color 0.0956(0.1607) |Steps 458(455.21) | Grad Norm 4.5939(16.5399) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 10.4990(10.5420) | Bit/dim 1.5613(1.6554) | Xent 0.3705(0.4412) | Xent Color 0.2172(0.5856) | Loss 4.0511(4.8368) | Error 0.1256(0.1407) | Error Color 0.0678(0.1385) |Steps 476(461.28) | Grad Norm 5.9548(13.1558) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 10.5502(10.6763) | Bit/dim 1.5157(1.6211) | Xent 0.3237(0.4163) | Xent Color 0.2007(0.4856) | Loss 3.8920(4.6118) | Error 0.1033(0.1330) | Error Color 0.0756(0.1195) |Steps 440(464.60) | Grad Norm 10.8444(11.4259) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 11.3006(10.7753) | Bit/dim 1.4821(1.5884) | Xent 0.3049(0.3978) | Xent Color 0.1414(0.4010) | Loss 3.9089(4.4252) | Error 0.0989(0.1271) | Error Color 0.0400(0.1010) |Steps 488(467.69) | Grad Norm 2.0039(10.4279) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 10.5739(10.7942) | Bit/dim 1.4620(1.5582) | Xent 0.3303(0.3840) | Xent Color 0.1147(0.3315) | Loss 3.7785(4.2747) | Error 0.1111(0.1231) | Error Color 0.0244(0.0846) |Steps 452(467.10) | Grad Norm 3.2377(9.0398) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 10.4411(10.7674) | Bit/dim 1.4547(1.5302) | Xent 0.3350(0.3678) | Xent Color 0.1048(0.2727) | Loss 3.7808(4.1455) | Error 0.1133(0.1180) | Error Color 0.0267(0.0693) |Steps 434(464.62) | Grad Norm 5.5249(8.0011) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 65.4543, Epoch Time 804.0533(684.6026), Bit/dim 1.4352(best: 1.4441), Xent 0.2044, Xent Color 0.0380. Loss 1.4958, Error 0.0650(best: 0.0657), Error Color 0.0038(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 10.5541(10.7229) | Bit/dim 1.4366(1.5070) | Xent 0.3570(0.3608) | Xent Color 0.0894(0.2249) | Loss 3.7346(4.6402) | Error 0.0978(0.1156) | Error Color 0.0233(0.0573) |Steps 452(461.78) | Grad Norm 1.8628(6.7061) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 10.6524(10.7300) | Bit/dim 1.4188(1.4853) | Xent 0.3030(0.3488) | Xent Color 0.0715(0.1856) | Loss 3.7455(4.3970) | Error 0.0900(0.1118) | Error Color 0.0156(0.0468) |Steps 464(461.48) | Grad Norm 2.1731(5.6278) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 10.3770(10.6600) | Bit/dim 1.4162(1.4657) | Xent 0.2935(0.3410) | Xent Color 0.0777(0.1556) | Loss 3.7123(4.2071) | Error 0.0989(0.1088) | Error Color 0.0233(0.0396) |Steps 458(459.54) | Grad Norm 1.3969(4.9445) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 11.6261(10.7372) | Bit/dim 1.3984(1.4374) | Xent 0.2833(0.3239) | Xent Color 0.1319(0.1569) | Loss 3.6573(3.9848) | Error 0.0911(0.1032) | Error Color 0.0433(0.0467) |Steps 464(462.46) | Grad Norm 9.3225(9.6759) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 10.5691(10.7430) | Bit/dim 1.3922(1.4265) | Xent 0.3042(0.3170) | Xent Color 0.0568(0.1374) | Loss 3.5954(3.8942) | Error 0.0922(0.1017) | Error Color 0.0133(0.0403) |Steps 446(461.09) | Grad Norm 4.7808(8.7971) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 11.1840(10.7277) | Bit/dim 1.3756(1.4135) | Xent 0.3002(0.3105) | Xent Color 0.0479(0.1181) | Loss 3.6601(3.8167) | Error 0.0944(0.0990) | Error Color 0.0056(0.0339) |Steps 482(459.07) | Grad Norm 3.3856(7.8626) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 59.2572, Epoch Time 787.6936(687.6953), Bit/dim 1.3711(best: 1.4352), Xent 0.1822, Xent Color 0.0219. Loss 1.4221, Error 0.0561(best: 0.0650), Error Color 0.0010(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 11.0261(10.7061) | Bit/dim 1.3737(1.4015) | Xent 0.2740(0.3026) | Xent Color 0.0515(0.1021) | Loss 3.6031(4.2368) | Error 0.0900(0.0964) | Error Color 0.0078(0.0279) |Steps 482(455.67) | Grad Norm 3.1412(6.9836) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 10.3853(10.7028) | Bit/dim 1.3728(1.3910) | Xent 0.2727(0.2976) | Xent Color 0.0565(0.0889) | Loss 3.5381(4.0519) | Error 0.0833(0.0955) | Error Color 0.0133(0.0237) |Steps 440(453.36) | Grad Norm 4.5589(6.3486) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 10.2051(10.6339) | Bit/dim 1.3459(1.3797) | Xent 0.2676(0.2931) | Xent Color 0.0505(0.0772) | Loss 3.5147(3.9084) | Error 0.0844(0.0934) | Error Color 0.0167(0.0200) |Steps 428(451.13) | Grad Norm 2.9832(5.4646) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 10.0263(10.5667) | Bit/dim 1.3417(1.3695) | Xent 0.2407(0.2866) | Xent Color 0.0552(0.0684) | Loss 3.4252(3.8002) | Error 0.0800(0.0922) | Error Color 0.0144(0.0170) |Steps 440(449.18) | Grad Norm 7.1418(5.0462) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 10.4612(10.5683) | Bit/dim 1.3307(1.3610) | Xent 0.2834(0.2870) | Xent Color 0.0484(0.0612) | Loss 3.4603(3.7219) | Error 0.0967(0.0926) | Error Color 0.0100(0.0147) |Steps 464(451.83) | Grad Norm 5.7690(4.7639) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 10.3481(10.5715) | Bit/dim 1.3299(1.3517) | Xent 0.3004(0.2828) | Xent Color 0.0547(0.0564) | Loss 3.5131(3.6570) | Error 0.0944(0.0914) | Error Color 0.0133(0.0135) |Steps 428(449.05) | Grad Norm 10.0918(5.0669) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 10.4293(10.5142) | Bit/dim 1.3386(1.3439) | Xent 0.2669(0.2800) | Xent Color 0.0475(0.0523) | Loss 3.4567(3.6052) | Error 0.0922(0.0904) | Error Color 0.0122(0.0121) |Steps 410(446.40) | Grad Norm 9.6980(5.5536) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 59.1751, Epoch Time 775.1584(690.3192), Bit/dim 1.3203(best: 1.3711), Xent 0.1598, Xent Color 0.0143. Loss 1.3638, Error 0.0501(best: 0.0561), Error Color 0.0012(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 10.7903(10.4903) | Bit/dim 1.3260(1.3372) | Xent 0.2837(0.2747) | Xent Color 0.0345(0.0485) | Loss 3.5014(3.9999) | Error 0.0867(0.0882) | Error Color 0.0056(0.0111) |Steps 446(446.87) | Grad Norm 7.4882(5.7466) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 10.7994(10.4685) | Bit/dim 1.2944(1.3302) | Xent 0.2442(0.2715) | Xent Color 0.0430(0.0446) | Loss 3.4379(3.8528) | Error 0.0722(0.0873) | Error Color 0.0100(0.0099) |Steps 422(442.79) | Grad Norm 1.9355(5.5419) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 10.4501(10.4531) | Bit/dim 1.2860(1.3212) | Xent 0.2236(0.2642) | Xent Color 0.0316(0.0418) | Loss 3.3370(3.7312) | Error 0.0756(0.0857) | Error Color 0.0056(0.0091) |Steps 434(443.19) | Grad Norm 3.3941(5.2985) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 10.3271(10.4391) | Bit/dim 1.2794(1.3134) | Xent 0.2660(0.2653) | Xent Color 0.0303(0.0388) | Loss 3.2914(3.6459) | Error 0.0767(0.0854) | Error Color 0.0044(0.0084) |Steps 428(441.97) | Grad Norm 3.1078(4.6288) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 10.4110(10.4119) | Bit/dim 1.2848(1.3039) | Xent 0.2410(0.2598) | Xent Color 0.0239(0.0360) | Loss 3.4137(3.5721) | Error 0.0856(0.0835) | Error Color 0.0033(0.0075) |Steps 440(440.28) | Grad Norm 5.6953(4.2704) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 9.8222(10.3917) | Bit/dim 1.2807(1.2980) | Xent 0.2584(0.2596) | Xent Color 0.0246(0.0335) | Loss 3.3749(3.5239) | Error 0.0878(0.0836) | Error Color 0.0056(0.0069) |Steps 422(439.89) | Grad Norm 3.4331(4.1020) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 60.7234, Epoch Time 768.2280(692.6565), Bit/dim 1.2722(best: 1.3203), Xent 0.1442, Xent Color 0.0137. Loss 1.3116, Error 0.0458(best: 0.0501), Error Color 0.0019(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 10.9420(10.4204) | Bit/dim 1.2668(1.2911) | Xent 0.2438(0.2554) | Xent Color 0.0202(0.0314) | Loss 3.4013(4.0150) | Error 0.0756(0.0826) | Error Color 0.0022(0.0060) |Steps 476(442.50) | Grad Norm 3.3567(4.0976) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 10.0976(10.4109) | Bit/dim 1.2666(1.2847) | Xent 0.2431(0.2537) | Xent Color 0.0247(0.0307) | Loss 3.3044(3.8432) | Error 0.0822(0.0817) | Error Color 0.0044(0.0059) |Steps 446(441.15) | Grad Norm 5.2128(4.5553) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.9384(10.3425) | Bit/dim 1.2529(1.2775) | Xent 0.2678(0.2524) | Xent Color 0.0175(0.0293) | Loss 3.2727(3.7029) | Error 0.0900(0.0808) | Error Color 0.0056(0.0060) |Steps 410(438.57) | Grad Norm 2.7748(4.3593) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 10.2479(10.3399) | Bit/dim 1.2349(1.2709) | Xent 0.2443(0.2481) | Xent Color 0.0295(0.0278) | Loss 3.2775(3.6024) | Error 0.0833(0.0799) | Error Color 0.0044(0.0054) |Steps 452(438.33) | Grad Norm 1.9960(3.9568) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 10.5371(10.3253) | Bit/dim 1.2290(1.2631) | Xent 0.2258(0.2439) | Xent Color 0.0181(0.0270) | Loss 3.2524(3.5202) | Error 0.0711(0.0784) | Error Color 0.0011(0.0051) |Steps 446(437.66) | Grad Norm 3.3604(4.0159) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 10.7074(10.3785) | Bit/dim 1.2399(1.2568) | Xent 0.2718(0.2426) | Xent Color 0.0365(0.0268) | Loss 3.3499(3.4651) | Error 0.0800(0.0775) | Error Color 0.0100(0.0052) |Steps 476(438.48) | Grad Norm 12.6194(5.2403) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 12.2272(10.5467) | Bit/dim 1.5559(1.3542) | Xent 0.5051(0.3544) | Xent Color 0.7726(0.4060) | Loss 4.5434(3.8514) | Error 0.1733(0.1110) | Error Color 0.3211(0.0890) |Steps 530(450.98) | Grad Norm 35.9740(21.5655) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 65.8581, Epoch Time 779.7038(695.2679), Bit/dim 1.5629(best: 1.2722), Xent 0.2485, Xent Color 0.2492. Loss 1.6873, Error 0.0838(best: 0.0458), Error Color 0.0813(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 12.9792(10.9669) | Bit/dim 1.4747(1.3986) | Xent 0.4031(0.3658) | Xent Color 0.2709(0.4018) | Loss 4.0805(4.5556) | Error 0.1367(0.1169) | Error Color 0.1000(0.1053) |Steps 500(470.69) | Grad Norm 8.3223(19.2747) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 11.9933(11.2218) | Bit/dim 1.3665(1.4009) | Xent 0.3029(0.3613) | Xent Color 0.1405(0.3416) | Loss 3.7537(4.3730) | Error 0.0933(0.1147) | Error Color 0.0433(0.0922) |Steps 488(479.37) | Grad Norm 5.5373(15.9177) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 11.4895(11.3442) | Bit/dim 1.3116(1.3836) | Xent 0.3088(0.3448) | Xent Color 0.1050(0.2809) | Loss 3.6039(4.1865) | Error 0.1011(0.1100) | Error Color 0.0233(0.0749) |Steps 506(489.92) | Grad Norm 1.8134(12.6573) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 11.0771(11.3585) | Bit/dim 1.2750(1.3596) | Xent 0.3147(0.3354) | Xent Color 0.0854(0.2315) | Loss 3.5465(4.0167) | Error 0.0989(0.1077) | Error Color 0.0256(0.0612) |Steps 500(490.80) | Grad Norm 3.2208(10.0528) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 11.4868(11.3578) | Bit/dim 1.2500(1.3344) | Xent 0.2584(0.3198) | Xent Color 0.0698(0.1912) | Loss 3.3641(3.8632) | Error 0.0889(0.1037) | Error Color 0.0167(0.0498) |Steps 476(491.58) | Grad Norm 2.5978(8.0863) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 11.1324(11.3396) | Bit/dim 1.2369(1.3114) | Xent 0.2881(0.3091) | Xent Color 0.0563(0.1578) | Loss 3.3653(3.7421) | Error 0.0911(0.0994) | Error Color 0.0133(0.0400) |Steps 488(489.22) | Grad Norm 1.7530(6.5457) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 59.7514, Epoch Time 845.4454(699.7732), Bit/dim 1.2307(best: 1.2722), Xent 0.1593, Xent Color 0.0246. Loss 1.2766, Error 0.0492(best: 0.0458), Error Color 0.0013(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 10.8447(11.3172) | Bit/dim 1.2398(1.2922) | Xent 0.2268(0.2957) | Xent Color 0.0551(0.1324) | Loss 3.2837(4.2243) | Error 0.0711(0.0955) | Error Color 0.0100(0.0328) |Steps 482(484.89) | Grad Norm 1.5845(5.3515) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 10.9253(11.1934) | Bit/dim 1.2170(1.2745) | Xent 0.2877(0.2851) | Xent Color 0.0457(0.1107) | Loss 3.3118(3.9774) | Error 0.0911(0.0920) | Error Color 0.0056(0.0264) |Steps 458(477.35) | Grad Norm 2.2560(4.4064) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 11.1595(11.1606) | Bit/dim 1.2035(1.2569) | Xent 0.2188(0.2753) | Xent Color 0.0410(0.0944) | Loss 3.2455(3.7932) | Error 0.0667(0.0887) | Error Color 0.0078(0.0222) |Steps 446(470.01) | Grad Norm 2.4857(3.8592) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 11.1710(11.0769) | Bit/dim 1.2133(1.2427) | Xent 0.2847(0.2696) | Xent Color 0.0409(0.0810) | Loss 3.2525(3.6531) | Error 0.0878(0.0863) | Error Color 0.0033(0.0184) |Steps 416(463.10) | Grad Norm 3.5287(3.4508) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 11.2616(10.9905) | Bit/dim 1.1903(1.2288) | Xent 0.2358(0.2622) | Xent Color 0.0395(0.0703) | Loss 3.2367(3.5403) | Error 0.0744(0.0832) | Error Color 0.0056(0.0155) |Steps 476(462.22) | Grad Norm 2.7446(3.2568) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 10.8135(10.8854) | Bit/dim 1.1812(1.2165) | Xent 0.1985(0.2521) | Xent Color 0.0337(0.0616) | Loss 3.2300(3.4425) | Error 0.0611(0.0805) | Error Color 0.0044(0.0131) |Steps 482(457.40) | Grad Norm 1.6067(3.0910) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 10.6113(10.8309) | Bit/dim 1.1627(1.2055) | Xent 0.2139(0.2439) | Xent Color 0.0316(0.0542) | Loss 3.1144(3.3570) | Error 0.0733(0.0777) | Error Color 0.0056(0.0111) |Steps 452(452.91) | Grad Norm 2.0193(2.8725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 56.9274, Epoch Time 790.1043(702.4831), Bit/dim 1.1672(best: 1.2307), Xent 0.1407, Xent Color 0.0125. Loss 1.2055, Error 0.0431(best: 0.0458), Error Color 0.0009(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 9.9331(10.7308) | Bit/dim 1.1662(1.1942) | Xent 0.1929(0.2422) | Xent Color 0.0297(0.0491) | Loss 3.0375(3.7994) | Error 0.0589(0.0770) | Error Color 0.0044(0.0099) |Steps 416(446.25) | Grad Norm 2.3115(2.7945) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 10.4577(10.6440) | Bit/dim 1.1550(1.1854) | Xent 0.2443(0.2344) | Xent Color 0.0286(0.0445) | Loss 3.1521(3.6208) | Error 0.0822(0.0756) | Error Color 0.0033(0.0087) |Steps 452(443.52) | Grad Norm 2.2989(2.6614) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 10.4160(10.6042) | Bit/dim 1.1404(1.1759) | Xent 0.2259(0.2340) | Xent Color 0.0278(0.0403) | Loss 3.1076(3.4875) | Error 0.0756(0.0753) | Error Color 0.0022(0.0073) |Steps 410(442.50) | Grad Norm 1.9160(2.6160) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.9451(10.5782) | Bit/dim 1.1356(1.1678) | Xent 0.2527(0.2312) | Xent Color 0.0348(0.0371) | Loss 3.0857(3.3851) | Error 0.0778(0.0736) | Error Color 0.0078(0.0067) |Steps 422(439.14) | Grad Norm 2.5205(2.6018) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 10.0994(10.4923) | Bit/dim 1.1213(1.1579) | Xent 0.2415(0.2258) | Xent Color 0.0270(0.0339) | Loss 2.9659(3.2919) | Error 0.0789(0.0722) | Error Color 0.0089(0.0061) |Steps 434(436.02) | Grad Norm 2.3992(2.5949) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 10.6317(10.3898) | Bit/dim 1.1322(1.1497) | Xent 0.2253(0.2229) | Xent Color 0.0250(0.0316) | Loss 3.0264(3.2244) | Error 0.0689(0.0714) | Error Color 0.0044(0.0055) |Steps 434(436.17) | Grad Norm 3.2089(2.5995) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 10.2241(10.3444) | Bit/dim 1.1143(1.1414) | Xent 0.2180(0.2200) | Xent Color 0.0290(0.0293) | Loss 2.9731(3.1663) | Error 0.0656(0.0695) | Error Color 0.0056(0.0050) |Steps 416(434.82) | Grad Norm 2.0690(2.5521) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 56.8695, Epoch Time 760.3196(704.2182), Bit/dim 1.1196(best: 1.1672), Xent 0.1287, Xent Color 0.0080. Loss 1.1538, Error 0.0433(best: 0.0431), Error Color 0.0002(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 10.2511(10.3147) | Bit/dim 1.1218(1.1352) | Xent 0.1828(0.2178) | Xent Color 0.0311(0.0281) | Loss 3.0161(3.5374) | Error 0.0611(0.0698) | Error Color 0.0078(0.0049) |Steps 422(433.92) | Grad Norm 5.6677(3.3712) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 10.5867(10.3605) | Bit/dim 1.0998(1.1280) | Xent 0.1946(0.2152) | Xent Color 0.0284(0.0273) | Loss 2.9772(3.3940) | Error 0.0633(0.0684) | Error Color 0.0078(0.0050) |Steps 434(434.20) | Grad Norm 5.6919(4.0091) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 9.8852(10.3027) | Bit/dim 1.0987(1.1211) | Xent 0.2021(0.2134) | Xent Color 0.0198(0.0263) | Loss 2.9271(3.2857) | Error 0.0678(0.0674) | Error Color 0.0044(0.0047) |Steps 416(430.69) | Grad Norm 1.9141(4.1061) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 10.5292(10.2623) | Bit/dim 1.0910(1.1142) | Xent 0.2004(0.2114) | Xent Color 0.0205(0.0250) | Loss 2.9962(3.1995) | Error 0.0689(0.0674) | Error Color 0.0044(0.0044) |Steps 446(431.36) | Grad Norm 3.7632(4.2960) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 10.3381(10.2325) | Bit/dim 1.0870(1.1084) | Xent 0.1980(0.2048) | Xent Color 0.0243(0.0243) | Loss 2.8942(3.1286) | Error 0.0622(0.0652) | Error Color 0.0044(0.0042) |Steps 422(430.19) | Grad Norm 6.2608(4.9153) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.6176(10.2708) | Bit/dim 1.0878(1.1029) | Xent 0.2008(0.2033) | Xent Color 0.0203(0.0237) | Loss 2.9441(3.0854) | Error 0.0722(0.0652) | Error Color 0.0022(0.0040) |Steps 452(430.64) | Grad Norm 11.1537(5.7969) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 55.6189, Epoch Time 754.5971(705.7296), Bit/dim 1.0794(best: 1.1196), Xent 0.1200, Xent Color 0.0066. Loss 1.1110, Error 0.0393(best: 0.0431), Error Color 0.0006(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 10.6772(10.3109) | Bit/dim 1.0796(1.0970) | Xent 0.2032(0.2020) | Xent Color 0.0157(0.0223) | Loss 2.9454(3.5295) | Error 0.0622(0.0648) | Error Color 0.0011(0.0035) |Steps 410(431.64) | Grad Norm 7.2106(5.7074) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 10.4835(10.3037) | Bit/dim 1.0658(1.0901) | Xent 0.2356(0.2016) | Xent Color 0.0214(0.0222) | Loss 2.9230(3.3655) | Error 0.0744(0.0646) | Error Color 0.0044(0.0036) |Steps 434(429.81) | Grad Norm 10.5094(6.2479) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 10.2302(10.2812) | Bit/dim 1.0783(1.0879) | Xent 0.1684(0.1977) | Xent Color 0.0278(0.0221) | Loss 2.8914(3.2472) | Error 0.0456(0.0631) | Error Color 0.0056(0.0038) |Steps 428(428.22) | Grad Norm 13.8048(8.0168) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 10.5663(10.2836) | Bit/dim 1.0717(1.0827) | Xent 0.1881(0.1952) | Xent Color 0.0179(0.0217) | Loss 2.8719(3.1514) | Error 0.0600(0.0628) | Error Color 0.0022(0.0037) |Steps 446(429.03) | Grad Norm 5.8073(7.9453) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 10.8815(10.2764) | Bit/dim 1.0667(1.0763) | Xent 0.1986(0.1960) | Xent Color 0.0213(0.0208) | Loss 2.8867(3.0827) | Error 0.0544(0.0630) | Error Color 0.0044(0.0034) |Steps 428(429.69) | Grad Norm 6.9092(7.6555) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 10.4627(10.2821) | Bit/dim 1.0654(1.0714) | Xent 0.1887(0.1988) | Xent Color 0.0185(0.0201) | Loss 2.8485(3.0277) | Error 0.0656(0.0632) | Error Color 0.0011(0.0033) |Steps 434(427.79) | Grad Norm 8.0719(7.7549) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 10.1602(10.2211) | Bit/dim 1.0842(1.0715) | Xent 0.2103(0.1957) | Xent Color 0.0562(0.0214) | Loss 2.9580(2.9939) | Error 0.0656(0.0625) | Error Color 0.0178(0.0040) |Steps 440(426.84) | Grad Norm 28.6532(9.3389) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 57.1531, Epoch Time 754.0576(707.1794), Bit/dim 1.1851(best: 1.0794), Xent 0.1562, Xent Color 1.0559. Loss 1.4881, Error 0.0490(best: 0.0393), Error Color 0.1809(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 11.5241(10.4147) | Bit/dim 1.7331(1.1822) | Xent 0.2480(0.2419) | Xent Color 2.5204(0.5746) | Loss 5.2840(3.8903) | Error 0.0667(0.0770) | Error Color 0.5667(0.1157) |Steps 488(436.91) | Grad Norm 82.0370(25.8966) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 12.4449(10.8539) | Bit/dim 1.5478(1.3251) | Xent 0.4943(0.3251) | Xent Color 0.6402(0.7975) | Loss 4.4540(4.2030) | Error 0.1767(0.1049) | Error Color 0.2533(0.1794) |Steps 554(461.25) | Grad Norm 7.4642(27.1637) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 12.7338(11.2662) | Bit/dim 1.4153(1.3678) | Xent 0.3081(0.3447) | Xent Color 0.4497(0.7237) | Loss 4.0135(4.2079) | Error 0.1078(0.1109) | Error Color 0.1767(0.1858) |Steps 560(486.06) | Grad Norm 4.1059(21.3787) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 11.8161(11.5103) | Bit/dim 1.3048(1.3635) | Xent 0.2737(0.3324) | Xent Color 0.3495(0.6389) | Loss 3.7143(4.1201) | Error 0.0967(0.1073) | Error Color 0.1222(0.1763) |Steps 554(501.61) | Grad Norm 2.8133(16.7888) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 12.5184(11.7465) | Bit/dim 1.2289(1.3354) | Xent 0.2829(0.3155) | Xent Color 0.2425(0.5440) | Loss 3.5757(3.9772) | Error 0.0956(0.1020) | Error Color 0.0856(0.1527) |Steps 542(510.12) | Grad Norm 3.6570(13.2174) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 12.3658(11.8506) | Bit/dim 1.1702(1.2992) | Xent 0.2349(0.2998) | Xent Color 0.1694(0.4513) | Loss 3.3575(3.8293) | Error 0.0800(0.0964) | Error Color 0.0444(0.1263) |Steps 530(514.18) | Grad Norm 5.0703(10.7162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 64.0730, Epoch Time 883.0904(712.4568), Bit/dim 1.1453(best: 1.0794), Xent 0.1426, Xent Color 0.0652. Loss 1.1972, Error 0.0460(best: 0.0393), Error Color 0.0039(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 11.8549(11.8990) | Bit/dim 1.1390(1.2610) | Xent 0.2509(0.2847) | Xent Color 0.1220(0.3696) | Loss 3.2571(4.3191) | Error 0.0833(0.0921) | Error Color 0.0256(0.1025) |Steps 518(517.19) | Grad Norm 5.4388(9.0494) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 11.4643(11.8408) | Bit/dim 1.1145(1.2258) | Xent 0.2184(0.2685) | Xent Color 0.1011(0.3007) | Loss 3.1109(4.0176) | Error 0.0744(0.0861) | Error Color 0.0244(0.0819) |Steps 512(514.30) | Grad Norm 4.1372(7.5930) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 11.4154(11.7712) | Bit/dim 1.0935(1.1947) | Xent 0.2249(0.2578) | Xent Color 0.0928(0.2473) | Loss 3.1439(3.7890) | Error 0.0700(0.0826) | Error Color 0.0189(0.0665) |Steps 506(511.72) | Grad Norm 4.9454(6.5659) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 11.2696(11.7110) | Bit/dim 1.0902(1.1692) | Xent 0.2241(0.2491) | Xent Color 0.0809(0.2046) | Loss 3.1390(3.6067) | Error 0.0744(0.0796) | Error Color 0.0178(0.0540) |Steps 488(506.72) | Grad Norm 5.7461(6.2159) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 11.6401(11.6397) | Bit/dim 1.0931(1.1476) | Xent 0.1941(0.2424) | Xent Color 0.0759(0.1706) | Loss 3.0791(3.4657) | Error 0.0622(0.0780) | Error Color 0.0167(0.0441) |Steps 494(501.91) | Grad Norm 2.3261(5.4780) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 11.3135(11.5916) | Bit/dim 1.0831(1.1289) | Xent 0.1715(0.2353) | Xent Color 0.0631(0.1424) | Loss 3.0137(3.3539) | Error 0.0556(0.0762) | Error Color 0.0122(0.0357) |Steps 494(498.99) | Grad Norm 1.7823(4.7253) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 11.9231(11.5256) | Bit/dim 1.0524(1.1136) | Xent 0.2335(0.2311) | Xent Color 0.0519(0.1207) | Loss 3.0048(3.2697) | Error 0.0756(0.0744) | Error Color 0.0056(0.0295) |Steps 494(497.45) | Grad Norm 4.5424(4.4229) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 60.9053, Epoch Time 842.3618(716.3539), Bit/dim 1.0640(best: 1.0794), Xent 0.1260, Xent Color 0.0192. Loss 1.1003, Error 0.0397(best: 0.0393), Error Color 0.0005(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 11.3070(11.4365) | Bit/dim 1.0583(1.1002) | Xent 0.1963(0.2277) | Xent Color 0.0548(0.1036) | Loss 2.9990(3.7273) | Error 0.0633(0.0735) | Error Color 0.0133(0.0246) |Steps 488(493.19) | Grad Norm 2.0442(4.2680) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 11.6087(11.4145) | Bit/dim 1.0558(1.0895) | Xent 0.1611(0.2238) | Xent Color 0.0497(0.0896) | Loss 2.9010(3.5319) | Error 0.0467(0.0719) | Error Color 0.0156(0.0209) |Steps 476(487.02) | Grad Norm 2.9684(4.3584) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 11.4756(11.3798) | Bit/dim 1.0593(1.0808) | Xent 0.2420(0.2176) | Xent Color 0.0520(0.0778) | Loss 3.0348(3.3862) | Error 0.0722(0.0697) | Error Color 0.0089(0.0171) |Steps 494(483.31) | Grad Norm 5.2838(4.2880) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 11.1328(11.4041) | Bit/dim 1.0527(1.0719) | Xent 0.1626(0.2094) | Xent Color 0.0488(0.0693) | Loss 2.9414(3.2736) | Error 0.0511(0.0663) | Error Color 0.0100(0.0147) |Steps 470(482.11) | Grad Norm 3.7123(4.0051) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 11.1474(11.4754) | Bit/dim 1.0225(1.0645) | Xent 0.1777(0.2019) | Xent Color 0.0572(0.0622) | Loss 2.9315(3.1885) | Error 0.0522(0.0638) | Error Color 0.0133(0.0132) |Steps 482(481.09) | Grad Norm 8.9378(4.6097) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 11.2897(11.4497) | Bit/dim 1.0322(1.0579) | Xent 0.1717(0.1983) | Xent Color 0.0335(0.0563) | Loss 2.9305(3.1237) | Error 0.0589(0.0630) | Error Color 0.0056(0.0118) |Steps 476(479.37) | Grad Norm 6.7727(5.1357) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 11.4961(11.3879) | Bit/dim 1.0188(1.0513) | Xent 0.1992(0.2013) | Xent Color 0.0350(0.0513) | Loss 2.9428(3.0754) | Error 0.0611(0.0638) | Error Color 0.0056(0.0103) |Steps 494(478.54) | Grad Norm 2.9055(4.8237) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 59.8026, Epoch Time 831.3278(719.8031), Bit/dim 1.0314(best: 1.0640), Xent 0.1111, Xent Color 0.0134. Loss 1.0625, Error 0.0361(best: 0.0393), Error Color 0.0004(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 11.5174(11.3330) | Bit/dim 1.0375(1.0460) | Xent 0.1817(0.2014) | Xent Color 0.0292(0.0476) | Loss 2.9377(3.4983) | Error 0.0556(0.0634) | Error Color 0.0011(0.0092) |Steps 476(477.72) | Grad Norm 5.6327(5.2963) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 11.4166(11.3262) | Bit/dim 1.0285(1.0412) | Xent 0.1838(0.1948) | Xent Color 0.0283(0.0439) | Loss 2.9211(3.3486) | Error 0.0600(0.0620) | Error Color 0.0044(0.0083) |Steps 482(477.29) | Grad Norm 4.2376(4.9534) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 10.9150(11.3653) | Bit/dim 1.0186(1.0360) | Xent 0.2248(0.1922) | Xent Color 0.0368(0.0409) | Loss 2.8661(3.2283) | Error 0.0622(0.0605) | Error Color 0.0078(0.0075) |Steps 476(476.20) | Grad Norm 8.9755(5.1812) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 10.9656(11.3594) | Bit/dim 1.0156(1.0329) | Xent 0.1782(0.1918) | Xent Color 0.0266(0.0397) | Loss 2.8635(3.1445) | Error 0.0589(0.0603) | Error Color 0.0033(0.0074) |Steps 464(476.19) | Grad Norm 3.7443(6.7300) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 10.8785(11.2954) | Bit/dim 1.0368(1.0300) | Xent 0.2008(0.1905) | Xent Color 0.0324(0.0383) | Loss 2.9629(3.0813) | Error 0.0667(0.0602) | Error Color 0.0044(0.0072) |Steps 464(474.97) | Grad Norm 16.1884(7.8226) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 11.0494(11.2723) | Bit/dim 1.0065(1.0262) | Xent 0.2039(0.1893) | Xent Color 0.0265(0.0366) | Loss 2.8427(3.0258) | Error 0.0722(0.0599) | Error Color 0.0022(0.0068) |Steps 500(474.51) | Grad Norm 4.9987(8.3072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 58.8465, Epoch Time 822.6626(722.8889), Bit/dim 1.0120(best: 1.0314), Xent 0.1103, Xent Color 0.0102. Loss 1.0421, Error 0.0362(best: 0.0361), Error Color 0.0008(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.5094(11.2023) | Bit/dim 1.0181(1.0227) | Xent 0.1557(0.1828) | Xent Color 0.0195(0.0350) | Loss 2.8724(3.5364) | Error 0.0511(0.0581) | Error Color 0.0022(0.0064) |Steps 470(473.44) | Grad Norm 3.9945(8.4137) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 11.4401(11.2053) | Bit/dim 1.0037(1.0188) | Xent 0.1703(0.1806) | Xent Color 0.0204(0.0326) | Loss 2.8828(3.3619) | Error 0.0578(0.0581) | Error Color 0.0033(0.0057) |Steps 482(472.91) | Grad Norm 2.4081(7.9484) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 11.2990(11.2369) | Bit/dim 0.9974(1.0149) | Xent 0.1803(0.1792) | Xent Color 0.0248(0.0305) | Loss 2.7799(3.2262) | Error 0.0511(0.0567) | Error Color 0.0033(0.0051) |Steps 470(472.49) | Grad Norm 7.9529(8.3434) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 10.7266(11.1490) | Bit/dim 0.9884(1.0096) | Xent 0.1895(0.1763) | Xent Color 0.0300(0.0288) | Loss 2.7386(3.1160) | Error 0.0567(0.0553) | Error Color 0.0056(0.0048) |Steps 452(468.99) | Grad Norm 2.2083(7.6898) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 11.4194(11.1859) | Bit/dim 1.0022(1.0061) | Xent 0.1584(0.1769) | Xent Color 0.0168(0.0275) | Loss 2.8494(3.0480) | Error 0.0489(0.0555) | Error Color 0.0022(0.0044) |Steps 476(470.89) | Grad Norm 7.8195(7.2345) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 11.1155(11.1506) | Bit/dim 1.0174(1.0054) | Xent 0.1815(0.1752) | Xent Color 0.0366(0.0262) | Loss 2.8812(2.9921) | Error 0.0556(0.0555) | Error Color 0.0111(0.0042) |Steps 476(470.40) | Grad Norm 19.4137(8.0136) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 10.3328(11.1134) | Bit/dim 0.9974(1.0046) | Xent 0.1990(0.1775) | Xent Color 0.0210(0.0271) | Loss 2.8006(2.9486) | Error 0.0567(0.0564) | Error Color 0.0033(0.0045) |Steps 476(470.48) | Grad Norm 7.6189(9.8727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 61.1733, Epoch Time 816.6886(725.7029), Bit/dim 1.0044(best: 1.0120), Xent 0.1019, Xent Color 0.0075. Loss 1.0318, Error 0.0322(best: 0.0361), Error Color 0.0004(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 11.4156(11.1030) | Bit/dim 0.9844(1.0025) | Xent 0.1730(0.1729) | Xent Color 0.0230(0.0267) | Loss 2.8018(3.4056) | Error 0.0611(0.0558) | Error Color 0.0044(0.0046) |Steps 476(470.56) | Grad Norm 10.0428(10.4018) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 10.5285(11.0955) | Bit/dim 0.9773(0.9978) | Xent 0.1558(0.1710) | Xent Color 0.0238(0.0249) | Loss 2.7279(3.2492) | Error 0.0444(0.0555) | Error Color 0.0056(0.0042) |Steps 464(469.69) | Grad Norm 5.5217(9.2082) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 10.6680(11.1044) | Bit/dim 0.9814(0.9940) | Xent 0.1289(0.1713) | Xent Color 0.0208(0.0235) | Loss 2.7713(3.1295) | Error 0.0433(0.0543) | Error Color 0.0033(0.0036) |Steps 458(467.48) | Grad Norm 7.9086(8.3460) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 10.6870(11.1380) | Bit/dim 0.9984(0.9954) | Xent 0.1626(0.1707) | Xent Color 0.0314(0.0250) | Loss 2.8721(3.0559) | Error 0.0511(0.0540) | Error Color 0.0056(0.0044) |Steps 482(470.58) | Grad Norm 13.2366(10.8461) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 11.3609(11.1461) | Bit/dim 0.9771(0.9949) | Xent 0.1504(0.1660) | Xent Color 0.0149(0.0251) | Loss 2.8012(2.9952) | Error 0.0489(0.0524) | Error Color 0.0000(0.0045) |Steps 476(470.60) | Grad Norm 6.3202(11.8898) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 9.9180(11.0901) | Bit/dim 1.0036(0.9953) | Xent 0.1755(0.1658) | Xent Color 0.2862(0.0350) | Loss 2.9457(2.9555) | Error 0.0544(0.0527) | Error Color 0.1156(0.0089) |Steps 464(469.81) | Grad Norm 68.6214(15.0049) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 60.1403, Epoch Time 810.4375(728.2450), Bit/dim 2.1370(best: 1.0044), Xent 0.2211, Xent Color 0.7147. Loss 2.3709, Error 0.0716(best: 0.0322), Error Color 0.3388(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 11.1174(11.0008) | Bit/dim 2.0515(1.3220) | Xent 0.2766(0.2008) | Xent Color 0.9029(0.9309) | Loss 5.2261(4.4905) | Error 0.0867(0.0641) | Error Color 0.3433(0.1474) |Steps 476(465.77) | Grad Norm 10.9544(32.3827) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 11.1698(11.0664) | Bit/dim 1.7765(1.4641) | Xent 0.2159(0.2271) | Xent Color 0.5457(0.8870) | Loss 4.4283(4.5641) | Error 0.0722(0.0727) | Error Color 0.2011(0.1884) |Steps 506(469.65) | Grad Norm 6.6604(26.5587) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 11.5825(11.1568) | Bit/dim 1.6355(1.5226) | Xent 0.2263(0.2333) | Xent Color 0.3824(0.7640) | Loss 4.2205(4.4933) | Error 0.0644(0.0740) | Error Color 0.1400(0.1812) |Steps 494(471.99) | Grad Norm 4.3466(21.0651) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 10.8515(11.1404) | Bit/dim 1.5273(1.5333) | Xent 0.1913(0.2250) | Xent Color 0.2591(0.6419) | Loss 3.8919(4.3555) | Error 0.0600(0.0710) | Error Color 0.0956(0.1630) |Steps 476(471.41) | Grad Norm 3.9498(16.5749) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 9.6056(10.9902) | Bit/dim 1.4482(1.5177) | Xent 0.1950(0.2174) | Xent Color 0.2202(0.5345) | Loss 3.6476(4.1974) | Error 0.0667(0.0693) | Error Color 0.0889(0.1420) |Steps 440(466.40) | Grad Norm 4.5268(13.1956) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 9.8573(10.7961) | Bit/dim 1.3932(1.4908) | Xent 0.1686(0.2051) | Xent Color 0.1349(0.4365) | Loss 3.4855(4.0343) | Error 0.0533(0.0649) | Error Color 0.0411(0.1185) |Steps 422(459.31) | Grad Norm 2.8167(10.6843) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 10.3589(10.6849) | Bit/dim 1.3513(1.4568) | Xent 0.1643(0.1941) | Xent Color 0.1129(0.3546) | Loss 3.3430(3.8852) | Error 0.0544(0.0617) | Error Color 0.0367(0.0975) |Steps 416(451.87) | Grad Norm 3.9570(8.7906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 56.3800, Epoch Time 791.2823(730.1361), Bit/dim 1.3258(best: 1.0044), Xent 0.0994, Xent Color 0.0433. Loss 1.3615, Error 0.0330(best: 0.0322), Error Color 0.0045(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.6189(10.5687) | Bit/dim 1.2842(1.4208) | Xent 0.1909(0.1889) | Xent Color 0.0809(0.2861) | Loss 3.3303(4.2205) | Error 0.0611(0.0598) | Error Color 0.0178(0.0780) |Steps 428(445.70) | Grad Norm 2.1803(7.2733) | Total Time 0.00(0.00)\n",
      "Iter 2920 | Time 9.8117(10.4337) | Bit/dim 1.2530(1.3834) | Xent 0.1498(0.1831) | Xent Color 0.0797(0.2320) | Loss 3.2183(3.9762) | Error 0.0422(0.0581) | Error Color 0.0167(0.0628) |Steps 422(441.01) | Grad Norm 4.0555(6.3171) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 9.9667(10.3021) | Bit/dim 1.2221(1.3475) | Xent 0.1536(0.1763) | Xent Color 0.0634(0.1892) | Loss 3.1871(3.7767) | Error 0.0489(0.0560) | Error Color 0.0167(0.0506) |Steps 422(435.42) | Grad Norm 4.9966(5.7811) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 10.4430(10.2787) | Bit/dim 1.2092(1.3145) | Xent 0.1413(0.1719) | Xent Color 0.0495(0.1552) | Loss 3.1788(3.6201) | Error 0.0489(0.0545) | Error Color 0.0078(0.0410) |Steps 422(432.25) | Grad Norm 1.2895(5.1717) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 10.1999(10.2388) | Bit/dim 1.1822(1.2826) | Xent 0.1872(0.1679) | Xent Color 0.0552(0.1281) | Loss 3.1313(3.4886) | Error 0.0578(0.0530) | Error Color 0.0133(0.0331) |Steps 428(429.85) | Grad Norm 4.0723(4.6377) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 9.7854(10.2011) | Bit/dim 1.1633(1.2528) | Xent 0.1736(0.1674) | Xent Color 0.0466(0.1069) | Loss 2.9834(3.3768) | Error 0.0556(0.0524) | Error Color 0.0111(0.0270) |Steps 392(428.07) | Grad Norm 5.9763(4.2734) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 10.3602(10.1668) | Bit/dim 1.1404(1.2255) | Xent 0.1521(0.1629) | Xent Color 0.0419(0.0903) | Loss 3.0616(3.2908) | Error 0.0478(0.0513) | Error Color 0.0067(0.0223) |Steps 428(427.62) | Grad Norm 5.8600(4.2525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 55.1812, Epoch Time 742.1688(730.4971), Bit/dim 1.1428(best: 1.0044), Xent 0.0888, Xent Color 0.0126. Loss 1.1682, Error 0.0284(best: 0.0322), Error Color 0.0007(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 10.1279(10.1677) | Bit/dim 1.1159(1.2009) | Xent 0.1533(0.1624) | Xent Color 0.0355(0.0778) | Loss 3.0133(3.6342) | Error 0.0400(0.0511) | Error Color 0.0056(0.0187) |Steps 416(426.55) | Grad Norm 3.5400(4.2325) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 9.9924(10.1364) | Bit/dim 1.1305(1.1805) | Xent 0.1700(0.1602) | Xent Color 0.0493(0.0689) | Loss 2.9980(3.4577) | Error 0.0622(0.0513) | Error Color 0.0189(0.0166) |Steps 440(426.18) | Grad Norm 11.1369(5.0217) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 9.8474(10.1346) | Bit/dim 1.0983(1.1611) | Xent 0.1129(0.1550) | Xent Color 0.0384(0.0622) | Loss 2.9123(3.3255) | Error 0.0378(0.0496) | Error Color 0.0078(0.0148) |Steps 416(425.93) | Grad Norm 7.4744(5.8693) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 10.7694(10.1800) | Bit/dim 1.0854(1.1424) | Xent 0.1524(0.1518) | Xent Color 0.0340(0.0548) | Loss 2.9298(3.2170) | Error 0.0411(0.0485) | Error Color 0.0078(0.0128) |Steps 416(423.66) | Grad Norm 3.2458(5.6190) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 10.0300(10.1594) | Bit/dim 1.0698(1.1272) | Xent 0.1824(0.1532) | Xent Color 0.0287(0.0477) | Loss 2.9101(3.1307) | Error 0.0567(0.0488) | Error Color 0.0056(0.0107) |Steps 392(419.23) | Grad Norm 3.1720(5.2092) | Total Time 0.00(0.00)\n",
      "Iter 3030 | Time 10.2428(10.1678) | Bit/dim 1.0740(1.1136) | Xent 0.1399(0.1483) | Xent Color 0.0266(0.0426) | Loss 2.8849(3.0611) | Error 0.0400(0.0468) | Error Color 0.0044(0.0093) |Steps 422(419.47) | Grad Norm 2.4878(5.0321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 56.6842, Epoch Time 748.3693(731.0332), Bit/dim 1.0609(best: 1.0044), Xent 0.0834, Xent Color 0.0079. Loss 1.0837, Error 0.0271(best: 0.0284), Error Color 0.0002(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 9.8999(10.1265) | Bit/dim 1.0600(1.0985) | Xent 0.1377(0.1470) | Xent Color 0.0223(0.0387) | Loss 2.8887(3.4995) | Error 0.0567(0.0471) | Error Color 0.0044(0.0081) |Steps 434(418.17) | Grad Norm 5.3949(4.9245) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 10.3822(10.0971) | Bit/dim 1.0682(1.0876) | Xent 0.1306(0.1458) | Xent Color 0.0239(0.0355) | Loss 2.8588(3.3291) | Error 0.0356(0.0470) | Error Color 0.0044(0.0073) |Steps 422(417.56) | Grad Norm 5.7063(5.1936) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 9.6076(10.1252) | Bit/dim 1.0341(1.0771) | Xent 0.1799(0.1472) | Xent Color 0.0273(0.0335) | Loss 2.7601(3.1984) | Error 0.0656(0.0478) | Error Color 0.0078(0.0069) |Steps 386(414.50) | Grad Norm 6.4115(5.7097) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 10.1150(10.1106) | Bit/dim 1.0357(1.0682) | Xent 0.1463(0.1468) | Xent Color 0.0288(0.0318) | Loss 2.7383(3.0955) | Error 0.0478(0.0470) | Error Color 0.0067(0.0065) |Steps 410(413.96) | Grad Norm 5.1552(5.9154) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 9.3718(10.0395) | Bit/dim 1.0273(1.0611) | Xent 0.1764(0.1465) | Xent Color 0.0249(0.0325) | Loss 2.7579(3.0204) | Error 0.0478(0.0464) | Error Color 0.0044(0.0071) |Steps 404(415.48) | Grad Norm 11.7726(7.5506) | Total Time 0.00(0.00)\n",
      "Iter 3090 | Time 9.7515(10.0103) | Bit/dim 1.0007(1.0529) | Xent 0.1249(0.1413) | Xent Color 0.0350(0.0312) | Loss 2.6862(2.9537) | Error 0.0356(0.0451) | Error Color 0.0078(0.0066) |Steps 404(416.34) | Grad Norm 10.1402(7.9254) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 9.5452(9.9781) | Bit/dim 1.0140(1.0459) | Xent 0.1455(0.1416) | Xent Color 0.0233(0.0298) | Loss 2.7111(2.9048) | Error 0.0522(0.0455) | Error Color 0.0067(0.0065) |Steps 404(413.86) | Grad Norm 5.8070(7.8567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 55.3099, Epoch Time 735.1457(731.1566), Bit/dim 1.0194(best: 1.0044), Xent 0.0777, Xent Color 0.0046. Loss 1.0400, Error 0.0245(best: 0.0271), Error Color 0.0002(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 10.4456(10.0301) | Bit/dim 1.0228(1.0378) | Xent 0.1079(0.1356) | Xent Color 0.0161(0.0265) | Loss 2.7346(3.3027) | Error 0.0311(0.0437) | Error Color 0.0011(0.0053) |Steps 428(413.88) | Grad Norm 5.1976(6.7967) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 9.9678(9.9982) | Bit/dim 1.0003(1.0303) | Xent 0.1210(0.1346) | Xent Color 0.0217(0.0247) | Loss 2.7122(3.1560) | Error 0.0344(0.0428) | Error Color 0.0056(0.0047) |Steps 416(413.36) | Grad Norm 9.1566(6.5651) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 10.7544(9.9779) | Bit/dim 1.0098(1.0250) | Xent 0.1414(0.1367) | Xent Color 0.0248(0.0235) | Loss 2.7470(3.0494) | Error 0.0478(0.0432) | Error Color 0.0067(0.0044) |Steps 404(412.93) | Grad Norm 7.6519(6.7529) | Total Time 0.00(0.00)\n",
      "Iter 3140 | Time 9.3158(10.0450) | Bit/dim 0.9940(1.0211) | Xent 0.1359(0.1390) | Xent Color 0.0157(0.0235) | Loss 2.6908(2.9676) | Error 0.0444(0.0441) | Error Color 0.0011(0.0043) |Steps 416(411.86) | Grad Norm 5.1932(8.1668) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 10.4955(10.0133) | Bit/dim 1.0078(1.0166) | Xent 0.1709(0.1385) | Xent Color 0.0218(0.0241) | Loss 2.7571(2.9074) | Error 0.0533(0.0441) | Error Color 0.0033(0.0048) |Steps 416(411.51) | Grad Norm 13.2812(9.1855) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 9.3747(9.9965) | Bit/dim 1.0046(1.0132) | Xent 0.1596(0.1396) | Xent Color 0.0191(0.0229) | Loss 2.7199(2.8607) | Error 0.0467(0.0444) | Error Color 0.0033(0.0044) |Steps 398(410.26) | Grad Norm 11.1422(9.6866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 57.1786, Epoch Time 741.3478(731.4623), Bit/dim 0.9885(best: 1.0044), Xent 0.0759, Xent Color 0.0036. Loss 1.0084, Error 0.0254(best: 0.0245), Error Color 0.0001(best: 0.0002)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 10.6941(10.0554) | Bit/dim 0.9926(1.0076) | Xent 0.1605(0.1357) | Xent Color 0.0154(0.0213) | Loss 2.7454(3.3524) | Error 0.0433(0.0429) | Error Color 0.0033(0.0040) |Steps 404(411.49) | Grad Norm 8.6923(8.8944) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 9.7314(10.0326) | Bit/dim 0.9772(1.0024) | Xent 0.1072(0.1339) | Xent Color 0.0111(0.0205) | Loss 2.6778(3.1811) | Error 0.0333(0.0418) | Error Color 0.0000(0.0038) |Steps 404(413.10) | Grad Norm 9.1884(8.9409) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 9.8317(10.0784) | Bit/dim 0.9794(0.9981) | Xent 0.1094(0.1319) | Xent Color 0.0245(0.0199) | Loss 2.7095(3.0539) | Error 0.0367(0.0416) | Error Color 0.0044(0.0038) |Steps 428(413.65) | Grad Norm 6.3641(9.3833) | Total Time 0.00(0.00)\n",
      "Iter 3200 | Time 10.0520(10.0729) | Bit/dim 0.9855(0.9938) | Xent 0.1323(0.1319) | Xent Color 0.0151(0.0187) | Loss 2.7131(2.9560) | Error 0.0467(0.0416) | Error Color 0.0022(0.0034) |Steps 440(413.95) | Grad Norm 11.2108(9.2756) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 10.6192(10.0553) | Bit/dim 0.9663(0.9887) | Xent 0.1373(0.1296) | Xent Color 0.0132(0.0176) | Loss 2.6225(2.8786) | Error 0.0367(0.0402) | Error Color 0.0011(0.0029) |Steps 416(411.44) | Grad Norm 5.8925(8.6780) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 10.4008(10.0676) | Bit/dim 1.0397(0.9884) | Xent 0.1626(0.1297) | Xent Color 0.0918(0.0202) | Loss 2.8748(2.8308) | Error 0.0611(0.0409) | Error Color 0.0300(0.0040) |Steps 416(412.09) | Grad Norm 44.9926(10.5461) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 9.7988(10.1453) | Bit/dim 2.0148(1.2883) | Xent 0.8404(0.2801) | Xent Color 1.0486(1.1480) | Loss 5.3900(3.9222) | Error 0.2578(0.0831) | Error Color 0.3756(0.1467) |Steps 434(416.51) | Grad Norm 13.9932(31.0711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 62.8962, Epoch Time 755.1809(732.1739), Bit/dim 1.8624(best: 0.9885), Xent 0.2543, Xent Color 0.3346. Loss 2.0096, Error 0.0804(best: 0.0245), Error Color 0.1287(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 11.1921(10.3683) | Bit/dim 1.7430(1.4386) | Xent 0.3788(0.3344) | Xent Color 0.3786(1.0550) | Loss 4.5114(4.7240) | Error 0.1200(0.1032) | Error Color 0.1444(0.1862) |Steps 488(431.74) | Grad Norm 5.0749(25.7338) | Total Time 0.00(0.00)\n",
      "Iter 3250 | Time 12.9482(10.6719) | Bit/dim 1.6029(1.5016) | Xent 0.3378(0.3388) | Xent Color 0.2879(0.8600) | Loss 4.1679(4.6160) | Error 0.1033(0.1053) | Error Color 0.1033(0.1677) |Steps 554(450.75) | Grad Norm 3.8011(20.2850) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 11.3579(10.8670) | Bit/dim 1.4748(1.5086) | Xent 0.2353(0.3184) | Xent Color 0.1933(0.6941) | Loss 3.8164(4.4408) | Error 0.0800(0.0996) | Error Color 0.0644(0.1452) |Steps 512(464.70) | Grad Norm 3.0414(15.9229) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 11.3513(10.9772) | Bit/dim 1.3896(1.4860) | Xent 0.1966(0.2880) | Xent Color 0.1024(0.5485) | Loss 3.6580(4.2396) | Error 0.0578(0.0906) | Error Color 0.0289(0.1183) |Steps 494(470.42) | Grad Norm 3.8919(12.4668) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 11.1847(11.1192) | Bit/dim 1.3045(1.4458) | Xent 0.1971(0.2680) | Xent Color 0.1365(0.4374) | Loss 3.4413(4.0452) | Error 0.0656(0.0840) | Error Color 0.0356(0.0973) |Steps 506(476.19) | Grad Norm 3.3343(9.9125) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 11.8092(11.2658) | Bit/dim 1.2198(1.3927) | Xent 0.1499(0.2456) | Xent Color 0.1207(0.3509) | Loss 3.2479(3.8506) | Error 0.0533(0.0777) | Error Color 0.0333(0.0794) |Steps 506(479.37) | Grad Norm 3.0662(7.8510) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 11.6505(11.3733) | Bit/dim 1.1689(1.3402) | Xent 0.1330(0.2260) | Xent Color 0.0881(0.2826) | Loss 3.1459(3.6745) | Error 0.0489(0.0720) | Error Color 0.0256(0.0649) |Steps 518(480.60) | Grad Norm 3.8512(6.4565) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 61.6286, Epoch Time 840.8590(735.4344), Bit/dim 1.1672(best: 0.9885), Xent 0.1029, Xent Color 0.0363. Loss 1.2020, Error 0.0336(best: 0.0245), Error Color 0.0037(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 11.0268(11.3585) | Bit/dim 1.1263(1.2889) | Xent 0.1807(0.2091) | Xent Color 0.0929(0.2309) | Loss 3.0388(3.9591) | Error 0.0567(0.0665) | Error Color 0.0267(0.0538) |Steps 464(479.38) | Grad Norm 1.8857(5.2288) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 12.2400(11.4231) | Bit/dim 1.1052(1.2435) | Xent 0.1610(0.1956) | Xent Color 0.0599(0.1885) | Loss 3.0540(3.7186) | Error 0.0567(0.0623) | Error Color 0.0122(0.0439) |Steps 458(479.04) | Grad Norm 4.0742(4.4855) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 11.2735(11.3813) | Bit/dim 1.0864(1.2041) | Xent 0.1328(0.1878) | Xent Color 0.0639(0.1569) | Loss 2.9931(3.5338) | Error 0.0467(0.0593) | Error Color 0.0200(0.0367) |Steps 488(479.33) | Grad Norm 6.0997(4.2096) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 11.7022(11.3973) | Bit/dim 1.0758(1.1691) | Xent 0.1658(0.1791) | Xent Color 0.0444(0.1303) | Loss 2.9291(3.3815) | Error 0.0489(0.0569) | Error Color 0.0111(0.0304) |Steps 464(476.18) | Grad Norm 1.7613(3.6872) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 11.7909(11.3306) | Bit/dim 1.0299(1.1370) | Xent 0.1648(0.1706) | Xent Color 0.0527(0.1118) | Loss 2.8569(3.2557) | Error 0.0489(0.0536) | Error Color 0.0100(0.0265) |Steps 458(474.91) | Grad Norm 3.9938(4.0822) | Total Time 0.00(0.00)\n",
      "Iter 3360 | Time 11.4344(11.3244) | Bit/dim 1.0473(1.1123) | Xent 0.1329(0.1662) | Xent Color 0.0401(0.0959) | Loss 2.9196(3.1632) | Error 0.0389(0.0515) | Error Color 0.0067(0.0232) |Steps 476(474.77) | Grad Norm 5.0742(4.3296) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 61.5375, Epoch Time 830.6749(738.2917), Bit/dim 1.0258(best: 0.9885), Xent 0.0902, Xent Color 0.0139. Loss 1.0519, Error 0.0277(best: 0.0245), Error Color 0.0006(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 11.1224(11.2854) | Bit/dim 1.0051(1.0887) | Xent 0.1740(0.1635) | Xent Color 0.0527(0.0834) | Loss 2.7957(3.5941) | Error 0.0556(0.0513) | Error Color 0.0122(0.0203) |Steps 458(472.18) | Grad Norm 5.1466(4.4804) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 10.9377(11.2680) | Bit/dim 1.0146(1.0704) | Xent 0.1254(0.1582) | Xent Color 0.0341(0.0728) | Loss 2.7513(3.3954) | Error 0.0411(0.0500) | Error Color 0.0056(0.0180) |Steps 488(473.59) | Grad Norm 2.5725(4.6359) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 11.4297(11.2553) | Bit/dim 1.0074(1.0538) | Xent 0.1442(0.1542) | Xent Color 0.0329(0.0635) | Loss 2.8599(3.2431) | Error 0.0389(0.0480) | Error Color 0.0078(0.0155) |Steps 476(473.40) | Grad Norm 2.3816(4.4554) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 11.2660(11.3104) | Bit/dim 0.9960(1.0392) | Xent 0.1154(0.1492) | Xent Color 0.0316(0.0551) | Loss 2.8184(3.1235) | Error 0.0389(0.0472) | Error Color 0.0100(0.0130) |Steps 470(472.92) | Grad Norm 3.2540(3.9827) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 10.7273(11.3442) | Bit/dim 0.9841(1.0249) | Xent 0.1005(0.1455) | Xent Color 0.0379(0.0499) | Loss 2.7374(3.0353) | Error 0.0333(0.0457) | Error Color 0.0100(0.0118) |Steps 446(470.82) | Grad Norm 3.5820(3.5765) | Total Time 0.00(0.00)\n",
      "Iter 3420 | Time 11.0580(11.3401) | Bit/dim 0.9855(1.0141) | Xent 0.1333(0.1437) | Xent Color 0.0239(0.0438) | Loss 2.7689(2.9662) | Error 0.0411(0.0445) | Error Color 0.0044(0.0099) |Steps 476(470.22) | Grad Norm 2.0508(3.2736) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 11.6389(11.3353) | Bit/dim 0.9856(1.0047) | Xent 0.1642(0.1430) | Xent Color 0.0224(0.0398) | Loss 2.9070(2.9104) | Error 0.0511(0.0444) | Error Color 0.0022(0.0089) |Steps 494(470.72) | Grad Norm 4.5043(3.4786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 61.7876, Epoch Time 830.7177(741.0644), Bit/dim 0.9717(best: 0.9885), Xent 0.0821, Xent Color 0.0104. Loss 0.9948, Error 0.0270(best: 0.0245), Error Color 0.0007(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 11.0573(11.2970) | Bit/dim 0.9591(0.9962) | Xent 0.1221(0.1411) | Xent Color 0.0216(0.0371) | Loss 2.7157(3.3283) | Error 0.0422(0.0446) | Error Color 0.0033(0.0081) |Steps 488(472.51) | Grad Norm 6.6787(4.1144) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 10.6383(11.3031) | Bit/dim 0.9660(0.9884) | Xent 0.1542(0.1396) | Xent Color 0.0176(0.0340) | Loss 2.7245(3.1701) | Error 0.0500(0.0437) | Error Color 0.0011(0.0073) |Steps 464(471.28) | Grad Norm 2.7348(4.0960) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 12.2160(11.2829) | Bit/dim 0.9700(0.9810) | Xent 0.1212(0.1361) | Xent Color 0.0242(0.0312) | Loss 2.7931(3.0555) | Error 0.0411(0.0427) | Error Color 0.0044(0.0064) |Steps 512(472.36) | Grad Norm 3.4253(3.8616) | Total Time 0.00(0.00)\n",
      "Iter 3470 | Time 11.3034(11.3101) | Bit/dim 0.9582(0.9747) | Xent 0.1061(0.1355) | Xent Color 0.0194(0.0291) | Loss 2.6607(2.9640) | Error 0.0422(0.0420) | Error Color 0.0011(0.0057) |Steps 458(469.41) | Grad Norm 3.4169(3.7520) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 10.7583(11.1862) | Bit/dim 0.9568(0.9688) | Xent 0.1471(0.1354) | Xent Color 0.0301(0.0279) | Loss 2.7015(2.8944) | Error 0.0444(0.0424) | Error Color 0.0100(0.0057) |Steps 446(466.83) | Grad Norm 3.7952(4.3575) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 13.0941(11.4087) | Bit/dim 1.7081(1.0863) | Xent 0.6096(0.2754) | Xent Color 0.5722(0.5476) | Loss 4.7139(3.3776) | Error 0.2100(0.0865) | Error Color 0.2444(0.0849) |Steps 578(475.31) | Grad Norm 28.6709(19.5334) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 69.3361, Epoch Time 846.5498(744.2290), Bit/dim 1.5689(best: 0.9717), Xent 0.2205, Xent Color 0.1675. Loss 1.6659, Error 0.0718(best: 0.0245), Error Color 0.0529(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 13.2815(11.6881) | Bit/dim 1.5154(1.2410) | Xent 0.3251(0.2939) | Xent Color 0.2094(0.4954) | Loss 4.1027(4.3379) | Error 0.0944(0.0905) | Error Color 0.0722(0.0975) |Steps 566(495.39) | Grad Norm 7.6576(18.2226) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 12.2880(11.8875) | Bit/dim 1.3763(1.2927) | Xent 0.2122(0.2889) | Xent Color 0.1361(0.4093) | Loss 3.7085(4.2126) | Error 0.0589(0.0896) | Error Color 0.0389(0.0861) |Steps 524(508.57) | Grad Norm 3.8690(15.1177) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 12.5366(11.8783) | Bit/dim 1.2636(1.2989) | Xent 0.2083(0.2663) | Xent Color 0.1063(0.3347) | Loss 3.4407(4.0351) | Error 0.0633(0.0828) | Error Color 0.0278(0.0730) |Steps 506(509.74) | Grad Norm 2.9714(12.1568) | Total Time 0.00(0.00)\n",
      "Iter 3530 | Time 12.2634(11.9045) | Bit/dim 1.2000(1.2779) | Xent 0.1458(0.2418) | Xent Color 0.0720(0.2708) | Loss 3.2480(3.8495) | Error 0.0522(0.0762) | Error Color 0.0122(0.0596) |Steps 518(513.44) | Grad Norm 3.2569(9.7546) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 12.9621(11.9526) | Bit/dim 1.1415(1.2465) | Xent 0.1690(0.2216) | Xent Color 0.0691(0.2199) | Loss 3.1621(3.6813) | Error 0.0589(0.0697) | Error Color 0.0133(0.0483) |Steps 512(514.72) | Grad Norm 2.9648(8.3392) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 12.6491(12.0959) | Bit/dim 1.0992(1.2131) | Xent 0.1690(0.2063) | Xent Color 0.0785(0.1801) | Loss 3.1529(3.5435) | Error 0.0578(0.0657) | Error Color 0.0222(0.0398) |Steps 548(521.12) | Grad Norm 3.2299(7.2662) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 12.2682(12.1656) | Bit/dim 1.0812(1.1807) | Xent 0.1522(0.1951) | Xent Color 0.0479(0.1473) | Loss 3.0988(3.4168) | Error 0.0444(0.0620) | Error Color 0.0100(0.0322) |Steps 518(522.11) | Grad Norm 1.3255(6.0814) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 64.3927, Epoch Time 891.1087(748.6354), Bit/dim 1.0647(best: 0.9717), Xent 0.0913, Xent Color 0.0202. Loss 1.0926, Error 0.0310(best: 0.0245), Error Color 0.0013(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 12.2900(12.1247) | Bit/dim 1.0535(1.1493) | Xent 0.1372(0.1825) | Xent Color 0.0493(0.1220) | Loss 2.9557(3.8576) | Error 0.0400(0.0578) | Error Color 0.0056(0.0263) |Steps 524(521.25) | Grad Norm 3.8907(5.0968) | Total Time 0.00(0.00)\n",
      "Iter 3580 | Time 12.9357(12.1278) | Bit/dim 1.0318(1.1210) | Xent 0.1831(0.1720) | Xent Color 0.0513(0.1033) | Loss 3.0148(3.6247) | Error 0.0467(0.0538) | Error Color 0.0133(0.0225) |Steps 536(521.75) | Grad Norm 1.8795(4.4996) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 13.0049(12.1837) | Bit/dim 1.0222(1.0962) | Xent 0.1127(0.1638) | Xent Color 0.0445(0.0880) | Loss 2.9644(3.4446) | Error 0.0322(0.0504) | Error Color 0.0078(0.0191) |Steps 524(521.70) | Grad Norm 2.7297(4.1610) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 11.4823(12.1467) | Bit/dim 1.0091(1.0743) | Xent 0.1444(0.1578) | Xent Color 0.0408(0.0759) | Loss 2.8869(3.3038) | Error 0.0422(0.0488) | Error Color 0.0100(0.0164) |Steps 524(521.08) | Grad Norm 5.2244(3.9463) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 11.9489(12.0980) | Bit/dim 0.9959(1.0560) | Xent 0.1426(0.1517) | Xent Color 0.0342(0.0662) | Loss 2.8141(3.1871) | Error 0.0489(0.0467) | Error Color 0.0067(0.0144) |Steps 530(520.76) | Grad Norm 2.6204(4.2155) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 12.0599(12.0584) | Bit/dim 0.9944(1.0403) | Xent 0.1100(0.1445) | Xent Color 0.0303(0.0584) | Loss 2.8546(3.1044) | Error 0.0367(0.0444) | Error Color 0.0056(0.0126) |Steps 530(519.54) | Grad Norm 1.8693(4.4491) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 12.5008(12.0514) | Bit/dim 0.9850(1.0275) | Xent 0.1364(0.1405) | Xent Color 0.0505(0.0532) | Loss 2.8636(3.0341) | Error 0.0389(0.0439) | Error Color 0.0189(0.0116) |Steps 506(515.92) | Grad Norm 16.3791(5.3364) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 61.9116, Epoch Time 879.7937(752.5701), Bit/dim 0.9877(best: 0.9717), Xent 0.0799, Xent Color 0.0216. Loss 1.0130, Error 0.0274(best: 0.0245), Error Color 0.0018(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 11.7727(12.0277) | Bit/dim 0.9911(1.0166) | Xent 0.1349(0.1386) | Xent Color 0.2717(0.0615) | Loss 2.9384(3.4316) | Error 0.0356(0.0427) | Error Color 0.1167(0.0164) |Steps 506(512.92) | Grad Norm 61.6853(10.0166) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 12.2053(12.0803) | Bit/dim 2.0004(1.2525) | Xent 0.3431(0.2651) | Xent Color 0.6174(0.5742) | Loss 5.2078(4.0223) | Error 0.1100(0.0820) | Error Color 0.2478(0.1095) |Steps 548(516.27) | Grad Norm 12.5885(23.9762) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 12.8106(12.3098) | Bit/dim 1.6474(1.3954) | Xent 0.2547(0.2848) | Xent Color 0.2118(0.4953) | Loss 4.4076(4.2058) | Error 0.0822(0.0881) | Error Color 0.0744(0.1059) |Steps 554(529.50) | Grad Norm 4.9462(19.3170) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 11.6666(12.2872) | Bit/dim 1.4160(1.4278) | Xent 0.1721(0.2688) | Xent Color 0.1394(0.4053) | Loss 3.7541(4.1536) | Error 0.0656(0.0839) | Error Color 0.0311(0.0898) |Steps 482(524.08) | Grad Norm 3.1269(15.3078) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 11.3111(12.1820) | Bit/dim 1.2795(1.4006) | Xent 0.1688(0.2442) | Xent Color 0.1198(0.3358) | Loss 3.4702(4.0063) | Error 0.0600(0.0773) | Error Color 0.0344(0.0769) |Steps 506(518.56) | Grad Norm 2.5952(12.0344) | Total Time 0.00(0.00)\n",
      "Iter 3690 | Time 12.6081(12.1199) | Bit/dim 1.2053(1.3562) | Xent 0.1815(0.2254) | Xent Color 0.0818(0.2765) | Loss 3.3178(3.8418) | Error 0.0556(0.0712) | Error Color 0.0222(0.0650) |Steps 494(513.03) | Grad Norm 2.1010(9.4766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 64.9675, Epoch Time 889.4001(756.6750), Bit/dim 1.1576(best: 0.9717), Xent 0.1029, Xent Color 0.0298. Loss 1.1907, Error 0.0338(best: 0.0245), Error Color 0.0014(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 11.7995(12.0930) | Bit/dim 1.1356(1.3055) | Xent 0.1531(0.2085) | Xent Color 0.0853(0.2268) | Loss 3.1417(4.3016) | Error 0.0533(0.0659) | Error Color 0.0222(0.0540) |Steps 512(512.27) | Grad Norm 1.9377(7.5838) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 11.5901(12.0058) | Bit/dim 1.1155(1.2574) | Xent 0.1510(0.1948) | Xent Color 0.0671(0.1852) | Loss 3.1877(4.0183) | Error 0.0544(0.0617) | Error Color 0.0144(0.0440) |Steps 512(512.27) | Grad Norm 1.7118(6.1036) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 11.9880(11.9842) | Bit/dim 1.0664(1.2128) | Xent 0.1447(0.1851) | Xent Color 0.0636(0.1521) | Loss 3.1033(3.7806) | Error 0.0389(0.0576) | Error Color 0.0156(0.0359) |Steps 506(508.30) | Grad Norm 1.7180(4.9541) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 11.3360(11.9759) | Bit/dim 1.0487(1.1739) | Xent 0.1416(0.1713) | Xent Color 0.0567(0.1273) | Loss 3.0074(3.5874) | Error 0.0489(0.0536) | Error Color 0.0133(0.0302) |Steps 482(507.99) | Grad Norm 1.7718(4.0900) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 12.2846(11.9534) | Bit/dim 1.0482(1.1416) | Xent 0.1179(0.1620) | Xent Color 0.0427(0.1064) | Loss 3.0537(3.4447) | Error 0.0389(0.0505) | Error Color 0.0056(0.0246) |Steps 506(506.71) | Grad Norm 2.5352(3.4973) | Total Time 0.00(0.00)\n",
      "Iter 3750 | Time 11.7145(11.9488) | Bit/dim 1.0352(1.1146) | Xent 0.1544(0.1536) | Xent Color 0.0488(0.0909) | Loss 2.9813(3.3321) | Error 0.0489(0.0482) | Error Color 0.0100(0.0212) |Steps 518(506.52) | Grad Norm 1.4194(3.1298) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 12.1218(11.9683) | Bit/dim 1.0197(1.0910) | Xent 0.1442(0.1480) | Xent Color 0.0441(0.0776) | Loss 3.0255(3.2412) | Error 0.0467(0.0466) | Error Color 0.0078(0.0179) |Steps 536(508.11) | Grad Norm 1.6307(2.7794) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 64.5289, Epoch Time 873.5719(760.1819), Bit/dim 1.0186(best: 0.9717), Xent 0.0803, Xent Color 0.0105. Loss 1.0413, Error 0.0261(best: 0.0245), Error Color 0.0004(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 11.9379(11.9878) | Bit/dim 1.0261(1.0719) | Xent 0.1255(0.1418) | Xent Color 0.0331(0.0667) | Loss 2.9886(3.7011) | Error 0.0422(0.0449) | Error Color 0.0056(0.0151) |Steps 518(508.75) | Grad Norm 0.9310(2.5083) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 11.5397(11.9663) | Bit/dim 0.9942(1.0541) | Xent 0.1165(0.1383) | Xent Color 0.0362(0.0586) | Loss 2.8309(3.4991) | Error 0.0278(0.0431) | Error Color 0.0044(0.0128) |Steps 494(508.48) | Grad Norm 1.8140(2.2909) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 12.1173(11.9762) | Bit/dim 1.0001(1.0403) | Xent 0.1135(0.1356) | Xent Color 0.0428(0.0524) | Loss 2.9137(3.3409) | Error 0.0411(0.0422) | Error Color 0.0089(0.0111) |Steps 494(507.35) | Grad Norm 3.1976(2.5137) | Total Time 0.00(0.00)\n",
      "Iter 3800 | Time 11.1893(11.8709) | Bit/dim 0.9946(1.0272) | Xent 0.1340(0.1308) | Xent Color 0.0242(0.0467) | Loss 2.8381(3.2051) | Error 0.0456(0.0412) | Error Color 0.0011(0.0097) |Steps 470(503.24) | Grad Norm 1.7938(2.4419) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 12.5097(11.8488) | Bit/dim 0.9838(1.0160) | Xent 0.1288(0.1286) | Xent Color 0.0317(0.0431) | Loss 2.9066(3.1164) | Error 0.0322(0.0402) | Error Color 0.0044(0.0089) |Steps 494(502.90) | Grad Norm 1.6513(2.2388) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 11.5235(11.7540) | Bit/dim 0.9702(1.0062) | Xent 0.0776(0.1258) | Xent Color 0.0280(0.0396) | Loss 2.6822(3.0387) | Error 0.0233(0.0386) | Error Color 0.0044(0.0082) |Steps 482(500.19) | Grad Norm 2.4512(2.1016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 64.4097, Epoch Time 862.9398(763.2647), Bit/dim 0.9719(best: 0.9717), Xent 0.0716, Xent Color 0.0071. Loss 0.9916, Error 0.0230(best: 0.0245), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 12.0230(11.7478) | Bit/dim 0.9831(0.9988) | Xent 0.1269(0.1251) | Xent Color 0.0280(0.0368) | Loss 2.8821(3.6123) | Error 0.0356(0.0385) | Error Color 0.0056(0.0076) |Steps 506(500.96) | Grad Norm 1.7420(2.0319) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 12.0365(11.7907) | Bit/dim 0.9610(0.9903) | Xent 0.0767(0.1208) | Xent Color 0.0349(0.0343) | Loss 2.7909(3.4046) | Error 0.0278(0.0374) | Error Color 0.0100(0.0070) |Steps 506(500.38) | Grad Norm 3.2682(2.0854) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 11.7835(11.8091) | Bit/dim 0.9654(0.9831) | Xent 0.1218(0.1206) | Xent Color 0.0296(0.0320) | Loss 2.8244(3.2509) | Error 0.0400(0.0371) | Error Color 0.0100(0.0065) |Steps 458(498.44) | Grad Norm 1.7751(2.1716) | Total Time 0.00(0.00)\n",
      "Iter 3860 | Time 11.7936(11.7950) | Bit/dim 0.9573(0.9773) | Xent 0.1226(0.1235) | Xent Color 0.0223(0.0299) | Loss 2.8018(3.1294) | Error 0.0411(0.0378) | Error Color 0.0044(0.0058) |Steps 464(494.97) | Grad Norm 2.9624(2.1365) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 11.8378(11.7571) | Bit/dim 0.9561(0.9715) | Xent 0.0838(0.1192) | Xent Color 0.0217(0.0290) | Loss 2.6908(3.0344) | Error 0.0244(0.0368) | Error Color 0.0022(0.0057) |Steps 464(492.15) | Grad Norm 1.7982(2.0721) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 11.3937(11.6734) | Bit/dim 0.9458(0.9647) | Xent 0.1396(0.1217) | Xent Color 0.0215(0.0274) | Loss 2.7618(2.9615) | Error 0.0467(0.0377) | Error Color 0.0011(0.0051) |Steps 476(490.48) | Grad Norm 4.6214(2.3078) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 11.3241(11.6469) | Bit/dim 0.9438(0.9592) | Xent 0.1035(0.1205) | Xent Color 0.0193(0.0265) | Loss 2.6736(2.9044) | Error 0.0378(0.0375) | Error Color 0.0022(0.0050) |Steps 464(486.37) | Grad Norm 3.4597(2.4972) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 64.3765, Epoch Time 859.9735(766.1660), Bit/dim 0.9417(best: 0.9717), Xent 0.0711, Xent Color 0.0053. Loss 0.9608, Error 0.0220(best: 0.0230), Error Color 0.0004(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 11.1343(11.6556) | Bit/dim 0.9286(0.9537) | Xent 0.1201(0.1169) | Xent Color 0.0298(0.0253) | Loss 2.6640(3.3872) | Error 0.0422(0.0360) | Error Color 0.0056(0.0047) |Steps 446(482.43) | Grad Norm 1.9066(2.4217) | Total Time 0.00(0.00)\n",
      "Iter 3910 | Time 12.0798(11.6013) | Bit/dim 0.9417(0.9489) | Xent 0.1061(0.1159) | Xent Color 0.0236(0.0243) | Loss 2.7769(3.2171) | Error 0.0389(0.0361) | Error Color 0.0033(0.0044) |Steps 488(480.69) | Grad Norm 3.3709(2.4649) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 11.0239(11.5205) | Bit/dim 0.9448(0.9456) | Xent 0.1088(0.1130) | Xent Color 0.0239(0.0237) | Loss 2.7167(3.0856) | Error 0.0344(0.0349) | Error Color 0.0033(0.0042) |Steps 458(477.39) | Grad Norm 2.4881(2.4206) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 11.4505(11.4729) | Bit/dim 0.9326(0.9409) | Xent 0.1125(0.1137) | Xent Color 0.0256(0.0227) | Loss 2.6799(2.9876) | Error 0.0378(0.0357) | Error Color 0.0044(0.0039) |Steps 452(473.33) | Grad Norm 2.7518(2.4774) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 11.8114(11.4883) | Bit/dim 0.9289(0.9376) | Xent 0.1296(0.1141) | Xent Color 0.0202(0.0224) | Loss 2.7465(2.9169) | Error 0.0400(0.0358) | Error Color 0.0044(0.0040) |Steps 452(474.05) | Grad Norm 3.4334(2.8123) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 11.5490(11.4702) | Bit/dim 0.9313(0.9340) | Xent 0.1123(0.1122) | Xent Color 0.0190(0.0211) | Loss 2.6497(2.8560) | Error 0.0356(0.0347) | Error Color 0.0033(0.0035) |Steps 464(472.15) | Grad Norm 1.8716(2.6784) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 10.9165(11.4544) | Bit/dim 0.9156(0.9301) | Xent 0.0831(0.1091) | Xent Color 0.0186(0.0205) | Loss 2.7140(2.8130) | Error 0.0289(0.0341) | Error Color 0.0022(0.0036) |Steps 470(471.61) | Grad Norm 2.5963(2.7135) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 62.0249, Epoch Time 836.7255(768.2827), Bit/dim 0.9160(best: 0.9417), Xent 0.0653, Xent Color 0.0044. Loss 0.9335, Error 0.0218(best: 0.0220), Error Color 0.0002(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 10.8731(11.4673) | Bit/dim 0.9144(0.9262) | Xent 0.1158(0.1077) | Xent Color 0.0217(0.0198) | Loss 2.7132(3.2376) | Error 0.0344(0.0338) | Error Color 0.0056(0.0036) |Steps 470(472.41) | Grad Norm 3.3508(2.8191) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 12.4321(11.4289) | Bit/dim 0.9011(0.9226) | Xent 0.1372(0.1067) | Xent Color 0.0303(0.0194) | Loss 2.6749(3.0890) | Error 0.0422(0.0338) | Error Color 0.0044(0.0034) |Steps 476(471.40) | Grad Norm 5.0637(3.2111) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 11.2976(11.3328) | Bit/dim 0.9170(0.9190) | Xent 0.0769(0.1067) | Xent Color 0.0169(0.0188) | Loss 2.6547(2.9733) | Error 0.0222(0.0338) | Error Color 0.0044(0.0034) |Steps 446(468.02) | Grad Norm 5.5768(3.5486) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 11.2722(11.2754) | Bit/dim 0.9075(0.9159) | Xent 0.1192(0.1071) | Xent Color 0.0091(0.0180) | Loss 2.7143(2.8873) | Error 0.0400(0.0336) | Error Color 0.0000(0.0032) |Steps 458(465.16) | Grad Norm 1.4298(3.7998) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 10.3559(11.1712) | Bit/dim 0.9143(0.9135) | Xent 0.1191(0.1066) | Xent Color 0.0251(0.0189) | Loss 2.6219(2.8206) | Error 0.0344(0.0333) | Error Color 0.0067(0.0035) |Steps 422(459.92) | Grad Norm 11.2749(4.9527) | Total Time 0.00(0.00)\n",
      "Iter 4020 | Time 10.8727(11.1128) | Bit/dim 0.9014(0.9107) | Xent 0.0899(0.1061) | Xent Color 0.0113(0.0185) | Loss 2.6358(2.7684) | Error 0.0289(0.0329) | Error Color 0.0011(0.0035) |Steps 446(457.74) | Grad Norm 3.5081(5.3140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 61.5440, Epoch Time 815.5555(769.7009), Bit/dim 0.8969(best: 0.9160), Xent 0.0642, Xent Color 0.0026. Loss 0.9136, Error 0.0211(best: 0.0218), Error Color 0.0000(best: 0.0001)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 10.7435(11.0238) | Bit/dim 0.9006(0.9077) | Xent 0.1051(0.1061) | Xent Color 0.0127(0.0174) | Loss 2.5663(3.2517) | Error 0.0378(0.0335) | Error Color 0.0033(0.0033) |Steps 422(454.16) | Grad Norm 5.0067(5.1729) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 11.0560(10.9743) | Bit/dim 0.8987(0.9054) | Xent 0.1090(0.1075) | Xent Color 0.0302(0.0175) | Loss 2.6558(3.0910) | Error 0.0378(0.0335) | Error Color 0.0056(0.0032) |Steps 422(451.88) | Grad Norm 7.7400(5.2752) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 10.6732(10.9512) | Bit/dim 0.9041(0.9033) | Xent 0.1082(0.1067) | Xent Color 0.0105(0.0168) | Loss 2.5817(2.9680) | Error 0.0322(0.0328) | Error Color 0.0000(0.0030) |Steps 422(450.90) | Grad Norm 4.7497(4.9524) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 11.6708(10.9288) | Bit/dim 0.8865(0.8985) | Xent 0.1151(0.1052) | Xent Color 0.0112(0.0158) | Loss 2.5669(2.8647) | Error 0.0322(0.0327) | Error Color 0.0022(0.0027) |Steps 416(448.46) | Grad Norm 4.1648(4.5991) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 10.5379(10.8629) | Bit/dim 0.9062(0.8971) | Xent 0.0837(0.1069) | Xent Color 0.0246(0.0155) | Loss 2.6512(2.7948) | Error 0.0278(0.0331) | Error Color 0.0033(0.0026) |Steps 422(446.87) | Grad Norm 12.9204(5.2191) | Total Time 0.00(0.00)\n",
      "Iter 4080 | Time 10.6677(10.8376) | Bit/dim 0.8918(0.8948) | Xent 0.0746(0.1034) | Xent Color 0.0100(0.0159) | Loss 2.5381(2.7415) | Error 0.0256(0.0324) | Error Color 0.0000(0.0029) |Steps 446(446.16) | Grad Norm 4.7094(6.5096) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 10.5813(10.7738) | Bit/dim 0.8802(0.8911) | Xent 0.0923(0.1025) | Xent Color 0.0153(0.0155) | Loss 2.5355(2.6932) | Error 0.0300(0.0324) | Error Color 0.0022(0.0027) |Steps 410(442.58) | Grad Norm 3.5449(6.9344) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 58.3640, Epoch Time 790.5612(770.3267), Bit/dim 0.8874(best: 0.8969), Xent 0.0583, Xent Color 0.0027. Loss 0.9027, Error 0.0190(best: 0.0211), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 10.2793(10.7591) | Bit/dim 0.8740(0.8888) | Xent 0.1135(0.0991) | Xent Color 0.0135(0.0150) | Loss 2.5591(3.1328) | Error 0.0322(0.0314) | Error Color 0.0022(0.0026) |Steps 440(443.31) | Grad Norm 3.9784(6.9837) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 10.5501(10.7245) | Bit/dim 0.8735(0.8842) | Xent 0.0881(0.0980) | Xent Color 0.0116(0.0141) | Loss 2.5217(2.9732) | Error 0.0222(0.0312) | Error Color 0.0011(0.0023) |Steps 428(441.24) | Grad Norm 2.2815(6.2974) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 10.0506(10.6581) | Bit/dim 0.8870(0.8831) | Xent 0.1177(0.0987) | Xent Color 0.0259(0.0136) | Loss 2.5049(2.8606) | Error 0.0322(0.0308) | Error Color 0.0089(0.0022) |Steps 410(438.87) | Grad Norm 23.1272(7.1315) | Total Time 0.00(0.00)\n",
      "Iter 4130 | Time 10.7907(10.6982) | Bit/dim 1.5315(1.0160) | Xent 0.2569(0.1476) | Xent Color 0.4438(0.4378) | Loss 4.0510(3.2420) | Error 0.0767(0.0467) | Error Color 0.1867(0.0887) |Steps 482(443.38) | Grad Norm 16.9503(21.9086) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 11.1457(10.8084) | Bit/dim 1.2966(1.1230) | Xent 0.1664(0.1638) | Xent Color 0.1828(0.4024) | Loss 3.4699(3.3728) | Error 0.0622(0.0514) | Error Color 0.0589(0.0960) |Steps 482(450.23) | Grad Norm 4.8477(18.9574) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 11.2596(10.8937) | Bit/dim 1.1263(1.1418) | Xent 0.1596(0.1664) | Xent Color 0.1291(0.3380) | Loss 3.1597(3.3363) | Error 0.0511(0.0522) | Error Color 0.0344(0.0833) |Steps 476(455.06) | Grad Norm 4.4110(15.1187) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 59.0271, Epoch Time 797.9852(771.1565), Bit/dim 1.0590(best: 0.8874), Xent 0.0891, Xent Color 0.0300. Loss 1.0887, Error 0.0282(best: 0.0190), Error Color 0.0024(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 11.9392(11.0237) | Bit/dim 1.0551(1.1254) | Xent 0.1524(0.1613) | Xent Color 0.0814(0.2742) | Loss 2.9117(3.8038) | Error 0.0456(0.0513) | Error Color 0.0178(0.0680) |Steps 476(458.23) | Grad Norm 3.2092(12.0597) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 11.4080(11.1786) | Bit/dim 0.9934(1.0965) | Xent 0.1190(0.1541) | Xent Color 0.0468(0.2170) | Loss 2.7891(3.5549) | Error 0.0467(0.0496) | Error Color 0.0100(0.0534) |Steps 482(461.57) | Grad Norm 1.9383(9.4798) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 11.7099(11.2399) | Bit/dim 0.9598(1.0626) | Xent 0.1355(0.1447) | Xent Color 0.0453(0.1730) | Loss 2.7604(3.3435) | Error 0.0389(0.0464) | Error Color 0.0133(0.0425) |Steps 476(464.91) | Grad Norm 1.9303(7.4558) | Total Time 0.00(0.00)\n",
      "Iter 4190 | Time 11.7482(11.2595) | Bit/dim 0.9283(1.0324) | Xent 0.0921(0.1347) | Xent Color 0.0428(0.1394) | Loss 2.6424(3.1745) | Error 0.0267(0.0430) | Error Color 0.0100(0.0336) |Steps 482(466.03) | Grad Norm 1.1945(5.9069) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 11.5103(11.3480) | Bit/dim 0.9130(1.0035) | Xent 0.1110(0.1283) | Xent Color 0.0348(0.1136) | Loss 2.7016(3.0360) | Error 0.0333(0.0409) | Error Color 0.0044(0.0273) |Steps 476(468.30) | Grad Norm 1.3960(4.7385) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 12.4213(11.3958) | Bit/dim 0.9164(0.9809) | Xent 0.1251(0.1241) | Xent Color 0.0394(0.0933) | Loss 2.6700(2.9353) | Error 0.0344(0.0389) | Error Color 0.0089(0.0220) |Steps 476(471.15) | Grad Norm 2.3043(3.9825) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 10.9694(11.3977) | Bit/dim 0.8936(0.9606) | Xent 0.0883(0.1177) | Xent Color 0.0351(0.0778) | Loss 2.5987(2.8501) | Error 0.0267(0.0366) | Error Color 0.0056(0.0178) |Steps 476(473.23) | Grad Norm 2.1736(3.4728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 59.3267, Epoch Time 837.8031(773.1559), Bit/dim 0.8998(best: 0.8874), Xent 0.0665, Xent Color 0.0077. Loss 0.9183, Error 0.0219(best: 0.0190), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4230 | Time 11.4846(11.4358) | Bit/dim 0.8998(0.9439) | Xent 0.1267(0.1134) | Xent Color 0.0316(0.0663) | Loss 2.6365(3.2895) | Error 0.0422(0.0357) | Error Color 0.0089(0.0153) |Steps 476(472.36) | Grad Norm 1.9441(3.1982) | Total Time 0.00(0.00)\n",
      "Iter 4240 | Time 11.6413(11.4502) | Bit/dim 0.8954(0.9304) | Xent 0.0741(0.1078) | Xent Color 0.0340(0.0566) | Loss 2.5752(3.1044) | Error 0.0267(0.0340) | Error Color 0.0067(0.0126) |Steps 488(471.27) | Grad Norm 2.4864(2.8951) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 10.6785(11.3921) | Bit/dim 0.8687(0.9188) | Xent 0.1014(0.1060) | Xent Color 0.0227(0.0489) | Loss 2.5486(2.9648) | Error 0.0367(0.0328) | Error Color 0.0033(0.0107) |Steps 464(469.38) | Grad Norm 2.6365(2.7035) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 11.1843(11.3194) | Bit/dim 0.8761(0.9080) | Xent 0.0803(0.1025) | Xent Color 0.0304(0.0432) | Loss 2.5169(2.8494) | Error 0.0222(0.0315) | Error Color 0.0078(0.0090) |Steps 428(464.14) | Grad Norm 2.9009(2.5770) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 11.2707(11.1600) | Bit/dim 0.8846(0.9007) | Xent 0.1144(0.1010) | Xent Color 0.0197(0.0380) | Loss 2.5075(2.7644) | Error 0.0356(0.0305) | Error Color 0.0022(0.0075) |Steps 446(458.70) | Grad Norm 1.9283(2.5827) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 10.6699(11.0550) | Bit/dim 0.8769(0.8941) | Xent 0.1158(0.0993) | Xent Color 0.0226(0.0343) | Loss 2.5375(2.6987) | Error 0.0400(0.0302) | Error Color 0.0033(0.0068) |Steps 452(454.21) | Grad Norm 3.4947(2.7360) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 11.0668(11.0397) | Bit/dim 0.8644(0.8883) | Xent 0.0811(0.0980) | Xent Color 0.0264(0.0312) | Loss 2.4461(2.6490) | Error 0.0311(0.0300) | Error Color 0.0044(0.0059) |Steps 428(451.66) | Grad Norm 3.0871(2.9868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 56.7291, Epoch Time 811.2069(774.2974), Bit/dim 0.8690(best: 0.8874), Xent 0.0628, Xent Color 0.0050. Loss 0.8860, Error 0.0202(best: 0.0190), Error Color 0.0001(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4300 | Time 10.6417(11.0293) | Bit/dim 0.8780(0.8840) | Xent 0.1087(0.0974) | Xent Color 0.0253(0.0291) | Loss 2.4985(3.0219) | Error 0.0367(0.0302) | Error Color 0.0044(0.0054) |Steps 428(448.09) | Grad Norm 4.5313(3.1445) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 10.6829(10.9643) | Bit/dim 0.8695(0.8788) | Xent 0.0476(0.0961) | Xent Color 0.0180(0.0271) | Loss 2.5004(2.8808) | Error 0.0122(0.0299) | Error Color 0.0022(0.0049) |Steps 434(445.40) | Grad Norm 2.6101(3.0879) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 11.1147(10.9087) | Bit/dim 0.8730(0.8746) | Xent 0.0788(0.0967) | Xent Color 0.0203(0.0258) | Loss 2.4402(2.7736) | Error 0.0289(0.0295) | Error Color 0.0022(0.0045) |Steps 440(443.85) | Grad Norm 5.4593(3.1174) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 11.5426(10.9230) | Bit/dim 0.8678(0.8710) | Xent 0.0799(0.0932) | Xent Color 0.0186(0.0243) | Loss 2.5348(2.6948) | Error 0.0211(0.0282) | Error Color 0.0011(0.0043) |Steps 458(441.10) | Grad Norm 5.7723(3.5560) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 10.3503(10.8450) | Bit/dim 0.8553(0.8678) | Xent 0.0733(0.0919) | Xent Color 0.0138(0.0228) | Loss 2.4185(2.6325) | Error 0.0189(0.0281) | Error Color 0.0000(0.0040) |Steps 422(440.51) | Grad Norm 6.8762(4.0619) | Total Time 0.00(0.00)\n",
      "Iter 4350 | Time 10.8308(10.8500) | Bit/dim 0.8377(0.8640) | Xent 0.0636(0.0912) | Xent Color 0.0195(0.0216) | Loss 2.4032(2.5829) | Error 0.0233(0.0276) | Error Color 0.0056(0.0039) |Steps 416(441.60) | Grad Norm 3.9054(4.2231) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 56.1644, Epoch Time 790.9650(774.7974), Bit/dim 0.8535(best: 0.8690), Xent 0.0594, Xent Color 0.0034. Loss 0.8692, Error 0.0185(best: 0.0190), Error Color 0.0000(best: 0.0000)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4360 | Time 11.3095(10.8345) | Bit/dim 0.8469(0.8607) | Xent 0.0965(0.0924) | Xent Color 0.0172(0.0207) | Loss 2.4532(3.0375) | Error 0.0267(0.0279) | Error Color 0.0033(0.0036) |Steps 440(441.24) | Grad Norm 3.1715(4.2994) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 10.8538(10.8347) | Bit/dim 0.8595(0.8584) | Xent 0.0877(0.0922) | Xent Color 0.0116(0.0197) | Loss 2.4344(2.8839) | Error 0.0278(0.0284) | Error Color 0.0011(0.0033) |Steps 452(440.69) | Grad Norm 4.0096(4.8616) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 10.9251(10.8311) | Bit/dim 0.8387(0.8553) | Xent 0.0989(0.0909) | Xent Color 0.0173(0.0188) | Loss 2.4797(2.7660) | Error 0.0278(0.0280) | Error Color 0.0033(0.0031) |Steps 434(440.67) | Grad Norm 4.9642(5.0324) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 11.7962(10.8230) | Bit/dim 0.8420(0.8539) | Xent 0.0813(0.0903) | Xent Color 0.0157(0.0181) | Loss 2.4624(2.6826) | Error 0.0189(0.0274) | Error Color 0.0022(0.0031) |Steps 440(439.71) | Grad Norm 7.6001(5.8664) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond_multiscale_beta.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/infocnf_conditional_disentangle_colormnist_bs900_sratio_1_4th_drop_0_5_rl_stdscale_6_2cond_linear_multiscale_beta_100_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --cond_nn linear --y_color 10 --y_class 10 --beta 100.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
