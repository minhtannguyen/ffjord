{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "        \n",
      "        raise SystemExit(0)\n",
      "    \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=1800, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs1800_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 17.5998(34.0919) | Bit/dim 8.7408(8.9446) | Xent 2.2795(2.2999) | Loss 21.7513(22.3302) | Error 0.7567(0.8605) Steps 508(502.60) | Grad Norm 22.4776(29.1592) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 18.1732(29.6836) | Bit/dim 8.5515(8.8628) | Xent 2.2188(2.2864) | Loss 21.2111(22.1228) | Error 0.7422(0.8322) Steps 484(499.59) | Grad Norm 9.2362(25.1953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 74.9470, Epoch Time 583.8388(583.8388), Bit/dim 8.3881(best: inf), Xent 2.1758, Loss 9.4759, Error 0.7368(best: inf)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0030 | Time 16.6632(26.3888) | Bit/dim 8.3368(8.7485) | Xent 2.1801(2.2620) | Loss 20.8366(22.4763) | Error 0.7472(0.8103) Steps 502(498.00) | Grad Norm 7.5442(20.4970) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 17.6530(23.9876) | Bit/dim 8.2143(8.6191) | Xent 2.1302(2.2310) | Loss 20.6026(22.0109) | Error 0.7306(0.7916) Steps 496(497.94) | Grad Norm 5.7379(16.8287) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 18.3591(22.2700) | Bit/dim 7.9049(8.4689) | Xent 2.0955(2.1983) | Loss 20.0712(21.5446) | Error 0.7294(0.7737) Steps 502(498.32) | Grad Norm 5.2493(13.9100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 71.6922, Epoch Time 562.2984(583.1926), Bit/dim 7.7937(best: 8.3881), Xent 2.0707, Loss 8.8291, Error 0.6964(best: 0.7368)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0060 | Time 17.3820(21.1330) | Bit/dim 7.6466(8.2875) | Xent 2.0514(2.1660) | Loss 19.3488(21.6344) | Error 0.6861(0.7549) Steps 490(503.33) | Grad Norm 5.2931(11.6528) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 17.7099(20.3689) | Bit/dim 7.3727(8.0768) | Xent 2.0580(2.1394) | Loss 18.6578(20.9282) | Error 0.6833(0.7376) Steps 526(504.91) | Grad Norm 4.4261(9.8246) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 18.3319(19.7889) | Bit/dim 7.1698(7.8599) | Xent 2.0771(2.1182) | Loss 18.2895(20.2938) | Error 0.6994(0.7234) Steps 520(509.70) | Grad Norm 2.8802(8.1575) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 72.8440, Epoch Time 577.9461(583.0352), Bit/dim 7.1549(best: 7.7937), Xent 2.0572, Loss 8.1835, Error 0.6799(best: 0.6964)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0090 | Time 18.6869(19.3677) | Bit/dim 7.0895(7.6653) | Xent 2.0928(2.1043) | Loss 18.2007(20.2708) | Error 0.7294(0.7170) Steps 520(514.04) | Grad Norm 3.5190(6.7557) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 18.7343(19.2456) | Bit/dim 7.0198(7.5029) | Xent 2.0704(2.0946) | Loss 18.1381(19.6868) | Error 0.7100(0.7133) Steps 520(516.23) | Grad Norm 1.5518(5.6164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 75.4385, Epoch Time 598.2207(583.4908), Bit/dim 6.9972(best: 7.1549), Xent 2.0486, Loss 8.0215, Error 0.6869(best: 0.6799)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0110 | Time 20.3724(19.2360) | Bit/dim 6.9970(7.3729) | Xent 2.0595(2.0869) | Loss 18.0546(19.9286) | Error 0.7044(0.7099) Steps 556(521.45) | Grad Norm 2.2768(4.7291) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 20.2150(19.2527) | Bit/dim 6.9577(7.2673) | Xent 2.0377(2.0773) | Loss 17.7829(19.3946) | Error 0.6833(0.7060) Steps 514(525.55) | Grad Norm 5.2005(4.5113) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 20.0740(19.4925) | Bit/dim 6.9311(7.1816) | Xent 2.0535(2.0681) | Loss 17.8413(18.9820) | Error 0.7022(0.7019) Steps 562(530.77) | Grad Norm 6.7319(4.3985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 76.7316, Epoch Time 628.5434(584.8423), Bit/dim 6.8935(best: 6.9972), Xent 2.0172, Loss 7.9021, Error 0.6726(best: 0.6799)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0140 | Time 19.0186(19.5835) | Bit/dim 6.8725(7.1041) | Xent 2.0294(2.0566) | Loss 17.5230(19.2425) | Error 0.6861(0.6973) Steps 538(533.84) | Grad Norm 9.4754(4.4350) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 20.6937(19.6956) | Bit/dim 6.8017(7.0329) | Xent 2.0172(2.0463) | Loss 17.3407(18.7978) | Error 0.6811(0.6938) Steps 526(535.73) | Grad Norm 9.6490(4.9311) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 19.6001(19.8286) | Bit/dim 6.7152(6.9580) | Xent 2.1003(2.0411) | Loss 17.4781(18.4436) | Error 0.7400(0.6930) Steps 544(539.58) | Grad Norm 55.1550(8.0913) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 77.1180, Epoch Time 636.6323(586.3960), Bit/dim 6.6685(best: 6.8935), Xent 2.0023, Loss 7.6696, Error 0.6748(best: 0.6726)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0170 | Time 19.3662(19.9730) | Bit/dim 6.5648(6.8726) | Xent 1.9992(2.0363) | Loss 17.0576(18.7047) | Error 0.6856(0.6928) Steps 550(542.70) | Grad Norm 26.4193(14.5290) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 18.4545(19.8138) | Bit/dim 6.3861(6.7665) | Xent 2.0564(2.0302) | Loss 16.6393(18.2110) | Error 0.7206(0.6914) Steps 538(543.15) | Grad Norm 78.5446(20.6404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 77.5971, Epoch Time 631.7510(587.7567), Bit/dim 6.2540(best: 6.6685), Xent 1.9872, Loss 7.2476, Error 0.6648(best: 0.6726)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0190 | Time 20.3290(19.8425) | Bit/dim 6.2420(6.6656) | Xent 1.9966(2.0540) | Loss 39.8321(18.5242) | Error 0.6700(0.7044) Steps 538(544.83) | Grad Norm 11.7570(37.9762) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 18.3466(19.7997) | Bit/dim 6.1013(6.5390) | Xent 2.0901(2.0492) | Loss 16.0575(17.9218) | Error 0.7411(0.7053) Steps 532(545.46) | Grad Norm 64.8986(38.8405) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 18.7363(19.7575) | Bit/dim 6.2460(6.4087) | Xent 3.1308(2.0895) | Loss 17.7187(17.4688) | Error 0.8889(0.7157) Steps 568(547.19) | Grad Norm 361.4219(57.7578) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 77.5074, Epoch Time 633.0558(589.1157), Bit/dim 5.9840(best: 6.2540), Xent 2.0410, Loss 7.0045, Error 0.6813(best: 0.6648)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0220 | Time 19.8786(19.8668) | Bit/dim 5.9606(6.3100) | Xent 2.0472(2.0924) | Loss 15.8150(17.7158) | Error 0.7022(0.7199) Steps 562(550.25) | Grad Norm 31.3584(58.1465) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 19.4268(19.6635) | Bit/dim 5.8187(6.1978) | Xent 2.0840(2.0924) | Loss 15.4988(17.1890) | Error 0.7267(0.7212) Steps 562(550.27) | Grad Norm 32.3717(48.5886) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 19.7897(19.4879) | Bit/dim 5.7669(6.0885) | Xent 2.0249(2.0784) | Loss 15.3231(16.7255) | Error 0.6861(0.7143) Steps 538(547.72) | Grad Norm 9.9049(41.1322) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 75.8870, Epoch Time 609.9547(589.7408), Bit/dim 5.7333(best: 5.9840), Xent 2.0106, Loss 6.7386, Error 0.6761(best: 0.6648)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0250 | Time 19.0581(19.3307) | Bit/dim 5.7230(5.9945) | Xent 2.0071(2.0619) | Loss 15.3236(16.9274) | Error 0.6828(0.7075) Steps 568(546.07) | Grad Norm 6.7505(36.7584) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 18.2948(19.2227) | Bit/dim 5.6567(5.9160) | Xent 1.9722(2.0465) | Loss 14.9690(16.4763) | Error 0.6800(0.7035) Steps 508(544.13) | Grad Norm 23.0735(35.8732) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 19.4520(19.1296) | Bit/dim 5.6385(5.8461) | Xent 1.9940(2.0291) | Loss 15.2135(16.0995) | Error 0.6800(0.6972) Steps 544(543.05) | Grad Norm 21.3970(30.9580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 76.5228, Epoch Time 603.0213(590.1393), Bit/dim 5.6420(best: 5.7333), Xent 1.9575, Loss 6.6208, Error 0.6502(best: 0.6648)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0280 | Time 18.9112(19.0635) | Bit/dim 5.6276(5.7942) | Xent 2.0189(2.0342) | Loss 15.0407(16.3400) | Error 0.7389(0.7032) Steps 544(541.87) | Grad Norm 53.1620(39.8559) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 19.0073(19.0850) | Bit/dim 5.5968(5.7477) | Xent 1.9752(2.0216) | Loss 15.0247(15.9937) | Error 0.6639(0.6971) Steps 532(539.27) | Grad Norm 13.6776(33.5426) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 77.1946, Epoch Time 607.6082(590.6633), Bit/dim 5.5825(best: 5.6420), Xent 1.9479, Loss 6.5565, Error 0.6509(best: 0.6502)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0300 | Time 18.0922(18.9825) | Bit/dim 5.6051(5.7076) | Xent 1.9494(2.0055) | Loss 14.8885(16.3840) | Error 0.6517(0.6887) Steps 520(538.48) | Grad Norm 4.6954(26.8392) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 18.7449(18.8580) | Bit/dim 5.5713(5.6700) | Xent 1.9594(1.9901) | Loss 14.9360(15.9766) | Error 0.6622(0.6825) Steps 526(539.45) | Grad Norm 4.7163(21.5710) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 18.5472(18.9655) | Bit/dim 5.5379(5.6370) | Xent 1.9391(1.9749) | Loss 14.8271(15.6657) | Error 0.6733(0.6760) Steps 532(539.20) | Grad Norm 9.5127(18.6035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 76.4701, Epoch Time 601.7992(590.9974), Bit/dim 5.5222(best: 5.5825), Xent 1.9081, Loss 6.4762, Error 0.6413(best: 0.6502)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0330 | Time 19.9841(19.0245) | Bit/dim 5.5467(5.6087) | Xent 1.9182(1.9621) | Loss 14.8639(16.0499) | Error 0.6483(0.6720) Steps 562(540.16) | Grad Norm 21.4318(19.7919) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 18.8278(19.2131) | Bit/dim 5.5179(5.5808) | Xent 1.9685(1.9571) | Loss 14.8690(15.7105) | Error 0.6706(0.6725) Steps 526(541.95) | Grad Norm 43.3339(24.7910) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 19.0637(19.3559) | Bit/dim 5.4541(5.5572) | Xent 1.9533(1.9593) | Loss 14.5946(15.4540) | Error 0.6694(0.6770) Steps 574(543.75) | Grad Norm 44.8267(32.4266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 75.6762, Epoch Time 625.1038(592.0206), Bit/dim 5.4640(best: 5.5222), Xent 1.9646, Loss 6.4462, Error 0.6972(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0360 | Time 19.9992(19.3342) | Bit/dim 5.3806(5.5256) | Xent 1.8898(1.9529) | Loss 14.4541(15.7806) | Error 0.6572(0.6738) Steps 550(545.14) | Grad Norm 27.8161(32.3953) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 20.2293(19.4197) | Bit/dim 5.3883(5.4919) | Xent 1.9086(1.9426) | Loss 14.3469(15.4387) | Error 0.6456(0.6688) Steps 556(546.49) | Grad Norm 30.1236(29.2222) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 79.8524, Epoch Time 625.8358(593.0350), Bit/dim 5.3434(best: 5.4640), Xent 1.8781, Loss 6.2824, Error 0.6309(best: 0.6413)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0380 | Time 18.8730(19.4812) | Bit/dim 5.3751(5.4562) | Xent 1.9119(1.9324) | Loss 14.3728(15.8537) | Error 0.6639(0.6656) Steps 538(548.02) | Grad Norm 30.6643(25.8818) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 21.2360(19.7127) | Bit/dim 5.2659(5.4180) | Xent 1.8853(1.9193) | Loss 14.2289(15.4398) | Error 0.6406(0.6611) Steps 562(551.83) | Grad Norm 15.3376(23.9524) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 20.5094(19.8990) | Bit/dim 5.3042(5.3825) | Xent 1.8880(1.9102) | Loss 14.4825(15.1350) | Error 0.6644(0.6595) Steps 574(556.94) | Grad Norm 25.6906(25.2590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 78.9636, Epoch Time 640.1436(594.4483), Bit/dim 5.2618(best: 5.3434), Xent 1.9529, Loss 6.2383, Error 0.6973(best: 0.6309)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0410 | Time 19.6154(19.9031) | Bit/dim 5.2191(5.3504) | Xent 1.8730(1.9117) | Loss 14.1884(15.5260) | Error 0.6367(0.6613) Steps 586(558.96) | Grad Norm 20.2102(30.2844) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 19.5483(19.9730) | Bit/dim 5.3196(5.3219) | Xent 1.9796(1.9145) | Loss 14.5098(15.1754) | Error 0.6994(0.6647) Steps 562(559.52) | Grad Norm 62.9356(32.3910) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 20.2218(19.9169) | Bit/dim 5.3392(5.3167) | Xent 2.0521(1.9533) | Loss 14.4724(14.9716) | Error 0.7267(0.6819) Steps 568(558.28) | Grad Norm 58.9376(39.0438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 79.9606, Epoch Time 636.3657(595.7058), Bit/dim 5.2910(best: 5.2618), Xent 1.9778, Loss 6.2799, Error 0.6796(best: 0.6309)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0440 | Time 18.9825(19.8803) | Bit/dim 5.2555(5.2966) | Xent 1.9235(1.9581) | Loss 14.1593(15.3610) | Error 0.6522(0.6833) Steps 568(557.23) | Grad Norm 13.3320(33.2743) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 20.8807(19.9041) | Bit/dim 5.1478(5.2640) | Xent 1.9256(1.9505) | Loss 13.8610(15.0075) | Error 0.6717(0.6801) Steps 532(554.21) | Grad Norm 20.2583(29.0672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 76.7720, Epoch Time 637.6830(596.9651), Bit/dim 5.1135(best: 5.2618), Xent 1.8676, Loss 6.0473, Error 0.6281(best: 0.6309)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0460 | Time 20.4120(20.0539) | Bit/dim 5.1223(5.2340) | Xent 1.8996(1.9391) | Loss 37.9990(15.4475) | Error 0.6656(0.6752) Steps 586(554.15) | Grad Norm 6.7519(24.7996) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 19.4420(20.0727) | Bit/dim 5.0814(5.1971) | Xent 1.8764(1.9251) | Loss 13.8857(15.0134) | Error 0.6589(0.6705) Steps 526(551.33) | Grad Norm 7.5121(22.6994) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 22.1249(20.2768) | Bit/dim 5.0303(5.1678) | Xent 1.8855(1.9097) | Loss 13.7562(14.6904) | Error 0.6594(0.6648) Steps 568(551.79) | Grad Norm 21.6414(22.8154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 77.3649, Epoch Time 651.8905(598.6129), Bit/dim 5.0698(best: 5.1135), Xent 1.8297, Loss 5.9847, Error 0.6250(best: 0.6281)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0490 | Time 21.8462(20.4817) | Bit/dim 5.0364(5.1403) | Xent 1.8211(1.9017) | Loss 13.5976(15.0772) | Error 0.6411(0.6617) Steps 574(552.27) | Grad Norm 19.6462(26.2157) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 20.6519(20.5992) | Bit/dim 5.0167(5.1093) | Xent 1.8531(1.8907) | Loss 13.5271(14.6898) | Error 0.6522(0.6586) Steps 556(554.61) | Grad Norm 27.8021(25.1172) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 21.0015(20.7640) | Bit/dim 4.9498(5.0752) | Xent 1.8412(1.8784) | Loss 13.4238(14.3741) | Error 0.6428(0.6550) Steps 586(557.84) | Grad Norm 27.1615(24.4153) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 81.1421, Epoch Time 668.1947(600.7004), Bit/dim 4.9509(best: 5.0698), Xent 1.7913, Loss 5.8466, Error 0.6103(best: 0.6250)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0520 | Time 21.1673(20.9418) | Bit/dim 4.9611(5.0486) | Xent 1.9621(1.8852) | Loss 13.7365(14.7831) | Error 0.7006(0.6594) Steps 604(561.66) | Grad Norm 61.2841(31.3840) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 22.8288(21.1992) | Bit/dim 4.9219(5.0228) | Xent 1.9267(1.8943) | Loss 13.5776(14.4642) | Error 0.6806(0.6651) Steps 556(562.40) | Grad Norm 19.9548(31.0650) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 22.5640(21.5379) | Bit/dim 4.9111(4.9926) | Xent 1.8939(1.8928) | Loss 13.5406(14.1992) | Error 0.6761(0.6645) Steps 598(566.23) | Grad Norm 21.8284(27.0705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 81.2770, Epoch Time 693.0768(603.4716), Bit/dim 4.9025(best: 4.9509), Xent 1.8539, Loss 5.8294, Error 0.6343(best: 0.6103)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0550 | Time 22.0896(21.6856) | Bit/dim 4.8920(4.9765) | Xent 1.8918(1.8934) | Loss 13.4817(14.5734) | Error 0.6528(0.6648) Steps 592(572.99) | Grad Norm 28.7560(30.9649) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 22.7776(21.9026) | Bit/dim 4.9027(4.9552) | Xent 1.8162(1.8827) | Loss 13.3129(14.2762) | Error 0.6267(0.6602) Steps 550(575.17) | Grad Norm 27.0885(30.0490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 81.2934, Epoch Time 702.9345(606.4555), Bit/dim 4.8326(best: 4.9025), Xent 1.7663, Loss 5.7158, Error 0.6062(best: 0.6103)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0570 | Time 21.8222(22.0625) | Bit/dim 4.8564(4.9281) | Xent 1.8360(1.8669) | Loss 13.2150(14.7107) | Error 0.6367(0.6548) Steps 574(576.03) | Grad Norm 16.7561(27.9603) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 23.4944(22.1389) | Bit/dim 4.8124(4.9018) | Xent 1.8437(1.8470) | Loss 13.1526(14.3023) | Error 0.6550(0.6484) Steps 568(578.09) | Grad Norm 47.6850(27.4458) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 23.2169(22.1882) | Bit/dim 4.8847(4.9087) | Xent 2.0051(1.9043) | Loss 13.5358(14.1387) | Error 0.7206(0.6683) Steps 598(579.63) | Grad Norm 25.0804(36.7415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 80.4828, Epoch Time 699.1983(609.2378), Bit/dim 4.8533(best: 4.8326), Xent 1.9058, Loss 5.8062, Error 0.6654(best: 0.6062)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0600 | Time 23.6461(22.1968) | Bit/dim 4.8419(4.8917) | Xent 1.8893(1.9135) | Loss 13.2715(14.5455) | Error 0.6622(0.6718) Steps 610(580.31) | Grad Norm 17.4393(30.9896) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 21.9669(22.1840) | Bit/dim 4.7648(4.8672) | Xent 1.8455(1.9103) | Loss 12.9571(14.1819) | Error 0.6467(0.6713) Steps 556(578.72) | Grad Norm 16.8842(26.2477) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 21.5313(22.2350) | Bit/dim 4.7634(4.8402) | Xent 1.8356(1.8905) | Loss 13.1358(13.8925) | Error 0.6444(0.6645) Steps 580(577.70) | Grad Norm 23.9053(23.2443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 81.1754, Epoch Time 702.1193(612.0243), Bit/dim 4.7435(best: 4.8326), Xent 1.7610, Loss 5.6240, Error 0.6159(best: 0.6062)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0630 | Time 23.1063(22.3695) | Bit/dim 4.7147(4.8146) | Xent 1.7949(1.8649) | Loss 12.8940(14.2062) | Error 0.6294(0.6547) Steps 580(578.17) | Grad Norm 24.7711(22.7823) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 23.6869(22.4092) | Bit/dim 4.8187(4.8033) | Xent 1.8890(1.8553) | Loss 13.1799(13.9076) | Error 0.6661(0.6517) Steps 568(579.89) | Grad Norm 54.1477(26.9322) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 81.7443, Epoch Time 711.5571(615.0102), Bit/dim 4.7393(best: 4.7435), Xent 1.7640, Loss 5.6213, Error 0.6096(best: 0.6062)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0650 | Time 22.7747(22.5560) | Bit/dim 4.7484(4.7905) | Xent 1.7988(1.8430) | Loss 13.0205(14.3693) | Error 0.6311(0.6478) Steps 592(585.99) | Grad Norm 29.7588(27.4566) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 23.8075(22.6125) | Bit/dim 4.7134(4.7705) | Xent 1.7348(1.8235) | Loss 12.8685(13.9818) | Error 0.6128(0.6423) Steps 598(589.52) | Grad Norm 26.8390(25.9657) | Total Time 0.00(0.00)\n",
      "Iter 0670 | Time 23.2086(22.7504) | Bit/dim 4.6935(4.7510) | Xent 1.7926(1.8060) | Loss 12.7433(13.6842) | Error 0.6339(0.6363) Steps 580(589.00) | Grad Norm 34.6509(26.5906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 83.5793, Epoch Time 722.7234(618.2416), Bit/dim 4.6659(best: 4.7393), Xent 1.6959, Loss 5.5139, Error 0.5964(best: 0.6062)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0680 | Time 23.8752(22.8566) | Bit/dim 4.6627(4.7301) | Xent 1.7497(1.7864) | Loss 12.8873(14.1080) | Error 0.6194(0.6299) Steps 604(591.82) | Grad Norm 35.8108(27.1326) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 22.0717(22.9041) | Bit/dim 4.6379(4.7202) | Xent 1.7508(1.7858) | Loss 12.8125(13.7891) | Error 0.6239(0.6309) Steps 610(592.53) | Grad Norm 16.4514(30.3803) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 23.2415(22.9452) | Bit/dim 4.6469(4.7032) | Xent 1.7165(1.7788) | Loss 12.9827(13.5430) | Error 0.6156(0.6289) Steps 622(595.75) | Grad Norm 8.1099(29.8160) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 83.9697, Epoch Time 723.2513(621.3919), Bit/dim 4.6458(best: 4.6659), Xent 1.6622, Loss 5.4769, Error 0.5789(best: 0.5964)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0710 | Time 24.5365(22.9734) | Bit/dim 4.7182(4.6891) | Xent 1.8260(1.7664) | Loss 12.9892(13.9533) | Error 0.6422(0.6244) Steps 586(597.69) | Grad Norm 66.6340(30.1264) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 23.1994(22.9661) | Bit/dim 4.6445(4.6926) | Xent 1.7745(1.7797) | Loss 12.9442(13.6827) | Error 0.6311(0.6304) Steps 610(596.57) | Grad Norm 19.3666(33.0166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 82.4631, Epoch Time 723.3629(624.4511), Bit/dim 4.6174(best: 4.6458), Xent 1.6932, Loss 5.4640, Error 0.5999(best: 0.5789)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0730 | Time 24.8240(23.0862) | Bit/dim 4.6016(4.6766) | Xent 1.7580(1.7757) | Loss 36.6329(14.1667) | Error 0.6244(0.6302) Steps 592(595.92) | Grad Norm 12.7430(29.0460) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 25.0675(23.1884) | Bit/dim 4.6131(4.6570) | Xent 1.6956(1.7550) | Loss 12.7157(13.7723) | Error 0.6106(0.6218) Steps 562(597.14) | Grad Norm 8.4901(24.2723) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 24.2560(23.3571) | Bit/dim 4.8149(4.6467) | Xent 1.8051(1.7417) | Loss 13.0834(13.4792) | Error 0.6444(0.6186) Steps 604(601.40) | Grad Norm 63.7368(25.0133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 83.9709, Epoch Time 740.7736(627.9407), Bit/dim 4.6162(best: 4.6174), Xent 1.7109, Loss 5.4717, Error 0.6118(best: 0.5789)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0760 | Time 24.2435(23.3598) | Bit/dim 4.6336(4.6483) | Xent 1.7737(1.7497) | Loss 12.7460(13.9891) | Error 0.6339(0.6223) Steps 610(601.48) | Grad Norm 23.1358(27.3240) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 23.1138(23.2909) | Bit/dim 4.5544(4.6305) | Xent 1.7002(1.7434) | Loss 12.6060(13.6297) | Error 0.6044(0.6211) Steps 610(601.63) | Grad Norm 14.2613(24.7231) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 22.8794(23.2639) | Bit/dim 4.5880(4.6126) | Xent 1.7092(1.7243) | Loss 12.5019(13.3272) | Error 0.6106(0.6158) Steps 616(600.81) | Grad Norm 17.6804(21.5821) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 84.8269, Epoch Time 728.6739(630.9627), Bit/dim 4.5535(best: 4.6162), Xent 1.6220, Loss 5.3645, Error 0.5752(best: 0.5789)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0790 | Time 24.4763(23.3476) | Bit/dim 4.6076(4.6040) | Xent 1.7175(1.7107) | Loss 12.7948(13.7438) | Error 0.6128(0.6102) Steps 646(602.40) | Grad Norm 40.7139(23.8041) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 24.2638(23.3947) | Bit/dim 4.5127(4.5921) | Xent 1.6462(1.7059) | Loss 12.5185(13.4467) | Error 0.5767(0.6082) Steps 610(606.85) | Grad Norm 6.7917(24.2983) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 22.2542(23.3171) | Bit/dim 4.5006(4.5850) | Xent 1.7631(1.7137) | Loss 12.4445(13.2301) | Error 0.6389(0.6104) Steps 580(610.18) | Grad Norm 36.1166(27.3363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 84.6423, Epoch Time 733.6626(634.0437), Bit/dim 4.5315(best: 4.5535), Xent 1.6084, Loss 5.3357, Error 0.5704(best: 0.5752)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0820 | Time 22.7396(23.2600) | Bit/dim 4.5216(4.5673) | Xent 1.6363(1.7053) | Loss 12.5270(13.5688) | Error 0.5889(0.6084) Steps 622(610.96) | Grad Norm 11.9695(25.0290) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 22.2507(23.4065) | Bit/dim 4.5026(4.5503) | Xent 1.6405(1.6835) | Loss 12.3226(13.2483) | Error 0.5928(0.6009) Steps 592(609.60) | Grad Norm 16.2976(22.3683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 85.1691, Epoch Time 738.9325(637.1904), Bit/dim 4.5030(best: 4.5315), Xent 1.5974, Loss 5.3018, Error 0.5706(best: 0.5704)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0840 | Time 23.0883(23.4609) | Bit/dim 4.4617(4.5365) | Xent 1.6168(1.6694) | Loss 11.9957(13.7360) | Error 0.5783(0.5970) Steps 574(608.39) | Grad Norm 17.0102(21.6399) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 23.9838(23.5068) | Bit/dim 4.4979(4.5219) | Xent 1.6001(1.6505) | Loss 12.3595(13.3509) | Error 0.5744(0.5908) Steps 646(614.23) | Grad Norm 22.7783(20.1628) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 23.4664(23.5039) | Bit/dim 4.4730(4.5047) | Xent 1.5913(1.6381) | Loss 12.0329(13.0498) | Error 0.5722(0.5871) Steps 598(615.26) | Grad Norm 20.9038(19.3536) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 86.7626, Epoch Time 742.2803(640.3431), Bit/dim 4.4505(best: 4.5030), Xent 1.5588, Loss 5.2299, Error 0.5626(best: 0.5704)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0870 | Time 23.5032(23.5490) | Bit/dim 4.4255(4.4901) | Xent 1.6287(1.6333) | Loss 12.2020(13.4834) | Error 0.5817(0.5845) Steps 604(620.58) | Grad Norm 30.1423(20.6020) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 24.1659(23.5861) | Bit/dim 4.4650(4.4869) | Xent 1.6370(1.6470) | Loss 12.2994(13.1908) | Error 0.6022(0.5891) Steps 610(621.65) | Grad Norm 15.3729(24.2259) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 23.8359(23.6267) | Bit/dim 4.4110(4.4723) | Xent 1.5812(1.6414) | Loss 12.1596(12.9414) | Error 0.5750(0.5875) Steps 610(621.58) | Grad Norm 8.1979(21.6599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 88.2069, Epoch Time 744.8761(643.4791), Bit/dim 4.4114(best: 4.4505), Xent 1.5255, Loss 5.1741, Error 0.5469(best: 0.5626)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0900 | Time 22.7883(23.5503) | Bit/dim 4.4423(4.4622) | Xent 1.6222(1.6311) | Loss 12.1890(13.3335) | Error 0.5911(0.5850) Steps 622(621.98) | Grad Norm 23.5868(21.1600) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 22.5170(23.6171) | Bit/dim 4.3957(4.4453) | Xent 1.5753(1.6225) | Loss 12.1379(13.0234) | Error 0.5689(0.5839) Steps 622(621.03) | Grad Norm 13.5775(19.2105) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 85.0602, Epoch Time 739.6963(646.3656), Bit/dim 4.4198(best: 4.4114), Xent 1.5565, Loss 5.1981, Error 0.5557(best: 0.5469)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0920 | Time 23.8563(23.6743) | Bit/dim 4.3833(4.4327) | Xent 1.5945(1.6114) | Loss 12.1194(13.5300) | Error 0.5783(0.5807) Steps 616(621.90) | Grad Norm 25.5231(19.2500) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 24.1127(23.7314) | Bit/dim 4.3579(4.4168) | Xent 1.5509(1.6032) | Loss 11.8749(13.1394) | Error 0.5733(0.5780) Steps 598(622.55) | Grad Norm 16.7087(19.7522) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 24.1645(23.7057) | Bit/dim 4.3414(4.4048) | Xent 1.6111(1.5971) | Loss 12.0008(12.8623) | Error 0.5917(0.5757) Steps 634(623.14) | Grad Norm 7.9939(18.7441) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 88.7049, Epoch Time 754.1357(649.5987), Bit/dim 4.3810(best: 4.4114), Xent 1.4795, Loss 5.1207, Error 0.5343(best: 0.5469)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0950 | Time 22.6496(23.8859) | Bit/dim 4.3810(4.3974) | Xent 1.5802(1.5958) | Loss 11.9801(13.3423) | Error 0.5783(0.5756) Steps 634(626.88) | Grad Norm 18.1636(19.8947) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 24.4441(23.9414) | Bit/dim 4.3799(4.3874) | Xent 1.5627(1.5890) | Loss 12.0773(12.9872) | Error 0.5794(0.5740) Steps 646(626.99) | Grad Norm 19.2743(19.8521) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 23.6345(23.9053) | Bit/dim 4.3238(4.3766) | Xent 1.5234(1.5797) | Loss 11.8481(12.7078) | Error 0.5522(0.5708) Steps 616(628.34) | Grad Norm 15.9035(20.2321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 85.7769, Epoch Time 750.9157(652.6382), Bit/dim 4.3237(best: 4.3810), Xent 1.4811, Loss 5.0643, Error 0.5326(best: 0.5343)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 0980 | Time 23.7552(23.9549) | Bit/dim 4.3404(4.3635) | Xent 1.5264(1.5683) | Loss 11.7661(13.0560) | Error 0.5461(0.5668) Steps 634(629.78) | Grad Norm 18.9720(19.7807) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 26.3406(24.0551) | Bit/dim 4.3809(4.3626) | Xent 1.5791(1.5783) | Loss 11.9616(12.7937) | Error 0.5683(0.5701) Steps 634(630.90) | Grad Norm 19.3560(20.4261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 85.8308, Epoch Time 759.5620(655.8459), Bit/dim 4.3019(best: 4.3237), Xent 1.4633, Loss 5.0336, Error 0.5281(best: 0.5326)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1000 | Time 25.8813(24.2266) | Bit/dim 4.2953(4.3536) | Xent 1.4794(1.5715) | Loss 36.8101(13.3006) | Error 0.5344(0.5678) Steps 616(628.02) | Grad Norm 9.9323(19.8811) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 22.6936(24.0099) | Bit/dim 4.2956(4.3402) | Xent 1.5532(1.5644) | Loss 11.6210(12.9128) | Error 0.5633(0.5662) Steps 616(625.11) | Grad Norm 18.2503(20.4318) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 23.2802(24.1983) | Bit/dim 4.2577(4.3248) | Xent 1.4762(1.5526) | Loss 11.7360(12.6133) | Error 0.5267(0.5613) Steps 574(622.39) | Grad Norm 23.5941(19.1890) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 87.0946, Epoch Time 754.7255(658.8123), Bit/dim 4.2556(best: 4.3019), Xent 1.4172, Loss 4.9642, Error 0.5124(best: 0.5281)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1030 | Time 25.4347(24.1522) | Bit/dim 4.2697(4.3081) | Xent 1.4595(1.5377) | Loss 11.8498(13.0613) | Error 0.5333(0.5562) Steps 658(625.84) | Grad Norm 10.1788(16.9569) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 25.1411(24.2257) | Bit/dim 4.3006(4.3112) | Xent 1.4900(1.5417) | Loss 11.7575(12.7493) | Error 0.5483(0.5571) Steps 634(624.82) | Grad Norm 13.4240(19.4749) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 25.6378(24.3872) | Bit/dim 4.2748(4.3022) | Xent 1.4788(1.5340) | Loss 11.6929(12.5005) | Error 0.5550(0.5558) Steps 640(630.64) | Grad Norm 13.0260(18.3962) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 85.5100, Epoch Time 766.1755(662.0332), Bit/dim 4.2491(best: 4.2556), Xent 1.4194, Loss 4.9588, Error 0.5138(best: 0.5124)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1060 | Time 25.2324(24.4833) | Bit/dim 4.2350(4.2867) | Xent 1.4378(1.5200) | Loss 11.6069(12.8945) | Error 0.5244(0.5513) Steps 670(630.91) | Grad Norm 14.6014(17.6289) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 23.4664(24.3295) | Bit/dim 4.2316(4.2720) | Xent 1.4336(1.5037) | Loss 11.7005(12.5578) | Error 0.5261(0.5459) Steps 634(627.04) | Grad Norm 12.1196(16.4644) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 27.3400(24.4443) | Bit/dim 4.2700(4.2646) | Xent 1.5613(1.5169) | Loss 11.7825(12.3380) | Error 0.5717(0.5487) Steps 616(624.47) | Grad Norm 17.3657(19.5583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 86.0687, Epoch Time 763.7881(665.0858), Bit/dim 4.2332(best: 4.2491), Xent 1.4981, Loss 4.9823, Error 0.5493(best: 0.5124)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1090 | Time 24.6560(24.6736) | Bit/dim 4.2113(4.2555) | Xent 1.4223(1.5069) | Loss 11.6240(12.7241) | Error 0.5267(0.5465) Steps 676(626.43) | Grad Norm 7.9384(17.2025) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 24.9346(24.5443) | Bit/dim 4.2314(4.2439) | Xent 1.5087(1.4975) | Loss 11.5553(12.4178) | Error 0.5511(0.5429) Steps 646(627.14) | Grad Norm 29.7515(17.1299) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 88.2695, Epoch Time 770.2440(668.2406), Bit/dim 4.2361(best: 4.2332), Xent 1.4459, Loss 4.9591, Error 0.5189(best: 0.5124)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1110 | Time 23.5286(24.4175) | Bit/dim 4.2583(4.2388) | Xent 1.4265(1.5040) | Loss 11.6176(12.9112) | Error 0.5139(0.5430) Steps 622(624.29) | Grad Norm 17.4409(19.2220) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 24.0776(24.3237) | Bit/dim 4.2135(4.2416) | Xent 1.4985(1.5202) | Loss 11.5796(12.6035) | Error 0.5417(0.5473) Steps 634(623.17) | Grad Norm 17.0848(20.0763) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 24.5792(24.4318) | Bit/dim 4.1945(4.2337) | Xent 1.4695(1.5104) | Loss 11.6162(12.3358) | Error 0.5311(0.5457) Steps 610(624.93) | Grad Norm 15.1934(19.3303) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 85.0064, Epoch Time 758.7949(670.9572), Bit/dim 4.1864(best: 4.2332), Xent 1.3882, Loss 4.8805, Error 0.5056(best: 0.5124)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1140 | Time 24.9048(24.4463) | Bit/dim 4.1569(4.2196) | Xent 1.4529(1.4952) | Loss 11.4363(12.7378) | Error 0.5189(0.5397) Steps 616(621.79) | Grad Norm 12.9838(16.9170) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 23.2708(24.1926) | Bit/dim 4.1509(4.2033) | Xent 1.4401(1.4754) | Loss 11.2979(12.3905) | Error 0.5228(0.5326) Steps 580(619.95) | Grad Norm 15.3645(14.8256) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 24.5786(24.1475) | Bit/dim 4.1431(4.1899) | Xent 1.4398(1.4601) | Loss 11.4013(12.1305) | Error 0.5356(0.5290) Steps 610(617.38) | Grad Norm 15.1426(14.0421) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 86.9087, Epoch Time 751.9975(673.3884), Bit/dim 4.2200(best: 4.1864), Xent 1.5479, Loss 4.9940, Error 0.5402(best: 0.5056)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1170 | Time 23.9907(24.1340) | Bit/dim 4.2140(4.2039) | Xent 1.6652(1.5059) | Loss 11.8537(12.6163) | Error 0.6106(0.5440) Steps 628(619.02) | Grad Norm 12.5295(18.7533) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 22.1748(24.1591) | Bit/dim 4.1642(4.1996) | Xent 1.5175(1.5108) | Loss 11.5836(12.3589) | Error 0.5628(0.5475) Steps 610(620.91) | Grad Norm 14.0602(17.1465) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 84.4868, Epoch Time 749.4741(675.6710), Bit/dim 4.1482(best: 4.1864), Xent 1.3868, Loss 4.8416, Error 0.5021(best: 0.5056)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1190 | Time 23.0934(24.0061) | Bit/dim 4.1366(4.1890) | Xent 1.4404(1.5010) | Loss 11.3136(12.8042) | Error 0.5272(0.5459) Steps 634(617.39) | Grad Norm 5.6401(14.5627) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 22.8968(23.9128) | Bit/dim 4.1532(4.1756) | Xent 1.3768(1.4751) | Loss 11.2054(12.4052) | Error 0.5022(0.5371) Steps 616(616.26) | Grad Norm 6.7872(12.5763) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 23.5498(23.9276) | Bit/dim 4.1300(4.1615) | Xent 1.3904(1.4523) | Loss 11.3182(12.1049) | Error 0.5061(0.5292) Steps 586(617.52) | Grad Norm 10.3815(11.4374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 85.8125, Epoch Time 744.5598(677.7377), Bit/dim 4.1140(best: 4.1482), Xent 1.3231, Loss 4.7755, Error 0.4792(best: 0.5021)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1220 | Time 23.5843(23.7834) | Bit/dim 4.1305(4.1510) | Xent 1.3014(1.4351) | Loss 11.3384(12.5464) | Error 0.4772(0.5219) Steps 646(616.81) | Grad Norm 14.8852(11.7307) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 22.8668(23.9149) | Bit/dim 4.1257(4.1421) | Xent 1.3698(1.4268) | Loss 11.1048(12.2088) | Error 0.4944(0.5181) Steps 616(615.60) | Grad Norm 20.3624(14.0941) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 23.1454(23.9323) | Bit/dim 4.0920(4.1321) | Xent 1.3241(1.4126) | Loss 11.1772(11.9458) | Error 0.4856(0.5124) Steps 616(616.07) | Grad Norm 7.1141(13.3198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 85.8842, Epoch Time 750.6425(679.9248), Bit/dim 4.1004(best: 4.1140), Xent 1.2878, Loss 4.7443, Error 0.4677(best: 0.4792)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1250 | Time 24.2391(24.0464) | Bit/dim 4.1102(4.1228) | Xent 1.3585(1.3963) | Loss 11.2825(12.3527) | Error 0.4883(0.5059) Steps 646(617.68) | Grad Norm 22.3365(13.3026) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 22.4353(24.1426) | Bit/dim 4.0895(4.1151) | Xent 1.3165(1.3831) | Loss 11.2059(12.0467) | Error 0.4822(0.5010) Steps 610(616.93) | Grad Norm 7.1361(12.5508) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 86.1840, Epoch Time 760.8415(682.3523), Bit/dim 4.0766(best: 4.1004), Xent 1.2821, Loss 4.7177, Error 0.4598(best: 0.4677)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1270 | Time 23.7393(24.0881) | Bit/dim 4.0579(4.1061) | Xent 1.2996(1.3721) | Loss 35.1936(12.5278) | Error 0.4678(0.4972) Steps 574(615.24) | Grad Norm 9.8339(12.3015) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 24.5272(24.1540) | Bit/dim 4.0680(4.1017) | Xent 1.3421(1.3684) | Loss 11.1863(12.1680) | Error 0.4972(0.4954) Steps 586(612.55) | Grad Norm 9.2294(13.7213) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 24.1114(24.1253) | Bit/dim 4.0618(4.0947) | Xent 1.3172(1.3599) | Loss 10.8842(11.8956) | Error 0.4839(0.4928) Steps 634(613.53) | Grad Norm 10.5566(13.2642) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 85.5489, Epoch Time 754.5250(684.5175), Bit/dim 4.0640(best: 4.0766), Xent 1.2811, Loss 4.7045, Error 0.4627(best: 0.4598)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1300 | Time 23.2980(24.0113) | Bit/dim 4.1080(4.0875) | Xent 1.3425(1.3527) | Loss 11.1660(12.3564) | Error 0.4822(0.4903) Steps 610(614.77) | Grad Norm 23.8460(13.0858) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 24.9226(24.1155) | Bit/dim 4.0474(4.0811) | Xent 1.2887(1.3453) | Loss 11.0787(12.0313) | Error 0.4750(0.4872) Steps 628(616.11) | Grad Norm 6.6670(13.3801) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 24.1617(24.1863) | Bit/dim 4.0565(4.0752) | Xent 1.3098(1.3415) | Loss 11.0505(11.7858) | Error 0.4767(0.4859) Steps 616(615.29) | Grad Norm 23.7193(13.7527) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 85.0701, Epoch Time 755.6298(686.6509), Bit/dim 4.0501(best: 4.0640), Xent 1.2673, Loss 4.6838, Error 0.4565(best: 0.4598)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1330 | Time 23.7112(24.1114) | Bit/dim 4.0324(4.0687) | Xent 1.3028(1.3367) | Loss 11.0821(12.1808) | Error 0.4867(0.4848) Steps 664(616.73) | Grad Norm 12.6627(14.1510) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 23.6464(24.1036) | Bit/dim 4.0703(4.0641) | Xent 1.3093(1.3289) | Loss 10.9604(11.8887) | Error 0.4783(0.4818) Steps 616(617.47) | Grad Norm 11.7058(13.1631) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 23.1852(24.1418) | Bit/dim 4.0588(4.0594) | Xent 1.3143(1.3234) | Loss 11.0920(11.6742) | Error 0.4800(0.4792) Steps 640(621.40) | Grad Norm 4.0860(12.2662) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 84.9605, Epoch Time 753.3129(688.6507), Bit/dim 4.0342(best: 4.0501), Xent 1.2503, Loss 4.6593, Error 0.4537(best: 0.4565)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1360 | Time 23.7835(24.1885) | Bit/dim 4.0690(4.0530) | Xent 1.3542(1.3151) | Loss 11.1383(12.0678) | Error 0.4944(0.4757) Steps 628(624.84) | Grad Norm 23.7128(12.3043) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 23.3591(24.2906) | Bit/dim 4.0485(4.0485) | Xent 1.3405(1.3184) | Loss 10.9318(11.7919) | Error 0.4778(0.4760) Steps 610(622.29) | Grad Norm 21.9859(13.6326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 86.5952, Epoch Time 765.4672(690.9552), Bit/dim 4.0328(best: 4.0342), Xent 1.2225, Loss 4.6440, Error 0.4417(best: 0.4537)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1380 | Time 23.6251(24.3078) | Bit/dim 4.0431(4.0447) | Xent 1.2445(1.3139) | Loss 11.0026(12.2721) | Error 0.4450(0.4734) Steps 622(623.93) | Grad Norm 13.0807(13.4712) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 23.7042(24.3562) | Bit/dim 4.0059(4.0403) | Xent 1.2780(1.3103) | Loss 10.9318(11.9431) | Error 0.4578(0.4724) Steps 616(622.71) | Grad Norm 12.9221(14.0294) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 84.3000, Epoch Time 757.7108(692.9579), Bit/dim 4.0201(best: 4.0328), Xent 1.2432, Loss 4.6416, Error 0.4531(best: 0.4417)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1410 | Time 23.7543(24.3687) | Bit/dim 4.0075(4.0318) | Xent 1.2366(1.3029) | Loss 10.8718(12.1399) | Error 0.4467(0.4706) Steps 616(623.33) | Grad Norm 7.5274(14.2836) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 23.5972(24.2076) | Bit/dim 4.0139(4.0245) | Xent 1.2682(1.2897) | Loss 11.0702(11.8095) | Error 0.4478(0.4655) Steps 646(623.43) | Grad Norm 11.5503(13.0354) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 25.0441(24.2637) | Bit/dim 4.0065(4.0182) | Xent 1.2435(1.2890) | Loss 10.8931(11.5640) | Error 0.4456(0.4649) Steps 628(625.56) | Grad Norm 23.7289(12.4997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 87.1162, Epoch Time 757.7537(694.9018), Bit/dim 4.0074(best: 4.0201), Xent 1.2102, Loss 4.6125, Error 0.4369(best: 0.4417)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1440 | Time 23.5081(24.2925) | Bit/dim 3.9926(4.0151) | Xent 1.2577(1.2849) | Loss 10.9879(11.9612) | Error 0.4456(0.4636) Steps 652(626.88) | Grad Norm 13.1298(12.8202) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 23.9503(24.1781) | Bit/dim 3.9803(4.0110) | Xent 1.2594(1.2821) | Loss 10.8182(11.6852) | Error 0.4678(0.4629) Steps 622(627.53) | Grad Norm 17.4970(13.3278) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 85.1946, Epoch Time 755.4947(696.7195), Bit/dim 3.9914(best: 4.0074), Xent 1.2163, Loss 4.5995, Error 0.4412(best: 0.4369)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1460 | Time 23.8712(24.1757) | Bit/dim 3.9885(4.0066) | Xent 1.2457(1.2784) | Loss 10.9517(12.2001) | Error 0.4506(0.4619) Steps 652(627.18) | Grad Norm 11.9130(13.2766) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 24.9660(24.2418) | Bit/dim 3.9839(4.0026) | Xent 1.2357(1.2737) | Loss 10.8803(11.8483) | Error 0.4528(0.4597) Steps 652(626.38) | Grad Norm 8.0813(12.6007) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 23.0922(24.1882) | Bit/dim 3.9906(3.9970) | Xent 1.1964(1.2616) | Loss 10.5725(11.5739) | Error 0.4383(0.4560) Steps 616(624.81) | Grad Norm 9.7882(11.3474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 86.6682, Epoch Time 754.0994(698.4409), Bit/dim 3.9969(best: 3.9914), Xent 1.2450, Loss 4.6194, Error 0.4498(best: 0.4369)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1490 | Time 26.0231(24.1902) | Bit/dim 4.0198(3.9945) | Xent 1.2867(1.2659) | Loss 11.0096(12.0409) | Error 0.4494(0.4563) Steps 646(623.91) | Grad Norm 26.1952(13.0032) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 23.6975(24.2601) | Bit/dim 3.9702(3.9945) | Xent 1.2361(1.2717) | Loss 10.8601(11.7346) | Error 0.4450(0.4585) Steps 640(623.45) | Grad Norm 16.1876(14.3490) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 22.6577(24.1712) | Bit/dim 3.9958(3.9941) | Xent 1.2502(1.2707) | Loss 10.9315(11.5030) | Error 0.4517(0.4587) Steps 628(623.96) | Grad Norm 7.9309(14.2889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 84.2120, Epoch Time 759.1048(700.2609), Bit/dim 3.9736(best: 3.9914), Xent 1.1825, Loss 4.5649, Error 0.4286(best: 0.4369)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1520 | Time 24.8449(24.1968) | Bit/dim 3.9644(3.9903) | Xent 1.2366(1.2568) | Loss 10.7518(11.8815) | Error 0.4478(0.4529) Steps 658(623.43) | Grad Norm 6.1564(12.7238) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 25.6241(24.2427) | Bit/dim 3.9517(3.9826) | Xent 1.1665(1.2433) | Loss 10.5907(11.5840) | Error 0.4183(0.4482) Steps 598(623.69) | Grad Norm 7.8413(11.3504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 85.6326, Epoch Time 755.4423(701.9163), Bit/dim 3.9565(best: 3.9736), Xent 1.1793, Loss 4.5462, Error 0.4248(best: 0.4286)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1540 | Time 23.8561(24.1475) | Bit/dim 3.9709(3.9758) | Xent 1.2105(1.2376) | Loss 35.0039(12.0915) | Error 0.4383(0.4458) Steps 622(623.98) | Grad Norm 7.6142(10.7174) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 25.1676(24.1640) | Bit/dim 3.9600(3.9705) | Xent 1.2673(1.2320) | Loss 10.7303(11.7294) | Error 0.4450(0.4440) Steps 592(623.29) | Grad Norm 12.0407(11.0027) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 24.7493(24.2181) | Bit/dim 3.9436(3.9676) | Xent 1.1892(1.2310) | Loss 10.6875(11.4772) | Error 0.4194(0.4444) Steps 598(621.07) | Grad Norm 6.9138(10.6465) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 87.3545, Epoch Time 759.4381(703.6420), Bit/dim 3.9571(best: 3.9565), Xent 1.1566, Loss 4.5354, Error 0.4197(best: 0.4248)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1570 | Time 23.5788(24.1909) | Bit/dim 3.9517(3.9621) | Xent 1.1840(1.2235) | Loss 10.6667(11.9309) | Error 0.4244(0.4419) Steps 622(625.74) | Grad Norm 15.5857(11.4264) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 24.7890(24.0959) | Bit/dim 3.9517(3.9579) | Xent 1.2005(1.2132) | Loss 10.8171(11.5984) | Error 0.4300(0.4375) Steps 622(623.80) | Grad Norm 11.2475(11.5155) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 24.4052(24.0153) | Bit/dim 3.9511(3.9570) | Xent 1.2587(1.2138) | Loss 10.4419(11.3577) | Error 0.4550(0.4378) Steps 574(622.55) | Grad Norm 12.5039(12.3513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 84.1791, Epoch Time 746.1481(704.9171), Bit/dim 3.9458(best: 3.9565), Xent 1.1428, Loss 4.5172, Error 0.4155(best: 0.4197)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1600 | Time 24.3090(23.9732) | Bit/dim 3.9480(3.9537) | Xent 1.2281(1.2106) | Loss 10.8547(11.7688) | Error 0.4433(0.4361) Steps 646(619.50) | Grad Norm 9.6464(11.8153) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 24.3572(24.0331) | Bit/dim 3.9631(3.9516) | Xent 1.1736(1.1987) | Loss 10.7800(11.4745) | Error 0.4328(0.4318) Steps 640(618.05) | Grad Norm 7.7910(11.4966) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 24.6418(24.0180) | Bit/dim 3.9614(3.9483) | Xent 1.1579(1.1917) | Loss 10.6023(11.2461) | Error 0.4183(0.4297) Steps 616(616.04) | Grad Norm 16.4870(11.1569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 84.6700, Epoch Time 751.0520(706.3012), Bit/dim 3.9353(best: 3.9458), Xent 1.1230, Loss 4.4968, Error 0.4074(best: 0.4155)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1630 | Time 24.6848(24.0999) | Bit/dim 3.9439(3.9439) | Xent 1.2567(1.1864) | Loss 10.7076(11.6464) | Error 0.4411(0.4276) Steps 616(617.78) | Grad Norm 15.8968(11.8864) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 23.1086(23.9556) | Bit/dim 3.9195(3.9423) | Xent 1.2079(1.1942) | Loss 10.5119(11.3931) | Error 0.4406(0.4312) Steps 622(619.53) | Grad Norm 18.1403(13.8461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 83.6793, Epoch Time 751.3559(707.6528), Bit/dim 3.9342(best: 3.9353), Xent 1.1153, Loss 4.4918, Error 0.4048(best: 0.4074)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1650 | Time 23.1412(24.0549) | Bit/dim 3.9292(3.9412) | Xent 1.1711(1.1907) | Loss 10.6362(11.8792) | Error 0.4194(0.4299) Steps 640(617.85) | Grad Norm 14.7452(13.0158) | Total Time 0.00(0.00)\n",
      "Iter 1660 | Time 23.5183(24.1012) | Bit/dim 3.9213(3.9367) | Xent 1.1597(1.1781) | Loss 10.5929(11.5408) | Error 0.4200(0.4261) Steps 610(617.25) | Grad Norm 14.8138(12.8480) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 23.6746(24.1254) | Bit/dim 3.9295(3.9347) | Xent 1.1651(1.1753) | Loss 10.5163(11.2928) | Error 0.4117(0.4243) Steps 586(616.54) | Grad Norm 16.3152(13.0830) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 84.0726, Epoch Time 754.5491(709.0597), Bit/dim 3.9288(best: 3.9342), Xent 1.1150, Loss 4.4863, Error 0.4013(best: 0.4048)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1680 | Time 23.9544(24.0092) | Bit/dim 3.9318(3.9319) | Xent 1.1378(1.1703) | Loss 10.5651(11.7253) | Error 0.4100(0.4212) Steps 574(615.28) | Grad Norm 12.7856(12.7356) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 24.8172(24.0036) | Bit/dim 3.9322(3.9311) | Xent 1.1663(1.1741) | Loss 10.6365(11.4384) | Error 0.4111(0.4212) Steps 610(613.47) | Grad Norm 18.1710(14.2888) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 23.0490(23.9891) | Bit/dim 3.9301(3.9287) | Xent 1.1092(1.1697) | Loss 10.4783(11.2088) | Error 0.3939(0.4209) Steps 622(613.55) | Grad Norm 6.0893(13.5655) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 82.3889, Epoch Time 745.6209(710.1566), Bit/dim 3.9189(best: 3.9288), Xent 1.0796, Loss 4.4587, Error 0.3871(best: 0.4013)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1710 | Time 24.2606(24.1062) | Bit/dim 3.9186(3.9251) | Xent 1.1398(1.1548) | Loss 10.3783(11.5790) | Error 0.4244(0.4171) Steps 598(613.71) | Grad Norm 9.6281(12.8177) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 23.3209(24.0506) | Bit/dim 3.9064(3.9209) | Xent 1.1107(1.1457) | Loss 10.3279(11.2884) | Error 0.4078(0.4124) Steps 634(613.67) | Grad Norm 12.8068(12.6916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 82.5532, Epoch Time 751.2981(711.3908), Bit/dim 3.9065(best: 3.9189), Xent 1.1102, Loss 4.4616, Error 0.4020(best: 0.3871)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1730 | Time 25.0755(24.0694) | Bit/dim 3.9102(3.9180) | Xent 1.1084(1.1457) | Loss 10.5494(11.7635) | Error 0.3922(0.4107) Steps 628(616.09) | Grad Norm 11.2108(12.9172) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 24.5435(24.1214) | Bit/dim 3.8965(3.9148) | Xent 1.1270(1.1404) | Loss 10.5762(11.4369) | Error 0.4050(0.4082) Steps 598(616.68) | Grad Norm 7.5314(12.9679) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 25.7235(24.2264) | Bit/dim 3.9085(3.9164) | Xent 1.1358(1.1399) | Loss 10.6192(11.1994) | Error 0.3989(0.4078) Steps 610(617.33) | Grad Norm 10.5108(13.3172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 84.8255, Epoch Time 757.6139(712.7775), Bit/dim 3.8985(best: 3.9065), Xent 1.0984, Loss 4.4477, Error 0.3920(best: 0.3871)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1760 | Time 22.9535(24.0332) | Bit/dim 3.9016(3.9116) | Xent 1.1325(1.1367) | Loss 10.4834(11.6508) | Error 0.3994(0.4062) Steps 592(614.43) | Grad Norm 17.9416(13.3214) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 25.0132(23.9811) | Bit/dim 3.8934(3.9087) | Xent 1.0645(1.1231) | Loss 10.2597(11.3314) | Error 0.3822(0.4009) Steps 604(612.31) | Grad Norm 5.5585(12.0436) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 23.3872(24.0378) | Bit/dim 3.9243(3.9050) | Xent 1.4068(1.1306) | Loss 10.8650(11.1236) | Error 0.4983(0.4040) Steps 616(611.10) | Grad Norm 37.7192(13.5812) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 83.1846, Epoch Time 745.2334(713.7512), Bit/dim 3.9136(best: 3.8985), Xent 1.1243, Loss 4.4758, Error 0.4005(best: 0.3871)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1790 | Time 24.6267(24.1706) | Bit/dim 3.9017(3.9048) | Xent 1.1187(1.1405) | Loss 10.4966(11.5454) | Error 0.3994(0.4087) Steps 604(615.01) | Grad Norm 11.8748(13.4613) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 23.2955(24.1377) | Bit/dim 3.8795(3.9044) | Xent 1.1353(1.1431) | Loss 10.3764(11.2873) | Error 0.4067(0.4108) Steps 628(615.30) | Grad Norm 7.4374(12.7975) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 82.7608, Epoch Time 751.8045(714.8928), Bit/dim 3.8888(best: 3.8985), Xent 1.0477, Loss 4.4127, Error 0.3777(best: 0.3871)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1810 | Time 22.5878(24.0007) | Bit/dim 3.8680(3.9021) | Xent 1.0620(1.1315) | Loss 33.6466(11.7560) | Error 0.3722(0.4050) Steps 622(612.12) | Grad Norm 5.0174(11.6036) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 24.4379(23.9476) | Bit/dim 3.8860(3.8978) | Xent 1.0697(1.1181) | Loss 10.3467(11.3884) | Error 0.3961(0.4007) Steps 598(610.17) | Grad Norm 6.8098(10.4665) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 24.8798(24.0664) | Bit/dim 3.9003(3.8953) | Xent 1.0412(1.1057) | Loss 10.5542(11.1381) | Error 0.3694(0.3963) Steps 664(612.85) | Grad Norm 10.2919(9.9459) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 84.1922, Epoch Time 751.8590(716.0018), Bit/dim 3.8819(best: 3.8888), Xent 1.0382, Loss 4.4010, Error 0.3726(best: 0.3777)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1840 | Time 25.6909(24.1851) | Bit/dim 3.8993(3.8909) | Xent 1.0453(1.0973) | Loss 10.4360(11.5880) | Error 0.3578(0.3923) Steps 610(613.55) | Grad Norm 9.7145(9.6813) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 24.8174(24.1855) | Bit/dim 3.8984(3.8862) | Xent 1.1709(1.0931) | Loss 10.5790(11.2810) | Error 0.4094(0.3905) Steps 604(612.12) | Grad Norm 20.0322(9.8528) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 23.2462(24.0788) | Bit/dim 3.8776(3.8857) | Xent 1.1143(1.0967) | Loss 10.3657(11.0608) | Error 0.4089(0.3925) Steps 628(612.42) | Grad Norm 16.8957(10.8637) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 83.3281, Epoch Time 753.0709(717.1138), Bit/dim 3.8960(best: 3.8819), Xent 1.0967, Loss 4.4444, Error 0.3897(best: 0.3726)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1870 | Time 24.0985(24.0829) | Bit/dim 3.9219(3.8878) | Xent 1.0538(1.1119) | Loss 10.4354(11.4961) | Error 0.3711(0.3974) Steps 610(613.67) | Grad Norm 11.3792(13.3655) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 23.9921(24.0013) | Bit/dim 3.8789(3.8864) | Xent 1.1042(1.1111) | Loss 10.4731(11.2199) | Error 0.3850(0.3977) Steps 616(612.64) | Grad Norm 7.9183(12.9393) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 24.6213(23.9861) | Bit/dim 3.8890(3.8843) | Xent 1.1039(1.1048) | Loss 10.4776(11.0097) | Error 0.3922(0.3950) Steps 586(612.07) | Grad Norm 12.3956(12.2414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 81.9501, Epoch Time 744.5689(717.9375), Bit/dim 3.8682(best: 3.8819), Xent 1.0285, Loss 4.3825, Error 0.3695(best: 0.3726)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1900 | Time 23.6351(23.9708) | Bit/dim 3.8615(3.8799) | Xent 1.0239(1.0923) | Loss 10.3660(11.3371) | Error 0.3672(0.3914) Steps 610(611.53) | Grad Norm 10.6055(11.9341) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 23.4673(23.9000) | Bit/dim 3.8293(3.8757) | Xent 1.0667(1.0826) | Loss 10.2785(11.0801) | Error 0.3778(0.3878) Steps 610(608.57) | Grad Norm 15.1272(11.1352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 84.2625, Epoch Time 745.4649(718.7633), Bit/dim 3.8724(best: 3.8682), Xent 1.0392, Loss 4.3920, Error 0.3699(best: 0.3695)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1920 | Time 23.1230(23.8500) | Bit/dim 3.8808(3.8727) | Xent 1.0975(1.0758) | Loss 10.3939(11.5331) | Error 0.3911(0.3857) Steps 610(610.02) | Grad Norm 15.1573(10.6974) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 24.0683(23.7946) | Bit/dim 3.8864(3.8715) | Xent 1.0918(1.0770) | Loss 10.4454(11.2209) | Error 0.3839(0.3862) Steps 592(605.34) | Grad Norm 10.5079(10.8617) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 23.8960(23.9072) | Bit/dim 3.8585(3.8698) | Xent 1.0903(1.0786) | Loss 10.2940(10.9930) | Error 0.3933(0.3858) Steps 640(607.28) | Grad Norm 16.4085(12.2695) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 84.5995, Epoch Time 748.3696(719.6515), Bit/dim 3.8641(best: 3.8682), Xent 1.0224, Loss 4.3753, Error 0.3675(best: 0.3695)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1950 | Time 23.5053(23.9513) | Bit/dim 3.8598(3.8692) | Xent 1.0809(1.0700) | Loss 10.4688(11.4456) | Error 0.3983(0.3828) Steps 598(606.19) | Grad Norm 15.7974(12.4425) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 23.0020(23.9474) | Bit/dim 3.8451(3.8669) | Xent 1.0624(1.0646) | Loss 10.1825(11.1454) | Error 0.3739(0.3808) Steps 598(606.23) | Grad Norm 12.8372(12.2245) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 23.8220(23.9877) | Bit/dim 3.8688(3.8670) | Xent 1.1079(1.0657) | Loss 10.3192(10.9413) | Error 0.3967(0.3803) Steps 628(607.79) | Grad Norm 19.4404(12.0528) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 83.1565, Epoch Time 748.7850(720.5255), Bit/dim 3.8607(best: 3.8641), Xent 1.0512, Loss 4.3863, Error 0.3743(best: 0.3675)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 1980 | Time 24.2843(24.0503) | Bit/dim 3.8692(3.8653) | Xent 1.0511(1.0645) | Loss 10.4590(11.3619) | Error 0.3739(0.3805) Steps 616(608.47) | Grad Norm 9.8098(12.8677) | Total Time 0.00(0.00)\n",
      "Iter 1990 | Time 23.9127(23.9927) | Bit/dim 3.8497(3.8630) | Xent 1.0066(1.0591) | Loss 10.3020(11.0883) | Error 0.3583(0.3787) Steps 586(608.33) | Grad Norm 6.5180(11.5585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 85.4106, Epoch Time 750.1302(721.4136), Bit/dim 3.8601(best: 3.8607), Xent 1.0296, Loss 4.3749, Error 0.3697(best: 0.3675)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2000 | Time 24.9221(23.9640) | Bit/dim 3.8398(3.8610) | Xent 1.0000(1.0525) | Loss 10.2827(11.5888) | Error 0.3589(0.3769) Steps 610(608.18) | Grad Norm 10.1115(11.0065) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 23.9421(23.8929) | Bit/dim 3.8485(3.8570) | Xent 0.9871(1.0493) | Loss 10.2563(11.2341) | Error 0.3461(0.3743) Steps 622(609.01) | Grad Norm 8.5068(11.0525) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 23.7175(23.7256) | Bit/dim 3.8542(3.8564) | Xent 1.0301(1.0455) | Loss 10.1995(10.9814) | Error 0.3678(0.3732) Steps 586(606.85) | Grad Norm 7.2719(10.4992) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 83.2551, Epoch Time 734.5718(721.8084), Bit/dim 3.8506(best: 3.8601), Xent 0.9984, Loss 4.3498, Error 0.3557(best: 0.3675)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2030 | Time 24.5239(23.7687) | Bit/dim 3.8421(3.8537) | Xent 1.0543(1.0425) | Loss 10.1620(11.3981) | Error 0.3711(0.3718) Steps 598(606.36) | Grad Norm 11.1113(9.9033) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 24.3928(23.7766) | Bit/dim 3.8614(3.8509) | Xent 1.0733(1.0417) | Loss 10.3762(11.1126) | Error 0.3789(0.3710) Steps 646(609.08) | Grad Norm 10.2758(10.9283) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 25.3026(23.8917) | Bit/dim 3.8447(3.8488) | Xent 1.0678(1.0434) | Loss 10.4172(10.8999) | Error 0.3856(0.3719) Steps 658(613.67) | Grad Norm 13.8568(11.3725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 83.1134, Epoch Time 751.3462(722.6945), Bit/dim 3.8446(best: 3.8506), Xent 1.0072, Loss 4.3482, Error 0.3568(best: 0.3557)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2060 | Time 24.1335(23.9058) | Bit/dim 3.8591(3.8487) | Xent 1.0086(1.0366) | Loss 10.3850(11.3083) | Error 0.3533(0.3694) Steps 586(611.17) | Grad Norm 10.5433(10.6568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 85.5201, Epoch Time 746.0592(723.3955), Bit/dim 3.8416(best: 3.8446), Xent 0.9887, Loss 4.3359, Error 0.3490(best: 0.3557)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2080 | Time 25.5003(23.9140) | Bit/dim 3.8095(3.8441) | Xent 0.9919(1.0300) | Loss 34.4352(11.5451) | Error 0.3561(0.3668) Steps 634(609.10) | Grad Norm 11.9355(11.1409) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 23.9983(24.0105) | Bit/dim 3.8441(3.8441) | Xent 0.9986(1.0280) | Loss 10.2899(11.2190) | Error 0.3472(0.3658) Steps 598(609.10) | Grad Norm 10.2951(11.4859) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 23.4315(23.9507) | Bit/dim 3.8329(3.8407) | Xent 1.0350(1.0282) | Loss 10.2960(10.9682) | Error 0.3600(0.3665) Steps 610(609.27) | Grad Norm 10.4408(11.0396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 84.1354, Epoch Time 752.4812(724.2680), Bit/dim 3.8427(best: 3.8416), Xent 0.9784, Loss 4.3319, Error 0.3467(best: 0.3490)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2110 | Time 23.4998(23.9600) | Bit/dim 3.8137(3.8385) | Xent 0.9922(1.0232) | Loss 10.1881(11.4269) | Error 0.3622(0.3649) Steps 586(608.86) | Grad Norm 16.6258(11.0434) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 24.5636(24.0616) | Bit/dim 3.8514(3.8384) | Xent 1.0745(1.0241) | Loss 10.4088(11.1213) | Error 0.3683(0.3653) Steps 592(606.07) | Grad Norm 15.1313(11.7389) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 23.0373(23.9586) | Bit/dim 3.8325(3.8388) | Xent 1.0334(1.0294) | Loss 10.2835(10.9083) | Error 0.3700(0.3670) Steps 598(606.75) | Grad Norm 12.1370(12.7590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 82.3450, Epoch Time 748.9834(725.0095), Bit/dim 3.8325(best: 3.8416), Xent 0.9903, Loss 4.3276, Error 0.3512(best: 0.3467)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2140 | Time 24.3362(23.9461) | Bit/dim 3.8560(3.8399) | Xent 1.0633(1.0294) | Loss 10.3981(11.3379) | Error 0.3861(0.3677) Steps 616(610.17) | Grad Norm 16.3538(12.6193) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 24.2224(23.9576) | Bit/dim 3.8467(3.8388) | Xent 0.9575(1.0278) | Loss 10.1438(11.0582) | Error 0.3528(0.3678) Steps 610(611.24) | Grad Norm 15.2731(12.6289) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 23.0635(23.8104) | Bit/dim 3.8305(3.8371) | Xent 1.0767(1.0249) | Loss 10.3692(10.8409) | Error 0.3761(0.3670) Steps 634(610.40) | Grad Norm 15.7247(12.4698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 84.4689, Epoch Time 742.2602(725.5270), Bit/dim 3.8242(best: 3.8325), Xent 1.0012, Loss 4.3248, Error 0.3597(best: 0.3467)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2170 | Time 22.9402(23.7673) | Bit/dim 3.8376(3.8348) | Xent 1.0532(1.0276) | Loss 10.1959(11.2323) | Error 0.3772(0.3673) Steps 598(610.10) | Grad Norm 19.4731(12.8093) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 22.8462(23.8416) | Bit/dim 3.7935(3.8311) | Xent 1.0218(1.0209) | Loss 10.0740(10.9704) | Error 0.3683(0.3651) Steps 610(611.82) | Grad Norm 13.2976(12.1842) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 85.2508, Epoch Time 747.2671(726.1792), Bit/dim 3.8228(best: 3.8242), Xent 0.9815, Loss 4.3135, Error 0.3499(best: 0.3467)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2190 | Time 23.4845(23.8716) | Bit/dim 3.8587(3.8314) | Xent 1.0643(1.0139) | Loss 10.2801(11.4448) | Error 0.3717(0.3614) Steps 610(610.84) | Grad Norm 11.8363(11.3588) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 23.9761(23.9606) | Bit/dim 3.8212(3.8309) | Xent 1.0333(1.0163) | Loss 10.2752(11.1357) | Error 0.3606(0.3620) Steps 604(610.71) | Grad Norm 15.8880(12.3099) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 23.9599(23.9449) | Bit/dim 3.8498(3.8319) | Xent 1.0056(1.0176) | Loss 10.0982(10.8983) | Error 0.3444(0.3615) Steps 610(613.14) | Grad Norm 13.1717(12.1258) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 83.8514, Epoch Time 750.0611(726.8957), Bit/dim 3.8238(best: 3.8228), Xent 0.9541, Loss 4.3009, Error 0.3398(best: 0.3467)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2220 | Time 23.7414(23.9179) | Bit/dim 3.8337(3.8298) | Xent 0.9796(1.0133) | Loss 10.1445(11.3364) | Error 0.3544(0.3613) Steps 598(611.52) | Grad Norm 14.8232(12.2335) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 22.8358(23.9379) | Bit/dim 3.8434(3.8276) | Xent 0.9319(1.0052) | Loss 10.1328(11.0322) | Error 0.3278(0.3597) Steps 628(612.65) | Grad Norm 5.4108(11.2799) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 23.0201(23.8828) | Bit/dim 3.8297(3.8236) | Xent 0.9564(1.0000) | Loss 10.2348(10.8093) | Error 0.3528(0.3581) Steps 610(613.52) | Grad Norm 12.4700(10.5124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 85.4331, Epoch Time 746.9493(727.4973), Bit/dim 3.8123(best: 3.8228), Xent 0.9412, Loss 4.2829, Error 0.3365(best: 0.3398)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2250 | Time 23.6903(23.8449) | Bit/dim 3.8242(3.8226) | Xent 0.9865(0.9901) | Loss 10.1777(11.2007) | Error 0.3606(0.3546) Steps 586(611.38) | Grad Norm 12.8642(10.4621) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 24.0705(23.8201) | Bit/dim 3.8151(3.8190) | Xent 1.0559(0.9911) | Loss 10.1868(10.9330) | Error 0.3739(0.3549) Steps 598(609.78) | Grad Norm 16.4651(10.6731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 84.5127, Epoch Time 748.6557(728.1320), Bit/dim 3.8311(best: 3.8123), Xent 1.0069, Loss 4.3345, Error 0.3570(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2270 | Time 23.2245(23.9337) | Bit/dim 3.8295(3.8206) | Xent 1.0144(1.0066) | Loss 10.2299(11.4646) | Error 0.3572(0.3596) Steps 604(612.15) | Grad Norm 9.5351(12.8991) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 22.9680(23.8060) | Bit/dim 3.8423(3.8229) | Xent 1.0460(1.0076) | Loss 10.2003(11.1413) | Error 0.3744(0.3606) Steps 586(607.92) | Grad Norm 15.7425(13.2648) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 24.4556(23.8032) | Bit/dim 3.7781(3.8190) | Xent 0.9813(1.0066) | Loss 10.0715(10.8937) | Error 0.3511(0.3600) Steps 634(608.92) | Grad Norm 10.1631(12.9761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 83.8843, Epoch Time 743.2127(728.5845), Bit/dim 3.8173(best: 3.8123), Xent 0.9626, Loss 4.2986, Error 0.3435(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2300 | Time 22.6655(23.7906) | Bit/dim 3.8306(3.8199) | Xent 0.9499(0.9966) | Loss 10.0396(11.3399) | Error 0.3456(0.3568) Steps 592(609.94) | Grad Norm 9.4632(11.9902) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 25.8409(23.8253) | Bit/dim 3.8332(3.8203) | Xent 1.0030(0.9966) | Loss 10.2550(11.0351) | Error 0.3567(0.3555) Steps 616(609.44) | Grad Norm 15.7662(12.3511) | Total Time 0.00(0.00)\n",
      "Iter 2320 | Time 24.9057(23.8917) | Bit/dim 3.7911(3.8171) | Xent 1.0012(0.9919) | Loss 10.1754(10.8098) | Error 0.3589(0.3540) Steps 622(611.17) | Grad Norm 10.6882(12.1331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 86.0004, Epoch Time 747.5606(729.1537), Bit/dim 3.8094(best: 3.8123), Xent 0.9544, Loss 4.2866, Error 0.3403(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2330 | Time 23.2350(23.7287) | Bit/dim 3.8135(3.8144) | Xent 0.9177(0.9855) | Loss 10.2329(11.2214) | Error 0.3272(0.3524) Steps 628(610.40) | Grad Norm 8.3698(11.5478) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 23.8044(23.8142) | Bit/dim 3.8221(3.8115) | Xent 0.9554(0.9761) | Loss 10.2135(10.9376) | Error 0.3411(0.3492) Steps 610(608.70) | Grad Norm 7.8301(10.9042) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 84.7024, Epoch Time 743.9459(729.5975), Bit/dim 3.8062(best: 3.8094), Xent 0.9716, Loss 4.2920, Error 0.3475(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2350 | Time 24.9615(23.8763) | Bit/dim 3.8093(3.8103) | Xent 1.0060(0.9808) | Loss 33.9887(11.4639) | Error 0.3722(0.3507) Steps 616(609.08) | Grad Norm 18.2521(11.9878) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 23.0006(23.9317) | Bit/dim 3.8153(3.8076) | Xent 0.9322(0.9760) | Loss 10.1185(11.1157) | Error 0.3278(0.3488) Steps 604(608.85) | Grad Norm 6.9007(12.0267) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 23.7457(23.8885) | Bit/dim 3.8026(3.8074) | Xent 0.9643(0.9758) | Loss 10.1540(10.8556) | Error 0.3450(0.3486) Steps 616(610.04) | Grad Norm 8.9884(11.5106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 83.8705, Epoch Time 747.9671(730.1486), Bit/dim 3.8070(best: 3.8062), Xent 0.9591, Loss 4.2865, Error 0.3418(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2380 | Time 23.0445(23.8593) | Bit/dim 3.8081(3.8086) | Xent 0.9783(0.9749) | Loss 10.0852(11.3293) | Error 0.3411(0.3482) Steps 604(609.61) | Grad Norm 10.9576(11.6162) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 24.3062(23.8855) | Bit/dim 3.8028(3.8075) | Xent 0.9015(0.9689) | Loss 10.0772(11.0179) | Error 0.3150(0.3456) Steps 586(608.28) | Grad Norm 5.8090(10.9676) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 24.3940(23.8204) | Bit/dim 3.8000(3.8038) | Xent 0.9241(0.9652) | Loss 10.0351(10.7634) | Error 0.3350(0.3445) Steps 604(606.39) | Grad Norm 6.8439(11.0768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 85.1829, Epoch Time 746.2272(730.6310), Bit/dim 3.8012(best: 3.8062), Xent 0.9312, Loss 4.2668, Error 0.3309(best: 0.3365)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2410 | Time 24.1199(23.7981) | Bit/dim 3.7952(3.8004) | Xent 0.9273(0.9659) | Loss 9.9997(11.1779) | Error 0.3367(0.3456) Steps 598(606.70) | Grad Norm 13.3056(11.6468) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 23.8282(23.8591) | Bit/dim 3.8085(3.8002) | Xent 0.9813(0.9634) | Loss 10.1116(10.9160) | Error 0.3422(0.3442) Steps 610(610.86) | Grad Norm 11.8352(11.2458) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 24.1473(23.8355) | Bit/dim 3.7714(3.7993) | Xent 0.9740(0.9613) | Loss 10.1929(10.7131) | Error 0.3478(0.3442) Steps 580(610.34) | Grad Norm 10.2370(10.9846) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 83.7729, Epoch Time 743.9466(731.0304), Bit/dim 3.7904(best: 3.8012), Xent 0.9193, Loss 4.2501, Error 0.3272(best: 0.3309)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2440 | Time 23.8073(23.7997) | Bit/dim 3.7836(3.7971) | Xent 0.9263(0.9635) | Loss 10.1265(11.0949) | Error 0.3300(0.3446) Steps 610(610.48) | Grad Norm 13.8896(11.6254) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 24.7996(23.8546) | Bit/dim 3.8011(3.7961) | Xent 1.0191(0.9618) | Loss 10.2253(10.8272) | Error 0.3600(0.3431) Steps 598(610.15) | Grad Norm 19.8967(11.6200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 84.2214, Epoch Time 743.9543(731.4181), Bit/dim 3.7982(best: 3.7904), Xent 0.9537, Loss 4.2750, Error 0.3452(best: 0.3272)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2460 | Time 22.8990(23.7858) | Bit/dim 3.7926(3.7948) | Xent 0.9528(0.9634) | Loss 10.1255(11.3257) | Error 0.3394(0.3433) Steps 616(608.96) | Grad Norm 12.3363(11.7416) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 23.4756(23.7304) | Bit/dim 3.7936(3.7946) | Xent 0.8976(0.9588) | Loss 9.9626(11.0004) | Error 0.3211(0.3410) Steps 586(610.61) | Grad Norm 10.5473(11.4831) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 22.6414(23.7675) | Bit/dim 3.7886(3.7942) | Xent 1.0135(0.9552) | Loss 10.1077(10.7516) | Error 0.3650(0.3398) Steps 574(610.30) | Grad Norm 10.4313(11.0106) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 85.4631, Epoch Time 743.9689(731.7947), Bit/dim 3.7871(best: 3.7904), Xent 0.9203, Loss 4.2473, Error 0.3233(best: 0.3272)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2490 | Time 24.0871(23.7507) | Bit/dim 3.7807(3.7921) | Xent 0.9423(0.9492) | Loss 10.1642(11.2110) | Error 0.3461(0.3384) Steps 610(609.06) | Grad Norm 7.7017(10.7164) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 23.8994(23.7803) | Bit/dim 3.8010(3.7922) | Xent 0.9074(0.9555) | Loss 10.2145(10.9278) | Error 0.3239(0.3406) Steps 604(610.07) | Grad Norm 6.7162(12.2735) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 24.3721(23.9228) | Bit/dim 3.8059(3.7931) | Xent 0.9391(0.9586) | Loss 9.9163(10.7264) | Error 0.3333(0.3427) Steps 580(612.15) | Grad Norm 8.7973(11.9031) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 84.7986, Epoch Time 748.3109(732.2901), Bit/dim 3.7864(best: 3.7871), Xent 0.9187, Loss 4.2458, Error 0.3290(best: 0.3233)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2520 | Time 24.3524(23.9149) | Bit/dim 3.8188(3.7943) | Xent 0.8650(0.9476) | Loss 10.1032(11.1233) | Error 0.3117(0.3390) Steps 580(609.00) | Grad Norm 8.3922(10.9657) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 22.4807(23.9607) | Bit/dim 3.7757(3.7903) | Xent 0.9293(0.9426) | Loss 9.9941(10.8410) | Error 0.3211(0.3366) Steps 610(609.90) | Grad Norm 5.9474(10.3168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 85.0911, Epoch Time 750.1857(732.8270), Bit/dim 3.7800(best: 3.7864), Xent 0.9278, Loss 4.2440, Error 0.3327(best: 0.3233)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2540 | Time 25.9394(23.9796) | Bit/dim 3.7909(3.7871) | Xent 0.9181(0.9367) | Loss 10.1993(11.3414) | Error 0.3278(0.3342) Steps 640(611.97) | Grad Norm 7.1635(9.8631) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 23.0018(23.9973) | Bit/dim 3.7917(3.7845) | Xent 0.9281(0.9378) | Loss 10.0313(11.0003) | Error 0.3222(0.3336) Steps 616(611.12) | Grad Norm 10.6512(10.1843) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 24.7555(23.9611) | Bit/dim 3.7829(3.7860) | Xent 0.9466(0.9525) | Loss 10.1768(10.7718) | Error 0.3472(0.3395) Steps 604(611.37) | Grad Norm 9.5628(11.4217) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 83.2998, Epoch Time 747.8723(733.2784), Bit/dim 3.7805(best: 3.7800), Xent 0.9315, Loss 4.2463, Error 0.3351(best: 0.3233)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2570 | Time 24.1802(23.8475) | Bit/dim 3.8173(3.7875) | Xent 0.9048(0.9594) | Loss 10.0246(11.2208) | Error 0.3322(0.3430) Steps 634(610.73) | Grad Norm 10.1276(11.2906) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 23.2277(23.8624) | Bit/dim 3.7645(3.7875) | Xent 0.8993(0.9477) | Loss 10.0625(10.9150) | Error 0.3272(0.3389) Steps 598(609.20) | Grad Norm 7.2266(10.3988) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 22.4776(23.7964) | Bit/dim 3.8258(3.7869) | Xent 0.9281(0.9418) | Loss 9.9923(10.6916) | Error 0.3344(0.3367) Steps 604(609.76) | Grad Norm 6.0714(10.1602) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 84.9247, Epoch Time 743.4128(733.5824), Bit/dim 3.7836(best: 3.7800), Xent 0.8865, Loss 4.2268, Error 0.3164(best: 0.3233)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2600 | Time 24.1381(23.8148) | Bit/dim 3.7818(3.7857) | Xent 0.8975(0.9327) | Loss 10.1106(11.1017) | Error 0.3211(0.3332) Steps 610(609.32) | Grad Norm 8.6966(9.6511) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 23.1643(23.7343) | Bit/dim 3.7596(3.7817) | Xent 0.8870(0.9251) | Loss 10.0834(10.8200) | Error 0.3144(0.3301) Steps 604(610.13) | Grad Norm 10.7068(9.5891) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 85.5318, Epoch Time 738.7302(733.7368), Bit/dim 3.7871(best: 3.7800), Xent 0.9156, Loss 4.2449, Error 0.3279(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2620 | Time 25.0250(23.6811) | Bit/dim 3.7926(3.7828) | Xent 0.9056(0.9337) | Loss 33.3544(11.3170) | Error 0.3228(0.3337) Steps 604(610.52) | Grad Norm 9.1952(10.5882) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 23.3253(23.7722) | Bit/dim 3.7849(3.7833) | Xent 0.9261(0.9327) | Loss 10.0433(10.9951) | Error 0.3250(0.3335) Steps 610(608.71) | Grad Norm 7.9863(11.2117) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 23.9082(23.7640) | Bit/dim 3.7935(3.7878) | Xent 0.9499(0.9457) | Loss 10.1240(10.7731) | Error 0.3622(0.3383) Steps 610(608.65) | Grad Norm 17.0774(13.0596) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 85.0212, Epoch Time 750.1815(734.2302), Bit/dim 3.7790(best: 3.7800), Xent 0.9069, Loss 4.2324, Error 0.3251(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2650 | Time 24.6696(23.8975) | Bit/dim 3.7578(3.7863) | Xent 0.8868(0.9393) | Loss 9.8573(11.2430) | Error 0.3256(0.3358) Steps 616(606.69) | Grad Norm 6.3824(11.9720) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 24.3069(23.8645) | Bit/dim 3.7611(3.7852) | Xent 0.9175(0.9296) | Loss 9.8617(10.9116) | Error 0.3250(0.3321) Steps 592(606.78) | Grad Norm 13.2210(11.7548) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 23.7765(23.7967) | Bit/dim 3.7648(3.7819) | Xent 0.9193(0.9232) | Loss 9.9970(10.6684) | Error 0.3239(0.3300) Steps 592(606.69) | Grad Norm 9.0153(10.9527) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 85.1487, Epoch Time 748.0843(734.6458), Bit/dim 3.7714(best: 3.7790), Xent 0.8936, Loss 4.2182, Error 0.3204(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2680 | Time 23.5830(23.8831) | Bit/dim 3.7672(3.7781) | Xent 0.8959(0.9185) | Loss 10.0256(11.1104) | Error 0.3161(0.3276) Steps 634(611.33) | Grad Norm 11.9421(11.0279) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 23.1478(23.7691) | Bit/dim 3.7622(3.7765) | Xent 0.8840(0.9171) | Loss 10.1773(10.8380) | Error 0.3167(0.3265) Steps 634(614.34) | Grad Norm 11.0991(11.1780) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 23.0856(23.8613) | Bit/dim 3.7696(3.7744) | Xent 0.9314(0.9174) | Loss 10.1295(10.6404) | Error 0.3328(0.3275) Steps 640(616.18) | Grad Norm 9.6420(11.2330) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 84.5702, Epoch Time 745.8264(734.9812), Bit/dim 3.7832(best: 3.7714), Xent 0.9606, Loss 4.2635, Error 0.3406(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2710 | Time 23.9448(23.8418) | Bit/dim 3.7721(3.7750) | Xent 0.8496(0.9119) | Loss 9.9343(11.0193) | Error 0.3050(0.3258) Steps 592(615.36) | Grad Norm 7.6536(11.2046) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 23.9212(23.8415) | Bit/dim 3.7804(3.7726) | Xent 0.9456(0.9101) | Loss 10.1739(10.7628) | Error 0.3333(0.3250) Steps 658(615.00) | Grad Norm 15.5145(10.7627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 84.0326, Epoch Time 746.6285(735.3306), Bit/dim 3.7726(best: 3.7714), Xent 0.9546, Loss 4.2499, Error 0.3440(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2730 | Time 24.4448(23.8740) | Bit/dim 3.7252(3.7711) | Xent 0.9334(0.9141) | Loss 9.7579(11.2495) | Error 0.3350(0.3263) Steps 622(613.93) | Grad Norm 13.4932(11.6446) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 22.9651(23.8680) | Bit/dim 3.7772(3.7725) | Xent 0.8912(0.9093) | Loss 9.8023(10.9230) | Error 0.3028(0.3245) Steps 604(612.11) | Grad Norm 14.6010(11.7125) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 23.7144(23.7924) | Bit/dim 3.7733(3.7688) | Xent 0.8978(0.9067) | Loss 10.1087(10.6765) | Error 0.3250(0.3239) Steps 628(610.73) | Grad Norm 11.0696(11.1560) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 84.3058, Epoch Time 742.7515(735.5533), Bit/dim 3.7689(best: 3.7714), Xent 0.8664, Loss 4.2021, Error 0.3079(best: 0.3164)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2760 | Time 23.6191(23.7942) | Bit/dim 3.7562(3.7681) | Xent 0.8961(0.8968) | Loss 9.9618(11.1300) | Error 0.3222(0.3211) Steps 634(610.68) | Grad Norm 6.5907(10.2166) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 23.4458(23.8682) | Bit/dim 3.7826(3.7676) | Xent 0.8711(0.8914) | Loss 10.0619(10.8362) | Error 0.3161(0.3191) Steps 580(612.07) | Grad Norm 10.2138(10.0325) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 23.0721(23.9096) | Bit/dim 3.7671(3.7655) | Xent 0.9060(0.8918) | Loss 9.8270(10.5991) | Error 0.3311(0.3192) Steps 604(610.41) | Grad Norm 9.7113(10.7032) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 84.7164, Epoch Time 750.9611(736.0155), Bit/dim 3.7603(best: 3.7689), Xent 0.8807, Loss 4.2007, Error 0.3097(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2790 | Time 23.4538(23.9732) | Bit/dim 3.7511(3.7653) | Xent 0.8488(0.8925) | Loss 9.8912(11.0185) | Error 0.3039(0.3195) Steps 628(613.65) | Grad Norm 5.5478(10.5188) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 24.0233(23.8737) | Bit/dim 3.7660(3.7635) | Xent 0.8840(0.8885) | Loss 10.1118(10.7398) | Error 0.3194(0.3184) Steps 610(613.66) | Grad Norm 8.5767(9.9332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 85.0544, Epoch Time 749.0504(736.4065), Bit/dim 3.7604(best: 3.7603), Xent 0.8834, Loss 4.2021, Error 0.3150(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2810 | Time 25.2650(23.9716) | Bit/dim 3.7564(3.7635) | Xent 0.8262(0.8826) | Loss 9.9515(11.2426) | Error 0.2939(0.3157) Steps 586(614.67) | Grad Norm 5.4468(10.0291) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 25.0285(23.9052) | Bit/dim 3.7697(3.7622) | Xent 0.8408(0.8747) | Loss 9.8885(10.8884) | Error 0.2994(0.3126) Steps 562(610.83) | Grad Norm 5.7443(9.3682) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 22.2461(23.9384) | Bit/dim 3.7880(3.7619) | Xent 0.9067(0.8835) | Loss 9.9073(10.6553) | Error 0.3261(0.3160) Steps 598(611.61) | Grad Norm 17.2194(10.2367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 84.7682, Epoch Time 749.0488(736.7858), Bit/dim 3.7574(best: 3.7603), Xent 0.8777, Loss 4.1962, Error 0.3120(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2840 | Time 24.7015(23.9277) | Bit/dim 3.7618(3.7618) | Xent 0.8612(0.8849) | Loss 9.9344(11.1243) | Error 0.3128(0.3164) Steps 610(609.84) | Grad Norm 10.7465(10.3136) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 23.3915(23.9166) | Bit/dim 3.7502(3.7599) | Xent 0.8432(0.8819) | Loss 9.9375(10.8257) | Error 0.3144(0.3163) Steps 622(610.97) | Grad Norm 7.0555(11.1027) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 23.1412(23.8135) | Bit/dim 3.7499(3.7601) | Xent 0.9182(0.8851) | Loss 9.9649(10.5939) | Error 0.3122(0.3166) Steps 574(609.17) | Grad Norm 11.0839(10.8643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 85.5521, Epoch Time 743.5769(736.9895), Bit/dim 3.7641(best: 3.7574), Xent 0.8689, Loss 4.1985, Error 0.3083(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2870 | Time 24.5205(23.8346) | Bit/dim 3.7522(3.7594) | Xent 0.8228(0.8850) | Loss 9.7222(11.0060) | Error 0.3022(0.3168) Steps 610(607.24) | Grad Norm 8.7033(11.0841) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 25.0090(23.9156) | Bit/dim 3.7352(3.7591) | Xent 0.8166(0.8794) | Loss 9.8002(10.7255) | Error 0.2878(0.3152) Steps 604(609.33) | Grad Norm 8.2486(10.5284) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 85.0670, Epoch Time 752.0688(737.4419), Bit/dim 3.7704(best: 3.7574), Xent 0.9079, Loss 4.2243, Error 0.3199(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2890 | Time 23.0170(23.9277) | Bit/dim 3.7856(3.7596) | Xent 0.9018(0.8795) | Loss 33.9063(11.2426) | Error 0.3206(0.3151) Steps 610(610.75) | Grad Norm 21.7681(11.3076) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 24.6274(23.9559) | Bit/dim 3.7569(3.7617) | Xent 0.8751(0.8759) | Loss 9.9993(10.9117) | Error 0.3078(0.3127) Steps 598(609.78) | Grad Norm 9.7449(11.4244) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 22.9967(23.9391) | Bit/dim 3.7804(3.7611) | Xent 0.8544(0.8734) | Loss 10.1278(10.6762) | Error 0.3011(0.3120) Steps 616(610.71) | Grad Norm 7.6452(11.3395) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 85.9222, Epoch Time 748.7657(737.7816), Bit/dim 3.7481(best: 3.7574), Xent 0.8489, Loss 4.1725, Error 0.3033(best: 0.3079)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2920 | Time 26.0107(23.9579) | Bit/dim 3.7374(3.7564) | Xent 0.9065(0.8695) | Loss 10.0326(11.1344) | Error 0.3178(0.3105) Steps 586(610.94) | Grad Norm 13.3395(11.3306) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 24.4203(23.9001) | Bit/dim 3.7710(3.7586) | Xent 0.8339(0.8762) | Loss 10.0144(10.8424) | Error 0.3050(0.3137) Steps 634(612.08) | Grad Norm 7.8794(11.5931) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 23.4009(23.9686) | Bit/dim 3.7681(3.7584) | Xent 0.8230(0.8665) | Loss 9.8221(10.6024) | Error 0.2900(0.3091) Steps 586(616.76) | Grad Norm 7.4921(11.0271) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 85.1262, Epoch Time 750.5713(738.1653), Bit/dim 3.7481(best: 3.7481), Xent 0.8438, Loss 4.1700, Error 0.3025(best: 0.3033)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2950 | Time 24.5797(23.9702) | Bit/dim 3.7641(3.7574) | Xent 0.8118(0.8589) | Loss 9.9022(11.0557) | Error 0.2828(0.3064) Steps 622(614.64) | Grad Norm 9.1788(10.4588) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 23.3704(23.9990) | Bit/dim 3.7352(3.7546) | Xent 0.7754(0.8531) | Loss 9.8362(10.7490) | Error 0.2794(0.3051) Steps 598(614.80) | Grad Norm 5.5007(9.9718) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 23.6549(23.9181) | Bit/dim 3.7392(3.7522) | Xent 0.9537(0.8610) | Loss 9.8622(10.5260) | Error 0.3311(0.3065) Steps 598(613.56) | Grad Norm 18.9830(10.8555) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 86.0453, Epoch Time 750.3297(738.5303), Bit/dim 3.7576(best: 3.7481), Xent 1.0104, Loss 4.2628, Error 0.3470(best: 0.3025)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 2980 | Time 23.5475(23.8909) | Bit/dim 3.7336(3.7529) | Xent 0.8835(0.8712) | Loss 9.9858(10.9246) | Error 0.3300(0.3112) Steps 610(613.26) | Grad Norm 13.4979(11.7722) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 25.2165(23.9453) | Bit/dim 3.7439(3.7528) | Xent 0.9394(0.8766) | Loss 10.1822(10.6862) | Error 0.3256(0.3122) Steps 634(615.78) | Grad Norm 17.5619(12.6190) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 86.1642, Epoch Time 752.7423(738.9566), Bit/dim 3.7594(best: 3.7481), Xent 0.8811, Loss 4.1999, Error 0.3101(best: 0.3025)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3000 | Time 22.8728(23.9760) | Bit/dim 3.7413(3.7557) | Xent 0.8954(0.8743) | Loss 9.8659(11.1964) | Error 0.3239(0.3119) Steps 598(612.89) | Grad Norm 12.7605(11.9057) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 24.0318(23.8740) | Bit/dim 3.7696(3.7557) | Xent 0.9327(0.8745) | Loss 10.1305(10.8740) | Error 0.3239(0.3119) Steps 616(611.05) | Grad Norm 16.4625(11.9019) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 23.7870(23.8765) | Bit/dim 3.7422(3.7536) | Xent 0.8019(0.8692) | Loss 9.9017(10.6167) | Error 0.2950(0.3107) Steps 592(609.95) | Grad Norm 11.4199(11.3867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 87.2792, Epoch Time 743.8710(739.1041), Bit/dim 3.7433(best: 3.7481), Xent 0.8498, Loss 4.1682, Error 0.3025(best: 0.3025)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3030 | Time 24.4032(23.7555) | Bit/dim 3.7795(3.7519) | Xent 0.8128(0.8562) | Loss 9.9290(11.0231) | Error 0.2944(0.3066) Steps 622(608.76) | Grad Norm 5.0889(10.4346) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 22.9575(23.8001) | Bit/dim 3.7398(3.7507) | Xent 0.9404(0.8585) | Loss 10.1117(10.7405) | Error 0.3278(0.3070) Steps 616(610.36) | Grad Norm 23.3216(10.6614) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 23.1024(23.8666) | Bit/dim 3.7680(3.7506) | Xent 0.8274(0.8638) | Loss 10.0245(10.5400) | Error 0.2878(0.3076) Steps 598(612.64) | Grad Norm 11.6664(11.3749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 86.0934, Epoch Time 750.6858(739.4515), Bit/dim 3.7533(best: 3.7433), Xent 0.8386, Loss 4.1726, Error 0.2976(best: 0.3025)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3060 | Time 25.0790(23.9264) | Bit/dim 3.7446(3.7485) | Xent 0.8483(0.8616) | Loss 9.9366(10.9581) | Error 0.2956(0.3067) Steps 628(613.71) | Grad Norm 8.7258(11.2129) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 24.1905(24.0398) | Bit/dim 3.7484(3.7478) | Xent 0.8637(0.8553) | Loss 10.0679(10.6765) | Error 0.3028(0.3045) Steps 628(613.00) | Grad Norm 13.6149(11.4547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 87.0972, Epoch Time 755.4218(739.9306), Bit/dim 3.7492(best: 3.7433), Xent 0.8426, Loss 4.1705, Error 0.3016(best: 0.2976)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3080 | Time 23.3108(24.0247) | Bit/dim 3.7505(3.7496) | Xent 0.8336(0.8546) | Loss 9.8864(11.2126) | Error 0.2978(0.3039) Steps 616(614.59) | Grad Norm 9.1591(11.5740) | Total Time 0.00(0.00)\n",
      "Iter 3090 | Time 23.8533(23.9333) | Bit/dim 3.7424(3.7488) | Xent 0.8129(0.8487) | Loss 9.7678(10.8624) | Error 0.2872(0.3027) Steps 598(614.05) | Grad Norm 10.0521(11.1516) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 24.2989(23.8380) | Bit/dim 3.7424(3.7464) | Xent 0.8371(0.8487) | Loss 9.7583(10.6072) | Error 0.3078(0.3038) Steps 604(613.33) | Grad Norm 9.8615(10.9075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 86.5602, Epoch Time 744.2608(740.0605), Bit/dim 3.7402(best: 3.7433), Xent 0.8459, Loss 4.1632, Error 0.3014(best: 0.2976)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3110 | Time 23.8811(23.8385) | Bit/dim 3.7456(3.7464) | Xent 0.7911(0.8462) | Loss 9.7206(11.0897) | Error 0.2722(0.3021) Steps 574(613.64) | Grad Norm 8.2037(10.9127) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 25.2734(23.9146) | Bit/dim 3.7275(3.7429) | Xent 0.8132(0.8406) | Loss 9.7479(10.7736) | Error 0.2939(0.3011) Steps 622(614.81) | Grad Norm 8.0424(10.5038) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 22.9010(23.7202) | Bit/dim 3.7362(3.7423) | Xent 0.8105(0.8363) | Loss 9.8314(10.5409) | Error 0.2900(0.2998) Steps 622(612.66) | Grad Norm 5.8646(9.7709) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 84.7361, Epoch Time 743.4609(740.1625), Bit/dim 3.7560(best: 3.7402), Xent 0.9173, Loss 4.2146, Error 0.3173(best: 0.2976)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3140 | Time 23.1660(23.8446) | Bit/dim 3.7601(3.7441) | Xent 0.8312(0.8372) | Loss 9.7976(10.9913) | Error 0.3067(0.3003) Steps 574(613.28) | Grad Norm 8.1029(10.1475) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 85.1937, Epoch Time 751.3827(740.4991), Bit/dim 3.7423(best: 3.7402), Xent 0.8362, Loss 4.1604, Error 0.2949(best: 0.2976)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3160 | Time 23.5062(23.8991) | Bit/dim 3.7346(3.7420) | Xent 0.8374(0.8400) | Loss 33.3936(11.2048) | Error 0.3100(0.3004) Steps 592(611.03) | Grad Norm 9.8031(10.4523) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 23.7002(23.8882) | Bit/dim 3.7409(3.7404) | Xent 0.8047(0.8322) | Loss 9.9184(10.8443) | Error 0.2950(0.2978) Steps 604(609.46) | Grad Norm 6.6832(10.2566) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 25.2867(24.0414) | Bit/dim 3.7200(3.7403) | Xent 0.8322(0.8291) | Loss 9.9316(10.5916) | Error 0.2989(0.2959) Steps 622(612.41) | Grad Norm 8.7063(9.8674) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 84.8657, Epoch Time 753.3950(740.8860), Bit/dim 3.7332(best: 3.7402), Xent 0.8362, Loss 4.1513, Error 0.2937(best: 0.2949)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3190 | Time 25.1770(24.0494) | Bit/dim 3.7379(3.7392) | Xent 0.7975(0.8282) | Loss 9.8506(11.0760) | Error 0.2900(0.2961) Steps 604(612.40) | Grad Norm 11.9806(9.6332) | Total Time 0.00(0.00)\n",
      "Iter 3200 | Time 23.0400(24.0014) | Bit/dim 3.7668(3.7397) | Xent 0.8999(0.8344) | Loss 10.0652(10.7564) | Error 0.3172(0.2974) Steps 604(612.76) | Grad Norm 16.3930(11.0757) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 24.1606(24.0163) | Bit/dim 3.7381(3.7408) | Xent 0.8333(0.8422) | Loss 9.8692(10.5475) | Error 0.3006(0.3003) Steps 640(614.97) | Grad Norm 7.8757(11.2306) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 85.9148, Epoch Time 749.5664(741.1464), Bit/dim 3.7422(best: 3.7332), Xent 0.8247, Loss 4.1546, Error 0.2937(best: 0.2937)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3220 | Time 23.4207(23.9254) | Bit/dim 3.7430(3.7403) | Xent 0.8309(0.8366) | Loss 9.8466(10.9869) | Error 0.3094(0.2990) Steps 586(612.32) | Grad Norm 5.4763(10.6034) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 23.0639(23.8927) | Bit/dim 3.7696(3.7417) | Xent 0.8314(0.8314) | Loss 9.8025(10.6958) | Error 0.3106(0.2987) Steps 574(611.47) | Grad Norm 10.8634(10.3926) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 24.4592(23.9747) | Bit/dim 3.7158(3.7387) | Xent 0.8274(0.8293) | Loss 9.8136(10.4782) | Error 0.3011(0.2976) Steps 640(612.44) | Grad Norm 4.4863(9.8192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 86.1443, Epoch Time 750.2427(741.4193), Bit/dim 3.7314(best: 3.7332), Xent 0.8273, Loss 4.1450, Error 0.2926(best: 0.2937)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3250 | Time 24.5184(23.8887) | Bit/dim 3.7309(3.7346) | Xent 0.8511(0.8289) | Loss 9.9913(10.8537) | Error 0.3028(0.2968) Steps 640(613.19) | Grad Norm 19.5595(10.3289) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 23.0943(23.8793) | Bit/dim 3.7199(3.7356) | Xent 0.8515(0.8351) | Loss 9.7765(10.6094) | Error 0.3044(0.3001) Steps 586(614.64) | Grad Norm 12.4265(10.6155) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 87.6714, Epoch Time 748.2863(741.6253), Bit/dim 3.7327(best: 3.7314), Xent 0.8085, Loss 4.1370, Error 0.2847(best: 0.2926)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3270 | Time 23.9640(23.8875) | Bit/dim 3.7186(3.7339) | Xent 0.7695(0.8244) | Loss 9.8578(11.0977) | Error 0.2806(0.2965) Steps 640(613.19) | Grad Norm 7.8661(9.6166) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 24.1622(23.8415) | Bit/dim 3.7299(3.7331) | Xent 0.8424(0.8215) | Loss 9.6976(10.7581) | Error 0.3033(0.2944) Steps 616(612.19) | Grad Norm 9.8550(9.3030) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 22.4812(23.8611) | Bit/dim 3.7553(3.7329) | Xent 0.8261(0.8149) | Loss 9.8678(10.5032) | Error 0.2828(0.2916) Steps 616(609.99) | Grad Norm 13.0691(9.3273) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 87.0374, Epoch Time 748.3787(741.8279), Bit/dim 3.7385(best: 3.7314), Xent 0.8513, Loss 4.1642, Error 0.3011(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3310 | Time 23.5203(23.9180) | Bit/dim 3.7113(3.7320) | Xent 0.7889(0.8028) | Loss 9.8016(10.6544) | Error 0.2833(0.2870) Steps 610(610.86) | Grad Norm 7.7734(8.8527) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 23.9164(23.8131) | Bit/dim 3.7258(3.7290) | Xent 0.7850(0.7994) | Loss 9.8465(10.4404) | Error 0.2733(0.2855) Steps 622(614.12) | Grad Norm 7.6751(8.7598) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 86.1907, Epoch Time 746.7020(741.9741), Bit/dim 3.7245(best: 3.7314), Xent 0.8229, Loss 4.1359, Error 0.2908(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3330 | Time 24.5077(23.8786) | Bit/dim 3.7213(3.7263) | Xent 0.9671(0.8025) | Loss 10.0489(10.8696) | Error 0.3500(0.2866) Steps 598(615.51) | Grad Norm 30.6216(9.8306) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 23.0394(23.6706) | Bit/dim 3.7321(3.7282) | Xent 0.8050(0.8175) | Loss 9.9204(10.6196) | Error 0.2806(0.2911) Steps 622(612.05) | Grad Norm 5.9294(10.1008) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 88.2491, Epoch Time 742.4041(741.9870), Bit/dim 3.7322(best: 3.7245), Xent 0.8252, Loss 4.1448, Error 0.2932(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3350 | Time 24.3644(23.6839) | Bit/dim 3.7324(3.7290) | Xent 0.8246(0.8157) | Loss 9.8828(11.1185) | Error 0.2939(0.2909) Steps 604(611.58) | Grad Norm 10.0556(9.9084) | Total Time 0.00(0.00)\n",
      "Iter 3360 | Time 23.9749(23.6317) | Bit/dim 3.7433(3.7291) | Xent 0.8124(0.8138) | Loss 9.8769(10.7864) | Error 0.2978(0.2902) Steps 610(610.79) | Grad Norm 9.3093(10.2376) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 23.4550(23.5982) | Bit/dim 3.7167(3.7284) | Xent 0.7869(0.8081) | Loss 9.7545(10.5248) | Error 0.2817(0.2878) Steps 598(611.60) | Grad Norm 13.8598(10.1392) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 86.5707, Epoch Time 740.9495(741.9559), Bit/dim 3.7407(best: 3.7245), Xent 0.9678, Loss 4.2247, Error 0.3399(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3380 | Time 23.2698(23.6286) | Bit/dim 3.7531(3.7286) | Xent 0.9274(0.8235) | Loss 10.1110(10.9959) | Error 0.3244(0.2925) Steps 628(612.23) | Grad Norm 19.4458(11.8854) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 23.0097(23.6675) | Bit/dim 3.7117(3.7292) | Xent 0.8023(0.8243) | Loss 9.6806(10.6899) | Error 0.2756(0.2920) Steps 622(614.92) | Grad Norm 5.6552(11.5838) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 23.2005(23.7102) | Bit/dim 3.7064(3.7294) | Xent 0.8004(0.8167) | Loss 9.7387(10.4503) | Error 0.2839(0.2898) Steps 616(613.13) | Grad Norm 12.1521(11.3848) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 86.9624, Epoch Time 747.4594(742.1210), Bit/dim 3.7297(best: 3.7245), Xent 0.7993, Loss 4.1293, Error 0.2868(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3410 | Time 25.5576(23.7497) | Bit/dim 3.6961(3.7287) | Xent 0.7897(0.8086) | Loss 9.8213(10.8852) | Error 0.2650(0.2859) Steps 628(613.26) | Grad Norm 12.3796(11.2175) | Total Time 0.00(0.00)\n",
      "Iter 3420 | Time 24.2402(23.9207) | Bit/dim 3.7194(3.7273) | Xent 0.7329(0.8029) | Loss 9.8226(10.6172) | Error 0.2556(0.2836) Steps 628(616.19) | Grad Norm 8.7116(10.8084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 86.4719, Epoch Time 751.0562(742.3891), Bit/dim 3.7177(best: 3.7245), Xent 0.8098, Loss 4.1226, Error 0.2861(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3430 | Time 23.9431(23.8465) | Bit/dim 3.7312(3.7255) | Xent 0.7916(0.7995) | Loss 34.7364(11.1563) | Error 0.2983(0.2835) Steps 604(616.25) | Grad Norm 7.0517(10.6872) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 24.5806(23.9183) | Bit/dim 3.7150(3.7243) | Xent 0.7720(0.7990) | Loss 9.8841(10.8083) | Error 0.2800(0.2846) Steps 634(618.20) | Grad Norm 7.4483(10.7399) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 23.0044(23.8665) | Bit/dim 3.7226(3.7245) | Xent 0.7994(0.8024) | Loss 9.8397(10.5608) | Error 0.2806(0.2851) Steps 616(620.06) | Grad Norm 10.3326(11.1162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 87.0778, Epoch Time 750.8344(742.6424), Bit/dim 3.7205(best: 3.7177), Xent 0.8004, Loss 4.1207, Error 0.2824(best: 0.2847)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3460 | Time 24.5082(23.8252) | Bit/dim 3.7201(3.7234) | Xent 0.7488(0.7990) | Loss 9.9109(11.0525) | Error 0.2556(0.2851) Steps 646(620.25) | Grad Norm 7.2740(10.5010) | Total Time 0.00(0.00)\n",
      "Iter 3470 | Time 24.5832(23.8682) | Bit/dim 3.7472(3.7253) | Xent 0.7664(0.7919) | Loss 9.8356(10.7258) | Error 0.2761(0.2820) Steps 604(618.21) | Grad Norm 12.4626(10.5216) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 24.7688(23.8854) | Bit/dim 3.7410(3.7242) | Xent 0.7894(0.7961) | Loss 9.5858(10.4712) | Error 0.2817(0.2834) Steps 562(616.55) | Grad Norm 9.9106(10.6177) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 88.2934, Epoch Time 752.8604(742.9490), Bit/dim 3.7288(best: 3.7177), Xent 0.8219, Loss 4.1397, Error 0.2894(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3490 | Time 22.7487(23.8267) | Bit/dim 3.7199(3.7251) | Xent 0.8057(0.7911) | Loss 9.8735(10.8933) | Error 0.2894(0.2817) Steps 622(616.75) | Grad Norm 10.8046(10.5158) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 23.1700(23.9972) | Bit/dim 3.7048(3.7228) | Xent 0.7461(0.7854) | Loss 9.8046(10.6136) | Error 0.2694(0.2800) Steps 622(615.84) | Grad Norm 7.1617(10.1657) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 23.7344(23.9475) | Bit/dim 3.6941(3.7199) | Xent 0.7963(0.7835) | Loss 9.7878(10.3814) | Error 0.2872(0.2793) Steps 616(611.42) | Grad Norm 7.8899(9.7763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 87.4081, Epoch Time 752.5128(743.2359), Bit/dim 3.7185(best: 3.7177), Xent 0.8224, Loss 4.1297, Error 0.2900(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3520 | Time 23.3185(23.9410) | Bit/dim 3.7171(3.7197) | Xent 0.8190(0.7812) | Loss 9.8504(10.7994) | Error 0.2794(0.2775) Steps 628(610.74) | Grad Norm 15.1325(10.3870) | Total Time 0.00(0.00)\n",
      "Iter 3530 | Time 23.1132(24.0182) | Bit/dim 3.6848(3.7174) | Xent 0.7360(0.7815) | Loss 9.7914(10.5380) | Error 0.2628(0.2787) Steps 616(611.91) | Grad Norm 10.8227(10.1965) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 88.4603, Epoch Time 757.3842(743.6603), Bit/dim 3.7123(best: 3.7177), Xent 0.8137, Loss 4.1192, Error 0.2884(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3540 | Time 25.8788(24.1202) | Bit/dim 3.7287(3.7180) | Xent 0.7409(0.7806) | Loss 9.7976(11.0311) | Error 0.2694(0.2790) Steps 652(612.87) | Grad Norm 9.3714(10.1294) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 23.2721(24.1244) | Bit/dim 3.7349(3.7186) | Xent 0.7432(0.7761) | Loss 9.8801(10.7037) | Error 0.2700(0.2771) Steps 634(613.09) | Grad Norm 6.4811(9.5852) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 24.8015(24.0180) | Bit/dim 3.7065(3.7157) | Xent 0.8148(0.7750) | Loss 9.9065(10.4550) | Error 0.2833(0.2767) Steps 628(611.25) | Grad Norm 10.0339(9.5927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 88.2842, Epoch Time 752.3906(743.9222), Bit/dim 3.7115(best: 3.7123), Xent 0.7918, Loss 4.1074, Error 0.2834(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3570 | Time 23.6224(24.0529) | Bit/dim 3.7216(3.7168) | Xent 0.7672(0.7700) | Loss 9.7448(10.8878) | Error 0.2639(0.2753) Steps 640(612.97) | Grad Norm 11.4702(9.1808) | Total Time 0.00(0.00)\n",
      "Iter 3580 | Time 23.3286(23.9670) | Bit/dim 3.7050(3.7147) | Xent 0.8350(0.7710) | Loss 9.8211(10.5908) | Error 0.2883(0.2757) Steps 610(613.12) | Grad Norm 17.1613(9.8658) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 24.1715(24.0424) | Bit/dim 3.7350(3.7165) | Xent 0.7698(0.7709) | Loss 9.7994(10.3828) | Error 0.2689(0.2758) Steps 652(614.35) | Grad Norm 9.4077(10.1154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 87.0949, Epoch Time 757.9181(744.3421), Bit/dim 3.7219(best: 3.7115), Xent 0.8214, Loss 4.1326, Error 0.2877(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3600 | Time 22.9147(24.0474) | Bit/dim 3.7289(3.7165) | Xent 0.7745(0.7700) | Loss 9.8564(10.8126) | Error 0.2817(0.2757) Steps 610(615.19) | Grad Norm 12.0779(10.4016) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 23.8571(24.0251) | Bit/dim 3.6780(3.7153) | Xent 0.7961(0.7709) | Loss 9.6781(10.5299) | Error 0.2894(0.2760) Steps 604(615.14) | Grad Norm 11.8241(10.1779) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 86.7829, Epoch Time 754.1474(744.6363), Bit/dim 3.7151(best: 3.7115), Xent 0.7915, Loss 4.1109, Error 0.2793(best: 0.2824)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3620 | Time 23.6441(24.0993) | Bit/dim 3.6844(3.7139) | Xent 0.7616(0.7695) | Loss 9.6397(11.0222) | Error 0.2772(0.2763) Steps 616(614.48) | Grad Norm 10.8121(10.0919) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 23.6493(24.1426) | Bit/dim 3.7037(3.7130) | Xent 0.8080(0.7649) | Loss 9.8585(10.6859) | Error 0.2889(0.2744) Steps 640(613.59) | Grad Norm 9.2640(9.8372) | Total Time 0.00(0.00)\n",
      "Iter 3640 | Time 25.0594(24.1630) | Bit/dim 3.7025(3.7128) | Xent 0.8771(0.7736) | Loss 9.8987(10.4588) | Error 0.3000(0.2768) Steps 628(614.87) | Grad Norm 23.0943(10.6879) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 88.5203, Epoch Time 759.6761(745.0875), Bit/dim 3.7161(best: 3.7115), Xent 0.7955, Loss 4.1139, Error 0.2776(best: 0.2793)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3650 | Time 23.5287(24.1524) | Bit/dim 3.7101(3.7133) | Xent 0.7473(0.7719) | Loss 9.6791(10.9478) | Error 0.2844(0.2767) Steps 616(613.50) | Grad Norm 8.5668(10.5341) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 22.9018(24.1492) | Bit/dim 3.7329(3.7113) | Xent 0.7448(0.7615) | Loss 9.8456(10.6295) | Error 0.2811(0.2733) Steps 592(612.67) | Grad Norm 9.0721(9.6847) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 24.0114(24.2793) | Bit/dim 3.7174(3.7087) | Xent 0.7086(0.7589) | Loss 9.7447(10.3924) | Error 0.2589(0.2728) Steps 628(615.77) | Grad Norm 6.7107(9.8847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 88.5977, Epoch Time 762.7364(745.6169), Bit/dim 3.7081(best: 3.7115), Xent 0.7829, Loss 4.0996, Error 0.2781(best: 0.2776)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3680 | Time 24.7904(24.2479) | Bit/dim 3.7108(3.7090) | Xent 0.7212(0.7571) | Loss 9.8669(10.8305) | Error 0.2539(0.2715) Steps 628(613.92) | Grad Norm 8.6882(9.6814) | Total Time 0.00(0.00)\n",
      "Iter 3690 | Time 22.9600(24.1231) | Bit/dim 3.7185(3.7097) | Xent 0.7407(0.7654) | Loss 9.5436(10.5532) | Error 0.2700(0.2739) Steps 604(617.04) | Grad Norm 9.6667(10.2728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 87.4677, Epoch Time 756.3300(745.9383), Bit/dim 3.7078(best: 3.7081), Xent 0.7818, Loss 4.0986, Error 0.2747(best: 0.2776)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3700 | Time 23.7921(24.1887) | Bit/dim 3.7169(3.7089) | Xent 0.7228(0.7597) | Loss 35.1061(11.1006) | Error 0.2528(0.2727) Steps 628(617.47) | Grad Norm 5.6110(9.5863) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 23.1079(24.2320) | Bit/dim 3.6842(3.7082) | Xent 0.7478(0.7529) | Loss 9.8006(10.7381) | Error 0.2672(0.2702) Steps 646(620.55) | Grad Norm 9.7666(9.4337) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 25.7349(24.3447) | Bit/dim 3.6815(3.7082) | Xent 0.7056(0.7462) | Loss 9.6596(10.4734) | Error 0.2544(0.2683) Steps 616(618.94) | Grad Norm 9.4348(9.2031) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 88.0191, Epoch Time 761.4582(746.4039), Bit/dim 3.7140(best: 3.7078), Xent 0.8425, Loss 4.1352, Error 0.2930(best: 0.2747)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3730 | Time 24.6652(24.1659) | Bit/dim 3.7187(3.7085) | Xent 0.8286(0.7550) | Loss 9.8775(10.9714) | Error 0.2933(0.2713) Steps 616(616.88) | Grad Norm 9.7769(9.8759) | Total Time 0.00(0.00)\n",
      "Iter 3750 | Time 24.3583(24.1316) | Bit/dim 3.7164(3.7080) | Xent 0.7615(0.7501) | Loss 9.7918(10.4270) | Error 0.2822(0.2692) Steps 610(617.64) | Grad Norm 12.1589(9.7671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 88.0831, Epoch Time 753.6453(746.6212), Bit/dim 3.6961(best: 3.7078), Xent 0.7877, Loss 4.0900, Error 0.2738(best: 0.2747)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3760 | Time 23.2474(24.1897) | Bit/dim 3.6815(3.7051) | Xent 0.7057(0.7463) | Loss 9.5990(10.8778) | Error 0.2550(0.2677) Steps 634(617.91) | Grad Norm 11.5983(10.0263) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 23.8390(24.1607) | Bit/dim 3.6968(3.7056) | Xent 0.7116(0.7425) | Loss 9.7487(10.5797) | Error 0.2583(0.2659) Steps 658(622.19) | Grad Norm 5.5095(9.3897) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 24.1971(24.3528) | Bit/dim 3.7244(3.7075) | Xent 0.8212(0.7554) | Loss 9.9059(10.3743) | Error 0.2917(0.2701) Steps 646(618.85) | Grad Norm 15.9606(10.5476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 88.4982, Epoch Time 767.1537(747.2372), Bit/dim 3.7107(best: 3.6961), Xent 0.8423, Loss 4.1318, Error 0.2957(best: 0.2738)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3790 | Time 24.3391(24.3013) | Bit/dim 3.7316(3.7114) | Xent 0.8225(0.7587) | Loss 9.9644(10.8076) | Error 0.2939(0.2709) Steps 634(621.17) | Grad Norm 12.0411(10.7967) | Total Time 0.00(0.00)\n",
      "Iter 3800 | Time 24.4437(24.1959) | Bit/dim 3.6775(3.7085) | Xent 0.7953(0.7596) | Loss 9.9113(10.5411) | Error 0.2778(0.2718) Steps 616(622.07) | Grad Norm 12.2111(11.0166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 86.2537, Epoch Time 755.0874(747.4727), Bit/dim 3.7022(best: 3.6961), Xent 0.7747, Loss 4.0896, Error 0.2713(best: 0.2738)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3810 | Time 24.1780(24.2439) | Bit/dim 3.6732(3.7069) | Xent 0.6988(0.7553) | Loss 9.5589(10.9979) | Error 0.2422(0.2689) Steps 640(624.48) | Grad Norm 8.1576(11.0210) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 23.3959(24.4539) | Bit/dim 3.7348(3.7073) | Xent 0.7346(0.7456) | Loss 9.6994(10.6682) | Error 0.2639(0.2656) Steps 616(625.37) | Grad Norm 11.1190(10.4078) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 25.2733(24.4682) | Bit/dim 3.7065(3.7037) | Xent 0.7441(0.7410) | Loss 9.8580(10.4277) | Error 0.2622(0.2648) Steps 598(625.21) | Grad Norm 9.0913(9.9492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 88.1123, Epoch Time 774.7503(748.2910), Bit/dim 3.7074(best: 3.6961), Xent 0.7810, Loss 4.0979, Error 0.2709(best: 0.2713)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3840 | Time 24.3820(24.5615) | Bit/dim 3.7166(3.7036) | Xent 0.7016(0.7378) | Loss 9.9008(10.9230) | Error 0.2428(0.2636) Steps 652(629.09) | Grad Norm 8.5122(9.6979) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 23.6496(24.5015) | Bit/dim 3.6906(3.7026) | Xent 0.7258(0.7307) | Loss 9.7892(10.6092) | Error 0.2472(0.2599) Steps 634(629.13) | Grad Norm 10.1200(9.2737) | Total Time 0.00(0.00)\n",
      "Iter 3860 | Time 25.0038(24.5094) | Bit/dim 3.7145(3.7028) | Xent 0.7531(0.7420) | Loss 9.7640(10.3929) | Error 0.2800(0.2647) Steps 592(625.39) | Grad Norm 8.2098(9.7743) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 89.3585, Epoch Time 769.6700(748.9324), Bit/dim 3.7124(best: 3.6961), Xent 0.7960, Loss 4.1104, Error 0.2788(best: 0.2709)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3870 | Time 24.1222(24.5497) | Bit/dim 3.6999(3.7049) | Xent 0.7133(0.7379) | Loss 9.7366(10.8368) | Error 0.2644(0.2632) Steps 622(624.18) | Grad Norm 11.9735(10.0799) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 24.7993(24.6581) | Bit/dim 3.7078(3.7040) | Xent 0.7340(0.7387) | Loss 9.8287(10.5555) | Error 0.2617(0.2640) Steps 634(622.04) | Grad Norm 11.0900(10.1982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 89.8394, Epoch Time 774.8247(749.7091), Bit/dim 3.7135(best: 3.6961), Xent 0.7834, Loss 4.1051, Error 0.2763(best: 0.2709)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3890 | Time 25.8093(24.6487) | Bit/dim 3.6992(3.7026) | Xent 0.6901(0.7449) | Loss 9.7128(11.1039) | Error 0.2394(0.2659) Steps 646(624.11) | Grad Norm 8.1317(10.8665) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 24.5416(24.5593) | Bit/dim 3.6962(3.7031) | Xent 0.7802(0.7449) | Loss 9.7909(10.7481) | Error 0.2883(0.2666) Steps 622(623.57) | Grad Norm 12.1002(10.9650) | Total Time 0.00(0.00)\n",
      "Iter 3910 | Time 24.4790(24.5308) | Bit/dim 3.6998(3.7017) | Xent 0.7383(0.7412) | Loss 9.8082(10.4779) | Error 0.2528(0.2637) Steps 628(622.22) | Grad Norm 7.7537(10.1455) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 89.5678, Epoch Time 766.3923(750.2096), Bit/dim 3.7010(best: 3.6961), Xent 0.7654, Loss 4.0837, Error 0.2715(best: 0.2709)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3920 | Time 23.6227(24.4944) | Bit/dim 3.7188(3.7012) | Xent 0.7064(0.7318) | Loss 9.6957(10.9554) | Error 0.2583(0.2615) Steps 586(621.76) | Grad Norm 11.0461(9.9158) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 24.4512(24.5635) | Bit/dim 3.7116(3.6999) | Xent 0.7341(0.7358) | Loss 9.7685(10.6304) | Error 0.2522(0.2630) Steps 610(619.34) | Grad Norm 15.2727(10.4836) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 24.1645(24.5276) | Bit/dim 3.7050(3.6997) | Xent 0.7226(0.7390) | Loss 9.9098(10.4049) | Error 0.2500(0.2644) Steps 652(623.47) | Grad Norm 9.9391(10.9583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 90.3031, Epoch Time 771.7747(750.8566), Bit/dim 3.7092(best: 3.6961), Xent 0.7713, Loss 4.0949, Error 0.2744(best: 0.2709)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3950 | Time 23.1517(24.4437) | Bit/dim 3.6748(3.6987) | Xent 0.7098(0.7334) | Loss 9.6715(10.8412) | Error 0.2744(0.2626) Steps 646(624.02) | Grad Norm 8.4878(10.1836) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 25.5430(24.5176) | Bit/dim 3.7154(3.6988) | Xent 0.7331(0.7289) | Loss 9.7796(10.5532) | Error 0.2572(0.2602) Steps 592(622.86) | Grad Norm 15.2531(9.8190) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 88.8093, Epoch Time 767.7177(751.3624), Bit/dim 3.6970(best: 3.6961), Xent 0.7561, Loss 4.0750, Error 0.2649(best: 0.2709)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 3970 | Time 25.9035(24.5642) | Bit/dim 3.7069(3.6985) | Xent 0.6921(0.7221) | Loss 35.8400(11.1102) | Error 0.2511(0.2580) Steps 646(625.64) | Grad Norm 6.0483(9.6399) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 24.7417(24.6867) | Bit/dim 3.7025(3.6977) | Xent 0.7240(0.7174) | Loss 9.8701(10.7543) | Error 0.2556(0.2556) Steps 652(631.99) | Grad Norm 7.4300(9.5756) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 24.1289(24.6024) | Bit/dim 3.6987(3.6978) | Xent 0.7185(0.7230) | Loss 9.8299(10.4815) | Error 0.2533(0.2580) Steps 634(628.64) | Grad Norm 12.7704(10.0973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 89.2352, Epoch Time 774.4020(752.0536), Bit/dim 3.7006(best: 3.6961), Xent 0.7616, Loss 4.0814, Error 0.2667(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4000 | Time 25.2488(24.6257) | Bit/dim 3.7163(3.6977) | Xent 0.6929(0.7203) | Loss 9.6947(10.9611) | Error 0.2561(0.2581) Steps 586(626.68) | Grad Norm 5.5296(9.3904) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 25.0095(24.6896) | Bit/dim 3.6872(3.6978) | Xent 0.6675(0.7170) | Loss 9.7348(10.6367) | Error 0.2422(0.2561) Steps 634(626.83) | Grad Norm 8.0966(9.2683) | Total Time 0.00(0.00)\n",
      "Iter 4020 | Time 23.0972(24.5647) | Bit/dim 3.6988(3.6951) | Xent 0.7158(0.7195) | Loss 9.7100(10.3976) | Error 0.2622(0.2573) Steps 634(628.97) | Grad Norm 12.7682(9.4221) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 89.2881, Epoch Time 771.7813(752.6454), Bit/dim 3.7023(best: 3.6961), Xent 0.7710, Loss 4.0878, Error 0.2717(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4030 | Time 24.0218(24.6046) | Bit/dim 3.7019(3.6954) | Xent 0.7018(0.7103) | Loss 9.7446(10.8322) | Error 0.2489(0.2542) Steps 628(627.16) | Grad Norm 16.0202(9.3945) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 24.7552(24.7067) | Bit/dim 3.7106(3.6962) | Xent 0.8021(0.7159) | Loss 9.8829(10.5557) | Error 0.2817(0.2549) Steps 646(627.73) | Grad Norm 15.2435(10.4124) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 25.6161(24.7368) | Bit/dim 3.6823(3.6973) | Xent 0.7678(0.7287) | Loss 9.7777(10.3580) | Error 0.2817(0.2603) Steps 658(630.86) | Grad Norm 10.6937(11.0552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 91.1769, Epoch Time 779.2449(753.4434), Bit/dim 3.7000(best: 3.6961), Xent 0.7915, Loss 4.0957, Error 0.2818(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4070 | Time 24.5326(24.7117) | Bit/dim 3.7141(3.6972) | Xent 0.7064(0.7212) | Loss 9.7650(10.5054) | Error 0.2417(0.2577) Steps 634(631.76) | Grad Norm 5.9297(10.1263) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 89.2294, Epoch Time 773.7792(754.0535), Bit/dim 3.7001(best: 3.6961), Xent 0.7965, Loss 4.0983, Error 0.2747(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4080 | Time 25.2910(24.7576) | Bit/dim 3.6832(3.6955) | Xent 0.6750(0.7177) | Loss 9.6715(11.0181) | Error 0.2289(0.2560) Steps 634(632.79) | Grad Norm 11.3553(9.9595) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 25.4643(24.6860) | Bit/dim 3.6700(3.6921) | Xent 0.6633(0.7093) | Loss 9.6912(10.6615) | Error 0.2422(0.2538) Steps 664(632.60) | Grad Norm 6.6716(9.4551) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 24.1259(24.7685) | Bit/dim 3.6888(3.6934) | Xent 0.7028(0.7158) | Loss 9.5952(10.4202) | Error 0.2639(0.2554) Steps 622(631.69) | Grad Norm 7.7087(10.1591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 89.4397, Epoch Time 776.7509(754.7344), Bit/dim 3.6976(best: 3.6961), Xent 0.7987, Loss 4.0969, Error 0.2763(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4110 | Time 25.3597(24.9318) | Bit/dim 3.6961(3.6935) | Xent 0.7347(0.7212) | Loss 9.7178(10.9216) | Error 0.2639(0.2578) Steps 640(632.61) | Grad Norm 10.5898(10.6086) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 24.6136(25.0335) | Bit/dim 3.7150(3.6949) | Xent 0.7415(0.7184) | Loss 9.8262(10.6168) | Error 0.2706(0.2566) Steps 640(632.30) | Grad Norm 12.3706(10.4884) | Total Time 0.00(0.00)\n",
      "Iter 4130 | Time 24.6726(25.1239) | Bit/dim 3.6956(3.6962) | Xent 0.7467(0.7262) | Loss 9.7872(10.4019) | Error 0.2689(0.2592) Steps 646(634.48) | Grad Norm 10.6784(10.9443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 89.8299, Epoch Time 793.7572(755.9051), Bit/dim 3.6920(best: 3.6961), Xent 0.7664, Loss 4.0752, Error 0.2711(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4140 | Time 25.1776(25.1479) | Bit/dim 3.6794(3.6963) | Xent 0.7156(0.7149) | Loss 9.7502(10.8124) | Error 0.2583(0.2560) Steps 628(635.93) | Grad Norm 9.2370(10.1479) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 26.0438(25.1466) | Bit/dim 3.6578(3.6959) | Xent 0.7023(0.7081) | Loss 9.6371(10.5120) | Error 0.2528(0.2532) Steps 664(636.95) | Grad Norm 8.1067(9.5456) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 89.5322, Epoch Time 788.6943(756.8888), Bit/dim 3.6937(best: 3.6920), Xent 0.8178, Loss 4.1026, Error 0.2851(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4160 | Time 25.7247(25.1780) | Bit/dim 3.6996(3.6939) | Xent 0.7219(0.7087) | Loss 9.8329(11.0341) | Error 0.2589(0.2534) Steps 658(638.71) | Grad Norm 9.4194(9.5614) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 24.7295(25.0285) | Bit/dim 3.7100(3.6929) | Xent 0.7239(0.7048) | Loss 9.8289(10.6795) | Error 0.2556(0.2518) Steps 622(636.41) | Grad Norm 14.1310(9.3828) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 25.2912(25.1302) | Bit/dim 3.6701(3.6886) | Xent 0.6886(0.7038) | Loss 9.7310(10.4159) | Error 0.2456(0.2514) Steps 616(638.19) | Grad Norm 7.2935(9.2669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 92.0000, Epoch Time 788.3027(757.8312), Bit/dim 3.6906(best: 3.6920), Xent 0.7429, Loss 4.0621, Error 0.2608(best: 0.2649)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4190 | Time 24.7959(25.1854) | Bit/dim 3.6867(3.6890) | Xent 0.6767(0.6998) | Loss 9.6537(10.9066) | Error 0.2350(0.2498) Steps 616(639.55) | Grad Norm 5.5322(9.3245) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 24.9174(25.1898) | Bit/dim 3.6865(3.6884) | Xent 0.6802(0.6954) | Loss 9.7448(10.5862) | Error 0.2439(0.2482) Steps 658(639.46) | Grad Norm 13.1088(9.1745) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 26.6356(25.2222) | Bit/dim 3.6824(3.6872) | Xent 0.6888(0.6918) | Loss 9.7508(10.3524) | Error 0.2467(0.2457) Steps 640(642.07) | Grad Norm 11.4265(9.1574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 90.2160, Epoch Time 787.9483(758.7347), Bit/dim 3.6854(best: 3.6906), Xent 0.7378, Loss 4.0543, Error 0.2592(best: 0.2608)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4220 | Time 25.1853(25.2669) | Bit/dim 3.6486(3.6865) | Xent 0.6363(0.6874) | Loss 9.5566(10.8021) | Error 0.2278(0.2449) Steps 646(642.48) | Grad Norm 9.7987(9.2506) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 26.2589(25.2324) | Bit/dim 3.7135(3.6875) | Xent 0.7050(0.6947) | Loss 9.7274(10.5197) | Error 0.2428(0.2480) Steps 634(641.47) | Grad Norm 8.8149(9.8478) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 91.2274, Epoch Time 792.4840(759.7472), Bit/dim 3.6933(best: 3.6854), Xent 0.7755, Loss 4.0811, Error 0.2682(best: 0.2592)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4240 | Time 25.4537(25.2509) | Bit/dim 3.7052(3.6861) | Xent 0.6680(0.6894) | Loss 35.9664(11.0928) | Error 0.2383(0.2451) Steps 628(638.87) | Grad Norm 9.9408(9.9453) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 24.6795(25.2617) | Bit/dim 3.6662(3.6851) | Xent 0.7381(0.6926) | Loss 9.6656(10.7358) | Error 0.2633(0.2464) Steps 652(642.65) | Grad Norm 15.1580(9.8538) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 25.8764(25.1905) | Bit/dim 3.7029(3.6873) | Xent 0.6715(0.6908) | Loss 9.6833(10.4567) | Error 0.2494(0.2455) Steps 658(643.75) | Grad Norm 6.8590(9.6768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 92.0162, Epoch Time 792.0327(760.7157), Bit/dim 3.6876(best: 3.6854), Xent 0.7368, Loss 4.0561, Error 0.2568(best: 0.2592)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4270 | Time 25.2826(25.3854) | Bit/dim 3.6941(3.6872) | Xent 0.6304(0.6832) | Loss 9.6434(10.9737) | Error 0.2322(0.2433) Steps 682(648.40) | Grad Norm 5.4682(9.3230) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 25.5476(25.4265) | Bit/dim 3.6879(3.6862) | Xent 0.6557(0.6796) | Loss 9.7004(10.6357) | Error 0.2278(0.2421) Steps 640(648.81) | Grad Norm 13.6268(9.3007) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 24.3164(25.3969) | Bit/dim 3.6891(3.6886) | Xent 0.7144(0.6811) | Loss 9.6852(10.3832) | Error 0.2572(0.2445) Steps 652(646.95) | Grad Norm 9.5304(9.4075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 91.0495, Epoch Time 797.2564(761.8120), Bit/dim 3.6913(best: 3.6854), Xent 0.8191, Loss 4.1008, Error 0.2856(best: 0.2568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4300 | Time 27.0946(25.3817) | Bit/dim 3.6731(3.6866) | Xent 0.7546(0.6975) | Loss 9.8373(10.8574) | Error 0.2756(0.2501) Steps 670(649.29) | Grad Norm 12.1636(10.9186) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 24.8401(25.3706) | Bit/dim 3.6748(3.6876) | Xent 0.6781(0.6995) | Loss 9.6547(10.5613) | Error 0.2378(0.2506) Steps 634(647.37) | Grad Norm 5.5824(10.6593) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 26.1809(25.4106) | Bit/dim 3.6770(3.6880) | Xent 0.7189(0.6955) | Loss 9.7458(10.3308) | Error 0.2644(0.2496) Steps 652(646.88) | Grad Norm 7.0925(9.6485) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 89.9896, Epoch Time 794.5537(762.7942), Bit/dim 3.6809(best: 3.6854), Xent 0.7429, Loss 4.0524, Error 0.2616(best: 0.2568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4330 | Time 24.3341(25.4952) | Bit/dim 3.6985(3.6862) | Xent 0.6772(0.6845) | Loss 9.6083(10.7275) | Error 0.2311(0.2440) Steps 658(647.89) | Grad Norm 5.9081(8.9386) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 25.7279(25.5008) | Bit/dim 3.6761(3.6858) | Xent 0.6857(0.6855) | Loss 9.7271(10.4573) | Error 0.2422(0.2450) Steps 658(650.41) | Grad Norm 11.9034(9.7849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 91.0903, Epoch Time 801.6723(763.9606), Bit/dim 3.6928(best: 3.6809), Xent 0.7802, Loss 4.0829, Error 0.2707(best: 0.2568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4350 | Time 25.6595(25.4991) | Bit/dim 3.6728(3.6872) | Xent 0.6883(0.6876) | Loss 9.7371(10.9784) | Error 0.2517(0.2454) Steps 694(651.01) | Grad Norm 12.1640(10.0976) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 27.3785(25.4914) | Bit/dim 3.6770(3.6869) | Xent 0.6977(0.6932) | Loss 9.5504(10.6399) | Error 0.2567(0.2484) Steps 628(650.30) | Grad Norm 12.0434(10.2907) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 26.0746(25.5533) | Bit/dim 3.6936(3.6879) | Xent 0.6584(0.6884) | Loss 9.6480(10.3863) | Error 0.2306(0.2463) Steps 652(651.68) | Grad Norm 8.3542(9.8703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 92.8057, Epoch Time 800.6479(765.0612), Bit/dim 3.6848(best: 3.6809), Xent 0.7197, Loss 4.0446, Error 0.2482(best: 0.2568)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4380 | Time 26.2360(25.6699) | Bit/dim 3.6758(3.6864) | Xent 0.6946(0.6812) | Loss 9.5891(10.8707) | Error 0.2478(0.2427) Steps 664(654.79) | Grad Norm 13.5856(9.7519) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 25.5731(25.7278) | Bit/dim 3.6885(3.6857) | Xent 0.6589(0.6831) | Loss 9.7436(10.5648) | Error 0.2328(0.2449) Steps 652(656.40) | Grad Norm 8.4528(9.8870) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 25.4660(25.6621) | Bit/dim 3.6726(3.6857) | Xent 0.6455(0.6775) | Loss 9.6964(10.3308) | Error 0.2328(0.2425) Steps 676(657.02) | Grad Norm 9.5380(9.8275) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 93.0344, Epoch Time 807.4811(766.3338), Bit/dim 3.6788(best: 3.6809), Xent 0.7261, Loss 4.0418, Error 0.2516(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4410 | Time 25.7487(25.7611) | Bit/dim 3.6438(3.6844) | Xent 0.6728(0.6697) | Loss 9.6420(10.7753) | Error 0.2517(0.2402) Steps 688(657.76) | Grad Norm 10.4178(9.2217) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 26.2361(25.7991) | Bit/dim 3.6659(3.6836) | Xent 0.6989(0.6675) | Loss 9.7563(10.4814) | Error 0.2528(0.2395) Steps 628(654.40) | Grad Norm 9.2438(8.7749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 89.5024, Epoch Time 804.3222(767.4734), Bit/dim 3.6807(best: 3.6788), Xent 0.7340, Loss 4.0477, Error 0.2564(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4430 | Time 23.8326(25.6429) | Bit/dim 3.6647(3.6810) | Xent 0.6369(0.6612) | Loss 9.6055(11.0176) | Error 0.2322(0.2369) Steps 646(655.25) | Grad Norm 6.4688(8.3462) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 26.0371(25.6719) | Bit/dim 3.6869(3.6816) | Xent 0.6871(0.6613) | Loss 9.5350(10.6534) | Error 0.2500(0.2374) Steps 610(652.52) | Grad Norm 18.9243(9.2765) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 24.9696(25.7265) | Bit/dim 3.6917(3.6811) | Xent 0.6814(0.6667) | Loss 9.7421(10.3944) | Error 0.2428(0.2397) Steps 658(655.04) | Grad Norm 11.6382(9.9538) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 92.0202, Epoch Time 804.8103(768.5935), Bit/dim 3.6789(best: 3.6788), Xent 0.7377, Loss 4.0478, Error 0.2554(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4460 | Time 23.5903(25.6826) | Bit/dim 3.6909(3.6806) | Xent 0.6075(0.6610) | Loss 9.4190(10.8816) | Error 0.2222(0.2372) Steps 658(657.02) | Grad Norm 7.3286(9.0743) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 27.0153(25.7822) | Bit/dim 3.6685(3.6789) | Xent 0.6378(0.6553) | Loss 9.5690(10.5521) | Error 0.2378(0.2350) Steps 646(657.25) | Grad Norm 6.9179(8.9150) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 24.6838(25.7427) | Bit/dim 3.6711(3.6783) | Xent 0.6466(0.6602) | Loss 9.5486(10.3217) | Error 0.2322(0.2353) Steps 640(657.62) | Grad Norm 7.1427(9.4409) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 91.7101, Epoch Time 803.0003(769.6257), Bit/dim 3.6788(best: 3.6788), Xent 0.7265, Loss 4.0420, Error 0.2540(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4490 | Time 26.8418(25.8456) | Bit/dim 3.6999(3.6796) | Xent 0.7934(0.6768) | Loss 9.8993(10.7943) | Error 0.2850(0.2411) Steps 664(660.03) | Grad Norm 12.5419(10.6326) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 27.0248(25.8748) | Bit/dim 3.6829(3.6811) | Xent 0.6430(0.6800) | Loss 9.6832(10.4985) | Error 0.2367(0.2431) Steps 700(658.83) | Grad Norm 8.9559(10.3880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 93.4069, Epoch Time 812.2873(770.9056), Bit/dim 3.6795(best: 3.6788), Xent 0.7277, Loss 4.0433, Error 0.2520(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4510 | Time 24.6808(25.8206) | Bit/dim 3.6976(3.6818) | Xent 0.6430(0.6759) | Loss 36.0327(11.0775) | Error 0.2244(0.2412) Steps 658(662.10) | Grad Norm 5.0823(9.3744) | Total Time 0.00(0.00)\n",
      "Iter 4520 | Time 26.0447(25.8815) | Bit/dim 3.6700(3.6806) | Xent 0.6956(0.6727) | Loss 9.7249(10.7100) | Error 0.2456(0.2398) Steps 664(661.90) | Grad Norm 16.5599(9.6430) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 26.0991(25.9853) | Bit/dim 3.6862(3.6810) | Xent 0.6946(0.6663) | Loss 9.7908(10.4329) | Error 0.2500(0.2380) Steps 658(658.27) | Grad Norm 12.4867(9.6245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 91.8593, Epoch Time 812.1827(772.1439), Bit/dim 3.6788(best: 3.6788), Xent 0.7290, Loss 4.0433, Error 0.2549(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4540 | Time 25.8828(25.9163) | Bit/dim 3.6622(3.6792) | Xent 0.6655(0.6649) | Loss 9.5470(10.9180) | Error 0.2217(0.2376) Steps 640(659.19) | Grad Norm 12.7984(9.8845) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 24.2601(25.6818) | Bit/dim 3.6739(3.6799) | Xent 0.6113(0.6631) | Loss 9.5318(10.5798) | Error 0.2111(0.2365) Steps 628(657.94) | Grad Norm 8.4818(10.0734) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 25.5506(25.7121) | Bit/dim 3.6741(3.6800) | Xent 0.6436(0.6605) | Loss 9.5820(10.3373) | Error 0.2372(0.2364) Steps 658(658.44) | Grad Norm 9.5176(10.3382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 93.6159, Epoch Time 798.9625(772.9485), Bit/dim 3.6817(best: 3.6788), Xent 0.7205, Loss 4.0420, Error 0.2506(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4570 | Time 24.9727(25.7104) | Bit/dim 3.6679(3.6811) | Xent 0.6392(0.6605) | Loss 9.5556(10.8205) | Error 0.2200(0.2355) Steps 664(663.11) | Grad Norm 6.9051(10.1094) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 26.9600(25.7876) | Bit/dim 3.6993(3.6800) | Xent 0.6938(0.6617) | Loss 9.7406(10.5156) | Error 0.2539(0.2355) Steps 652(662.47) | Grad Norm 9.9489(9.8559) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 26.0283(25.7080) | Bit/dim 3.6596(3.6778) | Xent 0.6369(0.6565) | Loss 9.6698(10.2873) | Error 0.2250(0.2340) Steps 694(664.53) | Grad Norm 10.0115(9.4382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 92.2415, Epoch Time 805.4291(773.9229), Bit/dim 3.6779(best: 3.6788), Xent 0.7353, Loss 4.0456, Error 0.2571(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4600 | Time 26.9650(25.7764) | Bit/dim 3.6743(3.6793) | Xent 0.7132(0.6498) | Loss 9.7286(10.7141) | Error 0.2572(0.2313) Steps 670(667.59) | Grad Norm 16.8102(9.4459) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 25.6388(25.6560) | Bit/dim 3.6596(3.6814) | Xent 0.6635(0.6576) | Loss 9.6172(10.4545) | Error 0.2350(0.2352) Steps 664(666.39) | Grad Norm 7.9070(10.0837) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 92.5346, Epoch Time 807.5047(774.9303), Bit/dim 3.6799(best: 3.6779), Xent 0.7541, Loss 4.0570, Error 0.2643(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4620 | Time 26.3833(25.8243) | Bit/dim 3.6623(3.6782) | Xent 0.6244(0.6560) | Loss 9.6521(10.9903) | Error 0.2233(0.2344) Steps 676(665.94) | Grad Norm 6.3450(9.4570) | Total Time 0.00(0.00)\n",
      "Iter 4630 | Time 26.1404(25.9305) | Bit/dim 3.6731(3.6766) | Xent 0.6313(0.6480) | Loss 9.6966(10.6308) | Error 0.2161(0.2309) Steps 694(670.62) | Grad Norm 9.3498(9.1754) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 25.1822(25.9443) | Bit/dim 3.6677(3.6753) | Xent 0.5859(0.6474) | Loss 9.5547(10.3627) | Error 0.2094(0.2319) Steps 688(669.60) | Grad Norm 5.2923(9.6284) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 91.5309, Epoch Time 813.8654(776.0984), Bit/dim 3.6753(best: 3.6779), Xent 0.7523, Loss 4.0515, Error 0.2589(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4650 | Time 26.2714(25.9071) | Bit/dim 3.6554(3.6746) | Xent 0.6621(0.6482) | Loss 9.5229(10.8442) | Error 0.2367(0.2326) Steps 658(667.45) | Grad Norm 12.2089(9.5686) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 26.0653(25.8921) | Bit/dim 3.6762(3.6755) | Xent 0.6026(0.6430) | Loss 9.5449(10.5246) | Error 0.2111(0.2297) Steps 664(670.21) | Grad Norm 6.2222(9.4233) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 27.1788(26.0194) | Bit/dim 3.6650(3.6735) | Xent 0.6180(0.6451) | Loss 9.5591(10.2950) | Error 0.2289(0.2306) Steps 658(669.71) | Grad Norm 7.4929(9.3645) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 93.9197, Epoch Time 813.0059(777.2056), Bit/dim 3.6773(best: 3.6753), Xent 0.7293, Loss 4.0420, Error 0.2522(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4680 | Time 24.5450(26.0362) | Bit/dim 3.6784(3.6737) | Xent 0.6501(0.6460) | Loss 9.5856(10.7429) | Error 0.2389(0.2312) Steps 670(671.84) | Grad Norm 18.7516(9.9917) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 26.0234(26.0479) | Bit/dim 3.6837(3.6746) | Xent 0.6808(0.6469) | Loss 9.7397(10.4608) | Error 0.2489(0.2321) Steps 688(673.78) | Grad Norm 12.3473(10.4652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 91.6819, Epoch Time 812.0846(778.2520), Bit/dim 3.6744(best: 3.6753), Xent 0.7746, Loss 4.0617, Error 0.2643(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4700 | Time 26.2122(26.0680) | Bit/dim 3.6974(3.6752) | Xent 0.6013(0.6446) | Loss 9.6840(11.0178) | Error 0.2161(0.2313) Steps 652(673.42) | Grad Norm 7.7643(10.4260) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 25.5609(26.1098) | Bit/dim 3.6776(3.6757) | Xent 0.6658(0.6502) | Loss 9.7982(10.6660) | Error 0.2272(0.2328) Steps 694(674.93) | Grad Norm 12.3576(10.9057) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 25.8626(26.0227) | Bit/dim 3.6757(3.6759) | Xent 0.6616(0.6520) | Loss 9.7275(10.4075) | Error 0.2417(0.2333) Steps 658(676.50) | Grad Norm 7.4973(10.3095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 93.2608, Epoch Time 814.2349(779.3315), Bit/dim 3.6764(best: 3.6744), Xent 0.7380, Loss 4.0454, Error 0.2598(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4730 | Time 25.5193(25.9199) | Bit/dim 3.6842(3.6775) | Xent 0.6231(0.6443) | Loss 9.6998(10.9012) | Error 0.2250(0.2300) Steps 694(678.65) | Grad Norm 11.0116(10.0658) | Total Time 0.00(0.00)\n",
      "Iter 4740 | Time 25.1284(25.9551) | Bit/dim 3.6585(3.6753) | Xent 0.6098(0.6404) | Loss 9.6215(10.5628) | Error 0.2122(0.2289) Steps 682(678.53) | Grad Norm 6.2886(9.6788) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 24.3900(25.9846) | Bit/dim 3.6604(3.6729) | Xent 0.6257(0.6414) | Loss 9.4977(10.3210) | Error 0.2344(0.2290) Steps 688(680.12) | Grad Norm 11.9424(9.8227) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 92.2867, Epoch Time 811.9640(780.3104), Bit/dim 3.6755(best: 3.6744), Xent 0.7460, Loss 4.0485, Error 0.2578(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4760 | Time 26.4229(26.0725) | Bit/dim 3.6746(3.6734) | Xent 0.6563(0.6342) | Loss 9.5994(10.7782) | Error 0.2256(0.2266) Steps 694(679.37) | Grad Norm 6.1058(9.0978) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 27.7737(26.3469) | Bit/dim 3.6660(3.6727) | Xent 0.6587(0.6353) | Loss 9.7037(10.4790) | Error 0.2422(0.2274) Steps 682(679.72) | Grad Norm 17.5082(9.8146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 93.0895, Epoch Time 831.7648(781.8541), Bit/dim 3.6830(best: 3.6744), Xent 0.7523, Loss 4.0592, Error 0.2557(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4780 | Time 26.2711(26.4455) | Bit/dim 3.6790(3.6708) | Xent 0.6313(0.6410) | Loss 36.1977(11.0582) | Error 0.2217(0.2293) Steps 700(681.79) | Grad Norm 11.5871(9.9848) | Total Time 0.00(0.00)\n",
      "Iter 4790 | Time 26.9019(26.4030) | Bit/dim 3.6777(3.6731) | Xent 0.6367(0.6384) | Loss 9.7023(10.6945) | Error 0.2256(0.2283) Steps 694(684.09) | Grad Norm 7.8763(10.0128) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 25.8325(26.2779) | Bit/dim 3.6853(3.6712) | Xent 0.6023(0.6310) | Loss 9.6844(10.4094) | Error 0.2128(0.2264) Steps 700(683.47) | Grad Norm 9.1929(9.5709) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 93.1763, Epoch Time 816.7838(782.9020), Bit/dim 3.6702(best: 3.6744), Xent 0.7482, Loss 4.0443, Error 0.2568(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4810 | Time 26.2100(26.2769) | Bit/dim 3.6575(3.6680) | Xent 0.6275(0.6270) | Loss 9.6429(10.9238) | Error 0.2194(0.2245) Steps 676(681.14) | Grad Norm 7.0004(9.2845) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 26.8966(26.3105) | Bit/dim 3.6809(3.6701) | Xent 0.6455(0.6353) | Loss 9.7310(10.5916) | Error 0.2206(0.2270) Steps 694(681.16) | Grad Norm 9.4892(10.4269) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 25.4774(26.1575) | Bit/dim 3.6573(3.6704) | Xent 0.6182(0.6363) | Loss 9.6380(10.3478) | Error 0.2183(0.2279) Steps 694(681.38) | Grad Norm 6.8207(10.1394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 93.6331, Epoch Time 817.8599(783.9507), Bit/dim 3.6681(best: 3.6702), Xent 0.7209, Loss 4.0286, Error 0.2487(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4850 | Time 24.7603(26.1603) | Bit/dim 3.6878(3.6664) | Xent 0.5976(0.6238) | Loss 9.6476(10.4829) | Error 0.2200(0.2233) Steps 688(685.07) | Grad Norm 7.7366(8.7957) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 25.9568(26.2556) | Bit/dim 3.6619(3.6673) | Xent 0.7132(0.6283) | Loss 9.7082(10.2663) | Error 0.2494(0.2244) Steps 724(689.58) | Grad Norm 11.8979(8.9560) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 94.0187, Epoch Time 820.9374(785.0603), Bit/dim 3.6742(best: 3.6681), Xent 0.7747, Loss 4.0616, Error 0.2682(best: 0.2482)\n",
      "===> Using batch size 1800. Total 27 iterations/epoch.\n",
      "Iter 4870 | Time 25.9920(26.3652) | Bit/dim 3.6560(3.6676) | Xent 0.6185(0.6345) | Loss 9.4975(10.6807) | Error 0.2094(0.2261) Steps 682(688.73) | Grad Norm 7.1833(9.6972) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 1800 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs1800_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
