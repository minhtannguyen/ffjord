{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_conditional_exploretol2.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.odenvp_conditional_exploretol2 as odenvp\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=True, choices=[True, False])\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__))\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"./data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"./data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"./data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"./data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"./data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            './data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "\n",
      "    return bits_per_dim, loss_xent\n",
      "\n",
      "# def compute_bits_per_dim_and_xent(x, y, model, size_cond=10):\n",
      "#     zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "#     # Don't use data parallelize if batch size is small.\n",
      "#     # if x.shape[0] < 200:\n",
      "#     #     model = model.module\n",
      "    \n",
      "#     z, delta_logp = model(x, zero)  # run model forward\n",
      "#     z_noise, z_cond = z[:,:-size_cond], z[:,-size_cond:] # split z into z for noise and z for conditional signal\n",
      "    \n",
      "#     # compute bits_per_dim\n",
      "#     logpz = standard_normal_logprob(z_noise).view(z_noise.shape[0], -1).sum(1, keepdim=True)  # logp(z_noise)\n",
      "#     logpx = logpz - delta_logp\n",
      "#     logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "#     bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "#     # compute xentropy loss\n",
      "#     L_xent = torch.nn.CrossEntropyLoss()\n",
      "#     loss_xent = L_xent(z_cond, y.to(x.get_device()))\n",
      "\n",
      "#     return bits_per_dim, loss_xent\n",
      "\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    fixed_y = torch.randint(high=10, size=(100,)).type(torch.long).to(device, non_blocking=True)\n",
      "    fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "    with torch.no_grad():\n",
      "        mean, logs = model.module._prior(fixed_y_onehot)\n",
      "        fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    \n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss = float(\"inf\")\n",
      "    itr = 0\n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent = compute_bits_per_dim_conditional(x, y, model)\n",
      "                # loss =  loss_nll + args.weight_y * loss_xent\n",
      "                loss =  loss_nll\n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            if regularization_coeffs:\n",
      "                loss_track = loss_nll + reg_loss + total_time * args.time_penalty\n",
      "            else:\n",
      "                loss_track = loss_nll + total_time * args.time_penalty\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss_track.item())\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, loss_meter.val, loss_meter.avg, steps_meter.val,\n",
      "                        steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "\n",
      "            itr += 1\n",
      "\n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        # loss =  loss_nll + args.weight_y * loss_xent\n",
      "                    else:\n",
      "                        loss_nll = compute_bits_per_dim(x, model)\n",
      "                    losses.append(loss_nll.cpu().numpy())\n",
      "                \n",
      "                loss = np.mean(losses)\n",
      "                logger.info(\"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}, Bit/dim {:.4f}\".format(epoch, time.time() - start, time.time() - start_epoch, loss))\n",
      "                \n",
      "                if loss < best_loss:\n",
      "                    best_loss = loss\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                    }, os.path.join(args.save, \"checkpt.pth\"))\n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='experiments/cnf_cond_bs8K_100timeslr_exploretol2', solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=8000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1568, bias=True)\n",
      "  (project_class): LinearZeros(in_features=784, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 828890\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0000 | Time 64.1990(64.1990) | Bit/dim 119532.3750(119532.3750) | Steps 350(350.00) | Grad Norm 1472509.8891(1472509.8891) | Total Time 10.00(10.00)\n",
      "Iter 0001 | Time 26.9327(63.0810) | Bit/dim 94479.8672(118780.7998) | Steps 356(350.18) | Grad Norm 1231038.9809(1465265.7618) | Total Time 10.00(10.00)\n",
      "Iter 0002 | Time 25.5327(61.9546) | Bit/dim 58923.4844(116985.0803) | Steps 368(350.71) | Grad Norm 800910.2939(1445335.0978) | Total Time 10.00(10.00)\n",
      "Iter 0003 | Time 26.7134(60.8973) | Bit/dim 35622.1523(114544.1925) | Steps 392(351.95) | Grad Norm 316184.6111(1411460.5832) | Total Time 10.00(10.00)\n",
      "Iter 0004 | Time 28.5725(59.9276) | Bit/dim 38261.4453(112255.7101) | Steps 404(353.51) | Grad Norm 512525.1906(1384492.5214) | Total Time 10.00(10.00)\n",
      "Iter 0005 | Time 27.0018(58.9398) | Bit/dim 45917.0781(110265.5511) | Steps 404(355.03) | Grad Norm 808192.2335(1367203.5128) | Total Time 10.00(10.00)\n",
      "Iter 0006 | Time 26.8443(57.9769) | Bit/dim 38615.7891(108116.0582) | Steps 404(356.50) | Grad Norm 771158.9742(1349322.1766) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 8.7354, Epoch Time 246.9662, Bit/dim 18782.8418\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0007 | Time 28.1730(57.0828) | Bit/dim 19632.7363(105461.5586) | Steps 404(357.92) | Grad Norm 538839.9916(1325007.7111) | Total Time 10.00(10.00)\n",
      "Iter 0008 | Time 27.0687(56.1824) | Bit/dim 3130.8518(102391.6374) | Steps 404(359.31) | Grad Norm 298408.0598(1294209.7215) | Total Time 10.00(10.00)\n",
      "Iter 0009 | Time 25.9768(55.2762) | Bit/dim -3711.1541(99208.5536) | Steps 380(359.93) | Grad Norm 205872.8402(1261559.6151) | Total Time 10.00(10.00)\n",
      "Iter 0010 | Time 26.5566(54.4146) | Bit/dim -5515.7173(96066.8255) | Steps 380(360.53) | Grad Norm 253911.5537(1231330.1732) | Total Time 10.00(10.00)\n",
      "Iter 0011 | Time 25.8599(53.5580) | Bit/dim -8079.1187(92942.4472) | Steps 380(361.11) | Grad Norm 283148.4598(1202884.7218) | Total Time 10.00(10.00)\n",
      "Iter 0012 | Time 26.6854(52.7518) | Bit/dim -14177.6396(89728.8446) | Steps 392(362.04) | Grad Norm 257071.9336(1174510.3382) | Total Time 10.00(10.00)\n",
      "Iter 0013 | Time 26.6842(51.9698) | Bit/dim -22387.0781(86365.3669) | Steps 410(363.48) | Grad Norm 196491.7541(1145169.7807) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 8.8210, Epoch Time 208.3084, Bit/dim -30306.9785\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0014 | Time 27.1038(51.2238) | Bit/dim -30073.4648(82872.2019) | Steps 404(364.69) | Grad Norm 133833.8813(1114829.7037) | Total Time 10.00(10.00)\n",
      "Iter 0015 | Time 26.8610(50.4929) | Bit/dim -35361.4141(79325.1935) | Steps 398(365.69) | Grad Norm 91090.3612(1084117.5234) | Total Time 10.00(10.00)\n",
      "Iter 0016 | Time 26.8774(49.7845) | Bit/dim -38490.8438(75790.7123) | Steps 392(366.48) | Grad Norm 67604.1666(1053622.1227) | Total Time 10.00(10.00)\n",
      "Iter 0017 | Time 27.9281(49.1288) | Bit/dim -39751.7070(72324.4398) | Steps 404(367.61) | Grad Norm 58392.2092(1023765.2253) | Total Time 10.00(10.00)\n",
      "Iter 0018 | Time 27.7758(48.4882) | Bit/dim -39906.8281(68957.5017) | Steps 404(368.70) | Grad Norm 67578.7596(995079.6313) | Total Time 10.00(10.00)\n",
      "Iter 0019 | Time 29.7211(47.9252) | Bit/dim -39543.2539(65702.4791) | Steps 422(370.30) | Grad Norm 81871.4249(967683.3851) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 31.5827(47.4349) | Bit/dim -39480.8555(62546.9790) | Steps 428(372.03) | Grad Norm 84785.1134(941196.4370) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 9.2427, Epoch Time 219.3923, Bit/dim -40106.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0021 | Time 31.1003(46.9449) | Bit/dim -39971.0977(59471.4367) | Steps 434(373.89) | Grad Norm 73721.6919(915172.1946) | Total Time 10.00(10.00)\n",
      "Iter 0022 | Time 32.2341(46.5035) | Bit/dim -40701.9844(56466.2341) | Steps 440(375.87) | Grad Norm 55446.2181(889380.4153) | Total Time 10.00(10.00)\n",
      "Iter 0023 | Time 34.8517(46.1540) | Bit/dim -41008.0625(53542.0052) | Steps 464(378.52) | Grad Norm 40536.9338(863915.1109) | Total Time 10.00(10.00)\n",
      "Iter 0024 | Time 35.3855(45.8309) | Bit/dim -40857.0508(50710.0335) | Steps 464(381.08) | Grad Norm 37476.8646(839121.9635) | Total Time 10.00(10.00)\n",
      "Iter 0025 | Time 34.7060(45.4972) | Bit/dim -40393.7539(47976.9199) | Steps 464(383.57) | Grad Norm 42788.6031(815231.9627) | Total Time 10.00(10.00)\n",
      "Iter 0026 | Time 37.0045(45.2424) | Bit/dim -40019.3867(45337.0307) | Steps 470(386.16) | Grad Norm 46889.8506(792181.6993) | Total Time 10.00(10.00)\n",
      "Iter 0027 | Time 34.8625(44.9310) | Bit/dim -39963.0625(42778.0279) | Steps 470(388.68) | Grad Norm 46123.6210(769799.9570) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 9.6242, Epoch Time 262.1948, Bit/dim -40335.1680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0028 | Time 35.1559(44.6378) | Bit/dim -40191.8438(40288.9317) | Steps 470(391.12) | Grad Norm 41616.7373(747954.4604) | Total Time 10.00(10.00)\n",
      "Iter 0029 | Time 33.0884(44.2913) | Bit/dim -40741.1641(37858.0289) | Steps 458(393.12) | Grad Norm 36163.8735(726600.7428) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 34.2868(43.9911) | Bit/dim -41342.8086(35482.0037) | Steps 458(395.07) | Grad Norm 32418.5058(705775.2757) | Total Time 10.00(10.00)\n",
      "Iter 0031 | Time 33.7776(43.6847) | Bit/dim -42034.5742(33156.5064) | Steps 452(396.78) | Grad Norm 30838.0010(685527.1574) | Total Time 10.00(10.00)\n",
      "Iter 0032 | Time 32.7413(43.3564) | Bit/dim -42745.1289(30879.4573) | Steps 452(398.43) | Grad Norm 29628.4698(665850.1968) | Total Time 10.00(10.00)\n",
      "Iter 0033 | Time 33.3523(43.0563) | Bit/dim -43568.2891(28646.0250) | Steps 440(399.68) | Grad Norm 26804.9418(646678.8392) | Total Time 10.00(10.00)\n",
      "Iter 0034 | Time 31.7507(42.7171) | Bit/dim -44297.2344(26457.7272) | Steps 440(400.89) | Grad Norm 22554.2956(627955.1028) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 8.8521, Epoch Time 255.4355, Bit/dim -45016.2344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0035 | Time 31.7640(42.3885) | Bit/dim -44891.5898(24317.2477) | Steps 428(401.70) | Grad Norm 18291.6486(609665.1992) | Total Time 10.00(10.00)\n",
      "Iter 0036 | Time 28.9492(41.9854) | Bit/dim -45214.1758(22231.3050) | Steps 416(402.13) | Grad Norm 15907.7896(591852.4769) | Total Time 10.00(10.00)\n",
      "Iter 0037 | Time 27.6154(41.5543) | Bit/dim -45405.2461(20202.2084) | Steps 410(402.37) | Grad Norm 15692.2087(574567.6689) | Total Time 10.00(10.00)\n",
      "Iter 0038 | Time 27.1076(41.1209) | Bit/dim -45542.5977(18229.8642) | Steps 416(402.78) | Grad Norm 16150.5851(557815.1564) | Total Time 10.00(10.00)\n",
      "Iter 0039 | Time 27.9177(40.7248) | Bit/dim -45643.9414(16313.6501) | Steps 404(402.81) | Grad Norm 16065.8949(541562.6785) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 28.1151(40.3465) | Bit/dim -45691.2578(14453.5028) | Steps 404(402.85) | Grad Norm 16308.4419(525805.0514) | Total Time 10.00(10.00)\n",
      "Iter 0041 | Time 26.3939(39.9279) | Bit/dim -45861.1602(12644.0630) | Steps 398(402.70) | Grad Norm 18530.3927(510586.8117) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 8.0467, Epoch Time 218.5669, Bit/dim -46113.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0042 | Time 26.1824(39.5155) | Bit/dim -45983.4531(10885.2375) | Steps 392(402.38) | Grad Norm 21539.2055(495915.3835) | Total Time 10.00(10.00)\n",
      "Iter 0043 | Time 26.0684(39.1121) | Bit/dim -46249.7617(9171.1875) | Steps 386(401.89) | Grad Norm 19822.8760(481632.6083) | Total Time 10.00(10.00)\n",
      "Iter 0044 | Time 26.1070(38.7220) | Bit/dim -46582.6172(7498.5734) | Steps 386(401.41) | Grad Norm 13819.7617(467598.2229) | Total Time 10.00(10.00)\n",
      "Iter 0045 | Time 27.0677(38.3723) | Bit/dim -46857.8203(5867.8815) | Steps 386(400.95) | Grad Norm 8304.0981(453819.3991) | Total Time 10.00(10.00)\n",
      "Iter 0046 | Time 27.1964(38.0371) | Bit/dim -47003.1328(4281.7511) | Steps 392(400.68) | Grad Norm 7851.1452(440440.3515) | Total Time 10.00(10.00)\n",
      "Iter 0047 | Time 27.5619(37.7228) | Bit/dim -47020.7344(2742.6765) | Steps 398(400.60) | Grad Norm 9715.2601(427518.5988) | Total Time 10.00(10.00)\n",
      "Iter 0048 | Time 29.1188(37.4647) | Bit/dim -47098.0625(1247.4544) | Steps 398(400.52) | Grad Norm 10824.6109(415017.7791) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 8.4269, Epoch Time 209.9798, Bit/dim -47275.0312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generatedo _generatedo _generate\n",
      "\n",
      "do _generate\n",
      "\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0049 | Time 28.1169(37.1843) | Bit/dim -47135.6211(-204.0379) | Steps 398(400.45) | Grad Norm 11404.6989(402909.3867) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 28.6504(36.9282) | Bit/dim -47248.5508(-1615.3733) | Steps 392(400.20) | Grad Norm 11797.7476(391176.0375) | Total Time 10.00(10.00)\n",
      "Iter 0051 | Time 27.3697(36.6415) | Bit/dim -47439.5547(-2990.0987) | Steps 392(399.95) | Grad Norm 11466.7076(379784.7576) | Total Time 10.00(10.00)\n",
      "Iter 0052 | Time 28.0178(36.3828) | Bit/dim -47566.8789(-4327.4021) | Steps 386(399.53) | Grad Norm 9631.9208(368680.1725) | Total Time 10.00(10.00)\n",
      "Iter 0053 | Time 26.8445(36.0966) | Bit/dim -47736.6719(-5629.6802) | Steps 386(399.13) | Grad Norm 6961.0567(357828.5991) | Total Time 10.00(10.00)\n",
      "Iter 0054 | Time 26.7817(35.8172) | Bit/dim -47835.6016(-6895.8579) | Steps 386(398.73) | Grad Norm 6546.5353(347290.1372) | Total Time 10.00(10.00)\n",
      "Iter 0055 | Time 27.9596(35.5814) | Bit/dim -47921.5547(-8126.6288) | Steps 392(398.53) | Grad Norm 8569.2246(337128.5098) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 8.3299, Epoch Time 214.3537, Bit/dim -48104.1289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0056 | Time 26.7351(35.3161) | Bit/dim -48023.0586(-9323.5217) | Steps 392(398.33) | Grad Norm 8863.5640(327280.5614) | Total Time 10.00(10.00)\n",
      "Iter 0057 | Time 26.9266(35.0644) | Bit/dim -48020.5781(-10484.4333) | Steps 392(398.14) | Grad Norm 7517.2538(317687.6622) | Total Time 10.00(10.00)\n",
      "Iter 0058 | Time 26.2894(34.8011) | Bit/dim -48105.5781(-11613.0677) | Steps 386(397.78) | Grad Norm 7597.9935(308384.9721) | Total Time 10.00(10.00)\n",
      "Iter 0059 | Time 26.2591(34.5449) | Bit/dim -48199.1289(-12710.6495) | Steps 386(397.43) | Grad Norm 8528.1937(299389.2688) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 26.7947(34.3124) | Bit/dim -48244.0156(-13776.6505) | Steps 392(397.26) | Grad Norm 7571.2361(290634.7278) | Total Time 10.00(10.00)\n",
      "Iter 0061 | Time 27.2697(34.1011) | Bit/dim -48352.1484(-14813.9154) | Steps 392(397.11) | Grad Norm 5190.4845(282071.4005) | Total Time 10.00(10.00)\n",
      "Iter 0062 | Time 26.3080(33.8673) | Bit/dim -48414.0508(-15821.9195) | Steps 392(396.95) | Grad Norm 5502.9016(273774.3455) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 8.4924, Epoch Time 207.4143, Bit/dim -48612.2227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0063 | Time 28.5628(33.7082) | Bit/dim -48493.3516(-16802.0625) | Steps 404(397.16) | Grad Norm 6704.0160(265762.2356) | Total Time 10.00(10.00)\n",
      "Iter 0064 | Time 29.6409(33.5861) | Bit/dim -48585.3711(-17755.5617) | Steps 398(397.19) | Grad Norm 5128.6002(257943.2266) | Total Time 10.00(10.00)\n",
      "Iter 0065 | Time 27.7539(33.4112) | Bit/dim -48592.6836(-18680.6754) | Steps 386(396.85) | Grad Norm 4300.0380(250333.9309) | Total Time 10.00(10.00)\n",
      "Iter 0066 | Time 27.6804(33.2392) | Bit/dim -48650.6289(-19579.7740) | Steps 386(396.53) | Grad Norm 6443.5552(243017.2196) | Total Time 10.00(10.00)\n",
      "Iter 0067 | Time 27.4873(33.0667) | Bit/dim -48697.0430(-20453.2921) | Steps 392(396.39) | Grad Norm 5029.3675(235877.5841) | Total Time 10.00(10.00)\n",
      "Iter 0068 | Time 27.6473(32.9041) | Bit/dim -48774.8125(-21302.9377) | Steps 398(396.44) | Grad Norm 4685.6788(228941.8269) | Total Time 10.00(10.00)\n",
      "Iter 0069 | Time 28.3473(32.7674) | Bit/dim -48853.7500(-22129.4620) | Steps 392(396.31) | Grad Norm 6709.2499(222274.8496) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 8.1991, Epoch Time 217.6213, Bit/dim -49034.3008\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0070 | Time 26.5079(32.5796) | Bit/dim -48896.7227(-22932.4799) | Steps 386(396.00) | Grad Norm 3464.0349(215710.5252) | Total Time 10.00(10.00)\n",
      "Iter 0071 | Time 26.6879(32.4029) | Bit/dim -48960.1250(-23713.3092) | Steps 374(395.34) | Grad Norm 5435.6365(209402.2785) | Total Time 10.00(10.00)\n",
      "Iter 0072 | Time 27.4797(32.2552) | Bit/dim -48977.4062(-24471.2321) | Steps 386(395.06) | Grad Norm 4024.2101(203240.9365) | Total Time 10.00(10.00)\n",
      "Iter 0073 | Time 27.1115(32.1009) | Bit/dim -49065.1328(-25209.0491) | Steps 386(394.79) | Grad Norm 6792.0582(197347.4701) | Total Time 10.00(10.00)\n",
      "Iter 0074 | Time 26.8549(31.9435) | Bit/dim -49096.4727(-25925.6719) | Steps 386(394.52) | Grad Norm 5605.7941(191595.2198) | Total Time 10.00(10.00)\n",
      "Iter 0075 | Time 28.5492(31.8416) | Bit/dim -49131.2383(-26621.8388) | Steps 386(394.27) | Grad Norm 5701.6844(186018.4138) | Total Time 10.00(10.00)\n",
      "Iter 0076 | Time 27.8748(31.7226) | Bit/dim -49112.5469(-27296.5601) | Steps 392(394.20) | Grad Norm 3890.6683(180554.5814) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 8.0652, Epoch Time 211.3746, Bit/dim -49337.8633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0077 | Time 27.8774(31.6073) | Bit/dim -49190.2305(-27953.3702) | Steps 392(394.13) | Grad Norm 6839.6547(175343.1336) | Total Time 10.00(10.00)\n",
      "Iter 0078 | Time 28.1624(31.5039) | Bit/dim -49256.5430(-28592.4654) | Steps 380(393.71) | Grad Norm 3663.8570(170192.7553) | Total Time 10.00(10.00)\n",
      "Iter 0079 | Time 28.7981(31.4228) | Bit/dim -49297.2539(-29213.6090) | Steps 404(394.02) | Grad Norm 4225.2001(165213.7286) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 28.7076(31.3413) | Bit/dim -49326.5195(-29816.9964) | Steps 404(394.32) | Grad Norm 8332.9488(160507.3052) | Total Time 10.00(10.00)\n",
      "Iter 0081 | Time 29.7591(31.2938) | Bit/dim -49378.0781(-30403.8288) | Steps 410(394.79) | Grad Norm 11764.1017(156045.0091) | Total Time 10.00(10.00)\n",
      "Iter 0082 | Time 29.3918(31.2368) | Bit/dim -49429.8828(-30974.6104) | Steps 398(394.88) | Grad Norm 20167.2445(151968.6762) | Total Time 10.00(10.00)\n",
      "Iter 0083 | Time 28.3736(31.1509) | Bit/dim -49396.6719(-31527.2723) | Steps 410(395.34) | Grad Norm 44139.2863(148733.7945) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 8.2131, Epoch Time 221.6832, Bit/dim -48942.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0084 | Time 28.4537(31.0700) | Bit/dim -48804.7773(-32045.5974) | Steps 392(395.24) | Grad Norm 170770.1583(149394.8854) | Total Time 10.00(10.00)\n",
      "Iter 0085 | Time 31.1977(31.0738) | Bit/dim -45357.0117(-32444.9399) | Steps 434(396.40) | Grad Norm 112533.0397(148289.0300) | Total Time 10.00(10.00)\n",
      "Iter 0086 | Time 32.2238(31.1083) | Bit/dim -45854.7578(-32847.2344) | Steps 434(397.53) | Grad Norm 88106.9905(146483.5689) | Total Time 10.00(10.00)\n",
      "Iter 0087 | Time 31.0926(31.1078) | Bit/dim -48208.6836(-33308.0779) | Steps 428(398.44) | Grad Norm 63352.2058(143989.6280) | Total Time 10.00(10.00)\n",
      "Iter 0088 | Time 27.0747(30.9868) | Bit/dim -45087.0781(-33661.4479) | Steps 392(398.25) | Grad Norm 303468.3763(148773.9904) | Total Time 10.00(10.00)\n",
      "Iter 0089 | Time 32.4425(31.0305) | Bit/dim -47981.9531(-34091.0630) | Steps 416(398.78) | Grad Norm 49308.5379(145790.0268) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 32.3210(31.0692) | Bit/dim -46491.8828(-34463.0876) | Steps 428(399.66) | Grad Norm 56256.1616(143104.0109) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 8.9210, Epoch Time 235.9477, Bit/dim -46449.1797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generatedo _generate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0091 | Time 30.4106(31.0495) | Bit/dim -46344.1172(-34819.5185) | Steps 422(400.33) | Grad Norm 46323.8590(140200.6063) | Total Time 10.00(10.00)\n",
      "Iter 0092 | Time 31.4661(31.0620) | Bit/dim -46582.4062(-35172.4051) | Steps 428(401.16) | Grad Norm 98422.1330(138947.2521) | Total Time 10.00(10.00)\n",
      "Iter 0093 | Time 29.7076(31.0213) | Bit/dim -47500.3125(-35542.2424) | Steps 422(401.78) | Grad Norm 31456.9461(135722.5429) | Total Time 10.00(10.00)\n",
      "Iter 0094 | Time 31.0192(31.0213) | Bit/dim -47781.7539(-35909.4277) | Steps 428(402.57) | Grad Norm 18441.2768(132204.1050) | Total Time 10.00(10.00)\n",
      "Iter 0095 | Time 29.7475(30.9831) | Bit/dim -47683.7148(-36262.6563) | Steps 410(402.79) | Grad Norm 16806.6668(128742.1818) | Total Time 10.00(10.00)\n",
      "Iter 0096 | Time 27.8319(30.8885) | Bit/dim -47268.6094(-36592.8349) | Steps 398(402.65) | Grad Norm 31083.8162(125812.4308) | Total Time 10.00(10.00)\n",
      "Iter 0097 | Time 28.1037(30.8050) | Bit/dim -47400.5781(-36917.0672) | Steps 398(402.51) | Grad Norm 33473.8220(123042.2726) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 7.9469, Epoch Time 228.5985, Bit/dim -48068.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0098 | Time 27.9795(30.7202) | Bit/dim -47935.7461(-37247.6276) | Steps 398(402.37) | Grad Norm 22786.9022(120034.6115) | Total Time 10.00(10.00)\n",
      "Iter 0099 | Time 29.3606(30.6794) | Bit/dim -48275.0508(-37578.4503) | Steps 404(402.42) | Grad Norm 12275.4688(116801.8372) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 30.0929(30.6618) | Bit/dim -48286.4883(-37899.6914) | Steps 428(403.19) | Grad Norm 12992.8331(113687.5671) | Total Time 10.00(10.00)\n",
      "Iter 0101 | Time 29.8924(30.6387) | Bit/dim -48190.9453(-38208.4290) | Steps 428(403.93) | Grad Norm 20606.3310(110895.1300) | Total Time 10.00(10.00)\n",
      "Iter 0102 | Time 30.3790(30.6310) | Bit/dim -48261.4492(-38510.0196) | Steps 428(404.66) | Grad Norm 21548.2003(108214.7221) | Total Time 10.00(10.00)\n",
      "Iter 0103 | Time 30.2320(30.6190) | Bit/dim -48534.2969(-38810.7480) | Steps 422(405.18) | Grad Norm 14959.7258(105417.0722) | Total Time 10.00(10.00)\n",
      "Iter 0104 | Time 29.9576(30.5991) | Bit/dim -48669.0508(-39106.4970) | Steps 422(405.68) | Grad Norm 11188.4664(102590.2140) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 8.2026, Epoch Time 228.5535, Bit/dim -48731.3359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0105 | Time 30.6010(30.5992) | Bit/dim -48633.7188(-39392.3137) | Steps 416(405.99) | Grad Norm 22071.9919(100174.6674) | Total Time 10.00(10.00)\n",
      "Iter 0106 | Time 29.2008(30.5572) | Bit/dim -48672.7500(-39670.7268) | Steps 404(405.93) | Grad Norm 22817.3405(97853.9476) | Total Time 10.00(10.00)\n",
      "Iter 0107 | Time 30.8075(30.5647) | Bit/dim -48978.6914(-39949.9657) | Steps 404(405.87) | Grad Norm 10621.2474(95236.9666) | Total Time 10.00(10.00)\n",
      "Iter 0108 | Time 28.0608(30.4896) | Bit/dim -49107.0898(-40224.6794) | Steps 398(405.64) | Grad Norm 6958.5380(92588.6137) | Total Time 10.00(10.00)\n",
      "Iter 0109 | Time 27.6197(30.4035) | Bit/dim -49023.1445(-40488.6334) | Steps 386(405.05) | Grad Norm 11857.2647(90166.6732) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 27.2996(30.3104) | Bit/dim -49095.4688(-40746.8385) | Steps 392(404.66) | Grad Norm 12612.9797(87840.0624) | Total Time 10.00(10.00)\n",
      "Iter 0111 | Time 27.7829(30.2346) | Bit/dim -49226.9453(-41001.2417) | Steps 398(404.46) | Grad Norm 8389.8995(85456.5575) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 8.4600, Epoch Time 222.1495, Bit/dim -49497.1484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0112 | Time 28.0066(30.1677) | Bit/dim -49358.4219(-41251.9571) | Steps 398(404.26) | Grad Norm 7132.2580(83106.8286) | Total Time 10.00(10.00)\n",
      "Iter 0113 | Time 28.0834(30.1052) | Bit/dim -49442.7656(-41497.6813) | Steps 392(403.90) | Grad Norm 9542.8532(80899.9093) | Total Time 10.00(10.00)\n",
      "Iter 0114 | Time 28.6066(30.0603) | Bit/dim -49566.7344(-41739.7529) | Steps 392(403.54) | Grad Norm 8313.7389(78722.3242) | Total Time 10.00(10.00)\n",
      "Iter 0115 | Time 27.9510(29.9970) | Bit/dim -49693.9297(-41978.3782) | Steps 392(403.19) | Grad Norm 7606.8767(76588.8608) | Total Time 10.00(10.00)\n",
      "Iter 0116 | Time 27.4415(29.9203) | Bit/dim -49746.8086(-42211.4311) | Steps 386(402.68) | Grad Norm 12677.4603(74671.5187) | Total Time 10.00(10.00)\n",
      "Iter 0117 | Time 28.5545(29.8793) | Bit/dim -49886.9844(-42441.6977) | Steps 386(402.18) | Grad Norm 11455.5066(72775.0384) | Total Time 10.00(10.00)\n",
      "Iter 0118 | Time 27.0258(29.7937) | Bit/dim -50037.5430(-42669.5731) | Steps 380(401.51) | Grad Norm 5598.8498(70759.7527) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 8.2709, Epoch Time 216.2991, Bit/dim -50233.7617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0119 | Time 27.5545(29.7266) | Bit/dim -50063.4688(-42891.3900) | Steps 380(400.87) | Grad Norm 18118.3039(69180.5093) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 27.0730(29.6470) | Bit/dim -50219.2383(-43111.2254) | Steps 380(400.24) | Grad Norm 12562.5937(67481.9718) | Total Time 10.00(10.00)\n",
      "Iter 0121 | Time 29.4895(29.6422) | Bit/dim -50352.2578(-43328.4564) | Steps 392(399.99) | Grad Norm 8682.3914(65717.9844) | Total Time 10.00(10.00)\n",
      "Iter 0122 | Time 28.7501(29.6155) | Bit/dim -50361.8242(-43539.4574) | Steps 392(399.75) | Grad Norm 13435.2502(64149.5024) | Total Time 10.00(10.00)\n",
      "Iter 0123 | Time 29.0079(29.5972) | Bit/dim -50547.0078(-43749.6839) | Steps 392(399.52) | Grad Norm 14285.8355(62653.5923) | Total Time 10.00(10.00)\n",
      "Iter 0124 | Time 28.4065(29.5615) | Bit/dim -50621.1016(-43955.8265) | Steps 380(398.93) | Grad Norm 4452.7590(60907.5673) | Total Time 10.00(10.00)\n",
      "Iter 0125 | Time 30.9825(29.6041) | Bit/dim -50707.0391(-44158.3628) | Steps 404(399.09) | Grad Norm 23479.9094(59784.7376) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 8.5247, Epoch Time 222.1199, Bit/dim -50919.4805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0126 | Time 29.5957(29.6039) | Bit/dim -50787.1406(-44357.2262) | Steps 404(399.23) | Grad Norm 17870.3312(58527.3054) | Total Time 10.00(10.00)\n",
      "Iter 0127 | Time 29.9785(29.6151) | Bit/dim -50915.1250(-44553.9631) | Steps 398(399.20) | Grad Norm 12346.4448(57141.8796) | Total Time 10.00(10.00)\n",
      "Iter 0128 | Time 29.7985(29.6206) | Bit/dim -50956.7383(-44746.0464) | Steps 404(399.34) | Grad Norm 14053.6040(55849.2313) | Total Time 10.00(10.00)\n",
      "Iter 0129 | Time 30.5444(29.6483) | Bit/dim -51082.5742(-44936.1422) | Steps 398(399.30) | Grad Norm 6345.2474(54364.1118) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 31.5963(29.7068) | Bit/dim -51119.0195(-45121.6285) | Steps 404(399.44) | Grad Norm 17865.4527(53269.1520) | Total Time 10.00(10.00)\n",
      "Iter 0131 | Time 30.6035(29.7337) | Bit/dim -51151.9531(-45302.5383) | Steps 410(399.76) | Grad Norm 34679.7359(52711.4696) | Total Time 10.00(10.00)\n",
      "Iter 0132 | Time 31.9622(29.8005) | Bit/dim -50481.1328(-45457.8961) | Steps 440(400.97) | Grad Norm 115718.9754(54601.6947) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 8.2676, Epoch Time 234.5611, Bit/dim -42313.3867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n",
      "do _generate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 0133 | Time 29.1693(29.7816) | Bit/dim -42166.2344(-45359.1463) | Steps 404(401.06) | Grad Norm 609365.3057(71244.6031) | Total Time 10.00(10.00)\n",
      "Iter 0134 | Time 39.2660(30.0661) | Bit/dim 136757.0469(-39895.6605) | Steps 488(403.67) | Grad Norm 14535555.5669(505173.9320) | Total Time 10.00(10.00)\n",
      "Iter 0135 | Time 42.6358(30.4432) | Bit/dim -33067.8750(-39690.8269) | Steps 542(407.82) | Grad Norm 105822.9955(493193.4039) | Total Time 10.00(10.00)\n",
      "Iter 0136 | Time 45.9564(30.9086) | Bit/dim -35575.1758(-39567.3574) | Steps 650(415.08) | Grad Norm 188789.1903(484061.2775) | Total Time 10.00(10.00)\n",
      "Iter 0137 | Time 76.3031(32.2705) | Bit/dim -39655.2070(-39569.9929) | Steps 1106(435.81) | Grad Norm 1177341490403.8855(35320714251.5557) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p train_cnf_conditional_exploretol2.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 8000 --save experiments/cnf_cond_bs8K_100timeslr_exploretol2 --conditional True --lr 0.01 --warmup_iters 113 --atol 1e-4  --rtol 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
