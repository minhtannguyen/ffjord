{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_60_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 3310 | Time 17.3642(17.9658) | Bit/dim 3.7534(3.7461) | Xent 0.9003(0.8408) | Loss 9.7124(10.2254) | Error 0.2856(0.2986) Steps 0(0.00) | Grad Norm 15.2947(9.7429) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 18.8210(17.7726) | Bit/dim 3.7641(3.7480) | Xent 0.8212(0.8434) | Loss 9.6506(10.0466) | Error 0.2978(0.3015) Steps 0(0.00) | Grad Norm 11.9355(10.3724) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 18.5887(17.5841) | Bit/dim 3.7310(3.7482) | Xent 0.8332(0.8427) | Loss 9.7901(9.9219) | Error 0.2978(0.3013) Steps 0(0.00) | Grad Norm 9.9973(10.4997) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 16.4595(17.3694) | Bit/dim 3.7293(3.7472) | Xent 0.8006(0.8368) | Loss 9.4822(9.8203) | Error 0.2756(0.2992) Steps 0(0.00) | Grad Norm 12.0295(10.3066) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 16.3354(17.2181) | Bit/dim 3.7487(3.7457) | Xent 0.8385(0.8251) | Loss 9.4247(9.7291) | Error 0.3089(0.2952) Steps 0(0.00) | Grad Norm 10.0053(9.9279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 91.1400, Epoch Time 1064.6415(1000.8277), Bit/dim 3.7389(best: inf), Xent 0.8566, Loss 4.1672, Error 0.3035(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 16.0687(17.0961) | Bit/dim 3.7199(3.7412) | Xent 0.8176(0.8259) | Loss 9.3671(10.2955) | Error 0.2744(0.2941) Steps 0(0.00) | Grad Norm 5.7212(9.4310) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 14.9958(16.9924) | Bit/dim 3.7463(3.7393) | Xent 0.7935(0.8196) | Loss 9.5390(10.0871) | Error 0.2789(0.2918) Steps 0(0.00) | Grad Norm 7.1273(9.2362) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 17.1811(17.0776) | Bit/dim 3.7263(3.7405) | Xent 0.7748(0.8160) | Loss 9.5450(9.9282) | Error 0.2700(0.2901) Steps 0(0.00) | Grad Norm 7.1470(9.4608) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 16.7312(16.9959) | Bit/dim 3.7639(3.7410) | Xent 0.7571(0.8139) | Loss 9.5735(9.8079) | Error 0.2722(0.2899) Steps 0(0.00) | Grad Norm 10.5401(9.1032) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 19.0162(16.9657) | Bit/dim 3.7105(3.7405) | Xent 0.7673(0.8125) | Loss 9.5356(9.7254) | Error 0.2900(0.2896) Steps 0(0.00) | Grad Norm 4.8277(9.0036) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 17.6479(16.9210) | Bit/dim 3.7402(3.7391) | Xent 0.8421(0.8150) | Loss 9.7183(9.6717) | Error 0.3000(0.2903) Steps 0(0.00) | Grad Norm 8.1369(8.6690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 87.6914, Epoch Time 1035.7729(1001.8761), Bit/dim 3.7334(best: 3.7389), Xent 0.8228, Loss 4.1448, Error 0.2916(best: 0.3035)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 17.4533(16.9542) | Bit/dim 3.6985(3.7365) | Xent 0.9111(0.8206) | Loss 9.4803(10.2229) | Error 0.3144(0.2906) Steps 0(0.00) | Grad Norm 9.0812(9.0908) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.7927(16.9085) | Bit/dim 3.7409(3.7376) | Xent 0.7638(0.8195) | Loss 9.5556(10.0374) | Error 0.2767(0.2904) Steps 0(0.00) | Grad Norm 10.7944(9.0706) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 16.6003(16.8289) | Bit/dim 3.7014(3.7354) | Xent 0.8098(0.8107) | Loss 9.4017(9.8744) | Error 0.2900(0.2883) Steps 0(0.00) | Grad Norm 8.2928(8.3781) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.4063(16.9075) | Bit/dim 3.7087(3.7328) | Xent 0.8665(0.8132) | Loss 9.3930(9.7654) | Error 0.3189(0.2900) Steps 0(0.00) | Grad Norm 8.9702(8.7264) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 17.4723(16.8895) | Bit/dim 3.7515(3.7359) | Xent 0.8432(0.8140) | Loss 9.7150(9.7070) | Error 0.2978(0.2912) Steps 0(0.00) | Grad Norm 13.5594(8.9100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 89.8483, Epoch Time 1034.8960(1002.8667), Bit/dim 3.7259(best: 3.7334), Xent 0.8103, Loss 4.1310, Error 0.2851(best: 0.2916)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 16.0744(16.7747) | Bit/dim 3.7340(3.7344) | Xent 0.8828(0.8165) | Loss 9.6138(10.3416) | Error 0.3011(0.2913) Steps 0(0.00) | Grad Norm 9.8458(9.4420) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 17.3834(16.8444) | Bit/dim 3.7249(3.7365) | Xent 0.8349(0.8102) | Loss 9.4555(10.1211) | Error 0.2900(0.2883) Steps 0(0.00) | Grad Norm 10.6943(9.6826) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 16.1522(16.8493) | Bit/dim 3.7472(3.7351) | Xent 0.7573(0.8075) | Loss 9.4482(9.9662) | Error 0.2833(0.2885) Steps 0(0.00) | Grad Norm 11.1178(9.9277) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 17.0680(16.9529) | Bit/dim 3.7294(3.7369) | Xent 0.7600(0.8082) | Loss 9.4599(9.8551) | Error 0.2567(0.2889) Steps 0(0.00) | Grad Norm 12.4746(10.2776) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 16.5068(17.0020) | Bit/dim 3.7220(3.7334) | Xent 0.7826(0.8006) | Loss 9.5186(9.7438) | Error 0.3044(0.2867) Steps 0(0.00) | Grad Norm 7.5847(9.8047) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 17.9119(17.0571) | Bit/dim 3.7153(3.7297) | Xent 0.8195(0.7993) | Loss 9.6806(9.6733) | Error 0.2756(0.2859) Steps 0(0.00) | Grad Norm 6.4120(9.4475) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 85.6177, Epoch Time 1040.2782(1003.9890), Bit/dim 3.7277(best: 3.7259), Xent 0.7920, Loss 4.1237, Error 0.2792(best: 0.2851)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 17.5654(17.3397) | Bit/dim 3.7408(3.7274) | Xent 0.8147(0.7916) | Loss 9.4621(10.1622) | Error 0.2978(0.2836) Steps 0(0.00) | Grad Norm 14.0954(9.5109) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 17.4647(17.3060) | Bit/dim 3.7365(3.7300) | Xent 0.8561(0.7973) | Loss 9.6676(9.9693) | Error 0.2933(0.2858) Steps 0(0.00) | Grad Norm 18.8421(9.7710) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.2162(17.2125) | Bit/dim 3.7250(3.7305) | Xent 0.7850(0.8013) | Loss 9.3781(9.8437) | Error 0.2733(0.2859) Steps 0(0.00) | Grad Norm 8.2441(10.1666) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 17.4172(17.1245) | Bit/dim 3.7141(3.7306) | Xent 0.7842(0.8025) | Loss 9.3307(9.7591) | Error 0.2533(0.2861) Steps 0(0.00) | Grad Norm 5.2500(10.2684) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 16.0151(17.1198) | Bit/dim 3.7097(3.7293) | Xent 0.8491(0.8062) | Loss 9.0721(9.6870) | Error 0.2900(0.2875) Steps 0(0.00) | Grad Norm 7.1524(9.8388) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 88.8614, Epoch Time 1052.7497(1005.4519), Bit/dim 3.7230(best: 3.7259), Xent 0.7993, Loss 4.1227, Error 0.2803(best: 0.2792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 17.1258(17.1488) | Bit/dim 3.7157(3.7267) | Xent 0.7421(0.7980) | Loss 9.5367(10.2534) | Error 0.2711(0.2844) Steps 0(0.00) | Grad Norm 8.1735(9.4312) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 17.9186(17.1894) | Bit/dim 3.7359(3.7269) | Xent 0.7876(0.7953) | Loss 9.5518(10.0434) | Error 0.2900(0.2835) Steps 0(0.00) | Grad Norm 10.4453(9.5480) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 16.1402(17.0709) | Bit/dim 3.6881(3.7248) | Xent 0.8598(0.7967) | Loss 9.3816(9.8948) | Error 0.3122(0.2850) Steps 0(0.00) | Grad Norm 9.5582(9.2556) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 16.5488(17.0239) | Bit/dim 3.7253(3.7251) | Xent 0.8115(0.7976) | Loss 9.1297(9.7811) | Error 0.2911(0.2844) Steps 0(0.00) | Grad Norm 10.2770(9.5099) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 16.5803(17.1944) | Bit/dim 3.6862(3.7237) | Xent 0.9870(0.8103) | Loss 9.5443(9.7054) | Error 0.3400(0.2877) Steps 0(0.00) | Grad Norm 11.7553(10.3102) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 16.6425(17.1556) | Bit/dim 3.7394(3.7282) | Xent 0.8164(0.8158) | Loss 9.4775(9.6572) | Error 0.2933(0.2905) Steps 0(0.00) | Grad Norm 8.8529(10.1406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 87.7223, Epoch Time 1048.9172(1006.7558), Bit/dim 3.7214(best: 3.7230), Xent 0.8359, Loss 4.1393, Error 0.2936(best: 0.2792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 17.4597(17.0283) | Bit/dim 3.7382(3.7305) | Xent 0.7939(0.8073) | Loss 9.5675(10.1747) | Error 0.2833(0.2872) Steps 0(0.00) | Grad Norm 10.5885(9.6597) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 17.0195(17.2949) | Bit/dim 3.7197(3.7264) | Xent 0.7870(0.7945) | Loss 9.5271(9.9793) | Error 0.2911(0.2834) Steps 0(0.00) | Grad Norm 8.9972(9.4238) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.5706(17.1585) | Bit/dim 3.7315(3.7236) | Xent 0.7550(0.7916) | Loss 9.3027(9.8225) | Error 0.2711(0.2821) Steps 0(0.00) | Grad Norm 5.3180(9.2370) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.4213(17.0409) | Bit/dim 3.6854(3.7249) | Xent 0.7875(0.7863) | Loss 9.2396(9.7131) | Error 0.2867(0.2790) Steps 0(0.00) | Grad Norm 7.2752(8.8163) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.0102(17.0879) | Bit/dim 3.7469(3.7232) | Xent 0.7486(0.7851) | Loss 9.2573(9.6327) | Error 0.2678(0.2779) Steps 0(0.00) | Grad Norm 7.2817(8.5378) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 87.3500, Epoch Time 1041.5343(1007.7992), Bit/dim 3.7290(best: 3.7214), Xent 0.8363, Loss 4.1472, Error 0.2929(best: 0.2792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 19.9743(17.0848) | Bit/dim 3.7195(3.7232) | Xent 0.8202(0.7876) | Loss 9.5915(10.2146) | Error 0.2911(0.2801) Steps 0(0.00) | Grad Norm 21.7409(9.3616) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 17.0853(17.0225) | Bit/dim 3.7240(3.7240) | Xent 0.7842(0.7851) | Loss 9.5714(10.0085) | Error 0.2944(0.2801) Steps 0(0.00) | Grad Norm 7.6250(9.2928) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 16.6616(17.0099) | Bit/dim 3.7114(3.7204) | Xent 0.7586(0.7806) | Loss 9.2780(9.8540) | Error 0.2878(0.2793) Steps 0(0.00) | Grad Norm 7.6679(8.8317) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 19.2069(17.1090) | Bit/dim 3.7726(3.7222) | Xent 0.8010(0.7900) | Loss 9.7259(9.7628) | Error 0.2767(0.2826) Steps 0(0.00) | Grad Norm 7.8114(9.3743) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 17.6289(17.1563) | Bit/dim 3.7416(3.7200) | Xent 0.7647(0.7896) | Loss 9.5364(9.6648) | Error 0.2700(0.2822) Steps 0(0.00) | Grad Norm 10.3641(9.5913) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.8198(17.1234) | Bit/dim 3.7815(3.7188) | Xent 0.7611(0.7822) | Loss 9.4718(9.5926) | Error 0.2944(0.2802) Steps 0(0.00) | Grad Norm 12.1168(9.2982) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 87.7696, Epoch Time 1047.8726(1009.0014), Bit/dim 3.7193(best: 3.7214), Xent 0.7798, Loss 4.1092, Error 0.2734(best: 0.2792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 16.8910(17.0431) | Bit/dim 3.7276(3.7186) | Xent 0.7579(0.7850) | Loss 9.2457(10.1368) | Error 0.2711(0.2803) Steps 0(0.00) | Grad Norm 11.7158(9.7742) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 17.7524(17.1070) | Bit/dim 3.7036(3.7161) | Xent 0.7094(0.7816) | Loss 9.3690(9.9513) | Error 0.2478(0.2786) Steps 0(0.00) | Grad Norm 6.5719(9.7126) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 17.1656(17.1083) | Bit/dim 3.7190(3.7162) | Xent 0.7352(0.7737) | Loss 9.2820(9.8083) | Error 0.2711(0.2754) Steps 0(0.00) | Grad Norm 10.6023(9.2793) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 18.8372(17.0600) | Bit/dim 3.6921(3.7150) | Xent 0.7112(0.7619) | Loss 9.4148(9.6881) | Error 0.2600(0.2717) Steps 0(0.00) | Grad Norm 5.2085(8.6472) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.7246(17.0436) | Bit/dim 3.7487(3.7138) | Xent 0.7889(0.7576) | Loss 9.6137(9.6134) | Error 0.2889(0.2710) Steps 0(0.00) | Grad Norm 9.8563(8.6038) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 86.3123, Epoch Time 1042.0143(1009.9918), Bit/dim 3.7172(best: 3.7193), Xent 0.7868, Loss 4.1106, Error 0.2791(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 15.8007(17.0258) | Bit/dim 3.7045(3.7132) | Xent 0.8080(0.7538) | Loss 9.2565(10.1505) | Error 0.2844(0.2690) Steps 0(0.00) | Grad Norm 11.3218(8.5796) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 16.7494(16.9455) | Bit/dim 3.7359(3.7130) | Xent 0.7248(0.7522) | Loss 9.4571(9.9474) | Error 0.2511(0.2680) Steps 0(0.00) | Grad Norm 6.8631(9.0265) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 16.3580(16.9707) | Bit/dim 3.6977(3.7125) | Xent 0.7336(0.7522) | Loss 9.5292(9.8133) | Error 0.2489(0.2686) Steps 0(0.00) | Grad Norm 8.0782(9.0439) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 16.5593(17.2186) | Bit/dim 3.7143(3.7119) | Xent 0.8041(0.7611) | Loss 9.2372(9.7156) | Error 0.2933(0.2716) Steps 0(0.00) | Grad Norm 7.4158(9.1289) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 17.5955(17.2566) | Bit/dim 3.7141(3.7144) | Xent 0.7379(0.7627) | Loss 9.4975(9.6321) | Error 0.2656(0.2727) Steps 0(0.00) | Grad Norm 6.9023(9.1196) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 18.2018(17.5279) | Bit/dim 3.6903(3.7146) | Xent 0.7387(0.7623) | Loss 9.4326(9.5779) | Error 0.2511(0.2713) Steps 0(0.00) | Grad Norm 6.4311(8.9648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 86.2691, Epoch Time 1059.4172(1011.4745), Bit/dim 3.7127(best: 3.7172), Xent 0.7954, Loss 4.1104, Error 0.2818(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.4304(17.4323) | Bit/dim 3.7154(3.7119) | Xent 0.8449(0.7595) | Loss 9.3971(10.1103) | Error 0.3078(0.2712) Steps 0(0.00) | Grad Norm 15.2513(9.1483) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 15.6901(17.2643) | Bit/dim 3.7062(3.7122) | Xent 0.7063(0.7571) | Loss 9.2515(9.9058) | Error 0.2644(0.2710) Steps 0(0.00) | Grad Norm 5.3494(9.0886) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 17.9562(17.2814) | Bit/dim 3.7522(3.7114) | Xent 0.8014(0.7534) | Loss 9.4685(9.7841) | Error 0.2711(0.2692) Steps 0(0.00) | Grad Norm 7.9178(8.3472) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 17.0789(17.3272) | Bit/dim 3.7278(3.7115) | Xent 0.7092(0.7452) | Loss 9.0641(9.6666) | Error 0.2556(0.2666) Steps 0(0.00) | Grad Norm 6.7337(8.4092) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 17.2022(17.1632) | Bit/dim 3.7214(3.7092) | Xent 0.8123(0.7478) | Loss 9.3807(9.5782) | Error 0.2900(0.2675) Steps 0(0.00) | Grad Norm 7.8464(8.3687) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 89.1003, Epoch Time 1042.0633(1012.3922), Bit/dim 3.7126(best: 3.7127), Xent 0.8314, Loss 4.1282, Error 0.2979(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 16.0150(17.0467) | Bit/dim 3.7526(3.7109) | Xent 0.7978(0.7717) | Loss 9.2203(10.2337) | Error 0.2800(0.2760) Steps 0(0.00) | Grad Norm 11.4323(9.4781) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 17.2941(17.1168) | Bit/dim 3.6747(3.7106) | Xent 0.7762(0.7625) | Loss 9.4553(10.0137) | Error 0.2689(0.2734) Steps 0(0.00) | Grad Norm 4.2624(9.0076) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 16.8862(17.0649) | Bit/dim 3.7222(3.7133) | Xent 0.6846(0.7489) | Loss 9.1442(9.8250) | Error 0.2400(0.2688) Steps 0(0.00) | Grad Norm 9.8941(8.8204) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 18.9062(17.1160) | Bit/dim 3.6906(3.7099) | Xent 0.8136(0.7518) | Loss 9.4770(9.7125) | Error 0.2989(0.2706) Steps 0(0.00) | Grad Norm 9.1656(8.5860) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 16.5839(16.9716) | Bit/dim 3.7075(3.7092) | Xent 0.7700(0.7456) | Loss 9.3425(9.6173) | Error 0.2711(0.2677) Steps 0(0.00) | Grad Norm 7.8382(8.6583) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 16.0339(16.8743) | Bit/dim 3.7004(3.7094) | Xent 0.7563(0.7494) | Loss 9.3483(9.5596) | Error 0.2878(0.2696) Steps 0(0.00) | Grad Norm 8.8513(9.1141) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 89.0421, Epoch Time 1038.9985(1013.1904), Bit/dim 3.7073(best: 3.7126), Xent 0.7806, Loss 4.0976, Error 0.2770(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 17.3376(16.8947) | Bit/dim 3.7125(3.7102) | Xent 0.7492(0.7475) | Loss 9.4345(10.0573) | Error 0.2511(0.2675) Steps 0(0.00) | Grad Norm 11.8017(9.1874) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.6232(16.8706) | Bit/dim 3.6684(3.7070) | Xent 0.7139(0.7417) | Loss 9.2462(9.8651) | Error 0.2656(0.2668) Steps 0(0.00) | Grad Norm 10.5002(9.0743) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.5495(16.7830) | Bit/dim 3.7288(3.7086) | Xent 0.7293(0.7391) | Loss 9.1686(9.7265) | Error 0.2556(0.2642) Steps 0(0.00) | Grad Norm 7.8722(8.6934) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 17.4129(16.7231) | Bit/dim 3.7366(3.7059) | Xent 0.7441(0.7391) | Loss 9.4443(9.6227) | Error 0.2611(0.2638) Steps 0(0.00) | Grad Norm 8.3300(8.7682) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 17.1663(16.8187) | Bit/dim 3.6947(3.7039) | Xent 0.7116(0.7380) | Loss 9.4686(9.5615) | Error 0.2367(0.2621) Steps 0(0.00) | Grad Norm 6.0965(8.4234) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 88.1613, Epoch Time 1027.7250(1013.6264), Bit/dim 3.7058(best: 3.7073), Xent 0.7770, Loss 4.0943, Error 0.2754(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 17.3034(16.7976) | Bit/dim 3.6896(3.7041) | Xent 0.7622(0.7374) | Loss 9.2340(10.1762) | Error 0.2700(0.2633) Steps 0(0.00) | Grad Norm 9.1304(8.3895) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 15.3816(16.6475) | Bit/dim 3.6725(3.7050) | Xent 0.7330(0.7367) | Loss 9.0250(9.9431) | Error 0.2711(0.2628) Steps 0(0.00) | Grad Norm 12.0110(8.4382) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 15.8901(16.8448) | Bit/dim 3.6807(3.7049) | Xent 0.7028(0.7331) | Loss 9.3323(9.7947) | Error 0.2511(0.2611) Steps 0(0.00) | Grad Norm 7.2143(8.2273) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.7517(16.8222) | Bit/dim 3.7125(3.7046) | Xent 0.6987(0.7394) | Loss 9.2700(9.6671) | Error 0.2600(0.2640) Steps 0(0.00) | Grad Norm 8.3694(8.6109) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 17.6557(16.8751) | Bit/dim 3.7087(3.7066) | Xent 0.6835(0.7381) | Loss 9.4583(9.5940) | Error 0.2400(0.2643) Steps 0(0.00) | Grad Norm 5.3798(8.6593) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 18.0976(16.9327) | Bit/dim 3.6865(3.7042) | Xent 0.7213(0.7379) | Loss 9.4065(9.5187) | Error 0.2644(0.2629) Steps 0(0.00) | Grad Norm 5.8023(8.1077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 88.8744, Epoch Time 1037.1845(1014.3332), Bit/dim 3.6997(best: 3.7058), Xent 0.7461, Loss 4.0727, Error 0.2628(best: 0.2734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 18.7355(17.1854) | Bit/dim 3.7047(3.7034) | Xent 0.6995(0.7281) | Loss 9.1159(10.0500) | Error 0.2578(0.2593) Steps 0(0.00) | Grad Norm 7.7465(7.8479) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.7334(17.0860) | Bit/dim 3.7103(3.7041) | Xent 0.7447(0.7260) | Loss 9.5594(9.8770) | Error 0.2700(0.2596) Steps 0(0.00) | Grad Norm 7.1864(8.2749) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 17.4925(17.0501) | Bit/dim 3.7009(3.7032) | Xent 0.7694(0.7383) | Loss 9.3375(9.7469) | Error 0.2800(0.2630) Steps 0(0.00) | Grad Norm 12.7875(9.1372) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 16.5716(17.0842) | Bit/dim 3.6770(3.7020) | Xent 0.7949(0.7415) | Loss 9.3706(9.6444) | Error 0.2856(0.2643) Steps 0(0.00) | Grad Norm 10.2293(9.6736) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 18.6451(16.9526) | Bit/dim 3.6753(3.7010) | Xent 0.6817(0.7388) | Loss 9.1495(9.5467) | Error 0.2411(0.2633) Steps 0(0.00) | Grad Norm 4.3424(9.3749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 87.3394, Epoch Time 1043.9860(1015.2227), Bit/dim 3.7052(best: 3.6997), Xent 0.7538, Loss 4.0821, Error 0.2664(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.4961(16.9921) | Bit/dim 3.6996(3.7013) | Xent 0.7380(0.7297) | Loss 9.2372(10.1542) | Error 0.2644(0.2602) Steps 0(0.00) | Grad Norm 9.8371(8.6543) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 17.1786(17.0357) | Bit/dim 3.6982(3.6983) | Xent 0.6747(0.7243) | Loss 9.2302(9.9213) | Error 0.2378(0.2582) Steps 0(0.00) | Grad Norm 8.6269(8.1522) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.4179(16.9377) | Bit/dim 3.6910(3.6978) | Xent 0.6977(0.7188) | Loss 9.2993(9.7583) | Error 0.2422(0.2558) Steps 0(0.00) | Grad Norm 7.4584(7.8392) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 16.2496(16.9401) | Bit/dim 3.7103(3.6995) | Xent 0.7466(0.7229) | Loss 9.2383(9.6411) | Error 0.2644(0.2581) Steps 0(0.00) | Grad Norm 9.9758(8.6682) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 16.1437(16.9646) | Bit/dim 3.7312(3.7004) | Xent 0.7105(0.7238) | Loss 9.3526(9.5889) | Error 0.2422(0.2576) Steps 0(0.00) | Grad Norm 11.9317(8.7821) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 18.4598(17.0817) | Bit/dim 3.6737(3.6998) | Xent 0.7381(0.7281) | Loss 9.3576(9.5390) | Error 0.2622(0.2587) Steps 0(0.00) | Grad Norm 8.1695(8.9964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 88.1781, Epoch Time 1042.8395(1016.0512), Bit/dim 3.7064(best: 3.6997), Xent 0.7597, Loss 4.0862, Error 0.2653(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.5718(17.2086) | Bit/dim 3.7072(3.7018) | Xent 0.6882(0.7304) | Loss 9.2366(10.0599) | Error 0.2422(0.2582) Steps 0(0.00) | Grad Norm 10.7111(9.3272) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 17.2180(17.2696) | Bit/dim 3.6832(3.6997) | Xent 0.7165(0.7269) | Loss 9.1660(9.8675) | Error 0.2522(0.2572) Steps 0(0.00) | Grad Norm 8.3447(9.4319) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 18.9082(17.3840) | Bit/dim 3.7085(3.6998) | Xent 0.7627(0.7281) | Loss 9.5670(9.7586) | Error 0.2778(0.2570) Steps 0(0.00) | Grad Norm 14.7928(9.7242) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 16.7599(17.4112) | Bit/dim 3.6807(3.6976) | Xent 0.7383(0.7222) | Loss 9.0268(9.6221) | Error 0.2678(0.2550) Steps 0(0.00) | Grad Norm 8.0069(9.0273) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 17.6936(17.3915) | Bit/dim 3.6801(3.6958) | Xent 0.7789(0.7211) | Loss 9.3433(9.5426) | Error 0.2878(0.2558) Steps 0(0.00) | Grad Norm 7.5018(8.8819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 88.9786, Epoch Time 1068.0216(1017.6104), Bit/dim 3.6960(best: 3.6997), Xent 0.7715, Loss 4.0817, Error 0.2676(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 18.1797(17.2761) | Bit/dim 3.7224(3.6951) | Xent 0.7236(0.7228) | Loss 9.4095(10.1332) | Error 0.2467(0.2565) Steps 0(0.00) | Grad Norm 12.2750(9.3559) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 17.7553(17.1497) | Bit/dim 3.6847(3.6946) | Xent 0.7720(0.7213) | Loss 9.5482(9.9128) | Error 0.2778(0.2568) Steps 0(0.00) | Grad Norm 15.4005(9.4106) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 16.3501(17.1587) | Bit/dim 3.7296(3.6960) | Xent 0.8097(0.7192) | Loss 9.2176(9.7314) | Error 0.2856(0.2552) Steps 0(0.00) | Grad Norm 11.2741(9.2926) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 16.5114(17.2415) | Bit/dim 3.6921(3.6959) | Xent 0.7463(0.7243) | Loss 9.3423(9.6402) | Error 0.2711(0.2579) Steps 0(0.00) | Grad Norm 7.6067(9.1146) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 17.1751(17.1392) | Bit/dim 3.7052(3.6966) | Xent 0.7228(0.7219) | Loss 9.3149(9.5593) | Error 0.2556(0.2575) Steps 0(0.00) | Grad Norm 5.3096(9.3664) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 20.2942(17.2986) | Bit/dim 3.6768(3.6971) | Xent 0.7375(0.7162) | Loss 9.3972(9.4952) | Error 0.2567(0.2551) Steps 0(0.00) | Grad Norm 9.5917(9.0353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 89.4959, Epoch Time 1051.2203(1018.6187), Bit/dim 3.6941(best: 3.6960), Xent 0.7400, Loss 4.0641, Error 0.2574(best: 0.2628)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 17.2674(17.3203) | Bit/dim 3.6923(3.6946) | Xent 0.6904(0.7105) | Loss 9.4722(10.0145) | Error 0.2589(0.2539) Steps 0(0.00) | Grad Norm 7.5614(8.6882) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 17.2591(17.3332) | Bit/dim 3.7230(3.6950) | Xent 0.6984(0.7072) | Loss 9.2803(9.8219) | Error 0.2622(0.2526) Steps 0(0.00) | Grad Norm 6.8226(8.8255) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 18.8742(17.2959) | Bit/dim 3.6903(3.6956) | Xent 0.7495(0.7012) | Loss 9.4461(9.6950) | Error 0.2778(0.2502) Steps 0(0.00) | Grad Norm 10.2668(8.5407) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 17.2255(17.2151) | Bit/dim 3.6738(3.6934) | Xent 0.6633(0.7028) | Loss 9.2283(9.6022) | Error 0.2367(0.2507) Steps 0(0.00) | Grad Norm 9.7234(8.7570) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 17.1278(17.1351) | Bit/dim 3.7029(3.6925) | Xent 0.6956(0.7010) | Loss 9.2113(9.5190) | Error 0.2456(0.2493) Steps 0(0.00) | Grad Norm 7.9581(8.6107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 89.1406, Epoch Time 1055.7461(1019.7325), Bit/dim 3.6911(best: 3.6941), Xent 0.7608, Loss 4.0715, Error 0.2669(best: 0.2574)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 17.8354(17.3464) | Bit/dim 3.6624(3.6915) | Xent 0.7759(0.7027) | Loss 9.3309(10.0921) | Error 0.2711(0.2501) Steps 0(0.00) | Grad Norm 13.0173(8.7268) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 18.6076(17.5265) | Bit/dim 3.6663(3.6913) | Xent 0.6923(0.7000) | Loss 9.4630(9.8983) | Error 0.2556(0.2488) Steps 0(0.00) | Grad Norm 6.5478(8.6134) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 18.4988(17.5451) | Bit/dim 3.6797(3.6892) | Xent 0.6737(0.6921) | Loss 9.1994(9.7301) | Error 0.2311(0.2458) Steps 0(0.00) | Grad Norm 9.4044(8.3378) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 18.9688(17.5623) | Bit/dim 3.6778(3.6889) | Xent 0.7192(0.6909) | Loss 9.3875(9.6232) | Error 0.2678(0.2466) Steps 0(0.00) | Grad Norm 6.7034(8.0128) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 18.1102(17.5151) | Bit/dim 3.6763(3.6904) | Xent 0.7298(0.6956) | Loss 9.3916(9.5396) | Error 0.2478(0.2480) Steps 0(0.00) | Grad Norm 8.0679(8.0665) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 16.8248(17.4926) | Bit/dim 3.6904(3.6908) | Xent 0.6998(0.6978) | Loss 9.2078(9.4719) | Error 0.2567(0.2486) Steps 0(0.00) | Grad Norm 4.7788(7.9909) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 89.5295, Epoch Time 1075.9356(1021.4186), Bit/dim 3.6924(best: 3.6911), Xent 0.7348, Loss 4.0598, Error 0.2573(best: 0.2574)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 18.3053(17.7794) | Bit/dim 3.7134(3.6913) | Xent 0.7106(0.7005) | Loss 9.3182(9.9963) | Error 0.2544(0.2503) Steps 0(0.00) | Grad Norm 6.7708(8.5851) | Total Time 0.00(0.00)\n",
      "Iter 4420 | Time 20.3252(17.7745) | Bit/dim 3.7123(3.6890) | Xent 0.6719(0.6996) | Loss 9.3470(9.8115) | Error 0.2278(0.2504) Steps 0(0.00) | Grad Norm 8.6319(8.8020) | Total Time 0.00(0.00)\n",
      "Iter 4430 | Time 18.6418(17.6399) | Bit/dim 3.6954(3.6919) | Xent 0.7384(0.6945) | Loss 9.2409(9.6730) | Error 0.2578(0.2488) Steps 0(0.00) | Grad Norm 8.3327(8.5050) | Total Time 0.00(0.00)\n",
      "Iter 4440 | Time 17.2412(17.6091) | Bit/dim 3.7008(3.6929) | Xent 0.7509(0.6949) | Loss 9.2661(9.5862) | Error 0.2600(0.2497) Steps 0(0.00) | Grad Norm 10.6753(8.4051) | Total Time 0.00(0.00)\n",
      "Iter 4450 | Time 16.1492(17.4490) | Bit/dim 3.6574(3.6883) | Xent 0.6848(0.6956) | Loss 9.2048(9.4794) | Error 0.2411(0.2489) Steps 0(0.00) | Grad Norm 6.9388(8.2720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 88.9376, Epoch Time 1073.4036(1022.9781), Bit/dim 3.6928(best: 3.6911), Xent 0.7381, Loss 4.0619, Error 0.2556(best: 0.2573)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 18.0656(17.5381) | Bit/dim 3.6620(3.6881) | Xent 0.7287(0.6961) | Loss 9.3134(10.0993) | Error 0.2767(0.2492) Steps 0(0.00) | Grad Norm 11.0219(8.3562) | Total Time 0.00(0.00)\n",
      "Iter 4470 | Time 18.0450(17.4954) | Bit/dim 3.6645(3.6900) | Xent 0.7194(0.6913) | Loss 9.3423(9.8943) | Error 0.2633(0.2468) Steps 0(0.00) | Grad Norm 17.8831(8.9488) | Total Time 0.00(0.00)\n",
      "Iter 4480 | Time 16.9342(17.4677) | Bit/dim 3.6921(3.6932) | Xent 0.7202(0.7007) | Loss 9.0668(9.7498) | Error 0.2678(0.2502) Steps 0(0.00) | Grad Norm 9.7366(9.1405) | Total Time 0.00(0.00)\n",
      "Iter 4490 | Time 17.6079(17.4897) | Bit/dim 3.7004(3.6917) | Xent 0.6990(0.6953) | Loss 9.2651(9.6215) | Error 0.2489(0.2480) Steps 0(0.00) | Grad Norm 7.3092(9.2124) | Total Time 0.00(0.00)\n",
      "Iter 4500 | Time 15.8738(17.3851) | Bit/dim 3.7161(3.6888) | Xent 0.7216(0.6969) | Loss 9.4158(9.5478) | Error 0.2589(0.2481) Steps 0(0.00) | Grad Norm 8.0123(8.8294) | Total Time 0.00(0.00)\n",
      "Iter 4510 | Time 17.2540(17.3568) | Bit/dim 3.6651(3.6853) | Xent 0.6444(0.6929) | Loss 9.2133(9.4616) | Error 0.2356(0.2466) Steps 0(0.00) | Grad Norm 9.2646(8.5244) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 89.4889, Epoch Time 1065.4373(1024.2519), Bit/dim 3.6868(best: 3.6911), Xent 0.7119, Loss 4.0427, Error 0.2456(best: 0.2556)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 16.8003(17.4640) | Bit/dim 3.6592(3.6816) | Xent 0.6761(0.6810) | Loss 9.3100(9.9881) | Error 0.2333(0.2429) Steps 0(0.00) | Grad Norm 6.7120(8.1675) | Total Time 0.00(0.00)\n",
      "Iter 4530 | Time 17.4215(17.2715) | Bit/dim 3.6777(3.6819) | Xent 0.6974(0.6832) | Loss 9.0709(9.7820) | Error 0.2478(0.2425) Steps 0(0.00) | Grad Norm 9.8484(8.1004) | Total Time 0.00(0.00)\n",
      "Iter 4540 | Time 17.3349(17.2593) | Bit/dim 3.7009(3.6810) | Xent 0.6859(0.6804) | Loss 9.2930(9.6198) | Error 0.2444(0.2418) Steps 0(0.00) | Grad Norm 14.5026(8.4768) | Total Time 0.00(0.00)\n",
      "Iter 4550 | Time 20.9657(17.3737) | Bit/dim 3.6865(3.6862) | Xent 0.6761(0.6817) | Loss 9.4502(9.5432) | Error 0.2411(0.2424) Steps 0(0.00) | Grad Norm 6.8952(8.8113) | Total Time 0.00(0.00)\n",
      "Iter 4560 | Time 15.2119(17.3846) | Bit/dim 3.6971(3.6863) | Xent 0.6413(0.6836) | Loss 9.1550(9.4651) | Error 0.2178(0.2429) Steps 0(0.00) | Grad Norm 8.7755(8.9895) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 89.8276, Epoch Time 1066.4937(1025.5192), Bit/dim 3.6960(best: 3.6868), Xent 0.7268, Loss 4.0594, Error 0.2561(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 18.8914(17.5405) | Bit/dim 3.6644(3.6875) | Xent 0.6958(0.6799) | Loss 9.3934(10.0895) | Error 0.2489(0.2410) Steps 0(0.00) | Grad Norm 4.7104(8.8761) | Total Time 0.00(0.00)\n",
      "Iter 4580 | Time 16.1869(17.3813) | Bit/dim 3.6944(3.6856) | Xent 0.6845(0.6789) | Loss 9.0656(9.8713) | Error 0.2456(0.2410) Steps 0(0.00) | Grad Norm 10.3390(8.6659) | Total Time 0.00(0.00)\n",
      "Iter 4590 | Time 18.3443(17.4155) | Bit/dim 3.6935(3.6869) | Xent 0.7203(0.6849) | Loss 9.3942(9.7375) | Error 0.2433(0.2416) Steps 0(0.00) | Grad Norm 9.9554(9.0950) | Total Time 0.00(0.00)\n",
      "Iter 4600 | Time 16.4081(17.2625) | Bit/dim 3.6919(3.6854) | Xent 0.6984(0.7016) | Loss 9.1711(9.6191) | Error 0.2556(0.2469) Steps 0(0.00) | Grad Norm 7.6056(9.2375) | Total Time 0.00(0.00)\n",
      "Iter 4610 | Time 19.6984(17.4244) | Bit/dim 3.7193(3.6878) | Xent 0.7152(0.7038) | Loss 9.5811(9.5428) | Error 0.2556(0.2491) Steps 0(0.00) | Grad Norm 9.5574(9.2046) | Total Time 0.00(0.00)\n",
      "Iter 4620 | Time 16.9483(17.3849) | Bit/dim 3.6784(3.6858) | Xent 0.6823(0.6993) | Loss 9.3405(9.4550) | Error 0.2400(0.2472) Steps 0(0.00) | Grad Norm 6.7011(8.5394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 90.1864, Epoch Time 1062.2742(1026.6218), Bit/dim 3.6886(best: 3.6868), Xent 0.7439, Loss 4.0605, Error 0.2579(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 16.7353(17.3944) | Bit/dim 3.6902(3.6860) | Xent 0.6072(0.6885) | Loss 9.2749(9.9859) | Error 0.2111(0.2439) Steps 0(0.00) | Grad Norm 4.8487(7.9471) | Total Time 0.00(0.00)\n",
      "Iter 4640 | Time 15.7613(17.3625) | Bit/dim 3.6689(3.6829) | Xent 0.6322(0.6786) | Loss 8.9694(9.7833) | Error 0.2211(0.2404) Steps 0(0.00) | Grad Norm 8.1528(7.8157) | Total Time 0.00(0.00)\n",
      "Iter 4650 | Time 17.0099(17.3263) | Bit/dim 3.6973(3.6826) | Xent 0.7278(0.6732) | Loss 9.3159(9.6507) | Error 0.2556(0.2396) Steps 0(0.00) | Grad Norm 7.4643(7.6657) | Total Time 0.00(0.00)\n",
      "Iter 4660 | Time 15.8464(17.3254) | Bit/dim 3.7048(3.6831) | Xent 0.7473(0.6727) | Loss 9.4460(9.5612) | Error 0.2567(0.2385) Steps 0(0.00) | Grad Norm 6.1963(7.5651) | Total Time 0.00(0.00)\n",
      "Iter 4670 | Time 16.8145(17.3259) | Bit/dim 3.6632(3.6795) | Xent 0.6516(0.6697) | Loss 9.2729(9.4850) | Error 0.2222(0.2376) Steps 0(0.00) | Grad Norm 6.8160(7.6236) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 88.7442, Epoch Time 1060.1843(1027.6287), Bit/dim 3.6819(best: 3.6868), Xent 0.7360, Loss 4.0499, Error 0.2615(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 17.5753(17.3080) | Bit/dim 3.6605(3.6773) | Xent 0.6795(0.6785) | Loss 9.3169(10.0831) | Error 0.2289(0.2401) Steps 0(0.00) | Grad Norm 12.9813(8.5321) | Total Time 0.00(0.00)\n",
      "Iter 4690 | Time 17.3322(17.3986) | Bit/dim 3.6837(3.6749) | Xent 0.6848(0.6773) | Loss 9.4615(9.8672) | Error 0.2544(0.2400) Steps 0(0.00) | Grad Norm 11.4624(8.8761) | Total Time 0.00(0.00)\n",
      "Iter 4700 | Time 15.9646(17.3271) | Bit/dim 3.6947(3.6762) | Xent 0.6688(0.6743) | Loss 9.3367(9.7034) | Error 0.2411(0.2393) Steps 0(0.00) | Grad Norm 8.8758(8.4072) | Total Time 0.00(0.00)\n",
      "Iter 4710 | Time 18.1860(17.2856) | Bit/dim 3.6902(3.6794) | Xent 0.6591(0.6804) | Loss 9.2521(9.5933) | Error 0.2278(0.2414) Steps 0(0.00) | Grad Norm 6.8458(8.8702) | Total Time 0.00(0.00)\n",
      "Iter 4720 | Time 17.5960(17.4062) | Bit/dim 3.6706(3.6805) | Xent 0.6276(0.6754) | Loss 9.3598(9.5206) | Error 0.2156(0.2393) Steps 0(0.00) | Grad Norm 5.9574(8.6488) | Total Time 0.00(0.00)\n",
      "Iter 4730 | Time 19.1496(17.4812) | Bit/dim 3.6546(3.6839) | Xent 0.6514(0.6749) | Loss 9.3253(9.4691) | Error 0.2267(0.2411) Steps 0(0.00) | Grad Norm 8.8856(8.8303) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 90.1924, Epoch Time 1066.0510(1028.7813), Bit/dim 3.6853(best: 3.6819), Xent 0.7179, Loss 4.0443, Error 0.2521(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 16.8081(17.4997) | Bit/dim 3.6780(3.6812) | Xent 0.5789(0.6693) | Loss 9.1213(9.9621) | Error 0.2144(0.2384) Steps 0(0.00) | Grad Norm 4.8707(8.1352) | Total Time 0.00(0.00)\n",
      "Iter 4750 | Time 16.6258(17.4724) | Bit/dim 3.6700(3.6802) | Xent 0.6325(0.6542) | Loss 9.0047(9.7556) | Error 0.2100(0.2329) Steps 0(0.00) | Grad Norm 4.6664(7.2418) | Total Time 0.00(0.00)\n",
      "Iter 4760 | Time 16.9855(17.4859) | Bit/dim 3.6201(3.6764) | Xent 0.6619(0.6563) | Loss 8.9636(9.6205) | Error 0.2378(0.2331) Steps 0(0.00) | Grad Norm 9.1233(7.2457) | Total Time 0.00(0.00)\n",
      "Iter 4770 | Time 17.6366(17.5243) | Bit/dim 3.6703(3.6775) | Xent 0.6410(0.6512) | Loss 9.4667(9.5424) | Error 0.2344(0.2327) Steps 0(0.00) | Grad Norm 4.9562(7.3920) | Total Time 0.00(0.00)\n",
      "Iter 4780 | Time 17.1463(17.4733) | Bit/dim 3.6671(3.6786) | Xent 0.6918(0.6589) | Loss 8.8648(9.4640) | Error 0.2700(0.2345) Steps 0(0.00) | Grad Norm 8.0766(7.3831) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 88.3821, Epoch Time 1066.8621(1029.9238), Bit/dim 3.6765(best: 3.6819), Xent 0.7122, Loss 4.0326, Error 0.2475(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 17.4266(17.3919) | Bit/dim 3.6497(3.6767) | Xent 0.6282(0.6533) | Loss 9.1987(10.0594) | Error 0.2122(0.2318) Steps 0(0.00) | Grad Norm 8.8574(7.4676) | Total Time 0.00(0.00)\n",
      "Iter 4800 | Time 18.0773(17.3845) | Bit/dim 3.6627(3.6764) | Xent 0.6236(0.6512) | Loss 9.1126(9.8386) | Error 0.2322(0.2320) Steps 0(0.00) | Grad Norm 10.3722(7.4463) | Total Time 0.00(0.00)\n",
      "Iter 4810 | Time 17.3804(17.6735) | Bit/dim 3.6609(3.6733) | Xent 0.6820(0.6461) | Loss 9.2948(9.6900) | Error 0.2411(0.2299) Steps 0(0.00) | Grad Norm 10.8592(7.7416) | Total Time 0.00(0.00)\n",
      "Iter 4820 | Time 18.6274(17.7768) | Bit/dim 3.6905(3.6735) | Xent 0.6452(0.6471) | Loss 9.2994(9.5743) | Error 0.2300(0.2302) Steps 0(0.00) | Grad Norm 6.8072(7.7592) | Total Time 0.00(0.00)\n",
      "Iter 4830 | Time 18.8205(17.8185) | Bit/dim 3.6945(3.6727) | Xent 0.6446(0.6436) | Loss 9.4968(9.4870) | Error 0.2233(0.2285) Steps 0(0.00) | Grad Norm 8.9935(7.4388) | Total Time 0.00(0.00)\n",
      "Iter 4840 | Time 16.6186(17.7521) | Bit/dim 3.6894(3.6737) | Xent 0.6870(0.6542) | Loss 9.1361(9.4167) | Error 0.2378(0.2329) Steps 0(0.00) | Grad Norm 11.0683(8.2290) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 90.2289, Epoch Time 1087.1590(1031.6408), Bit/dim 3.6743(best: 3.6765), Xent 0.7334, Loss 4.0411, Error 0.2552(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 19.3478(17.7856) | Bit/dim 3.6873(3.6749) | Xent 0.6669(0.6488) | Loss 9.3671(9.9761) | Error 0.2289(0.2315) Steps 0(0.00) | Grad Norm 6.3301(7.9890) | Total Time 0.00(0.00)\n",
      "Iter 4860 | Time 18.1262(17.6243) | Bit/dim 3.6427(3.6738) | Xent 0.6176(0.6533) | Loss 8.9899(9.7662) | Error 0.2267(0.2324) Steps 0(0.00) | Grad Norm 6.6467(8.3554) | Total Time 0.00(0.00)\n",
      "Iter 4870 | Time 17.1873(17.6499) | Bit/dim 3.6583(3.6743) | Xent 0.5765(0.6524) | Loss 9.1464(9.6408) | Error 0.2111(0.2334) Steps 0(0.00) | Grad Norm 6.1503(8.3815) | Total Time 0.00(0.00)\n",
      "Iter 4880 | Time 17.1401(17.5662) | Bit/dim 3.6561(3.6721) | Xent 0.7187(0.6598) | Loss 9.2243(9.5503) | Error 0.2533(0.2351) Steps 0(0.00) | Grad Norm 15.1939(8.6290) | Total Time 0.00(0.00)\n",
      "Iter 4890 | Time 17.1560(17.5251) | Bit/dim 3.7039(3.6721) | Xent 0.5900(0.6546) | Loss 9.2309(9.4585) | Error 0.2000(0.2326) Steps 0(0.00) | Grad Norm 5.8199(8.2349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 91.0554, Epoch Time 1071.6677(1032.8416), Bit/dim 3.6731(best: 3.6743), Xent 0.6989, Loss 4.0225, Error 0.2458(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 15.7678(17.4993) | Bit/dim 3.6829(3.6733) | Xent 0.6489(0.6482) | Loss 9.0898(10.0679) | Error 0.2322(0.2305) Steps 0(0.00) | Grad Norm 7.2646(7.7572) | Total Time 0.00(0.00)\n",
      "Iter 4910 | Time 17.0814(17.4879) | Bit/dim 3.6766(3.6717) | Xent 0.6868(0.6478) | Loss 9.0090(9.8298) | Error 0.2533(0.2310) Steps 0(0.00) | Grad Norm 15.1937(8.6636) | Total Time 0.00(0.00)\n",
      "Iter 4920 | Time 17.6831(17.4309) | Bit/dim 3.6761(3.6741) | Xent 0.7032(0.6625) | Loss 9.3213(9.6978) | Error 0.2511(0.2356) Steps 0(0.00) | Grad Norm 10.6604(9.3080) | Total Time 0.00(0.00)\n",
      "Iter 4930 | Time 16.3464(17.4536) | Bit/dim 3.6571(3.6745) | Xent 0.6379(0.6658) | Loss 9.1007(9.5890) | Error 0.2311(0.2368) Steps 0(0.00) | Grad Norm 5.4790(8.9021) | Total Time 0.00(0.00)\n",
      "Iter 4940 | Time 16.1479(17.3553) | Bit/dim 3.6953(3.6761) | Xent 0.6384(0.6641) | Loss 9.1004(9.4925) | Error 0.2189(0.2366) Steps 0(0.00) | Grad Norm 5.6817(8.8175) | Total Time 0.00(0.00)\n",
      "Iter 4950 | Time 17.0421(17.4443) | Bit/dim 3.6632(3.6748) | Xent 0.6170(0.6616) | Loss 9.1224(9.4270) | Error 0.2200(0.2357) Steps 0(0.00) | Grad Norm 7.0052(8.6243) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 90.3553, Epoch Time 1064.7482(1033.7988), Bit/dim 3.6791(best: 3.6731), Xent 0.7028, Loss 4.0305, Error 0.2514(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 16.9578(17.3884) | Bit/dim 3.6518(3.6739) | Xent 0.6648(0.6539) | Loss 9.1997(9.9449) | Error 0.2367(0.2331) Steps 0(0.00) | Grad Norm 11.6280(8.6406) | Total Time 0.00(0.00)\n",
      "Iter 4970 | Time 16.8805(17.2432) | Bit/dim 3.6490(3.6736) | Xent 0.5889(0.6510) | Loss 9.1136(9.7325) | Error 0.2089(0.2335) Steps 0(0.00) | Grad Norm 4.7134(8.4643) | Total Time 0.00(0.00)\n",
      "Iter 4980 | Time 19.3705(17.3061) | Bit/dim 3.6639(3.6736) | Xent 0.5874(0.6365) | Loss 9.0395(9.5955) | Error 0.2100(0.2279) Steps 0(0.00) | Grad Norm 5.3391(7.8530) | Total Time 0.00(0.00)\n",
      "Iter 4990 | Time 16.9392(17.3376) | Bit/dim 3.7104(3.6753) | Xent 0.5889(0.6309) | Loss 9.3470(9.5025) | Error 0.2133(0.2264) Steps 0(0.00) | Grad Norm 6.8101(7.6224) | Total Time 0.00(0.00)\n",
      "Iter 5000 | Time 19.9948(17.3233) | Bit/dim 3.6491(3.6716) | Xent 0.6380(0.6312) | Loss 9.1146(9.4057) | Error 0.2222(0.2256) Steps 0(0.00) | Grad Norm 5.6623(7.0005) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 89.5001, Epoch Time 1058.7042(1034.5460), Bit/dim 3.6708(best: 3.6731), Xent 0.7037, Loss 4.0226, Error 0.2452(best: 0.2456)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 17.9854(17.3894) | Bit/dim 3.6677(3.6678) | Xent 0.5976(0.6295) | Loss 9.2819(10.0001) | Error 0.2122(0.2241) Steps 0(0.00) | Grad Norm 6.6751(7.2519) | Total Time 0.00(0.00)\n",
      "Iter 5020 | Time 16.7081(17.3249) | Bit/dim 3.6990(3.6701) | Xent 0.6477(0.6298) | Loss 9.2578(9.7908) | Error 0.2433(0.2264) Steps 0(0.00) | Grad Norm 10.1577(7.9525) | Total Time 0.00(0.00)\n",
      "Iter 5030 | Time 18.8851(17.5336) | Bit/dim 3.6834(3.6729) | Xent 0.6301(0.6346) | Loss 9.3553(9.6534) | Error 0.2244(0.2287) Steps 0(0.00) | Grad Norm 9.3138(8.6263) | Total Time 0.00(0.00)\n",
      "Iter 5040 | Time 17.9010(17.5173) | Bit/dim 3.7017(3.6727) | Xent 0.6115(0.6355) | Loss 9.1670(9.5433) | Error 0.2189(0.2281) Steps 0(0.00) | Grad Norm 6.4453(8.2552) | Total Time 0.00(0.00)\n",
      "Iter 5050 | Time 17.1476(17.6250) | Bit/dim 3.7075(3.6716) | Xent 0.6422(0.6342) | Loss 9.3257(9.4600) | Error 0.2422(0.2270) Steps 0(0.00) | Grad Norm 8.1078(8.0721) | Total Time 0.00(0.00)\n",
      "Iter 5060 | Time 16.5023(17.7522) | Bit/dim 3.6477(3.6695) | Xent 0.6677(0.6390) | Loss 9.0141(9.4006) | Error 0.2267(0.2286) Steps 0(0.00) | Grad Norm 10.9466(8.3865) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 90.2877, Epoch Time 1081.1725(1035.9448), Bit/dim 3.6712(best: 3.6708), Xent 0.7622, Loss 4.0524, Error 0.2649(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 17.9644(17.8489) | Bit/dim 3.6664(3.6712) | Xent 0.5609(0.6341) | Loss 9.0681(9.9329) | Error 0.2133(0.2266) Steps 0(0.00) | Grad Norm 7.6423(8.6582) | Total Time 0.00(0.00)\n",
      "Iter 5080 | Time 17.2608(17.7379) | Bit/dim 3.6899(3.6722) | Xent 0.6363(0.6312) | Loss 9.1452(9.7416) | Error 0.2256(0.2258) Steps 0(0.00) | Grad Norm 6.9792(8.4340) | Total Time 0.00(0.00)\n",
      "Iter 5090 | Time 19.3610(17.8482) | Bit/dim 3.6747(3.6688) | Xent 0.5970(0.6286) | Loss 9.0481(9.5891) | Error 0.2167(0.2247) Steps 0(0.00) | Grad Norm 7.4987(8.2967) | Total Time 0.00(0.00)\n",
      "Iter 5100 | Time 18.0770(17.8881) | Bit/dim 3.6728(3.6695) | Xent 0.5929(0.6208) | Loss 9.0996(9.4834) | Error 0.2111(0.2210) Steps 0(0.00) | Grad Norm 8.1137(8.0530) | Total Time 0.00(0.00)\n",
      "Iter 5110 | Time 17.7473(17.6929) | Bit/dim 3.6746(3.6674) | Xent 0.5996(0.6187) | Loss 9.2661(9.3804) | Error 0.2078(0.2195) Steps 0(0.00) | Grad Norm 6.5409(7.8443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 89.7854, Epoch Time 1088.4031(1037.5185), Bit/dim 3.6668(best: 3.6708), Xent 0.7001, Loss 4.0169, Error 0.2456(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 17.3262(17.7994) | Bit/dim 3.6856(3.6681) | Xent 0.6042(0.6170) | Loss 9.4125(9.9912) | Error 0.2122(0.2193) Steps 0(0.00) | Grad Norm 7.4864(7.6163) | Total Time 0.00(0.00)\n",
      "Iter 5130 | Time 16.6082(17.7074) | Bit/dim 3.6266(3.6655) | Xent 0.5954(0.6183) | Loss 9.0342(9.7793) | Error 0.2089(0.2200) Steps 0(0.00) | Grad Norm 6.2789(7.7042) | Total Time 0.00(0.00)\n",
      "Iter 5140 | Time 16.5398(17.7690) | Bit/dim 3.6391(3.6651) | Xent 0.7457(0.6241) | Loss 8.8160(9.6146) | Error 0.2767(0.2219) Steps 0(0.00) | Grad Norm 12.7585(8.3441) | Total Time 0.00(0.00)\n",
      "Iter 5150 | Time 17.2441(17.6388) | Bit/dim 3.6538(3.6697) | Xent 0.6611(0.6294) | Loss 9.2757(9.5166) | Error 0.2378(0.2232) Steps 0(0.00) | Grad Norm 5.6502(8.3168) | Total Time 0.00(0.00)\n",
      "Iter 5160 | Time 16.6426(17.4254) | Bit/dim 3.6437(3.6676) | Xent 0.5944(0.6310) | Loss 8.9533(9.4163) | Error 0.2144(0.2242) Steps 0(0.00) | Grad Norm 7.2833(8.3782) | Total Time 0.00(0.00)\n",
      "Iter 5170 | Time 17.0116(17.4657) | Bit/dim 3.6767(3.6687) | Xent 0.6863(0.6288) | Loss 9.2018(9.3689) | Error 0.2544(0.2230) Steps 0(0.00) | Grad Norm 12.3323(8.2554) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 90.1808, Epoch Time 1067.4207(1038.4156), Bit/dim 3.6800(best: 3.6668), Xent 0.7038, Loss 4.0319, Error 0.2515(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 17.4967(17.4678) | Bit/dim 3.6860(3.6702) | Xent 0.5662(0.6218) | Loss 9.1881(9.8803) | Error 0.2067(0.2210) Steps 0(0.00) | Grad Norm 6.5826(8.4106) | Total Time 0.00(0.00)\n",
      "Iter 5190 | Time 16.6302(17.5009) | Bit/dim 3.6757(3.6703) | Xent 0.5232(0.6142) | Loss 8.9927(9.6883) | Error 0.1778(0.2180) Steps 0(0.00) | Grad Norm 7.2867(8.0213) | Total Time 0.00(0.00)\n",
      "Iter 5200 | Time 17.3771(17.6145) | Bit/dim 3.6447(3.6680) | Xent 0.6417(0.6098) | Loss 8.9590(9.5556) | Error 0.2467(0.2176) Steps 0(0.00) | Grad Norm 8.4313(7.7656) | Total Time 0.00(0.00)\n",
      "Iter 5210 | Time 16.9834(17.5054) | Bit/dim 3.6651(3.6668) | Xent 0.6815(0.6207) | Loss 9.3448(9.4718) | Error 0.2378(0.2206) Steps 0(0.00) | Grad Norm 15.2012(8.5094) | Total Time 0.00(0.00)\n",
      "Iter 5220 | Time 18.8239(17.5595) | Bit/dim 3.6818(3.6628) | Xent 0.7280(0.6270) | Loss 9.3427(9.3981) | Error 0.2600(0.2235) Steps 0(0.00) | Grad Norm 7.7773(8.2727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 90.9647, Epoch Time 1077.8584(1039.5989), Bit/dim 3.6623(best: 3.6668), Xent 0.7113, Loss 4.0179, Error 0.2523(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 18.2911(17.6732) | Bit/dim 3.6587(3.6658) | Xent 0.6065(0.6254) | Loss 9.1586(10.0384) | Error 0.2333(0.2238) Steps 0(0.00) | Grad Norm 5.8555(8.3000) | Total Time 0.00(0.00)\n",
      "Iter 5240 | Time 17.8277(17.5926) | Bit/dim 3.6580(3.6625) | Xent 0.7010(0.6284) | Loss 9.0932(9.8018) | Error 0.2544(0.2262) Steps 0(0.00) | Grad Norm 10.8819(8.5830) | Total Time 0.00(0.00)\n",
      "Iter 5250 | Time 16.9815(17.7482) | Bit/dim 3.6919(3.6653) | Xent 0.6022(0.6281) | Loss 9.3317(9.6459) | Error 0.2156(0.2241) Steps 0(0.00) | Grad Norm 6.6938(8.2304) | Total Time 0.00(0.00)\n",
      "Iter 5260 | Time 18.0998(17.8048) | Bit/dim 3.6547(3.6645) | Xent 0.6080(0.6237) | Loss 9.1481(9.5193) | Error 0.2133(0.2220) Steps 0(0.00) | Grad Norm 6.2450(7.9216) | Total Time 0.00(0.00)\n",
      "Iter 5270 | Time 18.2273(17.7956) | Bit/dim 3.6622(3.6617) | Xent 0.6048(0.6168) | Loss 9.1375(9.4149) | Error 0.2067(0.2199) Steps 0(0.00) | Grad Norm 7.4469(7.6286) | Total Time 0.00(0.00)\n",
      "Iter 5280 | Time 17.4483(17.7435) | Bit/dim 3.6608(3.6655) | Xent 0.5788(0.6104) | Loss 9.2011(9.3309) | Error 0.1989(0.2180) Steps 0(0.00) | Grad Norm 6.8507(7.6130) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 90.7603, Epoch Time 1086.8301(1041.0158), Bit/dim 3.6573(best: 3.6623), Xent 0.7130, Loss 4.0138, Error 0.2491(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 17.5543(17.7391) | Bit/dim 3.6723(3.6638) | Xent 0.5747(0.6068) | Loss 9.3047(9.8661) | Error 0.2178(0.2175) Steps 0(0.00) | Grad Norm 7.5819(7.6780) | Total Time 0.00(0.00)\n",
      "Iter 5300 | Time 19.4386(17.8460) | Bit/dim 3.6626(3.6654) | Xent 0.6166(0.6023) | Loss 9.2382(9.6932) | Error 0.2156(0.2141) Steps 0(0.00) | Grad Norm 10.4182(8.0813) | Total Time 0.00(0.00)\n",
      "Iter 5310 | Time 18.1975(17.7457) | Bit/dim 3.6720(3.6657) | Xent 0.5839(0.6030) | Loss 9.0962(9.5503) | Error 0.2111(0.2145) Steps 0(0.00) | Grad Norm 7.1381(7.9380) | Total Time 0.00(0.00)\n",
      "Iter 5320 | Time 18.5044(17.7934) | Bit/dim 3.6792(3.6635) | Xent 0.6558(0.6045) | Loss 9.3021(9.4445) | Error 0.2278(0.2146) Steps 0(0.00) | Grad Norm 13.1372(8.1279) | Total Time 0.00(0.00)\n",
      "Iter 5330 | Time 16.7928(17.8449) | Bit/dim 3.7292(3.6637) | Xent 0.6310(0.6086) | Loss 9.1220(9.3770) | Error 0.2189(0.2162) Steps 0(0.00) | Grad Norm 6.3797(8.2568) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 90.8201, Epoch Time 1090.6397(1042.5045), Bit/dim 3.6658(best: 3.6573), Xent 0.6993, Loss 4.0155, Error 0.2434(best: 0.2452)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 17.9950(17.7977) | Bit/dim 3.6754(3.6640) | Xent 0.6436(0.6050) | Loss 9.2362(9.9930) | Error 0.2389(0.2160) Steps 0(0.00) | Grad Norm 10.0411(8.2734) | Total Time 0.00(0.00)\n",
      "Iter 5350 | Time 16.4315(17.9302) | Bit/dim 3.6732(3.6629) | Xent 0.5704(0.6066) | Loss 9.2402(9.7734) | Error 0.1978(0.2150) Steps 0(0.00) | Grad Norm 5.9637(8.0891) | Total Time 0.00(0.00)\n",
      "Iter 5360 | Time 17.4737(17.9323) | Bit/dim 3.6661(3.6630) | Xent 0.6531(0.6059) | Loss 9.1613(9.6257) | Error 0.2422(0.2149) Steps 0(0.00) | Grad Norm 7.3770(7.7571) | Total Time 0.00(0.00)\n",
      "Iter 5370 | Time 16.9948(18.0083) | Bit/dim 3.6379(3.6626) | Xent 0.5512(0.5919) | Loss 8.9685(9.4873) | Error 0.2056(0.2113) Steps 0(0.00) | Grad Norm 4.6560(7.4844) | Total Time 0.00(0.00)\n",
      "Iter 5380 | Time 17.5417(18.0419) | Bit/dim 3.6758(3.6661) | Xent 0.5801(0.6014) | Loss 9.0827(9.4217) | Error 0.2133(0.2149) Steps 0(0.00) | Grad Norm 6.5373(8.3885) | Total Time 0.00(0.00)\n",
      "Iter 5390 | Time 18.8474(18.0507) | Bit/dim 3.6048(3.6614) | Xent 0.6537(0.6098) | Loss 9.2290(9.3687) | Error 0.2433(0.2179) Steps 0(0.00) | Grad Norm 6.1950(8.6000) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 91.0013, Epoch Time 1102.8662(1044.3154), Bit/dim 3.6601(best: 3.6573), Xent 0.7091, Loss 4.0147, Error 0.2474(best: 0.2434)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 18.2812(17.9484) | Bit/dim 3.6609(3.6592) | Xent 0.5428(0.6003) | Loss 9.1431(9.8826) | Error 0.1944(0.2134) Steps 0(0.00) | Grad Norm 5.5306(7.9902) | Total Time 0.00(0.00)\n",
      "Iter 5410 | Time 18.6031(17.9140) | Bit/dim 3.6603(3.6606) | Xent 0.6580(0.5972) | Loss 9.1803(9.6683) | Error 0.2456(0.2133) Steps 0(0.00) | Grad Norm 7.2373(7.7800) | Total Time 0.00(0.00)\n",
      "Iter 5420 | Time 17.7602(17.8790) | Bit/dim 3.6426(3.6590) | Xent 0.5835(0.5929) | Loss 9.1596(9.5326) | Error 0.2256(0.2138) Steps 0(0.00) | Grad Norm 6.1640(7.5887) | Total Time 0.00(0.00)\n",
      "Iter 5430 | Time 17.6302(17.7007) | Bit/dim 3.6446(3.6589) | Xent 0.5703(0.5972) | Loss 9.0514(9.4403) | Error 0.1922(0.2141) Steps 0(0.00) | Grad Norm 9.3478(7.7102) | Total Time 0.00(0.00)\n",
      "Iter 5440 | Time 17.1103(17.5763) | Bit/dim 3.6897(3.6590) | Xent 0.5935(0.6010) | Loss 9.0675(9.3628) | Error 0.2300(0.2155) Steps 0(0.00) | Grad Norm 9.2277(8.2749) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 90.7484, Epoch Time 1075.0820(1045.2384), Bit/dim 3.6617(best: 3.6573), Xent 0.6750, Loss 3.9991, Error 0.2340(best: 0.2434)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 17.8053(17.6435) | Bit/dim 3.6624(3.6590) | Xent 0.5743(0.6039) | Loss 9.1250(9.9964) | Error 0.1956(0.2161) Steps 0(0.00) | Grad Norm 8.6667(8.1988) | Total Time 0.00(0.00)\n",
      "Iter 5460 | Time 19.5593(17.6845) | Bit/dim 3.6746(3.6609) | Xent 0.5852(0.5981) | Loss 9.2609(9.7942) | Error 0.2011(0.2141) Steps 0(0.00) | Grad Norm 10.5451(8.3745) | Total Time 0.00(0.00)\n",
      "Iter 5470 | Time 17.3233(17.6483) | Bit/dim 3.6643(3.6646) | Xent 0.6224(0.6013) | Loss 9.1795(9.6310) | Error 0.2233(0.2151) Steps 0(0.00) | Grad Norm 6.0574(8.8521) | Total Time 0.00(0.00)\n",
      "Iter 5480 | Time 18.4055(17.6822) | Bit/dim 3.6294(3.6642) | Xent 0.6445(0.6005) | Loss 9.1676(9.5033) | Error 0.2311(0.2155) Steps 0(0.00) | Grad Norm 7.2161(8.1957) | Total Time 0.00(0.00)\n",
      "Iter 5490 | Time 18.6833(17.7729) | Bit/dim 3.6296(3.6604) | Xent 0.5794(0.5877) | Loss 9.2321(9.4159) | Error 0.2100(0.2110) Steps 0(0.00) | Grad Norm 5.5972(7.5747) | Total Time 0.00(0.00)\n",
      "Iter 5500 | Time 17.9002(17.8690) | Bit/dim 3.6409(3.6585) | Xent 0.6314(0.5889) | Loss 9.1015(9.3451) | Error 0.2122(0.2109) Steps 0(0.00) | Grad Norm 10.3831(7.3900) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 90.6575, Epoch Time 1088.2182(1046.5278), Bit/dim 3.6668(best: 3.6573), Xent 0.6882, Loss 4.0109, Error 0.2418(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 17.5985(17.8715) | Bit/dim 3.6728(3.6567) | Xent 0.5731(0.5808) | Loss 9.0382(9.8263) | Error 0.2089(0.2083) Steps 0(0.00) | Grad Norm 11.9329(7.6678) | Total Time 0.00(0.00)\n",
      "Iter 5520 | Time 17.2853(18.0254) | Bit/dim 3.6514(3.6550) | Xent 0.6073(0.5764) | Loss 9.0290(9.6346) | Error 0.2256(0.2073) Steps 0(0.00) | Grad Norm 7.9874(7.4954) | Total Time 0.00(0.00)\n",
      "Iter 5530 | Time 18.3426(17.9546) | Bit/dim 3.6698(3.6565) | Xent 0.6189(0.5819) | Loss 9.2158(9.5218) | Error 0.2156(0.2077) Steps 0(0.00) | Grad Norm 9.6912(7.9835) | Total Time 0.00(0.00)\n",
      "Iter 5540 | Time 17.3045(17.7773) | Bit/dim 3.6408(3.6570) | Xent 0.5629(0.5863) | Loss 8.7531(9.3954) | Error 0.1989(0.2100) Steps 0(0.00) | Grad Norm 6.4636(7.9321) | Total Time 0.00(0.00)\n",
      "Iter 5550 | Time 18.9401(17.7949) | Bit/dim 3.6592(3.6585) | Xent 0.5779(0.5851) | Loss 9.1510(9.3455) | Error 0.2078(0.2103) Steps 0(0.00) | Grad Norm 5.9925(7.6331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 89.6880, Epoch Time 1088.3050(1047.7811), Bit/dim 3.6630(best: 3.6573), Xent 0.7646, Loss 4.0453, Error 0.2569(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 17.4868(17.7087) | Bit/dim 3.6597(3.6594) | Xent 0.5726(0.5879) | Loss 8.9940(9.9534) | Error 0.2022(0.2102) Steps 0(0.00) | Grad Norm 14.2512(8.1130) | Total Time 0.00(0.00)\n",
      "Iter 5570 | Time 17.4519(17.8833) | Bit/dim 3.6770(3.6607) | Xent 0.6089(0.5934) | Loss 9.0757(9.7429) | Error 0.2244(0.2131) Steps 0(0.00) | Grad Norm 8.5517(8.1199) | Total Time 0.00(0.00)\n",
      "Iter 5580 | Time 17.1924(17.8710) | Bit/dim 3.6566(3.6614) | Xent 0.5169(0.5849) | Loss 9.0667(9.5715) | Error 0.1767(0.2089) Steps 0(0.00) | Grad Norm 4.9063(7.4572) | Total Time 0.00(0.00)\n",
      "Iter 5590 | Time 17.0102(17.9444) | Bit/dim 3.6258(3.6572) | Xent 0.5817(0.5808) | Loss 9.0426(9.4530) | Error 0.2044(0.2073) Steps 0(0.00) | Grad Norm 7.0545(7.4117) | Total Time 0.00(0.00)\n",
      "Iter 5600 | Time 20.7403(18.0034) | Bit/dim 3.6577(3.6545) | Xent 0.5899(0.5867) | Loss 9.0904(9.3707) | Error 0.2033(0.2085) Steps 0(0.00) | Grad Norm 6.3174(7.3811) | Total Time 0.00(0.00)\n",
      "Iter 5610 | Time 20.2678(18.1519) | Bit/dim 3.6478(3.6535) | Xent 0.5814(0.5816) | Loss 9.2372(9.3189) | Error 0.2011(0.2066) Steps 0(0.00) | Grad Norm 8.5935(7.2643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 89.9485, Epoch Time 1103.9634(1049.4666), Bit/dim 3.6616(best: 3.6573), Xent 0.6992, Loss 4.0112, Error 0.2414(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 17.8887(18.0821) | Bit/dim 3.6643(3.6542) | Xent 0.5171(0.5801) | Loss 9.1522(9.8496) | Error 0.1767(0.2052) Steps 0(0.00) | Grad Norm 5.6679(7.0773) | Total Time 0.00(0.00)\n",
      "Iter 5630 | Time 19.2064(18.0472) | Bit/dim 3.6450(3.6557) | Xent 0.5802(0.5744) | Loss 9.3547(9.6601) | Error 0.2089(0.2038) Steps 0(0.00) | Grad Norm 8.1067(7.1743) | Total Time 0.00(0.00)\n",
      "Iter 5640 | Time 18.3876(18.0737) | Bit/dim 3.6469(3.6557) | Xent 0.5531(0.5753) | Loss 9.2081(9.5357) | Error 0.1889(0.2036) Steps 0(0.00) | Grad Norm 5.8948(7.4130) | Total Time 0.00(0.00)\n",
      "Iter 5650 | Time 18.9370(18.0667) | Bit/dim 3.6527(3.6558) | Xent 0.6479(0.5823) | Loss 9.3211(9.4425) | Error 0.2367(0.2068) Steps 0(0.00) | Grad Norm 7.8182(7.9736) | Total Time 0.00(0.00)\n",
      "Iter 5660 | Time 17.8804(18.0931) | Bit/dim 3.6201(3.6549) | Xent 0.5904(0.5836) | Loss 9.0291(9.3724) | Error 0.2044(0.2069) Steps 0(0.00) | Grad Norm 12.8773(8.2513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 91.5105, Epoch Time 1101.5713(1051.0297), Bit/dim 3.6558(best: 3.6573), Xent 0.6966, Loss 4.0041, Error 0.2429(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 19.2476(18.1360) | Bit/dim 3.6636(3.6546) | Xent 0.5367(0.5820) | Loss 9.2770(10.0004) | Error 0.1833(0.2056) Steps 0(0.00) | Grad Norm 11.5678(8.2961) | Total Time 0.00(0.00)\n",
      "Iter 5680 | Time 17.7257(17.9843) | Bit/dim 3.6847(3.6570) | Xent 0.5932(0.5748) | Loss 9.3602(9.7702) | Error 0.2167(0.2035) Steps 0(0.00) | Grad Norm 8.1397(7.9841) | Total Time 0.00(0.00)\n",
      "Iter 5690 | Time 18.0523(18.1053) | Bit/dim 3.6282(3.6549) | Xent 0.6006(0.5770) | Loss 9.0740(9.6204) | Error 0.2144(0.2047) Steps 0(0.00) | Grad Norm 6.7040(7.8609) | Total Time 0.00(0.00)\n",
      "Iter 5700 | Time 17.6569(18.0956) | Bit/dim 3.6571(3.6542) | Xent 0.6225(0.5795) | Loss 9.3342(9.4948) | Error 0.2256(0.2052) Steps 0(0.00) | Grad Norm 14.1242(8.2940) | Total Time 0.00(0.00)\n",
      "Iter 5710 | Time 18.5404(17.9778) | Bit/dim 3.6426(3.6520) | Xent 0.5864(0.5829) | Loss 9.1658(9.3779) | Error 0.2044(0.2067) Steps 0(0.00) | Grad Norm 8.0693(8.5252) | Total Time 0.00(0.00)\n",
      "Iter 5720 | Time 17.7308(18.0630) | Bit/dim 3.6380(3.6525) | Xent 0.5654(0.5821) | Loss 9.1868(9.3331) | Error 0.1856(0.2050) Steps 0(0.00) | Grad Norm 8.6306(8.5462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 93.0115, Epoch Time 1102.1510(1052.5633), Bit/dim 3.6587(best: 3.6558), Xent 0.7187, Loss 4.0181, Error 0.2474(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 20.4520(18.0999) | Bit/dim 3.6539(3.6535) | Xent 0.5281(0.5827) | Loss 9.2491(9.8984) | Error 0.1944(0.2059) Steps 0(0.00) | Grad Norm 6.8303(8.5200) | Total Time 0.00(0.00)\n",
      "Iter 5740 | Time 19.6516(18.0586) | Bit/dim 3.6587(3.6526) | Xent 0.6093(0.5800) | Loss 9.2209(9.7011) | Error 0.2211(0.2048) Steps 0(0.00) | Grad Norm 6.1382(8.3104) | Total Time 0.00(0.00)\n",
      "Iter 5750 | Time 18.4853(18.0078) | Bit/dim 3.6382(3.6511) | Xent 0.5406(0.5728) | Loss 9.1867(9.5508) | Error 0.2000(0.2034) Steps 0(0.00) | Grad Norm 4.9182(7.7527) | Total Time 0.00(0.00)\n",
      "Iter 5760 | Time 18.5126(18.0383) | Bit/dim 3.6597(3.6529) | Xent 0.5087(0.5661) | Loss 9.1405(9.4494) | Error 0.1889(0.2019) Steps 0(0.00) | Grad Norm 6.6734(7.4565) | Total Time 0.00(0.00)\n",
      "Iter 5770 | Time 19.2929(18.0735) | Bit/dim 3.6431(3.6517) | Xent 0.6167(0.5669) | Loss 9.1350(9.3517) | Error 0.2200(0.2027) Steps 0(0.00) | Grad Norm 9.5838(7.5537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 92.4973, Epoch Time 1102.1969(1054.0524), Bit/dim 3.6496(best: 3.6558), Xent 0.6698, Loss 3.9845, Error 0.2292(best: 0.2340)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 16.4475(18.1469) | Bit/dim 3.6099(3.6500) | Xent 0.6036(0.5679) | Loss 8.7526(9.9779) | Error 0.2100(0.2032) Steps 0(0.00) | Grad Norm 9.0400(7.5212) | Total Time 0.00(0.00)\n",
      "Iter 5790 | Time 17.8253(18.1150) | Bit/dim 3.6665(3.6503) | Xent 0.6122(0.5641) | Loss 9.2268(9.7478) | Error 0.2056(0.2020) Steps 0(0.00) | Grad Norm 9.7652(7.4104) | Total Time 0.00(0.00)\n",
      "Iter 5800 | Time 18.4369(18.1820) | Bit/dim 3.6365(3.6513) | Xent 0.6062(0.5622) | Loss 9.2100(9.5819) | Error 0.1933(0.2003) Steps 0(0.00) | Grad Norm 8.7653(7.1870) | Total Time 0.00(0.00)\n",
      "Iter 5810 | Time 19.4327(18.2544) | Bit/dim 3.6488(3.6531) | Xent 0.6465(0.5694) | Loss 9.4088(9.4874) | Error 0.2267(0.2027) Steps 0(0.00) | Grad Norm 10.3355(7.4540) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run1/epoch_60_checkpt.pth --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
