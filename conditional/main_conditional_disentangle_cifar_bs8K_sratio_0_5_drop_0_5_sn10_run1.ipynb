{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn10_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 99.8518(99.8518) | Bit/dim 11.0255(11.0255) | Xent 2.3026(2.3026) | Loss 12.1768(12.1768) | Error 0.8990(0.8990) Steps 574(574.00) | Grad Norm 68.4490(68.4490) | Total Time 14.00(14.00)\n",
      "Iter 0002 | Time 49.7100(98.3475) | Bit/dim 10.5590(11.0116) | Xent 2.2925(2.3023) | Loss 11.7053(12.1627) | Error 0.7664(0.8950) Steps 574(574.00) | Grad Norm 57.9512(68.1341) | Total Time 14.00(14.00)\n",
      "Iter 0003 | Time 47.7628(96.8300) | Bit/dim 10.0551(10.9829) | Xent 2.2796(2.3016) | Loss 11.1949(12.1337) | Error 0.7694(0.8913) Steps 574(574.00) | Grad Norm 45.0267(67.4409) | Total Time 14.00(14.00)\n",
      "Iter 0004 | Time 47.4392(95.3482) | Bit/dim 9.5925(10.9411) | Xent 2.2610(2.3004) | Loss 10.7230(12.0913) | Error 0.7615(0.8874) Steps 574(574.00) | Grad Norm 31.3387(66.3578) | Total Time 14.00(14.00)\n",
      "Iter 0005 | Time 46.2695(93.8759) | Bit/dim 9.1777(10.8882) | Xent 2.2416(2.2986) | Loss 10.2985(12.0376) | Error 0.7584(0.8835) Steps 574(574.00) | Grad Norm 19.2371(64.9442) | Total Time 14.00(14.00)\n",
      "Iter 0006 | Time 47.4160(92.4821) | Bit/dim 8.9903(10.8313) | Xent 2.2226(2.2963) | Loss 10.1016(11.9795) | Error 0.7676(0.8800) Steps 574(574.00) | Grad Norm 19.1236(63.5696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 35.6316, Epoch Time 389.8855(389.8855), Bit/dim 8.8737(best: inf), Xent 2.1994, Loss 9.9734, Error 0.7610(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 59.0915(91.4804) | Bit/dim 8.8944(10.7732) | Xent 2.2047(2.2936) | Loss 9.9968(11.9200) | Error 0.7735(0.8768) Steps 574(574.00) | Grad Norm 25.0389(62.4137) | Total Time 14.00(14.00)\n",
      "Iter 0008 | Time 49.2186(90.2125) | Bit/dim 8.8697(10.7161) | Xent 2.1867(2.2904) | Loss 9.9631(11.8613) | Error 0.7675(0.8735) Steps 574(574.00) | Grad Norm 30.2182(61.4478) | Total Time 14.00(14.00)\n",
      "Iter 0009 | Time 47.8861(88.9427) | Bit/dim 8.8951(10.6615) | Xent 2.1635(2.2866) | Loss 9.9769(11.8048) | Error 0.7635(0.8702) Steps 574(574.00) | Grad Norm 31.3582(60.5451) | Total Time 14.00(14.00)\n",
      "Iter 0010 | Time 47.9188(87.7120) | Bit/dim 8.8300(10.6065) | Xent 2.1557(2.2827) | Loss 9.9079(11.7479) | Error 0.7491(0.8666) Steps 574(574.00) | Grad Norm 29.2089(59.6050) | Total Time 14.00(14.00)\n",
      "Iter 0011 | Time 47.7048(86.5118) | Bit/dim 8.7226(10.5500) | Xent 2.1340(2.2782) | Loss 9.7895(11.6891) | Error 0.7308(0.8625) Steps 574(574.00) | Grad Norm 23.9501(58.5354) | Total Time 14.00(14.00)\n",
      "Iter 0012 | Time 47.2867(85.3350) | Bit/dim 8.6132(10.4919) | Xent 2.1258(2.2736) | Loss 9.6761(11.6287) | Error 0.7252(0.8584) Steps 574(574.00) | Grad Norm 19.1279(57.3531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 25.4940, Epoch Time 340.1713(388.3941), Bit/dim 8.4984(best: 8.8737), Xent 2.1093, Loss 9.5530, Error 0.7151(best: 0.7610)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 46.8197(84.1796) | Bit/dim 8.5001(10.4321) | Xent 2.1263(2.2692) | Loss 9.5632(11.5667) | Error 0.7379(0.8548) Steps 574(574.00) | Grad Norm 17.1409(56.1468) | Total Time 14.00(14.00)\n",
      "Iter 0014 | Time 46.8826(83.0607) | Bit/dim 8.3882(10.3708) | Xent 2.1123(2.2645) | Loss 9.4443(11.5031) | Error 0.7351(0.8512) Steps 574(574.00) | Grad Norm 17.5284(54.9882) | Total Time 14.00(14.00)\n",
      "Iter 0015 | Time 46.8666(81.9748) | Bit/dim 8.2979(10.3086) | Xent 2.1009(2.2596) | Loss 9.3484(11.4384) | Error 0.7341(0.8477) Steps 574(574.00) | Grad Norm 17.8575(53.8743) | Total Time 14.00(14.00)\n",
      "Iter 0016 | Time 46.4701(80.9097) | Bit/dim 8.1739(10.2446) | Xent 2.0916(2.2546) | Loss 9.2197(11.3719) | Error 0.7245(0.8440) Steps 574(574.00) | Grad Norm 16.2097(52.7444) | Total Time 14.00(14.00)\n",
      "Iter 0017 | Time 46.7535(79.8850) | Bit/dim 8.0699(10.1794) | Xent 2.0718(2.2491) | Loss 9.1058(11.3039) | Error 0.6996(0.8397) Steps 574(574.00) | Grad Norm 14.1788(51.5874) | Total Time 14.00(14.00)\n",
      "Iter 0018 | Time 46.7338(78.8905) | Bit/dim 7.9465(10.1124) | Xent 2.0437(2.2429) | Loss 8.9683(11.2338) | Error 0.6821(0.8349) Steps 574(574.00) | Grad Norm 12.2469(50.4072) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 25.2992, Epoch Time 321.4433(386.3855), Bit/dim 7.8348(best: 8.4984), Xent 2.0415, Loss 8.8555, Error 0.6806(best: 0.7151)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 47.3350(77.9438) | Bit/dim 7.8399(10.0442) | Xent 2.0487(2.2371) | Loss 8.8642(11.1627) | Error 0.6880(0.8305) Steps 574(574.00) | Grad Norm 11.4633(49.2389) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 47.0894(77.0182) | Bit/dim 7.7359(9.9749) | Xent 2.0492(2.2314) | Loss 8.7605(11.0907) | Error 0.6886(0.8263) Steps 574(574.00) | Grad Norm 10.7715(48.0848) | Total Time 14.00(14.00)\n",
      "Iter 0021 | Time 46.9126(76.1150) | Bit/dim 7.6331(9.9047) | Xent 2.0360(2.2256) | Loss 8.6512(11.0175) | Error 0.6897(0.8222) Steps 574(574.00) | Grad Norm 9.3420(46.9226) | Total Time 14.00(14.00)\n",
      "Iter 0022 | Time 47.1678(75.2466) | Bit/dim 7.5588(9.8343) | Xent 2.0449(2.2202) | Loss 8.5813(10.9444) | Error 0.7023(0.8186) Steps 574(574.00) | Grad Norm 8.1692(45.7600) | Total Time 14.00(14.00)\n",
      "Iter 0023 | Time 46.6462(74.3886) | Bit/dim 7.5041(9.7644) | Xent 2.0437(2.2149) | Loss 8.5260(10.8718) | Error 0.6971(0.8149) Steps 574(574.00) | Grad Norm 8.4400(44.6404) | Total Time 14.00(14.00)\n",
      "Iter 0024 | Time 46.4842(73.5515) | Bit/dim 7.4681(9.6955) | Xent 2.0597(2.2102) | Loss 8.4979(10.8006) | Error 0.7095(0.8118) Steps 574(574.00) | Grad Norm 9.2960(43.5800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 25.5259, Epoch Time 322.9142(384.4814), Bit/dim 7.4301(best: 7.8348), Xent 2.0490, Loss 8.4545, Error 0.6980(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 46.9651(72.7539) | Bit/dim 7.4368(9.6278) | Xent 2.0524(2.2055) | Loss 8.4630(10.7305) | Error 0.7070(0.8086) Steps 574(574.00) | Grad Norm 9.1192(42.5462) | Total Time 14.00(14.00)\n",
      "Iter 0026 | Time 46.8571(71.9770) | Bit/dim 7.3742(9.5602) | Xent 2.0549(2.2010) | Loss 8.4017(10.6606) | Error 0.7047(0.8055) Steps 574(574.00) | Grad Norm 8.0843(41.5124) | Total Time 14.00(14.00)\n",
      "Iter 0027 | Time 46.2839(71.2062) | Bit/dim 7.3100(9.4926) | Xent 2.0609(2.1968) | Loss 8.3405(10.5910) | Error 0.6995(0.8023) Steps 574(574.00) | Grad Norm 6.9426(40.4753) | Total Time 14.00(14.00)\n",
      "Iter 0028 | Time 47.3703(70.4911) | Bit/dim 7.2449(9.4252) | Xent 2.0716(2.1930) | Loss 8.2807(10.5217) | Error 0.7097(0.7996) Steps 574(574.00) | Grad Norm 6.2733(39.4492) | Total Time 14.00(14.00)\n",
      "Iter 0029 | Time 46.2510(69.7639) | Bit/dim 7.1840(9.3580) | Xent 2.0641(2.1891) | Loss 8.2160(10.4525) | Error 0.7024(0.7966) Steps 574(574.00) | Grad Norm 5.4098(38.4280) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 49.6214(69.1596) | Bit/dim 7.1470(9.2916) | Xent 2.0624(2.1853) | Loss 8.1782(10.3843) | Error 0.7017(0.7938) Steps 580(574.18) | Grad Norm 4.6779(37.4155) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 25.2408, Epoch Time 323.9780(382.6663), Bit/dim 7.1272(best: 7.4301), Xent 2.0658, Loss 8.1601, Error 0.7082(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 48.5296(68.5407) | Bit/dim 7.1290(9.2268) | Xent 2.0668(2.1818) | Loss 8.1624(10.3177) | Error 0.7141(0.7914) Steps 580(574.35) | Grad Norm 4.7468(36.4355) | Total Time 14.00(14.00)\n",
      "Iter 0032 | Time 47.7008(67.9155) | Bit/dim 7.1102(9.1633) | Xent 2.0742(2.1786) | Loss 8.1473(10.2525) | Error 0.7348(0.7897) Steps 580(574.52) | Grad Norm 4.8242(35.4871) | Total Time 14.00(14.00)\n",
      "Iter 0033 | Time 48.6389(67.3372) | Bit/dim 7.1032(9.1015) | Xent 2.0772(2.1755) | Loss 8.1418(10.1892) | Error 0.7399(0.7882) Steps 580(574.69) | Grad Norm 4.9488(34.5710) | Total Time 14.00(14.00)\n",
      "Iter 0034 | Time 49.1200(66.7907) | Bit/dim 7.0906(9.0411) | Xent 2.0729(2.1724) | Loss 8.1271(10.1274) | Error 0.7412(0.7868) Steps 580(574.85) | Grad Norm 4.7589(33.6766) | Total Time 14.00(14.00)\n",
      "Iter 0035 | Time 48.8720(66.2531) | Bit/dim 7.0884(8.9826) | Xent 2.0589(2.1690) | Loss 8.1178(10.0671) | Error 0.7224(0.7849) Steps 580(575.00) | Grad Norm 5.1065(32.8195) | Total Time 14.00(14.00)\n",
      "Iter 0036 | Time 48.6835(65.7261) | Bit/dim 7.0598(8.9249) | Xent 2.0399(2.1652) | Loss 8.0797(10.0075) | Error 0.7134(0.7827) Steps 580(575.15) | Grad Norm 5.0726(31.9871) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 25.1853, Epoch Time 331.9827(381.1458), Bit/dim 7.0352(best: 7.1272), Xent 2.0353, Loss 8.0529, Error 0.6972(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 49.4341(65.2373) | Bit/dim 7.0328(8.8681) | Xent 2.0476(2.1616) | Loss 8.0566(9.9489) | Error 0.7065(0.7804) Steps 580(575.30) | Grad Norm 7.7264(31.2593) | Total Time 14.00(14.00)\n",
      "Iter 0038 | Time 48.3168(64.7297) | Bit/dim 7.0104(8.8124) | Xent 2.0714(2.1589) | Loss 8.0461(9.8918) | Error 0.7376(0.7792) Steps 580(575.44) | Grad Norm 14.0257(30.7423) | Total Time 14.00(14.00)\n",
      "Iter 0039 | Time 48.4210(64.2404) | Bit/dim 7.0129(8.7584) | Xent 2.1146(2.1576) | Loss 8.0701(9.8372) | Error 0.7726(0.7790) Steps 580(575.58) | Grad Norm 22.4045(30.4921) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 47.6226(63.7419) | Bit/dim 7.0105(8.7060) | Xent 2.1324(2.1568) | Loss 8.0767(9.7844) | Error 0.7710(0.7787) Steps 580(575.71) | Grad Norm 25.8196(30.3520) | Total Time 14.00(14.00)\n",
      "Iter 0041 | Time 48.9324(63.2976) | Bit/dim 6.9989(8.6547) | Xent 2.0742(2.1544) | Loss 8.0360(9.7319) | Error 0.7499(0.7779) Steps 580(575.84) | Grad Norm 14.9566(29.8901) | Total Time 14.00(14.00)\n",
      "Iter 0042 | Time 47.9042(62.8358) | Bit/dim 6.9831(8.6046) | Xent 2.0334(2.1507) | Loss 7.9998(9.6800) | Error 0.7170(0.7760) Steps 580(575.96) | Grad Norm 5.4734(29.1576) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 25.7669, Epoch Time 331.9519(379.6700), Bit/dim 6.9848(best: 7.0352), Xent 2.0788, Loss 8.0242, Error 0.7631(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 49.4681(62.4348) | Bit/dim 6.9821(8.5559) | Xent 2.0968(2.1491) | Loss 8.0305(9.6305) | Error 0.7730(0.7759) Steps 580(576.08) | Grad Norm 21.5033(28.9280) | Total Time 14.00(14.00)\n",
      "Iter 0044 | Time 48.8949(62.0286) | Bit/dim 6.9748(8.5085) | Xent 2.1275(2.1485) | Loss 8.0385(9.5827) | Error 0.7724(0.7758) Steps 586(576.38) | Grad Norm 25.4690(28.8242) | Total Time 14.00(14.00)\n",
      "Iter 0045 | Time 50.4655(61.6817) | Bit/dim 6.9577(8.4620) | Xent 2.0384(2.1452) | Loss 7.9769(9.5345) | Error 0.7305(0.7745) Steps 592(576.85) | Grad Norm 13.1491(28.3539) | Total Time 14.00(14.00)\n",
      "Iter 0046 | Time 49.6589(61.3210) | Bit/dim 6.9380(8.4162) | Xent 2.0256(2.1416) | Loss 7.9508(9.4870) | Error 0.7149(0.7727) Steps 592(577.30) | Grad Norm 5.9156(27.6808) | Total Time 14.00(14.00)\n",
      "Iter 0047 | Time 50.1356(60.9854) | Bit/dim 6.9350(8.3718) | Xent 2.0435(2.1386) | Loss 7.9568(9.4411) | Error 0.7190(0.7711) Steps 592(577.74) | Grad Norm 14.7579(27.2931) | Total Time 14.00(14.00)\n",
      "Iter 0048 | Time 49.9805(60.6553) | Bit/dim 6.9129(8.3280) | Xent 2.0218(2.1351) | Loss 7.9238(9.3956) | Error 0.7163(0.7694) Steps 592(578.17) | Grad Norm 9.5955(26.7622) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 26.0370, Epoch Time 340.1768(378.4852), Bit/dim 6.9118(best: 6.9848), Xent 1.9973, Loss 7.9104, Error 0.6854(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 51.6795(60.3860) | Bit/dim 6.9120(8.2856) | Xent 2.0051(2.1312) | Loss 7.9145(9.3512) | Error 0.6876(0.7670) Steps 598(578.77) | Grad Norm 3.5297(26.0652) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 52.1996(60.1404) | Bit/dim 6.9020(8.2441) | Xent 2.0288(2.1282) | Loss 7.9164(9.3081) | Error 0.7066(0.7652) Steps 598(579.34) | Grad Norm 12.0285(25.6441) | Total Time 14.00(14.00)\n",
      "Iter 0051 | Time 52.1148(59.8997) | Bit/dim 6.8959(8.2036) | Xent 2.0272(2.1251) | Loss 7.9095(9.2662) | Error 0.7199(0.7638) Steps 598(579.90) | Grad Norm 11.3971(25.2167) | Total Time 14.00(14.00)\n",
      "Iter 0052 | Time 51.1672(59.6377) | Bit/dim 6.8811(8.1639) | Xent 1.9979(2.1213) | Loss 7.8801(9.2246) | Error 0.6723(0.7611) Steps 598(580.45) | Grad Norm 2.3296(24.5301) | Total Time 14.00(14.00)\n",
      "Iter 0053 | Time 52.3717(59.4197) | Bit/dim 6.8738(8.1252) | Xent 2.0220(2.1183) | Loss 7.8848(9.1844) | Error 0.6935(0.7590) Steps 598(580.97) | Grad Norm 11.4131(24.1366) | Total Time 14.00(14.00)\n",
      "Iter 0054 | Time 52.8082(59.2214) | Bit/dim 6.8619(8.0873) | Xent 2.0367(2.1159) | Loss 7.8803(9.1453) | Error 0.7254(0.7580) Steps 598(581.48) | Grad Norm 16.6059(23.9107) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 25.9164, Epoch Time 353.8168(377.7451), Bit/dim 6.8428(best: 6.9118), Xent 2.0041, Loss 7.8448, Error 0.6731(best: 0.6806)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 52.3904(59.0164) | Bit/dim 6.8392(8.0499) | Xent 2.0123(2.1128) | Loss 7.8454(9.1063) | Error 0.6853(0.7558) Steps 598(581.98) | Grad Norm 11.4283(23.5362) | Total Time 14.00(14.00)\n",
      "Iter 0056 | Time 51.8661(58.8019) | Bit/dim 6.8140(8.0128) | Xent 1.9971(2.1093) | Loss 7.8125(9.0675) | Error 0.6758(0.7534) Steps 598(582.46) | Grad Norm 1.3956(22.8720) | Total Time 14.00(14.00)\n",
      "Iter 0057 | Time 52.5944(58.6157) | Bit/dim 6.8171(7.9769) | Xent 2.0133(2.1064) | Loss 7.8237(9.0302) | Error 0.7004(0.7518) Steps 598(582.93) | Grad Norm 11.3085(22.5251) | Total Time 14.00(14.00)\n",
      "Iter 0058 | Time 52.4952(58.4321) | Bit/dim 6.8087(7.9419) | Xent 2.0237(2.1039) | Loss 7.8206(8.9939) | Error 0.6954(0.7501) Steps 598(583.38) | Grad Norm 17.0907(22.3620) | Total Time 14.00(14.00)\n",
      "Iter 0059 | Time 51.4390(58.2223) | Bit/dim 6.7918(7.9074) | Xent 2.0155(2.1013) | Loss 7.7995(8.9580) | Error 0.7184(0.7492) Steps 598(583.82) | Grad Norm 16.8466(22.1966) | Total Time 14.00(14.00)\n",
      "Iter 0060 | Time 52.5662(58.0526) | Bit/dim 6.7644(7.8731) | Xent 1.9774(2.0976) | Loss 7.7531(8.9219) | Error 0.6683(0.7468) Steps 598(584.24) | Grad Norm 10.5099(21.8460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 26.3264, Epoch Time 358.9672(377.1818), Bit/dim 6.7388(best: 6.8428), Xent 1.9670, Loss 7.7223, Error 0.6522(best: 0.6731)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 50.8855(57.8376) | Bit/dim 6.7369(7.8390) | Xent 1.9865(2.0942) | Loss 7.7301(8.8861) | Error 0.6619(0.7442) Steps 598(584.66) | Grad Norm 3.2891(21.2893) | Total Time 14.00(14.00)\n",
      "Iter 0062 | Time 51.8475(57.6579) | Bit/dim 6.7094(7.8051) | Xent 1.9900(2.0911) | Loss 7.7044(8.8507) | Error 0.6883(0.7425) Steps 598(585.06) | Grad Norm 8.6320(20.9095) | Total Time 14.00(14.00)\n",
      "Iter 0063 | Time 52.3634(57.4991) | Bit/dim 6.7046(7.7721) | Xent 2.0172(2.0889) | Loss 7.7132(8.8166) | Error 0.7115(0.7416) Steps 592(585.26) | Grad Norm 14.9566(20.7309) | Total Time 14.00(14.00)\n",
      "Iter 0064 | Time 52.3445(57.3444) | Bit/dim 6.6760(7.7392) | Xent 2.0371(2.0873) | Loss 7.6945(8.7829) | Error 0.7458(0.7417) Steps 598(585.65) | Grad Norm 22.0764(20.7713) | Total Time 14.00(14.00)\n",
      "Iter 0065 | Time 50.7716(57.1472) | Bit/dim 6.6829(7.7075) | Xent 2.1033(2.0878) | Loss 7.7346(8.7514) | Error 0.7366(0.7416) Steps 592(585.84) | Grad Norm 31.9111(21.1055) | Total Time 14.00(14.00)\n",
      "Iter 0066 | Time 51.5721(56.9800) | Bit/dim 6.6988(7.6773) | Xent 2.1582(2.0899) | Loss 7.7779(8.7222) | Error 0.7804(0.7427) Steps 598(586.20) | Grad Norm 43.7258(21.7841) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 25.7975, Epoch Time 351.1131(376.3997), Bit/dim 6.6789(best: 6.7388), Xent 2.3666, Loss 7.8622, Error 0.8420(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 52.0962(56.8335) | Bit/dim 6.6736(7.6472) | Xent 2.4031(2.0993) | Loss 7.8752(8.6968) | Error 0.8518(0.7460) Steps 592(586.38) | Grad Norm 57.1287(22.8445) | Total Time 14.00(14.00)\n",
      "Iter 0068 | Time 50.8485(56.6539) | Bit/dim 6.6919(7.6185) | Xent 3.3866(2.1379) | Loss 8.3852(8.6875) | Error 0.8690(0.7497) Steps 586(586.36) | Grad Norm 91.9314(24.9171) | Total Time 14.00(14.00)\n",
      "Iter 0069 | Time 51.2775(56.4926) | Bit/dim 6.8471(7.5954) | Xent 3.1841(2.1693) | Loss 8.4391(8.6800) | Error 0.8035(0.7513) Steps 586(586.35) | Grad Norm 57.4499(25.8930) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 50.4333(56.3108) | Bit/dim 6.7773(7.5708) | Xent 2.7041(2.1854) | Loss 8.1293(8.6635) | Error 0.8223(0.7534) Steps 586(586.34) | Grad Norm 40.4180(26.3288) | Total Time 14.00(14.00)\n",
      "Iter 0071 | Time 49.7153(56.1130) | Bit/dim 6.6702(7.5438) | Xent 2.0569(2.1815) | Loss 7.6986(8.6346) | Error 0.7219(0.7525) Steps 580(586.15) | Grad Norm 23.1595(26.2337) | Total Time 14.00(14.00)\n",
      "Iter 0072 | Time 50.4939(55.9444) | Bit/dim 6.6411(7.5167) | Xent 2.1783(2.1814) | Loss 7.7303(8.6074) | Error 0.8145(0.7544) Steps 580(585.97) | Grad Norm 17.5112(25.9720) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 25.1553, Epoch Time 345.6124(375.4761), Bit/dim 6.8237(best: 6.6789), Xent 2.2057, Loss 7.9266, Error 0.8182(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 50.5999(55.7841) | Bit/dim 6.8177(7.4958) | Xent 2.2119(2.1823) | Loss 7.9237(8.5869) | Error 0.8079(0.7560) Steps 580(585.79) | Grad Norm 31.4774(26.1372) | Total Time 14.00(14.00)\n",
      "Iter 0074 | Time 50.5261(55.6263) | Bit/dim 6.6644(7.4708) | Xent 2.1834(2.1824) | Loss 7.7560(8.5620) | Error 0.7935(0.7571) Steps 580(585.62) | Grad Norm 21.2114(25.9894) | Total Time 14.00(14.00)\n",
      "Iter 0075 | Time 48.5328(55.4135) | Bit/dim 6.5298(7.4426) | Xent 2.1473(2.1813) | Loss 7.6034(8.5332) | Error 0.7699(0.7575) Steps 580(585.45) | Grad Norm 11.8871(25.5664) | Total Time 14.00(14.00)\n",
      "Iter 0076 | Time 47.9256(55.1889) | Bit/dim 6.5575(7.4160) | Xent 2.1980(2.1818) | Loss 7.6565(8.5069) | Error 0.7806(0.7582) Steps 574(585.10) | Grad Norm 23.2508(25.4969) | Total Time 14.00(14.00)\n",
      "Iter 0077 | Time 48.4237(54.9859) | Bit/dim 6.4610(7.3874) | Xent 2.2040(2.1825) | Loss 7.5630(8.4786) | Error 0.7719(0.7586) Steps 574(584.77) | Grad Norm 8.6456(24.9913) | Total Time 14.00(14.00)\n",
      "Iter 0078 | Time 47.7623(54.7692) | Bit/dim 6.5082(7.3610) | Xent 2.2755(2.1853) | Loss 7.6459(8.4536) | Error 0.8055(0.7600) Steps 574(584.45) | Grad Norm 19.9286(24.8395) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 25.5150, Epoch Time 340.0132(374.4122), Bit/dim 6.4191(best: 6.6789), Xent 2.1969, Loss 7.5176, Error 0.7717(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 49.3263(54.6059) | Bit/dim 6.4223(7.3328) | Xent 2.2176(2.1862) | Loss 7.5311(8.4260) | Error 0.7824(0.7607) Steps 580(584.31) | Grad Norm 11.5996(24.4423) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 48.9390(54.4359) | Bit/dim 6.3788(7.3042) | Xent 2.1170(2.1842) | Loss 7.4373(8.3963) | Error 0.7450(0.7602) Steps 580(584.18) | Grad Norm 10.7341(24.0310) | Total Time 14.00(14.00)\n",
      "Iter 0081 | Time 49.3353(54.2829) | Bit/dim 6.3961(7.2770) | Xent 2.1362(2.1827) | Loss 7.4642(8.3683) | Error 0.7890(0.7611) Steps 574(583.88) | Grad Norm 14.4939(23.7449) | Total Time 14.00(14.00)\n",
      "Iter 0082 | Time 47.9792(54.0938) | Bit/dim 6.2995(7.2476) | Xent 2.2028(2.1833) | Loss 7.4009(8.3393) | Error 0.8079(0.7625) Steps 574(583.58) | Grad Norm 7.4197(23.2552) | Total Time 14.00(14.00)\n",
      "Iter 0083 | Time 48.4264(53.9238) | Bit/dim 6.2751(7.2185) | Xent 2.1686(2.1829) | Loss 7.3594(8.3099) | Error 0.8120(0.7639) Steps 574(583.29) | Grad Norm 13.8979(22.9744) | Total Time 14.00(14.00)\n",
      "Iter 0084 | Time 48.2793(53.7544) | Bit/dim 6.2273(7.1887) | Xent 2.0837(2.1799) | Loss 7.2691(8.2787) | Error 0.7228(0.7627) Steps 574(583.02) | Grad Norm 5.5997(22.4532) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 25.1366, Epoch Time 333.0891(373.1725), Bit/dim 6.1943(best: 6.4191), Xent 2.1460, Loss 7.2673, Error 0.7774(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 47.1653(53.5568) | Bit/dim 6.1961(7.1590) | Xent 2.1447(2.1789) | Loss 7.2685(8.2484) | Error 0.7721(0.7630) Steps 574(582.75) | Grad Norm 11.7937(22.1334) | Total Time 14.00(14.00)\n",
      "Iter 0086 | Time 48.9570(53.4188) | Bit/dim 6.1279(7.1280) | Xent 2.0967(2.1764) | Loss 7.1762(8.2162) | Error 0.7360(0.7622) Steps 574(582.48) | Grad Norm 8.9488(21.7379) | Total Time 14.00(14.00)\n",
      "Iter 0087 | Time 48.9316(53.2841) | Bit/dim 6.1067(7.0974) | Xent 2.0956(2.1740) | Loss 7.1545(8.1844) | Error 0.7360(0.7614) Steps 574(582.23) | Grad Norm 7.5601(21.3125) | Total Time 14.00(14.00)\n",
      "Iter 0088 | Time 48.3027(53.1347) | Bit/dim 6.0710(7.0666) | Xent 2.0946(2.1716) | Loss 7.1182(8.1524) | Error 0.7371(0.7607) Steps 574(581.98) | Grad Norm 10.7329(20.9951) | Total Time 14.00(14.00)\n",
      "Iter 0089 | Time 48.6413(52.9999) | Bit/dim 5.9916(7.0343) | Xent 2.0853(2.1690) | Loss 7.0342(8.1188) | Error 0.7254(0.7596) Steps 568(581.56) | Grad Norm 5.3299(20.5252) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 47.5916(52.8377) | Bit/dim 5.9796(7.0027) | Xent 2.0930(2.1667) | Loss 7.0261(8.0861) | Error 0.7409(0.7590) Steps 562(580.98) | Grad Norm 9.3439(20.1897) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 24.7315, Epoch Time 329.6879(371.8680), Bit/dim 5.9393(best: 6.1943), Xent 2.0509, Loss 6.9648, Error 0.7077(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 47.4154(52.6750) | Bit/dim 5.9368(6.9707) | Xent 2.0653(2.1637) | Loss 6.9695(8.0526) | Error 0.7196(0.7579) Steps 562(580.41) | Grad Norm 4.7848(19.7276) | Total Time 14.00(14.00)\n",
      "Iter 0092 | Time 48.2502(52.5422) | Bit/dim 5.9231(6.9393) | Xent 2.0758(2.1610) | Loss 6.9610(8.0198) | Error 0.7361(0.7572) Steps 568(580.03) | Grad Norm 7.0107(19.3461) | Total Time 14.00(14.00)\n",
      "Iter 0093 | Time 47.6950(52.3968) | Bit/dim 5.8898(6.9078) | Xent 2.1152(2.1597) | Loss 6.9474(7.9876) | Error 0.7554(0.7572) Steps 568(579.67) | Grad Norm 17.2020(19.2818) | Total Time 14.00(14.00)\n",
      "Iter 0094 | Time 48.2311(52.2719) | Bit/dim 6.0794(6.8830) | Xent 2.7377(2.1770) | Loss 7.4483(7.9715) | Error 0.8266(0.7592) Steps 562(579.14) | Grad Norm 114.7368(22.1454) | Total Time 14.00(14.00)\n",
      "Iter 0095 | Time 49.7854(52.1973) | Bit/dim 9.1950(6.9523) | Xent 8.7437(2.3740) | Loss 13.5669(8.1393) | Error 0.8968(0.7634) Steps 580(579.17) | Grad Norm 273.4109(29.6834) | Total Time 14.00(14.00)\n",
      "Iter 0096 | Time 47.4468(52.0547) | Bit/dim 6.4227(6.9364) | Xent 2.2896(2.3715) | Loss 7.5675(8.1222) | Error 0.7776(0.7638) Steps 550(578.29) | Grad Norm 23.7870(29.5065) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 23.8528, Epoch Time 338.4526(370.8655), Bit/dim 7.5958(best: 5.9393), Xent 2.4632, Loss 8.8274, Error 0.7580(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 46.0009(51.8731) | Bit/dim 7.5959(6.9562) | Xent 2.5490(2.3768) | Loss 8.8704(8.1446) | Error 0.7949(0.7647) Steps 538(577.08) | Grad Norm 65.9234(30.5990) | Total Time 14.00(14.00)\n",
      "Iter 0098 | Time 48.4985(51.7719) | Bit/dim 6.8490(6.9530) | Xent 2.4452(2.3789) | Loss 8.0716(8.1424) | Error 0.8475(0.7672) Steps 568(576.81) | Grad Norm 32.5092(30.6563) | Total Time 14.00(14.00)\n",
      "Iter 0099 | Time 48.4405(51.6719) | Bit/dim 6.5154(6.9399) | Xent 2.4759(2.3818) | Loss 7.7534(8.1308) | Error 0.8310(0.7691) Steps 574(576.73) | Grad Norm 14.1335(30.1606) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 46.2087(51.5081) | Bit/dim 6.5502(6.9282) | Xent 2.6877(2.3909) | Loss 7.8940(8.1237) | Error 0.8364(0.7711) Steps 574(576.65) | Grad Norm 18.0794(29.7982) | Total Time 14.00(14.00)\n",
      "Iter 0101 | Time 47.0341(51.3738) | Bit/dim 6.5698(6.9174) | Xent 2.2926(2.3880) | Loss 7.7161(8.1114) | Error 0.7990(0.7720) Steps 574(576.57) | Grad Norm 10.3275(29.2141) | Total Time 14.00(14.00)\n",
      "Iter 0102 | Time 46.1107(51.2159) | Bit/dim 6.5823(6.9074) | Xent 2.3256(2.3861) | Loss 7.7451(8.1004) | Error 0.8063(0.7730) Steps 562(576.13) | Grad Norm 14.7399(28.7798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 24.3574, Epoch Time 322.0437(369.4009), Bit/dim 6.6324(best: 5.9393), Xent 2.4032, Loss 7.8340, Error 0.8356(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 46.1937(51.0653) | Bit/dim 6.6282(6.8990) | Xent 2.4373(2.3877) | Loss 7.8469(8.0928) | Error 0.8346(0.7749) Steps 556(575.53) | Grad Norm 17.5775(28.4438) | Total Time 14.00(14.00)\n",
      "Iter 0104 | Time 45.8761(50.9096) | Bit/dim 6.5903(6.8897) | Xent 2.2733(2.3842) | Loss 7.7270(8.0819) | Error 0.8079(0.7758) Steps 556(574.94) | Grad Norm 7.8766(27.8268) | Total Time 14.00(14.00)\n",
      "Iter 0105 | Time 45.6913(50.7530) | Bit/dim 6.5948(6.8809) | Xent 2.3167(2.3822) | Loss 7.7531(8.0720) | Error 0.8091(0.7768) Steps 556(574.37) | Grad Norm 9.1434(27.2663) | Total Time 14.00(14.00)\n",
      "Iter 0106 | Time 45.4744(50.5947) | Bit/dim 6.5935(6.8723) | Xent 2.3016(2.3798) | Loss 7.7443(8.0622) | Error 0.8139(0.7780) Steps 556(573.82) | Grad Norm 8.6919(26.7090) | Total Time 14.00(14.00)\n",
      "Iter 0107 | Time 47.6479(50.5063) | Bit/dim 6.5509(6.8626) | Xent 2.1976(2.3743) | Loss 7.6497(8.0498) | Error 0.7748(0.7779) Steps 556(573.29) | Grad Norm 5.8330(26.0827) | Total Time 14.00(14.00)\n",
      "Iter 0108 | Time 44.9429(50.3394) | Bit/dim 6.5074(6.8520) | Xent 2.2068(2.3693) | Loss 7.6108(8.0366) | Error 0.8023(0.7786) Steps 544(572.41) | Grad Norm 6.3225(25.4899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 24.0805, Epoch Time 315.5149(367.7843), Bit/dim 6.4758(best: 5.9393), Xent 2.2163, Loss 7.5839, Error 0.8011(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 45.2069(50.1854) | Bit/dim 6.4746(6.8406) | Xent 2.2457(2.3656) | Loss 7.5975(8.0234) | Error 0.8067(0.7794) Steps 544(571.56) | Grad Norm 8.6466(24.9846) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 45.7276(50.0517) | Bit/dim 6.4088(6.8277) | Xent 2.2417(2.3619) | Loss 7.5296(8.0086) | Error 0.8280(0.7809) Steps 556(571.09) | Grad Norm 8.1315(24.4790) | Total Time 14.00(14.00)\n",
      "Iter 0111 | Time 46.0011(49.9302) | Bit/dim 6.3822(6.8143) | Xent 2.2112(2.3573) | Loss 7.4878(7.9930) | Error 0.7856(0.7810) Steps 556(570.64) | Grad Norm 9.1747(24.0199) | Total Time 14.00(14.00)\n",
      "Iter 0112 | Time 46.6552(49.8319) | Bit/dim 6.3362(6.8000) | Xent 2.2346(2.3537) | Loss 7.4535(7.9768) | Error 0.8297(0.7825) Steps 568(570.56) | Grad Norm 9.4371(23.5824) | Total Time 14.00(14.00)\n",
      "Iter 0113 | Time 46.4833(49.7314) | Bit/dim 6.2825(6.7845) | Xent 2.2203(2.3497) | Loss 7.3926(7.9593) | Error 0.7997(0.7830) Steps 562(570.30) | Grad Norm 11.0986(23.2079) | Total Time 14.00(14.00)\n",
      "Iter 0114 | Time 46.0790(49.6219) | Bit/dim 6.1764(6.7662) | Xent 2.2919(2.3479) | Loss 7.3224(7.9402) | Error 0.8291(0.7844) Steps 562(570.05) | Grad Norm 17.2306(23.0286) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 24.6758, Epoch Time 316.0221(366.2314), Bit/dim 6.2005(best: 5.9393), Xent 2.4845, Loss 7.4427, Error 0.8496(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 46.6548(49.5329) | Bit/dim 6.1847(6.7488) | Xent 2.4971(2.3524) | Loss 7.4333(7.9250) | Error 0.8473(0.7863) Steps 562(569.81) | Grad Norm 42.1301(23.6016) | Total Time 14.00(14.00)\n",
      "Iter 0116 | Time 46.1732(49.4321) | Bit/dim 6.3323(6.7363) | Xent 3.7453(2.3942) | Loss 8.2050(7.9334) | Error 0.8878(0.7893) Steps 556(569.40) | Grad Norm 64.7451(24.8359) | Total Time 14.00(14.00)\n",
      "Iter 0117 | Time 45.6870(49.3197) | Bit/dim 6.1224(6.7179) | Xent 2.3093(2.3916) | Loss 7.2771(7.9137) | Error 0.8322(0.7906) Steps 550(568.81) | Grad Norm 9.7966(24.3848) | Total Time 14.00(14.00)\n",
      "Iter 0118 | Time 44.2093(49.1664) | Bit/dim 6.3400(6.7065) | Xent 2.8006(2.4039) | Loss 7.7403(7.9085) | Error 0.8652(0.7929) Steps 544(568.07) | Grad Norm 40.0769(24.8555) | Total Time 14.00(14.00)\n",
      "Iter 0119 | Time 45.6160(49.0599) | Bit/dim 6.1838(6.6908) | Xent 2.2685(2.3999) | Loss 7.3180(7.8908) | Error 0.8075(0.7933) Steps 538(567.17) | Grad Norm 12.4449(24.4832) | Total Time 14.00(14.00)\n",
      "Iter 0120 | Time 43.5578(48.8948) | Bit/dim 6.1025(6.6732) | Xent 2.4060(2.4000) | Loss 7.3055(7.8732) | Error 0.8011(0.7935) Steps 532(566.11) | Grad Norm 14.3067(24.1779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 24.2229, Epoch Time 311.6157(364.5930), Bit/dim 6.0504(best: 5.9393), Xent 2.2625, Loss 7.1817, Error 0.8037(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 45.4581(48.7917) | Bit/dim 6.0385(6.6542) | Xent 2.2741(2.3963) | Loss 7.1755(7.8523) | Error 0.8006(0.7937) Steps 550(565.63) | Grad Norm 10.7171(23.7741) | Total Time 14.00(14.00)\n",
      "Iter 0122 | Time 46.3798(48.7194) | Bit/dim 6.0235(6.6352) | Xent 2.2006(2.3904) | Loss 7.1238(7.8304) | Error 0.8166(0.7944) Steps 550(565.16) | Grad Norm 9.1224(23.3345) | Total Time 14.00(14.00)\n",
      "Iter 0123 | Time 46.6100(48.6561) | Bit/dim 5.9696(6.6153) | Xent 2.3109(2.3880) | Loss 7.1251(7.8093) | Error 0.8610(0.7964) Steps 556(564.88) | Grad Norm 9.9546(22.9331) | Total Time 14.00(14.00)\n",
      "Iter 0124 | Time 46.8104(48.6007) | Bit/dim 5.9782(6.5962) | Xent 2.2292(2.3832) | Loss 7.0928(7.7878) | Error 0.8515(0.7981) Steps 556(564.62) | Grad Norm 7.8134(22.4795) | Total Time 14.00(14.00)\n",
      "Iter 0125 | Time 48.0425(48.5840) | Bit/dim 5.9423(6.5765) | Xent 2.1908(2.3775) | Loss 7.0377(7.7653) | Error 0.8040(0.7983) Steps 562(564.54) | Grad Norm 4.9856(21.9547) | Total Time 14.00(14.00)\n",
      "Iter 0126 | Time 48.6894(48.5871) | Bit/dim 5.9272(6.5571) | Xent 2.2130(2.3725) | Loss 7.0337(7.7433) | Error 0.8005(0.7983) Steps 562(564.46) | Grad Norm 5.9466(21.4745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 24.8134, Epoch Time 322.2468(363.3226), Bit/dim 5.9036(best: 5.9393), Xent 2.1862, Loss 6.9967, Error 0.7672(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 47.6929(48.5603) | Bit/dim 5.9055(6.5375) | Xent 2.1931(2.3672) | Loss 7.0020(7.7211) | Error 0.7748(0.7976) Steps 562(564.39) | Grad Norm 5.3839(20.9918) | Total Time 14.00(14.00)\n",
      "Iter 0128 | Time 48.1840(48.5490) | Bit/dim 5.8686(6.5174) | Xent 2.1851(2.3617) | Loss 6.9612(7.6983) | Error 0.7868(0.7973) Steps 556(564.14) | Grad Norm 4.7818(20.5055) | Total Time 14.00(14.00)\n",
      "Iter 0129 | Time 47.1991(48.5085) | Bit/dim 5.8560(6.4976) | Xent 2.1842(2.3564) | Loss 6.9481(7.6758) | Error 0.7916(0.7971) Steps 562(564.07) | Grad Norm 4.6158(20.0288) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 46.7665(48.4563) | Bit/dim 5.8331(6.4777) | Xent 2.1975(2.3516) | Loss 6.9318(7.6535) | Error 0.7901(0.7969) Steps 556(563.83) | Grad Norm 7.5759(19.6552) | Total Time 14.00(14.00)\n",
      "Iter 0131 | Time 48.7413(48.4648) | Bit/dim 5.8055(6.4575) | Xent 2.2036(2.3472) | Loss 6.9073(7.6311) | Error 0.7856(0.7966) Steps 556(563.60) | Grad Norm 13.8776(19.4819) | Total Time 14.00(14.00)\n",
      "Iter 0132 | Time 48.1416(48.4551) | Bit/dim 5.7915(6.4375) | Xent 2.3871(2.3484) | Loss 6.9850(7.6117) | Error 0.8243(0.7974) Steps 562(563.55) | Grad Norm 28.2322(19.7444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 24.1720, Epoch Time 326.5132(362.2183), Bit/dim 5.8112(best: 5.9036), Xent 2.9996, Loss 7.3111, Error 0.8718(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 47.5146(48.4269) | Bit/dim 5.8049(6.4185) | Xent 2.9750(2.3672) | Loss 7.2924(7.6021) | Error 0.8675(0.7995) Steps 550(563.14) | Grad Norm 59.7445(20.9444) | Total Time 14.00(14.00)\n",
      "Iter 0134 | Time 47.6260(48.4029) | Bit/dim 5.8414(6.4012) | Xent 3.5052(2.4013) | Loss 7.5940(7.6019) | Error 0.8396(0.8007) Steps 556(562.93) | Grad Norm 42.4354(21.5891) | Total Time 14.00(14.00)\n",
      "Iter 0135 | Time 45.1606(48.3056) | Bit/dim 5.8129(6.3836) | Xent 2.6956(2.4101) | Loss 7.1607(7.5886) | Error 0.8235(0.8014) Steps 544(562.36) | Grad Norm 17.9421(21.4797) | Total Time 14.00(14.00)\n",
      "Iter 0136 | Time 44.6521(48.1960) | Bit/dim 5.8469(6.3675) | Xent 2.6966(2.4187) | Loss 7.1952(7.5768) | Error 0.8694(0.8034) Steps 544(561.81) | Grad Norm 23.0650(21.5273) | Total Time 14.00(14.00)\n",
      "Iter 0137 | Time 44.7843(48.0936) | Bit/dim 5.7932(6.3503) | Xent 2.3566(2.4169) | Loss 6.9715(7.5587) | Error 0.8135(0.8037) Steps 550(561.46) | Grad Norm 9.8506(21.1770) | Total Time 14.00(14.00)\n",
      "Iter 0138 | Time 47.1263(48.0646) | Bit/dim 5.7705(6.3329) | Xent 2.3542(2.4150) | Loss 6.9476(7.5403) | Error 0.8289(0.8045) Steps 544(560.93) | Grad Norm 14.6129(20.9800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 23.4369, Epoch Time 315.3700(360.8128), Bit/dim 5.7722(best: 5.8112), Xent 2.3549, Loss 6.9497, Error 0.7930(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 46.2374(48.0098) | Bit/dim 5.7603(6.3157) | Xent 2.3692(2.4136) | Loss 6.9449(7.5225) | Error 0.7975(0.8043) Steps 544(560.42) | Grad Norm 14.6064(20.7888) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 47.7161(48.0010) | Bit/dim 5.7576(6.2989) | Xent 2.1627(2.4061) | Loss 6.8390(7.5020) | Error 0.7824(0.8036) Steps 562(560.47) | Grad Norm 5.2457(20.3225) | Total Time 14.00(14.00)\n",
      "Iter 0141 | Time 48.5890(48.0186) | Bit/dim 5.7611(6.2828) | Xent 2.2736(2.4021) | Loss 6.8979(7.4839) | Error 0.8514(0.8051) Steps 562(560.52) | Grad Norm 9.1575(19.9876) | Total Time 14.00(14.00)\n",
      "Iter 0142 | Time 47.7779(48.0114) | Bit/dim 5.7180(6.2659) | Xent 2.2205(2.3967) | Loss 6.8282(7.4642) | Error 0.8320(0.8059) Steps 556(560.38) | Grad Norm 6.9610(19.5968) | Total Time 14.00(14.00)\n",
      "Iter 0143 | Time 47.1858(47.9867) | Bit/dim 5.7191(6.2495) | Xent 2.1360(2.3888) | Loss 6.7871(7.4439) | Error 0.7476(0.8041) Steps 562(560.43) | Grad Norm 4.1425(19.1332) | Total Time 14.00(14.00)\n",
      "Iter 0144 | Time 47.2993(47.9660) | Bit/dim 5.6674(6.2320) | Xent 2.1410(2.3814) | Loss 6.7379(7.4227) | Error 0.7609(0.8028) Steps 556(560.30) | Grad Norm 3.5962(18.6670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 24.5512, Epoch Time 324.8521(359.7340), Bit/dim 5.6619(best: 5.7722), Xent 2.1441, Loss 6.7339, Error 0.7443(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 47.5128(47.9524) | Bit/dim 5.6692(6.2151) | Xent 2.1579(2.3747) | Loss 6.7481(7.4025) | Error 0.7702(0.8018) Steps 550(559.99) | Grad Norm 3.1199(18.2006) | Total Time 14.00(14.00)\n",
      "Iter 0146 | Time 47.7870(47.9475) | Bit/dim 5.6577(6.1984) | Xent 2.1732(2.3686) | Loss 6.7443(7.3827) | Error 0.7816(0.8012) Steps 556(559.87) | Grad Norm 3.9627(17.7735) | Total Time 14.00(14.00)\n",
      "Iter 0147 | Time 46.7608(47.9119) | Bit/dim 5.6629(6.1823) | Xent 2.1671(2.3626) | Loss 6.7465(7.3636) | Error 0.7830(0.8007) Steps 556(559.75) | Grad Norm 4.0523(17.3619) | Total Time 14.00(14.00)\n",
      "Iter 0148 | Time 48.3130(47.9239) | Bit/dim 5.6263(6.1656) | Xent 2.1375(2.3558) | Loss 6.6951(7.3436) | Error 0.7806(0.8001) Steps 556(559.64) | Grad Norm 2.5730(16.9182) | Total Time 14.00(14.00)\n",
      "Iter 0149 | Time 48.0098(47.9265) | Bit/dim 5.6109(6.1490) | Xent 2.1349(2.3492) | Loss 6.6783(7.3236) | Error 0.7651(0.7990) Steps 562(559.71) | Grad Norm 2.4671(16.4847) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 47.7698(47.9218) | Bit/dim 5.6006(6.1325) | Xent 2.1257(2.3425) | Loss 6.6634(7.3038) | Error 0.7569(0.7978) Steps 556(559.60) | Grad Norm 2.9631(16.0790) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 24.5401, Epoch Time 326.0245(358.7227), Bit/dim 5.5998(best: 5.6619), Xent 2.1033, Loss 6.6515, Error 0.7465(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 48.0950(47.9270) | Bit/dim 5.5965(6.1165) | Xent 2.1204(2.3358) | Loss 6.6567(7.2844) | Error 0.7559(0.7965) Steps 556(559.49) | Grad Norm 2.5729(15.6738) | Total Time 14.00(14.00)\n",
      "Iter 0152 | Time 47.9784(47.9285) | Bit/dim 5.5801(6.1004) | Xent 2.1192(2.3294) | Loss 6.6398(7.2651) | Error 0.7602(0.7954) Steps 556(559.39) | Grad Norm 1.9695(15.2627) | Total Time 14.00(14.00)\n",
      "Iter 0153 | Time 47.0117(47.9010) | Bit/dim 5.5719(6.0845) | Xent 2.0951(2.3223) | Loss 6.6194(7.2457) | Error 0.7403(0.7938) Steps 556(559.28) | Grad Norm 1.7483(14.8573) | Total Time 14.00(14.00)\n",
      "Iter 0154 | Time 47.2599(47.8818) | Bit/dim 5.5592(6.0688) | Xent 2.0984(2.3156) | Loss 6.6084(7.2266) | Error 0.7428(0.7922) Steps 556(559.19) | Grad Norm 2.3994(14.4835) | Total Time 14.00(14.00)\n",
      "Iter 0155 | Time 48.4019(47.8974) | Bit/dim 5.5653(6.0537) | Xent 2.1046(2.3093) | Loss 6.6176(7.2083) | Error 0.7451(0.7908) Steps 556(559.09) | Grad Norm 2.9468(14.1374) | Total Time 14.00(14.00)\n",
      "Iter 0156 | Time 47.0257(47.8712) | Bit/dim 5.5504(6.0386) | Xent 2.0949(2.3028) | Loss 6.5979(7.1900) | Error 0.7315(0.7890) Steps 550(558.82) | Grad Norm 2.8435(13.7986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 24.2706, Epoch Time 325.9064(357.7382), Bit/dim 5.5299(best: 5.5998), Xent 2.0731, Loss 6.5664, Error 0.7098(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 47.2965(47.8540) | Bit/dim 5.5283(6.0233) | Xent 2.0891(2.2964) | Loss 6.5728(7.1715) | Error 0.7309(0.7873) Steps 556(558.73) | Grad Norm 2.0599(13.4465) | Total Time 14.00(14.00)\n",
      "Iter 0158 | Time 47.8635(47.8543) | Bit/dim 5.5051(6.0077) | Xent 2.0953(2.2904) | Loss 6.5527(7.1529) | Error 0.7400(0.7859) Steps 556(558.65) | Grad Norm 1.6605(13.0929) | Total Time 14.00(14.00)\n",
      "Iter 0159 | Time 47.5076(47.8439) | Bit/dim 5.5028(5.9926) | Xent 2.0881(2.2843) | Loss 6.5469(7.1347) | Error 0.7331(0.7843) Steps 556(558.57) | Grad Norm 2.0073(12.7603) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 48.9444(47.8769) | Bit/dim 5.4983(5.9777) | Xent 2.0778(2.2781) | Loss 6.5372(7.1168) | Error 0.7366(0.7829) Steps 550(558.31) | Grad Norm 2.2915(12.4462) | Total Time 14.00(14.00)\n",
      "Iter 0161 | Time 48.8954(47.9074) | Bit/dim 5.5033(5.9635) | Xent 2.1053(2.2729) | Loss 6.5559(7.1000) | Error 0.7441(0.7817) Steps 550(558.07) | Grad Norm 2.4298(12.1458) | Total Time 14.00(14.00)\n",
      "Iter 0162 | Time 48.3718(47.9214) | Bit/dim 5.4811(5.9490) | Xent 2.0832(2.2673) | Loss 6.5227(7.0827) | Error 0.7371(0.7804) Steps 556(558.00) | Grad Norm 2.1412(11.8456) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 23.9379, Epoch Time 328.4270(356.8589), Bit/dim 5.4715(best: 5.5299), Xent 2.0642, Loss 6.5035, Error 0.7238(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 47.6289(47.9126) | Bit/dim 5.4622(5.9344) | Xent 2.0739(2.2615) | Loss 6.4991(7.0651) | Error 0.7344(0.7790) Steps 550(557.76) | Grad Norm 2.1665(11.5552) | Total Time 14.00(14.00)\n",
      "Iter 0164 | Time 46.0628(47.8571) | Bit/dim 5.4505(5.9199) | Xent 2.0740(2.2558) | Loss 6.4875(7.0478) | Error 0.7225(0.7773) Steps 544(557.35) | Grad Norm 1.7665(11.2616) | Total Time 14.00(14.00)\n",
      "Iter 0165 | Time 47.9474(47.8598) | Bit/dim 5.4481(5.9057) | Xent 2.0746(2.2504) | Loss 6.4854(7.0309) | Error 0.7324(0.7759) Steps 550(557.13) | Grad Norm 1.3474(10.9642) | Total Time 14.00(14.00)\n",
      "Iter 0166 | Time 47.6521(47.8536) | Bit/dim 5.4396(5.8918) | Xent 2.0791(2.2453) | Loss 6.4791(7.0144) | Error 0.7241(0.7744) Steps 556(557.10) | Grad Norm 1.8536(10.6908) | Total Time 14.00(14.00)\n",
      "Iter 0167 | Time 46.8475(47.8234) | Bit/dim 5.4334(5.8780) | Xent 2.0642(2.2398) | Loss 6.4655(6.9979) | Error 0.7177(0.7727) Steps 544(556.70) | Grad Norm 2.3525(10.4407) | Total Time 14.00(14.00)\n",
      "Iter 0168 | Time 47.2069(47.8049) | Bit/dim 5.4225(5.8643) | Xent 2.0588(2.2344) | Loss 6.4519(6.9815) | Error 0.7211(0.7711) Steps 550(556.50) | Grad Norm 1.4434(10.1708) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 24.3218, Epoch Time 323.1566(355.8478), Bit/dim 5.4060(best: 5.4715), Xent 2.0409, Loss 6.4265, Error 0.6994(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 47.2847(47.7893) | Bit/dim 5.4013(5.8505) | Xent 2.0575(2.2291) | Loss 6.4300(6.9650) | Error 0.7077(0.7692) Steps 550(556.31) | Grad Norm 1.9208(9.9233) | Total Time 14.00(14.00)\n",
      "Iter 0170 | Time 47.0141(47.7660) | Bit/dim 5.3961(5.8368) | Xent 2.0711(2.2243) | Loss 6.4317(6.9490) | Error 0.7209(0.7678) Steps 544(555.94) | Grad Norm 1.3787(9.6669) | Total Time 14.00(14.00)\n",
      "Iter 0171 | Time 46.3019(47.7221) | Bit/dim 5.3850(5.8233) | Xent 2.0582(2.2194) | Loss 6.4142(6.9330) | Error 0.7114(0.7661) Steps 550(555.76) | Grad Norm 1.4217(9.4196) | Total Time 14.00(14.00)\n",
      "Iter 0172 | Time 45.9333(47.6685) | Bit/dim 5.3598(5.8094) | Xent 2.0358(2.2139) | Loss 6.3778(6.9163) | Error 0.7125(0.7645) Steps 550(555.59) | Grad Norm 1.2710(9.1751) | Total Time 14.00(14.00)\n",
      "Iter 0173 | Time 48.0156(47.6789) | Bit/dim 5.3635(5.7960) | Xent 2.0558(2.2091) | Loss 6.3914(6.9005) | Error 0.7145(0.7630) Steps 550(555.42) | Grad Norm 1.1573(8.9346) | Total Time 14.00(14.00)\n",
      "Iter 0174 | Time 47.4286(47.6714) | Bit/dim 5.3538(5.7827) | Xent 2.0373(2.2040) | Loss 6.3724(6.8847) | Error 0.7047(0.7612) Steps 550(555.26) | Grad Norm 1.4151(8.7090) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 24.5311, Epoch Time 321.8282(354.8273), Bit/dim 5.3342(best: 5.4060), Xent 2.0237, Loss 6.3460, Error 0.6904(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 47.5818(47.6687) | Bit/dim 5.3288(5.7691) | Xent 2.0427(2.1991) | Loss 6.3502(6.8687) | Error 0.7143(0.7598) Steps 550(555.10) | Grad Norm 1.1090(8.4810) | Total Time 14.00(14.00)\n",
      "Iter 0176 | Time 47.6220(47.6673) | Bit/dim 5.3324(5.7560) | Xent 2.0282(2.1940) | Loss 6.3465(6.8530) | Error 0.7091(0.7583) Steps 556(555.13) | Grad Norm 1.1124(8.2599) | Total Time 14.00(14.00)\n",
      "Iter 0177 | Time 46.9954(47.6471) | Bit/dim 5.3135(5.7427) | Xent 2.0328(2.1892) | Loss 6.3299(6.8373) | Error 0.7096(0.7569) Steps 550(554.97) | Grad Norm 1.1141(8.0456) | Total Time 14.00(14.00)\n",
      "Iter 0178 | Time 47.7124(47.6491) | Bit/dim 5.2996(5.7294) | Xent 2.0488(2.1849) | Loss 6.3241(6.8219) | Error 0.7136(0.7556) Steps 562(555.18) | Grad Norm 1.6238(7.8529) | Total Time 14.00(14.00)\n",
      "Iter 0179 | Time 47.5026(47.6447) | Bit/dim 5.3013(5.7166) | Xent 2.0300(2.1803) | Loss 6.3163(6.8067) | Error 0.7123(0.7543) Steps 556(555.21) | Grad Norm 1.3421(7.6576) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 48.3288(47.6652) | Bit/dim 5.2796(5.7035) | Xent 2.0236(2.1756) | Loss 6.2914(6.7913) | Error 0.7023(0.7527) Steps 556(555.23) | Grad Norm 1.7893(7.4815) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 24.5842, Epoch Time 326.0907(353.9652), Bit/dim 5.2639(best: 5.3342), Xent 2.0090, Loss 6.2685, Error 0.6929(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 48.8041(47.6994) | Bit/dim 5.2736(5.6906) | Xent 2.0204(2.1709) | Loss 6.2837(6.7761) | Error 0.7064(0.7513) Steps 562(555.43) | Grad Norm 1.9720(7.3163) | Total Time 14.00(14.00)\n",
      "Iter 0182 | Time 47.2042(47.6845) | Bit/dim 5.2415(5.6771) | Xent 2.0184(2.1664) | Loss 6.2507(6.7603) | Error 0.7077(0.7500) Steps 550(555.27) | Grad Norm 2.3384(7.1669) | Total Time 14.00(14.00)\n",
      "Iter 0183 | Time 48.3264(47.7038) | Bit/dim 5.2573(5.6645) | Xent 2.0249(2.1621) | Loss 6.2697(6.7456) | Error 0.6987(0.7485) Steps 562(555.47) | Grad Norm 1.1975(6.9878) | Total Time 14.00(14.00)\n",
      "Iter 0184 | Time 48.1013(47.7157) | Bit/dim 5.2508(5.6521) | Xent 2.0141(2.1577) | Loss 6.2579(6.7310) | Error 0.7040(0.7471) Steps 562(555.67) | Grad Norm 2.9668(6.8672) | Total Time 14.00(14.00)\n",
      "Iter 0185 | Time 48.4633(47.7381) | Bit/dim 5.2341(5.6396) | Xent 2.0029(2.1530) | Loss 6.2356(6.7161) | Error 0.6975(0.7456) Steps 562(555.86) | Grad Norm 1.3921(6.7030) | Total Time 14.00(14.00)\n",
      "Iter 0186 | Time 48.4395(47.7592) | Bit/dim 5.2242(5.6271) | Xent 2.0097(2.1487) | Loss 6.2291(6.7015) | Error 0.6994(0.7443) Steps 562(556.04) | Grad Norm 3.1111(6.5952) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 24.5195, Epoch Time 329.6937(353.2370), Bit/dim 5.2091(best: 5.2639), Xent 1.9895, Loss 6.2038, Error 0.6805(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 47.7056(47.7576) | Bit/dim 5.2202(5.6149) | Xent 2.0176(2.1448) | Loss 6.2290(6.6873) | Error 0.6992(0.7429) Steps 562(556.22) | Grad Norm 1.7915(6.4511) | Total Time 14.00(14.00)\n",
      "Iter 0188 | Time 47.9119(47.7622) | Bit/dim 5.2105(5.6028) | Xent 1.9794(2.1398) | Loss 6.2002(6.6727) | Error 0.6899(0.7413) Steps 562(556.40) | Grad Norm 3.3054(6.3567) | Total Time 14.00(14.00)\n",
      "Iter 0189 | Time 48.2238(47.7760) | Bit/dim 5.2199(5.5913) | Xent 1.9814(2.1351) | Loss 6.2106(6.6588) | Error 0.6864(0.7397) Steps 562(556.56) | Grad Norm 2.6801(6.2464) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 49.1319(47.8167) | Bit/dim 5.2096(5.5798) | Xent 2.0041(2.1312) | Loss 6.2117(6.6454) | Error 0.7065(0.7387) Steps 568(556.91) | Grad Norm 3.3692(6.1601) | Total Time 14.00(14.00)\n",
      "Iter 0191 | Time 48.1464(47.8266) | Bit/dim 5.1930(5.5682) | Xent 2.0044(2.1274) | Loss 6.1952(6.6319) | Error 0.7029(0.7376) Steps 562(557.06) | Grad Norm 4.2767(6.1036) | Total Time 14.00(14.00)\n",
      "Iter 0192 | Time 49.1096(47.8651) | Bit/dim 5.1924(5.5570) | Xent 1.9835(2.1230) | Loss 6.1841(6.6185) | Error 0.6937(0.7363) Steps 568(557.39) | Grad Norm 3.1194(6.0141) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 25.5871, Epoch Time 331.6114(352.5882), Bit/dim 5.1842(best: 5.2091), Xent 1.9713, Loss 6.1698, Error 0.6745(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 48.5920(47.8869) | Bit/dim 5.1993(5.5462) | Xent 1.9731(2.1185) | Loss 6.1859(6.6055) | Error 0.6801(0.7346) Steps 568(557.71) | Grad Norm 3.4879(5.9383) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 48.2354(47.8974) | Bit/dim 5.1753(5.5351) | Xent 1.9736(2.1142) | Loss 6.1621(6.5922) | Error 0.6865(0.7332) Steps 556(557.65) | Grad Norm 2.1339(5.8242) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 46.9109(47.8678) | Bit/dim 5.1526(5.5236) | Xent 1.9979(2.1107) | Loss 6.1515(6.5790) | Error 0.6992(0.7321) Steps 556(557.61) | Grad Norm 3.0627(5.7413) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 48.0296(47.8726) | Bit/dim 5.1550(5.5126) | Xent 1.9920(2.1071) | Loss 6.1510(6.5661) | Error 0.6875(0.7308) Steps 562(557.74) | Grad Norm 3.6484(5.6785) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 48.2304(47.8834) | Bit/dim 5.1713(5.5023) | Xent 1.9755(2.1032) | Loss 6.1590(6.5539) | Error 0.6820(0.7293) Steps 556(557.68) | Grad Norm 2.9722(5.5973) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 48.6347(47.9059) | Bit/dim 5.1581(5.4920) | Xent 1.9703(2.0992) | Loss 6.1432(6.5416) | Error 0.6834(0.7280) Steps 544(557.27) | Grad Norm 2.3295(5.4993) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 24.3889, Epoch Time 328.3433(351.8609), Bit/dim 5.1393(best: 5.1842), Xent 1.9749, Loss 6.1268, Error 0.6827(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.0233(47.8794) | Bit/dim 5.1387(5.4814) | Xent 1.9820(2.0957) | Loss 6.1297(6.5292) | Error 0.6929(0.7269) Steps 544(556.88) | Grad Norm 5.7037(5.5054) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 46.1985(47.8290) | Bit/dim 5.1465(5.4713) | Xent 1.9976(2.0927) | Loss 6.1453(6.5177) | Error 0.7001(0.7261) Steps 544(556.49) | Grad Norm 8.4498(5.5938) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 47.4157(47.8166) | Bit/dim 5.1423(5.4615) | Xent 1.9967(2.0899) | Loss 6.1406(6.5064) | Error 0.6975(0.7252) Steps 544(556.12) | Grad Norm 9.3335(5.7060) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 47.4684(47.8061) | Bit/dim 5.1402(5.4518) | Xent 1.9763(2.0865) | Loss 6.1283(6.4951) | Error 0.6904(0.7242) Steps 544(555.75) | Grad Norm 7.9978(5.7747) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 48.0211(47.8126) | Bit/dim 5.1184(5.4418) | Xent 1.9739(2.0831) | Loss 6.1053(6.4834) | Error 0.6815(0.7229) Steps 550(555.58) | Grad Norm 3.7447(5.7138) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 46.3383(47.7684) | Bit/dim 5.1188(5.4321) | Xent 1.9531(2.0792) | Loss 6.0954(6.4717) | Error 0.6843(0.7218) Steps 538(555.05) | Grad Norm 2.6969(5.6233) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 24.1423, Epoch Time 322.0727(350.9672), Bit/dim 5.1212(best: 5.1393), Xent 1.9564, Loss 6.0994, Error 0.6710(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 46.9643(47.7442) | Bit/dim 5.1267(5.4230) | Xent 1.9636(2.0757) | Loss 6.1085(6.4608) | Error 0.6843(0.7206) Steps 544(554.72) | Grad Norm 7.6412(5.6838) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 47.1521(47.7265) | Bit/dim 5.1150(5.4137) | Xent 2.0002(2.0734) | Loss 6.1151(6.4505) | Error 0.7054(0.7202) Steps 550(554.58) | Grad Norm 11.6913(5.8641) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 47.6796(47.7251) | Bit/dim 5.1225(5.4050) | Xent 2.0208(2.0719) | Loss 6.1329(6.4409) | Error 0.7052(0.7197) Steps 544(554.26) | Grad Norm 11.0588(6.0199) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 46.8130(47.6977) | Bit/dim 5.1051(5.3960) | Xent 1.9542(2.0683) | Loss 6.0822(6.4302) | Error 0.6805(0.7185) Steps 544(553.95) | Grad Norm 5.3176(5.9988) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 47.5600(47.6936) | Bit/dim 5.0914(5.3869) | Xent 1.9752(2.0655) | Loss 6.0790(6.4196) | Error 0.6943(0.7178) Steps 538(553.47) | Grad Norm 6.5826(6.0163) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 47.4109(47.6851) | Bit/dim 5.1030(5.3784) | Xent 2.0179(2.0641) | Loss 6.1119(6.4104) | Error 0.7089(0.7175) Steps 538(553.01) | Grad Norm 11.9868(6.1955) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 23.5270, Epoch Time 322.7854(350.1218), Bit/dim 5.0859(best: 5.1212), Xent 1.9570, Loss 6.0644, Error 0.6864(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 46.2281(47.6414) | Bit/dim 5.1058(5.3702) | Xent 1.9739(2.0614) | Loss 6.0927(6.4009) | Error 0.6959(0.7169) Steps 544(552.74) | Grad Norm 7.1063(6.2228) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 47.1924(47.6279) | Bit/dim 5.0811(5.3615) | Xent 1.9648(2.0585) | Loss 6.0635(6.3908) | Error 0.6840(0.7159) Steps 538(552.30) | Grad Norm 3.8350(6.1512) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 46.9758(47.6084) | Bit/dim 5.0733(5.3529) | Xent 1.9703(2.0559) | Loss 6.0584(6.3808) | Error 0.7004(0.7154) Steps 532(551.69) | Grad Norm 8.9792(6.2360) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 46.8118(47.5845) | Bit/dim 5.0669(5.3443) | Xent 1.9481(2.0526) | Loss 6.0410(6.3706) | Error 0.6751(0.7142) Steps 538(551.28) | Grad Norm 3.9603(6.1677) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 47.0838(47.5694) | Bit/dim 5.0688(5.3360) | Xent 1.9388(2.0492) | Loss 6.0382(6.3606) | Error 0.6745(0.7130) Steps 544(551.06) | Grad Norm 5.2431(6.1400) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 47.8335(47.5774) | Bit/dim 5.0774(5.3283) | Xent 1.9334(2.0457) | Loss 6.0442(6.3511) | Error 0.6744(0.7119) Steps 532(550.49) | Grad Norm 5.6802(6.1262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 22.4515, Epoch Time 319.9143(349.2156), Bit/dim 5.0608(best: 5.0859), Xent 1.9155, Loss 6.0185, Error 0.6562(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 46.8798(47.5564) | Bit/dim 5.0732(5.3206) | Xent 1.9412(2.0426) | Loss 6.0438(6.3419) | Error 0.6699(0.7106) Steps 538(550.11) | Grad Norm 1.6891(5.9931) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 47.8420(47.5650) | Bit/dim 5.0741(5.3132) | Xent 1.9461(2.0397) | Loss 6.0472(6.3331) | Error 0.6756(0.7096) Steps 538(549.75) | Grad Norm 5.5649(5.9802) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 46.8643(47.5440) | Bit/dim 5.0540(5.3054) | Xent 1.9340(2.0365) | Loss 6.0210(6.3237) | Error 0.6699(0.7084) Steps 526(549.04) | Grad Norm 4.1261(5.9246) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 46.4083(47.5099) | Bit/dim 5.0457(5.2976) | Xent 1.9394(2.0336) | Loss 6.0155(6.3145) | Error 0.6783(0.7075) Steps 526(548.35) | Grad Norm 4.1222(5.8705) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 47.0436(47.4959) | Bit/dim 5.0350(5.2898) | Xent 1.9259(2.0304) | Loss 5.9980(6.3050) | Error 0.6723(0.7064) Steps 532(547.86) | Grad Norm 3.8412(5.8097) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 46.9524(47.4796) | Bit/dim 5.0413(5.2823) | Xent 1.9395(2.0277) | Loss 6.0110(6.2961) | Error 0.6809(0.7057) Steps 526(547.20) | Grad Norm 2.7010(5.7164) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 22.7706, Epoch Time 320.1470(348.3435), Bit/dim 5.0227(best: 5.0608), Xent 1.9183, Loss 5.9819, Error 0.6572(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 45.0444(47.4066) | Bit/dim 5.0342(5.2749) | Xent 1.9166(2.0243) | Loss 5.9925(6.2870) | Error 0.6619(0.7043) Steps 520(546.38) | Grad Norm 3.7329(5.6569) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 47.7363(47.4165) | Bit/dim 5.0262(5.2674) | Xent 1.9327(2.0216) | Loss 5.9925(6.2782) | Error 0.6713(0.7033) Steps 520(545.59) | Grad Norm 2.6031(5.5653) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 47.8454(47.4293) | Bit/dim 5.0046(5.2595) | Xent 1.9258(2.0187) | Loss 5.9675(6.2689) | Error 0.6661(0.7022) Steps 532(545.19) | Grad Norm 3.1781(5.4937) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 44.7199(47.3480) | Bit/dim 5.0090(5.2520) | Xent 1.9328(2.0161) | Loss 5.9754(6.2601) | Error 0.6700(0.7013) Steps 514(544.25) | Grad Norm 4.5616(5.4657) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 46.0057(47.3078) | Bit/dim 5.0308(5.2454) | Xent 1.9411(2.0139) | Loss 6.0013(6.2523) | Error 0.6779(0.7006) Steps 520(543.52) | Grad Norm 6.0348(5.4828) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 46.2931(47.2773) | Bit/dim 5.0343(5.2390) | Xent 1.9015(2.0105) | Loss 5.9851(6.2443) | Error 0.6536(0.6992) Steps 514(542.64) | Grad Norm 7.2719(5.5365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 22.2289, Epoch Time 315.2675(347.3512), Bit/dim 5.0532(best: 5.0227), Xent 1.9157, Loss 6.0111, Error 0.6635(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 47.7528(47.2916) | Bit/dim 5.0622(5.2337) | Xent 1.9361(2.0083) | Loss 6.0303(6.2379) | Error 0.6833(0.6987) Steps 526(542.14) | Grad Norm 13.8560(5.7860) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 42.8929(47.1596) | Bit/dim 5.3156(5.2362) | Xent 1.9561(2.0067) | Loss 6.2937(6.2396) | Error 0.6811(0.6982) Steps 520(541.47) | Grad Norm 14.7370(6.0546) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 45.7903(47.1186) | Bit/dim 5.1825(5.2346) | Xent 1.9208(2.0041) | Loss 6.1429(6.2367) | Error 0.6660(0.6972) Steps 526(541.01) | Grad Norm 8.3779(6.1243) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 47.1254(47.1188) | Bit/dim 5.2062(5.2337) | Xent 2.0113(2.0044) | Loss 6.2118(6.2359) | Error 0.7035(0.6974) Steps 514(540.20) | Grad Norm 30.0499(6.8420) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 43.6144(47.0136) | Bit/dim 5.3260(5.2365) | Xent 2.0286(2.0051) | Loss 6.3403(6.2390) | Error 0.7364(0.6985) Steps 514(539.41) | Grad Norm 15.0009(7.0868) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 43.1632(46.8981) | Bit/dim 5.4475(5.2428) | Xent 1.9401(2.0031) | Loss 6.4175(6.2444) | Error 0.6685(0.6976) Steps 520(538.83) | Grad Norm 11.2424(7.2115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 21.7976, Epoch Time 307.6683(346.1607), Bit/dim 5.2444(best: 5.0227), Xent 1.9576, Loss 6.2232, Error 0.6914(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 43.2707(46.7893) | Bit/dim 5.2466(5.2429) | Xent 1.9705(2.0022) | Loss 6.2318(6.2440) | Error 0.6996(0.6977) Steps 502(537.73) | Grad Norm 8.3187(7.2447) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 44.9375(46.7337) | Bit/dim 5.0478(5.2371) | Xent 1.9842(2.0016) | Loss 6.0399(6.2379) | Error 0.7065(0.6980) Steps 496(536.47) | Grad Norm 9.9240(7.3251) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 45.3352(46.6918) | Bit/dim 5.1913(5.2357) | Xent 2.0376(2.0027) | Loss 6.2102(6.2371) | Error 0.7335(0.6990) Steps 502(535.44) | Grad Norm 22.0936(7.7681) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 43.0969(46.5839) | Bit/dim 5.2375(5.2358) | Xent 2.0781(2.0050) | Loss 6.2765(6.2382) | Error 0.7310(0.7000) Steps 526(535.16) | Grad Norm 15.1640(7.9900) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 43.2616(46.4843) | Bit/dim 5.3924(5.2405) | Xent 1.9856(2.0044) | Loss 6.3852(6.2427) | Error 0.7044(0.7001) Steps 502(534.16) | Grad Norm 10.6440(8.0696) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 43.9168(46.4072) | Bit/dim 5.3383(5.2434) | Xent 1.9277(2.0021) | Loss 6.3022(6.2444) | Error 0.6686(0.6992) Steps 508(533.38) | Grad Norm 9.2132(8.1039) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 21.9599, Epoch Time 301.2199(344.8125), Bit/dim 5.1491(best: 5.0227), Xent 1.9963, Loss 6.1472, Error 0.7109(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 44.5461(46.3514) | Bit/dim 5.1487(5.2406) | Xent 2.0060(2.0022) | Loss 6.1517(6.2417) | Error 0.7097(0.6995) Steps 514(532.80) | Grad Norm 10.9599(8.1896) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 43.5893(46.2685) | Bit/dim 5.0733(5.2355) | Xent 1.9444(2.0005) | Loss 6.0455(6.2358) | Error 0.6770(0.6988) Steps 490(531.51) | Grad Norm 4.0964(8.0668) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 43.3983(46.1824) | Bit/dim 5.1424(5.2327) | Xent 1.9966(2.0003) | Loss 6.1407(6.2329) | Error 0.7005(0.6989) Steps 490(530.27) | Grad Norm 14.7779(8.2681) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 43.8031(46.1111) | Bit/dim 5.1192(5.2293) | Xent 2.0210(2.0010) | Loss 6.1297(6.2298) | Error 0.7226(0.6996) Steps 496(529.24) | Grad Norm 16.5391(8.5163) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 45.1825(46.0832) | Bit/dim 5.1395(5.2266) | Xent 1.9828(2.0004) | Loss 6.1309(6.2269) | Error 0.7041(0.6997) Steps 496(528.24) | Grad Norm 12.8127(8.6452) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 44.6186(46.0393) | Bit/dim 5.0910(5.2226) | Xent 1.9442(1.9987) | Loss 6.0631(6.2219) | Error 0.6854(0.6993) Steps 502(527.45) | Grad Norm 4.7002(8.5268) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 23.0722, Epoch Time 303.5779(343.5755), Bit/dim 5.0824(best: 5.0227), Xent 1.9859, Loss 6.0754, Error 0.7117(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 45.9812(46.0375) | Bit/dim 5.0671(5.2179) | Xent 1.9948(1.9986) | Loss 6.0645(6.2172) | Error 0.7060(0.6995) Steps 526(527.41) | Grad Norm 11.1802(8.6064) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 43.9589(45.9752) | Bit/dim 5.0410(5.2126) | Xent 1.9376(1.9968) | Loss 6.0098(6.2110) | Error 0.6778(0.6988) Steps 502(526.65) | Grad Norm 4.0939(8.4710) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 44.6150(45.9344) | Bit/dim 5.0516(5.2078) | Xent 1.9596(1.9957) | Loss 6.0314(6.2056) | Error 0.6907(0.6986) Steps 496(525.73) | Grad Norm 7.5670(8.4439) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 44.8745(45.9026) | Bit/dim 5.0769(5.2039) | Xent 1.9412(1.9940) | Loss 6.0475(6.2009) | Error 0.6855(0.6982) Steps 514(525.38) | Grad Norm 4.7719(8.3338) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 44.1692(45.8506) | Bit/dim 5.0584(5.1995) | Xent 1.9566(1.9929) | Loss 6.0367(6.1959) | Error 0.6849(0.6978) Steps 508(524.86) | Grad Norm 6.0326(8.2647) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 43.7883(45.7887) | Bit/dim 5.0338(5.1945) | Xent 1.9419(1.9914) | Loss 6.0047(6.1902) | Error 0.6793(0.6972) Steps 496(523.99) | Grad Norm 5.4466(8.1802) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 22.0468, Epoch Time 304.8417(342.4135), Bit/dim 5.0010(best: 5.0227), Xent 1.9379, Loss 5.9699, Error 0.6779(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 43.9983(45.7350) | Bit/dim 5.0084(5.1889) | Xent 1.9690(1.9907) | Loss 5.9928(6.1843) | Error 0.6959(0.6972) Steps 496(523.15) | Grad Norm 5.4066(8.0970) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 44.0324(45.6839) | Bit/dim 5.0186(5.1838) | Xent 1.9240(1.9887) | Loss 5.9806(6.1782) | Error 0.6575(0.6960) Steps 496(522.34) | Grad Norm 4.5419(7.9903) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 44.2353(45.6404) | Bit/dim 5.0247(5.1790) | Xent 1.9318(1.9870) | Loss 5.9906(6.1725) | Error 0.6743(0.6954) Steps 496(521.55) | Grad Norm 4.4813(7.8851) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 43.6405(45.5804) | Bit/dim 4.9913(5.1734) | Xent 1.9413(1.9856) | Loss 5.9619(6.1662) | Error 0.6820(0.6950) Steps 496(520.78) | Grad Norm 3.5579(7.7552) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 45.8405(45.5883) | Bit/dim 4.9739(5.1674) | Xent 1.9391(1.9842) | Loss 5.9434(6.1595) | Error 0.6786(0.6945) Steps 508(520.40) | Grad Norm 4.0333(7.6436) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 44.2642(45.5485) | Bit/dim 4.9612(5.1612) | Xent 1.9081(1.9819) | Loss 5.9153(6.1522) | Error 0.6654(0.6936) Steps 508(520.02) | Grad Norm 2.2557(7.4819) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 21.7984, Epoch Time 303.1052(341.2342), Bit/dim 4.9743(best: 5.0010), Xent 1.9226, Loss 5.9356, Error 0.6660(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 43.5079(45.4873) | Bit/dim 4.9740(5.1556) | Xent 1.9195(1.9801) | Loss 5.9338(6.1457) | Error 0.6634(0.6927) Steps 508(519.66) | Grad Norm 3.6284(7.3663) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 46.0723(45.5049) | Bit/dim 4.9691(5.1500) | Xent 1.9165(1.9782) | Loss 5.9274(6.1391) | Error 0.6598(0.6917) Steps 514(519.49) | Grad Norm 2.0213(7.2060) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 45.4293(45.5026) | Bit/dim 4.9519(5.1441) | Xent 1.9127(1.9762) | Loss 5.9082(6.1322) | Error 0.6726(0.6911) Steps 514(519.33) | Grad Norm 2.1023(7.0529) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 44.4001(45.4695) | Bit/dim 4.9620(5.1386) | Xent 1.9299(1.9748) | Loss 5.9269(6.1260) | Error 0.6791(0.6908) Steps 508(518.99) | Grad Norm 3.3105(6.9406) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 45.1070(45.4586) | Bit/dim 4.9417(5.1327) | Xent 1.9129(1.9730) | Loss 5.8981(6.1192) | Error 0.6670(0.6901) Steps 508(518.66) | Grad Norm 2.1892(6.7981) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 44.0085(45.4151) | Bit/dim 4.9434(5.1270) | Xent 1.9153(1.9712) | Loss 5.9011(6.1127) | Error 0.6611(0.6892) Steps 508(518.34) | Grad Norm 3.2956(6.6930) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 22.0968, Epoch Time 306.1368(340.1813), Bit/dim 4.9346(best: 4.9743), Xent 1.8890, Loss 5.8791, Error 0.6517(best: 0.6522)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 45.3552(45.4133) | Bit/dim 4.9385(5.1214) | Xent 1.9035(1.9692) | Loss 5.8902(6.1060) | Error 0.6586(0.6883) Steps 508(518.03) | Grad Norm 2.3711(6.5633) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 45.0314(45.4019) | Bit/dim 4.9433(5.1160) | Xent 1.8937(1.9669) | Loss 5.8901(6.0995) | Error 0.6621(0.6875) Steps 502(517.55) | Grad Norm 4.2875(6.4951) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 44.6853(45.3804) | Bit/dim 4.9375(5.1107) | Xent 1.8953(1.9648) | Loss 5.8851(6.0931) | Error 0.6575(0.6866) Steps 502(517.08) | Grad Norm 5.4682(6.4643) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 44.4926(45.3537) | Bit/dim 4.9183(5.1049) | Xent 1.8956(1.9627) | Loss 5.8661(6.0863) | Error 0.6549(0.6856) Steps 514(516.99) | Grad Norm 5.7475(6.4427) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 45.5565(45.3598) | Bit/dim 4.9091(5.0990) | Xent 1.9011(1.9609) | Loss 5.8596(6.0795) | Error 0.6584(0.6848) Steps 508(516.72) | Grad Norm 4.8347(6.3945) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 44.9972(45.3490) | Bit/dim 4.9023(5.0931) | Xent 1.8738(1.9582) | Loss 5.8392(6.0723) | Error 0.6515(0.6838) Steps 514(516.64) | Grad Norm 3.3655(6.3036) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 23.0654, Epoch Time 308.8636(339.2418), Bit/dim 4.8977(best: 4.9346), Xent 1.8656, Loss 5.8305, Error 0.6422(best: 0.6517)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 45.3212(45.3481) | Bit/dim 4.8957(5.0872) | Xent 1.8654(1.9555) | Loss 5.8284(6.0649) | Error 0.6467(0.6827) Steps 514(516.56) | Grad Norm 1.8334(6.1695) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 45.3022(45.3467) | Bit/dim 4.8969(5.0815) | Xent 1.8890(1.9535) | Loss 5.8414(6.0582) | Error 0.6546(0.6819) Steps 514(516.48) | Grad Norm 2.0742(6.0467) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 46.1259(45.3701) | Bit/dim 4.8946(5.0759) | Xent 1.8722(1.9510) | Loss 5.8307(6.0514) | Error 0.6454(0.6808) Steps 520(516.59) | Grad Norm 2.0206(5.9259) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 45.3024(45.3681) | Bit/dim 4.8719(5.0698) | Xent 1.8858(1.9491) | Loss 5.8148(6.0443) | Error 0.6549(0.6800) Steps 514(516.51) | Grad Norm 1.8013(5.8022) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 44.9921(45.3568) | Bit/dim 4.8697(5.0638) | Xent 1.8870(1.9472) | Loss 5.8132(6.0374) | Error 0.6596(0.6794) Steps 514(516.43) | Grad Norm 4.0941(5.7509) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 45.0021(45.3462) | Bit/dim 4.8932(5.0587) | Xent 1.9181(1.9463) | Loss 5.8523(6.0318) | Error 0.6758(0.6793) Steps 514(516.36) | Grad Norm 11.9291(5.9363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.3605, Epoch Time 309.9363(338.3626), Bit/dim 4.9907(best: 4.8977), Xent 2.1288, Loss 6.0552, Error 0.7236(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 45.8462(45.3612) | Bit/dim 4.9962(5.0568) | Xent 2.1546(1.9526) | Loss 6.0734(6.0331) | Error 0.7295(0.6808) Steps 514(516.29) | Grad Norm 25.4396(6.5214) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 45.6163(45.3688) | Bit/dim 5.0582(5.0568) | Xent 2.2587(1.9618) | Loss 6.1876(6.0377) | Error 0.7691(0.6834) Steps 520(516.40) | Grad Norm 33.0019(7.3158) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 44.8288(45.3526) | Bit/dim 5.0188(5.0557) | Xent 2.1220(1.9666) | Loss 6.0798(6.0390) | Error 0.7550(0.6856) Steps 514(516.33) | Grad Norm 14.0660(7.5183) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 44.1240(45.3158) | Bit/dim 5.0284(5.0549) | Xent 2.0047(1.9677) | Loss 6.0307(6.0387) | Error 0.7166(0.6865) Steps 514(516.26) | Grad Norm 8.7063(7.5539) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 47.4590(45.3801) | Bit/dim 5.0399(5.0544) | Xent 2.0011(1.9687) | Loss 6.0405(6.0388) | Error 0.7030(0.6870) Steps 514(516.19) | Grad Norm 13.5536(7.7339) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 45.5930(45.3864) | Bit/dim 4.9597(5.0516) | Xent 1.9723(1.9688) | Loss 5.9459(6.0360) | Error 0.6886(0.6871) Steps 514(516.13) | Grad Norm 4.2004(7.6279) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.7524, Epoch Time 311.5005(337.5567), Bit/dim 4.9777(best: 4.8977), Xent 1.9700, Loss 5.9627, Error 0.7021(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 45.3445(45.3852) | Bit/dim 4.9774(5.0493) | Xent 1.9908(1.9695) | Loss 5.9728(6.0341) | Error 0.7096(0.6877) Steps 514(516.06) | Grad Norm 5.2742(7.5573) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 45.8129(45.3980) | Bit/dim 4.9820(5.0473) | Xent 2.0089(1.9707) | Loss 5.9865(6.0327) | Error 0.7172(0.6886) Steps 520(516.18) | Grad Norm 4.7610(7.4734) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 45.9902(45.4158) | Bit/dim 4.9456(5.0443) | Xent 1.9792(1.9709) | Loss 5.9352(6.0297) | Error 0.6983(0.6889) Steps 520(516.30) | Grad Norm 3.2455(7.3466) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 45.4039(45.4154) | Bit/dim 4.9683(5.0420) | Xent 1.9644(1.9707) | Loss 5.9505(6.0274) | Error 0.6927(0.6890) Steps 514(516.23) | Grad Norm 4.6898(7.2669) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 44.9630(45.4019) | Bit/dim 4.9647(5.0397) | Xent 1.9649(1.9706) | Loss 5.9472(6.0250) | Error 0.6905(0.6891) Steps 514(516.16) | Grad Norm 3.2786(7.1472) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 44.8360(45.3849) | Bit/dim 4.9497(5.0370) | Xent 1.9536(1.9701) | Loss 5.9265(6.0220) | Error 0.6864(0.6890) Steps 526(516.45) | Grad Norm 2.7921(7.0166) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.2890, Epoch Time 310.1754(336.7353), Bit/dim 4.9470(best: 4.8977), Xent 1.9595, Loss 5.9268, Error 0.6915(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 46.5288(45.4192) | Bit/dim 4.9611(5.0347) | Xent 1.9719(1.9701) | Loss 5.9471(6.0198) | Error 0.6980(0.6893) Steps 508(516.20) | Grad Norm 6.1531(6.9907) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 43.9942(45.3764) | Bit/dim 4.9504(5.0322) | Xent 1.9654(1.9700) | Loss 5.9331(6.0172) | Error 0.6887(0.6892) Steps 496(515.60) | Grad Norm 6.0197(6.9615) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 45.0933(45.3680) | Bit/dim 4.9260(5.0290) | Xent 1.9443(1.9692) | Loss 5.8981(6.0136) | Error 0.6836(0.6891) Steps 496(515.01) | Grad Norm 2.2329(6.8197) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 45.7944(45.3807) | Bit/dim 4.9179(5.0257) | Xent 2.0099(1.9704) | Loss 5.9228(6.0109) | Error 0.7063(0.6896) Steps 508(514.80) | Grad Norm 10.4216(6.9277) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 44.1945(45.3452) | Bit/dim 4.9721(5.0241) | Xent 1.9890(1.9710) | Loss 5.9666(6.0095) | Error 0.7097(0.6902) Steps 502(514.41) | Grad Norm 9.6667(7.0099) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 46.5220(45.3805) | Bit/dim 4.8908(5.0201) | Xent 1.9584(1.9706) | Loss 5.8701(6.0054) | Error 0.6859(0.6901) Steps 508(514.22) | Grad Norm 3.6894(6.9103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.5527, Epoch Time 311.0783(335.9656), Bit/dim 5.0027(best: 4.8977), Xent 1.9896, Loss 5.9975, Error 0.6991(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 46.5048(45.4142) | Bit/dim 4.9998(5.0194) | Xent 2.0234(1.9722) | Loss 6.0115(6.0055) | Error 0.7130(0.6907) Steps 520(514.39) | Grad Norm 19.7261(7.2948) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 42.7477(45.3342) | Bit/dim 5.3644(5.0298) | Xent 1.9504(1.9715) | Loss 6.3397(6.0156) | Error 0.6939(0.6908) Steps 496(513.84) | Grad Norm 9.5931(7.3637) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 42.0129(45.2346) | Bit/dim 5.5015(5.0439) | Xent 1.9653(1.9713) | Loss 6.4842(6.0296) | Error 0.6967(0.6910) Steps 496(513.31) | Grad Norm 8.4848(7.3973) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 43.7923(45.1913) | Bit/dim 5.2793(5.0510) | Xent 1.9522(1.9708) | Loss 6.2554(6.0364) | Error 0.6887(0.6910) Steps 490(512.61) | Grad Norm 7.0128(7.3858) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 44.0902(45.1583) | Bit/dim 5.0886(5.0521) | Xent 1.9702(1.9708) | Loss 6.0737(6.0375) | Error 0.6896(0.6909) Steps 496(512.11) | Grad Norm 8.2840(7.4128) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 45.0989(45.1565) | Bit/dim 5.0930(5.0534) | Xent 1.9895(1.9713) | Loss 6.0877(6.0390) | Error 0.7036(0.6913) Steps 490(511.45) | Grad Norm 10.3675(7.5014) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 22.4910, Epoch Time 302.3222(334.9563), Bit/dim 5.0753(best: 4.8977), Xent 1.9137, Loss 6.0321, Error 0.6700(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 44.6923(45.1426) | Bit/dim 5.0698(5.0539) | Xent 1.9269(1.9700) | Loss 6.0332(6.0388) | Error 0.6758(0.6908) Steps 502(511.16) | Grad Norm 4.7318(7.4183) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 44.0740(45.1105) | Bit/dim 5.0882(5.0549) | Xent 1.9118(1.9682) | Loss 6.0441(6.0390) | Error 0.6634(0.6900) Steps 502(510.89) | Grad Norm 5.2494(7.3532) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 44.1397(45.0814) | Bit/dim 5.0434(5.0545) | Xent 1.9449(1.9675) | Loss 6.0159(6.0383) | Error 0.6835(0.6898) Steps 502(510.62) | Grad Norm 4.6956(7.2735) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 45.8005(45.1029) | Bit/dim 5.0324(5.0539) | Xent 1.9483(1.9670) | Loss 6.0066(6.0374) | Error 0.6901(0.6898) Steps 502(510.36) | Grad Norm 4.2912(7.1840) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 43.6744(45.0601) | Bit/dim 5.0122(5.0526) | Xent 1.9467(1.9664) | Loss 5.9856(6.0358) | Error 0.6972(0.6900) Steps 502(510.11) | Grad Norm 5.1591(7.1233) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 44.3257(45.0381) | Bit/dim 4.9947(5.0509) | Xent 1.9165(1.9649) | Loss 5.9530(6.0333) | Error 0.6770(0.6897) Steps 496(509.69) | Grad Norm 3.2109(7.0059) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 21.8428, Epoch Time 304.2515(334.0351), Bit/dim 4.9551(best: 4.8977), Xent 1.8943, Loss 5.9023, Error 0.6597(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 43.9265(45.0047) | Bit/dim 4.9560(5.0480) | Xent 1.9096(1.9632) | Loss 5.9108(6.0296) | Error 0.6663(0.6889) Steps 496(509.28) | Grad Norm 3.1308(6.8897) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 43.4525(44.9581) | Bit/dim 4.9318(5.0446) | Xent 1.9078(1.9615) | Loss 5.8857(6.0253) | Error 0.6691(0.6884) Steps 502(509.06) | Grad Norm 3.8322(6.7979) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 45.2776(44.9677) | Bit/dim 4.9258(5.0410) | Xent 1.9087(1.9600) | Loss 5.8802(6.0210) | Error 0.6766(0.6880) Steps 508(509.03) | Grad Norm 6.6213(6.7926) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 44.6360(44.9578) | Bit/dim 4.9245(5.0375) | Xent 1.9681(1.9602) | Loss 5.9085(6.0176) | Error 0.6995(0.6883) Steps 502(508.82) | Grad Norm 8.9067(6.8561) | Total Time 14.00(14.00)\n",
      "Iter 0311 | Time 46.9703(45.0182) | Bit/dim 4.9037(5.0335) | Xent 1.9713(1.9605) | Loss 5.8893(6.0137) | Error 0.7000(0.6887) Steps 520(509.15) | Grad Norm 10.6929(6.9712) | Total Time 14.00(14.00)\n",
      "Iter 0312 | Time 44.8846(45.0141) | Bit/dim 4.9186(5.0300) | Xent 1.9731(1.9609) | Loss 5.9052(6.0105) | Error 0.6951(0.6889) Steps 508(509.12) | Grad Norm 10.5060(7.0772) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 23.4864, Epoch Time 308.2336(333.2611), Bit/dim 4.9036(best: 4.8977), Xent 1.9184, Loss 5.8628, Error 0.6854(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 48.1378(45.1079) | Bit/dim 4.9138(5.0265) | Xent 1.9463(1.9605) | Loss 5.8870(6.0068) | Error 0.6789(0.6886) Steps 532(509.80) | Grad Norm 10.2099(7.1712) | Total Time 14.00(14.00)\n",
      "Iter 0314 | Time 44.1423(45.0789) | Bit/dim 4.9252(5.0235) | Xent 1.9822(1.9611) | Loss 5.9163(6.0041) | Error 0.7139(0.6893) Steps 502(509.57) | Grad Norm 8.6139(7.2145) | Total Time 14.00(14.00)\n",
      "Iter 0315 | Time 44.6759(45.0668) | Bit/dim 4.8933(5.0196) | Xent 1.9377(1.9604) | Loss 5.8621(5.9998) | Error 0.6869(0.6893) Steps 508(509.52) | Grad Norm 6.6982(7.1990) | Total Time 14.00(14.00)\n",
      "Iter 0316 | Time 48.2137(45.1612) | Bit/dim 4.8816(5.0155) | Xent 1.9476(1.9600) | Loss 5.8554(5.9955) | Error 0.6865(0.6892) Steps 502(509.30) | Grad Norm 5.7578(7.1557) | Total Time 14.00(14.00)\n",
      "Iter 0317 | Time 45.5242(45.1721) | Bit/dim 4.9092(5.0123) | Xent 1.9259(1.9590) | Loss 5.8721(5.9918) | Error 0.6761(0.6888) Steps 514(509.44) | Grad Norm 8.3571(7.1918) | Total Time 14.00(14.00)\n",
      "Iter 0318 | Time 46.4960(45.2118) | Bit/dim 4.8548(5.0075) | Xent 1.9205(1.9579) | Loss 5.8150(5.9865) | Error 0.6747(0.6884) Steps 514(509.58) | Grad Norm 3.1471(7.0705) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 22.6073, Epoch Time 315.3296(332.7232), Bit/dim 4.8990(best: 4.8977), Xent 1.9102, Loss 5.8541, Error 0.6705(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 45.7141(45.2269) | Bit/dim 4.8885(5.0040) | Xent 1.9363(1.9572) | Loss 5.8566(5.9826) | Error 0.6796(0.6881) Steps 514(509.71) | Grad Norm 9.7037(7.1494) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 46.4852(45.2646) | Bit/dim 4.8803(5.0003) | Xent 1.9511(1.9570) | Loss 5.8558(5.9788) | Error 0.6886(0.6881) Steps 502(509.48) | Grad Norm 7.9075(7.1722) | Total Time 14.00(14.00)\n",
      "Iter 0321 | Time 45.0001(45.2567) | Bit/dim 4.8522(4.9958) | Xent 1.8949(1.9552) | Loss 5.7997(5.9734) | Error 0.6596(0.6873) Steps 508(509.43) | Grad Norm 3.6948(7.0679) | Total Time 14.00(14.00)\n",
      "Iter 0322 | Time 46.8183(45.3035) | Bit/dim 4.9081(4.9932) | Xent 1.9358(1.9546) | Loss 5.8760(5.9705) | Error 0.6863(0.6872) Steps 532(510.11) | Grad Norm 14.7643(7.2988) | Total Time 14.00(14.00)\n",
      "Iter 0323 | Time 45.5934(45.3122) | Bit/dim 4.9708(4.9925) | Xent 1.9330(1.9539) | Loss 5.9374(5.9695) | Error 0.6813(0.6871) Steps 502(509.87) | Grad Norm 10.5952(7.3977) | Total Time 14.00(14.00)\n",
      "Iter 0324 | Time 44.8642(45.2988) | Bit/dim 4.9132(4.9901) | Xent 1.9280(1.9532) | Loss 5.8772(5.9667) | Error 0.6794(0.6868) Steps 508(509.81) | Grad Norm 8.3364(7.4258) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.9355, Epoch Time 312.7857(332.1250), Bit/dim 4.9778(best: 4.8977), Xent 1.9127, Loss 5.9341, Error 0.6749(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 47.1005(45.3529) | Bit/dim 4.9917(4.9902) | Xent 1.9548(1.9532) | Loss 5.9690(5.9668) | Error 0.6877(0.6869) Steps 514(509.94) | Grad Norm 19.4774(7.7874) | Total Time 14.00(14.00)\n",
      "Iter 0326 | Time 45.1924(45.3480) | Bit/dim 4.9917(4.9902) | Xent 1.9054(1.9518) | Loss 5.9444(5.9661) | Error 0.6630(0.6861) Steps 502(509.70) | Grad Norm 7.2870(7.7724) | Total Time 14.00(14.00)\n",
      "Iter 0327 | Time 44.5176(45.3231) | Bit/dim 5.0624(4.9924) | Xent 1.9230(1.9509) | Loss 6.0239(5.9678) | Error 0.6767(0.6859) Steps 496(509.29) | Grad Norm 8.2293(7.7861) | Total Time 14.00(14.00)\n",
      "Iter 0328 | Time 44.1666(45.2884) | Bit/dim 4.8694(4.9887) | Xent 1.9135(1.9498) | Loss 5.8262(5.9636) | Error 0.6716(0.6854) Steps 490(508.71) | Grad Norm 5.4995(7.7175) | Total Time 14.00(14.00)\n",
      "Iter 0329 | Time 46.7414(45.3320) | Bit/dim 4.9740(4.9883) | Xent 1.9337(1.9493) | Loss 5.9409(5.9629) | Error 0.6726(0.6851) Steps 508(508.69) | Grad Norm 16.2057(7.9721) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 44.2177(45.2986) | Bit/dim 4.9673(4.9876) | Xent 1.9169(1.9483) | Loss 5.9257(5.9618) | Error 0.6723(0.6847) Steps 490(508.13) | Grad Norm 6.1321(7.9169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 21.7978, Epoch Time 308.9631(331.4302), Bit/dim 5.0280(best: 4.8977), Xent 1.8713, Loss 5.9636, Error 0.6405(best: 0.6422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 44.2154(45.2661) | Bit/dim 5.0279(4.9888) | Xent 1.8883(1.9465) | Loss 5.9720(5.9621) | Error 0.6550(0.6838) Steps 490(507.58) | Grad Norm 6.7101(7.8807) | Total Time 14.00(14.00)\n",
      "Iter 0332 | Time 42.8452(45.1935) | Bit/dim 4.9021(4.9862) | Xent 1.9155(1.9456) | Loss 5.8599(5.9590) | Error 0.6754(0.6835) Steps 484(506.88) | Grad Norm 5.8375(7.8194) | Total Time 14.00(14.00)\n",
      "Iter 0333 | Time 43.5745(45.1449) | Bit/dim 4.9030(4.9837) | Xent 1.9248(1.9450) | Loss 5.8654(5.9562) | Error 0.6740(0.6832) Steps 490(506.37) | Grad Norm 7.0506(7.7963) | Total Time 14.00(14.00)\n",
      "Iter 0334 | Time 44.7892(45.1342) | Bit/dim 4.8705(4.9803) | Xent 1.9024(1.9437) | Loss 5.8217(5.9522) | Error 0.6686(0.6828) Steps 502(506.24) | Grad Norm 6.8982(7.7694) | Total Time 14.00(14.00)\n",
      "Iter 0335 | Time 42.5883(45.0578) | Bit/dim 4.8543(4.9766) | Xent 1.8961(1.9423) | Loss 5.8023(5.9477) | Error 0.6607(0.6821) Steps 490(505.75) | Grad Norm 3.2207(7.6329) | Total Time 14.00(14.00)\n",
      "Iter 0336 | Time 43.9063(45.0233) | Bit/dim 4.8661(4.9733) | Xent 1.9294(1.9419) | Loss 5.8308(5.9442) | Error 0.6851(0.6822) Steps 490(505.28) | Grad Norm 6.6948(7.6048) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 21.6389, Epoch Time 298.9367(330.4554), Bit/dim 4.8298(best: 4.8977), Xent 1.8627, Loss 5.7611, Error 0.6507(best: 0.6405)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 42.7546(44.9552) | Bit/dim 4.8251(4.9688) | Xent 1.8757(1.9399) | Loss 5.7629(5.9388) | Error 0.6640(0.6817) Steps 490(504.82) | Grad Norm 3.2587(7.4744) | Total Time 14.00(14.00)\n",
      "Iter 0338 | Time 42.4098(44.8789) | Bit/dim 4.8100(4.9640) | Xent 1.8964(1.9386) | Loss 5.7582(5.9333) | Error 0.6709(0.6814) Steps 484(504.20) | Grad Norm 4.6135(7.3886) | Total Time 14.00(14.00)\n",
      "Iter 0339 | Time 45.6762(44.9028) | Bit/dim 4.8209(4.9598) | Xent 1.8693(1.9365) | Loss 5.7556(5.9280) | Error 0.6511(0.6805) Steps 502(504.13) | Grad Norm 3.9357(7.2850) | Total Time 14.00(14.00)\n",
      "Iter 0340 | Time 44.2700(44.8838) | Bit/dim 4.7948(4.9548) | Xent 1.8758(1.9347) | Loss 5.7327(5.9221) | Error 0.6580(0.6798) Steps 502(504.07) | Grad Norm 4.6414(7.2057) | Total Time 14.00(14.00)\n",
      "Iter 0341 | Time 45.6141(44.9057) | Bit/dim 4.8056(4.9503) | Xent 1.8666(1.9326) | Loss 5.7389(5.9167) | Error 0.6475(0.6788) Steps 502(504.00) | Grad Norm 4.9706(7.1386) | Total Time 14.00(14.00)\n",
      "Iter 0342 | Time 44.7866(44.9021) | Bit/dim 4.8006(4.9458) | Xent 1.8476(1.9301) | Loss 5.7243(5.9109) | Error 0.6415(0.6777) Steps 502(503.94) | Grad Norm 3.7274(7.0363) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 21.8587, Epoch Time 302.8359(329.6268), Bit/dim 4.7838(best: 4.8298), Xent 1.8331, Loss 5.7003, Error 0.6392(best: 0.6405)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 46.1226(44.9388) | Bit/dim 4.7812(4.9409) | Xent 1.8581(1.9279) | Loss 5.7103(5.9049) | Error 0.6542(0.6770) Steps 508(504.07) | Grad Norm 5.1175(6.9787) | Total Time 14.00(14.00)\n",
      "Iter 0344 | Time 44.7876(44.9342) | Bit/dim 4.7657(4.9356) | Xent 1.8480(1.9255) | Loss 5.6897(5.8984) | Error 0.6501(0.6762) Steps 502(504.00) | Grad Norm 2.5679(6.8464) | Total Time 14.00(14.00)\n",
      "Iter 0345 | Time 43.4873(44.8908) | Bit/dim 4.7823(4.9310) | Xent 1.8430(1.9231) | Loss 5.7038(5.8926) | Error 0.6430(0.6752) Steps 502(503.94) | Grad Norm 3.4457(6.7444) | Total Time 14.00(14.00)\n",
      "Iter 0346 | Time 45.4728(44.9083) | Bit/dim 4.7710(4.9262) | Xent 1.8415(1.9206) | Loss 5.6918(5.8865) | Error 0.6376(0.6741) Steps 502(503.88) | Grad Norm 3.9605(6.6609) | Total Time 14.00(14.00)\n",
      "Iter 0347 | Time 44.4114(44.8934) | Bit/dim 4.7629(4.9213) | Xent 1.8440(1.9183) | Loss 5.6849(5.8805) | Error 0.6371(0.6730) Steps 508(504.01) | Grad Norm 5.3516(6.6216) | Total Time 14.00(14.00)\n",
      "Iter 0348 | Time 45.5897(44.9143) | Bit/dim 4.7902(4.9174) | Xent 1.9265(1.9186) | Loss 5.7534(5.8767) | Error 0.6745(0.6730) Steps 502(503.95) | Grad Norm 16.4970(6.9179) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 21.7995, Epoch Time 306.7234(328.9397), Bit/dim 4.9978(best: 4.7838), Xent 2.2464, Loss 6.1210, Error 0.7706(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 45.3446(44.9272) | Bit/dim 4.9917(4.9196) | Xent 2.2943(1.9298) | Loss 6.1389(5.8846) | Error 0.7796(0.6762) Steps 514(504.25) | Grad Norm 17.4992(7.2353) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 46.6979(44.9803) | Bit/dim 4.8702(4.9182) | Xent 2.0205(1.9326) | Loss 5.8805(5.8844) | Error 0.7077(0.6771) Steps 514(504.54) | Grad Norm 16.7454(7.5206) | Total Time 14.00(14.00)\n",
      "Iter 0351 | Time 46.5151(45.0263) | Bit/dim 5.0008(4.9206) | Xent 1.9723(1.9338) | Loss 5.9870(5.8875) | Error 0.6874(0.6774) Steps 538(505.55) | Grad Norm 14.5686(7.7320) | Total Time 14.00(14.00)\n",
      "Iter 0352 | Time 46.9943(45.0854) | Bit/dim 5.0632(4.9249) | Xent 1.9583(1.9345) | Loss 6.0424(5.8922) | Error 0.6847(0.6777) Steps 538(506.52) | Grad Norm 12.7649(7.8830) | Total Time 14.00(14.00)\n",
      "Iter 0353 | Time 45.6466(45.1022) | Bit/dim 4.8902(4.9239) | Xent 1.9891(1.9361) | Loss 5.8848(5.8919) | Error 0.7171(0.6789) Steps 520(506.92) | Grad Norm 5.4241(7.8093) | Total Time 14.00(14.00)\n",
      "Iter 0354 | Time 46.7800(45.1525) | Bit/dim 4.9007(4.9232) | Xent 1.9652(1.9370) | Loss 5.8833(5.8917) | Error 0.6994(0.6795) Steps 514(507.14) | Grad Norm 10.6467(7.8944) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 22.2035, Epoch Time 315.3551(328.5321), Bit/dim 4.9764(best: 4.7838), Xent 1.9304, Loss 5.9416, Error 0.6789(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 44.9055(45.1451) | Bit/dim 4.9804(4.9249) | Xent 1.9263(1.9367) | Loss 5.9436(5.8932) | Error 0.6793(0.6795) Steps 508(507.16) | Grad Norm 7.9278(7.8954) | Total Time 14.00(14.00)\n",
      "Iter 0356 | Time 44.2456(45.1182) | Bit/dim 4.9602(4.9260) | Xent 1.9259(1.9364) | Loss 5.9232(5.8941) | Error 0.6813(0.6795) Steps 502(507.01) | Grad Norm 7.2686(7.8766) | Total Time 14.00(14.00)\n",
      "Iter 0357 | Time 47.9543(45.2032) | Bit/dim 4.9171(4.9257) | Xent 1.9442(1.9366) | Loss 5.8892(5.8940) | Error 0.6901(0.6798) Steps 526(507.58) | Grad Norm 11.7244(7.9920) | Total Time 14.00(14.00)\n",
      "Iter 0358 | Time 45.7069(45.2183) | Bit/dim 4.8393(4.9231) | Xent 1.9602(1.9373) | Loss 5.8194(5.8917) | Error 0.6960(0.6803) Steps 514(507.77) | Grad Norm 3.6511(7.8618) | Total Time 14.00(14.00)\n",
      "Iter 0359 | Time 44.8338(45.2068) | Bit/dim 4.8453(4.9208) | Xent 1.9093(1.9365) | Loss 5.8000(5.8890) | Error 0.6673(0.6799) Steps 496(507.42) | Grad Norm 4.6477(7.7654) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 45.0768(45.2029) | Bit/dim 4.8356(4.9182) | Xent 1.9238(1.9361) | Loss 5.7975(5.8862) | Error 0.6860(0.6801) Steps 490(506.89) | Grad Norm 5.5801(7.6998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 21.9510, Epoch Time 309.9783(327.9755), Bit/dim 4.7961(best: 4.7838), Xent 1.8721, Loss 5.7322, Error 0.6466(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 43.4828(45.1513) | Bit/dim 4.7938(4.9145) | Xent 1.8905(1.9347) | Loss 5.7390(5.8818) | Error 0.6647(0.6796) Steps 490(506.39) | Grad Norm 2.9078(7.5560) | Total Time 14.00(14.00)\n",
      "Iter 0362 | Time 44.8674(45.1428) | Bit/dim 4.8322(4.9120) | Xent 1.8918(1.9334) | Loss 5.7781(5.8787) | Error 0.6559(0.6789) Steps 502(506.26) | Grad Norm 3.0087(7.4196) | Total Time 14.00(14.00)\n",
      "Iter 0363 | Time 46.8225(45.1932) | Bit/dim 4.7999(4.9086) | Xent 1.9217(1.9331) | Loss 5.7608(5.8752) | Error 0.6673(0.6786) Steps 508(506.31) | Grad Norm 4.4993(7.3320) | Total Time 14.00(14.00)\n",
      "Iter 0364 | Time 44.1860(45.1630) | Bit/dim 4.7684(4.9044) | Xent 1.9052(1.9322) | Loss 5.7210(5.8706) | Error 0.6674(0.6782) Steps 496(506.00) | Grad Norm 3.0974(7.2050) | Total Time 14.00(14.00)\n",
      "Iter 0365 | Time 44.0826(45.1306) | Bit/dim 4.7740(4.9005) | Xent 1.8857(1.9308) | Loss 5.7169(5.8659) | Error 0.6639(0.6778) Steps 490(505.52) | Grad Norm 3.4657(7.0928) | Total Time 14.00(14.00)\n",
      "Iter 0366 | Time 48.8942(45.2435) | Bit/dim 4.7889(4.8972) | Xent 1.8892(1.9296) | Loss 5.7335(5.8620) | Error 0.6663(0.6775) Steps 490(505.05) | Grad Norm 6.7669(7.0830) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 21.6875, Epoch Time 309.3442(327.4166), Bit/dim 4.7838(best: 4.7838), Xent 1.8635, Loss 5.7156, Error 0.6485(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 44.3463(45.2165) | Bit/dim 4.7869(4.8939) | Xent 1.8820(1.9282) | Loss 5.7279(5.8580) | Error 0.6644(0.6771) Steps 496(504.78) | Grad Norm 5.2603(7.0283) | Total Time 14.00(14.00)\n",
      "Iter 0368 | Time 45.5729(45.2272) | Bit/dim 4.7430(4.8893) | Xent 1.8865(1.9269) | Loss 5.6862(5.8528) | Error 0.6587(0.6765) Steps 496(504.52) | Grad Norm 2.7486(6.9000) | Total Time 14.00(14.00)\n",
      "Iter 0369 | Time 47.7588(45.3032) | Bit/dim 4.7524(4.8852) | Xent 1.8954(1.9260) | Loss 5.7001(5.8482) | Error 0.6713(0.6764) Steps 502(504.44) | Grad Norm 8.5872(6.9506) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 44.6453(45.2834) | Bit/dim 4.8668(4.8847) | Xent 1.9064(1.9254) | Loss 5.8200(5.8474) | Error 0.6666(0.6761) Steps 502(504.37) | Grad Norm 9.0359(7.0131) | Total Time 14.00(14.00)\n",
      "Iter 0371 | Time 45.4217(45.2876) | Bit/dim 4.7428(4.8804) | Xent 1.8883(1.9243) | Loss 5.6870(5.8426) | Error 0.6581(0.6755) Steps 502(504.30) | Grad Norm 3.7105(6.9140) | Total Time 14.00(14.00)\n",
      "Iter 0372 | Time 47.5802(45.3564) | Bit/dim 4.8890(4.8807) | Xent 1.9516(1.9251) | Loss 5.8648(5.8432) | Error 0.6764(0.6756) Steps 514(504.59) | Grad Norm 19.9727(7.3058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 22.6761, Epoch Time 313.0424(326.9854), Bit/dim 5.2808(best: 4.7838), Xent 1.9760, Loss 6.2688, Error 0.6992(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 41.5117(45.2410) | Bit/dim 5.2835(4.8928) | Xent 1.9854(1.9269) | Loss 6.2762(5.8562) | Error 0.7112(0.6766) Steps 502(504.51) | Grad Norm 9.7908(7.3804) | Total Time 14.00(14.00)\n",
      "Iter 0374 | Time 42.9558(45.1725) | Bit/dim 5.4496(4.9095) | Xent 1.9762(1.9284) | Loss 6.4377(5.8737) | Error 0.7059(0.6775) Steps 502(504.44) | Grad Norm 8.3250(7.4087) | Total Time 14.00(14.00)\n",
      "Iter 0375 | Time 43.3993(45.1193) | Bit/dim 5.2182(4.9187) | Xent 1.9523(1.9291) | Loss 6.1944(5.8833) | Error 0.6971(0.6781) Steps 502(504.36) | Grad Norm 7.2173(7.4030) | Total Time 14.00(14.00)\n",
      "Iter 0376 | Time 46.5452(45.1621) | Bit/dim 4.9592(4.9200) | Xent 1.9852(1.9308) | Loss 5.9518(5.8853) | Error 0.7150(0.6792) Steps 508(504.47) | Grad Norm 8.0562(7.4226) | Total Time 14.00(14.00)\n",
      "Iter 0377 | Time 49.2284(45.2841) | Bit/dim 4.9724(4.9215) | Xent 1.9766(1.9321) | Loss 5.9607(5.8876) | Error 0.6926(0.6796) Steps 532(505.30) | Grad Norm 11.6495(7.5494) | Total Time 14.00(14.00)\n",
      "Iter 0378 | Time 44.7890(45.2692) | Bit/dim 4.9941(4.9237) | Xent 1.9493(1.9327) | Loss 5.9688(5.8900) | Error 0.6833(0.6797) Steps 502(505.20) | Grad Norm 7.3304(7.5428) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 22.3083, Epoch Time 305.8481(326.3512), Bit/dim 4.9816(best: 4.7838), Xent 1.8815, Loss 5.9224, Error 0.6542(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 45.2303(45.2680) | Bit/dim 4.9821(4.9255) | Xent 1.8896(1.9314) | Loss 5.9269(5.8911) | Error 0.6596(0.6791) Steps 508(505.28) | Grad Norm 5.9854(7.4961) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 46.2938(45.2988) | Bit/dim 4.9504(4.9262) | Xent 1.9605(1.9322) | Loss 5.9306(5.8923) | Error 0.6967(0.6796) Steps 514(505.54) | Grad Norm 6.0414(7.4524) | Total Time 14.00(14.00)\n",
      "Iter 0381 | Time 46.3241(45.3296) | Bit/dim 4.9400(4.9266) | Xent 1.9189(1.9318) | Loss 5.8994(5.8925) | Error 0.6784(0.6796) Steps 508(505.62) | Grad Norm 4.6595(7.3686) | Total Time 14.00(14.00)\n",
      "Iter 0382 | Time 44.0051(45.2898) | Bit/dim 4.9067(4.9260) | Xent 1.9117(1.9312) | Loss 5.8625(5.8916) | Error 0.6800(0.6796) Steps 502(505.51) | Grad Norm 4.5602(7.2844) | Total Time 14.00(14.00)\n",
      "Iter 0383 | Time 44.9692(45.2802) | Bit/dim 4.8921(4.9250) | Xent 1.8912(1.9300) | Loss 5.8377(5.8900) | Error 0.6701(0.6793) Steps 508(505.58) | Grad Norm 3.7521(7.1784) | Total Time 14.00(14.00)\n",
      "Iter 0384 | Time 44.8962(45.2687) | Bit/dim 4.8531(4.9228) | Xent 1.8979(1.9291) | Loss 5.8020(5.8874) | Error 0.6751(0.6792) Steps 514(505.84) | Grad Norm 4.0289(7.0839) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 22.0307, Epoch Time 309.0190(325.8313), Bit/dim 4.8252(best: 4.7838), Xent 1.8878, Loss 5.7691, Error 0.6605(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 44.6429(45.2499) | Bit/dim 4.8318(4.9201) | Xent 1.9226(1.9289) | Loss 5.7931(5.8846) | Error 0.6706(0.6789) Steps 508(505.90) | Grad Norm 4.5246(7.0072) | Total Time 14.00(14.00)\n",
      "Iter 0386 | Time 45.0672(45.2444) | Bit/dim 4.8026(4.9166) | Xent 1.9115(1.9284) | Loss 5.7583(5.8808) | Error 0.6793(0.6790) Steps 508(505.96) | Grad Norm 7.1846(7.0125) | Total Time 14.00(14.00)\n",
      "Iter 0387 | Time 45.3208(45.2467) | Bit/dim 4.7890(4.9128) | Xent 1.9394(1.9287) | Loss 5.7586(5.8771) | Error 0.6925(0.6794) Steps 508(506.03) | Grad Norm 8.2536(7.0497) | Total Time 14.00(14.00)\n",
      "Iter 0388 | Time 45.0783(45.2417) | Bit/dim 4.7823(4.9088) | Xent 1.9654(1.9298) | Loss 5.7650(5.8737) | Error 0.6994(0.6800) Steps 508(506.09) | Grad Norm 9.4332(7.1212) | Total Time 14.00(14.00)\n",
      "Iter 0389 | Time 45.1321(45.2384) | Bit/dim 4.8143(4.9060) | Xent 1.9349(1.9299) | Loss 5.7817(5.8710) | Error 0.6883(0.6802) Steps 508(506.14) | Grad Norm 11.8936(7.2644) | Total Time 14.00(14.00)\n",
      "Iter 0390 | Time 45.2955(45.2401) | Bit/dim 4.7829(4.9023) | Xent 1.9998(1.9320) | Loss 5.7828(5.8683) | Error 0.7046(0.6809) Steps 502(506.02) | Grad Norm 12.9309(7.4344) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 22.1991, Epoch Time 307.9124(325.2937), Bit/dim 4.7710(best: 4.7838), Xent 1.9647, Loss 5.7533, Error 0.7013(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 44.6263(45.2217) | Bit/dim 4.7611(4.8981) | Xent 1.9743(1.9333) | Loss 5.7483(5.8647) | Error 0.7005(0.6815) Steps 508(506.08) | Grad Norm 12.5558(7.5880) | Total Time 14.00(14.00)\n",
      "Iter 0392 | Time 43.5064(45.1702) | Bit/dim 4.7719(4.8943) | Xent 1.9150(1.9328) | Loss 5.7294(5.8607) | Error 0.6784(0.6814) Steps 502(505.96) | Grad Norm 6.7929(7.5642) | Total Time 14.00(14.00)\n",
      "Iter 0393 | Time 46.6679(45.2152) | Bit/dim 4.7498(4.8900) | Xent 1.8928(1.9316) | Loss 5.6962(5.8557) | Error 0.6731(0.6812) Steps 508(506.02) | Grad Norm 5.0879(7.4899) | Total Time 14.00(14.00)\n",
      "Iter 0394 | Time 46.9632(45.2676) | Bit/dim 4.7406(4.8855) | Xent 1.9480(1.9321) | Loss 5.7145(5.8515) | Error 0.6983(0.6817) Steps 496(505.72) | Grad Norm 8.6250(7.5239) | Total Time 14.00(14.00)\n",
      "Iter 0395 | Time 45.3930(45.2714) | Bit/dim 4.7469(4.8813) | Xent 1.8782(1.9304) | Loss 5.6860(5.8465) | Error 0.6620(0.6811) Steps 496(505.42) | Grad Norm 5.2694(7.4563) | Total Time 14.00(14.00)\n",
      "Iter 0396 | Time 47.3135(45.3326) | Bit/dim 4.7146(4.8763) | Xent 1.9138(1.9299) | Loss 5.6716(5.8413) | Error 0.6810(0.6811) Steps 508(505.50) | Grad Norm 5.2264(7.3894) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 21.9962, Epoch Time 311.7100(324.8862), Bit/dim 4.7202(best: 4.7710), Xent 1.8464, Loss 5.6434, Error 0.6497(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 48.3041(45.4218) | Bit/dim 4.7244(4.8718) | Xent 1.8614(1.9279) | Loss 5.6551(5.8357) | Error 0.6589(0.6804) Steps 514(505.76) | Grad Norm 5.8513(7.3433) | Total Time 14.00(14.00)\n",
      "Iter 0398 | Time 43.2944(45.3580) | Bit/dim 4.7441(4.8679) | Xent 1.8679(1.9261) | Loss 5.6780(5.8310) | Error 0.6558(0.6797) Steps 508(505.82) | Grad Norm 3.7042(7.2341) | Total Time 14.00(14.00)\n",
      "Iter 0399 | Time 46.7255(45.3990) | Bit/dim 4.6984(4.8628) | Xent 1.8708(1.9244) | Loss 5.6338(5.8251) | Error 0.6680(0.6793) Steps 514(506.07) | Grad Norm 3.0824(7.1095) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 47.0780(45.4493) | Bit/dim 4.7306(4.8589) | Xent 1.8727(1.9229) | Loss 5.6669(5.8203) | Error 0.6646(0.6789) Steps 508(506.13) | Grad Norm 8.3781(7.1476) | Total Time 14.00(14.00)\n",
      "Iter 0401 | Time 44.7594(45.4286) | Bit/dim 4.8208(4.8577) | Xent 1.8448(1.9205) | Loss 5.7431(5.8180) | Error 0.6424(0.6778) Steps 508(506.18) | Grad Norm 8.1220(7.1768) | Total Time 14.00(14.00)\n",
      "Iter 0402 | Time 45.0671(45.4178) | Bit/dim 4.7532(4.8546) | Xent 1.8541(1.9185) | Loss 5.6803(5.8139) | Error 0.6472(0.6769) Steps 508(506.24) | Grad Norm 5.1771(7.1168) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 22.6658, Epoch Time 313.1525(324.5342), Bit/dim 4.8468(best: 4.7202), Xent 1.8263, Loss 5.7600, Error 0.6432(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 47.3404(45.4755) | Bit/dim 4.8391(4.8541) | Xent 1.8614(1.9168) | Loss 5.7698(5.8125) | Error 0.6598(0.6764) Steps 526(506.83) | Grad Norm 16.6388(7.4025) | Total Time 14.00(14.00)\n",
      "Iter 0404 | Time 46.4115(45.5036) | Bit/dim 5.0871(4.8611) | Xent 1.9312(1.9173) | Loss 6.0527(5.8197) | Error 0.6910(0.6768) Steps 526(507.41) | Grad Norm 13.9009(7.5974) | Total Time 14.00(14.00)\n",
      "Iter 0405 | Time 44.2989(45.4674) | Bit/dim 5.1694(4.8704) | Xent 1.8598(1.9155) | Loss 6.0993(5.8281) | Error 0.6522(0.6761) Steps 508(507.42) | Grad Norm 6.5302(7.5654) | Total Time 14.00(14.00)\n",
      "Iter 0406 | Time 45.2048(45.4595) | Bit/dim 4.9461(4.8726) | Xent 2.0011(1.9181) | Loss 5.9467(5.8317) | Error 0.7087(0.6771) Steps 508(507.44) | Grad Norm 11.7101(7.6898) | Total Time 14.00(14.00)\n",
      "Iter 0407 | Time 47.5073(45.5210) | Bit/dim 4.8925(4.8732) | Xent 2.0044(1.9207) | Loss 5.8947(5.8336) | Error 0.7274(0.6786) Steps 526(508.00) | Grad Norm 13.3658(7.8600) | Total Time 14.00(14.00)\n",
      "Iter 0408 | Time 44.9363(45.5034) | Bit/dim 4.8898(4.8737) | Xent 1.9846(1.9226) | Loss 5.8821(5.8350) | Error 0.7174(0.6797) Steps 520(508.36) | Grad Norm 11.7623(7.9771) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 22.7986, Epoch Time 313.7383(324.2103), Bit/dim 4.8840(best: 4.7202), Xent 1.8657, Loss 5.8169, Error 0.6483(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 46.2934(45.5271) | Bit/dim 4.8772(4.8738) | Xent 1.9016(1.9220) | Loss 5.8280(5.8348) | Error 0.6665(0.6793) Steps 526(508.89) | Grad Norm 6.9936(7.9476) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 46.2936(45.5501) | Bit/dim 4.9585(4.8764) | Xent 1.9649(1.9233) | Loss 5.9409(5.8380) | Error 0.6873(0.6796) Steps 532(509.58) | Grad Norm 12.4166(8.0817) | Total Time 14.00(14.00)\n",
      "Iter 0411 | Time 46.3384(45.5738) | Bit/dim 4.7986(4.8740) | Xent 1.9021(1.9226) | Loss 5.7497(5.8354) | Error 0.6743(0.6794) Steps 520(509.89) | Grad Norm 3.4897(7.9439) | Total Time 14.00(14.00)\n",
      "Iter 0412 | Time 46.9329(45.6146) | Bit/dim 4.8301(4.8727) | Xent 1.8981(1.9219) | Loss 5.7792(5.8337) | Error 0.6698(0.6791) Steps 544(510.92) | Grad Norm 6.1170(7.8891) | Total Time 14.00(14.00)\n",
      "Iter 0413 | Time 50.6317(45.7651) | Bit/dim 4.7904(4.8703) | Xent 1.8949(1.9211) | Loss 5.7379(5.8308) | Error 0.6701(0.6789) Steps 550(512.09) | Grad Norm 4.6225(7.7911) | Total Time 14.00(14.00)\n",
      "Iter 0414 | Time 49.6920(45.8829) | Bit/dim 4.7850(4.8677) | Xent 1.8840(1.9200) | Loss 5.7270(5.8277) | Error 0.6671(0.6785) Steps 550(513.23) | Grad Norm 5.4908(7.7221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 23.0680, Epoch Time 324.6313(324.2229), Bit/dim 4.7658(best: 4.7202), Xent 1.8419, Loss 5.6868, Error 0.6472(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 47.8228(45.9411) | Bit/dim 4.7679(4.8647) | Xent 1.8432(1.9177) | Loss 5.6895(5.8235) | Error 0.6454(0.6775) Steps 544(514.15) | Grad Norm 4.4082(7.6227) | Total Time 14.00(14.00)\n",
      "Iter 0416 | Time 46.5356(45.9589) | Bit/dim 4.7526(4.8613) | Xent 1.8795(1.9165) | Loss 5.6924(5.8196) | Error 0.6634(0.6771) Steps 526(514.50) | Grad Norm 3.8113(7.5083) | Total Time 14.00(14.00)\n",
      "Iter 0417 | Time 47.3153(45.9996) | Bit/dim 4.7223(4.8572) | Xent 1.8513(1.9146) | Loss 5.6480(5.8145) | Error 0.6456(0.6761) Steps 532(515.03) | Grad Norm 3.5707(7.3902) | Total Time 14.00(14.00)\n",
      "Iter 0418 | Time 47.9784(46.0590) | Bit/dim 4.7261(4.8532) | Xent 1.8824(1.9136) | Loss 5.6673(5.8100) | Error 0.6585(0.6756) Steps 526(515.36) | Grad Norm 3.9956(7.2884) | Total Time 14.00(14.00)\n",
      "Iter 0419 | Time 48.0660(46.1192) | Bit/dim 4.7025(4.8487) | Xent 1.8400(1.9114) | Loss 5.6225(5.8044) | Error 0.6441(0.6747) Steps 526(515.68) | Grad Norm 3.2887(7.1684) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 47.7143(46.1670) | Bit/dim 4.6783(4.8436) | Xent 1.8523(1.9096) | Loss 5.6044(5.7984) | Error 0.6469(0.6738) Steps 526(515.99) | Grad Norm 2.9545(7.0420) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.0705, Epoch Time 323.7736(324.2095), Bit/dim 4.6789(best: 4.7202), Xent 1.8169, Loss 5.5874, Error 0.6407(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 46.7260(46.1838) | Bit/dim 4.6813(4.8387) | Xent 1.8425(1.9076) | Loss 5.6026(5.7925) | Error 0.6491(0.6731) Steps 520(516.11) | Grad Norm 4.4181(6.9633) | Total Time 14.00(14.00)\n",
      "Iter 0422 | Time 46.7635(46.2012) | Bit/dim 4.6610(4.8334) | Xent 1.8432(1.9057) | Loss 5.5826(5.7862) | Error 0.6504(0.6724) Steps 508(515.86) | Grad Norm 3.3349(6.8544) | Total Time 14.00(14.00)\n",
      "Iter 0423 | Time 46.6844(46.2157) | Bit/dim 4.6504(4.8279) | Xent 1.8396(1.9037) | Loss 5.5702(5.7798) | Error 0.6487(0.6717) Steps 514(515.81) | Grad Norm 2.8702(6.7349) | Total Time 14.00(14.00)\n",
      "Iter 0424 | Time 48.8292(46.2941) | Bit/dim 4.6323(4.8220) | Xent 1.8379(1.9017) | Loss 5.5513(5.7729) | Error 0.6441(0.6709) Steps 526(516.11) | Grad Norm 3.1591(6.6276) | Total Time 14.00(14.00)\n",
      "Iter 0425 | Time 47.7784(46.3386) | Bit/dim 4.6301(4.8163) | Xent 1.8069(1.8989) | Loss 5.5336(5.7657) | Error 0.6371(0.6699) Steps 526(516.41) | Grad Norm 3.1183(6.5223) | Total Time 14.00(14.00)\n",
      "Iter 0426 | Time 48.0383(46.3896) | Bit/dim 4.6089(4.8101) | Xent 1.7986(1.8959) | Loss 5.5082(5.7580) | Error 0.6359(0.6688) Steps 532(516.88) | Grad Norm 2.8353(6.4117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 22.8281, Epoch Time 322.8728(324.1694), Bit/dim 4.6152(best: 4.6789), Xent 1.8040, Loss 5.5172, Error 0.6415(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 47.0320(46.4089) | Bit/dim 4.6123(4.8041) | Xent 1.8317(1.8939) | Loss 5.5282(5.7511) | Error 0.6489(0.6682) Steps 526(517.15) | Grad Norm 5.3189(6.3789) | Total Time 14.00(14.00)\n",
      "Iter 0428 | Time 48.8345(46.4817) | Bit/dim 4.6306(4.7989) | Xent 1.8917(1.8939) | Loss 5.5765(5.7459) | Error 0.6707(0.6683) Steps 532(517.60) | Grad Norm 12.7097(6.5689) | Total Time 14.00(14.00)\n",
      "Iter 0429 | Time 45.4243(46.4499) | Bit/dim 4.7072(4.7962) | Xent 2.1880(1.9027) | Loss 5.8012(5.7475) | Error 0.7590(0.6710) Steps 532(518.03) | Grad Norm 18.3394(6.9220) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 47.4557(46.4801) | Bit/dim 4.6641(4.7922) | Xent 2.0240(1.9063) | Loss 5.6761(5.7454) | Error 0.7076(0.6721) Steps 520(518.09) | Grad Norm 10.0214(7.0150) | Total Time 14.00(14.00)\n",
      "Iter 0431 | Time 46.2565(46.4734) | Bit/dim 4.6209(4.7871) | Xent 1.9430(1.9074) | Loss 5.5924(5.7408) | Error 0.6939(0.6728) Steps 502(517.61) | Grad Norm 5.8532(6.9801) | Total Time 14.00(14.00)\n",
      "Iter 0432 | Time 47.4387(46.5024) | Bit/dim 4.6289(4.7823) | Xent 1.8779(1.9065) | Loss 5.5679(5.7356) | Error 0.6696(0.6727) Steps 532(518.04) | Grad Norm 5.5579(6.9374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 23.7394, Epoch Time 321.4792(324.0887), Bit/dim 4.6420(best: 4.6152), Xent 1.8580, Loss 5.5710, Error 0.6647(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 48.8951(46.5741) | Bit/dim 4.6383(4.7780) | Xent 1.8830(1.9058) | Loss 5.5798(5.7309) | Error 0.6665(0.6725) Steps 538(518.64) | Grad Norm 4.7135(6.8707) | Total Time 14.00(14.00)\n",
      "Iter 0434 | Time 49.7686(46.6700) | Bit/dim 4.6127(4.7731) | Xent 1.8818(1.9051) | Loss 5.5536(5.7256) | Error 0.6716(0.6725) Steps 556(519.76) | Grad Norm 5.4260(6.8274) | Total Time 14.00(14.00)\n",
      "Iter 0435 | Time 47.0847(46.6824) | Bit/dim 4.6480(4.7693) | Xent 1.8583(1.9037) | Loss 5.5772(5.7212) | Error 0.6616(0.6722) Steps 526(519.95) | Grad Norm 7.2833(6.8411) | Total Time 14.00(14.00)\n",
      "Iter 0436 | Time 50.9094(46.8092) | Bit/dim 4.6669(4.7662) | Xent 1.8897(1.9033) | Loss 5.6118(5.7179) | Error 0.6627(0.6719) Steps 544(520.67) | Grad Norm 10.9701(6.9649) | Total Time 14.00(14.00)\n",
      "Iter 0437 | Time 44.0641(46.7269) | Bit/dim 4.8533(4.7688) | Xent 1.9364(1.9043) | Loss 5.8215(5.7210) | Error 0.6935(0.6725) Steps 514(520.47) | Grad Norm 12.2547(7.1236) | Total Time 14.00(14.00)\n",
      "Iter 0438 | Time 45.7897(46.6988) | Bit/dim 4.6292(4.7647) | Xent 1.9269(1.9050) | Loss 5.5926(5.7171) | Error 0.6876(0.6730) Steps 514(520.27) | Grad Norm 7.2386(7.1271) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 24.2003, Epoch Time 326.0801(324.1484), Bit/dim 4.7131(best: 4.6152), Xent 1.8454, Loss 5.6358, Error 0.6606(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 48.5564(46.7545) | Bit/dim 4.7130(4.7631) | Xent 1.9134(1.9052) | Loss 5.6697(5.7157) | Error 0.6807(0.6732) Steps 532(520.62) | Grad Norm 12.7822(7.2967) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 48.2374(46.7990) | Bit/dim 5.0232(4.7709) | Xent 1.9251(1.9058) | Loss 5.9858(5.7238) | Error 0.6843(0.6735) Steps 556(521.69) | Grad Norm 12.1608(7.4426) | Total Time 14.00(14.00)\n",
      "Iter 0441 | Time 44.6978(46.7359) | Bit/dim 4.9410(4.7760) | Xent 1.8438(1.9040) | Loss 5.8629(5.7280) | Error 0.6492(0.6728) Steps 514(521.46) | Grad Norm 7.6635(7.4493) | Total Time 14.00(14.00)\n",
      "Iter 0442 | Time 47.2136(46.7503) | Bit/dim 4.7636(4.7756) | Xent 2.0275(1.9077) | Loss 5.7773(5.7295) | Error 0.7012(0.6737) Steps 532(521.77) | Grad Norm 16.5916(7.7235) | Total Time 14.00(14.00)\n",
      "Iter 0443 | Time 45.2392(46.7049) | Bit/dim 4.8034(4.7765) | Xent 2.1800(1.9158) | Loss 5.8934(5.7344) | Error 0.7502(0.6760) Steps 532(522.08) | Grad Norm 11.1104(7.8251) | Total Time 14.00(14.00)\n",
      "Iter 0444 | Time 51.6484(46.8532) | Bit/dim 4.9026(4.7803) | Xent 2.0217(1.9190) | Loss 5.9135(5.7398) | Error 0.7290(0.6776) Steps 604(524.54) | Grad Norm 13.2074(7.9866) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 24.0854, Epoch Time 324.8225(324.1686), Bit/dim 4.7039(best: 4.6152), Xent 1.9414, Loss 5.6745, Error 0.6954(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 46.1597(46.8324) | Bit/dim 4.7061(4.7780) | Xent 1.9651(1.9204) | Loss 5.6887(5.7382) | Error 0.7080(0.6785) Steps 544(525.12) | Grad Norm 3.5702(7.8541) | Total Time 14.00(14.00)\n",
      "Iter 0446 | Time 49.8752(46.9237) | Bit/dim 4.7902(4.7784) | Xent 1.8991(1.9198) | Loss 5.7398(5.7383) | Error 0.6725(0.6783) Steps 556(526.05) | Grad Norm 6.5799(7.8159) | Total Time 14.00(14.00)\n",
      "Iter 0447 | Time 46.9067(46.9232) | Bit/dim 4.7792(4.7784) | Xent 1.9087(1.9194) | Loss 5.7335(5.7381) | Error 0.6753(0.6782) Steps 550(526.77) | Grad Norm 5.5354(7.7475) | Total Time 14.00(14.00)\n",
      "Iter 0448 | Time 47.9711(46.9546) | Bit/dim 4.7164(4.7766) | Xent 1.9232(1.9195) | Loss 5.6780(5.7363) | Error 0.6816(0.6783) Steps 562(527.82) | Grad Norm 3.8813(7.6315) | Total Time 14.00(14.00)\n",
      "Iter 0449 | Time 48.0184(46.9866) | Bit/dim 4.6964(4.7742) | Xent 1.9257(1.9197) | Loss 5.6592(5.7340) | Error 0.6840(0.6785) Steps 544(528.31) | Grad Norm 3.3553(7.5032) | Total Time 14.00(14.00)\n",
      "Iter 0450 | Time 47.6966(47.0079) | Bit/dim 4.7079(4.7722) | Xent 1.9003(1.9191) | Loss 5.6580(5.7317) | Error 0.6730(0.6783) Steps 526(528.24) | Grad Norm 3.4006(7.3801) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.1618, Epoch Time 324.9860(324.1931), Bit/dim 4.7181(best: 4.6152), Xent 1.8481, Loss 5.6421, Error 0.6520(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 47.1199(47.0112) | Bit/dim 4.7109(4.7703) | Xent 1.8889(1.9182) | Loss 5.6554(5.7294) | Error 0.6714(0.6781) Steps 526(528.17) | Grad Norm 3.4633(7.2626) | Total Time 14.00(14.00)\n",
      "Iter 0452 | Time 45.8545(46.9765) | Bit/dim 4.6902(4.7679) | Xent 1.8607(1.9165) | Loss 5.6206(5.7262) | Error 0.6635(0.6777) Steps 508(527.57) | Grad Norm 2.3098(7.1140) | Total Time 14.00(14.00)\n",
      "Iter 0453 | Time 46.6106(46.9655) | Bit/dim 4.6752(4.7651) | Xent 1.8600(1.9148) | Loss 5.6052(5.7225) | Error 0.6565(0.6770) Steps 508(526.98) | Grad Norm 2.4504(6.9741) | Total Time 14.00(14.00)\n",
      "Iter 0454 | Time 46.3157(46.9460) | Bit/dim 4.6681(4.7622) | Xent 1.8513(1.9129) | Loss 5.5937(5.7187) | Error 0.6519(0.6763) Steps 502(526.23) | Grad Norm 2.3307(6.8348) | Total Time 14.00(14.00)\n",
      "Iter 0455 | Time 45.8616(46.9135) | Bit/dim 4.6397(4.7586) | Xent 1.8555(1.9112) | Loss 5.5675(5.7141) | Error 0.6506(0.6755) Steps 496(525.32) | Grad Norm 1.9097(6.6871) | Total Time 14.00(14.00)\n",
      "Iter 0456 | Time 46.4112(46.8984) | Bit/dim 4.6410(4.7550) | Xent 1.8358(1.9089) | Loss 5.5589(5.7095) | Error 0.6426(0.6745) Steps 496(524.44) | Grad Norm 2.0022(6.5465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 22.1637, Epoch Time 315.7322(323.9393), Bit/dim 4.6339(best: 4.6152), Xent 1.8094, Loss 5.5385, Error 0.6369(best: 0.6392)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 47.7293(46.9234) | Bit/dim 4.6305(4.7513) | Xent 1.8531(1.9072) | Loss 5.5571(5.7049) | Error 0.6546(0.6739) Steps 502(523.77) | Grad Norm 2.1095(6.4134) | Total Time 14.00(14.00)\n",
      "Iter 0458 | Time 45.8104(46.8900) | Bit/dim 4.6200(4.7474) | Xent 1.8444(1.9054) | Loss 5.5422(5.7000) | Error 0.6535(0.6733) Steps 502(523.12) | Grad Norm 2.4787(6.2954) | Total Time 14.00(14.00)\n",
      "Iter 0459 | Time 47.4659(46.9073) | Bit/dim 4.6139(4.7433) | Xent 1.8374(1.9033) | Loss 5.5326(5.6950) | Error 0.6462(0.6725) Steps 508(522.66) | Grad Norm 2.2244(6.1732) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 47.4904(46.9247) | Bit/dim 4.5915(4.7388) | Xent 1.8133(1.9006) | Loss 5.4981(5.6891) | Error 0.6398(0.6715) Steps 520(522.58) | Grad Norm 2.4245(6.0608) | Total Time 14.00(14.00)\n",
      "Iter 0461 | Time 47.4823(46.9415) | Bit/dim 4.5834(4.7341) | Xent 1.8053(1.8978) | Loss 5.4860(5.6830) | Error 0.6380(0.6705) Steps 526(522.69) | Grad Norm 2.1876(5.9446) | Total Time 14.00(14.00)\n",
      "Iter 0462 | Time 47.0120(46.9436) | Bit/dim 4.5709(4.7292) | Xent 1.8341(1.8959) | Loss 5.4879(5.6772) | Error 0.6507(0.6699) Steps 514(522.43) | Grad Norm 2.4006(5.8383) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 22.9900, Epoch Time 321.3560(323.8618), Bit/dim 4.5740(best: 4.6152), Xent 1.7832, Loss 5.4656, Error 0.6307(best: 0.6369)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 46.4756(46.9295) | Bit/dim 4.5751(4.7246) | Xent 1.8051(1.8931) | Loss 5.4777(5.6712) | Error 0.6351(0.6689) Steps 508(521.99) | Grad Norm 2.3386(5.7333) | Total Time 14.00(14.00)\n",
      "Iter 0464 | Time 46.9229(46.9293) | Bit/dim 4.5619(4.7197) | Xent 1.8383(1.8915) | Loss 5.4811(5.6655) | Error 0.6512(0.6683) Steps 514(521.75) | Grad Norm 3.9162(5.6788) | Total Time 14.00(14.00)\n",
      "Iter 0465 | Time 43.9125(46.8388) | Bit/dim 4.5895(4.7158) | Xent 1.8443(1.8901) | Loss 5.5116(5.6609) | Error 0.6467(0.6677) Steps 502(521.16) | Grad Norm 8.4593(5.7622) | Total Time 14.00(14.00)\n",
      "Iter 0466 | Time 49.8741(46.9299) | Bit/dim 4.6239(4.7131) | Xent 2.2054(1.8995) | Loss 5.7266(5.6628) | Error 0.7352(0.6697) Steps 532(521.49) | Grad Norm 26.5113(6.3847) | Total Time 14.00(14.00)\n",
      "Iter 0467 | Time 43.1796(46.8174) | Bit/dim 5.1297(4.7256) | Xent 2.6688(1.9226) | Loss 6.4641(5.6869) | Error 0.7919(0.6734) Steps 514(521.26) | Grad Norm 15.8321(6.6681) | Total Time 14.00(14.00)\n",
      "Iter 0468 | Time 44.4014(46.7449) | Bit/dim 5.0737(4.7360) | Xent 2.3136(1.9343) | Loss 6.2304(5.7032) | Error 0.8086(0.6774) Steps 526(521.40) | Grad Norm 12.2084(6.8343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 24.1889, Epoch Time 314.2544(323.5736), Bit/dim 4.6656(best: 4.5740), Xent 2.0922, Loss 5.7117, Error 0.7717(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 46.6157(46.7410) | Bit/dim 4.6709(4.7341) | Xent 2.1217(1.9400) | Loss 5.7318(5.7040) | Error 0.7576(0.6798) Steps 538(521.90) | Grad Norm 4.9387(6.7774) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 50.1386(46.8430) | Bit/dim 4.8113(4.7364) | Xent 2.2694(1.9498) | Loss 5.9460(5.7113) | Error 0.7760(0.6827) Steps 562(523.10) | Grad Norm 17.1645(7.0890) | Total Time 14.00(14.00)\n",
      "Iter 0471 | Time 44.8758(46.7839) | Bit/dim 5.0529(4.7459) | Xent 2.1366(1.9554) | Loss 6.1212(5.7236) | Error 0.7688(0.6853) Steps 544(523.73) | Grad Norm 11.1547(7.2110) | Total Time 14.00(14.00)\n",
      "Iter 0472 | Time 46.7776(46.7838) | Bit/dim 5.0411(4.7547) | Xent 2.0870(1.9594) | Loss 6.0846(5.7344) | Error 0.7408(0.6870) Steps 550(524.52) | Grad Norm 8.3351(7.2447) | Total Time 14.00(14.00)\n",
      "Iter 0473 | Time 47.0891(46.7929) | Bit/dim 4.7816(4.7555) | Xent 2.0779(1.9629) | Loss 5.8205(5.7370) | Error 0.7366(0.6885) Steps 538(524.92) | Grad Norm 4.8859(7.1740) | Total Time 14.00(14.00)\n",
      "Iter 0474 | Time 50.5049(46.9043) | Bit/dim 4.9191(4.7604) | Xent 2.1514(1.9686) | Loss 5.9948(5.7447) | Error 0.7358(0.6899) Steps 574(526.40) | Grad Norm 12.5094(7.3340) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 23.7949, Epoch Time 325.1647(323.6213), Bit/dim 5.0427(best: 4.5740), Xent 2.0041, Loss 6.0448, Error 0.6976(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 45.7155(46.8686) | Bit/dim 5.0447(4.7690) | Xent 2.0150(1.9700) | Loss 6.0522(5.7540) | Error 0.6963(0.6901) Steps 532(526.56) | Grad Norm 5.0526(7.2656) | Total Time 14.00(14.00)\n",
      "Iter 0476 | Time 44.3408(46.7928) | Bit/dim 5.1556(4.7806) | Xent 2.1300(1.9748) | Loss 6.2206(5.7680) | Error 0.7659(0.6923) Steps 526(526.55) | Grad Norm 11.3941(7.3894) | Total Time 14.00(14.00)\n",
      "Iter 0477 | Time 47.0096(46.7993) | Bit/dim 5.0650(4.7891) | Xent 2.9123(2.0029) | Loss 6.5212(5.7906) | Error 0.8385(0.6967) Steps 556(527.43) | Grad Norm 18.7186(7.7293) | Total Time 14.00(14.00)\n",
      "Iter 0478 | Time 46.8837(46.8018) | Bit/dim 4.9417(4.7937) | Xent 2.8772(2.0291) | Loss 6.3803(5.8082) | Error 0.8124(0.7002) Steps 538(527.75) | Grad Norm 17.0889(8.0101) | Total Time 14.00(14.00)\n",
      "Iter 0479 | Time 47.1528(46.8123) | Bit/dim 4.8222(4.7945) | Xent 2.1387(2.0324) | Loss 5.8916(5.8107) | Error 0.7680(0.7022) Steps 538(528.06) | Grad Norm 8.5361(8.0259) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 46.3494(46.7985) | Bit/dim 4.8835(4.7972) | Xent 2.1009(2.0345) | Loss 5.9339(5.8144) | Error 0.7350(0.7032) Steps 544(528.53) | Grad Norm 8.8150(8.0496) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 23.9388, Epoch Time 316.8860(323.4193), Bit/dim 4.8097(best: 4.5740), Xent 2.1036, Loss 5.8615, Error 0.7387(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 46.9682(46.8036) | Bit/dim 4.8089(4.7975) | Xent 2.1393(2.0376) | Loss 5.8786(5.8164) | Error 0.7416(0.7044) Steps 532(528.64) | Grad Norm 6.1972(7.9940) | Total Time 14.00(14.00)\n",
      "Iter 0482 | Time 48.2168(46.8459) | Bit/dim 4.8344(4.7987) | Xent 2.0693(2.0386) | Loss 5.8691(5.8179) | Error 0.7350(0.7053) Steps 544(529.10) | Grad Norm 6.2226(7.9408) | Total Time 14.00(14.00)\n",
      "Iter 0483 | Time 46.5289(46.8364) | Bit/dim 4.8123(4.7991) | Xent 2.1133(2.0408) | Loss 5.8689(5.8195) | Error 0.7538(0.7067) Steps 532(529.19) | Grad Norm 7.4461(7.9260) | Total Time 14.00(14.00)\n",
      "Iter 0484 | Time 46.5887(46.8290) | Bit/dim 4.7431(4.7974) | Xent 2.0387(2.0408) | Loss 5.7625(5.8178) | Error 0.7279(0.7074) Steps 538(529.45) | Grad Norm 3.9483(7.8067) | Total Time 14.00(14.00)\n",
      "Iter 0485 | Time 48.6851(46.8847) | Bit/dim 4.7405(4.7957) | Xent 1.9913(2.0393) | Loss 5.7362(5.8153) | Error 0.6983(0.7071) Steps 550(530.07) | Grad Norm 4.5521(7.7090) | Total Time 14.00(14.00)\n",
      "Iter 0486 | Time 48.6842(46.9387) | Bit/dim 4.7419(4.7941) | Xent 1.9541(2.0367) | Loss 5.7189(5.8124) | Error 0.6974(0.7068) Steps 550(530.66) | Grad Norm 4.3313(7.6077) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 24.0774, Epoch Time 325.0273(323.4675), Bit/dim 4.7388(best: 4.5740), Xent 1.9279, Loss 5.7028, Error 0.6922(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 47.6102(46.9588) | Bit/dim 4.7531(4.7928) | Xent 1.9633(2.0345) | Loss 5.7347(5.8101) | Error 0.7003(0.7066) Steps 556(531.42) | Grad Norm 4.0659(7.5015) | Total Time 14.00(14.00)\n",
      "Iter 0488 | Time 49.9027(47.0471) | Bit/dim 4.7044(4.7902) | Xent 1.9688(2.0325) | Loss 5.6888(5.8065) | Error 0.7066(0.7066) Steps 562(532.34) | Grad Norm 4.3979(7.4083) | Total Time 14.00(14.00)\n",
      "Iter 0489 | Time 48.1729(47.0809) | Bit/dim 4.6757(4.7867) | Xent 1.9292(2.0294) | Loss 5.6403(5.8015) | Error 0.6879(0.7061) Steps 556(533.05) | Grad Norm 3.2337(7.2831) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 46.5895(47.0662) | Bit/dim 4.6736(4.7834) | Xent 1.9277(2.0264) | Loss 5.6374(5.7966) | Error 0.6825(0.7053) Steps 544(533.38) | Grad Norm 3.0433(7.1559) | Total Time 14.00(14.00)\n",
      "Iter 0491 | Time 45.2823(47.0126) | Bit/dim 4.6290(4.7787) | Xent 1.9657(2.0246) | Loss 5.6118(5.7910) | Error 0.6935(0.7050) Steps 526(533.16) | Grad Norm 3.9802(7.0606) | Total Time 14.00(14.00)\n",
      "Iter 0492 | Time 48.6201(47.0609) | Bit/dim 4.6140(4.7738) | Xent 1.9362(2.0219) | Loss 5.5821(5.7847) | Error 0.6750(0.7041) Steps 538(533.30) | Grad Norm 3.3987(6.9508) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 24.4485, Epoch Time 325.8426(323.5388), Bit/dim 4.6048(best: 4.5740), Xent 1.8790, Loss 5.5443, Error 0.6559(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 45.9710(47.0282) | Bit/dim 4.5979(4.7685) | Xent 1.9167(2.0188) | Loss 5.5562(5.7779) | Error 0.6775(0.7033) Steps 544(533.62) | Grad Norm 4.0967(6.8652) | Total Time 14.00(14.00)\n",
      "Iter 0494 | Time 47.1768(47.0326) | Bit/dim 4.6068(4.7637) | Xent 1.8961(2.0151) | Loss 5.5548(5.7712) | Error 0.6645(0.7021) Steps 544(533.94) | Grad Norm 4.7241(6.8009) | Total Time 14.00(14.00)\n",
      "Iter 0495 | Time 47.7298(47.0535) | Bit/dim 4.5997(4.7587) | Xent 1.9229(2.0123) | Loss 5.5612(5.7649) | Error 0.6838(0.7016) Steps 550(534.42) | Grad Norm 7.6840(6.8274) | Total Time 14.00(14.00)\n",
      "Iter 0496 | Time 48.0156(47.0824) | Bit/dim 4.5872(4.7536) | Xent 2.0389(2.0131) | Loss 5.6067(5.7601) | Error 0.7109(0.7019) Steps 550(534.89) | Grad Norm 11.2111(6.9589) | Total Time 14.00(14.00)\n",
      "Iter 0497 | Time 49.5176(47.1555) | Bit/dim 4.6052(4.7491) | Xent 2.2579(2.0205) | Loss 5.7341(5.7594) | Error 0.7483(0.7032) Steps 562(535.70) | Grad Norm 21.6053(7.3983) | Total Time 14.00(14.00)\n",
      "Iter 0498 | Time 47.5029(47.1659) | Bit/dim 4.9220(4.7543) | Xent 2.3758(2.0311) | Loss 6.1099(5.7699) | Error 0.7754(0.7054) Steps 556(536.31) | Grad Norm 17.4613(7.7002) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 26.0497, Epoch Time 327.1836(323.6481), Bit/dim 5.0160(best: 4.5740), Xent 2.0988, Loss 6.0654, Error 0.7276(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 51.3416(47.2912) | Bit/dim 5.0126(4.7621) | Xent 2.1193(2.0338) | Loss 6.0723(5.7790) | Error 0.7354(0.7063) Steps 604(538.34) | Grad Norm 13.6982(7.8802) | Total Time 14.00(14.00)\n",
      "Iter 0500 | Time 51.4781(47.4168) | Bit/dim 4.8803(4.7656) | Xent 2.0302(2.0337) | Loss 5.8954(5.7824) | Error 0.7051(0.7063) Steps 592(539.95) | Grad Norm 8.9072(7.9110) | Total Time 14.00(14.00)\n",
      "Iter 0501 | Time 51.0544(47.5259) | Bit/dim 4.8520(4.7682) | Xent 1.9811(2.0321) | Loss 5.8425(5.7843) | Error 0.7056(0.7063) Steps 628(542.59) | Grad Norm 6.6213(7.8723) | Total Time 14.00(14.00)\n",
      "Iter 0502 | Time 53.3313(47.7001) | Bit/dim 4.7984(4.7691) | Xent 1.9692(2.0302) | Loss 5.7830(5.7842) | Error 0.7087(0.7063) Steps 628(545.15) | Grad Norm 3.7020(7.7472) | Total Time 14.00(14.00)\n",
      "Iter 0503 | Time 53.0178(47.8596) | Bit/dim 4.8050(4.7702) | Xent 1.9932(2.0291) | Loss 5.8016(5.7847) | Error 0.7139(0.7066) Steps 628(547.64) | Grad Norm 5.7564(7.6874) | Total Time 14.00(14.00)\n",
      "Iter 0504 | Time 57.5224(48.1495) | Bit/dim 4.7880(4.7707) | Xent 1.9664(2.0272) | Loss 5.7713(5.7843) | Error 0.6997(0.7064) Steps 694(552.03) | Grad Norm 3.4360(7.5599) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 28.9241, Epoch Time 361.9567(324.7974), Bit/dim 4.7521(best: 4.5740), Xent 1.9448, Loss 5.7245, Error 0.6956(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 54.7137(48.3464) | Bit/dim 4.7617(4.7705) | Xent 1.9759(2.0257) | Loss 5.7496(5.7833) | Error 0.7047(0.7063) Steps 652(555.03) | Grad Norm 3.3496(7.4336) | Total Time 14.00(14.00)\n",
      "Iter 0506 | Time 53.5395(48.5022) | Bit/dim 4.7469(4.7697) | Xent 1.9583(2.0236) | Loss 5.7261(5.7816) | Error 0.6980(0.7061) Steps 622(557.04) | Grad Norm 3.3169(7.3101) | Total Time 14.00(14.00)\n",
      "Iter 0507 | Time 49.5675(48.5342) | Bit/dim 4.7151(4.7681) | Xent 1.9370(2.0210) | Loss 5.6835(5.7786) | Error 0.6816(0.7053) Steps 604(558.45) | Grad Norm 3.4560(7.1945) | Total Time 14.00(14.00)\n",
      "Iter 0508 | Time 51.4252(48.6209) | Bit/dim 4.6951(4.7659) | Xent 1.9149(2.0179) | Loss 5.6525(5.7748) | Error 0.6769(0.7045) Steps 592(559.45) | Grad Norm 2.3178(7.0482) | Total Time 14.00(14.00)\n",
      "Iter 0509 | Time 51.2430(48.6995) | Bit/dim 4.6820(4.7634) | Xent 1.9306(2.0152) | Loss 5.6474(5.7710) | Error 0.6843(0.7039) Steps 604(560.79) | Grad Norm 4.1087(6.9600) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 51.8124(48.7929) | Bit/dim 4.6934(4.7613) | Xent 1.9025(2.0119) | Loss 5.6446(5.7672) | Error 0.6765(0.7030) Steps 628(562.81) | Grad Norm 4.1365(6.8753) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 26.8682, Epoch Time 354.7782(325.6968), Bit/dim 4.6710(best: 4.5740), Xent 1.8656, Loss 5.6038, Error 0.6598(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 51.3306(48.8691) | Bit/dim 4.6737(4.7587) | Xent 1.8960(2.0084) | Loss 5.6217(5.7629) | Error 0.6730(0.7021) Steps 616(564.40) | Grad Norm 2.1731(6.7342) | Total Time 14.00(14.00)\n",
      "Iter 0512 | Time 53.9859(49.0226) | Bit/dim 4.7063(4.7571) | Xent 1.8798(2.0045) | Loss 5.6462(5.7594) | Error 0.6678(0.7011) Steps 604(565.59) | Grad Norm 6.5691(6.7293) | Total Time 14.00(14.00)\n",
      "Iter 0513 | Time 52.8839(49.1384) | Bit/dim 4.7490(4.7569) | Xent 1.8563(2.0001) | Loss 5.6772(5.7569) | Error 0.6411(0.6993) Steps 622(567.28) | Grad Norm 5.4667(6.6914) | Total Time 14.00(14.00)\n",
      "Iter 0514 | Time 50.3568(49.1750) | Bit/dim 4.6446(4.7535) | Xent 1.8321(1.9950) | Loss 5.5606(5.7510) | Error 0.6374(0.6975) Steps 598(568.20) | Grad Norm 3.4362(6.5937) | Total Time 14.00(14.00)\n",
      "Iter 0515 | Time 50.6161(49.2182) | Bit/dim 4.8183(4.7554) | Xent 1.8887(1.9919) | Loss 5.7627(5.7514) | Error 0.6714(0.6967) Steps 556(567.84) | Grad Norm 15.4964(6.8608) | Total Time 14.00(14.00)\n",
      "Iter 0516 | Time 48.4061(49.1938) | Bit/dim 5.1402(4.7670) | Xent 1.8847(1.9886) | Loss 6.0826(5.7613) | Error 0.6551(0.6954) Steps 586(568.38) | Grad Norm 8.1082(6.8982) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 25.1590, Epoch Time 348.0772(326.3682), Bit/dim 5.3586(best: 4.5740), Xent 1.8682, Loss 6.2927, Error 0.6533(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 46.4527(49.1116) | Bit/dim 5.3521(4.7845) | Xent 1.8826(1.9855) | Loss 6.2934(5.7773) | Error 0.6640(0.6945) Steps 574(568.55) | Grad Norm 6.3250(6.8810) | Total Time 14.00(14.00)\n",
      "Iter 0518 | Time 45.9250(49.0160) | Bit/dim 5.1262(4.7948) | Xent 1.8507(1.9814) | Loss 6.0515(5.7855) | Error 0.6430(0.6929) Steps 568(568.53) | Grad Norm 5.8370(6.8497) | Total Time 14.00(14.00)\n",
      "Iter 0519 | Time 48.5475(49.0019) | Bit/dim 4.7366(4.7930) | Xent 1.8624(1.9778) | Loss 5.6678(5.7820) | Error 0.6474(0.6916) Steps 562(568.34) | Grad Norm 5.2935(6.8030) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 49.0554(49.0036) | Bit/dim 5.0383(4.8004) | Xent 1.9132(1.9759) | Loss 5.9949(5.7883) | Error 0.6687(0.6909) Steps 568(568.33) | Grad Norm 23.1112(7.2923) | Total Time 14.00(14.00)\n",
      "Iter 0521 | Time 48.2482(48.9809) | Bit/dim 5.0688(4.8084) | Xent 1.9862(1.9762) | Loss 6.0618(5.7966) | Error 0.7186(0.6917) Steps 586(568.86) | Grad Norm 14.9225(7.5212) | Total Time 14.00(14.00)\n",
      "Iter 0522 | Time 46.4045(48.9036) | Bit/dim 5.4352(4.8272) | Xent 2.1380(1.9811) | Loss 6.5042(5.8178) | Error 0.7459(0.6933) Steps 568(568.83) | Grad Norm 23.0951(7.9884) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 25.5497, Epoch Time 325.6406(326.3464), Bit/dim 5.4307(best: 4.5740), Xent 2.1230, Loss 6.4922, Error 0.7618(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 46.8644(48.8424) | Bit/dim 5.4321(4.8454) | Xent 2.1420(1.9859) | Loss 6.5031(5.8383) | Error 0.7596(0.6953) Steps 568(568.81) | Grad Norm 12.6252(8.1275) | Total Time 14.00(14.00)\n",
      "Iter 0524 | Time 47.5282(48.8030) | Bit/dim 5.3338(4.8600) | Xent 1.8800(1.9827) | Loss 6.2739(5.8514) | Error 0.6580(0.6942) Steps 568(568.78) | Grad Norm 8.5065(8.1389) | Total Time 14.00(14.00)\n",
      "Iter 0525 | Time 48.6442(48.7982) | Bit/dim 5.2441(4.8716) | Xent 1.9257(1.9810) | Loss 6.2070(5.8621) | Error 0.6749(0.6936) Steps 610(570.02) | Grad Norm 8.5874(8.1523) | Total Time 14.00(14.00)\n",
      "Iter 0526 | Time 47.3737(48.7555) | Bit/dim 5.1840(4.8809) | Xent 1.9349(1.9796) | Loss 6.1514(5.8708) | Error 0.6825(0.6933) Steps 568(569.96) | Grad Norm 8.4319(8.1607) | Total Time 14.00(14.00)\n",
      "Iter 0527 | Time 51.5164(48.8383) | Bit/dim 4.9521(4.8831) | Xent 1.9543(1.9789) | Loss 5.9292(5.8725) | Error 0.6939(0.6933) Steps 610(571.16) | Grad Norm 5.4545(8.0795) | Total Time 14.00(14.00)\n",
      "Iter 0528 | Time 48.1766(48.8185) | Bit/dim 4.9310(4.8845) | Xent 1.9655(1.9785) | Loss 5.9138(5.8737) | Error 0.7051(0.6937) Steps 574(571.25) | Grad Norm 4.2087(7.9634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 27.9271, Epoch Time 333.3984(326.5579), Bit/dim 4.9596(best: 4.5740), Xent 1.9065, Loss 5.9128, Error 0.6801(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 50.1460(48.8583) | Bit/dim 4.9512(4.8865) | Xent 1.9262(1.9769) | Loss 5.9143(5.8750) | Error 0.6863(0.6934) Steps 628(572.95) | Grad Norm 4.2223(7.8512) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 50.8277(48.9174) | Bit/dim 4.9173(4.8874) | Xent 1.9161(1.9751) | Loss 5.8753(5.8750) | Error 0.6853(0.6932) Steps 628(574.60) | Grad Norm 4.0118(7.7360) | Total Time 14.00(14.00)\n",
      "Iter 0531 | Time 49.1401(48.9241) | Bit/dim 4.9107(4.8881) | Xent 1.8916(1.9726) | Loss 5.8565(5.8744) | Error 0.6611(0.6922) Steps 610(575.66) | Grad Norm 3.8616(7.6198) | Total Time 14.00(14.00)\n",
      "Iter 0532 | Time 50.6034(48.9744) | Bit/dim 4.8720(4.8877) | Xent 1.8796(1.9698) | Loss 5.8118(5.8725) | Error 0.6593(0.6912) Steps 634(577.41) | Grad Norm 5.6068(7.5594) | Total Time 14.00(14.00)\n",
      "Iter 0533 | Time 50.3654(49.0162) | Bit/dim 4.7986(4.8850) | Xent 1.8865(1.9673) | Loss 5.7419(5.8686) | Error 0.6585(0.6903) Steps 610(578.39) | Grad Norm 2.1563(7.3973) | Total Time 14.00(14.00)\n",
      "Iter 0534 | Time 50.1186(49.0492) | Bit/dim 4.7962(4.8823) | Xent 1.8995(1.9652) | Loss 5.7460(5.8649) | Error 0.6671(0.6896) Steps 616(579.52) | Grad Norm 3.8485(7.2908) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 27.6008, Epoch Time 343.9355(327.0793), Bit/dim 4.7865(best: 4.5740), Xent 1.8674, Loss 5.7202, Error 0.6503(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 49.7914(49.0715) | Bit/dim 4.7854(4.8794) | Xent 1.8885(1.9629) | Loss 5.7297(5.8609) | Error 0.6590(0.6887) Steps 604(580.25) | Grad Norm 2.8729(7.1583) | Total Time 14.00(14.00)\n",
      "Iter 0536 | Time 51.1791(49.1347) | Bit/dim 4.7699(4.8761) | Xent 1.9087(1.9613) | Loss 5.7243(5.8568) | Error 0.6691(0.6881) Steps 622(581.50) | Grad Norm 3.2574(7.0412) | Total Time 14.00(14.00)\n",
      "Iter 0537 | Time 51.6874(49.2113) | Bit/dim 4.7468(4.8722) | Xent 1.8737(1.9587) | Loss 5.6836(5.8516) | Error 0.6551(0.6871) Steps 604(582.18) | Grad Norm 2.0868(6.8926) | Total Time 14.00(14.00)\n",
      "Iter 0538 | Time 49.1218(49.2086) | Bit/dim 4.7427(4.8684) | Xent 1.8812(1.9564) | Loss 5.6833(5.8465) | Error 0.6607(0.6863) Steps 598(582.65) | Grad Norm 2.6890(6.7665) | Total Time 14.00(14.00)\n",
      "Iter 0539 | Time 50.4655(49.2463) | Bit/dim 4.7357(4.8644) | Xent 1.8812(1.9541) | Loss 5.6763(5.8414) | Error 0.6605(0.6855) Steps 592(582.93) | Grad Norm 2.1110(6.6268) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 49.6350(49.2580) | Bit/dim 4.7120(4.8598) | Xent 1.8695(1.9516) | Loss 5.6467(5.8356) | Error 0.6638(0.6849) Steps 586(583.03) | Grad Norm 3.5701(6.5351) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 25.9442, Epoch Time 342.9079(327.5541), Bit/dim 4.6883(best: 4.5740), Xent 1.8424, Loss 5.6095, Error 0.6441(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 49.7705(49.2734) | Bit/dim 4.6826(4.8545) | Xent 1.8588(1.9488) | Loss 5.6120(5.8289) | Error 0.6491(0.6838) Steps 568(582.58) | Grad Norm 2.0269(6.3999) | Total Time 14.00(14.00)\n",
      "Iter 0542 | Time 46.2756(49.1834) | Bit/dim 4.6707(4.8490) | Xent 1.8398(1.9455) | Loss 5.5906(5.8217) | Error 0.6540(0.6829) Steps 568(582.14) | Grad Norm 2.2027(6.2740) | Total Time 14.00(14.00)\n",
      "Iter 0543 | Time 46.1601(49.0927) | Bit/dim 4.6747(4.8438) | Xent 1.8537(1.9428) | Loss 5.6016(5.8151) | Error 0.6507(0.6819) Steps 538(580.81) | Grad Norm 2.0232(6.1464) | Total Time 14.00(14.00)\n",
      "Iter 0544 | Time 47.3041(49.0391) | Bit/dim 4.6545(4.8381) | Xent 1.8594(1.9403) | Loss 5.5842(5.8082) | Error 0.6531(0.6811) Steps 538(579.53) | Grad Norm 2.0728(6.0242) | Total Time 14.00(14.00)\n",
      "Iter 0545 | Time 48.0926(49.0107) | Bit/dim 4.6344(4.8320) | Xent 1.8559(1.9377) | Loss 5.5623(5.8008) | Error 0.6535(0.6802) Steps 544(578.46) | Grad Norm 1.8989(5.9005) | Total Time 14.00(14.00)\n",
      "Iter 0546 | Time 46.5545(48.9370) | Bit/dim 4.6429(4.8263) | Xent 1.8363(1.9347) | Loss 5.5611(5.7936) | Error 0.6509(0.6794) Steps 538(577.25) | Grad Norm 2.0997(5.7865) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 24.8587, Epoch Time 324.2298(327.4544), Bit/dim 4.6293(best: 4.5740), Xent 1.8090, Loss 5.5338, Error 0.6304(best: 0.6307)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 46.0820(48.8513) | Bit/dim 4.6259(4.8203) | Xent 1.8109(1.9310) | Loss 5.5313(5.7858) | Error 0.6300(0.6779) Steps 544(576.25) | Grad Norm 1.7613(5.6657) | Total Time 14.00(14.00)\n",
      "Iter 0548 | Time 47.7974(48.8197) | Bit/dim 4.6074(4.8139) | Xent 1.8287(1.9279) | Loss 5.5218(5.7778) | Error 0.6462(0.6769) Steps 544(575.28) | Grad Norm 1.6675(5.5458) | Total Time 14.00(14.00)\n",
      "Iter 0549 | Time 48.7930(48.8189) | Bit/dim 4.6001(4.8075) | Xent 1.8225(1.9247) | Loss 5.5114(5.7699) | Error 0.6334(0.6756) Steps 550(574.53) | Grad Norm 2.1322(5.4433) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 48.8498(48.8199) | Bit/dim 4.5909(4.8010) | Xent 1.8324(1.9220) | Loss 5.5071(5.7620) | Error 0.6449(0.6747) Steps 544(573.61) | Grad Norm 1.3656(5.3210) | Total Time 14.00(14.00)\n",
      "Iter 0551 | Time 46.4060(48.7474) | Bit/dim 4.5909(4.7947) | Xent 1.8193(1.9189) | Loss 5.5005(5.7541) | Error 0.6451(0.6738) Steps 538(572.54) | Grad Norm 2.1445(5.2257) | Total Time 14.00(14.00)\n",
      "Iter 0552 | Time 49.8427(48.7803) | Bit/dim 4.5769(4.7881) | Xent 1.8378(1.9165) | Loss 5.4958(5.7464) | Error 0.6432(0.6729) Steps 562(572.23) | Grad Norm 2.3080(5.1382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 26.1210, Epoch Time 329.3098(327.5101), Bit/dim 4.5682(best: 4.5740), Xent 1.7847, Loss 5.4605, Error 0.6214(best: 0.6304)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 50.3668(48.8279) | Bit/dim 4.5600(4.7813) | Xent 1.8146(1.9134) | Loss 5.4673(5.7380) | Error 0.6465(0.6721) Steps 574(572.28) | Grad Norm 1.5496(5.0305) | Total Time 14.00(14.00)\n",
      "Iter 0554 | Time 48.0242(48.8038) | Bit/dim 4.5670(4.7749) | Xent 1.8254(1.9108) | Loss 5.4797(5.7303) | Error 0.6449(0.6713) Steps 562(571.97) | Grad Norm 1.6291(4.9285) | Total Time 14.00(14.00)\n",
      "Iter 0555 | Time 48.7892(48.8033) | Bit/dim 4.5574(4.7684) | Xent 1.8090(1.9077) | Loss 5.4619(5.7222) | Error 0.6375(0.6703) Steps 562(571.67) | Grad Norm 2.2356(4.8477) | Total Time 14.00(14.00)\n",
      "Iter 0556 | Time 49.6229(48.8279) | Bit/dim 4.5457(4.7617) | Xent 1.7961(1.9044) | Loss 5.4437(5.7138) | Error 0.6356(0.6692) Steps 568(571.56) | Grad Norm 2.6641(4.7822) | Total Time 14.00(14.00)\n",
      "Iter 0557 | Time 48.2650(48.8110) | Bit/dim 4.5454(4.7552) | Xent 1.8048(1.9014) | Loss 5.4478(5.7059) | Error 0.6387(0.6683) Steps 562(571.27) | Grad Norm 2.5773(4.7160) | Total Time 14.00(14.00)\n",
      "Iter 0558 | Time 47.7763(48.7800) | Bit/dim 4.5470(4.7489) | Xent 1.7970(1.8982) | Loss 5.4454(5.6981) | Error 0.6258(0.6670) Steps 574(571.36) | Grad Norm 2.3397(4.6448) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 25.3507, Epoch Time 334.6526(327.7243), Bit/dim 4.5378(best: 4.5682), Xent 1.7625, Loss 5.4190, Error 0.6227(best: 0.6214)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 50.9806(48.8460) | Bit/dim 4.5372(4.7426) | Xent 1.7997(1.8953) | Loss 5.4371(5.6902) | Error 0.6402(0.6662) Steps 580(571.62) | Grad Norm 3.4154(4.6079) | Total Time 14.00(14.00)\n",
      "Iter 0560 | Time 46.5366(48.7767) | Bit/dim 4.5375(4.7364) | Xent 1.7912(1.8922) | Loss 5.4331(5.6825) | Error 0.6316(0.6652) Steps 556(571.15) | Grad Norm 5.1789(4.6250) | Total Time 14.00(14.00)\n",
      "Iter 0561 | Time 50.0599(48.8152) | Bit/dim 4.5327(4.7303) | Xent 1.8285(1.8903) | Loss 5.4470(5.6754) | Error 0.6450(0.6646) Steps 574(571.23) | Grad Norm 6.7506(4.6888) | Total Time 14.00(14.00)\n",
      "Iter 0562 | Time 46.5549(48.7474) | Bit/dim 4.5505(4.7249) | Xent 1.8647(1.8895) | Loss 5.4829(5.6697) | Error 0.6579(0.6644) Steps 538(570.24) | Grad Norm 8.1692(4.7932) | Total Time 14.00(14.00)\n",
      "Iter 0563 | Time 49.9197(48.7826) | Bit/dim 4.5234(4.7189) | Xent 1.8556(1.8885) | Loss 5.4511(5.6631) | Error 0.6544(0.6641) Steps 556(569.81) | Grad Norm 8.0793(4.8918) | Total Time 14.00(14.00)\n",
      "Iter 0564 | Time 49.2953(48.7980) | Bit/dim 4.5115(4.7127) | Xent 1.8081(1.8861) | Loss 5.4155(5.6557) | Error 0.6366(0.6633) Steps 556(569.39) | Grad Norm 4.4600(4.8788) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 24.7408, Epoch Time 334.8806(327.9390), Bit/dim 4.4984(best: 4.5378), Xent 1.7657, Loss 5.3813, Error 0.6216(best: 0.6214)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 48.5756(48.7913) | Bit/dim 4.4952(4.7061) | Xent 1.8053(1.8836) | Loss 5.3978(5.6480) | Error 0.6332(0.6624) Steps 556(568.99) | Grad Norm 2.5795(4.8098) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 46.9179(48.7351) | Bit/dim 4.5113(4.7003) | Xent 1.8233(1.8818) | Loss 5.4229(5.6412) | Error 0.6456(0.6619) Steps 550(568.42) | Grad Norm 6.4362(4.8586) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 46.0618(48.6549) | Bit/dim 4.5046(4.6944) | Xent 1.8182(1.8799) | Loss 5.4137(5.6344) | Error 0.6465(0.6614) Steps 544(567.69) | Grad Norm 6.2312(4.8998) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 48.7906(48.6590) | Bit/dim 4.5046(4.6887) | Xent 1.7990(1.8775) | Loss 5.4041(5.6275) | Error 0.6386(0.6607) Steps 556(567.34) | Grad Norm 4.5420(4.8891) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 47.2871(48.6178) | Bit/dim 4.5403(4.6843) | Xent 1.8061(1.8753) | Loss 5.4433(5.6219) | Error 0.6406(0.6601) Steps 550(566.82) | Grad Norm 4.7474(4.8848) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 49.2845(48.6378) | Bit/dim 4.4965(4.6786) | Xent 1.7826(1.8726) | Loss 5.3878(5.6149) | Error 0.6319(0.6593) Steps 550(566.31) | Grad Norm 4.1700(4.8634) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 24.6446, Epoch Time 327.8378(327.9360), Bit/dim 4.4917(best: 4.4984), Xent 1.8071, Loss 5.3952, Error 0.6453(best: 0.6214)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 48.1938(48.6245) | Bit/dim 4.4950(4.6731) | Xent 1.8400(1.8716) | Loss 5.4150(5.6089) | Error 0.6550(0.6591) Steps 538(565.47) | Grad Norm 5.4251(4.8802) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 47.2044(48.5819) | Bit/dim 4.4934(4.6677) | Xent 1.7760(1.8687) | Loss 5.3815(5.6021) | Error 0.6291(0.6582) Steps 544(564.82) | Grad Norm 2.5680(4.8109) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 49.1383(48.5986) | Bit/dim 4.5150(4.6632) | Xent 1.8085(1.8669) | Loss 5.4192(5.5966) | Error 0.6442(0.6578) Steps 550(564.38) | Grad Norm 7.1151(4.8800) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 48.0033(48.5807) | Bit/dim 4.5844(4.6608) | Xent 1.8045(1.8650) | Loss 5.4867(5.5933) | Error 0.6431(0.6574) Steps 538(563.59) | Grad Norm 8.8199(4.9982) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 48.7583(48.5861) | Bit/dim 4.5183(4.6565) | Xent 1.8315(1.8640) | Loss 5.4341(5.5885) | Error 0.6461(0.6570) Steps 550(563.18) | Grad Norm 8.3842(5.0998) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 46.6052(48.5266) | Bit/dim 4.5300(4.6527) | Xent 1.7817(1.8616) | Loss 5.4208(5.5835) | Error 0.6301(0.6562) Steps 544(562.60) | Grad Norm 6.2297(5.1337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 25.1186, Epoch Time 329.5875(327.9855), Bit/dim 4.4800(best: 4.4917), Xent 1.7905, Loss 5.3752, Error 0.6391(best: 0.6214)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 49.7995(48.5648) | Bit/dim 4.4826(4.6476) | Xent 1.8271(1.8605) | Loss 5.3961(5.5779) | Error 0.6514(0.6561) Steps 550(562.22) | Grad Norm 3.4979(5.0846) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 49.1765(48.5832) | Bit/dim 4.4679(4.6422) | Xent 1.7818(1.8582) | Loss 5.3588(5.5713) | Error 0.6375(0.6555) Steps 550(561.86) | Grad Norm 4.6121(5.0704) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 47.1755(48.5409) | Bit/dim 4.5134(4.6384) | Xent 1.7974(1.8563) | Loss 5.4121(5.5665) | Error 0.6394(0.6550) Steps 550(561.50) | Grad Norm 5.7120(5.0897) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 48.4613(48.5385) | Bit/dim 4.4800(4.6336) | Xent 1.7682(1.8537) | Loss 5.3641(5.5605) | Error 0.6349(0.6544) Steps 550(561.16) | Grad Norm 4.2509(5.0645) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 47.3655(48.5033) | Bit/dim 4.5184(4.6302) | Xent 1.7346(1.8501) | Loss 5.3857(5.5552) | Error 0.6098(0.6531) Steps 556(561.00) | Grad Norm 4.9797(5.0620) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 49.7126(48.5396) | Bit/dim 4.4739(4.6255) | Xent 1.7794(1.8480) | Loss 5.3637(5.5495) | Error 0.6315(0.6525) Steps 562(561.03) | Grad Norm 4.2822(5.0386) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 24.9812, Epoch Time 333.9712(328.1651), Bit/dim 4.4655(best: 4.4800), Xent 1.7155, Loss 5.3233, Error 0.6066(best: 0.6214)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 47.8640(48.5194) | Bit/dim 4.4666(4.6207) | Xent 1.7439(1.8449) | Loss 5.3385(5.5431) | Error 0.6209(0.6515) Steps 544(560.52) | Grad Norm 3.9902(5.0071) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 47.6164(48.4923) | Bit/dim 4.4598(4.6159) | Xent 1.7683(1.8426) | Loss 5.3439(5.5372) | Error 0.6285(0.6508) Steps 550(560.21) | Grad Norm 4.7076(4.9981) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 48.7316(48.4994) | Bit/dim 4.4554(4.6111) | Xent 1.7454(1.8397) | Loss 5.3281(5.5309) | Error 0.6180(0.6498) Steps 544(559.72) | Grad Norm 3.3839(4.9497) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 50.0954(48.5473) | Bit/dim 4.4454(4.6061) | Xent 1.7496(1.8370) | Loss 5.3202(5.5246) | Error 0.6291(0.6492) Steps 562(559.79) | Grad Norm 1.5002(4.8462) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 49.6484(48.5804) | Bit/dim 4.4525(4.6015) | Xent 1.7325(1.8338) | Loss 5.3187(5.5184) | Error 0.6173(0.6483) Steps 562(559.85) | Grad Norm 1.8388(4.7560) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 49.1059(48.5961) | Bit/dim 4.4401(4.5966) | Xent 1.7223(1.8305) | Loss 5.3013(5.5119) | Error 0.6176(0.6473) Steps 562(559.92) | Grad Norm 1.7846(4.6668) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 25.4433, Epoch Time 335.5255(328.3859), Bit/dim 4.4359(best: 4.4655), Xent 1.6860, Loss 5.2789, Error 0.5992(best: 0.6066)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 49.2722(48.6164) | Bit/dim 4.4412(4.5920) | Xent 1.7274(1.8274) | Loss 5.3049(5.5057) | Error 0.6144(0.6463) Steps 562(559.98) | Grad Norm 2.1831(4.5923) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 47.9185(48.5955) | Bit/dim 4.4457(4.5876) | Xent 1.7187(1.8241) | Loss 5.3050(5.4997) | Error 0.6091(0.6452) Steps 568(560.22) | Grad Norm 3.7462(4.5670) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 50.9077(48.6648) | Bit/dim 4.4638(4.5839) | Xent 1.7594(1.8222) | Loss 5.3435(5.4950) | Error 0.6332(0.6449) Steps 568(560.45) | Grad Norm 8.5703(4.6871) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 49.9280(48.7027) | Bit/dim 4.6222(4.5850) | Xent 1.9335(1.8255) | Loss 5.5890(5.4978) | Error 0.6734(0.6457) Steps 574(560.86) | Grad Norm 16.3447(5.0368) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 50.2019(48.7477) | Bit/dim 4.5840(4.5850) | Xent 2.1558(1.8354) | Loss 5.6619(5.5027) | Error 0.7480(0.6488) Steps 580(561.44) | Grad Norm 17.6586(5.4154) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 47.8671(48.7213) | Bit/dim 4.5468(4.5838) | Xent 1.8669(1.8364) | Loss 5.4803(5.5020) | Error 0.6562(0.6490) Steps 556(561.27) | Grad Norm 6.2786(5.4413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 26.0839, Epoch Time 338.7036(328.6954), Bit/dim 4.5909(best: 4.4359), Xent 2.0330, Loss 5.6074, Error 0.7030(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 49.7087(48.7509) | Bit/dim 4.5957(4.5842) | Xent 2.0902(1.8440) | Loss 5.6408(5.5062) | Error 0.7109(0.6509) Steps 568(561.47) | Grad Norm 13.0587(5.6699) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 51.5880(48.8360) | Bit/dim 4.7527(4.5893) | Xent 2.3948(1.8605) | Loss 5.9501(5.5195) | Error 0.8051(0.6555) Steps 622(563.29) | Grad Norm 13.8882(5.9164) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 49.2151(48.8474) | Bit/dim 4.8334(4.5966) | Xent 2.1638(1.8696) | Loss 5.9153(5.5314) | Error 0.7592(0.6586) Steps 574(563.61) | Grad Norm 11.3713(6.0801) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 53.9394(49.0002) | Bit/dim 4.8650(4.6046) | Xent 2.0372(1.8746) | Loss 5.8836(5.5420) | Error 0.7261(0.6606) Steps 604(564.82) | Grad Norm 14.7708(6.3408) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 50.7418(49.0524) | Bit/dim 5.1682(4.6215) | Xent 2.1479(1.8828) | Loss 6.2422(5.5630) | Error 0.7726(0.6640) Steps 628(566.72) | Grad Norm 10.5890(6.4682) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 51.5471(49.1272) | Bit/dim 5.3311(4.6428) | Xent 2.0733(1.8886) | Loss 6.3677(5.5871) | Error 0.7346(0.6661) Steps 634(568.74) | Grad Norm 5.9120(6.4515) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 25.8295, Epoch Time 349.4310(329.3175), Bit/dim 5.2015(best: 4.4359), Xent 2.0171, Loss 6.2101, Error 0.7216(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 51.9620(49.2123) | Bit/dim 5.2029(4.6596) | Xent 2.0741(1.8941) | Loss 6.2399(5.6067) | Error 0.7358(0.6682) Steps 628(570.51) | Grad Norm 4.1407(6.3822) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 50.2036(49.2420) | Bit/dim 5.0981(4.6728) | Xent 2.0930(1.9001) | Loss 6.1446(5.6228) | Error 0.7204(0.6698) Steps 592(571.16) | Grad Norm 7.7254(6.4225) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 49.5416(49.2510) | Bit/dim 4.9328(4.6806) | Xent 2.0471(1.9045) | Loss 5.9564(5.6328) | Error 0.7250(0.6714) Steps 580(571.42) | Grad Norm 4.9605(6.3787) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 50.3226(49.2832) | Bit/dim 4.8564(4.6859) | Xent 2.2602(1.9152) | Loss 5.9866(5.6434) | Error 0.7945(0.6751) Steps 580(571.68) | Grad Norm 15.1726(6.6425) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 49.0417(49.2759) | Bit/dim 5.0156(4.6958) | Xent 2.6902(1.9384) | Loss 6.3607(5.6650) | Error 0.7595(0.6777) Steps 568(571.57) | Grad Norm 18.6839(7.0037) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 49.4706(49.2818) | Bit/dim 4.8451(4.7002) | Xent 2.0906(1.9430) | Loss 5.8904(5.6717) | Error 0.7067(0.6785) Steps 580(571.82) | Grad Norm 6.6668(6.9936) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 25.9675, Epoch Time 343.1551(329.7326), Bit/dim 4.8891(best: 4.4359), Xent 2.2251, Loss 6.0017, Error 0.7624(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 51.0280(49.3341) | Bit/dim 4.9005(4.7062) | Xent 2.2611(1.9525) | Loss 6.0311(5.6825) | Error 0.7706(0.6813) Steps 610(572.97) | Grad Norm 12.2978(7.1527) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 52.3890(49.4258) | Bit/dim 4.8461(4.7104) | Xent 2.0316(1.9549) | Loss 5.8619(5.6879) | Error 0.7143(0.6823) Steps 604(573.90) | Grad Norm 3.2352(7.0352) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 51.4125(49.4854) | Bit/dim 4.7805(4.7125) | Xent 2.0234(1.9570) | Loss 5.7922(5.6910) | Error 0.7091(0.6831) Steps 616(575.16) | Grad Norm 3.0612(6.9160) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 51.7295(49.5527) | Bit/dim 4.7849(4.7147) | Xent 2.0374(1.9594) | Loss 5.8036(5.6944) | Error 0.7180(0.6841) Steps 574(575.13) | Grad Norm 5.9216(6.8862) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 48.1627(49.5110) | Bit/dim 4.7958(4.7171) | Xent 1.9805(1.9600) | Loss 5.7861(5.6971) | Error 0.7064(0.6848) Steps 574(575.09) | Grad Norm 6.3917(6.8713) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 48.4575(49.4794) | Bit/dim 4.7158(4.7171) | Xent 1.9772(1.9605) | Loss 5.7044(5.6974) | Error 0.7069(0.6855) Steps 574(575.06) | Grad Norm 2.5718(6.7423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 24.6176, Epoch Time 344.3229(330.1704), Bit/dim 4.7923(best: 4.4359), Xent 1.9921, Loss 5.7883, Error 0.7100(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 50.6528(49.5146) | Bit/dim 4.7950(4.7194) | Xent 2.0590(1.9635) | Loss 5.8246(5.7012) | Error 0.7235(0.6866) Steps 568(574.85) | Grad Norm 11.8676(6.8961) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 47.7219(49.4608) | Bit/dim 4.9072(4.7251) | Xent 2.0489(1.9660) | Loss 5.9316(5.7081) | Error 0.7149(0.6874) Steps 562(574.46) | Grad Norm 9.6043(6.9773) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 47.3489(49.3975) | Bit/dim 4.8537(4.7289) | Xent 2.0134(1.9675) | Loss 5.8604(5.7127) | Error 0.7132(0.6882) Steps 562(574.09) | Grad Norm 8.3250(7.0178) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 49.4474(49.3990) | Bit/dim 4.7852(4.7306) | Xent 2.0795(1.9708) | Loss 5.8250(5.7160) | Error 0.7264(0.6894) Steps 556(573.55) | Grad Norm 13.8240(7.2220) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 47.4048(49.3391) | Bit/dim 4.7076(4.7299) | Xent 1.9623(1.9706) | Loss 5.6887(5.7152) | Error 0.7012(0.6897) Steps 544(572.66) | Grad Norm 4.9025(7.1524) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 46.9911(49.2687) | Bit/dim 4.7033(4.7291) | Xent 1.9551(1.9701) | Loss 5.6808(5.7142) | Error 0.7096(0.6903) Steps 532(571.44) | Grad Norm 4.9408(7.0860) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 25.0499, Epoch Time 331.1080(330.1985), Bit/dim 4.6267(best: 4.4359), Xent 1.9145, Loss 5.5840, Error 0.6855(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 48.8077(49.2549) | Bit/dim 4.6424(4.7265) | Xent 1.9646(1.9699) | Loss 5.6247(5.7115) | Error 0.7000(0.6906) Steps 556(570.98) | Grad Norm 4.7583(7.0162) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 45.9373(49.1553) | Bit/dim 4.5986(4.7227) | Xent 1.9345(1.9689) | Loss 5.5659(5.7071) | Error 0.6823(0.6904) Steps 544(570.17) | Grad Norm 3.2178(6.9022) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 46.9233(49.0884) | Bit/dim 4.6285(4.7199) | Xent 1.9240(1.9675) | Loss 5.5906(5.7036) | Error 0.6778(0.6900) Steps 538(569.20) | Grad Norm 4.4312(6.8281) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 46.6399(49.0149) | Bit/dim 4.5947(4.7161) | Xent 1.9102(1.9658) | Loss 5.5498(5.6990) | Error 0.6796(0.6897) Steps 538(568.27) | Grad Norm 2.6583(6.7030) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 47.2040(48.9606) | Bit/dim 4.5988(4.7126) | Xent 1.9107(1.9642) | Loss 5.5541(5.6947) | Error 0.6846(0.6895) Steps 544(567.54) | Grad Norm 5.1159(6.6554) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 46.0802(48.8742) | Bit/dim 4.5757(4.7085) | Xent 1.8834(1.9617) | Loss 5.5174(5.6893) | Error 0.6667(0.6888) Steps 526(566.29) | Grad Norm 2.6191(6.5343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 23.8021, Epoch Time 321.7812(329.9460), Bit/dim 4.5720(best: 4.4359), Xent 1.8430, Loss 5.4935, Error 0.6429(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 45.2496(48.7655) | Bit/dim 4.5684(4.7043) | Xent 1.8719(1.9590) | Loss 5.5044(5.6838) | Error 0.6724(0.6883) Steps 532(565.26) | Grad Norm 2.9108(6.4256) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 47.3326(48.7225) | Bit/dim 4.5676(4.7002) | Xent 1.8720(1.9564) | Loss 5.5036(5.6784) | Error 0.6638(0.6876) Steps 544(564.63) | Grad Norm 3.0521(6.3244) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 48.5536(48.7174) | Bit/dim 4.5383(4.6953) | Xent 1.8466(1.9531) | Loss 5.4616(5.6719) | Error 0.6561(0.6867) Steps 544(564.01) | Grad Norm 1.7857(6.1882) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 46.0570(48.6376) | Bit/dim 4.5255(4.6902) | Xent 1.8468(1.9499) | Loss 5.4489(5.6652) | Error 0.6525(0.6856) Steps 538(563.23) | Grad Norm 2.1272(6.0664) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 49.1582(48.6532) | Bit/dim 4.5106(4.6848) | Xent 1.8390(1.9466) | Loss 5.4301(5.6581) | Error 0.6511(0.6846) Steps 556(563.01) | Grad Norm 1.5796(5.9318) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 49.0553(48.6653) | Bit/dim 4.5076(4.6795) | Xent 1.8381(1.9434) | Loss 5.4267(5.6512) | Error 0.6518(0.6836) Steps 544(562.44) | Grad Norm 2.1880(5.8195) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 25.0303, Epoch Time 326.9602(329.8564), Bit/dim 4.4998(best: 4.4359), Xent 1.7876, Loss 5.3936, Error 0.6235(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 47.6822(48.6358) | Bit/dim 4.5017(4.6742) | Xent 1.8173(1.9396) | Loss 5.4104(5.6440) | Error 0.6472(0.6825) Steps 550(562.07) | Grad Norm 1.3611(5.6857) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 49.3615(48.6575) | Bit/dim 4.4777(4.6683) | Xent 1.7935(1.9352) | Loss 5.3744(5.6359) | Error 0.6265(0.6808) Steps 550(561.71) | Grad Norm 1.6511(5.5647) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 48.9466(48.6662) | Bit/dim 4.4857(4.6628) | Xent 1.8143(1.9316) | Loss 5.3928(5.6286) | Error 0.6454(0.6798) Steps 556(561.53) | Grad Norm 2.1775(5.4631) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 49.5776(48.6936) | Bit/dim 4.4750(4.6572) | Xent 1.8133(1.9280) | Loss 5.3817(5.6212) | Error 0.6410(0.6786) Steps 556(561.37) | Grad Norm 2.7469(5.3816) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 49.2660(48.7107) | Bit/dim 4.4707(4.6516) | Xent 1.8163(1.9247) | Loss 5.3788(5.6139) | Error 0.6407(0.6775) Steps 556(561.21) | Grad Norm 2.3523(5.2907) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 50.4761(48.7637) | Bit/dim 4.4520(4.6456) | Xent 1.7997(1.9209) | Loss 5.3518(5.6061) | Error 0.6402(0.6764) Steps 544(560.69) | Grad Norm 1.4193(5.1746) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 25.0757, Epoch Time 338.1202(330.1043), Bit/dim 4.4513(best: 4.4359), Xent 1.7668, Loss 5.3347, Error 0.6194(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 49.0094(48.7711) | Bit/dim 4.4480(4.6397) | Xent 1.7838(1.9168) | Loss 5.3400(5.5981) | Error 0.6326(0.6751) Steps 556(560.55) | Grad Norm 2.7052(5.1005) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 49.6959(48.7988) | Bit/dim 4.4333(4.6335) | Xent 1.7881(1.9129) | Loss 5.3274(5.5900) | Error 0.6325(0.6738) Steps 562(560.59) | Grad Norm 2.0485(5.0089) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 49.0633(48.8068) | Bit/dim 4.4485(4.6279) | Xent 1.7795(1.9089) | Loss 5.3383(5.5824) | Error 0.6284(0.6724) Steps 550(560.28) | Grad Norm 1.5515(4.9052) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 50.1892(48.8482) | Bit/dim 4.4262(4.6219) | Xent 1.7850(1.9052) | Loss 5.3187(5.5745) | Error 0.6231(0.6709) Steps 556(560.15) | Grad Norm 2.0751(4.8203) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 48.7189(48.8443) | Bit/dim 4.4318(4.6162) | Xent 1.7792(1.9014) | Loss 5.3214(5.5669) | Error 0.6236(0.6695) Steps 544(559.66) | Grad Norm 1.2563(4.7134) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 48.7363(48.8411) | Bit/dim 4.4158(4.6102) | Xent 1.7798(1.8978) | Loss 5.3057(5.5591) | Error 0.6216(0.6681) Steps 538(559.01) | Grad Norm 1.7216(4.6236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 24.5869, Epoch Time 336.3027(330.2903), Bit/dim 4.4139(best: 4.4359), Xent 1.7307, Loss 5.2793, Error 0.6081(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 48.2759(48.8241) | Bit/dim 4.4222(4.6045) | Xent 1.7603(1.8937) | Loss 5.3024(5.5514) | Error 0.6267(0.6668) Steps 550(558.74) | Grad Norm 1.7234(4.5366) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 46.5068(48.7546) | Bit/dim 4.4146(4.5988) | Xent 1.7618(1.8897) | Loss 5.2955(5.5437) | Error 0.6230(0.6655) Steps 544(558.30) | Grad Norm 2.6243(4.4793) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 49.2695(48.7701) | Bit/dim 4.4228(4.5936) | Xent 1.7913(1.8868) | Loss 5.3185(5.5369) | Error 0.6347(0.6646) Steps 562(558.41) | Grad Norm 5.8327(4.5199) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 47.3113(48.7263) | Bit/dim 4.4900(4.5904) | Xent 1.8395(1.8853) | Loss 5.4098(5.5331) | Error 0.6575(0.6644) Steps 544(557.98) | Grad Norm 9.8658(4.6802) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 49.3203(48.7441) | Bit/dim 4.4768(4.5870) | Xent 1.9195(1.8864) | Loss 5.4366(5.5302) | Error 0.6634(0.6644) Steps 568(558.28) | Grad Norm 15.7713(5.0130) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 47.0600(48.6936) | Bit/dim 4.6776(4.5898) | Xent 1.9123(1.8871) | Loss 5.6337(5.5333) | Error 0.6731(0.6646) Steps 538(557.67) | Grad Norm 11.3832(5.2041) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 24.3891, Epoch Time 329.3908(330.2633), Bit/dim 4.5000(best: 4.4139), Xent 1.8561, Loss 5.4280, Error 0.6545(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 46.8723(48.6390) | Bit/dim 4.4976(4.5870) | Xent 1.8862(1.8871) | Loss 5.4406(5.5305) | Error 0.6734(0.6649) Steps 538(557.08) | Grad Norm 7.0090(5.2582) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 51.3075(48.7190) | Bit/dim 4.6837(4.5899) | Xent 1.9346(1.8885) | Loss 5.6511(5.5342) | Error 0.6850(0.6655) Steps 544(556.69) | Grad Norm 16.8190(5.6051) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 46.3629(48.6483) | Bit/dim 4.8636(4.5981) | Xent 1.9111(1.8892) | Loss 5.8192(5.5427) | Error 0.6919(0.6663) Steps 532(555.95) | Grad Norm 6.4567(5.6306) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 44.2023(48.5150) | Bit/dim 4.9800(4.6096) | Xent 1.8309(1.8875) | Loss 5.8955(5.5533) | Error 0.6441(0.6656) Steps 526(555.05) | Grad Norm 5.6842(5.6322) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 47.0587(48.4713) | Bit/dim 4.6802(4.6117) | Xent 1.8609(1.8867) | Loss 5.6106(5.5550) | Error 0.6627(0.6655) Steps 544(554.72) | Grad Norm 5.2069(5.6195) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 49.1494(48.4916) | Bit/dim 4.6595(4.6131) | Xent 1.9562(1.8888) | Loss 5.6375(5.5575) | Error 0.6780(0.6659) Steps 562(554.94) | Grad Norm 7.5594(5.6777) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 24.4361, Epoch Time 325.8045(330.1295), Bit/dim 4.5689(best: 4.4139), Xent 1.9236, Loss 5.5307, Error 0.6899(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 48.2975(48.4858) | Bit/dim 4.5652(4.6117) | Xent 1.9466(1.8905) | Loss 5.5385(5.5569) | Error 0.6949(0.6668) Steps 556(554.97) | Grad Norm 6.3697(5.6984) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 46.5239(48.4269) | Bit/dim 4.6296(4.6122) | Xent 1.8327(1.8888) | Loss 5.5459(5.5566) | Error 0.6511(0.6663) Steps 538(554.46) | Grad Norm 4.5325(5.6634) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 46.6171(48.3726) | Bit/dim 4.5763(4.6111) | Xent 1.8617(1.8880) | Loss 5.5071(5.5551) | Error 0.6621(0.6662) Steps 538(553.97) | Grad Norm 4.0914(5.6163) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 50.6609(48.4413) | Bit/dim 4.5657(4.6098) | Xent 1.8311(1.8862) | Loss 5.4812(5.5529) | Error 0.6539(0.6658) Steps 562(554.21) | Grad Norm 4.3429(5.5781) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 48.2324(48.4350) | Bit/dim 4.5251(4.6072) | Xent 1.8056(1.8838) | Loss 5.4279(5.5491) | Error 0.6402(0.6650) Steps 562(554.44) | Grad Norm 2.6599(5.4905) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 48.1001(48.4250) | Bit/dim 4.5414(4.6053) | Xent 1.8068(1.8815) | Loss 5.4448(5.5460) | Error 0.6381(0.6642) Steps 568(554.85) | Grad Norm 3.3792(5.4272) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 25.0822, Epoch Time 329.7946(330.1195), Bit/dim 4.5156(best: 4.4139), Xent 1.7502, Loss 5.3907, Error 0.6202(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 47.9822(48.4117) | Bit/dim 4.5138(4.6025) | Xent 1.7744(1.8783) | Loss 5.4010(5.5417) | Error 0.6259(0.6631) Steps 574(555.42) | Grad Norm 2.8628(5.3503) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 49.3166(48.4388) | Bit/dim 4.4884(4.5991) | Xent 1.7996(1.8759) | Loss 5.3882(5.5371) | Error 0.6380(0.6623) Steps 574(555.98) | Grad Norm 3.9003(5.3068) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 47.8701(48.4218) | Bit/dim 4.4708(4.5952) | Xent 1.7658(1.8726) | Loss 5.3537(5.5316) | Error 0.6234(0.6612) Steps 574(556.52) | Grad Norm 2.4215(5.2202) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 48.4465(48.4225) | Bit/dim 4.4670(4.5914) | Xent 1.7880(1.8701) | Loss 5.3610(5.5264) | Error 0.6438(0.6606) Steps 574(557.04) | Grad Norm 3.6637(5.1735) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 49.4191(48.4524) | Bit/dim 4.4502(4.5872) | Xent 1.7823(1.8675) | Loss 5.3413(5.5209) | Error 0.6344(0.6599) Steps 562(557.19) | Grad Norm 3.7588(5.1311) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 50.0613(48.5007) | Bit/dim 4.4549(4.5832) | Xent 1.7721(1.8646) | Loss 5.3410(5.5155) | Error 0.6380(0.6592) Steps 562(557.34) | Grad Norm 2.8791(5.0635) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 24.6386, Epoch Time 334.9678(330.2649), Bit/dim 4.4372(best: 4.4139), Xent 1.7207, Loss 5.2975, Error 0.6043(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 49.7713(48.5388) | Bit/dim 4.4331(4.5787) | Xent 1.7494(1.8611) | Loss 5.3077(5.5093) | Error 0.6285(0.6583) Steps 544(556.94) | Grad Norm 2.4520(4.9852) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 47.7284(48.5145) | Bit/dim 4.4398(4.5745) | Xent 1.7317(1.8573) | Loss 5.3056(5.5031) | Error 0.6094(0.6568) Steps 538(556.37) | Grad Norm 3.6390(4.9448) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 49.5667(48.5461) | Bit/dim 4.4427(4.5706) | Xent 1.7289(1.8534) | Loss 5.3072(5.4973) | Error 0.6260(0.6559) Steps 550(556.18) | Grad Norm 2.9073(4.8837) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 47.7026(48.5208) | Bit/dim 4.4111(4.5658) | Xent 1.7488(1.8503) | Loss 5.2855(5.4909) | Error 0.6180(0.6547) Steps 532(555.45) | Grad Norm 2.4002(4.8091) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 46.8203(48.4697) | Bit/dim 4.4140(4.5612) | Xent 1.7416(1.8470) | Loss 5.2848(5.4847) | Error 0.6206(0.6537) Steps 544(555.11) | Grad Norm 2.1749(4.7301) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 49.6004(48.5037) | Bit/dim 4.4152(4.5568) | Xent 1.7298(1.8435) | Loss 5.2801(5.4786) | Error 0.6185(0.6527) Steps 550(554.96) | Grad Norm 2.3589(4.6590) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 24.4449, Epoch Time 332.5686(330.3340), Bit/dim 4.4014(best: 4.4139), Xent 1.6836, Loss 5.2433, Error 0.5909(best: 0.5992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 49.3854(48.5301) | Bit/dim 4.4011(4.5522) | Xent 1.7104(1.8395) | Loss 5.2563(5.4719) | Error 0.6114(0.6514) Steps 556(554.99) | Grad Norm 2.2779(4.5876) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 49.4939(48.5590) | Bit/dim 4.3828(4.5471) | Xent 1.6943(1.8352) | Loss 5.2300(5.4647) | Error 0.6044(0.6500) Steps 556(555.02) | Grad Norm 3.1181(4.5435) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 50.4391(48.6154) | Bit/dim 4.3882(4.5423) | Xent 1.7175(1.8316) | Loss 5.2469(5.4581) | Error 0.6029(0.6486) Steps 544(554.69) | Grad Norm 3.9517(4.5257) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 47.5509(48.5835) | Bit/dim 4.3863(4.5376) | Xent 1.7167(1.8282) | Loss 5.2446(5.4517) | Error 0.6093(0.6474) Steps 544(554.37) | Grad Norm 3.2590(4.4877) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 47.1752(48.5412) | Bit/dim 4.3736(4.5327) | Xent 1.7003(1.8243) | Loss 5.2238(5.4449) | Error 0.6059(0.6462) Steps 538(553.88) | Grad Norm 1.9894(4.4128) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 47.6364(48.5141) | Bit/dim 4.3835(4.5282) | Xent 1.6993(1.8206) | Loss 5.2332(5.4385) | Error 0.6055(0.6450) Steps 538(553.40) | Grad Norm 3.6166(4.3889) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 24.5159, Epoch Time 333.1070(330.4172), Bit/dim 4.3861(best: 4.4014), Xent 1.6514, Loss 5.2118, Error 0.5802(best: 0.5909)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 46.9129(48.4661) | Bit/dim 4.3893(4.5241) | Xent 1.6770(1.8163) | Loss 5.2278(5.4322) | Error 0.5953(0.6435) Steps 538(552.94) | Grad Norm 3.1343(4.3512) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 49.2854(48.4906) | Bit/dim 4.3543(4.5190) | Xent 1.6894(1.8125) | Loss 5.1990(5.4252) | Error 0.6026(0.6422) Steps 550(552.85) | Grad Norm 1.5902(4.2684) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 49.0931(48.5087) | Bit/dim 4.3570(4.5141) | Xent 1.7040(1.8092) | Loss 5.2091(5.4187) | Error 0.6046(0.6411) Steps 544(552.58) | Grad Norm 3.8601(4.2562) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 46.3867(48.4451) | Bit/dim 4.3858(4.5103) | Xent 1.6839(1.8055) | Loss 5.2278(5.4130) | Error 0.6002(0.6399) Steps 538(552.15) | Grad Norm 4.0252(4.2492) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 47.0218(48.4024) | Bit/dim 4.3605(4.5058) | Xent 1.6751(1.8015) | Loss 5.1981(5.4066) | Error 0.6036(0.6388) Steps 538(551.72) | Grad Norm 4.7772(4.2651) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 48.2583(48.3980) | Bit/dim 4.3862(4.5022) | Xent 1.7126(1.7989) | Loss 5.2425(5.4016) | Error 0.6094(0.6379) Steps 544(551.49) | Grad Norm 9.0809(4.4095) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 24.3851, Epoch Time 327.7608(330.3375), Bit/dim 4.4281(best: 4.3861), Xent 1.8640, Loss 5.3601, Error 0.6718(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 48.7929(48.4099) | Bit/dim 4.4218(4.4998) | Xent 1.8885(1.8016) | Loss 5.3661(5.4006) | Error 0.6689(0.6388) Steps 562(551.81) | Grad Norm 15.0279(4.7281) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 46.5637(48.3545) | Bit/dim 4.5121(4.5002) | Xent 1.9213(1.8052) | Loss 5.4728(5.4027) | Error 0.6651(0.6396) Steps 538(551.39) | Grad Norm 13.4265(4.9890) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 49.4534(48.3875) | Bit/dim 4.4466(4.4985) | Xent 1.7700(1.8041) | Loss 5.3316(5.4006) | Error 0.6322(0.6394) Steps 556(551.53) | Grad Norm 9.0828(5.1119) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 48.9603(48.4046) | Bit/dim 4.5310(4.4995) | Xent 1.7900(1.8037) | Loss 5.4260(5.4014) | Error 0.6344(0.6393) Steps 574(552.20) | Grad Norm 7.5472(5.1849) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 50.7259(48.4743) | Bit/dim 4.4597(4.4983) | Xent 1.7991(1.8035) | Loss 5.3592(5.4001) | Error 0.6386(0.6392) Steps 610(553.94) | Grad Norm 6.0088(5.2096) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 50.7285(48.5419) | Bit/dim 4.4448(4.4967) | Xent 1.7894(1.8031) | Loss 5.3395(5.3983) | Error 0.6321(0.6390) Steps 574(554.54) | Grad Norm 7.0780(5.2657) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 24.9775, Epoch Time 336.9738(330.5366), Bit/dim 4.5936(best: 4.3861), Xent 1.8109, Loss 5.4990, Error 0.6525(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 49.0563(48.5573) | Bit/dim 4.5958(4.4997) | Xent 1.8287(1.8039) | Loss 5.5101(5.4016) | Error 0.6555(0.6395) Steps 574(555.12) | Grad Norm 8.4615(5.3616) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 45.8729(48.4768) | Bit/dim 4.4827(4.4992) | Xent 1.7418(1.8020) | Loss 5.3536(5.4002) | Error 0.6341(0.6394) Steps 544(554.79) | Grad Norm 3.4315(5.3037) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 48.0496(48.4640) | Bit/dim 4.5622(4.5011) | Xent 1.8428(1.8033) | Loss 5.4836(5.4027) | Error 0.6407(0.6394) Steps 550(554.65) | Grad Norm 11.2380(5.4817) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 47.5374(48.4362) | Bit/dim 4.5966(4.5039) | Xent 1.9028(1.8062) | Loss 5.5480(5.4071) | Error 0.6890(0.6409) Steps 568(555.05) | Grad Norm 8.3656(5.5682) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 47.8799(48.4195) | Bit/dim 4.5450(4.5052) | Xent 1.7571(1.8048) | Loss 5.4236(5.4076) | Error 0.6234(0.6404) Steps 556(555.07) | Grad Norm 5.6882(5.5718) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 47.3263(48.3867) | Bit/dim 4.5828(4.5075) | Xent 1.8808(1.8070) | Loss 5.5232(5.4110) | Error 0.6601(0.6410) Steps 550(554.92) | Grad Norm 8.5337(5.6607) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 25.3183, Epoch Time 327.8977(330.4574), Bit/dim 4.4779(best: 4.3861), Xent 1.7255, Loss 5.3406, Error 0.6262(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 47.2691(48.3532) | Bit/dim 4.4772(4.5066) | Xent 1.7318(1.8048) | Loss 5.3431(5.4090) | Error 0.6309(0.6407) Steps 556(554.96) | Grad Norm 3.0716(5.5830) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 48.5474(48.3590) | Bit/dim 4.5133(4.5068) | Xent 1.7696(1.8037) | Loss 5.3981(5.4087) | Error 0.6298(0.6403) Steps 568(555.35) | Grad Norm 2.9832(5.5050) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 49.6613(48.3981) | Bit/dim 4.5099(4.5069) | Xent 1.8323(1.8046) | Loss 5.4261(5.4092) | Error 0.6598(0.6409) Steps 562(555.55) | Grad Norm 4.1694(5.4649) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 47.3560(48.3668) | Bit/dim 4.5031(4.5068) | Xent 1.7249(1.8022) | Loss 5.3655(5.4079) | Error 0.6145(0.6401) Steps 550(555.38) | Grad Norm 2.6500(5.3805) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 46.8975(48.3227) | Bit/dim 4.4290(4.5044) | Xent 1.7373(1.8003) | Loss 5.2976(5.4046) | Error 0.6260(0.6397) Steps 556(555.40) | Grad Norm 2.6911(5.2998) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 50.5189(48.3886) | Bit/dim 4.4599(4.5031) | Xent 1.7440(1.7986) | Loss 5.3319(5.4024) | Error 0.6229(0.6392) Steps 568(555.78) | Grad Norm 4.9389(5.2890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 24.8432, Epoch Time 331.6813(330.4942), Bit/dim 4.4643(best: 4.3861), Xent 1.7054, Loss 5.3170, Error 0.6031(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 47.6045(48.3651) | Bit/dim 4.4592(4.5018) | Xent 1.7266(1.7964) | Loss 5.3225(5.4000) | Error 0.6191(0.6386) Steps 556(555.78) | Grad Norm 3.9587(5.2491) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 47.8044(48.3483) | Bit/dim 4.4332(4.4997) | Xent 1.7107(1.7938) | Loss 5.2885(5.3966) | Error 0.6150(0.6379) Steps 556(555.79) | Grad Norm 3.2354(5.1887) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 51.7126(48.4492) | Bit/dim 4.4309(4.4977) | Xent 1.7088(1.7913) | Loss 5.2852(5.3933) | Error 0.6100(0.6370) Steps 622(557.78) | Grad Norm 5.7984(5.2069) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 51.2026(48.5318) | Bit/dim 4.4073(4.4949) | Xent 1.7019(1.7886) | Loss 5.2582(5.3892) | Error 0.6076(0.6362) Steps 604(559.16) | Grad Norm 2.9805(5.1402) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 48.0905(48.5186) | Bit/dim 4.4260(4.4929) | Xent 1.6759(1.7852) | Loss 5.2639(5.3855) | Error 0.5978(0.6350) Steps 562(559.25) | Grad Norm 3.4862(5.0905) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 46.6015(48.4611) | Bit/dim 4.3987(4.4901) | Xent 1.6808(1.7821) | Loss 5.2391(5.3811) | Error 0.5941(0.6338) Steps 550(558.97) | Grad Norm 3.6786(5.0482) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 25.4429, Epoch Time 335.0846(330.6319), Bit/dim 4.3657(best: 4.3861), Xent 1.6411, Loss 5.1863, Error 0.5852(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 47.7017(48.4383) | Bit/dim 4.3707(4.4865) | Xent 1.6874(1.7792) | Loss 5.2144(5.3761) | Error 0.6035(0.6329) Steps 550(558.70) | Grad Norm 1.9945(4.9566) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 47.3154(48.4046) | Bit/dim 4.3544(4.4825) | Xent 1.6836(1.7764) | Loss 5.1962(5.3707) | Error 0.6041(0.6320) Steps 544(558.26) | Grad Norm 3.8466(4.9233) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 48.9976(48.4224) | Bit/dim 4.3670(4.4790) | Xent 1.6771(1.7734) | Loss 5.2055(5.3657) | Error 0.6048(0.6312) Steps 574(558.73) | Grad Norm 4.8285(4.9204) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 48.2892(48.4184) | Bit/dim 4.3560(4.4754) | Xent 1.6441(1.7695) | Loss 5.1781(5.3601) | Error 0.5837(0.6298) Steps 580(559.37) | Grad Norm 2.8475(4.8582) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 50.7586(48.4886) | Bit/dim 4.3507(4.4716) | Xent 1.6708(1.7666) | Loss 5.1862(5.3549) | Error 0.5979(0.6288) Steps 586(560.17) | Grad Norm 2.5086(4.7877) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 51.8912(48.5907) | Bit/dim 4.3338(4.4675) | Xent 1.6814(1.7640) | Loss 5.1745(5.3495) | Error 0.6018(0.6280) Steps 586(560.94) | Grad Norm 3.5440(4.7504) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 25.0868, Epoch Time 336.4135(330.8053), Bit/dim 4.3385(best: 4.3657), Xent 1.6317, Loss 5.1543, Error 0.5844(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 46.7555(48.5356) | Bit/dim 4.3379(4.4636) | Xent 1.6600(1.7609) | Loss 5.1678(5.3440) | Error 0.5948(0.6270) Steps 544(560.44) | Grad Norm 3.5261(4.7137) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 49.0818(48.5520) | Bit/dim 4.3230(4.4594) | Xent 1.6393(1.7572) | Loss 5.1427(5.3380) | Error 0.5894(0.6259) Steps 550(560.12) | Grad Norm 3.0794(4.6647) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 48.9733(48.5646) | Bit/dim 4.3114(4.4549) | Xent 1.6143(1.7529) | Loss 5.1185(5.3314) | Error 0.5754(0.6244) Steps 544(559.64) | Grad Norm 1.6081(4.5730) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 49.3758(48.5890) | Bit/dim 4.3200(4.4509) | Xent 1.6135(1.7488) | Loss 5.1268(5.3253) | Error 0.5745(0.6229) Steps 550(559.35) | Grad Norm 2.1480(4.5002) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 48.2458(48.5787) | Bit/dim 4.3177(4.4469) | Xent 1.6417(1.7456) | Loss 5.1386(5.3197) | Error 0.5877(0.6218) Steps 556(559.25) | Grad Norm 3.3429(4.4655) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 48.3505(48.5718) | Bit/dim 4.2985(4.4424) | Xent 1.6622(1.7431) | Loss 5.1296(5.3140) | Error 0.5926(0.6209) Steps 556(559.15) | Grad Norm 4.1554(4.4562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 25.4672, Epoch Time 332.8940(330.8680), Bit/dim 4.3073(best: 4.3385), Xent 1.5980, Loss 5.1063, Error 0.5716(best: 0.5802)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 49.8453(48.6100) | Bit/dim 4.3166(4.4387) | Xent 1.6298(1.7397) | Loss 5.1315(5.3085) | Error 0.5807(0.6197) Steps 580(559.78) | Grad Norm 5.2329(4.4795) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 48.0910(48.5945) | Bit/dim 4.3411(4.4357) | Xent 1.6720(1.7376) | Loss 5.1771(5.3046) | Error 0.5971(0.6191) Steps 544(559.30) | Grad Norm 6.5533(4.5417) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 51.8343(48.6917) | Bit/dim 4.3214(4.4323) | Xent 1.6941(1.7363) | Loss 5.1685(5.3005) | Error 0.5960(0.6184) Steps 592(560.28) | Grad Norm 8.9929(4.6753) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 46.0308(48.6118) | Bit/dim 4.3882(4.4310) | Xent 1.8411(1.7395) | Loss 5.3087(5.3007) | Error 0.6441(0.6191) Steps 544(559.80) | Grad Norm 10.7273(4.8568) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 49.2766(48.6318) | Bit/dim 4.3016(4.4271) | Xent 1.6637(1.7372) | Loss 5.1335(5.2957) | Error 0.5990(0.6185) Steps 562(559.86) | Grad Norm 3.8381(4.8263) | Total Time 14.00(14.00)\n",
      "Iter 0726 | Time 48.6177(48.6314) | Bit/dim 4.3170(4.4238) | Xent 1.6815(1.7355) | Loss 5.1578(5.2916) | Error 0.5961(0.6179) Steps 562(559.93) | Grad Norm 5.3157(4.8409) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 24.6942, Epoch Time 335.0741(330.9942), Bit/dim 4.3219(best: 4.3073), Xent 1.6378, Loss 5.1408, Error 0.5863(best: 0.5716)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 46.6483(48.5719) | Bit/dim 4.3313(4.4210) | Xent 1.6529(1.7330) | Loss 5.1577(5.2875) | Error 0.5911(0.6171) Steps 550(559.63) | Grad Norm 3.6773(4.8060) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 51.2251(48.6515) | Bit/dim 4.3182(4.4179) | Xent 1.6810(1.7315) | Loss 5.1587(5.2837) | Error 0.5968(0.6164) Steps 574(560.06) | Grad Norm 4.5514(4.7984) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 51.8109(48.7462) | Bit/dim 4.2927(4.4142) | Xent 1.6526(1.7291) | Loss 5.1190(5.2787) | Error 0.5922(0.6157) Steps 562(560.12) | Grad Norm 2.6534(4.7340) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 47.7959(48.7177) | Bit/dim 4.3327(4.4117) | Xent 1.6487(1.7267) | Loss 5.1570(5.2751) | Error 0.5889(0.6149) Steps 580(560.71) | Grad Norm 4.2345(4.7191) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 50.7171(48.7777) | Bit/dim 4.2679(4.4074) | Xent 1.6427(1.7242) | Loss 5.0893(5.2695) | Error 0.5866(0.6141) Steps 568(560.93) | Grad Norm 2.0009(4.6375) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 50.1607(48.8192) | Bit/dim 4.2814(4.4036) | Xent 1.6632(1.7224) | Loss 5.1130(5.2648) | Error 0.5973(0.6136) Steps 574(561.33) | Grad Norm 3.9071(4.6156) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 25.9025, Epoch Time 340.4997(331.2793), Bit/dim 4.2779(best: 4.3073), Xent 1.5890, Loss 5.0724, Error 0.5657(best: 0.5716)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 50.2009(48.8607) | Bit/dim 4.2722(4.3997) | Xent 1.6199(1.7193) | Loss 5.0821(5.2593) | Error 0.5835(0.6127) Steps 568(561.53) | Grad Norm 2.6098(4.5554) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 52.5418(48.9711) | Bit/dim 4.2843(4.3962) | Xent 1.6362(1.7168) | Loss 5.1025(5.2546) | Error 0.5816(0.6117) Steps 604(562.80) | Grad Norm 4.4356(4.5518) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 48.8685(48.9680) | Bit/dim 4.2912(4.3931) | Xent 1.6436(1.7146) | Loss 5.1130(5.2504) | Error 0.5825(0.6109) Steps 586(563.50) | Grad Norm 3.9128(4.5327) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 54.2842(49.1275) | Bit/dim 4.2736(4.3895) | Xent 1.6510(1.7127) | Loss 5.0991(5.2458) | Error 0.5930(0.6103) Steps 628(565.43) | Grad Norm 4.7274(4.5385) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 50.5125(49.1691) | Bit/dim 4.3207(4.3874) | Xent 1.6224(1.7100) | Loss 5.1319(5.2424) | Error 0.5740(0.6092) Steps 610(566.77) | Grad Norm 4.2716(4.5305) | Total Time 14.00(14.00)\n",
      "Iter 0738 | Time 51.8434(49.2493) | Bit/dim 4.3964(4.3877) | Xent 1.6542(1.7083) | Loss 5.2235(5.2419) | Error 0.5979(0.6089) Steps 592(567.52) | Grad Norm 5.9976(4.5745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 25.3044, Epoch Time 350.0791(331.8433), Bit/dim 4.3767(best: 4.2779), Xent 1.6354, Loss 5.1944, Error 0.5761(best: 0.5657)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 48.0358(49.2129) | Bit/dim 4.3705(4.3872) | Xent 1.6580(1.7068) | Loss 5.1995(5.2406) | Error 0.5837(0.6081) Steps 568(567.54) | Grad Norm 7.3053(4.6564) | Total Time 14.00(14.00)\n",
      "Iter 0784 | Time 54.3042(49.9664) | Bit/dim 4.3019(4.3951) | Xent 1.6752(1.7132) | Loss 5.1395(5.2517) | Error 0.5968(0.6121) Steps 592(580.18) | Grad Norm 1.9484(5.0760) | Total Time 14.00(14.00)\n",
      "Iter 0785 | Time 52.2879(50.0361) | Bit/dim 4.3037(4.3924) | Xent 1.6690(1.7118) | Loss 5.1382(5.2483) | Error 0.5991(0.6117) Steps 610(581.08) | Grad Norm 2.0272(4.9845) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn10_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 10 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
