{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run2/epoch_160_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run2', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 8810 | Time 21.2784(21.8790) | Bit/dim 3.6242(3.6386) | Xent 0.3276(0.4123) | Loss 8.8361(9.5558) | Error 0.1067(0.1446) Steps 868(850.80) | Grad Norm 4.4357(8.3647) | Total Time 0.00(0.00)\n",
      "Iter 8820 | Time 21.9316(21.8966) | Bit/dim 3.5826(3.6340) | Xent 0.3594(0.3956) | Loss 8.7195(9.3407) | Error 0.1256(0.1390) Steps 844(852.63) | Grad Norm 3.2189(7.0419) | Total Time 0.00(0.00)\n",
      "Iter 8830 | Time 21.8636(21.8722) | Bit/dim 3.6438(3.6315) | Xent 0.3400(0.3844) | Loss 8.8303(9.1849) | Error 0.1189(0.1349) Steps 844(852.71) | Grad Norm 2.4436(6.0662) | Total Time 0.00(0.00)\n",
      "Iter 8840 | Time 23.0409(21.9362) | Bit/dim 3.6083(3.6300) | Xent 0.3641(0.3757) | Loss 8.5670(9.0772) | Error 0.1233(0.1323) Steps 868(855.03) | Grad Norm 3.2831(5.3570) | Total Time 0.00(0.00)\n",
      "Iter 8850 | Time 21.7792(21.9756) | Bit/dim 3.6387(3.6280) | Xent 0.3818(0.3700) | Loss 8.8202(8.9942) | Error 0.1422(0.1306) Steps 844(858.20) | Grad Norm 2.7264(4.8469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 105.9261, Epoch Time 1352.6585(1175.7703), Bit/dim 3.6274(best: inf), Xent 0.6142, Loss 3.9345, Error 0.2048(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 21.0360(21.9320) | Bit/dim 3.5852(3.6233) | Xent 0.3489(0.3620) | Loss 8.6198(9.6071) | Error 0.1200(0.1279) Steps 850(856.80) | Grad Norm 3.7271(4.4754) | Total Time 0.00(0.00)\n",
      "Iter 8870 | Time 21.2603(21.9205) | Bit/dim 3.6431(3.6210) | Xent 0.3686(0.3572) | Loss 8.9066(9.3798) | Error 0.1322(0.1261) Steps 856(858.79) | Grad Norm 3.5547(4.1963) | Total Time 0.00(0.00)\n",
      "Iter 8880 | Time 22.5011(21.9493) | Bit/dim 3.6610(3.6233) | Xent 0.3205(0.3518) | Loss 8.7824(9.2270) | Error 0.1056(0.1239) Steps 850(858.48) | Grad Norm 4.6918(3.9953) | Total Time 0.00(0.00)\n",
      "Iter 8890 | Time 21.8647(21.8842) | Bit/dim 3.6223(3.6221) | Xent 0.3313(0.3474) | Loss 8.7174(9.0899) | Error 0.1078(0.1215) Steps 862(856.47) | Grad Norm 3.8683(3.8814) | Total Time 0.00(0.00)\n",
      "Iter 8900 | Time 21.6513(21.9756) | Bit/dim 3.6222(3.6220) | Xent 0.3427(0.3480) | Loss 8.7261(9.0006) | Error 0.1267(0.1220) Steps 850(856.64) | Grad Norm 3.0354(3.7565) | Total Time 0.00(0.00)\n",
      "Iter 8910 | Time 21.8018(21.9971) | Bit/dim 3.6099(3.6197) | Xent 0.3117(0.3441) | Loss 8.7950(8.9282) | Error 0.1056(0.1203) Steps 844(858.60) | Grad Norm 3.1978(3.7461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 99.5472, Epoch Time 1326.9652(1180.3061), Bit/dim 3.6226(best: 3.6274), Xent 0.6204, Loss 3.9328, Error 0.2021(best: 0.2048)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 22.3485(21.9218) | Bit/dim 3.6228(3.6187) | Xent 0.3455(0.3452) | Loss 8.7433(9.4789) | Error 0.1178(0.1201) Steps 838(858.41) | Grad Norm 2.9208(3.6249) | Total Time 0.00(0.00)\n",
      "Iter 8930 | Time 21.6284(21.8651) | Bit/dim 3.6505(3.6206) | Xent 0.3575(0.3435) | Loss 8.8815(9.2909) | Error 0.1233(0.1200) Steps 832(857.94) | Grad Norm 3.1810(3.5161) | Total Time 0.00(0.00)\n",
      "Iter 8940 | Time 22.0412(21.8955) | Bit/dim 3.5957(3.6201) | Xent 0.3720(0.3403) | Loss 8.7236(9.1453) | Error 0.1300(0.1195) Steps 862(858.78) | Grad Norm 4.1975(3.5600) | Total Time 0.00(0.00)\n",
      "Iter 8950 | Time 21.2589(21.8564) | Bit/dim 3.6037(3.6173) | Xent 0.3032(0.3356) | Loss 8.7494(9.0315) | Error 0.1022(0.1176) Steps 874(858.11) | Grad Norm 2.2673(3.5627) | Total Time 0.00(0.00)\n",
      "Iter 8960 | Time 21.6239(21.9028) | Bit/dim 3.6309(3.6192) | Xent 0.3330(0.3351) | Loss 8.7375(8.9650) | Error 0.1178(0.1174) Steps 868(859.35) | Grad Norm 3.8720(3.6543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 99.9192, Epoch Time 1318.1712(1184.4421), Bit/dim 3.6236(best: 3.6226), Xent 0.6269, Loss 3.9371, Error 0.2038(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 22.7682(21.9659) | Bit/dim 3.6509(3.6213) | Xent 0.3876(0.3394) | Loss 8.9000(9.6225) | Error 0.1344(0.1185) Steps 874(863.16) | Grad Norm 4.7366(3.6466) | Total Time 0.00(0.00)\n",
      "Iter 8980 | Time 21.4761(21.9747) | Bit/dim 3.5632(3.6178) | Xent 0.3316(0.3365) | Loss 8.6101(9.3916) | Error 0.1233(0.1170) Steps 862(860.21) | Grad Norm 2.8533(3.6165) | Total Time 0.00(0.00)\n",
      "Iter 8990 | Time 21.4201(21.9702) | Bit/dim 3.6326(3.6191) | Xent 0.3075(0.3379) | Loss 8.7283(9.2233) | Error 0.1078(0.1179) Steps 862(863.42) | Grad Norm 5.4031(3.6757) | Total Time 0.00(0.00)\n",
      "Iter 9000 | Time 23.0054(22.0526) | Bit/dim 3.5915(3.6184) | Xent 0.3490(0.3384) | Loss 8.6772(9.0991) | Error 0.1233(0.1179) Steps 844(862.66) | Grad Norm 3.9330(3.6988) | Total Time 0.00(0.00)\n",
      "Iter 9010 | Time 21.5181(21.9762) | Bit/dim 3.6523(3.6214) | Xent 0.3401(0.3378) | Loss 8.7927(9.0043) | Error 0.1267(0.1176) Steps 850(862.57) | Grad Norm 2.9074(3.6894) | Total Time 0.00(0.00)\n",
      "Iter 9020 | Time 21.2553(21.9726) | Bit/dim 3.6372(3.6221) | Xent 0.3075(0.3334) | Loss 8.7539(8.9363) | Error 0.1067(0.1167) Steps 862(860.77) | Grad Norm 2.7284(3.6936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 99.1282, Epoch Time 1330.4153(1188.8213), Bit/dim 3.6239(best: 3.6226), Xent 0.6318, Loss 3.9398, Error 0.2039(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 21.2378(21.9497) | Bit/dim 3.6332(3.6245) | Xent 0.3119(0.3314) | Loss 8.7799(9.4971) | Error 0.1178(0.1165) Steps 844(858.42) | Grad Norm 4.7481(3.7046) | Total Time 0.00(0.00)\n",
      "Iter 9040 | Time 22.4080(22.0265) | Bit/dim 3.6296(3.6231) | Xent 0.2948(0.3306) | Loss 8.7861(9.3071) | Error 0.0944(0.1157) Steps 868(858.79) | Grad Norm 4.0402(3.7137) | Total Time 0.00(0.00)\n",
      "Iter 9050 | Time 22.9465(22.1246) | Bit/dim 3.6604(3.6215) | Xent 0.3596(0.3311) | Loss 8.9487(9.1524) | Error 0.1178(0.1157) Steps 832(856.47) | Grad Norm 2.9657(3.5700) | Total Time 0.00(0.00)\n",
      "Iter 9060 | Time 21.5435(22.1134) | Bit/dim 3.5968(3.6199) | Xent 0.3131(0.3276) | Loss 8.6978(9.0338) | Error 0.1056(0.1144) Steps 844(855.51) | Grad Norm 2.7104(3.4084) | Total Time 0.00(0.00)\n",
      "Iter 9070 | Time 23.1685(22.1586) | Bit/dim 3.6371(3.6188) | Xent 0.3973(0.3330) | Loss 8.8851(8.9513) | Error 0.1367(0.1162) Steps 850(855.75) | Grad Norm 4.6864(3.5543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 99.6949, Epoch Time 1338.3697(1193.3077), Bit/dim 3.6242(best: 3.6226), Xent 0.6317, Loss 3.9401, Error 0.2048(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 22.8376(22.1670) | Bit/dim 3.6166(3.6177) | Xent 0.3498(0.3347) | Loss 8.7263(9.5965) | Error 0.1244(0.1175) Steps 862(857.71) | Grad Norm 3.6798(3.8624) | Total Time 0.00(0.00)\n",
      "Iter 9090 | Time 22.8467(22.1052) | Bit/dim 3.5758(3.6179) | Xent 0.3344(0.3305) | Loss 8.7399(9.3750) | Error 0.1133(0.1164) Steps 880(860.86) | Grad Norm 3.2741(3.7604) | Total Time 0.00(0.00)\n",
      "Iter 9100 | Time 21.3264(22.1376) | Bit/dim 3.6118(3.6177) | Xent 0.3494(0.3313) | Loss 8.8587(9.2103) | Error 0.1222(0.1174) Steps 832(856.74) | Grad Norm 4.7470(3.8275) | Total Time 0.00(0.00)\n",
      "Iter 9110 | Time 20.2079(22.0187) | Bit/dim 3.6120(3.6183) | Xent 0.3258(0.3328) | Loss 8.7469(9.0932) | Error 0.1156(0.1184) Steps 832(857.85) | Grad Norm 3.8489(3.9327) | Total Time 0.00(0.00)\n",
      "Iter 9120 | Time 22.3214(21.9781) | Bit/dim 3.6029(3.6199) | Xent 0.2830(0.3316) | Loss 8.5694(8.9946) | Error 0.1022(0.1176) Steps 826(858.50) | Grad Norm 3.6157(3.8836) | Total Time 0.00(0.00)\n",
      "Iter 9130 | Time 21.5968(21.9914) | Bit/dim 3.6087(3.6202) | Xent 0.3298(0.3296) | Loss 8.5971(8.9204) | Error 0.1256(0.1175) Steps 856(860.02) | Grad Norm 4.5888(4.0396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 99.4634, Epoch Time 1326.2639(1197.2964), Bit/dim 3.6229(best: 3.6226), Xent 0.6378, Loss 3.9418, Error 0.2071(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 22.6188(21.9003) | Bit/dim 3.5737(3.6189) | Xent 0.3375(0.3293) | Loss 8.6169(9.4742) | Error 0.1356(0.1176) Steps 886(859.88) | Grad Norm 6.2511(4.0824) | Total Time 0.00(0.00)\n",
      "Iter 9150 | Time 22.0323(21.8981) | Bit/dim 3.6521(3.6190) | Xent 0.3200(0.3288) | Loss 8.7613(9.2784) | Error 0.1067(0.1170) Steps 850(859.58) | Grad Norm 3.2809(3.9750) | Total Time 0.00(0.00)\n",
      "Iter 9160 | Time 21.7619(21.8861) | Bit/dim 3.5949(3.6196) | Xent 0.3581(0.3281) | Loss 8.5683(9.1307) | Error 0.1189(0.1161) Steps 844(857.37) | Grad Norm 4.8023(3.9023) | Total Time 0.00(0.00)\n",
      "Iter 9170 | Time 21.5807(21.8898) | Bit/dim 3.6044(3.6184) | Xent 0.3328(0.3299) | Loss 8.6153(9.0255) | Error 0.1211(0.1168) Steps 862(855.19) | Grad Norm 2.9935(3.8606) | Total Time 0.00(0.00)\n",
      "Iter 9180 | Time 23.7827(21.8944) | Bit/dim 3.5889(3.6188) | Xent 0.3150(0.3304) | Loss 8.5496(8.9491) | Error 0.1111(0.1169) Steps 820(856.00) | Grad Norm 4.0754(3.9482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 99.6515, Epoch Time 1318.2302(1200.9244), Bit/dim 3.6192(best: 3.6226), Xent 0.6374, Loss 3.9379, Error 0.2088(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 22.2385(21.9059) | Bit/dim 3.6596(3.6184) | Xent 0.3007(0.3289) | Loss 8.8858(9.5923) | Error 0.1089(0.1155) Steps 868(854.35) | Grad Norm 3.2823(3.9433) | Total Time 0.00(0.00)\n",
      "Iter 9200 | Time 21.7634(21.9035) | Bit/dim 3.6331(3.6192) | Xent 0.2936(0.3239) | Loss 8.6382(9.3657) | Error 0.0967(0.1141) Steps 838(856.46) | Grad Norm 3.3323(3.8578) | Total Time 0.00(0.00)\n",
      "Iter 9210 | Time 22.0407(21.8570) | Bit/dim 3.5510(3.6185) | Xent 0.3680(0.3240) | Loss 8.7548(9.1998) | Error 0.1278(0.1135) Steps 880(856.69) | Grad Norm 2.9084(3.8291) | Total Time 0.00(0.00)\n",
      "Iter 9220 | Time 22.0201(21.9046) | Bit/dim 3.6563(3.6181) | Xent 0.3207(0.3269) | Loss 8.7940(9.0820) | Error 0.1144(0.1146) Steps 838(858.46) | Grad Norm 4.6079(3.9922) | Total Time 0.00(0.00)\n",
      "Iter 9230 | Time 22.8247(21.9585) | Bit/dim 3.6117(3.6173) | Xent 0.3886(0.3355) | Loss 8.8144(8.9951) | Error 0.1378(0.1179) Steps 892(860.47) | Grad Norm 5.7761(3.9794) | Total Time 0.00(0.00)\n",
      "Iter 9240 | Time 22.4763(22.0516) | Bit/dim 3.5994(3.6175) | Xent 0.3029(0.3333) | Loss 8.6132(8.9245) | Error 0.1211(0.1179) Steps 874(862.78) | Grad Norm 4.2010(3.9280) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 99.1376, Epoch Time 1327.7182(1204.7282), Bit/dim 3.6253(best: 3.6192), Xent 0.6352, Loss 3.9429, Error 0.2049(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 21.6826(22.0090) | Bit/dim 3.5872(3.6173) | Xent 0.3285(0.3296) | Loss 8.7175(9.4614) | Error 0.1122(0.1163) Steps 874(864.45) | Grad Norm 3.7152(3.9210) | Total Time 0.00(0.00)\n",
      "Iter 9260 | Time 21.6578(22.0527) | Bit/dim 3.6172(3.6168) | Xent 0.3393(0.3296) | Loss 8.8599(9.2781) | Error 0.1244(0.1168) Steps 850(863.71) | Grad Norm 4.9241(3.8759) | Total Time 0.00(0.00)\n",
      "Iter 9270 | Time 22.4654(22.1766) | Bit/dim 3.5909(3.6172) | Xent 0.3553(0.3281) | Loss 8.6995(9.1444) | Error 0.1167(0.1154) Steps 880(864.64) | Grad Norm 4.3443(3.8724) | Total Time 0.00(0.00)\n",
      "Iter 9280 | Time 22.1035(22.1902) | Bit/dim 3.6370(3.6207) | Xent 0.3314(0.3282) | Loss 8.5926(9.0267) | Error 0.1200(0.1155) Steps 808(858.55) | Grad Norm 3.6334(3.8396) | Total Time 0.00(0.00)\n",
      "Iter 9290 | Time 21.3975(22.1455) | Bit/dim 3.6262(3.6164) | Xent 0.3038(0.3279) | Loss 8.6881(8.9462) | Error 0.1133(0.1159) Steps 862(862.97) | Grad Norm 2.7272(3.6963) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 100.6764, Epoch Time 1336.3881(1208.6780), Bit/dim 3.6219(best: 3.6192), Xent 0.6355, Loss 3.9397, Error 0.2018(best: 0.2021)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 22.0596(22.1080) | Bit/dim 3.6202(3.6196) | Xent 0.3192(0.3286) | Loss 8.7318(9.5997) | Error 0.1078(0.1165) Steps 892(866.14) | Grad Norm 2.7764(3.6551) | Total Time 0.00(0.00)\n",
      "Iter 9310 | Time 22.4248(22.0532) | Bit/dim 3.6234(3.6211) | Xent 0.3382(0.3268) | Loss 8.6944(9.3725) | Error 0.1289(0.1161) Steps 904(866.16) | Grad Norm 3.4797(3.6123) | Total Time 0.00(0.00)\n",
      "Iter 9320 | Time 22.5241(22.0475) | Bit/dim 3.5587(3.6185) | Xent 0.3162(0.3279) | Loss 8.7216(9.2045) | Error 0.1200(0.1170) Steps 868(865.05) | Grad Norm 4.3514(3.7144) | Total Time 0.00(0.00)\n",
      "Iter 9330 | Time 22.2492(22.0059) | Bit/dim 3.6494(3.6215) | Xent 0.3055(0.3249) | Loss 8.7378(9.0817) | Error 0.1100(0.1164) Steps 886(863.63) | Grad Norm 4.8704(3.8669) | Total Time 0.00(0.00)\n",
      "Iter 9340 | Time 22.8961(22.0240) | Bit/dim 3.5713(3.6174) | Xent 0.3842(0.3284) | Loss 8.8907(8.9970) | Error 0.1256(0.1160) Steps 874(863.15) | Grad Norm 5.0479(4.1577) | Total Time 0.00(0.00)\n",
      "Iter 9350 | Time 22.4837(22.0282) | Bit/dim 3.6660(3.6164) | Xent 0.3399(0.3297) | Loss 8.8481(8.9248) | Error 0.1144(0.1162) Steps 838(860.61) | Grad Norm 3.3474(3.9572) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 99.7628, Epoch Time 1327.2049(1212.2338), Bit/dim 3.6197(best: 3.6192), Xent 0.6432, Loss 3.9413, Error 0.2071(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 21.0591(21.9293) | Bit/dim 3.5942(3.6169) | Xent 0.3306(0.3275) | Loss 8.6541(9.4773) | Error 0.1156(0.1159) Steps 850(859.48) | Grad Norm 2.9599(3.9867) | Total Time 0.00(0.00)\n",
      "Iter 9370 | Time 21.7260(22.0381) | Bit/dim 3.5946(3.6158) | Xent 0.3199(0.3292) | Loss 8.6683(9.2804) | Error 0.1078(0.1156) Steps 844(859.92) | Grad Norm 3.0651(3.7956) | Total Time 0.00(0.00)\n",
      "Iter 9380 | Time 22.8054(22.0984) | Bit/dim 3.5924(3.6151) | Xent 0.3204(0.3294) | Loss 8.6957(9.1428) | Error 0.1078(0.1156) Steps 880(862.32) | Grad Norm 3.3376(3.8572) | Total Time 0.00(0.00)\n",
      "Iter 9390 | Time 22.6637(22.0371) | Bit/dim 3.6162(3.6161) | Xent 0.3311(0.3282) | Loss 8.8032(9.0336) | Error 0.1067(0.1144) Steps 910(862.75) | Grad Norm 5.4222(3.9235) | Total Time 0.00(0.00)\n",
      "Iter 9400 | Time 22.3879(22.0520) | Bit/dim 3.6277(3.6178) | Xent 0.3260(0.3253) | Loss 8.6282(8.9501) | Error 0.1089(0.1137) Steps 856(860.89) | Grad Norm 2.8991(3.9033) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 99.9297, Epoch Time 1329.7169(1215.7583), Bit/dim 3.6253(best: 3.6192), Xent 0.6402, Loss 3.9454, Error 0.2052(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 22.6830(22.0300) | Bit/dim 3.6114(3.6157) | Xent 0.3200(0.3249) | Loss 8.7817(9.5882) | Error 0.1133(0.1131) Steps 832(859.27) | Grad Norm 3.2120(4.0508) | Total Time 0.00(0.00)\n",
      "Iter 9420 | Time 22.9881(22.0427) | Bit/dim 3.6157(3.6168) | Xent 0.3299(0.3261) | Loss 8.7807(9.3652) | Error 0.1100(0.1144) Steps 886(861.29) | Grad Norm 4.2947(4.1173) | Total Time 0.00(0.00)\n",
      "Iter 9430 | Time 22.8145(22.0466) | Bit/dim 3.6482(3.6148) | Xent 0.3375(0.3254) | Loss 8.9303(9.1974) | Error 0.1233(0.1150) Steps 850(862.06) | Grad Norm 3.4978(3.9868) | Total Time 0.00(0.00)\n",
      "Iter 9440 | Time 21.6800(22.1228) | Bit/dim 3.6236(3.6170) | Xent 0.3519(0.3269) | Loss 8.6400(9.0858) | Error 0.1256(0.1163) Steps 844(863.27) | Grad Norm 5.7351(4.1889) | Total Time 0.00(0.00)\n",
      "Iter 9450 | Time 22.0854(22.1007) | Bit/dim 3.6167(3.6175) | Xent 0.2890(0.3227) | Loss 8.6419(8.9985) | Error 0.1078(0.1146) Steps 856(864.11) | Grad Norm 5.6955(4.3652) | Total Time 0.00(0.00)\n",
      "Iter 9460 | Time 22.4589(22.1198) | Bit/dim 3.6350(3.6183) | Xent 0.3275(0.3209) | Loss 8.7326(8.9135) | Error 0.1122(0.1145) Steps 814(863.61) | Grad Norm 3.7498(4.2837) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 99.6916, Epoch Time 1334.7119(1219.3269), Bit/dim 3.6244(best: 3.6192), Xent 0.6467, Loss 3.9478, Error 0.2053(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 21.6909(21.9835) | Bit/dim 3.6195(3.6196) | Xent 0.3628(0.3211) | Loss 8.8579(9.4832) | Error 0.1222(0.1144) Steps 862(864.99) | Grad Norm 4.8881(4.1572) | Total Time 0.00(0.00)\n",
      "Iter 9480 | Time 22.2569(22.0176) | Bit/dim 3.5982(3.6189) | Xent 0.3002(0.3187) | Loss 8.6437(9.2755) | Error 0.0922(0.1126) Steps 820(861.86) | Grad Norm 3.3563(4.1436) | Total Time 0.00(0.00)\n",
      "Iter 9490 | Time 20.8908(21.9515) | Bit/dim 3.6108(3.6189) | Xent 0.3031(0.3201) | Loss 8.4888(9.1287) | Error 0.1011(0.1122) Steps 868(864.01) | Grad Norm 3.7472(4.1171) | Total Time 0.00(0.00)\n",
      "Iter 9500 | Time 22.2778(22.0071) | Bit/dim 3.6176(3.6197) | Xent 0.3531(0.3212) | Loss 8.7108(9.0197) | Error 0.1289(0.1128) Steps 850(862.18) | Grad Norm 4.8799(4.0690) | Total Time 0.00(0.00)\n",
      "Iter 9510 | Time 21.3380(22.0041) | Bit/dim 3.6062(3.6158) | Xent 0.2993(0.3195) | Loss 8.6557(8.9359) | Error 0.1078(0.1129) Steps 856(861.84) | Grad Norm 3.0965(4.1076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 100.1397, Epoch Time 1325.4196(1222.5097), Bit/dim 3.6246(best: 3.6192), Xent 0.6455, Loss 3.9473, Error 0.2083(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 21.8096(22.0719) | Bit/dim 3.5933(3.6153) | Xent 0.3436(0.3210) | Loss 8.7631(9.5760) | Error 0.1256(0.1134) Steps 844(863.15) | Grad Norm 4.5160(4.0541) | Total Time 0.00(0.00)\n",
      "Iter 9530 | Time 22.4314(22.1344) | Bit/dim 3.6491(3.6179) | Xent 0.3322(0.3236) | Loss 8.8534(9.3582) | Error 0.1200(0.1146) Steps 874(863.38) | Grad Norm 3.1615(4.0898) | Total Time 0.00(0.00)\n",
      "Iter 9540 | Time 22.1343(22.1424) | Bit/dim 3.6110(3.6174) | Xent 0.3600(0.3231) | Loss 8.8056(9.1886) | Error 0.1311(0.1153) Steps 850(861.36) | Grad Norm 4.5352(4.0588) | Total Time 0.00(0.00)\n",
      "Iter 9550 | Time 22.0404(22.1149) | Bit/dim 3.5922(3.6174) | Xent 0.3057(0.3222) | Loss 8.5420(9.0732) | Error 0.1067(0.1146) Steps 874(864.63) | Grad Norm 3.2298(4.1089) | Total Time 0.00(0.00)\n",
      "Iter 9560 | Time 22.0571(22.1233) | Bit/dim 3.6553(3.6178) | Xent 0.3226(0.3241) | Loss 8.7193(8.9745) | Error 0.1078(0.1142) Steps 880(865.81) | Grad Norm 8.4171(4.4362) | Total Time 0.00(0.00)\n",
      "Iter 9570 | Time 21.9640(22.1032) | Bit/dim 3.5804(3.6171) | Xent 0.3296(0.3216) | Loss 8.7071(8.9047) | Error 0.1067(0.1124) Steps 868(865.68) | Grad Norm 4.5541(4.4117) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 99.7645, Epoch Time 1336.8197(1225.9390), Bit/dim 3.6237(best: 3.6192), Xent 0.6463, Loss 3.9469, Error 0.2064(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9580 | Time 22.2590(22.1154) | Bit/dim 3.6371(3.6181) | Xent 0.2975(0.3221) | Loss 8.7963(9.4441) | Error 0.1011(0.1132) Steps 838(864.70) | Grad Norm 4.7322(4.4657) | Total Time 0.00(0.00)\n",
      "Iter 9590 | Time 22.5992(22.1333) | Bit/dim 3.6137(3.6179) | Xent 0.3126(0.3223) | Loss 8.6408(9.2494) | Error 0.1044(0.1125) Steps 856(863.96) | Grad Norm 3.4797(4.4554) | Total Time 0.00(0.00)\n",
      "Iter 9600 | Time 22.4944(22.0985) | Bit/dim 3.5883(3.6181) | Xent 0.3344(0.3229) | Loss 8.7972(9.1154) | Error 0.1211(0.1132) Steps 898(866.46) | Grad Norm 3.7437(4.3677) | Total Time 0.00(0.00)\n",
      "Iter 9610 | Time 22.9678(22.1790) | Bit/dim 3.5952(3.6202) | Xent 0.3107(0.3238) | Loss 8.6365(9.0244) | Error 0.1044(0.1131) Steps 880(869.21) | Grad Norm 4.0500(4.2557) | Total Time 0.00(0.00)\n",
      "Iter 9620 | Time 22.0172(22.1505) | Bit/dim 3.6027(3.6175) | Xent 0.2999(0.3234) | Loss 8.5661(8.9437) | Error 0.1011(0.1128) Steps 862(868.85) | Grad Norm 2.8794(4.1611) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 98.6752, Epoch Time 1336.9000(1229.2678), Bit/dim 3.6271(best: 3.6192), Xent 0.6497, Loss 3.9520, Error 0.2033(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9630 | Time 21.7740(22.1830) | Bit/dim 3.6405(3.6210) | Xent 0.3274(0.3225) | Loss 8.8158(9.5766) | Error 0.1100(0.1124) Steps 874(864.52) | Grad Norm 3.2678(4.2429) | Total Time 0.00(0.00)\n",
      "Iter 9640 | Time 21.2894(22.1207) | Bit/dim 3.6110(3.6224) | Xent 0.2922(0.3179) | Loss 8.7507(9.3555) | Error 0.1156(0.1116) Steps 856(863.51) | Grad Norm 5.2276(4.1469) | Total Time 0.00(0.00)\n",
      "Iter 9650 | Time 22.2521(22.1743) | Bit/dim 3.6499(3.6198) | Xent 0.3360(0.3190) | Loss 8.8719(9.1989) | Error 0.1089(0.1120) Steps 892(868.15) | Grad Norm 5.5473(4.2374) | Total Time 0.00(0.00)\n",
      "Iter 9660 | Time 23.1575(22.1145) | Bit/dim 3.6420(3.6174) | Xent 0.2735(0.3167) | Loss 8.7882(9.0718) | Error 0.0956(0.1109) Steps 844(864.84) | Grad Norm 4.3614(4.3537) | Total Time 0.00(0.00)\n",
      "Iter 9670 | Time 22.1363(22.1045) | Bit/dim 3.6160(3.6194) | Xent 0.3092(0.3198) | Loss 8.6953(8.9884) | Error 0.1089(0.1132) Steps 802(862.15) | Grad Norm 4.6739(4.6688) | Total Time 0.00(0.00)\n",
      "Iter 9680 | Time 22.7375(22.1576) | Bit/dim 3.5954(3.6163) | Xent 0.3028(0.3176) | Loss 8.7286(8.9162) | Error 0.1100(0.1114) Steps 880(861.81) | Grad Norm 5.0515(4.8552) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 98.9249, Epoch Time 1333.5006(1232.3948), Bit/dim 3.6198(best: 3.6192), Xent 0.6446, Loss 3.9421, Error 0.2056(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9690 | Time 21.4607(22.1421) | Bit/dim 3.6076(3.6154) | Xent 0.3341(0.3203) | Loss 8.7572(9.4622) | Error 0.1167(0.1124) Steps 868(861.78) | Grad Norm 7.5398(5.0584) | Total Time 0.00(0.00)\n",
      "Iter 9700 | Time 22.8192(22.2119) | Bit/dim 3.6570(3.6171) | Xent 0.3134(0.3217) | Loss 8.9020(9.2821) | Error 0.1133(0.1139) Steps 880(864.79) | Grad Norm 5.1188(5.0335) | Total Time 0.00(0.00)\n",
      "Iter 9710 | Time 22.2855(22.2242) | Bit/dim 3.6147(3.6198) | Xent 0.3325(0.3196) | Loss 8.7195(9.1310) | Error 0.1133(0.1125) Steps 844(863.92) | Grad Norm 4.7473(4.8472) | Total Time 0.00(0.00)\n",
      "Iter 9720 | Time 22.8505(22.2146) | Bit/dim 3.6062(3.6183) | Xent 0.3050(0.3189) | Loss 8.7165(9.0193) | Error 0.1189(0.1119) Steps 862(865.67) | Grad Norm 3.5448(4.7179) | Total Time 0.00(0.00)\n",
      "Iter 9730 | Time 21.9290(22.2529) | Bit/dim 3.6378(3.6175) | Xent 0.3430(0.3198) | Loss 8.9090(8.9356) | Error 0.1333(0.1127) Steps 868(862.74) | Grad Norm 5.1540(4.7536) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 100.1322, Epoch Time 1343.4549(1235.7266), Bit/dim 3.6211(best: 3.6192), Xent 0.6431, Loss 3.9426, Error 0.2069(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9740 | Time 22.2615(22.3146) | Bit/dim 3.6202(3.6185) | Xent 0.3061(0.3207) | Loss 8.6726(9.5700) | Error 0.0956(0.1120) Steps 886(869.43) | Grad Norm 3.7890(4.6243) | Total Time 0.00(0.00)\n",
      "Iter 9750 | Time 21.6863(22.2348) | Bit/dim 3.5822(3.6184) | Xent 0.2867(0.3166) | Loss 8.6351(9.3511) | Error 0.0933(0.1106) Steps 898(869.56) | Grad Norm 5.3577(4.5522) | Total Time 0.00(0.00)\n",
      "Iter 9760 | Time 22.6725(22.2177) | Bit/dim 3.6331(3.6203) | Xent 0.3381(0.3171) | Loss 8.8342(9.1904) | Error 0.1200(0.1110) Steps 862(865.64) | Grad Norm 9.0523(4.8558) | Total Time 0.00(0.00)\n",
      "Iter 9770 | Time 22.2703(22.2253) | Bit/dim 3.6045(3.6172) | Xent 0.3519(0.3174) | Loss 8.7413(9.0555) | Error 0.1156(0.1106) Steps 838(861.75) | Grad Norm 4.9494(4.8790) | Total Time 0.00(0.00)\n",
      "Iter 9780 | Time 21.3591(22.2233) | Bit/dim 3.6491(3.6193) | Xent 0.3015(0.3194) | Loss 8.6390(8.9767) | Error 0.0967(0.1113) Steps 820(861.56) | Grad Norm 3.4334(4.8579) | Total Time 0.00(0.00)\n",
      "Iter 9790 | Time 22.7132(22.2622) | Bit/dim 3.5860(3.6196) | Xent 0.3158(0.3181) | Loss 8.6863(8.9139) | Error 0.1078(0.1106) Steps 904(861.81) | Grad Norm 5.0363(4.7924) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 100.4201, Epoch Time 1341.7211(1238.9065), Bit/dim 3.6222(best: 3.6192), Xent 0.6444, Loss 3.9444, Error 0.2039(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9800 | Time 22.5155(22.2498) | Bit/dim 3.6203(3.6187) | Xent 0.3335(0.3165) | Loss 8.8770(9.4719) | Error 0.1133(0.1104) Steps 886(863.26) | Grad Norm 4.6301(4.8014) | Total Time 0.00(0.00)\n",
      "Iter 9810 | Time 22.4661(22.2433) | Bit/dim 3.6151(3.6184) | Xent 0.3570(0.3173) | Loss 8.7165(9.2645) | Error 0.1189(0.1107) Steps 886(865.26) | Grad Norm 4.6355(4.5370) | Total Time 0.00(0.00)\n",
      "Iter 9820 | Time 23.0047(22.3603) | Bit/dim 3.6254(3.6186) | Xent 0.3461(0.3184) | Loss 8.7996(9.1298) | Error 0.1211(0.1106) Steps 880(865.88) | Grad Norm 4.6285(4.4621) | Total Time 0.00(0.00)\n",
      "Iter 9830 | Time 23.8254(22.3562) | Bit/dim 3.6270(3.6173) | Xent 0.3440(0.3207) | Loss 8.7843(9.0172) | Error 0.1189(0.1114) Steps 874(865.26) | Grad Norm 5.2927(4.4626) | Total Time 0.00(0.00)\n",
      "Iter 9840 | Time 22.9082(22.3649) | Bit/dim 3.6010(3.6195) | Xent 0.3060(0.3195) | Loss 8.5584(8.9393) | Error 0.0978(0.1118) Steps 904(863.76) | Grad Norm 5.4681(4.5407) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 100.3042, Epoch Time 1349.8784(1242.2356), Bit/dim 3.6257(best: 3.6192), Xent 0.6537, Loss 3.9525, Error 0.2060(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9850 | Time 22.6348(22.4164) | Bit/dim 3.6182(3.6204) | Xent 0.3029(0.3185) | Loss 8.7358(9.5772) | Error 0.1022(0.1112) Steps 874(863.04) | Grad Norm 5.5326(4.6298) | Total Time 0.00(0.00)\n",
      "Iter 9860 | Time 23.0887(22.5294) | Bit/dim 3.6246(3.6217) | Xent 0.3022(0.3176) | Loss 8.6048(9.3606) | Error 0.1178(0.1111) Steps 868(865.80) | Grad Norm 5.5110(4.7526) | Total Time 0.00(0.00)\n",
      "Iter 9870 | Time 22.7340(22.5473) | Bit/dim 3.5793(3.6207) | Xent 0.3453(0.3173) | Loss 8.7342(9.1872) | Error 0.1289(0.1115) Steps 874(865.40) | Grad Norm 3.4477(4.7514) | Total Time 0.00(0.00)\n",
      "Iter 9880 | Time 23.0099(22.5904) | Bit/dim 3.6264(3.6193) | Xent 0.3027(0.3187) | Loss 8.8234(9.0699) | Error 0.1122(0.1118) Steps 856(865.85) | Grad Norm 5.6279(4.6054) | Total Time 0.00(0.00)\n",
      "Iter 9890 | Time 22.0516(22.6083) | Bit/dim 3.6035(3.6181) | Xent 0.3231(0.3183) | Loss 8.6670(8.9901) | Error 0.1189(0.1124) Steps 850(867.11) | Grad Norm 5.3019(4.5622) | Total Time 0.00(0.00)\n",
      "Iter 9900 | Time 23.0442(22.6320) | Bit/dim 3.6139(3.6200) | Xent 0.3425(0.3137) | Loss 8.7030(8.9170) | Error 0.1211(0.1119) Steps 856(866.29) | Grad Norm 4.1343(4.3487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 100.6594, Epoch Time 1365.8557(1245.9442), Bit/dim 3.6260(best: 3.6192), Xent 0.6497, Loss 3.9509, Error 0.2082(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9910 | Time 21.8883(22.7069) | Bit/dim 3.6133(3.6195) | Xent 0.3052(0.3127) | Loss 8.6904(9.4735) | Error 0.0978(0.1109) Steps 856(869.97) | Grad Norm 3.1372(4.1883) | Total Time 0.00(0.00)\n",
      "Iter 9920 | Time 22.1923(22.8219) | Bit/dim 3.6411(3.6205) | Xent 0.3056(0.3095) | Loss 8.8218(9.2811) | Error 0.1033(0.1095) Steps 904(871.66) | Grad Norm 7.4036(4.3056) | Total Time 0.00(0.00)\n",
      "Iter 9930 | Time 23.1444(22.9397) | Bit/dim 3.6442(3.6215) | Xent 0.3146(0.3114) | Loss 8.8247(9.1448) | Error 0.1100(0.1097) Steps 874(873.66) | Grad Norm 5.4048(4.4761) | Total Time 0.00(0.00)\n",
      "Iter 9940 | Time 23.2093(22.9920) | Bit/dim 3.6199(3.6196) | Xent 0.2913(0.3135) | Loss 8.6338(9.0303) | Error 0.1000(0.1102) Steps 916(877.57) | Grad Norm 3.1838(4.9374) | Total Time 0.00(0.00)\n",
      "Iter 9950 | Time 22.3389(22.9621) | Bit/dim 3.6252(3.6196) | Xent 0.2920(0.3158) | Loss 8.7290(8.9641) | Error 0.1056(0.1115) Steps 892(877.34) | Grad Norm 4.5809(4.9431) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 100.3692, Epoch Time 1388.0634(1250.2078), Bit/dim 3.6273(best: 3.6192), Xent 0.6585, Loss 3.9566, Error 0.2093(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9960 | Time 22.9953(22.9588) | Bit/dim 3.6227(3.6218) | Xent 0.2919(0.3124) | Loss 8.7575(9.6198) | Error 0.1033(0.1100) Steps 874(877.52) | Grad Norm 4.6765(4.7336) | Total Time 0.00(0.00)\n",
      "Iter 9970 | Time 23.4238(23.0766) | Bit/dim 3.6349(3.6223) | Xent 0.3095(0.3088) | Loss 8.8080(9.3857) | Error 0.1167(0.1087) Steps 910(882.55) | Grad Norm 3.8871(4.4681) | Total Time 0.00(0.00)\n",
      "Iter 9980 | Time 24.0612(23.2124) | Bit/dim 3.6571(3.6240) | Xent 0.2897(0.3087) | Loss 8.8439(9.2155) | Error 0.1078(0.1093) Steps 928(885.47) | Grad Norm 4.5499(4.5457) | Total Time 0.00(0.00)\n",
      "Iter 9990 | Time 24.0282(23.2625) | Bit/dim 3.6556(3.6236) | Xent 0.3326(0.3091) | Loss 8.8458(9.0925) | Error 0.1122(0.1084) Steps 892(885.82) | Grad Norm 4.1442(4.4069) | Total Time 0.00(0.00)\n",
      "Iter 10000 | Time 23.4613(23.3583) | Bit/dim 3.6197(3.6243) | Xent 0.3446(0.3108) | Loss 8.7267(9.0029) | Error 0.1200(0.1093) Steps 880(885.56) | Grad Norm 4.5304(4.3578) | Total Time 0.00(0.00)\n",
      "Iter 10010 | Time 24.9698(23.5058) | Bit/dim 3.6161(3.6197) | Xent 0.3339(0.3149) | Loss 8.8460(8.9253) | Error 0.1122(0.1114) Steps 910(882.79) | Grad Norm 4.0443(4.4638) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 101.5427, Epoch Time 1413.1814(1255.0970), Bit/dim 3.6301(best: 3.6192), Xent 0.6567, Loss 3.9585, Error 0.2060(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10020 | Time 23.8930(23.6386) | Bit/dim 3.6460(3.6221) | Xent 0.2657(0.3128) | Loss 8.7658(9.4883) | Error 0.0878(0.1100) Steps 922(885.52) | Grad Norm 4.4507(5.1518) | Total Time 0.00(0.00)\n",
      "Iter 10030 | Time 24.7349(23.6963) | Bit/dim 3.6297(3.6241) | Xent 0.2950(0.3124) | Loss 8.7304(9.2929) | Error 0.1011(0.1090) Steps 850(882.64) | Grad Norm 5.5366(5.3901) | Total Time 0.00(0.00)\n",
      "Iter 10040 | Time 25.1305(23.7577) | Bit/dim 3.6199(3.6254) | Xent 0.3028(0.3118) | Loss 8.6328(9.1424) | Error 0.0989(0.1091) Steps 898(884.67) | Grad Norm 3.8882(5.7828) | Total Time 0.00(0.00)\n",
      "Iter 10050 | Time 23.9213(23.8370) | Bit/dim 3.6057(3.6224) | Xent 0.3077(0.3129) | Loss 8.7142(9.0276) | Error 0.1033(0.1099) Steps 916(887.62) | Grad Norm 7.1294(6.2302) | Total Time 0.00(0.00)\n",
      "Iter 10060 | Time 23.4597(23.8655) | Bit/dim 3.6146(3.6228) | Xent 0.3113(0.3160) | Loss 8.7815(8.9544) | Error 0.1089(0.1111) Steps 934(890.18) | Grad Norm 6.2350(6.8692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 101.0771, Epoch Time 1439.8397(1260.6393), Bit/dim 3.6334(best: 3.6192), Xent 0.6558, Loss 3.9613, Error 0.2063(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10070 | Time 25.0325(24.0888) | Bit/dim 3.5796(3.6265) | Xent 0.2755(0.3197) | Loss 8.6326(9.6121) | Error 0.0933(0.1128) Steps 892(894.72) | Grad Norm 5.0757(6.5950) | Total Time 0.00(0.00)\n",
      "Iter 10080 | Time 24.8053(24.2206) | Bit/dim 3.6364(3.6275) | Xent 0.2969(0.3199) | Loss 8.8321(9.3952) | Error 0.1100(0.1132) Steps 880(896.56) | Grad Norm 3.9573(6.4637) | Total Time 0.00(0.00)\n",
      "Iter 10090 | Time 24.8979(24.2874) | Bit/dim 3.6412(3.6291) | Xent 0.3129(0.3204) | Loss 8.7903(9.2314) | Error 0.1089(0.1132) Steps 922(899.57) | Grad Norm 3.4405(6.2341) | Total Time 0.00(0.00)\n",
      "Iter 10100 | Time 24.4558(24.4542) | Bit/dim 3.6121(3.6276) | Xent 0.3037(0.3154) | Loss 8.7159(9.1087) | Error 0.1111(0.1110) Steps 910(901.44) | Grad Norm 3.7438(5.8315) | Total Time 0.00(0.00)\n",
      "Iter 10110 | Time 24.9381(24.6529) | Bit/dim 3.6281(3.6257) | Xent 0.3010(0.3155) | Loss 8.7237(9.0143) | Error 0.0978(0.1109) Steps 946(904.32) | Grad Norm 3.9887(5.4266) | Total Time 0.00(0.00)\n",
      "Iter 10120 | Time 24.1915(24.7315) | Bit/dim 3.6496(3.6276) | Xent 0.2809(0.3148) | Loss 8.8396(8.9436) | Error 0.1044(0.1108) Steps 916(906.32) | Grad Norm 3.6919(5.4866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 101.9110, Epoch Time 1486.4351(1267.4132), Bit/dim 3.6368(best: 3.6192), Xent 0.6729, Loss 3.9733, Error 0.2101(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10130 | Time 24.2686(24.8238) | Bit/dim 3.6446(3.6291) | Xent 0.3029(0.3118) | Loss 8.7253(9.4900) | Error 0.1022(0.1095) Steps 910(908.85) | Grad Norm 6.2589(5.3907) | Total Time 0.00(0.00)\n",
      "Iter 10140 | Time 24.1871(24.8609) | Bit/dim 3.6200(3.6290) | Xent 0.2860(0.3101) | Loss 8.6633(9.3136) | Error 0.0967(0.1087) Steps 886(911.89) | Grad Norm 4.3355(5.2558) | Total Time 0.00(0.00)\n",
      "Iter 10150 | Time 25.3710(24.9626) | Bit/dim 3.6318(3.6287) | Xent 0.3135(0.3124) | Loss 8.7460(9.1723) | Error 0.1044(0.1090) Steps 946(915.05) | Grad Norm 4.9870(5.1100) | Total Time 0.00(0.00)\n",
      "Iter 10160 | Time 24.8581(24.9972) | Bit/dim 3.6112(3.6286) | Xent 0.3596(0.3160) | Loss 8.6411(9.0686) | Error 0.1211(0.1111) Steps 916(915.88) | Grad Norm 5.5905(5.2669) | Total Time 0.00(0.00)\n",
      "Iter 10170 | Time 25.2575(24.8859) | Bit/dim 3.7021(3.6477) | Xent 0.4530(0.3498) | Loss 9.1203(9.0571) | Error 0.1611(0.1232) Steps 898(909.92) | Grad Norm 14.6160(15.4842) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 102.9965, Epoch Time 1494.6406(1274.2300), Bit/dim 3.7000(best: 3.6192), Xent 0.7117, Loss 4.0558, Error 0.2275(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10180 | Time 25.4273(24.9067) | Bit/dim 3.6384(3.6570) | Xent 0.3958(0.3635) | Loss 8.9317(9.7382) | Error 0.1267(0.1285) Steps 922(911.07) | Grad Norm 6.5224(14.3180) | Total Time 0.00(0.00)\n",
      "Iter 10190 | Time 25.1802(24.9646) | Bit/dim 3.6636(3.6578) | Xent 0.3938(0.3637) | Loss 8.7851(9.5077) | Error 0.1300(0.1277) Steps 922(913.90) | Grad Norm 6.5735(13.0788) | Total Time 0.00(0.00)\n",
      "Iter 10200 | Time 24.9857(24.9737) | Bit/dim 3.6544(3.6582) | Xent 0.3268(0.3576) | Loss 8.8422(9.3423) | Error 0.1211(0.1265) Steps 934(915.88) | Grad Norm 5.8875(11.0505) | Total Time 0.00(0.00)\n",
      "Iter 10210 | Time 25.4916(25.0221) | Bit/dim 3.6146(3.6532) | Xent 0.3371(0.3501) | Loss 8.7372(9.1922) | Error 0.1100(0.1247) Steps 898(916.64) | Grad Norm 3.7333(9.6198) | Total Time 0.00(0.00)\n",
      "Iter 10220 | Time 24.3882(25.0822) | Bit/dim 3.7115(3.6543) | Xent 0.3264(0.3453) | Loss 8.9057(9.0980) | Error 0.1156(0.1230) Steps 928(917.15) | Grad Norm 3.7682(8.4126) | Total Time 0.00(0.00)\n",
      "Iter 10230 | Time 24.0299(25.1033) | Bit/dim 3.6465(3.6520) | Xent 0.3646(0.3400) | Loss 8.8210(9.0133) | Error 0.1367(0.1202) Steps 904(913.14) | Grad Norm 4.0793(7.3105) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 101.5646, Epoch Time 1502.9362(1281.0912), Bit/dim 3.6477(best: 3.6192), Xent 0.6531, Loss 3.9743, Error 0.2110(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10240 | Time 25.6360(25.2014) | Bit/dim 3.5997(3.6472) | Xent 0.3666(0.3408) | Loss 8.6999(9.5475) | Error 0.1256(0.1199) Steps 934(915.35) | Grad Norm 6.0992(6.8478) | Total Time 0.00(0.00)\n",
      "Iter 10250 | Time 25.9286(25.1948) | Bit/dim 3.6479(3.6454) | Xent 0.3036(0.3379) | Loss 8.7741(9.3536) | Error 0.1100(0.1195) Steps 904(913.27) | Grad Norm 3.2132(6.2942) | Total Time 0.00(0.00)\n",
      "Iter 10260 | Time 26.2583(25.3003) | Bit/dim 3.5918(3.6422) | Xent 0.3232(0.3331) | Loss 8.6315(9.1925) | Error 0.1100(0.1169) Steps 946(916.96) | Grad Norm 2.9429(5.7394) | Total Time 0.00(0.00)\n",
      "Iter 10270 | Time 25.3213(25.3030) | Bit/dim 3.6279(3.6425) | Xent 0.3289(0.3312) | Loss 8.7482(9.0826) | Error 0.1078(0.1161) Steps 892(915.04) | Grad Norm 4.6297(5.4739) | Total Time 0.00(0.00)\n",
      "Iter 10280 | Time 25.3471(25.2517) | Bit/dim 3.6629(3.6411) | Xent 0.3154(0.3280) | Loss 8.8024(8.9969) | Error 0.1222(0.1165) Steps 928(915.68) | Grad Norm 4.1868(5.3406) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 101.5638, Epoch Time 1512.7955(1288.0423), Bit/dim 3.6452(best: 3.6192), Xent 0.6499, Loss 3.9702, Error 0.2074(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10290 | Time 25.5517(25.2984) | Bit/dim 3.6157(3.6400) | Xent 0.3377(0.3297) | Loss 8.8593(9.6443) | Error 0.1267(0.1168) Steps 946(913.38) | Grad Norm 4.5127(5.0900) | Total Time 0.00(0.00)\n",
      "Iter 10300 | Time 25.5285(25.4246) | Bit/dim 3.6304(3.6392) | Xent 0.3084(0.3296) | Loss 8.6115(9.4128) | Error 0.0956(0.1157) Steps 892(914.26) | Grad Norm 4.4928(5.1306) | Total Time 0.00(0.00)\n",
      "Iter 10310 | Time 25.7698(25.5639) | Bit/dim 3.6320(3.6378) | Xent 0.3380(0.3311) | Loss 8.8396(9.2528) | Error 0.1156(0.1154) Steps 940(915.91) | Grad Norm 7.8032(5.1832) | Total Time 0.00(0.00)\n",
      "Iter 10320 | Time 26.2546(25.7164) | Bit/dim 3.6799(3.6399) | Xent 0.3030(0.3269) | Loss 8.8957(9.1322) | Error 0.1122(0.1145) Steps 928(915.09) | Grad Norm 4.3678(5.5893) | Total Time 0.00(0.00)\n",
      "Iter 10330 | Time 25.5543(25.7194) | Bit/dim 3.5882(3.6358) | Xent 0.3621(0.3257) | Loss 8.5978(9.0299) | Error 0.1256(0.1137) Steps 916(917.49) | Grad Norm 6.0227(5.4486) | Total Time 0.00(0.00)\n",
      "Iter 10340 | Time 25.3668(25.7803) | Bit/dim 3.6424(3.6398) | Xent 0.2681(0.3208) | Loss 8.7952(8.9808) | Error 0.0978(0.1130) Steps 946(921.87) | Grad Norm 3.5623(5.0768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 103.1980, Epoch Time 1544.9603(1295.7498), Bit/dim 3.6399(best: 3.6192), Xent 0.6608, Loss 3.9703, Error 0.2099(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10350 | Time 27.1897(25.9183) | Bit/dim 3.6388(3.6387) | Xent 0.3174(0.3216) | Loss 8.7791(9.5477) | Error 0.1078(0.1135) Steps 856(918.50) | Grad Norm 6.3416(5.1797) | Total Time 0.00(0.00)\n",
      "Iter 10360 | Time 26.0815(25.9496) | Bit/dim 3.6651(3.6400) | Xent 0.3316(0.3231) | Loss 8.8849(9.3499) | Error 0.1200(0.1141) Steps 952(922.08) | Grad Norm 4.0594(5.2451) | Total Time 0.00(0.00)\n",
      "Iter 10370 | Time 25.3171(25.9219) | Bit/dim 3.6524(3.6433) | Xent 0.3171(0.3226) | Loss 8.8352(9.2076) | Error 0.1200(0.1148) Steps 910(922.15) | Grad Norm 5.2397(5.1849) | Total Time 0.00(0.00)\n",
      "Iter 10380 | Time 26.7580(25.9117) | Bit/dim 3.6177(3.6385) | Xent 0.2852(0.3226) | Loss 8.5650(9.0870) | Error 0.1078(0.1147) Steps 892(923.76) | Grad Norm 6.1818(5.3688) | Total Time 0.00(0.00)\n",
      "Iter 10390 | Time 25.5695(25.8441) | Bit/dim 3.6083(3.6388) | Xent 0.3342(0.3240) | Loss 8.7064(9.0149) | Error 0.1233(0.1156) Steps 964(924.81) | Grad Norm 6.0172(5.3425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 102.9539, Epoch Time 1545.9634(1303.2562), Bit/dim 3.6398(best: 3.6192), Xent 0.6501, Loss 3.9649, Error 0.2105(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10400 | Time 26.8028(25.9058) | Bit/dim 3.5899(3.6375) | Xent 0.3196(0.3253) | Loss 8.7152(9.6479) | Error 0.1111(0.1160) Steps 958(927.45) | Grad Norm 8.8052(5.3032) | Total Time 0.00(0.00)\n",
      "Iter 10410 | Time 26.3181(26.0161) | Bit/dim 3.6462(3.6372) | Xent 0.3461(0.3259) | Loss 8.9331(9.4231) | Error 0.1167(0.1154) Steps 940(929.82) | Grad Norm 5.2521(5.2396) | Total Time 0.00(0.00)\n",
      "Iter 10420 | Time 25.6804(26.0872) | Bit/dim 3.6543(3.6390) | Xent 0.2940(0.3271) | Loss 8.8643(9.2637) | Error 0.1033(0.1167) Steps 940(931.76) | Grad Norm 4.4031(5.3601) | Total Time 0.00(0.00)\n",
      "Iter 10430 | Time 27.0037(26.0951) | Bit/dim 3.6454(3.6406) | Xent 0.3476(0.3314) | Loss 8.8173(9.1526) | Error 0.1200(0.1176) Steps 922(931.80) | Grad Norm 4.3178(5.1027) | Total Time 0.00(0.00)\n",
      "Iter 10440 | Time 26.7381(26.0785) | Bit/dim 3.6680(3.6407) | Xent 0.2599(0.3282) | Loss 8.7890(9.0610) | Error 0.0956(0.1154) Steps 934(934.28) | Grad Norm 4.6063(5.1555) | Total Time 0.00(0.00)\n",
      "Iter 10450 | Time 24.9368(25.9436) | Bit/dim 3.6416(3.6411) | Xent 0.3090(0.3284) | Loss 8.8008(8.9911) | Error 0.1133(0.1155) Steps 910(927.64) | Grad Norm 3.6802(5.1221) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 101.4601, Epoch Time 1555.3995(1310.8205), Bit/dim 3.6390(best: 3.6192), Xent 0.6576, Loss 3.9677, Error 0.2064(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10460 | Time 24.7436(25.7460) | Bit/dim 3.6263(3.6439) | Xent 0.3809(0.3284) | Loss 8.8102(9.5324) | Error 0.1311(0.1151) Steps 892(921.50) | Grad Norm 7.6164(5.1398) | Total Time 0.00(0.00)\n",
      "Iter 10470 | Time 24.4601(25.7313) | Bit/dim 3.6296(3.6428) | Xent 0.2635(0.3219) | Loss 8.6251(9.3345) | Error 0.1022(0.1142) Steps 892(921.63) | Grad Norm 4.6857(5.1748) | Total Time 0.00(0.00)\n",
      "Iter 10480 | Time 26.8013(25.7460) | Bit/dim 3.6398(3.6403) | Xent 0.3515(0.3214) | Loss 8.8519(9.1886) | Error 0.1278(0.1140) Steps 874(917.87) | Grad Norm 5.9875(5.3123) | Total Time 0.00(0.00)\n",
      "Iter 10490 | Time 25.1296(25.7183) | Bit/dim 3.6332(3.6401) | Xent 0.3323(0.3249) | Loss 8.6950(9.0911) | Error 0.1122(0.1152) Steps 910(915.05) | Grad Norm 5.6014(5.3805) | Total Time 0.00(0.00)\n",
      "Iter 10500 | Time 25.0974(25.6405) | Bit/dim 3.6316(3.6358) | Xent 0.2810(0.3276) | Loss 8.8141(9.0096) | Error 0.1033(0.1158) Steps 916(918.65) | Grad Norm 3.5928(5.3110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 102.6218, Epoch Time 1524.8702(1317.2420), Bit/dim 3.6367(best: 3.6192), Xent 0.6444, Loss 3.9588, Error 0.2062(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10510 | Time 25.3553(25.5395) | Bit/dim 3.6305(3.6343) | Xent 0.3436(0.3270) | Loss 8.7715(9.6601) | Error 0.1244(0.1156) Steps 904(914.64) | Grad Norm 11.1228(5.4343) | Total Time 0.00(0.00)\n",
      "Iter 10520 | Time 24.1547(25.4340) | Bit/dim 3.6394(3.6348) | Xent 0.3119(0.3268) | Loss 8.5705(9.4196) | Error 0.1111(0.1153) Steps 880(912.76) | Grad Norm 4.5441(5.4136) | Total Time 0.00(0.00)\n",
      "Iter 10530 | Time 26.1384(25.4871) | Bit/dim 3.6094(3.6355) | Xent 0.3449(0.3253) | Loss 8.7617(9.2704) | Error 0.1089(0.1135) Steps 892(914.67) | Grad Norm 4.4033(5.2877) | Total Time 0.00(0.00)\n",
      "Iter 10540 | Time 24.5577(25.4425) | Bit/dim 3.6465(3.6376) | Xent 0.3090(0.3209) | Loss 8.7667(9.1472) | Error 0.1167(0.1127) Steps 898(911.42) | Grad Norm 4.2077(4.9900) | Total Time 0.00(0.00)\n",
      "Iter 10550 | Time 25.2380(25.3109) | Bit/dim 3.6341(3.6354) | Xent 0.3396(0.3238) | Loss 8.8196(9.0506) | Error 0.1044(0.1136) Steps 892(908.96) | Grad Norm 9.2801(5.3366) | Total Time 0.00(0.00)\n",
      "Iter 10560 | Time 23.8901(25.2539) | Bit/dim 3.6405(3.6333) | Xent 0.2790(0.3227) | Loss 8.6750(8.9698) | Error 0.1022(0.1138) Steps 856(909.43) | Grad Norm 5.5398(5.4188) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 104.3110, Epoch Time 1511.7188(1323.0763), Bit/dim 3.6368(best: 3.6192), Xent 0.6485, Loss 3.9611, Error 0.2053(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10570 | Time 25.5588(25.2484) | Bit/dim 3.6325(3.6354) | Xent 0.2910(0.3224) | Loss 8.7125(9.5333) | Error 0.1033(0.1143) Steps 916(912.14) | Grad Norm 3.6976(5.1628) | Total Time 0.00(0.00)\n",
      "Iter 10580 | Time 25.4063(25.2555) | Bit/dim 3.6618(3.6356) | Xent 0.3316(0.3185) | Loss 8.8468(9.3357) | Error 0.1133(0.1129) Steps 904(909.63) | Grad Norm 4.9322(5.0486) | Total Time 0.00(0.00)\n",
      "Iter 10590 | Time 24.8936(25.2797) | Bit/dim 3.6637(3.6371) | Xent 0.3195(0.3203) | Loss 8.7044(9.1860) | Error 0.1244(0.1138) Steps 928(911.15) | Grad Norm 5.8202(5.1340) | Total Time 0.00(0.00)\n",
      "Iter 10600 | Time 24.9098(25.2657) | Bit/dim 3.6292(3.6362) | Xent 0.3831(0.3213) | Loss 8.8713(9.0686) | Error 0.1422(0.1147) Steps 940(914.55) | Grad Norm 6.6027(5.0419) | Total Time 0.00(0.00)\n",
      "Iter 10610 | Time 24.6952(25.1748) | Bit/dim 3.5936(3.6316) | Xent 0.3094(0.3212) | Loss 8.6035(8.9947) | Error 0.1067(0.1151) Steps 922(915.82) | Grad Norm 4.2156(5.0824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 102.9333, Epoch Time 1509.0889(1328.6567), Bit/dim 3.6387(best: 3.6192), Xent 0.6488, Loss 3.9631, Error 0.2072(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10620 | Time 25.7731(25.2681) | Bit/dim 3.6393(3.6301) | Xent 0.3147(0.3163) | Loss 8.8139(9.6300) | Error 0.1167(0.1135) Steps 922(914.32) | Grad Norm 5.3861(4.9810) | Total Time 0.00(0.00)\n",
      "Iter 10630 | Time 25.1354(25.2103) | Bit/dim 3.6483(3.6306) | Xent 0.3353(0.3185) | Loss 8.9054(9.4145) | Error 0.1233(0.1137) Steps 940(909.73) | Grad Norm 5.4378(5.0043) | Total Time 0.00(0.00)\n",
      "Iter 10640 | Time 25.4411(25.2341) | Bit/dim 3.6657(3.6321) | Xent 0.3127(0.3167) | Loss 8.6947(9.2453) | Error 0.1122(0.1131) Steps 868(908.29) | Grad Norm 5.8203(5.0804) | Total Time 0.00(0.00)\n",
      "Iter 10650 | Time 25.1950(25.1885) | Bit/dim 3.6442(3.6351) | Xent 0.3563(0.3185) | Loss 8.8110(9.1297) | Error 0.1222(0.1132) Steps 904(911.50) | Grad Norm 4.7268(5.0522) | Total Time 0.00(0.00)\n",
      "Iter 10660 | Time 24.8534(25.2838) | Bit/dim 3.6138(3.6332) | Xent 0.3289(0.3170) | Loss 8.7256(9.0354) | Error 0.1056(0.1117) Steps 904(914.04) | Grad Norm 6.3411(5.1063) | Total Time 0.00(0.00)\n",
      "Iter 10670 | Time 24.9317(25.3127) | Bit/dim 3.6419(3.6331) | Xent 0.3395(0.3210) | Loss 8.8246(8.9752) | Error 0.1100(0.1130) Steps 898(913.31) | Grad Norm 4.7881(5.7711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 102.2973, Epoch Time 1511.9578(1334.1557), Bit/dim 3.6343(best: 3.6192), Xent 0.6576, Loss 3.9631, Error 0.2080(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10680 | Time 25.6316(25.2153) | Bit/dim 3.6691(3.6312) | Xent 0.2728(0.3149) | Loss 8.8290(9.5247) | Error 0.0944(0.1105) Steps 880(910.54) | Grad Norm 4.1743(5.8777) | Total Time 0.00(0.00)\n",
      "Iter 10690 | Time 26.4328(25.3301) | Bit/dim 3.6183(3.6329) | Xent 0.3577(0.3149) | Loss 8.8421(9.3418) | Error 0.1211(0.1100) Steps 922(909.68) | Grad Norm 4.0945(5.6552) | Total Time 0.00(0.00)\n",
      "Iter 10700 | Time 24.3294(25.2567) | Bit/dim 3.6176(3.6309) | Xent 0.3345(0.3150) | Loss 8.6176(9.1933) | Error 0.1222(0.1109) Steps 904(914.48) | Grad Norm 3.6398(5.3962) | Total Time 0.00(0.00)\n",
      "Iter 10710 | Time 24.4075(25.0976) | Bit/dim 3.6278(3.6310) | Xent 0.3603(0.3183) | Loss 8.7702(9.0815) | Error 0.1300(0.1116) Steps 904(911.60) | Grad Norm 6.8195(5.8407) | Total Time 0.00(0.00)\n",
      "Iter 10720 | Time 24.9005(25.1687) | Bit/dim 3.6252(3.6319) | Xent 0.3032(0.3181) | Loss 8.7752(8.9999) | Error 0.1067(0.1121) Steps 934(914.07) | Grad Norm 6.4432(6.2500) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 102.2504, Epoch Time 1504.7931(1339.2749), Bit/dim 3.6342(best: 3.6192), Xent 0.6527, Loss 3.9606, Error 0.2053(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10730 | Time 24.8869(25.2181) | Bit/dim 3.6521(3.6324) | Xent 0.2669(0.3145) | Loss 8.7411(9.6448) | Error 0.0989(0.1109) Steps 934(912.96) | Grad Norm 2.8726(5.7798) | Total Time 0.00(0.00)\n",
      "Iter 10740 | Time 25.7401(25.1242) | Bit/dim 3.6368(3.6300) | Xent 0.2862(0.3147) | Loss 8.7191(9.4100) | Error 0.0967(0.1106) Steps 946(909.45) | Grad Norm 5.8116(5.5825) | Total Time 0.00(0.00)\n",
      "Iter 10750 | Time 23.9791(25.0444) | Bit/dim 3.6383(3.6284) | Xent 0.2909(0.3143) | Loss 8.6700(9.2394) | Error 0.0944(0.1103) Steps 904(910.05) | Grad Norm 3.7502(5.3604) | Total Time 0.00(0.00)\n",
      "Iter 10760 | Time 24.5528(25.0247) | Bit/dim 3.6513(3.6302) | Xent 0.3332(0.3161) | Loss 8.7844(9.1138) | Error 0.1033(0.1111) Steps 904(909.83) | Grad Norm 5.3949(5.1768) | Total Time 0.00(0.00)\n",
      "Iter 10770 | Time 25.8191(24.9596) | Bit/dim 3.6209(3.6292) | Xent 0.3451(0.3165) | Loss 8.7234(9.0252) | Error 0.1333(0.1119) Steps 922(909.74) | Grad Norm 6.4255(5.1742) | Total Time 0.00(0.00)\n",
      "Iter 10780 | Time 26.3194(25.1610) | Bit/dim 3.6093(3.6275) | Xent 0.2755(0.3121) | Loss 8.7924(8.9497) | Error 0.1011(0.1105) Steps 904(908.93) | Grad Norm 3.7504(5.0617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 103.2398, Epoch Time 1498.6601(1344.0564), Bit/dim 3.6314(best: 3.6192), Xent 0.6583, Loss 3.9605, Error 0.2056(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10790 | Time 24.5967(25.0987) | Bit/dim 3.5981(3.6265) | Xent 0.3479(0.3130) | Loss 8.7202(9.5175) | Error 0.1244(0.1111) Steps 910(910.53) | Grad Norm 6.4562(5.1731) | Total Time 0.00(0.00)\n",
      "Iter 10800 | Time 23.8250(24.9000) | Bit/dim 3.6184(3.6279) | Xent 0.3383(0.3130) | Loss 8.8460(9.3162) | Error 0.1178(0.1110) Steps 940(907.39) | Grad Norm 12.1481(5.6962) | Total Time 0.00(0.00)\n",
      "Iter 10810 | Time 25.6449(24.7925) | Bit/dim 3.6167(3.6251) | Xent 0.2780(0.3111) | Loss 8.7847(9.1611) | Error 0.0989(0.1095) Steps 970(909.20) | Grad Norm 3.6436(5.5793) | Total Time 0.00(0.00)\n",
      "Iter 10820 | Time 25.2936(24.7728) | Bit/dim 3.5931(3.6268) | Xent 0.2940(0.3104) | Loss 8.5240(9.0495) | Error 0.0956(0.1092) Steps 892(909.30) | Grad Norm 3.1571(5.4392) | Total Time 0.00(0.00)\n",
      "Iter 10830 | Time 25.4422(24.9008) | Bit/dim 3.6470(3.6279) | Xent 0.3092(0.3131) | Loss 8.7648(8.9872) | Error 0.1033(0.1099) Steps 916(911.81) | Grad Norm 4.0129(5.2995) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 101.8312, Epoch Time 1479.3796(1348.1161), Bit/dim 3.6326(best: 3.6192), Xent 0.6500, Loss 3.9576, Error 0.2057(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10840 | Time 23.9890(24.7556) | Bit/dim 3.6331(3.6283) | Xent 0.2520(0.3117) | Loss 8.7032(9.6011) | Error 0.0900(0.1096) Steps 862(907.87) | Grad Norm 3.9807(5.3895) | Total Time 0.00(0.00)\n",
      "Iter 10850 | Time 24.5558(24.7836) | Bit/dim 3.6016(3.6272) | Xent 0.3620(0.3108) | Loss 8.8311(9.3749) | Error 0.1256(0.1088) Steps 940(905.75) | Grad Norm 6.9797(5.1999) | Total Time 0.00(0.00)\n",
      "Iter 10860 | Time 24.9690(24.6931) | Bit/dim 3.6345(3.6256) | Xent 0.2908(0.3112) | Loss 8.7889(9.2184) | Error 0.1044(0.1100) Steps 904(903.82) | Grad Norm 5.8056(5.3108) | Total Time 0.00(0.00)\n",
      "Iter 10870 | Time 23.8729(24.7193) | Bit/dim 3.6216(3.6253) | Xent 0.3420(0.3120) | Loss 8.7819(9.1032) | Error 0.1200(0.1096) Steps 886(903.08) | Grad Norm 6.6895(5.2627) | Total Time 0.00(0.00)\n",
      "Iter 10880 | Time 23.9208(24.7505) | Bit/dim 3.6169(3.6228) | Xent 0.2745(0.3115) | Loss 8.6512(9.0027) | Error 0.0944(0.1093) Steps 880(903.95) | Grad Norm 7.5733(5.4265) | Total Time 0.00(0.00)\n",
      "Iter 10890 | Time 25.2458(24.6885) | Bit/dim 3.6245(3.6222) | Xent 0.3020(0.3096) | Loss 8.7609(8.9314) | Error 0.1089(0.1101) Steps 922(904.87) | Grad Norm 5.2186(5.9353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 102.0396, Epoch Time 1476.3846(1351.9642), Bit/dim 3.6291(best: 3.6192), Xent 0.6603, Loss 3.9593, Error 0.2084(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10900 | Time 25.3526(24.6336) | Bit/dim 3.5875(3.6184) | Xent 0.3153(0.3112) | Loss 8.6486(9.4870) | Error 0.1067(0.1096) Steps 940(907.34) | Grad Norm 7.5327(5.8302) | Total Time 0.00(0.00)\n",
      "Iter 10910 | Time 24.6829(24.7046) | Bit/dim 3.5994(3.6213) | Xent 0.3612(0.3105) | Loss 8.6344(9.2980) | Error 0.1267(0.1091) Steps 868(909.39) | Grad Norm 10.1734(6.0941) | Total Time 0.00(0.00)\n",
      "Iter 10920 | Time 25.9002(24.8194) | Bit/dim 3.6220(3.6249) | Xent 0.3202(0.3145) | Loss 8.6694(9.1614) | Error 0.1100(0.1100) Steps 868(907.42) | Grad Norm 3.6376(6.2802) | Total Time 0.00(0.00)\n",
      "Iter 10930 | Time 24.7214(24.7985) | Bit/dim 3.6362(3.6277) | Xent 0.3059(0.3106) | Loss 8.8155(9.0571) | Error 0.1156(0.1092) Steps 946(909.05) | Grad Norm 4.9717(5.8852) | Total Time 0.00(0.00)\n",
      "Iter 10940 | Time 24.3723(24.9521) | Bit/dim 3.6447(3.6278) | Xent 0.3301(0.3111) | Loss 8.8038(8.9796) | Error 0.1244(0.1099) Steps 892(910.11) | Grad Norm 4.8323(5.8539) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 102.0196, Epoch Time 1491.2175(1356.1418), Bit/dim 3.6289(best: 3.6192), Xent 0.6588, Loss 3.9583, Error 0.2060(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10950 | Time 24.3891(24.9266) | Bit/dim 3.5955(3.6257) | Xent 0.2876(0.3098) | Loss 8.7380(9.6441) | Error 0.1067(0.1095) Steps 922(909.55) | Grad Norm 4.3582(5.7780) | Total Time 0.00(0.00)\n",
      "Iter 10960 | Time 23.9332(24.9113) | Bit/dim 3.6473(3.6273) | Xent 0.3175(0.3073) | Loss 8.8107(9.4102) | Error 0.1111(0.1086) Steps 916(910.77) | Grad Norm 6.0193(5.5374) | Total Time 0.00(0.00)\n",
      "Iter 10970 | Time 25.7248(24.8841) | Bit/dim 3.6569(3.6278) | Xent 0.2900(0.3076) | Loss 8.8712(9.2494) | Error 0.1078(0.1084) Steps 958(910.82) | Grad Norm 6.6047(5.5280) | Total Time 0.00(0.00)\n",
      "Iter 10980 | Time 25.7467(24.8968) | Bit/dim 3.6083(3.6254) | Xent 0.3200(0.3114) | Loss 8.8652(9.1234) | Error 0.1156(0.1098) Steps 904(910.48) | Grad Norm 3.8324(5.7222) | Total Time 0.00(0.00)\n",
      "Iter 10990 | Time 24.6277(24.8456) | Bit/dim 3.6202(3.6244) | Xent 0.3340(0.3101) | Loss 8.8249(9.0266) | Error 0.1133(0.1084) Steps 886(906.81) | Grad Norm 6.9508(5.7475) | Total Time 0.00(0.00)\n",
      "Iter 11000 | Time 23.2601(24.6772) | Bit/dim 3.6360(3.6244) | Xent 0.2828(0.3105) | Loss 8.7433(8.9488) | Error 0.0967(0.1087) Steps 898(904.15) | Grad Norm 4.1801(5.4491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 101.7755, Epoch Time 1479.3318(1359.8375), Bit/dim 3.6273(best: 3.6192), Xent 0.6511, Loss 3.9529, Error 0.2062(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11010 | Time 25.0024(24.7918) | Bit/dim 3.6232(3.6236) | Xent 0.2645(0.3058) | Loss 8.6857(9.4871) | Error 0.0856(0.1071) Steps 904(903.92) | Grad Norm 4.9989(5.4225) | Total Time 0.00(0.00)\n",
      "Iter 11020 | Time 24.3600(24.8160) | Bit/dim 3.6568(3.6262) | Xent 0.3245(0.3062) | Loss 8.7958(9.3009) | Error 0.1033(0.1075) Steps 904(904.38) | Grad Norm 3.8810(5.5157) | Total Time 0.00(0.00)\n",
      "Iter 11030 | Time 24.7176(24.9895) | Bit/dim 3.6165(3.6234) | Xent 0.2968(0.3076) | Loss 8.7526(9.1564) | Error 0.1067(0.1087) Steps 880(907.55) | Grad Norm 5.1820(5.5586) | Total Time 0.00(0.00)\n",
      "Iter 11040 | Time 24.3734(24.9655) | Bit/dim 3.6411(3.6250) | Xent 0.2742(0.3082) | Loss 8.7197(9.0475) | Error 0.0944(0.1093) Steps 886(905.83) | Grad Norm 4.7058(5.4983) | Total Time 0.00(0.00)\n",
      "Iter 11050 | Time 24.2956(24.8226) | Bit/dim 3.6223(3.6259) | Xent 0.3237(0.3122) | Loss 8.6932(8.9734) | Error 0.1044(0.1109) Steps 922(903.88) | Grad Norm 5.9171(5.3520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 101.7052, Epoch Time 1489.5317(1363.7283), Bit/dim 3.6266(best: 3.6192), Xent 0.6611, Loss 3.9572, Error 0.2065(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11060 | Time 24.0838(24.7027) | Bit/dim 3.6334(3.6225) | Xent 0.3000(0.3101) | Loss 8.7070(9.6373) | Error 0.0989(0.1103) Steps 898(903.45) | Grad Norm 5.6271(5.4122) | Total Time 0.00(0.00)\n",
      "Iter 11070 | Time 25.0449(24.5814) | Bit/dim 3.6208(3.6224) | Xent 0.2980(0.3089) | Loss 8.7279(9.3952) | Error 0.1100(0.1095) Steps 976(902.63) | Grad Norm 3.8585(5.1110) | Total Time 0.00(0.00)\n",
      "Iter 11080 | Time 23.9622(24.4571) | Bit/dim 3.6445(3.6259) | Xent 0.2425(0.3043) | Loss 8.7315(9.2177) | Error 0.0844(0.1078) Steps 904(906.32) | Grad Norm 3.2800(4.8551) | Total Time 0.00(0.00)\n",
      "Iter 11090 | Time 22.8698(24.4284) | Bit/dim 3.6089(3.6241) | Xent 0.2548(0.3050) | Loss 8.6593(9.1007) | Error 0.0922(0.1085) Steps 904(905.95) | Grad Norm 6.1053(5.0604) | Total Time 0.00(0.00)\n",
      "Iter 11100 | Time 24.4843(24.4371) | Bit/dim 3.6186(3.6210) | Xent 0.3179(0.3081) | Loss 8.7516(9.0124) | Error 0.1056(0.1093) Steps 922(906.73) | Grad Norm 4.6993(5.4195) | Total Time 0.00(0.00)\n",
      "Iter 11110 | Time 25.7971(24.4024) | Bit/dim 3.6347(3.6227) | Xent 0.3264(0.3113) | Loss 8.7682(8.9490) | Error 0.1189(0.1106) Steps 910(906.21) | Grad Norm 3.9917(5.2967) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 100.8931, Epoch Time 1455.6251(1366.4852), Bit/dim 3.6265(best: 3.6192), Xent 0.6614, Loss 3.9571, Error 0.2079(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11120 | Time 25.7179(24.4576) | Bit/dim 3.5970(3.6222) | Xent 0.3266(0.3093) | Loss 8.7530(9.5100) | Error 0.1011(0.1099) Steps 874(907.35) | Grad Norm 6.8857(5.2434) | Total Time 0.00(0.00)\n",
      "Iter 11130 | Time 25.1768(24.4849) | Bit/dim 3.6380(3.6214) | Xent 0.2661(0.3092) | Loss 8.6279(9.2994) | Error 0.1022(0.1099) Steps 886(907.05) | Grad Norm 6.1104(5.4056) | Total Time 0.00(0.00)\n",
      "Iter 11140 | Time 24.2475(24.5546) | Bit/dim 3.5990(3.6202) | Xent 0.2862(0.3082) | Loss 8.6684(9.1562) | Error 0.0978(0.1092) Steps 940(909.38) | Grad Norm 5.2557(5.3809) | Total Time 0.00(0.00)\n",
      "Iter 11150 | Time 25.3706(24.6033) | Bit/dim 3.6355(3.6247) | Xent 0.2801(0.3061) | Loss 8.7635(9.0589) | Error 0.0967(0.1084) Steps 928(907.21) | Grad Norm 6.6217(5.3967) | Total Time 0.00(0.00)\n",
      "Iter 11160 | Time 24.5163(24.4642) | Bit/dim 3.6111(3.6210) | Xent 0.2986(0.3055) | Loss 8.7465(8.9822) | Error 0.1022(0.1079) Steps 892(906.72) | Grad Norm 5.0821(5.6502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 101.8628, Epoch Time 1469.7479(1369.5831), Bit/dim 3.6271(best: 3.6192), Xent 0.6648, Loss 3.9595, Error 0.2111(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11170 | Time 23.8035(24.4550) | Bit/dim 3.6390(3.6227) | Xent 0.3091(0.3008) | Loss 8.8809(9.6300) | Error 0.1000(0.1059) Steps 862(907.38) | Grad Norm 8.0727(5.6960) | Total Time 0.00(0.00)\n",
      "Iter 11180 | Time 23.5458(24.3821) | Bit/dim 3.6231(3.6229) | Xent 0.2942(0.3001) | Loss 8.7525(9.3932) | Error 0.1044(0.1059) Steps 886(903.00) | Grad Norm 5.0499(5.4931) | Total Time 0.00(0.00)\n",
      "Iter 11190 | Time 24.8664(24.3277) | Bit/dim 3.6081(3.6229) | Xent 0.2898(0.3032) | Loss 8.7008(9.2189) | Error 0.0989(0.1082) Steps 910(901.98) | Grad Norm 5.4310(5.5506) | Total Time 0.00(0.00)\n",
      "Iter 11200 | Time 27.3874(24.4269) | Bit/dim 3.6698(3.6262) | Xent 0.4046(0.3124) | Loss 9.0541(9.1215) | Error 0.1467(0.1116) Steps 934(905.54) | Grad Norm 17.7240(9.0980) | Total Time 0.00(0.00)\n",
      "Iter 11210 | Time 25.5266(24.5927) | Bit/dim 3.6464(3.6333) | Xent 0.2987(0.3171) | Loss 8.6989(9.0581) | Error 0.1022(0.1129) Steps 922(908.82) | Grad Norm 9.1877(9.2505) | Total Time 0.00(0.00)\n",
      "Iter 11220 | Time 25.3507(24.4382) | Bit/dim 3.6319(3.6297) | Xent 0.3177(0.3226) | Loss 8.7194(8.9753) | Error 0.1144(0.1148) Steps 928(905.34) | Grad Norm 6.2977(8.6825) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 100.4844, Epoch Time 1461.5728(1372.3428), Bit/dim 3.6370(best: 3.6192), Xent 0.6689, Loss 3.9715, Error 0.2060(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11230 | Time 24.2386(24.4320) | Bit/dim 3.6746(3.6289) | Xent 0.3287(0.3175) | Loss 8.8634(9.5147) | Error 0.1222(0.1137) Steps 916(906.72) | Grad Norm 5.5192(8.1467) | Total Time 0.00(0.00)\n",
      "Iter 11240 | Time 24.8551(24.4811) | Bit/dim 3.6324(3.6300) | Xent 0.2965(0.3151) | Loss 8.7231(9.3160) | Error 0.1156(0.1132) Steps 886(904.32) | Grad Norm 3.9030(7.1628) | Total Time 0.00(0.00)\n",
      "Iter 11250 | Time 26.0345(24.3970) | Bit/dim 3.6025(3.6257) | Xent 0.2764(0.3098) | Loss 8.6917(9.1527) | Error 0.0989(0.1105) Steps 982(903.77) | Grad Norm 4.5297(6.4460) | Total Time 0.00(0.00)\n",
      "Iter 11260 | Time 25.3173(24.4422) | Bit/dim 3.6400(3.6253) | Xent 0.3161(0.3074) | Loss 8.7829(9.0435) | Error 0.1111(0.1089) Steps 874(900.04) | Grad Norm 5.6598(5.9808) | Total Time 0.00(0.00)\n",
      "Iter 11270 | Time 24.1202(24.3212) | Bit/dim 3.6204(3.6214) | Xent 0.2697(0.3069) | Loss 8.6506(8.9484) | Error 0.0944(0.1086) Steps 850(897.72) | Grad Norm 3.9777(6.0425) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 101.0973, Epoch Time 1457.8128(1374.9069), Bit/dim 3.6243(best: 3.6192), Xent 0.6626, Loss 3.9556, Error 0.2097(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11280 | Time 25.2655(24.3127) | Bit/dim 3.6241(3.6219) | Xent 0.3002(0.3079) | Loss 8.5889(9.6043) | Error 0.1100(0.1099) Steps 922(899.18) | Grad Norm 4.2686(6.0283) | Total Time 0.00(0.00)\n",
      "Iter 11290 | Time 25.5084(24.3173) | Bit/dim 3.6400(3.6245) | Xent 0.3072(0.3065) | Loss 8.8704(9.3876) | Error 0.1167(0.1098) Steps 910(899.53) | Grad Norm 7.3306(5.8597) | Total Time 0.00(0.00)\n",
      "Iter 11300 | Time 24.8897(24.3003) | Bit/dim 3.6053(3.6234) | Xent 0.2911(0.3023) | Loss 8.8164(9.2230) | Error 0.1078(0.1076) Steps 910(903.02) | Grad Norm 3.9803(5.4996) | Total Time 0.00(0.00)\n",
      "Iter 11310 | Time 23.0361(24.2709) | Bit/dim 3.6129(3.6206) | Xent 0.3444(0.3011) | Loss 8.8487(9.0941) | Error 0.1267(0.1071) Steps 904(904.21) | Grad Norm 5.0872(5.5235) | Total Time 0.00(0.00)\n",
      "Iter 11320 | Time 23.1517(24.1807) | Bit/dim 3.5719(3.6202) | Xent 0.2845(0.3014) | Loss 8.5540(8.9935) | Error 0.1056(0.1068) Steps 862(898.88) | Grad Norm 4.3390(5.5386) | Total Time 0.00(0.00)\n",
      "Iter 11330 | Time 23.1696(24.1210) | Bit/dim 3.6150(3.6202) | Xent 0.3121(0.3048) | Loss 8.7054(8.9283) | Error 0.1067(0.1087) Steps 880(899.54) | Grad Norm 4.3054(5.3940) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 102.8665, Epoch Time 1448.8851(1377.1262), Bit/dim 3.6271(best: 3.6192), Xent 0.6617, Loss 3.9580, Error 0.2054(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11340 | Time 24.8206(24.2048) | Bit/dim 3.6672(3.6232) | Xent 0.2778(0.3015) | Loss 8.6691(9.4835) | Error 0.1000(0.1078) Steps 922(900.89) | Grad Norm 6.8088(5.2414) | Total Time 0.00(0.00)\n",
      "Iter 11350 | Time 23.8161(24.1121) | Bit/dim 3.5913(3.6211) | Xent 0.3066(0.3015) | Loss 8.7269(9.2882) | Error 0.1100(0.1079) Steps 904(899.53) | Grad Norm 4.5300(5.0822) | Total Time 0.00(0.00)\n",
      "Iter 11360 | Time 24.7831(24.1249) | Bit/dim 3.6442(3.6211) | Xent 0.3396(0.3023) | Loss 8.6641(9.1369) | Error 0.1033(0.1070) Steps 898(901.24) | Grad Norm 4.9898(4.8890) | Total Time 0.00(0.00)\n",
      "Iter 11370 | Time 26.5586(24.2629) | Bit/dim 3.6362(3.6209) | Xent 0.3486(0.3009) | Loss 8.9093(9.0450) | Error 0.1178(0.1061) Steps 910(903.67) | Grad Norm 4.3517(4.8126) | Total Time 0.00(0.00)\n",
      "Iter 11380 | Time 23.8771(24.3158) | Bit/dim 3.6089(3.6199) | Xent 0.3011(0.3024) | Loss 8.8012(8.9794) | Error 0.1033(0.1063) Steps 916(904.32) | Grad Norm 5.0152(4.7906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 102.8327, Epoch Time 1461.4525(1379.6560), Bit/dim 3.6238(best: 3.6192), Xent 0.6696, Loss 3.9586, Error 0.2109(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11390 | Time 23.2485(24.3807) | Bit/dim 3.6206(3.6191) | Xent 0.2862(0.3031) | Loss 8.6900(9.6417) | Error 0.0944(0.1064) Steps 886(906.68) | Grad Norm 7.8044(4.7771) | Total Time 0.00(0.00)\n",
      "Iter 11400 | Time 25.3911(24.4698) | Bit/dim 3.6125(3.6194) | Xent 0.2859(0.2994) | Loss 8.5927(9.4044) | Error 0.0989(0.1056) Steps 952(906.15) | Grad Norm 3.7506(4.9593) | Total Time 0.00(0.00)\n",
      "Iter 11410 | Time 23.0864(24.3985) | Bit/dim 3.6107(3.6186) | Xent 0.2364(0.3003) | Loss 8.7055(9.2361) | Error 0.0844(0.1058) Steps 892(909.48) | Grad Norm 7.1423(5.6759) | Total Time 0.00(0.00)\n",
      "Iter 11420 | Time 23.6719(24.3420) | Bit/dim 3.6013(3.6176) | Xent 0.3217(0.3013) | Loss 8.6147(9.1061) | Error 0.1233(0.1073) Steps 892(908.20) | Grad Norm 8.1478(6.5322) | Total Time 0.00(0.00)\n",
      "Iter 11430 | Time 25.3875(24.5075) | Bit/dim 3.6225(3.6196) | Xent 0.3280(0.3028) | Loss 8.8614(9.0292) | Error 0.1144(0.1076) Steps 916(908.07) | Grad Norm 5.2794(6.0689) | Total Time 0.00(0.00)\n",
      "Iter 11440 | Time 25.2773(24.6721) | Bit/dim 3.6197(3.6218) | Xent 0.2747(0.3018) | Loss 8.7157(8.9599) | Error 0.1089(0.1080) Steps 898(912.65) | Grad Norm 4.8182(5.8166) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 102.7922, Epoch Time 1473.9573(1382.4850), Bit/dim 3.6258(best: 3.6192), Xent 0.6620, Loss 3.9568, Error 0.2077(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11450 | Time 24.0553(24.5827) | Bit/dim 3.6101(3.6200) | Xent 0.2525(0.2984) | Loss 8.7522(9.5313) | Error 0.0856(0.1057) Steps 892(909.54) | Grad Norm 6.4957(5.6435) | Total Time 0.00(0.00)\n",
      "Iter 11460 | Time 24.3627(24.4795) | Bit/dim 3.6331(3.6203) | Xent 0.2990(0.2982) | Loss 8.7902(9.3337) | Error 0.0978(0.1063) Steps 940(906.21) | Grad Norm 3.1660(5.4096) | Total Time 0.00(0.00)\n",
      "Iter 11470 | Time 25.0318(24.6177) | Bit/dim 3.6314(3.6199) | Xent 0.3014(0.2948) | Loss 8.7113(9.1755) | Error 0.1000(0.1044) Steps 922(912.63) | Grad Norm 5.8200(5.4275) | Total Time 0.00(0.00)\n",
      "Iter 11480 | Time 24.7437(24.5970) | Bit/dim 3.6182(3.6194) | Xent 0.2863(0.2973) | Loss 8.5780(9.0645) | Error 0.0967(0.1046) Steps 904(914.47) | Grad Norm 3.7000(5.4107) | Total Time 0.00(0.00)\n",
      "Iter 11490 | Time 25.2207(24.6190) | Bit/dim 3.6141(3.6205) | Xent 0.3150(0.2981) | Loss 8.8590(8.9945) | Error 0.1078(0.1046) Steps 922(916.43) | Grad Norm 5.8512(5.3171) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 103.1474, Epoch Time 1473.2419(1385.2078), Bit/dim 3.6217(best: 3.6192), Xent 0.6707, Loss 3.9571, Error 0.2081(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11500 | Time 24.3676(24.6699) | Bit/dim 3.6326(3.6222) | Xent 0.3006(0.2973) | Loss 8.7425(9.6723) | Error 0.1078(0.1042) Steps 892(911.25) | Grad Norm 3.8821(5.0683) | Total Time 0.00(0.00)\n",
      "Iter 11510 | Time 23.8344(24.5607) | Bit/dim 3.6264(3.6221) | Xent 0.3061(0.2953) | Loss 8.7522(9.4211) | Error 0.1111(0.1043) Steps 910(910.36) | Grad Norm 5.8371(5.0538) | Total Time 0.00(0.00)\n",
      "Iter 11520 | Time 24.3984(24.4185) | Bit/dim 3.6041(3.6204) | Xent 0.3503(0.2980) | Loss 8.7586(9.2320) | Error 0.1233(0.1053) Steps 910(908.55) | Grad Norm 7.5048(5.4178) | Total Time 0.00(0.00)\n",
      "Iter 11530 | Time 24.6445(24.3708) | Bit/dim 3.5795(3.6190) | Xent 0.2777(0.2985) | Loss 8.6215(9.0958) | Error 0.0944(0.1054) Steps 910(906.48) | Grad Norm 5.2639(5.4771) | Total Time 0.00(0.00)\n",
      "Iter 11540 | Time 25.3853(24.5773) | Bit/dim 3.6522(3.6202) | Xent 0.2916(0.2979) | Loss 8.8053(9.0026) | Error 0.1011(0.1050) Steps 916(907.27) | Grad Norm 10.3501(6.2424) | Total Time 0.00(0.00)\n",
      "Iter 11550 | Time 24.0604(24.6980) | Bit/dim 3.6453(3.6209) | Xent 0.2953(0.2997) | Loss 8.8052(8.9414) | Error 0.1056(0.1058) Steps 910(908.21) | Grad Norm 4.6087(6.2471) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 101.7084, Epoch Time 1471.2483(1387.7890), Bit/dim 3.6294(best: 3.6192), Xent 0.6666, Loss 3.9627, Error 0.2078(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11560 | Time 24.8445(24.6985) | Bit/dim 3.6263(3.6203) | Xent 0.2997(0.2992) | Loss 8.8257(9.4993) | Error 0.1078(0.1055) Steps 958(907.99) | Grad Norm 5.4629(6.2681) | Total Time 0.00(0.00)\n",
      "Iter 11570 | Time 23.2175(24.5964) | Bit/dim 3.6097(3.6187) | Xent 0.3244(0.3000) | Loss 8.7780(9.3078) | Error 0.1233(0.1068) Steps 880(909.13) | Grad Norm 5.3734(6.6793) | Total Time 0.00(0.00)\n",
      "Iter 11580 | Time 24.0487(24.4681) | Bit/dim 3.5929(3.6168) | Xent 0.2850(0.2998) | Loss 8.7212(9.1633) | Error 0.1056(0.1063) Steps 898(908.21) | Grad Norm 6.6176(6.8763) | Total Time 0.00(0.00)\n",
      "Iter 11590 | Time 23.4722(24.4356) | Bit/dim 3.5900(3.6200) | Xent 0.2770(0.3002) | Loss 8.6902(9.0555) | Error 0.1011(0.1064) Steps 892(907.85) | Grad Norm 6.0291(6.5663) | Total Time 0.00(0.00)\n",
      "Iter 11600 | Time 24.1548(24.4572) | Bit/dim 3.6091(3.6179) | Xent 0.3291(0.3037) | Loss 8.7021(8.9718) | Error 0.1211(0.1072) Steps 868(906.70) | Grad Norm 4.9399(6.1938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 103.3592, Epoch Time 1465.6352(1390.1244), Bit/dim 3.6226(best: 3.6192), Xent 0.6753, Loss 3.9602, Error 0.2055(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11610 | Time 24.8462(24.5698) | Bit/dim 3.6692(3.6218) | Xent 0.2992(0.3016) | Loss 8.8766(9.6559) | Error 0.1167(0.1068) Steps 922(907.24) | Grad Norm 4.3403(6.1617) | Total Time 0.00(0.00)\n",
      "Iter 11620 | Time 25.2452(24.4857) | Bit/dim 3.6284(3.6165) | Xent 0.2874(0.2971) | Loss 8.6082(9.4018) | Error 0.1011(0.1054) Steps 928(904.43) | Grad Norm 4.0487(6.0006) | Total Time 0.00(0.00)\n",
      "Iter 11630 | Time 24.6361(24.5133) | Bit/dim 3.6186(3.6160) | Xent 0.3128(0.2985) | Loss 8.8571(9.2260) | Error 0.0989(0.1053) Steps 892(909.05) | Grad Norm 4.6772(5.6227) | Total Time 0.00(0.00)\n",
      "Iter 11640 | Time 24.8290(24.4663) | Bit/dim 3.6266(3.6173) | Xent 0.2707(0.2988) | Loss 8.7039(9.1059) | Error 0.0922(0.1051) Steps 934(912.34) | Grad Norm 3.4750(5.6952) | Total Time 0.00(0.00)\n",
      "Iter 11650 | Time 25.0480(24.3708) | Bit/dim 3.6107(3.6200) | Xent 0.2799(0.3001) | Loss 8.6748(9.0125) | Error 0.0911(0.1055) Steps 928(911.94) | Grad Norm 6.1796(6.9021) | Total Time 0.00(0.00)\n",
      "Iter 11660 | Time 25.6707(24.4164) | Bit/dim 3.6047(3.6197) | Xent 0.3324(0.2991) | Loss 8.7914(8.9382) | Error 0.1056(0.1053) Steps 958(916.15) | Grad Norm 5.3333(6.6718) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 102.1146, Epoch Time 1463.7551(1392.3333), Bit/dim 3.6216(best: 3.6192), Xent 0.6808, Loss 3.9620, Error 0.2091(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11670 | Time 24.4844(24.4540) | Bit/dim 3.5960(3.6151) | Xent 0.2783(0.2958) | Loss 8.7990(9.4939) | Error 0.1222(0.1054) Steps 898(916.66) | Grad Norm 5.6170(6.2564) | Total Time 0.00(0.00)\n",
      "Iter 11680 | Time 25.3178(24.5688) | Bit/dim 3.6184(3.6166) | Xent 0.3142(0.2977) | Loss 8.8057(9.2967) | Error 0.1067(0.1057) Steps 904(914.66) | Grad Norm 5.2701(5.9619) | Total Time 0.00(0.00)\n",
      "Iter 11690 | Time 25.3558(24.7359) | Bit/dim 3.6366(3.6197) | Xent 0.2766(0.3004) | Loss 8.7728(9.1569) | Error 0.0956(0.1066) Steps 922(916.07) | Grad Norm 7.3700(6.0975) | Total Time 0.00(0.00)\n",
      "Iter 11700 | Time 26.5368(24.9593) | Bit/dim 3.6093(3.6156) | Xent 0.2814(0.3001) | Loss 8.7132(9.0447) | Error 0.0978(0.1063) Steps 982(922.75) | Grad Norm 6.1964(6.0530) | Total Time 0.00(0.00)\n",
      "Iter 11710 | Time 24.1180(24.9535) | Bit/dim 3.5652(3.6151) | Xent 0.2751(0.2972) | Loss 8.5583(8.9505) | Error 0.0956(0.1049) Steps 904(922.35) | Grad Norm 3.2366(5.9336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 102.9108, Epoch Time 1498.3112(1395.5126), Bit/dim 3.6196(best: 3.6192), Xent 0.6771, Loss 3.9582, Error 0.2102(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11720 | Time 24.3847(24.9065) | Bit/dim 3.5808(3.6158) | Xent 0.3217(0.2985) | Loss 8.7982(9.6078) | Error 0.1111(0.1048) Steps 880(918.81) | Grad Norm 8.7806(6.0987) | Total Time 0.00(0.00)\n",
      "Iter 11730 | Time 24.4902(24.8951) | Bit/dim 3.6119(3.6190) | Xent 0.2848(0.2977) | Loss 8.6119(9.3847) | Error 0.1011(0.1048) Steps 898(918.30) | Grad Norm 4.6604(6.2503) | Total Time 0.00(0.00)\n",
      "Iter 11740 | Time 24.6505(24.8627) | Bit/dim 3.6164(3.6195) | Xent 0.3071(0.2951) | Loss 8.7337(9.2184) | Error 0.1111(0.1034) Steps 928(918.14) | Grad Norm 8.3477(6.1688) | Total Time 0.00(0.00)\n",
      "Iter 11750 | Time 27.6605(25.4518) | Bit/dim 3.6573(3.6267) | Xent 0.3244(0.3058) | Loss 8.8155(9.1131) | Error 0.1122(0.1078) Steps 988(931.98) | Grad Norm 7.6264(11.6716) | Total Time 0.00(0.00)\n",
      "Iter 11760 | Time 25.7114(25.5162) | Bit/dim 3.6206(3.6325) | Xent 0.2984(0.3094) | Loss 8.7781(9.0333) | Error 0.1056(0.1090) Steps 916(932.36) | Grad Norm 4.2535(10.2180) | Total Time 0.00(0.00)\n",
      "Iter 11770 | Time 24.6675(25.3100) | Bit/dim 3.6356(3.6312) | Xent 0.3135(0.3093) | Loss 8.7718(8.9630) | Error 0.1156(0.1096) Steps 898(926.86) | Grad Norm 3.2880(8.7403) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 104.3684, Epoch Time 1516.7612(1399.1501), Bit/dim 3.6362(best: 3.6192), Xent 0.6753, Loss 3.9738, Error 0.2082(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11780 | Time 25.7980(25.4462) | Bit/dim 3.6272(3.6327) | Xent 0.3235(0.3067) | Loss 8.7735(9.5071) | Error 0.1211(0.1089) Steps 904(926.06) | Grad Norm 6.2892(8.1191) | Total Time 0.00(0.00)\n",
      "Iter 11790 | Time 25.0686(25.4001) | Bit/dim 3.6333(3.6306) | Xent 0.3451(0.3086) | Loss 8.8856(9.3225) | Error 0.1167(0.1092) Steps 934(923.59) | Grad Norm 10.1400(7.5498) | Total Time 0.00(0.00)\n",
      "Iter 11800 | Time 24.1321(25.2180) | Bit/dim 3.5935(3.6265) | Xent 0.2982(0.3075) | Loss 8.6980(9.1720) | Error 0.1078(0.1089) Steps 910(920.14) | Grad Norm 4.8729(7.2519) | Total Time 0.00(0.00)\n",
      "Iter 11810 | Time 25.4478(25.2992) | Bit/dim 3.6014(3.6243) | Xent 0.2837(0.3074) | Loss 8.6027(9.0561) | Error 0.0989(0.1081) Steps 916(922.56) | Grad Norm 4.2270(6.8549) | Total Time 0.00(0.00)\n",
      "Iter 11820 | Time 24.6451(25.4982) | Bit/dim 3.6411(3.6227) | Xent 0.2671(0.3039) | Loss 8.8708(8.9850) | Error 0.0878(0.1064) Steps 934(925.48) | Grad Norm 8.2000(6.7164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 104.5376, Epoch Time 1525.2258(1402.9323), Bit/dim 3.6258(best: 3.6192), Xent 0.6698, Loss 3.9608, Error 0.2050(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11830 | Time 27.2148(25.7791) | Bit/dim 3.6373(3.6229) | Xent 0.3281(0.3021) | Loss 8.8455(9.6468) | Error 0.1100(0.1060) Steps 940(934.07) | Grad Norm 3.2332(6.7604) | Total Time 0.00(0.00)\n",
      "Iter 11840 | Time 26.1187(26.1206) | Bit/dim 3.6515(3.6257) | Xent 0.3031(0.2994) | Loss 8.7907(9.4261) | Error 0.1111(0.1053) Steps 940(944.24) | Grad Norm 4.9051(6.5602) | Total Time 0.00(0.00)\n",
      "Iter 11850 | Time 25.5504(25.8719) | Bit/dim 3.6133(3.6221) | Xent 0.3217(0.2993) | Loss 8.8157(9.2401) | Error 0.1189(0.1056) Steps 958(936.87) | Grad Norm 7.0819(6.2586) | Total Time 0.00(0.00)\n",
      "Iter 11860 | Time 25.9555(25.8849) | Bit/dim 3.6641(3.6216) | Xent 0.2984(0.2972) | Loss 8.8081(9.1002) | Error 0.0989(0.1047) Steps 934(938.69) | Grad Norm 5.7608(6.1677) | Total Time 0.00(0.00)\n",
      "Iter 11870 | Time 27.6320(26.2181) | Bit/dim 3.5954(3.6204) | Xent 0.3328(0.2989) | Loss 8.8073(9.0190) | Error 0.1244(0.1057) Steps 1024(947.38) | Grad Norm 3.9004(6.4563) | Total Time 0.00(0.00)\n",
      "Iter 11880 | Time 27.1014(26.0522) | Bit/dim 3.6074(3.6215) | Xent 0.3375(0.2995) | Loss 8.6845(8.9532) | Error 0.1078(0.1055) Steps 934(943.01) | Grad Norm 5.3876(8.0546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 104.1941, Epoch Time 1568.5964(1407.9023), Bit/dim 3.6301(best: 3.6192), Xent 0.6790, Loss 3.9696, Error 0.2103(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11890 | Time 25.1692(25.9039) | Bit/dim 3.6043(3.6233) | Xent 0.3145(0.2981) | Loss 8.7301(9.5127) | Error 0.1078(0.1050) Steps 928(940.78) | Grad Norm 6.1020(7.7217) | Total Time 0.00(0.00)\n",
      "Iter 11900 | Time 27.8469(26.0258) | Bit/dim 3.6211(3.6213) | Xent 0.3095(0.2965) | Loss 8.8577(9.3149) | Error 0.1089(0.1047) Steps 1060(939.64) | Grad Norm 5.3577(7.1524) | Total Time 0.00(0.00)\n",
      "Iter 11910 | Time 27.9174(26.1764) | Bit/dim 3.6360(3.6205) | Xent 0.2938(0.2967) | Loss 8.8861(9.1691) | Error 0.1211(0.1049) Steps 976(943.69) | Grad Norm 4.2058(6.9673) | Total Time 0.00(0.00)\n",
      "Iter 11920 | Time 25.6229(26.4159) | Bit/dim 3.6087(3.6219) | Xent 0.2538(0.2952) | Loss 8.6697(9.0569) | Error 0.0944(0.1040) Steps 970(950.97) | Grad Norm 4.8127(6.8983) | Total Time 0.00(0.00)\n",
      "Iter 11930 | Time 26.5548(26.5028) | Bit/dim 3.5943(3.6224) | Xent 0.3792(0.3008) | Loss 8.8287(8.9843) | Error 0.1344(0.1060) Steps 952(952.16) | Grad Norm 8.8792(6.8000) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 108.6244, Epoch Time 1582.5152(1413.1407), Bit/dim 3.6332(best: 3.6192), Xent 0.6762, Loss 3.9713, Error 0.2086(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11940 | Time 27.7692(26.6639) | Bit/dim 3.6724(3.6231) | Xent 0.2955(0.2990) | Loss 8.8318(9.6350) | Error 0.0956(0.1058) Steps 904(948.65) | Grad Norm 8.0860(7.1591) | Total Time 0.00(0.00)\n",
      "Iter 11950 | Time 26.1119(26.4794) | Bit/dim 3.6421(3.6215) | Xent 0.2923(0.2982) | Loss 8.7553(9.4033) | Error 0.1078(0.1052) Steps 892(940.72) | Grad Norm 5.8481(6.9794) | Total Time 0.00(0.00)\n",
      "Iter 11960 | Time 23.8659(26.0538) | Bit/dim 3.6098(3.6212) | Xent 0.3081(0.3001) | Loss 8.7140(9.2374) | Error 0.1156(0.1063) Steps 904(933.03) | Grad Norm 6.8905(6.6581) | Total Time 0.00(0.00)\n",
      "Iter 11970 | Time 25.6389(25.6694) | Bit/dim 3.6438(3.6228) | Xent 0.2921(0.2990) | Loss 8.6983(9.1039) | Error 0.1211(0.1060) Steps 904(930.11) | Grad Norm 8.5304(6.5677) | Total Time 0.00(0.00)\n",
      "Iter 11980 | Time 24.0693(25.4637) | Bit/dim 3.6351(3.6200) | Xent 0.2814(0.2934) | Loss 8.7572(9.0063) | Error 0.1033(0.1048) Steps 916(926.33) | Grad Norm 4.2221(6.3892) | Total Time 0.00(0.00)\n",
      "Iter 11990 | Time 24.6018(25.3016) | Bit/dim 3.6279(3.6235) | Xent 0.2646(0.2905) | Loss 8.7320(8.9451) | Error 0.1000(0.1037) Steps 934(924.74) | Grad Norm 3.6195(6.1208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 103.9644, Epoch Time 1511.9776(1416.1058), Bit/dim 3.6228(best: 3.6192), Xent 0.6817, Loss 3.9637, Error 0.2047(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12000 | Time 24.1065(25.1902) | Bit/dim 3.6434(3.6238) | Xent 0.3053(0.2885) | Loss 8.7326(9.5218) | Error 0.1000(0.1022) Steps 904(923.32) | Grad Norm 4.1886(5.9242) | Total Time 0.00(0.00)\n",
      "Iter 12010 | Time 25.1350(25.1864) | Bit/dim 3.6285(3.6225) | Xent 0.2759(0.2915) | Loss 8.7830(9.3231) | Error 0.0911(0.1027) Steps 964(925.51) | Grad Norm 5.2829(5.9312) | Total Time 0.00(0.00)\n",
      "Iter 12020 | Time 26.5460(25.1530) | Bit/dim 3.6101(3.6233) | Xent 0.3191(0.2927) | Loss 8.7677(9.1871) | Error 0.0933(0.1028) Steps 898(924.60) | Grad Norm 4.3189(6.3171) | Total Time 0.00(0.00)\n",
      "Iter 12030 | Time 26.0045(25.2681) | Bit/dim 3.6152(3.6223) | Xent 0.3217(0.2948) | Loss 8.7941(9.0793) | Error 0.1078(0.1031) Steps 970(929.30) | Grad Norm 5.6395(6.9082) | Total Time 0.00(0.00)\n",
      "Iter 12040 | Time 26.3191(25.3918) | Bit/dim 3.6180(3.6199) | Xent 0.3080(0.2962) | Loss 8.8783(8.9979) | Error 0.1022(0.1035) Steps 886(929.43) | Grad Norm 6.4718(6.5014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 103.4602, Epoch Time 1513.9224(1419.0403), Bit/dim 3.6239(best: 3.6192), Xent 0.6806, Loss 3.9642, Error 0.2100(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12050 | Time 24.3767(25.3024) | Bit/dim 3.5856(3.6176) | Xent 0.3360(0.2977) | Loss 8.6906(9.6421) | Error 0.1100(0.1041) Steps 892(928.92) | Grad Norm 5.1037(6.2809) | Total Time 0.00(0.00)\n",
      "Iter 12060 | Time 24.1238(25.0772) | Bit/dim 3.6094(3.6194) | Xent 0.3086(0.2984) | Loss 8.6572(9.4147) | Error 0.1178(0.1050) Steps 910(924.75) | Grad Norm 3.6209(6.2510) | Total Time 0.00(0.00)\n",
      "Iter 12070 | Time 23.5754(24.8019) | Bit/dim 3.6245(3.6193) | Xent 0.2827(0.2981) | Loss 8.7071(9.2341) | Error 0.0944(0.1052) Steps 886(919.42) | Grad Norm 8.4777(6.3278) | Total Time 0.00(0.00)\n",
      "Iter 12080 | Time 24.4547(24.5999) | Bit/dim 3.5850(3.6153) | Xent 0.3008(0.2967) | Loss 8.7017(9.0983) | Error 0.1100(0.1048) Steps 904(913.93) | Grad Norm 6.9367(6.4717) | Total Time 0.00(0.00)\n",
      "Iter 12090 | Time 24.5828(24.5487) | Bit/dim 3.6027(3.6152) | Xent 0.2468(0.2907) | Loss 8.7260(8.9865) | Error 0.0833(0.1030) Steps 934(915.70) | Grad Norm 3.6846(6.2624) | Total Time 0.00(0.00)\n",
      "Iter 12100 | Time 23.7067(24.4371) | Bit/dim 3.6125(3.6171) | Xent 0.3106(0.2921) | Loss 8.6792(8.9095) | Error 0.1200(0.1043) Steps 916(914.86) | Grad Norm 6.8353(6.0107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 101.6619, Epoch Time 1453.8028(1420.0831), Bit/dim 3.6230(best: 3.6192), Xent 0.6856, Loss 3.9658, Error 0.2107(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12110 | Time 24.2189(24.3805) | Bit/dim 3.5917(3.6167) | Xent 0.2597(0.2878) | Loss 8.7094(9.4613) | Error 0.1011(0.1033) Steps 922(914.10) | Grad Norm 6.1585(5.8274) | Total Time 0.00(0.00)\n",
      "Iter 12120 | Time 24.9934(24.3063) | Bit/dim 3.6236(3.6174) | Xent 0.2838(0.2853) | Loss 8.6773(9.2530) | Error 0.0933(0.1027) Steps 916(910.18) | Grad Norm 3.3412(5.6280) | Total Time 0.00(0.00)\n",
      "Iter 12130 | Time 24.8605(24.3894) | Bit/dim 3.6231(3.6180) | Xent 0.2284(0.2840) | Loss 8.7327(9.1160) | Error 0.0811(0.1016) Steps 952(912.06) | Grad Norm 3.9371(5.2216) | Total Time 0.00(0.00)\n",
      "Iter 12140 | Time 26.1124(24.5068) | Bit/dim 3.5997(3.6174) | Xent 0.3662(0.2858) | Loss 8.7722(9.0237) | Error 0.1256(0.1023) Steps 946(914.36) | Grad Norm 5.2805(5.0028) | Total Time 0.00(0.00)\n",
      "Iter 12150 | Time 25.2238(24.5980) | Bit/dim 3.5931(3.6167) | Xent 0.3021(0.2849) | Loss 8.6187(8.9443) | Error 0.1156(0.1019) Steps 880(914.46) | Grad Norm 4.8248(5.0796) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 103.6934, Epoch Time 1473.1048(1421.6738), Bit/dim 3.6214(best: 3.6192), Xent 0.6852, Loss 3.9641, Error 0.2080(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12160 | Time 26.3841(24.8574) | Bit/dim 3.6482(3.6186) | Xent 0.3316(0.2879) | Loss 8.7721(9.6197) | Error 0.1222(0.1027) Steps 970(916.51) | Grad Norm 22.2374(8.2528) | Total Time 0.00(0.00)\n",
      "Iter 12170 | Time 26.2072(25.6736) | Bit/dim 3.6447(3.6308) | Xent 0.3164(0.3018) | Loss 8.9509(9.4380) | Error 0.1078(0.1076) Steps 928(934.14) | Grad Norm 8.0764(10.1601) | Total Time 0.00(0.00)\n",
      "Iter 12180 | Time 23.6530(25.3980) | Bit/dim 3.6529(3.6327) | Xent 0.2692(0.3076) | Loss 8.7901(9.2715) | Error 0.1067(0.1099) Steps 886(930.78) | Grad Norm 5.7234(11.6247) | Total Time 0.00(0.00)\n",
      "Iter 12190 | Time 23.5843(24.9778) | Bit/dim 3.6505(3.6314) | Xent 0.2846(0.3098) | Loss 8.8048(9.1434) | Error 0.0911(0.1102) Steps 904(922.71) | Grad Norm 7.6639(10.8915) | Total Time 0.00(0.00)\n",
      "Iter 12200 | Time 25.7107(24.7136) | Bit/dim 3.6628(3.6258) | Xent 0.3173(0.3054) | Loss 8.7800(9.0317) | Error 0.1100(0.1091) Steps 934(916.19) | Grad Norm 5.1438(9.7315) | Total Time 0.00(0.00)\n",
      "Iter 12210 | Time 24.8942(24.4784) | Bit/dim 3.6073(3.6245) | Xent 0.3233(0.3026) | Loss 8.7263(8.9580) | Error 0.1144(0.1079) Steps 910(915.09) | Grad Norm 5.8725(8.8759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 102.1644, Epoch Time 1494.1634(1423.8485), Bit/dim 3.6234(best: 3.6192), Xent 0.6814, Loss 3.9641, Error 0.2075(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12220 | Time 23.9874(24.3672) | Bit/dim 3.6093(3.6215) | Xent 0.3019(0.2990) | Loss 8.6694(9.5230) | Error 0.1022(0.1058) Steps 904(915.76) | Grad Norm 3.6260(7.8041) | Total Time 0.00(0.00)\n",
      "Iter 12230 | Time 23.5808(24.2288) | Bit/dim 3.6365(3.6214) | Xent 0.3082(0.2953) | Loss 8.8485(9.3113) | Error 0.1178(0.1043) Steps 910(915.09) | Grad Norm 4.9363(7.4834) | Total Time 0.00(0.00)\n",
      "Iter 12240 | Time 23.9650(24.1879) | Bit/dim 3.6296(3.6216) | Xent 0.2627(0.2950) | Loss 8.8522(9.1687) | Error 0.0944(0.1050) Steps 916(912.18) | Grad Norm 7.3687(7.6823) | Total Time 0.00(0.00)\n",
      "Iter 12250 | Time 23.8424(24.1204) | Bit/dim 3.5862(3.6189) | Xent 0.2650(0.2903) | Loss 8.7283(9.0586) | Error 0.0911(0.1025) Steps 904(910.97) | Grad Norm 9.6539(7.5460) | Total Time 0.00(0.00)\n",
      "Iter 12260 | Time 25.4043(24.1499) | Bit/dim 3.6252(3.6190) | Xent 0.3169(0.2896) | Loss 8.7394(8.9728) | Error 0.1078(0.1021) Steps 922(909.45) | Grad Norm 4.4861(6.9624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 103.0449, Epoch Time 1444.7575(1424.4757), Bit/dim 3.6207(best: 3.6192), Xent 0.6830, Loss 3.9622, Error 0.2120(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12270 | Time 24.4102(24.2179) | Bit/dim 3.6026(3.6170) | Xent 0.2841(0.2866) | Loss 8.6827(9.6271) | Error 0.1067(0.1014) Steps 898(908.71) | Grad Norm 16.7758(6.7635) | Total Time 0.00(0.00)\n",
      "Iter 12280 | Time 26.0594(24.3781) | Bit/dim 3.6118(3.6172) | Xent 0.2793(0.2855) | Loss 8.6503(9.3935) | Error 0.0956(0.1015) Steps 904(908.02) | Grad Norm 7.8081(6.7110) | Total Time 0.00(0.00)\n",
      "Iter 12290 | Time 24.7888(24.3926) | Bit/dim 3.5948(3.6150) | Xent 0.3227(0.2855) | Loss 8.6711(9.2231) | Error 0.1122(0.1014) Steps 928(910.67) | Grad Norm 7.7138(6.6998) | Total Time 0.00(0.00)\n",
      "Iter 12300 | Time 25.3384(24.4192) | Bit/dim 3.6349(3.6157) | Xent 0.3157(0.2820) | Loss 8.7630(9.1030) | Error 0.1078(0.1003) Steps 892(906.40) | Grad Norm 5.2384(6.3781) | Total Time 0.00(0.00)\n",
      "Iter 12310 | Time 24.7185(24.3950) | Bit/dim 3.6321(3.6172) | Xent 0.3212(0.2854) | Loss 8.7641(9.0066) | Error 0.1178(0.1012) Steps 928(907.73) | Grad Norm 6.0541(6.2789) | Total Time 0.00(0.00)\n",
      "Iter 12320 | Time 24.1636(24.2865) | Bit/dim 3.6276(3.6179) | Xent 0.3025(0.2872) | Loss 8.7756(8.9314) | Error 0.1011(0.1019) Steps 952(908.59) | Grad Norm 8.0439(6.4544) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 102.8547, Epoch Time 1462.7706(1425.6246), Bit/dim 3.6202(best: 3.6192), Xent 0.6965, Loss 3.9684, Error 0.2128(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12330 | Time 24.0413(24.3042) | Bit/dim 3.6483(3.6185) | Xent 0.2616(0.2845) | Loss 8.6104(9.5011) | Error 0.0889(0.1003) Steps 904(908.30) | Grad Norm 5.2097(6.5308) | Total Time 0.00(0.00)\n",
      "Iter 12340 | Time 24.3150(24.3343) | Bit/dim 3.6006(3.6168) | Xent 0.2448(0.2870) | Loss 8.4668(9.2886) | Error 0.0933(0.1011) Steps 850(906.09) | Grad Norm 5.8306(6.1403) | Total Time 0.00(0.00)\n",
      "Iter 12350 | Time 24.8596(24.2487) | Bit/dim 3.6202(3.6136) | Xent 0.2514(0.2863) | Loss 8.7237(9.1326) | Error 0.0889(0.1006) Steps 880(903.16) | Grad Norm 7.5160(5.8767) | Total Time 0.00(0.00)\n",
      "Iter 12360 | Time 23.2866(24.1278) | Bit/dim 3.6138(3.6145) | Xent 0.3399(0.2861) | Loss 8.7497(9.0260) | Error 0.1267(0.1007) Steps 928(904.44) | Grad Norm 7.1843(5.6009) | Total Time 0.00(0.00)\n",
      "Iter 12370 | Time 25.0115(24.1539) | Bit/dim 3.6286(3.6170) | Xent 0.2967(0.2855) | Loss 8.7991(8.9420) | Error 0.1011(0.0999) Steps 874(906.01) | Grad Norm 6.3431(5.4927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 102.0499, Epoch Time 1448.2302(1426.3028), Bit/dim 3.6226(best: 3.6192), Xent 0.6929, Loss 3.9690, Error 0.2084(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12380 | Time 24.4310(24.1879) | Bit/dim 3.6135(3.6163) | Xent 0.2948(0.2875) | Loss 8.7001(9.6017) | Error 0.1122(0.1011) Steps 904(905.44) | Grad Norm 4.9819(5.4296) | Total Time 0.00(0.00)\n",
      "Iter 12390 | Time 24.5085(24.2110) | Bit/dim 3.6263(3.6166) | Xent 0.2815(0.2860) | Loss 8.7552(9.3845) | Error 0.0933(0.1001) Steps 916(906.66) | Grad Norm 4.5300(5.2715) | Total Time 0.00(0.00)\n",
      "Iter 12400 | Time 24.6484(24.1236) | Bit/dim 3.6107(3.6145) | Xent 0.2987(0.2844) | Loss 8.7233(9.2095) | Error 0.1133(0.1000) Steps 898(905.74) | Grad Norm 9.1088(5.3585) | Total Time 0.00(0.00)\n",
      "Iter 12410 | Time 25.7420(24.2983) | Bit/dim 3.5723(3.6154) | Xent 0.3061(0.2847) | Loss 8.6959(9.0775) | Error 0.1133(0.1007) Steps 964(911.77) | Grad Norm 4.8175(5.2628) | Total Time 0.00(0.00)\n",
      "Iter 12420 | Time 24.8330(24.5765) | Bit/dim 3.6233(3.6156) | Xent 0.3278(0.2869) | Loss 8.7485(8.9843) | Error 0.1144(0.1008) Steps 880(917.51) | Grad Norm 6.9829(5.6992) | Total Time 0.00(0.00)\n",
      "Iter 12430 | Time 25.9493(24.8381) | Bit/dim 3.5733(3.6171) | Xent 0.3047(0.2894) | Loss 8.7158(8.9235) | Error 0.0989(0.1015) Steps 916(922.45) | Grad Norm 5.7657(5.9531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 102.1868, Epoch Time 1480.3041(1427.9228), Bit/dim 3.6188(best: 3.6192), Xent 0.6990, Loss 3.9682, Error 0.2086(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12440 | Time 26.3148(24.8805) | Bit/dim 3.6106(3.6152) | Xent 0.3373(0.2875) | Loss 8.7813(9.4807) | Error 0.1300(0.1016) Steps 910(922.23) | Grad Norm 5.3239(5.9845) | Total Time 0.00(0.00)\n",
      "Iter 12450 | Time 25.7864(25.0254) | Bit/dim 3.6404(3.6153) | Xent 0.3008(0.2841) | Loss 8.8453(9.2825) | Error 0.1033(0.0995) Steps 904(926.02) | Grad Norm 3.9763(6.0566) | Total Time 0.00(0.00)\n",
      "Iter 12460 | Time 24.0945(24.8954) | Bit/dim 3.6185(3.6142) | Xent 0.3116(0.2826) | Loss 8.8807(9.1403) | Error 0.1100(0.0988) Steps 904(920.25) | Grad Norm 3.9384(5.7115) | Total Time 0.00(0.00)\n",
      "Iter 12470 | Time 23.0829(24.8754) | Bit/dim 3.6412(3.6168) | Xent 0.2912(0.2829) | Loss 8.8289(9.0332) | Error 0.1044(0.0990) Steps 922(916.39) | Grad Norm 4.2834(5.4963) | Total Time 0.00(0.00)\n",
      "Iter 12480 | Time 24.0886(24.7502) | Bit/dim 3.6164(3.6142) | Xent 0.2956(0.2843) | Loss 8.7990(8.9535) | Error 0.1078(0.1003) Steps 886(912.36) | Grad Norm 5.6278(5.4164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 103.0748, Epoch Time 1486.6413(1429.6844), Bit/dim 3.6223(best: 3.6188), Xent 0.6865, Loss 3.9655, Error 0.2068(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12490 | Time 24.2045(24.6705) | Bit/dim 3.6260(3.6162) | Xent 0.2979(0.2840) | Loss 8.8430(9.6123) | Error 0.1078(0.1008) Steps 922(912.65) | Grad Norm 5.4818(5.5236) | Total Time 0.00(0.00)\n",
      "Iter 12500 | Time 23.7704(24.5673) | Bit/dim 3.6085(3.6153) | Xent 0.2417(0.2855) | Loss 8.6921(9.3795) | Error 0.0933(0.1009) Steps 898(914.99) | Grad Norm 5.9906(6.1966) | Total Time 0.00(0.00)\n",
      "Iter 12510 | Time 23.7540(24.4800) | Bit/dim 3.6453(3.6149) | Xent 0.3362(0.2830) | Loss 8.8277(9.2085) | Error 0.1144(0.0994) Steps 964(916.34) | Grad Norm 7.3951(5.9768) | Total Time 0.00(0.00)\n",
      "Iter 12520 | Time 23.9068(24.3838) | Bit/dim 3.6364(3.6140) | Xent 0.2402(0.2848) | Loss 8.7544(9.0829) | Error 0.0933(0.1005) Steps 886(914.10) | Grad Norm 3.7094(6.5503) | Total Time 0.00(0.00)\n",
      "Iter 12530 | Time 23.4698(24.2609) | Bit/dim 3.6326(3.6130) | Xent 0.2853(0.2852) | Loss 8.7603(8.9825) | Error 0.0944(0.1002) Steps 874(913.23) | Grad Norm 4.8149(6.3991) | Total Time 0.00(0.00)\n",
      "Iter 12540 | Time 24.3389(24.1910) | Bit/dim 3.6288(3.6161) | Xent 0.2741(0.2852) | Loss 8.6761(8.9181) | Error 0.1111(0.1009) Steps 880(912.23) | Grad Norm 6.0656(6.0541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 103.0825, Epoch Time 1448.9207(1430.2614), Bit/dim 3.6175(best: 3.6188), Xent 0.6960, Loss 3.9655, Error 0.2091(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12550 | Time 24.1626(24.0991) | Bit/dim 3.6226(3.6151) | Xent 0.3107(0.2851) | Loss 8.8301(9.4935) | Error 0.1167(0.1008) Steps 946(911.15) | Grad Norm 5.9568(6.3506) | Total Time 0.00(0.00)\n",
      "Iter 12560 | Time 24.4967(24.0607) | Bit/dim 3.6263(3.6158) | Xent 0.2996(0.2869) | Loss 8.6293(9.3003) | Error 0.1078(0.1010) Steps 886(910.68) | Grad Norm 5.2659(6.2851) | Total Time 0.00(0.00)\n",
      "Iter 12570 | Time 25.1271(24.1159) | Bit/dim 3.6240(3.6161) | Xent 0.3072(0.2865) | Loss 8.8318(9.1528) | Error 0.1222(0.1016) Steps 952(914.97) | Grad Norm 4.8301(6.1132) | Total Time 0.00(0.00)\n",
      "Iter 12580 | Time 24.4688(24.1849) | Bit/dim 3.6018(3.6156) | Xent 0.2873(0.2864) | Loss 8.7176(9.0384) | Error 0.0944(0.1016) Steps 904(911.23) | Grad Norm 6.9639(6.0819) | Total Time 0.00(0.00)\n",
      "Iter 12590 | Time 24.1122(24.2512) | Bit/dim 3.5997(3.6143) | Xent 0.2778(0.2897) | Loss 8.6132(8.9597) | Error 0.0956(0.1014) Steps 916(912.45) | Grad Norm 3.7133(6.0239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 103.2092, Epoch Time 1451.6795(1430.9040), Bit/dim 3.6183(best: 3.6175), Xent 0.6905, Loss 3.9635, Error 0.2118(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12600 | Time 24.2668(24.1967) | Bit/dim 3.6179(3.6157) | Xent 0.2631(0.2872) | Loss 8.6886(9.5826) | Error 0.0878(0.0999) Steps 892(907.82) | Grad Norm 3.8845(5.8666) | Total Time 0.00(0.00)\n",
      "Iter 12610 | Time 25.9212(24.3045) | Bit/dim 3.6129(3.6145) | Xent 0.2950(0.2847) | Loss 8.6003(9.3442) | Error 0.1200(0.0999) Steps 856(905.71) | Grad Norm 4.6761(5.7997) | Total Time 0.00(0.00)\n",
      "Iter 12620 | Time 24.2812(24.2999) | Bit/dim 3.6206(3.6158) | Xent 0.2722(0.2865) | Loss 8.7623(9.2043) | Error 0.1011(0.1009) Steps 874(902.13) | Grad Norm 5.5412(5.9891) | Total Time 0.00(0.00)\n",
      "Iter 12630 | Time 23.6764(24.3530) | Bit/dim 3.6169(3.6172) | Xent 0.3207(0.2870) | Loss 8.8469(9.0859) | Error 0.1211(0.1018) Steps 892(901.50) | Grad Norm 12.7702(6.6819) | Total Time 0.00(0.00)\n",
      "Iter 12640 | Time 25.2553(24.3282) | Bit/dim 3.6145(3.6145) | Xent 0.3391(0.2909) | Loss 8.6349(8.9840) | Error 0.1156(0.1030) Steps 964(902.45) | Grad Norm 6.1732(6.8307) | Total Time 0.00(0.00)\n",
      "Iter 12650 | Time 23.9238(24.2744) | Bit/dim 3.6191(3.6138) | Xent 0.2959(0.2885) | Loss 8.8919(8.9118) | Error 0.1022(0.1014) Steps 910(901.41) | Grad Norm 6.6211(6.4518) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 102.3882, Epoch Time 1458.2349(1431.7239), Bit/dim 3.6188(best: 3.6175), Xent 0.6862, Loss 3.9619, Error 0.2059(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12660 | Time 23.0070(24.1816) | Bit/dim 3.5680(3.6100) | Xent 0.2629(0.2881) | Loss 8.6178(9.4758) | Error 0.0944(0.1014) Steps 886(900.06) | Grad Norm 6.5173(6.1430) | Total Time 0.00(0.00)\n",
      "Iter 12670 | Time 24.1052(24.1433) | Bit/dim 3.6209(3.6102) | Xent 0.3108(0.2880) | Loss 8.7358(9.2681) | Error 0.1144(0.1014) Steps 928(900.67) | Grad Norm 5.4736(5.8514) | Total Time 0.00(0.00)\n",
      "Iter 12680 | Time 23.3122(24.0876) | Bit/dim 3.6123(3.6140) | Xent 0.2542(0.2850) | Loss 8.6011(9.1360) | Error 0.0844(0.0999) Steps 874(901.13) | Grad Norm 4.2137(5.5892) | Total Time 0.00(0.00)\n",
      "Iter 12690 | Time 24.4611(24.1374) | Bit/dim 3.6318(3.6145) | Xent 0.2750(0.2830) | Loss 8.8847(9.0328) | Error 0.1100(0.0994) Steps 934(904.80) | Grad Norm 5.5008(5.6261) | Total Time 0.00(0.00)\n",
      "Iter 12700 | Time 22.9476(24.0691) | Bit/dim 3.5676(3.6129) | Xent 0.2677(0.2818) | Loss 8.5445(8.9502) | Error 0.0833(0.0989) Steps 886(905.63) | Grad Norm 8.3748(6.3773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 101.8449, Epoch Time 1436.1766(1431.8575), Bit/dim 3.6192(best: 3.6175), Xent 0.6948, Loss 3.9666, Error 0.2067(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12710 | Time 23.6173(23.8643) | Bit/dim 3.6254(3.6148) | Xent 0.2749(0.2814) | Loss 8.6465(9.6281) | Error 0.0911(0.0984) Steps 892(902.23) | Grad Norm 5.2720(6.4382) | Total Time 0.00(0.00)\n",
      "Iter 12720 | Time 24.2014(23.8744) | Bit/dim 3.6087(3.6118) | Xent 0.3006(0.2839) | Loss 8.8567(9.3829) | Error 0.0967(0.0984) Steps 898(902.37) | Grad Norm 5.4070(6.6359) | Total Time 0.00(0.00)\n",
      "Iter 12730 | Time 24.1231(23.8340) | Bit/dim 3.5937(3.6127) | Xent 0.2759(0.2820) | Loss 8.6926(9.2078) | Error 0.1056(0.0984) Steps 922(907.19) | Grad Norm 4.5543(6.7572) | Total Time 0.00(0.00)\n",
      "Iter 12740 | Time 25.3675(23.9476) | Bit/dim 3.5784(3.6142) | Xent 0.2841(0.2814) | Loss 8.6898(9.0868) | Error 0.1011(0.0986) Steps 910(907.30) | Grad Norm 4.8052(6.2867) | Total Time 0.00(0.00)\n",
      "Iter 12750 | Time 23.9800(23.9864) | Bit/dim 3.6133(3.6129) | Xent 0.2633(0.2802) | Loss 8.7720(8.9909) | Error 0.0956(0.0990) Steps 928(905.00) | Grad Norm 5.5787(5.8056) | Total Time 0.00(0.00)\n",
      "Iter 12760 | Time 23.9634(24.1153) | Bit/dim 3.5942(3.6121) | Xent 0.2654(0.2788) | Loss 8.6968(8.9124) | Error 0.0878(0.0984) Steps 922(905.10) | Grad Norm 8.1861(5.8198) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 102.1274, Epoch Time 1442.2803(1432.1702), Bit/dim 3.6209(best: 3.6175), Xent 0.6868, Loss 3.9643, Error 0.2089(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12770 | Time 25.3337(24.1172) | Bit/dim 3.6074(3.6114) | Xent 0.3460(0.2779) | Loss 8.9635(9.4841) | Error 0.1267(0.0972) Steps 934(909.17) | Grad Norm 7.7190(5.9760) | Total Time 0.00(0.00)\n",
      "Iter 12780 | Time 24.1660(23.9011) | Bit/dim 3.6053(3.6116) | Xent 0.3146(0.2793) | Loss 8.7392(9.2725) | Error 0.0956(0.0971) Steps 898(903.68) | Grad Norm 5.0591(5.6377) | Total Time 0.00(0.00)\n",
      "Iter 12790 | Time 25.1748(23.9652) | Bit/dim 3.6309(3.6115) | Xent 0.2566(0.2792) | Loss 8.7887(9.1256) | Error 0.0878(0.0976) Steps 946(904.24) | Grad Norm 6.5360(5.6062) | Total Time 0.00(0.00)\n",
      "Iter 12800 | Time 24.5000(24.3345) | Bit/dim 3.5869(3.6117) | Xent 0.2848(0.2762) | Loss 8.7567(9.0255) | Error 0.1044(0.0974) Steps 934(915.56) | Grad Norm 5.8011(5.6000) | Total Time 0.00(0.00)\n",
      "Iter 12810 | Time 25.3334(24.5184) | Bit/dim 3.6469(3.6152) | Xent 0.3008(0.2799) | Loss 8.9254(8.9619) | Error 0.1033(0.0974) Steps 904(917.54) | Grad Norm 5.5409(5.9641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 103.1309, Epoch Time 1463.1646(1433.1000), Bit/dim 3.6162(best: 3.6175), Xent 0.6836, Loss 3.9580, Error 0.2083(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12820 | Time 24.3706(24.5451) | Bit/dim 3.5642(3.6111) | Xent 0.2545(0.2794) | Loss 8.7344(9.6037) | Error 0.0867(0.0971) Steps 904(916.57) | Grad Norm 4.8285(6.0306) | Total Time 0.00(0.00)\n",
      "Iter 12830 | Time 24.7173(24.6286) | Bit/dim 3.5825(3.6112) | Xent 0.2385(0.2778) | Loss 8.6898(9.3761) | Error 0.0811(0.0969) Steps 904(914.37) | Grad Norm 4.5024(5.9431) | Total Time 0.00(0.00)\n",
      "Iter 12840 | Time 24.4341(24.7010) | Bit/dim 3.6222(3.6125) | Xent 0.2727(0.2769) | Loss 8.7651(9.2081) | Error 0.1022(0.0969) Steps 940(918.43) | Grad Norm 4.6477(5.6426) | Total Time 0.00(0.00)\n",
      "Iter 12850 | Time 23.3835(24.7763) | Bit/dim 3.5658(3.6112) | Xent 0.2560(0.2784) | Loss 8.6021(9.0893) | Error 0.0956(0.0975) Steps 880(914.96) | Grad Norm 6.7172(5.5494) | Total Time 0.00(0.00)\n",
      "Iter 12860 | Time 25.9720(24.7053) | Bit/dim 3.5981(3.6138) | Xent 0.3043(0.2814) | Loss 8.7496(8.9980) | Error 0.1211(0.0996) Steps 982(914.95) | Grad Norm 9.4664(6.2838) | Total Time 0.00(0.00)\n",
      "Iter 12870 | Time 25.7240(24.9551) | Bit/dim 3.6276(3.6153) | Xent 0.2674(0.2792) | Loss 8.7216(8.9359) | Error 0.0933(0.0991) Steps 880(921.15) | Grad Norm 8.5351(6.4930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 104.1684, Epoch Time 1495.0139(1434.9574), Bit/dim 3.6198(best: 3.6162), Xent 0.7050, Loss 3.9722, Error 0.2068(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12880 | Time 27.1948(24.9656) | Bit/dim 3.6058(3.6134) | Xent 0.2932(0.2809) | Loss 8.7455(9.5184) | Error 0.1044(0.1003) Steps 988(923.25) | Grad Norm 5.5870(6.1161) | Total Time 0.00(0.00)\n",
      "Iter 12890 | Time 25.2980(24.9377) | Bit/dim 3.6372(3.6170) | Xent 0.2505(0.2804) | Loss 8.7254(9.3186) | Error 0.0922(0.1001) Steps 904(919.35) | Grad Norm 5.0402(6.1809) | Total Time 0.00(0.00)\n",
      "Iter 12900 | Time 24.6708(25.0461) | Bit/dim 3.6033(3.6137) | Xent 0.3135(0.2829) | Loss 8.9068(9.1623) | Error 0.1089(0.1008) Steps 928(919.08) | Grad Norm 6.5162(6.0095) | Total Time 0.00(0.00)\n",
      "Iter 12910 | Time 24.1932(24.8551) | Bit/dim 3.6303(3.6136) | Xent 0.2954(0.2830) | Loss 8.9329(9.0486) | Error 0.1078(0.1006) Steps 940(916.62) | Grad Norm 14.3827(6.4083) | Total Time 0.00(0.00)\n",
      "Iter 12920 | Time 24.4305(24.7962) | Bit/dim 3.6199(3.6130) | Xent 0.2764(0.2809) | Loss 8.6562(8.9537) | Error 0.0900(0.0995) Steps 940(911.14) | Grad Norm 16.6140(7.2021) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 103.7704, Epoch Time 1486.7869(1436.5123), Bit/dim 3.6201(best: 3.6162), Xent 0.6942, Loss 3.9672, Error 0.2074(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12930 | Time 23.8042(24.6293) | Bit/dim 3.6268(3.6161) | Xent 0.2519(0.2795) | Loss 8.7014(9.6184) | Error 0.0844(0.0994) Steps 910(911.18) | Grad Norm 5.6429(6.9642) | Total Time 0.00(0.00)\n",
      "Iter 12940 | Time 24.8731(24.7338) | Bit/dim 3.6167(3.6178) | Xent 0.3326(0.2809) | Loss 8.7812(9.3905) | Error 0.1222(0.1003) Steps 928(918.76) | Grad Norm 6.8524(6.6982) | Total Time 0.00(0.00)\n",
      "Iter 12950 | Time 23.9204(24.7718) | Bit/dim 3.5961(3.6152) | Xent 0.2349(0.2776) | Loss 8.6668(9.2103) | Error 0.0811(0.0993) Steps 904(918.63) | Grad Norm 5.7224(6.3971) | Total Time 0.00(0.00)\n",
      "Iter 12960 | Time 25.4589(24.7145) | Bit/dim 3.6248(3.6146) | Xent 0.3049(0.2801) | Loss 8.7255(9.0873) | Error 0.1133(0.0999) Steps 940(917.02) | Grad Norm 5.7348(6.0765) | Total Time 0.00(0.00)\n",
      "Iter 12970 | Time 23.8572(24.6701) | Bit/dim 3.5959(3.6119) | Xent 0.2780(0.2777) | Loss 8.8378(8.9962) | Error 0.1011(0.0988) Steps 928(916.54) | Grad Norm 10.2213(6.1394) | Total Time 0.00(0.00)\n",
      "Iter 12980 | Time 26.0329(24.9728) | Bit/dim 3.6574(3.6144) | Xent 0.2908(0.2793) | Loss 9.0261(8.9460) | Error 0.1122(0.0989) Steps 934(923.20) | Grad Norm 6.3807(6.1954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 104.6981, Epoch Time 1491.0422(1438.1482), Bit/dim 3.6194(best: 3.6162), Xent 0.6958, Loss 3.9673, Error 0.2082(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12990 | Time 25.3950(25.1042) | Bit/dim 3.6070(3.6146) | Xent 0.3138(0.2808) | Loss 8.7089(9.5094) | Error 0.1000(0.0992) Steps 880(922.36) | Grad Norm 7.1155(6.4026) | Total Time 0.00(0.00)\n",
      "Iter 13000 | Time 23.6000(24.7469) | Bit/dim 3.6460(3.6159) | Xent 0.2440(0.2773) | Loss 8.7099(9.2991) | Error 0.0867(0.0977) Steps 916(918.88) | Grad Norm 4.7147(6.1382) | Total Time 0.00(0.00)\n",
      "Iter 13010 | Time 25.9685(24.9153) | Bit/dim 3.6101(3.6142) | Xent 0.3175(0.2781) | Loss 8.7947(9.1416) | Error 0.1056(0.0986) Steps 952(918.91) | Grad Norm 4.5808(5.9887) | Total Time 0.00(0.00)\n",
      "Iter 13020 | Time 25.3001(25.0296) | Bit/dim 3.5753(3.6136) | Xent 0.2356(0.2760) | Loss 8.6873(9.0320) | Error 0.0767(0.0983) Steps 952(921.02) | Grad Norm 6.7539(6.0125) | Total Time 0.00(0.00)\n",
      "Iter 13030 | Time 24.0378(24.9387) | Bit/dim 3.5983(3.6145) | Xent 0.2294(0.2747) | Loss 8.6346(8.9512) | Error 0.0756(0.0974) Steps 904(917.49) | Grad Norm 3.3921(5.7440) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 102.2318, Epoch Time 1485.8372(1439.5789), Bit/dim 3.6178(best: 3.6162), Xent 0.6949, Loss 3.9652, Error 0.2051(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13040 | Time 24.0784(24.6088) | Bit/dim 3.6101(3.6148) | Xent 0.2985(0.2721) | Loss 8.7370(9.6113) | Error 0.1044(0.0972) Steps 922(914.20) | Grad Norm 4.8549(5.4787) | Total Time 0.00(0.00)\n",
      "Iter 13050 | Time 25.2414(24.6323) | Bit/dim 3.5931(3.6134) | Xent 0.3081(0.2743) | Loss 8.6859(9.3864) | Error 0.1022(0.0972) Steps 880(910.58) | Grad Norm 3.9849(5.4024) | Total Time 0.00(0.00)\n",
      "Iter 13060 | Time 23.0030(24.3771) | Bit/dim 3.6274(3.6127) | Xent 0.2503(0.2707) | Loss 8.7692(9.2054) | Error 0.0900(0.0963) Steps 904(906.88) | Grad Norm 6.6298(5.6414) | Total Time 0.00(0.00)\n",
      "Iter 13070 | Time 23.7054(24.2127) | Bit/dim 3.6404(3.6141) | Xent 0.2294(0.2713) | Loss 8.8564(9.0870) | Error 0.0833(0.0960) Steps 934(906.67) | Grad Norm 4.6349(5.8381) | Total Time 0.00(0.00)\n",
      "Iter 13080 | Time 23.1582(23.9979) | Bit/dim 3.6373(3.6137) | Xent 0.2733(0.2717) | Loss 8.8328(9.0079) | Error 0.0944(0.0962) Steps 892(905.13) | Grad Norm 9.5587(6.0629) | Total Time 0.00(0.00)\n",
      "Iter 13090 | Time 23.7243(23.9150) | Bit/dim 3.6393(3.6117) | Xent 0.2451(0.2748) | Loss 8.7922(8.9245) | Error 0.0856(0.0975) Steps 928(903.97) | Grad Norm 9.3153(6.4346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 102.3176, Epoch Time 1429.9325(1439.2895), Bit/dim 3.6164(best: 3.6162), Xent 0.7060, Loss 3.9694, Error 0.2096(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13100 | Time 23.3101(23.8842) | Bit/dim 3.6541(3.6090) | Xent 0.2806(0.2744) | Loss 8.9436(9.4578) | Error 0.1044(0.0979) Steps 916(903.80) | Grad Norm 8.2102(6.4923) | Total Time 0.00(0.00)\n",
      "Iter 13110 | Time 24.4487(23.8978) | Bit/dim 3.6226(3.6087) | Xent 0.2758(0.2742) | Loss 8.6876(9.2600) | Error 0.0911(0.0984) Steps 892(904.30) | Grad Norm 4.0027(6.5820) | Total Time 0.00(0.00)\n",
      "Iter 13120 | Time 23.8999(23.9056) | Bit/dim 3.5847(3.6091) | Xent 0.2421(0.2720) | Loss 8.6458(9.1127) | Error 0.0856(0.0969) Steps 898(902.16) | Grad Norm 4.1939(6.5365) | Total Time 0.00(0.00)\n",
      "Iter 13130 | Time 23.6843(23.9007) | Bit/dim 3.6500(3.6117) | Xent 0.2866(0.2731) | Loss 8.9109(9.0179) | Error 0.1078(0.0969) Steps 904(903.35) | Grad Norm 6.9438(6.6035) | Total Time 0.00(0.00)\n",
      "Iter 13140 | Time 24.1816(23.8954) | Bit/dim 3.6157(3.6114) | Xent 0.3076(0.2789) | Loss 8.6344(8.9278) | Error 0.1056(0.0990) Steps 916(902.38) | Grad Norm 9.8200(7.1623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 103.2795, Epoch Time 1437.1924(1439.2266), Bit/dim 3.6206(best: 3.6162), Xent 0.6920, Loss 3.9666, Error 0.2108(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13150 | Time 25.5100(24.0022) | Bit/dim 3.5774(3.6106) | Xent 0.2777(0.2790) | Loss 8.6690(9.6026) | Error 0.1000(0.0991) Steps 934(904.16) | Grad Norm 4.3264(6.8554) | Total Time 0.00(0.00)\n",
      "Iter 13160 | Time 25.7296(24.3441) | Bit/dim 3.6498(3.6098) | Xent 0.2640(0.2768) | Loss 8.7677(9.3780) | Error 0.0933(0.0980) Steps 952(909.55) | Grad Norm 3.9514(6.8137) | Total Time 0.00(0.00)\n",
      "Iter 13170 | Time 25.5698(24.7267) | Bit/dim 3.6389(3.6125) | Xent 0.2964(0.2768) | Loss 8.7467(9.2043) | Error 0.1056(0.0980) Steps 874(918.08) | Grad Norm 6.9340(6.9840) | Total Time 0.00(0.00)\n",
      "Iter 13180 | Time 23.7287(24.7446) | Bit/dim 3.6195(3.6111) | Xent 0.2655(0.2748) | Loss 8.7963(9.0590) | Error 0.0989(0.0977) Steps 928(915.34) | Grad Norm 6.7436(6.7131) | Total Time 0.00(0.00)\n",
      "Iter 13190 | Time 24.4331(24.7683) | Bit/dim 3.5995(3.6126) | Xent 0.2657(0.2771) | Loss 8.7529(8.9790) | Error 0.0867(0.0985) Steps 970(917.84) | Grad Norm 5.6975(6.8064) | Total Time 0.00(0.00)\n",
      "Iter 13200 | Time 26.0467(24.9008) | Bit/dim 3.6069(3.6160) | Xent 0.2701(0.2774) | Loss 8.7112(8.9201) | Error 0.1067(0.0990) Steps 982(922.54) | Grad Norm 6.6291(6.6606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 104.9147, Epoch Time 1505.0157(1441.2003), Bit/dim 3.6193(best: 3.6162), Xent 0.7007, Loss 3.9697, Error 0.2094(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13210 | Time 25.1119(24.9424) | Bit/dim 3.6017(3.6130) | Xent 0.2536(0.2743) | Loss 8.6223(9.4982) | Error 0.0878(0.0979) Steps 910(924.70) | Grad Norm 5.4695(6.8341) | Total Time 0.00(0.00)\n",
      "Iter 13220 | Time 23.8126(24.7142) | Bit/dim 3.6157(3.6123) | Xent 0.2667(0.2715) | Loss 8.6479(9.2917) | Error 0.0889(0.0960) Steps 910(919.12) | Grad Norm 4.5548(6.4490) | Total Time 0.00(0.00)\n",
      "Iter 13230 | Time 23.3183(24.5023) | Bit/dim 3.6403(3.6131) | Xent 0.3045(0.2720) | Loss 8.8348(9.1372) | Error 0.1167(0.0966) Steps 868(915.75) | Grad Norm 3.7084(6.1236) | Total Time 0.00(0.00)\n",
      "Iter 13240 | Time 24.0355(24.3055) | Bit/dim 3.6233(3.6121) | Xent 0.2649(0.2745) | Loss 8.8406(9.0347) | Error 0.0978(0.0976) Steps 934(917.72) | Grad Norm 5.3016(5.8280) | Total Time 0.00(0.00)\n",
      "Iter 13250 | Time 23.6996(24.2660) | Bit/dim 3.6325(3.6151) | Xent 0.2798(0.2747) | Loss 8.6442(8.9447) | Error 0.1044(0.0977) Steps 916(915.42) | Grad Norm 4.1108(5.7194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 102.1778, Epoch Time 1455.6569(1441.6340), Bit/dim 3.6188(best: 3.6162), Xent 0.7010, Loss 3.9693, Error 0.2114(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13260 | Time 23.3164(24.3831) | Bit/dim 3.6124(3.6113) | Xent 0.2584(0.2713) | Loss 8.6140(9.5835) | Error 0.0878(0.0958) Steps 856(913.77) | Grad Norm 4.2027(5.6185) | Total Time 0.00(0.00)\n",
      "Iter 13270 | Time 25.3230(24.4177) | Bit/dim 3.6323(3.6145) | Xent 0.2757(0.2688) | Loss 8.7119(9.3595) | Error 0.0967(0.0950) Steps 958(911.87) | Grad Norm 4.2152(5.7891) | Total Time 0.00(0.00)\n",
      "Iter 13280 | Time 24.6034(24.3922) | Bit/dim 3.6128(3.6147) | Xent 0.2993(0.2709) | Loss 8.7042(9.1905) | Error 0.1144(0.0966) Steps 898(911.34) | Grad Norm 5.5705(5.9624) | Total Time 0.00(0.00)\n",
      "Iter 13290 | Time 25.2546(24.4461) | Bit/dim 3.5929(3.6142) | Xent 0.2596(0.2704) | Loss 8.6322(9.0704) | Error 0.0867(0.0963) Steps 946(914.50) | Grad Norm 3.6081(6.0324) | Total Time 0.00(0.00)\n",
      "Iter 13300 | Time 25.5969(24.6134) | Bit/dim 3.6027(3.6128) | Xent 0.2782(0.2723) | Loss 8.8078(8.9780) | Error 0.0967(0.0976) Steps 1000(924.71) | Grad Norm 8.6323(6.4761) | Total Time 0.00(0.00)\n",
      "Iter 13310 | Time 25.8502(24.7853) | Bit/dim 3.5916(3.6122) | Xent 0.2861(0.2751) | Loss 8.5597(8.9087) | Error 0.1089(0.0977) Steps 874(927.17) | Grad Norm 4.3116(6.2765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 103.5341, Epoch Time 1480.6434(1442.8042), Bit/dim 3.6172(best: 3.6162), Xent 0.7138, Loss 3.9741, Error 0.2080(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13320 | Time 25.6677(24.7372) | Bit/dim 3.6241(3.6115) | Xent 0.3189(0.2806) | Loss 8.8397(9.4947) | Error 0.1044(0.0987) Steps 964(928.05) | Grad Norm 7.8603(6.5768) | Total Time 0.00(0.00)\n",
      "Iter 13330 | Time 26.6119(24.7444) | Bit/dim 3.6142(3.6120) | Xent 0.2692(0.2788) | Loss 8.7249(9.2968) | Error 0.0856(0.0981) Steps 934(927.03) | Grad Norm 4.3249(6.3317) | Total Time 0.00(0.00)\n",
      "Iter 13340 | Time 26.4450(24.7619) | Bit/dim 3.5698(3.6125) | Xent 0.3037(0.2769) | Loss 8.6219(9.1430) | Error 0.1222(0.0978) Steps 916(923.05) | Grad Norm 6.2466(6.1460) | Total Time 0.00(0.00)\n",
      "Iter 13350 | Time 25.3232(24.7516) | Bit/dim 3.6248(3.6115) | Xent 0.2969(0.2739) | Loss 8.8572(9.0240) | Error 0.1056(0.0978) Steps 880(919.81) | Grad Norm 10.0514(6.5985) | Total Time 0.00(0.00)\n",
      "Iter 13360 | Time 24.9826(24.7617) | Bit/dim 3.6153(3.6104) | Xent 0.2693(0.2711) | Loss 8.7899(8.9308) | Error 0.0944(0.0962) Steps 910(917.03) | Grad Norm 9.0570(6.6957) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 103.2474, Epoch Time 1481.2234(1443.9568), Bit/dim 3.6137(best: 3.6162), Xent 0.7061, Loss 3.9668, Error 0.2063(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13370 | Time 25.8289(24.7957) | Bit/dim 3.5984(3.6097) | Xent 0.2375(0.2699) | Loss 8.7302(9.5973) | Error 0.0789(0.0955) Steps 952(918.00) | Grad Norm 4.2838(6.2844) | Total Time 0.00(0.00)\n",
      "Iter 13380 | Time 27.0313(24.9055) | Bit/dim 3.6010(3.6113) | Xent 0.2876(0.2673) | Loss 8.7112(9.3699) | Error 0.1011(0.0952) Steps 1036(923.74) | Grad Norm 4.0178(5.9622) | Total Time 0.00(0.00)\n",
      "Iter 13390 | Time 24.2981(24.8773) | Bit/dim 3.5672(3.6097) | Xent 0.2555(0.2674) | Loss 8.6550(9.1952) | Error 0.0878(0.0952) Steps 958(924.28) | Grad Norm 9.7620(6.6217) | Total Time 0.00(0.00)\n",
      "Iter 13400 | Time 25.7101(24.7655) | Bit/dim 3.6051(3.6103) | Xent 0.2393(0.2674) | Loss 8.6318(9.0693) | Error 0.0889(0.0957) Steps 934(922.93) | Grad Norm 6.2329(6.6924) | Total Time 0.00(0.00)\n",
      "Iter 13410 | Time 25.8908(25.0335) | Bit/dim 3.6159(3.6108) | Xent 0.2551(0.2715) | Loss 8.7714(8.9838) | Error 0.0944(0.0971) Steps 964(927.70) | Grad Norm 13.7634(6.8000) | Total Time 0.00(0.00)\n",
      "Iter 13420 | Time 24.1762(25.0089) | Bit/dim 3.6244(3.6116) | Xent 0.2330(0.2705) | Loss 8.7147(8.9222) | Error 0.0789(0.0967) Steps 880(924.81) | Grad Norm 6.6583(6.6455) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 101.3987, Epoch Time 1497.3774(1445.5594), Bit/dim 3.6203(best: 3.6137), Xent 0.7149, Loss 3.9777, Error 0.2083(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13430 | Time 25.4681(24.9243) | Bit/dim 3.5888(3.6070) | Xent 0.2693(0.2694) | Loss 8.7062(9.4577) | Error 0.0844(0.0956) Steps 958(923.92) | Grad Norm 4.5684(6.4162) | Total Time 0.00(0.00)\n",
      "Iter 13440 | Time 23.5262(24.8470) | Bit/dim 3.6450(3.6094) | Xent 0.2998(0.2674) | Loss 8.7627(9.2611) | Error 0.1167(0.0952) Steps 898(924.23) | Grad Norm 6.4322(6.6345) | Total Time 0.00(0.00)\n",
      "Iter 13450 | Time 26.2214(25.0119) | Bit/dim 3.5860(3.6094) | Xent 0.2223(0.2671) | Loss 8.6948(9.1115) | Error 0.0711(0.0948) Steps 898(924.03) | Grad Norm 4.4274(6.3781) | Total Time 0.00(0.00)\n",
      "Iter 13460 | Time 24.5656(24.9319) | Bit/dim 3.6267(3.6099) | Xent 0.2458(0.2711) | Loss 8.6961(9.0193) | Error 0.0900(0.0963) Steps 892(919.60) | Grad Norm 4.7474(6.0853) | Total Time 0.00(0.00)\n",
      "Iter 13470 | Time 25.7656(24.7983) | Bit/dim 3.5991(3.6107) | Xent 0.2889(0.2699) | Loss 8.6529(8.9316) | Error 0.1011(0.0958) Steps 964(922.14) | Grad Norm 5.4375(6.1041) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 103.5688, Epoch Time 1485.0318(1446.7436), Bit/dim 3.6166(best: 3.6137), Xent 0.7164, Loss 3.9748, Error 0.2119(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13480 | Time 25.4047(24.8207) | Bit/dim 3.5699(3.6099) | Xent 0.2500(0.2686) | Loss 8.5354(9.5971) | Error 0.0789(0.0947) Steps 964(925.08) | Grad Norm 7.0860(6.5041) | Total Time 0.00(0.00)\n",
      "Iter 13490 | Time 24.3014(24.7774) | Bit/dim 3.5892(3.6082) | Xent 0.2400(0.2669) | Loss 8.6405(9.3589) | Error 0.0822(0.0937) Steps 946(926.02) | Grad Norm 6.5441(6.8390) | Total Time 0.00(0.00)\n",
      "Iter 13500 | Time 26.5187(24.8410) | Bit/dim 3.6084(3.6101) | Xent 0.2343(0.2662) | Loss 8.6801(9.1989) | Error 0.0856(0.0939) Steps 940(926.91) | Grad Norm 6.6753(7.0796) | Total Time 0.00(0.00)\n",
      "Iter 13510 | Time 24.9051(24.8805) | Bit/dim 3.6106(3.6118) | Xent 0.2630(0.2648) | Loss 8.6798(9.0702) | Error 0.0989(0.0936) Steps 958(930.10) | Grad Norm 4.9369(6.7009) | Total Time 0.00(0.00)\n",
      "Iter 13520 | Time 25.4762(24.8485) | Bit/dim 3.6290(3.6133) | Xent 0.2534(0.2671) | Loss 8.7431(8.9851) | Error 0.0833(0.0936) Steps 946(933.15) | Grad Norm 5.4010(6.3985) | Total Time 0.00(0.00)\n",
      "Iter 13530 | Time 25.0663(24.9123) | Bit/dim 3.6213(3.6106) | Xent 0.2102(0.2651) | Loss 8.5897(8.9084) | Error 0.0744(0.0936) Steps 916(929.66) | Grad Norm 3.8571(6.0730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 103.3662, Epoch Time 1491.5185(1448.0868), Bit/dim 3.6183(best: 3.6137), Xent 0.7176, Loss 3.9772, Error 0.2112(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13540 | Time 23.8542(24.9739) | Bit/dim 3.6391(3.6131) | Xent 0.2789(0.2664) | Loss 8.8143(9.4935) | Error 0.0978(0.0942) Steps 928(931.12) | Grad Norm 4.0695(5.9649) | Total Time 0.00(0.00)\n",
      "Iter 13550 | Time 25.7729(25.0096) | Bit/dim 3.6352(3.6111) | Xent 0.2944(0.2669) | Loss 8.7309(9.2837) | Error 0.1056(0.0945) Steps 976(931.13) | Grad Norm 6.5750(6.0707) | Total Time 0.00(0.00)\n",
      "Iter 13560 | Time 25.0361(25.2212) | Bit/dim 3.6393(3.6112) | Xent 0.2756(0.2691) | Loss 8.7333(9.1333) | Error 0.0989(0.0949) Steps 910(936.01) | Grad Norm 4.4496(5.9729) | Total Time 0.00(0.00)\n",
      "Iter 13570 | Time 25.3723(25.2868) | Bit/dim 3.5957(3.6119) | Xent 0.2991(0.2696) | Loss 8.6610(9.0294) | Error 0.1222(0.0967) Steps 910(938.10) | Grad Norm 6.5472(6.0253) | Total Time 0.00(0.00)\n",
      "Iter 13580 | Time 23.6647(24.9719) | Bit/dim 3.5772(3.6097) | Xent 0.2568(0.2701) | Loss 8.6018(8.9318) | Error 0.0956(0.0967) Steps 892(930.82) | Grad Norm 6.9178(6.4138) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 103.5777, Epoch Time 1500.0612(1449.6461), Bit/dim 3.6138(best: 3.6137), Xent 0.7149, Loss 3.9712, Error 0.2100(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13590 | Time 23.8316(24.7358) | Bit/dim 3.6210(3.6077) | Xent 0.2787(0.2720) | Loss 8.7649(9.6201) | Error 0.1078(0.0975) Steps 928(927.21) | Grad Norm 9.7489(6.2816) | Total Time 0.00(0.00)\n",
      "Iter 13600 | Time 24.6780(24.6324) | Bit/dim 3.5864(3.6070) | Xent 0.2913(0.2700) | Loss 8.7924(9.3743) | Error 0.0911(0.0952) Steps 928(926.79) | Grad Norm 12.1502(6.6010) | Total Time 0.00(0.00)\n",
      "Iter 13610 | Time 26.1172(24.9860) | Bit/dim 3.6242(3.6083) | Xent 0.2712(0.2731) | Loss 8.7861(9.2092) | Error 0.1000(0.0971) Steps 1024(935.35) | Grad Norm 6.2575(6.6811) | Total Time 0.00(0.00)\n",
      "Iter 13620 | Time 25.2963(25.2455) | Bit/dim 3.6557(3.6089) | Xent 0.3103(0.2729) | Loss 8.8475(9.0776) | Error 0.1000(0.0964) Steps 940(941.87) | Grad Norm 6.3411(6.4102) | Total Time 0.00(0.00)\n",
      "Iter 13630 | Time 25.7064(25.3470) | Bit/dim 3.6214(3.6111) | Xent 0.2877(0.2702) | Loss 8.8152(8.9959) | Error 0.1000(0.0961) Steps 952(945.10) | Grad Norm 5.5630(6.2779) | Total Time 0.00(0.00)\n",
      "Iter 13640 | Time 24.0705(25.2828) | Bit/dim 3.6008(3.6104) | Xent 0.2433(0.2675) | Loss 8.7229(8.9169) | Error 0.0789(0.0946) Steps 874(936.22) | Grad Norm 3.5983(6.4034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 103.0055, Epoch Time 1510.6511(1451.4762), Bit/dim 3.6178(best: 3.6137), Xent 0.7248, Loss 3.9802, Error 0.2089(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 24.0242(25.0343) | Bit/dim 3.5797(3.6100) | Xent 0.2681(0.2645) | Loss 8.6623(9.4882) | Error 0.0967(0.0937) Steps 928(930.27) | Grad Norm 3.5796(6.0656) | Total Time 0.00(0.00)\n",
      "Iter 13660 | Time 23.4510(24.8319) | Bit/dim 3.5772(3.6069) | Xent 0.2560(0.2637) | Loss 8.5853(9.2783) | Error 0.0878(0.0928) Steps 904(927.81) | Grad Norm 7.9891(6.2018) | Total Time 0.00(0.00)\n",
      "Iter 13670 | Time 23.2048(24.6700) | Bit/dim 3.6020(3.6077) | Xent 0.2782(0.2683) | Loss 8.6003(9.1301) | Error 0.1044(0.0950) Steps 880(924.94) | Grad Norm 3.6228(6.6001) | Total Time 0.00(0.00)\n",
      "Iter 13680 | Time 24.4416(24.6834) | Bit/dim 3.6527(3.6118) | Xent 0.2726(0.2662) | Loss 8.8177(9.0246) | Error 0.0978(0.0944) Steps 922(924.64) | Grad Norm 5.4934(6.3309) | Total Time 0.00(0.00)\n",
      "Iter 13690 | Time 23.5307(24.5029) | Bit/dim 3.6211(3.6098) | Xent 0.2816(0.2679) | Loss 8.7683(8.9390) | Error 0.0978(0.0942) Steps 910(921.45) | Grad Norm 5.1320(6.0826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 102.9306, Epoch Time 1460.8994(1451.7589), Bit/dim 3.6161(best: 3.6137), Xent 0.7190, Loss 3.9756, Error 0.2129(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 22.1493(24.3986) | Bit/dim 3.6179(3.6103) | Xent 0.2370(0.2679) | Loss 8.7011(9.6190) | Error 0.0867(0.0941) Steps 892(920.77) | Grad Norm 6.7331(6.2963) | Total Time 0.00(0.00)\n",
      "Iter 13710 | Time 23.7837(24.4003) | Bit/dim 3.5832(3.6060) | Xent 0.2929(0.2682) | Loss 8.6069(9.3641) | Error 0.1100(0.0947) Steps 934(917.29) | Grad Norm 6.3203(6.4419) | Total Time 0.00(0.00)\n",
      "Iter 13720 | Time 26.9687(24.6807) | Bit/dim 3.6103(3.6091) | Xent 0.3434(0.2683) | Loss 8.8281(9.1949) | Error 0.1056(0.0936) Steps 964(920.74) | Grad Norm 8.3153(6.5283) | Total Time 0.00(0.00)\n",
      "Iter 13730 | Time 24.1706(24.8983) | Bit/dim 3.6050(3.6100) | Xent 0.2672(0.2706) | Loss 8.7805(9.0772) | Error 0.0967(0.0952) Steps 940(925.45) | Grad Norm 4.6606(6.6215) | Total Time 0.00(0.00)\n",
      "Iter 13740 | Time 23.7291(24.8200) | Bit/dim 3.6025(3.6089) | Xent 0.2630(0.2670) | Loss 8.7840(8.9839) | Error 0.0989(0.0940) Steps 928(920.98) | Grad Norm 6.1057(6.4175) | Total Time 0.00(0.00)\n",
      "Iter 13750 | Time 24.7462(24.5614) | Bit/dim 3.6125(3.6104) | Xent 0.2851(0.2706) | Loss 8.8341(8.9149) | Error 0.1022(0.0957) Steps 898(919.20) | Grad Norm 8.7415(6.6770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 102.0066, Epoch Time 1477.1543(1452.5208), Bit/dim 3.6110(best: 3.6137), Xent 0.7116, Loss 3.9668, Error 0.2132(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 24.1671(24.4784) | Bit/dim 3.6314(3.6109) | Xent 0.3075(0.2713) | Loss 8.7258(9.4938) | Error 0.1044(0.0959) Steps 964(921.04) | Grad Norm 5.7761(6.2957) | Total Time 0.00(0.00)\n",
      "Iter 13770 | Time 26.4586(24.6487) | Bit/dim 3.5952(3.6099) | Xent 0.2759(0.2695) | Loss 8.7978(9.2788) | Error 0.1044(0.0961) Steps 988(923.98) | Grad Norm 6.4902(6.0919) | Total Time 0.00(0.00)\n",
      "Iter 13780 | Time 26.9483(24.9354) | Bit/dim 3.5788(3.6079) | Xent 0.2392(0.2709) | Loss 8.5741(9.1257) | Error 0.0789(0.0960) Steps 964(928.47) | Grad Norm 6.6476(6.3306) | Total Time 0.00(0.00)\n",
      "Iter 13790 | Time 27.8825(25.1454) | Bit/dim 3.5874(3.6089) | Xent 0.2313(0.2690) | Loss 8.7231(9.0287) | Error 0.0889(0.0957) Steps 928(932.37) | Grad Norm 5.9844(6.6409) | Total Time 0.00(0.00)\n",
      "Iter 13800 | Time 24.1585(25.0191) | Bit/dim 3.5807(3.6074) | Xent 0.3153(0.2699) | Loss 8.8162(8.9446) | Error 0.1000(0.0955) Steps 922(927.43) | Grad Norm 9.0194(6.9588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 101.8648, Epoch Time 1495.1618(1453.8000), Bit/dim 3.6164(best: 3.6110), Xent 0.7176, Loss 3.9751, Error 0.2097(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 24.4801(24.8139) | Bit/dim 3.6284(3.6095) | Xent 0.2575(0.2694) | Loss 8.8105(9.6031) | Error 0.0956(0.0951) Steps 946(925.80) | Grad Norm 6.1099(6.8552) | Total Time 0.00(0.00)\n",
      "Iter 13820 | Time 24.4553(24.8728) | Bit/dim 3.6095(3.6094) | Xent 0.2980(0.2675) | Loss 8.8197(9.3699) | Error 0.0989(0.0950) Steps 886(927.86) | Grad Norm 5.8519(6.6212) | Total Time 0.00(0.00)\n",
      "Iter 13830 | Time 25.3577(24.8721) | Bit/dim 3.5933(3.6099) | Xent 0.2910(0.2703) | Loss 8.8387(9.2006) | Error 0.1089(0.0960) Steps 916(926.83) | Grad Norm 12.3466(6.6424) | Total Time 0.00(0.00)\n",
      "Iter 13840 | Time 25.7105(25.0092) | Bit/dim 3.6328(3.6113) | Xent 0.2804(0.2717) | Loss 8.8441(9.0881) | Error 0.0833(0.0956) Steps 964(925.37) | Grad Norm 6.6644(6.8945) | Total Time 0.00(0.00)\n",
      "Iter 13850 | Time 24.1961(24.9474) | Bit/dim 3.5839(3.6101) | Xent 0.2522(0.2692) | Loss 8.6555(8.9935) | Error 0.0944(0.0954) Steps 898(922.76) | Grad Norm 4.7163(7.1299) | Total Time 0.00(0.00)\n",
      "Iter 13860 | Time 24.9489(24.8509) | Bit/dim 3.6135(3.6100) | Xent 0.2651(0.2690) | Loss 8.7471(8.9210) | Error 0.0867(0.0953) Steps 940(926.07) | Grad Norm 9.8177(7.2628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 104.2909, Epoch Time 1489.7148(1454.8775), Bit/dim 3.6131(best: 3.6110), Xent 0.7150, Loss 3.9706, Error 0.2085(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.8369(24.8841) | Bit/dim 3.6170(3.6080) | Xent 0.2617(0.2672) | Loss 8.6895(9.4786) | Error 0.0944(0.0943) Steps 940(925.00) | Grad Norm 10.9857(7.2792) | Total Time 0.00(0.00)\n",
      "Iter 13880 | Time 25.5684(24.9414) | Bit/dim 3.6215(3.6076) | Xent 0.2630(0.2664) | Loss 8.7882(9.2823) | Error 0.1000(0.0932) Steps 940(924.84) | Grad Norm 8.4773(6.9035) | Total Time 0.00(0.00)\n",
      "Iter 13890 | Time 23.7883(24.8805) | Bit/dim 3.6122(3.6068) | Xent 0.2871(0.2641) | Loss 8.6692(9.1278) | Error 0.1022(0.0927) Steps 892(921.64) | Grad Norm 4.8599(6.3572) | Total Time 0.00(0.00)\n",
      "Iter 13900 | Time 25.8118(24.9989) | Bit/dim 3.6423(3.6083) | Xent 0.2683(0.2652) | Loss 8.6705(9.0183) | Error 0.0967(0.0937) Steps 892(917.60) | Grad Norm 5.0400(6.2293) | Total Time 0.00(0.00)\n",
      "Iter 13910 | Time 29.4998(25.0020) | Bit/dim 3.6967(3.6131) | Xent 0.3737(0.2727) | Loss 9.0396(8.9579) | Error 0.1311(0.0969) Steps 1042(923.74) | Grad Norm 40.6588(12.6469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 116.5339, Epoch Time 1540.4893(1457.4458), Bit/dim 3.6932(best: 3.6110), Xent 0.7960, Loss 4.0912, Error 0.2266(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 27.4113(26.1909) | Bit/dim 3.6232(3.6384) | Xent 0.3358(0.3017) | Loss 8.8278(9.7755) | Error 0.1222(0.1065) Steps 1000(958.82) | Grad Norm 9.3602(15.6759) | Total Time 0.00(0.00)\n",
      "Iter 13930 | Time 27.2890(26.5539) | Bit/dim 3.6503(3.6418) | Xent 0.3123(0.3049) | Loss 8.8712(9.5338) | Error 0.1133(0.1083) Steps 1000(962.99) | Grad Norm 8.1598(13.7700) | Total Time 0.00(0.00)\n",
      "Iter 13940 | Time 25.6760(26.4635) | Bit/dim 3.5951(3.6394) | Xent 0.2956(0.2977) | Loss 8.6815(9.3355) | Error 0.1033(0.1061) Steps 952(959.54) | Grad Norm 5.2094(11.7548) | Total Time 0.00(0.00)\n",
      "Iter 13950 | Time 26.0466(26.3896) | Bit/dim 3.6219(3.6369) | Xent 0.2727(0.2922) | Loss 8.7296(9.1870) | Error 0.0933(0.1042) Steps 922(957.57) | Grad Norm 7.1446(10.7030) | Total Time 0.00(0.00)\n",
      "Iter 13960 | Time 27.9919(26.4080) | Bit/dim 3.6322(3.6337) | Xent 0.2399(0.2845) | Loss 8.6256(9.0644) | Error 0.0867(0.1012) Steps 910(956.46) | Grad Norm 3.9574(9.2053) | Total Time 0.00(0.00)\n",
      "Iter 13970 | Time 28.4264(26.5493) | Bit/dim 3.6460(3.6282) | Xent 0.2913(0.2799) | Loss 8.8588(8.9760) | Error 0.1056(0.0997) Steps 970(957.57) | Grad Norm 8.1531(8.5943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 105.5208, Epoch Time 1598.5299(1461.6783), Bit/dim 3.6207(best: 3.6110), Xent 0.7104, Loss 3.9759, Error 0.2069(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 24.6581(26.3801) | Bit/dim 3.6146(3.6224) | Xent 0.2729(0.2793) | Loss 8.7603(9.5311) | Error 0.1011(0.0998) Steps 904(954.64) | Grad Norm 4.6231(8.0036) | Total Time 0.00(0.00)\n",
      "Iter 13990 | Time 25.7785(26.2154) | Bit/dim 3.6183(3.6191) | Xent 0.2609(0.2738) | Loss 8.8437(9.3159) | Error 0.0922(0.0975) Steps 922(946.90) | Grad Norm 4.8008(7.3169) | Total Time 0.00(0.00)\n",
      "Iter 14000 | Time 26.5973(26.3536) | Bit/dim 3.5846(3.6180) | Xent 0.2384(0.2680) | Loss 8.5364(9.1572) | Error 0.0756(0.0953) Steps 928(945.00) | Grad Norm 3.4801(6.7330) | Total Time 0.00(0.00)\n",
      "Iter 14010 | Time 26.7304(26.3148) | Bit/dim 3.6451(3.6181) | Xent 0.3238(0.2693) | Loss 8.9814(9.0632) | Error 0.1133(0.0959) Steps 970(946.24) | Grad Norm 6.8502(6.3377) | Total Time 0.00(0.00)\n",
      "Iter 14020 | Time 26.3057(26.3377) | Bit/dim 3.6244(3.6207) | Xent 0.2495(0.2693) | Loss 8.7260(8.9758) | Error 0.0900(0.0949) Steps 916(937.72) | Grad Norm 3.4085(6.1351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 105.6008, Epoch Time 1563.1199(1464.7216), Bit/dim 3.6152(best: 3.6110), Xent 0.7151, Loss 3.9727, Error 0.2062(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 27.0113(26.2752) | Bit/dim 3.6130(3.6163) | Xent 0.2655(0.2684) | Loss 8.7297(9.6313) | Error 0.1056(0.0949) Steps 982(941.03) | Grad Norm 8.7647(5.8249) | Total Time 0.00(0.00)\n",
      "Iter 14040 | Time 23.8720(26.0763) | Bit/dim 3.5636(3.6153) | Xent 0.2309(0.2619) | Loss 8.6508(9.3790) | Error 0.0756(0.0926) Steps 910(939.25) | Grad Norm 4.3098(5.7242) | Total Time 0.00(0.00)\n",
      "Iter 14050 | Time 26.4494(25.9509) | Bit/dim 3.6348(3.6165) | Xent 0.2323(0.2613) | Loss 8.7585(9.2083) | Error 0.0867(0.0935) Steps 934(941.33) | Grad Norm 4.8531(5.6408) | Total Time 0.00(0.00)\n",
      "Iter 14060 | Time 24.9498(25.5980) | Bit/dim 3.6002(3.6145) | Xent 0.2949(0.2596) | Loss 8.8027(9.0782) | Error 0.1133(0.0927) Steps 916(933.19) | Grad Norm 4.5310(5.5573) | Total Time 0.00(0.00)\n",
      "Iter 14070 | Time 24.9491(25.4349) | Bit/dim 3.6228(3.6149) | Xent 0.2518(0.2638) | Loss 8.6362(8.9790) | Error 0.0867(0.0942) Steps 898(925.32) | Grad Norm 5.2322(5.5728) | Total Time 0.00(0.00)\n",
      "Iter 14080 | Time 23.6017(25.2301) | Bit/dim 3.5880(3.6134) | Xent 0.2823(0.2671) | Loss 8.6090(8.9102) | Error 0.0978(0.0963) Steps 862(920.11) | Grad Norm 6.7571(5.8581) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 103.5129, Epoch Time 1507.6308(1466.0089), Bit/dim 3.6166(best: 3.6110), Xent 0.7173, Loss 3.9753, Error 0.2099(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 22.7576(25.0250) | Bit/dim 3.6276(3.6141) | Xent 0.2590(0.2632) | Loss 8.7389(9.4786) | Error 0.0956(0.0948) Steps 916(919.88) | Grad Norm 10.2546(6.4189) | Total Time 0.00(0.00)\n",
      "Iter 14100 | Time 23.5095(24.7629) | Bit/dim 3.5711(3.6123) | Xent 0.2601(0.2624) | Loss 8.6861(9.2753) | Error 0.0867(0.0939) Steps 904(916.69) | Grad Norm 8.0976(6.7235) | Total Time 0.00(0.00)\n",
      "Iter 14110 | Time 24.9011(24.6614) | Bit/dim 3.5943(3.6110) | Xent 0.2254(0.2635) | Loss 8.6558(9.1241) | Error 0.0922(0.0943) Steps 916(915.80) | Grad Norm 4.4970(6.7202) | Total Time 0.00(0.00)\n",
      "Iter 14120 | Time 23.8917(24.6783) | Bit/dim 3.5925(3.6107) | Xent 0.3217(0.2617) | Loss 8.7259(9.0181) | Error 0.1144(0.0934) Steps 934(916.54) | Grad Norm 8.6202(6.9657) | Total Time 0.00(0.00)\n",
      "Iter 14130 | Time 24.9419(24.6723) | Bit/dim 3.6059(3.6095) | Xent 0.2286(0.2638) | Loss 8.6010(8.9302) | Error 0.0800(0.0940) Steps 898(914.34) | Grad Norm 6.0131(6.8326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 102.4078, Epoch Time 1467.8625(1466.0645), Bit/dim 3.6158(best: 3.6110), Xent 0.7126, Loss 3.9721, Error 0.2072(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 25.2981(24.7387) | Bit/dim 3.5869(3.6124) | Xent 0.2978(0.2668) | Loss 8.5707(9.5994) | Error 0.1111(0.0942) Steps 952(914.86) | Grad Norm 10.2703(7.4723) | Total Time 0.00(0.00)\n",
      "Iter 14150 | Time 24.0706(24.7995) | Bit/dim 3.5929(3.6087) | Xent 0.2549(0.2658) | Loss 8.7717(9.3674) | Error 0.0956(0.0945) Steps 916(918.47) | Grad Norm 4.9669(7.2475) | Total Time 0.00(0.00)\n",
      "Iter 14160 | Time 23.4741(24.8744) | Bit/dim 3.6359(3.6099) | Xent 0.2384(0.2647) | Loss 8.6668(9.1962) | Error 0.0878(0.0937) Steps 868(917.98) | Grad Norm 4.0372(6.8156) | Total Time 0.00(0.00)\n",
      "Iter 14170 | Time 25.1793(24.9612) | Bit/dim 3.6168(3.6121) | Xent 0.2546(0.2667) | Loss 8.7490(9.0799) | Error 0.0967(0.0956) Steps 946(922.50) | Grad Norm 5.1974(7.0547) | Total Time 0.00(0.00)\n",
      "Iter 14180 | Time 24.4145(24.9094) | Bit/dim 3.6184(3.6105) | Xent 0.2860(0.2668) | Loss 8.7290(8.9726) | Error 0.1056(0.0952) Steps 946(923.01) | Grad Norm 4.4109(6.7716) | Total Time 0.00(0.00)\n",
      "Iter 14190 | Time 26.4874(25.0298) | Bit/dim 3.5892(3.6097) | Xent 0.2524(0.2621) | Loss 8.6701(8.8952) | Error 0.0922(0.0933) Steps 898(922.21) | Grad Norm 7.4169(6.4671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 103.3828, Epoch Time 1500.5955(1467.1004), Bit/dim 3.6149(best: 3.6110), Xent 0.7297, Loss 3.9798, Error 0.2116(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 24.5311(24.9368) | Bit/dim 3.6206(3.6084) | Xent 0.2840(0.2640) | Loss 8.8562(9.4752) | Error 0.0922(0.0936) Steps 892(926.18) | Grad Norm 9.4195(6.5265) | Total Time 0.00(0.00)\n",
      "Iter 14210 | Time 24.6454(25.0258) | Bit/dim 3.5947(3.6105) | Xent 0.2478(0.2601) | Loss 8.7529(9.2748) | Error 0.0844(0.0929) Steps 886(922.12) | Grad Norm 4.8419(6.5706) | Total Time 0.00(0.00)\n",
      "Iter 14220 | Time 24.1892(25.0198) | Bit/dim 3.6059(3.6091) | Xent 0.2565(0.2606) | Loss 8.7846(9.1270) | Error 0.0922(0.0929) Steps 898(920.76) | Grad Norm 7.4505(6.4950) | Total Time 0.00(0.00)\n",
      "Iter 14230 | Time 25.6005(24.9222) | Bit/dim 3.6514(3.6118) | Xent 0.3086(0.2622) | Loss 8.7859(9.0155) | Error 0.1122(0.0928) Steps 940(916.80) | Grad Norm 7.2684(7.7494) | Total Time 0.00(0.00)\n",
      "Iter 14240 | Time 26.0642(24.9874) | Bit/dim 3.6052(3.6093) | Xent 0.3002(0.2642) | Loss 8.7524(8.9391) | Error 0.1044(0.0937) Steps 910(918.99) | Grad Norm 7.2815(7.2559) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 109.8570, Epoch Time 1510.8025(1468.4115), Bit/dim 3.6216(best: 3.6110), Xent 0.7260, Loss 3.9846, Error 0.2084(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 26.6815(25.4923) | Bit/dim 3.6436(3.6127) | Xent 0.2525(0.2656) | Loss 8.8063(9.6210) | Error 0.0833(0.0934) Steps 880(930.31) | Grad Norm 4.1183(7.0041) | Total Time 0.00(0.00)\n",
      "Iter 14260 | Time 24.4295(25.4307) | Bit/dim 3.5953(3.6089) | Xent 0.2609(0.2625) | Loss 8.7541(9.3791) | Error 0.1078(0.0934) Steps 904(929.06) | Grad Norm 7.1488(6.9731) | Total Time 0.00(0.00)\n",
      "Iter 14270 | Time 24.1196(25.2011) | Bit/dim 3.5680(3.6064) | Xent 0.2370(0.2614) | Loss 8.5815(9.1913) | Error 0.0889(0.0928) Steps 910(928.08) | Grad Norm 6.7663(7.2093) | Total Time 0.00(0.00)\n",
      "Iter 14280 | Time 24.9679(25.1437) | Bit/dim 3.6145(3.6072) | Xent 0.2680(0.2650) | Loss 8.8167(9.0678) | Error 0.0822(0.0936) Steps 934(923.69) | Grad Norm 8.9505(7.3773) | Total Time 0.00(0.00)\n",
      "Iter 14290 | Time 26.1215(25.1518) | Bit/dim 3.6193(3.6100) | Xent 0.2260(0.2645) | Loss 8.7557(8.9752) | Error 0.0756(0.0928) Steps 898(922.61) | Grad Norm 6.1105(7.4010) | Total Time 0.00(0.00)\n",
      "Iter 14300 | Time 24.1995(24.9801) | Bit/dim 3.6013(3.6120) | Xent 0.2568(0.2651) | Loss 8.7102(8.9055) | Error 0.0978(0.0932) Steps 928(920.06) | Grad Norm 6.4253(7.2419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 102.5561, Epoch Time 1500.7962(1469.3830), Bit/dim 3.6224(best: 3.6110), Xent 0.7540, Loss 3.9994, Error 0.2124(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 25.4878(24.9793) | Bit/dim 3.6266(3.6100) | Xent 0.2871(0.2672) | Loss 8.6298(9.4517) | Error 0.1000(0.0935) Steps 880(920.72) | Grad Norm 6.8547(7.6303) | Total Time 0.00(0.00)\n",
      "Iter 14320 | Time 25.8096(24.8916) | Bit/dim 3.6125(3.6094) | Xent 0.2772(0.2657) | Loss 8.7831(9.2607) | Error 0.0956(0.0938) Steps 910(919.41) | Grad Norm 4.2579(7.5828) | Total Time 0.00(0.00)\n",
      "Iter 14330 | Time 25.0465(25.0047) | Bit/dim 3.6212(3.6120) | Xent 0.2334(0.2659) | Loss 8.6570(9.1251) | Error 0.0878(0.0939) Steps 910(919.56) | Grad Norm 4.6065(7.2198) | Total Time 0.00(0.00)\n",
      "Iter 14340 | Time 24.2717(24.8025) | Bit/dim 3.5683(3.6111) | Xent 0.3049(0.2654) | Loss 8.7253(9.0129) | Error 0.1078(0.0930) Steps 934(919.80) | Grad Norm 9.7665(7.4746) | Total Time 0.00(0.00)\n",
      "Iter 14350 | Time 23.7992(24.8151) | Bit/dim 3.5883(3.6103) | Xent 0.2515(0.2618) | Loss 8.7075(8.9310) | Error 0.0856(0.0918) Steps 880(919.86) | Grad Norm 5.6989(7.1120) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 102.6008, Epoch Time 1486.1141(1469.8849), Bit/dim 3.6134(best: 3.6110), Xent 0.7163, Loss 3.9715, Error 0.2104(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 25.8845(24.8281) | Bit/dim 3.6020(3.6110) | Xent 0.2844(0.2645) | Loss 8.6954(9.6047) | Error 0.0967(0.0929) Steps 916(917.69) | Grad Norm 6.9998(6.7630) | Total Time 0.00(0.00)\n",
      "Iter 14370 | Time 25.7559(24.6951) | Bit/dim 3.6229(3.6145) | Xent 0.2149(0.2638) | Loss 8.6853(9.3646) | Error 0.0800(0.0926) Steps 952(920.72) | Grad Norm 6.0368(6.7662) | Total Time 0.00(0.00)\n",
      "Iter 14380 | Time 24.7242(24.8216) | Bit/dim 3.6446(3.6107) | Xent 0.2551(0.2622) | Loss 8.7891(9.1885) | Error 0.0889(0.0925) Steps 934(919.17) | Grad Norm 3.3612(6.3360) | Total Time 0.00(0.00)\n",
      "Iter 14390 | Time 26.2830(24.8491) | Bit/dim 3.5914(3.6088) | Xent 0.3164(0.2631) | Loss 8.7439(9.0709) | Error 0.1089(0.0922) Steps 856(918.13) | Grad Norm 10.0102(6.3506) | Total Time 0.00(0.00)\n",
      "Iter 14400 | Time 25.4319(24.8407) | Bit/dim 3.6113(3.6100) | Xent 0.2582(0.2618) | Loss 8.7454(8.9756) | Error 0.0856(0.0910) Steps 964(919.44) | Grad Norm 5.9213(6.2141) | Total Time 0.00(0.00)\n",
      "Iter 14410 | Time 23.4920(24.7323) | Bit/dim 3.5827(3.6062) | Xent 0.2676(0.2632) | Loss 8.5889(8.8908) | Error 0.1011(0.0919) Steps 898(913.44) | Grad Norm 6.2768(6.6667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 103.4228, Epoch Time 1482.9475(1470.2768), Bit/dim 3.6143(best: 3.6110), Xent 0.7396, Loss 3.9841, Error 0.2102(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 24.3031(24.6326) | Bit/dim 3.6396(3.6077) | Xent 0.2424(0.2601) | Loss 8.7778(9.4702) | Error 0.0900(0.0913) Steps 886(910.60) | Grad Norm 5.3546(6.3814) | Total Time 0.00(0.00)\n",
      "Iter 14430 | Time 24.2013(24.5378) | Bit/dim 3.6066(3.6086) | Xent 0.2145(0.2555) | Loss 8.5777(9.2562) | Error 0.0722(0.0891) Steps 934(913.44) | Grad Norm 7.1151(6.2253) | Total Time 0.00(0.00)\n",
      "Iter 14440 | Time 23.1397(24.4605) | Bit/dim 3.6045(3.6064) | Xent 0.2591(0.2578) | Loss 8.5631(9.0929) | Error 0.1022(0.0909) Steps 904(910.60) | Grad Norm 4.9421(6.1961) | Total Time 0.00(0.00)\n",
      "Iter 14450 | Time 24.7621(24.5316) | Bit/dim 3.6076(3.6070) | Xent 0.2913(0.2571) | Loss 8.8685(8.9868) | Error 0.1122(0.0910) Steps 898(910.76) | Grad Norm 10.6700(6.1865) | Total Time 0.00(0.00)\n",
      "Iter 14460 | Time 27.2266(24.5508) | Bit/dim 3.6105(3.6078) | Xent 0.2973(0.2601) | Loss 8.8462(8.9095) | Error 0.1033(0.0917) Steps 916(910.61) | Grad Norm 6.5859(6.6044) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 102.8068, Epoch Time 1463.8049(1470.0827), Bit/dim 3.6165(best: 3.6110), Xent 0.7211, Loss 3.9770, Error 0.2112(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 24.9615(24.4830) | Bit/dim 3.6316(3.6089) | Xent 0.3185(0.2627) | Loss 8.9803(9.5877) | Error 0.0989(0.0925) Steps 976(913.97) | Grad Norm 11.0885(7.2432) | Total Time 0.00(0.00)\n",
      "Iter 14480 | Time 24.1526(24.4701) | Bit/dim 3.5807(3.6089) | Xent 0.2339(0.2605) | Loss 8.5684(9.3513) | Error 0.0889(0.0925) Steps 964(913.56) | Grad Norm 4.8114(7.3190) | Total Time 0.00(0.00)\n",
      "Iter 14490 | Time 24.5699(24.4326) | Bit/dim 3.6079(3.6060) | Xent 0.2675(0.2586) | Loss 8.7668(9.1686) | Error 0.0911(0.0919) Steps 910(912.85) | Grad Norm 6.0775(6.8740) | Total Time 0.00(0.00)\n",
      "Iter 14500 | Time 23.0126(24.5117) | Bit/dim 3.6268(3.6076) | Xent 0.2349(0.2573) | Loss 8.7447(9.0553) | Error 0.0789(0.0906) Steps 934(916.64) | Grad Norm 9.5804(6.9750) | Total Time 0.00(0.00)\n",
      "Iter 14510 | Time 24.2388(24.4957) | Bit/dim 3.6051(3.6109) | Xent 0.2828(0.2601) | Loss 8.7732(8.9746) | Error 0.1033(0.0922) Steps 922(916.56) | Grad Norm 4.2751(6.6819) | Total Time 0.00(0.00)\n",
      "Iter 14520 | Time 24.5067(24.5045) | Bit/dim 3.5868(3.6063) | Xent 0.2294(0.2587) | Loss 8.5885(8.8947) | Error 0.0756(0.0919) Steps 898(917.67) | Grad Norm 7.7875(6.5603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 102.8260, Epoch Time 1469.6718(1470.0703), Bit/dim 3.6162(best: 3.6110), Xent 0.7390, Loss 3.9857, Error 0.2101(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 24.2466(24.6330) | Bit/dim 3.5835(3.6059) | Xent 0.2448(0.2556) | Loss 8.4591(9.4440) | Error 0.0922(0.0911) Steps 904(914.94) | Grad Norm 8.6276(6.6685) | Total Time 0.00(0.00)\n",
      "Iter 14540 | Time 24.1798(24.5091) | Bit/dim 3.6151(3.6063) | Xent 0.2230(0.2570) | Loss 8.7189(9.2427) | Error 0.0778(0.0917) Steps 910(914.14) | Grad Norm 4.7194(6.1492) | Total Time 0.00(0.00)\n",
      "Iter 14550 | Time 24.2981(24.4706) | Bit/dim 3.6407(3.6083) | Xent 0.2581(0.2572) | Loss 8.7425(9.1029) | Error 0.0867(0.0912) Steps 886(912.45) | Grad Norm 11.8642(6.1334) | Total Time 0.00(0.00)\n",
      "Iter 14560 | Time 25.6423(24.5201) | Bit/dim 3.6273(3.6098) | Xent 0.2298(0.2546) | Loss 8.7945(8.9923) | Error 0.0867(0.0911) Steps 982(914.54) | Grad Norm 3.7626(6.2424) | Total Time 0.00(0.00)\n",
      "Iter 14570 | Time 25.8679(24.5365) | Bit/dim 3.5934(3.6056) | Xent 0.1987(0.2556) | Loss 8.7279(8.9207) | Error 0.0644(0.0908) Steps 898(914.74) | Grad Norm 6.5958(6.0273) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 101.8037, Epoch Time 1468.8162(1470.0327), Bit/dim 3.6157(best: 3.6110), Xent 0.7326, Loss 3.9820, Error 0.2116(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 24.8963(24.5274) | Bit/dim 3.6178(3.6065) | Xent 0.2656(0.2546) | Loss 8.8844(9.5971) | Error 0.0900(0.0896) Steps 904(913.96) | Grad Norm 6.1951(6.2120) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 23.8748(24.4534) | Bit/dim 3.6228(3.6074) | Xent 0.2483(0.2547) | Loss 8.7204(9.3616) | Error 0.0856(0.0896) Steps 886(914.40) | Grad Norm 4.6894(6.1752) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 27.0710(24.4938) | Bit/dim 3.5967(3.6085) | Xent 0.2711(0.2549) | Loss 8.7105(9.1902) | Error 0.1056(0.0902) Steps 880(911.49) | Grad Norm 6.9429(6.3893) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 24.2008(24.6709) | Bit/dim 3.6098(3.6081) | Xent 0.2387(0.2588) | Loss 8.5575(9.0636) | Error 0.0878(0.0913) Steps 892(916.87) | Grad Norm 9.3300(7.1704) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 23.9638(24.6970) | Bit/dim 3.6291(3.6071) | Xent 0.2619(0.2588) | Loss 8.7504(8.9634) | Error 0.1000(0.0920) Steps 898(917.20) | Grad Norm 5.8128(6.9843) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 25.6034(24.8401) | Bit/dim 3.5947(3.6083) | Xent 0.2477(0.2605) | Loss 8.7833(8.9122) | Error 0.0911(0.0920) Steps 964(920.82) | Grad Norm 6.8649(7.2934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 102.8035, Epoch Time 1485.4579(1470.4955), Bit/dim 3.6127(best: 3.6110), Xent 0.7345, Loss 3.9799, Error 0.2101(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 24.5888(24.9135) | Bit/dim 3.6157(3.6100) | Xent 0.2553(0.2581) | Loss 8.7383(9.4602) | Error 0.0922(0.0916) Steps 916(922.56) | Grad Norm 6.9374(7.1055) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 26.6966(25.0065) | Bit/dim 3.5982(3.6082) | Xent 0.2470(0.2593) | Loss 8.6970(9.2649) | Error 0.0944(0.0928) Steps 874(917.54) | Grad Norm 6.5098(6.7804) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 23.7622(24.9728) | Bit/dim 3.6169(3.6097) | Xent 0.2311(0.2613) | Loss 8.7576(9.1262) | Error 0.0756(0.0928) Steps 934(919.29) | Grad Norm 9.0315(6.7510) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 24.0292(24.9342) | Bit/dim 3.5748(3.6077) | Xent 0.2676(0.2596) | Loss 8.6415(9.0089) | Error 0.0922(0.0920) Steps 916(918.45) | Grad Norm 7.0744(6.3929) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 26.6386(25.0683) | Bit/dim 3.6139(3.6077) | Xent 0.1973(0.2589) | Loss 8.6411(8.9194) | Error 0.0800(0.0920) Steps 952(914.99) | Grad Norm 4.3429(5.9830) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 104.7394, Epoch Time 1507.3691(1471.6017), Bit/dim 3.6153(best: 3.6110), Xent 0.7322, Loss 3.9814, Error 0.2090(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 24.0255(25.1499) | Bit/dim 3.5817(3.6075) | Xent 0.2023(0.2593) | Loss 8.5509(9.5923) | Error 0.0744(0.0919) Steps 844(921.50) | Grad Norm 7.8100(5.8748) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 25.4739(25.3300) | Bit/dim 3.6078(3.6078) | Xent 0.2694(0.2538) | Loss 8.8202(9.3581) | Error 0.0911(0.0900) Steps 910(926.22) | Grad Norm 7.0120(6.0896) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 24.1272(25.2881) | Bit/dim 3.5805(3.6064) | Xent 0.2359(0.2550) | Loss 8.6038(9.1873) | Error 0.0889(0.0906) Steps 910(923.33) | Grad Norm 4.1800(5.9361) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 25.4603(25.0982) | Bit/dim 3.6028(3.6061) | Xent 0.2076(0.2561) | Loss 8.6567(9.0553) | Error 0.0711(0.0900) Steps 994(923.15) | Grad Norm 4.2719(5.8271) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 24.7197(25.0297) | Bit/dim 3.5713(3.6063) | Xent 0.2420(0.2515) | Loss 8.7599(8.9584) | Error 0.0856(0.0883) Steps 970(919.02) | Grad Norm 3.4118(5.7804) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 24.9158(24.8356) | Bit/dim 3.6163(3.6078) | Xent 0.2391(0.2529) | Loss 8.7304(8.8997) | Error 0.0867(0.0886) Steps 970(918.74) | Grad Norm 4.6078(5.7168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 103.4611, Epoch Time 1494.7528(1472.2962), Bit/dim 3.6114(best: 3.6110), Xent 0.7353, Loss 3.9791, Error 0.2079(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 24.1493(24.7373) | Bit/dim 3.5890(3.6043) | Xent 0.2054(0.2467) | Loss 8.6120(9.4532) | Error 0.0811(0.0864) Steps 844(916.34) | Grad Norm 3.8847(5.6003) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 24.3792(24.6562) | Bit/dim 3.5975(3.6055) | Xent 0.2750(0.2494) | Loss 8.7434(9.2513) | Error 0.0956(0.0876) Steps 874(913.24) | Grad Norm 6.2159(5.8680) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 23.4669(24.5553) | Bit/dim 3.6139(3.6063) | Xent 0.2913(0.2481) | Loss 8.7753(9.1074) | Error 0.1022(0.0878) Steps 886(914.72) | Grad Norm 6.0494(6.1396) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 23.5678(24.5332) | Bit/dim 3.5804(3.6073) | Xent 0.2795(0.2533) | Loss 8.5845(9.0049) | Error 0.0978(0.0890) Steps 934(913.57) | Grad Norm 8.4219(6.4783) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 24.9528(24.6501) | Bit/dim 3.5809(3.6077) | Xent 0.2504(0.2508) | Loss 8.7105(8.9348) | Error 0.0833(0.0872) Steps 940(922.39) | Grad Norm 5.3521(6.3442) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 106.0079, Epoch Time 1477.1826(1472.4428), Bit/dim 3.6126(best: 3.6110), Xent 0.7418, Loss 3.9835, Error 0.2092(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 25.7252(24.7848) | Bit/dim 3.5787(3.6074) | Xent 0.2493(0.2519) | Loss 8.6570(9.6027) | Error 0.0867(0.0879) Steps 898(922.83) | Grad Norm 7.3176(6.2109) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 23.8780(24.9459) | Bit/dim 3.5801(3.6067) | Xent 0.2602(0.2536) | Loss 8.6176(9.3662) | Error 0.0956(0.0893) Steps 916(929.36) | Grad Norm 6.9227(6.6790) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 24.2616(25.0783) | Bit/dim 3.6012(3.6069) | Xent 0.2342(0.2560) | Loss 8.6064(9.1907) | Error 0.0833(0.0903) Steps 916(931.02) | Grad Norm 10.0518(7.1167) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 25.4179(25.0290) | Bit/dim 3.6048(3.6044) | Xent 0.2643(0.2527) | Loss 8.7023(9.0503) | Error 0.0789(0.0886) Steps 892(922.58) | Grad Norm 6.1273(7.2892) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 24.8051(25.0096) | Bit/dim 3.6382(3.6058) | Xent 0.2199(0.2550) | Loss 8.7669(8.9580) | Error 0.0844(0.0900) Steps 916(926.66) | Grad Norm 5.6393(7.2676) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 25.7672(25.1375) | Bit/dim 3.5955(3.6058) | Xent 0.2613(0.2561) | Loss 8.5474(8.8888) | Error 0.0911(0.0910) Steps 952(928.48) | Grad Norm 8.6174(7.0843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 105.9885, Epoch Time 1511.8025(1473.6236), Bit/dim 3.6170(best: 3.6110), Xent 0.7382, Loss 3.9861, Error 0.2080(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 24.7535(25.1536) | Bit/dim 3.5683(3.6052) | Xent 0.2962(0.2564) | Loss 8.6827(9.4601) | Error 0.0978(0.0908) Steps 862(923.75) | Grad Norm 6.2092(6.7639) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 24.1259(24.8770) | Bit/dim 3.6033(3.6060) | Xent 0.2729(0.2532) | Loss 8.6728(9.2646) | Error 0.1033(0.0893) Steps 880(921.26) | Grad Norm 7.4041(6.6314) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 22.5922(24.6021) | Bit/dim 3.6168(3.6080) | Xent 0.2266(0.2510) | Loss 8.6330(9.1266) | Error 0.0744(0.0884) Steps 874(918.48) | Grad Norm 4.5835(6.3500) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 23.4838(24.4397) | Bit/dim 3.6018(3.6078) | Xent 0.2584(0.2495) | Loss 8.7635(9.0164) | Error 0.0922(0.0876) Steps 916(914.14) | Grad Norm 5.7472(6.1410) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 24.6753(24.5006) | Bit/dim 3.6249(3.6084) | Xent 0.2678(0.2505) | Loss 8.7666(8.9401) | Error 0.0900(0.0880) Steps 886(914.29) | Grad Norm 7.7897(6.1211) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 102.5751, Epoch Time 1464.3647(1473.3458), Bit/dim 3.6125(best: 3.6110), Xent 0.7313, Loss 3.9781, Error 0.2129(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 23.8808(24.5618) | Bit/dim 3.6055(3.6099) | Xent 0.2440(0.2489) | Loss 8.6480(9.5873) | Error 0.0789(0.0875) Steps 928(911.60) | Grad Norm 5.5117(6.1298) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 23.8396(24.5623) | Bit/dim 3.5758(3.6099) | Xent 0.2368(0.2467) | Loss 8.6890(9.3586) | Error 0.0844(0.0876) Steps 892(916.46) | Grad Norm 8.7604(6.0533) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 24.0347(24.5051) | Bit/dim 3.5929(3.6080) | Xent 0.2111(0.2450) | Loss 8.6310(9.1769) | Error 0.0756(0.0864) Steps 886(917.81) | Grad Norm 4.0174(6.0011) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 25.6749(24.8130) | Bit/dim 3.6139(3.6092) | Xent 0.2390(0.2439) | Loss 8.7298(9.0609) | Error 0.0900(0.0858) Steps 1006(926.52) | Grad Norm 4.2026(5.9365) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 28.4239(25.2113) | Bit/dim 3.6216(3.6064) | Xent 0.2244(0.2462) | Loss 8.8389(8.9741) | Error 0.0778(0.0863) Steps 1006(933.20) | Grad Norm 5.4235(6.0168) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 24.0979(25.2148) | Bit/dim 3.6234(3.6061) | Xent 0.2708(0.2506) | Loss 8.7213(8.9016) | Error 0.0989(0.0880) Steps 928(929.01) | Grad Norm 6.2701(8.4817) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 102.6662, Epoch Time 1505.1347(1474.2995), Bit/dim 3.6237(best: 3.6110), Xent 0.7748, Loss 4.0111, Error 0.2149(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 23.2615(24.7229) | Bit/dim 3.6162(3.6079) | Xent 0.2885(0.2567) | Loss 8.6862(9.4907) | Error 0.1111(0.0911) Steps 922(919.93) | Grad Norm 9.2738(9.0914) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 24.0188(24.3642) | Bit/dim 3.6214(3.6068) | Xent 0.3234(0.2622) | Loss 8.7857(9.2864) | Error 0.1144(0.0926) Steps 904(914.95) | Grad Norm 19.7401(11.0239) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 22.9840(23.9813) | Bit/dim 3.6062(3.6119) | Xent 0.2790(0.2684) | Loss 8.8035(9.1483) | Error 0.1000(0.0952) Steps 934(909.74) | Grad Norm 8.0897(10.7364) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 23.3040(23.7773) | Bit/dim 3.6032(3.6135) | Xent 0.2481(0.2700) | Loss 8.7176(9.0321) | Error 0.0956(0.0964) Steps 868(899.64) | Grad Norm 8.2920(10.0968) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 23.5956(23.6493) | Bit/dim 3.6405(3.6133) | Xent 0.2490(0.2645) | Loss 8.7491(8.9418) | Error 0.0844(0.0934) Steps 898(898.19) | Grad Norm 4.5789(8.8882) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 101.4605, Epoch Time 1398.9850(1472.0401), Bit/dim 3.6147(best: 3.6110), Xent 0.7337, Loss 3.9815, Error 0.2112(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 23.1494(23.6142) | Bit/dim 3.5937(3.6115) | Xent 0.2627(0.2625) | Loss 8.6175(9.6159) | Error 0.0933(0.0922) Steps 886(896.33) | Grad Norm 5.0953(7.8771) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 22.9798(23.5448) | Bit/dim 3.6111(3.6100) | Xent 0.2413(0.2569) | Loss 8.5104(9.3657) | Error 0.0900(0.0908) Steps 862(894.78) | Grad Norm 6.5767(7.3149) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 22.8376(23.5773) | Bit/dim 3.6334(3.6092) | Xent 0.2138(0.2544) | Loss 8.7537(9.1868) | Error 0.0800(0.0900) Steps 922(895.66) | Grad Norm 6.0614(7.1391) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 24.1446(23.4927) | Bit/dim 3.6052(3.6109) | Xent 0.2693(0.2572) | Loss 8.7055(9.0561) | Error 0.0989(0.0913) Steps 880(891.85) | Grad Norm 8.2999(7.5964) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 23.2884(23.4252) | Bit/dim 3.6034(3.6089) | Xent 0.2595(0.2581) | Loss 8.6410(8.9604) | Error 0.0956(0.0916) Steps 922(892.57) | Grad Norm 5.5952(7.7369) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 24.8513(23.4836) | Bit/dim 3.6201(3.6096) | Xent 0.2913(0.2603) | Loss 8.7436(8.8904) | Error 0.1078(0.0926) Steps 868(889.57) | Grad Norm 9.3649(7.8255) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 101.5856, Epoch Time 1407.9569(1470.1176), Bit/dim 3.6123(best: 3.6110), Xent 0.7394, Loss 3.9820, Error 0.2123(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 23.3162(23.4266) | Bit/dim 3.6167(3.6113) | Xent 0.2741(0.2579) | Loss 8.6148(9.4293) | Error 0.0933(0.0909) Steps 874(886.51) | Grad Norm 5.9544(7.0595) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 23.3770(23.5462) | Bit/dim 3.6101(3.6124) | Xent 0.2215(0.2548) | Loss 8.7002(9.2480) | Error 0.0844(0.0897) Steps 868(889.45) | Grad Norm 6.6240(6.5835) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 22.5654(23.6021) | Bit/dim 3.5460(3.6104) | Xent 0.2471(0.2554) | Loss 8.4802(9.1073) | Error 0.0856(0.0902) Steps 880(893.63) | Grad Norm 7.2409(6.6577) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 24.1609(23.6823) | Bit/dim 3.6152(3.6072) | Xent 0.2138(0.2528) | Loss 8.6902(9.0005) | Error 0.0700(0.0891) Steps 910(898.40) | Grad Norm 4.0014(6.7244) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 23.5928(23.7466) | Bit/dim 3.5921(3.6051) | Xent 0.2462(0.2558) | Loss 8.7070(8.9160) | Error 0.0889(0.0910) Steps 904(897.86) | Grad Norm 7.6068(6.9240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 102.4784, Epoch Time 1428.7258(1468.8758), Bit/dim 3.6171(best: 3.6110), Xent 0.7376, Loss 3.9858, Error 0.2051(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 24.4095(23.8635) | Bit/dim 3.6091(3.6031) | Xent 0.2639(0.2560) | Loss 8.6306(9.5775) | Error 0.0911(0.0916) Steps 880(902.37) | Grad Norm 5.9035(7.1496) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 25.0117(23.9570) | Bit/dim 3.6437(3.6070) | Xent 0.2137(0.2522) | Loss 8.7510(9.3401) | Error 0.0700(0.0903) Steps 862(895.02) | Grad Norm 8.9231(8.3315) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 23.9896(23.9748) | Bit/dim 3.6148(3.6074) | Xent 0.2215(0.2510) | Loss 8.5729(9.1701) | Error 0.0800(0.0899) Steps 862(895.78) | Grad Norm 5.2685(7.8360) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 25.7550(24.0039) | Bit/dim 3.5909(3.6076) | Xent 0.2685(0.2511) | Loss 8.6994(9.0520) | Error 0.0989(0.0906) Steps 862(896.94) | Grad Norm 5.2737(7.5025) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 24.0734(24.0416) | Bit/dim 3.6495(3.6070) | Xent 0.2644(0.2528) | Loss 8.8292(8.9497) | Error 0.0867(0.0904) Steps 916(898.06) | Grad Norm 6.8037(7.3592) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 24.4417(24.1582) | Bit/dim 3.5858(3.6075) | Xent 0.2275(0.2515) | Loss 8.5369(8.8888) | Error 0.0822(0.0895) Steps 910(899.41) | Grad Norm 5.9570(7.0603) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 103.2207, Epoch Time 1452.1210(1468.3732), Bit/dim 3.6105(best: 3.6110), Xent 0.7453, Loss 3.9831, Error 0.2090(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 23.7867(24.2446) | Bit/dim 3.5868(3.6061) | Xent 0.2685(0.2501) | Loss 8.7285(9.4710) | Error 0.0911(0.0893) Steps 868(901.27) | Grad Norm 5.4554(6.6176) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 23.9407(24.1313) | Bit/dim 3.5852(3.6054) | Xent 0.2509(0.2494) | Loss 8.5352(9.2579) | Error 0.0900(0.0890) Steps 904(901.63) | Grad Norm 7.8184(6.6539) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 24.1577(24.1096) | Bit/dim 3.5900(3.6072) | Xent 0.2546(0.2496) | Loss 8.4730(9.1061) | Error 0.0922(0.0892) Steps 910(900.99) | Grad Norm 5.5160(6.5445) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 25.0277(24.2662) | Bit/dim 3.6250(3.6053) | Xent 0.2427(0.2501) | Loss 8.7577(9.0088) | Error 0.0933(0.0890) Steps 988(908.80) | Grad Norm 4.8863(6.6242) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 25.0270(24.3092) | Bit/dim 3.6354(3.6073) | Xent 0.2385(0.2494) | Loss 8.7808(8.9317) | Error 0.0889(0.0891) Steps 856(906.10) | Grad Norm 7.1065(6.7177) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 103.8348, Epoch Time 1461.9084(1468.1792), Bit/dim 3.6116(best: 3.6105), Xent 0.7445, Loss 3.9838, Error 0.2124(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 24.9942(24.3917) | Bit/dim 3.6206(3.6068) | Xent 0.2437(0.2508) | Loss 8.6679(9.6119) | Error 0.0789(0.0892) Steps 910(908.35) | Grad Norm 5.5809(6.7499) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 24.9186(24.4408) | Bit/dim 3.5993(3.6077) | Xent 0.2673(0.2495) | Loss 8.7703(9.3770) | Error 0.1044(0.0894) Steps 868(905.81) | Grad Norm 7.2066(6.5128) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 24.6164(24.4798) | Bit/dim 3.6081(3.6086) | Xent 0.2490(0.2476) | Loss 8.7056(9.1962) | Error 0.0822(0.0883) Steps 910(906.22) | Grad Norm 4.2706(6.2770) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 24.7413(24.5942) | Bit/dim 3.6516(3.6088) | Xent 0.2284(0.2490) | Loss 8.7401(9.0654) | Error 0.0878(0.0892) Steps 922(913.46) | Grad Norm 8.1980(6.4775) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 25.4183(24.5742) | Bit/dim 3.6144(3.6062) | Xent 0.2399(0.2487) | Loss 8.7622(8.9624) | Error 0.0867(0.0891) Steps 964(918.61) | Grad Norm 4.3775(6.4049) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 25.5330(24.6214) | Bit/dim 3.6151(3.6037) | Xent 0.2929(0.2494) | Loss 8.7450(8.8903) | Error 0.1078(0.0889) Steps 940(912.88) | Grad Norm 10.0855(6.5939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 104.9393, Epoch Time 1477.5670(1468.4609), Bit/dim 3.6109(best: 3.6105), Xent 0.7461, Loss 3.9839, Error 0.2110(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 22.3934(24.6116) | Bit/dim 3.6172(3.6022) | Xent 0.2908(0.2499) | Loss 8.6485(9.4414) | Error 0.1100(0.0891) Steps 898(918.14) | Grad Norm 7.3159(6.6689) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 23.6834(24.6440) | Bit/dim 3.6024(3.6026) | Xent 0.2452(0.2467) | Loss 8.6906(9.2408) | Error 0.0833(0.0879) Steps 904(915.13) | Grad Norm 10.5305(7.7291) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 26.3442(24.9362) | Bit/dim 3.6041(3.6022) | Xent 0.2665(0.2501) | Loss 8.7978(9.1066) | Error 0.0889(0.0896) Steps 952(926.97) | Grad Norm 17.1472(8.3640) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 24.3804(25.1228) | Bit/dim 3.5752(3.6051) | Xent 0.2638(0.2530) | Loss 8.6473(9.0078) | Error 0.0978(0.0898) Steps 904(926.09) | Grad Norm 4.7349(8.3257) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 23.7772(24.9907) | Bit/dim 3.6296(3.6065) | Xent 0.2256(0.2522) | Loss 8.6175(8.9184) | Error 0.0844(0.0898) Steps 904(919.94) | Grad Norm 6.7590(7.7470) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 103.8845, Epoch Time 1498.1502(1469.3515), Bit/dim 3.6099(best: 3.6105), Xent 0.7387, Loss 3.9793, Error 0.2108(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 24.9215(24.8520) | Bit/dim 3.6625(3.6083) | Xent 0.2318(0.2501) | Loss 8.7802(9.5905) | Error 0.0889(0.0890) Steps 904(917.45) | Grad Norm 6.1105(7.1403) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 23.4166(24.8015) | Bit/dim 3.6323(3.6070) | Xent 0.2366(0.2450) | Loss 8.6374(9.3430) | Error 0.0856(0.0864) Steps 874(912.86) | Grad Norm 7.7857(6.9152) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 24.5534(24.7534) | Bit/dim 3.5905(3.6045) | Xent 0.2765(0.2462) | Loss 8.6733(9.1665) | Error 0.0922(0.0875) Steps 880(910.32) | Grad Norm 6.1672(6.5973) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 23.2139(24.5838) | Bit/dim 3.5923(3.6054) | Xent 0.2704(0.2461) | Loss 8.6352(9.0494) | Error 0.0978(0.0879) Steps 910(909.81) | Grad Norm 4.3462(6.4600) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 23.3302(24.6178) | Bit/dim 3.5797(3.6039) | Xent 0.2438(0.2455) | Loss 8.6273(8.9564) | Error 0.0844(0.0873) Steps 904(910.13) | Grad Norm 3.6754(6.1572) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 23.2462(24.5004) | Bit/dim 3.6662(3.6064) | Xent 0.1822(0.2466) | Loss 8.7794(8.8971) | Error 0.0533(0.0870) Steps 916(909.03) | Grad Norm 3.5229(5.9725) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 103.1997, Epoch Time 1468.7034(1469.3321), Bit/dim 3.6124(best: 3.6099), Xent 0.7612, Loss 3.9930, Error 0.2090(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 24.5385(24.4290) | Bit/dim 3.6008(3.6054) | Xent 0.2191(0.2433) | Loss 8.7101(9.4611) | Error 0.0833(0.0868) Steps 940(909.84) | Grad Norm 5.4031(6.1266) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 23.2846(24.5480) | Bit/dim 3.6293(3.6080) | Xent 0.2388(0.2438) | Loss 8.6404(9.2656) | Error 0.0889(0.0861) Steps 886(912.16) | Grad Norm 5.6030(6.1735) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 24.8995(24.6328) | Bit/dim 3.6297(3.6052) | Xent 0.2219(0.2471) | Loss 8.7216(9.1182) | Error 0.0811(0.0871) Steps 904(916.98) | Grad Norm 4.4829(6.2695) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 23.9718(24.6363) | Bit/dim 3.6034(3.6035) | Xent 0.2374(0.2462) | Loss 8.6999(9.0023) | Error 0.0811(0.0866) Steps 898(919.14) | Grad Norm 3.9357(6.2787) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 26.3259(24.9361) | Bit/dim 3.6113(3.6034) | Xent 0.2207(0.2434) | Loss 8.7000(8.9291) | Error 0.0800(0.0856) Steps 928(922.15) | Grad Norm 6.1134(6.1683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 107.5788, Epoch Time 1497.9679(1470.1912), Bit/dim 3.6190(best: 3.6099), Xent 0.7473, Loss 3.9926, Error 0.2134(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 26.1774(25.1318) | Bit/dim 3.6572(3.6057) | Xent 0.2244(0.2448) | Loss 8.8260(9.6099) | Error 0.0689(0.0867) Steps 928(922.15) | Grad Norm 4.4240(6.1484) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 23.9085(25.1284) | Bit/dim 3.6263(3.6053) | Xent 0.2410(0.2443) | Loss 8.7990(9.3681) | Error 0.0867(0.0868) Steps 904(921.73) | Grad Norm 4.5763(6.5109) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 23.5165(24.6847) | Bit/dim 3.5974(3.6030) | Xent 0.2382(0.2455) | Loss 8.4749(9.1676) | Error 0.0789(0.0870) Steps 862(914.69) | Grad Norm 5.2061(6.3143) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 24.1766(24.4112) | Bit/dim 3.6049(3.6054) | Xent 0.2447(0.2450) | Loss 8.6003(9.0380) | Error 0.0811(0.0867) Steps 886(908.16) | Grad Norm 4.8467(5.9787) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 24.6180(24.4191) | Bit/dim 3.6210(3.6054) | Xent 0.1917(0.2445) | Loss 8.8537(8.9543) | Error 0.0722(0.0859) Steps 934(912.15) | Grad Norm 3.8997(5.9958) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 24.2902(24.5036) | Bit/dim 3.5906(3.6058) | Xent 0.2289(0.2448) | Loss 8.5916(8.8865) | Error 0.0789(0.0865) Steps 904(915.79) | Grad Norm 4.1454(5.7515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 103.4763, Epoch Time 1464.0991(1470.0084), Bit/dim 3.6060(best: 3.6099), Xent 0.7507, Loss 3.9813, Error 0.2140(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 23.2863(24.3575) | Bit/dim 3.6242(3.6033) | Xent 0.2543(0.2441) | Loss 8.6518(9.4425) | Error 0.0878(0.0862) Steps 892(911.12) | Grad Norm 5.3386(6.3665) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 27.2103(24.7052) | Bit/dim 3.6133(3.6047) | Xent 0.1963(0.2443) | Loss 8.8288(9.2489) | Error 0.0678(0.0864) Steps 994(919.62) | Grad Norm 6.4439(6.3990) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 24.5225(24.9911) | Bit/dim 3.6027(3.6049) | Xent 0.2149(0.2475) | Loss 8.5703(9.0973) | Error 0.0756(0.0881) Steps 910(925.23) | Grad Norm 4.6070(6.4368) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 27.6429(25.1366) | Bit/dim 3.6116(3.6081) | Xent 0.2478(0.2497) | Loss 8.7872(8.9983) | Error 0.0867(0.0887) Steps 952(926.54) | Grad Norm 3.9224(6.3183) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 23.8694(25.0260) | Bit/dim 3.6139(3.6092) | Xent 0.2712(0.2478) | Loss 8.8091(8.9240) | Error 0.0922(0.0883) Steps 916(922.82) | Grad Norm 7.4189(6.4863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 104.4261, Epoch Time 1503.7057(1471.0193), Bit/dim 3.6098(best: 3.6060), Xent 0.7516, Loss 3.9856, Error 0.2124(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 25.2539(25.0400) | Bit/dim 3.6080(3.6068) | Xent 0.2316(0.2456) | Loss 8.7916(9.5974) | Error 0.0844(0.0873) Steps 952(925.31) | Grad Norm 5.8436(6.4979) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 26.8141(25.1226) | Bit/dim 3.6252(3.6071) | Xent 0.2278(0.2459) | Loss 8.7638(9.3623) | Error 0.0744(0.0867) Steps 952(929.00) | Grad Norm 4.5994(6.0231) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 23.4938(24.9831) | Bit/dim 3.5856(3.6055) | Xent 0.2400(0.2442) | Loss 8.5501(9.1659) | Error 0.0878(0.0869) Steps 892(926.04) | Grad Norm 10.0535(6.3083) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 25.5118(25.1485) | Bit/dim 3.6079(3.6059) | Xent 0.2667(0.2451) | Loss 8.7573(9.0396) | Error 0.1000(0.0880) Steps 904(924.10) | Grad Norm 5.9699(6.8319) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 26.1077(24.9380) | Bit/dim 3.6678(3.6073) | Xent 0.2336(0.2458) | Loss 8.7982(8.9475) | Error 0.0800(0.0882) Steps 892(918.30) | Grad Norm 8.0464(7.0515) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 23.9531(24.6785) | Bit/dim 3.6112(3.6090) | Xent 0.2681(0.2501) | Loss 8.7221(8.8899) | Error 0.0967(0.0896) Steps 898(910.79) | Grad Norm 11.1402(7.2273) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 102.0603, Epoch Time 1484.4677(1471.4228), Bit/dim 3.6123(best: 3.6060), Xent 0.7412, Loss 3.9829, Error 0.2113(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 23.1477(24.6432) | Bit/dim 3.6143(3.6061) | Xent 0.2705(0.2483) | Loss 8.6624(9.4441) | Error 0.1044(0.0890) Steps 916(908.78) | Grad Norm 8.7691(7.0719) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 25.0948(24.6613) | Bit/dim 3.6195(3.6078) | Xent 0.2572(0.2488) | Loss 8.6996(9.2577) | Error 0.0878(0.0888) Steps 916(913.37) | Grad Norm 4.7926(7.2946) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 26.1448(24.9841) | Bit/dim 3.5664(3.6054) | Xent 0.2802(0.2512) | Loss 8.7504(9.1189) | Error 0.0944(0.0897) Steps 982(919.62) | Grad Norm 7.1794(7.8561) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 25.3956(25.2990) | Bit/dim 3.5845(3.6062) | Xent 0.2412(0.2540) | Loss 8.6068(9.0170) | Error 0.0833(0.0907) Steps 934(927.70) | Grad Norm 4.8530(8.1176) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 25.6250(25.4062) | Bit/dim 3.6054(3.6079) | Xent 0.2806(0.2555) | Loss 8.6686(8.9428) | Error 0.1067(0.0909) Steps 856(930.61) | Grad Norm 7.2789(7.5911) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 104.3906, Epoch Time 1516.5370(1472.7762), Bit/dim 3.6065(best: 3.6060), Xent 0.7452, Loss 3.9791, Error 0.2139(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 25.4841(25.2742) | Bit/dim 3.6257(3.6070) | Xent 0.2537(0.2527) | Loss 8.7214(9.6032) | Error 0.0900(0.0897) Steps 892(924.55) | Grad Norm 6.8773(7.1521) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 24.5421(25.4182) | Bit/dim 3.6052(3.6060) | Xent 0.2442(0.2489) | Loss 8.6067(9.3611) | Error 0.0967(0.0891) Steps 880(931.00) | Grad Norm 7.0084(7.0504) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 25.6768(25.3588) | Bit/dim 3.6347(3.6052) | Xent 0.1927(0.2463) | Loss 8.8241(9.1913) | Error 0.0789(0.0881) Steps 910(930.23) | Grad Norm 3.8268(6.8590) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 24.4177(25.0964) | Bit/dim 3.5957(3.6066) | Xent 0.1983(0.2422) | Loss 8.4120(9.0534) | Error 0.0667(0.0869) Steps 940(928.37) | Grad Norm 5.6870(6.5155) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 23.6013(24.8867) | Bit/dim 3.6171(3.6067) | Xent 0.2314(0.2419) | Loss 8.7655(8.9632) | Error 0.0800(0.0860) Steps 868(922.70) | Grad Norm 6.1798(6.3876) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 26.1039(24.9438) | Bit/dim 3.6028(3.6060) | Xent 0.2603(0.2445) | Loss 8.7649(8.8933) | Error 0.0989(0.0873) Steps 880(919.23) | Grad Norm 4.2161(6.2205) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 103.8114, Epoch Time 1494.2506(1473.4204), Bit/dim 3.6146(best: 3.6060), Xent 0.7423, Loss 3.9858, Error 0.2112(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 25.5648(24.7503) | Bit/dim 3.5804(3.6037) | Xent 0.2230(0.2442) | Loss 8.5518(9.4575) | Error 0.0822(0.0869) Steps 976(916.27) | Grad Norm 8.2285(6.3685) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 25.0236(24.5933) | Bit/dim 3.5840(3.6049) | Xent 0.2542(0.2399) | Loss 8.7120(9.2616) | Error 0.0967(0.0855) Steps 934(914.34) | Grad Norm 5.1309(6.3254) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 24.8011(24.5184) | Bit/dim 3.6117(3.6030) | Xent 0.2750(0.2406) | Loss 8.6839(9.1001) | Error 0.0911(0.0857) Steps 874(912.75) | Grad Norm 6.1315(6.1939) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 25.8269(24.5737) | Bit/dim 3.6487(3.6035) | Xent 0.2134(0.2406) | Loss 8.8558(8.9865) | Error 0.0767(0.0858) Steps 916(909.96) | Grad Norm 5.2001(6.0907) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 24.8160(24.5693) | Bit/dim 3.6051(3.6052) | Xent 0.2067(0.2427) | Loss 8.6822(8.9156) | Error 0.0711(0.0863) Steps 934(909.61) | Grad Norm 8.2421(6.7037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 104.8373, Epoch Time 1463.5764(1473.1251), Bit/dim 3.6183(best: 3.6060), Xent 0.7616, Loss 3.9992, Error 0.2098(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.9026(24.7116) | Bit/dim 3.6451(3.6066) | Xent 0.2155(0.2386) | Loss 8.8780(9.5773) | Error 0.0744(0.0843) Steps 934(912.19) | Grad Norm 8.2804(6.9732) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 25.7268(24.9571) | Bit/dim 3.6126(3.6072) | Xent 0.2645(0.2415) | Loss 8.5676(9.3487) | Error 0.1044(0.0854) Steps 946(919.61) | Grad Norm 6.4618(7.0911) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 25.1674(25.1480) | Bit/dim 3.6696(3.6075) | Xent 0.2242(0.2413) | Loss 8.8121(9.1751) | Error 0.0800(0.0859) Steps 916(926.88) | Grad Norm 5.2032(7.1358) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 25.0571(25.1857) | Bit/dim 3.5995(3.6034) | Xent 0.2285(0.2448) | Loss 8.6465(9.0508) | Error 0.0833(0.0881) Steps 928(928.36) | Grad Norm 13.3925(7.3646) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 24.2122(25.0598) | Bit/dim 3.6103(3.6034) | Xent 0.2278(0.2433) | Loss 8.5445(8.9322) | Error 0.0767(0.0876) Steps 856(919.90) | Grad Norm 7.3486(7.4123) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 25.5101(24.9375) | Bit/dim 3.6233(3.6033) | Xent 0.2794(0.2419) | Loss 8.6572(8.8718) | Error 0.0922(0.0861) Steps 892(918.19) | Grad Norm 5.6774(6.7762) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 103.7692, Epoch Time 1510.0267(1474.2322), Bit/dim 3.6092(best: 3.6060), Xent 0.7521, Loss 3.9853, Error 0.2109(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 23.0632(24.7911) | Bit/dim 3.6044(3.6044) | Xent 0.2246(0.2415) | Loss 8.5755(9.4503) | Error 0.0833(0.0858) Steps 892(920.15) | Grad Norm 6.9040(6.5141) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 24.8448(24.6498) | Bit/dim 3.6242(3.6030) | Xent 0.2502(0.2437) | Loss 8.7936(9.2527) | Error 0.0944(0.0869) Steps 892(915.05) | Grad Norm 11.8060(6.7104) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 24.6265(24.4751) | Bit/dim 3.5738(3.6013) | Xent 0.2293(0.2396) | Loss 8.5562(9.0873) | Error 0.0822(0.0856) Steps 934(914.80) | Grad Norm 8.1524(6.8035) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 26.1671(24.6782) | Bit/dim 3.6466(3.6018) | Xent 0.2346(0.2410) | Loss 8.8846(8.9860) | Error 0.0833(0.0857) Steps 874(913.72) | Grad Norm 7.6587(7.6033) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 23.4923(24.5076) | Bit/dim 3.6178(3.6042) | Xent 0.2659(0.2429) | Loss 8.7994(8.9120) | Error 0.0944(0.0856) Steps 910(912.77) | Grad Norm 5.0635(7.5311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 103.1604, Epoch Time 1459.8143(1473.7996), Bit/dim 3.6101(best: 3.6060), Xent 0.7619, Loss 3.9910, Error 0.2128(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 25.3018(24.4426) | Bit/dim 3.6009(3.6056) | Xent 0.2681(0.2439) | Loss 8.8361(9.5971) | Error 0.0967(0.0858) Steps 952(912.22) | Grad Norm 5.3347(7.7206) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 25.9229(24.7433) | Bit/dim 3.6113(3.6075) | Xent 0.2728(0.2425) | Loss 8.6323(9.3637) | Error 0.0967(0.0858) Steps 970(913.36) | Grad Norm 7.3937(7.7079) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 25.5457(24.7797) | Bit/dim 3.6077(3.6053) | Xent 0.2588(0.2431) | Loss 8.7986(9.1930) | Error 0.0978(0.0863) Steps 970(915.23) | Grad Norm 6.1832(7.8137) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 23.8393(24.6402) | Bit/dim 3.5931(3.6034) | Xent 0.2426(0.2405) | Loss 8.7922(9.0603) | Error 0.0811(0.0845) Steps 934(916.34) | Grad Norm 7.7608(7.6145) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 27.5332(24.6822) | Bit/dim 3.6084(3.6061) | Xent 0.2298(0.2365) | Loss 8.7250(8.9662) | Error 0.0822(0.0839) Steps 874(909.70) | Grad Norm 13.5571(7.5380) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 25.3944(24.6791) | Bit/dim 3.6117(3.6037) | Xent 0.2435(0.2378) | Loss 8.6270(8.8875) | Error 0.0844(0.0842) Steps 874(905.26) | Grad Norm 9.8177(7.2123) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 104.2710, Epoch Time 1486.3002(1474.1746), Bit/dim 3.6105(best: 3.6060), Xent 0.7751, Loss 3.9980, Error 0.2131(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 23.8013(24.6383) | Bit/dim 3.6316(3.6028) | Xent 0.2325(0.2395) | Loss 8.6887(9.4786) | Error 0.0844(0.0848) Steps 904(907.18) | Grad Norm 13.2270(9.0308) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 22.1259(24.2280) | Bit/dim 3.6175(3.6058) | Xent 0.3073(0.2461) | Loss 8.8759(9.2832) | Error 0.1078(0.0872) Steps 910(906.48) | Grad Norm 7.0456(9.3858) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 23.5972(23.9888) | Bit/dim 3.6359(3.6088) | Xent 0.2376(0.2488) | Loss 8.7479(9.1320) | Error 0.0811(0.0881) Steps 916(906.46) | Grad Norm 4.8690(8.5414) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 22.9653(23.8489) | Bit/dim 3.6160(3.6096) | Xent 0.2071(0.2458) | Loss 8.7311(9.0244) | Error 0.0756(0.0871) Steps 904(905.28) | Grad Norm 6.2222(8.1986) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 24.2159(23.8546) | Bit/dim 3.6031(3.6072) | Xent 0.2462(0.2478) | Loss 8.7123(8.9280) | Error 0.0944(0.0890) Steps 904(906.68) | Grad Norm 4.2567(8.0887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 102.4352, Epoch Time 1424.6882(1472.6900), Bit/dim 3.6113(best: 3.6060), Xent 0.7667, Loss 3.9946, Error 0.2119(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 24.6996(24.0375) | Bit/dim 3.5945(3.6061) | Xent 0.2175(0.2438) | Loss 8.7067(9.5993) | Error 0.0733(0.0866) Steps 964(908.86) | Grad Norm 7.2456(8.0330) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 24.1473(24.1188) | Bit/dim 3.6141(3.6048) | Xent 0.1681(0.2418) | Loss 8.6369(9.3491) | Error 0.0589(0.0865) Steps 916(910.25) | Grad Norm 4.2406(7.4740) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 24.9948(24.2280) | Bit/dim 3.5951(3.6028) | Xent 0.2246(0.2403) | Loss 8.5944(9.1643) | Error 0.0800(0.0862) Steps 928(909.43) | Grad Norm 7.0144(7.0861) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 28.2950(24.5123) | Bit/dim 3.6390(3.6075) | Xent 0.2614(0.2413) | Loss 8.7887(9.0396) | Error 0.0989(0.0868) Steps 976(914.45) | Grad Norm 8.1114(7.9721) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 23.8863(24.5825) | Bit/dim 3.5910(3.6068) | Xent 0.2361(0.2423) | Loss 8.6595(8.9515) | Error 0.0800(0.0864) Steps 862(914.49) | Grad Norm 4.4350(8.2374) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 23.6832(24.2739) | Bit/dim 3.5889(3.6060) | Xent 0.2575(0.2421) | Loss 8.7555(8.8771) | Error 0.0933(0.0862) Steps 898(909.62) | Grad Norm 6.5142(7.5159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 100.4505, Epoch Time 1466.1346(1472.4934), Bit/dim 3.6112(best: 3.6060), Xent 0.7609, Loss 3.9916, Error 0.2127(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 24.0113(24.0835) | Bit/dim 3.6045(3.6040) | Xent 0.2263(0.2413) | Loss 8.5723(9.4362) | Error 0.0778(0.0859) Steps 916(907.61) | Grad Norm 6.1208(7.0350) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 23.4490(23.9068) | Bit/dim 3.5878(3.6031) | Xent 0.2757(0.2416) | Loss 8.6911(9.2306) | Error 0.0933(0.0857) Steps 916(907.28) | Grad Norm 6.3743(6.7806) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 24.6543(23.9591) | Bit/dim 3.5965(3.6037) | Xent 0.2281(0.2422) | Loss 8.6949(9.0886) | Error 0.0733(0.0860) Steps 886(906.33) | Grad Norm 8.3333(6.6243) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 23.2416(23.9727) | Bit/dim 3.6401(3.6041) | Xent 0.2467(0.2393) | Loss 8.8441(8.9738) | Error 0.0956(0.0849) Steps 940(908.80) | Grad Norm 6.4049(6.6566) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 23.3794(23.9205) | Bit/dim 3.6171(3.6043) | Xent 0.2582(0.2441) | Loss 8.7751(8.9033) | Error 0.0867(0.0863) Steps 874(906.54) | Grad Norm 8.5764(6.9713) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 101.8095, Epoch Time 1428.8352(1471.1836), Bit/dim 3.6068(best: 3.6060), Xent 0.7594, Loss 3.9865, Error 0.2112(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 24.2342(23.9048) | Bit/dim 3.5866(3.6013) | Xent 0.2755(0.2426) | Loss 8.5606(9.5759) | Error 0.0933(0.0859) Steps 892(904.11) | Grad Norm 7.5912(6.9079) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 24.0203(23.9300) | Bit/dim 3.6126(3.6000) | Xent 0.2577(0.2403) | Loss 8.8617(9.3368) | Error 0.0889(0.0848) Steps 928(909.46) | Grad Norm 5.3622(6.4623) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 23.6667(23.9187) | Bit/dim 3.5655(3.6009) | Xent 0.2406(0.2370) | Loss 8.5396(9.1668) | Error 0.0889(0.0842) Steps 886(911.45) | Grad Norm 4.6451(6.3831) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 24.3350(23.8429) | Bit/dim 3.5595(3.6042) | Xent 0.2153(0.2364) | Loss 8.4856(9.0463) | Error 0.0778(0.0845) Steps 922(911.08) | Grad Norm 5.9763(6.4553) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 24.2927(23.9440) | Bit/dim 3.6272(3.6040) | Xent 0.2214(0.2358) | Loss 8.7088(8.9476) | Error 0.0833(0.0837) Steps 940(912.37) | Grad Norm 4.3189(6.2232) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 23.9018(24.0724) | Bit/dim 3.5970(3.6039) | Xent 0.2509(0.2373) | Loss 8.8111(8.8796) | Error 0.0822(0.0842) Steps 952(913.51) | Grad Norm 8.1571(6.4277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 101.5725, Epoch Time 1439.7812(1470.2416), Bit/dim 3.6085(best: 3.6060), Xent 0.7607, Loss 3.9888, Error 0.2131(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 23.5144(23.9713) | Bit/dim 3.6203(3.6016) | Xent 0.2097(0.2388) | Loss 8.5580(9.4462) | Error 0.0744(0.0847) Steps 892(910.63) | Grad Norm 6.2771(6.5881) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 27.6437(24.6391) | Bit/dim 3.6335(3.6082) | Xent 0.2973(0.2486) | Loss 8.8774(9.2891) | Error 0.1011(0.0877) Steps 1000(928.49) | Grad Norm 14.8852(10.8135) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 25.0374(24.7790) | Bit/dim 3.5881(3.6121) | Xent 0.2727(0.2530) | Loss 8.7633(9.1596) | Error 0.1078(0.0899) Steps 946(928.57) | Grad Norm 11.4651(10.6356) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 24.4261(24.7152) | Bit/dim 3.6153(3.6136) | Xent 0.2392(0.2502) | Loss 8.7267(9.0501) | Error 0.0878(0.0893) Steps 916(926.25) | Grad Norm 4.6610(9.7901) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 24.1064(24.6038) | Bit/dim 3.6172(3.6142) | Xent 0.2594(0.2492) | Loss 8.7261(8.9615) | Error 0.0900(0.0895) Steps 892(918.87) | Grad Norm 4.9414(8.7938) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 101.6475, Epoch Time 1479.0872(1470.5069), Bit/dim 3.6134(best: 3.6060), Xent 0.7406, Loss 3.9837, Error 0.2087(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 23.8717(24.3074) | Bit/dim 3.5978(3.6140) | Xent 0.2755(0.2467) | Loss 8.7757(9.6449) | Error 0.0989(0.0882) Steps 892(915.15) | Grad Norm 5.0993(8.0507) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 23.6437(24.1343) | Bit/dim 3.6042(3.6109) | Xent 0.2559(0.2445) | Loss 8.7479(9.3970) | Error 0.0989(0.0881) Steps 892(914.86) | Grad Norm 5.4244(7.3681) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 22.6241(24.0296) | Bit/dim 3.5734(3.6092) | Xent 0.2484(0.2408) | Loss 8.6629(9.2166) | Error 0.0922(0.0870) Steps 904(912.43) | Grad Norm 5.2355(6.8859) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 23.6523(23.8868) | Bit/dim 3.5954(3.6072) | Xent 0.2301(0.2403) | Loss 8.5484(9.0738) | Error 0.0744(0.0868) Steps 916(909.91) | Grad Norm 4.8745(6.5828) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 24.6946(23.8747) | Bit/dim 3.5996(3.6044) | Xent 0.2487(0.2382) | Loss 8.6808(8.9659) | Error 0.0878(0.0860) Steps 898(909.99) | Grad Norm 6.1188(6.4506) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 23.8634(23.8305) | Bit/dim 3.6353(3.6032) | Xent 0.2494(0.2372) | Loss 8.7580(8.8975) | Error 0.0844(0.0849) Steps 880(909.46) | Grad Norm 5.3731(6.2099) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 102.5791, Epoch Time 1421.6725(1469.0419), Bit/dim 3.6085(best: 3.6060), Xent 0.7538, Loss 3.9854, Error 0.2122(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 24.6522(23.8222) | Bit/dim 3.6241(3.6040) | Xent 0.2374(0.2346) | Loss 8.6921(9.4688) | Error 0.0889(0.0832) Steps 922(907.23) | Grad Norm 7.2914(6.0302) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 24.1669(23.8032) | Bit/dim 3.6253(3.6059) | Xent 0.2410(0.2354) | Loss 8.6467(9.2646) | Error 0.0867(0.0827) Steps 898(905.59) | Grad Norm 5.0282(6.2392) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 23.6886(23.8615) | Bit/dim 3.5779(3.6039) | Xent 0.2251(0.2348) | Loss 8.6479(9.1066) | Error 0.0856(0.0830) Steps 916(907.88) | Grad Norm 4.6953(6.8288) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 23.4150(23.6479) | Bit/dim 3.6096(3.6059) | Xent 0.1984(0.2312) | Loss 8.5542(9.0035) | Error 0.0711(0.0819) Steps 856(904.68) | Grad Norm 4.5080(6.1922) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 22.8922(23.5375) | Bit/dim 3.6179(3.6018) | Xent 0.2340(0.2351) | Loss 8.7448(8.9102) | Error 0.0922(0.0837) Steps 868(900.05) | Grad Norm 5.5125(6.0491) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 101.8597, Epoch Time 1415.3652(1467.4316), Bit/dim 3.6076(best: 3.6060), Xent 0.7484, Loss 3.9818, Error 0.2102(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 23.6565(23.4486) | Bit/dim 3.5848(3.5996) | Xent 0.2507(0.2357) | Loss 8.6747(9.5731) | Error 0.0844(0.0834) Steps 904(897.32) | Grad Norm 8.6729(6.0231) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 23.0558(23.4901) | Bit/dim 3.5818(3.6007) | Xent 0.2177(0.2338) | Loss 8.6685(9.3401) | Error 0.0778(0.0823) Steps 886(898.25) | Grad Norm 5.6271(6.1674) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 22.6985(23.5193) | Bit/dim 3.5858(3.6024) | Xent 0.2386(0.2334) | Loss 8.6625(9.1749) | Error 0.0856(0.0827) Steps 844(895.51) | Grad Norm 5.7882(6.2792) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 24.2889(23.5084) | Bit/dim 3.5913(3.6009) | Xent 0.2552(0.2379) | Loss 8.7184(9.0341) | Error 0.0911(0.0847) Steps 928(898.17) | Grad Norm 10.3300(6.9602) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 23.7544(23.5089) | Bit/dim 3.5640(3.6002) | Xent 0.2481(0.2383) | Loss 8.6688(8.9447) | Error 0.0833(0.0846) Steps 892(898.11) | Grad Norm 6.9957(7.1132) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 24.8479(23.5419) | Bit/dim 3.6138(3.5997) | Xent 0.2316(0.2374) | Loss 8.7247(8.8770) | Error 0.0733(0.0835) Steps 904(899.38) | Grad Norm 5.5515(6.6852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 102.3669, Epoch Time 1414.3547(1465.8393), Bit/dim 3.6060(best: 3.6060), Xent 0.7596, Loss 3.9858, Error 0.2094(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 23.9735(23.6564) | Bit/dim 3.6117(3.6039) | Xent 0.2157(0.2346) | Loss 8.6454(9.4473) | Error 0.0711(0.0829) Steps 886(898.88) | Grad Norm 8.0465(6.4774) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 22.7784(23.6371) | Bit/dim 3.6007(3.6002) | Xent 0.2341(0.2340) | Loss 8.6544(9.2231) | Error 0.0833(0.0832) Steps 904(898.33) | Grad Norm 8.5877(6.4555) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 22.7694(23.6343) | Bit/dim 3.5880(3.6011) | Xent 0.2492(0.2363) | Loss 8.6478(9.0853) | Error 0.1022(0.0849) Steps 880(898.33) | Grad Norm 11.5085(6.6290) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 24.0559(23.7828) | Bit/dim 3.5772(3.6033) | Xent 0.2346(0.2337) | Loss 8.6747(8.9743) | Error 0.0789(0.0834) Steps 898(899.91) | Grad Norm 6.4025(6.4930) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 25.2826(24.0152) | Bit/dim 3.5488(3.6006) | Xent 0.1932(0.2356) | Loss 8.5595(8.8988) | Error 0.0744(0.0837) Steps 910(905.55) | Grad Norm 4.7376(6.9980) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 101.6964, Epoch Time 1443.0642(1465.1560), Bit/dim 3.6079(best: 3.6060), Xent 0.7780, Loss 3.9969, Error 0.2124(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 25.3542(24.1782) | Bit/dim 3.5736(3.5999) | Xent 0.2091(0.2339) | Loss 8.5459(9.5363) | Error 0.0722(0.0828) Steps 940(907.41) | Grad Norm 3.8847(7.0174) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 24.6678(24.3343) | Bit/dim 3.6124(3.6021) | Xent 0.2760(0.2340) | Loss 8.6808(9.3157) | Error 0.0944(0.0835) Steps 892(911.45) | Grad Norm 11.0056(7.1096) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 24.8868(24.4031) | Bit/dim 3.6072(3.5981) | Xent 0.2286(0.2311) | Loss 8.7685(9.1449) | Error 0.0756(0.0825) Steps 916(913.37) | Grad Norm 6.0457(6.7915) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 23.3584(24.4709) | Bit/dim 3.6163(3.6009) | Xent 0.2178(0.2350) | Loss 8.6665(9.0277) | Error 0.0856(0.0842) Steps 916(915.60) | Grad Norm 10.2459(7.0061) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 24.7467(24.5346) | Bit/dim 3.6151(3.6030) | Xent 0.2116(0.2344) | Loss 8.7505(8.9392) | Error 0.0667(0.0838) Steps 922(917.16) | Grad Norm 5.2176(7.0979) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 26.3876(24.7373) | Bit/dim 3.5682(3.6012) | Xent 0.2853(0.2371) | Loss 8.6042(8.8692) | Error 0.0944(0.0842) Steps 916(916.49) | Grad Norm 4.7918(6.9659) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 103.4461, Epoch Time 1486.5108(1465.7967), Bit/dim 3.6061(best: 3.6060), Xent 0.7587, Loss 3.9855, Error 0.2106(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 24.9531(24.9622) | Bit/dim 3.6384(3.6026) | Xent 0.2441(0.2360) | Loss 8.7098(9.4442) | Error 0.0767(0.0825) Steps 844(917.28) | Grad Norm 5.3701(6.7352) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 24.8527(25.0097) | Bit/dim 3.6093(3.6014) | Xent 0.2545(0.2351) | Loss 8.6374(9.2494) | Error 0.0789(0.0827) Steps 952(917.42) | Grad Norm 5.1124(7.0190) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 24.8236(24.7662) | Bit/dim 3.6276(3.6012) | Xent 0.2476(0.2359) | Loss 8.7873(9.0936) | Error 0.0900(0.0828) Steps 886(914.53) | Grad Norm 7.2074(7.0425) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 24.8210(24.5937) | Bit/dim 3.5637(3.5999) | Xent 0.2166(0.2325) | Loss 8.5806(8.9771) | Error 0.0800(0.0822) Steps 874(913.73) | Grad Norm 4.3287(6.6889) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 24.3074(24.4839) | Bit/dim 3.6240(3.6009) | Xent 0.2111(0.2353) | Loss 8.6621(8.9085) | Error 0.0722(0.0833) Steps 904(911.74) | Grad Norm 8.8066(7.3331) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 104.0017, Epoch Time 1472.7284(1466.0046), Bit/dim 3.6114(best: 3.6060), Xent 0.7722, Loss 3.9975, Error 0.2101(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 23.1157(24.2836) | Bit/dim 3.5534(3.6011) | Xent 0.2274(0.2344) | Loss 8.6903(9.5993) | Error 0.0856(0.0833) Steps 898(909.36) | Grad Norm 6.4997(7.2794) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 22.9997(24.2302) | Bit/dim 3.5753(3.6004) | Xent 0.2403(0.2367) | Loss 8.6523(9.3630) | Error 0.0889(0.0845) Steps 880(908.02) | Grad Norm 7.0372(7.1999) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 24.5575(24.2606) | Bit/dim 3.5880(3.6016) | Xent 0.2466(0.2395) | Loss 8.6619(9.1822) | Error 0.0867(0.0860) Steps 940(907.06) | Grad Norm 6.5092(7.3561) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 23.0232(24.1482) | Bit/dim 3.5826(3.6038) | Xent 0.2137(0.2384) | Loss 8.6934(9.0532) | Error 0.0778(0.0857) Steps 922(907.31) | Grad Norm 5.8464(7.1259) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 25.1912(24.1815) | Bit/dim 3.6242(3.6039) | Xent 0.2556(0.2379) | Loss 8.8643(8.9593) | Error 0.0889(0.0848) Steps 928(909.80) | Grad Norm 8.2966(7.1063) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 22.9953(24.0917) | Bit/dim 3.5837(3.6022) | Xent 0.2547(0.2361) | Loss 8.7442(8.8806) | Error 0.0844(0.0850) Steps 910(907.22) | Grad Norm 3.9931(6.6609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 101.8824, Epoch Time 1443.1526(1465.3191), Bit/dim 3.6090(best: 3.6060), Xent 0.7602, Loss 3.9891, Error 0.2096(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 23.9372(24.0053) | Bit/dim 3.5581(3.6007) | Xent 0.2465(0.2335) | Loss 8.5743(9.4423) | Error 0.0844(0.0839) Steps 904(908.84) | Grad Norm 4.9771(6.2470) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 24.1504(24.0604) | Bit/dim 3.5850(3.5992) | Xent 0.2426(0.2318) | Loss 8.7499(9.2353) | Error 0.0844(0.0826) Steps 862(907.44) | Grad Norm 7.4771(6.1129) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 24.1229(24.0458) | Bit/dim 3.5668(3.5973) | Xent 0.2278(0.2325) | Loss 8.6619(9.0824) | Error 0.0800(0.0828) Steps 910(905.96) | Grad Norm 5.1115(6.0986) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 23.6651(24.1150) | Bit/dim 3.6070(3.6009) | Xent 0.2032(0.2271) | Loss 8.6299(8.9730) | Error 0.0711(0.0809) Steps 940(906.56) | Grad Norm 4.3577(5.8378) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 24.7733(24.1947) | Bit/dim 3.6168(3.6030) | Xent 0.2210(0.2292) | Loss 8.7532(8.9012) | Error 0.0733(0.0818) Steps 886(910.56) | Grad Norm 5.1164(5.8760) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 103.5395, Epoch Time 1451.6299(1464.9084), Bit/dim 3.6090(best: 3.6060), Xent 0.7657, Loss 3.9919, Error 0.2134(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 24.1659(24.2300) | Bit/dim 3.6215(3.6022) | Xent 0.2552(0.2335) | Loss 8.7171(9.5709) | Error 0.0867(0.0830) Steps 886(914.20) | Grad Norm 6.5058(6.1441) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 24.2224(24.1548) | Bit/dim 3.5888(3.6015) | Xent 0.2275(0.2317) | Loss 8.5904(9.3364) | Error 0.0756(0.0812) Steps 880(906.97) | Grad Norm 8.9934(6.3866) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 24.4075(24.3956) | Bit/dim 3.6140(3.6025) | Xent 0.2054(0.2308) | Loss 8.6746(9.1781) | Error 0.0722(0.0810) Steps 880(910.04) | Grad Norm 4.0525(6.6661) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 25.6588(24.4909) | Bit/dim 3.6550(3.6040) | Xent 0.2162(0.2309) | Loss 8.6764(9.0501) | Error 0.0778(0.0821) Steps 910(910.06) | Grad Norm 4.9621(7.1081) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 24.2527(24.4341) | Bit/dim 3.6413(3.6047) | Xent 0.2453(0.2292) | Loss 8.8256(8.9485) | Error 0.0889(0.0814) Steps 910(909.43) | Grad Norm 9.5311(7.4088) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 25.5921(24.5304) | Bit/dim 3.5548(3.6022) | Xent 0.2523(0.2305) | Loss 8.6887(8.8714) | Error 0.0878(0.0824) Steps 910(907.21) | Grad Norm 6.3146(7.6085) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 102.6032, Epoch Time 1469.5670(1465.0482), Bit/dim 3.6078(best: 3.6060), Xent 0.7672, Loss 3.9914, Error 0.2132(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 23.3541(24.4671) | Bit/dim 3.5846(3.6050) | Xent 0.2343(0.2332) | Loss 8.7409(9.4558) | Error 0.0756(0.0835) Steps 922(905.86) | Grad Norm 5.4051(7.2800) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 25.9502(24.5473) | Bit/dim 3.5853(3.6023) | Xent 0.2048(0.2335) | Loss 8.7176(9.2504) | Error 0.0678(0.0833) Steps 856(903.82) | Grad Norm 7.0006(7.1898) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 24.5618(24.5545) | Bit/dim 3.5865(3.5985) | Xent 0.2138(0.2331) | Loss 8.6103(9.0941) | Error 0.0744(0.0835) Steps 946(909.62) | Grad Norm 5.0721(6.8940) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 24.6670(24.5643) | Bit/dim 3.6008(3.6000) | Xent 0.2356(0.2355) | Loss 8.7274(8.9925) | Error 0.0844(0.0838) Steps 892(911.17) | Grad Norm 6.6069(7.1281) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 24.4145(24.5484) | Bit/dim 3.6358(3.6016) | Xent 0.2691(0.2329) | Loss 8.8615(8.9099) | Error 0.1044(0.0828) Steps 898(911.60) | Grad Norm 8.6399(6.9064) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 103.1200, Epoch Time 1471.9724(1465.2559), Bit/dim 3.6119(best: 3.6060), Xent 0.7725, Loss 3.9981, Error 0.2116(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 25.4154(24.5753) | Bit/dim 3.6306(3.6050) | Xent 0.1813(0.2312) | Loss 8.6104(9.5749) | Error 0.0689(0.0828) Steps 916(911.59) | Grad Norm 6.8851(7.1786) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 23.0164(24.5133) | Bit/dim 3.5728(3.5990) | Xent 0.2140(0.2311) | Loss 8.4952(9.3278) | Error 0.0789(0.0827) Steps 916(914.66) | Grad Norm 5.1851(7.2574) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 23.1052(24.4266) | Bit/dim 3.6415(3.6016) | Xent 0.4299(0.2455) | Loss 8.9431(9.1834) | Error 0.1511(0.0882) Steps 910(915.96) | Grad Norm 35.0541(16.1291) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 22.5848(23.8160) | Bit/dim 3.6585(3.6194) | Xent 0.2936(0.2746) | Loss 8.8491(9.1074) | Error 0.1144(0.0988) Steps 880(903.92) | Grad Norm 13.4924(16.3053) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 23.4243(23.5092) | Bit/dim 3.6057(3.6244) | Xent 0.2943(0.2809) | Loss 8.7813(9.0351) | Error 0.1033(0.1003) Steps 874(895.96) | Grad Norm 8.7946(14.8737) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 22.5970(23.3570) | Bit/dim 3.6233(3.6251) | Xent 0.2433(0.2759) | Loss 8.6466(8.9534) | Error 0.0967(0.1002) Steps 862(892.79) | Grad Norm 6.5799(13.0054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 100.3208, Epoch Time 1405.8163(1463.4727), Bit/dim 3.6247(best: 3.6060), Xent 0.7550, Loss 4.0022, Error 0.2106(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 23.4289(23.3476) | Bit/dim 3.6410(3.6234) | Xent 0.2386(0.2673) | Loss 8.7674(9.4885) | Error 0.0844(0.0967) Steps 892(892.77) | Grad Norm 8.4156(11.9401) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 23.4626(23.2805) | Bit/dim 3.6110(3.6188) | Xent 0.2752(0.2590) | Loss 8.7647(9.2706) | Error 0.1022(0.0933) Steps 934(890.62) | Grad Norm 5.2243(10.3249) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 22.6997(23.2955) | Bit/dim 3.5870(3.6165) | Xent 0.2167(0.2546) | Loss 8.6995(9.1236) | Error 0.0800(0.0907) Steps 880(889.70) | Grad Norm 7.2206(9.3349) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 22.7258(23.2686) | Bit/dim 3.5994(3.6133) | Xent 0.2426(0.2527) | Loss 8.6433(9.0133) | Error 0.0844(0.0902) Steps 910(891.74) | Grad Norm 9.0335(8.7122) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 22.6479(23.2962) | Bit/dim 3.6101(3.6104) | Xent 0.1955(0.2496) | Loss 8.5497(8.9402) | Error 0.0689(0.0893) Steps 886(896.88) | Grad Norm 4.4515(8.6708) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 100.6303, Epoch Time 1398.5096(1461.5238), Bit/dim 3.6114(best: 3.6060), Xent 0.7803, Loss 4.0015, Error 0.2130(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 24.4826(23.3086) | Bit/dim 3.5997(3.6113) | Xent 0.2541(0.2471) | Loss 8.8445(9.6206) | Error 0.1022(0.0880) Steps 916(896.39) | Grad Norm 6.3188(8.1378) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 22.7691(23.2921) | Bit/dim 3.6081(3.6090) | Xent 0.2168(0.2411) | Loss 8.6754(9.3835) | Error 0.0767(0.0858) Steps 874(896.27) | Grad Norm 2.8671(7.5134) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 25.4690(23.4016) | Bit/dim 3.5691(3.6063) | Xent 0.2215(0.2388) | Loss 8.5678(9.1862) | Error 0.0744(0.0844) Steps 880(895.19) | Grad Norm 7.5558(7.1551) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 24.2703(23.5618) | Bit/dim 3.6248(3.6043) | Xent 0.2308(0.2338) | Loss 8.7776(9.0472) | Error 0.0767(0.0824) Steps 934(898.22) | Grad Norm 4.3041(6.7202) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 24.2598(23.7169) | Bit/dim 3.6281(3.6031) | Xent 0.2534(0.2348) | Loss 8.8459(8.9684) | Error 0.0944(0.0822) Steps 910(903.22) | Grad Norm 5.1927(6.4997) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 24.6943(23.9394) | Bit/dim 3.6075(3.6036) | Xent 0.2484(0.2333) | Loss 8.5866(8.9012) | Error 0.0900(0.0816) Steps 868(905.04) | Grad Norm 7.1365(6.2519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 101.7653, Epoch Time 1433.3905(1460.6798), Bit/dim 3.6042(best: 3.6060), Xent 0.7728, Loss 3.9907, Error 0.2129(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 22.9422(23.9831) | Bit/dim 3.6338(3.6063) | Xent 0.2291(0.2335) | Loss 8.8180(9.4708) | Error 0.0722(0.0816) Steps 856(905.29) | Grad Norm 5.8878(6.1897) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 24.0124(23.9609) | Bit/dim 3.5919(3.6030) | Xent 0.2370(0.2314) | Loss 8.5887(9.2637) | Error 0.0789(0.0804) Steps 958(906.81) | Grad Norm 4.8323(5.7811) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 23.4125(23.8222) | Bit/dim 3.6024(3.6016) | Xent 0.1981(0.2286) | Loss 8.6612(9.0974) | Error 0.0700(0.0794) Steps 886(907.29) | Grad Norm 6.2918(5.8752) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 23.9817(23.7531) | Bit/dim 3.5892(3.6006) | Xent 0.2225(0.2320) | Loss 8.5345(8.9821) | Error 0.0700(0.0808) Steps 916(905.33) | Grad Norm 6.9261(6.6181) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 23.6013(23.8424) | Bit/dim 3.5946(3.6007) | Xent 0.2466(0.2328) | Loss 8.6292(8.9076) | Error 0.0878(0.0814) Steps 910(908.68) | Grad Norm 7.7871(6.9700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 100.8421, Epoch Time 1429.7430(1459.7517), Bit/dim 3.6088(best: 3.6042), Xent 0.7649, Loss 3.9913, Error 0.2139(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 23.4144(23.8827) | Bit/dim 3.5988(3.6022) | Xent 0.2580(0.2316) | Loss 8.7208(9.5899) | Error 0.0822(0.0802) Steps 916(911.26) | Grad Norm 8.1055(6.6419) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 22.8016(23.8673) | Bit/dim 3.5950(3.6017) | Xent 0.2123(0.2284) | Loss 8.6514(9.3424) | Error 0.0756(0.0796) Steps 904(909.46) | Grad Norm 5.0664(6.4362) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 25.5572(23.9595) | Bit/dim 3.6142(3.6008) | Xent 0.1911(0.2273) | Loss 8.5419(9.1600) | Error 0.0667(0.0792) Steps 856(910.05) | Grad Norm 4.8972(6.4314) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 23.2717(23.9515) | Bit/dim 3.6180(3.5986) | Xent 0.2494(0.2309) | Loss 8.5534(9.0292) | Error 0.0889(0.0798) Steps 880(910.11) | Grad Norm 6.0256(6.6247) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 23.2731(23.9270) | Bit/dim 3.5908(3.5991) | Xent 0.2093(0.2281) | Loss 8.6444(8.9354) | Error 0.0767(0.0788) Steps 904(909.94) | Grad Norm 5.8125(6.3206) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 23.5657(23.8293) | Bit/dim 3.6195(3.6019) | Xent 0.2575(0.2321) | Loss 8.8191(8.8792) | Error 0.0822(0.0803) Steps 922(907.06) | Grad Norm 5.4486(6.7361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 102.9476, Epoch Time 1435.2989(1459.0181), Bit/dim 3.6096(best: 3.6042), Xent 0.7579, Loss 3.9886, Error 0.2096(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 23.8554(23.8425) | Bit/dim 3.6134(3.6033) | Xent 0.2497(0.2324) | Loss 8.6096(9.4615) | Error 0.0856(0.0813) Steps 874(903.96) | Grad Norm 4.3237(6.3000) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 22.6288(23.7857) | Bit/dim 3.5993(3.6055) | Xent 0.2150(0.2323) | Loss 8.6916(9.2560) | Error 0.0767(0.0815) Steps 874(904.19) | Grad Norm 10.1947(6.3533) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 23.5223(23.7752) | Bit/dim 3.6137(3.6034) | Xent 0.2430(0.2318) | Loss 8.7337(9.1073) | Error 0.0867(0.0823) Steps 946(903.41) | Grad Norm 5.2745(6.4084) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 23.5313(23.8034) | Bit/dim 3.6023(3.6012) | Xent 0.2215(0.2327) | Loss 8.6762(8.9958) | Error 0.0789(0.0829) Steps 886(903.51) | Grad Norm 4.3793(6.3384) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 23.8759(23.8105) | Bit/dim 3.6537(3.6040) | Xent 0.2347(0.2317) | Loss 8.8450(8.9206) | Error 0.0844(0.0822) Steps 874(904.29) | Grad Norm 6.9238(6.4057) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 101.3995, Epoch Time 1427.2697(1458.0657), Bit/dim 3.6065(best: 3.6042), Xent 0.7837, Loss 3.9983, Error 0.2121(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 24.5722(23.7537) | Bit/dim 3.6214(3.6009) | Xent 0.2217(0.2289) | Loss 8.8689(9.6005) | Error 0.0844(0.0812) Steps 910(904.93) | Grad Norm 9.7703(6.4387) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 23.6854(23.7180) | Bit/dim 3.5837(3.6014) | Xent 0.2066(0.2290) | Loss 8.5668(9.3512) | Error 0.0667(0.0815) Steps 910(903.09) | Grad Norm 6.7529(6.7918) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 23.7424(23.7651) | Bit/dim 3.5676(3.6008) | Xent 0.2517(0.2307) | Loss 8.6987(9.1925) | Error 0.0922(0.0825) Steps 922(909.78) | Grad Norm 6.8859(6.9559) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 24.1093(23.8094) | Bit/dim 3.5851(3.6005) | Xent 0.2279(0.2296) | Loss 8.7443(9.0640) | Error 0.0889(0.0826) Steps 898(910.40) | Grad Norm 5.0394(6.5811) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 23.8622(24.0164) | Bit/dim 3.5949(3.6012) | Xent 0.2161(0.2297) | Loss 8.7283(8.9676) | Error 0.0744(0.0822) Steps 910(906.18) | Grad Norm 6.1066(6.4482) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 24.2408(24.0898) | Bit/dim 3.6249(3.6008) | Xent 0.2234(0.2256) | Loss 8.7192(8.8843) | Error 0.0700(0.0809) Steps 922(905.05) | Grad Norm 6.0864(6.2888) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 102.6568, Epoch Time 1442.4994(1457.5987), Bit/dim 3.6076(best: 3.6042), Xent 0.7760, Loss 3.9956, Error 0.2151(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 23.6828(24.0640) | Bit/dim 3.5993(3.5976) | Xent 0.1960(0.2257) | Loss 8.5708(9.4538) | Error 0.0700(0.0812) Steps 904(908.88) | Grad Norm 6.5026(6.0630) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 24.1462(24.0130) | Bit/dim 3.6403(3.6005) | Xent 0.2058(0.2231) | Loss 8.7880(9.2541) | Error 0.0833(0.0801) Steps 946(908.98) | Grad Norm 3.7354(5.8925) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 23.6510(24.0211) | Bit/dim 3.6063(3.6029) | Xent 0.2094(0.2245) | Loss 8.7414(9.1201) | Error 0.0711(0.0799) Steps 934(911.68) | Grad Norm 6.0831(5.8767) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 22.5690(23.9669) | Bit/dim 3.5834(3.6012) | Xent 0.2349(0.2257) | Loss 8.5316(8.9954) | Error 0.0844(0.0802) Steps 904(912.67) | Grad Norm 6.3359(7.7851) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 24.2646(24.0680) | Bit/dim 3.5840(3.6012) | Xent 0.2165(0.2256) | Loss 8.5288(8.9199) | Error 0.0767(0.0797) Steps 910(915.17) | Grad Norm 6.2220(7.8124) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 102.0056, Epoch Time 1442.7030(1457.1518), Bit/dim 3.6030(best: 3.6042), Xent 0.7663, Loss 3.9862, Error 0.2109(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 24.1257(24.1622) | Bit/dim 3.6144(3.6022) | Xent 0.2396(0.2273) | Loss 8.6633(9.6083) | Error 0.0878(0.0808) Steps 904(914.48) | Grad Norm 4.6785(7.6171) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 23.6421(24.0381) | Bit/dim 3.6192(3.6027) | Xent 0.2538(0.2290) | Loss 8.8439(9.3691) | Error 0.0944(0.0818) Steps 940(912.21) | Grad Norm 8.0018(7.2311) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 24.2137(24.0756) | Bit/dim 3.6021(3.6013) | Xent 0.2340(0.2300) | Loss 8.7453(9.1934) | Error 0.0789(0.0817) Steps 886(914.33) | Grad Norm 4.8556(7.1012) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 24.8646(24.1173) | Bit/dim 3.5868(3.6009) | Xent 0.2566(0.2258) | Loss 8.7068(9.0515) | Error 0.0789(0.0799) Steps 874(916.13) | Grad Norm 3.8964(6.8366) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 24.0653(24.1233) | Bit/dim 3.5635(3.5988) | Xent 0.2306(0.2247) | Loss 8.6507(8.9456) | Error 0.0822(0.0796) Steps 928(916.10) | Grad Norm 6.0899(6.4086) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 24.4935(24.1371) | Bit/dim 3.5921(3.6009) | Xent 0.2044(0.2260) | Loss 8.7233(8.8839) | Error 0.0778(0.0808) Steps 940(916.25) | Grad Norm 6.2779(6.6332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 102.4392, Epoch Time 1446.3372(1456.8274), Bit/dim 3.6046(best: 3.6030), Xent 0.7759, Loss 3.9926, Error 0.2133(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 24.2636(24.3143) | Bit/dim 3.6265(3.6033) | Xent 0.2224(0.2277) | Loss 8.7095(9.4817) | Error 0.0811(0.0815) Steps 928(914.98) | Grad Norm 5.8303(6.7055) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 23.7724(24.5022) | Bit/dim 3.5900(3.6028) | Xent 0.2214(0.2251) | Loss 8.6325(9.2800) | Error 0.0722(0.0807) Steps 940(918.66) | Grad Norm 5.1599(6.4400) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 24.5178(24.6769) | Bit/dim 3.6055(3.6019) | Xent 0.2557(0.2245) | Loss 8.7065(9.1164) | Error 0.0867(0.0807) Steps 880(922.54) | Grad Norm 7.0915(6.5910) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 24.7872(24.6924) | Bit/dim 3.5812(3.5981) | Xent 0.2086(0.2272) | Loss 8.5884(9.0002) | Error 0.0744(0.0816) Steps 916(919.70) | Grad Norm 7.0869(6.5271) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 23.5416(24.6206) | Bit/dim 3.6058(3.5990) | Xent 0.2042(0.2237) | Loss 8.7211(8.9117) | Error 0.0789(0.0798) Steps 922(916.39) | Grad Norm 3.9463(6.3265) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 103.4198, Epoch Time 1487.9474(1457.7610), Bit/dim 3.6080(best: 3.6030), Xent 0.7842, Loss 4.0001, Error 0.2152(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 25.2482(24.6386) | Bit/dim 3.5625(3.5987) | Xent 0.2172(0.2243) | Loss 8.5242(9.5594) | Error 0.0778(0.0796) Steps 916(915.66) | Grad Norm 5.7232(6.4424) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 24.0056(24.6210) | Bit/dim 3.6078(3.6021) | Xent 0.2322(0.2238) | Loss 8.6111(9.3314) | Error 0.0800(0.0799) Steps 892(914.53) | Grad Norm 4.9207(6.6198) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 25.7819(24.6123) | Bit/dim 3.5696(3.5990) | Xent 0.2252(0.2225) | Loss 8.4933(9.1476) | Error 0.0767(0.0792) Steps 892(913.95) | Grad Norm 4.6804(6.4481) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 25.0728(24.5989) | Bit/dim 3.5927(3.5993) | Xent 0.2474(0.2240) | Loss 8.6921(9.0274) | Error 0.0756(0.0793) Steps 940(914.26) | Grad Norm 8.4076(7.2288) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 25.4272(24.7392) | Bit/dim 3.5829(3.5985) | Xent 0.2391(0.2287) | Loss 8.6881(8.9369) | Error 0.0889(0.0812) Steps 1012(923.72) | Grad Norm 10.9579(7.5857) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 24.6417(24.8294) | Bit/dim 3.6122(3.6011) | Xent 0.2173(0.2285) | Loss 8.6648(8.8804) | Error 0.0767(0.0816) Steps 964(930.41) | Grad Norm 7.3773(8.1778) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 102.7684, Epoch Time 1482.6412(1458.5074), Bit/dim 3.6115(best: 3.6030), Xent 0.7951, Loss 4.0091, Error 0.2145(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 24.0982(24.6573) | Bit/dim 3.5974(3.5987) | Xent 0.2483(0.2293) | Loss 8.6411(9.4468) | Error 0.0867(0.0824) Steps 934(925.74) | Grad Norm 5.5983(7.9898) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 22.8871(24.5022) | Bit/dim 3.5801(3.5998) | Xent 0.2171(0.2271) | Loss 8.7020(9.2439) | Error 0.0778(0.0817) Steps 910(920.96) | Grad Norm 4.9760(7.5031) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 24.8460(24.4025) | Bit/dim 3.6316(3.5998) | Xent 0.2412(0.2279) | Loss 8.7582(9.0961) | Error 0.0911(0.0815) Steps 910(915.19) | Grad Norm 8.6084(7.3633) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 24.3409(24.4471) | Bit/dim 3.6085(3.6014) | Xent 0.2155(0.2297) | Loss 8.6855(8.9936) | Error 0.0756(0.0823) Steps 934(916.61) | Grad Norm 7.1504(7.3059) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 24.9166(24.4925) | Bit/dim 3.5856(3.6004) | Xent 0.2604(0.2270) | Loss 8.6128(8.8929) | Error 0.1011(0.0817) Steps 934(918.74) | Grad Norm 8.6308(7.2766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 102.4945, Epoch Time 1460.3821(1458.5636), Bit/dim 3.6075(best: 3.6030), Xent 0.7818, Loss 3.9984, Error 0.2104(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 24.7323(24.6155) | Bit/dim 3.6098(3.6035) | Xent 0.1725(0.2266) | Loss 8.6119(9.5793) | Error 0.0656(0.0816) Steps 940(923.10) | Grad Norm 4.1493(6.8201) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 26.0274(24.7438) | Bit/dim 3.5720(3.6029) | Xent 0.1916(0.2243) | Loss 8.6027(9.3463) | Error 0.0656(0.0809) Steps 1018(931.08) | Grad Norm 3.6255(6.5190) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 23.5992(24.8129) | Bit/dim 3.5964(3.5992) | Xent 0.2404(0.2258) | Loss 8.7860(9.1666) | Error 0.0800(0.0814) Steps 922(932.24) | Grad Norm 7.6435(6.6391) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 24.8937(24.9040) | Bit/dim 3.5933(3.6012) | Xent 0.2089(0.2252) | Loss 8.6719(9.0440) | Error 0.0922(0.0807) Steps 862(933.49) | Grad Norm 4.7885(6.6214) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 25.5015(24.8812) | Bit/dim 3.5917(3.5997) | Xent 0.2170(0.2228) | Loss 8.6541(8.9410) | Error 0.0789(0.0799) Steps 964(932.77) | Grad Norm 7.2155(6.8616) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 27.6129(25.0304) | Bit/dim 3.6210(3.5997) | Xent 0.2184(0.2245) | Loss 8.6934(8.8804) | Error 0.0756(0.0805) Steps 952(932.04) | Grad Norm 5.4350(7.0497) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 104.6985, Epoch Time 1503.1321(1459.9007), Bit/dim 3.6086(best: 3.6030), Xent 0.8075, Loss 4.0124, Error 0.2175(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 26.0425(25.1305) | Bit/dim 3.6114(3.5983) | Xent 0.1552(0.2222) | Loss 8.7466(9.4524) | Error 0.0478(0.0794) Steps 952(934.18) | Grad Norm 4.3590(7.2934) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 23.5061(25.0097) | Bit/dim 3.6173(3.6004) | Xent 0.1895(0.2213) | Loss 8.6262(9.2586) | Error 0.0778(0.0794) Steps 922(933.04) | Grad Norm 8.2882(7.0733) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 23.5058(24.8076) | Bit/dim 3.5806(3.5987) | Xent 0.2174(0.2265) | Loss 8.6898(9.1134) | Error 0.0856(0.0820) Steps 904(932.39) | Grad Norm 8.8817(6.8996) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 23.4167(24.5772) | Bit/dim 3.6079(3.5993) | Xent 0.2309(0.2282) | Loss 8.6899(9.0051) | Error 0.0767(0.0824) Steps 904(925.95) | Grad Norm 5.7385(6.7008) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 24.1305(24.4913) | Bit/dim 3.6218(3.5999) | Xent 0.2434(0.2285) | Loss 8.7084(8.9182) | Error 0.0978(0.0819) Steps 910(921.75) | Grad Norm 10.2696(7.7525) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 109.8560, Epoch Time 1483.1923(1460.5994), Bit/dim 3.6088(best: 3.6030), Xent 0.7841, Loss 4.0009, Error 0.2156(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 25.4158(25.0080) | Bit/dim 3.5521(3.6001) | Xent 0.2600(0.2300) | Loss 8.6854(9.6124) | Error 0.0900(0.0825) Steps 1018(932.97) | Grad Norm 9.4228(7.9257) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 26.0928(25.4206) | Bit/dim 3.5787(3.6013) | Xent 0.2379(0.2313) | Loss 8.6943(9.3931) | Error 0.0856(0.0825) Steps 958(943.07) | Grad Norm 7.0002(7.7221) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 26.2395(25.7175) | Bit/dim 3.5910(3.6016) | Xent 0.1829(0.2279) | Loss 8.4587(9.2165) | Error 0.0622(0.0811) Steps 994(951.08) | Grad Norm 5.9668(7.4423) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 26.6271(25.9004) | Bit/dim 3.6169(3.6032) | Xent 0.2603(0.2276) | Loss 8.8455(9.0899) | Error 0.0833(0.0814) Steps 1012(953.98) | Grad Norm 17.3390(8.7851) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 26.5981(26.3106) | Bit/dim 3.5912(3.6071) | Xent 0.2147(0.2296) | Loss 8.6483(9.0047) | Error 0.0744(0.0819) Steps 994(963.95) | Grad Norm 5.5413(8.8615) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 23.0295(25.7721) | Bit/dim 3.6361(3.6075) | Xent 0.2520(0.2323) | Loss 8.8975(8.9244) | Error 0.0911(0.0822) Steps 904(955.85) | Grad Norm 12.1826(9.0079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 103.6561, Epoch Time 1570.5017(1463.8965), Bit/dim 3.6103(best: 3.6030), Xent 0.7964, Loss 4.0085, Error 0.2140(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 23.4545(25.3085) | Bit/dim 3.6265(3.6058) | Xent 0.2241(0.2315) | Loss 8.6801(9.4745) | Error 0.0744(0.0818) Steps 904(946.01) | Grad Norm 4.6988(8.2480) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 23.4828(24.9060) | Bit/dim 3.5957(3.6019) | Xent 0.2499(0.2314) | Loss 8.7319(9.2578) | Error 0.0933(0.0819) Steps 892(931.86) | Grad Norm 6.8794(7.6169) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 24.2012(24.7690) | Bit/dim 3.6010(3.6033) | Xent 0.2521(0.2270) | Loss 8.8260(9.1117) | Error 0.0811(0.0794) Steps 880(926.13) | Grad Norm 7.2047(6.9945) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 23.1346(24.5518) | Bit/dim 3.5860(3.6005) | Xent 0.2150(0.2255) | Loss 8.6983(8.9963) | Error 0.0700(0.0790) Steps 922(924.38) | Grad Norm 8.0921(7.2777) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 24.3182(24.4239) | Bit/dim 3.5857(3.5988) | Xent 0.2349(0.2252) | Loss 8.6929(8.9141) | Error 0.0833(0.0796) Steps 928(923.73) | Grad Norm 5.9696(6.9354) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 102.2122, Epoch Time 1439.7461(1463.1720), Bit/dim 3.6062(best: 3.6030), Xent 0.8056, Loss 4.0090, Error 0.2136(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 24.3447(24.2634) | Bit/dim 3.5763(3.5969) | Xent 0.1814(0.2237) | Loss 8.6625(9.5703) | Error 0.0667(0.0787) Steps 880(918.17) | Grad Norm 8.0128(7.7260) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 24.6406(24.0737) | Bit/dim 3.5814(3.5969) | Xent 0.2444(0.2269) | Loss 8.6606(9.3303) | Error 0.0889(0.0814) Steps 946(914.39) | Grad Norm 6.3665(7.9510) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 24.5901(24.0180) | Bit/dim 3.5956(3.5988) | Xent 0.2590(0.2249) | Loss 8.8026(9.1625) | Error 0.0911(0.0810) Steps 976(915.85) | Grad Norm 8.6356(7.5709) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 24.1691(23.9562) | Bit/dim 3.5997(3.5985) | Xent 0.2323(0.2246) | Loss 8.5962(9.0266) | Error 0.0944(0.0812) Steps 898(912.91) | Grad Norm 5.6708(7.1662) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 22.2753(23.8886) | Bit/dim 3.6092(3.5994) | Xent 0.2190(0.2231) | Loss 8.5452(8.9315) | Error 0.0756(0.0804) Steps 862(909.87) | Grad Norm 5.3296(7.0355) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 24.5372(23.9218) | Bit/dim 3.5668(3.5986) | Xent 0.2782(0.2282) | Loss 8.7076(8.8685) | Error 0.1056(0.0824) Steps 934(907.19) | Grad Norm 6.1405(7.7196) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 103.8547, Epoch Time 1431.5450(1462.2232), Bit/dim 3.6058(best: 3.6030), Xent 0.8115, Loss 4.0115, Error 0.2155(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 23.3242(23.8739) | Bit/dim 3.5996(3.5963) | Xent 0.2262(0.2277) | Loss 8.6229(9.4642) | Error 0.0833(0.0818) Steps 874(908.02) | Grad Norm 7.8788(7.6685) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 23.7314(23.7933) | Bit/dim 3.6355(3.5976) | Xent 0.2297(0.2260) | Loss 8.8750(9.2495) | Error 0.0878(0.0815) Steps 910(911.11) | Grad Norm 7.8878(7.5926) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 24.0109(23.8390) | Bit/dim 3.6235(3.5984) | Xent 0.2114(0.2224) | Loss 8.6345(9.0894) | Error 0.0900(0.0805) Steps 874(910.51) | Grad Norm 5.9401(7.1795) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 23.7116(23.8767) | Bit/dim 3.5797(3.5962) | Xent 0.2040(0.2238) | Loss 8.6446(8.9801) | Error 0.0767(0.0804) Steps 922(914.24) | Grad Norm 4.1871(6.7187) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 24.3243(23.8870) | Bit/dim 3.6209(3.5991) | Xent 0.2055(0.2226) | Loss 8.6845(8.9020) | Error 0.0711(0.0792) Steps 922(914.29) | Grad Norm 5.2755(6.5739) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 102.2094, Epoch Time 1429.6305(1461.2454), Bit/dim 3.6044(best: 3.6030), Xent 0.7849, Loss 3.9969, Error 0.2110(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 24.5762(23.7745) | Bit/dim 3.5784(3.6000) | Xent 0.2004(0.2220) | Loss 8.7014(9.5707) | Error 0.0622(0.0790) Steps 910(909.33) | Grad Norm 4.1704(6.2209) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 23.9743(23.7060) | Bit/dim 3.5871(3.5994) | Xent 0.1903(0.2231) | Loss 8.6280(9.3294) | Error 0.0644(0.0801) Steps 880(908.99) | Grad Norm 8.1187(6.2961) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 23.7675(23.8450) | Bit/dim 3.6474(3.6015) | Xent 0.2285(0.2237) | Loss 8.6815(9.1717) | Error 0.0811(0.0801) Steps 898(910.99) | Grad Norm 9.0985(6.4736) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 24.3011(23.8760) | Bit/dim 3.6044(3.5999) | Xent 0.2280(0.2228) | Loss 8.7061(9.0406) | Error 0.0767(0.0791) Steps 916(910.23) | Grad Norm 4.2395(6.2677) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 23.7222(23.9425) | Bit/dim 3.5749(3.5972) | Xent 0.1916(0.2213) | Loss 8.6251(8.9350) | Error 0.0656(0.0789) Steps 916(911.49) | Grad Norm 4.7540(6.0397) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 24.4946(23.9334) | Bit/dim 3.5891(3.5968) | Xent 0.1936(0.2209) | Loss 8.6960(8.8628) | Error 0.0678(0.0790) Steps 958(911.36) | Grad Norm 4.0499(6.0581) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 103.0255, Epoch Time 1436.3126(1460.4974), Bit/dim 3.6031(best: 3.6030), Xent 0.7958, Loss 4.0010, Error 0.2147(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 22.9669(23.7896) | Bit/dim 3.6040(3.5963) | Xent 0.2152(0.2211) | Loss 8.7088(9.4599) | Error 0.0889(0.0795) Steps 940(913.43) | Grad Norm 4.8741(6.1106) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 23.8340(23.8156) | Bit/dim 3.6371(3.5960) | Xent 0.2309(0.2208) | Loss 8.6504(9.2436) | Error 0.0844(0.0803) Steps 904(912.01) | Grad Norm 11.2077(7.7227) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 24.5396(23.7255) | Bit/dim 3.5731(3.5981) | Xent 0.2895(0.2224) | Loss 8.7095(9.0942) | Error 0.0989(0.0801) Steps 934(910.33) | Grad Norm 5.3458(7.7081) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 24.5139(23.6467) | Bit/dim 3.6011(3.5985) | Xent 0.2105(0.2240) | Loss 8.6273(8.9824) | Error 0.0678(0.0796) Steps 922(908.40) | Grad Norm 4.9093(7.1932) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 23.1320(23.7337) | Bit/dim 3.6026(3.6002) | Xent 0.2289(0.2226) | Loss 8.5540(8.8990) | Error 0.0878(0.0800) Steps 880(909.68) | Grad Norm 4.9300(6.7145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 101.8230, Epoch Time 1418.9986(1459.2524), Bit/dim 3.6083(best: 3.6030), Xent 0.7947, Loss 4.0056, Error 0.2171(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 24.3902(23.7214) | Bit/dim 3.5706(3.6001) | Xent 0.2220(0.2250) | Loss 8.5715(9.5801) | Error 0.0800(0.0804) Steps 892(909.73) | Grad Norm 6.8526(6.5057) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 24.3971(23.8237) | Bit/dim 3.5924(3.5981) | Xent 0.2823(0.2273) | Loss 8.7384(9.3478) | Error 0.1022(0.0820) Steps 928(912.30) | Grad Norm 9.8619(6.9877) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 24.3356(23.8975) | Bit/dim 3.6019(3.5975) | Xent 0.1973(0.2254) | Loss 8.6987(9.1699) | Error 0.0611(0.0806) Steps 940(914.43) | Grad Norm 7.8581(7.0092) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 24.0328(23.8780) | Bit/dim 3.6107(3.5978) | Xent 0.2252(0.2244) | Loss 8.8011(9.0316) | Error 0.0856(0.0807) Steps 892(913.12) | Grad Norm 5.3479(6.8295) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 24.5719(23.8936) | Bit/dim 3.5960(3.5973) | Xent 0.2528(0.2275) | Loss 8.6126(8.9382) | Error 0.0756(0.0807) Steps 916(910.92) | Grad Norm 13.6847(7.3352) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 24.5977(23.9485) | Bit/dim 3.6226(3.5985) | Xent 0.2292(0.2248) | Loss 8.7431(8.8669) | Error 0.0878(0.0803) Steps 880(906.90) | Grad Norm 8.9931(7.6125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 103.2009, Epoch Time 1441.8683(1458.7309), Bit/dim 3.6023(best: 3.6030), Xent 0.7830, Loss 3.9938, Error 0.2140(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 23.9373(23.9365) | Bit/dim 3.6154(3.5994) | Xent 0.2265(0.2241) | Loss 8.7225(9.4497) | Error 0.0789(0.0802) Steps 928(905.14) | Grad Norm 4.9910(7.5874) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 22.4242(23.9299) | Bit/dim 3.5912(3.6005) | Xent 0.2020(0.2194) | Loss 8.5379(9.2458) | Error 0.0733(0.0787) Steps 874(903.77) | Grad Norm 6.2030(7.2959) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 25.1979(23.9774) | Bit/dim 3.5746(3.5977) | Xent 0.2210(0.2185) | Loss 8.5775(9.0847) | Error 0.0867(0.0783) Steps 886(902.85) | Grad Norm 9.7076(7.1336) | Total Time 0.00(0.00)\n",
      "Iter 17970 | Time 24.0854(24.0485) | Bit/dim 3.6536(3.6006) | Xent 0.1846(0.2182) | Loss 8.8067(8.9857) | Error 0.0678(0.0781) Steps 910(905.63) | Grad Norm 6.3518(7.1067) | Total Time 0.00(0.00)\n",
      "Iter 17980 | Time 24.9811(24.2125) | Bit/dim 3.5853(3.5966) | Xent 0.2090(0.2199) | Loss 8.6690(8.9047) | Error 0.0667(0.0784) Steps 958(914.65) | Grad Norm 7.2477(7.2414) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 102.2622, Epoch Time 1449.0607(1458.4408), Bit/dim 3.6000(best: 3.6023), Xent 0.7894, Loss 3.9947, Error 0.2148(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 25.1986(24.2000) | Bit/dim 3.5876(3.5986) | Xent 0.2240(0.2208) | Loss 8.7114(9.5871) | Error 0.0844(0.0791) Steps 934(916.89) | Grad Norm 7.0423(7.2106) | Total Time 0.00(0.00)\n",
      "Iter 18000 | Time 24.9282(24.4085) | Bit/dim 3.5841(3.5966) | Xent 0.2065(0.2191) | Loss 8.6066(9.3369) | Error 0.0800(0.0785) Steps 958(922.11) | Grad Norm 7.7264(7.3560) | Total Time 0.00(0.00)\n",
      "Iter 18010 | Time 25.6957(24.7644) | Bit/dim 3.6061(3.5980) | Xent 0.2234(0.2209) | Loss 8.7489(9.1730) | Error 0.0756(0.0785) Steps 994(927.54) | Grad Norm 9.0917(7.4168) | Total Time 0.00(0.00)\n",
      "Iter 18020 | Time 23.9002(24.8955) | Bit/dim 3.5867(3.5970) | Xent 0.2408(0.2201) | Loss 8.6507(9.0297) | Error 0.0833(0.0777) Steps 916(929.28) | Grad Norm 5.3746(7.0193) | Total Time 0.00(0.00)\n",
      "Iter 18030 | Time 25.0130(24.7713) | Bit/dim 3.6195(3.5997) | Xent 0.2253(0.2215) | Loss 8.7155(8.9386) | Error 0.0778(0.0789) Steps 880(922.64) | Grad Norm 5.4817(6.9448) | Total Time 0.00(0.00)\n",
      "Iter 18040 | Time 24.0177(24.7795) | Bit/dim 3.5972(3.5987) | Xent 0.2596(0.2236) | Loss 8.7629(8.8673) | Error 0.0922(0.0787) Steps 934(919.81) | Grad Norm 9.3031(7.1401) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 104.4475, Epoch Time 1497.0837(1459.6001), Bit/dim 3.6068(best: 3.6000), Xent 0.8056, Loss 4.0096, Error 0.2131(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 26.2488(24.7269) | Bit/dim 3.6004(3.6012) | Xent 0.2573(0.2226) | Loss 8.8483(9.4573) | Error 0.0978(0.0787) Steps 934(921.24) | Grad Norm 20.3961(8.4797) | Total Time 0.00(0.00)\n",
      "Iter 18060 | Time 26.5044(24.9784) | Bit/dim 3.5892(3.6010) | Xent 0.2289(0.2238) | Loss 8.6909(9.2568) | Error 0.0833(0.0791) Steps 946(927.10) | Grad Norm 6.7806(8.6978) | Total Time 0.00(0.00)\n",
      "Iter 18070 | Time 25.9253(25.0215) | Bit/dim 3.5923(3.6014) | Xent 0.2408(0.2222) | Loss 8.7417(9.1095) | Error 0.0911(0.0788) Steps 910(931.26) | Grad Norm 5.5328(7.9491) | Total Time 0.00(0.00)\n",
      "Iter 18080 | Time 25.2414(24.9658) | Bit/dim 3.6058(3.6024) | Xent 0.2565(0.2223) | Loss 8.6423(8.9961) | Error 0.0844(0.0784) Steps 886(930.15) | Grad Norm 8.6680(7.7714) | Total Time 0.00(0.00)\n",
      "Iter 18090 | Time 26.7601(25.2222) | Bit/dim 3.5640(3.5960) | Xent 0.2325(0.2244) | Loss 8.6797(8.9064) | Error 0.0722(0.0788) Steps 886(937.19) | Grad Norm 12.1928(7.6937) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 104.2307, Epoch Time 1506.3859(1461.0037), Bit/dim 3.6082(best: 3.6000), Xent 0.8026, Loss 4.0094, Error 0.2110(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 23.8518(25.1699) | Bit/dim 3.5748(3.5976) | Xent 0.2412(0.2231) | Loss 8.7365(9.6068) | Error 0.0811(0.0791) Steps 898(932.55) | Grad Norm 6.7487(7.2595) | Total Time 0.00(0.00)\n",
      "Iter 18110 | Time 25.8792(25.3182) | Bit/dim 3.5961(3.5966) | Xent 0.2277(0.2233) | Loss 8.7333(9.3692) | Error 0.0689(0.0776) Steps 910(934.31) | Grad Norm 5.8724(7.1522) | Total Time 0.00(0.00)\n",
      "Iter 18120 | Time 26.0060(25.3696) | Bit/dim 3.6096(3.5962) | Xent 0.2116(0.2200) | Loss 8.8275(9.1857) | Error 0.0667(0.0763) Steps 934(931.68) | Grad Norm 4.6015(7.0429) | Total Time 0.00(0.00)\n",
      "Iter 18130 | Time 26.0522(25.2294) | Bit/dim 3.6273(3.5989) | Xent 0.1939(0.2195) | Loss 8.7157(9.0491) | Error 0.0711(0.0769) Steps 940(927.35) | Grad Norm 6.2754(6.9674) | Total Time 0.00(0.00)\n",
      "Iter 18140 | Time 26.9961(25.2252) | Bit/dim 3.6087(3.6010) | Xent 0.2516(0.2195) | Loss 8.5792(8.9496) | Error 0.0900(0.0780) Steps 1006(930.11) | Grad Norm 15.4557(7.9537) | Total Time 0.00(0.00)\n",
      "Iter 18150 | Time 26.6111(25.4215) | Bit/dim 3.5858(3.5992) | Xent 0.2343(0.2220) | Loss 8.6073(8.8760) | Error 0.0822(0.0791) Steps 952(932.84) | Grad Norm 23.0927(8.4720) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 103.1045, Epoch Time 1518.9952(1462.7434), Bit/dim 3.6088(best: 3.6000), Xent 0.8217, Loss 4.0196, Error 0.2122(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 23.5570(25.0167) | Bit/dim 3.6205(3.6025) | Xent 0.2557(0.2217) | Loss 8.7603(9.4657) | Error 0.0889(0.0787) Steps 892(925.03) | Grad Norm 7.3424(8.5125) | Total Time 0.00(0.00)\n",
      "Iter 18170 | Time 23.8210(24.7755) | Bit/dim 3.5885(3.6010) | Xent 0.2445(0.2215) | Loss 8.7394(9.2635) | Error 0.0856(0.0786) Steps 892(921.57) | Grad Norm 6.0989(8.0082) | Total Time 0.00(0.00)\n",
      "Iter 18180 | Time 23.9370(24.5827) | Bit/dim 3.5915(3.6012) | Xent 0.2004(0.2182) | Loss 8.6761(9.1075) | Error 0.0678(0.0779) Steps 898(922.13) | Grad Norm 5.2818(7.8235) | Total Time 0.00(0.00)\n",
      "Iter 18190 | Time 25.0445(24.4411) | Bit/dim 3.5971(3.5992) | Xent 0.2264(0.2176) | Loss 8.7694(8.9891) | Error 0.0856(0.0770) Steps 970(923.40) | Grad Norm 9.6286(7.6121) | Total Time 0.00(0.00)\n",
      "Iter 18200 | Time 24.1528(24.4019) | Bit/dim 3.5818(3.5972) | Xent 0.1883(0.2158) | Loss 8.5967(8.9015) | Error 0.0644(0.0766) Steps 916(923.87) | Grad Norm 8.4542(7.1679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 103.1315, Epoch Time 1446.2964(1462.2500), Bit/dim 3.6050(best: 3.6000), Xent 0.8014, Loss 4.0057, Error 0.2140(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 23.8043(24.3434) | Bit/dim 3.6008(3.5981) | Xent 0.2411(0.2157) | Loss 8.6170(9.5369) | Error 0.0889(0.0758) Steps 904(922.73) | Grad Norm 7.2013(7.2851) | Total Time 0.00(0.00)\n",
      "Iter 18220 | Time 25.8214(24.4757) | Bit/dim 3.5621(3.5972) | Xent 0.2175(0.2152) | Loss 8.5726(9.3061) | Error 0.0711(0.0762) Steps 898(920.45) | Grad Norm 6.3608(7.4530) | Total Time 0.00(0.00)\n",
      "Iter 18230 | Time 26.4950(24.7436) | Bit/dim 3.5964(3.5983) | Xent 0.2018(0.2195) | Loss 8.5795(9.1503) | Error 0.0789(0.0776) Steps 1030(935.03) | Grad Norm 6.0699(7.3499) | Total Time 0.00(0.00)\n",
      "Iter 18240 | Time 25.4345(24.9585) | Bit/dim 3.5810(3.5981) | Xent 0.2568(0.2177) | Loss 8.6432(9.0229) | Error 0.0911(0.0771) Steps 898(934.32) | Grad Norm 11.9470(7.7407) | Total Time 0.00(0.00)\n",
      "Iter 18250 | Time 25.6687(25.0812) | Bit/dim 3.6016(3.6011) | Xent 0.2118(0.2185) | Loss 8.7323(8.9334) | Error 0.0767(0.0771) Steps 886(929.73) | Grad Norm 5.9389(7.4696) | Total Time 0.00(0.00)\n",
      "Iter 18260 | Time 25.6394(25.0274) | Bit/dim 3.6280(3.6005) | Xent 0.2109(0.2163) | Loss 8.8536(8.8605) | Error 0.0856(0.0763) Steps 910(928.50) | Grad Norm 7.0487(6.9436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 102.9236, Epoch Time 1503.9715(1463.5016), Bit/dim 3.6072(best: 3.6000), Xent 0.7987, Loss 4.0065, Error 0.2134(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 24.2282(24.8392) | Bit/dim 3.6103(3.5967) | Xent 0.2115(0.2155) | Loss 8.7259(9.4430) | Error 0.0722(0.0762) Steps 934(922.63) | Grad Norm 6.0742(7.0444) | Total Time 0.00(0.00)\n",
      "Iter 18280 | Time 23.9936(24.8115) | Bit/dim 3.6095(3.6004) | Xent 0.2332(0.2146) | Loss 8.6936(9.2400) | Error 0.0833(0.0756) Steps 916(925.22) | Grad Norm 8.9080(7.0494) | Total Time 0.00(0.00)\n",
      "Iter 18290 | Time 25.0215(24.8360) | Bit/dim 3.5867(3.5989) | Xent 0.2078(0.2146) | Loss 8.6893(9.0925) | Error 0.0722(0.0755) Steps 910(924.88) | Grad Norm 6.0137(7.1919) | Total Time 0.00(0.00)\n",
      "Iter 18300 | Time 26.4776(24.9130) | Bit/dim 3.6118(3.5990) | Xent 0.2323(0.2168) | Loss 8.7093(8.9891) | Error 0.0856(0.0762) Steps 964(923.12) | Grad Norm 6.3585(7.2845) | Total Time 0.00(0.00)\n",
      "Iter 18310 | Time 25.4342(24.9654) | Bit/dim 3.6420(3.6015) | Xent 0.2072(0.2196) | Loss 8.7921(8.9192) | Error 0.0633(0.0779) Steps 952(925.96) | Grad Norm 4.1319(7.4100) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 107.2633, Epoch Time 1493.2719(1464.3948), Bit/dim 3.6100(best: 3.6000), Xent 0.7974, Loss 4.0087, Error 0.2135(best: 0.2018)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 26.3028(25.1146) | Bit/dim 3.6208(3.6025) | Xent 0.2248(0.2189) | Loss 8.7072(9.6021) | Error 0.0844(0.0784) Steps 928(931.02) | Grad Norm 6.7652(7.4698) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run2/epoch_160_checkpt.pth --seed 2 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
