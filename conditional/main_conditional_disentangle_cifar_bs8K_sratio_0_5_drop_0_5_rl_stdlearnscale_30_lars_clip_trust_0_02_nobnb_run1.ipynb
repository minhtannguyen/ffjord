{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars_nobnb.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class LARC_nobnb(LARC):\n",
      "    def __init__(self, optimizer, trust_coefficient=0.02, clip=True, eps=1e-8):\n",
      "        super(LARC_nobnb, self).__init__(optimizer, trust_coefficient, clip, eps)\n",
      "    \n",
      "    def step(self):\n",
      "        with torch.no_grad():\n",
      "            weight_decays = []\n",
      "            for group in self.optim.param_groups:\n",
      "                # absorb weight decay control from optimizer\n",
      "                weight_decay = group['weight_decay'] if 'weight_decay' in group else 0\n",
      "                weight_decays.append(weight_decay)\n",
      "                group['weight_decay'] = 0\n",
      "                for p in group['params']:\n",
      "                    if p.grad is None:\n",
      "                        continue\n",
      "                        \n",
      "                    if len(p.data.shape) <= 1:\n",
      "                        continue\n",
      "                        \n",
      "                    param_norm = torch.norm(p.data)\n",
      "                    grad_norm = torch.norm(p.grad.data)\n",
      "\n",
      "                    if param_norm != 0 and grad_norm != 0:\n",
      "                        # calculate adaptive lr + weight decay\n",
      "                        adaptive_lr = self.trust_coefficient * (param_norm) / (grad_norm + param_norm * weight_decay + self.eps)\n",
      "\n",
      "                        # clip learning rate for LARC\n",
      "                        if self.clip:\n",
      "                            # calculation of adaptive_lr so that when multiplied by lr it equals `min(adaptive_lr, lr)`\n",
      "                            adaptive_lr = min(adaptive_lr/group['lr'], 1)\n",
      "\n",
      "                        p.grad.data += weight_decay * p.data\n",
      "                        p.grad.data *= adaptive_lr\n",
      "\n",
      "        self.optim.step()\n",
      "        # return weight decay control to optimizer\n",
      "        for i, group in enumerate(self.optim.param_groups):\n",
      "            group['weight_decay'] = weight_decays[i]\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC_nobnb(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_scale_trust_0_02_nobnb_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.02, val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 104.5479(104.5479) | Bit/dim 8.8976(8.8976) | Xent 2.3026(2.3026) | Loss 20.9458(20.9458) | Error 0.8982(0.8982) Steps 430(430.00) | Grad Norm 27.4564(27.4564) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 41.0375(102.6426) | Bit/dim 8.8291(8.8956) | Xent 2.2924(2.3023) | Loss 20.7805(20.9409) | Error 0.7828(0.8948) Steps 478(431.44) | Grad Norm 24.9259(27.3805) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 37.9876(100.7029) | Bit/dim 8.7463(8.8911) | Xent 2.2799(2.3016) | Loss 20.6509(20.9322) | Error 0.7684(0.8910) Steps 490(433.20) | Grad Norm 20.9202(27.1867) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 38.5680(98.8389) | Bit/dim 8.6338(8.8834) | Xent 2.2637(2.3005) | Loss 19.9223(20.9019) | Error 0.7612(0.8871) Steps 466(434.18) | Grad Norm 16.1795(26.8565) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 36.6061(96.9719) | Bit/dim 8.5953(8.8747) | Xent 2.2461(2.2988) | Loss 20.0728(20.8770) | Error 0.7618(0.8833) Steps 442(434.42) | Grad Norm 10.9457(26.3792) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 36.4520(95.1563) | Bit/dim 8.4957(8.8634) | Xent 2.2218(2.2965) | Loss 19.7828(20.8442) | Error 0.7412(0.8791) Steps 454(435.00) | Grad Norm 7.9486(25.8262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 38.3757, Epoch Time 352.4971(352.4971), Bit/dim 8.4569(best: inf), Xent 2.1982, Loss 9.5560, Error 0.7412(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 47.3345(93.7216) | Bit/dim 8.4653(8.8514) | Xent 2.2004(2.2936) | Loss 22.4180(20.8914) | Error 0.7460(0.8751) Steps 460(435.75) | Grad Norm 7.8413(25.2867) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 38.9183(92.0775) | Bit/dim 8.4308(8.8388) | Xent 2.1860(2.2904) | Loss 19.8597(20.8604) | Error 0.7489(0.8713) Steps 454(436.30) | Grad Norm 10.9810(24.8575) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 34.9711(90.3643) | Bit/dim 8.4159(8.8261) | Xent 2.1674(2.2867) | Loss 19.3104(20.8139) | Error 0.7554(0.8678) Steps 448(436.65) | Grad Norm 13.8288(24.5267) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 35.6953(88.7243) | Bit/dim 8.3755(8.8126) | Xent 2.1617(2.2830) | Loss 19.5478(20.7760) | Error 0.7716(0.8649) Steps 454(437.17) | Grad Norm 15.4848(24.2554) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 36.5118(87.1579) | Bit/dim 8.3405(8.7984) | Xent 2.1551(2.2791) | Loss 19.4992(20.7377) | Error 0.7662(0.8620) Steps 430(436.96) | Grad Norm 16.0075(24.0080) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 36.0086(85.6234) | Bit/dim 8.2468(8.7819) | Xent 2.1397(2.2750) | Loss 19.4303(20.6984) | Error 0.7568(0.8588) Steps 466(437.83) | Grad Norm 14.2084(23.7140) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 19.7719, Epoch Time 267.2341(349.9392), Bit/dim 8.1825(best: 8.4569), Xent 2.1082, Loss 9.2366, Error 0.7309(best: 0.7412)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 39.1943(84.2305) | Bit/dim 8.2217(8.7651) | Xent 2.1104(2.2700) | Loss 21.6165(20.7260) | Error 0.7376(0.8552) Steps 484(439.21) | Grad Norm 11.2993(23.3415) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 33.3072(82.7028) | Bit/dim 8.1125(8.7455) | Xent 2.1007(2.2649) | Loss 18.8018(20.6682) | Error 0.7254(0.8513) Steps 430(438.94) | Grad Norm 7.3847(22.8628) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 38.0626(81.3636) | Bit/dim 8.0782(8.7255) | Xent 2.0867(2.2596) | Loss 18.4587(20.6020) | Error 0.7258(0.8475) Steps 454(439.39) | Grad Norm 5.9482(22.3554) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 38.4451(80.0761) | Bit/dim 7.9769(8.7030) | Xent 2.0705(2.2539) | Loss 18.8433(20.5492) | Error 0.7284(0.8439) Steps 418(438.75) | Grad Norm 8.5513(21.9413) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 38.7027(78.8349) | Bit/dim 7.9279(8.6798) | Xent 2.0766(2.2486) | Loss 18.4333(20.4857) | Error 0.7310(0.8406) Steps 460(439.38) | Grad Norm 11.2861(21.6216) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 39.9531(77.6684) | Bit/dim 7.8684(8.6554) | Xent 2.0641(2.2431) | Loss 18.7758(20.4344) | Error 0.7275(0.8372) Steps 466(440.18) | Grad Norm 11.6498(21.3225) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 18.9916, Epoch Time 264.8257(347.3858), Bit/dim 7.7774(best: 8.1825), Xent 2.0380, Loss 8.7963, Error 0.7065(best: 0.7309)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 41.2825(76.5769) | Bit/dim 7.7832(8.6293) | Xent 2.0528(2.2374) | Loss 21.1374(20.4555) | Error 0.7203(0.8337) Steps 490(441.68) | Grad Norm 10.2060(20.9890) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 41.7528(75.5321) | Bit/dim 7.6909(8.6011) | Xent 2.0317(2.2312) | Loss 18.1416(20.3861) | Error 0.6941(0.8295) Steps 442(441.69) | Grad Norm 6.8454(20.5647) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 39.2308(74.4431) | Bit/dim 7.6166(8.5716) | Xent 2.0279(2.2251) | Loss 17.4781(20.2989) | Error 0.6858(0.8252) Steps 436(441.52) | Grad Norm 4.6347(20.0868) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 38.2542(73.3574) | Bit/dim 7.5349(8.5405) | Xent 2.0246(2.2191) | Loss 17.8946(20.2267) | Error 0.6920(0.8212) Steps 454(441.89) | Grad Norm 6.7143(19.6856) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 36.7407(72.2589) | Bit/dim 7.4587(8.5080) | Xent 2.0327(2.2135) | Loss 17.1553(20.1346) | Error 0.7123(0.8179) Steps 424(441.35) | Grad Norm 8.9331(19.3630) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 43.5646(71.3981) | Bit/dim 7.3975(8.4747) | Xent 2.0365(2.2082) | Loss 17.6894(20.0612) | Error 0.7123(0.8147) Steps 442(441.37) | Grad Norm 10.5328(19.0981) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 20.5816, Epoch Time 280.8954(345.3911), Bit/dim 7.3318(best: 7.7774), Xent 2.0139, Loss 8.3388, Error 0.6918(best: 0.7065)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 41.3350(70.4962) | Bit/dim 7.3338(8.4405) | Xent 2.0119(2.2023) | Loss 20.3479(20.0698) | Error 0.6917(0.8110) Steps 424(440.85) | Grad Norm 7.6336(18.7542) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 40.4446(69.5946) | Bit/dim 7.2718(8.4054) | Xent 2.0134(2.1966) | Loss 17.3792(19.9891) | Error 0.6806(0.8071) Steps 484(442.15) | Grad Norm 4.3534(18.3221) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 38.0565(68.6485) | Bit/dim 7.2181(8.3698) | Xent 2.0145(2.1912) | Loss 17.4092(19.9117) | Error 0.6790(0.8033) Steps 466(442.86) | Grad Norm 5.3355(17.9325) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 38.1257(67.7328) | Bit/dim 7.1847(8.3342) | Xent 2.0239(2.1861) | Loss 17.1927(19.8301) | Error 0.6729(0.7994) Steps 478(443.92) | Grad Norm 7.0820(17.6070) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 43.3899(67.0025) | Bit/dim 7.1549(8.2989) | Xent 2.0272(2.1814) | Loss 17.4152(19.7577) | Error 0.6790(0.7958) Steps 478(444.94) | Grad Norm 6.7604(17.2816) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 39.1607(66.1673) | Bit/dim 7.1046(8.2630) | Xent 2.0249(2.1767) | Loss 16.9254(19.6727) | Error 0.6846(0.7924) Steps 454(445.21) | Grad Norm 3.0478(16.8546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 20.6416, Epoch Time 279.4483(343.4128), Bit/dim 7.0868(best: 7.3318), Xent 2.0243, Loss 8.0989, Error 0.6904(best: 0.6918)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 37.4318(65.3052) | Bit/dim 7.0868(8.2277) | Xent 2.0385(2.1725) | Loss 19.5017(19.6676) | Error 0.7085(0.7899) Steps 436(444.93) | Grad Norm 5.0912(16.5017) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 38.5047(64.5012) | Bit/dim 7.0703(8.1930) | Xent 2.0556(2.1690) | Loss 16.9313(19.5855) | Error 0.7335(0.7882) Steps 466(445.57) | Grad Norm 7.1946(16.2225) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 38.0465(63.7076) | Bit/dim 7.0507(8.1588) | Xent 2.0352(2.1650) | Loss 16.6994(19.4989) | Error 0.7235(0.7863) Steps 448(445.64) | Grad Norm 3.4049(15.8380) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 37.3448(62.9167) | Bit/dim 7.0364(8.1251) | Xent 2.0474(2.1615) | Loss 16.7653(19.4169) | Error 0.7203(0.7843) Steps 460(446.07) | Grad Norm 3.9754(15.4821) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 41.4022(62.2712) | Bit/dim 7.0190(8.0919) | Xent 2.0452(2.1580) | Loss 16.9831(19.3439) | Error 0.7100(0.7821) Steps 496(447.57) | Grad Norm 3.3824(15.1191) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 39.8648(61.5990) | Bit/dim 7.0157(8.0596) | Xent 2.0535(2.1549) | Loss 16.7823(19.2671) | Error 0.7176(0.7801) Steps 454(447.76) | Grad Norm 3.8525(14.7811) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 21.5299, Epoch Time 272.2344(341.2775), Bit/dim 7.0105(best: 7.0868), Xent 2.0408, Loss 8.0309, Error 0.7020(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 39.5770(60.9384) | Bit/dim 7.0076(8.0281) | Xent 2.0456(2.1516) | Loss 18.4713(19.2432) | Error 0.7147(0.7782) Steps 490(449.03) | Grad Norm 3.1164(14.4312) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 37.8419(60.2455) | Bit/dim 6.9975(7.9971) | Xent 2.0451(2.1484) | Loss 16.6267(19.1647) | Error 0.7161(0.7763) Steps 478(449.90) | Grad Norm 2.8205(14.0828) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 39.4886(59.6228) | Bit/dim 6.9984(7.9672) | Xent 2.0440(2.1453) | Loss 16.3261(19.0795) | Error 0.7179(0.7746) Steps 496(451.28) | Grad Norm 4.2430(13.7876) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 41.3279(59.0739) | Bit/dim 6.9965(7.9381) | Xent 2.0451(2.1422) | Loss 16.7039(19.0083) | Error 0.7214(0.7730) Steps 484(452.26) | Grad Norm 3.4243(13.4767) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 42.3567(58.5724) | Bit/dim 6.9849(7.9095) | Xent 2.0409(2.1392) | Loss 16.9859(18.9476) | Error 0.7031(0.7709) Steps 484(453.21) | Grad Norm 3.4082(13.1747) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 43.7503(58.1278) | Bit/dim 6.9798(7.8816) | Xent 2.0344(2.1361) | Loss 16.5485(18.8756) | Error 0.7004(0.7688) Steps 514(455.04) | Grad Norm 7.9920(13.0192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 21.7184, Epoch Time 284.8081(339.5834), Bit/dim 6.9832(best: 7.0105), Xent 2.0266, Loss 7.9965, Error 0.7094(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 43.8539(57.6995) | Bit/dim 6.9731(7.8543) | Xent 2.0340(2.1330) | Loss 19.2806(18.8878) | Error 0.7186(0.7672) Steps 466(455.37) | Grad Norm 14.1234(13.0523) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 45.2275(57.3254) | Bit/dim 6.9843(7.8282) | Xent 2.0918(2.1318) | Loss 16.6805(18.8216) | Error 0.7522(0.7668) Steps 514(457.13) | Grad Norm 32.2436(13.6281) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 48.8743(57.0718) | Bit/dim 6.9716(7.8025) | Xent 2.0290(2.1287) | Loss 16.9962(18.7668) | Error 0.7091(0.7651) Steps 490(458.11) | Grad Norm 17.5667(13.7462) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 48.7512(56.8222) | Bit/dim 6.9582(7.7772) | Xent 2.0118(2.1252) | Loss 16.4873(18.6984) | Error 0.6905(0.7628) Steps 538(460.51) | Grad Norm 11.7758(13.6871) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 49.1725(56.5927) | Bit/dim 6.9580(7.7526) | Xent 1.9989(2.1214) | Loss 16.6353(18.6365) | Error 0.6885(0.7606) Steps 502(461.75) | Grad Norm 9.8961(13.5734) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 47.0614(56.3068) | Bit/dim 6.9480(7.7285) | Xent 2.0209(2.1184) | Loss 16.8266(18.5822) | Error 0.7015(0.7588) Steps 490(462.60) | Grad Norm 18.8561(13.7319) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 21.1967, Epoch Time 322.6872(339.0765), Bit/dim 6.9572(best: 6.9832), Xent 2.1027, Loss 8.0085, Error 0.7519(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 41.5742(55.8648) | Bit/dim 6.9428(7.7049) | Xent 2.1073(2.1180) | Loss 19.0526(18.5963) | Error 0.7564(0.7588) Steps 472(462.88) | Grad Norm 40.4609(14.5337) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 47.5184(55.6144) | Bit/dim 6.9355(7.6818) | Xent 1.9924(2.1143) | Loss 16.6961(18.5393) | Error 0.6826(0.7565) Steps 484(463.52) | Grad Norm 9.4214(14.3804) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 43.0839(55.2385) | Bit/dim 6.9507(7.6599) | Xent 2.2665(2.1188) | Loss 16.7845(18.4867) | Error 0.7954(0.7576) Steps 466(463.59) | Grad Norm 59.8198(15.7436) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 42.8119(54.8657) | Bit/dim 6.9294(7.6380) | Xent 2.0823(2.1177) | Loss 15.8946(18.4089) | Error 0.7479(0.7573) Steps 496(464.56) | Grad Norm 30.8637(16.1972) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 49.3429(54.7000) | Bit/dim 6.9665(7.6178) | Xent 2.2918(2.1230) | Loss 17.0190(18.3672) | Error 0.7876(0.7583) Steps 550(467.13) | Grad Norm 62.8933(17.5980) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 43.2725(54.3572) | Bit/dim 6.9829(7.5988) | Xent 2.3655(2.1302) | Loss 16.9678(18.3252) | Error 0.8115(0.7599) Steps 490(467.81) | Grad Norm 65.5382(19.0363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 21.1945, Epoch Time 307.7164(338.1357), Bit/dim 6.8875(best: 6.9572), Xent 2.0117, Loss 7.8934, Error 0.7079(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 46.3326(54.1165) | Bit/dim 6.8760(7.5771) | Xent 2.0194(2.1269) | Loss 19.1841(18.3510) | Error 0.7135(0.7585) Steps 502(468.84) | Grad Norm 12.5105(18.8405) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 46.3961(53.8849) | Bit/dim 6.9763(7.5591) | Xent 2.1941(2.1289) | Loss 16.9457(18.3088) | Error 0.7762(0.7590) Steps 490(469.47) | Grad Norm 50.5083(19.7905) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 43.6250(53.5771) | Bit/dim 6.9675(7.5413) | Xent 2.2370(2.1322) | Loss 16.5666(18.2566) | Error 0.7879(0.7599) Steps 508(470.63) | Grad Norm 35.1864(20.2524) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 42.1151(53.2332) | Bit/dim 6.8713(7.5212) | Xent 2.1212(2.1318) | Loss 16.5209(18.2045) | Error 0.7745(0.7603) Steps 478(470.85) | Grad Norm 27.4784(20.4692) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 43.0957(52.9291) | Bit/dim 6.9126(7.5030) | Xent 2.0271(2.1287) | Loss 16.6307(18.1573) | Error 0.7046(0.7586) Steps 490(471.42) | Grad Norm 15.8609(20.3309) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 40.9646(52.5701) | Bit/dim 6.8680(7.4839) | Xent 2.0945(2.1277) | Loss 16.5484(18.1090) | Error 0.7170(0.7574) Steps 484(471.80) | Grad Norm 17.7921(20.2548) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.5638, Epoch Time 302.4578(337.0654), Bit/dim 6.8993(best: 6.8875), Xent 2.0115, Loss 7.9051, Error 0.7194(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 41.3482(52.2335) | Bit/dim 6.8893(7.4661) | Xent 2.0168(2.1244) | Loss 19.0955(18.1386) | Error 0.7083(0.7559) Steps 484(472.17) | Grad Norm 15.2593(20.1049) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 41.9889(51.9261) | Bit/dim 6.8658(7.4481) | Xent 2.0303(2.1215) | Loss 16.2547(18.0821) | Error 0.7188(0.7548) Steps 472(472.16) | Grad Norm 13.6997(19.9127) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 43.7460(51.6807) | Bit/dim 6.8033(7.4287) | Xent 2.0283(2.1187) | Loss 15.8788(18.0160) | Error 0.6991(0.7531) Steps 490(472.70) | Grad Norm 16.6963(19.8162) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 44.7702(51.4734) | Bit/dim 6.8403(7.4111) | Xent 2.0114(2.1155) | Loss 16.5420(17.9718) | Error 0.6824(0.7510) Steps 460(472.32) | Grad Norm 8.5493(19.4782) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 39.5352(51.1153) | Bit/dim 6.8324(7.3937) | Xent 2.0575(2.1138) | Loss 16.4570(17.9263) | Error 0.7341(0.7505) Steps 478(472.49) | Grad Norm 19.6352(19.4829) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 49.4273(51.0646) | Bit/dim 6.7693(7.3750) | Xent 2.0053(2.1105) | Loss 16.3816(17.8800) | Error 0.6793(0.7484) Steps 484(472.83) | Grad Norm 11.2721(19.2366) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 20.8484, Epoch Time 299.7677(335.9464), Bit/dim 6.7775(best: 6.8875), Xent 1.9990, Loss 7.7769, Error 0.6671(best: 0.6904)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 41.9345(50.7907) | Bit/dim 6.7754(7.3570) | Xent 2.0136(2.1076) | Loss 18.6675(17.9036) | Error 0.6780(0.7462) Steps 466(472.63) | Grad Norm 13.3220(19.0592) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 45.3400(50.6272) | Bit/dim 6.7354(7.3384) | Xent 2.0389(2.1056) | Loss 16.5572(17.8632) | Error 0.7165(0.7454) Steps 520(474.05) | Grad Norm 16.3824(18.9789) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 41.2514(50.3459) | Bit/dim 6.6988(7.3192) | Xent 1.9998(2.1024) | Loss 16.2294(17.8142) | Error 0.6725(0.7432) Steps 460(473.63) | Grad Norm 5.4506(18.5730) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 44.8421(50.1808) | Bit/dim 6.6849(7.3001) | Xent 2.0212(2.0999) | Loss 16.3480(17.7702) | Error 0.7009(0.7419) Steps 478(473.76) | Grad Norm 12.1364(18.3799) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 42.8153(49.9599) | Bit/dim 6.6605(7.2809) | Xent 1.9804(2.0964) | Loss 15.9876(17.7167) | Error 0.6709(0.7398) Steps 490(474.25) | Grad Norm 10.2810(18.1370) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 43.2524(49.7586) | Bit/dim 6.6143(7.2609) | Xent 1.9925(2.0932) | Loss 15.9893(17.6649) | Error 0.6750(0.7378) Steps 478(474.36) | Grad Norm 5.2866(17.7515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 20.5855, Epoch Time 298.7187(334.8296), Bit/dim 6.5940(best: 6.7775), Xent 1.9786, Loss 7.5833, Error 0.6664(best: 0.6671)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 43.9568(49.5846) | Bit/dim 6.5883(7.2408) | Xent 1.9963(2.0903) | Loss 17.8258(17.6698) | Error 0.6746(0.7359) Steps 496(475.01) | Grad Norm 18.1645(17.7638) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 41.2238(49.3338) | Bit/dim 6.5361(7.2196) | Xent 2.0340(2.0886) | Loss 15.9892(17.6193) | Error 0.7145(0.7353) Steps 460(474.56) | Grad Norm 24.8754(17.9772) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 41.7705(49.1069) | Bit/dim 6.4736(7.1972) | Xent 1.9993(2.0860) | Loss 15.1623(17.5456) | Error 0.6875(0.7339) Steps 460(474.12) | Grad Norm 18.4066(17.9901) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 39.9114(48.8310) | Bit/dim 6.4645(7.1753) | Xent 1.9988(2.0834) | Loss 15.7179(17.4908) | Error 0.7031(0.7329) Steps 472(474.06) | Grad Norm 28.3620(18.3012) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 41.5988(48.6140) | Bit/dim 6.3827(7.1515) | Xent 2.0144(2.0813) | Loss 15.3797(17.4275) | Error 0.7041(0.7321) Steps 496(474.72) | Grad Norm 25.4225(18.5149) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 40.9259(48.3834) | Bit/dim 6.3130(7.1263) | Xent 2.0238(2.0796) | Loss 15.4582(17.3684) | Error 0.7072(0.7313) Steps 448(473.91) | Grad Norm 36.8171(19.0639) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 20.7364, Epoch Time 289.4483(333.4682), Bit/dim 6.2927(best: 6.5940), Xent 1.9782, Loss 7.2818, Error 0.6663(best: 0.6664)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 44.8763(48.2782) | Bit/dim 6.2916(7.1013) | Xent 1.9871(2.0768) | Loss 18.0285(17.3882) | Error 0.6823(0.7298) Steps 490(474.40) | Grad Norm 36.3407(19.5822) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 43.8088(48.1441) | Bit/dim 6.1970(7.0742) | Xent 1.9986(2.0744) | Loss 15.3546(17.3272) | Error 0.6915(0.7287) Steps 466(474.14) | Grad Norm 31.1076(19.9280) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 45.0252(48.0505) | Bit/dim 6.1468(7.0463) | Xent 2.1068(2.0754) | Loss 15.2046(17.2635) | Error 0.7685(0.7299) Steps 472(474.08) | Grad Norm 65.3214(21.2898) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 41.5944(47.8568) | Bit/dim 6.1219(7.0186) | Xent 2.0011(2.0732) | Loss 14.8267(17.1904) | Error 0.7232(0.7297) Steps 454(473.48) | Grad Norm 29.8033(21.5452) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 41.4930(47.6659) | Bit/dim 6.0665(6.9900) | Xent 2.0057(2.0712) | Loss 14.7472(17.1171) | Error 0.6987(0.7288) Steps 442(472.53) | Grad Norm 32.5205(21.8745) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 43.7523(47.5485) | Bit/dim 5.9948(6.9602) | Xent 2.0563(2.0707) | Loss 14.7485(17.0460) | Error 0.7331(0.7289) Steps 502(473.42) | Grad Norm 57.0489(22.9297) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 21.0352, Epoch Time 299.8586(332.4599), Bit/dim 5.9874(best: 6.2927), Xent 2.1032, Loss 7.0390, Error 0.7587(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 40.6711(47.3422) | Bit/dim 5.9896(6.9311) | Xent 2.1166(2.0721) | Loss 17.0827(17.0471) | Error 0.7691(0.7301) Steps 460(473.01) | Grad Norm 79.4603(24.6256) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 43.4007(47.2239) | Bit/dim 5.9886(6.9028) | Xent 2.0125(2.0703) | Loss 14.6304(16.9746) | Error 0.7394(0.7304) Steps 484(473.34) | Grad Norm 46.0696(25.2689) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 44.7958(47.1511) | Bit/dim 5.9921(6.8755) | Xent 2.0225(2.0689) | Loss 14.9702(16.9145) | Error 0.6980(0.7294) Steps 484(473.66) | Grad Norm 72.9781(26.7002) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 42.8981(47.0235) | Bit/dim 6.0136(6.8496) | Xent 2.3831(2.0783) | Loss 15.0989(16.8600) | Error 0.8251(0.7323) Steps 472(473.61) | Grad Norm 126.7861(29.7028) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 41.9353(46.8709) | Bit/dim 5.9808(6.8236) | Xent 2.0670(2.0780) | Loss 14.5799(16.7916) | Error 0.7450(0.7327) Steps 484(473.93) | Grad Norm 41.9481(30.0701) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 42.3008(46.7338) | Bit/dim 6.1121(6.8022) | Xent 2.5979(2.0936) | Loss 15.5341(16.7539) | Error 0.8504(0.7362) Steps 478(474.05) | Grad Norm 193.3813(34.9695) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 21.1856, Epoch Time 295.7304(331.3580), Bit/dim 6.2114(best: 5.9874), Xent 2.3606, Loss 7.3918, Error 0.8221(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 42.4339(46.6048) | Bit/dim 6.2268(6.7849) | Xent 2.3875(2.1024) | Loss 17.7365(16.7834) | Error 0.8224(0.7388) Steps 484(474.35) | Grad Norm 126.4044(37.7125) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 42.6962(46.4875) | Bit/dim 6.5642(6.7783) | Xent 2.3027(2.1084) | Loss 15.9064(16.7571) | Error 0.7957(0.7405) Steps 490(474.82) | Grad Norm 194.1792(42.4065) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 39.7798(46.2863) | Bit/dim 6.1823(6.7604) | Xent 2.1647(2.1101) | Loss 15.2750(16.7126) | Error 0.7737(0.7415) Steps 472(474.73) | Grad Norm 116.6496(44.6338) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 44.4427(46.2310) | Bit/dim 6.0168(6.7381) | Xent 2.1494(2.1113) | Loss 14.8885(16.6579) | Error 0.7506(0.7418) Steps 496(475.37) | Grad Norm 83.4777(45.7991) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 42.8044(46.1282) | Bit/dim 5.8811(6.7124) | Xent 2.1042(2.1110) | Loss 14.6320(16.5971) | Error 0.7511(0.7420) Steps 472(475.27) | Grad Norm 60.4217(46.2378) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 42.1677(46.0094) | Bit/dim 5.9535(6.6897) | Xent 2.1106(2.1110) | Loss 14.6803(16.5396) | Error 0.7562(0.7425) Steps 490(475.71) | Grad Norm 60.0004(46.6507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 20.8618, Epoch Time 293.8451(330.2326), Bit/dim 5.8584(best: 5.9874), Xent 2.0680, Loss 6.8923, Error 0.7295(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 40.1401(45.8333) | Bit/dim 5.8497(6.6645) | Xent 2.0823(2.1102) | Loss 17.3655(16.5644) | Error 0.7392(0.7424) Steps 472(475.60) | Grad Norm 44.2328(46.5782) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 40.6935(45.6791) | Bit/dim 5.8882(6.6412) | Xent 2.0543(2.1085) | Loss 14.3943(16.4993) | Error 0.7299(0.7420) Steps 472(475.49) | Grad Norm 39.4187(46.3634) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 44.0934(45.6315) | Bit/dim 5.7945(6.6158) | Xent 2.0643(2.1072) | Loss 14.4157(16.4368) | Error 0.7249(0.7415) Steps 466(475.21) | Grad Norm 19.9858(45.5720) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 40.3490(45.4730) | Bit/dim 5.9093(6.5946) | Xent 2.1126(2.1073) | Loss 14.6749(16.3839) | Error 0.7471(0.7417) Steps 472(475.11) | Grad Norm 48.5424(45.6612) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 41.9468(45.3673) | Bit/dim 5.8544(6.5724) | Xent 2.0672(2.1061) | Loss 14.7067(16.3336) | Error 0.7218(0.7411) Steps 436(473.94) | Grad Norm 33.6924(45.3021) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 40.0434(45.2075) | Bit/dim 5.7694(6.5483) | Xent 2.0701(2.1050) | Loss 14.0116(16.2639) | Error 0.7224(0.7405) Steps 460(473.52) | Grad Norm 16.8093(44.4473) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 20.3422, Epoch Time 286.9052(328.9328), Bit/dim 5.7789(best: 5.8584), Xent 2.0353, Loss 6.7966, Error 0.6774(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 40.8162(45.0758) | Bit/dim 5.7840(6.5253) | Xent 2.0527(2.1035) | Loss 17.0137(16.2864) | Error 0.7021(0.7393) Steps 454(472.93) | Grad Norm 17.5687(43.6410) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 40.1270(44.9273) | Bit/dim 5.7056(6.5008) | Xent 2.0403(2.1016) | Loss 14.0321(16.2188) | Error 0.7163(0.7387) Steps 460(472.55) | Grad Norm 9.9585(42.6305) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 43.9150(44.8970) | Bit/dim 5.7164(6.4772) | Xent 2.0523(2.1001) | Loss 14.2435(16.1595) | Error 0.7363(0.7386) Steps 472(472.53) | Grad Norm 15.3362(41.8117) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 41.6976(44.8010) | Bit/dim 5.6550(6.4526) | Xent 2.0370(2.0982) | Loss 14.0599(16.0966) | Error 0.7103(0.7377) Steps 484(472.87) | Grad Norm 8.1521(40.8019) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 42.2201(44.7236) | Bit/dim 5.6393(6.4282) | Xent 2.0419(2.0965) | Loss 14.1738(16.0389) | Error 0.7069(0.7368) Steps 466(472.67) | Grad Norm 9.8385(39.8730) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 39.7522(44.5744) | Bit/dim 5.6222(6.4040) | Xent 2.0404(2.0948) | Loss 14.1905(15.9834) | Error 0.7031(0.7358) Steps 478(472.83) | Grad Norm 5.9299(38.8547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 20.8892, Epoch Time 287.9843(327.7043), Bit/dim 5.6039(best: 5.7789), Xent 2.0295, Loss 6.6186, Error 0.6987(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 40.9292(44.4651) | Bit/dim 5.6105(6.3802) | Xent 2.0466(2.0934) | Loss 16.9499(16.0124) | Error 0.7106(0.7350) Steps 496(473.52) | Grad Norm 9.9077(37.9863) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 42.1113(44.3944) | Bit/dim 5.5678(6.3558) | Xent 2.0134(2.0910) | Loss 13.4403(15.9353) | Error 0.6911(0.7337) Steps 442(472.58) | Grad Norm 4.2861(36.9753) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 41.1881(44.2983) | Bit/dim 5.5656(6.3321) | Xent 2.0135(2.0887) | Loss 13.9857(15.8768) | Error 0.7011(0.7327) Steps 448(471.84) | Grad Norm 7.7992(36.1000) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 41.6912(44.2200) | Bit/dim 5.5624(6.3090) | Xent 2.0103(2.0863) | Loss 14.0415(15.8217) | Error 0.6920(0.7315) Steps 466(471.66) | Grad Norm 14.9796(35.4664) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 39.7647(44.0864) | Bit/dim 5.5885(6.2874) | Xent 2.0554(2.0854) | Loss 13.8559(15.7627) | Error 0.7291(0.7314) Steps 442(470.77) | Grad Norm 35.3105(35.4617) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 38.6217(43.9224) | Bit/dim 5.5220(6.2644) | Xent 2.0055(2.0830) | Loss 13.8309(15.7048) | Error 0.7006(0.7305) Steps 448(470.09) | Grad Norm 18.5427(34.9541) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 19.5489, Epoch Time 282.4943(326.3480), Bit/dim 5.4735(best: 5.6039), Xent 1.9546, Loss 6.4508, Error 0.6623(best: 0.6663)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 45.1322(43.9587) | Bit/dim 5.4721(6.2407) | Xent 1.9820(2.0800) | Loss 16.4628(15.7275) | Error 0.6831(0.7291) Steps 436(469.07) | Grad Norm 10.3886(34.2171) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 43.0858(43.9325) | Bit/dim 5.4643(6.2174) | Xent 1.9628(2.0764) | Loss 13.4096(15.6580) | Error 0.6695(0.7273) Steps 472(469.16) | Grad Norm 5.0074(33.3409) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 40.7576(43.8373) | Bit/dim 5.4237(6.1936) | Xent 1.9636(2.0731) | Loss 13.3929(15.5900) | Error 0.6760(0.7258) Steps 466(469.06) | Grad Norm 9.7307(32.6325) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 45.4083(43.8844) | Bit/dim 5.4088(6.1700) | Xent 1.9471(2.0693) | Loss 13.3144(15.5218) | Error 0.6655(0.7240) Steps 472(469.15) | Grad Norm 12.2320(32.0205) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 41.8210(43.8225) | Bit/dim 5.3778(6.1463) | Xent 1.9518(2.0658) | Loss 13.4839(15.4606) | Error 0.6763(0.7225) Steps 442(468.34) | Grad Norm 11.8539(31.4155) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 39.3509(43.6884) | Bit/dim 5.3549(6.1225) | Xent 1.9476(2.0622) | Loss 13.4610(15.4006) | Error 0.6764(0.7211) Steps 454(467.91) | Grad Norm 15.0387(30.9242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 19.9922, Epoch Time 295.0021(325.4076), Bit/dim 5.3321(best: 5.4735), Xent 1.9525, Loss 6.3083, Error 0.6882(best: 0.6623)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 42.1123(43.6411) | Bit/dim 5.3285(6.0987) | Xent 1.9649(2.0593) | Loss 15.9450(15.4170) | Error 0.6866(0.7201) Steps 448(467.31) | Grad Norm 26.3818(30.7880) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 43.3668(43.6329) | Bit/dim 5.3463(6.0761) | Xent 1.9871(2.0571) | Loss 13.5871(15.3621) | Error 0.7001(0.7195) Steps 454(466.91) | Grad Norm 27.0898(30.6770) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 45.1043(43.6770) | Bit/dim 5.4819(6.0583) | Xent 2.0834(2.0579) | Loss 13.8821(15.3177) | Error 0.7418(0.7202) Steps 478(467.24) | Grad Norm 83.4686(32.2608) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 43.1043(43.6598) | Bit/dim 5.3288(6.0364) | Xent 1.9726(2.0554) | Loss 13.3074(15.2574) | Error 0.6961(0.7195) Steps 430(466.12) | Grad Norm 36.1556(32.3776) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 44.0258(43.6708) | Bit/dim 5.2569(6.0130) | Xent 1.9467(2.0521) | Loss 12.6896(15.1803) | Error 0.6614(0.7177) Steps 472(466.30) | Grad Norm 7.4754(31.6305) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 44.4953(43.6955) | Bit/dim 5.3714(5.9938) | Xent 1.9585(2.0493) | Loss 13.4524(15.1285) | Error 0.6860(0.7168) Steps 448(465.75) | Grad Norm 48.1229(32.1253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 19.9463, Epoch Time 300.0400(324.6466), Bit/dim 5.2909(best: 5.3321), Xent 1.9471, Loss 6.2645, Error 0.6707(best: 0.6623)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 42.8461(43.6701) | Bit/dim 5.2972(5.9729) | Xent 1.9736(2.0470) | Loss 15.9408(15.1529) | Error 0.6853(0.7158) Steps 436(464.86) | Grad Norm 28.8640(32.0275) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 41.2997(43.5989) | Bit/dim 5.2415(5.9509) | Xent 1.9388(2.0438) | Loss 13.1649(15.0932) | Error 0.6764(0.7146) Steps 454(464.53) | Grad Norm 8.7047(31.3278) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 43.2863(43.5896) | Bit/dim 5.2929(5.9312) | Xent 1.9751(2.0417) | Loss 13.2657(15.0384) | Error 0.6892(0.7139) Steps 484(465.12) | Grad Norm 28.1290(31.2318) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 38.9314(43.4498) | Bit/dim 5.2207(5.9099) | Xent 1.9774(2.0398) | Loss 12.9083(14.9745) | Error 0.6970(0.7134) Steps 436(464.24) | Grad Norm 16.5142(30.7903) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 44.2918(43.4751) | Bit/dim 5.2241(5.8893) | Xent 1.9477(2.0370) | Loss 13.2821(14.9237) | Error 0.6827(0.7125) Steps 490(465.02) | Grad Norm 8.3789(30.1180) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 42.9836(43.4603) | Bit/dim 5.1761(5.8679) | Xent 1.9367(2.0340) | Loss 13.1350(14.8701) | Error 0.6709(0.7112) Steps 478(465.41) | Grad Norm 14.0423(29.6357) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 20.2183, Epoch Time 292.1194(323.6708), Bit/dim 5.1796(best: 5.2909), Xent 1.9326, Loss 6.1458, Error 0.6718(best: 0.6623)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 41.6021(43.4046) | Bit/dim 5.1727(5.8471) | Xent 1.9632(2.0319) | Loss 15.5900(14.8917) | Error 0.6874(0.7105) Steps 460(465.24) | Grad Norm 13.5777(29.1539) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 43.7134(43.4139) | Bit/dim 5.1644(5.8266) | Xent 1.9100(2.0282) | Loss 12.9711(14.8340) | Error 0.6564(0.7089) Steps 472(465.45) | Grad Norm 8.2824(28.5278) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 43.8172(43.4260) | Bit/dim 5.1388(5.8059) | Xent 1.9328(2.0254) | Loss 12.9487(14.7775) | Error 0.6786(0.7080) Steps 460(465.28) | Grad Norm 7.5497(27.8985) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 43.4916(43.4279) | Bit/dim 5.1224(5.7854) | Xent 1.9392(2.0228) | Loss 12.9465(14.7226) | Error 0.6784(0.7071) Steps 460(465.12) | Grad Norm 16.2392(27.5487) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 45.6087(43.4933) | Bit/dim 5.1459(5.7663) | Xent 2.0521(2.0237) | Loss 12.9696(14.6700) | Error 0.7255(0.7076) Steps 466(465.15) | Grad Norm 32.5729(27.6994) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 42.0542(43.4502) | Bit/dim 5.1447(5.7476) | Xent 1.9663(2.0219) | Loss 12.9784(14.6192) | Error 0.6813(0.7068) Steps 454(464.82) | Grad Norm 18.8880(27.4351) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 20.5853, Epoch Time 300.0370(322.9618), Bit/dim 5.1405(best: 5.1796), Xent 1.9071, Loss 6.0940, Error 0.6572(best: 0.6623)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 46.3099(43.5360) | Bit/dim 5.1396(5.7294) | Xent 1.9325(2.0193) | Loss 15.7145(14.6521) | Error 0.6713(0.7058) Steps 490(465.57) | Grad Norm 21.8189(27.2666) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 46.4795(43.6243) | Bit/dim 5.1296(5.7114) | Xent 1.9274(2.0165) | Loss 13.1205(14.6061) | Error 0.6667(0.7046) Steps 478(465.94) | Grad Norm 12.9812(26.8380) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 43.6863(43.6261) | Bit/dim 5.0912(5.6928) | Xent 1.9203(2.0136) | Loss 12.6163(14.5464) | Error 0.6679(0.7035) Steps 478(466.31) | Grad Norm 6.2097(26.2192) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 43.5341(43.6234) | Bit/dim 5.1131(5.6754) | Xent 1.9426(2.0115) | Loss 12.4550(14.4837) | Error 0.6723(0.7026) Steps 478(466.66) | Grad Norm 16.3612(25.9234) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 45.6091(43.6829) | Bit/dim 5.0509(5.6566) | Xent 1.9285(2.0090) | Loss 12.9862(14.4388) | Error 0.6676(0.7015) Steps 472(466.82) | Grad Norm 8.8685(25.4118) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 42.7282(43.6543) | Bit/dim 5.0695(5.6390) | Xent 1.9375(2.0068) | Loss 12.7479(14.3880) | Error 0.6759(0.7007) Steps 460(466.61) | Grad Norm 12.8397(25.0346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 20.7271, Epoch Time 309.1763(322.5482), Bit/dim 5.0350(best: 5.1405), Xent 1.8835, Loss 5.9767, Error 0.6349(best: 0.6572)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 43.5089(43.6499) | Bit/dim 5.0452(5.6212) | Xent 1.9140(2.0041) | Loss 15.0925(14.4092) | Error 0.6591(0.6995) Steps 466(466.59) | Grad Norm 5.7101(24.4549) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 41.7551(43.5931) | Bit/dim 5.0185(5.6031) | Xent 1.9063(2.0011) | Loss 12.3894(14.3486) | Error 0.6664(0.6985) Steps 460(466.40) | Grad Norm 6.9897(23.9309) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 43.0191(43.5759) | Bit/dim 5.0377(5.5862) | Xent 1.9012(1.9981) | Loss 12.6065(14.2963) | Error 0.6650(0.6975) Steps 466(466.38) | Grad Norm 8.5428(23.4693) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 45.0539(43.6202) | Bit/dim 5.0014(5.5686) | Xent 1.9024(1.9953) | Loss 12.8254(14.2522) | Error 0.6538(0.6962) Steps 484(466.91) | Grad Norm 7.4111(22.9875) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 41.9458(43.5700) | Bit/dim 4.9809(5.5510) | Xent 1.8727(1.9916) | Loss 12.6137(14.2030) | Error 0.6465(0.6947) Steps 472(467.07) | Grad Norm 8.2574(22.5456) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 45.4964(43.6278) | Bit/dim 4.9855(5.5340) | Xent 1.8757(1.9881) | Loss 12.5199(14.1525) | Error 0.6516(0.6934) Steps 436(466.13) | Grad Norm 7.4177(22.0918) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.3343, Epoch Time 300.2729(321.8800), Bit/dim 4.9755(best: 5.0350), Xent 1.8546, Loss 5.9028, Error 0.6295(best: 0.6349)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 44.8940(43.6658) | Bit/dim 4.9818(5.5175) | Xent 1.8742(1.9847) | Loss 15.5274(14.1938) | Error 0.6514(0.6921) Steps 484(466.67) | Grad Norm 11.3047(21.7682) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 44.1165(43.6793) | Bit/dim 4.9637(5.5009) | Xent 1.9294(1.9830) | Loss 12.3500(14.1385) | Error 0.6807(0.6918) Steps 484(467.19) | Grad Norm 18.7751(21.6784) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 42.9549(43.6576) | Bit/dim 4.9642(5.4848) | Xent 2.0353(1.9846) | Loss 12.6156(14.0928) | Error 0.7243(0.6928) Steps 472(467.33) | Grad Norm 25.7186(21.7996) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 44.1800(43.6732) | Bit/dim 4.9682(5.4693) | Xent 1.9436(1.9834) | Loss 12.3237(14.0397) | Error 0.6924(0.6928) Steps 472(467.47) | Grad Norm 23.1655(21.8406) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 43.5954(43.6709) | Bit/dim 5.0117(5.4555) | Xent 1.9836(1.9834) | Loss 12.3509(13.9891) | Error 0.7040(0.6931) Steps 484(467.97) | Grad Norm 26.2343(21.9724) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 44.9773(43.7101) | Bit/dim 4.9249(5.4396) | Xent 1.8671(1.9799) | Loss 12.4168(13.9419) | Error 0.6476(0.6917) Steps 478(468.27) | Grad Norm 7.6282(21.5421) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 19.7697, Epoch Time 302.7239(321.3053), Bit/dim 4.9571(best: 4.9755), Xent 1.9391, Loss 5.9267, Error 0.7014(best: 0.6295)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 45.8615(43.7746) | Bit/dim 4.9643(5.4254) | Xent 1.9545(1.9791) | Loss 15.0009(13.9737) | Error 0.7056(0.6921) Steps 466(468.20) | Grad Norm 22.8681(21.5818) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 42.4831(43.7359) | Bit/dim 4.9465(5.4110) | Xent 1.9337(1.9778) | Loss 12.5437(13.9308) | Error 0.6807(0.6918) Steps 454(467.78) | Grad Norm 19.5692(21.5215) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 45.1277(43.7776) | Bit/dim 4.9137(5.3961) | Xent 1.8736(1.9746) | Loss 12.4511(13.8864) | Error 0.6550(0.6907) Steps 448(467.18) | Grad Norm 8.0040(21.1159) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 43.3312(43.7642) | Bit/dim 4.9112(5.3815) | Xent 1.8928(1.9722) | Loss 12.1888(13.8354) | Error 0.6716(0.6901) Steps 442(466.43) | Grad Norm 10.5445(20.7988) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 48.1027(43.8944) | Bit/dim 4.8993(5.3671) | Xent 1.8972(1.9699) | Loss 12.6606(13.8002) | Error 0.6614(0.6893) Steps 454(466.05) | Grad Norm 12.9469(20.5632) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 47.5019(44.0026) | Bit/dim 4.8534(5.3516) | Xent 1.8659(1.9668) | Loss 11.9347(13.7442) | Error 0.6547(0.6882) Steps 508(467.31) | Grad Norm 4.3480(20.0768) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.6671, Epoch Time 311.6105(321.0144), Bit/dim 4.8857(best: 4.9571), Xent 1.8302, Loss 5.8008, Error 0.6273(best: 0.6295)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 46.2271(44.0694) | Bit/dim 4.8738(5.3373) | Xent 1.8541(1.9634) | Loss 15.0347(13.7829) | Error 0.6409(0.6868) Steps 484(467.81) | Grad Norm 11.6539(19.8241) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 46.4003(44.1393) | Bit/dim 4.8751(5.3234) | Xent 1.8550(1.9602) | Loss 12.3978(13.7414) | Error 0.6490(0.6857) Steps 514(469.20) | Grad Norm 9.5609(19.5162) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 42.6445(44.0944) | Bit/dim 4.8472(5.3092) | Xent 1.8695(1.9575) | Loss 12.2108(13.6955) | Error 0.6515(0.6847) Steps 466(469.10) | Grad Norm 3.7986(19.0447) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 43.6983(44.0826) | Bit/dim 4.8546(5.2955) | Xent 1.8742(1.9550) | Loss 12.1686(13.6497) | Error 0.6502(0.6836) Steps 454(468.65) | Grad Norm 8.6341(18.7324) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 40.9151(43.9875) | Bit/dim 4.8462(5.2820) | Xent 1.8407(1.9515) | Loss 12.3490(13.6106) | Error 0.6441(0.6824) Steps 454(468.21) | Grad Norm 6.5143(18.3658) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 42.7716(43.9511) | Bit/dim 4.8392(5.2688) | Xent 1.8217(1.9476) | Loss 12.0772(13.5646) | Error 0.6311(0.6809) Steps 448(467.60) | Grad Norm 7.6223(18.0435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 21.5705, Epoch Time 303.0938(320.4768), Bit/dim 4.8242(best: 4.8857), Xent 1.8161, Loss 5.7322, Error 0.6264(best: 0.6273)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 40.8715(43.8587) | Bit/dim 4.8266(5.2555) | Xent 1.8564(1.9449) | Loss 14.7791(13.6011) | Error 0.6431(0.6798) Steps 448(467.02) | Grad Norm 11.0232(17.8329) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 44.5250(43.8787) | Bit/dim 4.7956(5.2417) | Xent 1.8098(1.9408) | Loss 12.1057(13.5562) | Error 0.6358(0.6784) Steps 478(467.35) | Grad Norm 6.9334(17.5059) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 43.4618(43.8662) | Bit/dim 4.7821(5.2279) | Xent 1.8340(1.9376) | Loss 12.1472(13.5139) | Error 0.6427(0.6774) Steps 472(467.49) | Grad Norm 4.8864(17.1273) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 46.7707(43.9533) | Bit/dim 4.7962(5.2150) | Xent 1.8470(1.9349) | Loss 12.4136(13.4809) | Error 0.6532(0.6766) Steps 460(467.26) | Grad Norm 12.3723(16.9847) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 44.3124(43.9641) | Bit/dim 4.8123(5.2029) | Xent 1.7998(1.9309) | Loss 11.9343(13.4345) | Error 0.6301(0.6753) Steps 460(467.04) | Grad Norm 15.2548(16.9328) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 44.0308(43.9661) | Bit/dim 4.8983(5.1937) | Xent 1.8925(1.9297) | Loss 12.3498(13.4020) | Error 0.6759(0.6753) Steps 484(467.55) | Grad Norm 26.2373(17.2119) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 21.3704, Epoch Time 303.9578(319.9812), Bit/dim 4.8114(best: 4.8242), Xent 1.7947, Loss 5.7088, Error 0.6285(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 45.6896(44.0178) | Bit/dim 4.8214(5.1826) | Xent 1.8362(1.9269) | Loss 14.8973(13.4469) | Error 0.6434(0.6743) Steps 478(467.87) | Grad Norm 17.4556(17.2192) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 43.1024(43.9903) | Bit/dim 4.8142(5.1715) | Xent 1.7863(1.9227) | Loss 12.2167(13.4100) | Error 0.6215(0.6727) Steps 484(468.35) | Grad Norm 7.2062(16.9188) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 42.6133(43.9490) | Bit/dim 4.7580(5.1591) | Xent 1.8597(1.9208) | Loss 12.2029(13.3737) | Error 0.6638(0.6725) Steps 454(467.92) | Grad Norm 10.1073(16.7145) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 45.3321(43.9905) | Bit/dim 4.7945(5.1482) | Xent 1.8389(1.9183) | Loss 11.9723(13.3317) | Error 0.6484(0.6717) Steps 454(467.50) | Grad Norm 16.8358(16.7181) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 42.8465(43.9562) | Bit/dim 4.8414(5.1390) | Xent 1.8608(1.9166) | Loss 12.1017(13.2948) | Error 0.6515(0.6711) Steps 484(468.00) | Grad Norm 20.2185(16.8231) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 47.3140(44.0569) | Bit/dim 4.8125(5.1292) | Xent 1.9489(1.9176) | Loss 12.1531(13.2605) | Error 0.7001(0.6720) Steps 496(468.84) | Grad Norm 23.1490(17.0129) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 21.4149, Epoch Time 307.1373(319.5959), Bit/dim 4.8507(best: 4.8114), Xent 1.9627, Loss 5.8320, Error 0.6816(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 44.1881(44.0608) | Bit/dim 4.8531(5.1209) | Xent 1.9784(1.9194) | Loss 15.4111(13.3251) | Error 0.6825(0.6723) Steps 478(469.11) | Grad Norm 46.3343(17.8926) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 48.5378(44.1951) | Bit/dim 4.8202(5.1119) | Xent 1.8095(1.9161) | Loss 12.4321(13.2983) | Error 0.6365(0.6712) Steps 490(469.74) | Grad Norm 16.4921(17.8505) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 42.1647(44.1342) | Bit/dim 4.8945(5.1053) | Xent 1.8743(1.9149) | Loss 12.3373(13.2694) | Error 0.6501(0.6706) Steps 490(470.35) | Grad Norm 18.4364(17.8681) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 45.0816(44.1627) | Bit/dim 4.8448(5.0975) | Xent 1.8485(1.9129) | Loss 12.3677(13.2424) | Error 0.6531(0.6701) Steps 484(470.76) | Grad Norm 11.8363(17.6872) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 46.2771(44.2261) | Bit/dim 4.8106(5.0889) | Xent 1.8815(1.9119) | Loss 12.4114(13.2175) | Error 0.6690(0.6700) Steps 490(471.33) | Grad Norm 22.9928(17.8463) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 48.9296(44.3672) | Bit/dim 4.7712(5.0794) | Xent 1.8720(1.9107) | Loss 12.2430(13.1882) | Error 0.6502(0.6695) Steps 490(471.89) | Grad Norm 20.5810(17.9284) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 21.4136, Epoch Time 314.8906(319.4548), Bit/dim 4.8134(best: 4.8114), Xent 1.8583, Loss 5.7425, Error 0.6534(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 43.8822(44.3526) | Bit/dim 4.8200(5.0716) | Xent 1.9025(1.9105) | Loss 14.8159(13.2371) | Error 0.6696(0.6695) Steps 472(471.90) | Grad Norm 25.0301(18.1414) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 46.6885(44.4227) | Bit/dim 4.8621(5.0653) | Xent 1.7994(1.9071) | Loss 12.3038(13.2091) | Error 0.6259(0.6682) Steps 496(472.62) | Grad Norm 10.2795(17.9056) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 47.1777(44.5054) | Bit/dim 4.7573(5.0561) | Xent 1.8539(1.9055) | Loss 12.3252(13.1825) | Error 0.6600(0.6679) Steps 466(472.42) | Grad Norm 19.0876(17.9410) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 45.8566(44.5459) | Bit/dim 4.8111(5.0487) | Xent 1.8704(1.9045) | Loss 11.9972(13.1470) | Error 0.6619(0.6677) Steps 490(472.95) | Grad Norm 18.6330(17.9618) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 44.8446(44.5549) | Bit/dim 4.7243(5.0390) | Xent 1.8397(1.9026) | Loss 11.8656(13.1085) | Error 0.6546(0.6673) Steps 466(472.74) | Grad Norm 8.0471(17.6643) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 44.7334(44.5602) | Bit/dim 4.7872(5.0314) | Xent 1.7926(1.8993) | Loss 12.2915(13.0840) | Error 0.6271(0.6661) Steps 466(472.54) | Grad Norm 11.1023(17.4675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 21.3553, Epoch Time 313.7747(319.2844), Bit/dim 4.7688(best: 4.8114), Xent 1.7385, Loss 5.6381, Error 0.6041(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 48.3824(44.6749) | Bit/dim 4.7650(5.0235) | Xent 1.7722(1.8954) | Loss 14.9340(13.1395) | Error 0.6208(0.6648) Steps 502(473.42) | Grad Norm 5.8497(17.1189) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 48.4095(44.7869) | Bit/dim 4.7628(5.0156) | Xent 1.8222(1.8932) | Loss 12.2522(13.1129) | Error 0.6406(0.6640) Steps 514(474.64) | Grad Norm 13.8929(17.0222) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 48.0875(44.8860) | Bit/dim 4.6961(5.0060) | Xent 1.7913(1.8902) | Loss 12.0825(13.0820) | Error 0.6355(0.6632) Steps 496(475.28) | Grad Norm 8.5443(16.7678) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 41.3121(44.7787) | Bit/dim 4.7035(4.9970) | Xent 1.7714(1.8866) | Loss 12.0127(13.0499) | Error 0.6193(0.6619) Steps 484(475.54) | Grad Norm 7.3049(16.4839) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 43.8927(44.7522) | Bit/dim 4.7170(4.9886) | Xent 1.7759(1.8833) | Loss 11.8617(13.0143) | Error 0.6249(0.6608) Steps 496(476.15) | Grad Norm 8.7229(16.2511) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 45.4281(44.7724) | Bit/dim 4.7019(4.9800) | Xent 1.7678(1.8798) | Loss 12.0088(12.9841) | Error 0.6267(0.6597) Steps 496(476.75) | Grad Norm 5.6806(15.9340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 22.3108, Epoch Time 316.8392(319.2110), Bit/dim 4.6630(best: 4.7688), Xent 1.7307, Loss 5.5283, Error 0.6041(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 46.5193(44.8248) | Bit/dim 4.6588(4.9703) | Xent 1.7742(1.8767) | Loss 14.6559(13.0343) | Error 0.6351(0.6590) Steps 484(476.97) | Grad Norm 9.3691(15.7371) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 45.2935(44.8389) | Bit/dim 4.6455(4.9606) | Xent 1.7773(1.8737) | Loss 11.7521(12.9958) | Error 0.6316(0.6582) Steps 490(477.36) | Grad Norm 8.1747(15.5102) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 50.5443(45.0101) | Bit/dim 4.6652(4.9517) | Xent 1.7768(1.8708) | Loss 11.9247(12.9637) | Error 0.6242(0.6572) Steps 514(478.46) | Grad Norm 7.0556(15.2565) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 50.2026(45.1658) | Bit/dim 4.6776(4.9435) | Xent 1.7732(1.8679) | Loss 12.1737(12.9400) | Error 0.6359(0.6565) Steps 490(478.80) | Grad Norm 20.9683(15.4279) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 47.8996(45.2478) | Bit/dim 4.7106(4.9365) | Xent 1.9913(1.8716) | Loss 12.0941(12.9146) | Error 0.6980(0.6578) Steps 532(480.40) | Grad Norm 40.4208(16.1777) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 51.0478(45.4218) | Bit/dim 4.6316(4.9274) | Xent 1.7503(1.8679) | Loss 11.8632(12.8831) | Error 0.6181(0.6566) Steps 526(481.77) | Grad Norm 11.4362(16.0354) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 22.3175, Epoch Time 332.6066(319.6129), Bit/dim 4.7160(best: 4.6630), Xent 1.9170, Loss 5.6745, Error 0.6605(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 48.7986(45.5231) | Bit/dim 4.7074(4.9208) | Xent 1.9441(1.8702) | Loss 14.9187(12.9441) | Error 0.6726(0.6571) Steps 502(482.37) | Grad Norm 39.5083(16.7396) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 50.8331(45.6824) | Bit/dim 4.6554(4.9128) | Xent 1.7969(1.8680) | Loss 12.0492(12.9173) | Error 0.6292(0.6562) Steps 514(483.32) | Grad Norm 15.7593(16.7102) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 50.7919(45.8357) | Bit/dim 4.6851(4.9060) | Xent 2.0077(1.8722) | Loss 12.2295(12.8966) | Error 0.7096(0.6578) Steps 508(484.06) | Grad Norm 30.2051(17.1151) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 49.2368(45.9378) | Bit/dim 4.6539(4.8984) | Xent 1.8766(1.8723) | Loss 11.9991(12.8697) | Error 0.6664(0.6581) Steps 532(485.50) | Grad Norm 17.1100(17.1149) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 47.5192(45.9852) | Bit/dim 4.6793(4.8919) | Xent 1.8247(1.8709) | Loss 11.8361(12.8387) | Error 0.6424(0.6576) Steps 538(487.08) | Grad Norm 13.8964(17.0184) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 48.2572(46.0534) | Bit/dim 4.6507(4.8846) | Xent 1.8312(1.8697) | Loss 11.8903(12.8103) | Error 0.6482(0.6573) Steps 520(488.06) | Grad Norm 15.6843(16.9783) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 21.4462, Epoch Time 335.7575(320.0972), Bit/dim 4.6859(best: 4.6630), Xent 2.0946, Loss 5.7332, Error 0.7301(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 53.1071(46.2650) | Bit/dim 4.6816(4.8785) | Xent 2.1594(1.8784) | Loss 15.5684(12.8930) | Error 0.7508(0.6601) Steps 550(489.92) | Grad Norm 33.5248(17.4747) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 43.2645(46.1750) | Bit/dim 4.6978(4.8731) | Xent 1.8981(1.8790) | Loss 12.0307(12.8671) | Error 0.6764(0.6606) Steps 496(490.11) | Grad Norm 10.1254(17.2542) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 46.1868(46.1753) | Bit/dim 4.6410(4.8661) | Xent 2.1983(1.8886) | Loss 12.1628(12.8460) | Error 0.7569(0.6635) Steps 508(490.64) | Grad Norm 38.2827(17.8851) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 45.8015(46.1641) | Bit/dim 4.6114(4.8585) | Xent 1.8817(1.8884) | Loss 11.9593(12.8194) | Error 0.6658(0.6636) Steps 478(490.26) | Grad Norm 5.4071(17.5108) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 43.0570(46.0709) | Bit/dim 4.6529(4.8523) | Xent 1.9531(1.8903) | Loss 12.0888(12.7975) | Error 0.6903(0.6644) Steps 502(490.61) | Grad Norm 14.2170(17.4119) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 44.2419(46.0160) | Bit/dim 4.6218(4.8454) | Xent 1.8281(1.8884) | Loss 11.8597(12.7693) | Error 0.6439(0.6638) Steps 496(490.78) | Grad Norm 5.3650(17.0505) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 22.1711, Epoch Time 316.6367(319.9934), Bit/dim 4.6528(best: 4.6630), Xent 1.8081, Loss 5.5568, Error 0.6388(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 47.0479(46.0470) | Bit/dim 4.6560(4.8397) | Xent 1.8666(1.8878) | Loss 14.6105(12.8246) | Error 0.6639(0.6638) Steps 490(490.75) | Grad Norm 16.6404(17.0382) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 46.5704(46.0627) | Bit/dim 4.6594(4.8343) | Xent 1.8053(1.8853) | Loss 11.7387(12.7920) | Error 0.6325(0.6628) Steps 514(491.45) | Grad Norm 8.2167(16.7736) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 46.8970(46.0877) | Bit/dim 4.6306(4.8282) | Xent 1.8118(1.8831) | Loss 12.0256(12.7690) | Error 0.6421(0.6622) Steps 514(492.13) | Grad Norm 9.7262(16.5622) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 47.9258(46.1429) | Bit/dim 4.6204(4.8220) | Xent 1.8028(1.8807) | Loss 11.9471(12.7444) | Error 0.6379(0.6615) Steps 496(492.24) | Grad Norm 9.5062(16.3505) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 47.1494(46.1731) | Bit/dim 4.5888(4.8150) | Xent 1.7918(1.8780) | Loss 11.9060(12.7192) | Error 0.6296(0.6605) Steps 502(492.54) | Grad Norm 12.3151(16.2294) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 45.1698(46.1430) | Bit/dim 4.6166(4.8090) | Xent 1.8159(1.8762) | Loss 11.6248(12.6864) | Error 0.6464(0.6601) Steps 496(492.64) | Grad Norm 9.3992(16.0245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 21.6547, Epoch Time 321.6344(320.0426), Bit/dim 4.5900(best: 4.6528), Xent 1.7497, Loss 5.4648, Error 0.6179(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 47.4317(46.1816) | Bit/dim 4.5992(4.8027) | Xent 1.8015(1.8739) | Loss 14.3090(12.7350) | Error 0.6356(0.6594) Steps 508(493.10) | Grad Norm 10.0922(15.8465) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 50.3069(46.3054) | Bit/dim 4.5883(4.7963) | Xent 1.7611(1.8705) | Loss 11.6661(12.7030) | Error 0.6265(0.6584) Steps 514(493.73) | Grad Norm 14.8575(15.8169) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 45.8987(46.2932) | Bit/dim 4.5428(4.7887) | Xent 1.7688(1.8675) | Loss 11.3954(12.6638) | Error 0.6190(0.6572) Steps 514(494.34) | Grad Norm 5.7183(15.5139) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 45.2917(46.2631) | Bit/dim 4.5553(4.7817) | Xent 1.7632(1.8644) | Loss 11.7217(12.6355) | Error 0.6291(0.6564) Steps 496(494.39) | Grad Norm 9.4729(15.3327) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 49.7599(46.3680) | Bit/dim 4.5385(4.7744) | Xent 1.7597(1.8612) | Loss 11.6590(12.6062) | Error 0.6211(0.6553) Steps 490(494.25) | Grad Norm 11.7028(15.2238) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 46.8985(46.3839) | Bit/dim 4.5367(4.7673) | Xent 1.7458(1.8578) | Loss 11.6149(12.5765) | Error 0.6149(0.6541) Steps 520(495.03) | Grad Norm 5.9082(14.9443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 21.8646, Epoch Time 325.4870(320.2060), Bit/dim 4.5480(best: 4.5900), Xent 1.6789, Loss 5.3874, Error 0.5891(best: 0.6041)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 47.9643(46.4314) | Bit/dim 4.5488(4.7607) | Xent 1.7139(1.8534) | Loss 14.2388(12.6263) | Error 0.6061(0.6526) Steps 496(495.06) | Grad Norm 6.9396(14.7042) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 49.1794(46.5138) | Bit/dim 4.5097(4.7532) | Xent 1.7393(1.8500) | Loss 11.5345(12.5936) | Error 0.6128(0.6514) Steps 538(496.34) | Grad Norm 7.5238(14.4888) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 46.5041(46.5135) | Bit/dim 4.5172(4.7461) | Xent 1.7382(1.8467) | Loss 11.6738(12.5660) | Error 0.6276(0.6507) Steps 514(496.87) | Grad Norm 5.5275(14.2199) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 50.7474(46.6405) | Bit/dim 4.5149(4.7392) | Xent 1.7128(1.8426) | Loss 11.5930(12.5368) | Error 0.6071(0.6494) Steps 508(497.21) | Grad Norm 5.9342(13.9714) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 54.1231(46.8650) | Bit/dim 4.4877(4.7316) | Xent 1.7249(1.8391) | Loss 11.5771(12.5080) | Error 0.6151(0.6484) Steps 508(497.53) | Grad Norm 9.1200(13.8258) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 50.7567(46.9818) | Bit/dim 4.4880(4.7243) | Xent 1.7622(1.8368) | Loss 11.5507(12.4793) | Error 0.6188(0.6475) Steps 502(497.67) | Grad Norm 19.9029(14.0081) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 21.7426, Epoch Time 340.4848(320.8143), Bit/dim 4.5186(best: 4.5480), Xent 1.7434, Loss 5.3903, Error 0.6171(best: 0.5891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 49.6147(47.0607) | Bit/dim 4.5149(4.7180) | Xent 1.7818(1.8352) | Loss 14.5717(12.5421) | Error 0.6236(0.6468) Steps 550(499.24) | Grad Norm 26.4197(14.3805) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 48.8703(47.1150) | Bit/dim 4.4898(4.7112) | Xent 1.7250(1.8319) | Loss 11.6252(12.5145) | Error 0.6119(0.6457) Steps 538(500.40) | Grad Norm 6.1737(14.1343) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 51.5658(47.2485) | Bit/dim 4.4748(4.7041) | Xent 1.7474(1.8293) | Loss 11.6820(12.4896) | Error 0.6100(0.6447) Steps 526(501.17) | Grad Norm 18.8180(14.2748) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 48.6990(47.2921) | Bit/dim 4.4948(4.6978) | Xent 1.8154(1.8289) | Loss 11.7895(12.4686) | Error 0.6421(0.6446) Steps 556(502.81) | Grad Norm 26.8122(14.6509) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 49.0384(47.3445) | Bit/dim 4.4740(4.6911) | Xent 1.7204(1.8256) | Loss 11.1273(12.4283) | Error 0.6158(0.6437) Steps 538(503.87) | Grad Norm 5.4946(14.3762) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 48.5107(47.3794) | Bit/dim 4.5217(4.6860) | Xent 1.7952(1.8247) | Loss 11.3429(12.3958) | Error 0.6296(0.6433) Steps 520(504.35) | Grad Norm 17.3287(14.4648) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 22.4523, Epoch Time 338.2994(321.3389), Bit/dim 4.5528(best: 4.5186), Xent 1.6710, Loss 5.3883, Error 0.5810(best: 0.5891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 47.0327(47.3690) | Bit/dim 4.5557(4.6821) | Xent 1.6957(1.8209) | Loss 14.2734(12.4521) | Error 0.5924(0.6418) Steps 520(504.82) | Grad Norm 9.2436(14.3082) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 50.9619(47.4768) | Bit/dim 4.4832(4.6761) | Xent 1.8429(1.8215) | Loss 11.4926(12.4233) | Error 0.6545(0.6422) Steps 508(504.92) | Grad Norm 28.5917(14.7367) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 50.0782(47.5549) | Bit/dim 4.4607(4.6697) | Xent 1.7670(1.8199) | Loss 11.4935(12.3954) | Error 0.6221(0.6416) Steps 550(506.27) | Grad Norm 21.5485(14.9410) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 51.2287(47.6651) | Bit/dim 4.5351(4.6656) | Xent 2.0424(1.8266) | Loss 11.6833(12.3741) | Error 0.6985(0.6433) Steps 544(507.40) | Grad Norm 40.4365(15.7059) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 49.0413(47.7064) | Bit/dim 4.4658(4.6596) | Xent 1.7128(1.8231) | Loss 11.5303(12.3487) | Error 0.6144(0.6424) Steps 496(507.06) | Grad Norm 9.3773(15.5160) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 52.2804(47.8436) | Bit/dim 4.4540(4.6535) | Xent 1.7320(1.8204) | Loss 11.4286(12.3211) | Error 0.6174(0.6416) Steps 520(507.45) | Grad Norm 10.9586(15.3793) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 22.5080, Epoch Time 341.8433(321.9540), Bit/dim 4.4528(best: 4.5186), Xent 1.7321, Loss 5.3189, Error 0.6157(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 50.7676(47.9313) | Bit/dim 4.4605(4.6477) | Xent 1.7821(1.8193) | Loss 14.6494(12.3910) | Error 0.6351(0.6415) Steps 544(508.54) | Grad Norm 12.8859(15.3045) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 49.3091(47.9726) | Bit/dim 4.4431(4.6415) | Xent 1.7792(1.8181) | Loss 11.5312(12.3652) | Error 0.6316(0.6412) Steps 526(509.07) | Grad Norm 10.4770(15.1597) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 50.0823(48.0359) | Bit/dim 4.4849(4.6368) | Xent 1.7863(1.8171) | Loss 11.5003(12.3392) | Error 0.6372(0.6410) Steps 490(508.49) | Grad Norm 22.2406(15.3721) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 46.1241(47.9786) | Bit/dim 4.5266(4.6335) | Xent 1.7210(1.8142) | Loss 11.4352(12.3121) | Error 0.6075(0.6400) Steps 526(509.02) | Grad Norm 11.3428(15.2512) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 49.3250(48.0190) | Bit/dim 4.4564(4.6282) | Xent 1.7393(1.8120) | Loss 11.3591(12.2835) | Error 0.6210(0.6395) Steps 532(509.71) | Grad Norm 9.8869(15.0903) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 47.9465(48.0168) | Bit/dim 4.4298(4.6223) | Xent 1.7195(1.8092) | Loss 11.1921(12.2508) | Error 0.6106(0.6386) Steps 532(510.38) | Grad Norm 7.8121(14.8719) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 22.2108, Epoch Time 334.3050(322.3245), Bit/dim 4.4645(best: 4.4528), Xent 1.6534, Loss 5.2912, Error 0.5826(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 47.6690(48.0064) | Bit/dim 4.4569(4.6173) | Xent 1.6937(1.8057) | Loss 14.0496(12.3048) | Error 0.6009(0.6375) Steps 562(511.93) | Grad Norm 5.7243(14.5975) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 45.2458(47.9235) | Bit/dim 4.4230(4.6115) | Xent 1.7101(1.8029) | Loss 11.2122(12.2720) | Error 0.6105(0.6367) Steps 520(512.17) | Grad Norm 9.1740(14.4348) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 46.6878(47.8865) | Bit/dim 4.4336(4.6061) | Xent 1.7187(1.8003) | Loss 11.2214(12.2405) | Error 0.6069(0.6358) Steps 520(512.40) | Grad Norm 12.5855(14.3793) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 50.7020(47.9709) | Bit/dim 4.4327(4.6009) | Xent 1.6918(1.7971) | Loss 11.1555(12.2079) | Error 0.5901(0.6344) Steps 544(513.35) | Grad Norm 6.2912(14.1367) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 47.5506(47.9583) | Bit/dim 4.4217(4.5956) | Xent 1.7017(1.7942) | Loss 11.0598(12.1735) | Error 0.6079(0.6336) Steps 526(513.73) | Grad Norm 15.7168(14.1841) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 48.9624(47.9884) | Bit/dim 4.4107(4.5900) | Xent 1.7603(1.7932) | Loss 11.5490(12.1547) | Error 0.6205(0.6332) Steps 514(513.74) | Grad Norm 19.6050(14.3467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 23.4078, Epoch Time 328.5556(322.5115), Bit/dim 4.4080(best: 4.4528), Xent 1.6300, Loss 5.2229, Error 0.5842(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 47.6136(47.9772) | Bit/dim 4.4002(4.5843) | Xent 1.6858(1.7900) | Loss 14.1452(12.2144) | Error 0.6042(0.6323) Steps 526(514.11) | Grad Norm 9.7643(14.2092) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 49.5717(48.0250) | Bit/dim 4.4198(4.5794) | Xent 1.6751(1.7865) | Loss 11.3119(12.1874) | Error 0.5899(0.6311) Steps 508(513.92) | Grad Norm 6.2165(13.9695) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 45.7841(47.9578) | Bit/dim 4.3789(4.5734) | Xent 1.6578(1.7827) | Loss 11.2219(12.1584) | Error 0.5897(0.6298) Steps 514(513.93) | Grad Norm 4.2029(13.6765) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 57.9228(48.2568) | Bit/dim 4.3762(4.5675) | Xent 1.6807(1.7796) | Loss 11.3955(12.1355) | Error 0.5969(0.6288) Steps 586(516.09) | Grad Norm 6.2212(13.4528) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 52.9450(48.3974) | Bit/dim 4.3939(4.5622) | Xent 1.6749(1.7765) | Loss 11.3989(12.1134) | Error 0.5979(0.6279) Steps 568(517.65) | Grad Norm 11.4286(13.3921) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 50.2767(48.4538) | Bit/dim 4.4124(4.5578) | Xent 1.7324(1.7751) | Loss 11.2725(12.0882) | Error 0.6214(0.6277) Steps 556(518.80) | Grad Norm 17.5941(13.5181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.1395, Epoch Time 346.4223(323.2288), Bit/dim 4.4026(best: 4.4080), Xent 1.7130, Loss 5.2591, Error 0.6150(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 47.8155(48.4346) | Bit/dim 4.4097(4.5533) | Xent 1.7558(1.7746) | Loss 14.3218(12.1552) | Error 0.6269(0.6277) Steps 538(519.37) | Grad Norm 19.2571(13.6903) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 55.1003(48.6346) | Bit/dim 4.4002(4.5487) | Xent 1.6516(1.7709) | Loss 11.6244(12.1393) | Error 0.5962(0.6267) Steps 532(519.75) | Grad Norm 12.2442(13.6469) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 49.8073(48.6698) | Bit/dim 4.4135(4.5447) | Xent 1.7515(1.7703) | Loss 11.4974(12.1200) | Error 0.6265(0.6267) Steps 562(521.02) | Grad Norm 22.7997(13.9215) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 47.9040(48.6468) | Bit/dim 4.4182(4.5409) | Xent 1.7312(1.7691) | Loss 11.4205(12.0990) | Error 0.6230(0.6266) Steps 562(522.25) | Grad Norm 10.5943(13.8217) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 49.1623(48.6623) | Bit/dim 4.3826(4.5361) | Xent 1.7165(1.7675) | Loss 11.3943(12.0779) | Error 0.6115(0.6262) Steps 532(522.54) | Grad Norm 9.1456(13.6814) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 50.4319(48.7154) | Bit/dim 4.3677(4.5311) | Xent 1.7140(1.7659) | Loss 11.3059(12.0547) | Error 0.6085(0.6256) Steps 538(523.00) | Grad Norm 11.4204(13.6136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 22.9599, Epoch Time 342.1712(323.7971), Bit/dim 4.4240(best: 4.4026), Xent 1.6579, Loss 5.2530, Error 0.5847(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 49.3509(48.7344) | Bit/dim 4.4327(4.5281) | Xent 1.6867(1.7636) | Loss 14.5063(12.1283) | Error 0.6040(0.6250) Steps 520(522.91) | Grad Norm 15.9979(13.6851) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 51.8732(48.8286) | Bit/dim 4.3563(4.5230) | Xent 1.6826(1.7611) | Loss 11.3183(12.1040) | Error 0.6024(0.6243) Steps 562(524.09) | Grad Norm 6.0868(13.4572) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 46.4559(48.7574) | Bit/dim 4.3386(4.5174) | Xent 1.6729(1.7585) | Loss 10.9556(12.0695) | Error 0.5964(0.6235) Steps 514(523.78) | Grad Norm 9.4894(13.3381) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 50.6837(48.8152) | Bit/dim 4.3663(4.5129) | Xent 1.7093(1.7570) | Loss 11.2349(12.0445) | Error 0.6086(0.6230) Steps 562(524.93) | Grad Norm 14.0593(13.3598) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 48.9652(48.8197) | Bit/dim 4.3326(4.5075) | Xent 1.7088(1.7556) | Loss 11.3483(12.0236) | Error 0.6128(0.6227) Steps 556(525.86) | Grad Norm 13.7892(13.3726) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 52.8020(48.9392) | Bit/dim 4.3461(4.5026) | Xent 1.6885(1.7536) | Loss 11.2961(12.0018) | Error 0.6013(0.6221) Steps 556(526.77) | Grad Norm 13.4090(13.3737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.6277, Epoch Time 341.0794(324.3155), Bit/dim 4.5505(best: 4.4026), Xent 1.7367, Loss 5.4189, Error 0.6195(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 47.9710(48.9101) | Bit/dim 4.5419(4.5038) | Xent 1.7693(1.7540) | Loss 14.5950(12.0796) | Error 0.6256(0.6222) Steps 562(527.82) | Grad Norm 23.5552(13.6792) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 48.2805(48.8912) | Bit/dim 4.4668(4.5027) | Xent 1.7455(1.7538) | Loss 11.3565(12.0579) | Error 0.6195(0.6221) Steps 532(527.95) | Grad Norm 20.3675(13.8798) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 46.8961(48.8314) | Bit/dim 4.3811(4.4991) | Xent 1.6766(1.7515) | Loss 11.3621(12.0370) | Error 0.5985(0.6214) Steps 562(528.97) | Grad Norm 6.9083(13.6707) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 47.5870(48.7941) | Bit/dim 4.4133(4.4965) | Xent 1.7342(1.7509) | Loss 11.2989(12.0149) | Error 0.6201(0.6214) Steps 532(529.06) | Grad Norm 13.4983(13.6655) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 49.8951(48.8271) | Bit/dim 4.3638(4.4925) | Xent 1.6532(1.7480) | Loss 11.1656(11.9894) | Error 0.5902(0.6204) Steps 526(528.97) | Grad Norm 8.7217(13.5172) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 51.6016(48.9103) | Bit/dim 4.3670(4.4887) | Xent 1.7012(1.7466) | Loss 11.3219(11.9694) | Error 0.6079(0.6200) Steps 556(529.78) | Grad Norm 14.1747(13.5369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.2668, Epoch Time 333.4396(324.5893), Bit/dim 4.3728(best: 4.4026), Xent 1.6379, Loss 5.1918, Error 0.5875(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 51.4835(48.9875) | Bit/dim 4.3719(4.4852) | Xent 1.7053(1.7454) | Loss 13.9801(12.0297) | Error 0.6048(0.6196) Steps 550(530.39) | Grad Norm 16.6894(13.6315) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 45.7152(48.8893) | Bit/dim 4.3653(4.4816) | Xent 1.6905(1.7437) | Loss 11.1283(12.0026) | Error 0.5995(0.6190) Steps 526(530.26) | Grad Norm 10.8214(13.5472) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 48.7541(48.8853) | Bit/dim 4.3447(4.4775) | Xent 1.6507(1.7409) | Loss 11.1925(11.9783) | Error 0.5901(0.6181) Steps 556(531.03) | Grad Norm 6.3298(13.3307) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 49.8942(48.9156) | Bit/dim 4.3217(4.4729) | Xent 1.6715(1.7388) | Loss 11.0045(11.9491) | Error 0.5981(0.6175) Steps 562(531.96) | Grad Norm 8.9222(13.1984) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 50.6002(48.9661) | Bit/dim 4.3389(4.4688) | Xent 1.6596(1.7365) | Loss 11.1957(11.9265) | Error 0.5924(0.6168) Steps 550(532.50) | Grad Norm 9.8640(13.0984) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 51.6424(49.0464) | Bit/dim 4.3210(4.4644) | Xent 1.6413(1.7336) | Loss 10.7923(11.8925) | Error 0.5880(0.6159) Steps 526(532.30) | Grad Norm 6.6447(12.9048) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.9418, Epoch Time 340.1268(325.0554), Bit/dim 4.3053(best: 4.3728), Xent 1.5902, Loss 5.1004, Error 0.5656(best: 0.5810)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 49.6087(49.0633) | Bit/dim 4.3024(4.4595) | Xent 1.6356(1.7307) | Loss 14.0182(11.9563) | Error 0.5834(0.6149) Steps 526(532.11) | Grad Norm 5.4237(12.6803) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 52.0617(49.1532) | Bit/dim 4.2953(4.4546) | Xent 1.6406(1.7280) | Loss 11.1847(11.9331) | Error 0.5786(0.6138) Steps 544(532.47) | Grad Norm 5.2014(12.4560) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 50.8610(49.2044) | Bit/dim 4.3088(4.4502) | Xent 1.6454(1.7255) | Loss 11.0330(11.9061) | Error 0.5891(0.6131) Steps 556(533.18) | Grad Norm 7.7412(12.3145) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 51.9520(49.2869) | Bit/dim 4.2930(4.4455) | Xent 1.6511(1.7233) | Loss 10.9894(11.8786) | Error 0.5839(0.6122) Steps 550(533.68) | Grad Norm 9.6523(12.2347) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 48.7611(49.2711) | Bit/dim 4.2950(4.4410) | Xent 1.6930(1.7224) | Loss 10.9784(11.8516) | Error 0.5979(0.6118) Steps 532(533.63) | Grad Norm 17.2768(12.3859) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 51.7436(49.3453) | Bit/dim 4.2996(4.4368) | Xent 1.6666(1.7207) | Loss 10.9090(11.8233) | Error 0.5891(0.6111) Steps 538(533.76) | Grad Norm 18.6228(12.5730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.0668, Epoch Time 346.7632(325.7066), Bit/dim 4.3223(best: 4.3053), Xent 1.5846, Loss 5.1146, Error 0.5673(best: 0.5656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 47.7637(49.2978) | Bit/dim 4.3143(4.4331) | Xent 1.6394(1.7182) | Loss 13.7033(11.8797) | Error 0.5864(0.6104) Steps 520(533.35) | Grad Norm 15.5552(12.6625) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 52.3138(49.3883) | Bit/dim 4.4917(4.4348) | Xent 1.7586(1.7195) | Loss 11.6073(11.8716) | Error 0.6452(0.6114) Steps 574(534.57) | Grad Norm 21.5957(12.9305) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 55.8846(49.5832) | Bit/dim 4.3194(4.4314) | Xent 1.6835(1.7184) | Loss 11.2446(11.8527) | Error 0.6060(0.6113) Steps 556(535.21) | Grad Norm 10.5922(12.8603) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 48.6097(49.5540) | Bit/dim 4.3462(4.4288) | Xent 1.9275(1.7246) | Loss 11.2323(11.8341) | Error 0.6827(0.6134) Steps 538(535.30) | Grad Norm 32.2809(13.4430) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 45.5929(49.4352) | Bit/dim 4.3695(4.4270) | Xent 1.6700(1.7230) | Loss 11.0831(11.8116) | Error 0.5948(0.6128) Steps 526(535.02) | Grad Norm 10.7514(13.3622) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 50.9640(49.4810) | Bit/dim 4.3360(4.4243) | Xent 1.7867(1.7249) | Loss 11.3380(11.7974) | Error 0.6485(0.6139) Steps 562(535.83) | Grad Norm 19.4057(13.5435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 22.4442, Epoch Time 341.2212(326.1721), Bit/dim 4.3610(best: 4.3053), Xent 1.8039, Loss 5.2630, Error 0.6509(best: 0.5656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 49.8385(49.4917) | Bit/dim 4.3642(4.4225) | Xent 1.8763(1.7295) | Loss 13.7587(11.8562) | Error 0.6687(0.6156) Steps 550(536.25) | Grad Norm 31.0207(14.0678) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 48.9537(49.4756) | Bit/dim 4.3036(4.4189) | Xent 1.6681(1.7276) | Loss 11.0716(11.8327) | Error 0.6078(0.6153) Steps 520(535.76) | Grad Norm 10.7218(13.9675) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 50.7024(49.5124) | Bit/dim 4.3187(4.4159) | Xent 1.7004(1.7268) | Loss 11.1384(11.8119) | Error 0.6071(0.6151) Steps 532(535.65) | Grad Norm 9.0850(13.8210) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 50.7957(49.5509) | Bit/dim 4.2807(4.4119) | Xent 1.6831(1.7255) | Loss 11.0929(11.7903) | Error 0.6056(0.6148) Steps 550(536.08) | Grad Norm 8.0881(13.6490) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 51.2629(49.6023) | Bit/dim 4.2826(4.4080) | Xent 1.6672(1.7237) | Loss 11.2103(11.7729) | Error 0.5920(0.6141) Steps 538(536.14) | Grad Norm 7.6340(13.4685) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 53.8022(49.7283) | Bit/dim 4.3112(4.4051) | Xent 1.7132(1.7234) | Loss 11.0852(11.7523) | Error 0.6222(0.6143) Steps 520(535.65) | Grad Norm 10.5836(13.3820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 22.2285, Epoch Time 345.9638(326.7658), Bit/dim 4.2665(best: 4.3053), Xent 1.6179, Loss 5.0754, Error 0.5787(best: 0.5656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 48.1459(49.6808) | Bit/dim 4.2751(4.4012) | Xent 1.6738(1.7219) | Loss 13.5559(11.8064) | Error 0.6033(0.6140) Steps 520(535.19) | Grad Norm 7.7421(13.2128) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 48.1896(49.6361) | Bit/dim 4.2327(4.3961) | Xent 1.6341(1.7193) | Loss 10.9306(11.7801) | Error 0.5837(0.6131) Steps 514(534.55) | Grad Norm 6.3915(13.0082) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 47.0390(49.5581) | Bit/dim 4.2451(4.3916) | Xent 1.6624(1.7176) | Loss 10.8440(11.7520) | Error 0.5945(0.6126) Steps 538(534.65) | Grad Norm 5.2081(12.7742) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 51.6158(49.6199) | Bit/dim 4.2361(4.3869) | Xent 1.6110(1.7144) | Loss 10.9738(11.7287) | Error 0.5729(0.6114) Steps 556(535.29) | Grad Norm 5.4995(12.5559) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 49.7882(49.6249) | Bit/dim 4.2467(4.3827) | Xent 1.6325(1.7119) | Loss 10.9540(11.7054) | Error 0.5807(0.6104) Steps 556(535.91) | Grad Norm 6.0722(12.3614) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 50.6407(49.6554) | Bit/dim 4.2320(4.3782) | Xent 1.6106(1.7089) | Loss 10.8706(11.6804) | Error 0.5751(0.6094) Steps 538(535.98) | Grad Norm 6.7972(12.1945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 23.0002, Epoch Time 336.1452(327.0472), Bit/dim 4.2271(best: 4.2665), Xent 1.5547, Loss 5.0045, Error 0.5556(best: 0.5656)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 48.3093(49.6150) | Bit/dim 4.2199(4.3735) | Xent 1.6138(1.7061) | Loss 13.6517(11.7395) | Error 0.5753(0.6084) Steps 544(536.22) | Grad Norm 8.0386(12.0698) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 50.3006(49.6356) | Bit/dim 4.2458(4.3696) | Xent 1.5952(1.7027) | Loss 10.6768(11.7076) | Error 0.5609(0.6069) Steps 538(536.27) | Grad Norm 10.6315(12.0267) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 53.5751(49.7538) | Bit/dim 4.2414(4.3658) | Xent 1.6810(1.7021) | Loss 11.1144(11.6899) | Error 0.5946(0.6066) Steps 544(536.50) | Grad Norm 15.2450(12.1232) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 51.6604(49.8110) | Bit/dim 4.2259(4.3616) | Xent 1.6061(1.6992) | Loss 11.0700(11.6713) | Error 0.5784(0.6057) Steps 574(537.63) | Grad Norm 12.7346(12.1416) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 56.1917(50.0024) | Bit/dim 4.2175(4.3573) | Xent 1.6197(1.6968) | Loss 11.0797(11.6535) | Error 0.5795(0.6049) Steps 544(537.82) | Grad Norm 9.9242(12.0750) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 46.0247(49.8831) | Bit/dim 4.2581(4.3543) | Xent 1.5892(1.6936) | Loss 10.7116(11.6253) | Error 0.5709(0.6039) Steps 544(538.00) | Grad Norm 8.8484(11.9782) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 23.3806, Epoch Time 348.6965(327.6967), Bit/dim 4.2482(best: 4.2271), Xent 1.5166, Loss 5.0065, Error 0.5440(best: 0.5556)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 52.5344(49.9626) | Bit/dim 4.2656(4.3516) | Xent 1.5989(1.6907) | Loss 13.9306(11.6944) | Error 0.5734(0.6030) Steps 556(538.54) | Grad Norm 9.8892(11.9156) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 47.9527(49.9023) | Bit/dim 4.2805(4.3495) | Xent 1.5744(1.6873) | Loss 10.9521(11.6721) | Error 0.5702(0.6020) Steps 544(538.71) | Grad Norm 13.1555(11.9528) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 51.7987(49.9592) | Bit/dim 4.2469(4.3464) | Xent 1.6188(1.6852) | Loss 11.1009(11.6550) | Error 0.5785(0.6013) Steps 526(538.33) | Grad Norm 16.7684(12.0972) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 50.7445(49.9828) | Bit/dim 4.2055(4.3422) | Xent 1.6169(1.6832) | Loss 10.9324(11.6333) | Error 0.5835(0.6008) Steps 550(538.68) | Grad Norm 12.5730(12.1115) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 47.6842(49.9138) | Bit/dim 4.2372(4.3390) | Xent 1.5785(1.6800) | Loss 10.6822(11.6048) | Error 0.5649(0.5997) Steps 532(538.48) | Grad Norm 8.8099(12.0125) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 48.2498(49.8639) | Bit/dim 4.1780(4.3342) | Xent 1.6057(1.6778) | Loss 10.7019(11.5777) | Error 0.5817(0.5992) Steps 538(538.46) | Grad Norm 9.9167(11.9496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.6863, Epoch Time 340.1715(328.0709), Bit/dim 4.1979(best: 4.2271), Xent 1.6798, Loss 5.0378, Error 0.5963(best: 0.5440)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 49.8760(49.8642) | Bit/dim 4.2013(4.3302) | Xent 1.7437(1.6798) | Loss 13.9673(11.6494) | Error 0.6090(0.5995) Steps 526(538.09) | Grad Norm 19.1381(12.1652) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 48.7534(49.8309) | Bit/dim 4.3054(4.3295) | Xent 1.7644(1.6823) | Loss 10.7128(11.6213) | Error 0.6367(0.6006) Steps 538(538.09) | Grad Norm 16.6958(12.3012) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 51.7157(49.8875) | Bit/dim 4.2268(4.3264) | Xent 1.6934(1.6826) | Loss 10.8257(11.5974) | Error 0.6090(0.6008) Steps 562(538.80) | Grad Norm 16.9061(12.4393) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 51.2078(49.9271) | Bit/dim 4.2350(4.3237) | Xent 1.6536(1.6818) | Loss 10.9231(11.5772) | Error 0.5941(0.6006) Steps 550(539.14) | Grad Norm 12.1241(12.4298) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 48.8662(49.8952) | Bit/dim 4.3165(4.3234) | Xent 1.6376(1.6804) | Loss 11.0534(11.5615) | Error 0.5913(0.6003) Steps 550(539.47) | Grad Norm 15.6336(12.5260) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 50.0039(49.8985) | Bit/dim 4.2552(4.3214) | Xent 1.7381(1.6822) | Loss 10.8496(11.5401) | Error 0.6136(0.6007) Steps 556(539.96) | Grad Norm 23.4775(12.8545) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 23.1896, Epoch Time 342.0658(328.4908), Bit/dim 4.2498(best: 4.1979), Xent 1.6235, Loss 5.0615, Error 0.5944(best: 0.5440)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 50.4125(49.9139) | Bit/dim 4.2488(4.3192) | Xent 1.7147(1.6831) | Loss 13.9884(11.6136) | Error 0.6148(0.6012) Steps 556(540.44) | Grad Norm 21.5962(13.1168) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 46.8058(49.8207) | Bit/dim 4.2809(4.3181) | Xent 1.6127(1.6810) | Loss 11.0727(11.5973) | Error 0.5766(0.6004) Steps 532(540.19) | Grad Norm 12.9699(13.1124) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 50.5510(49.8426) | Bit/dim 4.2272(4.3153) | Xent 1.6173(1.6791) | Loss 10.7922(11.5732) | Error 0.5956(0.6003) Steps 544(540.30) | Grad Norm 8.0363(12.9601) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 46.0146(49.7277) | Bit/dim 4.2459(4.3133) | Xent 1.6896(1.6794) | Loss 10.8742(11.5522) | Error 0.6022(0.6003) Steps 550(540.59) | Grad Norm 22.0015(13.2313) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 48.5806(49.6933) | Bit/dim 4.2323(4.3108) | Xent 1.6702(1.6792) | Loss 10.9634(11.5346) | Error 0.6010(0.6004) Steps 544(540.70) | Grad Norm 13.9841(13.2539) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 50.8659(49.7285) | Bit/dim 4.2124(4.3079) | Xent 1.6119(1.6771) | Loss 10.7619(11.5114) | Error 0.5799(0.5997) Steps 562(541.34) | Grad Norm 7.8319(13.0912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 23.6208, Epoch Time 335.5399(328.7022), Bit/dim 4.2177(best: 4.1979), Xent 1.6015, Loss 5.0185, Error 0.5731(best: 0.5440)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 49.7970(49.7306) | Bit/dim 4.2059(4.3048) | Xent 1.6769(1.6771) | Loss 12.9438(11.5544) | Error 0.6001(0.5998) Steps 550(541.60) | Grad Norm 13.6676(13.1085) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 46.9661(49.6476) | Bit/dim 4.2024(4.3017) | Xent 1.6283(1.6757) | Loss 10.8297(11.5326) | Error 0.5846(0.5993) Steps 538(541.49) | Grad Norm 5.6255(12.8840) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 53.9050(49.7754) | Bit/dim 4.2130(4.2991) | Xent 1.6103(1.6737) | Loss 10.6910(11.5074) | Error 0.5755(0.5986) Steps 538(541.38) | Grad Norm 11.0770(12.8298) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 53.9839(49.9016) | Bit/dim 4.1896(4.2958) | Xent 1.6439(1.6728) | Loss 11.0821(11.4946) | Error 0.5853(0.5982) Steps 538(541.28) | Grad Norm 13.6825(12.8554) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 50.4592(49.9183) | Bit/dim 4.1747(4.2922) | Xent 1.5864(1.6702) | Loss 10.9853(11.4793) | Error 0.5749(0.5975) Steps 544(541.36) | Grad Norm 8.5935(12.7276) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 47.2513(49.8383) | Bit/dim 4.1890(4.2891) | Xent 1.5863(1.6677) | Loss 10.1839(11.4405) | Error 0.5736(0.5968) Steps 538(541.26) | Grad Norm 7.7138(12.5771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 22.2374, Epoch Time 343.9864(329.1608), Bit/dim 4.1622(best: 4.1979), Xent 1.5183, Loss 4.9214, Error 0.5457(best: 0.5440)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 49.5015(49.8282) | Bit/dim 4.1743(4.2856) | Xent 1.5850(1.6652) | Loss 13.8583(11.5130) | Error 0.5766(0.5962) Steps 544(541.34) | Grad Norm 10.0370(12.5009) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 51.0626(49.8653) | Bit/dim 4.1749(4.2823) | Xent 1.5577(1.6620) | Loss 10.7554(11.4903) | Error 0.5594(0.5951) Steps 544(541.42) | Grad Norm 8.6348(12.3850) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 46.9456(49.7777) | Bit/dim 4.1624(4.2787) | Xent 1.5822(1.6596) | Loss 10.7169(11.4671) | Error 0.5730(0.5944) Steps 508(540.42) | Grad Norm 10.2948(12.3222) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 48.1979(49.7303) | Bit/dim 4.1500(4.2748) | Xent 1.5539(1.6564) | Loss 10.7328(11.4450) | Error 0.5569(0.5933) Steps 532(540.17) | Grad Norm 12.4595(12.3264) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 49.5362(49.7244) | Bit/dim 4.1485(4.2711) | Xent 1.5680(1.6538) | Loss 10.6535(11.4213) | Error 0.5742(0.5927) Steps 538(540.10) | Grad Norm 8.6718(12.2167) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 45.7650(49.6057) | Bit/dim 4.1196(4.2665) | Xent 1.5528(1.6507) | Loss 10.8159(11.4031) | Error 0.5629(0.5918) Steps 544(540.22) | Grad Norm 4.8098(11.9945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.0566, Epoch Time 332.4694(329.2600), Bit/dim 4.1209(best: 4.1622), Xent 1.4854, Loss 4.8636, Error 0.5330(best: 0.5440)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 48.8237(49.5822) | Bit/dim 4.1227(4.2622) | Xent 1.5514(1.6478) | Loss 13.7107(11.4724) | Error 0.5609(0.5909) Steps 544(540.33) | Grad Norm 5.7431(11.8070) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 46.3127(49.4841) | Bit/dim 4.1195(4.2579) | Xent 1.5500(1.6448) | Loss 10.6183(11.4467) | Error 0.5593(0.5899) Steps 520(539.72) | Grad Norm 7.6606(11.6826) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 49.7395(49.4918) | Bit/dim 4.1402(4.2544) | Xent 1.5436(1.6418) | Loss 10.6252(11.4221) | Error 0.5617(0.5891) Steps 550(540.03) | Grad Norm 9.5502(11.6186) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 47.7194(49.4386) | Bit/dim 4.1102(4.2501) | Xent 1.5343(1.6386) | Loss 10.5392(11.3956) | Error 0.5564(0.5881) Steps 532(539.79) | Grad Norm 10.5583(11.5868) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 46.5529(49.3520) | Bit/dim 4.1184(4.2461) | Xent 1.5640(1.6363) | Loss 10.4676(11.3678) | Error 0.5566(0.5872) Steps 550(540.10) | Grad Norm 11.6544(11.5888) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 48.5116(49.3268) | Bit/dim 4.1137(4.2421) | Xent 1.5818(1.6347) | Loss 10.4902(11.3414) | Error 0.5615(0.5864) Steps 544(540.21) | Grad Norm 15.7798(11.7146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 23.2779, Epoch Time 329.8411(329.2775), Bit/dim 4.1091(best: 4.1209), Xent 1.5018, Loss 4.8600, Error 0.5418(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 49.7337(49.3390) | Bit/dim 4.1278(4.2387) | Xent 1.5598(1.6325) | Loss 13.8452(11.4165) | Error 0.5639(0.5857) Steps 550(540.51) | Grad Norm 10.5826(11.6806) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 50.7545(49.3815) | Bit/dim 4.1064(4.2347) | Xent 1.5391(1.6297) | Loss 10.7373(11.3962) | Error 0.5517(0.5847) Steps 538(540.43) | Grad Norm 8.1001(11.5732) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 47.8757(49.3363) | Bit/dim 4.1099(4.2310) | Xent 1.4934(1.6256) | Loss 10.4481(11.3677) | Error 0.5394(0.5833) Steps 562(541.08) | Grad Norm 7.1217(11.4396) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 45.7315(49.2282) | Bit/dim 4.0746(4.2263) | Xent 1.5182(1.6223) | Loss 10.3299(11.3366) | Error 0.5513(0.5824) Steps 520(540.45) | Grad Norm 4.9561(11.2451) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 47.6382(49.1805) | Bit/dim 4.0814(4.2220) | Xent 1.5147(1.6191) | Loss 10.3110(11.3058) | Error 0.5441(0.5812) Steps 520(539.83) | Grad Norm 6.6377(11.1069) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 51.3332(49.2451) | Bit/dim 4.0971(4.2182) | Xent 1.5063(1.6157) | Loss 10.4410(11.2799) | Error 0.5479(0.5802) Steps 550(540.14) | Grad Norm 8.2186(11.0203) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 23.1047, Epoch Time 334.2459(329.4265), Bit/dim 4.1113(best: 4.1091), Xent 1.4939, Loss 4.8582, Error 0.5398(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 46.8626(49.1736) | Bit/dim 4.1144(4.2151) | Xent 1.5786(1.6146) | Loss 13.7531(11.3541) | Error 0.5655(0.5798) Steps 532(539.89) | Grad Norm 18.5945(11.2475) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 47.0901(49.1111) | Bit/dim 4.2075(4.2149) | Xent 1.7091(1.6174) | Loss 10.8519(11.3390) | Error 0.6186(0.5810) Steps 538(539.84) | Grad Norm 18.9842(11.4796) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 52.0061(49.1979) | Bit/dim 4.1024(4.2115) | Xent 1.6018(1.6170) | Loss 10.5376(11.3150) | Error 0.5746(0.5808) Steps 550(540.14) | Grad Norm 15.5956(11.6031) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 49.7339(49.2140) | Bit/dim 4.2788(4.2135) | Xent 1.8110(1.6228) | Loss 10.7184(11.2971) | Error 0.6389(0.5825) Steps 568(540.98) | Grad Norm 29.6524(12.1446) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 44.3989(49.0696) | Bit/dim 4.1803(4.2125) | Xent 1.5179(1.6197) | Loss 10.7887(11.2818) | Error 0.5525(0.5816) Steps 514(540.17) | Grad Norm 7.7927(12.0140) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 50.9745(49.1267) | Bit/dim 4.1666(4.2111) | Xent 1.5776(1.6184) | Loss 10.8906(11.2701) | Error 0.5667(0.5812) Steps 526(539.74) | Grad Norm 13.0368(12.0447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 22.2276, Epoch Time 331.8271(329.4985), Bit/dim 4.1568(best: 4.1091), Xent 1.6168, Loss 4.9652, Error 0.5820(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 48.5547(49.1095) | Bit/dim 4.1486(4.2093) | Xent 1.6762(1.6201) | Loss 13.5604(11.3388) | Error 0.6029(0.5818) Steps 526(539.33) | Grad Norm 14.9449(12.1317) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 49.5160(49.1217) | Bit/dim 4.1616(4.2078) | Xent 1.5752(1.6188) | Loss 10.7311(11.3206) | Error 0.5721(0.5815) Steps 562(540.01) | Grad Norm 7.9491(12.0062) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 51.4463(49.1915) | Bit/dim 4.1460(4.2060) | Xent 1.5565(1.6169) | Loss 10.7776(11.3043) | Error 0.5656(0.5810) Steps 580(541.21) | Grad Norm 7.7857(11.8796) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 51.4488(49.2592) | Bit/dim 4.1315(4.2037) | Xent 1.5978(1.6163) | Loss 10.8681(11.2912) | Error 0.5781(0.5810) Steps 568(542.02) | Grad Norm 15.5810(11.9906) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 51.5073(49.3266) | Bit/dim 4.1708(4.2028) | Xent 1.6306(1.6168) | Loss 10.8385(11.2776) | Error 0.5839(0.5810) Steps 580(543.15) | Grad Norm 16.1704(12.1160) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 53.7179(49.4584) | Bit/dim 4.1317(4.2006) | Xent 1.5638(1.6152) | Loss 10.7199(11.2609) | Error 0.5667(0.5806) Steps 544(543.18) | Grad Norm 7.2149(11.9690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 24.6010, Epoch Time 350.0210(330.1142), Bit/dim 4.1476(best: 4.1091), Xent 1.4881, Loss 4.8917, Error 0.5417(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 51.0557(49.5063) | Bit/dim 4.1494(4.1991) | Xent 1.5629(1.6136) | Loss 13.4770(11.3274) | Error 0.5605(0.5800) Steps 550(543.38) | Grad Norm 8.4947(11.8648) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 51.4813(49.5655) | Bit/dim 4.1458(4.1975) | Xent 1.5589(1.6120) | Loss 10.8067(11.3117) | Error 0.5621(0.5795) Steps 556(543.76) | Grad Norm 8.2842(11.7574) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 52.5744(49.6558) | Bit/dim 4.1183(4.1951) | Xent 1.5444(1.6099) | Loss 10.7410(11.2946) | Error 0.5564(0.5788) Steps 562(544.31) | Grad Norm 7.4441(11.6280) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 54.3621(49.7970) | Bit/dim 4.0933(4.1921) | Xent 1.5577(1.6084) | Loss 10.4380(11.2689) | Error 0.5630(0.5783) Steps 532(543.94) | Grad Norm 10.5022(11.5942) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 48.9278(49.7709) | Bit/dim 4.1050(4.1894) | Xent 1.5358(1.6062) | Loss 10.4893(11.2455) | Error 0.5506(0.5775) Steps 544(543.94) | Grad Norm 11.4508(11.5899) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 49.6500(49.7673) | Bit/dim 4.0923(4.1865) | Xent 1.5821(1.6055) | Loss 10.6578(11.2279) | Error 0.5585(0.5769) Steps 556(544.30) | Grad Norm 13.7468(11.6546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 22.4130, Epoch Time 349.9643(330.7097), Bit/dim 4.1012(best: 4.1091), Xent 1.5193, Loss 4.8608, Error 0.5440(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 48.3477(49.7247) | Bit/dim 4.1097(4.1842) | Xent 1.5845(1.6048) | Loss 13.8533(11.3067) | Error 0.5631(0.5765) Steps 538(544.12) | Grad Norm 13.4190(11.7075) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 53.7929(49.8468) | Bit/dim 4.1117(4.1820) | Xent 1.5135(1.6021) | Loss 10.6390(11.2866) | Error 0.5474(0.5756) Steps 532(543.75) | Grad Norm 8.9529(11.6249) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 50.2108(49.8577) | Bit/dim 4.0780(4.1789) | Xent 1.5094(1.5993) | Loss 10.1748(11.2533) | Error 0.5399(0.5745) Steps 544(543.76) | Grad Norm 7.8690(11.5122) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 46.5481(49.7584) | Bit/dim 4.0764(4.1759) | Xent 1.5444(1.5977) | Loss 10.4124(11.2281) | Error 0.5493(0.5738) Steps 538(543.59) | Grad Norm 11.6315(11.5158) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 49.6346(49.7547) | Bit/dim 4.1106(4.1739) | Xent 1.5271(1.5956) | Loss 10.5987(11.2092) | Error 0.5540(0.5732) Steps 550(543.78) | Grad Norm 10.5565(11.4870) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 49.2746(49.7403) | Bit/dim 4.0955(4.1715) | Xent 1.5060(1.5929) | Loss 10.5013(11.1879) | Error 0.5397(0.5722) Steps 538(543.61) | Grad Norm 9.8762(11.4387) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 22.9502, Epoch Time 338.3632(330.9393), Bit/dim 4.0721(best: 4.1012), Xent 1.4445, Loss 4.7944, Error 0.5211(best: 0.5330)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 52.1618(49.8129) | Bit/dim 4.0710(4.1685) | Xent 1.5038(1.5902) | Loss 13.5083(11.2576) | Error 0.5456(0.5714) Steps 556(543.98) | Grad Norm 6.0183(11.2761) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 46.8174(49.7231) | Bit/dim 4.0418(4.1647) | Xent 1.4720(1.5867) | Loss 10.4259(11.2326) | Error 0.5308(0.5702) Steps 538(543.80) | Grad Norm 4.6777(11.0781) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 48.9848(49.7009) | Bit/dim 4.0529(4.1614) | Xent 1.5080(1.5843) | Loss 10.4652(11.2096) | Error 0.5456(0.5694) Steps 550(543.98) | Grad Norm 5.8255(10.9205) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 50.8562(49.7356) | Bit/dim 4.0579(4.1583) | Xent 1.4732(1.5810) | Loss 10.1951(11.1791) | Error 0.5282(0.5682) Steps 532(543.62) | Grad Norm 5.1141(10.7463) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 50.0773(49.7458) | Bit/dim 4.0369(4.1546) | Xent 1.4838(1.5780) | Loss 10.5570(11.1605) | Error 0.5368(0.5673) Steps 550(543.82) | Grad Norm 3.4607(10.5278) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 51.4167(49.7959) | Bit/dim 4.0502(4.1515) | Xent 1.4674(1.5747) | Loss 10.5021(11.1407) | Error 0.5306(0.5662) Steps 568(544.54) | Grad Norm 7.7764(10.4452) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 23.1296, Epoch Time 342.6734(331.2913), Bit/dim 4.0487(best: 4.0721), Xent 1.4486, Loss 4.7731, Error 0.5175(best: 0.5211)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 47.1222(49.7157) | Bit/dim 4.0451(4.1483) | Xent 1.5011(1.5725) | Loss 13.3947(11.2083) | Error 0.5315(0.5651) Steps 544(544.53) | Grad Norm 16.3996(10.6239) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 45.6420(49.5935) | Bit/dim 4.0625(4.1457) | Xent 1.5426(1.5716) | Loss 10.4866(11.1867) | Error 0.5481(0.5646) Steps 538(544.33) | Grad Norm 16.4522(10.7987) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 51.6443(49.6550) | Bit/dim 4.0438(4.1427) | Xent 1.5088(1.5697) | Loss 10.4077(11.1633) | Error 0.5465(0.5641) Steps 562(544.86) | Grad Norm 10.4479(10.7882) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 47.6839(49.5959) | Bit/dim 4.0436(4.1397) | Xent 1.4734(1.5668) | Loss 10.3843(11.1400) | Error 0.5295(0.5630) Steps 532(544.47) | Grad Norm 6.8349(10.6696) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 47.5560(49.5347) | Bit/dim 4.0339(4.1365) | Xent 1.4689(1.5639) | Loss 10.3402(11.1160) | Error 0.5277(0.5620) Steps 544(544.46) | Grad Norm 8.2191(10.5961) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 53.8235(49.6634) | Bit/dim 4.0394(4.1336) | Xent 1.5348(1.5630) | Loss 10.4654(11.0964) | Error 0.5497(0.5616) Steps 550(544.63) | Grad Norm 13.4190(10.6808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 22.7969, Epoch Time 334.9448(331.4009), Bit/dim 4.0569(best: 4.0487), Xent 1.5479, Loss 4.8309, Error 0.5643(best: 0.5175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 50.8892(49.7001) | Bit/dim 4.0520(4.1312) | Xent 1.6125(1.5645) | Loss 13.4470(11.1670) | Error 0.5790(0.5621) Steps 538(544.43) | Grad Norm 16.5726(10.8575) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 49.9255(49.7069) | Bit/dim 4.0274(4.1280) | Xent 1.6163(1.5661) | Loss 10.3840(11.1435) | Error 0.5691(0.5623) Steps 562(544.95) | Grad Norm 19.1989(11.1078) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 53.6792(49.8261) | Bit/dim 4.0413(4.1254) | Xent 1.5410(1.5653) | Loss 10.6018(11.1272) | Error 0.5526(0.5620) Steps 580(546.01) | Grad Norm 16.0722(11.2567) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 49.7403(49.8235) | Bit/dim 4.0304(4.1226) | Xent 1.4809(1.5628) | Loss 10.2375(11.1005) | Error 0.5359(0.5613) Steps 556(546.31) | Grad Norm 9.9698(11.2181) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 47.6777(49.7591) | Bit/dim 4.0263(4.1197) | Xent 1.4873(1.5605) | Loss 10.2845(11.0761) | Error 0.5404(0.5606) Steps 544(546.24) | Grad Norm 6.9794(11.0909) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 49.0844(49.7389) | Bit/dim 4.0384(4.1173) | Xent 1.4719(1.5579) | Loss 10.3643(11.0547) | Error 0.5339(0.5598) Steps 532(545.81) | Grad Norm 9.2041(11.0343) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 22.7978, Epoch Time 342.6241(331.7376), Bit/dim 4.0438(best: 4.0487), Xent 1.4663, Loss 4.7769, Error 0.5332(best: 0.5175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 48.8591(49.7125) | Bit/dim 4.0521(4.1153) | Xent 1.5286(1.5570) | Loss 13.3894(11.1247) | Error 0.5501(0.5595) Steps 520(545.03) | Grad Norm 12.1785(11.0686) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 48.7966(49.6850) | Bit/dim 4.0033(4.1120) | Xent 1.4703(1.5544) | Loss 10.3297(11.1009) | Error 0.5305(0.5587) Steps 538(544.82) | Grad Norm 7.6352(10.9656) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 50.8179(49.7190) | Bit/dim 4.0241(4.1093) | Xent 1.4517(1.5513) | Loss 10.4548(11.0815) | Error 0.5272(0.5577) Steps 538(544.62) | Grad Norm 7.5373(10.8628) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 53.5098(49.8327) | Bit/dim 4.0473(4.1075) | Xent 1.4815(1.5492) | Loss 10.5287(11.0649) | Error 0.5373(0.5571) Steps 538(544.42) | Grad Norm 9.6873(10.8275) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 53.0190(49.9283) | Bit/dim 4.0246(4.1050) | Xent 1.4611(1.5466) | Loss 10.4790(11.0473) | Error 0.5308(0.5563) Steps 580(545.49) | Grad Norm 8.5278(10.7585) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 49.1483(49.9049) | Bit/dim 4.0030(4.1019) | Xent 1.4570(1.5439) | Loss 10.4799(11.0303) | Error 0.5250(0.5554) Steps 562(545.98) | Grad Norm 7.7515(10.6683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.6010, Epoch Time 346.9079(332.1927), Bit/dim 4.0177(best: 4.0438), Xent 1.4358, Loss 4.7356, Error 0.5127(best: 0.5175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 52.9441(49.9961) | Bit/dim 4.0209(4.0995) | Xent 1.4592(1.5413) | Loss 13.2179(11.0959) | Error 0.5256(0.5545) Steps 568(546.64) | Grad Norm 12.0360(10.7094) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 49.9873(49.9958) | Bit/dim 4.0077(4.0967) | Xent 1.5085(1.5404) | Loss 10.1669(11.0681) | Error 0.5419(0.5541) Steps 550(546.74) | Grad Norm 13.8335(10.8031) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 52.3778(50.0673) | Bit/dim 4.0005(4.0938) | Xent 1.4912(1.5389) | Loss 10.3167(11.0455) | Error 0.5421(0.5538) Steps 520(545.94) | Grad Norm 13.0479(10.8704) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 50.6396(50.0845) | Bit/dim 4.0274(4.0919) | Xent 1.4306(1.5356) | Loss 10.3855(11.0257) | Error 0.5189(0.5527) Steps 544(545.88) | Grad Norm 9.6710(10.8344) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 49.8219(50.0766) | Bit/dim 4.0304(4.0900) | Xent 1.4572(1.5333) | Loss 10.4272(11.0078) | Error 0.5292(0.5520) Steps 532(545.47) | Grad Norm 6.5639(10.7063) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 50.5606(50.0911) | Bit/dim 4.0026(4.0874) | Xent 1.4346(1.5303) | Loss 10.2611(10.9854) | Error 0.5105(0.5508) Steps 550(545.60) | Grad Norm 5.3383(10.5453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 22.5660, Epoch Time 348.1618(332.6718), Bit/dim 4.0117(best: 4.0177), Xent 1.4138, Loss 4.7186, Error 0.5101(best: 0.5127)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 47.1980(50.0043) | Bit/dim 4.0194(4.0853) | Xent 1.4486(1.5279) | Loss 13.3339(11.0558) | Error 0.5254(0.5500) Steps 544(545.56) | Grad Norm 12.7819(10.6124) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 48.6637(49.9641) | Bit/dim 4.0356(4.0839) | Xent 1.5575(1.5288) | Loss 10.5007(11.0392) | Error 0.5554(0.5502) Steps 520(544.79) | Grad Norm 19.6852(10.8846) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 53.1029(50.0583) | Bit/dim 4.0642(4.0833) | Xent 1.5966(1.5308) | Loss 10.8144(11.0324) | Error 0.5684(0.5507) Steps 562(545.30) | Grad Norm 15.5975(11.0260) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 53.8932(50.1733) | Bit/dim 4.0488(4.0822) | Xent 1.5079(1.5301) | Loss 10.1615(11.0063) | Error 0.5457(0.5506) Steps 580(546.35) | Grad Norm 8.3000(10.9442) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 53.6759(50.2784) | Bit/dim 4.0350(4.0808) | Xent 1.5690(1.5313) | Loss 10.4278(10.9890) | Error 0.5646(0.5510) Steps 550(546.46) | Grad Norm 14.4431(11.0491) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 47.7611(50.2029) | Bit/dim 4.0274(4.0792) | Xent 1.5185(1.5309) | Loss 10.4356(10.9724) | Error 0.5470(0.5509) Steps 538(546.20) | Grad Norm 13.2202(11.1143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.4129, Epoch Time 346.1607(333.0765), Bit/dim 4.0500(best: 4.0117), Xent 1.4978, Loss 4.7989, Error 0.5335(best: 0.5101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 48.4666(50.1508) | Bit/dim 4.0489(4.0783) | Xent 1.5497(1.5315) | Loss 13.3624(11.0441) | Error 0.5477(0.5508) Steps 526(545.60) | Grad Norm 16.0341(11.2619) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 46.0384(50.0274) | Bit/dim 4.0303(4.0769) | Xent 1.5505(1.5320) | Loss 10.4985(11.0277) | Error 0.5586(0.5510) Steps 532(545.19) | Grad Norm 15.5193(11.3896) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 47.6940(49.9574) | Bit/dim 4.0206(4.0752) | Xent 1.4634(1.5300) | Loss 10.4811(11.0113) | Error 0.5308(0.5504) Steps 544(545.15) | Grad Norm 11.0777(11.3802) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 48.4411(49.9119) | Bit/dim 4.0493(4.0744) | Xent 1.4778(1.5284) | Loss 10.4036(10.9931) | Error 0.5383(0.5500) Steps 532(544.76) | Grad Norm 6.6819(11.2393) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 50.3083(49.9238) | Bit/dim 4.0353(4.0732) | Xent 1.4531(1.5261) | Loss 10.4702(10.9774) | Error 0.5262(0.5493) Steps 538(544.55) | Grad Norm 7.5151(11.1276) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 49.3597(49.9069) | Bit/dim 4.0241(4.0717) | Xent 1.4317(1.5233) | Loss 10.3673(10.9591) | Error 0.5171(0.5483) Steps 562(545.08) | Grad Norm 8.4703(11.0478) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 22.8791, Epoch Time 331.5763(333.0315), Bit/dim 3.9938(best: 4.0117), Xent 1.4111, Loss 4.6993, Error 0.5071(best: 0.5101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 49.2056(49.8858) | Bit/dim 3.9868(4.0692) | Xent 1.4374(1.5207) | Loss 12.8850(11.0169) | Error 0.5154(0.5474) Steps 550(545.23) | Grad Norm 8.0572(10.9581) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 48.3519(49.8398) | Bit/dim 3.9997(4.0671) | Xent 1.4721(1.5193) | Loss 10.3122(10.9957) | Error 0.5329(0.5469) Steps 550(545.37) | Grad Norm 11.1354(10.9634) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 55.3481(50.0051) | Bit/dim 3.9989(4.0651) | Xent 1.4493(1.5172) | Loss 10.4283(10.9787) | Error 0.5275(0.5463) Steps 580(546.41) | Grad Norm 9.5817(10.9220) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 55.5313(50.1709) | Bit/dim 3.9722(4.0623) | Xent 1.4760(1.5159) | Loss 10.3194(10.9589) | Error 0.5404(0.5462) Steps 538(546.16) | Grad Norm 11.6237(10.9430) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 48.9422(50.1340) | Bit/dim 3.9844(4.0600) | Xent 1.4343(1.5135) | Loss 10.2148(10.9366) | Error 0.5140(0.5452) Steps 550(546.27) | Grad Norm 7.7833(10.8482) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 52.3534(50.2006) | Bit/dim 3.9813(4.0576) | Xent 1.4078(1.5103) | Loss 10.3812(10.9199) | Error 0.5041(0.5440) Steps 562(546.74) | Grad Norm 5.1169(10.6763) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 23.2929, Epoch Time 352.1394(333.6047), Bit/dim 3.9679(best: 3.9938), Xent 1.3619, Loss 4.6489, Error 0.4905(best: 0.5071)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 50.0227(50.1952) | Bit/dim 3.9663(4.0549) | Xent 1.4115(1.5074) | Loss 13.2768(10.9906) | Error 0.5157(0.5431) Steps 580(547.74) | Grad Norm 5.2628(10.5139) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 50.4060(50.2016) | Bit/dim 3.9672(4.0522) | Xent 1.4078(1.5044) | Loss 10.3904(10.9726) | Error 0.5109(0.5422) Steps 550(547.81) | Grad Norm 7.1843(10.4140) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 53.7113(50.3069) | Bit/dim 3.9759(4.0499) | Xent 1.4012(1.5013) | Loss 10.2399(10.9506) | Error 0.5049(0.5410) Steps 562(548.23) | Grad Norm 9.5689(10.3887) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 48.5702(50.2548) | Bit/dim 3.9791(4.0478) | Xent 1.4475(1.4997) | Loss 10.5232(10.9378) | Error 0.5242(0.5405) Steps 568(548.83) | Grad Norm 13.5701(10.4841) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 50.4090(50.2594) | Bit/dim 3.9559(4.0450) | Xent 1.5062(1.4999) | Loss 10.1674(10.9147) | Error 0.5424(0.5406) Steps 556(549.04) | Grad Norm 13.9453(10.5879) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 50.6410(50.2708) | Bit/dim 4.0165(4.0442) | Xent 1.6152(1.5033) | Loss 10.4159(10.8997) | Error 0.5667(0.5414) Steps 544(548.89) | Grad Norm 24.3948(11.0021) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 23.3897, Epoch Time 346.2504(333.9841), Bit/dim 4.0716(best: 3.9679), Xent 1.4381, Loss 4.7906, Error 0.5215(best: 0.4905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 46.7009(50.1637) | Bit/dim 4.0693(4.0449) | Xent 1.4761(1.5025) | Loss 13.0959(10.9656) | Error 0.5351(0.5412) Steps 556(549.10) | Grad Norm 10.8418(10.9973) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 50.9011(50.1859) | Bit/dim 3.9914(4.0433) | Xent 1.4708(1.5015) | Loss 10.3481(10.9471) | Error 0.5364(0.5410) Steps 550(549.13) | Grad Norm 6.7711(10.8705) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 51.7115(50.2316) | Bit/dim 3.9913(4.0418) | Xent 1.5072(1.5017) | Loss 10.3781(10.9300) | Error 0.5444(0.5411) Steps 568(549.70) | Grad Norm 10.9693(10.8735) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 50.8068(50.2489) | Bit/dim 4.0100(4.0408) | Xent 1.4538(1.5003) | Loss 10.2594(10.9099) | Error 0.5202(0.5405) Steps 562(550.07) | Grad Norm 8.7496(10.8098) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 49.1555(50.2161) | Bit/dim 3.9727(4.0388) | Xent 1.4597(1.4991) | Loss 10.2525(10.8902) | Error 0.5292(0.5402) Steps 556(550.24) | Grad Norm 10.8843(10.8120) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 48.9792(50.1790) | Bit/dim 3.9906(4.0373) | Xent 1.4379(1.4972) | Loss 10.1561(10.8682) | Error 0.5186(0.5395) Steps 544(550.06) | Grad Norm 9.6970(10.7786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 23.1646, Epoch Time 340.7200(334.1862), Bit/dim 3.9805(best: 3.9679), Xent 1.3585, Loss 4.6597, Error 0.4925(best: 0.4905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 51.2586(50.2114) | Bit/dim 3.9930(4.0360) | Xent 1.4119(1.4947) | Loss 13.1125(10.9355) | Error 0.5144(0.5388) Steps 550(550.06) | Grad Norm 6.6707(10.6553) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 49.2645(50.1830) | Bit/dim 3.9863(4.0345) | Xent 1.4069(1.4920) | Loss 10.0658(10.9094) | Error 0.5088(0.5379) Steps 514(548.97) | Grad Norm 7.6720(10.5658) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 49.1979(50.1534) | Bit/dim 3.9806(4.0329) | Xent 1.4033(1.4894) | Loss 10.1957(10.8880) | Error 0.5108(0.5371) Steps 538(548.64) | Grad Norm 10.4626(10.5627) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 46.8708(50.0549) | Bit/dim 3.9832(4.0314) | Xent 1.4186(1.4872) | Loss 10.2518(10.8689) | Error 0.5155(0.5364) Steps 532(548.15) | Grad Norm 8.0276(10.4867) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 52.5564(50.1300) | Bit/dim 3.9465(4.0289) | Xent 1.3819(1.4841) | Loss 10.1205(10.8465) | Error 0.5028(0.5354) Steps 556(548.38) | Grad Norm 3.7930(10.2859) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 51.4249(50.1688) | Bit/dim 3.9557(4.0267) | Xent 1.3882(1.4812) | Loss 10.1981(10.8270) | Error 0.4962(0.5342) Steps 532(547.89) | Grad Norm 5.3361(10.1374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.5831, Epoch Time 343.3711(334.4617), Bit/dim 3.9558(best: 3.9679), Xent 1.3514, Loss 4.6315, Error 0.4862(best: 0.4905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 50.1290(50.1676) | Bit/dim 3.9674(4.0249) | Xent 1.4120(1.4791) | Loss 13.1507(10.8967) | Error 0.5111(0.5335) Steps 526(547.23) | Grad Norm 8.8575(10.0990) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 51.8312(50.2175) | Bit/dim 3.9455(4.0225) | Xent 1.4246(1.4775) | Loss 10.0877(10.8724) | Error 0.5128(0.5329) Steps 526(546.60) | Grad Norm 8.5948(10.0539) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 46.0991(50.0940) | Bit/dim 3.9570(4.0205) | Xent 1.3801(1.4746) | Loss 10.0126(10.8467) | Error 0.4935(0.5317) Steps 520(545.80) | Grad Norm 7.0315(9.9632) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 47.8978(50.0281) | Bit/dim 3.9487(4.0184) | Xent 1.4073(1.4726) | Loss 10.1612(10.8261) | Error 0.5119(0.5311) Steps 532(545.38) | Grad Norm 9.7287(9.9562) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 51.6177(50.0758) | Bit/dim 3.9645(4.0168) | Xent 1.4239(1.4711) | Loss 10.2026(10.8074) | Error 0.5095(0.5305) Steps 544(545.34) | Grad Norm 13.0549(10.0491) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 50.9272(50.1013) | Bit/dim 3.9380(4.0144) | Xent 1.3924(1.4687) | Loss 10.0208(10.7838) | Error 0.4949(0.5294) Steps 544(545.30) | Grad Norm 7.6256(9.9764) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 22.9779, Epoch Time 340.5809(334.6453), Bit/dim 3.9369(best: 3.9558), Xent 1.3360, Loss 4.6049, Error 0.4823(best: 0.4862)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 49.7135(50.0897) | Bit/dim 3.9412(4.0122) | Xent 1.3975(1.4666) | Loss 12.0257(10.8210) | Error 0.5034(0.5286) Steps 532(544.90) | Grad Norm 9.4406(9.9603) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 47.5651(50.0140) | Bit/dim 3.9749(4.0111) | Xent 1.4246(1.4653) | Loss 10.1456(10.8008) | Error 0.5152(0.5282) Steps 538(544.70) | Grad Norm 13.4625(10.0654) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 51.9567(50.0722) | Bit/dim 3.9469(4.0092) | Xent 1.4317(1.4643) | Loss 10.2875(10.7854) | Error 0.5231(0.5281) Steps 550(544.86) | Grad Norm 10.3182(10.0730) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 51.6971(50.1210) | Bit/dim 3.9407(4.0071) | Xent 1.3875(1.4620) | Loss 10.3207(10.7714) | Error 0.4954(0.5271) Steps 562(545.37) | Grad Norm 8.7485(10.0333) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 50.6754(50.1376) | Bit/dim 3.9341(4.0049) | Xent 1.3743(1.4594) | Loss 10.0423(10.7496) | Error 0.5005(0.5263) Steps 562(545.87) | Grad Norm 6.0091(9.9125) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 50.7852(50.1570) | Bit/dim 3.9412(4.0030) | Xent 1.3497(1.4561) | Loss 10.1531(10.7317) | Error 0.4851(0.5251) Steps 568(546.53) | Grad Norm 4.4083(9.7474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 23.1744, Epoch Time 343.9332(334.9239), Bit/dim 3.9314(best: 3.9369), Xent 1.3128, Loss 4.5878, Error 0.4740(best: 0.4823)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 47.1879(50.0680) | Bit/dim 3.9299(4.0008) | Xent 1.3571(1.4531) | Loss 12.9646(10.7987) | Error 0.4921(0.5241) Steps 538(546.28) | Grad Norm 5.7640(9.6279) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 49.7913(50.0597) | Bit/dim 3.9187(3.9984) | Xent 1.3639(1.4505) | Loss 10.1177(10.7782) | Error 0.4976(0.5233) Steps 526(545.67) | Grad Norm 7.3566(9.5598) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 47.4765(49.9822) | Bit/dim 3.9269(3.9962) | Xent 1.3990(1.4489) | Loss 10.2067(10.7611) | Error 0.5055(0.5228) Steps 544(545.62) | Grad Norm 11.0664(9.6050) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 50.5699(49.9998) | Bit/dim 3.9561(3.9950) | Xent 1.4170(1.4480) | Loss 10.3546(10.7489) | Error 0.5070(0.5223) Steps 556(545.93) | Grad Norm 13.8820(9.7333) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 52.1512(50.0643) | Bit/dim 3.9435(3.9935) | Xent 1.3676(1.4455) | Loss 9.9661(10.7254) | Error 0.4921(0.5214) Steps 550(546.05) | Grad Norm 9.1022(9.7143) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 52.7334(50.1444) | Bit/dim 3.9159(3.9911) | Xent 1.3982(1.4441) | Loss 10.0933(10.7064) | Error 0.4984(0.5207) Steps 556(546.35) | Grad Norm 8.9915(9.6927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 23.6418, Epoch Time 343.1419(335.1705), Bit/dim 3.9254(best: 3.9314), Xent 1.4293, Loss 4.6400, Error 0.5096(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 48.1231(50.0838) | Bit/dim 3.9408(3.9896) | Xent 1.4629(1.4447) | Loss 13.0891(10.7779) | Error 0.5136(0.5205) Steps 562(546.82) | Grad Norm 14.8517(9.8474) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 54.3413(50.2115) | Bit/dim 3.9497(3.9884) | Xent 1.5457(1.4477) | Loss 10.3045(10.7637) | Error 0.5394(0.5210) Steps 532(546.37) | Grad Norm 14.9219(9.9997) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 52.2386(50.2723) | Bit/dim 3.9584(3.9875) | Xent 1.4030(1.4464) | Loss 10.0496(10.7423) | Error 0.5035(0.5205) Steps 520(545.58) | Grad Norm 8.9886(9.9693) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 45.0645(50.1161) | Bit/dim 3.9791(3.9873) | Xent 1.5703(1.4501) | Loss 10.3948(10.7319) | Error 0.5404(0.5211) Steps 526(545.00) | Grad Norm 17.4150(10.1927) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 47.0997(50.0256) | Bit/dim 3.9861(3.9872) | Xent 1.7014(1.4576) | Loss 10.5002(10.7249) | Error 0.5891(0.5231) Steps 526(544.43) | Grad Norm 18.5068(10.4421) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 53.9216(50.1425) | Bit/dim 4.1041(3.9907) | Xent 1.6409(1.4631) | Loss 10.7469(10.7256) | Error 0.5721(0.5246) Steps 580(545.49) | Grad Norm 21.0808(10.7613) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 23.6671, Epoch Time 343.5205(335.4210), Bit/dim 4.0638(best: 3.9254), Xent 1.5160, Loss 4.8218, Error 0.5442(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 53.7270(50.2500) | Bit/dim 4.0678(3.9931) | Xent 1.6099(1.4675) | Loss 13.5268(10.8096) | Error 0.5707(0.5260) Steps 622(547.79) | Grad Norm 16.2088(10.9247) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 49.1573(50.2172) | Bit/dim 4.0617(3.9951) | Xent 1.5941(1.4713) | Loss 10.2947(10.7942) | Error 0.5705(0.5273) Steps 568(548.40) | Grad Norm 14.9766(11.0463) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 55.3242(50.3704) | Bit/dim 4.0352(3.9963) | Xent 1.4796(1.4716) | Loss 10.4988(10.7853) | Error 0.5317(0.5275) Steps 580(549.34) | Grad Norm 6.3673(10.9059) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 52.3958(50.4312) | Bit/dim 4.0389(3.9976) | Xent 1.5205(1.4730) | Loss 10.4939(10.7766) | Error 0.5463(0.5280) Steps 586(550.44) | Grad Norm 8.6239(10.8374) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 53.5276(50.5241) | Bit/dim 4.0207(3.9983) | Xent 1.5571(1.4756) | Loss 10.5835(10.7708) | Error 0.5597(0.5290) Steps 574(551.15) | Grad Norm 13.4693(10.9164) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 49.0939(50.4812) | Bit/dim 4.0577(4.0001) | Xent 1.5765(1.4786) | Loss 10.1551(10.7523) | Error 0.5741(0.5303) Steps 538(550.76) | Grad Norm 12.6528(10.9685) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 23.0923, Epoch Time 354.3057(335.9875), Bit/dim 4.0222(best: 3.9254), Xent 1.4087, Loss 4.7265, Error 0.5167(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 55.1177(50.6203) | Bit/dim 4.0085(4.0003) | Xent 1.4757(1.4785) | Loss 13.5051(10.8349) | Error 0.5320(0.5304) Steps 562(551.09) | Grad Norm 6.7346(10.8415) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 51.1273(50.6355) | Bit/dim 4.0160(4.0008) | Xent 1.4791(1.4785) | Loss 10.2661(10.8178) | Error 0.5304(0.5304) Steps 568(551.60) | Grad Norm 8.8201(10.7808) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 47.7996(50.5504) | Bit/dim 4.0022(4.0008) | Xent 1.4262(1.4770) | Loss 10.4255(10.8061) | Error 0.5185(0.5300) Steps 538(551.19) | Grad Norm 5.9016(10.6345) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 49.7616(50.5267) | Bit/dim 4.0364(4.0019) | Xent 1.4160(1.4751) | Loss 10.2895(10.7906) | Error 0.5048(0.5293) Steps 550(551.16) | Grad Norm 9.2983(10.5944) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 47.8369(50.4461) | Bit/dim 4.0158(4.0023) | Xent 1.4232(1.4736) | Loss 10.1952(10.7727) | Error 0.5149(0.5288) Steps 532(550.58) | Grad Norm 7.2347(10.4936) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 55.7663(50.6057) | Bit/dim 4.0094(4.0025) | Xent 1.4257(1.4721) | Loss 10.1722(10.7547) | Error 0.5142(0.5284) Steps 562(550.92) | Grad Norm 5.1619(10.3336) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 22.5552, Epoch Time 349.1360(336.3820), Bit/dim 3.9755(best: 3.9254), Xent 1.3699, Loss 4.6605, Error 0.4881(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 49.9557(50.5862) | Bit/dim 3.9693(4.0015) | Xent 1.4204(1.4706) | Loss 13.3123(10.8314) | Error 0.5132(0.5279) Steps 544(550.72) | Grad Norm 5.6989(10.1946) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 48.1228(50.5123) | Bit/dim 3.9895(4.0012) | Xent 1.4323(1.4694) | Loss 10.2084(10.8127) | Error 0.5076(0.5273) Steps 538(550.33) | Grad Norm 9.1374(10.1629) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 55.7812(50.6703) | Bit/dim 3.9758(4.0004) | Xent 1.4374(1.4685) | Loss 10.1391(10.7925) | Error 0.5216(0.5272) Steps 526(549.60) | Grad Norm 10.2343(10.1650) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 50.9942(50.6800) | Bit/dim 3.9560(3.9991) | Xent 1.4107(1.4667) | Loss 10.2791(10.7771) | Error 0.5031(0.5264) Steps 538(549.26) | Grad Norm 9.6093(10.1483) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 50.5827(50.6771) | Bit/dim 3.9614(3.9979) | Xent 1.3967(1.4646) | Loss 10.0332(10.7548) | Error 0.5052(0.5258) Steps 544(549.10) | Grad Norm 7.5674(10.0709) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 53.8009(50.7708) | Bit/dim 3.9388(3.9962) | Xent 1.3582(1.4614) | Loss 10.1768(10.7375) | Error 0.4904(0.5247) Steps 544(548.95) | Grad Norm 3.6692(9.8789) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 22.7631, Epoch Time 351.3027(336.8296), Bit/dim 3.9428(best: 3.9254), Xent 1.3229, Loss 4.6042, Error 0.4778(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 50.2467(50.7551) | Bit/dim 3.9469(3.9947) | Xent 1.3799(1.4590) | Loss 13.2394(10.8125) | Error 0.5041(0.5241) Steps 550(548.98) | Grad Norm 6.2646(9.7704) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 49.0650(50.7044) | Bit/dim 3.9581(3.9936) | Xent 1.4168(1.4577) | Loss 10.1840(10.7937) | Error 0.5111(0.5237) Steps 538(548.65) | Grad Norm 9.1340(9.7513) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 51.3169(50.7228) | Bit/dim 3.9222(3.9915) | Xent 1.3686(1.4551) | Loss 10.1662(10.7748) | Error 0.4938(0.5228) Steps 562(549.05) | Grad Norm 7.5774(9.6861) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 52.5996(50.7791) | Bit/dim 3.9324(3.9897) | Xent 1.3509(1.4519) | Loss 9.9785(10.7509) | Error 0.4850(0.5217) Steps 526(548.36) | Grad Norm 6.9918(9.6053) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 50.8927(50.7825) | Bit/dim 3.9240(3.9877) | Xent 1.3496(1.4489) | Loss 9.9657(10.7274) | Error 0.4941(0.5209) Steps 526(547.69) | Grad Norm 5.6632(9.4870) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 48.5450(50.7154) | Bit/dim 3.9073(3.9853) | Xent 1.3269(1.4452) | Loss 10.0286(10.7064) | Error 0.4782(0.5196) Steps 538(547.40) | Grad Norm 3.1144(9.2959) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 23.6269, Epoch Time 345.4488(337.0882), Bit/dim 3.9068(best: 3.9254), Xent 1.2864, Loss 4.5500, Error 0.4600(best: 0.4740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 48.6015(50.6520) | Bit/dim 3.9043(3.9829) | Xent 1.3151(1.4413) | Loss 12.7034(10.7663) | Error 0.4720(0.5182) Steps 556(547.65) | Grad Norm 4.5657(9.1540) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 50.4632(50.6463) | Bit/dim 3.9258(3.9812) | Xent 1.3195(1.4376) | Loss 9.9849(10.7429) | Error 0.4760(0.5169) Steps 568(548.26) | Grad Norm 4.6562(9.0190) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 48.5663(50.5839) | Bit/dim 3.9189(3.9793) | Xent 1.2938(1.4333) | Loss 10.1553(10.7253) | Error 0.4575(0.5151) Steps 556(548.50) | Grad Norm 3.5094(8.8537) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 51.1946(50.6022) | Bit/dim 3.8897(3.9766) | Xent 1.3013(1.4294) | Loss 9.7872(10.6971) | Error 0.4651(0.5136) Steps 550(548.54) | Grad Norm 2.6970(8.6690) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 51.2803(50.6226) | Bit/dim 3.8961(3.9742) | Xent 1.3136(1.4259) | Loss 9.7106(10.6675) | Error 0.4744(0.5124) Steps 550(548.59) | Grad Norm 3.2125(8.5053) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 52.3551(50.6745) | Bit/dim 3.8805(3.9714) | Xent 1.3045(1.4222) | Loss 10.0728(10.6497) | Error 0.4715(0.5112) Steps 544(548.45) | Grad Norm 3.6080(8.3584) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 22.7603, Epoch Time 343.8091(337.2898), Bit/dim 3.8914(best: 3.9068), Xent 1.2532, Loss 4.5180, Error 0.4504(best: 0.4600)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 53.1146(50.7477) | Bit/dim 3.8911(3.9690) | Xent 1.3079(1.4188) | Loss 13.1352(10.7242) | Error 0.4631(0.5098) Steps 532(547.95) | Grad Norm 5.1053(8.2608) | Total Time 0.00(0.00)\n",
      "Iter 0506 | Time 45.2612(50.5831) | Bit/dim 3.9087(3.9672) | Xent 1.3453(1.4166) | Loss 9.8051(10.6967) | Error 0.4790(0.5089) Steps 526(547.30) | Grad Norm 8.2975(8.2619) | Total Time 0.00(0.00)\n",
      "Iter 0507 | Time 49.4981(50.5506) | Bit/dim 3.8953(3.9650) | Xent 1.4711(1.4182) | Loss 10.0629(10.6777) | Error 0.5214(0.5092) Steps 532(546.84) | Grad Norm 17.9819(8.5535) | Total Time 0.00(0.00)\n",
      "Iter 0508 | Time 48.1544(50.4787) | Bit/dim 3.9214(3.9637) | Xent 1.6972(1.4266) | Loss 10.3973(10.6692) | Error 0.5556(0.5106) Steps 562(547.29) | Grad Norm 16.2467(8.7843) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars_nobnb.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_scale_trust_0_02_nobnb_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0 --trust_coefficient 0.02 --clip True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
