{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run3/epoch_400_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run3', seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 2401 | Time 113.4760(62.4296) | Bit/dim 3.7852(3.8381) | Xent 1.3622(1.3978) | Loss 4.4663(4.5370) | Error 0.4921(0.4879) Steps 676(637.49) | Grad Norm 3.0051(29.1666) | Total Time 14.00(14.00)\n",
      "Iter 2402 | Time 62.9025(62.4438) | Bit/dim 3.7904(3.8367) | Xent 1.3554(1.3965) | Loss 4.4681(4.5349) | Error 0.4926(0.4881) Steps 682(638.82) | Grad Norm 2.8898(28.3783) | Total Time 14.00(14.00)\n",
      "Iter 2403 | Time 61.6740(62.4207) | Bit/dim 3.7905(3.8353) | Xent 1.3604(1.3955) | Loss 4.4707(4.5330) | Error 0.4929(0.4882) Steps 682(640.12) | Grad Norm 2.4706(27.6010) | Total Time 14.00(14.00)\n",
      "Iter 2404 | Time 63.6074(62.4563) | Bit/dim 3.7759(3.8335) | Xent 1.3636(1.3945) | Loss 4.4577(4.5307) | Error 0.4896(0.4883) Steps 694(641.73) | Grad Norm 2.1285(26.8369) | Total Time 14.00(14.00)\n",
      "Iter 2405 | Time 63.1121(62.4760) | Bit/dim 3.7659(3.8315) | Xent 1.3377(1.3928) | Loss 4.4347(4.5279) | Error 0.4829(0.4881) Steps 682(642.94) | Grad Norm 1.8596(26.0875) | Total Time 14.00(14.00)\n",
      "Iter 2406 | Time 61.4154(62.4442) | Bit/dim 3.7623(3.8294) | Xent 1.3297(1.3909) | Loss 4.4272(4.5248) | Error 0.4770(0.4878) Steps 688(644.29) | Grad Norm 1.5385(25.3511) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 41.3662, Epoch Time 483.2675(379.0524), Bit/dim 3.7680(best: inf), Xent 1.2640, Loss 4.4000, Error 0.4556(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 77.2114(62.8872) | Bit/dim 3.7721(3.8277) | Xent 1.3214(1.3888) | Loss 4.4328(4.5221) | Error 0.4772(0.4875) Steps 682(645.42) | Grad Norm 1.8001(24.6445) | Total Time 14.00(14.00)\n",
      "Iter 2408 | Time 65.0067(62.9508) | Bit/dim 3.7643(3.8258) | Xent 1.3060(1.3863) | Loss 4.4173(4.5189) | Error 0.4781(0.4872) Steps 694(646.88) | Grad Norm 1.3560(23.9459) | Total Time 14.00(14.00)\n",
      "Iter 2409 | Time 63.3520(62.9628) | Bit/dim 3.7540(3.8236) | Xent 1.2896(1.3834) | Loss 4.3988(4.5153) | Error 0.4581(0.4863) Steps 676(647.75) | Grad Norm 1.5943(23.2753) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 58.2977(62.8228) | Bit/dim 3.7553(3.8216) | Xent 1.3158(1.3814) | Loss 4.4132(4.5123) | Error 0.4714(0.4859) Steps 676(648.60) | Grad Norm 1.3577(22.6178) | Total Time 14.00(14.00)\n",
      "Iter 2411 | Time 60.9437(62.7665) | Bit/dim 3.7525(3.8195) | Xent 1.2948(1.3788) | Loss 4.3999(4.5089) | Error 0.4654(0.4852) Steps 676(649.42) | Grad Norm 1.2413(21.9765) | Total Time 14.00(14.00)\n",
      "Iter 2412 | Time 58.1282(62.6273) | Bit/dim 3.7676(3.8179) | Xent 1.3031(1.3765) | Loss 4.4192(4.5062) | Error 0.4685(0.4847) Steps 682(650.40) | Grad Norm 1.2774(21.3555) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 26.0139, Epoch Time 424.7792(380.4242), Bit/dim 3.7588(best: 3.7680), Xent 1.2434, Loss 4.3805, Error 0.4437(best: 0.4556)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 63.3419(62.6488) | Bit/dim 3.7603(3.8162) | Xent 1.3065(1.3744) | Loss 4.4135(4.5034) | Error 0.4670(0.4842) Steps 676(651.17) | Grad Norm 1.2667(20.7529) | Total Time 14.00(14.00)\n",
      "Iter 2414 | Time 60.2338(62.5763) | Bit/dim 3.7625(3.8146) | Xent 1.2900(1.3719) | Loss 4.4075(4.5006) | Error 0.4630(0.4836) Steps 682(652.09) | Grad Norm 1.2141(20.1667) | Total Time 14.00(14.00)\n",
      "Iter 2415 | Time 61.5175(62.5446) | Bit/dim 3.7476(3.8126) | Xent 1.2965(1.3696) | Loss 4.3959(4.4974) | Error 0.4676(0.4831) Steps 676(652.81) | Grad Norm 1.2044(19.5978) | Total Time 14.00(14.00)\n",
      "Iter 2416 | Time 60.9539(62.4968) | Bit/dim 3.7549(3.8109) | Xent 1.2972(1.3675) | Loss 4.4035(4.4946) | Error 0.4680(0.4826) Steps 682(653.69) | Grad Norm 1.3440(19.0502) | Total Time 14.00(14.00)\n",
      "Iter 2417 | Time 62.8854(62.5085) | Bit/dim 3.7609(3.8094) | Xent 1.2843(1.3650) | Loss 4.4030(4.4918) | Error 0.4603(0.4820) Steps 688(654.72) | Grad Norm 1.2647(18.5167) | Total Time 14.00(14.00)\n",
      "Iter 2418 | Time 60.8346(62.4583) | Bit/dim 3.7486(3.8075) | Xent 1.2938(1.3628) | Loss 4.3955(4.4890) | Error 0.4704(0.4816) Steps 682(655.53) | Grad Norm 1.2724(17.9993) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 25.8194, Epoch Time 411.3160(381.3510), Bit/dim 3.7525(best: 3.7588), Xent 1.2291, Loss 4.3670, Error 0.4385(best: 0.4437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 58.8156(62.3490) | Bit/dim 3.7556(3.8060) | Xent 1.2895(1.3606) | Loss 4.4004(4.4863) | Error 0.4653(0.4811) Steps 682(656.33) | Grad Norm 0.9786(17.4887) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 62.6095(62.3568) | Bit/dim 3.7416(3.8041) | Xent 1.2863(1.3584) | Loss 4.3847(4.4833) | Error 0.4620(0.4806) Steps 682(657.10) | Grad Norm 0.8848(16.9906) | Total Time 14.00(14.00)\n",
      "Iter 2421 | Time 61.4909(62.3308) | Bit/dim 3.7560(3.8026) | Xent 1.2847(1.3562) | Loss 4.3984(4.4807) | Error 0.4576(0.4799) Steps 682(657.85) | Grad Norm 0.8832(16.5074) | Total Time 14.00(14.00)\n",
      "Iter 2422 | Time 56.9793(62.1703) | Bit/dim 3.7514(3.8011) | Xent 1.2693(1.3536) | Loss 4.3861(4.4779) | Error 0.4564(0.4792) Steps 682(658.57) | Grad Norm 0.8201(16.0368) | Total Time 14.00(14.00)\n",
      "Iter 2423 | Time 61.2739(62.1434) | Bit/dim 3.7541(3.7997) | Xent 1.2667(1.3510) | Loss 4.3874(4.4752) | Error 0.4524(0.4784) Steps 682(659.27) | Grad Norm 0.8043(15.5798) | Total Time 14.00(14.00)\n",
      "Iter 2424 | Time 60.7113(62.1004) | Bit/dim 3.7458(3.7980) | Xent 1.2468(1.3479) | Loss 4.3692(4.4720) | Error 0.4456(0.4774) Steps 682(659.96) | Grad Norm 0.8348(15.1374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 26.0245, Epoch Time 403.2741(382.0087), Bit/dim 3.7505(best: 3.7525), Xent 1.2126, Loss 4.3568, Error 0.4352(best: 0.4385)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 60.4162(62.0499) | Bit/dim 3.7482(3.7966) | Xent 1.2565(1.3451) | Loss 4.3765(4.4691) | Error 0.4554(0.4767) Steps 682(660.62) | Grad Norm 0.8069(14.7075) | Total Time 14.00(14.00)\n",
      "Iter 2426 | Time 60.2107(61.9947) | Bit/dim 3.7444(3.7950) | Xent 1.2753(1.3430) | Loss 4.3821(4.4665) | Error 0.4587(0.4762) Steps 682(661.26) | Grad Norm 0.7983(14.2902) | Total Time 14.00(14.00)\n",
      "Iter 2427 | Time 59.8955(61.9318) | Bit/dim 3.7479(3.7936) | Xent 1.2427(1.3400) | Loss 4.3692(4.4636) | Error 0.4490(0.4754) Steps 676(661.70) | Grad Norm 0.7656(13.8845) | Total Time 14.00(14.00)\n",
      "Iter 2428 | Time 60.1563(61.8785) | Bit/dim 3.7419(3.7920) | Xent 1.2767(1.3381) | Loss 4.3802(4.4611) | Error 0.4546(0.4747) Steps 682(662.31) | Grad Norm 1.0099(13.4983) | Total Time 14.00(14.00)\n",
      "Iter 2429 | Time 59.4687(61.8062) | Bit/dim 3.7493(3.7907) | Xent 1.2506(1.3355) | Loss 4.3745(4.4585) | Error 0.4491(0.4740) Steps 676(662.72) | Grad Norm 0.8605(13.1191) | Total Time 14.00(14.00)\n",
      "Iter 2430 | Time 58.7100(61.7133) | Bit/dim 3.7552(3.7897) | Xent 1.2387(1.3326) | Loss 4.3746(4.4560) | Error 0.4503(0.4733) Steps 676(663.12) | Grad Norm 0.8513(12.7511) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 25.3836, Epoch Time 399.7100(382.5397), Bit/dim 3.7486(best: 3.7505), Xent 1.1999, Loss 4.3485, Error 0.4283(best: 0.4352)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 59.6034(61.6500) | Bit/dim 3.7443(3.7883) | Xent 1.2575(1.3303) | Loss 4.3731(4.4535) | Error 0.4625(0.4729) Steps 676(663.50) | Grad Norm 0.8752(12.3948) | Total Time 14.00(14.00)\n",
      "Iter 2432 | Time 56.8160(61.5050) | Bit/dim 3.7415(3.7869) | Xent 1.2554(1.3281) | Loss 4.3691(4.4509) | Error 0.4490(0.4722) Steps 676(663.88) | Grad Norm 0.6950(12.0438) | Total Time 14.00(14.00)\n",
      "Iter 2433 | Time 59.4110(61.4422) | Bit/dim 3.7556(3.7860) | Xent 1.2171(1.3247) | Loss 4.3642(4.4483) | Error 0.4366(0.4712) Steps 676(664.24) | Grad Norm 0.6135(11.7009) | Total Time 14.00(14.00)\n",
      "Iter 2434 | Time 60.7775(61.4222) | Bit/dim 3.7424(3.7847) | Xent 1.2533(1.3226) | Loss 4.3690(4.4460) | Error 0.4491(0.4705) Steps 682(664.78) | Grad Norm 0.7170(11.3714) | Total Time 14.00(14.00)\n",
      "Iter 2435 | Time 60.0874(61.3822) | Bit/dim 3.7484(3.7836) | Xent 1.2388(1.3201) | Loss 4.3678(4.4436) | Error 0.4486(0.4698) Steps 682(665.29) | Grad Norm 0.7150(11.0517) | Total Time 14.00(14.00)\n",
      "Iter 2436 | Time 58.7911(61.3045) | Bit/dim 3.7490(3.7825) | Xent 1.2496(1.3180) | Loss 4.3738(4.4415) | Error 0.4465(0.4691) Steps 670(665.43) | Grad Norm 0.9344(10.7482) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 25.4488, Epoch Time 396.5574(382.9602), Bit/dim 3.7468(best: 3.7486), Xent 1.1904, Loss 4.3419, Error 0.4268(best: 0.4283)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2437 | Time 61.4119(61.3077) | Bit/dim 3.7481(3.7815) | Xent 1.2402(1.3156) | Loss 4.3682(4.4393) | Error 0.4456(0.4684) Steps 676(665.75) | Grad Norm 0.6119(10.4441) | Total Time 14.00(14.00)\n",
      "Iter 2438 | Time 60.9116(61.2958) | Bit/dim 3.7366(3.7802) | Xent 1.2518(1.3137) | Loss 4.3625(4.4370) | Error 0.4526(0.4680) Steps 682(666.24) | Grad Norm 0.6914(10.1515) | Total Time 14.00(14.00)\n",
      "Iter 2439 | Time 56.9144(61.1644) | Bit/dim 3.7339(3.7788) | Xent 1.2481(1.3118) | Loss 4.3580(4.4346) | Error 0.4465(0.4673) Steps 676(666.53) | Grad Norm 0.6244(9.8657) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 61.6092(61.1777) | Bit/dim 3.7529(3.7780) | Xent 1.2389(1.3096) | Loss 4.3724(4.4328) | Error 0.4470(0.4667) Steps 670(666.64) | Grad Norm 2.5480(9.6462) | Total Time 14.00(14.00)\n",
      "Iter 2441 | Time 59.1648(61.1173) | Bit/dim 3.7493(3.7771) | Xent 1.2173(1.3068) | Loss 4.3579(4.4305) | Error 0.4339(0.4657) Steps 676(666.92) | Grad Norm 0.7199(9.3784) | Total Time 14.00(14.00)\n",
      "Iter 2442 | Time 58.7283(61.0456) | Bit/dim 3.7465(3.7762) | Xent 1.2389(1.3048) | Loss 4.3660(4.4286) | Error 0.4481(0.4652) Steps 676(667.19) | Grad Norm 0.6047(9.1152) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 25.2404, Epoch Time 399.9651(383.4704), Bit/dim 3.7445(best: 3.7468), Xent 1.1829, Loss 4.3360, Error 0.4263(best: 0.4268)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2443 | Time 59.8544(61.0099) | Bit/dim 3.7441(3.7753) | Xent 1.2285(1.3025) | Loss 4.3584(4.4265) | Error 0.4419(0.4645) Steps 676(667.45) | Grad Norm 0.5168(8.8572) | Total Time 14.00(14.00)\n",
      "Iter 2444 | Time 57.3745(60.9008) | Bit/dim 3.7475(3.7744) | Xent 1.2276(1.3002) | Loss 4.3614(4.4245) | Error 0.4385(0.4637) Steps 670(667.53) | Grad Norm 0.5562(8.6082) | Total Time 14.00(14.00)\n",
      "Iter 2445 | Time 59.6675(60.8638) | Bit/dim 3.7500(3.7737) | Xent 1.2295(1.2981) | Loss 4.3647(4.4227) | Error 0.4384(0.4630) Steps 670(667.60) | Grad Norm 0.4342(8.3630) | Total Time 14.00(14.00)\n",
      "Iter 2446 | Time 57.8224(60.7726) | Bit/dim 3.7317(3.7724) | Xent 1.2327(1.2962) | Loss 4.3481(4.4205) | Error 0.4399(0.4623) Steps 688(668.22) | Grad Norm 0.4165(8.1246) | Total Time 14.00(14.00)\n",
      "Iter 2447 | Time 61.4346(60.7925) | Bit/dim 3.7456(3.7716) | Xent 1.2406(1.2945) | Loss 4.3659(4.4189) | Error 0.4445(0.4617) Steps 676(668.45) | Grad Norm 0.4893(7.8955) | Total Time 14.00(14.00)\n",
      "Iter 2448 | Time 58.7191(60.7303) | Bit/dim 3.7409(3.7707) | Xent 1.2277(1.2925) | Loss 4.3547(4.4169) | Error 0.4387(0.4610) Steps 682(668.86) | Grad Norm 0.5058(7.6738) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 25.3525, Epoch Time 395.9283(383.8441), Bit/dim 3.7432(best: 3.7445), Xent 1.1778, Loss 4.3321, Error 0.4227(best: 0.4263)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2449 | Time 61.5883(60.7560) | Bit/dim 3.7431(3.7699) | Xent 1.2620(1.2916) | Loss 4.3741(4.4157) | Error 0.4520(0.4608) Steps 676(669.07) | Grad Norm 0.5044(7.4588) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 61.5129(60.7787) | Bit/dim 3.7408(3.7690) | Xent 1.2265(1.2896) | Loss 4.3540(4.4138) | Error 0.4385(0.4601) Steps 676(669.28) | Grad Norm 0.5375(7.2511) | Total Time 14.00(14.00)\n",
      "Iter 2451 | Time 60.2200(60.7619) | Bit/dim 3.7450(3.7683) | Xent 1.2339(1.2879) | Loss 4.3619(4.4122) | Error 0.4450(0.4596) Steps 682(669.66) | Grad Norm 0.6305(7.0525) | Total Time 14.00(14.00)\n",
      "Iter 2452 | Time 59.7286(60.7309) | Bit/dim 3.7448(3.7676) | Xent 1.2280(1.2861) | Loss 4.3588(4.4106) | Error 0.4416(0.4591) Steps 676(669.85) | Grad Norm 0.5288(6.8568) | Total Time 14.00(14.00)\n",
      "Iter 2453 | Time 57.4542(60.6326) | Bit/dim 3.7405(3.7668) | Xent 1.2208(1.2842) | Loss 4.3509(4.4089) | Error 0.4386(0.4585) Steps 670(669.85) | Grad Norm 0.8109(6.6754) | Total Time 14.00(14.00)\n",
      "Iter 2454 | Time 59.0679(60.5857) | Bit/dim 3.7419(3.7660) | Xent 1.2024(1.2817) | Loss 4.3431(4.4069) | Error 0.4367(0.4578) Steps 670(669.86) | Grad Norm 0.6226(6.4938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 25.5321, Epoch Time 400.5721(384.3460), Bit/dim 3.7419(best: 3.7432), Xent 1.1736, Loss 4.3287, Error 0.4219(best: 0.4227)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2455 | Time 59.9279(60.5660) | Bit/dim 3.7421(3.7653) | Xent 1.2169(1.2798) | Loss 4.3506(4.4052) | Error 0.4441(0.4574) Steps 670(669.86) | Grad Norm 0.4821(6.3135) | Total Time 14.00(14.00)\n",
      "Iter 2456 | Time 59.3872(60.5306) | Bit/dim 3.7362(3.7644) | Xent 1.2193(1.2780) | Loss 4.3458(4.4034) | Error 0.4414(0.4569) Steps 676(670.05) | Grad Norm 0.5091(6.1393) | Total Time 14.00(14.00)\n",
      "Iter 2457 | Time 58.4448(60.4680) | Bit/dim 3.7449(3.7638) | Xent 1.2315(1.2766) | Loss 4.3607(4.4021) | Error 0.4391(0.4564) Steps 670(670.05) | Grad Norm 0.5635(5.9721) | Total Time 14.00(14.00)\n",
      "Iter 2458 | Time 59.8163(60.4485) | Bit/dim 3.7446(3.7633) | Xent 1.2175(1.2748) | Loss 4.3533(4.4007) | Error 0.4367(0.4558) Steps 676(670.22) | Grad Norm 0.5857(5.8105) | Total Time 14.00(14.00)\n",
      "Iter 2459 | Time 57.1948(60.3509) | Bit/dim 3.7444(3.7627) | Xent 1.2098(1.2729) | Loss 4.3493(4.3991) | Error 0.4336(0.4552) Steps 670(670.22) | Grad Norm 0.5255(5.6519) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 60.3535(60.3509) | Bit/dim 3.7287(3.7617) | Xent 1.2206(1.2713) | Loss 4.3390(4.3973) | Error 0.4371(0.4546) Steps 670(670.21) | Grad Norm 0.5012(5.4974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 25.4337, Epoch Time 396.3860(384.7072), Bit/dim 3.7396(best: 3.7419), Xent 1.1682, Loss 4.3237, Error 0.4212(best: 0.4219)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2461 | Time 58.3078(60.2896) | Bit/dim 3.7444(3.7612) | Xent 1.2334(1.2702) | Loss 4.3611(4.3962) | Error 0.4440(0.4543) Steps 676(670.38) | Grad Norm 0.7691(5.3556) | Total Time 14.00(14.00)\n",
      "Iter 2462 | Time 58.2856(60.2295) | Bit/dim 3.7432(3.7606) | Xent 1.1970(1.2680) | Loss 4.3417(4.3946) | Error 0.4303(0.4536) Steps 670(670.37) | Grad Norm 0.4111(5.2072) | Total Time 14.00(14.00)\n",
      "Iter 2463 | Time 59.3494(60.2031) | Bit/dim 3.7344(3.7598) | Xent 1.2109(1.2662) | Loss 4.3399(4.3930) | Error 0.4341(0.4530) Steps 676(670.54) | Grad Norm 0.4938(5.0658) | Total Time 14.00(14.00)\n",
      "Iter 2464 | Time 60.8130(60.2214) | Bit/dim 3.7390(3.7592) | Xent 1.2148(1.2647) | Loss 4.3464(4.3916) | Error 0.4289(0.4523) Steps 670(670.53) | Grad Norm 0.6511(4.9334) | Total Time 14.00(14.00)\n",
      "Iter 2465 | Time 58.4599(60.1686) | Bit/dim 3.7327(3.7584) | Xent 1.2331(1.2638) | Loss 4.3493(4.3903) | Error 0.4417(0.4520) Steps 676(670.69) | Grad Norm 0.5974(4.8033) | Total Time 14.00(14.00)\n",
      "Iter 2466 | Time 60.8171(60.1880) | Bit/dim 3.7396(3.7579) | Xent 1.2147(1.2623) | Loss 4.3470(4.3890) | Error 0.4387(0.4516) Steps 682(671.03) | Grad Norm 0.6350(4.6782) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 25.6873, Epoch Time 397.4856(385.0905), Bit/dim 3.7392(best: 3.7396), Xent 1.1630, Loss 4.3207, Error 0.4180(best: 0.4212)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2467 | Time 58.4845(60.1369) | Bit/dim 3.7377(3.7572) | Xent 1.2119(1.2608) | Loss 4.3437(4.3876) | Error 0.4321(0.4510) Steps 676(671.18) | Grad Norm 0.4331(4.5509) | Total Time 14.00(14.00)\n",
      "Iter 2468 | Time 58.5896(60.0905) | Bit/dim 3.7533(3.7571) | Xent 1.2148(1.2594) | Loss 4.3607(4.3868) | Error 0.4383(0.4506) Steps 670(671.14) | Grad Norm 0.5797(4.4318) | Total Time 14.00(14.00)\n",
      "Iter 2469 | Time 61.4814(60.1322) | Bit/dim 3.7271(3.7562) | Xent 1.2160(1.2581) | Loss 4.3351(4.3853) | Error 0.4396(0.4503) Steps 670(671.11) | Grad Norm 0.5948(4.3167) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 61.4131(60.1707) | Bit/dim 3.7400(3.7557) | Xent 1.1963(1.2562) | Loss 4.3382(4.3839) | Error 0.4275(0.4496) Steps 676(671.26) | Grad Norm 0.4786(4.2015) | Total Time 14.00(14.00)\n",
      "Iter 2471 | Time 62.1609(60.2304) | Bit/dim 3.7327(3.7551) | Xent 1.2174(1.2551) | Loss 4.3414(4.3826) | Error 0.4366(0.4492) Steps 676(671.40) | Grad Norm 0.4620(4.0893) | Total Time 14.00(14.00)\n",
      "Iter 2472 | Time 60.2359(60.2305) | Bit/dim 3.7337(3.7544) | Xent 1.2193(1.2540) | Loss 4.3433(4.3814) | Error 0.4380(0.4489) Steps 676(671.54) | Grad Norm 0.6767(3.9869) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 25.2497, Epoch Time 403.0326(385.6288), Bit/dim 3.7369(best: 3.7392), Xent 1.1619, Loss 4.3179, Error 0.4185(best: 0.4180)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2473 | Time 59.5620(60.2105) | Bit/dim 3.7335(3.7538) | Xent 1.2057(1.2525) | Loss 4.3364(4.3801) | Error 0.4324(0.4484) Steps 670(671.49) | Grad Norm 0.3411(3.8776) | Total Time 14.00(14.00)\n",
      "Iter 2474 | Time 57.9225(60.1418) | Bit/dim 3.7364(3.7533) | Xent 1.2225(1.2516) | Loss 4.3477(4.3791) | Error 0.4375(0.4480) Steps 682(671.81) | Grad Norm 0.5720(3.7784) | Total Time 14.00(14.00)\n",
      "Iter 2475 | Time 60.5571(60.1543) | Bit/dim 3.7325(3.7526) | Xent 1.2022(1.2502) | Loss 4.3336(4.3777) | Error 0.4367(0.4477) Steps 670(671.75) | Grad Norm 0.4693(3.6791) | Total Time 14.00(14.00)\n",
      "Iter 2476 | Time 58.3962(60.1016) | Bit/dim 3.7312(3.7520) | Xent 1.2224(1.2493) | Loss 4.3424(4.3767) | Error 0.4361(0.4473) Steps 676(671.88) | Grad Norm 0.4751(3.5830) | Total Time 14.00(14.00)\n",
      "Iter 2477 | Time 59.3937(60.0803) | Bit/dim 3.7397(3.7516) | Xent 1.1822(1.2473) | Loss 4.3308(4.3753) | Error 0.4250(0.4467) Steps 664(671.64) | Grad Norm 0.5927(3.4933) | Total Time 14.00(14.00)\n",
      "Iter 2478 | Time 60.4512(60.0914) | Bit/dim 3.7380(3.7512) | Xent 1.2045(1.2460) | Loss 4.3403(4.3742) | Error 0.4406(0.4465) Steps 670(671.59) | Grad Norm 0.3983(3.4004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 25.6300, Epoch Time 397.7070(385.9911), Bit/dim 3.7368(best: 3.7369), Xent 1.1568, Loss 4.3152, Error 0.4172(best: 0.4180)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2479 | Time 60.8067(60.1129) | Bit/dim 3.7494(3.7512) | Xent 1.2208(1.2453) | Loss 4.3597(4.3738) | Error 0.4341(0.4461) Steps 670(671.54) | Grad Norm 0.4824(3.3129) | Total Time 14.00(14.00)\n",
      "Iter 2480 | Time 59.4405(60.0927) | Bit/dim 3.7304(3.7505) | Xent 1.1997(1.2439) | Loss 4.3302(4.3725) | Error 0.4294(0.4456) Steps 670(671.50) | Grad Norm 0.4851(3.2281) | Total Time 14.00(14.00)\n",
      "Iter 2481 | Time 60.5734(60.1072) | Bit/dim 3.7324(3.7500) | Xent 1.2007(1.2426) | Loss 4.3327(4.3713) | Error 0.4304(0.4452) Steps 670(671.45) | Grad Norm 0.5244(3.1470) | Total Time 14.00(14.00)\n",
      "Iter 2482 | Time 59.0432(60.0752) | Bit/dim 3.7400(3.7497) | Xent 1.2024(1.2414) | Loss 4.3411(4.3704) | Error 0.4310(0.4447) Steps 670(671.41) | Grad Norm 0.4224(3.0652) | Total Time 14.00(14.00)\n",
      "Iter 2483 | Time 58.2703(60.0211) | Bit/dim 3.7261(3.7490) | Xent 1.1957(1.2400) | Loss 4.3240(4.3690) | Error 0.4377(0.4445) Steps 670(671.37) | Grad Norm 0.8497(2.9988) | Total Time 14.00(14.00)\n",
      "Iter 2484 | Time 58.0198(59.9610) | Bit/dim 3.7267(3.7483) | Xent 1.1966(1.2387) | Loss 4.3250(4.3677) | Error 0.4260(0.4440) Steps 670(671.33) | Grad Norm 0.6780(2.9291) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 25.7523, Epoch Time 397.8567(386.3471), Bit/dim 3.7350(best: 3.7368), Xent 1.1555, Loss 4.3128, Error 0.4170(best: 0.4172)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2485 | Time 59.0177(59.9327) | Bit/dim 3.7355(3.7479) | Xent 1.1945(1.2374) | Loss 4.3327(4.3666) | Error 0.4320(0.4436) Steps 676(671.47) | Grad Norm 0.4445(2.8546) | Total Time 14.00(14.00)\n",
      "Iter 2486 | Time 59.6471(59.9242) | Bit/dim 3.7328(3.7475) | Xent 1.2076(1.2365) | Loss 4.3366(4.3657) | Error 0.4266(0.4431) Steps 670(671.42) | Grad Norm 0.8693(2.7950) | Total Time 14.00(14.00)\n",
      "Iter 2487 | Time 59.9056(59.9236) | Bit/dim 3.7413(3.7473) | Xent 1.1880(1.2351) | Loss 4.3353(4.3648) | Error 0.4293(0.4427) Steps 670(671.38) | Grad Norm 0.3613(2.7220) | Total Time 14.00(14.00)\n",
      "Iter 2488 | Time 62.1171(59.9894) | Bit/dim 3.7338(3.7469) | Xent 1.1971(1.2339) | Loss 4.3323(4.3638) | Error 0.4357(0.4425) Steps 670(671.34) | Grad Norm 0.8522(2.6659) | Total Time 14.00(14.00)\n",
      "Iter 2489 | Time 58.1435(59.9340) | Bit/dim 3.7327(3.7465) | Xent 1.1871(1.2325) | Loss 4.3262(4.3627) | Error 0.4275(0.4420) Steps 664(671.12) | Grad Norm 0.6332(2.6050) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 58.2418(59.8833) | Bit/dim 3.7281(3.7459) | Xent 1.1808(1.2310) | Loss 4.3185(4.3614) | Error 0.4179(0.4413) Steps 670(671.09) | Grad Norm 0.6373(2.5459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 25.5455, Epoch Time 398.3981(386.7086), Bit/dim 3.7345(best: 3.7350), Xent 1.1512, Loss 4.3101, Error 0.4162(best: 0.4170)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2491 | Time 60.4152(59.8992) | Bit/dim 3.7378(3.7457) | Xent 1.1901(1.2297) | Loss 4.3329(4.3605) | Error 0.4269(0.4409) Steps 664(670.87) | Grad Norm 0.4278(2.4824) | Total Time 14.00(14.00)\n",
      "Iter 2492 | Time 58.2489(59.8497) | Bit/dim 3.7348(3.7453) | Xent 1.2074(1.2291) | Loss 4.3385(4.3599) | Error 0.4346(0.4407) Steps 670(670.85) | Grad Norm 0.4848(2.4224) | Total Time 14.00(14.00)\n",
      "Iter 2493 | Time 59.7374(59.8464) | Bit/dim 3.7289(3.7448) | Xent 1.2027(1.2283) | Loss 4.3302(4.3590) | Error 0.4319(0.4404) Steps 676(671.00) | Grad Norm 0.5338(2.3658) | Total Time 14.00(14.00)\n",
      "Iter 2494 | Time 57.9905(59.7907) | Bit/dim 3.7277(3.7443) | Xent 1.1777(1.2267) | Loss 4.3166(4.3577) | Error 0.4264(0.4400) Steps 658(670.61) | Grad Norm 0.5602(2.3116) | Total Time 14.00(14.00)\n",
      "Iter 2495 | Time 59.4576(59.7807) | Bit/dim 3.7298(3.7439) | Xent 1.1988(1.2259) | Loss 4.3292(4.3569) | Error 0.4274(0.4396) Steps 664(670.41) | Grad Norm 0.4677(2.2563) | Total Time 14.00(14.00)\n",
      "Iter 2496 | Time 62.5870(59.8649) | Bit/dim 3.7251(3.7433) | Xent 1.1991(1.2251) | Loss 4.3247(4.3559) | Error 0.4333(0.4394) Steps 670(670.40) | Grad Norm 0.7199(2.2102) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 25.3501, Epoch Time 399.2766(387.0857), Bit/dim 3.7326(best: 3.7345), Xent 1.1497, Loss 4.3075, Error 0.4154(best: 0.4162)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2497 | Time 58.0340(59.8100) | Bit/dim 3.7386(3.7432) | Xent 1.1784(1.2237) | Loss 4.3278(4.3550) | Error 0.4170(0.4388) Steps 676(670.57) | Grad Norm 0.3853(2.1555) | Total Time 14.00(14.00)\n",
      "Iter 2498 | Time 58.4334(59.7687) | Bit/dim 3.7247(3.7426) | Xent 1.1854(1.2226) | Loss 4.3174(4.3539) | Error 0.4271(0.4384) Steps 670(670.55) | Grad Norm 0.4251(2.1036) | Total Time 14.00(14.00)\n",
      "Iter 2499 | Time 59.9995(59.7756) | Bit/dim 3.7263(3.7421) | Xent 1.1961(1.2218) | Loss 4.3243(4.3530) | Error 0.4271(0.4381) Steps 670(670.53) | Grad Norm 0.4421(2.0537) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 63.2516(59.8799) | Bit/dim 3.7407(3.7421) | Xent 1.2060(1.2213) | Loss 4.3436(4.3527) | Error 0.4294(0.4378) Steps 682(670.88) | Grad Norm 0.7689(2.0152) | Total Time 14.00(14.00)\n",
      "Iter 2501 | Time 59.0670(59.8555) | Bit/dim 3.7223(3.7415) | Xent 1.2071(1.2209) | Loss 4.3258(4.3519) | Error 0.4321(0.4376) Steps 664(670.67) | Grad Norm 1.0644(1.9866) | Total Time 14.00(14.00)\n",
      "Iter 2502 | Time 60.2581(59.8676) | Bit/dim 3.7318(3.7412) | Xent 1.2045(1.2204) | Loss 4.3341(4.3514) | Error 0.4295(0.4374) Steps 670(670.65) | Grad Norm 0.5838(1.9446) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 25.1801, Epoch Time 400.3110(387.4824), Bit/dim 3.7321(best: 3.7326), Xent 1.1476, Loss 4.3059, Error 0.4163(best: 0.4154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2503 | Time 59.6042(59.8597) | Bit/dim 3.7329(3.7410) | Xent 1.1791(1.2191) | Loss 4.3224(4.3505) | Error 0.4211(0.4369) Steps 670(670.63) | Grad Norm 0.3923(1.8980) | Total Time 14.00(14.00)\n",
      "Iter 2504 | Time 57.7331(59.7959) | Bit/dim 3.7352(3.7408) | Xent 1.1879(1.2182) | Loss 4.3292(4.3499) | Error 0.4290(0.4367) Steps 670(670.61) | Grad Norm 0.4077(1.8533) | Total Time 14.00(14.00)\n",
      "Iter 2505 | Time 58.7064(59.7632) | Bit/dim 3.7223(3.7402) | Xent 1.1957(1.2175) | Loss 4.3202(4.3490) | Error 0.4315(0.4365) Steps 676(670.78) | Grad Norm 0.8526(1.8233) | Total Time 14.00(14.00)\n",
      "Iter 2506 | Time 61.9834(59.8298) | Bit/dim 3.7327(3.7400) | Xent 1.1807(1.2164) | Loss 4.3231(4.3482) | Error 0.4314(0.4364) Steps 664(670.57) | Grad Norm 0.5811(1.7860) | Total Time 14.00(14.00)\n",
      "Iter 2507 | Time 60.4653(59.8488) | Bit/dim 3.7322(3.7398) | Xent 1.1935(1.2157) | Loss 4.3289(4.3476) | Error 0.4284(0.4361) Steps 682(670.91) | Grad Norm 0.6491(1.7519) | Total Time 14.00(14.00)\n",
      "Iter 2508 | Time 61.1864(59.8890) | Bit/dim 3.7285(3.7394) | Xent 1.1860(1.2148) | Loss 4.3215(4.3469) | Error 0.4244(0.4358) Steps 670(670.89) | Grad Norm 0.7572(1.7221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0418 | Time 24.9668, Epoch Time 400.4179(387.8705), Bit/dim 3.7306(best: 3.7321), Xent 1.1444, Loss 4.3028, Error 0.4119(best: 0.4154)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2509 | Time 61.1541(59.9269) | Bit/dim 3.7304(3.7392) | Xent 1.1723(1.2136) | Loss 4.3165(4.3459) | Error 0.4147(0.4351) Steps 670(670.86) | Grad Norm 0.5484(1.6868) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 60.9069(59.9563) | Bit/dim 3.7266(3.7388) | Xent 1.1860(1.2127) | Loss 4.3197(4.3452) | Error 0.4317(0.4350) Steps 658(670.47) | Grad Norm 0.4899(1.6509) | Total Time 14.00(14.00)\n",
      "Iter 2511 | Time 61.2958(59.9965) | Bit/dim 3.7305(3.7385) | Xent 1.2080(1.2126) | Loss 4.3345(4.3448) | Error 0.4353(0.4350) Steps 664(670.28) | Grad Norm 0.6497(1.6209) | Total Time 14.00(14.00)\n",
      "Iter 2512 | Time 58.8142(59.9610) | Bit/dim 3.7291(3.7383) | Xent 1.1907(1.2119) | Loss 4.3244(4.3442) | Error 0.4237(0.4347) Steps 664(670.09) | Grad Norm 0.4531(1.5859) | Total Time 14.00(14.00)\n",
      "Iter 2513 | Time 58.2218(59.9089) | Bit/dim 3.7241(3.7378) | Xent 1.1916(1.2113) | Loss 4.3199(4.3435) | Error 0.4273(0.4345) Steps 664(669.91) | Grad Norm 0.4240(1.5510) | Total Time 14.00(14.00)\n",
      "Iter 2514 | Time 59.6926(59.9024) | Bit/dim 3.7320(3.7377) | Xent 1.1737(1.2102) | Loss 4.3188(4.3428) | Error 0.4216(0.4341) Steps 670(669.91) | Grad Norm 0.4475(1.5179) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0419 | Time 25.1798, Epoch Time 400.7771(388.2577), Bit/dim 3.7303(best: 3.7306), Xent 1.1414, Loss 4.3010, Error 0.4143(best: 0.4119)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2515 | Time 57.6397(59.8345) | Bit/dim 3.7284(3.7374) | Xent 1.1626(1.2088) | Loss 4.3097(4.3418) | Error 0.4170(0.4336) Steps 658(669.55) | Grad Norm 0.6943(1.4932) | Total Time 14.00(14.00)\n",
      "Iter 2516 | Time 60.5926(59.8572) | Bit/dim 3.7333(3.7373) | Xent 1.1687(1.2076) | Loss 4.3177(4.3410) | Error 0.4160(0.4331) Steps 676(669.75) | Grad Norm 0.4438(1.4617) | Total Time 14.00(14.00)\n",
      "Iter 2517 | Time 58.2919(59.8103) | Bit/dim 3.7235(3.7368) | Xent 1.1985(1.2073) | Loss 4.3228(4.3405) | Error 0.4295(0.4329) Steps 658(669.40) | Grad Norm 0.3807(1.4293) | Total Time 14.00(14.00)\n",
      "Iter 2518 | Time 60.3835(59.8275) | Bit/dim 3.7282(3.7366) | Xent 1.2025(1.2072) | Loss 4.3295(4.3402) | Error 0.4303(0.4329) Steps 664(669.23) | Grad Norm 0.4962(1.4013) | Total Time 14.00(14.00)\n",
      "Iter 2519 | Time 58.6330(59.7916) | Bit/dim 3.7435(3.7368) | Xent 1.1827(1.2064) | Loss 4.3349(4.3400) | Error 0.4226(0.4326) Steps 664(669.08) | Grad Norm 0.5451(1.3756) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 60.7329(59.8199) | Bit/dim 3.7131(3.7361) | Xent 1.1901(1.2059) | Loss 4.3082(4.3390) | Error 0.4290(0.4325) Steps 658(668.74) | Grad Norm 0.5636(1.3512) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0420 | Time 24.9839, Epoch Time 397.2440(388.5273), Bit/dim 3.7299(best: 3.7303), Xent 1.1398, Loss 4.2998, Error 0.4121(best: 0.4119)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2521 | Time 59.0652(59.7972) | Bit/dim 3.7345(3.7360) | Xent 1.1676(1.2048) | Loss 4.3183(4.3384) | Error 0.4180(0.4320) Steps 670(668.78) | Grad Norm 0.5281(1.3265) | Total Time 14.00(14.00)\n",
      "Iter 2522 | Time 59.9999(59.8033) | Bit/dim 3.7181(3.7355) | Xent 1.1991(1.2046) | Loss 4.3177(4.3378) | Error 0.4345(0.4321) Steps 658(668.46) | Grad Norm 0.6698(1.3068) | Total Time 14.00(14.00)\n",
      "Iter 2523 | Time 59.5404(59.7954) | Bit/dim 3.7181(3.7350) | Xent 1.1773(1.2038) | Loss 4.3068(4.3369) | Error 0.4220(0.4318) Steps 670(668.50) | Grad Norm 0.3969(1.2795) | Total Time 14.00(14.00)\n",
      "Iter 2524 | Time 59.2550(59.7792) | Bit/dim 3.7295(3.7348) | Xent 1.1734(1.2029) | Loss 4.3161(4.3362) | Error 0.4221(0.4315) Steps 664(668.37) | Grad Norm 0.5351(1.2572) | Total Time 14.00(14.00)\n",
      "Iter 2525 | Time 58.5768(59.7431) | Bit/dim 3.7290(3.7346) | Xent 1.1975(1.2027) | Loss 4.3277(4.3360) | Error 0.4325(0.4315) Steps 664(668.24) | Grad Norm 0.5806(1.2369) | Total Time 14.00(14.00)\n",
      "Iter 2526 | Time 60.2067(59.7571) | Bit/dim 3.7350(3.7346) | Xent 1.1705(1.2017) | Loss 4.3203(4.3355) | Error 0.4179(0.4311) Steps 664(668.11) | Grad Norm 0.8819(1.2263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0421 | Time 24.9591, Epoch Time 397.4594(388.7952), Bit/dim 3.7282(best: 3.7299), Xent 1.1354, Loss 4.2959, Error 0.4126(best: 0.4119)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2527 | Time 61.0702(59.7964) | Bit/dim 3.7366(3.7347) | Xent 1.1789(1.2011) | Loss 4.3261(4.3352) | Error 0.4209(0.4308) Steps 646(667.45) | Grad Norm 0.7593(1.2123) | Total Time 14.00(14.00)\n",
      "Iter 2528 | Time 60.3192(59.8121) | Bit/dim 3.7270(3.7345) | Xent 1.1875(1.2007) | Loss 4.3207(4.3348) | Error 0.4339(0.4309) Steps 658(667.16) | Grad Norm 0.4752(1.1901) | Total Time 14.00(14.00)\n",
      "Iter 2529 | Time 58.6274(59.7766) | Bit/dim 3.7214(3.7341) | Xent 1.1905(1.2004) | Loss 4.3167(4.3343) | Error 0.4231(0.4307) Steps 640(666.35) | Grad Norm 0.5810(1.1719) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 59.4903(59.7680) | Bit/dim 3.7240(3.7338) | Xent 1.1827(1.1998) | Loss 4.3154(4.3337) | Error 0.4235(0.4305) Steps 658(666.10) | Grad Norm 0.6046(1.1548) | Total Time 14.00(14.00)\n",
      "Iter 2531 | Time 58.7408(59.7372) | Bit/dim 3.7229(3.7335) | Xent 1.1978(1.1998) | Loss 4.3218(4.3333) | Error 0.4319(0.4305) Steps 658(665.86) | Grad Norm 0.5698(1.1373) | Total Time 14.00(14.00)\n",
      "Iter 2532 | Time 59.7176(59.7366) | Bit/dim 3.7242(3.7332) | Xent 1.1490(1.1982) | Loss 4.2987(4.3323) | Error 0.4083(0.4298) Steps 652(665.44) | Grad Norm 0.5726(1.1204) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0422 | Time 25.3674, Epoch Time 399.0004(389.1014), Bit/dim 3.7272(best: 3.7282), Xent 1.1339, Loss 4.2942, Error 0.4101(best: 0.4119)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2533 | Time 60.7632(59.7674) | Bit/dim 3.7249(3.7329) | Xent 1.1700(1.1974) | Loss 4.3099(4.3316) | Error 0.4224(0.4296) Steps 664(665.40) | Grad Norm 0.3710(1.0979) | Total Time 14.00(14.00)\n",
      "Iter 2534 | Time 58.7503(59.7369) | Bit/dim 3.7259(3.7327) | Xent 1.1855(1.1970) | Loss 4.3186(4.3312) | Error 0.4346(0.4298) Steps 670(665.54) | Grad Norm 0.5474(1.0814) | Total Time 14.00(14.00)\n",
      "Iter 2535 | Time 56.3996(59.6368) | Bit/dim 3.7302(3.7326) | Xent 1.1662(1.1961) | Loss 4.3133(4.3307) | Error 0.4161(0.4294) Steps 664(665.49) | Grad Norm 0.4810(1.0633) | Total Time 14.00(14.00)\n",
      "Iter 2536 | Time 59.6509(59.6372) | Bit/dim 3.7192(3.7322) | Xent 1.1862(1.1958) | Loss 4.3123(4.3301) | Error 0.4360(0.4295) Steps 658(665.26) | Grad Norm 0.8720(1.0576) | Total Time 14.00(14.00)\n",
      "Iter 2537 | Time 62.1067(59.7113) | Bit/dim 3.7309(3.7322) | Xent 1.1730(1.1951) | Loss 4.3174(4.3298) | Error 0.4183(0.4292) Steps 676(665.59) | Grad Norm 0.5932(1.0437) | Total Time 14.00(14.00)\n",
      "Iter 2538 | Time 60.2583(59.7277) | Bit/dim 3.7243(3.7320) | Xent 1.1551(1.1939) | Loss 4.3018(4.3289) | Error 0.4113(0.4287) Steps 646(665.00) | Grad Norm 0.5068(1.0276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0423 | Time 25.1024, Epoch Time 398.6853(389.3889), Bit/dim 3.7261(best: 3.7272), Xent 1.1317, Loss 4.2919, Error 0.4110(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2539 | Time 60.9288(59.7637) | Bit/dim 3.7334(3.7320) | Xent 1.1671(1.1931) | Loss 4.3170(4.3286) | Error 0.4179(0.4283) Steps 664(664.97) | Grad Norm 0.4357(1.0098) | Total Time 14.00(14.00)\n",
      "Iter 2540 | Time 58.2656(59.7188) | Bit/dim 3.7148(3.7315) | Xent 1.2030(1.1934) | Loss 4.3163(4.3282) | Error 0.4353(0.4286) Steps 658(664.76) | Grad Norm 0.3882(0.9912) | Total Time 14.00(14.00)\n",
      "Iter 2541 | Time 59.3488(59.7077) | Bit/dim 3.7237(3.7313) | Xent 1.1670(1.1926) | Loss 4.3073(4.3276) | Error 0.4117(0.4281) Steps 652(664.38) | Grad Norm 0.7190(0.9830) | Total Time 14.00(14.00)\n",
      "Iter 2542 | Time 58.2066(59.6626) | Bit/dim 3.7205(3.7309) | Xent 1.1709(1.1920) | Loss 4.3059(4.3269) | Error 0.4214(0.4279) Steps 664(664.37) | Grad Norm 0.7029(0.9746) | Total Time 14.00(14.00)\n",
      "Iter 2543 | Time 59.4509(59.6563) | Bit/dim 3.7342(3.7310) | Xent 1.1958(1.1921) | Loss 4.3321(4.3271) | Error 0.4301(0.4279) Steps 670(664.54) | Grad Norm 0.5560(0.9620) | Total Time 14.00(14.00)\n",
      "Iter 2544 | Time 59.2797(59.6450) | Bit/dim 3.7245(3.7308) | Xent 1.1681(1.1914) | Loss 4.3086(4.3265) | Error 0.4224(0.4278) Steps 658(664.34) | Grad Norm 0.4893(0.9479) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0424 | Time 25.0497, Epoch Time 396.1628(389.5921), Bit/dim 3.7263(best: 3.7261), Xent 1.1293, Loss 4.2910, Error 0.4080(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2545 | Time 57.2203(59.5723) | Bit/dim 3.7254(3.7307) | Xent 1.1828(1.1911) | Loss 4.3168(4.3262) | Error 0.4177(0.4275) Steps 658(664.15) | Grad Norm 0.3590(0.9302) | Total Time 14.00(14.00)\n",
      "Iter 2546 | Time 64.0457(59.7065) | Bit/dim 3.7227(3.7304) | Xent 1.1620(1.1902) | Loss 4.3036(4.3256) | Error 0.4224(0.4273) Steps 658(663.96) | Grad Norm 0.5291(0.9182) | Total Time 14.00(14.00)\n",
      "Iter 2547 | Time 59.7661(59.7082) | Bit/dim 3.7283(3.7304) | Xent 1.1666(1.1895) | Loss 4.3116(4.3251) | Error 0.4205(0.4271) Steps 658(663.79) | Grad Norm 0.4521(0.9042) | Total Time 14.00(14.00)\n",
      "Iter 2548 | Time 61.0009(59.7470) | Bit/dim 3.7288(3.7303) | Xent 1.1931(1.1896) | Loss 4.3254(4.3251) | Error 0.4253(0.4270) Steps 664(663.79) | Grad Norm 0.6477(0.8965) | Total Time 14.00(14.00)\n",
      "Iter 2549 | Time 61.2922(59.7934) | Bit/dim 3.7262(3.7302) | Xent 1.1766(1.1892) | Loss 4.3145(4.3248) | Error 0.4209(0.4269) Steps 664(663.80) | Grad Norm 0.5277(0.8854) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 61.9910(59.8593) | Bit/dim 3.7122(3.7297) | Xent 1.1725(1.1887) | Loss 4.2984(4.3240) | Error 0.4194(0.4266) Steps 670(663.98) | Grad Norm 0.4319(0.8718) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0425 | Time 24.9859, Epoch Time 406.0404(390.0856), Bit/dim 3.7249(best: 3.7261), Xent 1.1285, Loss 4.2892, Error 0.4098(best: 0.4080)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2551 | Time 61.7533(59.9161) | Bit/dim 3.7207(3.7294) | Xent 1.1770(1.1884) | Loss 4.3092(4.3236) | Error 0.4205(0.4264) Steps 646(663.44) | Grad Norm 0.8695(0.8717) | Total Time 14.00(14.00)\n",
      "Iter 2552 | Time 58.7055(59.8798) | Bit/dim 3.7196(3.7291) | Xent 1.1692(1.1878) | Loss 4.3042(4.3230) | Error 0.4207(0.4263) Steps 640(662.74) | Grad Norm 0.6361(0.8647) | Total Time 14.00(14.00)\n",
      "Iter 2553 | Time 58.7175(59.8449) | Bit/dim 3.7303(3.7291) | Xent 1.1568(1.1869) | Loss 4.3087(4.3226) | Error 0.4180(0.4260) Steps 670(662.96) | Grad Norm 0.5340(0.8548) | Total Time 14.00(14.00)\n",
      "Iter 2554 | Time 60.2438(59.8569) | Bit/dim 3.7246(3.7290) | Xent 1.1764(1.1866) | Loss 4.3128(4.3223) | Error 0.4215(0.4259) Steps 664(662.99) | Grad Norm 0.7268(0.8509) | Total Time 14.00(14.00)\n",
      "Iter 2555 | Time 60.3768(59.8725) | Bit/dim 3.7267(3.7289) | Xent 1.1604(1.1858) | Loss 4.3069(4.3218) | Error 0.4200(0.4257) Steps 658(662.84) | Grad Norm 0.8692(0.8515) | Total Time 14.00(14.00)\n",
      "Iter 2556 | Time 58.3127(59.8257) | Bit/dim 3.7180(3.7286) | Xent 1.1780(1.1856) | Loss 4.3070(4.3214) | Error 0.4225(0.4256) Steps 658(662.70) | Grad Norm 0.7098(0.8472) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0426 | Time 25.1394, Epoch Time 398.9536(390.3516), Bit/dim 3.7241(best: 3.7249), Xent 1.1269, Loss 4.2876, Error 0.4039(best: 0.4080)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2557 | Time 60.5506(59.8475) | Bit/dim 3.7168(3.7282) | Xent 1.1658(1.1850) | Loss 4.2997(4.3207) | Error 0.4203(0.4255) Steps 670(662.91) | Grad Norm 0.3638(0.8327) | Total Time 14.00(14.00)\n",
      "Iter 2558 | Time 60.5590(59.8688) | Bit/dim 3.7206(3.7280) | Xent 1.1635(1.1843) | Loss 4.3023(4.3202) | Error 0.4159(0.4252) Steps 658(662.77) | Grad Norm 0.4269(0.8205) | Total Time 14.00(14.00)\n",
      "Iter 2559 | Time 58.6820(59.8332) | Bit/dim 3.7195(3.7278) | Xent 1.1735(1.1840) | Loss 4.3063(4.3198) | Error 0.4209(0.4250) Steps 658(662.62) | Grad Norm 0.6766(0.8162) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 61.2838(59.8767) | Bit/dim 3.7259(3.7277) | Xent 1.1665(1.1835) | Loss 4.3091(4.3194) | Error 0.4169(0.4248) Steps 658(662.49) | Grad Norm 0.8922(0.8185) | Total Time 14.00(14.00)\n",
      "Iter 2561 | Time 59.3948(59.8623) | Bit/dim 3.7277(3.7277) | Xent 1.1684(1.1830) | Loss 4.3119(4.3192) | Error 0.4231(0.4247) Steps 646(661.99) | Grad Norm 0.6465(0.8133) | Total Time 14.00(14.00)\n",
      "Iter 2562 | Time 59.9902(59.8661) | Bit/dim 3.7229(3.7276) | Xent 1.1615(1.1824) | Loss 4.3037(4.3188) | Error 0.4100(0.4243) Steps 658(661.87) | Grad Norm 0.5329(0.8049) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0427 | Time 25.2341, Epoch Time 401.6491(390.6905), Bit/dim 3.7232(best: 3.7241), Xent 1.1232, Loss 4.2847, Error 0.4077(best: 0.4039)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2563 | Time 56.5423(59.7664) | Bit/dim 3.7260(3.7275) | Xent 1.1717(1.1821) | Loss 4.3118(4.3185) | Error 0.4164(0.4241) Steps 664(661.93) | Grad Norm 0.6164(0.7993) | Total Time 14.00(14.00)\n",
      "Iter 2564 | Time 58.0831(59.7159) | Bit/dim 3.7136(3.7271) | Xent 1.1611(1.1814) | Loss 4.2942(4.3178) | Error 0.4165(0.4238) Steps 664(662.00) | Grad Norm 0.5281(0.7911) | Total Time 14.00(14.00)\n",
      "Iter 2565 | Time 60.6301(59.7433) | Bit/dim 3.7259(3.7271) | Xent 1.1732(1.1812) | Loss 4.3126(4.3177) | Error 0.4217(0.4238) Steps 664(662.06) | Grad Norm 0.5191(0.7830) | Total Time 14.00(14.00)\n",
      "Iter 2566 | Time 58.4867(59.7056) | Bit/dim 3.7244(3.7270) | Xent 1.1576(1.1805) | Loss 4.3032(4.3172) | Error 0.4159(0.4235) Steps 658(661.94) | Grad Norm 0.7911(0.7832) | Total Time 14.00(14.00)\n",
      "Iter 2567 | Time 61.1355(59.7485) | Bit/dim 3.7222(3.7268) | Xent 1.1600(1.1799) | Loss 4.3022(4.3168) | Error 0.4187(0.4234) Steps 664(662.00) | Grad Norm 0.6005(0.7777) | Total Time 14.00(14.00)\n",
      "Iter 2568 | Time 60.7612(59.7789) | Bit/dim 3.7143(3.7265) | Xent 1.1529(1.1790) | Loss 4.2908(4.3160) | Error 0.4096(0.4230) Steps 640(661.34) | Grad Norm 0.4912(0.7691) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0428 | Time 25.1782, Epoch Time 396.4677(390.8639), Bit/dim 3.7229(best: 3.7232), Xent 1.1217, Loss 4.2837, Error 0.4056(best: 0.4039)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2569 | Time 60.3317(59.7955) | Bit/dim 3.7138(3.7261) | Xent 1.1704(1.1788) | Loss 4.2990(4.3155) | Error 0.4217(0.4229) Steps 658(661.24) | Grad Norm 0.4181(0.7586) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 59.7616(59.7945) | Bit/dim 3.7254(3.7261) | Xent 1.1744(1.1787) | Loss 4.3126(4.3154) | Error 0.4229(0.4229) Steps 664(661.32) | Grad Norm 0.4309(0.7488) | Total Time 14.00(14.00)\n",
      "Iter 2571 | Time 58.7408(59.7628) | Bit/dim 3.7099(3.7256) | Xent 1.1592(1.1781) | Loss 4.2895(4.3146) | Error 0.4140(0.4227) Steps 640(660.68) | Grad Norm 0.6227(0.7450) | Total Time 14.00(14.00)\n",
      "Iter 2572 | Time 59.7762(59.7632) | Bit/dim 3.7200(3.7254) | Xent 1.1696(1.1778) | Loss 4.3048(4.3143) | Error 0.4273(0.4228) Steps 664(660.78) | Grad Norm 0.7514(0.7452) | Total Time 14.00(14.00)\n",
      "Iter 2573 | Time 59.1549(59.7450) | Bit/dim 3.7252(3.7254) | Xent 1.1470(1.1769) | Loss 4.2987(4.3139) | Error 0.4106(0.4224) Steps 646(660.34) | Grad Norm 0.5598(0.7396) | Total Time 14.00(14.00)\n",
      "Iter 2574 | Time 58.6531(59.7122) | Bit/dim 3.7294(3.7255) | Xent 1.1619(1.1764) | Loss 4.3103(4.3137) | Error 0.4121(0.4221) Steps 640(659.73) | Grad Norm 0.4516(0.7310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0429 | Time 25.3606, Epoch Time 397.5612(391.0648), Bit/dim 3.7210(best: 3.7229), Xent 1.1199, Loss 4.2810, Error 0.4033(best: 0.4039)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2575 | Time 60.6777(59.7412) | Bit/dim 3.7274(3.7256) | Xent 1.1843(1.1767) | Loss 4.3196(4.3139) | Error 0.4267(0.4223) Steps 646(659.31) | Grad Norm 0.6235(0.7278) | Total Time 14.00(14.00)\n",
      "Iter 2576 | Time 61.9147(59.8064) | Bit/dim 3.7126(3.7252) | Xent 1.1548(1.1760) | Loss 4.2900(4.3132) | Error 0.4199(0.4222) Steps 640(658.74) | Grad Norm 0.3964(0.7178) | Total Time 14.00(14.00)\n",
      "Iter 2577 | Time 60.5966(59.8301) | Bit/dim 3.7080(3.7247) | Xent 1.1513(1.1753) | Loss 4.2836(4.3123) | Error 0.4089(0.4218) Steps 664(658.89) | Grad Norm 0.7745(0.7195) | Total Time 14.00(14.00)\n",
      "Iter 2578 | Time 59.7929(59.8290) | Bit/dim 3.7215(3.7246) | Xent 1.1655(1.1750) | Loss 4.3042(4.3121) | Error 0.4170(0.4217) Steps 658(658.87) | Grad Norm 0.3961(0.7098) | Total Time 14.00(14.00)\n",
      "Iter 2579 | Time 61.8521(59.8897) | Bit/dim 3.7272(3.7247) | Xent 1.1591(1.1745) | Loss 4.3068(4.3119) | Error 0.4157(0.4215) Steps 658(658.84) | Grad Norm 0.7748(0.7118) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 58.2730(59.8412) | Bit/dim 3.7230(3.7246) | Xent 1.1661(1.1743) | Loss 4.3061(4.3117) | Error 0.4281(0.4217) Steps 664(659.00) | Grad Norm 0.6974(0.7113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0430 | Time 25.1265, Epoch Time 403.7857(391.4464), Bit/dim 3.7214(best: 3.7210), Xent 1.1162, Loss 4.2795, Error 0.4016(best: 0.4033)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2581 | Time 61.7161(59.8974) | Bit/dim 3.7151(3.7243) | Xent 1.1486(1.1735) | Loss 4.2894(4.3111) | Error 0.4140(0.4215) Steps 664(659.15) | Grad Norm 0.5149(0.7054) | Total Time 14.00(14.00)\n",
      "Iter 2582 | Time 60.2847(59.9091) | Bit/dim 3.7240(3.7243) | Xent 1.1588(1.1731) | Loss 4.3034(4.3108) | Error 0.4135(0.4212) Steps 646(658.75) | Grad Norm 0.4049(0.6964) | Total Time 14.00(14.00)\n",
      "Iter 2583 | Time 59.4457(59.8952) | Bit/dim 3.7119(3.7239) | Xent 1.1439(1.1722) | Loss 4.2839(4.3100) | Error 0.4127(0.4210) Steps 646(658.37) | Grad Norm 0.7119(0.6969) | Total Time 14.00(14.00)\n",
      "Iter 2584 | Time 59.2910(59.8770) | Bit/dim 3.7129(3.7236) | Xent 1.1768(1.1723) | Loss 4.3013(4.3098) | Error 0.4294(0.4212) Steps 664(658.54) | Grad Norm 0.7598(0.6988) | Total Time 14.00(14.00)\n",
      "Iter 2585 | Time 57.4176(59.8032) | Bit/dim 3.7238(3.7236) | Xent 1.1752(1.1724) | Loss 4.3114(4.3098) | Error 0.4223(0.4212) Steps 664(658.70) | Grad Norm 0.9537(0.7064) | Total Time 14.00(14.00)\n",
      "Iter 2586 | Time 59.5856(59.7967) | Bit/dim 3.7228(3.7236) | Xent 1.1454(1.1716) | Loss 4.2955(4.3094) | Error 0.4109(0.4209) Steps 664(658.86) | Grad Norm 0.4510(0.6988) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0431 | Time 25.0399, Epoch Time 398.3790(391.6544), Bit/dim 3.7201(best: 3.7210), Xent 1.1162, Loss 4.2782, Error 0.4004(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2587 | Time 58.6287(59.7617) | Bit/dim 3.7243(3.7236) | Xent 1.1563(1.1711) | Loss 4.3024(4.3092) | Error 0.4157(0.4208) Steps 646(658.47) | Grad Norm 0.5182(0.6933) | Total Time 14.00(14.00)\n",
      "Iter 2588 | Time 59.8740(59.7650) | Bit/dim 3.7196(3.7235) | Xent 1.1639(1.1709) | Loss 4.3016(4.3090) | Error 0.4204(0.4208) Steps 658(658.46) | Grad Norm 1.0704(0.7047) | Total Time 14.00(14.00)\n",
      "Iter 2589 | Time 60.6467(59.7915) | Bit/dim 3.7096(3.7231) | Xent 1.1640(1.1707) | Loss 4.2917(4.3084) | Error 0.4233(0.4208) Steps 664(658.63) | Grad Norm 0.5723(0.7007) | Total Time 14.00(14.00)\n",
      "Iter 2590 | Time 60.3252(59.8075) | Bit/dim 3.7115(3.7227) | Xent 1.1380(1.1697) | Loss 4.2804(4.3076) | Error 0.4117(0.4206) Steps 664(658.79) | Grad Norm 0.6580(0.6994) | Total Time 14.00(14.00)\n",
      "Iter 2591 | Time 59.9126(59.8107) | Bit/dim 3.7079(3.7223) | Xent 1.1509(1.1692) | Loss 4.2833(4.3069) | Error 0.4069(0.4202) Steps 658(658.76) | Grad Norm 0.5176(0.6940) | Total Time 14.00(14.00)\n",
      "Iter 2592 | Time 59.5128(59.8017) | Bit/dim 3.7348(3.7227) | Xent 1.1731(1.1693) | Loss 4.3214(4.3073) | Error 0.4215(0.4202) Steps 670(659.10) | Grad Norm 0.4306(0.6861) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0432 | Time 25.2263, Epoch Time 399.5886(391.8924), Bit/dim 3.7190(best: 3.7201), Xent 1.1160, Loss 4.2770, Error 0.4018(best: 0.4004)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2593 | Time 59.8791(59.8040) | Bit/dim 3.7228(3.7227) | Xent 1.1536(1.1688) | Loss 4.2996(4.3071) | Error 0.4155(0.4201) Steps 640(658.53) | Grad Norm 0.5486(0.6819) | Total Time 14.00(14.00)\n",
      "Iter 2594 | Time 60.0203(59.8105) | Bit/dim 3.7174(3.7225) | Xent 1.1455(1.1681) | Loss 4.2902(4.3066) | Error 0.4124(0.4198) Steps 646(658.15) | Grad Norm 0.5635(0.6784) | Total Time 14.00(14.00)\n",
      "Iter 2595 | Time 60.2527(59.8238) | Bit/dim 3.7156(3.7223) | Xent 1.1507(1.1676) | Loss 4.2909(4.3061) | Error 0.4060(0.4194) Steps 646(657.79) | Grad Norm 0.5297(0.6739) | Total Time 14.00(14.00)\n",
      "Iter 2596 | Time 61.1742(59.8643) | Bit/dim 3.7143(3.7221) | Xent 1.1690(1.1676) | Loss 4.2988(4.3059) | Error 0.4187(0.4194) Steps 664(657.97) | Grad Norm 0.3856(0.6653) | Total Time 14.00(14.00)\n",
      "Iter 2597 | Time 61.4669(59.9124) | Bit/dim 3.7205(3.7220) | Xent 1.1389(1.1668) | Loss 4.2899(4.3054) | Error 0.4104(0.4191) Steps 646(657.61) | Grad Norm 0.4486(0.6588) | Total Time 14.00(14.00)\n",
      "Iter 2598 | Time 61.3100(59.9543) | Bit/dim 3.7096(3.7216) | Xent 1.1766(1.1671) | Loss 4.2980(4.3052) | Error 0.4199(0.4191) Steps 658(657.63) | Grad Norm 1.0921(0.6718) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0433 | Time 24.6585, Epoch Time 404.2673(392.2637), Bit/dim 3.7190(best: 3.7190), Xent 1.1121, Loss 4.2751, Error 0.4022(best: 0.4004)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2599 | Time 61.5350(60.0017) | Bit/dim 3.7198(3.7216) | Xent 1.1610(1.1669) | Loss 4.3003(4.3050) | Error 0.4189(0.4191) Steps 670(658.00) | Grad Norm 0.6307(0.6705) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 61.4355(60.0447) | Bit/dim 3.7172(3.7215) | Xent 1.1429(1.1662) | Loss 4.2887(4.3045) | Error 0.4083(0.4188) Steps 652(657.82) | Grad Norm 0.4145(0.6629) | Total Time 14.00(14.00)\n",
      "Iter 2601 | Time 57.6361(59.9725) | Bit/dim 3.7162(3.7213) | Xent 1.1550(1.1658) | Loss 4.2937(4.3042) | Error 0.4109(0.4186) Steps 640(657.28) | Grad Norm 0.4823(0.6574) | Total Time 14.00(14.00)\n",
      "Iter 2602 | Time 62.4208(60.0459) | Bit/dim 3.7217(3.7213) | Xent 1.1604(1.1657) | Loss 4.3019(4.3041) | Error 0.4104(0.4183) Steps 652(657.12) | Grad Norm 0.6014(0.6558) | Total Time 14.00(14.00)\n",
      "Iter 2603 | Time 58.3642(59.9955) | Bit/dim 3.7201(3.7213) | Xent 1.1636(1.1656) | Loss 4.3019(4.3041) | Error 0.4156(0.4182) Steps 646(656.79) | Grad Norm 1.4836(0.6806) | Total Time 14.00(14.00)\n",
      "Iter 2604 | Time 59.1655(59.9706) | Bit/dim 3.7076(3.7209) | Xent 1.1505(1.1651) | Loss 4.2829(4.3034) | Error 0.4177(0.4182) Steps 658(656.83) | Grad Norm 1.0885(0.6928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0434 | Time 24.8609, Epoch Time 400.9675(392.5248), Bit/dim 3.7177(best: 3.7190), Xent 1.1110, Loss 4.2732, Error 0.4005(best: 0.4004)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2605 | Time 60.7178(59.9930) | Bit/dim 3.7214(3.7209) | Xent 1.1504(1.1647) | Loss 4.2966(4.3032) | Error 0.4124(0.4181) Steps 646(656.50) | Grad Norm 0.5272(0.6879) | Total Time 14.00(14.00)\n",
      "Iter 2606 | Time 57.8859(59.9298) | Bit/dim 3.7065(3.7205) | Xent 1.1436(1.1641) | Loss 4.2782(4.3025) | Error 0.4105(0.4178) Steps 646(656.19) | Grad Norm 0.7001(0.6882) | Total Time 14.00(14.00)\n",
      "Iter 2607 | Time 60.7339(59.9539) | Bit/dim 3.7143(3.7203) | Xent 1.1560(1.1638) | Loss 4.2923(4.3022) | Error 0.4141(0.4177) Steps 646(655.88) | Grad Norm 1.3276(0.7074) | Total Time 14.00(14.00)\n",
      "Iter 2608 | Time 58.1366(59.8994) | Bit/dim 3.7185(3.7202) | Xent 1.1481(1.1634) | Loss 4.2925(4.3019) | Error 0.4060(0.4174) Steps 640(655.41) | Grad Norm 1.3403(0.7264) | Total Time 14.00(14.00)\n",
      "Iter 2609 | Time 59.0308(59.8733) | Bit/dim 3.7204(3.7202) | Xent 1.1544(1.1631) | Loss 4.2976(4.3018) | Error 0.4203(0.4174) Steps 646(655.12) | Grad Norm 0.4883(0.7192) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 58.3394(59.8273) | Bit/dim 3.7156(3.7201) | Xent 1.1554(1.1629) | Loss 4.2933(4.3015) | Error 0.4155(0.4174) Steps 664(655.39) | Grad Norm 0.6738(0.7179) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0435 | Time 25.0445, Epoch Time 395.2524(392.6066), Bit/dim 3.7181(best: 3.7177), Xent 1.1112, Loss 4.2737, Error 0.4008(best: 0.4004)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2611 | Time 59.4555(59.8162) | Bit/dim 3.7026(3.7196) | Xent 1.1444(1.1623) | Loss 4.2748(4.3007) | Error 0.4124(0.4172) Steps 652(655.29) | Grad Norm 0.6215(0.7150) | Total Time 14.00(14.00)\n",
      "Iter 2612 | Time 59.0033(59.7918) | Bit/dim 3.7144(3.7194) | Xent 1.1549(1.1621) | Loss 4.2918(4.3004) | Error 0.4115(0.4171) Steps 646(655.01) | Grad Norm 0.5611(0.7104) | Total Time 14.00(14.00)\n",
      "Iter 2613 | Time 58.6836(59.7585) | Bit/dim 3.7114(3.7192) | Xent 1.1498(1.1617) | Loss 4.2863(4.3000) | Error 0.4189(0.4171) Steps 664(655.28) | Grad Norm 0.5377(0.7052) | Total Time 14.00(14.00)\n",
      "Iter 2614 | Time 59.0314(59.7367) | Bit/dim 3.7265(3.7194) | Xent 1.1548(1.1615) | Loss 4.3039(4.3001) | Error 0.4151(0.4171) Steps 664(655.54) | Grad Norm 0.6436(0.7033) | Total Time 14.00(14.00)\n",
      "Iter 2615 | Time 59.7071(59.7358) | Bit/dim 3.7211(3.7194) | Xent 1.1509(1.1612) | Loss 4.2965(4.3000) | Error 0.4181(0.4171) Steps 658(655.61) | Grad Norm 0.4049(0.6944) | Total Time 14.00(14.00)\n",
      "Iter 2616 | Time 59.8303(59.7387) | Bit/dim 3.7183(3.7194) | Xent 1.1283(1.1602) | Loss 4.2824(4.2995) | Error 0.4000(0.4166) Steps 646(655.33) | Grad Norm 0.4415(0.6868) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0436 | Time 24.9349, Epoch Time 396.4806(392.7228), Bit/dim 3.7164(best: 3.7177), Xent 1.1069, Loss 4.2699, Error 0.3941(best: 0.4004)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2617 | Time 62.6216(59.8251) | Bit/dim 3.7161(3.7193) | Xent 1.1598(1.1602) | Loss 4.2960(4.2994) | Error 0.4217(0.4167) Steps 634(654.69) | Grad Norm 0.5866(0.6838) | Total Time 14.00(14.00)\n",
      "Iter 2618 | Time 59.3398(59.8106) | Bit/dim 3.7111(3.7190) | Xent 1.1427(1.1597) | Loss 4.2824(4.2989) | Error 0.4034(0.4163) Steps 652(654.61) | Grad Norm 0.6926(0.6841) | Total Time 14.00(14.00)\n",
      "Iter 2619 | Time 58.6879(59.7769) | Bit/dim 3.7251(3.7192) | Xent 1.1503(1.1594) | Loss 4.3003(4.2989) | Error 0.4125(0.4162) Steps 640(654.17) | Grad Norm 0.4585(0.6773) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 60.8576(59.8093) | Bit/dim 3.7103(3.7190) | Xent 1.1356(1.1587) | Loss 4.2780(4.2983) | Error 0.4039(0.4159) Steps 646(653.92) | Grad Norm 0.5840(0.6745) | Total Time 14.00(14.00)\n",
      "Iter 2621 | Time 60.4058(59.8272) | Bit/dim 3.7196(3.7190) | Xent 1.1497(1.1584) | Loss 4.2944(4.2982) | Error 0.4121(0.4157) Steps 652(653.86) | Grad Norm 0.6257(0.6730) | Total Time 14.00(14.00)\n",
      "Iter 2622 | Time 59.4578(59.8161) | Bit/dim 3.7135(3.7188) | Xent 1.1424(1.1579) | Loss 4.2847(4.2978) | Error 0.4147(0.4157) Steps 652(653.81) | Grad Norm 0.7038(0.6740) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0437 | Time 24.8433, Epoch Time 401.8376(392.9963), Bit/dim 3.7154(best: 3.7164), Xent 1.1060, Loss 4.2683, Error 0.3970(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2623 | Time 58.3954(59.7735) | Bit/dim 3.7190(3.7188) | Xent 1.1459(1.1576) | Loss 4.2919(4.2976) | Error 0.4074(0.4155) Steps 646(653.57) | Grad Norm 0.4582(0.6675) | Total Time 14.00(14.00)\n",
      "Iter 2624 | Time 59.6550(59.7700) | Bit/dim 3.7155(3.7187) | Xent 1.1427(1.1571) | Loss 4.2868(4.2973) | Error 0.4069(0.4152) Steps 664(653.89) | Grad Norm 0.6108(0.6658) | Total Time 14.00(14.00)\n",
      "Iter 2625 | Time 61.6730(59.8271) | Bit/dim 3.7100(3.7185) | Xent 1.1413(1.1566) | Loss 4.2806(4.2968) | Error 0.4113(0.4151) Steps 658(654.01) | Grad Norm 1.0268(0.6766) | Total Time 14.00(14.00)\n",
      "Iter 2626 | Time 59.8301(59.8271) | Bit/dim 3.7269(3.7187) | Xent 1.1427(1.1562) | Loss 4.2983(4.2968) | Error 0.4079(0.4149) Steps 652(653.95) | Grad Norm 0.4810(0.6707) | Total Time 14.00(14.00)\n",
      "Iter 2627 | Time 58.5634(59.7892) | Bit/dim 3.7139(3.7186) | Xent 1.1376(1.1557) | Loss 4.2827(4.2964) | Error 0.4055(0.4146) Steps 652(653.89) | Grad Norm 0.5575(0.6674) | Total Time 14.00(14.00)\n",
      "Iter 2628 | Time 60.1657(59.8005) | Bit/dim 3.7027(3.7181) | Xent 1.1480(1.1554) | Loss 4.2767(4.2958) | Error 0.4042(0.4143) Steps 664(654.19) | Grad Norm 0.5766(0.6646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0438 | Time 25.0007, Epoch Time 398.9787(393.1757), Bit/dim 3.7151(best: 3.7154), Xent 1.1049, Loss 4.2676, Error 0.3962(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2629 | Time 58.0425(59.7478) | Bit/dim 3.7120(3.7179) | Xent 1.1413(1.1550) | Loss 4.2826(4.2954) | Error 0.4135(0.4143) Steps 646(653.95) | Grad Norm 0.4293(0.6576) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 59.0220(59.7260) | Bit/dim 3.7205(3.7180) | Xent 1.1304(1.1543) | Loss 4.2857(4.2951) | Error 0.3972(0.4137) Steps 640(653.53) | Grad Norm 0.6146(0.6563) | Total Time 14.00(14.00)\n",
      "Iter 2631 | Time 57.6030(59.6623) | Bit/dim 3.7159(3.7179) | Xent 1.1455(1.1540) | Loss 4.2887(4.2949) | Error 0.4054(0.4135) Steps 646(653.30) | Grad Norm 0.3491(0.6471) | Total Time 14.00(14.00)\n",
      "Iter 2632 | Time 61.9100(59.7297) | Bit/dim 3.7155(3.7179) | Xent 1.1537(1.1540) | Loss 4.2924(4.2948) | Error 0.4177(0.4136) Steps 664(653.63) | Grad Norm 0.5320(0.6436) | Total Time 14.00(14.00)\n",
      "Iter 2633 | Time 59.1138(59.7113) | Bit/dim 3.7062(3.7175) | Xent 1.1474(1.1538) | Loss 4.2799(4.2944) | Error 0.4106(0.4135) Steps 658(653.76) | Grad Norm 0.4821(0.6388) | Total Time 14.00(14.00)\n",
      "Iter 2634 | Time 60.2687(59.7280) | Bit/dim 3.7124(3.7173) | Xent 1.1333(1.1532) | Loss 4.2791(4.2939) | Error 0.4016(0.4132) Steps 664(654.06) | Grad Norm 0.5786(0.6370) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0439 | Time 24.8432, Epoch Time 396.2489(393.2679), Bit/dim 3.7146(best: 3.7151), Xent 1.1028, Loss 4.2660, Error 0.3973(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2635 | Time 58.1122(59.6795) | Bit/dim 3.7173(3.7173) | Xent 1.1374(1.1527) | Loss 4.2859(4.2937) | Error 0.4051(0.4129) Steps 634(653.46) | Grad Norm 0.4024(0.6299) | Total Time 14.00(14.00)\n",
      "Iter 2636 | Time 56.8698(59.5952) | Bit/dim 3.7132(3.7172) | Xent 1.1620(1.1530) | Loss 4.2942(4.2937) | Error 0.4191(0.4131) Steps 640(653.06) | Grad Norm 0.6810(0.6315) | Total Time 14.00(14.00)\n",
      "Iter 2637 | Time 57.2744(59.5256) | Bit/dim 3.7027(3.7168) | Xent 1.1369(1.1525) | Loss 4.2712(4.2930) | Error 0.4035(0.4128) Steps 664(653.39) | Grad Norm 0.4352(0.6256) | Total Time 14.00(14.00)\n",
      "Iter 2638 | Time 61.5216(59.5855) | Bit/dim 3.7164(3.7168) | Xent 1.1329(1.1519) | Loss 4.2829(4.2927) | Error 0.4091(0.4127) Steps 664(653.70) | Grad Norm 0.4814(0.6212) | Total Time 14.00(14.00)\n",
      "Iter 2639 | Time 62.7067(59.6791) | Bit/dim 3.7074(3.7165) | Xent 1.1161(1.1508) | Loss 4.2655(4.2919) | Error 0.3981(0.4123) Steps 664(654.01) | Grad Norm 0.9018(0.6297) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 60.3879(59.7004) | Bit/dim 3.7244(3.7167) | Xent 1.1497(1.1508) | Loss 4.2992(4.2921) | Error 0.4144(0.4123) Steps 652(653.95) | Grad Norm 0.7985(0.6347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0440 | Time 25.2162, Epoch Time 397.6279(393.3987), Bit/dim 3.7129(best: 3.7146), Xent 1.1006, Loss 4.2633, Error 0.3949(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2641 | Time 61.1094(59.7427) | Bit/dim 3.7068(3.7164) | Xent 1.1287(1.1501) | Loss 4.2712(4.2915) | Error 0.4048(0.4121) Steps 652(653.89) | Grad Norm 0.8526(0.6413) | Total Time 14.00(14.00)\n",
      "Iter 2642 | Time 58.1248(59.6941) | Bit/dim 3.7123(3.7163) | Xent 1.1282(1.1495) | Loss 4.2764(4.2911) | Error 0.4073(0.4120) Steps 646(653.66) | Grad Norm 0.4792(0.6364) | Total Time 14.00(14.00)\n",
      "Iter 2643 | Time 60.1994(59.7093) | Bit/dim 3.7240(3.7165) | Xent 1.1371(1.1491) | Loss 4.2926(4.2911) | Error 0.4071(0.4118) Steps 646(653.43) | Grad Norm 0.4354(0.6304) | Total Time 14.00(14.00)\n",
      "Iter 2644 | Time 60.1717(59.7232) | Bit/dim 3.7143(3.7165) | Xent 1.1389(1.1488) | Loss 4.2837(4.2909) | Error 0.4048(0.4116) Steps 658(653.57) | Grad Norm 0.5798(0.6289) | Total Time 14.00(14.00)\n",
      "Iter 2645 | Time 57.5968(59.6594) | Bit/dim 3.6974(3.7159) | Xent 1.1428(1.1486) | Loss 4.2688(4.2902) | Error 0.4060(0.4114) Steps 664(653.88) | Grad Norm 0.3708(0.6211) | Total Time 14.00(14.00)\n",
      "Iter 2646 | Time 60.0530(59.6712) | Bit/dim 3.7160(3.7159) | Xent 1.1245(1.1479) | Loss 4.2782(4.2899) | Error 0.4022(0.4112) Steps 652(653.82) | Grad Norm 0.4661(0.6165) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0441 | Time 24.9066, Epoch Time 397.8145(393.5312), Bit/dim 3.7135(best: 3.7129), Xent 1.0983, Loss 4.2626, Error 0.3964(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2647 | Time 59.8530(59.6766) | Bit/dim 3.7141(3.7159) | Xent 1.1414(1.1477) | Loss 4.2848(4.2897) | Error 0.4066(0.4110) Steps 640(653.41) | Grad Norm 0.4249(0.6107) | Total Time 14.00(14.00)\n",
      "Iter 2648 | Time 57.4558(59.6100) | Bit/dim 3.7145(3.7158) | Xent 1.1592(1.1481) | Loss 4.2941(4.2898) | Error 0.4115(0.4110) Steps 646(653.19) | Grad Norm 0.5340(0.6084) | Total Time 14.00(14.00)\n",
      "Iter 2649 | Time 59.9996(59.6217) | Bit/dim 3.7056(3.7155) | Xent 1.1461(1.1480) | Loss 4.2786(4.2895) | Error 0.4091(0.4110) Steps 664(653.51) | Grad Norm 0.4511(0.6037) | Total Time 14.00(14.00)\n",
      "Iter 2650 | Time 60.2243(59.6398) | Bit/dim 3.7119(3.7154) | Xent 1.1441(1.1479) | Loss 4.2839(4.2893) | Error 0.4120(0.4110) Steps 658(653.64) | Grad Norm 0.5810(0.6030) | Total Time 14.00(14.00)\n",
      "Iter 2651 | Time 58.6991(59.6115) | Bit/dim 3.7011(3.7150) | Xent 1.1305(1.1474) | Loss 4.2663(4.2886) | Error 0.4061(0.4109) Steps 628(652.87) | Grad Norm 0.6678(0.6049) | Total Time 14.00(14.00)\n",
      "Iter 2652 | Time 57.3795(59.5446) | Bit/dim 3.7172(3.7150) | Xent 1.1272(1.1468) | Loss 4.2808(4.2884) | Error 0.4061(0.4107) Steps 640(652.49) | Grad Norm 0.5881(0.6044) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0442 | Time 24.7146, Epoch Time 393.9372(393.5434), Bit/dim 3.7126(best: 3.7129), Xent 1.0984, Loss 4.2618, Error 0.3931(best: 0.3941)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2653 | Time 57.3225(59.4779) | Bit/dim 3.7098(3.7149) | Xent 1.1367(1.1465) | Loss 4.2782(4.2881) | Error 0.4015(0.4105) Steps 658(652.65) | Grad Norm 0.5945(0.6041) | Total Time 14.00(14.00)\n",
      "Iter 2654 | Time 60.0889(59.4963) | Bit/dim 3.7148(3.7149) | Xent 1.1413(1.1463) | Loss 4.2855(4.2880) | Error 0.4110(0.4105) Steps 658(652.81) | Grad Norm 0.3925(0.5978) | Total Time 14.00(14.00)\n",
      "Iter 2655 | Time 57.8367(59.4465) | Bit/dim 3.7164(3.7149) | Xent 1.1296(1.1458) | Loss 4.2812(4.2878) | Error 0.4049(0.4103) Steps 652(652.79) | Grad Norm 0.6074(0.5981) | Total Time 14.00(14.00)\n",
      "Iter 2656 | Time 61.5552(59.5097) | Bit/dim 3.7049(3.7146) | Xent 1.1216(1.1451) | Loss 4.2657(4.2872) | Error 0.4039(0.4101) Steps 670(653.31) | Grad Norm 0.4390(0.5933) | Total Time 14.00(14.00)\n",
      "Iter 2657 | Time 61.4795(59.5688) | Bit/dim 3.7116(3.7145) | Xent 1.1262(1.1445) | Loss 4.2747(4.2868) | Error 0.3976(0.4097) Steps 646(653.09) | Grad Norm 0.4847(0.5900) | Total Time 14.00(14.00)\n",
      "Iter 2658 | Time 60.2153(59.5882) | Bit/dim 3.7070(3.7143) | Xent 1.1386(1.1443) | Loss 4.2763(4.2865) | Error 0.4111(0.4098) Steps 646(652.87) | Grad Norm 0.4140(0.5848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0443 | Time 24.4122, Epoch Time 398.6750(393.6973), Bit/dim 3.7118(best: 3.7126), Xent 1.0952, Loss 4.2594, Error 0.3894(best: 0.3931)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2659 | Time 60.5411(59.6168) | Bit/dim 3.7065(3.7141) | Xent 1.1251(1.1438) | Loss 4.2691(4.2859) | Error 0.4028(0.4096) Steps 640(652.49) | Grad Norm 0.6062(0.5854) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 59.9637(59.6272) | Bit/dim 3.6974(3.7136) | Xent 1.1502(1.1439) | Loss 4.2725(4.2855) | Error 0.4066(0.4095) Steps 640(652.11) | Grad Norm 0.4267(0.5807) | Total Time 14.00(14.00)\n",
      "Iter 2661 | Time 58.5844(59.5959) | Bit/dim 3.7187(3.7137) | Xent 1.1281(1.1435) | Loss 4.2828(4.2855) | Error 0.4025(0.4093) Steps 658(652.29) | Grad Norm 0.4454(0.5766) | Total Time 14.00(14.00)\n",
      "Iter 2662 | Time 59.9162(59.6055) | Bit/dim 3.7103(3.7136) | Xent 1.1145(1.1426) | Loss 4.2675(4.2849) | Error 0.4032(0.4091) Steps 658(652.46) | Grad Norm 0.4246(0.5720) | Total Time 14.00(14.00)\n",
      "Iter 2663 | Time 60.8365(59.6425) | Bit/dim 3.7125(3.7136) | Xent 1.1448(1.1427) | Loss 4.2849(4.2849) | Error 0.4147(0.4093) Steps 640(652.09) | Grad Norm 0.7703(0.5780) | Total Time 14.00(14.00)\n",
      "Iter 2664 | Time 56.2808(59.5416) | Bit/dim 3.7150(3.7136) | Xent 1.1348(1.1424) | Loss 4.2823(4.2848) | Error 0.4061(0.4092) Steps 640(651.73) | Grad Norm 0.9455(0.5890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0444 | Time 24.5284, Epoch Time 396.0804(393.7688), Bit/dim 3.7112(best: 3.7118), Xent 1.0930, Loss 4.2577, Error 0.3919(best: 0.3894)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2665 | Time 57.5276(59.4812) | Bit/dim 3.7068(3.7134) | Xent 1.1233(1.1419) | Loss 4.2685(4.2844) | Error 0.4032(0.4090) Steps 652(651.73) | Grad Norm 0.7596(0.5941) | Total Time 14.00(14.00)\n",
      "Iter 2666 | Time 59.1385(59.4709) | Bit/dim 3.7042(3.7131) | Xent 1.1425(1.1419) | Loss 4.2754(4.2841) | Error 0.4135(0.4091) Steps 640(651.38) | Grad Norm 0.7845(0.5998) | Total Time 14.00(14.00)\n",
      "Iter 2667 | Time 57.4180(59.4093) | Bit/dim 3.7125(3.7131) | Xent 1.1363(1.1417) | Loss 4.2807(4.2840) | Error 0.4086(0.4091) Steps 640(651.04) | Grad Norm 0.6112(0.6002) | Total Time 14.00(14.00)\n",
      "Iter 2668 | Time 61.0809(59.4595) | Bit/dim 3.7107(3.7131) | Xent 1.1234(1.1412) | Loss 4.2724(4.2836) | Error 0.3982(0.4088) Steps 640(650.71) | Grad Norm 0.5297(0.5981) | Total Time 14.00(14.00)\n",
      "Iter 2669 | Time 60.0041(59.4758) | Bit/dim 3.7180(3.7132) | Xent 1.1285(1.1408) | Loss 4.2823(4.2836) | Error 0.4109(0.4088) Steps 634(650.21) | Grad Norm 0.6999(0.6011) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 59.2423(59.4688) | Bit/dim 3.7056(3.7130) | Xent 1.1333(1.1406) | Loss 4.2723(4.2833) | Error 0.4123(0.4089) Steps 640(649.90) | Grad Norm 0.5513(0.5996) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0445 | Time 24.4786, Epoch Time 394.7376(393.7979), Bit/dim 3.7111(best: 3.7112), Xent 1.0927, Loss 4.2575, Error 0.3932(best: 0.3894)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2671 | Time 59.7755(59.4780) | Bit/dim 3.7139(3.7130) | Xent 1.1302(1.1402) | Loss 4.2790(4.2831) | Error 0.4042(0.4088) Steps 646(649.78) | Grad Norm 0.5989(0.5996) | Total Time 14.00(14.00)\n",
      "Iter 2672 | Time 60.9222(59.5213) | Bit/dim 3.7119(3.7130) | Xent 1.1415(1.1403) | Loss 4.2827(4.2831) | Error 0.4016(0.4086) Steps 640(649.49) | Grad Norm 0.4946(0.5964) | Total Time 14.00(14.00)\n",
      "Iter 2673 | Time 60.4560(59.5494) | Bit/dim 3.7046(3.7127) | Xent 1.1183(1.1396) | Loss 4.2638(4.2825) | Error 0.4005(0.4083) Steps 640(649.21) | Grad Norm 0.8215(0.6032) | Total Time 14.00(14.00)\n",
      "Iter 2674 | Time 57.0954(59.4758) | Bit/dim 3.7174(3.7129) | Xent 1.1394(1.1396) | Loss 4.2871(4.2827) | Error 0.4056(0.4083) Steps 640(648.93) | Grad Norm 0.5569(0.6018) | Total Time 14.00(14.00)\n",
      "Iter 2675 | Time 58.2672(59.4395) | Bit/dim 3.7062(3.7127) | Xent 1.1228(1.1391) | Loss 4.2676(4.2822) | Error 0.4010(0.4080) Steps 664(649.38) | Grad Norm 0.5868(0.6014) | Total Time 14.00(14.00)\n",
      "Iter 2676 | Time 59.7866(59.4499) | Bit/dim 3.7068(3.7125) | Xent 1.1137(1.1383) | Loss 4.2637(4.2817) | Error 0.3945(0.4076) Steps 640(649.10) | Grad Norm 0.7437(0.6056) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0446 | Time 24.9904, Epoch Time 396.9524(393.8925), Bit/dim 3.7090(best: 3.7111), Xent 1.0910, Loss 4.2545, Error 0.3897(best: 0.3894)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2677 | Time 57.2134(59.3828) | Bit/dim 3.7172(3.7126) | Xent 1.1390(1.1384) | Loss 4.2867(4.2818) | Error 0.4097(0.4077) Steps 640(648.83) | Grad Norm 0.3395(0.5976) | Total Time 14.00(14.00)\n",
      "Iter 2678 | Time 60.2653(59.4093) | Bit/dim 3.7148(3.7127) | Xent 1.1254(1.1380) | Loss 4.2775(4.2817) | Error 0.4044(0.4076) Steps 664(649.28) | Grad Norm 0.6249(0.5985) | Total Time 14.00(14.00)\n",
      "Iter 2679 | Time 61.6015(59.4751) | Bit/dim 3.7099(3.7126) | Xent 1.1243(1.1376) | Loss 4.2721(4.2814) | Error 0.4021(0.4074) Steps 640(649.00) | Grad Norm 0.4362(0.5936) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 61.5094(59.5361) | Bit/dim 3.7012(3.7123) | Xent 1.1307(1.1374) | Loss 4.2666(4.2810) | Error 0.4060(0.4074) Steps 640(648.73) | Grad Norm 0.6603(0.5956) | Total Time 14.00(14.00)\n",
      "Iter 2681 | Time 58.9898(59.5197) | Bit/dim 3.7053(3.7121) | Xent 1.1414(1.1375) | Loss 4.2760(4.2808) | Error 0.4123(0.4075) Steps 634(648.29) | Grad Norm 0.5107(0.5930) | Total Time 14.00(14.00)\n",
      "Iter 2682 | Time 61.2102(59.5704) | Bit/dim 3.7099(3.7120) | Xent 1.1223(1.1370) | Loss 4.2710(4.2805) | Error 0.4020(0.4074) Steps 634(647.86) | Grad Norm 0.5756(0.5925) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0447 | Time 24.8157, Epoch Time 400.9318(394.1037), Bit/dim 3.7098(best: 3.7090), Xent 1.0906, Loss 4.2551, Error 0.3902(best: 0.3894)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2683 | Time 58.1447(59.5276) | Bit/dim 3.7158(3.7121) | Xent 1.1240(1.1366) | Loss 4.2778(4.2804) | Error 0.4041(0.4073) Steps 640(647.63) | Grad Norm 0.4407(0.5880) | Total Time 14.00(14.00)\n",
      "Iter 2684 | Time 57.1127(59.4552) | Bit/dim 3.6982(3.7117) | Xent 1.1280(1.1364) | Loss 4.2622(4.2799) | Error 0.4031(0.4072) Steps 652(647.76) | Grad Norm 0.4706(0.5845) | Total Time 14.00(14.00)\n",
      "Iter 2685 | Time 59.9849(59.4711) | Bit/dim 3.7022(3.7114) | Xent 1.1123(1.1357) | Loss 4.2583(4.2792) | Error 0.4016(0.4070) Steps 646(647.71) | Grad Norm 0.5256(0.5827) | Total Time 14.00(14.00)\n",
      "Iter 2686 | Time 59.1852(59.4625) | Bit/dim 3.7134(3.7115) | Xent 1.1355(1.1356) | Loss 4.2811(4.2793) | Error 0.4081(0.4070) Steps 652(647.83) | Grad Norm 0.7688(0.5883) | Total Time 14.00(14.00)\n",
      "Iter 2687 | Time 56.4721(59.3728) | Bit/dim 3.7127(3.7115) | Xent 1.1253(1.1353) | Loss 4.2753(4.2792) | Error 0.3998(0.4068) Steps 634(647.42) | Grad Norm 0.6497(0.5901) | Total Time 14.00(14.00)\n",
      "Iter 2688 | Time 60.9837(59.4211) | Bit/dim 3.7120(3.7115) | Xent 1.1243(1.1350) | Loss 4.2741(4.2790) | Error 0.4062(0.4068) Steps 670(648.10) | Grad Norm 0.7265(0.5942) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0448 | Time 24.5901, Epoch Time 391.7765(394.0339), Bit/dim 3.7097(best: 3.7090), Xent 1.0880, Loss 4.2537, Error 0.3883(best: 0.3894)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2689 | Time 59.2000(59.4145) | Bit/dim 3.7081(3.7114) | Xent 1.1496(1.1354) | Loss 4.2830(4.2791) | Error 0.4091(0.4069) Steps 652(648.21) | Grad Norm 0.7120(0.5977) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 59.6381(59.4212) | Bit/dim 3.7033(3.7112) | Xent 1.1267(1.1352) | Loss 4.2666(4.2788) | Error 0.4090(0.4069) Steps 688(649.41) | Grad Norm 0.7644(0.6027) | Total Time 14.00(14.00)\n",
      "Iter 2691 | Time 59.1875(59.4142) | Bit/dim 3.7067(3.7110) | Xent 1.1346(1.1352) | Loss 4.2740(4.2786) | Error 0.4084(0.4070) Steps 640(649.13) | Grad Norm 0.9405(0.6129) | Total Time 14.00(14.00)\n",
      "Iter 2692 | Time 60.3174(59.4413) | Bit/dim 3.7079(3.7109) | Xent 1.1121(1.1345) | Loss 4.2639(4.2782) | Error 0.4001(0.4068) Steps 640(648.85) | Grad Norm 0.4080(0.6067) | Total Time 14.00(14.00)\n",
      "Iter 2693 | Time 58.1977(59.4040) | Bit/dim 3.7110(3.7109) | Xent 1.1366(1.1345) | Loss 4.2793(4.2782) | Error 0.4038(0.4067) Steps 646(648.77) | Grad Norm 0.6268(0.6073) | Total Time 14.00(14.00)\n",
      "Iter 2694 | Time 60.2960(59.4307) | Bit/dim 3.7100(3.7109) | Xent 1.1044(1.1336) | Loss 4.2622(4.2777) | Error 0.3954(0.4063) Steps 652(648.86) | Grad Norm 0.5757(0.6064) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0449 | Time 24.5807, Epoch Time 397.1398(394.1271), Bit/dim 3.7088(best: 3.7090), Xent 1.0872, Loss 4.2524, Error 0.3906(best: 0.3883)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2695 | Time 57.1852(59.3634) | Bit/dim 3.7069(3.7108) | Xent 1.1267(1.1334) | Loss 4.2702(4.2775) | Error 0.4028(0.4062) Steps 646(648.78) | Grad Norm 0.5569(0.6049) | Total Time 14.00(14.00)\n",
      "Iter 2696 | Time 56.5590(59.2792) | Bit/dim 3.6998(3.7105) | Xent 1.1449(1.1338) | Loss 4.2722(4.2773) | Error 0.4061(0.4062) Steps 640(648.51) | Grad Norm 0.7065(0.6079) | Total Time 14.00(14.00)\n",
      "Iter 2697 | Time 59.8962(59.2978) | Bit/dim 3.7042(3.7103) | Xent 1.1459(1.1341) | Loss 4.2771(4.2773) | Error 0.4067(0.4062) Steps 634(648.08) | Grad Norm 0.5200(0.6053) | Total Time 14.00(14.00)\n",
      "Iter 2698 | Time 59.4712(59.3030) | Bit/dim 3.7191(3.7105) | Xent 1.1090(1.1334) | Loss 4.2736(4.2772) | Error 0.4030(0.4061) Steps 652(648.20) | Grad Norm 0.6113(0.6055) | Total Time 14.00(14.00)\n",
      "Iter 2699 | Time 55.9250(59.2016) | Bit/dim 3.7102(3.7105) | Xent 1.1249(1.1331) | Loss 4.2726(4.2771) | Error 0.4014(0.4060) Steps 628(647.59) | Grad Norm 0.7664(0.6103) | Total Time 14.00(14.00)\n",
      "Iter 2700 | Time 60.1665(59.2306) | Bit/dim 3.7035(3.7103) | Xent 1.1071(1.1323) | Loss 4.2570(4.2765) | Error 0.3965(0.4057) Steps 640(647.36) | Grad Norm 0.4807(0.6064) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0450 | Time 24.2272, Epoch Time 388.7322(393.9652), Bit/dim 3.7082(best: 3.7088), Xent 1.0868, Loss 4.2516, Error 0.3886(best: 0.3883)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2701 | Time 58.4033(59.2057) | Bit/dim 3.7113(3.7103) | Xent 1.1314(1.1323) | Loss 4.2770(4.2765) | Error 0.4105(0.4059) Steps 646(647.32) | Grad Norm 0.4291(0.6011) | Total Time 14.00(14.00)\n",
      "Iter 2702 | Time 59.3788(59.2109) | Bit/dim 3.7007(3.7101) | Xent 1.1304(1.1323) | Loss 4.2659(4.2762) | Error 0.4052(0.4058) Steps 646(647.28) | Grad Norm 0.4379(0.5962) | Total Time 14.00(14.00)\n",
      "Iter 2703 | Time 56.5597(59.1314) | Bit/dim 3.7055(3.7099) | Xent 1.1214(1.1319) | Loss 4.2662(4.2759) | Error 0.4056(0.4058) Steps 634(646.88) | Grad Norm 0.4155(0.5908) | Total Time 14.00(14.00)\n",
      "Iter 2704 | Time 57.6488(59.0869) | Bit/dim 3.7187(3.7102) | Xent 1.1178(1.1315) | Loss 4.2776(4.2759) | Error 0.3974(0.4056) Steps 646(646.86) | Grad Norm 0.3735(0.5843) | Total Time 14.00(14.00)\n",
      "Iter 2705 | Time 58.2630(59.0622) | Bit/dim 3.7064(3.7101) | Xent 1.1217(1.1312) | Loss 4.2673(4.2757) | Error 0.3976(0.4053) Steps 634(646.47) | Grad Norm 0.3570(0.5775) | Total Time 14.00(14.00)\n",
      "Iter 2706 | Time 60.2935(59.0991) | Bit/dim 3.6973(3.7097) | Xent 1.1160(1.1308) | Loss 4.2553(4.2751) | Error 0.4030(0.4053) Steps 640(646.28) | Grad Norm 0.6203(0.5787) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0451 | Time 24.1728, Epoch Time 390.3335(393.8563), Bit/dim 3.7080(best: 3.7082), Xent 1.0837, Loss 4.2498, Error 0.3860(best: 0.3883)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2707 | Time 60.8632(59.1521) | Bit/dim 3.6971(3.7093) | Xent 1.1348(1.1309) | Loss 4.2645(4.2748) | Error 0.4036(0.4052) Steps 640(646.09) | Grad Norm 0.4135(0.5738) | Total Time 14.00(14.00)\n",
      "Iter 2708 | Time 59.8334(59.1725) | Bit/dim 3.7123(3.7094) | Xent 1.1122(1.1303) | Loss 4.2684(4.2746) | Error 0.3989(0.4050) Steps 652(646.27) | Grad Norm 0.3836(0.5681) | Total Time 14.00(14.00)\n",
      "Iter 2709 | Time 60.3880(59.2090) | Bit/dim 3.7063(3.7093) | Xent 1.1350(1.1305) | Loss 4.2738(4.2745) | Error 0.4045(0.4050) Steps 640(646.08) | Grad Norm 0.4435(0.5643) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 60.7912(59.2564) | Bit/dim 3.6995(3.7090) | Xent 1.1122(1.1299) | Loss 4.2556(4.2740) | Error 0.3925(0.4046) Steps 670(646.80) | Grad Norm 1.0058(0.5776) | Total Time 14.00(14.00)\n",
      "Iter 2711 | Time 58.5382(59.2349) | Bit/dim 3.7128(3.7091) | Xent 1.1219(1.1297) | Loss 4.2738(4.2740) | Error 0.4059(0.4047) Steps 652(646.95) | Grad Norm 0.6529(0.5798) | Total Time 14.00(14.00)\n",
      "Iter 2712 | Time 57.3456(59.1782) | Bit/dim 3.7089(3.7091) | Xent 1.1200(1.1294) | Loss 4.2688(4.2738) | Error 0.4021(0.4046) Steps 646(646.92) | Grad Norm 0.6739(0.5827) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0452 | Time 23.9934, Epoch Time 397.5073(393.9658), Bit/dim 3.7078(best: 3.7080), Xent 1.0825, Loss 4.2491, Error 0.3868(best: 0.3860)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2713 | Time 59.2084(59.1791) | Bit/dim 3.7066(3.7090) | Xent 1.1069(1.1287) | Loss 4.2601(4.2734) | Error 0.3980(0.4044) Steps 646(646.90) | Grad Norm 1.1263(0.5990) | Total Time 14.00(14.00)\n",
      "Iter 2714 | Time 60.3642(59.2147) | Bit/dim 3.7089(3.7090) | Xent 1.1247(1.1286) | Loss 4.2712(4.2733) | Error 0.4032(0.4044) Steps 634(646.51) | Grad Norm 0.9408(0.6092) | Total Time 14.00(14.00)\n",
      "Iter 2715 | Time 59.9360(59.2363) | Bit/dim 3.7024(3.7088) | Xent 1.1215(1.1284) | Loss 4.2632(4.2730) | Error 0.4000(0.4042) Steps 640(646.31) | Grad Norm 0.6769(0.6113) | Total Time 14.00(14.00)\n",
      "Iter 2716 | Time 58.9711(59.2283) | Bit/dim 3.7037(3.7087) | Xent 1.1185(1.1281) | Loss 4.2629(4.2727) | Error 0.4029(0.4042) Steps 628(645.76) | Grad Norm 0.4726(0.6071) | Total Time 14.00(14.00)\n",
      "Iter 2717 | Time 59.2529(59.2291) | Bit/dim 3.7033(3.7085) | Xent 1.1079(1.1275) | Loss 4.2572(4.2723) | Error 0.4006(0.4041) Steps 634(645.41) | Grad Norm 0.9276(0.6167) | Total Time 14.00(14.00)\n",
      "Iter 2718 | Time 59.4832(59.2367) | Bit/dim 3.7070(3.7085) | Xent 1.1161(1.1271) | Loss 4.2651(4.2720) | Error 0.4029(0.4041) Steps 640(645.25) | Grad Norm 0.6272(0.6170) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0453 | Time 24.3929, Epoch Time 397.2390(394.0640), Bit/dim 3.7075(best: 3.7078), Xent 1.0819, Loss 4.2485, Error 0.3845(best: 0.3860)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2719 | Time 61.5763(59.3069) | Bit/dim 3.6989(3.7082) | Xent 1.1051(1.1265) | Loss 4.2515(4.2714) | Error 0.3895(0.4036) Steps 670(645.99) | Grad Norm 1.0405(0.6297) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 57.2149(59.2441) | Bit/dim 3.7013(3.7080) | Xent 1.1167(1.1262) | Loss 4.2597(4.2711) | Error 0.3985(0.4035) Steps 634(645.63) | Grad Norm 0.8444(0.6362) | Total Time 14.00(14.00)\n",
      "Iter 2721 | Time 55.9368(59.1449) | Bit/dim 3.7157(3.7082) | Xent 1.1128(1.1258) | Loss 4.2721(4.2711) | Error 0.4015(0.4034) Steps 640(645.46) | Grad Norm 0.4938(0.6319) | Total Time 14.00(14.00)\n",
      "Iter 2722 | Time 58.0329(59.1116) | Bit/dim 3.7028(3.7081) | Xent 1.1097(1.1253) | Loss 4.2576(4.2707) | Error 0.4002(0.4033) Steps 640(645.30) | Grad Norm 0.4427(0.6262) | Total Time 14.00(14.00)\n",
      "Iter 2723 | Time 57.5269(59.0640) | Bit/dim 3.7054(3.7080) | Xent 1.1146(1.1250) | Loss 4.2627(4.2705) | Error 0.3912(0.4029) Steps 652(645.50) | Grad Norm 0.4300(0.6203) | Total Time 14.00(14.00)\n",
      "Iter 2724 | Time 58.7747(59.0553) | Bit/dim 3.7074(3.7080) | Xent 1.1365(1.1253) | Loss 4.2757(4.2706) | Error 0.4011(0.4029) Steps 634(645.15) | Grad Norm 0.6054(0.6199) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0454 | Time 24.2182, Epoch Time 388.7012(393.9031), Bit/dim 3.7060(best: 3.7075), Xent 1.0812, Loss 4.2466, Error 0.3887(best: 0.3845)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2725 | Time 59.7225(59.0753) | Bit/dim 3.7064(3.7079) | Xent 1.1318(1.1255) | Loss 4.2723(4.2707) | Error 0.4107(0.4031) Steps 634(644.82) | Grad Norm 0.3676(0.6123) | Total Time 14.00(14.00)\n",
      "Iter 2726 | Time 56.9496(59.0116) | Bit/dim 3.7096(3.7080) | Xent 1.1199(1.1253) | Loss 4.2696(4.2706) | Error 0.4095(0.4033) Steps 640(644.68) | Grad Norm 0.5606(0.6108) | Total Time 14.00(14.00)\n",
      "Iter 2727 | Time 61.2860(59.0798) | Bit/dim 3.6947(3.7076) | Xent 1.1014(1.1246) | Loss 4.2454(4.2699) | Error 0.3932(0.4030) Steps 640(644.54) | Grad Norm 0.5345(0.6085) | Total Time 14.00(14.00)\n",
      "Iter 2728 | Time 57.7334(59.0394) | Bit/dim 3.7132(3.7077) | Xent 1.1070(1.1241) | Loss 4.2667(4.2698) | Error 0.3976(0.4029) Steps 634(644.22) | Grad Norm 0.4810(0.6047) | Total Time 14.00(14.00)\n",
      "Iter 2729 | Time 61.1941(59.1041) | Bit/dim 3.6982(3.7075) | Xent 1.1119(1.1237) | Loss 4.2542(4.2693) | Error 0.3955(0.4026) Steps 646(644.27) | Grad Norm 0.6091(0.6048) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 60.2978(59.1399) | Bit/dim 3.7074(3.7074) | Xent 1.1186(1.1236) | Loss 4.2667(4.2692) | Error 0.3960(0.4024) Steps 640(644.14) | Grad Norm 0.4956(0.6015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0455 | Time 24.5596, Epoch Time 397.3112(394.0053), Bit/dim 3.7052(best: 3.7060), Xent 1.0780, Loss 4.2442, Error 0.3869(best: 0.3845)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2731 | Time 59.6219(59.1543) | Bit/dim 3.7020(3.7073) | Xent 1.1164(1.1234) | Loss 4.2602(4.2690) | Error 0.4067(0.4026) Steps 640(644.02) | Grad Norm 0.5406(0.5997) | Total Time 14.00(14.00)\n",
      "Iter 2732 | Time 56.5800(59.0771) | Bit/dim 3.7064(3.7073) | Xent 1.1186(1.1232) | Loss 4.2657(4.2689) | Error 0.4042(0.4026) Steps 640(643.90) | Grad Norm 0.4631(0.5956) | Total Time 14.00(14.00)\n",
      "Iter 2733 | Time 56.9209(59.0124) | Bit/dim 3.6981(3.7070) | Xent 1.1114(1.1229) | Loss 4.2537(4.2684) | Error 0.4038(0.4026) Steps 646(643.96) | Grad Norm 0.6790(0.5981) | Total Time 14.00(14.00)\n",
      "Iter 2734 | Time 59.8099(59.0363) | Bit/dim 3.7075(3.7070) | Xent 1.1096(1.1225) | Loss 4.2623(4.2682) | Error 0.3959(0.4024) Steps 640(643.84) | Grad Norm 1.1007(0.6132) | Total Time 14.00(14.00)\n",
      "Iter 2735 | Time 57.4569(58.9890) | Bit/dim 3.7043(3.7069) | Xent 1.0758(1.1211) | Loss 4.2422(4.2675) | Error 0.3848(0.4019) Steps 634(643.55) | Grad Norm 0.6842(0.6153) | Total Time 14.00(14.00)\n",
      "Iter 2736 | Time 56.5495(58.9158) | Bit/dim 3.7086(3.7070) | Xent 1.1291(1.1213) | Loss 4.2732(4.2676) | Error 0.4061(0.4020) Steps 640(643.44) | Grad Norm 0.7665(0.6198) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0456 | Time 24.1593, Epoch Time 387.0603(393.7970), Bit/dim 3.7055(best: 3.7052), Xent 1.0778, Loss 4.2444, Error 0.3842(best: 0.3845)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2737 | Time 57.8241(58.8830) | Bit/dim 3.6969(3.7067) | Xent 1.1140(1.1211) | Loss 4.2540(4.2672) | Error 0.4035(0.4021) Steps 640(643.34) | Grad Norm 0.9254(0.6290) | Total Time 14.00(14.00)\n",
      "Iter 2738 | Time 56.7223(58.8182) | Bit/dim 3.6999(3.7065) | Xent 1.1081(1.1207) | Loss 4.2540(4.2668) | Error 0.3916(0.4018) Steps 628(642.88) | Grad Norm 0.6581(0.6299) | Total Time 14.00(14.00)\n",
      "Iter 2739 | Time 57.1177(58.7672) | Bit/dim 3.7064(3.7065) | Xent 1.1219(1.1207) | Loss 4.2674(4.2668) | Error 0.4005(0.4017) Steps 640(642.79) | Grad Norm 0.7780(0.6343) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 57.3164(58.7237) | Bit/dim 3.7126(3.7067) | Xent 1.1008(1.1201) | Loss 4.2631(4.2667) | Error 0.3914(0.4014) Steps 634(642.53) | Grad Norm 0.5659(0.6323) | Total Time 14.00(14.00)\n",
      "Iter 2741 | Time 60.4507(58.7755) | Bit/dim 3.7027(3.7065) | Xent 1.1116(1.1199) | Loss 4.2585(4.2665) | Error 0.4038(0.4015) Steps 634(642.27) | Grad Norm 0.7559(0.6360) | Total Time 14.00(14.00)\n",
      "Iter 2742 | Time 57.4015(58.7343) | Bit/dim 3.7014(3.7064) | Xent 1.1090(1.1196) | Loss 4.2559(4.2662) | Error 0.3994(0.4014) Steps 634(642.02) | Grad Norm 0.5668(0.6339) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0457 | Time 24.2597, Epoch Time 386.7232(393.5848), Bit/dim 3.7040(best: 3.7052), Xent 1.0762, Loss 4.2421, Error 0.3847(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2743 | Time 62.1559(58.8369) | Bit/dim 3.7031(3.7063) | Xent 1.1198(1.1196) | Loss 4.2630(4.2661) | Error 0.3959(0.4013) Steps 628(641.60) | Grad Norm 0.4826(0.6294) | Total Time 14.00(14.00)\n",
      "Iter 2744 | Time 58.3830(58.8233) | Bit/dim 3.7052(3.7062) | Xent 1.1192(1.1195) | Loss 4.2647(4.2660) | Error 0.4030(0.4013) Steps 640(641.56) | Grad Norm 0.7917(0.6342) | Total Time 14.00(14.00)\n",
      "Iter 2745 | Time 58.4219(58.8112) | Bit/dim 3.7169(3.7066) | Xent 1.1114(1.1193) | Loss 4.2726(4.2662) | Error 0.3941(0.4011) Steps 640(641.51) | Grad Norm 0.7866(0.6388) | Total Time 14.00(14.00)\n",
      "Iter 2746 | Time 57.4034(58.7690) | Bit/dim 3.6999(3.7064) | Xent 1.1141(1.1191) | Loss 4.2569(4.2659) | Error 0.4048(0.4012) Steps 634(641.28) | Grad Norm 0.6442(0.6390) | Total Time 14.00(14.00)\n",
      "Iter 2747 | Time 58.2777(58.7543) | Bit/dim 3.7021(3.7062) | Xent 1.0863(1.1182) | Loss 4.2453(4.2653) | Error 0.3904(0.4009) Steps 622(640.71) | Grad Norm 0.6620(0.6397) | Total Time 14.00(14.00)\n",
      "Iter 2748 | Time 57.6505(58.7212) | Bit/dim 3.6978(3.7060) | Xent 1.1203(1.1182) | Loss 4.2580(4.2651) | Error 0.4010(0.4009) Steps 628(640.32) | Grad Norm 0.9144(0.6479) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0458 | Time 23.8169, Epoch Time 391.6349(393.5263), Bit/dim 3.7036(best: 3.7040), Xent 1.0768, Loss 4.2420, Error 0.3858(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2749 | Time 56.4924(58.6543) | Bit/dim 3.7092(3.7061) | Xent 1.1120(1.1180) | Loss 4.2652(4.2651) | Error 0.3991(0.4008) Steps 628(639.95) | Grad Norm 0.8647(0.6544) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 58.2441(58.6420) | Bit/dim 3.7097(3.7062) | Xent 1.1068(1.1177) | Loss 4.2631(4.2650) | Error 0.3988(0.4008) Steps 640(639.96) | Grad Norm 0.5945(0.6526) | Total Time 14.00(14.00)\n",
      "Iter 2751 | Time 56.1353(58.5668) | Bit/dim 3.6961(3.7059) | Xent 1.0928(1.1170) | Loss 4.2425(4.2644) | Error 0.3891(0.4004) Steps 640(639.96) | Grad Norm 0.3926(0.6448) | Total Time 14.00(14.00)\n",
      "Iter 2752 | Time 59.1185(58.5833) | Bit/dim 3.7099(3.7060) | Xent 1.1067(1.1167) | Loss 4.2633(4.2643) | Error 0.3899(0.4001) Steps 646(640.14) | Grad Norm 0.7733(0.6487) | Total Time 14.00(14.00)\n",
      "Iter 2753 | Time 60.2890(58.6345) | Bit/dim 3.7074(3.7061) | Xent 1.1279(1.1170) | Loss 4.2713(4.2645) | Error 0.3999(0.4001) Steps 640(640.13) | Grad Norm 1.2350(0.6662) | Total Time 14.00(14.00)\n",
      "Iter 2754 | Time 60.2617(58.6833) | Bit/dim 3.6871(3.7055) | Xent 1.1106(1.1168) | Loss 4.2424(4.2639) | Error 0.4039(0.4002) Steps 634(639.95) | Grad Norm 0.4625(0.6601) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0459 | Time 23.7802, Epoch Time 389.7957(393.4144), Bit/dim 3.7042(best: 3.7036), Xent 1.0743, Loss 4.2413, Error 0.3858(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2755 | Time 57.9149(58.6603) | Bit/dim 3.6980(3.7053) | Xent 1.0990(1.1163) | Loss 4.2474(4.2634) | Error 0.3976(0.4001) Steps 640(639.95) | Grad Norm 0.6523(0.6599) | Total Time 14.00(14.00)\n",
      "Iter 2756 | Time 57.5590(58.6272) | Bit/dim 3.7034(3.7052) | Xent 1.0993(1.1158) | Loss 4.2530(4.2631) | Error 0.3914(0.3999) Steps 634(639.77) | Grad Norm 0.6923(0.6609) | Total Time 14.00(14.00)\n",
      "Iter 2757 | Time 58.4535(58.6220) | Bit/dim 3.7019(3.7051) | Xent 1.0825(1.1148) | Loss 4.2432(4.2625) | Error 0.3808(0.3993) Steps 628(639.42) | Grad Norm 0.4473(0.6545) | Total Time 14.00(14.00)\n",
      "Iter 2758 | Time 56.3957(58.5552) | Bit/dim 3.6941(3.7048) | Xent 1.1209(1.1149) | Loss 4.2545(4.2622) | Error 0.3972(0.3992) Steps 640(639.44) | Grad Norm 0.5168(0.6503) | Total Time 14.00(14.00)\n",
      "Iter 2759 | Time 60.3091(58.6078) | Bit/dim 3.7142(3.7051) | Xent 1.1323(1.1155) | Loss 4.2803(4.2628) | Error 0.4064(0.3995) Steps 634(639.27) | Grad Norm 0.5594(0.6476) | Total Time 14.00(14.00)\n",
      "Iter 2760 | Time 54.8586(58.4954) | Bit/dim 3.7018(3.7050) | Xent 1.1097(1.1153) | Loss 4.2567(4.2626) | Error 0.3951(0.3993) Steps 646(639.48) | Grad Norm 0.6659(0.6482) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0460 | Time 23.8355, Epoch Time 384.8129(393.1563), Bit/dim 3.7039(best: 3.7036), Xent 1.0729, Loss 4.2403, Error 0.3856(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2761 | Time 58.5059(58.4957) | Bit/dim 3.6947(3.7046) | Xent 1.1087(1.1151) | Loss 4.2491(4.2622) | Error 0.4028(0.3994) Steps 652(639.85) | Grad Norm 0.5347(0.6448) | Total Time 14.00(14.00)\n",
      "Iter 2762 | Time 58.4922(58.4956) | Bit/dim 3.7092(3.7048) | Xent 1.0978(1.1146) | Loss 4.2581(4.2621) | Error 0.3952(0.3993) Steps 628(639.50) | Grad Norm 0.4825(0.6399) | Total Time 14.00(14.00)\n",
      "Iter 2763 | Time 55.1353(58.3948) | Bit/dim 3.6986(3.7046) | Xent 1.0993(1.1141) | Loss 4.2483(4.2617) | Error 0.3901(0.3990) Steps 640(639.51) | Grad Norm 0.4633(0.6346) | Total Time 14.00(14.00)\n",
      "Iter 2764 | Time 56.3536(58.3335) | Bit/dim 3.7035(3.7046) | Xent 1.1050(1.1138) | Loss 4.2559(4.2615) | Error 0.4015(0.3991) Steps 628(639.17) | Grad Norm 0.6895(0.6362) | Total Time 14.00(14.00)\n",
      "Iter 2765 | Time 56.5565(58.2802) | Bit/dim 3.6966(3.7043) | Xent 1.0966(1.1133) | Loss 4.2449(4.2610) | Error 0.3854(0.3987) Steps 622(638.65) | Grad Norm 0.8901(0.6439) | Total Time 14.00(14.00)\n",
      "Iter 2766 | Time 56.8532(58.2374) | Bit/dim 3.7058(3.7044) | Xent 1.1186(1.1135) | Loss 4.2651(4.2611) | Error 0.4029(0.3988) Steps 628(638.33) | Grad Norm 0.6015(0.6426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0461 | Time 23.8743, Epoch Time 381.2925(392.8004), Bit/dim 3.7022(best: 3.7036), Xent 1.0721, Loss 4.2383, Error 0.3849(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2767 | Time 54.6614(58.1301) | Bit/dim 3.6943(3.7041) | Xent 1.0880(1.1127) | Loss 4.2383(4.2604) | Error 0.3930(0.3986) Steps 640(638.38) | Grad Norm 0.5238(0.6390) | Total Time 14.00(14.00)\n",
      "Iter 2768 | Time 57.9131(58.1236) | Bit/dim 3.7038(3.7041) | Xent 1.1016(1.1124) | Loss 4.2546(4.2603) | Error 0.3919(0.3984) Steps 646(638.61) | Grad Norm 0.7564(0.6425) | Total Time 14.00(14.00)\n",
      "Iter 2769 | Time 56.5605(58.0767) | Bit/dim 3.7073(3.7042) | Xent 1.1187(1.1126) | Loss 4.2666(4.2604) | Error 0.4011(0.3985) Steps 634(638.47) | Grad Norm 0.6834(0.6438) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 57.7434(58.0667) | Bit/dim 3.6957(3.7039) | Xent 1.1004(1.1122) | Loss 4.2459(4.2600) | Error 0.3948(0.3984) Steps 640(638.52) | Grad Norm 1.0837(0.6570) | Total Time 14.00(14.00)\n",
      "Iter 2771 | Time 56.9830(58.0342) | Bit/dim 3.6989(3.7038) | Xent 1.1093(1.1121) | Loss 4.2536(4.2598) | Error 0.3971(0.3984) Steps 628(638.20) | Grad Norm 0.4809(0.6517) | Total Time 14.00(14.00)\n",
      "Iter 2772 | Time 59.0639(58.0651) | Bit/dim 3.7018(3.7037) | Xent 1.0973(1.1117) | Loss 4.2504(4.2595) | Error 0.3952(0.3983) Steps 652(638.62) | Grad Norm 1.0829(0.6646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0462 | Time 24.1160, Epoch Time 382.4286(392.4893), Bit/dim 3.7026(best: 3.7022), Xent 1.0696, Loss 4.2374, Error 0.3834(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2773 | Time 61.3630(58.1640) | Bit/dim 3.6888(3.7032) | Xent 1.1295(1.1122) | Loss 4.2535(4.2594) | Error 0.4067(0.3985) Steps 640(638.66) | Grad Norm 0.9418(0.6729) | Total Time 14.00(14.00)\n",
      "Iter 2774 | Time 56.5328(58.1151) | Bit/dim 3.7061(3.7033) | Xent 1.1059(1.1120) | Loss 4.2591(4.2593) | Error 0.3948(0.3984) Steps 640(638.70) | Grad Norm 0.8026(0.6768) | Total Time 14.00(14.00)\n",
      "Iter 2775 | Time 58.3060(58.1208) | Bit/dim 3.7063(3.7034) | Xent 1.1150(1.1121) | Loss 4.2638(4.2595) | Error 0.3962(0.3983) Steps 610(637.84) | Grad Norm 0.5352(0.6726) | Total Time 14.00(14.00)\n",
      "Iter 2776 | Time 57.2536(58.0948) | Bit/dim 3.7142(3.7037) | Xent 1.0853(1.1113) | Loss 4.2569(4.2594) | Error 0.3874(0.3980) Steps 634(637.72) | Grad Norm 0.7116(0.6737) | Total Time 14.00(14.00)\n",
      "Iter 2777 | Time 57.3236(58.0717) | Bit/dim 3.6939(3.7035) | Xent 1.1118(1.1113) | Loss 4.2498(4.2591) | Error 0.3948(0.3979) Steps 634(637.61) | Grad Norm 1.5461(0.6999) | Total Time 14.00(14.00)\n",
      "Iter 2778 | Time 56.9165(58.0370) | Bit/dim 3.7027(3.7034) | Xent 1.0962(1.1109) | Loss 4.2508(4.2589) | Error 0.3861(0.3976) Steps 634(637.50) | Grad Norm 0.8273(0.7037) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0463 | Time 23.6057, Epoch Time 386.7073(392.3158), Bit/dim 3.7021(best: 3.7022), Xent 1.0689, Loss 4.2365, Error 0.3828(best: 0.3834)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2779 | Time 59.6453(58.0853) | Bit/dim 3.7066(3.7035) | Xent 1.0939(1.1104) | Loss 4.2536(4.2587) | Error 0.3855(0.3972) Steps 622(637.04) | Grad Norm 0.6391(0.7018) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 58.9267(58.1105) | Bit/dim 3.7020(3.7035) | Xent 1.1057(1.1102) | Loss 4.2549(4.2586) | Error 0.3914(0.3970) Steps 628(636.77) | Grad Norm 0.6891(0.7014) | Total Time 14.00(14.00)\n",
      "Iter 2781 | Time 59.9512(58.1657) | Bit/dim 3.7017(3.7034) | Xent 1.1255(1.1107) | Loss 4.2645(4.2588) | Error 0.4060(0.3973) Steps 634(636.68) | Grad Norm 0.5931(0.6982) | Total Time 14.00(14.00)\n",
      "Iter 2782 | Time 61.0760(58.2530) | Bit/dim 3.6996(3.7033) | Xent 1.1060(1.1105) | Loss 4.2526(4.2586) | Error 0.3961(0.3973) Steps 640(636.78) | Grad Norm 0.6798(0.6976) | Total Time 14.00(14.00)\n",
      "Iter 2783 | Time 59.1346(58.2795) | Bit/dim 3.7041(3.7033) | Xent 1.1164(1.1107) | Loss 4.2623(4.2587) | Error 0.4015(0.3974) Steps 634(636.70) | Grad Norm 0.5581(0.6934) | Total Time 14.00(14.00)\n",
      "Iter 2784 | Time 56.7512(58.2336) | Bit/dim 3.6950(3.7031) | Xent 1.0729(1.1096) | Loss 4.2314(4.2579) | Error 0.3879(0.3971) Steps 646(636.98) | Grad Norm 1.4002(0.7146) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0464 | Time 23.6610, Epoch Time 394.8496(392.3918), Bit/dim 3.7016(best: 3.7021), Xent 1.0683, Loss 4.2357, Error 0.3797(best: 0.3828)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2785 | Time 57.8923(58.2234) | Bit/dim 3.6991(3.7030) | Xent 1.0941(1.1091) | Loss 4.2462(4.2575) | Error 0.3949(0.3970) Steps 634(636.89) | Grad Norm 0.6527(0.7128) | Total Time 14.00(14.00)\n",
      "Iter 2786 | Time 57.2197(58.1933) | Bit/dim 3.7011(3.7029) | Xent 1.0852(1.1084) | Loss 4.2438(4.2571) | Error 0.3829(0.3966) Steps 634(636.80) | Grad Norm 0.7656(0.7144) | Total Time 14.00(14.00)\n",
      "Iter 2787 | Time 59.8510(58.2430) | Bit/dim 3.6954(3.7027) | Xent 1.1114(1.1085) | Loss 4.2511(4.2569) | Error 0.3998(0.3967) Steps 640(636.90) | Grad Norm 0.5635(0.7098) | Total Time 14.00(14.00)\n",
      "Iter 2788 | Time 57.4789(58.2201) | Bit/dim 3.6918(3.7024) | Xent 1.1017(1.1083) | Loss 4.2426(4.2565) | Error 0.3956(0.3967) Steps 628(636.63) | Grad Norm 0.4839(0.7031) | Total Time 14.00(14.00)\n",
      "Iter 2789 | Time 57.9319(58.2115) | Bit/dim 3.7015(3.7023) | Xent 1.1063(1.1082) | Loss 4.2546(4.2564) | Error 0.3992(0.3968) Steps 628(636.37) | Grad Norm 0.5909(0.6997) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 57.5151(58.1906) | Bit/dim 3.7079(3.7025) | Xent 1.0939(1.1078) | Loss 4.2549(4.2564) | Error 0.4009(0.3969) Steps 658(637.02) | Grad Norm 0.7028(0.6998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0465 | Time 23.5518, Epoch Time 387.0051(392.2302), Bit/dim 3.7011(best: 3.7016), Xent 1.0670, Loss 4.2346, Error 0.3797(best: 0.3797)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2791 | Time 58.3196(58.1944) | Bit/dim 3.7004(3.7024) | Xent 1.0965(1.1075) | Loss 4.2487(4.2562) | Error 0.3935(0.3968) Steps 640(637.11) | Grad Norm 0.8968(0.7057) | Total Time 14.00(14.00)\n",
      "Iter 2792 | Time 56.2609(58.1364) | Bit/dim 3.6963(3.7023) | Xent 1.1126(1.1076) | Loss 4.2526(4.2561) | Error 0.4016(0.3969) Steps 628(636.84) | Grad Norm 0.5148(0.7000) | Total Time 14.00(14.00)\n",
      "Iter 2793 | Time 58.8364(58.1574) | Bit/dim 3.6940(3.7020) | Xent 1.0806(1.1068) | Loss 4.2343(4.2554) | Error 0.3944(0.3968) Steps 616(636.21) | Grad Norm 1.0927(0.7117) | Total Time 14.00(14.00)\n",
      "Iter 2794 | Time 58.6364(58.1718) | Bit/dim 3.7072(3.7022) | Xent 1.1116(1.1069) | Loss 4.2629(4.2556) | Error 0.3941(0.3968) Steps 628(635.97) | Grad Norm 1.0182(0.7209) | Total Time 14.00(14.00)\n",
      "Iter 2795 | Time 55.7730(58.0998) | Bit/dim 3.6988(3.7021) | Xent 1.1162(1.1072) | Loss 4.2569(4.2557) | Error 0.4024(0.3969) Steps 628(635.73) | Grad Norm 0.6130(0.7177) | Total Time 14.00(14.00)\n",
      "Iter 2796 | Time 58.6768(58.1171) | Bit/dim 3.7035(3.7021) | Xent 1.0990(1.1070) | Loss 4.2530(4.2556) | Error 0.3919(0.3968) Steps 640(635.85) | Grad Norm 0.5775(0.7135) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0466 | Time 24.0515, Epoch Time 386.2005(392.0493), Bit/dim 3.7011(best: 3.7011), Xent 1.0646, Loss 4.2334, Error 0.3778(best: 0.3797)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2797 | Time 57.1579(58.0884) | Bit/dim 3.7091(3.7023) | Xent 1.0952(1.1066) | Loss 4.2567(4.2556) | Error 0.3925(0.3966) Steps 628(635.62) | Grad Norm 1.1110(0.7254) | Total Time 14.00(14.00)\n",
      "Iter 2798 | Time 57.0541(58.0573) | Bit/dim 3.7017(3.7023) | Xent 1.1058(1.1066) | Loss 4.2546(4.2556) | Error 0.4018(0.3968) Steps 616(635.03) | Grad Norm 0.8969(0.7306) | Total Time 14.00(14.00)\n",
      "Iter 2799 | Time 58.3431(58.0659) | Bit/dim 3.6940(3.7020) | Xent 1.1051(1.1066) | Loss 4.2465(4.2553) | Error 0.3939(0.3967) Steps 634(635.00) | Grad Norm 1.0493(0.7401) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 60.5195(58.1395) | Bit/dim 3.6978(3.7019) | Xent 1.0924(1.1061) | Loss 4.2440(4.2550) | Error 0.3902(0.3965) Steps 634(634.97) | Grad Norm 1.0577(0.7497) | Total Time 14.00(14.00)\n",
      "Iter 2801 | Time 56.6769(58.0956) | Bit/dim 3.6929(3.7017) | Xent 1.0932(1.1057) | Loss 4.2395(4.2545) | Error 0.3842(0.3962) Steps 616(634.40) | Grad Norm 0.6228(0.7459) | Total Time 14.00(14.00)\n",
      "Iter 2802 | Time 56.1444(58.0371) | Bit/dim 3.7033(3.7017) | Xent 1.0954(1.1054) | Loss 4.2510(4.2544) | Error 0.3901(0.3960) Steps 616(633.85) | Grad Norm 1.0821(0.7559) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0467 | Time 23.8474, Epoch Time 385.2581(391.8456), Bit/dim 3.7008(best: 3.7011), Xent 1.0633, Loss 4.2325, Error 0.3787(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2803 | Time 56.4806(57.9904) | Bit/dim 3.7132(3.7020) | Xent 1.0984(1.1052) | Loss 4.2624(4.2547) | Error 0.3935(0.3959) Steps 628(633.67) | Grad Norm 0.6297(0.7522) | Total Time 14.00(14.00)\n",
      "Iter 2804 | Time 55.5120(57.9161) | Bit/dim 3.6972(3.7019) | Xent 1.0845(1.1046) | Loss 4.2394(4.2542) | Error 0.3839(0.3955) Steps 634(633.68) | Grad Norm 0.6403(0.7488) | Total Time 14.00(14.00)\n",
      "Iter 2805 | Time 58.2746(57.9268) | Bit/dim 3.6925(3.7016) | Xent 1.1124(1.1048) | Loss 4.2487(4.2540) | Error 0.3996(0.3957) Steps 628(633.51) | Grad Norm 0.7111(0.7477) | Total Time 14.00(14.00)\n",
      "Iter 2806 | Time 58.2151(57.9355) | Bit/dim 3.6979(3.7015) | Xent 1.0872(1.1043) | Loss 4.2415(4.2537) | Error 0.3892(0.3955) Steps 616(632.99) | Grad Norm 0.6432(0.7445) | Total Time 14.00(14.00)\n",
      "Iter 2807 | Time 61.3158(58.0369) | Bit/dim 3.6895(3.7011) | Xent 1.1222(1.1048) | Loss 4.2506(4.2536) | Error 0.4010(0.3956) Steps 634(633.02) | Grad Norm 0.7909(0.7459) | Total Time 14.00(14.00)\n",
      "Iter 2808 | Time 56.8038(57.9999) | Bit/dim 3.6993(3.7011) | Xent 1.1015(1.1047) | Loss 4.2500(4.2535) | Error 0.3922(0.3955) Steps 640(633.23) | Grad Norm 0.9206(0.7512) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0468 | Time 23.2284, Epoch Time 385.2069(391.6464), Bit/dim 3.6999(best: 3.7008), Xent 1.0623, Loss 4.2310, Error 0.3827(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2809 | Time 60.3858(58.0715) | Bit/dim 3.7059(3.7012) | Xent 1.1033(1.1047) | Loss 4.2575(4.2536) | Error 0.3935(0.3955) Steps 640(633.43) | Grad Norm 0.5661(0.7456) | Total Time 14.00(14.00)\n",
      "Iter 2810 | Time 57.1493(58.0438) | Bit/dim 3.6898(3.7009) | Xent 1.1005(1.1046) | Loss 4.2400(4.2532) | Error 0.3991(0.3956) Steps 634(633.45) | Grad Norm 1.1346(0.7573) | Total Time 14.00(14.00)\n",
      "Iter 2811 | Time 57.9121(58.0398) | Bit/dim 3.6973(3.7008) | Xent 1.0937(1.1042) | Loss 4.2441(4.2529) | Error 0.3885(0.3954) Steps 622(633.10) | Grad Norm 1.5435(0.7809) | Total Time 14.00(14.00)\n",
      "Iter 2812 | Time 58.4754(58.0529) | Bit/dim 3.7020(3.7008) | Xent 1.0941(1.1039) | Loss 4.2491(4.2528) | Error 0.3881(0.3952) Steps 622(632.77) | Grad Norm 0.9878(0.7871) | Total Time 14.00(14.00)\n",
      "Iter 2813 | Time 58.6165(58.0698) | Bit/dim 3.6982(3.7007) | Xent 1.0973(1.1037) | Loss 4.2469(4.2526) | Error 0.3935(0.3951) Steps 610(632.09) | Grad Norm 0.9034(0.7906) | Total Time 14.00(14.00)\n",
      "Iter 2814 | Time 57.4568(58.0514) | Bit/dim 3.7006(3.7007) | Xent 1.0960(1.1035) | Loss 4.2486(4.2525) | Error 0.3985(0.3952) Steps 634(632.14) | Grad Norm 2.0757(0.8291) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0469 | Time 23.5950, Epoch Time 388.9427(391.5653), Bit/dim 3.7000(best: 3.6999), Xent 1.0620, Loss 4.2310, Error 0.3820(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2815 | Time 63.7768(58.2232) | Bit/dim 3.6968(3.7006) | Xent 1.0970(1.1033) | Loss 4.2453(4.2523) | Error 0.3880(0.3950) Steps 628(632.02) | Grad Norm 1.1067(0.8374) | Total Time 14.00(14.00)\n",
      "Iter 2816 | Time 55.5522(58.1431) | Bit/dim 3.7009(3.7006) | Xent 1.1154(1.1037) | Loss 4.2586(4.2525) | Error 0.3972(0.3951) Steps 616(631.54) | Grad Norm 0.6724(0.8325) | Total Time 14.00(14.00)\n",
      "Iter 2817 | Time 56.0311(58.0797) | Bit/dim 3.6921(3.7004) | Xent 1.0861(1.1031) | Loss 4.2352(4.2519) | Error 0.3859(0.3948) Steps 616(631.07) | Grad Norm 1.0485(0.8390) | Total Time 14.00(14.00)\n",
      "Iter 2818 | Time 58.1084(58.0806) | Bit/dim 3.6931(3.7001) | Xent 1.0717(1.1022) | Loss 4.2289(4.2513) | Error 0.3905(0.3947) Steps 646(631.52) | Grad Norm 1.5408(0.8600) | Total Time 14.00(14.00)\n",
      "Iter 2819 | Time 56.9701(58.0472) | Bit/dim 3.6932(3.6999) | Xent 1.0833(1.1016) | Loss 4.2349(4.2508) | Error 0.3879(0.3944) Steps 622(631.24) | Grad Norm 1.5050(0.8794) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 55.5669(57.9728) | Bit/dim 3.7137(3.7004) | Xent 1.1117(1.1019) | Loss 4.2695(4.2513) | Error 0.4018(0.3947) Steps 622(630.96) | Grad Norm 0.7479(0.8754) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0470 | Time 23.5804, Epoch Time 385.2688(391.3764), Bit/dim 3.6996(best: 3.6999), Xent 1.0609, Loss 4.2300, Error 0.3789(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2821 | Time 57.6280(57.9625) | Bit/dim 3.6965(3.7002) | Xent 1.1049(1.1020) | Loss 4.2489(4.2512) | Error 0.3919(0.3946) Steps 622(630.69) | Grad Norm 1.1038(0.8823) | Total Time 14.00(14.00)\n",
      "Iter 2822 | Time 59.7254(58.0154) | Bit/dim 3.6996(3.7002) | Xent 1.1014(1.1020) | Loss 4.2503(4.2512) | Error 0.4005(0.3948) Steps 634(630.79) | Grad Norm 1.4781(0.9002) | Total Time 14.00(14.00)\n",
      "Iter 2823 | Time 58.0016(58.0150) | Bit/dim 3.6964(3.7001) | Xent 1.0945(1.1018) | Loss 4.2436(4.2510) | Error 0.3861(0.3945) Steps 628(630.71) | Grad Norm 1.1468(0.9076) | Total Time 14.00(14.00)\n",
      "Iter 2824 | Time 58.7404(58.0367) | Bit/dim 3.6920(3.6999) | Xent 1.0900(1.1014) | Loss 4.2370(4.2506) | Error 0.3902(0.3944) Steps 622(630.44) | Grad Norm 0.9924(0.9101) | Total Time 14.00(14.00)\n",
      "Iter 2825 | Time 57.5590(58.0224) | Bit/dim 3.6988(3.6998) | Xent 1.0845(1.1009) | Loss 4.2411(4.2503) | Error 0.3841(0.3941) Steps 634(630.55) | Grad Norm 0.5212(0.8984) | Total Time 14.00(14.00)\n",
      "Iter 2826 | Time 59.0401(58.0529) | Bit/dim 3.7024(3.6999) | Xent 1.0891(1.1006) | Loss 4.2469(4.2502) | Error 0.3930(0.3940) Steps 610(629.93) | Grad Norm 1.5877(0.9191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0471 | Time 23.2404, Epoch Time 389.4226(391.3178), Bit/dim 3.6994(best: 3.6996), Xent 1.0610, Loss 4.2299, Error 0.3786(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2827 | Time 58.1643(58.0563) | Bit/dim 3.7013(3.6999) | Xent 1.0982(1.1005) | Loss 4.2504(4.2502) | Error 0.3996(0.3942) Steps 616(629.52) | Grad Norm 1.6269(0.9403) | Total Time 14.00(14.00)\n",
      "Iter 2828 | Time 58.9782(58.0839) | Bit/dim 3.6916(3.6997) | Xent 1.0779(1.0998) | Loss 4.2305(4.2496) | Error 0.3862(0.3940) Steps 628(629.47) | Grad Norm 1.1228(0.9458) | Total Time 14.00(14.00)\n",
      "Iter 2829 | Time 56.1560(58.0261) | Bit/dim 3.6919(3.6995) | Xent 1.0991(1.0998) | Loss 4.2414(4.2494) | Error 0.3925(0.3939) Steps 616(629.07) | Grad Norm 1.2809(0.9559) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 55.4039(57.9474) | Bit/dim 3.7062(3.6997) | Xent 1.0894(1.0995) | Loss 4.2509(4.2494) | Error 0.3956(0.3940) Steps 622(628.85) | Grad Norm 1.5961(0.9751) | Total Time 14.00(14.00)\n",
      "Iter 2831 | Time 54.3460(57.8394) | Bit/dim 3.7047(3.6998) | Xent 1.1000(1.0995) | Loss 4.2547(4.2496) | Error 0.3890(0.3938) Steps 628(628.83) | Grad Norm 1.1721(0.9810) | Total Time 14.00(14.00)\n",
      "Iter 2832 | Time 58.8115(57.8685) | Bit/dim 3.6962(3.6997) | Xent 1.0958(1.0994) | Loss 4.2441(4.2494) | Error 0.3904(0.3937) Steps 616(628.44) | Grad Norm 0.6936(0.9724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0472 | Time 23.4873, Epoch Time 381.0395(391.0095), Bit/dim 3.6994(best: 3.6994), Xent 1.0576, Loss 4.2282, Error 0.3779(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2833 | Time 56.1236(57.8162) | Bit/dim 3.7072(3.6999) | Xent 1.1004(1.0994) | Loss 4.2574(4.2496) | Error 0.3976(0.3938) Steps 640(628.79) | Grad Norm 1.1358(0.9773) | Total Time 14.00(14.00)\n",
      "Iter 2834 | Time 56.6044(57.7798) | Bit/dim 3.6909(3.6997) | Xent 1.0885(1.0991) | Loss 4.2352(4.2492) | Error 0.3879(0.3937) Steps 622(628.59) | Grad Norm 1.2911(0.9867) | Total Time 14.00(14.00)\n",
      "Iter 2835 | Time 57.4917(57.7712) | Bit/dim 3.6965(3.6996) | Xent 1.0985(1.0991) | Loss 4.2457(4.2491) | Error 0.3946(0.3937) Steps 610(628.03) | Grad Norm 1.3414(0.9973) | Total Time 14.00(14.00)\n",
      "Iter 2836 | Time 58.8036(57.8022) | Bit/dim 3.6990(3.6995) | Xent 1.0774(1.0984) | Loss 4.2377(4.2488) | Error 0.3805(0.3933) Steps 610(627.49) | Grad Norm 5.4632(1.1313) | Total Time 14.00(14.00)\n",
      "Iter 2837 | Time 59.4446(57.8514) | Bit/dim 3.6904(3.6993) | Xent 1.0756(1.0977) | Loss 4.2282(4.2481) | Error 0.3869(0.3931) Steps 634(627.68) | Grad Norm 1.4271(1.1402) | Total Time 14.00(14.00)\n",
      "Iter 2838 | Time 57.8086(57.8502) | Bit/dim 3.7013(3.6993) | Xent 1.0936(1.0976) | Loss 4.2481(4.2481) | Error 0.3922(0.3931) Steps 610(627.15) | Grad Norm 1.3490(1.1464) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0473 | Time 23.2094, Epoch Time 385.2612(390.8370), Bit/dim 3.6981(best: 3.6994), Xent 1.0569, Loss 4.2266, Error 0.3776(best: 0.3778)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2839 | Time 57.0964(57.8275) | Bit/dim 3.6877(3.6990) | Xent 1.1018(1.0977) | Loss 4.2386(4.2479) | Error 0.4002(0.3933) Steps 622(627.00) | Grad Norm 0.4291(1.1249) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 59.7103(57.8840) | Bit/dim 3.6976(3.6989) | Xent 1.0862(1.0974) | Loss 4.2407(4.2476) | Error 0.3905(0.3932) Steps 640(627.39) | Grad Norm 0.6202(1.1098) | Total Time 14.00(14.00)\n",
      "Iter 2841 | Time 59.2601(57.9253) | Bit/dim 3.7050(3.6991) | Xent 1.0948(1.0973) | Loss 4.2524(4.2478) | Error 0.3941(0.3932) Steps 616(627.05) | Grad Norm 0.5625(1.0934) | Total Time 14.00(14.00)\n",
      "Iter 2842 | Time 56.7910(57.8913) | Bit/dim 3.6993(3.6991) | Xent 1.0866(1.0970) | Loss 4.2426(4.2476) | Error 0.3905(0.3931) Steps 628(627.08) | Grad Norm 1.3634(1.1015) | Total Time 14.00(14.00)\n",
      "Iter 2843 | Time 55.8798(57.8309) | Bit/dim 3.6969(3.6991) | Xent 1.0752(1.0963) | Loss 4.2345(4.2472) | Error 0.3835(0.3929) Steps 616(626.74) | Grad Norm 0.6625(1.0883) | Total Time 14.00(14.00)\n",
      "Iter 2844 | Time 59.6475(57.8854) | Bit/dim 3.6953(3.6990) | Xent 1.0909(1.0962) | Loss 4.2408(4.2470) | Error 0.3885(0.3927) Steps 610(626.24) | Grad Norm 0.8705(1.0818) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0474 | Time 23.3635, Epoch Time 387.3950(390.7338), Bit/dim 3.6976(best: 3.6981), Xent 1.0539, Loss 4.2246, Error 0.3775(best: 0.3776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2845 | Time 57.9683(57.8879) | Bit/dim 3.6908(3.6987) | Xent 1.0924(1.0961) | Loss 4.2370(4.2467) | Error 0.3896(0.3926) Steps 634(626.47) | Grad Norm 0.5802(1.0667) | Total Time 14.00(14.00)\n",
      "Iter 2846 | Time 58.4222(57.9039) | Bit/dim 3.6959(3.6986) | Xent 1.0910(1.0959) | Loss 4.2414(4.2466) | Error 0.3904(0.3926) Steps 616(626.16) | Grad Norm 0.7574(1.0574) | Total Time 14.00(14.00)\n",
      "Iter 2847 | Time 56.2726(57.8550) | Bit/dim 3.6993(3.6986) | Xent 1.0729(1.0952) | Loss 4.2358(4.2463) | Error 0.3816(0.3922) Steps 634(626.40) | Grad Norm 0.9021(1.0528) | Total Time 14.00(14.00)\n",
      "Iter 2848 | Time 58.0328(57.8603) | Bit/dim 3.6956(3.6986) | Xent 1.0791(1.0947) | Loss 4.2351(4.2459) | Error 0.3859(0.3920) Steps 610(625.90) | Grad Norm 1.4177(1.0637) | Total Time 14.00(14.00)\n",
      "Iter 2849 | Time 58.9458(57.8929) | Bit/dim 3.6874(3.6982) | Xent 1.0851(1.0944) | Loss 4.2299(4.2454) | Error 0.3859(0.3919) Steps 616(625.61) | Grad Norm 1.0669(1.0638) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 57.8130(57.8905) | Bit/dim 3.7125(3.6986) | Xent 1.0920(1.0944) | Loss 4.2585(4.2458) | Error 0.3918(0.3919) Steps 610(625.14) | Grad Norm 0.7617(1.0548) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0475 | Time 23.3372, Epoch Time 386.0954(390.5946), Bit/dim 3.6973(best: 3.6976), Xent 1.0523, Loss 4.2234, Error 0.3753(best: 0.3775)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2851 | Time 55.7895(57.8275) | Bit/dim 3.6916(3.6984) | Xent 1.0768(1.0938) | Loss 4.2300(4.2454) | Error 0.3848(0.3916) Steps 634(625.40) | Grad Norm 1.0567(1.0548) | Total Time 14.00(14.00)\n",
      "Iter 2852 | Time 59.6292(57.8815) | Bit/dim 3.6975(3.6984) | Xent 1.0834(1.0935) | Loss 4.2392(4.2452) | Error 0.3841(0.3914) Steps 634(625.66) | Grad Norm 0.4123(1.0355) | Total Time 14.00(14.00)\n",
      "Iter 2853 | Time 56.5377(57.8412) | Bit/dim 3.6997(3.6984) | Xent 1.0880(1.0934) | Loss 4.2437(4.2451) | Error 0.3875(0.3913) Steps 616(625.37) | Grad Norm 0.5876(1.0221) | Total Time 14.00(14.00)\n",
      "Iter 2854 | Time 57.7365(57.8381) | Bit/dim 3.6956(3.6984) | Xent 1.0789(1.0929) | Loss 4.2350(4.2448) | Error 0.3860(0.3911) Steps 634(625.63) | Grad Norm 0.7493(1.0139) | Total Time 14.00(14.00)\n",
      "Iter 2855 | Time 54.9895(57.7526) | Bit/dim 3.6999(3.6984) | Xent 1.0963(1.0930) | Loss 4.2481(4.2449) | Error 0.3909(0.3911) Steps 646(626.24) | Grad Norm 0.7517(1.0060) | Total Time 14.00(14.00)\n",
      "Iter 2856 | Time 54.6365(57.6591) | Bit/dim 3.6913(3.6982) | Xent 1.0886(1.0929) | Loss 4.2356(4.2446) | Error 0.3915(0.3911) Steps 610(625.75) | Grad Norm 0.5501(0.9924) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0476 | Time 23.0654, Epoch Time 377.7861(390.2103), Bit/dim 3.6967(best: 3.6973), Xent 1.0537, Loss 4.2236, Error 0.3750(best: 0.3753)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2857 | Time 56.6179(57.6279) | Bit/dim 3.6937(3.6981) | Xent 1.0903(1.0928) | Loss 4.2389(4.2445) | Error 0.3874(0.3910) Steps 622(625.64) | Grad Norm 0.5682(0.9796) | Total Time 14.00(14.00)\n",
      "Iter 2858 | Time 55.1479(57.5535) | Bit/dim 3.6978(3.6980) | Xent 1.0757(1.0923) | Loss 4.2356(4.2442) | Error 0.3831(0.3908) Steps 634(625.89) | Grad Norm 0.6217(0.9689) | Total Time 14.00(14.00)\n",
      "Iter 2859 | Time 58.7548(57.5895) | Bit/dim 3.7000(3.6981) | Xent 1.0834(1.0920) | Loss 4.2417(4.2441) | Error 0.3869(0.3907) Steps 610(625.42) | Grad Norm 0.7901(0.9635) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 58.3107(57.6112) | Bit/dim 3.6887(3.6978) | Xent 1.0921(1.0920) | Loss 4.2348(4.2438) | Error 0.3924(0.3907) Steps 616(625.13) | Grad Norm 0.5178(0.9502) | Total Time 14.00(14.00)\n",
      "Iter 2861 | Time 55.6064(57.5510) | Bit/dim 3.6983(3.6978) | Xent 1.0773(1.0916) | Loss 4.2370(4.2436) | Error 0.3940(0.3908) Steps 610(624.68) | Grad Norm 0.5522(0.9382) | Total Time 14.00(14.00)\n",
      "Iter 2862 | Time 56.2666(57.5125) | Bit/dim 3.6988(3.6979) | Xent 1.1028(1.0919) | Loss 4.2501(4.2438) | Error 0.3938(0.3909) Steps 610(624.24) | Grad Norm 1.0305(0.9410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0477 | Time 23.1787, Epoch Time 379.3970(389.8859), Bit/dim 3.6976(best: 3.6967), Xent 1.0527, Loss 4.2240, Error 0.3745(best: 0.3750)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2863 | Time 59.6473(57.5765) | Bit/dim 3.6898(3.6976) | Xent 1.0871(1.0918) | Loss 4.2333(4.2435) | Error 0.3876(0.3908) Steps 628(624.35) | Grad Norm 1.0691(0.9448) | Total Time 14.00(14.00)\n",
      "Iter 2864 | Time 57.1201(57.5628) | Bit/dim 3.6982(3.6976) | Xent 1.0864(1.0916) | Loss 4.2414(4.2435) | Error 0.3866(0.3907) Steps 616(624.10) | Grad Norm 0.6508(0.9360) | Total Time 14.00(14.00)\n",
      "Iter 2865 | Time 55.6453(57.5053) | Bit/dim 3.6926(3.6975) | Xent 1.0738(1.0911) | Loss 4.2295(4.2430) | Error 0.3801(0.3904) Steps 622(624.04) | Grad Norm 0.4316(0.9209) | Total Time 14.00(14.00)\n",
      "Iter 2866 | Time 57.3337(57.5002) | Bit/dim 3.7025(3.6976) | Xent 1.0827(1.0908) | Loss 4.2439(4.2431) | Error 0.3882(0.3903) Steps 616(623.80) | Grad Norm 1.0980(0.9262) | Total Time 14.00(14.00)\n",
      "Iter 2867 | Time 57.7669(57.5082) | Bit/dim 3.6944(3.6975) | Xent 1.1154(1.0916) | Loss 4.2522(4.2433) | Error 0.3991(0.3906) Steps 616(623.56) | Grad Norm 1.3984(0.9404) | Total Time 14.00(14.00)\n",
      "Iter 2868 | Time 56.3296(57.4728) | Bit/dim 3.6917(3.6974) | Xent 1.0860(1.0914) | Loss 4.2347(4.2431) | Error 0.3881(0.3905) Steps 610(623.16) | Grad Norm 0.8094(0.9364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0478 | Time 23.1155, Epoch Time 382.7864(389.6730), Bit/dim 3.6964(best: 3.6967), Xent 1.0519, Loss 4.2224, Error 0.3759(best: 0.3745)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2869 | Time 56.6739(57.4488) | Bit/dim 3.7046(3.6976) | Xent 1.0734(1.0909) | Loss 4.2413(4.2430) | Error 0.3836(0.3903) Steps 610(622.76) | Grad Norm 0.8237(0.9331) | Total Time 14.00(14.00)\n",
      "Iter 2870 | Time 56.8298(57.4303) | Bit/dim 3.6876(3.6973) | Xent 1.0848(1.0907) | Loss 4.2300(4.2426) | Error 0.3870(0.3902) Steps 628(622.92) | Grad Norm 1.4626(0.9489) | Total Time 14.00(14.00)\n",
      "Iter 2871 | Time 57.3402(57.4276) | Bit/dim 3.6893(3.6970) | Xent 1.0949(1.0908) | Loss 4.2368(4.2425) | Error 0.3911(0.3902) Steps 622(622.89) | Grad Norm 1.4404(0.9637) | Total Time 14.00(14.00)\n",
      "Iter 2872 | Time 56.5794(57.4021) | Bit/dim 3.6987(3.6971) | Xent 1.0902(1.0908) | Loss 4.2438(4.2425) | Error 0.3940(0.3903) Steps 616(622.68) | Grad Norm 0.3540(0.9454) | Total Time 14.00(14.00)\n",
      "Iter 2873 | Time 56.1970(57.3660) | Bit/dim 3.6928(3.6970) | Xent 1.0975(1.0910) | Loss 4.2416(4.2425) | Error 0.3962(0.3905) Steps 622(622.66) | Grad Norm 1.3789(0.9584) | Total Time 14.00(14.00)\n",
      "Iter 2874 | Time 56.4007(57.3370) | Bit/dim 3.6982(3.6970) | Xent 1.0716(1.0904) | Loss 4.2340(4.2422) | Error 0.3796(0.3902) Steps 610(622.28) | Grad Norm 1.1944(0.9655) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0479 | Time 23.2976, Epoch Time 378.6751(389.3430), Bit/dim 3.6959(best: 3.6964), Xent 1.0503, Loss 4.2210, Error 0.3775(best: 0.3745)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2875 | Time 55.0703(57.2690) | Bit/dim 3.6951(3.6970) | Xent 1.0940(1.0905) | Loss 4.2421(4.2422) | Error 0.3861(0.3901) Steps 622(622.28) | Grad Norm 0.8071(0.9607) | Total Time 14.00(14.00)\n",
      "Iter 2876 | Time 58.0063(57.2911) | Bit/dim 3.6911(3.6968) | Xent 1.0707(1.0899) | Loss 4.2265(4.2417) | Error 0.3860(0.3899) Steps 610(621.91) | Grad Norm 0.4350(0.9450) | Total Time 14.00(14.00)\n",
      "Iter 2877 | Time 54.9842(57.2219) | Bit/dim 3.6975(3.6968) | Xent 1.0893(1.0899) | Loss 4.2421(4.2418) | Error 0.3902(0.3900) Steps 628(622.09) | Grad Norm 0.7252(0.9384) | Total Time 14.00(14.00)\n",
      "Iter 2878 | Time 56.0958(57.1881) | Bit/dim 3.6934(3.6967) | Xent 1.0824(1.0897) | Loss 4.2346(4.2415) | Error 0.3882(0.3899) Steps 640(622.63) | Grad Norm 1.2217(0.9469) | Total Time 14.00(14.00)\n",
      "Iter 2879 | Time 58.7763(57.2358) | Bit/dim 3.6951(3.6966) | Xent 1.0880(1.0896) | Loss 4.2391(4.2415) | Error 0.3860(0.3898) Steps 634(622.97) | Grad Norm 1.2992(0.9574) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 57.6754(57.2490) | Bit/dim 3.6920(3.6965) | Xent 1.0849(1.0895) | Loss 4.2344(4.2413) | Error 0.3874(0.3897) Steps 616(622.76) | Grad Norm 0.7923(0.9525) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0480 | Time 23.0783, Epoch Time 379.3603(389.0435), Bit/dim 3.6957(best: 3.6959), Xent 1.0496, Loss 4.2205, Error 0.3770(best: 0.3745)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2881 | Time 56.1192(57.2151) | Bit/dim 3.6910(3.6963) | Xent 1.0879(1.0894) | Loss 4.2349(4.2411) | Error 0.3955(0.3899) Steps 610(622.38) | Grad Norm 0.7849(0.9475) | Total Time 14.00(14.00)\n",
      "Iter 2882 | Time 57.0552(57.2103) | Bit/dim 3.7028(3.6965) | Xent 1.0978(1.0897) | Loss 4.2518(4.2414) | Error 0.3865(0.3898) Steps 628(622.55) | Grad Norm 1.1984(0.9550) | Total Time 14.00(14.00)\n",
      "Iter 2883 | Time 59.7907(57.2877) | Bit/dim 3.6997(3.6966) | Xent 1.0764(1.0893) | Loss 4.2379(4.2413) | Error 0.3882(0.3897) Steps 610(622.17) | Grad Norm 0.8384(0.9515) | Total Time 14.00(14.00)\n",
      "Iter 2884 | Time 59.9026(57.3661) | Bit/dim 3.6938(3.6965) | Xent 1.0682(1.0887) | Loss 4.2279(4.2409) | Error 0.3840(0.3896) Steps 610(621.80) | Grad Norm 0.9110(0.9503) | Total Time 14.00(14.00)\n",
      "Iter 2885 | Time 55.2288(57.3020) | Bit/dim 3.6956(3.6965) | Xent 1.0866(1.0886) | Loss 4.2389(4.2408) | Error 0.3846(0.3894) Steps 634(622.17) | Grad Norm 1.1677(0.9568) | Total Time 14.00(14.00)\n",
      "Iter 2886 | Time 59.4696(57.3671) | Bit/dim 3.6851(3.6962) | Xent 1.0617(1.0878) | Loss 4.2159(4.2401) | Error 0.3800(0.3891) Steps 610(621.80) | Grad Norm 1.1029(0.9612) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0481 | Time 22.9128, Epoch Time 385.9442(388.9506), Bit/dim 3.6953(best: 3.6957), Xent 1.0478, Loss 4.2191, Error 0.3739(best: 0.3745)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2887 | Time 59.0536(57.4176) | Bit/dim 3.6875(3.6959) | Xent 1.0738(1.0874) | Loss 4.2244(4.2396) | Error 0.3864(0.3891) Steps 640(622.35) | Grad Norm 0.9498(0.9608) | Total Time 14.00(14.00)\n",
      "Iter 2888 | Time 55.8481(57.3706) | Bit/dim 3.7030(3.6961) | Xent 1.0953(1.0876) | Loss 4.2507(4.2399) | Error 0.3936(0.3892) Steps 616(622.16) | Grad Norm 0.8588(0.9578) | Total Time 14.00(14.00)\n",
      "Iter 2889 | Time 55.4798(57.3138) | Bit/dim 3.6953(3.6961) | Xent 1.0774(1.0873) | Loss 4.2340(4.2398) | Error 0.3898(0.3892) Steps 610(621.80) | Grad Norm 1.9689(0.9881) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 56.0767(57.2767) | Bit/dim 3.6933(3.6960) | Xent 1.0833(1.0872) | Loss 4.2349(4.2396) | Error 0.3850(0.3891) Steps 622(621.80) | Grad Norm 1.4875(1.0031) | Total Time 14.00(14.00)\n",
      "Iter 2891 | Time 58.5995(57.3164) | Bit/dim 3.6953(3.6960) | Xent 1.0913(1.0873) | Loss 4.2410(4.2397) | Error 0.3920(0.3892) Steps 616(621.63) | Grad Norm 0.7672(0.9960) | Total Time 14.00(14.00)\n",
      "Iter 2892 | Time 57.4367(57.3200) | Bit/dim 3.6932(3.6959) | Xent 1.0561(1.0864) | Loss 4.2212(4.2391) | Error 0.3746(0.3887) Steps 634(622.00) | Grad Norm 1.4924(1.0109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0482 | Time 23.1756, Epoch Time 381.2123(388.7184), Bit/dim 3.6965(best: 3.6953), Xent 1.0462, Loss 4.2196, Error 0.3741(best: 0.3739)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2893 | Time 59.5114(57.3858) | Bit/dim 3.6955(3.6959) | Xent 1.0579(1.0855) | Loss 4.2245(4.2387) | Error 0.3802(0.3885) Steps 616(621.82) | Grad Norm 1.9479(1.0390) | Total Time 14.00(14.00)\n",
      "Iter 2894 | Time 58.2104(57.4105) | Bit/dim 3.7033(3.6961) | Xent 1.0782(1.0853) | Loss 4.2424(4.2388) | Error 0.3885(0.3885) Steps 622(621.82) | Grad Norm 0.8110(1.0322) | Total Time 14.00(14.00)\n",
      "Iter 2895 | Time 55.6425(57.3575) | Bit/dim 3.6961(3.6961) | Xent 1.0937(1.0856) | Loss 4.2429(4.2389) | Error 0.3921(0.3886) Steps 616(621.65) | Grad Norm 0.9764(1.0305) | Total Time 14.00(14.00)\n",
      "Iter 2896 | Time 55.7482(57.3092) | Bit/dim 3.6840(3.6958) | Xent 1.0867(1.0856) | Loss 4.2273(4.2386) | Error 0.3945(0.3888) Steps 610(621.30) | Grad Norm 0.7882(1.0232) | Total Time 14.00(14.00)\n",
      "Iter 2897 | Time 56.3434(57.2802) | Bit/dim 3.6922(3.6957) | Xent 1.0755(1.0853) | Loss 4.2300(4.2383) | Error 0.3881(0.3887) Steps 616(621.14) | Grad Norm 0.5356(1.0086) | Total Time 14.00(14.00)\n",
      "Iter 2898 | Time 58.3492(57.3123) | Bit/dim 3.6918(3.6955) | Xent 1.0814(1.0852) | Loss 4.2325(4.2381) | Error 0.3871(0.3887) Steps 610(620.81) | Grad Norm 1.1404(1.0126) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0483 | Time 23.2572, Epoch Time 382.6110(388.5352), Bit/dim 3.6958(best: 3.6953), Xent 1.0452, Loss 4.2184, Error 0.3738(best: 0.3739)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2899 | Time 59.5717(57.3801) | Bit/dim 3.6910(3.6954) | Xent 1.0751(1.0849) | Loss 4.2286(4.2378) | Error 0.3851(0.3886) Steps 610(620.48) | Grad Norm 0.7705(1.0053) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 56.4076(57.3509) | Bit/dim 3.7017(3.6956) | Xent 1.0813(1.0848) | Loss 4.2423(4.2380) | Error 0.3830(0.3884) Steps 610(620.17) | Grad Norm 0.5683(0.9922) | Total Time 14.00(14.00)\n",
      "Iter 2901 | Time 57.6362(57.3594) | Bit/dim 3.6957(3.6956) | Xent 1.0798(1.0846) | Loss 4.2356(4.2379) | Error 0.3938(0.3886) Steps 610(619.86) | Grad Norm 1.6724(1.0126) | Total Time 14.00(14.00)\n",
      "Iter 2902 | Time 59.2384(57.4158) | Bit/dim 3.6926(3.6955) | Xent 1.0807(1.0845) | Loss 4.2329(4.2377) | Error 0.3865(0.3885) Steps 610(619.57) | Grad Norm 1.0003(1.0122) | Total Time 14.00(14.00)\n",
      "Iter 2903 | Time 56.2939(57.3821) | Bit/dim 3.6974(3.6956) | Xent 1.0687(1.0840) | Loss 4.2317(4.2376) | Error 0.3802(0.3883) Steps 610(619.28) | Grad Norm 0.6797(1.0022) | Total Time 14.00(14.00)\n",
      "Iter 2904 | Time 56.5488(57.3571) | Bit/dim 3.6867(3.6953) | Xent 1.0623(1.0834) | Loss 4.2178(4.2370) | Error 0.3808(0.3880) Steps 634(619.72) | Grad Norm 0.6332(0.9912) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0484 | Time 23.1187, Epoch Time 384.1726(388.4043), Bit/dim 3.6940(best: 3.6953), Xent 1.0437, Loss 4.2159, Error 0.3728(best: 0.3738)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2905 | Time 55.9041(57.3136) | Bit/dim 3.6899(3.6951) | Xent 1.0944(1.0837) | Loss 4.2371(4.2370) | Error 0.3891(0.3881) Steps 628(619.97) | Grad Norm 0.4967(0.9763) | Total Time 14.00(14.00)\n",
      "Iter 2906 | Time 57.1553(57.3088) | Bit/dim 3.6949(3.6951) | Xent 1.0548(1.0828) | Loss 4.2223(4.2365) | Error 0.3764(0.3877) Steps 610(619.67) | Grad Norm 0.4167(0.9596) | Total Time 14.00(14.00)\n",
      "Iter 2907 | Time 57.5758(57.3168) | Bit/dim 3.6893(3.6950) | Xent 1.0717(1.0825) | Loss 4.2252(4.2362) | Error 0.3790(0.3875) Steps 616(619.56) | Grad Norm 0.7906(0.9545) | Total Time 14.00(14.00)\n",
      "Iter 2908 | Time 58.5689(57.3544) | Bit/dim 3.6988(3.6951) | Xent 1.0832(1.0825) | Loss 4.2404(4.2363) | Error 0.3862(0.3874) Steps 622(619.63) | Grad Norm 0.5862(0.9434) | Total Time 14.00(14.00)\n",
      "Iter 2909 | Time 57.3362(57.3538) | Bit/dim 3.6929(3.6950) | Xent 1.0716(1.0822) | Loss 4.2287(4.2361) | Error 0.3834(0.3873) Steps 610(619.34) | Grad Norm 0.7500(0.9376) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 55.1711(57.2884) | Bit/dim 3.6954(3.6950) | Xent 1.0650(1.0817) | Loss 4.2280(4.2359) | Error 0.3830(0.3872) Steps 610(619.06) | Grad Norm 0.8097(0.9338) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0485 | Time 23.2390, Epoch Time 380.4763(388.1665), Bit/dim 3.6935(best: 3.6940), Xent 1.0442, Loss 4.2156, Error 0.3725(best: 0.3728)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2911 | Time 56.9271(57.2775) | Bit/dim 3.6922(3.6949) | Xent 1.0575(1.0810) | Loss 4.2210(4.2354) | Error 0.3801(0.3870) Steps 616(618.97) | Grad Norm 0.5433(0.9221) | Total Time 14.00(14.00)\n",
      "Iter 2912 | Time 58.7538(57.3218) | Bit/dim 3.6895(3.6948) | Xent 1.0729(1.0807) | Loss 4.2259(4.2351) | Error 0.3835(0.3869) Steps 628(619.24) | Grad Norm 0.5242(0.9101) | Total Time 14.00(14.00)\n",
      "Iter 2913 | Time 54.6678(57.2422) | Bit/dim 3.6853(3.6945) | Xent 1.0764(1.0806) | Loss 4.2235(4.2348) | Error 0.3801(0.3867) Steps 616(619.15) | Grad Norm 0.6718(0.9030) | Total Time 14.00(14.00)\n",
      "Iter 2914 | Time 57.1692(57.2400) | Bit/dim 3.7028(3.6947) | Xent 1.0768(1.0805) | Loss 4.2412(4.2350) | Error 0.3855(0.3866) Steps 610(618.87) | Grad Norm 0.8340(0.9009) | Total Time 14.00(14.00)\n",
      "Iter 2915 | Time 56.8667(57.2288) | Bit/dim 3.6934(3.6947) | Xent 1.0879(1.0807) | Loss 4.2374(4.2350) | Error 0.3841(0.3866) Steps 634(619.33) | Grad Norm 0.7851(0.8974) | Total Time 14.00(14.00)\n",
      "Iter 2916 | Time 57.5330(57.2379) | Bit/dim 3.6884(3.6945) | Xent 1.0586(1.0800) | Loss 4.2177(4.2345) | Error 0.3796(0.3863) Steps 616(619.23) | Grad Norm 1.3067(0.9097) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0486 | Time 23.1067, Epoch Time 380.7756(387.9447), Bit/dim 3.6939(best: 3.6935), Xent 1.0409, Loss 4.2143, Error 0.3689(best: 0.3725)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2917 | Time 59.7124(57.3122) | Bit/dim 3.6905(3.6944) | Xent 1.0648(1.0796) | Loss 4.2229(4.2342) | Error 0.3795(0.3861) Steps 616(619.13) | Grad Norm 1.5487(0.9289) | Total Time 14.00(14.00)\n",
      "Iter 2918 | Time 56.5459(57.2892) | Bit/dim 3.6971(3.6945) | Xent 1.0731(1.0794) | Loss 4.2337(4.2342) | Error 0.3812(0.3860) Steps 616(619.04) | Grad Norm 0.9498(0.9295) | Total Time 14.00(14.00)\n",
      "Iter 2919 | Time 56.0364(57.2516) | Bit/dim 3.6893(3.6943) | Xent 1.0797(1.0794) | Loss 4.2292(4.2340) | Error 0.3856(0.3860) Steps 610(618.76) | Grad Norm 0.9073(0.9288) | Total Time 14.00(14.00)\n",
      "Iter 2920 | Time 59.1364(57.3081) | Bit/dim 3.6922(3.6942) | Xent 1.0653(1.0790) | Loss 4.2248(4.2337) | Error 0.3866(0.3860) Steps 610(618.50) | Grad Norm 0.6052(0.9191) | Total Time 14.00(14.00)\n",
      "Iter 2921 | Time 54.4361(57.2220) | Bit/dim 3.6954(3.6943) | Xent 1.0750(1.0788) | Loss 4.2329(4.2337) | Error 0.3830(0.3859) Steps 610(618.25) | Grad Norm 0.4195(0.9042) | Total Time 14.00(14.00)\n",
      "Iter 2922 | Time 54.3750(57.1366) | Bit/dim 3.6950(3.6943) | Xent 1.0641(1.0784) | Loss 4.2270(4.2335) | Error 0.3802(0.3857) Steps 610(618.00) | Grad Norm 0.5364(0.8931) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0487 | Time 23.1822, Epoch Time 379.0870(387.6790), Bit/dim 3.6943(best: 3.6935), Xent 1.0399, Loss 4.2143, Error 0.3688(best: 0.3689)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2923 | Time 56.2906(57.1112) | Bit/dim 3.6924(3.6942) | Xent 1.0760(1.0783) | Loss 4.2304(4.2334) | Error 0.3819(0.3856) Steps 616(617.94) | Grad Norm 0.5982(0.8843) | Total Time 14.00(14.00)\n",
      "Iter 2924 | Time 59.1794(57.1732) | Bit/dim 3.6977(3.6943) | Xent 1.0724(1.0782) | Loss 4.2339(4.2334) | Error 0.3856(0.3856) Steps 628(618.24) | Grad Norm 0.5198(0.8733) | Total Time 14.00(14.00)\n",
      "Iter 2925 | Time 56.0342(57.1391) | Bit/dim 3.6855(3.6941) | Xent 1.0595(1.0776) | Loss 4.2153(4.2329) | Error 0.3821(0.3855) Steps 610(617.99) | Grad Norm 0.6631(0.8670) | Total Time 14.00(14.00)\n",
      "Iter 2926 | Time 58.1652(57.1698) | Bit/dim 3.6992(3.6942) | Xent 1.0728(1.0775) | Loss 4.2356(4.2330) | Error 0.3815(0.3854) Steps 616(617.93) | Grad Norm 0.4137(0.8534) | Total Time 14.00(14.00)\n",
      "Iter 2927 | Time 56.7667(57.1577) | Bit/dim 3.6954(3.6943) | Xent 1.0747(1.0774) | Loss 4.2327(4.2330) | Error 0.3841(0.3854) Steps 616(617.88) | Grad Norm 0.4750(0.8421) | Total Time 14.00(14.00)\n",
      "Iter 2928 | Time 56.7449(57.1454) | Bit/dim 3.6814(3.6939) | Xent 1.0674(1.0771) | Loss 4.2151(4.2324) | Error 0.3825(0.3853) Steps 616(617.82) | Grad Norm 0.8334(0.8418) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0488 | Time 22.9518, Epoch Time 382.1061(387.5118), Bit/dim 3.6936(best: 3.6935), Xent 1.0422, Loss 4.2147, Error 0.3697(best: 0.3688)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2929 | Time 54.9839(57.0805) | Bit/dim 3.6897(3.6938) | Xent 1.0706(1.0769) | Loss 4.2249(4.2322) | Error 0.3774(0.3850) Steps 628(618.12) | Grad Norm 0.4552(0.8302) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 58.7595(57.1309) | Bit/dim 3.6996(3.6939) | Xent 1.0806(1.0770) | Loss 4.2399(4.2324) | Error 0.3846(0.3850) Steps 616(618.06) | Grad Norm 0.7643(0.8282) | Total Time 14.00(14.00)\n",
      "Iter 2931 | Time 55.5752(57.0842) | Bit/dim 3.6848(3.6937) | Xent 1.0596(1.0765) | Loss 4.2146(4.2319) | Error 0.3792(0.3849) Steps 616(618.00) | Grad Norm 0.6137(0.8218) | Total Time 14.00(14.00)\n",
      "Iter 2932 | Time 58.4068(57.1239) | Bit/dim 3.6895(3.6935) | Xent 1.0617(1.0760) | Loss 4.2204(4.2315) | Error 0.3854(0.3849) Steps 634(618.48) | Grad Norm 0.7306(0.8191) | Total Time 14.00(14.00)\n",
      "Iter 2933 | Time 55.1389(57.0643) | Bit/dim 3.6996(3.6937) | Xent 1.0741(1.0760) | Loss 4.2367(4.2317) | Error 0.3844(0.3849) Steps 616(618.40) | Grad Norm 0.6284(0.8133) | Total Time 14.00(14.00)\n",
      "Iter 2934 | Time 55.2732(57.0106) | Bit/dim 3.6860(3.6935) | Xent 1.0633(1.0756) | Loss 4.2176(4.2313) | Error 0.3711(0.3844) Steps 616(618.33) | Grad Norm 0.9442(0.8173) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0489 | Time 23.3565, Epoch Time 376.7918(387.1902), Bit/dim 3.6927(best: 3.6935), Xent 1.0401, Loss 4.2128, Error 0.3679(best: 0.3688)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2935 | Time 54.9170(56.9478) | Bit/dim 3.6887(3.6933) | Xent 1.0644(1.0753) | Loss 4.2209(4.2310) | Error 0.3804(0.3843) Steps 622(618.44) | Grad Norm 0.5006(0.8078) | Total Time 14.00(14.00)\n",
      "Iter 2936 | Time 58.5860(56.9969) | Bit/dim 3.6911(3.6933) | Xent 1.0531(1.0746) | Loss 4.2177(4.2306) | Error 0.3784(0.3841) Steps 634(618.91) | Grad Norm 0.8316(0.8085) | Total Time 14.00(14.00)\n",
      "Iter 2937 | Time 59.2317(57.0640) | Bit/dim 3.6881(3.6931) | Xent 1.0809(1.0748) | Loss 4.2285(4.2305) | Error 0.3814(0.3841) Steps 610(618.64) | Grad Norm 1.1147(0.8177) | Total Time 14.00(14.00)\n",
      "Iter 2938 | Time 57.2729(57.0703) | Bit/dim 3.6957(3.6932) | Xent 1.0625(1.0744) | Loss 4.2270(4.2304) | Error 0.3858(0.3841) Steps 610(618.38) | Grad Norm 0.6150(0.8116) | Total Time 14.00(14.00)\n",
      "Iter 2939 | Time 55.0632(57.0100) | Bit/dim 3.6884(3.6931) | Xent 1.0648(1.0741) | Loss 4.2208(4.2301) | Error 0.3808(0.3840) Steps 622(618.49) | Grad Norm 1.3081(0.8265) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 58.8492(57.0652) | Bit/dim 3.7000(3.6933) | Xent 1.0895(1.0746) | Loss 4.2447(4.2305) | Error 0.3944(0.3843) Steps 610(618.24) | Grad Norm 62.9112(2.6890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0490 | Time 23.0640, Epoch Time 382.6981(387.0555), Bit/dim 3.6932(best: 3.6927), Xent 1.0400, Loss 4.2132, Error 0.3675(best: 0.3679)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2941 | Time 55.7897(57.0270) | Bit/dim 3.7065(3.6937) | Xent 1.0775(1.0747) | Loss 4.2453(4.2310) | Error 0.3812(0.3842) Steps 616(618.17) | Grad Norm 0.9315(2.6363) | Total Time 14.00(14.00)\n",
      "Iter 2942 | Time 58.9644(57.0851) | Bit/dim 3.6894(3.6935) | Xent 1.0575(1.0741) | Loss 4.2181(4.2306) | Error 0.3781(0.3840) Steps 610(617.92) | Grad Norm 0.7107(2.5785) | Total Time 14.00(14.00)\n",
      "Iter 2943 | Time 55.3322(57.0325) | Bit/dim 3.6898(3.6934) | Xent 1.0770(1.0742) | Loss 4.2283(4.2305) | Error 0.3850(0.3841) Steps 616(617.87) | Grad Norm 0.6430(2.5205) | Total Time 14.00(14.00)\n",
      "Iter 2944 | Time 59.5548(57.1082) | Bit/dim 3.6901(3.6933) | Xent 1.0656(1.0740) | Loss 4.2229(4.2303) | Error 0.3770(0.3839) Steps 616(617.81) | Grad Norm 1.1682(2.4799) | Total Time 14.00(14.00)\n",
      "Iter 2945 | Time 55.5377(57.0610) | Bit/dim 3.6913(3.6933) | Xent 1.0775(1.0741) | Loss 4.2301(4.2303) | Error 0.3822(0.3838) Steps 610(617.58) | Grad Norm 0.4085(2.4178) | Total Time 14.00(14.00)\n",
      "Iter 2946 | Time 56.8797(57.0556) | Bit/dim 3.6838(3.6930) | Xent 1.0591(1.0736) | Loss 4.2133(4.2298) | Error 0.3731(0.3835) Steps 622(617.71) | Grad Norm 0.8644(2.3712) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0491 | Time 23.1661, Epoch Time 380.8624(386.8697), Bit/dim 3.6931(best: 3.6927), Xent 1.0396, Loss 4.2129, Error 0.3705(best: 0.3675)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2947 | Time 59.0089(57.1142) | Bit/dim 3.6899(3.6929) | Xent 1.0618(1.0733) | Loss 4.2208(4.2295) | Error 0.3782(0.3833) Steps 616(617.66) | Grad Norm 1278.1388(40.6442) | Total Time 14.00(14.00)\n",
      "Iter 2948 | Time 56.3987(57.0927) | Bit/dim 3.6879(3.6927) | Xent 1.0802(1.0735) | Loss 4.2280(4.2295) | Error 0.3872(0.3835) Steps 616(617.61) | Grad Norm 1.0649(39.4568) | Total Time 14.00(14.00)\n",
      "Iter 2949 | Time 56.7875(57.0836) | Bit/dim 3.6920(3.6927) | Xent 1.0499(1.0728) | Loss 4.2169(4.2291) | Error 0.3785(0.3833) Steps 610(617.38) | Grad Norm 1.2985(38.3121) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 54.9443(57.0194) | Bit/dim 3.6956(3.6928) | Xent 1.0687(1.0727) | Loss 4.2299(4.2291) | Error 0.3838(0.3833) Steps 610(617.16) | Grad Norm 0.5361(37.1788) | Total Time 14.00(14.00)\n",
      "Iter 2951 | Time 56.0246(56.9896) | Bit/dim 3.6887(3.6927) | Xent 1.0768(1.0728) | Loss 4.2271(4.2291) | Error 0.3885(0.3835) Steps 610(616.94) | Grad Norm 0.6754(36.0837) | Total Time 14.00(14.00)\n",
      "Iter 2952 | Time 57.2660(56.9979) | Bit/dim 3.6921(3.6927) | Xent 1.0598(1.0724) | Loss 4.2220(4.2288) | Error 0.3805(0.3834) Steps 610(616.74) | Grad Norm 0.7950(35.0250) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0492 | Time 22.8227, Epoch Time 378.8469(386.6290), Bit/dim 3.6918(best: 3.6927), Xent 1.0392, Loss 4.2114, Error 0.3674(best: 0.3675)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2953 | Time 59.7892(57.0816) | Bit/dim 3.6816(3.6923) | Xent 1.0585(1.0720) | Loss 4.2109(4.2283) | Error 0.3830(0.3834) Steps 610(616.53) | Grad Norm 1.0689(34.0063) | Total Time 14.00(14.00)\n",
      "Iter 2954 | Time 57.9325(57.1071) | Bit/dim 3.6944(3.6924) | Xent 1.0536(1.0714) | Loss 4.2212(4.2281) | Error 0.3820(0.3833) Steps 610(616.34) | Grad Norm 0.5821(33.0036) | Total Time 14.00(14.00)\n",
      "Iter 2955 | Time 57.5312(57.1198) | Bit/dim 3.6905(3.6923) | Xent 1.0617(1.0711) | Loss 4.2213(4.2279) | Error 0.3865(0.3834) Steps 616(616.33) | Grad Norm 0.9845(32.0430) | Total Time 14.00(14.00)\n",
      "Iter 2956 | Time 58.8278(57.1711) | Bit/dim 3.6935(3.6924) | Xent 1.0786(1.0714) | Loss 4.2328(4.2280) | Error 0.3841(0.3834) Steps 610(616.14) | Grad Norm 0.7889(31.1054) | Total Time 14.00(14.00)\n",
      "Iter 2957 | Time 54.5658(57.0929) | Bit/dim 3.6920(3.6924) | Xent 1.0632(1.0711) | Loss 4.2236(4.2279) | Error 0.3789(0.3833) Steps 622(616.31) | Grad Norm 1.6681(30.2223) | Total Time 14.00(14.00)\n",
      "Iter 2958 | Time 57.5324(57.1061) | Bit/dim 3.6885(3.6922) | Xent 1.0764(1.0713) | Loss 4.2267(4.2279) | Error 0.3798(0.3832) Steps 616(616.30) | Grad Norm 0.8589(29.3414) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0493 | Time 23.1381, Epoch Time 384.7784(386.5735), Bit/dim 3.6914(best: 3.6918), Xent 1.0377, Loss 4.2103, Error 0.3694(best: 0.3674)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2959 | Time 58.6354(57.1520) | Bit/dim 3.6851(3.6920) | Xent 1.0621(1.0710) | Loss 4.2161(4.2275) | Error 0.3788(0.3831) Steps 616(616.29) | Grad Norm 1.2762(28.4994) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 56.3230(57.1271) | Bit/dim 3.6927(3.6920) | Xent 1.0712(1.0710) | Loss 4.2283(4.2275) | Error 0.3864(0.3832) Steps 610(616.11) | Grad Norm 1.5093(27.6897) | Total Time 14.00(14.00)\n",
      "Iter 2961 | Time 55.5461(57.0797) | Bit/dim 3.6905(3.6920) | Xent 1.0722(1.0710) | Loss 4.2266(4.2275) | Error 0.3859(0.3832) Steps 640(616.82) | Grad Norm 0.9696(26.8881) | Total Time 14.00(14.00)\n",
      "Iter 2962 | Time 57.5041(57.0924) | Bit/dim 3.6891(3.6919) | Xent 1.0711(1.0710) | Loss 4.2247(4.2274) | Error 0.3806(0.3832) Steps 616(616.80) | Grad Norm 1.2254(26.1182) | Total Time 14.00(14.00)\n",
      "Iter 2963 | Time 54.8511(57.0252) | Bit/dim 3.6917(3.6919) | Xent 1.0601(1.0707) | Loss 4.2218(4.2273) | Error 0.3806(0.3831) Steps 610(616.59) | Grad Norm 1.4804(25.3791) | Total Time 14.00(14.00)\n",
      "Iter 2964 | Time 60.4635(57.1283) | Bit/dim 3.6929(3.6919) | Xent 1.0612(1.0704) | Loss 4.2235(4.2271) | Error 0.3776(0.3829) Steps 610(616.40) | Grad Norm 1.3136(24.6571) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0494 | Time 22.5876, Epoch Time 381.3728(386.4175), Bit/dim 3.6913(best: 3.6914), Xent 1.0352, Loss 4.2089, Error 0.3686(best: 0.3674)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2965 | Time 56.2282(57.1013) | Bit/dim 3.6945(3.6920) | Xent 1.0425(1.0696) | Loss 4.2157(4.2268) | Error 0.3675(0.3825) Steps 610(616.20) | Grad Norm 1.3179(23.9570) | Total Time 14.00(14.00)\n",
      "Iter 2966 | Time 54.8484(57.0337) | Bit/dim 3.6905(3.6920) | Xent 1.0590(1.0693) | Loss 4.2200(4.2266) | Error 0.3770(0.3823) Steps 610(616.02) | Grad Norm 0.9125(23.2656) | Total Time 14.00(14.00)\n",
      "Iter 2967 | Time 53.6175(56.9313) | Bit/dim 3.6848(3.6918) | Xent 1.0597(1.0690) | Loss 4.2146(4.2262) | Error 0.3785(0.3822) Steps 616(616.02) | Grad Norm 0.9602(22.5965) | Total Time 14.00(14.00)\n",
      "Iter 2968 | Time 55.8286(56.8982) | Bit/dim 3.6908(3.6917) | Xent 1.0631(1.0688) | Loss 4.2223(4.2261) | Error 0.3788(0.3821) Steps 622(616.20) | Grad Norm 1.4692(21.9627) | Total Time 14.00(14.00)\n",
      "Iter 2969 | Time 54.7700(56.8343) | Bit/dim 3.6869(3.6916) | Xent 1.0777(1.0691) | Loss 4.2258(4.2261) | Error 0.3872(0.3822) Steps 610(616.01) | Grad Norm 1.2737(21.3420) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 54.4827(56.7638) | Bit/dim 3.6885(3.6915) | Xent 1.0735(1.0692) | Loss 4.2253(4.2261) | Error 0.3828(0.3823) Steps 610(615.83) | Grad Norm 0.7043(20.7229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0495 | Time 23.0036, Epoch Time 368.3717(385.8761), Bit/dim 3.6916(best: 3.6913), Xent 1.0355, Loss 4.2093, Error 0.3667(best: 0.3674)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2971 | Time 56.4765(56.7552) | Bit/dim 3.6905(3.6915) | Xent 1.0791(1.0695) | Loss 4.2301(4.2262) | Error 0.3805(0.3822) Steps 616(615.84) | Grad Norm 1.4622(20.1450) | Total Time 14.00(14.00)\n",
      "Iter 2972 | Time 56.1650(56.7375) | Bit/dim 3.6885(3.6914) | Xent 1.0766(1.0697) | Loss 4.2268(4.2262) | Error 0.3852(0.3823) Steps 616(615.84) | Grad Norm 1.0634(19.5726) | Total Time 14.00(14.00)\n",
      "Iter 2973 | Time 55.1439(56.6896) | Bit/dim 3.6872(3.6912) | Xent 1.0635(1.0695) | Loss 4.2190(4.2260) | Error 0.3799(0.3822) Steps 628(616.21) | Grad Norm 0.4924(19.0002) | Total Time 14.00(14.00)\n",
      "Iter 2974 | Time 56.4092(56.6812) | Bit/dim 3.6845(3.6910) | Xent 1.0567(1.0691) | Loss 4.2128(4.2256) | Error 0.3796(0.3821) Steps 616(616.20) | Grad Norm 1.1301(18.4641) | Total Time 14.00(14.00)\n",
      "Iter 2975 | Time 57.4310(56.7037) | Bit/dim 3.6964(3.6912) | Xent 1.0601(1.0689) | Loss 4.2265(4.2256) | Error 0.3811(0.3821) Steps 628(616.55) | Grad Norm 1.0671(17.9422) | Total Time 14.00(14.00)\n",
      "Iter 2976 | Time 57.0813(56.7151) | Bit/dim 3.6852(3.6910) | Xent 1.0614(1.0686) | Loss 4.2159(4.2253) | Error 0.3801(0.3821) Steps 616(616.54) | Grad Norm 0.6827(17.4244) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0496 | Time 23.3482, Epoch Time 377.5613(385.6266), Bit/dim 3.6910(best: 3.6913), Xent 1.0326, Loss 4.2073, Error 0.3681(best: 0.3667)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2977 | Time 57.0715(56.7257) | Bit/dim 3.6789(3.6907) | Xent 1.0713(1.0687) | Loss 4.2146(4.2250) | Error 0.3860(0.3822) Steps 616(616.52) | Grad Norm 0.6704(16.9218) | Total Time 14.00(14.00)\n",
      "Iter 2978 | Time 57.0324(56.7349) | Bit/dim 3.6823(3.6904) | Xent 1.0658(1.0686) | Loss 4.2152(4.2247) | Error 0.3769(0.3820) Steps 622(616.69) | Grad Norm 1.2737(16.4523) | Total Time 14.00(14.00)\n",
      "Iter 2979 | Time 55.0237(56.6836) | Bit/dim 3.7001(3.6907) | Xent 1.0618(1.0684) | Loss 4.2310(4.2249) | Error 0.3902(0.3823) Steps 610(616.48) | Grad Norm 1.3071(15.9980) | Total Time 14.00(14.00)\n",
      "Iter 2980 | Time 56.3194(56.6727) | Bit/dim 3.6906(3.6907) | Xent 1.0586(1.0681) | Loss 4.2199(4.2248) | Error 0.3751(0.3820) Steps 610(616.29) | Grad Norm 0.6477(15.5375) | Total Time 14.00(14.00)\n",
      "Iter 2981 | Time 57.2956(56.6914) | Bit/dim 3.6866(3.6906) | Xent 1.0425(1.0674) | Loss 4.2078(4.2243) | Error 0.3651(0.3815) Steps 622(616.46) | Grad Norm 0.4974(15.0863) | Total Time 14.00(14.00)\n",
      "Iter 2982 | Time 59.1081(56.7639) | Bit/dim 3.6944(3.6907) | Xent 1.0660(1.0673) | Loss 4.2274(4.2244) | Error 0.3824(0.3816) Steps 628(616.81) | Grad Norm 0.9615(14.6625) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0497 | Time 23.4160, Epoch Time 380.7177(385.4794), Bit/dim 3.6912(best: 3.6910), Xent 1.0330, Loss 4.2077, Error 0.3654(best: 0.3667)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2983 | Time 57.6001(56.7890) | Bit/dim 3.6931(3.6908) | Xent 1.0449(1.0667) | Loss 4.2156(4.2241) | Error 0.3688(0.3812) Steps 610(616.60) | Grad Norm 0.4739(14.2369) | Total Time 14.00(14.00)\n",
      "Iter 2984 | Time 57.3904(56.8070) | Bit/dim 3.6933(3.6908) | Xent 1.0711(1.0668) | Loss 4.2288(4.2242) | Error 0.3785(0.3811) Steps 616(616.59) | Grad Norm 0.5133(13.8251) | Total Time 14.00(14.00)\n",
      "Iter 2985 | Time 57.9792(56.8422) | Bit/dim 3.6801(3.6905) | Xent 1.0637(1.0667) | Loss 4.2120(4.2239) | Error 0.3840(0.3812) Steps 616(616.57) | Grad Norm 0.6001(13.4284) | Total Time 14.00(14.00)\n",
      "Iter 2986 | Time 57.2397(56.8541) | Bit/dim 3.6865(3.6904) | Xent 1.0694(1.0668) | Loss 4.2212(4.2238) | Error 0.3839(0.3813) Steps 616(616.55) | Grad Norm 0.5029(13.0406) | Total Time 14.00(14.00)\n",
      "Iter 2987 | Time 56.6172(56.8470) | Bit/dim 3.6914(3.6904) | Xent 1.0554(1.0664) | Loss 4.2191(4.2236) | Error 0.3824(0.3813) Steps 616(616.53) | Grad Norm 0.4853(12.6640) | Total Time 14.00(14.00)\n",
      "Iter 2988 | Time 56.4842(56.8361) | Bit/dim 3.6930(3.6905) | Xent 1.0474(1.0659) | Loss 4.2167(4.2234) | Error 0.3742(0.3811) Steps 616(616.52) | Grad Norm 0.5802(12.3015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0498 | Time 23.3935, Epoch Time 382.1279(385.3788), Bit/dim 3.6907(best: 3.6910), Xent 1.0326, Loss 4.2070, Error 0.3655(best: 0.3654)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2989 | Time 59.4245(56.9138) | Bit/dim 3.6965(3.6907) | Xent 1.0473(1.0653) | Loss 4.2201(4.2233) | Error 0.3769(0.3810) Steps 616(616.50) | Grad Norm 0.6500(11.9519) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 56.4966(56.9012) | Bit/dim 3.6899(3.6907) | Xent 1.0538(1.0650) | Loss 4.2167(4.2231) | Error 0.3752(0.3808) Steps 616(616.49) | Grad Norm 0.7476(11.6158) | Total Time 14.00(14.00)\n",
      "Iter 2991 | Time 56.2197(56.8808) | Bit/dim 3.6889(3.6906) | Xent 1.0522(1.0646) | Loss 4.2150(4.2229) | Error 0.3712(0.3805) Steps 622(616.65) | Grad Norm 0.9122(11.2947) | Total Time 14.00(14.00)\n",
      "Iter 2992 | Time 59.5709(56.9615) | Bit/dim 3.6871(3.6905) | Xent 1.0759(1.0649) | Loss 4.2250(4.2230) | Error 0.3865(0.3807) Steps 616(616.63) | Grad Norm 0.4955(10.9707) | Total Time 14.00(14.00)\n",
      "Iter 2993 | Time 56.9163(56.9601) | Bit/dim 3.6906(3.6905) | Xent 1.0712(1.0651) | Loss 4.2262(4.2231) | Error 0.3755(0.3805) Steps 616(616.61) | Grad Norm 0.4385(10.6547) | Total Time 14.00(14.00)\n",
      "Iter 2994 | Time 55.3394(56.9115) | Bit/dim 3.6803(3.6902) | Xent 1.0575(1.0649) | Loss 4.2090(4.2226) | Error 0.3785(0.3805) Steps 622(616.78) | Grad Norm 0.7910(10.3588) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0499 | Time 23.3152, Epoch Time 382.8707(385.3036), Bit/dim 3.6902(best: 3.6907), Xent 1.0306, Loss 4.2055, Error 0.3643(best: 0.3654)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2995 | Time 55.4468(56.8676) | Bit/dim 3.6965(3.6904) | Xent 1.0444(1.0643) | Loss 4.2187(4.2225) | Error 0.3768(0.3804) Steps 616(616.75) | Grad Norm 0.4156(10.0605) | Total Time 14.00(14.00)\n",
      "Iter 2996 | Time 58.3536(56.9122) | Bit/dim 3.6811(3.6901) | Xent 1.0685(1.0644) | Loss 4.2154(4.2223) | Error 0.3790(0.3803) Steps 616(616.73) | Grad Norm 0.8522(9.7843) | Total Time 14.00(14.00)\n",
      "Iter 2997 | Time 58.0208(56.9454) | Bit/dim 3.6835(3.6899) | Xent 1.0651(1.0644) | Loss 4.2160(4.2221) | Error 0.3771(0.3802) Steps 616(616.71) | Grad Norm 0.7027(9.5118) | Total Time 14.00(14.00)\n",
      "Iter 2998 | Time 57.7861(56.9706) | Bit/dim 3.6827(3.6897) | Xent 1.0543(1.0641) | Loss 4.2099(4.2217) | Error 0.3739(0.3800) Steps 622(616.87) | Grad Norm 0.7762(9.2498) | Total Time 14.00(14.00)\n",
      "Iter 2999 | Time 54.8984(56.9085) | Bit/dim 3.6894(3.6897) | Xent 1.0657(1.0642) | Loss 4.2222(4.2218) | Error 0.3771(0.3799) Steps 622(617.02) | Grad Norm 0.9765(9.0016) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 60.0815(57.0037) | Bit/dim 3.6980(3.6899) | Xent 1.0526(1.0638) | Loss 4.2243(4.2218) | Error 0.3741(0.3798) Steps 616(616.99) | Grad Norm 0.5933(8.7493) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0500 | Time 23.1536, Epoch Time 383.0797(385.2369), Bit/dim 3.6903(best: 3.6902), Xent 1.0285, Loss 4.2045, Error 0.3674(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3001 | Time 56.8627(56.9994) | Bit/dim 3.7002(3.6902) | Xent 1.0625(1.0638) | Loss 4.2315(4.2221) | Error 0.3845(0.3799) Steps 622(617.14) | Grad Norm 0.5049(8.5020) | Total Time 14.00(14.00)\n",
      "Iter 3002 | Time 54.4652(56.9234) | Bit/dim 3.6880(3.6902) | Xent 1.0609(1.0637) | Loss 4.2185(4.2220) | Error 0.3769(0.3798) Steps 616(617.11) | Grad Norm 0.7657(8.2699) | Total Time 14.00(14.00)\n",
      "Iter 3003 | Time 56.5283(56.9115) | Bit/dim 3.6864(3.6901) | Xent 1.0513(1.0633) | Loss 4.2121(4.2217) | Error 0.3769(0.3797) Steps 616(617.07) | Grad Norm 0.8315(8.0467) | Total Time 14.00(14.00)\n",
      "Iter 3004 | Time 54.2194(56.8308) | Bit/dim 3.6880(3.6900) | Xent 1.0561(1.0631) | Loss 4.2160(4.2216) | Error 0.3742(0.3796) Steps 616(617.04) | Grad Norm 0.4398(7.8185) | Total Time 14.00(14.00)\n",
      "Iter 3005 | Time 56.7545(56.8285) | Bit/dim 3.6860(3.6899) | Xent 1.0588(1.0630) | Loss 4.2154(4.2214) | Error 0.3741(0.3794) Steps 622(617.19) | Grad Norm 0.6158(7.6024) | Total Time 14.00(14.00)\n",
      "Iter 3006 | Time 58.4856(56.8782) | Bit/dim 3.6851(3.6897) | Xent 1.0386(1.0622) | Loss 4.2044(4.2209) | Error 0.3750(0.3793) Steps 616(617.15) | Grad Norm 0.7268(7.3962) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0501 | Time 23.1770, Epoch Time 376.0977(384.9627), Bit/dim 3.6891(best: 3.6902), Xent 1.0271, Loss 4.2027, Error 0.3647(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3007 | Time 57.1633(56.8868) | Bit/dim 3.6938(3.6899) | Xent 1.0506(1.0619) | Loss 4.2191(4.2208) | Error 0.3700(0.3790) Steps 616(617.12) | Grad Norm 0.9218(7.2019) | Total Time 14.00(14.00)\n",
      "Iter 3008 | Time 54.2517(56.8077) | Bit/dim 3.7037(3.6903) | Xent 1.0669(1.0620) | Loss 4.2372(4.2213) | Error 0.3794(0.3790) Steps 622(617.27) | Grad Norm 0.5200(7.0015) | Total Time 14.00(14.00)\n",
      "Iter 3009 | Time 57.5080(56.8287) | Bit/dim 3.6741(3.6898) | Xent 1.0489(1.0616) | Loss 4.1986(4.2206) | Error 0.3749(0.3789) Steps 640(617.95) | Grad Norm 0.8141(6.8159) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 54.2140(56.7503) | Bit/dim 3.6793(3.6895) | Xent 1.0355(1.0609) | Loss 4.1970(4.2199) | Error 0.3728(0.3787) Steps 628(618.25) | Grad Norm 0.5252(6.6271) | Total Time 14.00(14.00)\n",
      "Iter 3011 | Time 55.1129(56.7012) | Bit/dim 3.6955(3.6897) | Xent 1.0567(1.0607) | Loss 4.2238(4.2200) | Error 0.3745(0.3786) Steps 616(618.18) | Grad Norm 1.0564(6.4600) | Total Time 14.00(14.00)\n",
      "Iter 3012 | Time 54.6359(56.6392) | Bit/dim 3.6862(3.6896) | Xent 1.0532(1.0605) | Loss 4.2128(4.2198) | Error 0.3739(0.3784) Steps 628(618.48) | Grad Norm 0.8376(6.2914) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0502 | Time 23.5014, Epoch Time 372.1666(384.5788), Bit/dim 3.6888(best: 3.6891), Xent 1.0268, Loss 4.2022, Error 0.3645(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3013 | Time 57.3398(56.6602) | Bit/dim 3.6905(3.6896) | Xent 1.0580(1.0604) | Loss 4.2195(4.2198) | Error 0.3736(0.3783) Steps 616(618.40) | Grad Norm 0.5482(6.1191) | Total Time 14.00(14.00)\n",
      "Iter 3014 | Time 58.0844(56.7029) | Bit/dim 3.7014(3.6899) | Xent 1.0391(1.0598) | Loss 4.2210(4.2198) | Error 0.3746(0.3782) Steps 616(618.33) | Grad Norm 0.4366(5.9486) | Total Time 14.00(14.00)\n",
      "Iter 3015 | Time 56.3741(56.6931) | Bit/dim 3.6837(3.6897) | Xent 1.0514(1.0595) | Loss 4.2094(4.2195) | Error 0.3774(0.3782) Steps 622(618.44) | Grad Norm 0.7389(5.7923) | Total Time 14.00(14.00)\n",
      "Iter 3016 | Time 54.0648(56.6142) | Bit/dim 3.6765(3.6893) | Xent 1.0501(1.0593) | Loss 4.2015(4.2190) | Error 0.3724(0.3780) Steps 616(618.37) | Grad Norm 1.2825(5.6570) | Total Time 14.00(14.00)\n",
      "Iter 3017 | Time 56.1811(56.6012) | Bit/dim 3.6841(3.6892) | Xent 1.0741(1.0597) | Loss 4.2211(4.2190) | Error 0.3865(0.3782) Steps 616(618.30) | Grad Norm 1.0534(5.5189) | Total Time 14.00(14.00)\n",
      "Iter 3018 | Time 57.0415(56.6144) | Bit/dim 3.6896(3.6892) | Xent 1.0573(1.0596) | Loss 4.2183(4.2190) | Error 0.3784(0.3782) Steps 622(618.41) | Grad Norm 1.4089(5.3956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0503 | Time 23.0857, Epoch Time 377.8041(384.3756), Bit/dim 3.6890(best: 3.6888), Xent 1.0273, Loss 4.2027, Error 0.3652(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3019 | Time 56.4366(56.6091) | Bit/dim 3.6891(3.6892) | Xent 1.0600(1.0596) | Loss 4.2191(4.2190) | Error 0.3809(0.3783) Steps 616(618.33) | Grad Norm 1.5889(5.2814) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 55.3666(56.5718) | Bit/dim 3.6809(3.6890) | Xent 1.0345(1.0589) | Loss 4.1981(4.2184) | Error 0.3680(0.3780) Steps 628(618.62) | Grad Norm 0.9328(5.1509) | Total Time 14.00(14.00)\n",
      "Iter 3021 | Time 56.5810(56.5721) | Bit/dim 3.6827(3.6888) | Xent 1.0517(1.0587) | Loss 4.2086(4.2181) | Error 0.3751(0.3779) Steps 622(618.73) | Grad Norm 1.2180(5.0329) | Total Time 14.00(14.00)\n",
      "Iter 3022 | Time 58.0296(56.6158) | Bit/dim 3.6987(3.6891) | Xent 1.0575(1.0586) | Loss 4.2275(4.2184) | Error 0.3820(0.3780) Steps 616(618.64) | Grad Norm 1.7870(4.9356) | Total Time 14.00(14.00)\n",
      "Iter 3023 | Time 55.1291(56.5712) | Bit/dim 3.6852(3.6889) | Xent 1.0784(1.0592) | Loss 4.2244(4.2186) | Error 0.3895(0.3784) Steps 634(619.10) | Grad Norm 0.9888(4.8172) | Total Time 14.00(14.00)\n",
      "Iter 3024 | Time 55.5705(56.5412) | Bit/dim 3.6912(3.6890) | Xent 1.0479(1.0589) | Loss 4.2151(4.2185) | Error 0.3730(0.3782) Steps 616(619.01) | Grad Norm 0.9652(4.7016) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0504 | Time 23.0985, Epoch Time 375.7089(384.1156), Bit/dim 3.6890(best: 3.6888), Xent 1.0266, Loss 4.2023, Error 0.3652(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3025 | Time 57.5267(56.5708) | Bit/dim 3.6910(3.6891) | Xent 1.0572(1.0588) | Loss 4.2196(4.2185) | Error 0.3806(0.3783) Steps 616(618.92) | Grad Norm 1.6925(4.6113) | Total Time 14.00(14.00)\n",
      "Iter 3026 | Time 56.7416(56.5759) | Bit/dim 3.6850(3.6890) | Xent 1.0410(1.0583) | Loss 4.2055(4.2181) | Error 0.3701(0.3781) Steps 616(618.83) | Grad Norm 1.5197(4.5186) | Total Time 14.00(14.00)\n",
      "Iter 3027 | Time 54.6869(56.5192) | Bit/dim 3.6901(3.6890) | Xent 1.0462(1.0579) | Loss 4.2132(4.2180) | Error 0.3729(0.3779) Steps 622(618.93) | Grad Norm 0.4753(4.3973) | Total Time 14.00(14.00)\n",
      "Iter 3028 | Time 56.7413(56.5259) | Bit/dim 3.6860(3.6889) | Xent 1.0614(1.0580) | Loss 4.2167(4.2179) | Error 0.3805(0.3780) Steps 616(618.84) | Grad Norm 1.2756(4.3036) | Total Time 14.00(14.00)\n",
      "Iter 3029 | Time 55.9760(56.5094) | Bit/dim 3.6853(3.6888) | Xent 1.0472(1.0577) | Loss 4.2089(4.2176) | Error 0.3715(0.3778) Steps 616(618.76) | Grad Norm 1.7638(4.2274) | Total Time 14.00(14.00)\n",
      "Iter 3030 | Time 55.5250(56.4799) | Bit/dim 3.6838(3.6886) | Xent 1.0674(1.0580) | Loss 4.2174(4.2176) | Error 0.3830(0.3779) Steps 616(618.67) | Grad Norm 0.5075(4.1158) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0505 | Time 22.8995, Epoch Time 375.5469(383.8585), Bit/dim 3.6892(best: 3.6888), Xent 1.0258, Loss 4.2021, Error 0.3642(best: 0.3643)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3031 | Time 57.8152(56.5199) | Bit/dim 3.6814(3.6884) | Xent 1.0514(1.0578) | Loss 4.2071(4.2173) | Error 0.3745(0.3778) Steps 622(618.77) | Grad Norm 1.3968(4.0343) | Total Time 14.00(14.00)\n",
      "Iter 3032 | Time 56.6421(56.5236) | Bit/dim 3.6866(3.6884) | Xent 1.0498(1.0576) | Loss 4.2115(4.2172) | Error 0.3764(0.3778) Steps 622(618.87) | Grad Norm 1.8097(3.9675) | Total Time 14.00(14.00)\n",
      "Iter 3033 | Time 58.0113(56.5682) | Bit/dim 3.6933(3.6885) | Xent 1.0379(1.0570) | Loss 4.2123(4.2170) | Error 0.3656(0.3774) Steps 616(618.78) | Grad Norm 0.9522(3.8771) | Total Time 14.00(14.00)\n",
      "Iter 3034 | Time 57.9180(56.6087) | Bit/dim 3.6861(3.6884) | Xent 1.0627(1.0572) | Loss 4.2174(4.2170) | Error 0.3809(0.3775) Steps 622(618.88) | Grad Norm 1.5026(3.8058) | Total Time 14.00(14.00)\n",
      "Iter 3035 | Time 54.5020(56.5455) | Bit/dim 3.6900(3.6885) | Xent 1.0491(1.0569) | Loss 4.2145(4.2169) | Error 0.3754(0.3775) Steps 616(618.79) | Grad Norm 1.7671(3.7447) | Total Time 14.00(14.00)\n",
      "Iter 3036 | Time 56.0754(56.5314) | Bit/dim 3.6861(3.6884) | Xent 1.0324(1.0562) | Loss 4.2023(4.2165) | Error 0.3686(0.3772) Steps 622(618.89) | Grad Norm 0.4053(3.6445) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0506 | Time 23.3199, Epoch Time 380.0606(383.7446), Bit/dim 3.6886(best: 3.6888), Xent 1.0241, Loss 4.2006, Error 0.3621(best: 0.3642)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3037 | Time 55.6979(56.5064) | Bit/dim 3.6883(3.6884) | Xent 1.0389(1.0557) | Loss 4.2077(4.2162) | Error 0.3750(0.3771) Steps 616(618.80) | Grad Norm 1.5766(3.5825) | Total Time 14.00(14.00)\n",
      "Iter 3038 | Time 54.9839(56.4607) | Bit/dim 3.6859(3.6883) | Xent 1.0661(1.0560) | Loss 4.2189(4.2163) | Error 0.3824(0.3773) Steps 616(618.72) | Grad Norm 0.7635(3.4979) | Total Time 14.00(14.00)\n",
      "Iter 3039 | Time 56.4608(56.4607) | Bit/dim 3.6915(3.6884) | Xent 1.0270(1.0551) | Loss 4.2050(4.2160) | Error 0.3665(0.3770) Steps 616(618.64) | Grad Norm 0.9987(3.4229) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 53.5781(56.3743) | Bit/dim 3.6902(3.6885) | Xent 1.0560(1.0551) | Loss 4.2182(4.2160) | Error 0.3796(0.3770) Steps 616(618.56) | Grad Norm 0.5694(3.3373) | Total Time 14.00(14.00)\n",
      "Iter 3041 | Time 57.5253(56.4088) | Bit/dim 3.6827(3.6883) | Xent 1.0449(1.0548) | Loss 4.2051(4.2157) | Error 0.3711(0.3769) Steps 634(619.02) | Grad Norm 1.4406(3.2804) | Total Time 14.00(14.00)\n",
      "Iter 3042 | Time 58.5862(56.4741) | Bit/dim 3.6859(3.6882) | Xent 1.0547(1.0548) | Loss 4.2132(4.2156) | Error 0.3784(0.3769) Steps 616(618.93) | Grad Norm 1.2117(3.2183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0507 | Time 23.2043, Epoch Time 375.7815(383.5057), Bit/dim 3.6885(best: 3.6886), Xent 1.0225, Loss 4.1998, Error 0.3639(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3043 | Time 58.2042(56.5260) | Bit/dim 3.6869(3.6882) | Xent 1.0282(1.0540) | Loss 4.2010(4.2152) | Error 0.3651(0.3766) Steps 616(618.84) | Grad Norm 1.2304(3.1587) | Total Time 14.00(14.00)\n",
      "Iter 3044 | Time 54.8606(56.4760) | Bit/dim 3.6748(3.6878) | Xent 1.0267(1.0532) | Loss 4.1881(4.2144) | Error 0.3639(0.3762) Steps 628(619.12) | Grad Norm 1.4595(3.1077) | Total Time 14.00(14.00)\n",
      "Iter 3045 | Time 57.8414(56.5170) | Bit/dim 3.6874(3.6878) | Xent 1.0733(1.0538) | Loss 4.2240(4.2147) | Error 0.3822(0.3764) Steps 616(619.02) | Grad Norm 0.6008(3.0325) | Total Time 14.00(14.00)\n",
      "Iter 3046 | Time 54.9949(56.4713) | Bit/dim 3.6936(3.6880) | Xent 1.0664(1.0542) | Loss 4.2268(4.2150) | Error 0.3839(0.3766) Steps 616(618.93) | Grad Norm 0.5643(2.9585) | Total Time 14.00(14.00)\n",
      "Iter 3047 | Time 57.5306(56.5031) | Bit/dim 3.6889(3.6880) | Xent 1.0576(1.0543) | Loss 4.2177(4.2151) | Error 0.3866(0.3769) Steps 622(619.03) | Grad Norm 1.2465(2.9071) | Total Time 14.00(14.00)\n",
      "Iter 3048 | Time 55.9306(56.4859) | Bit/dim 3.6894(3.6880) | Xent 1.0580(1.0544) | Loss 4.2184(4.2152) | Error 0.3815(0.3770) Steps 616(618.93) | Grad Norm 1.0121(2.8503) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0508 | Time 23.3667, Epoch Time 378.3494(383.3510), Bit/dim 3.6887(best: 3.6885), Xent 1.0230, Loss 4.2002, Error 0.3653(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3049 | Time 55.3172(56.4509) | Bit/dim 3.6851(3.6879) | Xent 1.0513(1.0543) | Loss 4.2108(4.2151) | Error 0.3745(0.3770) Steps 616(618.85) | Grad Norm 0.8547(2.7904) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 56.2075(56.4436) | Bit/dim 3.6875(3.6879) | Xent 1.0376(1.0538) | Loss 4.2063(4.2148) | Error 0.3724(0.3768) Steps 616(618.76) | Grad Norm 1.1854(2.7423) | Total Time 14.00(14.00)\n",
      "Iter 3051 | Time 56.9865(56.4599) | Bit/dim 3.6887(3.6879) | Xent 1.0585(1.0539) | Loss 4.2179(4.2149) | Error 0.3821(0.3770) Steps 616(618.68) | Grad Norm 1.0778(2.6923) | Total Time 14.00(14.00)\n",
      "Iter 3052 | Time 56.6530(56.4657) | Bit/dim 3.6944(3.6881) | Xent 1.0566(1.0540) | Loss 4.2227(4.2152) | Error 0.3729(0.3769) Steps 616(618.60) | Grad Norm 1.0951(2.6444) | Total Time 14.00(14.00)\n",
      "Iter 3053 | Time 57.9395(56.5099) | Bit/dim 3.6813(3.6879) | Xent 1.0556(1.0541) | Loss 4.2091(4.2150) | Error 0.3739(0.3768) Steps 616(618.52) | Grad Norm 1.1854(2.6006) | Total Time 14.00(14.00)\n",
      "Iter 3054 | Time 56.3114(56.5039) | Bit/dim 3.6778(3.6876) | Xent 1.0443(1.0538) | Loss 4.1999(4.2145) | Error 0.3686(0.3765) Steps 628(618.80) | Grad Norm 0.4769(2.5369) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0509 | Time 23.2111, Epoch Time 378.5647(383.2074), Bit/dim 3.6880(best: 3.6885), Xent 1.0212, Loss 4.1986, Error 0.3629(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3055 | Time 55.6017(56.4769) | Bit/dim 3.6857(3.6876) | Xent 1.0587(1.0539) | Loss 4.2150(4.2145) | Error 0.3794(0.3766) Steps 628(619.08) | Grad Norm 0.8615(2.4867) | Total Time 14.00(14.00)\n",
      "Iter 3056 | Time 55.9826(56.4620) | Bit/dim 3.6919(3.6877) | Xent 1.0519(1.0539) | Loss 4.2179(4.2146) | Error 0.3758(0.3766) Steps 616(618.99) | Grad Norm 1.5257(2.4578) | Total Time 14.00(14.00)\n",
      "Iter 3057 | Time 56.8745(56.4744) | Bit/dim 3.6917(3.6878) | Xent 1.0397(1.0534) | Loss 4.2116(4.2145) | Error 0.3736(0.3765) Steps 616(618.90) | Grad Norm 1.3468(2.4245) | Total Time 14.00(14.00)\n",
      "Iter 3058 | Time 57.3065(56.4994) | Bit/dim 3.6787(3.6875) | Xent 1.0565(1.0535) | Loss 4.2070(4.2143) | Error 0.3786(0.3766) Steps 616(618.81) | Grad Norm 1.0006(2.3818) | Total Time 14.00(14.00)\n",
      "Iter 3059 | Time 56.6530(56.5040) | Bit/dim 3.6854(3.6875) | Xent 1.0324(1.0529) | Loss 4.2016(4.2139) | Error 0.3685(0.3763) Steps 616(618.73) | Grad Norm 0.8172(2.3348) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 56.7228(56.5105) | Bit/dim 3.6793(3.6872) | Xent 1.0496(1.0528) | Loss 4.2042(4.2136) | Error 0.3789(0.3764) Steps 616(618.65) | Grad Norm 1.1175(2.2983) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0510 | Time 23.5689, Epoch Time 377.9765(383.0505), Bit/dim 3.6872(best: 3.6880), Xent 1.0214, Loss 4.1979, Error 0.3635(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3061 | Time 57.9796(56.5546) | Bit/dim 3.6772(3.6869) | Xent 1.0630(1.0531) | Loss 4.2087(4.2135) | Error 0.3772(0.3764) Steps 616(618.57) | Grad Norm 0.9523(2.2579) | Total Time 14.00(14.00)\n",
      "Iter 3062 | Time 59.3857(56.6395) | Bit/dim 3.6845(3.6869) | Xent 1.0563(1.0532) | Loss 4.2127(4.2135) | Error 0.3805(0.3765) Steps 622(618.67) | Grad Norm 1.0942(2.2230) | Total Time 14.00(14.00)\n",
      "Iter 3063 | Time 57.2729(56.6585) | Bit/dim 3.6975(3.6872) | Xent 1.0362(1.0527) | Loss 4.2156(4.2135) | Error 0.3718(0.3764) Steps 634(619.13) | Grad Norm 0.7558(2.1790) | Total Time 14.00(14.00)\n",
      "Iter 3064 | Time 58.1890(56.7045) | Bit/dim 3.6839(3.6871) | Xent 1.0395(1.0523) | Loss 4.2037(4.2132) | Error 0.3712(0.3762) Steps 616(619.03) | Grad Norm 1.1878(2.1493) | Total Time 14.00(14.00)\n",
      "Iter 3065 | Time 56.6394(56.7025) | Bit/dim 3.6891(3.6871) | Xent 1.0455(1.0521) | Loss 4.2119(4.2132) | Error 0.3686(0.3760) Steps 622(619.12) | Grad Norm 1.6038(2.1329) | Total Time 14.00(14.00)\n",
      "Iter 3066 | Time 58.2076(56.7477) | Bit/dim 3.6855(3.6871) | Xent 1.0556(1.0522) | Loss 4.2133(4.2132) | Error 0.3776(0.3761) Steps 616(619.03) | Grad Norm 1.3366(2.1090) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0511 | Time 23.3569, Epoch Time 386.8489(383.1644), Bit/dim 3.6876(best: 3.6872), Xent 1.0186, Loss 4.1969, Error 0.3632(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3067 | Time 58.0967(56.7881) | Bit/dim 3.6900(3.6872) | Xent 1.0466(1.0520) | Loss 4.2133(4.2132) | Error 0.3730(0.3760) Steps 628(619.30) | Grad Norm 1.0278(2.0766) | Total Time 14.00(14.00)\n",
      "Iter 3068 | Time 55.8572(56.7602) | Bit/dim 3.6883(3.6872) | Xent 1.0386(1.0516) | Loss 4.2076(4.2130) | Error 0.3722(0.3759) Steps 616(619.20) | Grad Norm 1.6323(2.0633) | Total Time 14.00(14.00)\n",
      "Iter 3069 | Time 57.6166(56.7859) | Bit/dim 3.6752(3.6869) | Xent 1.0406(1.0513) | Loss 4.1955(4.2125) | Error 0.3664(0.3756) Steps 616(619.10) | Grad Norm 1.3202(2.0410) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 55.5542(56.7489) | Bit/dim 3.6869(3.6869) | Xent 1.0400(1.0510) | Loss 4.2069(4.2123) | Error 0.3768(0.3756) Steps 616(619.01) | Grad Norm 0.8187(2.0043) | Total Time 14.00(14.00)\n",
      "Iter 3071 | Time 57.8932(56.7833) | Bit/dim 3.6855(3.6868) | Xent 1.0500(1.0509) | Loss 4.2104(4.2123) | Error 0.3722(0.3755) Steps 616(618.92) | Grad Norm 1.5842(1.9917) | Total Time 14.00(14.00)\n",
      "Iter 3072 | Time 57.0230(56.7905) | Bit/dim 3.6932(3.6870) | Xent 1.0622(1.0513) | Loss 4.2243(4.2126) | Error 0.3736(0.3754) Steps 604(618.47) | Grad Norm 1.7952(1.9858) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0512 | Time 23.4726, Epoch Time 381.0836(383.1020), Bit/dim 3.6869(best: 3.6872), Xent 1.0154, Loss 4.1946, Error 0.3597(best: 0.3621)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3073 | Time 56.5739(56.7840) | Bit/dim 3.6831(3.6869) | Xent 1.0551(1.0514) | Loss 4.2107(4.2126) | Error 0.3800(0.3756) Steps 622(618.58) | Grad Norm 0.5705(1.9433) | Total Time 14.00(14.00)\n",
      "Iter 3074 | Time 55.9721(56.7596) | Bit/dim 3.6898(3.6870) | Xent 1.0323(1.0508) | Loss 4.2060(4.2124) | Error 0.3731(0.3755) Steps 616(618.50) | Grad Norm 1.2646(1.9230) | Total Time 14.00(14.00)\n",
      "Iter 3075 | Time 58.3706(56.8079) | Bit/dim 3.6860(3.6869) | Xent 1.0444(1.0506) | Loss 4.2082(4.2123) | Error 0.3714(0.3754) Steps 616(618.43) | Grad Norm 1.1100(1.8986) | Total Time 14.00(14.00)\n",
      "Iter 3076 | Time 58.2263(56.8505) | Bit/dim 3.6778(3.6867) | Xent 1.0582(1.0508) | Loss 4.2069(4.2121) | Error 0.3754(0.3754) Steps 616(618.35) | Grad Norm 0.5289(1.8575) | Total Time 14.00(14.00)\n",
      "Iter 3077 | Time 56.3063(56.8342) | Bit/dim 3.6892(3.6867) | Xent 1.0397(1.0505) | Loss 4.2091(4.2120) | Error 0.3822(0.3756) Steps 616(618.28) | Grad Norm 0.5229(1.8175) | Total Time 14.00(14.00)\n",
      "Iter 3078 | Time 58.8623(56.8950) | Bit/dim 3.6912(3.6869) | Xent 1.0499(1.0505) | Loss 4.2161(4.2121) | Error 0.3752(0.3756) Steps 628(618.57) | Grad Norm 0.7802(1.7863) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0513 | Time 23.3504, Epoch Time 383.5451(383.1153), Bit/dim 3.6862(best: 3.6869), Xent 1.0170, Loss 4.1947, Error 0.3600(best: 0.3597)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3079 | Time 56.2760(56.8764) | Bit/dim 3.6757(3.6865) | Xent 1.0440(1.0503) | Loss 4.1977(4.2117) | Error 0.3746(0.3756) Steps 628(618.86) | Grad Norm 0.6783(1.7531) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 56.0078(56.8504) | Bit/dim 3.6905(3.6867) | Xent 1.0361(1.0499) | Loss 4.2086(4.2116) | Error 0.3689(0.3754) Steps 622(618.95) | Grad Norm 1.2261(1.7373) | Total Time 14.00(14.00)\n",
      "Iter 3081 | Time 57.2993(56.8638) | Bit/dim 3.6861(3.6866) | Xent 1.0635(1.0503) | Loss 4.2178(4.2118) | Error 0.3839(0.3756) Steps 616(618.86) | Grad Norm 0.7829(1.7087) | Total Time 14.00(14.00)\n",
      "Iter 3082 | Time 57.9035(56.8950) | Bit/dim 3.6847(3.6866) | Xent 1.0610(1.0506) | Loss 4.2152(4.2119) | Error 0.3829(0.3758) Steps 616(618.78) | Grad Norm 0.5174(1.6729) | Total Time 14.00(14.00)\n",
      "Iter 3083 | Time 56.4139(56.8806) | Bit/dim 3.6867(3.6866) | Xent 1.0414(1.0503) | Loss 4.2074(4.2118) | Error 0.3716(0.3757) Steps 616(618.69) | Grad Norm 0.8464(1.6481) | Total Time 14.00(14.00)\n",
      "Iter 3084 | Time 55.1012(56.8272) | Bit/dim 3.6965(3.6869) | Xent 1.0307(1.0497) | Loss 4.2119(4.2118) | Error 0.3639(0.3753) Steps 622(618.79) | Grad Norm 1.3767(1.6400) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0514 | Time 23.6278, Epoch Time 378.5550(382.9785), Bit/dim 3.6873(best: 3.6862), Xent 1.0157, Loss 4.1952, Error 0.3602(best: 0.3597)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3085 | Time 57.8383(56.8576) | Bit/dim 3.6876(3.6869) | Xent 1.0282(1.0491) | Loss 4.2017(4.2115) | Error 0.3704(0.3752) Steps 622(618.89) | Grad Norm 0.8015(1.6148) | Total Time 14.00(14.00)\n",
      "Iter 3086 | Time 59.5256(56.9376) | Bit/dim 3.6882(3.6869) | Xent 1.0240(1.0483) | Loss 4.2002(4.2111) | Error 0.3640(0.3749) Steps 622(618.98) | Grad Norm 0.7191(1.5879) | Total Time 14.00(14.00)\n",
      "Iter 3087 | Time 57.5167(56.9550) | Bit/dim 3.6762(3.6866) | Xent 1.0486(1.0483) | Loss 4.2005(4.2108) | Error 0.3788(0.3750) Steps 622(619.07) | Grad Norm 1.4039(1.5824) | Total Time 14.00(14.00)\n",
      "Iter 3088 | Time 57.1060(56.9595) | Bit/dim 3.6897(3.6867) | Xent 1.0469(1.0483) | Loss 4.2131(4.2109) | Error 0.3729(0.3749) Steps 610(618.80) | Grad Norm 0.7922(1.5587) | Total Time 14.00(14.00)\n",
      "Iter 3089 | Time 56.8275(56.9555) | Bit/dim 3.6893(3.6868) | Xent 1.0577(1.0486) | Loss 4.2182(4.2111) | Error 0.3838(0.3752) Steps 628(619.08) | Grad Norm 2.0644(1.5739) | Total Time 14.00(14.00)\n",
      "Iter 3090 | Time 56.2264(56.9337) | Bit/dim 3.6804(3.6866) | Xent 1.0079(1.0474) | Loss 4.1844(4.2103) | Error 0.3552(0.3746) Steps 616(618.98) | Grad Norm 0.5491(1.5431) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0515 | Time 23.2326, Epoch Time 383.8051(383.0033), Bit/dim 3.6863(best: 3.6862), Xent 1.0139, Loss 4.1933, Error 0.3605(best: 0.3597)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3091 | Time 55.8710(56.9018) | Bit/dim 3.6798(3.6864) | Xent 1.0273(1.0468) | Loss 4.1934(4.2098) | Error 0.3646(0.3743) Steps 616(618.90) | Grad Norm 0.6647(1.5168) | Total Time 14.00(14.00)\n",
      "Iter 3092 | Time 57.9668(56.9337) | Bit/dim 3.6859(3.6864) | Xent 1.0415(1.0466) | Loss 4.2067(4.2097) | Error 0.3678(0.3741) Steps 616(618.81) | Grad Norm 0.8725(1.4975) | Total Time 14.00(14.00)\n",
      "Iter 3093 | Time 57.5371(56.9518) | Bit/dim 3.6849(3.6863) | Xent 1.0436(1.0465) | Loss 4.2067(4.2096) | Error 0.3692(0.3739) Steps 604(618.36) | Grad Norm 0.8891(1.4792) | Total Time 14.00(14.00)\n",
      "Iter 3094 | Time 55.5007(56.9083) | Bit/dim 3.6795(3.6861) | Xent 1.0636(1.0470) | Loss 4.2113(4.2096) | Error 0.3846(0.3743) Steps 616(618.29) | Grad Norm 1.1659(1.4698) | Total Time 14.00(14.00)\n",
      "Iter 3095 | Time 56.1339(56.8851) | Bit/dim 3.6846(3.6861) | Xent 1.0575(1.0473) | Loss 4.2134(4.2098) | Error 0.3782(0.3744) Steps 622(618.40) | Grad Norm 1.6684(1.4758) | Total Time 14.00(14.00)\n",
      "Iter 3096 | Time 59.6840(56.9690) | Bit/dim 3.6938(3.6863) | Xent 1.0202(1.0465) | Loss 4.2039(4.2096) | Error 0.3615(0.3740) Steps 622(618.51) | Grad Norm 0.4287(1.4444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0516 | Time 23.2195, Epoch Time 381.9957(382.9731), Bit/dim 3.6865(best: 3.6862), Xent 1.0142, Loss 4.1936, Error 0.3595(best: 0.3597)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3097 | Time 57.6784(56.9903) | Bit/dim 3.6777(3.6861) | Xent 1.0473(1.0466) | Loss 4.2014(4.2093) | Error 0.3780(0.3741) Steps 622(618.62) | Grad Norm 1.8097(1.4553) | Total Time 14.00(14.00)\n",
      "Iter 3098 | Time 55.7703(56.9537) | Bit/dim 3.6886(3.6861) | Xent 1.0514(1.0467) | Loss 4.2143(4.2095) | Error 0.3721(0.3741) Steps 628(618.90) | Grad Norm 1.1692(1.4467) | Total Time 14.00(14.00)\n",
      "Iter 3099 | Time 57.8592(56.9809) | Bit/dim 3.6820(3.6860) | Xent 1.0410(1.0465) | Loss 4.2025(4.2093) | Error 0.3750(0.3741) Steps 622(618.99) | Grad Norm 0.6622(1.4232) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 53.4843(56.8760) | Bit/dim 3.6801(3.6858) | Xent 1.0427(1.0464) | Loss 4.2015(4.2090) | Error 0.3670(0.3739) Steps 616(618.90) | Grad Norm 1.5922(1.4283) | Total Time 14.00(14.00)\n",
      "Iter 3101 | Time 59.4615(56.9536) | Bit/dim 3.6932(3.6861) | Xent 1.0372(1.0461) | Loss 4.2118(4.2091) | Error 0.3670(0.3737) Steps 616(618.81) | Grad Norm 0.7521(1.4080) | Total Time 14.00(14.00)\n",
      "Iter 3102 | Time 59.7277(57.0368) | Bit/dim 3.6822(3.6859) | Xent 1.0225(1.0454) | Loss 4.1934(4.2087) | Error 0.3676(0.3735) Steps 622(618.91) | Grad Norm 0.6105(1.3841) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0517 | Time 23.1131, Epoch Time 382.6578(382.9636), Bit/dim 3.6856(best: 3.6862), Xent 1.0139, Loss 4.1925, Error 0.3621(best: 0.3595)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3103 | Time 57.7145(57.0571) | Bit/dim 3.6821(3.6858) | Xent 1.0491(1.0455) | Loss 4.2066(4.2086) | Error 0.3786(0.3736) Steps 604(618.46) | Grad Norm 0.5548(1.3592) | Total Time 14.00(14.00)\n",
      "Iter 3104 | Time 55.1718(57.0005) | Bit/dim 3.6879(3.6859) | Xent 1.0393(1.0454) | Loss 4.2075(4.2086) | Error 0.3684(0.3735) Steps 634(618.93) | Grad Norm 1.1760(1.3537) | Total Time 14.00(14.00)\n",
      "Iter 3105 | Time 57.3684(57.0116) | Bit/dim 3.6791(3.6857) | Xent 1.0346(1.0450) | Loss 4.1964(4.2082) | Error 0.3661(0.3733) Steps 616(618.84) | Grad Norm 0.5629(1.3300) | Total Time 14.00(14.00)\n",
      "Iter 3106 | Time 59.1202(57.0748) | Bit/dim 3.6883(3.6858) | Xent 1.0394(1.0449) | Loss 4.2081(4.2082) | Error 0.3756(0.3733) Steps 604(618.40) | Grad Norm 2.3866(1.3617) | Total Time 14.00(14.00)\n",
      "Iter 3107 | Time 58.8748(57.1288) | Bit/dim 3.6859(3.6858) | Xent 1.0357(1.0446) | Loss 4.2038(4.2081) | Error 0.3712(0.3733) Steps 622(618.50) | Grad Norm 0.6829(1.3413) | Total Time 14.00(14.00)\n",
      "Iter 3108 | Time 55.5613(57.0818) | Bit/dim 3.6783(3.6855) | Xent 1.0423(1.0445) | Loss 4.1994(4.2078) | Error 0.3711(0.3732) Steps 628(618.79) | Grad Norm 1.8746(1.3573) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0518 | Time 23.4848, Epoch Time 382.8422(382.9600), Bit/dim 3.6853(best: 3.6856), Xent 1.0134, Loss 4.1920, Error 0.3616(best: 0.3595)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3109 | Time 58.4526(57.1229) | Bit/dim 3.6911(3.6857) | Xent 1.0482(1.0446) | Loss 4.2152(4.2080) | Error 0.3768(0.3733) Steps 622(618.89) | Grad Norm 1.4615(1.3604) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 57.9309(57.1472) | Bit/dim 3.6817(3.6856) | Xent 1.0427(1.0446) | Loss 4.2030(4.2079) | Error 0.3730(0.3733) Steps 616(618.80) | Grad Norm 1.6260(1.3684) | Total Time 14.00(14.00)\n",
      "Iter 3111 | Time 57.0246(57.1435) | Bit/dim 3.6911(3.6858) | Xent 1.0381(1.0444) | Loss 4.2101(4.2079) | Error 0.3729(0.3733) Steps 604(618.35) | Grad Norm 1.7921(1.3811) | Total Time 14.00(14.00)\n",
      "Iter 3112 | Time 57.5487(57.1557) | Bit/dim 3.6736(3.6854) | Xent 1.0402(1.0442) | Loss 4.1937(4.2075) | Error 0.3666(0.3731) Steps 616(618.28) | Grad Norm 0.6511(1.3592) | Total Time 14.00(14.00)\n",
      "Iter 3113 | Time 56.4591(57.1348) | Bit/dim 3.6831(3.6853) | Xent 1.0287(1.0438) | Loss 4.1974(4.2072) | Error 0.3720(0.3731) Steps 616(618.22) | Grad Norm 1.0373(1.3495) | Total Time 14.00(14.00)\n",
      "Iter 3114 | Time 56.6181(57.1193) | Bit/dim 3.6879(3.6854) | Xent 1.0491(1.0439) | Loss 4.2124(4.2074) | Error 0.3778(0.3732) Steps 622(618.33) | Grad Norm 2.7322(1.3910) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0519 | Time 23.0996, Epoch Time 382.8031(382.9553), Bit/dim 3.6857(best: 3.6853), Xent 1.0128, Loss 4.1921, Error 0.3603(best: 0.3595)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3115 | Time 55.0812(57.0581) | Bit/dim 3.6821(3.6853) | Xent 1.0381(1.0438) | Loss 4.2012(4.2072) | Error 0.3739(0.3732) Steps 616(618.26) | Grad Norm 0.9512(1.3778) | Total Time 14.00(14.00)\n",
      "Iter 3116 | Time 57.9742(57.0856) | Bit/dim 3.6836(3.6852) | Xent 1.0375(1.0436) | Loss 4.2024(4.2070) | Error 0.3712(0.3732) Steps 604(617.83) | Grad Norm 2.4665(1.4105) | Total Time 14.00(14.00)\n",
      "Iter 3117 | Time 58.8693(57.1391) | Bit/dim 3.6833(3.6852) | Xent 1.0478(1.0437) | Loss 4.2072(4.2070) | Error 0.3628(0.3728) Steps 622(617.96) | Grad Norm 1.8922(1.4249) | Total Time 14.00(14.00)\n",
      "Iter 3118 | Time 56.5599(57.1217) | Bit/dim 3.6808(3.6851) | Xent 1.0419(1.0437) | Loss 4.2017(4.2069) | Error 0.3799(0.3731) Steps 616(617.90) | Grad Norm 0.7888(1.4059) | Total Time 14.00(14.00)\n",
      "Iter 3119 | Time 58.3358(57.1582) | Bit/dim 3.6861(3.6851) | Xent 1.0408(1.0436) | Loss 4.2065(4.2069) | Error 0.3710(0.3730) Steps 622(618.02) | Grad Norm 3.0873(1.4563) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 56.2804(57.1318) | Bit/dim 3.6869(3.6851) | Xent 1.0308(1.0432) | Loss 4.2023(4.2067) | Error 0.3658(0.3728) Steps 628(618.32) | Grad Norm 2.0495(1.4741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0520 | Time 23.1697, Epoch Time 381.6515(382.9161), Bit/dim 3.6852(best: 3.6853), Xent 1.0100, Loss 4.1903, Error 0.3582(best: 0.3595)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3121 | Time 55.6325(57.0868) | Bit/dim 3.6796(3.6850) | Xent 1.0431(1.0432) | Loss 4.2011(4.2066) | Error 0.3742(0.3728) Steps 604(617.89) | Grad Norm 0.7114(1.4512) | Total Time 14.00(14.00)\n",
      "Iter 3122 | Time 58.1687(57.1193) | Bit/dim 3.6877(3.6851) | Xent 1.0417(1.0431) | Loss 4.2085(4.2066) | Error 0.3746(0.3729) Steps 604(617.47) | Grad Norm 3.6249(1.5164) | Total Time 14.00(14.00)\n",
      "Iter 3123 | Time 56.3330(57.0957) | Bit/dim 3.6934(3.6853) | Xent 1.0279(1.0427) | Loss 4.2073(4.2066) | Error 0.3690(0.3728) Steps 616(617.43) | Grad Norm 2.0667(1.5329) | Total Time 14.00(14.00)\n",
      "Iter 3124 | Time 55.8347(57.0579) | Bit/dim 3.6806(3.6852) | Xent 1.0349(1.0424) | Loss 4.1981(4.2064) | Error 0.3766(0.3729) Steps 628(617.75) | Grad Norm 2.0224(1.5476) | Total Time 14.00(14.00)\n",
      "Iter 3125 | Time 57.6865(57.0767) | Bit/dim 3.6831(3.6851) | Xent 1.0203(1.0418) | Loss 4.1933(4.2060) | Error 0.3631(0.3726) Steps 622(617.87) | Grad Norm 3.1349(1.5952) | Total Time 14.00(14.00)\n",
      "Iter 3126 | Time 57.8081(57.0987) | Bit/dim 3.6808(3.6850) | Xent 1.0537(1.0421) | Loss 4.2077(4.2060) | Error 0.3798(0.3728) Steps 616(617.82) | Grad Norm 1.1910(1.5831) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0521 | Time 23.2589, Epoch Time 380.5981(382.8466), Bit/dim 3.6840(best: 3.6852), Xent 1.0105, Loss 4.1892, Error 0.3604(best: 0.3582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3127 | Time 57.0496(57.0972) | Bit/dim 3.6837(3.6849) | Xent 1.0237(1.0416) | Loss 4.1956(4.2057) | Error 0.3682(0.3727) Steps 622(617.94) | Grad Norm 1.6612(1.5854) | Total Time 14.00(14.00)\n",
      "Iter 3128 | Time 54.4708(57.0184) | Bit/dim 3.6952(3.6852) | Xent 1.0348(1.0414) | Loss 4.2126(4.2059) | Error 0.3696(0.3726) Steps 622(618.07) | Grad Norm 3.4317(1.6408) | Total Time 14.00(14.00)\n",
      "Iter 3129 | Time 57.3023(57.0269) | Bit/dim 3.6809(3.6851) | Xent 1.0340(1.0412) | Loss 4.1979(4.2057) | Error 0.3732(0.3726) Steps 604(617.64) | Grad Norm 0.8465(1.6170) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 55.1801(56.9715) | Bit/dim 3.6831(3.6851) | Xent 1.0408(1.0411) | Loss 4.2035(4.2056) | Error 0.3682(0.3725) Steps 628(617.95) | Grad Norm 2.5004(1.6435) | Total Time 14.00(14.00)\n",
      "Iter 3131 | Time 56.8936(56.9692) | Bit/dim 3.6758(3.6848) | Xent 1.0328(1.0409) | Loss 4.1922(4.2052) | Error 0.3659(0.3723) Steps 622(618.08) | Grad Norm 1.6655(1.6442) | Total Time 14.00(14.00)\n",
      "Iter 3132 | Time 56.5257(56.9559) | Bit/dim 3.6826(3.6847) | Xent 1.0347(1.0407) | Loss 4.2000(4.2051) | Error 0.3731(0.3723) Steps 616(618.01) | Grad Norm 0.9692(1.6239) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0522 | Time 23.4683, Epoch Time 376.4947(382.6560), Bit/dim 3.6850(best: 3.6840), Xent 1.0101, Loss 4.1901, Error 0.3604(best: 0.3582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3133 | Time 56.9481(56.9557) | Bit/dim 3.6823(3.6846) | Xent 1.0611(1.0413) | Loss 4.2128(4.2053) | Error 0.3739(0.3723) Steps 622(618.13) | Grad Norm 2.5160(1.6507) | Total Time 14.00(14.00)\n",
      "Iter 3134 | Time 57.0061(56.9572) | Bit/dim 3.6851(3.6847) | Xent 1.0120(1.0404) | Loss 4.1911(4.2049) | Error 0.3665(0.3722) Steps 628(618.43) | Grad Norm 1.3614(1.6420) | Total Time 14.00(14.00)\n",
      "Iter 3135 | Time 58.5089(57.0037) | Bit/dim 3.6813(3.6845) | Xent 1.0332(1.0402) | Loss 4.1979(4.2047) | Error 0.3666(0.3720) Steps 616(618.36) | Grad Norm 0.6589(1.6125) | Total Time 14.00(14.00)\n",
      "Iter 3136 | Time 56.4912(56.9883) | Bit/dim 3.6923(3.6848) | Xent 1.0252(1.0398) | Loss 4.2049(4.2047) | Error 0.3644(0.3718) Steps 616(618.29) | Grad Norm 1.7550(1.6168) | Total Time 14.00(14.00)\n",
      "Iter 3137 | Time 56.2918(56.9674) | Bit/dim 3.6782(3.6846) | Xent 1.0183(1.0391) | Loss 4.1873(4.2041) | Error 0.3662(0.3716) Steps 616(618.22) | Grad Norm 0.9772(1.5976) | Total Time 14.00(14.00)\n",
      "Iter 3138 | Time 55.6451(56.9278) | Bit/dim 3.6787(3.6844) | Xent 1.0353(1.0390) | Loss 4.1963(4.2039) | Error 0.3730(0.3716) Steps 616(618.15) | Grad Norm 0.7155(1.5711) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0523 | Time 23.3266, Epoch Time 379.9065(382.5736), Bit/dim 3.6841(best: 3.6840), Xent 1.0076, Loss 4.1879, Error 0.3600(best: 0.3582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3139 | Time 53.8959(56.8368) | Bit/dim 3.6844(3.6844) | Xent 1.0470(1.0393) | Loss 4.2079(4.2040) | Error 0.3735(0.3717) Steps 622(618.27) | Grad Norm 1.6417(1.5732) | Total Time 14.00(14.00)\n",
      "Iter 3140 | Time 55.7083(56.8030) | Bit/dim 3.6832(3.6844) | Xent 1.0232(1.0388) | Loss 4.1947(4.2038) | Error 0.3689(0.3716) Steps 634(618.74) | Grad Norm 1.0878(1.5587) | Total Time 14.00(14.00)\n",
      "Iter 3141 | Time 58.2894(56.8476) | Bit/dim 3.6808(3.6843) | Xent 1.0181(1.0382) | Loss 4.1899(4.2033) | Error 0.3661(0.3714) Steps 616(618.66) | Grad Norm 0.6382(1.5311) | Total Time 14.00(14.00)\n",
      "Iter 3142 | Time 57.7007(56.8732) | Bit/dim 3.6868(3.6843) | Xent 1.0406(1.0382) | Loss 4.2071(4.2035) | Error 0.3719(0.3715) Steps 628(618.94) | Grad Norm 1.1076(1.5184) | Total Time 14.00(14.00)\n",
      "Iter 3143 | Time 57.7795(56.9003) | Bit/dim 3.6886(3.6845) | Xent 1.0479(1.0385) | Loss 4.2126(4.2037) | Error 0.3774(0.3716) Steps 616(618.85) | Grad Norm 0.6425(1.4921) | Total Time 14.00(14.00)\n",
      "Iter 3144 | Time 60.9226(57.0210) | Bit/dim 3.6815(3.6844) | Xent 1.0271(1.0382) | Loss 4.1950(4.2035) | Error 0.3679(0.3715) Steps 622(618.94) | Grad Norm 0.8702(1.4734) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0524 | Time 23.2105, Epoch Time 383.0322(382.5873), Bit/dim 3.6840(best: 3.6840), Xent 1.0066, Loss 4.1874, Error 0.3568(best: 0.3582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3145 | Time 57.5701(57.0375) | Bit/dim 3.6826(3.6843) | Xent 1.0652(1.0390) | Loss 4.2152(4.2038) | Error 0.3842(0.3719) Steps 616(618.85) | Grad Norm 0.8728(1.4554) | Total Time 14.00(14.00)\n",
      "Iter 3146 | Time 55.5409(56.9926) | Bit/dim 3.6864(3.6844) | Xent 1.0193(1.0384) | Loss 4.1961(4.2036) | Error 0.3631(0.3716) Steps 622(618.95) | Grad Norm 0.8671(1.4378) | Total Time 14.00(14.00)\n",
      "Iter 3147 | Time 61.2958(57.1217) | Bit/dim 3.6760(3.6841) | Xent 1.0199(1.0378) | Loss 4.1860(4.2031) | Error 0.3629(0.3714) Steps 604(618.50) | Grad Norm 1.0799(1.4270) | Total Time 14.00(14.00)\n",
      "Iter 3148 | Time 58.5906(57.1657) | Bit/dim 3.6818(3.6841) | Xent 1.0263(1.0375) | Loss 4.1950(4.2028) | Error 0.3736(0.3714) Steps 604(618.07) | Grad Norm 0.6060(1.4024) | Total Time 14.00(14.00)\n",
      "Iter 3149 | Time 57.2276(57.1676) | Bit/dim 3.6910(3.6843) | Xent 1.0390(1.0375) | Loss 4.2105(4.2030) | Error 0.3669(0.3713) Steps 628(618.36) | Grad Norm 0.9246(1.3881) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 58.2665(57.2006) | Bit/dim 3.6774(3.6841) | Xent 1.0204(1.0370) | Loss 4.1876(4.2026) | Error 0.3656(0.3711) Steps 616(618.29) | Grad Norm 1.1047(1.3796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0525 | Time 23.6096, Epoch Time 387.6618(382.7395), Bit/dim 3.6843(best: 3.6840), Xent 1.0063, Loss 4.1874, Error 0.3561(best: 0.3568)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3151 | Time 56.9483(57.1930) | Bit/dim 3.6928(3.6843) | Xent 1.0210(1.0365) | Loss 4.2033(4.2026) | Error 0.3645(0.3709) Steps 622(618.40) | Grad Norm 0.8966(1.3651) | Total Time 14.00(14.00)\n",
      "Iter 3152 | Time 58.0929(57.2200) | Bit/dim 3.6752(3.6841) | Xent 1.0341(1.0365) | Loss 4.1922(4.2023) | Error 0.3701(0.3709) Steps 622(618.51) | Grad Norm 0.6825(1.3446) | Total Time 14.00(14.00)\n",
      "Iter 3153 | Time 59.4310(57.2863) | Bit/dim 3.6815(3.6840) | Xent 1.0303(1.0363) | Loss 4.1967(4.2021) | Error 0.3695(0.3709) Steps 604(618.08) | Grad Norm 1.3579(1.3450) | Total Time 14.00(14.00)\n",
      "Iter 3154 | Time 60.4508(57.3813) | Bit/dim 3.6847(3.6840) | Xent 1.0233(1.0359) | Loss 4.1964(4.2020) | Error 0.3679(0.3708) Steps 616(618.01) | Grad Norm 0.9992(1.3346) | Total Time 14.00(14.00)\n",
      "Iter 3155 | Time 60.6433(57.4791) | Bit/dim 3.6772(3.6838) | Xent 1.0440(1.0361) | Loss 4.1992(4.2019) | Error 0.3781(0.3710) Steps 646(618.85) | Grad Norm 1.4564(1.3383) | Total Time 14.00(14.00)\n",
      "Iter 3156 | Time 56.3655(57.4457) | Bit/dim 3.6831(3.6838) | Xent 1.0342(1.0361) | Loss 4.2001(4.2018) | Error 0.3750(0.3711) Steps 634(619.31) | Grad Norm 1.5114(1.3435) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0526 | Time 23.3708, Epoch Time 390.8468(382.9828), Bit/dim 3.6841(best: 3.6840), Xent 1.0065, Loss 4.1873, Error 0.3587(best: 0.3561)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3157 | Time 56.7177(57.4239) | Bit/dim 3.6743(3.6835) | Xent 1.0438(1.0363) | Loss 4.1962(4.2016) | Error 0.3719(0.3711) Steps 616(619.21) | Grad Norm 0.8266(1.3280) | Total Time 14.00(14.00)\n",
      "Iter 3158 | Time 55.4161(57.3636) | Bit/dim 3.6813(3.6834) | Xent 1.0180(1.0358) | Loss 4.1903(4.2013) | Error 0.3658(0.3710) Steps 604(618.75) | Grad Norm 1.9683(1.3472) | Total Time 14.00(14.00)\n",
      "Iter 3159 | Time 59.1195(57.4163) | Bit/dim 3.6774(3.6832) | Xent 1.0426(1.0360) | Loss 4.1988(4.2012) | Error 0.3644(0.3708) Steps 604(618.31) | Grad Norm 0.9423(1.3350) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 56.5549(57.3905) | Bit/dim 3.6863(3.6833) | Xent 1.0519(1.0364) | Loss 4.2122(4.2016) | Error 0.3785(0.3710) Steps 616(618.24) | Grad Norm 1.1259(1.3287) | Total Time 14.00(14.00)\n",
      "Iter 3161 | Time 54.7433(57.3111) | Bit/dim 3.6957(3.6837) | Xent 1.0361(1.0364) | Loss 4.2137(4.2019) | Error 0.3694(0.3710) Steps 616(618.17) | Grad Norm 1.0570(1.3206) | Total Time 14.00(14.00)\n",
      "Iter 3162 | Time 59.3682(57.3728) | Bit/dim 3.6868(3.6838) | Xent 1.0050(1.0355) | Loss 4.1893(4.2015) | Error 0.3612(0.3707) Steps 604(617.75) | Grad Norm 0.7591(1.3038) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0527 | Time 23.4028, Epoch Time 380.9005(382.9203), Bit/dim 3.6836(best: 3.6840), Xent 1.0031, Loss 4.1852, Error 0.3552(best: 0.3561)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3163 | Time 58.4155(57.4041) | Bit/dim 3.6848(3.6838) | Xent 1.0188(1.0350) | Loss 4.1942(4.2013) | Error 0.3628(0.3704) Steps 616(617.70) | Grad Norm 1.0666(1.2966) | Total Time 14.00(14.00)\n",
      "Iter 3164 | Time 57.2021(57.3980) | Bit/dim 3.6831(3.6838) | Xent 1.0082(1.0342) | Loss 4.1872(4.2009) | Error 0.3610(0.3702) Steps 628(618.00) | Grad Norm 0.5730(1.2749) | Total Time 14.00(14.00)\n",
      "Iter 3165 | Time 57.4060(57.3982) | Bit/dim 3.6857(3.6839) | Xent 1.0358(1.0342) | Loss 4.2036(4.2010) | Error 0.3684(0.3701) Steps 634(618.48) | Grad Norm 1.0248(1.2674) | Total Time 14.00(14.00)\n",
      "Iter 3166 | Time 55.3179(57.3358) | Bit/dim 3.6846(3.6839) | Xent 1.0284(1.0341) | Loss 4.1988(4.2009) | Error 0.3690(0.3701) Steps 628(618.77) | Grad Norm 0.4525(1.2430) | Total Time 14.00(14.00)\n",
      "Iter 3167 | Time 58.0210(57.3564) | Bit/dim 3.6864(3.6840) | Xent 1.0534(1.0346) | Loss 4.2131(4.2013) | Error 0.3754(0.3702) Steps 616(618.69) | Grad Norm 0.6809(1.2261) | Total Time 14.00(14.00)\n",
      "Iter 3168 | Time 58.9568(57.4044) | Bit/dim 3.6751(3.6837) | Xent 1.0062(1.0338) | Loss 4.1782(4.2006) | Error 0.3601(0.3699) Steps 622(618.79) | Grad Norm 0.9345(1.2174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0528 | Time 23.3641, Epoch Time 384.1054(382.9558), Bit/dim 3.6847(best: 3.6836), Xent 1.0024, Loss 4.1859, Error 0.3568(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3169 | Time 59.4129(57.4647) | Bit/dim 3.6862(3.6838) | Xent 1.0224(1.0335) | Loss 4.1975(4.2005) | Error 0.3622(0.3697) Steps 628(619.06) | Grad Norm 1.3307(1.2208) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 59.7058(57.5319) | Bit/dim 3.6719(3.6834) | Xent 1.0156(1.0329) | Loss 4.1797(4.1999) | Error 0.3668(0.3696) Steps 622(619.15) | Grad Norm 0.6002(1.2022) | Total Time 14.00(14.00)\n",
      "Iter 3171 | Time 57.1221(57.5196) | Bit/dim 3.6850(3.6835) | Xent 1.0414(1.0332) | Loss 4.2057(4.2001) | Error 0.3704(0.3696) Steps 634(619.60) | Grad Norm 1.2071(1.2023) | Total Time 14.00(14.00)\n",
      "Iter 3172 | Time 58.5414(57.5502) | Bit/dim 3.6839(3.6835) | Xent 1.0400(1.0334) | Loss 4.2040(4.2002) | Error 0.3689(0.3696) Steps 604(619.13) | Grad Norm 1.4446(1.2096) | Total Time 14.00(14.00)\n",
      "Iter 3173 | Time 58.4025(57.5758) | Bit/dim 3.6807(3.6834) | Xent 1.0225(1.0331) | Loss 4.1919(4.1999) | Error 0.3595(0.3693) Steps 616(619.03) | Grad Norm 0.9047(1.2004) | Total Time 14.00(14.00)\n",
      "Iter 3174 | Time 56.3764(57.5398) | Bit/dim 3.6882(3.6835) | Xent 1.0276(1.0329) | Loss 4.2020(4.2000) | Error 0.3662(0.3692) Steps 628(619.30) | Grad Norm 2.0125(1.2248) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0529 | Time 23.1549, Epoch Time 388.1273(383.1110), Bit/dim 3.6837(best: 3.6836), Xent 1.0020, Loss 4.1847, Error 0.3569(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3175 | Time 59.3923(57.5954) | Bit/dim 3.6800(3.6834) | Xent 1.0179(1.0324) | Loss 4.1890(4.1997) | Error 0.3614(0.3690) Steps 616(619.20) | Grad Norm 0.6026(1.2061) | Total Time 14.00(14.00)\n",
      "Iter 3176 | Time 60.0890(57.6702) | Bit/dim 3.6778(3.6833) | Xent 1.0102(1.0318) | Loss 4.1829(4.1991) | Error 0.3611(0.3687) Steps 616(619.11) | Grad Norm 2.2285(1.2368) | Total Time 14.00(14.00)\n",
      "Iter 3177 | Time 57.9595(57.6789) | Bit/dim 3.6774(3.6831) | Xent 1.0269(1.0316) | Loss 4.1908(4.1989) | Error 0.3678(0.3687) Steps 622(619.20) | Grad Norm 0.9036(1.2268) | Total Time 14.00(14.00)\n",
      "Iter 3178 | Time 62.1124(57.8119) | Bit/dim 3.6927(3.6834) | Xent 1.0330(1.0317) | Loss 4.2092(4.1992) | Error 0.3728(0.3688) Steps 604(618.74) | Grad Norm 1.4227(1.2327) | Total Time 14.00(14.00)\n",
      "Iter 3179 | Time 58.5595(57.8343) | Bit/dim 3.6834(3.6834) | Xent 1.0505(1.0322) | Loss 4.2086(4.1995) | Error 0.3699(0.3689) Steps 628(619.02) | Grad Norm 1.9068(1.2529) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 57.8231(57.8340) | Bit/dim 3.6802(3.6833) | Xent 1.0221(1.0319) | Loss 4.1912(4.1992) | Error 0.3650(0.3687) Steps 628(619.29) | Grad Norm 1.2131(1.2517) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0530 | Time 23.4784, Epoch Time 395.1740(383.4729), Bit/dim 3.6830(best: 3.6836), Xent 1.0021, Loss 4.1841, Error 0.3575(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3181 | Time 57.4595(57.8228) | Bit/dim 3.6785(3.6831) | Xent 1.0298(1.0319) | Loss 4.1934(4.1991) | Error 0.3690(0.3688) Steps 622(619.37) | Grad Norm 1.5216(1.2598) | Total Time 14.00(14.00)\n",
      "Iter 3182 | Time 56.6894(57.7888) | Bit/dim 3.6876(3.6833) | Xent 1.0224(1.0316) | Loss 4.1988(4.1991) | Error 0.3642(0.3686) Steps 634(619.81) | Grad Norm 2.9419(1.3103) | Total Time 14.00(14.00)\n",
      "Iter 3183 | Time 59.5344(57.8411) | Bit/dim 3.6855(3.6833) | Xent 1.0183(1.0312) | Loss 4.1946(4.1989) | Error 0.3565(0.3683) Steps 634(620.23) | Grad Norm 1.2139(1.3074) | Total Time 14.00(14.00)\n",
      "Iter 3184 | Time 59.0551(57.8775) | Bit/dim 3.6789(3.6832) | Xent 1.0240(1.0310) | Loss 4.1909(4.1987) | Error 0.3698(0.3683) Steps 622(620.29) | Grad Norm 1.5262(1.3139) | Total Time 14.00(14.00)\n",
      "Iter 3185 | Time 58.1928(57.8870) | Bit/dim 3.6820(3.6832) | Xent 1.0382(1.0312) | Loss 4.2011(4.1988) | Error 0.3742(0.3685) Steps 640(620.88) | Grad Norm 1.6865(1.3251) | Total Time 14.00(14.00)\n",
      "Iter 3186 | Time 59.4065(57.9326) | Bit/dim 3.6795(3.6831) | Xent 1.0072(1.0305) | Loss 4.1831(4.1983) | Error 0.3655(0.3684) Steps 610(620.55) | Grad Norm 0.9454(1.3137) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0531 | Time 23.5076, Epoch Time 389.3126(383.6481), Bit/dim 3.6830(best: 3.6830), Xent 1.0023, Loss 4.1842, Error 0.3568(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3187 | Time 57.6840(57.9251) | Bit/dim 3.6817(3.6830) | Xent 1.0253(1.0303) | Loss 4.1944(4.1982) | Error 0.3669(0.3683) Steps 616(620.41) | Grad Norm 2.4678(1.3483) | Total Time 14.00(14.00)\n",
      "Iter 3188 | Time 59.0899(57.9601) | Bit/dim 3.6960(3.6834) | Xent 1.0353(1.0305) | Loss 4.2137(4.1986) | Error 0.3724(0.3685) Steps 640(621.00) | Grad Norm 1.3340(1.3479) | Total Time 14.00(14.00)\n",
      "Iter 3189 | Time 57.4051(57.9434) | Bit/dim 3.6916(3.6837) | Xent 1.0309(1.0305) | Loss 4.2071(4.1989) | Error 0.3666(0.3684) Steps 634(621.39) | Grad Norm 2.3961(1.3794) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 57.8774(57.9414) | Bit/dim 3.6655(3.6831) | Xent 1.0424(1.0308) | Loss 4.1867(4.1985) | Error 0.3714(0.3685) Steps 640(621.95) | Grad Norm 1.8562(1.3937) | Total Time 14.00(14.00)\n",
      "Iter 3191 | Time 57.0937(57.9160) | Bit/dim 3.6813(3.6831) | Xent 1.0209(1.0305) | Loss 4.1918(4.1983) | Error 0.3604(0.3683) Steps 616(621.77) | Grad Norm 0.8869(1.3785) | Total Time 14.00(14.00)\n",
      "Iter 3192 | Time 57.8556(57.9142) | Bit/dim 3.6778(3.6829) | Xent 1.0013(1.0297) | Loss 4.1785(4.1977) | Error 0.3590(0.3680) Steps 646(622.50) | Grad Norm 2.2559(1.4048) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0532 | Time 23.4445, Epoch Time 385.9603(383.7174), Bit/dim 3.6825(best: 3.6830), Xent 1.0012, Loss 4.1831, Error 0.3565(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3193 | Time 55.7216(57.8484) | Bit/dim 3.6761(3.6827) | Xent 1.0209(1.0294) | Loss 4.1865(4.1974) | Error 0.3680(0.3680) Steps 628(622.66) | Grad Norm 2.0831(1.4251) | Total Time 14.00(14.00)\n",
      "Iter 3194 | Time 56.2646(57.8009) | Bit/dim 3.6783(3.6826) | Xent 1.0201(1.0291) | Loss 4.1884(4.1971) | Error 0.3646(0.3679) Steps 616(622.46) | Grad Norm 0.6103(1.4007) | Total Time 14.00(14.00)\n",
      "Iter 3195 | Time 58.7699(57.8300) | Bit/dim 3.6822(3.6826) | Xent 1.0284(1.0291) | Loss 4.1964(4.1971) | Error 0.3769(0.3681) Steps 622(622.45) | Grad Norm 2.4436(1.4320) | Total Time 14.00(14.00)\n",
      "Iter 3196 | Time 57.6925(57.8258) | Bit/dim 3.6850(3.6826) | Xent 1.0403(1.0294) | Loss 4.2051(4.1973) | Error 0.3695(0.3682) Steps 628(622.62) | Grad Norm 2.3136(1.4584) | Total Time 14.00(14.00)\n",
      "Iter 3197 | Time 57.2477(57.8085) | Bit/dim 3.6887(3.6828) | Xent 1.0346(1.0296) | Loss 4.2060(4.1976) | Error 0.3679(0.3682) Steps 604(622.06) | Grad Norm 1.6612(1.4645) | Total Time 14.00(14.00)\n",
      "Iter 3198 | Time 58.7414(57.8365) | Bit/dim 3.6786(3.6827) | Xent 1.0315(1.0296) | Loss 4.1944(4.1975) | Error 0.3684(0.3682) Steps 622(622.06) | Grad Norm 3.3611(1.5214) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0533 | Time 23.2238, Epoch Time 383.1909(383.7016), Bit/dim 3.6820(best: 3.6825), Xent 1.0011, Loss 4.1826, Error 0.3544(best: 0.3552)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3199 | Time 56.9756(57.8107) | Bit/dim 3.6737(3.6824) | Xent 1.0417(1.0300) | Loss 4.1945(4.1974) | Error 0.3721(0.3683) Steps 622(622.05) | Grad Norm 1.5188(1.5213) | Total Time 14.00(14.00)\n",
      "Iter 3200 | Time 58.1529(57.8209) | Bit/dim 3.6845(3.6825) | Xent 1.0281(1.0299) | Loss 4.1985(4.1974) | Error 0.3698(0.3683) Steps 634(622.41) | Grad Norm 3.1805(1.5711) | Total Time 14.00(14.00)\n",
      "Iter 3201 | Time 56.0678(57.7683) | Bit/dim 3.6808(3.6824) | Xent 1.0200(1.0296) | Loss 4.1908(4.1972) | Error 0.3610(0.3681) Steps 628(622.58) | Grad Norm 2.7900(1.6077) | Total Time 14.00(14.00)\n",
      "Iter 3202 | Time 59.5180(57.8208) | Bit/dim 3.6817(3.6824) | Xent 1.0164(1.0293) | Loss 4.1899(4.1970) | Error 0.3660(0.3681) Steps 640(623.10) | Grad Norm 1.0949(1.5923) | Total Time 14.00(14.00)\n",
      "Iter 3203 | Time 58.7112(57.8475) | Bit/dim 3.6848(3.6825) | Xent 1.0376(1.0295) | Loss 4.2036(4.1972) | Error 0.3691(0.3681) Steps 616(622.89) | Grad Norm 3.1943(1.6403) | Total Time 14.00(14.00)\n",
      "Iter 3204 | Time 59.0087(57.8824) | Bit/dim 3.6813(3.6824) | Xent 1.0247(1.0294) | Loss 4.1937(4.1971) | Error 0.3620(0.3679) Steps 634(623.22) | Grad Norm 1.7308(1.6431) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0534 | Time 23.0377, Epoch Time 387.0923(383.8034), Bit/dim 3.6826(best: 3.6820), Xent 1.0002, Loss 4.1827, Error 0.3572(best: 0.3544)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3205 | Time 56.4257(57.8387) | Bit/dim 3.6900(3.6827) | Xent 1.0404(1.0297) | Loss 4.2102(4.1975) | Error 0.3722(0.3680) Steps 634(623.55) | Grad Norm 1.9450(1.6521) | Total Time 14.00(14.00)\n",
      "Iter 3206 | Time 59.8718(57.8997) | Bit/dim 3.6752(3.6824) | Xent 1.0266(1.0296) | Loss 4.1885(4.1972) | Error 0.3612(0.3678) Steps 622(623.50) | Grad Norm 1.7973(1.6565) | Total Time 14.00(14.00)\n",
      "Iter 3207 | Time 60.7313(57.9846) | Bit/dim 3.6870(3.6826) | Xent 1.0296(1.0296) | Loss 4.2018(4.1974) | Error 0.3699(0.3679) Steps 604(622.91) | Grad Norm 1.0501(1.6383) | Total Time 14.00(14.00)\n",
      "Iter 3208 | Time 59.4181(58.0276) | Bit/dim 3.6734(3.6823) | Xent 1.0338(1.0297) | Loss 4.1903(4.1972) | Error 0.3685(0.3679) Steps 634(623.25) | Grad Norm 1.3353(1.6292) | Total Time 14.00(14.00)\n",
      "Iter 3209 | Time 55.8890(57.9635) | Bit/dim 3.6766(3.6821) | Xent 1.0248(1.0296) | Loss 4.1890(4.1969) | Error 0.3614(0.3677) Steps 622(623.21) | Grad Norm 2.4410(1.6535) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 58.5584(57.9813) | Bit/dim 3.6859(3.6822) | Xent 0.9939(1.0285) | Loss 4.1829(4.1965) | Error 0.3498(0.3672) Steps 610(622.81) | Grad Norm 1.2354(1.6410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0535 | Time 23.5063, Epoch Time 389.8051(383.9834), Bit/dim 3.6830(best: 3.6820), Xent 0.9999, Loss 4.1829, Error 0.3547(best: 0.3544)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3211 | Time 57.2497(57.9594) | Bit/dim 3.6902(3.6825) | Xent 1.0322(1.0286) | Loss 4.2063(4.1968) | Error 0.3718(0.3673) Steps 622(622.79) | Grad Norm 2.5630(1.6687) | Total Time 14.00(14.00)\n",
      "Iter 3212 | Time 59.1570(57.9953) | Bit/dim 3.6754(3.6823) | Xent 1.0192(1.0283) | Loss 4.1850(4.1964) | Error 0.3616(0.3671) Steps 616(622.59) | Grad Norm 2.1787(1.6840) | Total Time 14.00(14.00)\n",
      "Iter 3213 | Time 57.7926(57.9892) | Bit/dim 3.6856(3.6824) | Xent 1.0193(1.0281) | Loss 4.1953(4.1964) | Error 0.3630(0.3670) Steps 616(622.39) | Grad Norm 1.1443(1.6678) | Total Time 14.00(14.00)\n",
      "Iter 3214 | Time 59.7145(58.0410) | Bit/dim 3.6866(3.6825) | Xent 1.0364(1.0283) | Loss 4.2048(4.1967) | Error 0.3699(0.3671) Steps 622(622.38) | Grad Norm 1.8730(1.6739) | Total Time 14.00(14.00)\n",
      "Iter 3215 | Time 56.9901(58.0094) | Bit/dim 3.6731(3.6822) | Xent 1.0237(1.0282) | Loss 4.1850(4.1963) | Error 0.3666(0.3671) Steps 610(622.01) | Grad Norm 1.4497(1.6672) | Total Time 14.00(14.00)\n",
      "Iter 3216 | Time 59.2660(58.0471) | Bit/dim 3.6763(3.6820) | Xent 1.0029(1.0274) | Loss 4.1778(4.1958) | Error 0.3616(0.3669) Steps 616(621.83) | Grad Norm 0.9252(1.6449) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0536 | Time 23.5293, Epoch Time 389.0951(384.1368), Bit/dim 3.6828(best: 3.6820), Xent 0.9995, Loss 4.1825, Error 0.3543(best: 0.3544)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3217 | Time 56.5334(58.0017) | Bit/dim 3.6788(3.6819) | Xent 1.0465(1.0280) | Loss 4.2021(4.1959) | Error 0.3752(0.3672) Steps 634(622.19) | Grad Norm 2.8701(1.6817) | Total Time 14.00(14.00)\n",
      "Iter 3218 | Time 60.4892(58.0764) | Bit/dim 3.6837(3.6820) | Xent 1.0316(1.0281) | Loss 4.1995(4.1961) | Error 0.3639(0.3671) Steps 610(621.82) | Grad Norm 0.8485(1.6567) | Total Time 14.00(14.00)\n",
      "Iter 3219 | Time 57.0714(58.0462) | Bit/dim 3.6724(3.6817) | Xent 1.0061(1.0274) | Loss 4.1755(4.1954) | Error 0.3582(0.3668) Steps 616(621.65) | Grad Norm 1.3894(1.6487) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 58.3923(58.0566) | Bit/dim 3.6875(3.6819) | Xent 1.0210(1.0272) | Loss 4.1980(4.1955) | Error 0.3659(0.3668) Steps 634(622.02) | Grad Norm 2.8737(1.6854) | Total Time 14.00(14.00)\n",
      "Iter 3221 | Time 60.7126(58.1363) | Bit/dim 3.6813(3.6819) | Xent 1.0057(1.0266) | Loss 4.1842(4.1952) | Error 0.3609(0.3666) Steps 610(621.66) | Grad Norm 1.2605(1.6727) | Total Time 14.00(14.00)\n",
      "Iter 3222 | Time 59.5795(58.1796) | Bit/dim 3.6764(3.6817) | Xent 1.0326(1.0268) | Loss 4.1927(4.1951) | Error 0.3685(0.3667) Steps 616(621.49) | Grad Norm 2.7375(1.7046) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0537 | Time 23.2555, Epoch Time 391.8165(384.3672), Bit/dim 3.6824(best: 3.6820), Xent 0.9987, Loss 4.1817, Error 0.3561(best: 0.3543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3223 | Time 56.4531(58.1278) | Bit/dim 3.6695(3.6813) | Xent 1.0409(1.0272) | Loss 4.1899(4.1949) | Error 0.3738(0.3669) Steps 628(621.69) | Grad Norm 2.7803(1.7369) | Total Time 14.00(14.00)\n",
      "Iter 3224 | Time 59.0211(58.1546) | Bit/dim 3.6845(3.6814) | Xent 1.0222(1.0271) | Loss 4.1956(4.1950) | Error 0.3671(0.3669) Steps 640(622.23) | Grad Norm 0.7877(1.7084) | Total Time 14.00(14.00)\n",
      "Iter 3225 | Time 58.4301(58.1628) | Bit/dim 3.6897(3.6817) | Xent 1.0313(1.0272) | Loss 4.2053(4.1953) | Error 0.3651(0.3668) Steps 604(621.69) | Grad Norm 3.1738(1.7524) | Total Time 14.00(14.00)\n",
      "Iter 3226 | Time 58.6792(58.1783) | Bit/dim 3.6754(3.6815) | Xent 1.0076(1.0266) | Loss 4.1792(4.1948) | Error 0.3642(0.3668) Steps 604(621.16) | Grad Norm 1.7091(1.7511) | Total Time 14.00(14.00)\n",
      "Iter 3227 | Time 57.7669(58.1660) | Bit/dim 3.6812(3.6815) | Xent 1.0116(1.0261) | Loss 4.1870(4.1946) | Error 0.3651(0.3667) Steps 622(621.18) | Grad Norm 2.1247(1.7623) | Total Time 14.00(14.00)\n",
      "Iter 3228 | Time 62.0351(58.2821) | Bit/dim 3.6879(3.6817) | Xent 1.0099(1.0257) | Loss 4.1929(4.1945) | Error 0.3569(0.3664) Steps 622(621.21) | Grad Norm 2.5504(1.7859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0538 | Time 23.1929, Epoch Time 391.0781(384.5685), Bit/dim 3.6817(best: 3.6820), Xent 0.9934, Loss 4.1784, Error 0.3523(best: 0.3543)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3229 | Time 59.1832(58.3091) | Bit/dim 3.6735(3.6814) | Xent 1.0141(1.0253) | Loss 4.1806(4.1941) | Error 0.3612(0.3663) Steps 622(621.23) | Grad Norm 0.7152(1.7538) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 58.6349(58.3189) | Bit/dim 3.6719(3.6811) | Xent 1.0358(1.0256) | Loss 4.1898(4.1940) | Error 0.3666(0.3663) Steps 616(621.07) | Grad Norm 2.3565(1.7719) | Total Time 14.00(14.00)\n",
      "Iter 3231 | Time 57.5245(58.2950) | Bit/dim 3.6833(3.6812) | Xent 1.0066(1.0251) | Loss 4.1867(4.1937) | Error 0.3578(0.3660) Steps 604(620.56) | Grad Norm 1.3405(1.7590) | Total Time 14.00(14.00)\n",
      "Iter 3232 | Time 57.3354(58.2662) | Bit/dim 3.6789(3.6811) | Xent 1.0308(1.0252) | Loss 4.1943(4.1938) | Error 0.3684(0.3661) Steps 628(620.78) | Grad Norm 0.8156(1.7307) | Total Time 14.00(14.00)\n",
      "Iter 3233 | Time 59.7603(58.3111) | Bit/dim 3.6869(3.6813) | Xent 1.0098(1.0248) | Loss 4.1918(4.1937) | Error 0.3628(0.3660) Steps 622(620.82) | Grad Norm 1.2006(1.7148) | Total Time 14.00(14.00)\n",
      "Iter 3234 | Time 58.1880(58.3074) | Bit/dim 3.6810(3.6813) | Xent 1.0034(1.0241) | Loss 4.1827(4.1934) | Error 0.3540(0.3656) Steps 604(620.32) | Grad Norm 1.0979(1.6962) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0539 | Time 23.3747, Epoch Time 389.6917(384.7222), Bit/dim 3.6823(best: 3.6817), Xent 0.9949, Loss 4.1797, Error 0.3563(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3235 | Time 58.3204(58.3078) | Bit/dim 3.6844(3.6814) | Xent 1.0064(1.0236) | Loss 4.1876(4.1932) | Error 0.3648(0.3656) Steps 604(619.83) | Grad Norm 1.5637(1.6923) | Total Time 14.00(14.00)\n",
      "Iter 3236 | Time 59.5702(58.3456) | Bit/dim 3.6863(3.6815) | Xent 1.0124(1.0233) | Loss 4.1925(4.1932) | Error 0.3638(0.3655) Steps 634(620.25) | Grad Norm 1.5934(1.6893) | Total Time 14.00(14.00)\n",
      "Iter 3237 | Time 57.3007(58.3143) | Bit/dim 3.6862(3.6817) | Xent 1.0255(1.0233) | Loss 4.1990(4.1933) | Error 0.3641(0.3655) Steps 616(620.12) | Grad Norm 2.2296(1.7055) | Total Time 14.00(14.00)\n",
      "Iter 3238 | Time 56.5247(58.2606) | Bit/dim 3.6827(3.6817) | Xent 0.9932(1.0224) | Loss 4.1793(4.1929) | Error 0.3534(0.3651) Steps 640(620.72) | Grad Norm 1.0720(1.6865) | Total Time 14.00(14.00)\n",
      "Iter 3239 | Time 59.3550(58.2934) | Bit/dim 3.6775(3.6816) | Xent 1.0164(1.0222) | Loss 4.1857(4.1927) | Error 0.3620(0.3650) Steps 622(620.76) | Grad Norm 2.0222(1.6966) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 60.3207(58.3543) | Bit/dim 3.6707(3.6813) | Xent 1.0309(1.0225) | Loss 4.1862(4.1925) | Error 0.3698(0.3652) Steps 634(621.16) | Grad Norm 1.2929(1.6845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0540 | Time 23.4672, Epoch Time 390.4737(384.8947), Bit/dim 3.6817(best: 3.6817), Xent 0.9964, Loss 4.1798, Error 0.3550(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3241 | Time 57.5717(58.3308) | Bit/dim 3.6756(3.6811) | Xent 1.0131(1.0222) | Loss 4.1822(4.1922) | Error 0.3564(0.3649) Steps 604(620.64) | Grad Norm 1.2682(1.6720) | Total Time 14.00(14.00)\n",
      "Iter 3242 | Time 57.1553(58.2955) | Bit/dim 3.6801(3.6811) | Xent 1.0094(1.0218) | Loss 4.1848(4.1920) | Error 0.3641(0.3649) Steps 634(621.04) | Grad Norm 1.8606(1.6776) | Total Time 14.00(14.00)\n",
      "Iter 3243 | Time 59.6915(58.3374) | Bit/dim 3.6813(3.6811) | Xent 1.0129(1.0216) | Loss 4.1877(4.1919) | Error 0.3620(0.3648) Steps 622(621.07) | Grad Norm 0.9872(1.6569) | Total Time 14.00(14.00)\n",
      "Iter 3244 | Time 58.5059(58.3425) | Bit/dim 3.6893(3.6813) | Xent 1.0103(1.0212) | Loss 4.1945(4.1919) | Error 0.3586(0.3646) Steps 634(621.46) | Grad Norm 1.1267(1.6410) | Total Time 14.00(14.00)\n",
      "Iter 3245 | Time 58.0730(58.3344) | Bit/dim 3.6746(3.6811) | Xent 1.0229(1.0213) | Loss 4.1861(4.1918) | Error 0.3624(0.3646) Steps 628(621.66) | Grad Norm 0.7020(1.6128) | Total Time 14.00(14.00)\n",
      "Iter 3246 | Time 57.5840(58.3119) | Bit/dim 3.6823(3.6812) | Xent 1.0230(1.0213) | Loss 4.1938(4.1918) | Error 0.3705(0.3647) Steps 616(621.49) | Grad Norm 1.0840(1.5970) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0541 | Time 23.5762, Epoch Time 387.6204(384.9765), Bit/dim 3.6814(best: 3.6817), Xent 0.9932, Loss 4.1780, Error 0.3561(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3247 | Time 57.5901(58.2902) | Bit/dim 3.6751(3.6810) | Xent 1.0097(1.0210) | Loss 4.1800(4.1915) | Error 0.3660(0.3648) Steps 646(622.22) | Grad Norm 0.9519(1.5776) | Total Time 14.00(14.00)\n",
      "Iter 3248 | Time 59.8228(58.3362) | Bit/dim 3.6801(3.6809) | Xent 1.0025(1.0204) | Loss 4.1814(4.1912) | Error 0.3532(0.3644) Steps 610(621.85) | Grad Norm 0.5972(1.5482) | Total Time 14.00(14.00)\n",
      "Iter 3249 | Time 60.9529(58.4147) | Bit/dim 3.6816(3.6810) | Xent 1.0178(1.0204) | Loss 4.1905(4.1911) | Error 0.3631(0.3644) Steps 640(622.40) | Grad Norm 0.5081(1.5170) | Total Time 14.00(14.00)\n",
      "Iter 3250 | Time 58.8824(58.4287) | Bit/dim 3.6878(3.6812) | Xent 1.0312(1.0207) | Loss 4.2034(4.1915) | Error 0.3704(0.3646) Steps 646(623.11) | Grad Norm 0.7391(1.4937) | Total Time 14.00(14.00)\n",
      "Iter 3251 | Time 58.1683(58.4209) | Bit/dim 3.6887(3.6814) | Xent 1.0148(1.0205) | Loss 4.1961(4.1916) | Error 0.3601(0.3644) Steps 634(623.43) | Grad Norm 0.9401(1.4771) | Total Time 14.00(14.00)\n",
      "Iter 3252 | Time 59.8632(58.4642) | Bit/dim 3.6681(3.6810) | Xent 1.0098(1.0202) | Loss 4.1729(4.1911) | Error 0.3591(0.3643) Steps 646(624.11) | Grad Norm 0.7375(1.4549) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0542 | Time 23.1309, Epoch Time 393.9326(385.2452), Bit/dim 3.6810(best: 3.6814), Xent 0.9923, Loss 4.1772, Error 0.3533(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3253 | Time 60.6330(58.5292) | Bit/dim 3.6763(3.6809) | Xent 1.0220(1.0202) | Loss 4.1873(4.1910) | Error 0.3658(0.3643) Steps 604(623.51) | Grad Norm 0.6499(1.4307) | Total Time 14.00(14.00)\n",
      "Iter 3254 | Time 60.8192(58.5979) | Bit/dim 3.6778(3.6808) | Xent 1.0075(1.0199) | Loss 4.1815(4.1907) | Error 0.3615(0.3642) Steps 628(623.64) | Grad Norm 1.5047(1.4329) | Total Time 14.00(14.00)\n",
      "Iter 3255 | Time 58.0107(58.5803) | Bit/dim 3.6894(3.6810) | Xent 1.0172(1.0198) | Loss 4.1980(4.1909) | Error 0.3659(0.3643) Steps 640(624.13) | Grad Norm 0.8399(1.4152) | Total Time 14.00(14.00)\n",
      "Iter 3256 | Time 60.4682(58.6370) | Bit/dim 3.6857(3.6812) | Xent 1.0143(1.0196) | Loss 4.1929(4.1910) | Error 0.3591(0.3641) Steps 616(623.89) | Grad Norm 1.7721(1.4259) | Total Time 14.00(14.00)\n",
      "Iter 3257 | Time 58.3551(58.6285) | Bit/dim 3.6830(3.6812) | Xent 1.0174(1.0195) | Loss 4.1917(4.1910) | Error 0.3722(0.3644) Steps 646(624.55) | Grad Norm 1.0218(1.4137) | Total Time 14.00(14.00)\n",
      "Iter 3258 | Time 62.1274(58.7335) | Bit/dim 3.6698(3.6809) | Xent 1.0241(1.0197) | Loss 4.1819(4.1907) | Error 0.3644(0.3644) Steps 646(625.20) | Grad Norm 1.1401(1.4055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0543 | Time 23.4900, Epoch Time 399.4090(385.6701), Bit/dim 3.6815(best: 3.6810), Xent 0.9910, Loss 4.1770, Error 0.3526(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3259 | Time 57.8744(58.7077) | Bit/dim 3.6696(3.6805) | Xent 1.0226(1.0198) | Loss 4.1809(4.1904) | Error 0.3652(0.3644) Steps 640(625.64) | Grad Norm 1.3861(1.4050) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 61.9669(58.8055) | Bit/dim 3.6802(3.6805) | Xent 1.0372(1.0203) | Loss 4.1988(4.1907) | Error 0.3679(0.3645) Steps 634(625.89) | Grad Norm 0.6982(1.3837) | Total Time 14.00(14.00)\n",
      "Iter 3261 | Time 57.9348(58.7793) | Bit/dim 3.6797(3.6805) | Xent 1.0189(1.0202) | Loss 4.1891(4.1906) | Error 0.3712(0.3647) Steps 634(626.13) | Grad Norm 1.4570(1.3859) | Total Time 14.00(14.00)\n",
      "Iter 3262 | Time 57.7919(58.7497) | Bit/dim 3.6732(3.6803) | Xent 1.0019(1.0197) | Loss 4.1742(4.1901) | Error 0.3602(0.3646) Steps 640(626.55) | Grad Norm 1.2953(1.3832) | Total Time 14.00(14.00)\n",
      "Iter 3263 | Time 63.3284(58.8871) | Bit/dim 3.6797(3.6803) | Xent 1.0138(1.0195) | Loss 4.1866(4.1900) | Error 0.3676(0.3647) Steps 640(626.95) | Grad Norm 0.8075(1.3660) | Total Time 14.00(14.00)\n",
      "Iter 3264 | Time 59.8421(58.9157) | Bit/dim 3.6919(3.6806) | Xent 1.0004(1.0189) | Loss 4.1920(4.1901) | Error 0.3595(0.3645) Steps 634(627.16) | Grad Norm 1.7607(1.3778) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0544 | Time 23.2077, Epoch Time 397.5901(386.0277), Bit/dim 3.6805(best: 3.6810), Xent 0.9901, Loss 4.1756, Error 0.3513(best: 0.3523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3265 | Time 59.9163(58.9458) | Bit/dim 3.6773(3.6805) | Xent 1.0125(1.0188) | Loss 4.1836(4.1899) | Error 0.3585(0.3643) Steps 628(627.19) | Grad Norm 0.6364(1.3556) | Total Time 14.00(14.00)\n",
      "Iter 3266 | Time 57.3472(58.8978) | Bit/dim 3.6795(3.6805) | Xent 1.0258(1.0190) | Loss 4.1924(4.1900) | Error 0.3642(0.3643) Steps 616(626.85) | Grad Norm 0.9496(1.3434) | Total Time 14.00(14.00)\n",
      "Iter 3267 | Time 59.4201(58.9135) | Bit/dim 3.6879(3.6807) | Xent 1.0044(1.0185) | Loss 4.1901(4.1900) | Error 0.3644(0.3643) Steps 634(627.07) | Grad Norm 1.2708(1.3412) | Total Time 14.00(14.00)\n",
      "Iter 3268 | Time 58.2073(58.8923) | Bit/dim 3.6710(3.6804) | Xent 1.0240(1.0187) | Loss 4.1830(4.1898) | Error 0.3724(0.3646) Steps 640(627.46) | Grad Norm 0.7538(1.3236) | Total Time 14.00(14.00)\n",
      "Iter 3269 | Time 59.7882(58.9192) | Bit/dim 3.6793(3.6804) | Xent 0.9971(1.0180) | Loss 4.1779(4.1894) | Error 0.3575(0.3644) Steps 628(627.47) | Grad Norm 1.3199(1.3235) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 59.4786(58.9359) | Bit/dim 3.6815(3.6804) | Xent 1.0107(1.0178) | Loss 4.1868(4.1893) | Error 0.3596(0.3642) Steps 622(627.31) | Grad Norm 1.1249(1.3175) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0545 | Time 23.3971, Epoch Time 393.2270(386.2437), Bit/dim 3.6804(best: 3.6805), Xent 0.9902, Loss 4.1755, Error 0.3515(best: 0.3513)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3271 | Time 60.0076(58.9681) | Bit/dim 3.6786(3.6804) | Xent 1.0131(1.0177) | Loss 4.1852(4.1892) | Error 0.3671(0.3643) Steps 628(627.33) | Grad Norm 1.0981(1.3109) | Total Time 14.00(14.00)\n",
      "Iter 3272 | Time 55.6472(58.8685) | Bit/dim 3.6812(3.6804) | Xent 1.0107(1.0175) | Loss 4.1865(4.1891) | Error 0.3579(0.3641) Steps 640(627.71) | Grad Norm 1.4513(1.3151) | Total Time 14.00(14.00)\n",
      "Iter 3273 | Time 59.4179(58.8849) | Bit/dim 3.6799(3.6804) | Xent 1.0154(1.0174) | Loss 4.1876(4.1891) | Error 0.3610(0.3640) Steps 634(627.90) | Grad Norm 1.3734(1.3169) | Total Time 14.00(14.00)\n",
      "Iter 3274 | Time 57.6573(58.8481) | Bit/dim 3.6685(3.6800) | Xent 1.0010(1.0169) | Loss 4.1690(4.1885) | Error 0.3632(0.3640) Steps 640(628.26) | Grad Norm 0.7011(1.2984) | Total Time 14.00(14.00)\n",
      "Iter 3275 | Time 59.7429(58.8750) | Bit/dim 3.6921(3.6804) | Xent 1.0036(1.0165) | Loss 4.1939(4.1886) | Error 0.3622(0.3639) Steps 616(627.89) | Grad Norm 0.6326(1.2784) | Total Time 14.00(14.00)\n",
      "Iter 3276 | Time 61.2353(58.9458) | Bit/dim 3.6772(3.6803) | Xent 1.0103(1.0163) | Loss 4.1824(4.1884) | Error 0.3629(0.3639) Steps 628(627.90) | Grad Norm 0.4708(1.2542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0546 | Time 23.6870, Epoch Time 392.6386(386.4355), Bit/dim 3.6810(best: 3.6804), Xent 0.9893, Loss 4.1756, Error 0.3523(best: 0.3513)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3277 | Time 59.9011(58.9744) | Bit/dim 3.6782(3.6802) | Xent 1.0047(1.0160) | Loss 4.1806(4.1882) | Error 0.3558(0.3637) Steps 640(628.26) | Grad Norm 0.8300(1.2415) | Total Time 14.00(14.00)\n",
      "Iter 3278 | Time 59.8745(59.0014) | Bit/dim 3.6830(3.6803) | Xent 1.0021(1.0156) | Loss 4.1840(4.1881) | Error 0.3580(0.3635) Steps 622(628.07) | Grad Norm 0.6238(1.2230) | Total Time 14.00(14.00)\n",
      "Iter 3279 | Time 58.4700(58.9855) | Bit/dim 3.6804(3.6803) | Xent 1.0112(1.0154) | Loss 4.1859(4.1880) | Error 0.3605(0.3634) Steps 616(627.71) | Grad Norm 0.6897(1.2070) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 58.4876(58.9706) | Bit/dim 3.6781(3.6802) | Xent 1.0408(1.0162) | Loss 4.1985(4.1883) | Error 0.3725(0.3637) Steps 616(627.36) | Grad Norm 0.9155(1.1982) | Total Time 14.00(14.00)\n",
      "Iter 3281 | Time 59.8077(58.9957) | Bit/dim 3.6749(3.6801) | Xent 1.0020(1.0158) | Loss 4.1759(4.1880) | Error 0.3634(0.3637) Steps 628(627.38) | Grad Norm 0.5009(1.1773) | Total Time 14.00(14.00)\n",
      "Iter 3282 | Time 56.8695(58.9319) | Bit/dim 3.6810(3.6801) | Xent 0.9970(1.0152) | Loss 4.1795(4.1877) | Error 0.3561(0.3634) Steps 646(627.94) | Grad Norm 0.8975(1.1689) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0547 | Time 23.5745, Epoch Time 392.3467(386.6129), Bit/dim 3.6814(best: 3.6804), Xent 0.9881, Loss 4.1755, Error 0.3537(best: 0.3513)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3283 | Time 58.5340(58.9199) | Bit/dim 3.6746(3.6799) | Xent 1.0176(1.0153) | Loss 4.1834(4.1876) | Error 0.3640(0.3635) Steps 646(628.48) | Grad Norm 0.9604(1.1626) | Total Time 14.00(14.00)\n",
      "Iter 3284 | Time 60.0985(58.9553) | Bit/dim 3.6783(3.6799) | Xent 1.0225(1.0155) | Loss 4.1896(4.1876) | Error 0.3678(0.3636) Steps 646(629.00) | Grad Norm 0.7159(1.1492) | Total Time 14.00(14.00)\n",
      "Iter 3285 | Time 58.6866(58.9472) | Bit/dim 3.6806(3.6799) | Xent 1.0101(1.0153) | Loss 4.1857(4.1876) | Error 0.3590(0.3634) Steps 628(628.97) | Grad Norm 1.4747(1.1590) | Total Time 14.00(14.00)\n",
      "Iter 3286 | Time 62.6305(59.0577) | Bit/dim 3.6795(3.6799) | Xent 1.0081(1.0151) | Loss 4.1835(4.1875) | Error 0.3689(0.3636) Steps 646(629.48) | Grad Norm 0.6221(1.1429) | Total Time 14.00(14.00)\n",
      "Iter 3287 | Time 57.7663(59.0190) | Bit/dim 3.6782(3.6798) | Xent 1.0143(1.0151) | Loss 4.1853(4.1874) | Error 0.3604(0.3635) Steps 628(629.44) | Grad Norm 1.9488(1.1671) | Total Time 14.00(14.00)\n",
      "Iter 3288 | Time 60.4637(59.0623) | Bit/dim 3.6834(3.6800) | Xent 1.0016(1.0147) | Loss 4.1842(4.1873) | Error 0.3548(0.3633) Steps 646(629.94) | Grad Norm 0.6774(1.1524) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0548 | Time 23.6454, Epoch Time 397.2459(386.9318), Bit/dim 3.6806(best: 3.6804), Xent 0.9886, Loss 4.1750, Error 0.3515(best: 0.3513)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3289 | Time 60.3854(59.1020) | Bit/dim 3.6757(3.6798) | Xent 1.0123(1.0146) | Loss 4.1818(4.1871) | Error 0.3562(0.3630) Steps 646(630.42) | Grad Norm 1.3118(1.1572) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 61.5866(59.1766) | Bit/dim 3.6858(3.6800) | Xent 1.0157(1.0147) | Loss 4.1936(4.1873) | Error 0.3619(0.3630) Steps 646(630.89) | Grad Norm 0.9999(1.1525) | Total Time 14.00(14.00)\n",
      "Iter 3291 | Time 55.9366(59.0794) | Bit/dim 3.6765(3.6799) | Xent 1.0017(1.0143) | Loss 4.1774(4.1870) | Error 0.3605(0.3629) Steps 634(630.98) | Grad Norm 0.5328(1.1339) | Total Time 14.00(14.00)\n",
      "Iter 3292 | Time 56.1012(58.9900) | Bit/dim 3.6822(3.6800) | Xent 0.9785(1.0132) | Loss 4.1715(4.1866) | Error 0.3446(0.3624) Steps 634(631.07) | Grad Norm 2.0118(1.1602) | Total Time 14.00(14.00)\n",
      "Iter 3293 | Time 57.2507(58.9378) | Bit/dim 3.6675(3.6796) | Xent 1.0205(1.0134) | Loss 4.1778(4.1863) | Error 0.3625(0.3624) Steps 628(630.98) | Grad Norm 1.5811(1.1728) | Total Time 14.00(14.00)\n",
      "Iter 3294 | Time 59.1384(58.9439) | Bit/dim 3.6859(3.6798) | Xent 1.0170(1.0135) | Loss 4.1945(4.1865) | Error 0.3672(0.3625) Steps 640(631.25) | Grad Norm 0.5287(1.1535) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0549 | Time 23.5971, Epoch Time 389.4451(387.0072), Bit/dim 3.6806(best: 3.6804), Xent 0.9890, Loss 4.1751, Error 0.3498(best: 0.3513)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3295 | Time 59.7413(58.9678) | Bit/dim 3.6716(3.6795) | Xent 0.9927(1.0129) | Loss 4.1679(4.1860) | Error 0.3515(0.3622) Steps 646(631.69) | Grad Norm 1.2511(1.1564) | Total Time 14.00(14.00)\n",
      "Iter 3296 | Time 57.5941(58.9266) | Bit/dim 3.6853(3.6797) | Xent 1.0126(1.0129) | Loss 4.1916(4.1862) | Error 0.3606(0.3622) Steps 646(632.12) | Grad Norm 1.3775(1.1631) | Total Time 14.00(14.00)\n",
      "Iter 3297 | Time 60.8477(58.9842) | Bit/dim 3.6805(3.6797) | Xent 1.0061(1.0127) | Loss 4.1836(4.1861) | Error 0.3610(0.3621) Steps 640(632.36) | Grad Norm 1.0341(1.1592) | Total Time 14.00(14.00)\n",
      "Iter 3298 | Time 56.3691(58.9058) | Bit/dim 3.6776(3.6797) | Xent 1.0235(1.0130) | Loss 4.1893(4.1862) | Error 0.3692(0.3623) Steps 628(632.23) | Grad Norm 1.8006(1.1784) | Total Time 14.00(14.00)\n",
      "Iter 3299 | Time 58.6216(58.8972) | Bit/dim 3.6791(3.6797) | Xent 1.0001(1.0126) | Loss 4.1791(4.1860) | Error 0.3562(0.3622) Steps 622(631.92) | Grad Norm 1.8124(1.1975) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 59.5930(58.9181) | Bit/dim 3.6757(3.6795) | Xent 1.0065(1.0124) | Loss 4.1790(4.1858) | Error 0.3580(0.3620) Steps 622(631.62) | Grad Norm 0.7216(1.1832) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0550 | Time 23.5617, Epoch Time 391.6196(387.1456), Bit/dim 3.6798(best: 3.6804), Xent 0.9877, Loss 4.1736, Error 0.3521(best: 0.3498)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3301 | Time 59.8700(58.9467) | Bit/dim 3.6698(3.6792) | Xent 1.0017(1.0121) | Loss 4.1707(4.1853) | Error 0.3561(0.3618) Steps 646(632.05) | Grad Norm 1.4708(1.1918) | Total Time 14.00(14.00)\n",
      "Iter 3302 | Time 59.1998(58.9543) | Bit/dim 3.6777(3.6792) | Xent 1.0292(1.0126) | Loss 4.1923(4.1855) | Error 0.3618(0.3618) Steps 640(632.29) | Grad Norm 1.5679(1.2031) | Total Time 14.00(14.00)\n",
      "Iter 3303 | Time 57.5800(58.9130) | Bit/dim 3.6760(3.6791) | Xent 1.0070(1.0125) | Loss 4.1795(4.1853) | Error 0.3614(0.3618) Steps 628(632.16) | Grad Norm 1.0280(1.1978) | Total Time 14.00(14.00)\n",
      "Iter 3304 | Time 57.7963(58.8795) | Bit/dim 3.6869(3.6793) | Xent 1.0063(1.0123) | Loss 4.1900(4.1855) | Error 0.3652(0.3619) Steps 622(631.86) | Grad Norm 2.1347(1.2259) | Total Time 14.00(14.00)\n",
      "Iter 3305 | Time 58.4802(58.8675) | Bit/dim 3.6767(3.6793) | Xent 0.9921(1.0117) | Loss 4.1728(4.1851) | Error 0.3502(0.3616) Steps 628(631.74) | Grad Norm 0.8796(1.2156) | Total Time 14.00(14.00)\n",
      "Iter 3306 | Time 56.5739(58.7987) | Bit/dim 3.6808(3.6793) | Xent 1.0252(1.0121) | Loss 4.1934(4.1853) | Error 0.3679(0.3618) Steps 646(632.17) | Grad Norm 1.2864(1.2177) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0551 | Time 23.4332, Epoch Time 388.2931(387.1800), Bit/dim 3.6801(best: 3.6798), Xent 0.9851, Loss 4.1726, Error 0.3487(best: 0.3498)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3307 | Time 58.0287(58.7756) | Bit/dim 3.6801(3.6793) | Xent 0.9998(1.0117) | Loss 4.1800(4.1852) | Error 0.3534(0.3615) Steps 634(632.22) | Grad Norm 0.8508(1.2067) | Total Time 14.00(14.00)\n",
      "Iter 3308 | Time 59.1899(58.7881) | Bit/dim 3.6790(3.6793) | Xent 0.9983(1.0113) | Loss 4.1781(4.1850) | Error 0.3548(0.3613) Steps 646(632.64) | Grad Norm 1.4247(1.2132) | Total Time 14.00(14.00)\n",
      "Iter 3309 | Time 57.2684(58.7425) | Bit/dim 3.6715(3.6791) | Xent 1.0060(1.0111) | Loss 4.1745(4.1847) | Error 0.3589(0.3612) Steps 628(632.50) | Grad Norm 1.3650(1.2178) | Total Time 14.00(14.00)\n",
      "Iter 3310 | Time 59.3781(58.7615) | Bit/dim 3.6761(3.6790) | Xent 1.0129(1.0112) | Loss 4.1825(4.1846) | Error 0.3639(0.3613) Steps 646(632.90) | Grad Norm 0.6552(1.2009) | Total Time 14.00(14.00)\n",
      "Iter 3311 | Time 56.1072(58.6819) | Bit/dim 3.6725(3.6788) | Xent 1.0203(1.0115) | Loss 4.1827(4.1845) | Error 0.3718(0.3616) Steps 634(632.94) | Grad Norm 1.2794(1.2032) | Total Time 14.00(14.00)\n",
      "Iter 3312 | Time 57.9714(58.6606) | Bit/dim 3.6888(3.6791) | Xent 1.0233(1.0118) | Loss 4.2005(4.1850) | Error 0.3665(0.3618) Steps 634(632.97) | Grad Norm 0.7935(1.1910) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0552 | Time 23.4468, Epoch Time 386.9841(387.1742), Bit/dim 3.6800(best: 3.6798), Xent 0.9852, Loss 4.1726, Error 0.3516(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3313 | Time 59.9703(58.6999) | Bit/dim 3.6866(3.6793) | Xent 1.0018(1.0115) | Loss 4.1875(4.1851) | Error 0.3545(0.3616) Steps 646(633.36) | Grad Norm 0.5703(1.1723) | Total Time 14.00(14.00)\n",
      "Iter 3314 | Time 59.5880(58.7265) | Bit/dim 3.6806(3.6794) | Xent 1.0133(1.0116) | Loss 4.1872(4.1851) | Error 0.3639(0.3616) Steps 622(633.02) | Grad Norm 0.4929(1.1520) | Total Time 14.00(14.00)\n",
      "Iter 3315 | Time 60.0144(58.7652) | Bit/dim 3.6746(3.6792) | Xent 1.0024(1.0113) | Loss 4.1758(4.1849) | Error 0.3628(0.3617) Steps 640(633.23) | Grad Norm 0.9830(1.1469) | Total Time 14.00(14.00)\n",
      "Iter 3316 | Time 57.3712(58.7234) | Bit/dim 3.6846(3.6794) | Xent 0.9943(1.0108) | Loss 4.1818(4.1848) | Error 0.3576(0.3615) Steps 634(633.25) | Grad Norm 0.8337(1.1375) | Total Time 14.00(14.00)\n",
      "Iter 3317 | Time 62.0084(58.8219) | Bit/dim 3.6637(3.6789) | Xent 1.0085(1.0107) | Loss 4.1680(4.1843) | Error 0.3548(0.3613) Steps 640(633.45) | Grad Norm 0.7481(1.1258) | Total Time 14.00(14.00)\n",
      "Iter 3318 | Time 57.6286(58.7861) | Bit/dim 3.6777(3.6789) | Xent 1.0150(1.0108) | Loss 4.1852(4.1843) | Error 0.3619(0.3614) Steps 646(633.83) | Grad Norm 0.6011(1.1101) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0553 | Time 23.5451, Epoch Time 395.6818(387.4294), Bit/dim 3.6792(best: 3.6798), Xent 0.9863, Loss 4.1723, Error 0.3514(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3319 | Time 58.9200(58.7901) | Bit/dim 3.6744(3.6787) | Xent 0.9995(1.0105) | Loss 4.1741(4.1840) | Error 0.3612(0.3614) Steps 634(633.84) | Grad Norm 0.6438(1.0961) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 59.0506(58.7979) | Bit/dim 3.6842(3.6789) | Xent 0.9926(1.0100) | Loss 4.1805(4.1839) | Error 0.3482(0.3610) Steps 640(634.02) | Grad Norm 1.0426(1.0945) | Total Time 14.00(14.00)\n",
      "Iter 3321 | Time 58.2856(58.7826) | Bit/dim 3.6677(3.6786) | Xent 1.0266(1.0105) | Loss 4.1810(4.1838) | Error 0.3610(0.3610) Steps 616(633.48) | Grad Norm 1.4523(1.1052) | Total Time 14.00(14.00)\n",
      "Iter 3322 | Time 58.1566(58.7638) | Bit/dim 3.6814(3.6787) | Xent 1.0103(1.0105) | Loss 4.1866(4.1839) | Error 0.3649(0.3611) Steps 616(632.96) | Grad Norm 1.2911(1.1108) | Total Time 14.00(14.00)\n",
      "Iter 3323 | Time 60.7311(58.8228) | Bit/dim 3.6793(3.6787) | Xent 0.9866(1.0097) | Loss 4.1726(4.1835) | Error 0.3525(0.3608) Steps 622(632.63) | Grad Norm 1.0959(1.1103) | Total Time 14.00(14.00)\n",
      "Iter 3324 | Time 58.0807(58.8005) | Bit/dim 3.6810(3.6787) | Xent 0.9987(1.0094) | Loss 4.1804(4.1834) | Error 0.3572(0.3607) Steps 622(632.31) | Grad Norm 1.5659(1.1240) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0554 | Time 23.8704, Epoch Time 392.7601(387.5893), Bit/dim 3.6792(best: 3.6792), Xent 0.9842, Loss 4.1713, Error 0.3506(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3325 | Time 59.9736(58.8357) | Bit/dim 3.6814(3.6788) | Xent 0.9964(1.0090) | Loss 4.1796(4.1833) | Error 0.3521(0.3605) Steps 622(632.00) | Grad Norm 0.9380(1.1184) | Total Time 14.00(14.00)\n",
      "Iter 3326 | Time 59.1432(58.8450) | Bit/dim 3.6782(3.6788) | Xent 1.0073(1.0090) | Loss 4.1819(4.1833) | Error 0.3615(0.3605) Steps 628(631.88) | Grad Norm 1.2013(1.1209) | Total Time 14.00(14.00)\n",
      "Iter 3327 | Time 61.9566(58.9383) | Bit/dim 3.6765(3.6787) | Xent 1.0111(1.0090) | Loss 4.1820(4.1833) | Error 0.3624(0.3605) Steps 646(632.30) | Grad Norm 0.9508(1.1158) | Total Time 14.00(14.00)\n",
      "Iter 3328 | Time 57.1719(58.8853) | Bit/dim 3.6811(3.6788) | Xent 1.0089(1.0090) | Loss 4.1855(4.1833) | Error 0.3629(0.3606) Steps 628(632.17) | Grad Norm 1.2344(1.1194) | Total Time 14.00(14.00)\n",
      "Iter 3329 | Time 58.3672(58.8698) | Bit/dim 3.6691(3.6785) | Xent 1.0150(1.0092) | Loss 4.1766(4.1831) | Error 0.3576(0.3605) Steps 634(632.23) | Grad Norm 1.8661(1.1418) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 56.8546(58.8093) | Bit/dim 3.6779(3.6785) | Xent 0.9967(1.0088) | Loss 4.1762(4.1829) | Error 0.3586(0.3605) Steps 628(632.10) | Grad Norm 1.1182(1.1411) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0555 | Time 23.8110, Epoch Time 392.7231(387.7433), Bit/dim 3.6789(best: 3.6792), Xent 0.9851, Loss 4.1715, Error 0.3537(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3331 | Time 58.8198(58.8096) | Bit/dim 3.6760(3.6784) | Xent 1.0134(1.0090) | Loss 4.1827(4.1829) | Error 0.3634(0.3606) Steps 640(632.34) | Grad Norm 1.6412(1.1561) | Total Time 14.00(14.00)\n",
      "Iter 3332 | Time 56.4956(58.7402) | Bit/dim 3.6827(3.6785) | Xent 0.9878(1.0083) | Loss 4.1766(4.1827) | Error 0.3561(0.3604) Steps 628(632.21) | Grad Norm 1.3611(1.1622) | Total Time 14.00(14.00)\n",
      "Iter 3333 | Time 56.9697(58.6871) | Bit/dim 3.6745(3.6784) | Xent 0.9913(1.0078) | Loss 4.1701(4.1823) | Error 0.3595(0.3604) Steps 622(631.90) | Grad Norm 1.7815(1.1808) | Total Time 14.00(14.00)\n",
      "Iter 3334 | Time 59.0915(58.6992) | Bit/dim 3.6792(3.6784) | Xent 1.0104(1.0079) | Loss 4.1844(4.1824) | Error 0.3582(0.3603) Steps 640(632.14) | Grad Norm 2.7188(1.2269) | Total Time 14.00(14.00)\n",
      "Iter 3335 | Time 58.6953(58.6991) | Bit/dim 3.6755(3.6784) | Xent 1.0100(1.0080) | Loss 4.1805(4.1823) | Error 0.3588(0.3603) Steps 646(632.56) | Grad Norm 1.4835(1.2346) | Total Time 14.00(14.00)\n",
      "Iter 3336 | Time 60.2450(58.7455) | Bit/dim 3.6753(3.6783) | Xent 1.0276(1.0086) | Loss 4.1891(4.1825) | Error 0.3686(0.3605) Steps 646(632.96) | Grad Norm 2.2771(1.2659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0556 | Time 23.7369, Epoch Time 389.7422(387.8033), Bit/dim 3.6790(best: 3.6789), Xent 0.9827, Loss 4.1703, Error 0.3522(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3337 | Time 57.4435(58.7064) | Bit/dim 3.6788(3.6783) | Xent 1.0115(1.0086) | Loss 4.1846(4.1826) | Error 0.3586(0.3605) Steps 628(632.81) | Grad Norm 1.1097(1.2612) | Total Time 14.00(14.00)\n",
      "Iter 3338 | Time 58.2637(58.6931) | Bit/dim 3.6818(3.6784) | Xent 1.0010(1.0084) | Loss 4.1823(4.1826) | Error 0.3574(0.3604) Steps 622(632.49) | Grad Norm 2.8675(1.3094) | Total Time 14.00(14.00)\n",
      "Iter 3339 | Time 57.6336(58.6614) | Bit/dim 3.6714(3.6782) | Xent 1.0057(1.0083) | Loss 4.1742(4.1823) | Error 0.3585(0.3603) Steps 634(632.54) | Grad Norm 1.8484(1.3256) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 57.3158(58.6210) | Bit/dim 3.6788(3.6782) | Xent 1.0090(1.0084) | Loss 4.1833(4.1824) | Error 0.3588(0.3603) Steps 646(632.94) | Grad Norm 1.8298(1.3407) | Total Time 14.00(14.00)\n",
      "Iter 3341 | Time 59.6590(58.6521) | Bit/dim 3.6745(3.6781) | Xent 1.0135(1.0085) | Loss 4.1812(4.1823) | Error 0.3640(0.3604) Steps 628(632.79) | Grad Norm 2.5953(1.3783) | Total Time 14.00(14.00)\n",
      "Iter 3342 | Time 56.4632(58.5865) | Bit/dim 3.6750(3.6780) | Xent 0.9974(1.0082) | Loss 4.1737(4.1821) | Error 0.3561(0.3603) Steps 628(632.65) | Grad Norm 1.4225(1.3797) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0557 | Time 23.5506, Epoch Time 386.0193(387.7498), Bit/dim 3.6775(best: 3.6789), Xent 0.9833, Loss 4.1691, Error 0.3527(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3343 | Time 59.0407(58.6001) | Bit/dim 3.6693(3.6777) | Xent 1.0036(1.0080) | Loss 4.1711(4.1818) | Error 0.3628(0.3603) Steps 640(632.87) | Grad Norm 2.8940(1.4251) | Total Time 14.00(14.00)\n",
      "Iter 3344 | Time 57.5210(58.5677) | Bit/dim 3.6737(3.6776) | Xent 1.0113(1.0081) | Loss 4.1793(4.1817) | Error 0.3592(0.3603) Steps 628(632.72) | Grad Norm 1.4637(1.4263) | Total Time 14.00(14.00)\n",
      "Iter 3345 | Time 61.2160(58.6472) | Bit/dim 3.6900(3.6780) | Xent 1.0091(1.0082) | Loss 4.1946(4.1821) | Error 0.3630(0.3604) Steps 628(632.58) | Grad Norm 2.2341(1.4505) | Total Time 14.00(14.00)\n",
      "Iter 3346 | Time 58.9025(58.6548) | Bit/dim 3.6849(3.6782) | Xent 1.0038(1.0080) | Loss 4.1868(4.1822) | Error 0.3526(0.3602) Steps 640(632.80) | Grad Norm 2.2816(1.4754) | Total Time 14.00(14.00)\n",
      "Iter 3347 | Time 58.2081(58.6414) | Bit/dim 3.6679(3.6779) | Xent 0.9992(1.0078) | Loss 4.1675(4.1818) | Error 0.3609(0.3602) Steps 616(632.30) | Grad Norm 1.4841(1.4757) | Total Time 14.00(14.00)\n",
      "Iter 3348 | Time 58.7161(58.6437) | Bit/dim 3.6833(3.6780) | Xent 0.9996(1.0075) | Loss 4.1831(4.1818) | Error 0.3561(0.3601) Steps 634(632.35) | Grad Norm 2.3967(1.5033) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0558 | Time 23.6768, Epoch Time 392.7703(387.9004), Bit/dim 3.6791(best: 3.6775), Xent 0.9794, Loss 4.1688, Error 0.3473(best: 0.3487)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3349 | Time 59.0808(58.6568) | Bit/dim 3.6756(3.6780) | Xent 0.9888(1.0070) | Loss 4.1700(4.1815) | Error 0.3554(0.3599) Steps 652(632.94) | Grad Norm 1.1939(1.4940) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 61.0204(58.7277) | Bit/dim 3.6799(3.6780) | Xent 0.9937(1.0066) | Loss 4.1768(4.1813) | Error 0.3518(0.3597) Steps 640(633.15) | Grad Norm 3.3656(1.5502) | Total Time 14.00(14.00)\n",
      "Iter 3351 | Time 56.4569(58.6596) | Bit/dim 3.6838(3.6782) | Xent 0.9836(1.0059) | Loss 4.1756(4.1811) | Error 0.3449(0.3592) Steps 640(633.36) | Grad Norm 1.7927(1.5575) | Total Time 14.00(14.00)\n",
      "Iter 3352 | Time 62.6388(58.7789) | Bit/dim 3.6735(3.6781) | Xent 0.9855(1.0053) | Loss 4.1663(4.1807) | Error 0.3530(0.3590) Steps 640(633.56) | Grad Norm 1.7900(1.5644) | Total Time 14.00(14.00)\n",
      "Iter 3353 | Time 58.0623(58.7574) | Bit/dim 3.6681(3.6778) | Xent 1.0175(1.0056) | Loss 4.1769(4.1806) | Error 0.3656(0.3592) Steps 646(633.93) | Grad Norm 2.3057(1.5867) | Total Time 14.00(14.00)\n",
      "Iter 3354 | Time 60.2113(58.8011) | Bit/dim 3.6849(3.6780) | Xent 1.0219(1.0061) | Loss 4.1959(4.1810) | Error 0.3682(0.3595) Steps 646(634.29) | Grad Norm 0.4327(1.5520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0559 | Time 23.7088, Epoch Time 397.0428(388.1747), Bit/dim 3.6786(best: 3.6775), Xent 0.9839, Loss 4.1705, Error 0.3506(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3355 | Time 60.8818(58.8635) | Bit/dim 3.6795(3.6780) | Xent 0.9999(1.0059) | Loss 4.1795(4.1810) | Error 0.3569(0.3594) Steps 640(634.46) | Grad Norm 2.5552(1.5821) | Total Time 14.00(14.00)\n",
      "Iter 3356 | Time 60.0681(58.8996) | Bit/dim 3.6811(3.6781) | Xent 0.9933(1.0056) | Loss 4.1778(4.1809) | Error 0.3552(0.3593) Steps 640(634.63) | Grad Norm 1.2542(1.5723) | Total Time 14.00(14.00)\n",
      "Iter 3357 | Time 57.5071(58.8578) | Bit/dim 3.6672(3.6778) | Xent 1.0173(1.0059) | Loss 4.1758(4.1807) | Error 0.3656(0.3595) Steps 634(634.61) | Grad Norm 1.8065(1.5793) | Total Time 14.00(14.00)\n",
      "Iter 3358 | Time 57.1090(58.8054) | Bit/dim 3.6698(3.6776) | Xent 1.0001(1.0057) | Loss 4.1698(4.1804) | Error 0.3590(0.3595) Steps 634(634.59) | Grad Norm 2.0193(1.5925) | Total Time 14.00(14.00)\n",
      "Iter 3359 | Time 57.8035(58.7753) | Bit/dim 3.6757(3.6775) | Xent 1.0111(1.0059) | Loss 4.1812(4.1804) | Error 0.3551(0.3593) Steps 628(634.39) | Grad Norm 0.5780(1.5621) | Total Time 14.00(14.00)\n",
      "Iter 3360 | Time 57.1419(58.7263) | Bit/dim 3.6826(3.6776) | Xent 1.0052(1.0059) | Loss 4.1852(4.1806) | Error 0.3656(0.3595) Steps 640(634.56) | Grad Norm 1.7267(1.5670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0560 | Time 23.6258, Epoch Time 389.7104(388.2207), Bit/dim 3.6783(best: 3.6775), Xent 0.9789, Loss 4.1677, Error 0.3486(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3361 | Time 57.8270(58.6993) | Bit/dim 3.6856(3.6779) | Xent 1.0055(1.0059) | Loss 4.1883(4.1808) | Error 0.3586(0.3595) Steps 652(635.09) | Grad Norm 1.6359(1.5691) | Total Time 14.00(14.00)\n",
      "Iter 3362 | Time 59.0309(58.7093) | Bit/dim 3.6687(3.6776) | Xent 0.9981(1.0056) | Loss 4.1677(4.1804) | Error 0.3589(0.3595) Steps 634(635.05) | Grad Norm 0.5756(1.5393) | Total Time 14.00(14.00)\n",
      "Iter 3363 | Time 57.2867(58.6666) | Bit/dim 3.6821(3.6777) | Xent 0.9900(1.0052) | Loss 4.1771(4.1803) | Error 0.3502(0.3592) Steps 634(635.02) | Grad Norm 1.9682(1.5522) | Total Time 14.00(14.00)\n",
      "Iter 3364 | Time 59.5332(58.6926) | Bit/dim 3.6801(3.6778) | Xent 1.0251(1.0058) | Loss 4.1926(4.1807) | Error 0.3681(0.3595) Steps 646(635.35) | Grad Norm 0.8130(1.5300) | Total Time 14.00(14.00)\n",
      "Iter 3365 | Time 55.7283(58.6037) | Bit/dim 3.6750(3.6777) | Xent 1.0028(1.0057) | Loss 4.1764(4.1806) | Error 0.3595(0.3595) Steps 628(635.13) | Grad Norm 0.9628(1.5130) | Total Time 14.00(14.00)\n",
      "Iter 3366 | Time 60.1205(58.6492) | Bit/dim 3.6717(3.6775) | Xent 0.9938(1.0053) | Loss 4.1686(4.1802) | Error 0.3619(0.3596) Steps 634(635.10) | Grad Norm 2.5034(1.5427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0561 | Time 23.6938, Epoch Time 388.7120(388.2355), Bit/dim 3.6788(best: 3.6775), Xent 0.9783, Loss 4.1679, Error 0.3509(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3367 | Time 58.0108(58.6300) | Bit/dim 3.6758(3.6775) | Xent 1.0022(1.0052) | Loss 4.1769(4.1801) | Error 0.3510(0.3593) Steps 640(635.24) | Grad Norm 0.6602(1.5162) | Total Time 14.00(14.00)\n",
      "Iter 3368 | Time 58.4813(58.6256) | Bit/dim 3.6821(3.6776) | Xent 1.0106(1.0054) | Loss 4.1874(4.1803) | Error 0.3619(0.3594) Steps 634(635.21) | Grad Norm 2.4882(1.5454) | Total Time 14.00(14.00)\n",
      "Iter 3369 | Time 60.0796(58.6692) | Bit/dim 3.6826(3.6778) | Xent 1.0048(1.0054) | Loss 4.1850(4.1805) | Error 0.3586(0.3593) Steps 640(635.35) | Grad Norm 1.0514(1.5305) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 58.0991(58.6521) | Bit/dim 3.6715(3.6776) | Xent 1.0032(1.0053) | Loss 4.1731(4.1802) | Error 0.3572(0.3593) Steps 646(635.67) | Grad Norm 1.4876(1.5293) | Total Time 14.00(14.00)\n",
      "Iter 3371 | Time 58.5114(58.6479) | Bit/dim 3.6717(3.6774) | Xent 1.0014(1.0052) | Loss 4.1724(4.1800) | Error 0.3595(0.3593) Steps 640(635.80) | Grad Norm 1.9855(1.5429) | Total Time 14.00(14.00)\n",
      "Iter 3372 | Time 58.6355(58.6475) | Bit/dim 3.6804(3.6775) | Xent 1.0063(1.0052) | Loss 4.1835(4.1801) | Error 0.3579(0.3593) Steps 646(636.11) | Grad Norm 0.9844(1.5262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0562 | Time 23.9160, Epoch Time 391.2811(388.3268), Bit/dim 3.6777(best: 3.6775), Xent 0.9777, Loss 4.1665, Error 0.3485(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3373 | Time 57.0013(58.5981) | Bit/dim 3.6761(3.6775) | Xent 1.0060(1.0052) | Loss 4.1791(4.1801) | Error 0.3590(0.3592) Steps 640(636.22) | Grad Norm 1.9943(1.5402) | Total Time 14.00(14.00)\n",
      "Iter 3374 | Time 59.1894(58.6158) | Bit/dim 3.6768(3.6774) | Xent 0.9933(1.0049) | Loss 4.1735(4.1799) | Error 0.3526(0.3590) Steps 646(636.52) | Grad Norm 1.8086(1.5483) | Total Time 14.00(14.00)\n",
      "Iter 3375 | Time 61.5949(58.7052) | Bit/dim 3.6514(3.6767) | Xent 1.0199(1.0053) | Loss 4.1613(4.1793) | Error 0.3630(0.3592) Steps 628(636.26) | Grad Norm 1.2963(1.5407) | Total Time 14.00(14.00)\n",
      "Iter 3376 | Time 58.4349(58.6971) | Bit/dim 3.6845(3.6769) | Xent 1.0052(1.0053) | Loss 4.1871(4.1796) | Error 0.3555(0.3591) Steps 646(636.55) | Grad Norm 1.9547(1.5531) | Total Time 14.00(14.00)\n",
      "Iter 3377 | Time 56.0054(58.6164) | Bit/dim 3.6812(3.6770) | Xent 0.9941(1.0050) | Loss 4.1782(4.1795) | Error 0.3545(0.3589) Steps 622(636.12) | Grad Norm 0.9304(1.5345) | Total Time 14.00(14.00)\n",
      "Iter 3378 | Time 59.3655(58.6388) | Bit/dim 3.6827(3.6772) | Xent 0.9965(1.0047) | Loss 4.1809(4.1796) | Error 0.3541(0.3588) Steps 628(635.87) | Grad Norm 1.3013(1.5275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0563 | Time 23.4106, Epoch Time 390.4929(388.3918), Bit/dim 3.6770(best: 3.6775), Xent 0.9780, Loss 4.1660, Error 0.3495(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3379 | Time 60.0456(58.6810) | Bit/dim 3.6687(3.6769) | Xent 0.9874(1.0042) | Loss 4.1624(4.1791) | Error 0.3522(0.3586) Steps 640(636.00) | Grad Norm 2.6577(1.5614) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 59.7941(58.7144) | Bit/dim 3.6726(3.6768) | Xent 1.0045(1.0042) | Loss 4.1749(4.1789) | Error 0.3604(0.3586) Steps 646(636.30) | Grad Norm 1.6353(1.5636) | Total Time 14.00(14.00)\n",
      "Iter 3381 | Time 56.8486(58.6585) | Bit/dim 3.6844(3.6770) | Xent 0.9932(1.0039) | Loss 4.1810(4.1790) | Error 0.3535(0.3585) Steps 622(635.87) | Grad Norm 2.8557(1.6023) | Total Time 14.00(14.00)\n",
      "Iter 3382 | Time 58.7250(58.6605) | Bit/dim 3.6788(3.6771) | Xent 1.0036(1.0039) | Loss 4.1806(4.1790) | Error 0.3562(0.3584) Steps 634(635.81) | Grad Norm 1.0748(1.5865) | Total Time 14.00(14.00)\n",
      "Iter 3383 | Time 59.8684(58.6967) | Bit/dim 3.6697(3.6769) | Xent 0.9983(1.0037) | Loss 4.1688(4.1787) | Error 0.3589(0.3584) Steps 646(636.12) | Grad Norm 2.0522(1.6005) | Total Time 14.00(14.00)\n",
      "Iter 3384 | Time 59.1972(58.7117) | Bit/dim 3.6849(3.6771) | Xent 0.9877(1.0032) | Loss 4.1787(4.1787) | Error 0.3548(0.3583) Steps 628(635.87) | Grad Norm 1.2107(1.5888) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0564 | Time 23.6683, Epoch Time 393.5842(388.5476), Bit/dim 3.6771(best: 3.6770), Xent 0.9779, Loss 4.1661, Error 0.3490(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3385 | Time 58.8401(58.7156) | Bit/dim 3.6757(3.6771) | Xent 1.0077(1.0034) | Loss 4.1795(4.1788) | Error 0.3581(0.3583) Steps 646(636.18) | Grad Norm 1.1749(1.5764) | Total Time 14.00(14.00)\n",
      "Iter 3386 | Time 61.5675(58.8011) | Bit/dim 3.6691(3.6768) | Xent 1.0035(1.0034) | Loss 4.1708(4.1785) | Error 0.3595(0.3583) Steps 646(636.47) | Grad Norm 2.7650(1.6120) | Total Time 14.00(14.00)\n",
      "Iter 3387 | Time 59.3523(58.8177) | Bit/dim 3.6693(3.6766) | Xent 1.0039(1.0034) | Loss 4.1713(4.1783) | Error 0.3601(0.3584) Steps 646(636.76) | Grad Norm 1.2300(1.6006) | Total Time 14.00(14.00)\n",
      "Iter 3388 | Time 56.1437(58.7374) | Bit/dim 3.6710(3.6764) | Xent 0.9852(1.0028) | Loss 4.1636(4.1779) | Error 0.3499(0.3581) Steps 628(636.49) | Grad Norm 2.8587(1.6383) | Total Time 14.00(14.00)\n",
      "Iter 3389 | Time 60.4652(58.7893) | Bit/dim 3.6784(3.6765) | Xent 0.9961(1.0026) | Loss 4.1764(4.1778) | Error 0.3565(0.3581) Steps 640(636.60) | Grad Norm 1.1277(1.6230) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 57.1735(58.7408) | Bit/dim 3.6864(3.6768) | Xent 0.9933(1.0024) | Loss 4.1831(4.1780) | Error 0.3529(0.3579) Steps 628(636.34) | Grad Norm 2.0223(1.6350) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0565 | Time 23.7385, Epoch Time 392.8021(388.6752), Bit/dim 3.6776(best: 3.6770), Xent 0.9773, Loss 4.1662, Error 0.3490(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3391 | Time 58.3984(58.7305) | Bit/dim 3.6806(3.6769) | Xent 0.9944(1.0021) | Loss 4.1778(4.1780) | Error 0.3601(0.3580) Steps 640(636.45) | Grad Norm 2.2419(1.6532) | Total Time 14.00(14.00)\n",
      "Iter 3392 | Time 55.7976(58.6425) | Bit/dim 3.6706(3.6767) | Xent 0.9964(1.0020) | Loss 4.1688(4.1777) | Error 0.3561(0.3579) Steps 646(636.74) | Grad Norm 1.4177(1.6461) | Total Time 14.00(14.00)\n",
      "Iter 3393 | Time 58.4143(58.6357) | Bit/dim 3.6856(3.6770) | Xent 0.9907(1.0016) | Loss 4.1809(4.1778) | Error 0.3514(0.3577) Steps 634(636.66) | Grad Norm 2.6548(1.6764) | Total Time 14.00(14.00)\n",
      "Iter 3394 | Time 57.8325(58.6116) | Bit/dim 3.6717(3.6768) | Xent 0.9861(1.0011) | Loss 4.1648(4.1774) | Error 0.3505(0.3575) Steps 634(636.58) | Grad Norm 1.1198(1.6597) | Total Time 14.00(14.00)\n",
      "Iter 3395 | Time 58.4144(58.6057) | Bit/dim 3.6759(3.6768) | Xent 0.9964(1.0010) | Loss 4.1741(4.1773) | Error 0.3612(0.3576) Steps 640(636.68) | Grad Norm 1.2126(1.6463) | Total Time 14.00(14.00)\n",
      "Iter 3396 | Time 60.1640(58.6524) | Bit/dim 3.6719(3.6766) | Xent 1.0076(1.0012) | Loss 4.1757(4.1772) | Error 0.3560(0.3576) Steps 616(636.06) | Grad Norm 1.4320(1.6399) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0566 | Time 23.7214, Epoch Time 388.1707(388.6601), Bit/dim 3.6769(best: 3.6770), Xent 0.9739, Loss 4.1639, Error 0.3479(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3397 | Time 58.6749(58.6531) | Bit/dim 3.6754(3.6766) | Xent 0.9905(1.0009) | Loss 4.1706(4.1770) | Error 0.3549(0.3575) Steps 622(635.64) | Grad Norm 0.6557(1.6103) | Total Time 14.00(14.00)\n",
      "Iter 3398 | Time 57.2936(58.6123) | Bit/dim 3.6662(3.6763) | Xent 1.0034(1.0010) | Loss 4.1679(4.1768) | Error 0.3619(0.3576) Steps 646(635.95) | Grad Norm 2.0340(1.6230) | Total Time 14.00(14.00)\n",
      "Iter 3399 | Time 56.0422(58.5352) | Bit/dim 3.6856(3.6766) | Xent 0.9972(1.0008) | Loss 4.1842(4.1770) | Error 0.3551(0.3576) Steps 640(636.07) | Grad Norm 1.5061(1.6195) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 56.4325(58.4721) | Bit/dim 3.6742(3.6765) | Xent 0.9961(1.0007) | Loss 4.1722(4.1769) | Error 0.3604(0.3577) Steps 628(635.83) | Grad Norm 0.8754(1.5972) | Total Time 14.00(14.00)\n",
      "Iter 3401 | Time 60.0908(58.5207) | Bit/dim 3.6748(3.6764) | Xent 0.9893(1.0004) | Loss 4.1694(4.1766) | Error 0.3502(0.3574) Steps 646(636.13) | Grad Norm 1.8073(1.6035) | Total Time 14.00(14.00)\n",
      "Iter 3402 | Time 59.5795(58.5525) | Bit/dim 3.6779(3.6765) | Xent 0.9995(1.0003) | Loss 4.1776(4.1767) | Error 0.3541(0.3573) Steps 646(636.43) | Grad Norm 3.1777(1.6507) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0567 | Time 23.8220, Epoch Time 387.6182(388.6288), Bit/dim 3.6772(best: 3.6769), Xent 0.9743, Loss 4.1644, Error 0.3455(best: 0.3473)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3403 | Time 57.9396(58.5341) | Bit/dim 3.6892(3.6769) | Xent 0.9789(0.9997) | Loss 4.1786(4.1767) | Error 0.3484(0.3571) Steps 628(636.18) | Grad Norm 1.6188(1.6498) | Total Time 14.00(14.00)\n",
      "Iter 3404 | Time 60.6174(58.5966) | Bit/dim 3.6745(3.6768) | Xent 1.0021(0.9998) | Loss 4.1755(4.1767) | Error 0.3601(0.3572) Steps 628(635.93) | Grad Norm 1.7571(1.6530) | Total Time 14.00(14.00)\n",
      "Iter 3405 | Time 57.2030(58.5548) | Bit/dim 3.6713(3.6766) | Xent 0.9807(0.9992) | Loss 4.1616(4.1762) | Error 0.3568(0.3571) Steps 616(635.33) | Grad Norm 2.1075(1.6666) | Total Time 14.00(14.00)\n",
      "Iter 3406 | Time 58.1558(58.5428) | Bit/dim 3.6699(3.6764) | Xent 1.0251(1.0000) | Loss 4.1825(4.1764) | Error 0.3695(0.3575) Steps 640(635.47) | Grad Norm 1.1576(1.6514) | Total Time 14.00(14.00)\n",
      "Iter 3407 | Time 60.2322(58.5935) | Bit/dim 3.6774(3.6765) | Xent 0.9846(0.9995) | Loss 4.1697(4.1762) | Error 0.3516(0.3573) Steps 646(635.79) | Grad Norm 1.1116(1.6352) | Total Time 14.00(14.00)\n",
      "Iter 3408 | Time 57.3467(58.5561) | Bit/dim 3.6788(3.6765) | Xent 1.0005(0.9995) | Loss 4.1791(4.1763) | Error 0.3580(0.3574) Steps 634(635.73) | Grad Norm 2.0411(1.6473) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0568 | Time 23.7195, Epoch Time 390.7868(388.6936), Bit/dim 3.6771(best: 3.6769), Xent 0.9731, Loss 4.1637, Error 0.3437(best: 0.3455)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3409 | Time 60.3836(58.6109) | Bit/dim 3.6821(3.6767) | Xent 0.9761(0.9988) | Loss 4.1702(4.1761) | Error 0.3472(0.3571) Steps 628(635.50) | Grad Norm 0.5129(1.6133) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 58.8175(58.6171) | Bit/dim 3.6832(3.6769) | Xent 1.0014(0.9989) | Loss 4.1839(4.1764) | Error 0.3631(0.3572) Steps 646(635.82) | Grad Norm 1.4473(1.6083) | Total Time 14.00(14.00)\n",
      "Iter 3411 | Time 61.6654(58.7085) | Bit/dim 3.6724(3.6768) | Xent 0.9962(0.9988) | Loss 4.1705(4.1762) | Error 0.3602(0.3573) Steps 616(635.22) | Grad Norm 0.8675(1.5861) | Total Time 14.00(14.00)\n",
      "Iter 3412 | Time 58.7099(58.7086) | Bit/dim 3.6740(3.6767) | Xent 1.0091(0.9991) | Loss 4.1786(4.1762) | Error 0.3599(0.3574) Steps 634(635.19) | Grad Norm 1.7591(1.5913) | Total Time 14.00(14.00)\n",
      "Iter 3413 | Time 59.5634(58.7342) | Bit/dim 3.6753(3.6766) | Xent 0.9838(0.9987) | Loss 4.1672(4.1760) | Error 0.3470(0.3571) Steps 640(635.33) | Grad Norm 0.5886(1.5612) | Total Time 14.00(14.00)\n",
      "Iter 3414 | Time 59.1580(58.7469) | Bit/dim 3.6694(3.6764) | Xent 1.0086(0.9990) | Loss 4.1737(4.1759) | Error 0.3665(0.3574) Steps 652(635.83) | Grad Norm 0.5287(1.5302) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0569 | Time 23.7999, Epoch Time 397.6348(388.9618), Bit/dim 3.6769(best: 3.6769), Xent 0.9730, Loss 4.1634, Error 0.3453(best: 0.3437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3415 | Time 59.2067(58.7607) | Bit/dim 3.6803(3.6765) | Xent 0.9982(0.9990) | Loss 4.1795(4.1760) | Error 0.3605(0.3575) Steps 634(635.78) | Grad Norm 0.5170(1.4998) | Total Time 14.00(14.00)\n",
      "Iter 3416 | Time 56.5545(58.6946) | Bit/dim 3.6800(3.6766) | Xent 0.9886(0.9986) | Loss 4.1743(4.1760) | Error 0.3525(0.3573) Steps 634(635.72) | Grad Norm 0.8009(1.4789) | Total Time 14.00(14.00)\n",
      "Iter 3417 | Time 57.6960(58.6646) | Bit/dim 3.6816(3.6768) | Xent 0.9864(0.9983) | Loss 4.1748(4.1759) | Error 0.3486(0.3571) Steps 634(635.67) | Grad Norm 0.7356(1.4566) | Total Time 14.00(14.00)\n",
      "Iter 3418 | Time 62.3020(58.7737) | Bit/dim 3.6747(3.6767) | Xent 0.9990(0.9983) | Loss 4.1743(4.1759) | Error 0.3581(0.3571) Steps 646(635.98) | Grad Norm 1.5435(1.4592) | Total Time 14.00(14.00)\n",
      "Iter 3419 | Time 57.8478(58.7459) | Bit/dim 3.6769(3.6767) | Xent 0.9739(0.9976) | Loss 4.1639(4.1755) | Error 0.3570(0.3571) Steps 640(636.10) | Grad Norm 1.2251(1.4522) | Total Time 14.00(14.00)\n",
      "Iter 3420 | Time 60.9422(58.8118) | Bit/dim 3.6614(3.6763) | Xent 1.0033(0.9977) | Loss 4.1630(4.1751) | Error 0.3555(0.3570) Steps 634(636.04) | Grad Norm 1.2797(1.4470) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0570 | Time 23.8285, Epoch Time 393.9242(389.1107), Bit/dim 3.6779(best: 3.6769), Xent 0.9725, Loss 4.1642, Error 0.3437(best: 0.3437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3421 | Time 59.5501(58.8340) | Bit/dim 3.6776(3.6763) | Xent 0.9828(0.9973) | Loss 4.1690(4.1750) | Error 0.3534(0.3569) Steps 634(635.98) | Grad Norm 2.2267(1.4704) | Total Time 14.00(14.00)\n",
      "Iter 3422 | Time 59.3210(58.8486) | Bit/dim 3.6740(3.6762) | Xent 0.9804(0.9968) | Loss 4.1642(4.1746) | Error 0.3540(0.3568) Steps 616(635.38) | Grad Norm 0.8717(1.4524) | Total Time 14.00(14.00)\n",
      "Iter 3423 | Time 59.9146(58.8806) | Bit/dim 3.6788(3.6763) | Xent 1.0135(0.9973) | Loss 4.1856(4.1750) | Error 0.3611(0.3570) Steps 634(635.34) | Grad Norm 0.7912(1.4326) | Total Time 14.00(14.00)\n",
      "Iter 3424 | Time 59.8676(58.9102) | Bit/dim 3.6620(3.6759) | Xent 0.9934(0.9972) | Loss 4.1586(4.1745) | Error 0.3572(0.3570) Steps 646(635.66) | Grad Norm 1.4495(1.4331) | Total Time 14.00(14.00)\n",
      "Iter 3425 | Time 56.2865(58.8315) | Bit/dim 3.6763(3.6759) | Xent 1.0030(0.9973) | Loss 4.1778(4.1746) | Error 0.3591(0.3570) Steps 640(635.79) | Grad Norm 0.8028(1.4142) | Total Time 14.00(14.00)\n",
      "Iter 3426 | Time 58.2079(58.8128) | Bit/dim 3.6830(3.6761) | Xent 0.9801(0.9968) | Loss 4.1731(4.1745) | Error 0.3480(0.3568) Steps 628(635.55) | Grad Norm 0.7728(1.3949) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0571 | Time 24.0441, Epoch Time 392.7215(389.2190), Bit/dim 3.6765(best: 3.6769), Xent 0.9719, Loss 4.1625, Error 0.3469(best: 0.3437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3427 | Time 60.2154(58.8548) | Bit/dim 3.6826(3.6763) | Xent 1.0021(0.9970) | Loss 4.1837(4.1748) | Error 0.3642(0.3570) Steps 646(635.87) | Grad Norm 1.1340(1.3871) | Total Time 14.00(14.00)\n",
      "Iter 3428 | Time 59.1684(58.8642) | Bit/dim 3.6765(3.6763) | Xent 1.0028(0.9972) | Loss 4.1779(4.1749) | Error 0.3565(0.3570) Steps 628(635.63) | Grad Norm 1.3206(1.3851) | Total Time 14.00(14.00)\n",
      "Iter 3429 | Time 58.3634(58.8492) | Bit/dim 3.6671(3.6760) | Xent 0.9708(0.9964) | Loss 4.1525(4.1742) | Error 0.3462(0.3567) Steps 628(635.40) | Grad Norm 0.5611(1.3604) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 59.5954(58.8716) | Bit/dim 3.6688(3.6758) | Xent 0.9943(0.9963) | Loss 4.1660(4.1740) | Error 0.3559(0.3566) Steps 646(635.72) | Grad Norm 0.5726(1.3368) | Total Time 14.00(14.00)\n",
      "Iter 3431 | Time 57.2084(58.8217) | Bit/dim 3.6696(3.6756) | Xent 0.9985(0.9964) | Loss 4.1688(4.1738) | Error 0.3614(0.3568) Steps 628(635.49) | Grad Norm 0.4772(1.3110) | Total Time 14.00(14.00)\n",
      "Iter 3432 | Time 57.7779(58.7904) | Bit/dim 3.6865(3.6760) | Xent 0.9844(0.9960) | Loss 4.1787(4.1740) | Error 0.3508(0.3566) Steps 628(635.26) | Grad Norm 0.6225(1.2903) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0572 | Time 23.5967, Epoch Time 391.6860(389.2930), Bit/dim 3.6764(best: 3.6765), Xent 0.9696, Loss 4.1612, Error 0.3439(best: 0.3437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3433 | Time 59.1207(58.8003) | Bit/dim 3.6828(3.6762) | Xent 0.9792(0.9955) | Loss 4.1724(4.1739) | Error 0.3502(0.3564) Steps 622(634.87) | Grad Norm 0.7262(1.2734) | Total Time 14.00(14.00)\n",
      "Iter 3434 | Time 61.2437(58.8736) | Bit/dim 3.6700(3.6760) | Xent 0.9860(0.9952) | Loss 4.1630(4.1736) | Error 0.3525(0.3563) Steps 634(634.84) | Grad Norm 5.6027(1.4033) | Total Time 14.00(14.00)\n",
      "Iter 3435 | Time 61.8901(58.9641) | Bit/dim 3.6797(3.6761) | Xent 0.9963(0.9953) | Loss 4.1779(4.1737) | Error 0.3502(0.3561) Steps 616(634.27) | Grad Norm 0.7532(1.3838) | Total Time 14.00(14.00)\n",
      "Iter 3436 | Time 55.5427(58.8615) | Bit/dim 3.6780(3.6762) | Xent 0.9877(0.9950) | Loss 4.1719(4.1737) | Error 0.3560(0.3561) Steps 622(633.91) | Grad Norm 0.6762(1.3625) | Total Time 14.00(14.00)\n",
      "Iter 3437 | Time 59.6050(58.8838) | Bit/dim 3.6795(3.6763) | Xent 0.9830(0.9947) | Loss 4.1710(4.1736) | Error 0.3490(0.3559) Steps 628(633.73) | Grad Norm 0.6027(1.3398) | Total Time 14.00(14.00)\n",
      "Iter 3438 | Time 59.3341(58.8973) | Bit/dim 3.6638(3.6759) | Xent 0.9987(0.9948) | Loss 4.1631(4.1733) | Error 0.3541(0.3558) Steps 628(633.56) | Grad Norm 1.0518(1.3311) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0573 | Time 23.5147, Epoch Time 395.9472(389.4926), Bit/dim 3.6762(best: 3.6764), Xent 0.9697, Loss 4.1610, Error 0.3416(best: 0.3437)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3439 | Time 59.3879(58.9120) | Bit/dim 3.6578(3.6753) | Xent 0.9857(0.9945) | Loss 4.1507(4.1726) | Error 0.3509(0.3557) Steps 634(633.57) | Grad Norm 1.9889(1.3508) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 58.9531(58.9132) | Bit/dim 3.6733(3.6753) | Xent 1.0018(0.9947) | Loss 4.1742(4.1726) | Error 0.3621(0.3559) Steps 646(633.94) | Grad Norm 1.0118(1.3407) | Total Time 14.00(14.00)\n",
      "Iter 3441 | Time 61.9639(59.0047) | Bit/dim 3.6887(3.6757) | Xent 1.0166(0.9954) | Loss 4.1970(4.1734) | Error 0.3680(0.3562) Steps 628(633.76) | Grad Norm 0.8993(1.3274) | Total Time 14.00(14.00)\n",
      "Iter 3442 | Time 56.3982(58.9266) | Bit/dim 3.6668(3.6754) | Xent 0.9903(0.9952) | Loss 4.1619(4.1730) | Error 0.3539(0.3562) Steps 622(633.41) | Grad Norm 1.6670(1.3376) | Total Time 14.00(14.00)\n",
      "Iter 3443 | Time 57.7570(58.8915) | Bit/dim 3.6856(3.6757) | Xent 0.9973(0.9953) | Loss 4.1843(4.1734) | Error 0.3611(0.3563) Steps 628(633.25) | Grad Norm 1.9779(1.3568) | Total Time 14.00(14.00)\n",
      "Iter 3444 | Time 57.9028(58.8618) | Bit/dim 3.6719(3.6756) | Xent 0.9935(0.9953) | Loss 4.1687(4.1732) | Error 0.3530(0.3562) Steps 628(633.09) | Grad Norm 1.5121(1.3615) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0574 | Time 23.7066, Epoch Time 391.7437(389.5602), Bit/dim 3.6768(best: 3.6762), Xent 0.9688, Loss 4.1612, Error 0.3429(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3445 | Time 59.8255(58.8907) | Bit/dim 3.6814(3.6758) | Xent 0.9881(0.9950) | Loss 4.1755(4.1733) | Error 0.3484(0.3560) Steps 634(633.12) | Grad Norm 1.2623(1.3585) | Total Time 14.00(14.00)\n",
      "Iter 3446 | Time 61.1393(58.9582) | Bit/dim 3.6725(3.6757) | Xent 1.0101(0.9955) | Loss 4.1776(4.1734) | Error 0.3608(0.3561) Steps 634(633.15) | Grad Norm 2.4220(1.3904) | Total Time 14.00(14.00)\n",
      "Iter 3447 | Time 58.6207(58.9481) | Bit/dim 3.6851(3.6760) | Xent 0.9752(0.9949) | Loss 4.1727(4.1734) | Error 0.3425(0.3557) Steps 640(633.35) | Grad Norm 1.6168(1.3972) | Total Time 14.00(14.00)\n",
      "Iter 3448 | Time 59.8496(58.9751) | Bit/dim 3.6634(3.6756) | Xent 0.9878(0.9947) | Loss 4.1573(4.1729) | Error 0.3555(0.3557) Steps 640(633.55) | Grad Norm 3.3898(1.4570) | Total Time 14.00(14.00)\n",
      "Iter 3449 | Time 57.0401(58.9170) | Bit/dim 3.6776(3.6757) | Xent 0.9852(0.9944) | Loss 4.1702(4.1728) | Error 0.3548(0.3557) Steps 634(633.56) | Grad Norm 2.1658(1.4782) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 59.2552(58.9272) | Bit/dim 3.6647(3.6753) | Xent 0.9774(0.9939) | Loss 4.1534(4.1723) | Error 0.3512(0.3556) Steps 628(633.40) | Grad Norm 2.2590(1.5017) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0575 | Time 23.8406, Epoch Time 395.2588(389.7311), Bit/dim 3.6763(best: 3.6762), Xent 0.9706, Loss 4.1616, Error 0.3428(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3451 | Time 60.6978(58.9803) | Bit/dim 3.6766(3.6754) | Xent 0.9818(0.9935) | Loss 4.1674(4.1721) | Error 0.3486(0.3553) Steps 634(633.42) | Grad Norm 2.5092(1.5319) | Total Time 14.00(14.00)\n",
      "Iter 3452 | Time 60.2044(59.0170) | Bit/dim 3.6730(3.6753) | Xent 0.9839(0.9932) | Loss 4.1649(4.1719) | Error 0.3534(0.3553) Steps 646(633.79) | Grad Norm 1.4457(1.5293) | Total Time 14.00(14.00)\n",
      "Iter 3453 | Time 59.6458(59.0359) | Bit/dim 3.6787(3.6754) | Xent 0.9885(0.9931) | Loss 4.1729(4.1719) | Error 0.3550(0.3553) Steps 634(633.80) | Grad Norm 2.5646(1.5604) | Total Time 14.00(14.00)\n",
      "Iter 3454 | Time 56.6136(58.9632) | Bit/dim 3.6631(3.6750) | Xent 0.9985(0.9932) | Loss 4.1624(4.1716) | Error 0.3578(0.3554) Steps 640(633.99) | Grad Norm 1.6375(1.5627) | Total Time 14.00(14.00)\n",
      "Iter 3455 | Time 58.4066(58.9465) | Bit/dim 3.6790(3.6751) | Xent 0.9658(0.9924) | Loss 4.1619(4.1714) | Error 0.3400(0.3549) Steps 616(633.45) | Grad Norm 2.7957(1.5997) | Total Time 14.00(14.00)\n",
      "Iter 3456 | Time 61.6348(59.0272) | Bit/dim 3.6785(3.6752) | Xent 0.9992(0.9926) | Loss 4.1781(4.1716) | Error 0.3554(0.3549) Steps 634(633.46) | Grad Norm 1.2389(1.5888) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0576 | Time 23.6945, Epoch Time 396.3938(389.9310), Bit/dim 3.6759(best: 3.6762), Xent 0.9696, Loss 4.1607, Error 0.3445(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3457 | Time 58.9829(59.0258) | Bit/dim 3.6776(3.6753) | Xent 0.9984(0.9928) | Loss 4.1768(4.1717) | Error 0.3534(0.3549) Steps 634(633.48) | Grad Norm 2.4699(1.6153) | Total Time 14.00(14.00)\n",
      "Iter 3458 | Time 60.1410(59.0593) | Bit/dim 3.6755(3.6753) | Xent 0.9914(0.9928) | Loss 4.1712(4.1717) | Error 0.3579(0.3550) Steps 628(633.31) | Grad Norm 1.5645(1.6138) | Total Time 14.00(14.00)\n",
      "Iter 3459 | Time 58.6557(59.0472) | Bit/dim 3.6749(3.6753) | Xent 1.0028(0.9931) | Loss 4.1764(4.1718) | Error 0.3539(0.3549) Steps 646(633.69) | Grad Norm 2.3773(1.6367) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 60.4558(59.0895) | Bit/dim 3.6799(3.6754) | Xent 0.9820(0.9927) | Loss 4.1709(4.1718) | Error 0.3472(0.3547) Steps 652(634.24) | Grad Norm 2.0913(1.6503) | Total Time 14.00(14.00)\n",
      "Iter 3461 | Time 58.2564(59.0645) | Bit/dim 3.6694(3.6753) | Xent 0.9770(0.9923) | Loss 4.1578(4.1714) | Error 0.3464(0.3544) Steps 646(634.60) | Grad Norm 0.9133(1.6282) | Total Time 14.00(14.00)\n",
      "Iter 3462 | Time 58.3194(59.0421) | Bit/dim 3.6728(3.6752) | Xent 0.9987(0.9924) | Loss 4.1722(4.1714) | Error 0.3634(0.3547) Steps 622(634.22) | Grad Norm 3.1244(1.6731) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0577 | Time 23.5007, Epoch Time 394.2279(390.0599), Bit/dim 3.6765(best: 3.6759), Xent 0.9660, Loss 4.1595, Error 0.3417(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3463 | Time 58.6084(59.0291) | Bit/dim 3.6761(3.6752) | Xent 0.9757(0.9919) | Loss 4.1640(4.1712) | Error 0.3529(0.3547) Steps 646(634.57) | Grad Norm 1.5097(1.6682) | Total Time 14.00(14.00)\n",
      "Iter 3464 | Time 58.6565(59.0179) | Bit/dim 3.6747(3.6752) | Xent 1.0002(0.9922) | Loss 4.1748(4.1713) | Error 0.3590(0.3548) Steps 634(634.55) | Grad Norm 2.5420(1.6944) | Total Time 14.00(14.00)\n",
      "Iter 3465 | Time 59.2268(59.0242) | Bit/dim 3.6627(3.6748) | Xent 0.9892(0.9921) | Loss 4.1573(4.1709) | Error 0.3550(0.3548) Steps 634(634.54) | Grad Norm 1.5133(1.6890) | Total Time 14.00(14.00)\n",
      "Iter 3466 | Time 58.7702(59.0166) | Bit/dim 3.6773(3.6749) | Xent 1.0015(0.9924) | Loss 4.1781(4.1711) | Error 0.3574(0.3549) Steps 640(634.70) | Grad Norm 2.8842(1.7248) | Total Time 14.00(14.00)\n",
      "Iter 3467 | Time 58.7867(59.0097) | Bit/dim 3.6777(3.6750) | Xent 0.9836(0.9921) | Loss 4.1695(4.1710) | Error 0.3495(0.3547) Steps 628(634.50) | Grad Norm 1.6734(1.7233) | Total Time 14.00(14.00)\n",
      "Iter 3468 | Time 58.4103(58.9917) | Bit/dim 3.6733(3.6749) | Xent 0.9768(0.9917) | Loss 4.1617(4.1708) | Error 0.3518(0.3546) Steps 628(634.31) | Grad Norm 1.5737(1.7188) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0578 | Time 23.6471, Epoch Time 391.7077(390.1094), Bit/dim 3.6764(best: 3.6759), Xent 0.9703, Loss 4.1615, Error 0.3422(best: 0.3416)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 3469 | Time 58.8483(58.9874) | Bit/dim 3.6680(3.6747) | Xent 0.9926(0.9917) | Loss 4.1643(4.1706) | Error 0.3524(0.3545) Steps 640(634.48) | Grad Norm 2.1769(1.7325) | Total Time 14.00(14.00)\n",
      "Iter 3470 | Time 56.3233(58.9075) | Bit/dim 3.6781(3.6748) | Xent 0.9802(0.9913) | Loss 4.1682(4.1705) | Error 0.3506(0.3544) Steps 640(634.64) | Grad Norm 1.0138(1.7110) | Total Time 14.00(14.00)\n",
      "Iter 3471 | Time 60.1371(58.9444) | Bit/dim 3.6649(3.6745) | Xent 1.0039(0.9917) | Loss 4.1668(4.1704) | Error 0.3554(0.3545) Steps 646(634.98) | Grad Norm 2.1118(1.7230) | Total Time 14.00(14.00)\n",
      "Iter 3472 | Time 63.3353(59.0761) | Bit/dim 3.6719(3.6745) | Xent 0.9816(0.9914) | Loss 4.1628(4.1702) | Error 0.3498(0.3543) Steps 634(634.95) | Grad Norm 1.7891(1.7250) | Total Time 14.00(14.00)\n",
      "Iter 3473 | Time 59.6515(59.0933) | Bit/dim 3.6764(3.6745) | Xent 0.9972(0.9916) | Loss 4.1750(4.1703) | Error 0.3534(0.3543) Steps 628(634.75) | Grad Norm 1.6749(1.7235) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run3/epoch_400_checkpt.pth --seed 3 --conditional True --controlled_tol True --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
