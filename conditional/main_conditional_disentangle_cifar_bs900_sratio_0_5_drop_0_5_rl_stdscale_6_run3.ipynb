{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run3/epoch_235_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run3', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 12930 | Time 24.4653(23.8217) | Bit/dim 3.5936(3.5872) | Xent 0.1478(0.2214) | Loss 9.2764(10.3678) | Error 0.0544(0.0778) Steps 922(918.02) | Grad Norm 4.0060(8.5731) | Total Time 0.00(0.00)\n",
      "Iter 12940 | Time 22.6487(23.6840) | Bit/dim 3.5632(3.5836) | Xent 0.1482(0.2025) | Loss 9.2676(10.0722) | Error 0.0489(0.0707) Steps 898(920.05) | Grad Norm 2.9109(7.3121) | Total Time 0.00(0.00)\n",
      "Iter 12950 | Time 22.9557(23.5237) | Bit/dim 3.5766(3.5794) | Xent 0.1288(0.1839) | Loss 8.9740(9.8307) | Error 0.0411(0.0638) Steps 898(919.00) | Grad Norm 3.1685(6.1046) | Total Time 0.00(0.00)\n",
      "Iter 12960 | Time 23.3949(23.4422) | Bit/dim 3.5638(3.5741) | Xent 0.1161(0.1711) | Loss 9.2935(9.6711) | Error 0.0356(0.0585) Steps 934(923.65) | Grad Norm 3.0112(5.1997) | Total Time 0.00(0.00)\n",
      "Iter 12970 | Time 22.8682(23.4096) | Bit/dim 3.5488(3.5710) | Xent 0.1059(0.1590) | Loss 9.2072(9.5471) | Error 0.0344(0.0539) Steps 940(923.86) | Grad Norm 2.1085(4.5225) | Total Time 0.00(0.00)\n",
      "Iter 12980 | Time 22.8451(23.3402) | Bit/dim 3.5568(3.5684) | Xent 0.1348(0.1523) | Loss 9.1828(9.4606) | Error 0.0489(0.0516) Steps 916(923.56) | Grad Norm 2.5856(4.0077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 110.1799, Epoch Time 1427.4372(1328.3497), Bit/dim 3.5691(best: inf), Xent 0.7899, Loss 3.9640, Error 0.2042(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12990 | Time 23.1104(23.3578) | Bit/dim 3.5969(3.5683) | Xent 0.1599(0.1500) | Loss 9.3190(10.1576) | Error 0.0522(0.0511) Steps 922(923.81) | Grad Norm 2.5917(3.6722) | Total Time 0.00(0.00)\n",
      "Iter 13000 | Time 23.8274(23.3184) | Bit/dim 3.5285(3.5642) | Xent 0.1533(0.1473) | Loss 9.2711(9.9139) | Error 0.0478(0.0501) Steps 934(923.60) | Grad Norm 3.0212(3.3862) | Total Time 0.00(0.00)\n",
      "Iter 13010 | Time 22.5479(23.3396) | Bit/dim 3.5542(3.5650) | Xent 0.1427(0.1451) | Loss 9.2159(9.7395) | Error 0.0478(0.0500) Steps 922(923.21) | Grad Norm 1.8710(3.2381) | Total Time 0.00(0.00)\n",
      "Iter 13020 | Time 22.6154(23.3258) | Bit/dim 3.5912(3.5638) | Xent 0.1363(0.1423) | Loss 9.2160(9.6035) | Error 0.0522(0.0490) Steps 898(923.60) | Grad Norm 3.3783(3.1396) | Total Time 0.00(0.00)\n",
      "Iter 13030 | Time 23.5498(23.3194) | Bit/dim 3.6101(3.5665) | Xent 0.1207(0.1420) | Loss 9.3351(9.5111) | Error 0.0411(0.0487) Steps 922(920.98) | Grad Norm 2.3799(3.0043) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 108.0673, Epoch Time 1406.9828(1330.7087), Bit/dim 3.5721(best: 3.5691), Xent 0.7991, Loss 3.9716, Error 0.2044(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13040 | Time 23.8971(23.2308) | Bit/dim 3.5799(3.5669) | Xent 0.1093(0.1386) | Loss 9.2699(10.3368) | Error 0.0322(0.0464) Steps 940(923.64) | Grad Norm 2.3783(2.9633) | Total Time 0.00(0.00)\n",
      "Iter 13050 | Time 23.3411(23.1951) | Bit/dim 3.5762(3.5640) | Xent 0.1113(0.1368) | Loss 9.2866(10.0416) | Error 0.0322(0.0459) Steps 940(926.03) | Grad Norm 2.0202(2.8407) | Total Time 0.00(0.00)\n",
      "Iter 13060 | Time 22.6713(23.1836) | Bit/dim 3.5652(3.5653) | Xent 0.1477(0.1389) | Loss 9.1979(9.8263) | Error 0.0522(0.0477) Steps 922(925.76) | Grad Norm 2.6088(2.9062) | Total Time 0.00(0.00)\n",
      "Iter 13070 | Time 22.6969(23.2144) | Bit/dim 3.5463(3.5625) | Xent 0.1013(0.1377) | Loss 9.0999(9.6644) | Error 0.0311(0.0474) Steps 928(925.81) | Grad Norm 3.4339(2.9113) | Total Time 0.00(0.00)\n",
      "Iter 13080 | Time 22.7565(23.2196) | Bit/dim 3.5965(3.5631) | Xent 0.1005(0.1353) | Loss 9.3363(9.5426) | Error 0.0367(0.0461) Steps 922(924.02) | Grad Norm 2.4498(2.9061) | Total Time 0.00(0.00)\n",
      "Iter 13090 | Time 23.0594(23.2222) | Bit/dim 3.5729(3.5622) | Xent 0.1496(0.1350) | Loss 9.1069(9.4570) | Error 0.0600(0.0460) Steps 922(924.02) | Grad Norm 3.4817(2.9718) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 107.3232, Epoch Time 1400.5825(1332.8049), Bit/dim 3.5741(best: 3.5691), Xent 0.8090, Loss 3.9786, Error 0.2048(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13100 | Time 23.4822(23.3054) | Bit/dim 3.5724(3.5618) | Xent 0.1379(0.1345) | Loss 9.2063(10.1577) | Error 0.0444(0.0461) Steps 910(924.82) | Grad Norm 4.2216(2.9956) | Total Time 0.00(0.00)\n",
      "Iter 13110 | Time 23.2144(23.2934) | Bit/dim 3.5692(3.5612) | Xent 0.1076(0.1338) | Loss 9.1504(9.8995) | Error 0.0311(0.0455) Steps 946(926.70) | Grad Norm 2.8284(2.9695) | Total Time 0.00(0.00)\n",
      "Iter 13120 | Time 23.9241(23.3081) | Bit/dim 3.5730(3.5616) | Xent 0.1255(0.1315) | Loss 9.3271(9.7233) | Error 0.0389(0.0441) Steps 958(927.02) | Grad Norm 3.9146(2.9643) | Total Time 0.00(0.00)\n",
      "Iter 13130 | Time 23.3441(23.3441) | Bit/dim 3.6028(3.5659) | Xent 0.1505(0.1326) | Loss 9.3308(9.6075) | Error 0.0500(0.0446) Steps 946(928.12) | Grad Norm 3.2352(3.1187) | Total Time 0.00(0.00)\n",
      "Iter 13140 | Time 22.7158(23.4202) | Bit/dim 3.5615(3.5630) | Xent 0.1711(0.1330) | Loss 9.4028(9.5248) | Error 0.0589(0.0449) Steps 934(930.90) | Grad Norm 3.1872(3.0931) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 108.5033, Epoch Time 1416.6118(1335.3191), Bit/dim 3.5797(best: 3.5691), Xent 0.8265, Loss 3.9930, Error 0.2060(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13150 | Time 22.9276(23.4336) | Bit/dim 3.5439(3.5640) | Xent 0.1133(0.1305) | Loss 9.0728(10.3291) | Error 0.0344(0.0439) Steps 910(930.75) | Grad Norm 3.2522(3.1447) | Total Time 0.00(0.00)\n",
      "Iter 13160 | Time 22.9864(23.4604) | Bit/dim 3.5773(3.5634) | Xent 0.1289(0.1292) | Loss 9.4179(10.0435) | Error 0.0467(0.0438) Steps 922(931.16) | Grad Norm 3.3381(3.0950) | Total Time 0.00(0.00)\n",
      "Iter 13170 | Time 23.4726(23.4525) | Bit/dim 3.5456(3.5641) | Xent 0.1370(0.1283) | Loss 9.0458(9.8286) | Error 0.0478(0.0440) Steps 916(931.43) | Grad Norm 3.0068(3.0768) | Total Time 0.00(0.00)\n",
      "Iter 13180 | Time 24.3209(23.5447) | Bit/dim 3.5722(3.5678) | Xent 0.1415(0.1295) | Loss 9.2050(9.6702) | Error 0.0467(0.0442) Steps 934(933.13) | Grad Norm 3.0632(3.1743) | Total Time 0.00(0.00)\n",
      "Iter 13190 | Time 24.1291(23.5510) | Bit/dim 3.5483(3.5677) | Xent 0.1335(0.1307) | Loss 9.1660(9.5599) | Error 0.0522(0.0446) Steps 898(932.08) | Grad Norm 3.9581(3.4418) | Total Time 0.00(0.00)\n",
      "Iter 13200 | Time 23.9348(23.5999) | Bit/dim 3.5795(3.5682) | Xent 0.1394(0.1308) | Loss 9.2069(9.4643) | Error 0.0422(0.0447) Steps 892(927.52) | Grad Norm 3.3311(3.6630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 108.6423, Epoch Time 1423.6030(1337.9676), Bit/dim 3.5786(best: 3.5691), Xent 0.8349, Loss 3.9961, Error 0.2064(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13210 | Time 24.3894(23.6258) | Bit/dim 3.5770(3.5703) | Xent 0.1376(0.1311) | Loss 9.2010(10.1898) | Error 0.0367(0.0443) Steps 964(929.10) | Grad Norm 3.6316(3.7281) | Total Time 0.00(0.00)\n",
      "Iter 13220 | Time 23.6412(23.7717) | Bit/dim 3.5931(3.5733) | Xent 0.1221(0.1340) | Loss 9.3103(9.9555) | Error 0.0467(0.0458) Steps 958(935.17) | Grad Norm 4.0370(4.0584) | Total Time 0.00(0.00)\n",
      "Iter 13230 | Time 24.5051(23.9180) | Bit/dim 3.5555(3.5736) | Xent 0.1308(0.1348) | Loss 9.2763(9.7772) | Error 0.0422(0.0458) Steps 958(935.32) | Grad Norm 2.9137(3.9703) | Total Time 0.00(0.00)\n",
      "Iter 13240 | Time 22.8353(24.0271) | Bit/dim 3.5609(3.5757) | Xent 0.1177(0.1333) | Loss 9.1179(9.6344) | Error 0.0422(0.0453) Steps 928(935.07) | Grad Norm 3.3473(3.8120) | Total Time 0.00(0.00)\n",
      "Iter 13250 | Time 23.6075(24.0655) | Bit/dim 3.5508(3.5740) | Xent 0.1279(0.1317) | Loss 9.1397(9.5338) | Error 0.0467(0.0446) Steps 928(935.46) | Grad Norm 2.6784(3.6753) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 110.1162, Epoch Time 1455.0759(1341.4809), Bit/dim 3.5870(best: 3.5691), Xent 0.8333, Loss 4.0037, Error 0.2056(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13260 | Time 25.4981(24.1332) | Bit/dim 3.6134(3.5729) | Xent 0.1332(0.1340) | Loss 9.2669(10.3443) | Error 0.0567(0.0454) Steps 916(937.26) | Grad Norm 4.6643(3.6899) | Total Time 0.00(0.00)\n",
      "Iter 13270 | Time 24.7773(24.2926) | Bit/dim 3.6115(3.5757) | Xent 0.1351(0.1320) | Loss 9.3122(10.0632) | Error 0.0544(0.0451) Steps 934(940.14) | Grad Norm 4.4473(3.8095) | Total Time 0.00(0.00)\n",
      "Iter 13280 | Time 23.3932(24.3757) | Bit/dim 3.5794(3.5775) | Xent 0.1304(0.1321) | Loss 9.3009(9.8563) | Error 0.0433(0.0453) Steps 910(939.72) | Grad Norm 5.2969(3.7116) | Total Time 0.00(0.00)\n",
      "Iter 13290 | Time 24.4450(24.4530) | Bit/dim 3.5712(3.5796) | Xent 0.1055(0.1343) | Loss 9.1279(9.7067) | Error 0.0344(0.0454) Steps 958(942.44) | Grad Norm 7.5730(4.4896) | Total Time 0.00(0.00)\n",
      "Iter 13300 | Time 25.8393(24.4880) | Bit/dim 3.5645(3.5783) | Xent 0.1377(0.1335) | Loss 9.2055(9.5899) | Error 0.0456(0.0453) Steps 922(944.49) | Grad Norm 4.1520(5.2855) | Total Time 0.00(0.00)\n",
      "Iter 13310 | Time 24.7558(24.5746) | Bit/dim 3.5837(3.5774) | Xent 0.1283(0.1327) | Loss 9.2497(9.5081) | Error 0.0456(0.0450) Steps 964(945.92) | Grad Norm 14.2597(5.6086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 110.3248, Epoch Time 1485.6034(1345.8046), Bit/dim 3.5782(best: 3.5691), Xent 0.8292, Loss 3.9928, Error 0.2078(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13320 | Time 26.4276(24.5575) | Bit/dim 3.6266(3.5799) | Xent 0.1598(0.1375) | Loss 9.4936(10.2458) | Error 0.0600(0.0470) Steps 1000(948.75) | Grad Norm 15.8257(7.3528) | Total Time 0.00(0.00)\n",
      "Iter 13330 | Time 26.4188(24.9318) | Bit/dim 3.6106(3.5951) | Xent 0.1888(0.1491) | Loss 9.3781(10.0267) | Error 0.0656(0.0508) Steps 994(955.87) | Grad Norm 9.0184(7.7215) | Total Time 0.00(0.00)\n",
      "Iter 13340 | Time 25.3653(25.2759) | Bit/dim 3.6098(3.5969) | Xent 0.1424(0.1520) | Loss 9.4049(9.8516) | Error 0.0500(0.0512) Steps 988(962.50) | Grad Norm 4.7340(7.4202) | Total Time 0.00(0.00)\n",
      "Iter 13350 | Time 25.9528(25.5652) | Bit/dim 3.5811(3.5926) | Xent 0.1399(0.1487) | Loss 9.3028(9.7041) | Error 0.0422(0.0498) Steps 976(964.32) | Grad Norm 2.7656(6.3080) | Total Time 0.00(0.00)\n",
      "Iter 13360 | Time 24.0034(25.5031) | Bit/dim 3.5777(3.5865) | Xent 0.1221(0.1450) | Loss 9.2569(9.5891) | Error 0.0378(0.0489) Steps 928(962.13) | Grad Norm 2.5190(5.4658) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 111.7892, Epoch Time 1544.8162(1351.7749), Bit/dim 3.5845(best: 3.5691), Xent 0.8382, Loss 4.0036, Error 0.2068(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13370 | Time 25.8667(25.5227) | Bit/dim 3.5919(3.5851) | Xent 0.1126(0.1391) | Loss 9.2143(10.3901) | Error 0.0433(0.0472) Steps 1000(961.92) | Grad Norm 5.6537(5.9066) | Total Time 0.00(0.00)\n",
      "Iter 13380 | Time 24.1959(25.5129) | Bit/dim 3.6078(3.5865) | Xent 0.1281(0.1394) | Loss 9.2240(10.1060) | Error 0.0433(0.0480) Steps 946(961.33) | Grad Norm 8.3837(12.6922) | Total Time 0.00(0.00)\n",
      "Iter 13390 | Time 26.9596(25.6076) | Bit/dim 3.6216(3.5894) | Xent 0.1633(0.1419) | Loss 9.4462(9.9028) | Error 0.0600(0.0487) Steps 1024(962.93) | Grad Norm 8.8321(12.9274) | Total Time 0.00(0.00)\n",
      "Iter 13400 | Time 26.1016(25.9399) | Bit/dim 3.6038(3.5957) | Xent 0.1669(0.1500) | Loss 9.4707(9.7728) | Error 0.0600(0.0517) Steps 982(969.94) | Grad Norm 16.6698(14.8876) | Total Time 0.00(0.00)\n",
      "Iter 13410 | Time 26.9352(26.2180) | Bit/dim 3.6371(3.6031) | Xent 0.1817(0.1548) | Loss 9.4320(9.6834) | Error 0.0644(0.0524) Steps 988(978.84) | Grad Norm 10.8991(14.9885) | Total Time 0.00(0.00)\n",
      "Iter 13420 | Time 25.8062(26.3349) | Bit/dim 3.5978(3.6015) | Xent 0.1305(0.1511) | Loss 9.1591(9.5801) | Error 0.0433(0.0507) Steps 940(977.97) | Grad Norm 2.9916(12.5954) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 114.8416, Epoch Time 1578.8977(1358.5886), Bit/dim 3.5945(best: 3.5691), Xent 0.8354, Loss 4.0122, Error 0.2082(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13430 | Time 25.5708(26.3752) | Bit/dim 3.5664(3.5961) | Xent 0.1376(0.1450) | Loss 9.2897(10.2806) | Error 0.0478(0.0487) Steps 964(978.07) | Grad Norm 18.1225(11.3244) | Total Time 0.00(0.00)\n",
      "Iter 13440 | Time 29.1244(26.5669) | Bit/dim 3.6078(3.5976) | Xent 0.1836(0.1450) | Loss 9.3242(10.0326) | Error 0.0644(0.0486) Steps 1048(982.97) | Grad Norm 33.9981(13.3258) | Total Time 0.00(0.00)\n",
      "Iter 13450 | Time 28.1213(27.0319) | Bit/dim 3.6690(3.6120) | Xent 0.2144(0.1588) | Loss 9.5676(9.8991) | Error 0.0544(0.0513) Steps 994(994.95) | Grad Norm 47.5988(25.7201) | Total Time 0.00(0.00)\n",
      "Iter 13460 | Time 26.0446(27.2115) | Bit/dim 3.5793(3.6142) | Xent 0.1455(0.1786) | Loss 9.2721(9.7907) | Error 0.0511(0.0561) Steps 1000(1000.83) | Grad Norm 3.6064(24.7002) | Total Time 0.00(0.00)\n",
      "Iter 13470 | Time 25.8509(26.8785) | Bit/dim 3.5698(3.6063) | Xent 0.1359(0.1698) | Loss 9.2289(9.6490) | Error 0.0456(0.0546) Steps 952(990.96) | Grad Norm 5.0624(19.3867) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 111.3818, Epoch Time 1613.9528(1366.2495), Bit/dim 3.5865(best: 3.5691), Xent 0.8293, Loss 4.0011, Error 0.2086(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13480 | Time 24.8155(26.5061) | Bit/dim 3.5845(3.6009) | Xent 0.1116(0.1595) | Loss 9.2136(10.4328) | Error 0.0311(0.0522) Steps 970(982.36) | Grad Norm 4.2206(15.3723) | Total Time 0.00(0.00)\n",
      "Iter 13490 | Time 25.9051(26.3374) | Bit/dim 3.5718(3.5941) | Xent 0.1110(0.1519) | Loss 9.2631(10.1211) | Error 0.0322(0.0497) Steps 946(977.30) | Grad Norm 3.9707(12.9322) | Total Time 0.00(0.00)\n",
      "Iter 13500 | Time 25.2974(26.0987) | Bit/dim 3.5519(3.5869) | Xent 0.1410(0.1483) | Loss 9.1838(9.8861) | Error 0.0511(0.0498) Steps 928(968.63) | Grad Norm 3.9013(11.2458) | Total Time 0.00(0.00)\n",
      "Iter 13510 | Time 25.7370(25.8713) | Bit/dim 3.5771(3.5826) | Xent 0.1292(0.1442) | Loss 9.1399(9.7215) | Error 0.0367(0.0482) Steps 928(965.26) | Grad Norm 2.7457(9.3386) | Total Time 0.00(0.00)\n",
      "Iter 13520 | Time 25.6801(25.6702) | Bit/dim 3.5842(3.5809) | Xent 0.1333(0.1367) | Loss 9.3430(9.5987) | Error 0.0467(0.0457) Steps 934(958.48) | Grad Norm 3.8610(8.5305) | Total Time 0.00(0.00)\n",
      "Iter 13530 | Time 25.1779(25.5907) | Bit/dim 3.5694(3.5806) | Xent 0.1571(0.1363) | Loss 9.2426(9.5107) | Error 0.0544(0.0455) Steps 976(959.11) | Grad Norm 78.2991(11.6163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 115.0234, Epoch Time 1529.7592(1371.1548), Bit/dim 3.6095(best: 3.5691), Xent 0.8921, Loss 4.0556, Error 0.2142(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13540 | Time 30.2511(26.3778) | Bit/dim 3.9494(3.6588) | Xent 0.6687(0.2630) | Loss 10.5899(10.5324) | Error 0.1933(0.0792) Steps 1078(976.75) | Grad Norm 108.8131(38.3376) | Total Time 0.00(0.00)\n",
      "Iter 13550 | Time 29.9863(27.2617) | Bit/dim 3.7533(3.7056) | Xent 0.3426(0.3292) | Loss 9.8890(10.4619) | Error 0.1244(0.1001) Steps 1060(1000.60) | Grad Norm 30.4980(53.8445) | Total Time 0.00(0.00)\n",
      "Iter 13560 | Time 29.0987(27.5472) | Bit/dim 3.6669(3.7001) | Xent 0.2940(0.3188) | Loss 9.6887(10.2553) | Error 0.1089(0.1001) Steps 1060(1004.39) | Grad Norm 12.9122(43.8060) | Total Time 0.00(0.00)\n",
      "Iter 13570 | Time 27.2476(27.5111) | Bit/dim 3.5982(3.6826) | Xent 0.1739(0.2911) | Loss 9.3157(10.0544) | Error 0.0633(0.0943) Steps 982(1004.88) | Grad Norm 5.6138(35.1248) | Total Time 0.00(0.00)\n",
      "Iter 13580 | Time 26.2771(27.4049) | Bit/dim 3.6281(3.6620) | Xent 0.1572(0.2564) | Loss 9.4905(9.8705) | Error 0.0478(0.0838) Steps 1000(1004.22) | Grad Norm 4.8310(27.5609) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 116.9875, Epoch Time 1687.5867(1380.6478), Bit/dim 3.6078(best: 3.5691), Xent 0.8124, Loss 4.0140, Error 0.2070(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13590 | Time 25.8105(27.4074) | Bit/dim 3.5787(3.6443) | Xent 0.1566(0.2304) | Loss 9.1281(10.6419) | Error 0.0589(0.0763) Steps 964(1003.43) | Grad Norm 9.1420(21.7074) | Total Time 0.00(0.00)\n",
      "Iter 13600 | Time 26.9062(27.2701) | Bit/dim 3.5537(3.6291) | Xent 0.1480(0.2072) | Loss 9.2189(10.2883) | Error 0.0589(0.0690) Steps 988(998.09) | Grad Norm 4.0596(16.9799) | Total Time 0.00(0.00)\n",
      "Iter 13610 | Time 26.1534(27.1235) | Bit/dim 3.5678(3.6142) | Xent 0.1243(0.1861) | Loss 9.2065(10.0074) | Error 0.0433(0.0619) Steps 970(995.24) | Grad Norm 2.8654(13.5633) | Total Time 0.00(0.00)\n",
      "Iter 13620 | Time 26.4352(26.9696) | Bit/dim 3.6089(3.6062) | Xent 0.1411(0.1716) | Loss 9.3913(9.8075) | Error 0.0422(0.0567) Steps 970(990.14) | Grad Norm 3.1704(10.8937) | Total Time 0.00(0.00)\n",
      "Iter 13630 | Time 27.1106(26.9139) | Bit/dim 3.5390(3.5997) | Xent 0.1182(0.1604) | Loss 9.1555(9.6592) | Error 0.0367(0.0536) Steps 1006(989.10) | Grad Norm 2.2979(8.9302) | Total Time 0.00(0.00)\n",
      "Iter 13640 | Time 26.3507(26.8125) | Bit/dim 3.5654(3.5931) | Xent 0.1205(0.1511) | Loss 9.1799(9.5500) | Error 0.0411(0.0502) Steps 982(987.98) | Grad Norm 3.1013(7.3950) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 115.6198, Epoch Time 1602.9282(1387.3162), Bit/dim 3.5826(best: 3.5691), Xent 0.8332, Loss 3.9992, Error 0.2066(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 26.4152(26.7761) | Bit/dim 3.5513(3.5875) | Xent 0.1481(0.1441) | Loss 9.2107(10.2565) | Error 0.0567(0.0488) Steps 964(983.14) | Grad Norm 2.7447(6.3262) | Total Time 0.00(0.00)\n",
      "Iter 13660 | Time 26.2217(26.6924) | Bit/dim 3.5873(3.5865) | Xent 0.1209(0.1385) | Loss 9.2617(9.9970) | Error 0.0456(0.0468) Steps 952(980.17) | Grad Norm 2.2719(5.5101) | Total Time 0.00(0.00)\n",
      "Iter 13670 | Time 26.5664(26.6499) | Bit/dim 3.5712(3.5853) | Xent 0.1401(0.1343) | Loss 9.2968(9.8011) | Error 0.0489(0.0451) Steps 982(979.66) | Grad Norm 5.2092(5.4430) | Total Time 0.00(0.00)\n",
      "Iter 13680 | Time 27.1479(26.6835) | Bit/dim 3.5714(3.5828) | Xent 0.1145(0.1309) | Loss 9.3580(9.6606) | Error 0.0344(0.0438) Steps 1012(982.69) | Grad Norm 2.3294(4.8649) | Total Time 0.00(0.00)\n",
      "Iter 13690 | Time 27.0044(26.5910) | Bit/dim 3.5430(3.5775) | Xent 0.1268(0.1288) | Loss 9.2635(9.5464) | Error 0.0456(0.0431) Steps 1006(978.93) | Grad Norm 3.4871(4.5614) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 116.2835, Epoch Time 1595.1029(1393.5498), Bit/dim 3.5812(best: 3.5691), Xent 0.8475, Loss 4.0050, Error 0.2075(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 26.4269(26.5743) | Bit/dim 3.5787(3.5766) | Xent 0.1335(0.1285) | Loss 9.2754(10.3661) | Error 0.0422(0.0431) Steps 958(973.16) | Grad Norm 3.6325(5.0007) | Total Time 0.00(0.00)\n",
      "Iter 13710 | Time 26.9473(26.5383) | Bit/dim 3.5996(3.5779) | Xent 0.1219(0.1276) | Loss 9.3650(10.0790) | Error 0.0389(0.0429) Steps 994(973.55) | Grad Norm 5.3985(5.9801) | Total Time 0.00(0.00)\n",
      "Iter 13720 | Time 25.6129(26.3435) | Bit/dim 3.6174(3.5770) | Xent 0.1259(0.1306) | Loss 9.2797(9.8658) | Error 0.0433(0.0437) Steps 970(970.53) | Grad Norm 4.6390(9.2895) | Total Time 0.00(0.00)\n",
      "Iter 13730 | Time 25.5671(26.2495) | Bit/dim 3.5764(3.5744) | Xent 0.1377(0.1319) | Loss 9.3609(9.7111) | Error 0.0467(0.0439) Steps 970(969.40) | Grad Norm 4.8339(8.5949) | Total Time 0.00(0.00)\n",
      "Iter 13740 | Time 25.8280(26.0765) | Bit/dim 3.5756(3.5752) | Xent 0.1210(0.1324) | Loss 9.1923(9.5865) | Error 0.0422(0.0443) Steps 940(967.13) | Grad Norm 5.6095(9.2991) | Total Time 0.00(0.00)\n",
      "Iter 13750 | Time 25.9784(25.9901) | Bit/dim 3.5616(3.5719) | Xent 0.1178(0.1294) | Loss 9.2234(9.4909) | Error 0.0356(0.0434) Steps 928(966.15) | Grad Norm 14.2849(8.1854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 112.6758, Epoch Time 1560.1445(1398.5476), Bit/dim 3.5806(best: 3.5691), Xent 0.8501, Loss 4.0056, Error 0.2071(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 24.9983(25.8692) | Bit/dim 3.5693(3.5710) | Xent 0.1321(0.1296) | Loss 9.2375(10.2017) | Error 0.0422(0.0432) Steps 958(965.49) | Grad Norm 18.9366(11.5773) | Total Time 0.00(0.00)\n",
      "Iter 13770 | Time 27.4609(26.1047) | Bit/dim 3.6967(3.5905) | Xent 0.2778(0.1490) | Loss 9.6349(10.0097) | Error 0.0844(0.0502) Steps 1006(972.73) | Grad Norm 24.6878(16.6894) | Total Time 0.00(0.00)\n",
      "Iter 13780 | Time 27.8408(26.7585) | Bit/dim 3.6914(3.6166) | Xent 0.2175(0.1737) | Loss 9.5922(9.9070) | Error 0.0733(0.0588) Steps 1024(984.95) | Grad Norm 26.9745(18.8071) | Total Time 0.00(0.00)\n",
      "Iter 13790 | Time 27.6957(27.2223) | Bit/dim 3.6127(3.6209) | Xent 0.1392(0.1725) | Loss 9.3892(9.7805) | Error 0.0511(0.0590) Steps 1000(998.08) | Grad Norm 12.4444(17.9241) | Total Time 0.00(0.00)\n",
      "Iter 13800 | Time 28.0014(27.3696) | Bit/dim 3.5890(3.6165) | Xent 0.1621(0.1688) | Loss 9.2159(9.6562) | Error 0.0522(0.0576) Steps 1006(1000.54) | Grad Norm 5.8599(15.9482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 120.3407, Epoch Time 1649.3270(1406.0710), Bit/dim 3.6027(best: 3.5691), Xent 0.8326, Loss 4.0190, Error 0.2062(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 27.2714(27.4557) | Bit/dim 3.5541(3.6091) | Xent 0.1184(0.1616) | Loss 9.2225(10.5020) | Error 0.0333(0.0551) Steps 1018(1002.59) | Grad Norm 6.5508(13.9009) | Total Time 0.00(0.00)\n",
      "Iter 13820 | Time 27.0599(27.3869) | Bit/dim 3.5754(3.6044) | Xent 0.1413(0.1544) | Loss 9.3597(10.1838) | Error 0.0511(0.0527) Steps 1018(1002.12) | Grad Norm 28.4198(13.4989) | Total Time 0.00(0.00)\n",
      "Iter 13830 | Time 29.0505(27.5836) | Bit/dim 3.6387(3.6067) | Xent 0.1503(0.1529) | Loss 9.4456(9.9740) | Error 0.0522(0.0517) Steps 1042(1008.46) | Grad Norm 8.7554(14.3628) | Total Time 0.00(0.00)\n",
      "Iter 13840 | Time 29.1235(28.0005) | Bit/dim 3.6645(3.6130) | Xent 0.1914(0.1597) | Loss 9.5241(9.8363) | Error 0.0667(0.0539) Steps 1048(1018.59) | Grad Norm 30.2970(20.6668) | Total Time 0.00(0.00)\n",
      "Iter 13850 | Time 30.9872(28.6343) | Bit/dim 3.6723(3.6246) | Xent 0.2545(0.1733) | Loss 9.6324(9.7603) | Error 0.0744(0.0577) Steps 1102(1029.48) | Grad Norm 102.3613(32.0587) | Total Time 0.00(0.00)\n",
      "Iter 13860 | Time 30.7767(29.0648) | Bit/dim 3.6380(3.6316) | Xent 0.1890(0.1800) | Loss 9.4928(9.7023) | Error 0.0589(0.0601) Steps 1102(1041.03) | Grad Norm 65.0344(37.6700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 123.9819, Epoch Time 1731.8457(1415.8443), Bit/dim 3.6347(best: 3.5691), Xent 0.8417, Loss 4.0555, Error 0.2138(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 29.5353(29.1312) | Bit/dim 3.6008(3.6274) | Xent 0.1706(0.1758) | Loss 9.4451(10.3901) | Error 0.0500(0.0594) Steps 1024(1038.61) | Grad Norm 41.5336(35.2270) | Total Time 0.00(0.00)\n",
      "Iter 13880 | Time 28.5821(29.1174) | Bit/dim 3.6047(3.6187) | Xent 0.1538(0.1686) | Loss 9.4404(10.1241) | Error 0.0556(0.0574) Steps 1066(1039.10) | Grad Norm 28.1302(33.8551) | Total Time 0.00(0.00)\n",
      "Iter 13890 | Time 32.3517(29.4636) | Bit/dim 3.8626(3.6492) | Xent 0.4197(0.2019) | Loss 10.2404(10.0395) | Error 0.1267(0.0660) Steps 1120(1047.69) | Grad Norm 442.6207(73.3123) | Total Time 0.00(0.00)\n",
      "Iter 13900 | Time 30.7325(30.0263) | Bit/dim 3.7982(3.7044) | Xent 0.5048(0.2987) | Loss 10.1766(10.1437) | Error 0.1422(0.0854) Steps 1108(1071.37) | Grad Norm 56.2231(116.0264) | Total Time 0.00(0.00)\n",
      "Iter 13910 | Time 28.6891(29.8543) | Bit/dim 3.6669(3.7005) | Xent 0.2614(0.2967) | Loss 9.5906(10.0110) | Error 0.0956(0.0888) Steps 1018(1068.42) | Grad Norm 9.3950(92.4773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 120.3708, Epoch Time 1779.3316(1426.7489), Bit/dim 3.6342(best: 3.5691), Xent 0.7944, Loss 4.0314, Error 0.2141(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 27.6024(29.4192) | Bit/dim 3.6113(3.6822) | Xent 0.1814(0.2718) | Loss 9.4313(10.7795) | Error 0.0567(0.0834) Steps 1000(1055.45) | Grad Norm 12.1754(71.1874) | Total Time 0.00(0.00)\n",
      "Iter 13930 | Time 29.1559(29.0897) | Bit/dim 3.5837(3.6614) | Xent 0.1644(0.2453) | Loss 9.3746(10.4137) | Error 0.0500(0.0768) Steps 1018(1046.68) | Grad Norm 6.2242(54.2596) | Total Time 0.00(0.00)\n",
      "Iter 13940 | Time 27.9189(28.7828) | Bit/dim 3.5916(3.6425) | Xent 0.1478(0.2218) | Loss 9.3664(10.1305) | Error 0.0544(0.0710) Steps 982(1035.93) | Grad Norm 5.0114(41.4820) | Total Time 0.00(0.00)\n",
      "Iter 13950 | Time 31.6778(28.9139) | Bit/dim 3.8350(3.6610) | Xent 0.3693(0.2350) | Loss 10.0266(10.0209) | Error 0.1267(0.0758) Steps 1132(1041.01) | Grad Norm 67.3456(59.1709) | Total Time 0.00(0.00)\n",
      "Iter 13960 | Time 32.2662(29.7224) | Bit/dim 3.7934(3.7044) | Xent 0.3142(0.2608) | Loss 9.9527(10.0265) | Error 0.0867(0.0847) Steps 1138(1057.92) | Grad Norm 98.3481(58.7343) | Total Time 0.00(0.00)\n",
      "Iter 13970 | Time 32.2750(30.3839) | Bit/dim 3.7608(3.7239) | Xent 0.2796(0.2720) | Loss 9.8945(9.9968) | Error 0.0944(0.0885) Steps 1114(1071.82) | Grad Norm 102.6648(77.3233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 130.0272, Epoch Time 1783.1924(1437.4422), Bit/dim 3.7432(best: 3.5691), Xent 0.8781, Loss 4.1822, Error 0.2281(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 31.7120(30.6584) | Bit/dim 3.6649(3.7152) | Xent 0.1971(0.2622) | Loss 9.4620(10.6837) | Error 0.0622(0.0858) Steps 1114(1077.34) | Grad Norm 52.0268(74.6008) | Total Time 0.00(0.00)\n",
      "Iter 13990 | Time 31.4776(30.6709) | Bit/dim 3.6540(3.7004) | Xent 0.1831(0.2422) | Loss 9.5825(10.3740) | Error 0.0578(0.0794) Steps 1060(1076.34) | Grad Norm 36.9873(65.9724) | Total Time 0.00(0.00)\n",
      "Iter 14000 | Time 30.8835(30.5573) | Bit/dim 3.6241(3.6809) | Xent 0.1612(0.2266) | Loss 9.4489(10.1397) | Error 0.0511(0.0742) Steps 1036(1071.51) | Grad Norm 9.7550(52.9008) | Total Time 0.00(0.00)\n",
      "Iter 14010 | Time 31.0604(30.4608) | Bit/dim 3.6443(3.6629) | Xent 0.1618(0.2114) | Loss 9.4709(9.9506) | Error 0.0611(0.0703) Steps 1006(1062.21) | Grad Norm 32.6894(44.9069) | Total Time 0.00(0.00)\n",
      "Iter 14020 | Time 28.9401(30.3454) | Bit/dim 3.6315(3.6494) | Xent 0.1544(0.1989) | Loss 9.5043(9.8093) | Error 0.0467(0.0661) Steps 1048(1063.76) | Grad Norm 10.7756(36.4839) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 122.7110, Epoch Time 1816.1413(1448.8032), Bit/dim 3.6130(best: 3.5691), Xent 0.8216, Loss 4.0237, Error 0.2061(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 29.0753(30.1939) | Bit/dim 3.5757(3.6383) | Xent 0.1449(0.1856) | Loss 9.1622(10.6244) | Error 0.0533(0.0623) Steps 1012(1057.81) | Grad Norm 11.1689(29.7268) | Total Time 0.00(0.00)\n",
      "Iter 14040 | Time 29.5673(30.0517) | Bit/dim 3.5887(3.6293) | Xent 0.1505(0.1746) | Loss 9.4039(10.2906) | Error 0.0511(0.0592) Steps 1048(1054.99) | Grad Norm 6.5360(26.3526) | Total Time 0.00(0.00)\n",
      "Iter 14050 | Time 29.3771(30.0992) | Bit/dim 3.5976(3.6222) | Xent 0.1638(0.1692) | Loss 9.3898(10.0550) | Error 0.0589(0.0577) Steps 1048(1054.57) | Grad Norm 19.2119(23.4174) | Total Time 0.00(0.00)\n",
      "Iter 14060 | Time 28.3845(30.0550) | Bit/dim 3.5945(3.6177) | Xent 0.1561(0.1642) | Loss 9.1221(9.8862) | Error 0.0600(0.0558) Steps 1012(1054.03) | Grad Norm 19.4552(21.6643) | Total Time 0.00(0.00)\n",
      "Iter 14070 | Time 29.5628(29.9542) | Bit/dim 3.6079(3.6141) | Xent 0.1295(0.1568) | Loss 9.3846(9.7493) | Error 0.0389(0.0535) Steps 1054(1052.80) | Grad Norm 231.0729(31.1148) | Total Time 0.00(0.00)\n",
      "Iter 14080 | Time 31.1319(30.1659) | Bit/dim 3.6842(3.6229) | Xent 0.1946(0.1687) | Loss 9.6171(9.6862) | Error 0.0711(0.0571) Steps 1054(1054.03) | Grad Norm 49.9608(47.7802) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 126.8524, Epoch Time 1797.4779(1459.2634), Bit/dim 3.6558(best: 3.5691), Xent 0.8653, Loss 4.0884, Error 0.2151(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 32.0996(30.3273) | Bit/dim 3.6324(3.6281) | Xent 0.1832(0.1715) | Loss 9.5272(10.4424) | Error 0.0600(0.0576) Steps 1114(1059.53) | Grad Norm 24.2063(48.8045) | Total Time 0.00(0.00)\n",
      "Iter 14100 | Time 31.3313(30.6527) | Bit/dim 3.7989(3.6414) | Xent 0.3091(0.1791) | Loss 9.8156(10.2144) | Error 0.0889(0.0592) Steps 1114(1063.91) | Grad Norm 332.9681(68.6165) | Total Time 0.00(0.00)\n",
      "Iter 14110 | Time 33.6623(31.0452) | Bit/dim 3.6838(3.6637) | Xent 0.2705(0.2026) | Loss 9.7823(10.1040) | Error 0.0922(0.0671) Steps 1144(1077.71) | Grad Norm 31.6893(82.0151) | Total Time 0.00(0.00)\n",
      "Iter 14120 | Time 31.7957(31.2465) | Bit/dim 3.6558(3.6667) | Xent 0.1628(0.2047) | Loss 9.4920(9.9712) | Error 0.0556(0.0668) Steps 1102(1080.67) | Grad Norm 24.6200(78.2565) | Total Time 0.00(0.00)\n",
      "Iter 14130 | Time 32.3966(31.4010) | Bit/dim 3.6128(3.6557) | Xent 0.1381(0.1948) | Loss 9.5002(9.8365) | Error 0.0478(0.0645) Steps 1060(1082.29) | Grad Norm 31.6773(67.5765) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 127.1096, Epoch Time 1884.4108(1472.0178), Bit/dim 3.6267(best: 3.5691), Xent 0.8301, Loss 4.0418, Error 0.2097(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 30.7004(31.3038) | Bit/dim 3.6291(3.6449) | Xent 0.1557(0.1836) | Loss 9.3443(10.6670) | Error 0.0544(0.0613) Steps 1114(1081.86) | Grad Norm 39.0955(56.5699) | Total Time 0.00(0.00)\n",
      "Iter 14150 | Time 30.7601(31.0505) | Bit/dim 3.6498(3.6383) | Xent 0.1378(0.1712) | Loss 9.5446(10.3375) | Error 0.0533(0.0574) Steps 1084(1077.53) | Grad Norm 18.1127(47.6282) | Total Time 0.00(0.00)\n",
      "Iter 14160 | Time 30.6240(30.9044) | Bit/dim 3.5783(3.6283) | Xent 0.1531(0.1644) | Loss 9.3394(10.0873) | Error 0.0467(0.0551) Steps 1060(1074.79) | Grad Norm 27.3737(43.8061) | Total Time 0.00(0.00)\n",
      "Iter 14170 | Time 29.7750(30.7831) | Bit/dim 3.6522(3.6233) | Xent 0.1239(0.1576) | Loss 9.4504(9.8904) | Error 0.0456(0.0530) Steps 1108(1073.83) | Grad Norm 424.9520(51.2453) | Total Time 0.00(0.00)\n",
      "Iter 14180 | Time 34.1873(31.1448) | Bit/dim 4.1763(3.6714) | Xent 0.7660(0.2130) | Loss 11.2318(9.9296) | Error 0.2233(0.0678) Steps 1138(1084.21) | Grad Norm 1244.1081(128.4097) | Total Time 0.00(0.00)\n",
      "Iter 14190 | Time 33.6698(31.7608) | Bit/dim 3.7437(3.7346) | Xent 0.3730(0.2992) | Loss 10.0029(10.0727) | Error 0.1167(0.0914) Steps 1156(1103.41) | Grad Norm 92.0086(159.0535) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 130.1918, Epoch Time 1870.0035(1483.9574), Bit/dim 3.7511(best: 3.5691), Xent 0.8406, Loss 4.1714, Error 0.2298(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 30.8776(31.7973) | Bit/dim 3.7176(3.7272) | Xent 0.2243(0.2993) | Loss 9.6552(10.8158) | Error 0.0744(0.0935) Steps 1096(1107.92) | Grad Norm 67.9550(139.2448) | Total Time 0.00(0.00)\n",
      "Iter 14210 | Time 31.4735(31.6472) | Bit/dim 3.6369(3.7084) | Xent 0.1687(0.2760) | Loss 9.5144(10.4876) | Error 0.0678(0.0884) Steps 1078(1101.81) | Grad Norm 27.4042(115.6160) | Total Time 0.00(0.00)\n",
      "Iter 14220 | Time 31.5445(31.3094) | Bit/dim 3.6492(3.6897) | Xent 0.1596(0.2501) | Loss 9.5748(10.2163) | Error 0.0522(0.0807) Steps 1048(1091.49) | Grad Norm 7.1861(89.7545) | Total Time 0.00(0.00)\n",
      "Iter 14230 | Time 30.6610(31.1022) | Bit/dim 3.6096(3.6709) | Xent 0.1673(0.2255) | Loss 9.3903(10.0034) | Error 0.0511(0.0731) Steps 1042(1083.43) | Grad Norm 30.7099(71.2247) | Total Time 0.00(0.00)\n",
      "Iter 14240 | Time 29.1495(30.8778) | Bit/dim 3.5863(3.6551) | Xent 0.1607(0.2091) | Loss 9.3027(9.8355) | Error 0.0533(0.0684) Steps 1012(1073.39) | Grad Norm 12.4478(55.2684) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 125.2494, Epoch Time 1839.5340(1494.6247), Bit/dim 3.6105(best: 3.5691), Xent 0.8291, Loss 4.0251, Error 0.2100(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 30.3862(30.7567) | Bit/dim 3.6199(3.6419) | Xent 0.1610(0.1938) | Loss 9.3024(10.6616) | Error 0.0589(0.0642) Steps 1030(1070.11) | Grad Norm 11.3407(44.5231) | Total Time 0.00(0.00)\n",
      "Iter 14260 | Time 29.9232(30.6136) | Bit/dim 3.6216(3.6321) | Xent 0.1276(0.1796) | Loss 9.3344(10.3255) | Error 0.0367(0.0595) Steps 1042(1066.89) | Grad Norm 7.8319(36.7343) | Total Time 0.00(0.00)\n",
      "Iter 14270 | Time 30.7637(30.4317) | Bit/dim 3.5774(3.6213) | Xent 0.1529(0.1710) | Loss 9.4489(10.0668) | Error 0.0567(0.0573) Steps 1084(1063.00) | Grad Norm 6.2501(30.7587) | Total Time 0.00(0.00)\n",
      "Iter 14280 | Time 31.3184(30.3996) | Bit/dim 3.5664(3.6131) | Xent 0.1436(0.1644) | Loss 9.2443(9.8813) | Error 0.0433(0.0550) Steps 1096(1061.66) | Grad Norm 17.9366(37.5892) | Total Time 0.00(0.00)\n",
      "Iter 14290 | Time 29.0755(29.9553) | Bit/dim 3.5694(3.6099) | Xent 0.1380(0.1576) | Loss 9.3291(9.7478) | Error 0.0489(0.0535) Steps 1030(1052.96) | Grad Norm 73.6204(35.6866) | Total Time 0.00(0.00)\n",
      "Iter 14300 | Time 28.5546(29.6682) | Bit/dim 3.6021(3.6079) | Xent 0.1401(0.1558) | Loss 9.2892(9.6431) | Error 0.0567(0.0529) Steps 1042(1048.15) | Grad Norm 9.0669(30.6693) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 120.3909, Epoch Time 1770.5381(1502.9021), Bit/dim 3.6005(best: 3.5691), Xent 0.8306, Loss 4.0158, Error 0.2071(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 29.3857(29.5097) | Bit/dim 3.6023(3.6042) | Xent 0.1223(0.1485) | Loss 9.2973(10.3653) | Error 0.0456(0.0506) Steps 1048(1044.54) | Grad Norm 219.0778(31.2010) | Total Time 0.00(0.00)\n",
      "Iter 14320 | Time 26.2094(29.1097) | Bit/dim 3.6211(3.6034) | Xent 0.1415(0.1456) | Loss 9.2705(10.1002) | Error 0.0511(0.0495) Steps 958(1034.67) | Grad Norm 6.2772(35.6279) | Total Time 0.00(0.00)\n",
      "Iter 14330 | Time 26.7705(28.5923) | Bit/dim 3.5623(3.6009) | Xent 0.1407(0.1445) | Loss 9.2527(9.8871) | Error 0.0467(0.0498) Steps 1012(1022.73) | Grad Norm 92.2234(30.2561) | Total Time 0.00(0.00)\n",
      "Iter 14340 | Time 26.0344(27.9886) | Bit/dim 3.5973(3.6000) | Xent 0.1343(0.1452) | Loss 9.3189(9.7328) | Error 0.0422(0.0498) Steps 970(1010.31) | Grad Norm 4.5785(25.8919) | Total Time 0.00(0.00)\n",
      "Iter 14350 | Time 26.2016(27.4256) | Bit/dim 3.6272(3.5989) | Xent 0.1437(0.1442) | Loss 9.3034(9.6242) | Error 0.0433(0.0488) Steps 970(998.26) | Grad Norm 4.6847(21.8029) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 114.8681, Epoch Time 1630.7825(1506.7385), Bit/dim 3.5975(best: 3.5691), Xent 0.8484, Loss 4.0217, Error 0.2108(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 26.0887(27.1943) | Bit/dim 3.5698(3.5974) | Xent 0.1144(0.1427) | Loss 9.2721(10.4607) | Error 0.0300(0.0483) Steps 1006(992.78) | Grad Norm 31.5459(22.3703) | Total Time 0.00(0.00)\n",
      "Iter 14370 | Time 27.1054(27.2005) | Bit/dim 3.6040(3.5986) | Xent 0.1683(0.1433) | Loss 9.3866(10.1752) | Error 0.0700(0.0489) Steps 976(991.41) | Grad Norm 118.8664(37.4184) | Total Time 0.00(0.00)\n",
      "Iter 14380 | Time 29.1446(27.4948) | Bit/dim 3.7635(3.6208) | Xent 0.2213(0.1565) | Loss 9.7518(10.0114) | Error 0.0789(0.0540) Steps 1048(995.35) | Grad Norm 250.6183(66.9718) | Total Time 0.00(0.00)\n",
      "Iter 14390 | Time 30.8666(28.2369) | Bit/dim 3.7908(3.6670) | Xent 0.3407(0.2011) | Loss 9.9950(10.0012) | Error 0.1144(0.0690) Steps 1084(1013.54) | Grad Norm 73.8939(79.1791) | Total Time 0.00(0.00)\n",
      "Iter 14400 | Time 32.1112(28.9494) | Bit/dim 3.7654(3.6948) | Xent 0.3643(0.2385) | Loss 9.9622(9.9867) | Error 0.1300(0.0809) Steps 1078(1031.15) | Grad Norm 31.8928(77.7019) | Total Time 0.00(0.00)\n",
      "Iter 14410 | Time 30.5245(29.5271) | Bit/dim 3.7270(3.7032) | Xent 0.1927(0.2438) | Loss 9.5859(9.9218) | Error 0.0667(0.0839) Steps 1060(1043.31) | Grad Norm 11.1387(68.0144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 126.8160, Epoch Time 1756.5290(1514.2322), Bit/dim 3.7135(best: 3.5691), Xent 0.8017, Loss 4.1143, Error 0.2158(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 30.3962(30.0220) | Bit/dim 3.6540(3.7004) | Xent 0.2241(0.2385) | Loss 9.3861(10.6097) | Error 0.0833(0.0823) Steps 1066(1053.20) | Grad Norm 33.5873(56.3472) | Total Time 0.00(0.00)\n",
      "Iter 14430 | Time 31.4096(30.2620) | Bit/dim 3.6622(3.6922) | Xent 0.1780(0.2259) | Loss 9.5935(10.3240) | Error 0.0600(0.0781) Steps 1060(1054.83) | Grad Norm 15.9905(48.4108) | Total Time 0.00(0.00)\n",
      "Iter 14440 | Time 30.8791(30.3852) | Bit/dim 3.6220(3.6786) | Xent 0.1993(0.2148) | Loss 9.3431(10.1008) | Error 0.0544(0.0742) Steps 1096(1057.62) | Grad Norm 8.2009(40.3469) | Total Time 0.00(0.00)\n",
      "Iter 14450 | Time 30.2667(30.5049) | Bit/dim 3.6112(3.6644) | Xent 0.1570(0.2001) | Loss 9.4003(9.9218) | Error 0.0556(0.0691) Steps 1036(1060.87) | Grad Norm 9.7768(34.9779) | Total Time 0.00(0.00)\n",
      "Iter 14460 | Time 30.7567(30.4651) | Bit/dim 3.6070(3.6515) | Xent 0.1732(0.1899) | Loss 9.4651(9.7884) | Error 0.0533(0.0657) Steps 1054(1061.68) | Grad Norm 9.8969(32.0738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 125.5594, Epoch Time 1839.0021(1523.9753), Bit/dim 3.6272(best: 3.5691), Xent 0.8225, Loss 4.0385, Error 0.2065(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 30.8486(30.4485) | Bit/dim 3.6049(3.6418) | Xent 0.1382(0.1828) | Loss 9.3476(10.5899) | Error 0.0544(0.0636) Steps 1042(1060.85) | Grad Norm 40.8341(28.4176) | Total Time 0.00(0.00)\n",
      "Iter 14480 | Time 30.8192(30.5686) | Bit/dim 3.6343(3.6351) | Xent 0.1323(0.1729) | Loss 9.4016(10.2821) | Error 0.0478(0.0610) Steps 1108(1064.26) | Grad Norm 11.0944(25.9395) | Total Time 0.00(0.00)\n",
      "Iter 14490 | Time 30.6891(30.6707) | Bit/dim 3.6039(3.6284) | Xent 0.1600(0.1667) | Loss 9.4059(10.0434) | Error 0.0500(0.0581) Steps 1072(1064.03) | Grad Norm 22.7292(29.6764) | Total Time 0.00(0.00)\n",
      "Iter 14500 | Time 31.0696(30.7465) | Bit/dim 3.6446(3.6305) | Xent 0.1916(0.1662) | Loss 9.4600(9.8967) | Error 0.0689(0.0576) Steps 1048(1064.35) | Grad Norm 57.0805(59.7458) | Total Time 0.00(0.00)\n",
      "Iter 14510 | Time 31.6376(30.8844) | Bit/dim 3.6153(3.6293) | Xent 0.1295(0.1680) | Loss 9.3215(9.7763) | Error 0.0356(0.0581) Steps 1090(1067.52) | Grad Norm 32.3354(58.8197) | Total Time 0.00(0.00)\n",
      "Iter 14520 | Time 31.2250(31.0107) | Bit/dim 3.6386(3.6322) | Xent 0.2025(0.1692) | Loss 9.4868(9.6915) | Error 0.0700(0.0577) Steps 1060(1070.65) | Grad Norm 80.1766(63.2348) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 130.2353, Epoch Time 1855.2520(1533.9136), Bit/dim 3.6615(best: 3.5691), Xent 0.8357, Loss 4.0794, Error 0.2140(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 31.9046(31.1969) | Bit/dim 3.6371(3.6392) | Xent 0.2125(0.1742) | Loss 9.5693(10.4487) | Error 0.0689(0.0586) Steps 1072(1076.23) | Grad Norm 106.8993(69.3286) | Total Time 0.00(0.00)\n",
      "Iter 14540 | Time 33.5046(31.6128) | Bit/dim 3.6781(3.6523) | Xent 0.2023(0.1820) | Loss 9.7052(10.2296) | Error 0.0656(0.0610) Steps 1138(1087.71) | Grad Norm 52.0932(82.1565) | Total Time 0.00(0.00)\n",
      "Iter 14550 | Time 34.1126(32.0616) | Bit/dim 3.7951(3.6752) | Xent 0.3319(0.2190) | Loss 9.9724(10.1317) | Error 0.1022(0.0693) Steps 1168(1101.12) | Grad Norm 247.0463(140.6204) | Total Time 0.00(0.00)\n",
      "Iter 14560 | Time 34.4297(32.7205) | Bit/dim 3.7636(3.7140) | Xent 0.3905(0.2702) | Loss 9.9177(10.1443) | Error 0.1222(0.0811) Steps 1192(1119.56) | Grad Norm 289.1451(198.5989) | Total Time 0.00(0.00)\n",
      "Iter 14570 | Time 34.3967(32.9954) | Bit/dim 3.7315(3.7294) | Xent 0.2641(0.2860) | Loss 9.7255(10.0960) | Error 0.0856(0.0862) Steps 1120(1127.02) | Grad Norm 105.4740(184.6200) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 132.1332, Epoch Time 1977.0620(1547.2081), Bit/dim 3.7057(best: 3.5691), Xent 0.8088, Loss 4.1101, Error 0.2140(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 32.5536(33.0406) | Bit/dim 3.7009(3.7197) | Xent 0.1733(0.2708) | Loss 9.6968(10.9326) | Error 0.0544(0.0836) Steps 1114(1127.35) | Grad Norm 114.8882(165.8252) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 32.7272(33.0394) | Bit/dim 3.6680(3.7058) | Xent 0.1888(0.2543) | Loss 9.5931(10.5843) | Error 0.0667(0.0793) Steps 1150(1127.15) | Grad Norm 54.5557(147.7065) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 31.2514(32.7596) | Bit/dim 3.6204(3.6905) | Xent 0.2032(0.2349) | Loss 9.6036(10.3136) | Error 0.0711(0.0747) Steps 1108(1122.27) | Grad Norm 60.0465(127.0270) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 31.4933(32.5792) | Bit/dim 3.6341(3.6770) | Xent 0.1536(0.2187) | Loss 9.4141(10.0961) | Error 0.0422(0.0709) Steps 1096(1117.49) | Grad Norm 32.4198(109.6020) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 32.6417(32.3588) | Bit/dim 3.6499(3.6658) | Xent 0.1664(0.2031) | Loss 9.5368(9.9314) | Error 0.0522(0.0667) Steps 1102(1108.26) | Grad Norm 12.8628(87.3279) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 31.8265(32.2448) | Bit/dim 3.6043(3.6538) | Xent 0.1507(0.1927) | Loss 9.4210(9.7944) | Error 0.0489(0.0643) Steps 1120(1102.10) | Grad Norm 53.6992(72.6632) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 129.4169, Epoch Time 1921.6277(1558.4407), Bit/dim 3.6269(best: 3.5691), Xent 0.8266, Loss 4.0401, Error 0.2073(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 31.3910(32.0562) | Bit/dim 3.6574(3.6433) | Xent 0.1627(0.1838) | Loss 9.4820(10.5040) | Error 0.0544(0.0618) Steps 1108(1098.30) | Grad Norm 44.2771(63.1190) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 31.9934(31.9232) | Bit/dim 3.5722(3.6369) | Xent 0.1065(0.1725) | Loss 9.3011(10.2248) | Error 0.0356(0.0584) Steps 1084(1095.24) | Grad Norm 40.0995(54.3507) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 32.1150(31.7782) | Bit/dim 3.6414(3.6324) | Xent 0.1490(0.1663) | Loss 9.5215(10.0215) | Error 0.0489(0.0566) Steps 1102(1088.82) | Grad Norm 42.9574(50.4927) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 27.7333(31.2237) | Bit/dim 3.7438(3.6434) | Xent 0.2962(0.1774) | Loss 9.8012(9.8991) | Error 0.1033(0.0603) Steps 1006(1080.33) | Grad Norm 14.7087(137.6579) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 25.9833(29.9276) | Bit/dim 3.7230(3.6671) | Xent 0.2518(0.1961) | Loss 9.6467(9.8428) | Error 0.0922(0.0666) Steps 952(1052.27) | Grad Norm 5.7724(103.6108) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 111.9436, Epoch Time 1761.5270(1564.5332), Bit/dim 3.6981(best: 3.5691), Xent 0.8379, Loss 4.1171, Error 0.2156(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 24.5030(28.7379) | Bit/dim 3.6767(3.6739) | Xent 0.2091(0.1991) | Loss 9.5125(10.6609) | Error 0.0733(0.0681) Steps 922(1029.67) | Grad Norm 5.7534(77.9598) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 25.1060(27.8262) | Bit/dim 3.6783(3.6736) | Xent 0.1656(0.1968) | Loss 9.5433(10.3562) | Error 0.0500(0.0681) Steps 970(1010.82) | Grad Norm 4.9575(58.7869) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 25.5336(27.2749) | Bit/dim 3.6360(3.6690) | Xent 0.1990(0.1894) | Loss 9.3519(10.1126) | Error 0.0722(0.0662) Steps 982(995.92) | Grad Norm 4.3443(44.4037) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 27.4390(26.8981) | Bit/dim 3.6183(3.6598) | Xent 0.1553(0.1827) | Loss 9.4589(9.9187) | Error 0.0467(0.0629) Steps 964(984.04) | Grad Norm 3.3469(33.7171) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 25.2601(26.6210) | Bit/dim 3.6365(3.6533) | Xent 0.1473(0.1745) | Loss 9.3691(9.7848) | Error 0.0511(0.0602) Steps 946(978.67) | Grad Norm 3.2368(25.8352) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 26.4090(26.3957) | Bit/dim 3.6576(3.6447) | Xent 0.1335(0.1681) | Loss 9.4824(9.6593) | Error 0.0422(0.0582) Steps 1006(975.68) | Grad Norm 3.3935(20.1177) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 113.4877, Epoch Time 1543.8670(1563.9133), Bit/dim 3.6261(best: 3.5691), Xent 0.8364, Loss 4.0443, Error 0.2092(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 25.9238(26.2160) | Bit/dim 3.5788(3.6380) | Xent 0.1641(0.1646) | Loss 9.2667(10.3484) | Error 0.0589(0.0568) Steps 964(973.21) | Grad Norm 3.8065(15.8480) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 24.8271(25.9637) | Bit/dim 3.5887(3.6323) | Xent 0.1528(0.1581) | Loss 9.2964(10.0823) | Error 0.0544(0.0542) Steps 940(969.61) | Grad Norm 3.0768(12.7821) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 26.1925(26.0141) | Bit/dim 3.5858(3.6245) | Xent 0.1379(0.1539) | Loss 9.3641(9.8965) | Error 0.0544(0.0526) Steps 958(969.52) | Grad Norm 3.6482(10.5686) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 26.5165(26.0176) | Bit/dim 3.5877(3.6166) | Xent 0.1361(0.1490) | Loss 9.2218(9.7436) | Error 0.0422(0.0507) Steps 994(969.68) | Grad Norm 4.0734(9.1508) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 26.9829(25.9673) | Bit/dim 3.5797(3.6122) | Xent 0.1453(0.1471) | Loss 9.3942(9.6407) | Error 0.0500(0.0503) Steps 1000(969.66) | Grad Norm 3.7319(8.5149) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 114.5483, Epoch Time 1554.7123(1563.6372), Bit/dim 3.6092(best: 3.5691), Xent 0.8483, Loss 4.0334, Error 0.2108(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 25.0651(26.0773) | Bit/dim 3.5919(3.6094) | Xent 0.1124(0.1434) | Loss 9.2010(10.4693) | Error 0.0333(0.0488) Steps 934(972.24) | Grad Norm 3.5361(13.6346) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 27.4035(26.2797) | Bit/dim 3.6181(3.6103) | Xent 0.1208(0.1429) | Loss 9.2538(10.1769) | Error 0.0411(0.0492) Steps 994(972.80) | Grad Norm 11.6261(15.4528) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 26.0148(26.3448) | Bit/dim 3.5812(3.6090) | Xent 0.1351(0.1416) | Loss 9.3247(9.9556) | Error 0.0522(0.0490) Steps 964(976.75) | Grad Norm 9.2986(18.3367) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 25.3084(26.3693) | Bit/dim 3.5619(3.6051) | Xent 0.1213(0.1420) | Loss 9.2507(9.7840) | Error 0.0400(0.0488) Steps 946(976.03) | Grad Norm 37.7439(19.6375) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 26.0641(26.4444) | Bit/dim 3.6171(3.6007) | Xent 0.1779(0.1436) | Loss 9.4492(9.6625) | Error 0.0611(0.0492) Steps 988(979.18) | Grad Norm 151.9066(36.9548) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 27.0900(26.5705) | Bit/dim 3.6657(3.6033) | Xent 0.1892(0.1467) | Loss 9.4004(9.5770) | Error 0.0733(0.0504) Steps 1000(981.35) | Grad Norm 13.7902(43.7752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 117.2763, Epoch Time 1601.6262(1564.7769), Bit/dim 3.6582(best: 3.5691), Xent 0.9019, Loss 4.1092, Error 0.2221(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 28.0178(27.0092) | Bit/dim 3.7810(3.6245) | Xent 0.3582(0.1714) | Loss 9.8635(10.3735) | Error 0.1044(0.0578) Steps 1018(989.22) | Grad Norm 131.7384(66.9697) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 30.3472(27.6459) | Bit/dim 3.9116(3.6893) | Xent 0.4978(0.2510) | Loss 10.4156(10.3436) | Error 0.1500(0.0836) Steps 1066(1003.47) | Grad Norm 23.9591(61.6635) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 29.0216(28.1566) | Bit/dim 3.8316(3.7342) | Xent 0.4596(0.3151) | Loss 10.1809(10.3037) | Error 0.1589(0.1056) Steps 1042(1012.72) | Grad Norm 13.8262(50.2659) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 30.9360(28.6412) | Bit/dim 3.7668(3.7471) | Xent 0.3239(0.3350) | Loss 9.8024(10.2178) | Error 0.1144(0.1125) Steps 1060(1023.40) | Grad Norm 12.8380(41.3614) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 29.8529(28.9990) | Bit/dim 3.7010(3.7446) | Xent 0.2987(0.3286) | Loss 9.5705(10.1027) | Error 0.1067(0.1121) Steps 1024(1030.85) | Grad Norm 13.7803(34.5310) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 120.9596, Epoch Time 1760.1198(1570.6372), Bit/dim 3.7085(best: 3.5691), Xent 0.8071, Loss 4.1120, Error 0.2161(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 30.0333(29.1813) | Bit/dim 3.7020(3.7327) | Xent 0.2218(0.3068) | Loss 9.6230(10.8588) | Error 0.0800(0.1055) Steps 1042(1030.93) | Grad Norm 7.2465(28.1375) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 29.6237(29.2680) | Bit/dim 3.6667(3.7166) | Xent 0.1947(0.2839) | Loss 9.4103(10.5090) | Error 0.0656(0.0980) Steps 1030(1032.57) | Grad Norm 6.8120(23.1160) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 29.0406(29.2033) | Bit/dim 3.6693(3.7023) | Xent 0.1835(0.2604) | Loss 9.4834(10.2488) | Error 0.0644(0.0897) Steps 1048(1033.86) | Grad Norm 10.2265(19.3007) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 29.7325(29.1511) | Bit/dim 3.6559(3.6855) | Xent 0.1782(0.2422) | Loss 9.5949(10.0421) | Error 0.0678(0.0838) Steps 1078(1032.75) | Grad Norm 15.1755(19.8302) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 29.0031(29.1766) | Bit/dim 3.6236(3.6704) | Xent 0.1675(0.2223) | Loss 9.3986(9.8780) | Error 0.0633(0.0776) Steps 1024(1032.46) | Grad Norm 12.9496(25.5056) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 30.1107(29.3078) | Bit/dim 3.7078(3.6641) | Xent 0.4191(0.2269) | Loss 9.8412(9.7845) | Error 0.1244(0.0786) Steps 1036(1037.17) | Grad Norm 49.9372(95.2536) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 127.1285, Epoch Time 1758.4292(1576.2709), Bit/dim 3.7429(best: 3.5691), Xent 1.0205, Loss 4.2531, Error 0.2642(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 30.6904(29.7953) | Bit/dim 3.7391(3.6905) | Xent 0.4370(0.2855) | Loss 9.9377(10.6458) | Error 0.1178(0.0945) Steps 1060(1051.99) | Grad Norm 46.7016(89.2652) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 32.9601(30.4822) | Bit/dim 3.6735(3.6972) | Xent 0.2546(0.2917) | Loss 9.5634(10.4299) | Error 0.0878(0.0980) Steps 1108(1067.08) | Grad Norm 62.4939(85.1722) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 29.7653(30.8034) | Bit/dim 3.6837(3.6950) | Xent 0.2144(0.2807) | Loss 9.5335(10.2214) | Error 0.0722(0.0948) Steps 1060(1075.24) | Grad Norm 39.8135(71.7899) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 31.1532(30.9625) | Bit/dim 3.6778(3.6907) | Xent 0.2257(0.2621) | Loss 9.6937(10.0655) | Error 0.0733(0.0892) Steps 1084(1080.71) | Grad Norm 26.3572(138.1411) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 30.1079(30.7135) | Bit/dim 3.6965(3.7009) | Xent 0.3352(0.2711) | Loss 9.8000(9.9813) | Error 0.1133(0.0930) Steps 1066(1073.61) | Grad Norm 11.3113(106.8466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 117.7115, Epoch Time 1846.5593(1584.3796), Bit/dim 3.7231(best: 3.5691), Xent 0.7795, Loss 4.1128, Error 0.2125(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 28.5796(30.1081) | Bit/dim 3.6904(3.7033) | Xent 0.2542(0.2724) | Loss 9.5476(10.7796) | Error 0.0833(0.0945) Steps 1012(1053.43) | Grad Norm 7.4366(81.5830) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 26.6957(29.4869) | Bit/dim 3.6553(3.6976) | Xent 0.1784(0.2561) | Loss 9.3064(10.4507) | Error 0.0611(0.0888) Steps 1000(1039.03) | Grad Norm 5.1238(61.9069) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 27.3970(29.0882) | Bit/dim 3.6493(3.6846) | Xent 0.2010(0.2343) | Loss 9.4740(10.1830) | Error 0.0722(0.0817) Steps 1012(1030.01) | Grad Norm 6.8590(48.4525) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 27.5455(28.7460) | Bit/dim 3.6396(3.6732) | Xent 0.1595(0.2153) | Loss 9.4057(9.9818) | Error 0.0556(0.0751) Steps 994(1020.33) | Grad Norm 83.0715(41.8168) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 29.0487(28.5338) | Bit/dim 3.6514(3.6629) | Xent 0.1666(0.1990) | Loss 9.5624(9.8365) | Error 0.0533(0.0689) Steps 1060(1021.18) | Grad Norm 99.2919(56.1721) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 29.2730(28.5452) | Bit/dim 3.6272(3.6528) | Xent 0.1652(0.1913) | Loss 9.5083(9.7254) | Error 0.0567(0.0667) Steps 1030(1023.35) | Grad Norm 853.7502(124.0286) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 122.3895, Epoch Time 1681.8540(1587.3038), Bit/dim 3.6307(best: 3.5691), Xent 0.8355, Loss 4.0484, Error 0.2098(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 29.2528(28.7470) | Bit/dim 3.6164(3.6436) | Xent 0.1324(0.1784) | Loss 9.4749(10.4452) | Error 0.0456(0.0620) Steps 1036(1028.14) | Grad Norm 74.3430(149.8714) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 30.4261(28.8966) | Bit/dim 3.6395(3.6368) | Xent 0.1415(0.1699) | Loss 9.3573(10.1700) | Error 0.0522(0.0586) Steps 1036(1026.10) | Grad Norm 141.8018(147.6305) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 30.7118(29.0567) | Bit/dim 3.6480(3.6358) | Xent 0.1851(0.1668) | Loss 9.4904(9.9808) | Error 0.0644(0.0578) Steps 1012(1028.88) | Grad Norm 54.2909(172.7945) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 31.5782(29.4690) | Bit/dim 3.6706(3.6455) | Xent 0.1852(0.1768) | Loss 9.6301(9.8593) | Error 0.0589(0.0605) Steps 1126(1042.53) | Grad Norm 110.5725(139.1813) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 31.3509(29.9144) | Bit/dim 3.6805(3.6540) | Xent 0.2160(0.1817) | Loss 9.6310(9.7855) | Error 0.0722(0.0630) Steps 1048(1052.45) | Grad Norm 79.0034(123.0260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 129.0489, Epoch Time 1806.3894(1593.8764), Bit/dim 3.6897(best: 3.5691), Xent 0.8196, Loss 4.0995, Error 0.2116(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 31.1198(30.3663) | Bit/dim 3.6472(3.6599) | Xent 0.2403(0.1864) | Loss 9.4517(10.6451) | Error 0.0833(0.0644) Steps 1072(1061.34) | Grad Norm 65.6592(110.6156) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 32.0210(30.7475) | Bit/dim 3.6696(3.6645) | Xent 0.2085(0.1924) | Loss 9.6163(10.3724) | Error 0.0689(0.0659) Steps 1144(1073.46) | Grad Norm 80.8061(106.1083) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 30.9577(31.0272) | Bit/dim 3.6672(3.6634) | Xent 0.1660(0.1905) | Loss 9.5227(10.1672) | Error 0.0578(0.0657) Steps 1078(1081.62) | Grad Norm 69.5468(94.2586) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 31.7273(31.3033) | Bit/dim 3.6627(3.6596) | Xent 0.1584(0.1889) | Loss 9.5172(9.9977) | Error 0.0556(0.0649) Steps 1114(1088.12) | Grad Norm 39.3894(79.7862) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 31.6107(31.2758) | Bit/dim 3.6293(3.6525) | Xent 0.1775(0.1807) | Loss 9.3397(9.8439) | Error 0.0656(0.0621) Steps 1072(1088.74) | Grad Norm 66.7734(66.3607) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 30.9139(31.3292) | Bit/dim 3.6599(3.6505) | Xent 0.1734(0.1751) | Loss 9.5347(9.7471) | Error 0.0644(0.0598) Steps 1084(1086.68) | Grad Norm 35.4218(57.5260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 128.3373, Epoch Time 1886.7889(1602.6638), Bit/dim 3.6448(best: 3.5691), Xent 0.8236, Loss 4.0566, Error 0.2103(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 32.0740(31.3856) | Bit/dim 3.6538(3.6478) | Xent 0.1617(0.1723) | Loss 9.4342(10.4931) | Error 0.0578(0.0587) Steps 1144(1090.95) | Grad Norm 22.7785(55.8548) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 32.0271(31.5081) | Bit/dim 3.6705(3.6469) | Xent 0.1849(0.1689) | Loss 9.6430(10.2367) | Error 0.0611(0.0578) Steps 1102(1091.17) | Grad Norm 36.0047(53.3509) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 31.9957(31.5170) | Bit/dim 3.6193(3.6430) | Xent 0.1620(0.1653) | Loss 9.4034(10.0286) | Error 0.0600(0.0567) Steps 1078(1091.02) | Grad Norm 34.8127(50.1186) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 32.3231(31.6069) | Bit/dim 3.6477(3.6401) | Xent 0.1315(0.1612) | Loss 9.4578(9.8796) | Error 0.0411(0.0540) Steps 1108(1089.94) | Grad Norm 113.6676(54.9323) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 32.1039(31.4410) | Bit/dim 3.5928(3.6363) | Xent 0.1384(0.1583) | Loss 9.3341(9.7588) | Error 0.0478(0.0535) Steps 1078(1086.11) | Grad Norm 91.3067(59.0510) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 128.7007, Epoch Time 1879.5326(1610.9698), Bit/dim 3.6333(best: 3.5691), Xent 0.8518, Loss 4.0593, Error 0.2121(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 32.4560(31.3410) | Bit/dim 3.5742(3.6309) | Xent 0.1382(0.1546) | Loss 9.2452(10.5723) | Error 0.0433(0.0517) Steps 1108(1083.10) | Grad Norm 31.2631(72.2490) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 31.1955(31.3001) | Bit/dim 3.6206(3.6309) | Xent 0.1371(0.1533) | Loss 9.3161(10.2751) | Error 0.0478(0.0517) Steps 1126(1085.02) | Grad Norm 68.1842(69.1394) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 32.6042(31.4986) | Bit/dim 3.6233(3.6338) | Xent 0.1966(0.1598) | Loss 9.4688(10.0750) | Error 0.0767(0.0546) Steps 1126(1095.40) | Grad Norm 62.8877(82.6443) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 32.1884(31.7669) | Bit/dim 3.6642(3.6394) | Xent 0.2163(0.1682) | Loss 9.5136(9.9411) | Error 0.0756(0.0569) Steps 1072(1100.65) | Grad Norm 97.8124(94.6058) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 33.5947(32.1096) | Bit/dim 3.6663(3.6459) | Xent 0.2100(0.1726) | Loss 9.6060(9.8400) | Error 0.0656(0.0587) Steps 1132(1107.68) | Grad Norm 129.6367(99.8246) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 31.8271(32.3416) | Bit/dim 3.6978(3.6568) | Xent 0.2290(0.1842) | Loss 9.6331(9.8020) | Error 0.0844(0.0623) Steps 1126(1112.44) | Grad Norm 385.3874(104.4192) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 133.9973, Epoch Time 1925.0288(1620.3916), Bit/dim 3.6856(best: 3.5691), Xent 0.8570, Loss 4.1141, Error 0.2161(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 34.1719(32.6422) | Bit/dim 3.6408(3.6605) | Xent 0.1752(0.1901) | Loss 9.4322(10.5850) | Error 0.0544(0.0646) Steps 1198(1116.46) | Grad Norm 73.1258(104.7021) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 32.4228(32.7220) | Bit/dim 3.6380(3.6630) | Xent 0.1735(0.1897) | Loss 9.4791(10.3258) | Error 0.0589(0.0637) Steps 1114(1119.29) | Grad Norm 51.3329(106.2005) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 32.1622(32.6814) | Bit/dim 3.6657(3.6630) | Xent 0.2138(0.1890) | Loss 9.5409(10.1229) | Error 0.0678(0.0639) Steps 1120(1120.02) | Grad Norm 117.1904(102.9061) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 32.6526(32.7005) | Bit/dim 3.6588(3.6627) | Xent 0.1834(0.1909) | Loss 9.5347(9.9697) | Error 0.0622(0.0645) Steps 1144(1120.93) | Grad Norm 81.9095(106.8110) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 33.5688(32.8956) | Bit/dim 3.6663(3.6667) | Xent 0.1493(0.1914) | Loss 9.5672(9.8937) | Error 0.0467(0.0638) Steps 1138(1125.80) | Grad Norm 59.8588(106.0416) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 133.5479, Epoch Time 1968.5276(1630.8357), Bit/dim 3.6683(best: 3.5691), Xent 0.8230, Loss 4.0798, Error 0.2103(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 34.0561(32.9422) | Bit/dim 3.6354(3.6667) | Xent 0.1769(0.1884) | Loss 9.3878(10.7408) | Error 0.0589(0.0629) Steps 1144(1121.98) | Grad Norm 50.3119(98.5093) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 33.0089(32.9178) | Bit/dim 3.6416(3.6630) | Xent 0.1854(0.1838) | Loss 9.5800(10.4355) | Error 0.0622(0.0603) Steps 1162(1123.66) | Grad Norm 78.3490(91.6486) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 32.1856(32.8944) | Bit/dim 3.6515(3.6592) | Xent 0.1488(0.1801) | Loss 9.5493(10.2010) | Error 0.0444(0.0596) Steps 1078(1120.45) | Grad Norm 164.7734(90.6856) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 33.0799(32.8009) | Bit/dim 3.6427(3.6582) | Xent 0.1828(0.1760) | Loss 9.5634(10.0246) | Error 0.0678(0.0590) Steps 1144(1119.53) | Grad Norm 78.7327(84.8415) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 33.5831(32.8418) | Bit/dim 3.6450(3.6582) | Xent 0.1909(0.1784) | Loss 9.5653(9.9119) | Error 0.0633(0.0598) Steps 1096(1121.67) | Grad Norm 510.9229(102.6309) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 33.7926(33.0803) | Bit/dim 3.7137(3.6663) | Xent 0.2410(0.1916) | Loss 9.7290(9.8575) | Error 0.0967(0.0640) Steps 1126(1127.77) | Grad Norm 116.2631(123.6063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 135.9825, Epoch Time 1969.8305(1641.0055), Bit/dim 3.7226(best: 3.5691), Xent 0.8897, Loss 4.1675, Error 0.2205(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 33.1638(33.2905) | Bit/dim 3.7129(3.6740) | Xent 0.2596(0.2013) | Loss 9.7119(10.6390) | Error 0.0900(0.0667) Steps 1108(1130.80) | Grad Norm 102.0167(136.1369) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 34.1545(33.3806) | Bit/dim 3.6785(3.6860) | Xent 0.2116(0.2119) | Loss 9.6531(10.4012) | Error 0.0756(0.0705) Steps 1132(1130.28) | Grad Norm 21.4201(117.9915) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 32.1506(33.3157) | Bit/dim 3.6799(3.6864) | Xent 0.2221(0.2107) | Loss 9.6101(10.1991) | Error 0.0833(0.0707) Steps 1096(1128.96) | Grad Norm 104.1791(105.3820) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 32.1882(33.1562) | Bit/dim 3.6624(3.6807) | Xent 0.1550(0.2038) | Loss 9.4778(10.0401) | Error 0.0511(0.0690) Steps 1120(1126.88) | Grad Norm 25.0279(88.6379) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 31.9272(32.9349) | Bit/dim 3.6688(3.6738) | Xent 0.1516(0.1947) | Loss 9.5396(9.9060) | Error 0.0633(0.0659) Steps 1108(1122.43) | Grad Norm 51.8532(87.3327) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 132.0013, Epoch Time 1976.0129(1651.0558), Bit/dim 3.6571(best: 3.5691), Xent 0.8464, Loss 4.0803, Error 0.2100(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 33.4440(32.9653) | Bit/dim 3.6781(3.6659) | Xent 0.1614(0.1878) | Loss 9.5872(10.7215) | Error 0.0533(0.0643) Steps 1156(1119.69) | Grad Norm 102.3534(81.0092) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 32.4703(32.7956) | Bit/dim 3.6728(3.6577) | Xent 0.1616(0.1825) | Loss 9.6940(10.3961) | Error 0.0511(0.0624) Steps 1114(1116.23) | Grad Norm 54.3969(75.8555) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 31.8339(32.7551) | Bit/dim 3.6792(3.6557) | Xent 0.1619(0.1785) | Loss 9.5409(10.1783) | Error 0.0567(0.0611) Steps 1102(1114.81) | Grad Norm 140.7795(73.6212) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 32.7693(32.8269) | Bit/dim 3.6291(3.6519) | Xent 0.1877(0.1743) | Loss 9.4172(9.9952) | Error 0.0644(0.0595) Steps 1144(1117.15) | Grad Norm 43.5339(72.5044) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 32.1720(32.7437) | Bit/dim 3.6349(3.6516) | Xent 0.1365(0.1719) | Loss 9.4049(9.8660) | Error 0.0500(0.0594) Steps 1090(1114.59) | Grad Norm 97.2704(83.4864) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 36.1299(32.9783) | Bit/dim 3.6982(3.6547) | Xent 0.2071(0.1731) | Loss 9.7165(9.7942) | Error 0.0578(0.0593) Steps 1150(1119.19) | Grad Norm 141.6002(93.2737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 134.6041, Epoch Time 1956.5270(1660.2199), Bit/dim 3.6854(best: 3.5691), Xent 0.8779, Loss 4.1244, Error 0.2134(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 35.7583(33.1681) | Bit/dim 3.6930(3.6587) | Xent 0.2198(0.1787) | Loss 9.7703(10.5478) | Error 0.0778(0.0613) Steps 1174(1122.50) | Grad Norm 142.2326(103.4040) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 33.5104(33.4386) | Bit/dim 3.6720(3.6662) | Xent 0.1901(0.1861) | Loss 9.5297(10.3233) | Error 0.0733(0.0629) Steps 1120(1126.56) | Grad Norm 49.4170(109.3632) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 32.7400(33.5355) | Bit/dim 3.6630(3.6748) | Xent 0.1569(0.1888) | Loss 9.5531(10.1556) | Error 0.0556(0.0641) Steps 1138(1131.20) | Grad Norm 165.9094(124.1611) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 32.9788(33.6485) | Bit/dim 3.6676(3.6753) | Xent 0.1802(0.1917) | Loss 9.5277(10.0227) | Error 0.0633(0.0653) Steps 1132(1132.84) | Grad Norm 70.9476(125.4491) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 34.2412(33.7219) | Bit/dim 3.6824(3.6772) | Xent 0.1995(0.1903) | Loss 9.7452(9.9252) | Error 0.0700(0.0652) Steps 1114(1133.51) | Grad Norm 38.8166(122.9234) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 141.5957, Epoch Time 2024.6408(1671.1525), Bit/dim 3.7512(best: 3.5691), Xent 0.8885, Loss 4.1955, Error 0.2255(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 34.9270(33.8010) | Bit/dim 3.7244(3.6921) | Xent 0.2680(0.2113) | Loss 9.8344(10.8571) | Error 0.0889(0.0710) Steps 1180(1140.94) | Grad Norm 136.6804(157.1078) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 33.7781(33.9389) | Bit/dim 3.6815(3.6983) | Xent 0.2079(0.2148) | Loss 9.6429(10.5673) | Error 0.0633(0.0728) Steps 1198(1143.86) | Grad Norm 81.6776(137.1980) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 34.7316(33.9228) | Bit/dim 3.7069(3.6973) | Xent 0.2086(0.2122) | Loss 9.7492(10.3254) | Error 0.0711(0.0715) Steps 1138(1140.91) | Grad Norm 50.5047(137.9017) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 33.8338(33.9322) | Bit/dim 3.6308(3.6888) | Xent 0.1449(0.2091) | Loss 9.4145(10.1355) | Error 0.0511(0.0705) Steps 1114(1144.33) | Grad Norm 174.8384(136.6009) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 34.6788(33.7996) | Bit/dim 3.6725(3.6838) | Xent 0.1728(0.1998) | Loss 9.5097(9.9865) | Error 0.0689(0.0677) Steps 1174(1144.05) | Grad Norm 140.2592(125.4930) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 32.6705(33.7151) | Bit/dim 3.6756(3.6799) | Xent 0.2053(0.1949) | Loss 9.5987(9.8804) | Error 0.0744(0.0662) Steps 1132(1140.67) | Grad Norm 165.1137(132.6506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 136.2701, Epoch Time 2017.0046(1681.5281), Bit/dim 3.6771(best: 3.5691), Xent 0.8341, Loss 4.0942, Error 0.2108(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 33.9317(33.8844) | Bit/dim 3.7183(3.6814) | Xent 0.2681(0.1994) | Loss 9.6331(10.6434) | Error 0.0822(0.0671) Steps 1120(1141.03) | Grad Norm 727.2315(164.9131) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 35.7282(34.1251) | Bit/dim 3.8283(3.7262) | Xent 0.4046(0.2739) | Loss 10.2004(10.5502) | Error 0.1189(0.0844) Steps 1216(1152.71) | Grad Norm 181.9079(228.1666) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 34.6338(34.4497) | Bit/dim 3.7719(3.7530) | Xent 0.3453(0.3123) | Loss 9.9424(10.4630) | Error 0.1189(0.0946) Steps 1192(1163.48) | Grad Norm 252.9644(258.6896) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 34.6271(34.8008) | Bit/dim 3.7234(3.7545) | Xent 0.2435(0.3010) | Loss 9.8390(10.3216) | Error 0.0900(0.0939) Steps 1174(1169.83) | Grad Norm 88.7701(227.6090) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 34.6284(34.8073) | Bit/dim 3.6837(3.7390) | Xent 0.2008(0.2804) | Loss 9.6647(10.1628) | Error 0.0700(0.0895) Steps 1174(1164.24) | Grad Norm 80.6434(188.5133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 139.3787, Epoch Time 2078.3317(1693.4322), Bit/dim 3.7009(best: 3.5691), Xent 0.8262, Loss 4.1140, Error 0.2141(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 35.4713(34.6649) | Bit/dim 3.6654(3.7236) | Xent 0.1628(0.2576) | Loss 9.6280(10.9776) | Error 0.0511(0.0826) Steps 1210(1160.25) | Grad Norm 60.7001(165.2696) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 33.4531(34.4682) | Bit/dim 3.6597(3.7108) | Xent 0.1993(0.2380) | Loss 9.6316(10.6178) | Error 0.0678(0.0765) Steps 1132(1156.65) | Grad Norm 83.0487(139.6245) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 33.9633(34.3338) | Bit/dim 3.6892(3.6990) | Xent 0.1420(0.2215) | Loss 9.6841(10.3553) | Error 0.0422(0.0716) Steps 1162(1153.77) | Grad Norm 73.4862(133.6887) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 32.7930(34.0890) | Bit/dim 3.6873(3.6860) | Xent 0.1763(0.2083) | Loss 9.5675(10.1390) | Error 0.0633(0.0682) Steps 1090(1145.11) | Grad Norm 394.9207(126.3827) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 33.4494(33.8260) | Bit/dim 3.6874(3.6814) | Xent 0.2050(0.2013) | Loss 9.6000(9.9925) | Error 0.0722(0.0669) Steps 1156(1143.11) | Grad Norm 217.7689(141.0737) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 34.9565(34.0495) | Bit/dim 3.7275(3.6929) | Xent 0.2103(0.2077) | Loss 9.7624(9.9294) | Error 0.0733(0.0699) Steps 1168(1150.14) | Grad Norm 160.7702(138.2462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 139.9901, Epoch Time 2022.4913(1703.3040), Bit/dim 3.7472(best: 3.5691), Xent 0.8554, Loss 4.1749, Error 0.2151(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 33.5915(34.2779) | Bit/dim 3.7430(3.7031) | Xent 0.2548(0.2179) | Loss 9.7035(10.7182) | Error 0.0933(0.0741) Steps 1150(1157.94) | Grad Norm 63.9302(136.7735) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 33.9391(34.2430) | Bit/dim 3.7278(3.7100) | Xent 0.2655(0.2234) | Loss 9.6972(10.4717) | Error 0.0956(0.0758) Steps 1168(1158.29) | Grad Norm 134.6121(126.1121) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 33.7352(34.3508) | Bit/dim 3.7028(3.7051) | Xent 0.2308(0.2215) | Loss 9.6541(10.2686) | Error 0.0778(0.0750) Steps 1138(1157.26) | Grad Norm 137.9647(136.6613) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 34.4115(34.4124) | Bit/dim 3.6578(3.6977) | Xent 0.1587(0.2127) | Loss 9.6854(10.1102) | Error 0.0478(0.0719) Steps 1174(1159.65) | Grad Norm 56.2769(126.7206) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 32.3786(34.1194) | Bit/dim 3.6717(3.6899) | Xent 0.1950(0.2037) | Loss 9.5572(9.9817) | Error 0.0633(0.0689) Steps 1138(1152.22) | Grad Norm 120.4932(110.7136) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 135.6250, Epoch Time 2037.5596(1713.3316), Bit/dim 3.6663(best: 3.5691), Xent 0.8401, Loss 4.0864, Error 0.2131(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 33.5873(33.9255) | Bit/dim 3.6701(3.6851) | Xent 0.1692(0.1957) | Loss 9.5330(10.8224) | Error 0.0544(0.0660) Steps 1150(1147.47) | Grad Norm 112.4941(109.0732) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 33.8420(33.9356) | Bit/dim 3.6890(3.6869) | Xent 0.1973(0.1965) | Loss 9.5825(10.5215) | Error 0.0644(0.0659) Steps 1108(1149.12) | Grad Norm 74.7733(130.8866) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 32.4701(33.5821) | Bit/dim 3.7460(3.6941) | Xent 0.2541(0.2132) | Loss 9.8147(10.3222) | Error 0.0867(0.0717) Steps 1126(1141.42) | Grad Norm 38.8271(132.2806) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 29.5633(33.0963) | Bit/dim 3.6845(3.6957) | Xent 0.1967(0.2208) | Loss 9.4915(10.1422) | Error 0.0767(0.0751) Steps 1060(1127.85) | Grad Norm 53.5239(126.2617) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 30.6005(32.5629) | Bit/dim 3.6829(3.6877) | Xent 0.1895(0.2159) | Loss 9.4171(9.9889) | Error 0.0711(0.0744) Steps 1084(1116.43) | Grad Norm 31.2900(104.3903) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 33.1536(32.4842) | Bit/dim 3.7621(3.6948) | Xent 0.3442(0.2266) | Loss 9.9686(9.9341) | Error 0.1167(0.0781) Steps 1162(1117.66) | Grad Norm 113.1969(132.8458) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 139.0628, Epoch Time 1940.3317(1720.1416), Bit/dim 3.8055(best: 3.5691), Xent 0.8644, Loss 4.2376, Error 0.2260(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 37.2457(33.1854) | Bit/dim 4.0063(3.7574) | Xent 0.4532(0.2745) | Loss 10.5663(10.8742) | Error 0.1422(0.0923) Steps 1252(1132.24) | Grad Norm 253.1529(181.4711) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 35.5797(33.9804) | Bit/dim 4.0349(3.8264) | Xent 0.6327(0.3454) | Loss 10.8489(10.8494) | Error 0.2056(0.1142) Steps 1168(1156.77) | Grad Norm 298.9603(243.7079) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 35.0813(34.4192) | Bit/dim 3.8090(3.8488) | Xent 0.3195(0.3658) | Loss 10.0562(10.7281) | Error 0.1056(0.1209) Steps 1168(1166.81) | Grad Norm 93.8220(241.0322) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 35.0546(34.6668) | Bit/dim 3.7987(3.8383) | Xent 0.2694(0.3577) | Loss 9.9584(10.5547) | Error 0.0933(0.1187) Steps 1180(1169.87) | Grad Norm 45.5537(215.2512) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 35.1093(34.6700) | Bit/dim 3.7090(3.8114) | Xent 0.2254(0.3294) | Loss 9.5922(10.3562) | Error 0.0744(0.1111) Steps 1144(1167.81) | Grad Norm 16.1064(181.9793) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 138.8804, Epoch Time 2100.5111(1731.5527), Bit/dim 3.7105(best: 3.5691), Xent 0.7875, Loss 4.1043, Error 0.2122(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 34.3962(34.7094) | Bit/dim 3.6626(3.7815) | Xent 0.2424(0.2981) | Loss 9.6681(11.1562) | Error 0.0844(0.1005) Steps 1108(1164.32) | Grad Norm 60.0586(152.9090) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 34.7879(34.6153) | Bit/dim 3.6551(3.7567) | Xent 0.2006(0.2704) | Loss 9.4591(10.7557) | Error 0.0722(0.0908) Steps 1156(1159.63) | Grad Norm 97.1589(132.2258) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 34.8330(34.4679) | Bit/dim 3.6668(3.7324) | Xent 0.1948(0.2447) | Loss 9.5543(10.4421) | Error 0.0678(0.0825) Steps 1186(1154.00) | Grad Norm 49.6234(110.4765) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 32.8194(34.1546) | Bit/dim 3.6487(3.7146) | Xent 0.1746(0.2309) | Loss 9.6110(10.2255) | Error 0.0667(0.0778) Steps 1180(1150.53) | Grad Norm 105.2000(98.9377) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 34.1930(33.8697) | Bit/dim 3.6383(3.6963) | Xent 0.1812(0.2172) | Loss 9.4858(10.0432) | Error 0.0578(0.0734) Steps 1144(1142.81) | Grad Norm 24.3536(94.6972) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 33.8250(33.7842) | Bit/dim 3.6700(3.6876) | Xent 0.1419(0.2049) | Loss 9.5832(9.9157) | Error 0.0522(0.0693) Steps 1126(1140.32) | Grad Norm 14.3464(95.9916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 135.7102, Epoch Time 2012.0257(1739.9669), Bit/dim 3.6639(best: 3.5691), Xent 0.8220, Loss 4.0749, Error 0.2102(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 31.3543(33.5516) | Bit/dim 3.6702(3.6787) | Xent 0.1538(0.1959) | Loss 9.6294(10.6258) | Error 0.0544(0.0667) Steps 1132(1135.55) | Grad Norm 24.7995(85.1461) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 33.8474(33.4316) | Bit/dim 3.6630(3.6714) | Xent 0.1451(0.1899) | Loss 9.5419(10.3519) | Error 0.0511(0.0648) Steps 1078(1131.06) | Grad Norm 61.8757(105.0357) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 34.1530(33.5266) | Bit/dim 3.7166(3.6793) | Xent 0.1798(0.1934) | Loss 9.5908(10.1742) | Error 0.0511(0.0653) Steps 1078(1130.33) | Grad Norm 119.5369(100.6861) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 32.2390(33.6613) | Bit/dim 3.6517(3.6808) | Xent 0.1657(0.1913) | Loss 9.4392(10.0306) | Error 0.0533(0.0647) Steps 1114(1139.02) | Grad Norm 38.2045(95.0760) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 34.4125(33.6876) | Bit/dim 3.7009(3.6820) | Xent 0.2068(0.1921) | Loss 9.6490(9.9254) | Error 0.0744(0.0649) Steps 1174(1138.38) | Grad Norm 83.7944(91.7868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 135.0808, Epoch Time 2002.0188(1747.8285), Bit/dim 3.6971(best: 3.5691), Xent 0.8613, Loss 4.1278, Error 0.2192(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 33.5833(33.8715) | Bit/dim 3.6707(3.6789) | Xent 0.1844(0.1896) | Loss 9.5910(10.7897) | Error 0.0667(0.0635) Steps 1174(1143.85) | Grad Norm 63.6814(103.1135) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 32.9930(33.9039) | Bit/dim 3.6617(3.6772) | Xent 0.2481(0.1883) | Loss 9.6941(10.4751) | Error 0.0844(0.0630) Steps 1126(1144.53) | Grad Norm 112.0938(106.0906) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 33.7753(33.9051) | Bit/dim 3.6834(3.6756) | Xent 0.1527(0.1852) | Loss 9.4256(10.2471) | Error 0.0511(0.0627) Steps 1102(1144.38) | Grad Norm 167.2745(104.0331) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 33.2759(33.8166) | Bit/dim 3.6615(3.6717) | Xent 0.2078(0.1865) | Loss 9.4431(10.0676) | Error 0.0778(0.0619) Steps 1120(1138.32) | Grad Norm 113.4937(129.1051) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 34.4514(33.5766) | Bit/dim 3.6575(3.6719) | Xent 0.2189(0.1861) | Loss 9.6520(9.9514) | Error 0.0778(0.0628) Steps 1156(1137.49) | Grad Norm 30.8045(110.7079) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 34.0018(33.5328) | Bit/dim 3.6583(3.6674) | Xent 0.1964(0.1847) | Loss 9.7511(9.8580) | Error 0.0667(0.0620) Steps 1180(1139.03) | Grad Norm 36.1783(89.8691) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 135.7759, Epoch Time 2003.3982(1755.4956), Bit/dim 3.6657(best: 3.5691), Xent 0.8259, Loss 4.0786, Error 0.2100(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 31.3330(33.2911) | Bit/dim 3.6649(3.6634) | Xent 0.1667(0.1799) | Loss 9.4638(10.5737) | Error 0.0556(0.0602) Steps 1108(1134.03) | Grad Norm 84.9277(106.0688) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 32.1876(32.9096) | Bit/dim 3.6307(3.6572) | Xent 0.1691(0.1764) | Loss 9.4508(10.2835) | Error 0.0567(0.0591) Steps 1132(1123.75) | Grad Norm 37.7482(116.8075) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 31.4346(32.7878) | Bit/dim 3.6301(3.6524) | Xent 0.1683(0.1783) | Loss 9.4649(10.0861) | Error 0.0622(0.0605) Steps 1120(1124.99) | Grad Norm 211.9634(112.1441) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 33.0142(32.7599) | Bit/dim 3.6625(3.6542) | Xent 0.1786(0.1762) | Loss 9.6271(9.9559) | Error 0.0567(0.0602) Steps 1096(1123.76) | Grad Norm 77.2480(143.8419) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 32.6114(32.6542) | Bit/dim 3.6668(3.6511) | Xent 0.1644(0.1726) | Loss 9.5619(9.8439) | Error 0.0667(0.0589) Steps 1126(1120.19) | Grad Norm 650.5630(147.9601) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 138.0106, Epoch Time 1946.2976(1761.2196), Bit/dim 3.7160(best: 3.5691), Xent 0.8713, Loss 4.1516, Error 0.2195(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 34.0341(32.9631) | Bit/dim 3.7155(3.6669) | Xent 0.2414(0.1888) | Loss 9.7384(10.7737) | Error 0.0778(0.0635) Steps 1114(1122.84) | Grad Norm 252.4657(156.7146) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 34.8365(33.4218) | Bit/dim 3.7516(3.6873) | Xent 0.3094(0.2031) | Loss 9.9675(10.5216) | Error 0.0900(0.0679) Steps 1198(1135.84) | Grad Norm 192.2327(163.4464) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 32.8008(33.5863) | Bit/dim 3.6916(3.6969) | Xent 0.1835(0.2079) | Loss 9.4404(10.3069) | Error 0.0611(0.0695) Steps 1132(1139.68) | Grad Norm 147.3104(152.0298) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 33.7013(33.7964) | Bit/dim 3.6972(3.7008) | Xent 0.1900(0.2147) | Loss 9.7542(10.1744) | Error 0.0633(0.0722) Steps 1162(1147.58) | Grad Norm 113.0152(164.3078) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 34.5535(33.9125) | Bit/dim 3.6937(3.6995) | Xent 0.2254(0.2128) | Loss 9.7069(10.0464) | Error 0.0722(0.0714) Steps 1138(1150.68) | Grad Norm 114.7510(157.5317) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 33.6150(33.8230) | Bit/dim 3.7087(3.6957) | Xent 0.1964(0.2069) | Loss 9.7759(9.9506) | Error 0.0689(0.0698) Steps 1126(1147.07) | Grad Norm 315.6039(149.1756) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 136.3210, Epoch Time 2033.3096(1769.3823), Bit/dim 3.7238(best: 3.5691), Xent 0.9469, Loss 4.1973, Error 0.2363(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 34.3371(33.9066) | Bit/dim 3.6813(3.6956) | Xent 0.1844(0.2090) | Loss 9.8524(10.6996) | Error 0.0622(0.0700) Steps 1168(1152.18) | Grad Norm 14.9407(145.3585) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 32.9880(33.8296) | Bit/dim 3.6834(3.6936) | Xent 0.2081(0.2051) | Loss 9.6289(10.4245) | Error 0.0700(0.0693) Steps 1138(1153.00) | Grad Norm 45.8077(126.5951) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 33.6362(33.6874) | Bit/dim 3.6898(3.6860) | Xent 0.1459(0.1985) | Loss 9.7322(10.2161) | Error 0.0500(0.0674) Steps 1174(1151.30) | Grad Norm 22.1749(109.0849) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 31.7633(33.3684) | Bit/dim 3.6509(3.6821) | Xent 0.1664(0.1915) | Loss 9.5070(10.0405) | Error 0.0578(0.0651) Steps 1126(1141.16) | Grad Norm 117.5433(95.0577) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 34.7389(33.3572) | Bit/dim 3.6739(3.6764) | Xent 0.1984(0.1903) | Loss 9.6853(9.9255) | Error 0.0767(0.0654) Steps 1120(1138.46) | Grad Norm 23.4811(94.4092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 135.2196, Epoch Time 1983.0854(1775.7934), Bit/dim 3.6622(best: 3.5691), Xent 0.8419, Loss 4.0831, Error 0.2093(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 32.3158(33.1728) | Bit/dim 3.6363(3.6688) | Xent 0.1735(0.1869) | Loss 9.5589(10.7984) | Error 0.0622(0.0647) Steps 1078(1134.58) | Grad Norm 35.1167(79.9377) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 32.1871(33.0242) | Bit/dim 3.6473(3.6653) | Xent 0.1258(0.1783) | Loss 9.4678(10.4686) | Error 0.0400(0.0613) Steps 1120(1132.84) | Grad Norm 105.2388(80.6188) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 31.8822(32.9855) | Bit/dim 3.6375(3.6630) | Xent 0.1987(0.1792) | Loss 9.5651(10.2412) | Error 0.0667(0.0616) Steps 1156(1134.50) | Grad Norm 58.8058(84.9748) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 32.8194(32.9471) | Bit/dim 3.6500(3.6569) | Xent 0.1643(0.1793) | Loss 9.4988(10.0624) | Error 0.0567(0.0615) Steps 1120(1135.82) | Grad Norm 51.0309(85.6938) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 32.1639(32.7052) | Bit/dim 3.6469(3.6527) | Xent 0.1479(0.1752) | Loss 9.5018(9.9150) | Error 0.0511(0.0601) Steps 1114(1131.62) | Grad Norm 168.7660(88.4103) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 34.4566(32.7895) | Bit/dim 3.6797(3.6569) | Xent 0.1919(0.1792) | Loss 9.5537(9.8337) | Error 0.0678(0.0614) Steps 1162(1133.92) | Grad Norm 71.3040(93.1448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 133.6362, Epoch Time 1950.1860(1781.0252), Bit/dim 3.7080(best: 3.5691), Xent 0.8857, Loss 4.1508, Error 0.2209(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 34.0994(33.1014) | Bit/dim 3.7456(3.6773) | Xent 0.3229(0.1979) | Loss 9.8368(10.6361) | Error 0.1089(0.0670) Steps 1144(1137.39) | Grad Norm 165.0231(113.2802) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 34.5614(33.5166) | Bit/dim 3.7258(3.6998) | Xent 0.2307(0.2175) | Loss 9.7806(10.4407) | Error 0.0756(0.0729) Steps 1186(1146.67) | Grad Norm 72.7198(112.6019) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 35.5091(33.8364) | Bit/dim 3.7467(3.7128) | Xent 0.2308(0.2268) | Loss 9.9566(10.2959) | Error 0.0800(0.0765) Steps 1192(1154.63) | Grad Norm 113.4939(108.6729) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 33.8595(34.0587) | Bit/dim 3.6940(3.7164) | Xent 0.2141(0.2262) | Loss 9.8057(10.1614) | Error 0.0744(0.0763) Steps 1168(1156.53) | Grad Norm 44.8450(106.1851) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 34.0155(34.0417) | Bit/dim 3.7020(3.7145) | Xent 0.1898(0.2218) | Loss 9.6438(10.0504) | Error 0.0678(0.0747) Steps 1132(1153.05) | Grad Norm 72.0118(95.8770) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 135.8216, Epoch Time 2042.3272(1788.8642), Bit/dim 3.6931(best: 3.5691), Xent 0.8440, Loss 4.1151, Error 0.2158(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 34.4530(33.9092) | Bit/dim 3.6831(3.7039) | Xent 0.1881(0.2152) | Loss 9.7239(10.8874) | Error 0.0600(0.0728) Steps 1168(1149.82) | Grad Norm 49.6400(87.0978) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 34.5050(33.8728) | Bit/dim 3.6548(3.6958) | Xent 0.1943(0.2086) | Loss 9.6856(10.5501) | Error 0.0689(0.0698) Steps 1198(1149.65) | Grad Norm 126.8038(94.4907) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 34.3880(33.7441) | Bit/dim 3.6616(3.6900) | Xent 0.1556(0.1993) | Loss 9.5764(10.3062) | Error 0.0489(0.0666) Steps 1102(1143.48) | Grad Norm 55.0857(96.1508) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 33.2682(33.5698) | Bit/dim 3.6561(3.6793) | Xent 0.2081(0.1931) | Loss 9.5311(10.0941) | Error 0.0711(0.0647) Steps 1108(1139.30) | Grad Norm 103.3413(107.2716) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 32.6147(33.6052) | Bit/dim 3.6695(3.6780) | Xent 0.1786(0.1920) | Loss 9.4451(9.9692) | Error 0.0633(0.0646) Steps 1150(1139.74) | Grad Norm 108.8174(118.2437) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 33.4875(33.5045) | Bit/dim 3.6463(3.6775) | Xent 0.2111(0.1919) | Loss 9.6455(9.8855) | Error 0.0756(0.0646) Steps 1084(1134.41) | Grad Norm 101.4910(108.7553) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 135.5073, Epoch Time 1992.5469(1794.9747), Bit/dim 3.6842(best: 3.5691), Xent 0.8429, Loss 4.1056, Error 0.2124(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 34.7572(33.5708) | Bit/dim 3.6755(3.6775) | Xent 0.1850(0.1873) | Loss 9.5641(10.6723) | Error 0.0611(0.0629) Steps 1198(1138.69) | Grad Norm 34.4527(92.4465) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 32.4165(33.4577) | Bit/dim 3.7169(3.6791) | Xent 0.1691(0.1841) | Loss 9.8160(10.3985) | Error 0.0622(0.0615) Steps 1156(1136.41) | Grad Norm 22.5455(94.5415) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 33.4586(33.4023) | Bit/dim 3.7162(3.6809) | Xent 0.1910(0.1890) | Loss 9.7385(10.1940) | Error 0.0678(0.0631) Steps 1114(1134.92) | Grad Norm 108.9855(108.4549) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 33.7098(33.4633) | Bit/dim 3.6784(3.6816) | Xent 0.1600(0.1881) | Loss 9.7050(10.0511) | Error 0.0589(0.0635) Steps 1150(1136.54) | Grad Norm 40.7588(97.8133) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 33.1658(33.4193) | Bit/dim 3.6690(3.6803) | Xent 0.2196(0.1888) | Loss 9.5248(9.9502) | Error 0.0733(0.0637) Steps 1120(1136.53) | Grad Norm 69.0006(91.3897) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 136.0674, Epoch Time 1991.0705(1800.8576), Bit/dim 3.6832(best: 3.5691), Xent 0.8306, Loss 4.0984, Error 0.2117(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 33.1625(33.3299) | Bit/dim 3.7048(3.6792) | Xent 0.1463(0.1862) | Loss 9.6752(10.8344) | Error 0.0500(0.0628) Steps 1150(1134.74) | Grad Norm 36.9887(80.8818) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 34.9298(33.5416) | Bit/dim 3.7618(3.6909) | Xent 0.2985(0.2049) | Loss 10.1042(10.5799) | Error 0.1033(0.0682) Steps 1186(1143.70) | Grad Norm 80.4396(132.8185) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 34.4553(34.0161) | Bit/dim 3.7550(3.7122) | Xent 0.2710(0.2196) | Loss 9.9246(10.4257) | Error 0.0911(0.0722) Steps 1168(1158.63) | Grad Norm 38.7707(112.6474) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 34.5087(34.2342) | Bit/dim 3.7035(3.7200) | Xent 0.1998(0.2259) | Loss 9.7462(10.2703) | Error 0.0611(0.0746) Steps 1210(1160.11) | Grad Norm 43.7731(108.3412) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 33.5217(34.2932) | Bit/dim 3.7457(3.7167) | Xent 0.1924(0.2236) | Loss 9.9106(10.1428) | Error 0.0644(0.0743) Steps 1144(1164.21) | Grad Norm 163.2112(102.4278) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 31.1883(34.1899) | Bit/dim 3.6792(3.7116) | Xent 0.2088(0.2190) | Loss 9.6591(10.0365) | Error 0.0656(0.0732) Steps 1102(1158.57) | Grad Norm 24.2170(104.7786) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 135.8252, Epoch Time 2045.6644(1808.2018), Bit/dim 3.7169(best: 3.5691), Xent 0.8726, Loss 4.1532, Error 0.2235(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 32.6280(34.0599) | Bit/dim 3.6739(3.7043) | Xent 0.1980(0.2167) | Loss 9.6369(10.7550) | Error 0.0644(0.0727) Steps 1090(1156.55) | Grad Norm 215.7091(110.6098) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 33.7457(33.9088) | Bit/dim 3.6712(3.6951) | Xent 0.1803(0.2041) | Loss 9.4258(10.4437) | Error 0.0744(0.0688) Steps 1090(1148.57) | Grad Norm 63.3854(98.0942) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 32.9795(33.4711) | Bit/dim 3.6543(3.6864) | Xent 0.1523(0.1931) | Loss 9.5507(10.2152) | Error 0.0578(0.0653) Steps 1162(1138.67) | Grad Norm 78.3299(97.5267) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 32.2341(33.2560) | Bit/dim 3.6571(3.6786) | Xent 0.1855(0.1903) | Loss 9.6440(10.0481) | Error 0.0644(0.0648) Steps 1150(1136.37) | Grad Norm 19.8102(91.3414) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 32.4101(33.0852) | Bit/dim 3.6467(3.6721) | Xent 0.1879(0.1884) | Loss 9.5607(9.9188) | Error 0.0578(0.0644) Steps 1144(1133.19) | Grad Norm 11.6451(73.6081) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 134.9680, Epoch Time 1963.4371(1812.8589), Bit/dim 3.6684(best: 3.5691), Xent 0.8397, Loss 4.0882, Error 0.2128(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 32.0254(32.9012) | Bit/dim 3.6751(3.6672) | Xent 0.1766(0.1856) | Loss 9.6391(10.8067) | Error 0.0567(0.0634) Steps 1126(1132.82) | Grad Norm 61.6108(68.7163) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 33.8499(32.9520) | Bit/dim 3.6960(3.6717) | Xent 0.1650(0.1828) | Loss 9.6698(10.5023) | Error 0.0611(0.0626) Steps 1180(1135.19) | Grad Norm 18.7690(79.2284) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 33.5282(33.1169) | Bit/dim 3.7168(3.6756) | Xent 0.1662(0.1840) | Loss 9.5913(10.2824) | Error 0.0567(0.0626) Steps 1144(1140.90) | Grad Norm 43.8754(69.4053) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 34.7802(33.3442) | Bit/dim 3.6809(3.6778) | Xent 0.1882(0.1845) | Loss 9.6709(10.1387) | Error 0.0633(0.0629) Steps 1204(1148.47) | Grad Norm 74.5761(64.5076) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 33.1571(33.1827) | Bit/dim 3.6528(3.6767) | Xent 0.1991(0.1868) | Loss 9.6644(9.9980) | Error 0.0711(0.0626) Steps 1156(1139.40) | Grad Norm 164.3245(74.9428) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 33.4234(33.2634) | Bit/dim 3.7137(3.6804) | Xent 0.1760(0.1864) | Loss 9.6137(9.9125) | Error 0.0578(0.0630) Steps 1186(1144.28) | Grad Norm 51.2464(74.9404) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 137.1402, Epoch Time 1986.0075(1818.0533), Bit/dim 3.6944(best: 3.5691), Xent 0.8620, Loss 4.1254, Error 0.2155(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 32.5511(33.2827) | Bit/dim 3.6457(3.6815) | Xent 0.1732(0.1869) | Loss 9.3773(10.6601) | Error 0.0656(0.0634) Steps 1120(1143.32) | Grad Norm 50.6708(68.4306) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 33.0388(33.1818) | Bit/dim 3.7116(3.6793) | Xent 0.1581(0.1829) | Loss 9.8521(10.3884) | Error 0.0544(0.0621) Steps 1120(1136.01) | Grad Norm 32.7206(63.5658) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 32.6617(33.1227) | Bit/dim 3.7089(3.6770) | Xent 0.1599(0.1814) | Loss 9.5380(10.1763) | Error 0.0533(0.0617) Steps 1096(1133.96) | Grad Norm 37.7005(67.4545) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 33.4915(32.8786) | Bit/dim 3.6236(3.6732) | Xent 0.1643(0.1754) | Loss 9.2719(9.9968) | Error 0.0556(0.0597) Steps 1102(1127.54) | Grad Norm 81.5881(91.0079) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 35.0372(33.0626) | Bit/dim 3.7033(3.6834) | Xent 0.2465(0.1949) | Loss 9.7082(9.9390) | Error 0.0878(0.0658) Steps 1198(1134.57) | Grad Norm 141.0971(103.4624) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 140.9588, Epoch Time 1980.6223(1822.9304), Bit/dim 3.7554(best: 3.5691), Xent 0.8967, Loss 4.2038, Error 0.2187(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 35.1820(33.4079) | Bit/dim 3.7245(3.6964) | Xent 0.2444(0.2074) | Loss 9.8488(10.8795) | Error 0.0833(0.0700) Steps 1210(1141.58) | Grad Norm 37.3793(110.3960) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 33.1291(33.8973) | Bit/dim 3.7140(3.7086) | Xent 0.2621(0.2136) | Loss 9.6618(10.6095) | Error 0.0822(0.0713) Steps 1132(1152.76) | Grad Norm 25.7775(98.2859) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 35.3583(34.2309) | Bit/dim 3.6978(3.7084) | Xent 0.2193(0.2192) | Loss 9.6802(10.3958) | Error 0.0756(0.0728) Steps 1180(1161.05) | Grad Norm 62.3048(85.7000) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 36.0127(34.3457) | Bit/dim 3.6705(3.7055) | Xent 0.1884(0.2144) | Loss 9.6059(10.2158) | Error 0.0733(0.0719) Steps 1234(1164.69) | Grad Norm 120.6768(89.2005) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 32.6618(34.2107) | Bit/dim 3.6697(3.6994) | Xent 0.1985(0.2076) | Loss 9.6193(10.0657) | Error 0.0622(0.0691) Steps 1120(1160.60) | Grad Norm 19.4879(86.5775) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 32.7847(33.9375) | Bit/dim 3.6955(3.6929) | Xent 0.1775(0.1980) | Loss 9.6560(9.9499) | Error 0.0600(0.0662) Steps 1120(1155.73) | Grad Norm 51.7028(81.7773) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 136.6712, Epoch Time 2047.8906(1829.6792), Bit/dim 3.6798(best: 3.5691), Xent 0.8657, Loss 4.1126, Error 0.2132(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 33.2607(33.8067) | Bit/dim 3.6474(3.6869) | Xent 0.1756(0.1883) | Loss 9.5886(10.6655) | Error 0.0578(0.0635) Steps 1150(1155.35) | Grad Norm 16.7837(72.2338) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 31.3631(33.5256) | Bit/dim 3.6482(3.6786) | Xent 0.2228(0.1845) | Loss 9.5677(10.3805) | Error 0.0744(0.0624) Steps 1060(1144.50) | Grad Norm 179.1107(72.7790) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 33.2305(33.2956) | Bit/dim 3.6500(3.6729) | Xent 0.1625(0.1801) | Loss 9.6042(10.1680) | Error 0.0522(0.0604) Steps 1102(1136.04) | Grad Norm 62.4950(74.4933) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 32.3759(33.0194) | Bit/dim 3.6625(3.6692) | Xent 0.1363(0.1749) | Loss 9.6291(10.0081) | Error 0.0478(0.0587) Steps 1144(1134.39) | Grad Norm 45.7054(72.2448) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 32.0316(32.8902) | Bit/dim 3.6236(3.6649) | Xent 0.2056(0.1752) | Loss 9.4743(9.8869) | Error 0.0644(0.0585) Steps 1126(1131.92) | Grad Norm 33.1820(66.3655) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 134.8921, Epoch Time 1954.1360(1833.4129), Bit/dim 3.6640(best: 3.5691), Xent 0.8773, Loss 4.1027, Error 0.2139(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 32.2143(32.8431) | Bit/dim 3.6855(3.6649) | Xent 0.1801(0.1696) | Loss 9.7057(10.7917) | Error 0.0622(0.0571) Steps 1090(1129.91) | Grad Norm 65.3130(58.5262) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 33.2136(32.8479) | Bit/dim 3.6633(3.6651) | Xent 0.2015(0.1700) | Loss 9.5481(10.4820) | Error 0.0711(0.0574) Steps 1096(1128.24) | Grad Norm 54.8886(61.7294) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 32.3638(32.6210) | Bit/dim 3.6851(3.6604) | Xent 0.1515(0.1619) | Loss 9.6284(10.2331) | Error 0.0533(0.0546) Steps 1138(1121.93) | Grad Norm 50.6036(62.1870) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 31.9384(32.4843) | Bit/dim 3.6524(3.6562) | Xent 0.1321(0.1615) | Loss 9.4984(10.0540) | Error 0.0467(0.0542) Steps 1132(1119.73) | Grad Norm 11.4013(63.0437) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 33.3837(32.4050) | Bit/dim 3.6825(3.6533) | Xent 0.2367(0.1654) | Loss 9.6896(9.9184) | Error 0.0811(0.0554) Steps 1156(1114.25) | Grad Norm 69.6534(84.7417) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 33.1924(32.4637) | Bit/dim 3.6614(3.6590) | Xent 0.1853(0.1724) | Loss 9.6635(9.8471) | Error 0.0589(0.0576) Steps 1162(1116.73) | Grad Norm 101.5139(87.1202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 139.1423, Epoch Time 1936.8585(1836.5163), Bit/dim 3.6902(best: 3.5691), Xent 0.8873, Loss 4.1338, Error 0.2149(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 36.7840(33.1019) | Bit/dim 4.3972(3.7220) | Xent 1.6114(0.2753) | Loss 12.7725(10.8742) | Error 0.3511(0.0806) Steps 1216(1135.20) | Grad Norm 1291.0703(196.5768) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 38.6847(34.5617) | Bit/dim 4.3967(3.9039) | Xent 1.1094(0.5027) | Loss 12.3064(11.2692) | Error 0.2856(0.1374) Steps 1306(1178.25) | Grad Norm 560.5344(288.9539) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 38.4537(35.4569) | Bit/dim 3.9805(3.9550) | Xent 0.5083(0.5531) | Loss 10.6937(11.2355) | Error 0.1656(0.1593) Steps 1300(1202.10) | Grad Norm 105.6985(257.6296) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 36.4493(35.4745) | Bit/dim 3.8428(3.9318) | Xent 0.3466(0.5233) | Loss 10.0512(10.9807) | Error 0.1256(0.1565) Steps 1180(1197.48) | Grad Norm 51.4864(216.6971) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 33.9780(35.2493) | Bit/dim 3.7599(3.8923) | Xent 0.3244(0.4714) | Loss 9.8884(10.7288) | Error 0.1111(0.1448) Steps 1144(1188.63) | Grad Norm 109.3461(176.6588) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 136.5416, Epoch Time 2142.7862(1845.7044), Bit/dim 3.7429(best: 3.5691), Xent 0.8063, Loss 4.1461, Error 0.2206(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 32.3765(34.8463) | Bit/dim 3.7017(3.8511) | Xent 0.2534(0.4122) | Loss 9.6924(11.4364) | Error 0.0978(0.1299) Steps 1120(1175.49) | Grad Norm 15.5544(138.7254) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 34.2146(34.4930) | Bit/dim 3.6978(3.8110) | Xent 0.2351(0.3641) | Loss 9.7511(10.9771) | Error 0.0767(0.1164) Steps 1138(1164.17) | Grad Norm 29.5009(112.7960) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 34.6976(34.0924) | Bit/dim 3.6480(3.7766) | Xent 0.1470(0.3198) | Loss 9.3041(10.6180) | Error 0.0467(0.1034) Steps 1192(1158.56) | Grad Norm 39.3658(96.6364) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 32.3401(33.6862) | Bit/dim 3.6825(3.7499) | Xent 0.1751(0.2838) | Loss 9.6639(10.3496) | Error 0.0544(0.0929) Steps 1120(1148.07) | Grad Norm 31.0391(77.7026) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 32.8232(33.4871) | Bit/dim 3.7006(3.7313) | Xent 0.1717(0.2575) | Loss 9.6924(10.1540) | Error 0.0589(0.0847) Steps 1096(1142.12) | Grad Norm 19.3307(76.3192) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 32.9070(33.2294) | Bit/dim 3.9656(3.7272) | Xent 1.0291(0.2776) | Loss 11.2066(10.0880) | Error 0.1644(0.0837) Steps 1180(1134.44) | Grad Norm 659.9757(182.5759) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 134.6218, Epoch Time 1963.3398(1849.2334), Bit/dim 3.8657(best: 3.5691), Xent 1.1953, Loss 4.4634, Error 0.2646(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 34.5585(33.3782) | Bit/dim 3.7931(3.7407) | Xent 0.3297(0.2927) | Loss 9.9648(10.9075) | Error 0.1111(0.0892) Steps 1180(1144.72) | Grad Norm 30.1259(161.5286) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 33.5201(33.4296) | Bit/dim 3.7546(3.7507) | Xent 0.2478(0.2988) | Loss 9.8740(10.6709) | Error 0.0922(0.0938) Steps 1168(1145.95) | Grad Norm 20.2652(124.4920) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 32.7271(33.3926) | Bit/dim 3.7114(3.7472) | Xent 0.2552(0.2939) | Loss 9.6966(10.4642) | Error 0.0867(0.0942) Steps 1144(1142.47) | Grad Norm 15.5357(95.6692) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 32.5858(33.3291) | Bit/dim 3.7376(3.7451) | Xent 0.2663(0.2812) | Loss 9.8951(10.2922) | Error 0.0856(0.0911) Steps 1150(1143.14) | Grad Norm 9.0612(73.3313) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 30.7763(32.9768) | Bit/dim 3.6919(3.7353) | Xent 0.2317(0.2621) | Loss 9.6588(10.1350) | Error 0.0789(0.0866) Steps 1096(1134.17) | Grad Norm 8.6286(56.7172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 129.7417, Epoch Time 1964.1577(1852.6812), Bit/dim 3.6942(best: 3.5691), Xent 0.8017, Loss 4.0951, Error 0.2125(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 30.1209(32.5353) | Bit/dim 3.6693(3.7225) | Xent 0.1699(0.2450) | Loss 9.6519(10.9352) | Error 0.0622(0.0812) Steps 1072(1117.98) | Grad Norm 19.3100(44.9480) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 31.1084(32.2635) | Bit/dim 3.6807(3.7117) | Xent 0.2210(0.2337) | Loss 9.6828(10.5990) | Error 0.0800(0.0780) Steps 1114(1112.94) | Grad Norm 8.1741(36.0223) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 31.5783(31.9724) | Bit/dim 3.6650(3.7000) | Xent 0.2092(0.2212) | Loss 9.6493(10.3345) | Error 0.0800(0.0743) Steps 1066(1100.30) | Grad Norm 6.9232(29.6443) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 31.3615(31.7588) | Bit/dim 3.6543(3.6889) | Xent 0.1972(0.2089) | Loss 9.5605(10.1247) | Error 0.0722(0.0702) Steps 1072(1098.97) | Grad Norm 6.7674(24.7986) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 30.6847(31.4223) | Bit/dim 3.6571(3.6815) | Xent 0.1631(0.2016) | Loss 9.6010(9.9710) | Error 0.0533(0.0678) Steps 1102(1090.92) | Grad Norm 6.1425(33.3391) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 33.9350(31.8336) | Bit/dim 3.6600(3.6803) | Xent 0.1834(0.2003) | Loss 9.7879(9.8924) | Error 0.0667(0.0678) Steps 1096(1099.52) | Grad Norm 368.6380(50.1311) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 137.0641, Epoch Time 1883.4059(1853.6029), Bit/dim 3.6942(best: 3.5691), Xent 0.8467, Loss 4.1175, Error 0.2172(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 34.8976(32.5340) | Bit/dim 3.6974(3.6863) | Xent 0.1914(0.2028) | Loss 9.8029(10.6882) | Error 0.0633(0.0683) Steps 1174(1117.94) | Grad Norm 23.2479(44.7122) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 32.9706(32.7845) | Bit/dim 3.6706(3.6872) | Xent 0.2121(0.2048) | Loss 9.6158(10.4292) | Error 0.0756(0.0690) Steps 1120(1126.39) | Grad Norm 26.4877(39.0090) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 33.1976(32.9603) | Bit/dim 3.6674(3.6823) | Xent 0.1726(0.2008) | Loss 9.7079(10.2283) | Error 0.0567(0.0677) Steps 1114(1131.89) | Grad Norm 7.5969(36.2390) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 31.9330(32.7132) | Bit/dim 3.7444(3.6809) | Xent 0.1582(0.1958) | Loss 9.7072(10.0703) | Error 0.0478(0.0658) Steps 1096(1123.12) | Grad Norm 20.5134(32.4723) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 31.0236(32.5617) | Bit/dim 3.6636(3.6756) | Xent 0.1782(0.1906) | Loss 9.5544(9.9307) | Error 0.0600(0.0643) Steps 1078(1120.35) | Grad Norm 534.7535(68.8088) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 129.5129, Epoch Time 1960.9303(1856.8227), Bit/dim 3.6858(best: 3.5691), Xent 0.8654, Loss 4.1185, Error 0.2189(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 27.5992(31.8751) | Bit/dim 3.6522(3.6750) | Xent 0.1880(0.1933) | Loss 9.6000(10.7835) | Error 0.0644(0.0648) Steps 1006(1106.06) | Grad Norm 18.6757(94.6851) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 25.7015(30.4151) | Bit/dim 3.6844(3.6738) | Xent 0.1936(0.1949) | Loss 9.6841(10.4626) | Error 0.0622(0.0661) Steps 976(1074.79) | Grad Norm 7.3187(73.4713) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 26.8023(29.4407) | Bit/dim 3.6536(3.6705) | Xent 0.1275(0.1890) | Loss 9.4930(10.2080) | Error 0.0433(0.0646) Steps 988(1049.18) | Grad Norm 8.5180(57.5236) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 27.5383(28.8017) | Bit/dim 3.6186(3.6602) | Xent 0.1755(0.1825) | Loss 9.4602(10.0000) | Error 0.0667(0.0633) Steps 994(1035.10) | Grad Norm 349.7650(58.3629) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 27.1705(28.3475) | Bit/dim 3.6635(3.6550) | Xent 0.1212(0.1753) | Loss 9.4996(9.8512) | Error 0.0444(0.0611) Steps 988(1020.80) | Grad Norm 30.6303(49.7985) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 26.8269(27.8707) | Bit/dim 3.6390(3.6458) | Xent 0.1546(0.1731) | Loss 9.4631(9.7271) | Error 0.0578(0.0604) Steps 976(1009.12) | Grad Norm 9.4152(43.6095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 114.7006, Epoch Time 1613.5431(1849.5243), Bit/dim 3.6332(best: 3.5691), Xent 0.8131, Loss 4.0398, Error 0.2067(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 27.9721(27.6117) | Bit/dim 3.6156(3.6406) | Xent 0.1735(0.1692) | Loss 9.3764(10.4437) | Error 0.0667(0.0586) Steps 982(1003.19) | Grad Norm 27.9200(40.7941) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 27.2276(27.4950) | Bit/dim 3.6488(3.6376) | Xent 0.1560(0.1669) | Loss 9.4223(10.1779) | Error 0.0533(0.0581) Steps 970(999.30) | Grad Norm 190.9555(46.2234) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 29.8080(27.5277) | Bit/dim 3.6278(3.6354) | Xent 0.1684(0.1663) | Loss 9.4502(9.9893) | Error 0.0600(0.0575) Steps 1096(1004.08) | Grad Norm 722.4852(129.3862) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 35.1678(29.0393) | Bit/dim 3.8740(3.6785) | Xent 0.4110(0.2211) | Loss 10.4645(10.0338) | Error 0.1122(0.0706) Steps 1186(1044.82) | Grad Norm 72.7114(112.2127) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 35.9974(30.7549) | Bit/dim 3.8259(3.7104) | Xent 0.3049(0.2528) | Loss 10.1059(10.0640) | Error 0.0978(0.0789) Steps 1204(1091.41) | Grad Norm 22.4649(92.9722) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 139.9731, Epoch Time 1835.8637(1849.1145), Bit/dim 3.7430(best: 3.5691), Xent 0.8219, Loss 4.1539, Error 0.2251(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 36.5413(31.9476) | Bit/dim 3.7190(3.7159) | Xent 0.2321(0.2552) | Loss 9.9115(11.0139) | Error 0.0822(0.0799) Steps 1180(1114.81) | Grad Norm 10.8246(79.1831) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 34.0243(32.7470) | Bit/dim 3.7068(3.7114) | Xent 0.2186(0.2454) | Loss 9.7649(10.6785) | Error 0.0744(0.0783) Steps 1126(1131.12) | Grad Norm 13.1531(72.4622) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 32.0285(32.8274) | Bit/dim 3.6408(3.6997) | Xent 0.1758(0.2290) | Loss 9.5316(10.3962) | Error 0.0589(0.0737) Steps 1138(1133.71) | Grad Norm 11.7112(55.9456) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 34.4982(33.0028) | Bit/dim 3.6654(3.6888) | Xent 0.1710(0.2151) | Loss 9.5465(10.1754) | Error 0.0689(0.0706) Steps 1144(1135.69) | Grad Norm 12.8034(44.3375) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 33.3467(32.9661) | Bit/dim 3.6383(3.6790) | Xent 0.1828(0.2066) | Loss 9.6372(10.0153) | Error 0.0644(0.0680) Steps 1126(1133.47) | Grad Norm 12.9425(36.4889) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 32.0400(32.7183) | Bit/dim 3.6491(3.6696) | Xent 0.1944(0.1967) | Loss 9.6436(9.8920) | Error 0.0589(0.0649) Steps 1114(1128.45) | Grad Norm 7.8414(31.3089) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 135.0673, Epoch Time 1997.2133(1853.5575), Bit/dim 3.6543(best: 3.5691), Xent 0.8198, Loss 4.0641, Error 0.2117(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 33.7315(32.4689) | Bit/dim 3.7104(3.6640) | Xent 0.2085(0.1898) | Loss 9.6742(10.6393) | Error 0.0811(0.0640) Steps 1180(1124.50) | Grad Norm 695.9090(156.8234) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 40.5001(33.7111) | Bit/dim 4.0601(3.7242) | Xent 0.6065(0.2584) | Loss 11.0547(10.5941) | Error 0.1600(0.0796) Steps 1348(1154.45) | Grad Norm 118.6905(211.0857) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 38.4551(34.9913) | Bit/dim 4.1388(3.8315) | Xent 0.6055(0.3564) | Loss 11.2527(10.7840) | Error 0.1622(0.1040) Steps 1294(1192.30) | Grad Norm 110.8809(192.0251) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 40.7098(36.1356) | Bit/dim 4.0469(3.9061) | Xent 0.6414(0.4253) | Loss 11.0873(10.8919) | Error 0.1778(0.1242) Steps 1348(1221.68) | Grad Norm 97.4469(195.0297) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 38.5625(36.9882) | Bit/dim 3.9660(3.9318) | Xent 0.4250(0.4460) | Loss 10.5068(10.8593) | Error 0.1478(0.1346) Steps 1312(1243.32) | Grad Norm 84.8902(172.4220) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 150.5136, Epoch Time 2223.9592(1864.6695), Bit/dim 3.8804(best: 3.5691), Xent 0.8313, Loss 4.2961, Error 0.2300(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 37.9003(37.4394) | Bit/dim 3.8389(3.9172) | Xent 0.3684(0.4275) | Loss 10.2056(11.6987) | Error 0.1233(0.1318) Steps 1210(1247.07) | Grad Norm 37.5006(140.2503) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 37.4907(37.4348) | Bit/dim 3.7436(3.8842) | Xent 0.2872(0.3923) | Loss 9.9650(11.2557) | Error 0.0944(0.1228) Steps 1234(1246.03) | Grad Norm 38.8801(111.3631) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 35.9063(37.3024) | Bit/dim 3.7526(3.8511) | Xent 0.2350(0.3599) | Loss 9.8167(10.9256) | Error 0.0767(0.1151) Steps 1222(1239.21) | Grad Norm 24.2170(88.3565) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 35.5156(37.0330) | Bit/dim 3.7286(3.8240) | Xent 0.2066(0.3282) | Loss 9.8412(10.6513) | Error 0.0622(0.1057) Steps 1216(1232.23) | Grad Norm 12.7085(72.8153) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 35.3904(36.6157) | Bit/dim 3.7172(3.7956) | Xent 0.2409(0.2977) | Loss 9.8930(10.4209) | Error 0.0800(0.0971) Steps 1192(1220.19) | Grad Norm 16.5730(60.3949) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 37.3582(36.6082) | Bit/dim 3.6822(3.7710) | Xent 0.2508(0.2750) | Loss 9.6837(10.2370) | Error 0.0867(0.0903) Steps 1192(1216.47) | Grad Norm 17.6734(50.4037) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 143.4032, Epoch Time 2181.7090(1874.1807), Bit/dim 3.7083(best: 3.5691), Xent 0.8267, Loss 4.1217, Error 0.2134(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 35.6512(36.3376) | Bit/dim 3.7226(3.7511) | Xent 0.1924(0.2520) | Loss 9.8550(10.9398) | Error 0.0644(0.0830) Steps 1216(1208.94) | Grad Norm 11.0960(43.2754) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 35.2930(35.9825) | Bit/dim 3.6822(3.7335) | Xent 0.1779(0.2365) | Loss 9.7578(10.6078) | Error 0.0633(0.0783) Steps 1192(1197.95) | Grad Norm 30.9599(37.0228) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 35.9530(35.7695) | Bit/dim 3.6814(3.7200) | Xent 0.1630(0.2236) | Loss 9.6934(10.3639) | Error 0.0544(0.0745) Steps 1210(1191.35) | Grad Norm 14.6135(35.2583) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 34.5120(35.3482) | Bit/dim 3.6670(3.7091) | Xent 0.1855(0.2128) | Loss 9.7273(10.1828) | Error 0.0622(0.0716) Steps 1126(1182.66) | Grad Norm 8.0156(30.3181) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 33.6766(35.1437) | Bit/dim 3.6882(3.6984) | Xent 0.2024(0.2046) | Loss 9.7186(10.0247) | Error 0.0733(0.0702) Steps 1120(1171.43) | Grad Norm 30.9591(28.4844) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 139.8746, Epoch Time 2075.3422(1880.2156), Bit/dim 3.6725(best: 3.5691), Xent 0.8415, Loss 4.0932, Error 0.2137(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 33.4355(35.0479) | Bit/dim 3.6348(3.6902) | Xent 0.1685(0.1965) | Loss 9.5239(10.8794) | Error 0.0633(0.0672) Steps 1168(1172.76) | Grad Norm 29.1980(30.3121) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 35.5066(34.7956) | Bit/dim 3.6247(3.6809) | Xent 0.1697(0.1873) | Loss 9.4979(10.5332) | Error 0.0678(0.0638) Steps 1174(1168.75) | Grad Norm 19.0260(27.0767) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 33.3642(34.4223) | Bit/dim 3.6391(3.6723) | Xent 0.1404(0.1815) | Loss 9.4908(10.2751) | Error 0.0467(0.0614) Steps 1132(1161.52) | Grad Norm 28.7400(26.6499) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 34.8111(34.1441) | Bit/dim 3.6343(3.6670) | Xent 0.1567(0.1756) | Loss 9.4423(10.0906) | Error 0.0500(0.0592) Steps 1138(1153.69) | Grad Norm 26.1591(25.2646) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 33.9661(33.8234) | Bit/dim 3.6457(3.6650) | Xent 0.1593(0.1734) | Loss 9.6079(9.9590) | Error 0.0489(0.0589) Steps 1156(1148.25) | Grad Norm 214.3278(39.6146) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 32.2809(33.6161) | Bit/dim 3.6785(3.6621) | Xent 0.1661(0.1729) | Loss 9.6932(9.8638) | Error 0.0578(0.0587) Steps 1144(1145.82) | Grad Norm 259.9655(53.6189) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 136.1249, Epoch Time 1997.6639(1883.7390), Bit/dim 3.6626(best: 3.5691), Xent 0.8614, Loss 4.0933, Error 0.2152(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 32.9777(33.2820) | Bit/dim 3.7355(3.6672) | Xent 0.1895(0.1757) | Loss 9.7368(10.6386) | Error 0.0689(0.0596) Steps 1168(1140.93) | Grad Norm 29.5324(66.2558) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 37.4772(33.8183) | Bit/dim 3.8057(3.6869) | Xent 0.2846(0.1978) | Loss 10.1598(10.4473) | Error 0.0900(0.0650) Steps 1276(1159.04) | Grad Norm 36.5256(102.0467) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 38.0384(34.6629) | Bit/dim 3.8341(3.7185) | Xent 0.2996(0.2286) | Loss 10.0991(10.3692) | Error 0.0889(0.0739) Steps 1240(1182.00) | Grad Norm 90.0218(95.7017) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 36.4537(35.2960) | Bit/dim 3.7498(3.7348) | Xent 0.2313(0.2398) | Loss 9.8828(10.2767) | Error 0.0767(0.0779) Steps 1252(1195.82) | Grad Norm 36.0837(83.7807) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 35.6989(35.7566) | Bit/dim 3.7189(3.7391) | Xent 0.2369(0.2445) | Loss 9.7320(10.1895) | Error 0.0856(0.0805) Steps 1204(1198.45) | Grad Norm 43.3418(70.4734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 151.6441, Epoch Time 2142.1130(1891.4902), Bit/dim 3.7354(best: 3.5691), Xent 0.8542, Loss 4.1625, Error 0.2220(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 37.2735(36.0673) | Bit/dim 3.7119(3.7370) | Xent 0.1987(0.2360) | Loss 9.7510(11.1008) | Error 0.0678(0.0785) Steps 1216(1207.59) | Grad Norm 19.4747(59.1087) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 34.8529(36.0889) | Bit/dim 3.7212(3.7294) | Xent 0.1858(0.2245) | Loss 9.7819(10.7482) | Error 0.0533(0.0750) Steps 1216(1208.84) | Grad Norm 12.8892(59.0573) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 37.7689(36.1754) | Bit/dim 3.7161(3.7227) | Xent 0.1960(0.2152) | Loss 9.6774(10.4884) | Error 0.0600(0.0723) Steps 1204(1213.09) | Grad Norm 19.6586(53.2383) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 35.5158(35.9462) | Bit/dim 3.7013(3.7111) | Xent 0.1749(0.2080) | Loss 9.8051(10.2846) | Error 0.0567(0.0698) Steps 1186(1210.48) | Grad Norm 81.9154(48.6501) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 35.1248(35.7091) | Bit/dim 3.6810(3.7053) | Xent 0.1933(0.2045) | Loss 9.6708(10.1220) | Error 0.0678(0.0687) Steps 1204(1204.28) | Grad Norm 71.3288(45.4267) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 34.9987(35.5938) | Bit/dim 3.7070(3.7001) | Xent 0.1912(0.2004) | Loss 9.7701(10.0136) | Error 0.0633(0.0676) Steps 1168(1199.11) | Grad Norm 32.5826(41.3679) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 143.6141, Epoch Time 2128.9167(1898.6130), Bit/dim 3.6940(best: 3.5691), Xent 0.8503, Loss 4.1191, Error 0.2124(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 35.5174(35.4787) | Bit/dim 3.6882(3.6930) | Xent 0.1583(0.1947) | Loss 9.7938(10.7589) | Error 0.0489(0.0659) Steps 1162(1194.04) | Grad Norm 13.5161(39.1185) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 35.2865(35.3048) | Bit/dim 3.6490(3.6863) | Xent 0.1541(0.1876) | Loss 9.6125(10.4634) | Error 0.0522(0.0633) Steps 1174(1192.11) | Grad Norm 23.7553(35.9752) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 35.1960(35.0890) | Bit/dim 3.6631(3.6810) | Xent 0.1821(0.1840) | Loss 9.6750(10.2536) | Error 0.0622(0.0626) Steps 1150(1183.56) | Grad Norm 41.4762(33.2198) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 34.3039(34.8543) | Bit/dim 3.6268(3.6757) | Xent 0.1889(0.1792) | Loss 9.5973(10.0851) | Error 0.0567(0.0610) Steps 1192(1179.37) | Grad Norm 26.1820(30.8634) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 33.7740(34.6327) | Bit/dim 3.6442(3.6691) | Xent 0.1592(0.1714) | Loss 9.6305(9.9473) | Error 0.0489(0.0578) Steps 1168(1171.35) | Grad Norm 34.4124(29.5144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 139.0658, Epoch Time 2048.8808(1903.1211), Bit/dim 3.6678(best: 3.5691), Xent 0.8484, Loss 4.0920, Error 0.2145(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 33.2357(34.2511) | Bit/dim 3.6297(3.6672) | Xent 0.1423(0.1663) | Loss 9.4623(10.8108) | Error 0.0511(0.0562) Steps 1114(1164.16) | Grad Norm 33.8320(32.8191) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 33.8632(34.0781) | Bit/dim 3.6762(3.6631) | Xent 0.1598(0.1627) | Loss 9.6433(10.4817) | Error 0.0489(0.0550) Steps 1174(1157.53) | Grad Norm 19.9184(46.6297) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 34.4953(34.1036) | Bit/dim 3.6579(3.6624) | Xent 0.1481(0.1613) | Loss 9.5178(10.2461) | Error 0.0433(0.0546) Steps 1186(1157.96) | Grad Norm 126.2183(54.1711) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 34.3364(34.2970) | Bit/dim 3.6494(3.6605) | Xent 0.1814(0.1641) | Loss 9.6293(10.0816) | Error 0.0644(0.0561) Steps 1156(1156.72) | Grad Norm 17.9396(48.0692) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 34.1964(34.2833) | Bit/dim 3.6696(3.6638) | Xent 0.2001(0.1683) | Loss 9.5642(9.9720) | Error 0.0633(0.0574) Steps 1150(1159.89) | Grad Norm 140.0695(84.9344) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 34.4239(34.4076) | Bit/dim 3.7070(3.6706) | Xent 0.2216(0.1832) | Loss 9.6680(9.9082) | Error 0.0800(0.0611) Steps 1168(1164.68) | Grad Norm 52.2980(161.9504) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 149.4545, Epoch Time 2051.4266(1907.5702), Bit/dim 3.8075(best: 3.5691), Xent 0.9282, Loss 4.2716, Error 0.2402(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 41.3598(35.9101) | Bit/dim 4.4296(3.8029) | Xent 1.1005(0.3360) | Loss 12.5397(11.1892) | Error 0.2844(0.0958) Steps 1348(1207.52) | Grad Norm 327.1497(181.1736) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 38.7201(37.3261) | Bit/dim 4.1877(3.9428) | Xent 0.6421(0.4552) | Loss 11.3224(11.3686) | Error 0.2022(0.1305) Steps 1336(1245.67) | Grad Norm 103.3860(190.1235) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 37.6586(38.0852) | Bit/dim 3.9642(3.9770) | Xent 0.4953(0.4784) | Loss 10.5758(11.2783) | Error 0.1656(0.1431) Steps 1264(1266.68) | Grad Norm 65.9297(168.1505) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 37.8603(38.1291) | Bit/dim 3.8229(3.9551) | Xent 0.3322(0.4557) | Loss 10.1397(11.0427) | Error 0.1133(0.1417) Steps 1252(1265.76) | Grad Norm 25.1343(143.6509) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 37.5575(37.8649) | Bit/dim 3.7682(3.9145) | Xent 0.2961(0.4168) | Loss 10.0079(10.7757) | Error 0.0989(0.1323) Steps 1204(1252.50) | Grad Norm 116.0776(121.4142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 145.9278, Epoch Time 2320.5190(1919.9587), Bit/dim 3.7714(best: 3.5691), Xent 0.8229, Loss 4.1828, Error 0.2189(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 35.0035(37.5022) | Bit/dim 3.7285(3.8737) | Xent 0.2202(0.3714) | Loss 9.6964(11.5392) | Error 0.0744(0.1189) Steps 1168(1239.17) | Grad Norm 33.1412(102.6263) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 36.3811(37.0476) | Bit/dim 3.7568(3.8360) | Xent 0.1935(0.3321) | Loss 9.7357(11.0741) | Error 0.0644(0.1071) Steps 1168(1226.74) | Grad Norm 49.9099(85.6864) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 35.8770(36.7031) | Bit/dim 3.6890(3.8025) | Xent 0.1712(0.2978) | Loss 9.6752(10.7252) | Error 0.0578(0.0975) Steps 1216(1219.81) | Grad Norm 27.5623(87.5031) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 33.9147(36.1960) | Bit/dim 3.6965(3.7776) | Xent 0.1914(0.2718) | Loss 9.6642(10.4603) | Error 0.0733(0.0898) Steps 1120(1204.91) | Grad Norm 28.1367(86.0267) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 32.7420(35.6438) | Bit/dim 3.6922(3.7509) | Xent 0.2183(0.2500) | Loss 9.7371(10.2417) | Error 0.0856(0.0832) Steps 1162(1185.87) | Grad Norm 49.5256(75.3998) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 33.5039(35.2762) | Bit/dim 3.6485(3.7321) | Xent 0.2056(0.2313) | Loss 9.5955(10.0845) | Error 0.0789(0.0781) Steps 1162(1172.16) | Grad Norm 45.0620(68.4947) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 139.6688, Epoch Time 2084.3249(1924.8897), Bit/dim 3.6830(best: 3.5691), Xent 0.8429, Loss 4.1045, Error 0.2126(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 34.3578(35.1081) | Bit/dim 3.6920(3.7188) | Xent 0.1491(0.2164) | Loss 9.6068(10.8167) | Error 0.0544(0.0729) Steps 1138(1169.96) | Grad Norm 18.0618(80.5896) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 34.8978(35.1577) | Bit/dim 3.7041(3.7097) | Xent 0.1751(0.2068) | Loss 9.6366(10.5111) | Error 0.0600(0.0696) Steps 1168(1173.63) | Grad Norm 21.5744(74.2248) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 34.5057(35.2896) | Bit/dim 3.7123(3.7036) | Xent 0.2047(0.2024) | Loss 9.8110(10.2943) | Error 0.0744(0.0684) Steps 1180(1173.64) | Grad Norm 34.2815(62.8749) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 37.0990(35.1314) | Bit/dim 3.6929(3.6973) | Xent 0.1781(0.2001) | Loss 9.6446(10.1324) | Error 0.0600(0.0672) Steps 1204(1170.15) | Grad Norm 55.5511(67.9812) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 33.7989(35.2741) | Bit/dim 3.6755(3.6914) | Xent 0.1760(0.1943) | Loss 9.6873(9.9995) | Error 0.0656(0.0663) Steps 1162(1175.72) | Grad Norm 85.6909(62.0484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 142.5221, Epoch Time 2098.0821(1930.0854), Bit/dim 3.6942(best: 3.5691), Xent 0.8554, Loss 4.1219, Error 0.2200(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 35.0874(35.1284) | Bit/dim 3.6931(3.6902) | Xent 0.1511(0.1904) | Loss 9.6441(10.8878) | Error 0.0500(0.0644) Steps 1150(1174.73) | Grad Norm 32.7222(56.3755) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 35.7848(34.9770) | Bit/dim 3.6880(3.6857) | Xent 0.1824(0.1894) | Loss 9.7585(10.5722) | Error 0.0567(0.0637) Steps 1210(1174.56) | Grad Norm 510.1833(77.9264) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 35.6941(35.2034) | Bit/dim 3.7060(3.6890) | Xent 0.2095(0.1867) | Loss 9.8070(10.3437) | Error 0.0711(0.0633) Steps 1168(1182.54) | Grad Norm 42.8284(84.6631) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 34.8511(35.2467) | Bit/dim 3.6795(3.6822) | Xent 0.1788(0.1831) | Loss 9.5533(10.1570) | Error 0.0589(0.0617) Steps 1156(1182.98) | Grad Norm 34.9029(67.8485) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 34.9868(35.1899) | Bit/dim 3.6649(3.6772) | Xent 0.1655(0.1784) | Loss 9.6385(10.0157) | Error 0.0589(0.0602) Steps 1210(1182.82) | Grad Norm 25.6679(56.8393) | Total Time 0.00(0.00)\n",
      "Iter 17930 | Time 33.7781(35.0837) | Bit/dim 3.6045(3.6712) | Xent 0.1916(0.1750) | Loss 9.4997(9.9055) | Error 0.0600(0.0594) Steps 1162(1173.63) | Grad Norm 45.4803(53.5671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 139.3708, Epoch Time 2084.6406(1934.7221), Bit/dim 3.6682(best: 3.5691), Xent 0.8488, Loss 4.0926, Error 0.2112(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 33.2769(34.7902) | Bit/dim 3.6287(3.6644) | Xent 0.1625(0.1674) | Loss 9.5926(10.6691) | Error 0.0544(0.0570) Steps 1180(1173.64) | Grad Norm 8.7770(62.0892) | Total Time 0.00(0.00)\n",
      "Iter 17950 | Time 32.4847(34.2178) | Bit/dim 3.6686(3.6616) | Xent 0.1821(0.1673) | Loss 9.7577(10.3798) | Error 0.0533(0.0570) Steps 1126(1160.73) | Grad Norm 13.7569(70.3185) | Total Time 0.00(0.00)\n",
      "Iter 17960 | Time 31.3424(33.5448) | Bit/dim 3.6260(3.6544) | Xent 0.1647(0.1683) | Loss 9.5571(10.1576) | Error 0.0511(0.0569) Steps 1114(1143.86) | Grad Norm 5.1011(79.1963) | Total Time 0.00(0.00)\n",
      "Iter 17970 | Time 32.5448(32.8978) | Bit/dim 3.6406(3.6493) | Xent 0.1708(0.1668) | Loss 9.5381(9.9879) | Error 0.0578(0.0563) Steps 1102(1128.02) | Grad Norm 33.8712(73.4379) | Total Time 0.00(0.00)\n",
      "Iter 17980 | Time 29.3289(32.1707) | Bit/dim 3.6118(3.6420) | Xent 0.1622(0.1646) | Loss 9.4059(9.8462) | Error 0.0567(0.0558) Steps 1006(1111.04) | Grad Norm 19.9824(78.7797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 127.2447, Epoch Time 1890.2388(1933.3876), Bit/dim 3.6434(best: 3.5691), Xent 0.8460, Loss 4.0664, Error 0.2123(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 29.0436(31.4488) | Bit/dim 3.6227(3.6409) | Xent 0.1658(0.1629) | Loss 9.3675(10.6827) | Error 0.0544(0.0550) Steps 1054(1098.93) | Grad Norm 4016.8604(212.0714) | Total Time 0.00(0.00)\n",
      "Iter 18000 | Time 36.4008(32.4212) | Bit/dim 3.8662(3.6710) | Xent 0.3489(0.2096) | Loss 10.3663(10.5195) | Error 0.1022(0.0673) Steps 1210(1128.13) | Grad Norm 69.2470(182.3481) | Total Time 0.00(0.00)\n",
      "Iter 18010 | Time 35.4377(33.7030) | Bit/dim 3.8217(3.7254) | Xent 0.3540(0.2634) | Loss 10.1784(10.5024) | Error 0.1167(0.0807) Steps 1204(1163.41) | Grad Norm 30.2829(162.2170) | Total Time 0.00(0.00)\n",
      "Iter 18020 | Time 37.3622(34.5529) | Bit/dim 3.8072(3.7458) | Xent 0.3006(0.2840) | Loss 9.9204(10.4001) | Error 0.1033(0.0884) Steps 1246(1178.04) | Grad Norm 18.2177(126.8778) | Total Time 0.00(0.00)\n",
      "Iter 18030 | Time 36.4139(35.1459) | Bit/dim 3.6797(3.7403) | Xent 0.2213(0.2733) | Loss 9.7207(10.2528) | Error 0.0733(0.0868) Steps 1228(1185.82) | Grad Norm 11.1207(98.3359) | Total Time 0.00(0.00)\n",
      "Iter 18040 | Time 35.9184(35.3377) | Bit/dim 3.6894(3.7326) | Xent 0.2498(0.2613) | Loss 9.7627(10.1285) | Error 0.0722(0.0835) Steps 1192(1189.92) | Grad Norm 14.4909(76.7934) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 143.9080, Epoch Time 2126.9872(1939.1956), Bit/dim 3.7029(best: 3.5691), Xent 0.8282, Loss 4.1170, Error 0.2142(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 35.6153(35.5626) | Bit/dim 3.6849(3.7213) | Xent 0.1798(0.2452) | Loss 9.7181(10.8649) | Error 0.0611(0.0796) Steps 1174(1191.83) | Grad Norm 9.0740(59.7378) | Total Time 0.00(0.00)\n",
      "Iter 18060 | Time 37.5569(35.6808) | Bit/dim 3.6690(3.7113) | Xent 0.2024(0.2299) | Loss 9.7454(10.5654) | Error 0.0667(0.0756) Steps 1216(1192.49) | Grad Norm 18.6382(49.3916) | Total Time 0.00(0.00)\n",
      "Iter 18070 | Time 35.5856(35.7456) | Bit/dim 3.6521(3.6993) | Xent 0.1664(0.2152) | Loss 9.4714(10.3200) | Error 0.0556(0.0721) Steps 1144(1186.09) | Grad Norm 12.5989(39.6220) | Total Time 0.00(0.00)\n",
      "Iter 18080 | Time 36.5017(35.6255) | Bit/dim 3.6465(3.6890) | Xent 0.1520(0.2023) | Loss 9.6432(10.1427) | Error 0.0411(0.0672) Steps 1216(1184.57) | Grad Norm 9.2897(42.1268) | Total Time 0.00(0.00)\n",
      "Iter 18090 | Time 33.0979(35.3384) | Bit/dim 3.6508(3.6800) | Xent 0.1678(0.1914) | Loss 9.7194(10.0058) | Error 0.0589(0.0641) Steps 1180(1179.27) | Grad Norm 12.1149(43.4381) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 139.7461, Epoch Time 2110.2273(1944.3265), Bit/dim 3.6715(best: 3.5691), Xent 0.8572, Loss 4.1001, Error 0.2144(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 34.9153(35.1689) | Bit/dim 3.6551(3.6752) | Xent 0.1665(0.1870) | Loss 9.5255(10.8735) | Error 0.0633(0.0629) Steps 1204(1178.20) | Grad Norm 408.8535(50.3512) | Total Time 0.00(0.00)\n",
      "Iter 18110 | Time 35.6294(35.0795) | Bit/dim 3.7199(3.6794) | Xent 0.2120(0.1917) | Loss 9.7667(10.5749) | Error 0.0744(0.0637) Steps 1156(1178.25) | Grad Norm 42.8047(106.1056) | Total Time 0.00(0.00)\n",
      "Iter 18120 | Time 36.2245(35.4078) | Bit/dim 3.6954(3.6861) | Xent 0.2914(0.2079) | Loss 9.7998(10.3747) | Error 0.0956(0.0679) Steps 1198(1185.34) | Grad Norm 3014.5826(230.0142) | Total Time 0.00(0.00)\n",
      "Iter 18130 | Time 43.7134(37.1197) | Bit/dim 5.2757(3.9460) | Xent 2.2174(0.4981) | Loss 15.4271(11.1111) | Error 0.4889(0.1349) Steps 1402(1236.02) | Grad Norm 739.7170(277.6414) | Total Time 0.00(0.00)\n",
      "Iter 18140 | Time 42.5131(38.6687) | Bit/dim 4.5728(4.2037) | Xent 1.0616(0.7696) | Loss 12.6776(11.8635) | Error 0.3233(0.2062) Steps 1384(1280.66) | Grad Norm 174.6581(357.1189) | Total Time 0.00(0.00)\n",
      "Iter 18150 | Time 41.9541(39.1650) | Bit/dim 4.2549(4.2535) | Xent 0.7726(0.8153) | Loss 11.6302(11.9173) | Error 0.2789(0.2337) Steps 1306(1292.33) | Grad Norm 82.1565(293.3061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 156.0525, Epoch Time 2313.1817(1955.3922), Bit/dim 4.2583(best: 3.5691), Xent 0.8702, Loss 4.6934, Error 0.2946(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 38.6771(39.1456) | Bit/dim 4.0705(4.2290) | Xent 0.5713(0.7774) | Loss 10.9233(12.5794) | Error 0.2033(0.2324) Steps 1204(1278.32) | Grad Norm 116.0995(236.7156) | Total Time 0.00(0.00)\n",
      "Iter 18170 | Time 36.9584(38.8719) | Bit/dim 3.9958(4.1756) | Xent 0.4390(0.7077) | Loss 10.4182(12.0866) | Error 0.1467(0.2175) Steps 1192(1264.70) | Grad Norm 28.9212(191.4877) | Total Time 0.00(0.00)\n",
      "Iter 18180 | Time 38.1404(38.3760) | Bit/dim 3.9589(4.1131) | Xent 0.3778(0.6361) | Loss 10.4374(11.6538) | Error 0.1333(0.1993) Steps 1174(1249.23) | Grad Norm 72.2419(162.4894) | Total Time 0.00(0.00)\n",
      "Iter 18190 | Time 34.8845(37.7479) | Bit/dim 3.8302(4.0521) | Xent 0.3714(0.5646) | Loss 10.0848(11.2698) | Error 0.1389(0.1803) Steps 1198(1235.65) | Grad Norm 29.2280(136.5320) | Total Time 0.00(0.00)\n",
      "Iter 18200 | Time 35.6224(37.1662) | Bit/dim 3.8434(3.9999) | Xent 0.3713(0.5078) | Loss 10.1042(10.9702) | Error 0.1311(0.1646) Steps 1150(1218.07) | Grad Norm 26.1775(113.5430) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 140.3799, Epoch Time 2192.2062(1962.4966), Bit/dim 3.8183(best: 3.5691), Xent 0.7551, Loss 4.1958, Error 0.2177(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 34.6977(36.5688) | Bit/dim 3.7806(3.9476) | Xent 0.3004(0.4576) | Loss 9.9732(11.6763) | Error 0.1078(0.1508) Steps 1120(1202.96) | Grad Norm 40.1055(94.7491) | Total Time 0.00(0.00)\n",
      "Iter 18220 | Time 34.2640(36.1750) | Bit/dim 3.8006(3.9056) | Xent 0.2655(0.4155) | Loss 9.8392(11.2092) | Error 0.0944(0.1391) Steps 1126(1190.91) | Grad Norm 100.5354(84.6137) | Total Time 0.00(0.00)\n",
      "Iter 18230 | Time 35.3706(35.7372) | Bit/dim 3.7765(3.8693) | Xent 0.2707(0.3784) | Loss 9.8114(10.8611) | Error 0.0967(0.1279) Steps 1144(1184.13) | Grad Norm 1111.4439(115.0418) | Total Time 0.00(0.00)\n",
      "Iter 18240 | Time 34.1445(35.5550) | Bit/dim 3.8040(3.8479) | Xent 0.2530(0.3613) | Loss 9.9169(10.6273) | Error 0.0933(0.1231) Steps 1162(1182.13) | Grad Norm 26.6910(122.4937) | Total Time 0.00(0.00)\n",
      "Iter 18250 | Time 34.6720(35.5898) | Bit/dim 3.7932(3.8321) | Xent 0.2218(0.3363) | Loss 9.7120(10.4511) | Error 0.0700(0.1153) Steps 1192(1184.67) | Grad Norm 26.2364(97.0486) | Total Time 0.00(0.00)\n",
      "Iter 18260 | Time 34.7740(35.4775) | Bit/dim 3.7241(3.8120) | Xent 0.2166(0.3127) | Loss 9.7771(10.2911) | Error 0.0767(0.1071) Steps 1186(1181.86) | Grad Norm 35.7741(77.7374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 139.5132, Epoch Time 2090.1012(1966.3247), Bit/dim 3.7505(best: 3.5691), Xent 0.7722, Loss 4.1366, Error 0.2173(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 32.5994(35.1368) | Bit/dim 3.7432(3.7916) | Xent 0.2489(0.2937) | Loss 9.7506(10.9751) | Error 0.0811(0.1010) Steps 1144(1173.69) | Grad Norm 88.7149(65.1154) | Total Time 0.00(0.00)\n",
      "Iter 18280 | Time 34.0077(34.8201) | Bit/dim 3.6924(3.7713) | Xent 0.1977(0.2734) | Loss 9.6226(10.6428) | Error 0.0667(0.0948) Steps 1156(1170.42) | Grad Norm 18.8793(57.9553) | Total Time 0.00(0.00)\n",
      "Iter 18290 | Time 33.5737(34.5261) | Bit/dim 3.7231(3.7555) | Xent 0.2206(0.2554) | Loss 9.7005(10.3935) | Error 0.0822(0.0889) Steps 1102(1161.79) | Grad Norm 25.6257(49.3334) | Total Time 0.00(0.00)\n",
      "Iter 18300 | Time 33.2894(34.2786) | Bit/dim 3.7057(3.7382) | Xent 0.1799(0.2380) | Loss 9.6383(10.1890) | Error 0.0667(0.0837) Steps 1108(1153.15) | Grad Norm 31.9319(44.2588) | Total Time 0.00(0.00)\n",
      "Iter 18310 | Time 31.6489(34.0277) | Bit/dim 3.6804(3.7242) | Xent 0.1633(0.2274) | Loss 9.4946(10.0434) | Error 0.0633(0.0790) Steps 1144(1145.97) | Grad Norm 10.7999(47.2498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 135.9010, Epoch Time 2008.7792(1967.5984), Bit/dim 3.6990(best: 3.5691), Xent 0.7983, Loss 4.0981, Error 0.2109(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 34.2616(33.9412) | Bit/dim 3.6904(3.7130) | Xent 0.1678(0.2182) | Loss 9.6395(10.8458) | Error 0.0611(0.0764) Steps 1144(1139.73) | Grad Norm 18.8135(48.6086) | Total Time 0.00(0.00)\n",
      "Iter 18330 | Time 33.8077(33.8062) | Bit/dim 3.6599(3.7082) | Xent 0.2322(0.2103) | Loss 9.6101(10.5263) | Error 0.0700(0.0722) Steps 1120(1138.45) | Grad Norm 18.4695(42.3554) | Total Time 0.00(0.00)\n",
      "Iter 18340 | Time 32.7418(33.5509) | Bit/dim 3.6404(3.6984) | Xent 0.1920(0.2054) | Loss 9.5543(10.2811) | Error 0.0678(0.0706) Steps 1054(1131.99) | Grad Norm 99.0868(57.6958) | Total Time 0.00(0.00)\n",
      "Iter 18350 | Time 29.5408(32.7614) | Bit/dim 3.6410(3.6926) | Xent 0.1918(0.2040) | Loss 9.4984(10.0971) | Error 0.0689(0.0700) Steps 1054(1119.97) | Grad Norm 32.8118(97.6502) | Total Time 0.00(0.00)\n",
      "Iter 18360 | Time 31.0432(32.2353) | Bit/dim 3.6417(3.6827) | Xent 0.1728(0.1971) | Loss 9.5311(9.9535) | Error 0.0567(0.0676) Steps 1126(1108.81) | Grad Norm 6.7061(74.6834) | Total Time 0.00(0.00)\n",
      "Iter 18370 | Time 30.2178(31.8024) | Bit/dim 3.6497(3.6736) | Xent 0.1627(0.1901) | Loss 9.4076(9.8345) | Error 0.0478(0.0653) Steps 1084(1097.83) | Grad Norm 65.3001(59.2665) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 127.1038, Epoch Time 1897.1844(1965.4860), Bit/dim 3.6520(best: 3.5691), Xent 0.7849, Loss 4.0445, Error 0.2077(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18380 | Time 30.8232(31.3384) | Bit/dim 3.6089(3.6650) | Xent 0.1477(0.1813) | Loss 9.4711(10.5623) | Error 0.0500(0.0620) Steps 1072(1090.05) | Grad Norm 5.2744(45.4476) | Total Time 0.00(0.00)\n",
      "Iter 18390 | Time 29.8789(30.8595) | Bit/dim 3.6040(3.6544) | Xent 0.1605(0.1757) | Loss 9.3353(10.2670) | Error 0.0600(0.0601) Steps 1036(1078.32) | Grad Norm 332.4572(67.6046) | Total Time 0.00(0.00)\n",
      "Iter 18400 | Time 28.3253(30.3643) | Bit/dim 3.6464(3.6489) | Xent 0.1695(0.1706) | Loss 9.3997(10.0589) | Error 0.0589(0.0583) Steps 1024(1070.74) | Grad Norm 17.2305(57.6373) | Total Time 0.00(0.00)\n",
      "Iter 18410 | Time 29.6157(29.9568) | Bit/dim 3.6140(3.6471) | Xent 0.1686(0.1702) | Loss 9.4874(9.9072) | Error 0.0589(0.0587) Steps 1006(1059.45) | Grad Norm 6.7673(47.3499) | Total Time 0.00(0.00)\n",
      "Iter 18420 | Time 28.1069(29.7351) | Bit/dim 3.6676(3.6452) | Xent 0.1432(0.1655) | Loss 9.5546(9.7963) | Error 0.0433(0.0566) Steps 1024(1054.48) | Grad Norm 16.0331(111.7432) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 118.4030, Epoch Time 1738.5141(1958.6768), Bit/dim 3.6694(best: 3.5691), Xent 0.8212, Loss 4.0800, Error 0.2140(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18430 | Time 26.6459(29.0905) | Bit/dim 3.6570(3.6507) | Xent 0.1625(0.1721) | Loss 9.5106(10.6495) | Error 0.0544(0.0585) Steps 994(1041.45) | Grad Norm 20.8533(86.1242) | Total Time 0.00(0.00)\n",
      "Iter 18440 | Time 26.4436(28.5172) | Bit/dim 3.6518(3.6484) | Xent 0.1716(0.1739) | Loss 9.3746(10.3383) | Error 0.0511(0.0588) Steps 1012(1030.74) | Grad Norm 6.9894(66.7294) | Total Time 0.00(0.00)\n",
      "Iter 18450 | Time 25.6576(27.9857) | Bit/dim 3.6456(3.6458) | Xent 0.1555(0.1718) | Loss 9.3387(10.1014) | Error 0.0533(0.0577) Steps 976(1020.84) | Grad Norm 211.6409(64.1660) | Total Time 0.00(0.00)\n",
      "Iter 18460 | Time 28.6591(27.9308) | Bit/dim 3.6682(3.6464) | Xent 0.1949(0.1707) | Loss 9.4820(9.9457) | Error 0.0656(0.0572) Steps 1024(1017.46) | Grad Norm 317.6558(98.6588) | Total Time 0.00(0.00)\n",
      "Iter 18470 | Time 33.5078(28.6652) | Bit/dim 3.6633(3.6521) | Xent 0.2401(0.1801) | Loss 9.7470(9.8577) | Error 0.0756(0.0610) Steps 1168(1035.53) | Grad Norm 29.3123(116.4401) | Total Time 0.00(0.00)\n",
      "Iter 18480 | Time 34.1790(29.8387) | Bit/dim 3.6856(3.6644) | Xent 0.2522(0.2075) | Loss 9.7305(9.8438) | Error 0.0856(0.0697) Steps 1156(1066.05) | Grad Norm 22.1860(92.8430) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 136.2215, Epoch Time 1739.0278(1952.0873), Bit/dim 3.7136(best: 3.5691), Xent 0.8460, Loss 4.1366, Error 0.2240(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18490 | Time 33.5565(30.8554) | Bit/dim 3.6862(3.6726) | Xent 0.2171(0.2214) | Loss 9.7497(10.6686) | Error 0.0600(0.0736) Steps 1120(1092.16) | Grad Norm 11.6441(74.0025) | Total Time 0.00(0.00)\n",
      "Iter 18500 | Time 32.9795(31.5152) | Bit/dim 3.6892(3.6746) | Xent 0.1789(0.2169) | Loss 9.8227(10.4100) | Error 0.0622(0.0727) Steps 1180(1106.26) | Grad Norm 9.4474(67.9973) | Total Time 0.00(0.00)\n",
      "Iter 18510 | Time 32.5464(31.8952) | Bit/dim 3.6581(3.6728) | Xent 0.1932(0.2108) | Loss 9.5304(10.1962) | Error 0.0633(0.0708) Steps 1138(1115.61) | Grad Norm 9.9916(60.5592) | Total Time 0.00(0.00)\n",
      "Iter 18520 | Time 32.1517(32.1985) | Bit/dim 3.6777(3.6720) | Xent 0.2135(0.2047) | Loss 9.6948(10.0600) | Error 0.0767(0.0687) Steps 1162(1125.69) | Grad Norm 29.3680(52.0025) | Total Time 0.00(0.00)\n",
      "Iter 18530 | Time 31.4865(32.2937) | Bit/dim 3.6845(3.6703) | Xent 0.1952(0.2018) | Loss 9.7283(9.9443) | Error 0.0689(0.0680) Steps 1138(1126.81) | Grad Norm 597.5444(94.4067) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 140.6188, Epoch Time 1987.0391(1953.1359), Bit/dim 3.7767(best: 3.5691), Xent 0.8871, Loss 4.2202, Error 0.2359(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18540 | Time 37.2278(33.0900) | Bit/dim 3.7994(3.6912) | Xent 0.3328(0.2286) | Loss 10.1129(10.9213) | Error 0.1122(0.0768) Steps 1210(1142.11) | Grad Norm 43.1378(93.3089) | Total Time 0.00(0.00)\n",
      "Iter 18550 | Time 37.2017(33.9570) | Bit/dim 3.8023(3.7251) | Xent 0.4086(0.2621) | Loss 10.2255(10.7186) | Error 0.1411(0.0883) Steps 1204(1161.30) | Grad Norm 40.8853(83.0269) | Total Time 0.00(0.00)\n",
      "Iter 18560 | Time 37.4609(34.8273) | Bit/dim 3.7818(3.7518) | Xent 0.3210(0.2837) | Loss 9.9324(10.5656) | Error 0.1089(0.0955) Steps 1162(1180.73) | Grad Norm 25.0912(68.1474) | Total Time 0.00(0.00)\n",
      "Iter 18570 | Time 35.7588(35.4436) | Bit/dim 3.7795(3.7597) | Xent 0.3421(0.2871) | Loss 10.0369(10.4222) | Error 0.1111(0.0969) Steps 1204(1191.47) | Grad Norm 18.2118(55.9940) | Total Time 0.00(0.00)\n",
      "Iter 18580 | Time 35.7436(35.7392) | Bit/dim 3.7398(3.7549) | Xent 0.2445(0.2816) | Loss 9.7221(10.2716) | Error 0.0878(0.0955) Steps 1192(1194.51) | Grad Norm 33.4040(48.1027) | Total Time 0.00(0.00)\n",
      "Iter 18590 | Time 34.2005(35.5526) | Bit/dim 3.7860(3.7505) | Xent 0.3306(0.2747) | Loss 10.0759(10.1745) | Error 0.1044(0.0930) Steps 1186(1190.42) | Grad Norm 168.6743(68.7811) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 141.0097, Epoch Time 2165.2042(1959.4979), Bit/dim 3.7395(best: 3.5691), Xent 0.8378, Loss 4.1584, Error 0.2182(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18600 | Time 33.6284(35.6126) | Bit/dim 3.7294(3.7451) | Xent 0.1974(0.2654) | Loss 9.8142(10.9351) | Error 0.0711(0.0900) Steps 1180(1189.31) | Grad Norm 32.5147(64.1679) | Total Time 0.00(0.00)\n",
      "Iter 18610 | Time 36.3930(35.5762) | Bit/dim 3.6866(3.7381) | Xent 0.1988(0.2535) | Loss 9.8270(10.6486) | Error 0.0544(0.0855) Steps 1234(1192.01) | Grad Norm 18.8219(63.5011) | Total Time 0.00(0.00)\n",
      "Iter 18620 | Time 34.8141(35.5288) | Bit/dim 3.6868(3.7273) | Xent 0.1687(0.2396) | Loss 9.6486(10.3965) | Error 0.0633(0.0805) Steps 1204(1188.11) | Grad Norm 11.7804(51.3179) | Total Time 0.00(0.00)\n",
      "Iter 18630 | Time 35.5039(35.6528) | Bit/dim 3.6553(3.7176) | Xent 0.1821(0.2256) | Loss 9.6998(10.2190) | Error 0.0667(0.0763) Steps 1216(1191.42) | Grad Norm 13.8579(43.4007) | Total Time 0.00(0.00)\n",
      "Iter 18640 | Time 32.8698(35.4715) | Bit/dim 3.6640(3.7055) | Xent 0.1823(0.2118) | Loss 9.5587(10.0524) | Error 0.0756(0.0723) Steps 1132(1189.17) | Grad Norm 29.3984(41.2719) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 138.0551, Epoch Time 2103.3450(1963.8134), Bit/dim 3.6782(best: 3.5691), Xent 0.8300, Loss 4.0932, Error 0.2155(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18650 | Time 34.4441(35.1450) | Bit/dim 3.6622(3.6961) | Xent 0.1701(0.2024) | Loss 9.5617(10.9247) | Error 0.0556(0.0689) Steps 1156(1179.66) | Grad Norm 31.7864(38.6083) | Total Time 0.00(0.00)\n",
      "Iter 18660 | Time 34.5983(34.8287) | Bit/dim 3.6580(3.6856) | Xent 0.1893(0.1943) | Loss 9.7342(10.5697) | Error 0.0611(0.0665) Steps 1156(1171.98) | Grad Norm 16.9750(51.1416) | Total Time 0.00(0.00)\n",
      "Iter 18670 | Time 32.1093(34.1921) | Bit/dim 3.6402(3.6790) | Xent 0.1599(0.1911) | Loss 9.4880(10.3098) | Error 0.0567(0.0657) Steps 1108(1156.92) | Grad Norm 8.9241(88.1380) | Total Time 0.00(0.00)\n",
      "Iter 18680 | Time 31.6571(33.4221) | Bit/dim 3.6123(3.6692) | Xent 0.1535(0.1848) | Loss 9.5154(10.1026) | Error 0.0644(0.0635) Steps 1120(1138.50) | Grad Norm 16.0775(82.8907) | Total Time 0.00(0.00)\n",
      "Iter 18690 | Time 30.4654(32.6900) | Bit/dim 3.6426(3.6599) | Xent 0.1492(0.1741) | Loss 9.4579(9.9427) | Error 0.0511(0.0597) Steps 1066(1124.09) | Grad Norm 4.1437(62.6111) | Total Time 0.00(0.00)\n",
      "Iter 18700 | Time 29.5633(31.9355) | Bit/dim 3.6067(3.6540) | Xent 0.1336(0.1684) | Loss 9.5005(9.8171) | Error 0.0533(0.0576) Steps 1024(1109.54) | Grad Norm 5.8697(57.1824) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 124.1566, Epoch Time 1896.3581(1961.7897), Bit/dim 3.6359(best: 3.5691), Xent 0.8323, Loss 4.0520, Error 0.2102(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18710 | Time 28.5387(31.2244) | Bit/dim 3.6133(3.6456) | Xent 0.1377(0.1631) | Loss 9.4274(10.5093) | Error 0.0444(0.0557) Steps 1084(1097.02) | Grad Norm 721.2817(90.2394) | Total Time 0.00(0.00)\n",
      "Iter 18720 | Time 26.6229(30.4609) | Bit/dim 3.6844(3.6476) | Xent 0.1442(0.1622) | Loss 9.5633(10.2466) | Error 0.0567(0.0560) Steps 1006(1080.04) | Grad Norm 16.9702(162.6247) | Total Time 0.00(0.00)\n",
      "Iter 18730 | Time 25.4814(29.3885) | Bit/dim 3.6695(3.6543) | Xent 0.1767(0.1664) | Loss 9.4367(10.0646) | Error 0.0578(0.0575) Steps 1000(1055.01) | Grad Norm 6.1085(122.1932) | Total Time 0.00(0.00)\n",
      "Iter 18740 | Time 25.9696(28.4846) | Bit/dim 3.6639(3.6538) | Xent 0.1649(0.1686) | Loss 9.5688(9.9191) | Error 0.0467(0.0571) Steps 994(1034.35) | Grad Norm 5.0158(91.8314) | Total Time 0.00(0.00)\n",
      "Iter 18750 | Time 26.4028(27.7655) | Bit/dim 3.6507(3.6546) | Xent 0.1975(0.1691) | Loss 9.5268(9.8040) | Error 0.0744(0.0572) Steps 994(1017.63) | Grad Norm 7.7663(69.3689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 115.3994, Epoch Time 1620.6319(1951.5550), Bit/dim 3.6536(best: 3.5691), Xent 0.8236, Loss 4.0654, Error 0.2105(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18760 | Time 26.2140(27.3857) | Bit/dim 3.6774(3.6532) | Xent 0.1655(0.1679) | Loss 9.5955(10.6564) | Error 0.0567(0.0573) Steps 982(1007.36) | Grad Norm 5.1035(52.7530) | Total Time 0.00(0.00)\n",
      "Iter 18770 | Time 26.5915(27.1198) | Bit/dim 3.6803(3.6498) | Xent 0.1160(0.1637) | Loss 9.4582(10.3373) | Error 0.0322(0.0557) Steps 1006(1003.85) | Grad Norm 4.0364(40.6088) | Total Time 0.00(0.00)\n",
      "Iter 18780 | Time 26.4060(26.7925) | Bit/dim 3.6420(3.6453) | Xent 0.1740(0.1634) | Loss 9.5138(10.1021) | Error 0.0544(0.0558) Steps 964(996.34) | Grad Norm 4.9899(32.2724) | Total Time 0.00(0.00)\n",
      "Iter 18790 | Time 26.8646(26.5851) | Bit/dim 3.6268(3.6422) | Xent 0.1688(0.1609) | Loss 9.6010(9.9446) | Error 0.0567(0.0552) Steps 1006(990.91) | Grad Norm 6.1691(28.3474) | Total Time 0.00(0.00)\n",
      "Iter 18800 | Time 24.5082(26.4103) | Bit/dim 3.6223(3.6384) | Xent 0.1754(0.1573) | Loss 9.3508(9.7970) | Error 0.0656(0.0539) Steps 970(985.11) | Grad Norm 13.5409(24.2127) | Total Time 0.00(0.00)\n",
      "Iter 18810 | Time 25.5476(26.2546) | Bit/dim 3.6143(3.6347) | Xent 0.1236(0.1540) | Loss 9.3592(9.6947) | Error 0.0422(0.0523) Steps 952(979.86) | Grad Norm 29.4799(24.0127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 113.7671, Epoch Time 1566.0850(1939.9909), Bit/dim 3.6401(best: 3.5691), Xent 0.8480, Loss 4.0641, Error 0.2115(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18820 | Time 26.1624(26.1362) | Bit/dim 3.6429(3.6319) | Xent 0.1259(0.1498) | Loss 9.4095(10.4007) | Error 0.0456(0.0511) Steps 976(977.82) | Grad Norm 5.5323(21.1134) | Total Time 0.00(0.00)\n",
      "Iter 18830 | Time 26.2683(26.1597) | Bit/dim 3.6425(3.6289) | Xent 0.1535(0.1486) | Loss 9.2861(10.1266) | Error 0.0456(0.0508) Steps 910(974.23) | Grad Norm 6.9327(25.3205) | Total Time 0.00(0.00)\n",
      "Iter 18840 | Time 25.5926(26.1730) | Bit/dim 3.6335(3.6272) | Xent 0.1426(0.1473) | Loss 9.4372(9.9424) | Error 0.0511(0.0503) Steps 970(976.75) | Grad Norm 223.7536(30.4751) | Total Time 0.00(0.00)\n",
      "Iter 18850 | Time 26.8609(26.2068) | Bit/dim 3.5964(3.6245) | Xent 0.1549(0.1475) | Loss 9.4172(9.8121) | Error 0.0544(0.0503) Steps 946(978.27) | Grad Norm 19.9199(32.2625) | Total Time 0.00(0.00)\n",
      "Iter 18860 | Time 25.4227(26.3491) | Bit/dim 3.6687(3.6241) | Xent 0.1528(0.1469) | Loss 9.4582(9.7118) | Error 0.0500(0.0499) Steps 976(977.32) | Grad Norm 26.7249(39.4430) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 116.6528, Epoch Time 1578.4825(1929.1456), Bit/dim 3.6327(best: 3.5691), Xent 0.8388, Loss 4.0522, Error 0.2098(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18870 | Time 25.7308(26.3111) | Bit/dim 3.6414(3.6248) | Xent 0.1604(0.1484) | Loss 9.4608(10.5665) | Error 0.0444(0.0499) Steps 982(980.58) | Grad Norm 144.8425(49.6070) | Total Time 0.00(0.00)\n",
      "Iter 18880 | Time 26.7624(26.3168) | Bit/dim 3.6531(3.6259) | Xent 0.1756(0.1470) | Loss 9.5417(10.2655) | Error 0.0544(0.0495) Steps 976(985.45) | Grad Norm 48.6242(90.3049) | Total Time 0.00(0.00)\n",
      "Iter 18890 | Time 31.1977(26.9622) | Bit/dim 3.6493(3.6283) | Xent 0.1626(0.1503) | Loss 9.5720(10.0645) | Error 0.0522(0.0505) Steps 1102(1000.45) | Grad Norm 15.9187(182.3635) | Total Time 0.00(0.00)\n",
      "Iter 18900 | Time 34.7327(28.4181) | Bit/dim 3.8928(3.6638) | Xent 0.4892(0.1956) | Loss 10.4769(10.0314) | Error 0.1556(0.0647) Steps 1204(1036.58) | Grad Norm 86.6422(209.0687) | Total Time 0.00(0.00)\n",
      "Iter 18910 | Time 37.5960(30.5371) | Bit/dim 4.0500(3.7385) | Xent 0.6764(0.2841) | Loss 10.9066(10.1738) | Error 0.2278(0.0934) Steps 1276(1088.60) | Grad Norm 62.9635(363.1340) | Total Time 0.00(0.00)\n",
      "Iter 18920 | Time 43.5535(33.3956) | Bit/dim 5.3795(4.0645) | Xent 1.9322(0.6494) | Loss 15.6449(11.2365) | Error 0.5456(0.1872) Steps 1486(1168.66) | Grad Norm 268.1100(322.0045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 173.8304, Epoch Time 1974.4763(1930.5055), Bit/dim 5.4131(best: 3.5691), Xent 1.7271, Loss 6.2766, Error 0.5190(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18930 | Time 45.6910(36.0835) | Bit/dim 5.1472(4.3706) | Xent 1.5394(0.9239) | Loss 14.4208(13.1713) | Error 0.4800(0.2747) Steps 1492(1239.75) | Grad Norm 185.8922(312.8817) | Total Time 0.00(0.00)\n",
      "Iter 18940 | Time 42.2729(37.8293) | Bit/dim 4.6972(4.4998) | Xent 1.0810(1.0003) | Loss 12.7652(13.2189) | Error 0.3689(0.3077) Steps 1330(1277.49) | Grad Norm 68.0342(262.0867) | Total Time 0.00(0.00)\n",
      "Iter 18950 | Time 40.3216(38.7972) | Bit/dim 4.4792(4.5171) | Xent 0.9083(1.0066) | Loss 12.1038(13.0308) | Error 0.3056(0.3176) Steps 1276(1294.98) | Grad Norm 62.7116(215.3394) | Total Time 0.00(0.00)\n",
      "Iter 18960 | Time 41.9835(39.3878) | Bit/dim 4.3499(4.4864) | Xent 0.8867(0.9752) | Loss 11.8826(12.7433) | Error 0.3189(0.3138) Steps 1354(1300.16) | Grad Norm 37.7689(172.7697) | Total Time 0.00(0.00)\n",
      "Iter 18970 | Time 40.3196(39.7649) | Bit/dim 4.2894(4.4358) | Xent 0.8024(0.9299) | Loss 11.5375(12.4493) | Error 0.2722(0.3042) Steps 1240(1296.30) | Grad Norm 54.5821(139.9839) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 150.0855, Epoch Time 2466.7816(1946.5938), Bit/dim 4.2191(best: 3.5691), Xent 0.8754, Loss 4.6568, Error 0.2878(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18980 | Time 40.0021(39.8190) | Bit/dim 4.2119(4.3786) | Xent 0.7022(0.8825) | Loss 11.1398(13.1540) | Error 0.2422(0.2922) Steps 1270(1291.67) | Grad Norm 74.5636(117.1556) | Total Time 0.00(0.00)\n",
      "Iter 18990 | Time 38.6554(39.7508) | Bit/dim 4.1212(4.3173) | Xent 0.6990(0.8339) | Loss 10.9794(12.6332) | Error 0.2467(0.2783) Steps 1246(1288.71) | Grad Norm 32.8804(97.1616) | Total Time 0.00(0.00)\n",
      "Iter 19000 | Time 39.7304(39.7178) | Bit/dim 4.0752(4.2613) | Xent 0.6000(0.7842) | Loss 10.8225(12.2079) | Error 0.2033(0.2650) Steps 1288(1283.29) | Grad Norm 72.4944(83.0490) | Total Time 0.00(0.00)\n",
      "Iter 19010 | Time 37.1751(39.5354) | Bit/dim 4.0276(4.2058) | Xent 0.5734(0.7414) | Loss 10.7770(11.8595) | Error 0.2022(0.2512) Steps 1204(1270.35) | Grad Norm 57.1352(73.3926) | Total Time 0.00(0.00)\n",
      "Iter 19020 | Time 39.1456(39.1195) | Bit/dim 3.9673(4.1562) | Xent 0.5386(0.7043) | Loss 10.4911(11.5647) | Error 0.1989(0.2398) Steps 1174(1258.55) | Grad Norm 99.4863(76.4408) | Total Time 0.00(0.00)\n",
      "Iter 19030 | Time 36.2422(38.6199) | Bit/dim 3.9702(4.1110) | Xent 0.5607(0.6617) | Loss 10.6727(11.3168) | Error 0.1900(0.2259) Steps 1204(1244.91) | Grad Norm 219.6338(88.7293) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 140.3427, Epoch Time 2293.6306(1957.0049), Bit/dim 3.9852(best: 3.5691), Xent 0.7656, Loss 4.3680, Error 0.2470(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19040 | Time 35.9526(38.1518) | Bit/dim 3.9640(4.0705) | Xent 0.5041(0.6243) | Loss 10.5003(11.9559) | Error 0.1811(0.2140) Steps 1180(1237.96) | Grad Norm 62.4925(94.6381) | Total Time 0.00(0.00)\n",
      "Iter 19050 | Time 35.5531(37.7765) | Bit/dim 3.9484(4.0368) | Xent 0.5086(0.5893) | Loss 10.5034(11.5718) | Error 0.1767(0.2028) Steps 1168(1227.22) | Grad Norm 62.9490(87.4548) | Total Time 0.00(0.00)\n",
      "Iter 19060 | Time 38.0197(37.5907) | Bit/dim 3.9301(4.0093) | Xent 0.4813(0.5578) | Loss 10.4150(11.2790) | Error 0.1656(0.1926) Steps 1264(1222.24) | Grad Norm 172.3928(95.5364) | Total Time 0.00(0.00)\n",
      "Iter 19070 | Time 37.3731(37.2564) | Bit/dim 3.9153(3.9829) | Xent 0.4338(0.5274) | Loss 10.4364(11.0378) | Error 0.1522(0.1827) Steps 1138(1213.86) | Grad Norm 77.7541(95.9698) | Total Time 0.00(0.00)\n",
      "Iter 19080 | Time 34.9032(36.8285) | Bit/dim 3.8555(3.9556) | Xent 0.4046(0.5008) | Loss 10.2943(10.8326) | Error 0.1422(0.1738) Steps 1192(1210.07) | Grad Norm 94.5550(92.4193) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 136.8613, Epoch Time 2155.1628(1962.9497), Bit/dim 3.8814(best: 3.5691), Xent 0.7447, Loss 4.2537, Error 0.2318(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19090 | Time 34.9175(36.4190) | Bit/dim 3.8636(3.9342) | Xent 0.3411(0.4702) | Loss 10.2399(11.6073) | Error 0.1189(0.1639) Steps 1192(1199.06) | Grad Norm 84.3676(94.3951) | Total Time 0.00(0.00)\n",
      "Iter 19100 | Time 34.1549(36.1343) | Bit/dim 3.8499(3.9128) | Xent 0.3674(0.4462) | Loss 10.1111(11.2236) | Error 0.1322(0.1549) Steps 1150(1193.09) | Grad Norm 44.6371(101.5659) | Total Time 0.00(0.00)\n",
      "Iter 19110 | Time 35.3564(35.9700) | Bit/dim 3.8424(3.8935) | Xent 0.2961(0.4235) | Loss 10.0128(10.9293) | Error 0.1078(0.1477) Steps 1192(1191.97) | Grad Norm 46.2010(105.3528) | Total Time 0.00(0.00)\n",
      "Iter 19120 | Time 33.9131(35.7878) | Bit/dim 3.8817(3.8793) | Xent 0.5854(0.4072) | Loss 10.5274(10.7262) | Error 0.1867(0.1419) Steps 1156(1188.74) | Grad Norm 507.9273(124.1284) | Total Time 0.00(0.00)\n",
      "Iter 19130 | Time 35.6042(35.8529) | Bit/dim 3.8748(3.8765) | Xent 0.4040(0.4135) | Loss 10.2809(10.6092) | Error 0.1433(0.1434) Steps 1162(1188.86) | Grad Norm 566.1683(124.2710) | Total Time 0.00(0.00)\n",
      "Iter 19140 | Time 35.2390(35.9536) | Bit/dim 3.8398(3.8725) | Xent 0.3496(0.4046) | Loss 10.1851(10.5002) | Error 0.1233(0.1405) Steps 1168(1192.01) | Grad Norm 27.6975(101.5573) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 138.9587, Epoch Time 2119.7122(1967.6525), Bit/dim 3.8416(best: 3.5691), Xent 0.7281, Loss 4.2057, Error 0.2239(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19150 | Time 36.3039(35.7100) | Bit/dim 3.7752(3.8576) | Xent 0.3387(0.3843) | Loss 9.9350(11.1888) | Error 0.1200(0.1339) Steps 1174(1187.72) | Grad Norm 175.9457(88.0764) | Total Time 0.00(0.00)\n",
      "Iter 19160 | Time 37.2837(35.6074) | Bit/dim 3.8459(3.8490) | Xent 0.4266(0.3835) | Loss 10.1628(10.9077) | Error 0.1600(0.1331) Steps 1240(1184.58) | Grad Norm 30.2793(119.7378) | Total Time 0.00(0.00)\n",
      "Iter 19170 | Time 34.2750(35.5897) | Bit/dim 3.8812(3.8482) | Xent 0.3386(0.3777) | Loss 10.0885(10.7124) | Error 0.1178(0.1321) Steps 1120(1182.53) | Grad Norm 28.9340(95.7383) | Total Time 0.00(0.00)\n",
      "Iter 19180 | Time 32.7309(35.4559) | Bit/dim 3.7980(3.8402) | Xent 0.2490(0.3575) | Loss 9.8297(10.5260) | Error 0.0844(0.1247) Steps 1156(1182.53) | Grad Norm 84.6951(79.5567) | Total Time 0.00(0.00)\n",
      "Iter 19190 | Time 34.5218(35.2641) | Bit/dim 3.8112(3.8283) | Xent 0.3006(0.3402) | Loss 9.9502(10.3797) | Error 0.1089(0.1183) Steps 1126(1177.33) | Grad Norm 324.0192(78.1453) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 138.1448, Epoch Time 2085.9314(1971.2009), Bit/dim 3.7902(best: 3.5691), Xent 0.7093, Loss 4.1448, Error 0.2108(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19200 | Time 35.2982(35.2334) | Bit/dim 3.7903(3.8155) | Xent 0.2496(0.3256) | Loss 9.9561(11.2200) | Error 0.0856(0.1127) Steps 1138(1175.46) | Grad Norm 19.7855(75.5353) | Total Time 0.00(0.00)\n",
      "Iter 19210 | Time 33.0382(35.0266) | Bit/dim 3.7719(3.8016) | Xent 0.2687(0.3086) | Loss 9.7906(10.8633) | Error 0.1011(0.1068) Steps 1162(1172.25) | Grad Norm 279.7338(75.1552) | Total Time 0.00(0.00)\n",
      "Iter 19220 | Time 33.7390(34.6717) | Bit/dim 3.7357(3.7872) | Xent 0.2288(0.2898) | Loss 9.8838(10.5874) | Error 0.0844(0.1013) Steps 1150(1168.78) | Grad Norm 101.8312(85.6654) | Total Time 0.00(0.00)\n",
      "Iter 19230 | Time 34.2949(34.5628) | Bit/dim 3.7733(3.7754) | Xent 0.2209(0.2775) | Loss 9.8973(10.3929) | Error 0.0722(0.0969) Steps 1144(1167.71) | Grad Norm 25.4037(85.9235) | Total Time 0.00(0.00)\n",
      "Iter 19240 | Time 34.9525(34.5732) | Bit/dim 3.7177(3.7632) | Xent 0.2229(0.2640) | Loss 9.7216(10.2268) | Error 0.0722(0.0922) Steps 1144(1164.88) | Grad Norm 211.0519(75.7972) | Total Time 0.00(0.00)\n",
      "Iter 19250 | Time 33.6767(34.4032) | Bit/dim 3.7257(3.7553) | Xent 0.2159(0.2538) | Loss 9.7257(10.1157) | Error 0.0711(0.0879) Steps 1156(1163.10) | Grad Norm 36.1284(63.9076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 137.1559, Epoch Time 2042.8854(1973.3514), Bit/dim 3.7225(best: 3.5691), Xent 0.7260, Loss 4.0855, Error 0.2125(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19260 | Time 34.0428(34.3942) | Bit/dim 3.7402(3.7431) | Xent 0.2571(0.2428) | Loss 9.7681(10.8293) | Error 0.1011(0.0844) Steps 1180(1161.96) | Grad Norm 49.5209(77.7010) | Total Time 0.00(0.00)\n",
      "Iter 19270 | Time 34.2032(34.4631) | Bit/dim 3.7111(3.7383) | Xent 0.2285(0.2368) | Loss 9.7835(10.5525) | Error 0.0800(0.0822) Steps 1192(1167.00) | Grad Norm 20.0197(105.5791) | Total Time 0.00(0.00)\n",
      "Iter 19280 | Time 32.3665(34.3729) | Bit/dim 3.7448(3.7331) | Xent 0.1726(0.2278) | Loss 9.6912(10.3331) | Error 0.0600(0.0787) Steps 1156(1164.05) | Grad Norm 45.6977(82.4394) | Total Time 0.00(0.00)\n",
      "Iter 19290 | Time 32.7785(34.1095) | Bit/dim 3.6862(3.7244) | Xent 0.2517(0.2265) | Loss 9.7350(10.1664) | Error 0.0944(0.0793) Steps 1144(1162.14) | Grad Norm 25.9040(131.1377) | Total Time 0.00(0.00)\n",
      "Iter 19300 | Time 31.0395(33.4128) | Bit/dim 3.7125(3.7184) | Xent 0.2123(0.2308) | Loss 9.7961(10.0562) | Error 0.0733(0.0810) Steps 1096(1146.39) | Grad Norm 10.2952(100.6272) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 124.9189, Epoch Time 1977.2266(1973.4677), Bit/dim 3.7169(best: 3.5691), Xent 0.7648, Loss 4.0993, Error 0.2188(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19310 | Time 28.4014(32.4805) | Bit/dim 3.7306(3.7175) | Xent 0.2452(0.2301) | Loss 9.6748(10.8598) | Error 0.0967(0.0809) Steps 1006(1122.72) | Grad Norm 9.8997(77.0038) | Total Time 0.00(0.00)\n",
      "Iter 19320 | Time 30.8387(31.7844) | Bit/dim 3.6914(3.7083) | Xent 0.2111(0.2247) | Loss 9.7683(10.5371) | Error 0.0733(0.0784) Steps 1072(1108.02) | Grad Norm 11.6877(59.1753) | Total Time 0.00(0.00)\n",
      "Iter 19330 | Time 30.8864(31.2102) | Bit/dim 3.6620(3.7024) | Xent 0.1750(0.2149) | Loss 9.6642(10.2950) | Error 0.0600(0.0746) Steps 1132(1096.53) | Grad Norm 5.8369(45.6122) | Total Time 0.00(0.00)\n",
      "Iter 19340 | Time 30.4413(30.8642) | Bit/dim 3.6632(3.6927) | Xent 0.1852(0.2060) | Loss 9.6105(10.0964) | Error 0.0600(0.0713) Steps 1066(1088.55) | Grad Norm 6.7096(35.6050) | Total Time 0.00(0.00)\n",
      "Iter 19350 | Time 28.9512(30.6732) | Bit/dim 3.6797(3.6856) | Xent 0.2295(0.2023) | Loss 9.6082(9.9573) | Error 0.0800(0.0700) Steps 1024(1083.56) | Grad Norm 7.8062(28.1568) | Total Time 0.00(0.00)\n",
      "Iter 19360 | Time 31.0274(30.4314) | Bit/dim 3.6828(3.6818) | Xent 0.1687(0.1957) | Loss 9.6652(9.8514) | Error 0.0556(0.0673) Steps 1114(1078.61) | Grad Norm 9.0892(22.2307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 126.5683, Epoch Time 1783.4944(1967.7685), Bit/dim 3.6684(best: 3.5691), Xent 0.7540, Loss 4.0454, Error 0.2083(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19370 | Time 30.0698(30.3019) | Bit/dim 3.6491(3.6755) | Xent 0.2021(0.1888) | Loss 9.5755(10.5574) | Error 0.0678(0.0650) Steps 1090(1075.03) | Grad Norm 42.4549(19.6586) | Total Time 0.00(0.00)\n",
      "Iter 19380 | Time 31.4926(30.4197) | Bit/dim 3.6652(3.6735) | Xent 0.1945(0.1851) | Loss 9.4093(10.2874) | Error 0.0667(0.0642) Steps 1072(1077.50) | Grad Norm 467.7277(63.7907) | Total Time 0.00(0.00)\n",
      "Iter 19390 | Time 32.0933(30.8806) | Bit/dim 3.6985(3.6793) | Xent 0.2205(0.1959) | Loss 9.7598(10.1312) | Error 0.0756(0.0676) Steps 1114(1087.58) | Grad Norm 22.2766(179.5573) | Total Time 0.00(0.00)\n",
      "Iter 19400 | Time 31.0448(30.9841) | Bit/dim 3.6902(3.6787) | Xent 0.1874(0.2023) | Loss 9.7134(10.0039) | Error 0.0589(0.0694) Steps 1120(1090.20) | Grad Norm 10.6038(139.1974) | Total Time 0.00(0.00)\n",
      "Iter 19410 | Time 31.2853(31.0981) | Bit/dim 3.6475(3.6727) | Xent 0.1725(0.2009) | Loss 9.5483(9.8956) | Error 0.0500(0.0687) Steps 1126(1095.42) | Grad Norm 5.6364(108.0641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 130.7456, Epoch Time 1865.4200(1964.6980), Bit/dim 3.6606(best: 3.5691), Xent 0.7542, Loss 4.0377, Error 0.2072(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19420 | Time 30.3899(31.2388) | Bit/dim 3.6749(3.6695) | Xent 0.1777(0.1937) | Loss 9.6098(10.7906) | Error 0.0644(0.0665) Steps 1054(1095.63) | Grad Norm 31.0925(82.9606) | Total Time 0.00(0.00)\n",
      "Iter 19430 | Time 31.0593(31.2513) | Bit/dim 3.6116(3.6662) | Xent 0.1604(0.1874) | Loss 9.3878(10.4629) | Error 0.0544(0.0645) Steps 1078(1088.41) | Grad Norm 5.4938(71.2384) | Total Time 0.00(0.00)\n",
      "Iter 19440 | Time 31.3327(31.1653) | Bit/dim 3.6257(3.6606) | Xent 0.1651(0.1804) | Loss 9.6264(10.2262) | Error 0.0556(0.0624) Steps 1084(1086.89) | Grad Norm 20.1181(66.0079) | Total Time 0.00(0.00)\n",
      "Iter 19450 | Time 30.7928(30.9599) | Bit/dim 3.6178(3.6558) | Xent 0.1684(0.1768) | Loss 9.4719(10.0344) | Error 0.0556(0.0609) Steps 1048(1082.76) | Grad Norm 14.5024(53.1560) | Total Time 0.00(0.00)\n",
      "Iter 19460 | Time 33.6526(31.0712) | Bit/dim 3.6591(3.6530) | Xent 0.1655(0.1722) | Loss 9.4436(9.8926) | Error 0.0578(0.0594) Steps 1138(1086.46) | Grad Norm 139.1359(106.7258) | Total Time 0.00(0.00)\n",
      "Iter 19470 | Time 31.7694(31.4667) | Bit/dim 3.6868(3.6584) | Xent 0.2382(0.1738) | Loss 9.7537(9.8335) | Error 0.0778(0.0595) Steps 1114(1099.37) | Grad Norm 122.7307(199.3515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 135.7500, Epoch Time 1874.3297(1961.9870), Bit/dim 3.7053(best: 3.5691), Xent 0.8467, Loss 4.1286, Error 0.2197(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19480 | Time 35.9382(31.9054) | Bit/dim 3.7134(3.6722) | Xent 0.2387(0.1907) | Loss 9.8736(10.6202) | Error 0.0856(0.0652) Steps 1192(1111.82) | Grad Norm 47.7627(263.4838) | Total Time 0.00(0.00)\n",
      "Iter 19490 | Time 32.8019(32.6202) | Bit/dim 3.7146(3.6864) | Xent 0.2116(0.2079) | Loss 9.7072(10.4152) | Error 0.0756(0.0712) Steps 1150(1129.70) | Grad Norm 206.1500(237.4529) | Total Time 0.00(0.00)\n",
      "Iter 19500 | Time 36.7228(33.5701) | Bit/dim 3.8310(3.7159) | Xent 0.3140(0.2422) | Loss 10.2457(10.3288) | Error 0.1067(0.0811) Steps 1252(1155.09) | Grad Norm 55.7805(232.4212) | Total Time 0.00(0.00)\n",
      "Iter 19510 | Time 38.5914(34.7735) | Bit/dim 3.8725(3.7578) | Xent 0.3620(0.2727) | Loss 10.3591(10.3165) | Error 0.1178(0.0911) Steps 1246(1184.12) | Grad Norm 25.5458(186.7480) | Total Time 0.00(0.00)\n",
      "Iter 19520 | Time 37.8955(35.7293) | Bit/dim 3.8223(3.7840) | Xent 0.3452(0.2880) | Loss 10.0025(10.2800) | Error 0.1211(0.0965) Steps 1282(1205.55) | Grad Norm 30.6491(145.6291) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 146.2585, Epoch Time 2156.1762(1967.8127), Bit/dim 3.8355(best: 3.5691), Xent 0.7897, Loss 4.2303, Error 0.2240(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19530 | Time 35.9116(36.1522) | Bit/dim 3.8461(3.7960) | Xent 0.2759(0.2854) | Loss 10.0211(11.2338) | Error 0.1044(0.0959) Steps 1180(1211.50) | Grad Norm 51.5836(117.5939) | Total Time 0.00(0.00)\n",
      "Iter 19540 | Time 37.3853(36.3112) | Bit/dim 3.8327(3.7960) | Xent 0.2861(0.2802) | Loss 10.0780(10.9054) | Error 0.1033(0.0947) Steps 1282(1216.24) | Grad Norm 186.6352(144.2955) | Total Time 0.00(0.00)\n",
      "Iter 19550 | Time 39.4886(37.0528) | Bit/dim 3.9094(3.8196) | Xent 0.3625(0.2976) | Loss 10.3153(10.7459) | Error 0.1267(0.1009) Steps 1252(1231.80) | Grad Norm 37.0981(133.6695) | Total Time 0.00(0.00)\n",
      "Iter 19560 | Time 39.1830(37.3530) | Bit/dim 3.8568(3.8327) | Xent 0.3245(0.3052) | Loss 10.1107(10.6132) | Error 0.1044(0.1027) Steps 1294(1241.10) | Grad Norm 32.9208(107.9721) | Total Time 0.00(0.00)\n",
      "Iter 19570 | Time 35.7190(37.6095) | Bit/dim 3.7838(3.8321) | Xent 0.2955(0.3027) | Loss 9.8802(10.4808) | Error 0.0989(0.1013) Steps 1204(1244.82) | Grad Norm 25.2893(90.2120) | Total Time 0.00(0.00)\n",
      "Iter 19580 | Time 37.7096(37.7048) | Bit/dim 3.7954(3.8251) | Xent 0.2704(0.2953) | Loss 10.0064(10.3604) | Error 0.0944(0.0987) Steps 1252(1244.12) | Grad Norm 31.2080(73.7533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 146.4717, Epoch Time 2254.4627(1976.4122), Bit/dim 3.8059(best: 3.5691), Xent 0.7646, Loss 4.1882, Error 0.2178(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19590 | Time 38.6239(37.8053) | Bit/dim 3.8100(3.8152) | Xent 0.2392(0.2842) | Loss 10.0058(11.1232) | Error 0.0756(0.0952) Steps 1204(1241.65) | Grad Norm 33.7803(62.2484) | Total Time 0.00(0.00)\n",
      "Iter 19600 | Time 38.0275(37.5284) | Bit/dim 3.7622(3.8034) | Xent 0.2540(0.2755) | Loss 9.9796(10.8054) | Error 0.0756(0.0924) Steps 1204(1230.17) | Grad Norm 15.9153(56.1251) | Total Time 0.00(0.00)\n",
      "Iter 19610 | Time 37.0749(37.2782) | Bit/dim 3.7370(3.7899) | Xent 0.2152(0.2568) | Loss 9.8580(10.5538) | Error 0.0778(0.0864) Steps 1192(1222.92) | Grad Norm 22.7552(50.1452) | Total Time 0.00(0.00)\n",
      "Iter 19620 | Time 35.8747(36.8742) | Bit/dim 3.7753(3.7778) | Xent 0.2146(0.2453) | Loss 9.8084(10.3511) | Error 0.0756(0.0833) Steps 1174(1217.99) | Grad Norm 143.7351(61.6472) | Total Time 0.00(0.00)\n",
      "Iter 19630 | Time 34.3705(36.3945) | Bit/dim 3.6936(3.7632) | Xent 0.1771(0.2364) | Loss 9.7232(10.1962) | Error 0.0533(0.0795) Steps 1198(1209.80) | Grad Norm 21.4311(82.3461) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 139.1676, Epoch Time 2155.2686(1981.7778), Bit/dim 3.7027(best: 3.5691), Xent 0.7692, Loss 4.0874, Error 0.2103(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19640 | Time 34.2504(36.0010) | Bit/dim 3.7156(3.7448) | Xent 0.1822(0.2253) | Loss 9.6504(11.0520) | Error 0.0622(0.0756) Steps 1168(1201.27) | Grad Norm 20.3144(65.5395) | Total Time 0.00(0.00)\n",
      "Iter 19650 | Time 35.5697(35.6602) | Bit/dim 3.6748(3.7299) | Xent 0.1661(0.2142) | Loss 9.6567(10.6835) | Error 0.0578(0.0720) Steps 1222(1193.33) | Grad Norm 1099.1685(92.8217) | Total Time 0.00(0.00)\n",
      "Iter 19660 | Time 37.7444(35.6888) | Bit/dim 3.8690(3.7532) | Xent 0.3134(0.2507) | Loss 10.3136(10.5524) | Error 0.1056(0.0798) Steps 1264(1198.79) | Grad Norm 46.0500(249.9585) | Total Time 0.00(0.00)\n",
      "Iter 19670 | Time 41.1069(36.9476) | Bit/dim 4.0328(3.8135) | Xent 0.5195(0.3075) | Loss 11.0667(10.6188) | Error 0.1700(0.0972) Steps 1402(1235.99) | Grad Norm 77.2865(201.1838) | Total Time 0.00(0.00)\n",
      "Iter 19680 | Time 40.2192(37.9543) | Bit/dim 3.9362(3.8592) | Xent 0.4404(0.3387) | Loss 10.5556(10.6322) | Error 0.1278(0.1068) Steps 1324(1257.62) | Grad Norm 72.6898(166.1731) | Total Time 0.00(0.00)\n",
      "Iter 19690 | Time 40.2583(38.6239) | Bit/dim 3.9038(3.8774) | Xent 0.3559(0.3465) | Loss 10.3599(10.5865) | Error 0.1122(0.1105) Steps 1312(1278.02) | Grad Norm 36.3227(134.5174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 158.1896, Epoch Time 2272.1258(1990.4883), Bit/dim 3.9010(best: 3.5691), Xent 0.7970, Loss 4.2996, Error 0.2320(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19700 | Time 41.6472(39.0115) | Bit/dim 3.8332(3.8729) | Xent 0.2854(0.3357) | Loss 10.2424(11.3942) | Error 0.0922(0.1085) Steps 1366(1284.61) | Grad Norm 28.6712(107.3716) | Total Time 0.00(0.00)\n",
      "Iter 19710 | Time 41.0797(39.2079) | Bit/dim 3.8188(3.8628) | Xent 0.2723(0.3198) | Loss 10.0888(11.0564) | Error 0.0844(0.1040) Steps 1270(1287.12) | Grad Norm 19.0662(85.7870) | Total Time 0.00(0.00)\n",
      "Iter 19720 | Time 38.0033(39.1743) | Bit/dim 3.8108(3.8474) | Xent 0.3201(0.3057) | Loss 10.2534(10.8030) | Error 0.1011(0.0988) Steps 1282(1288.24) | Grad Norm 30.1390(69.1803) | Total Time 0.00(0.00)\n",
      "Iter 19730 | Time 38.3277(39.0567) | Bit/dim 3.7885(3.8332) | Xent 0.2241(0.2906) | Loss 10.0632(10.5862) | Error 0.0856(0.0946) Steps 1252(1280.97) | Grad Norm 17.1527(57.8821) | Total Time 0.00(0.00)\n",
      "Iter 19740 | Time 39.3577(39.0444) | Bit/dim 3.7614(3.8161) | Xent 0.1955(0.2726) | Loss 9.9791(10.4192) | Error 0.0556(0.0895) Steps 1258(1276.08) | Grad Norm 22.3293(47.7114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 149.7879, Epoch Time 2328.3788(2000.6250), Bit/dim 3.7667(best: 3.5691), Xent 0.7859, Loss 4.1597, Error 0.2145(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19750 | Time 38.7225(38.8567) | Bit/dim 3.7577(3.8011) | Xent 0.2248(0.2606) | Loss 9.8338(11.2736) | Error 0.0756(0.0868) Steps 1234(1271.54) | Grad Norm 16.4409(39.9463) | Total Time 0.00(0.00)\n",
      "Iter 19760 | Time 35.5478(38.5746) | Bit/dim 3.7446(3.7867) | Xent 0.1932(0.2477) | Loss 9.7029(10.8922) | Error 0.0689(0.0826) Steps 1252(1262.35) | Grad Norm 19.0121(34.8469) | Total Time 0.00(0.00)\n",
      "Iter 19770 | Time 38.7259(38.2772) | Bit/dim 3.7492(3.7741) | Xent 0.2091(0.2388) | Loss 10.0141(10.6221) | Error 0.0711(0.0797) Steps 1282(1252.74) | Grad Norm 21.0933(32.2046) | Total Time 0.00(0.00)\n",
      "Iter 19780 | Time 37.9226(38.0789) | Bit/dim 3.7533(3.7634) | Xent 0.2189(0.2308) | Loss 9.9289(10.4220) | Error 0.0711(0.0771) Steps 1228(1248.72) | Grad Norm 28.9113(30.3090) | Total Time 0.00(0.00)\n",
      "Iter 19790 | Time 37.2748(37.7884) | Bit/dim 3.7381(3.7576) | Xent 0.1757(0.2187) | Loss 9.7352(10.2624) | Error 0.0633(0.0732) Steps 1186(1242.51) | Grad Norm 13.9782(27.0789) | Total Time 0.00(0.00)\n",
      "Iter 19800 | Time 35.7894(37.5074) | Bit/dim 3.7000(3.7449) | Xent 0.2105(0.2145) | Loss 9.8544(10.1273) | Error 0.0789(0.0716) Steps 1234(1237.83) | Grad Norm 1465.8109(77.5509) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 146.7338, Epoch Time 2219.6919(2007.1970), Bit/dim 3.7383(best: 3.5691), Xent 0.8019, Loss 4.1393, Error 0.2189(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19810 | Time 38.0633(37.5706) | Bit/dim 3.8181(3.7549) | Xent 0.2926(0.2212) | Loss 10.3095(10.9588) | Error 0.0911(0.0735) Steps 1288(1237.88) | Grad Norm 55.6479(68.7775) | Total Time 0.00(0.00)\n",
      "Iter 19820 | Time 39.7035(38.2218) | Bit/dim 3.8341(3.7688) | Xent 0.2642(0.2286) | Loss 10.2008(10.7326) | Error 0.0822(0.0754) Steps 1234(1251.22) | Grad Norm 36.9568(59.4512) | Total Time 0.00(0.00)\n",
      "Iter 19830 | Time 39.1666(38.4148) | Bit/dim 3.7382(3.7716) | Xent 0.1815(0.2288) | Loss 9.8341(10.5311) | Error 0.0644(0.0761) Steps 1318(1257.29) | Grad Norm 28.4398(53.6710) | Total Time 0.00(0.00)\n",
      "Iter 19840 | Time 37.9767(38.6151) | Bit/dim 3.7460(3.7702) | Xent 0.1943(0.2270) | Loss 9.9224(10.3799) | Error 0.0544(0.0746) Steps 1258(1263.13) | Grad Norm 36.7879(46.3988) | Total Time 0.00(0.00)\n",
      "Iter 19850 | Time 38.6942(38.5078) | Bit/dim 3.7414(3.7667) | Xent 0.1827(0.2234) | Loss 9.7660(10.2579) | Error 0.0600(0.0747) Steps 1264(1260.12) | Grad Norm 13.0947(41.2891) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 149.3669, Epoch Time 2300.8386(2016.0063), Bit/dim 3.7428(best: 3.5691), Xent 0.7858, Loss 4.1356, Error 0.2130(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19860 | Time 37.6476(38.4431) | Bit/dim 3.7335(3.7601) | Xent 0.2070(0.2194) | Loss 9.6949(11.1126) | Error 0.0644(0.0730) Steps 1252(1256.43) | Grad Norm 25.8367(35.5458) | Total Time 0.00(0.00)\n",
      "Iter 19870 | Time 39.4508(38.3380) | Bit/dim 3.7561(3.7526) | Xent 0.2148(0.2136) | Loss 9.9176(10.7777) | Error 0.0744(0.0716) Steps 1240(1251.43) | Grad Norm 12.3042(33.4246) | Total Time 0.00(0.00)\n",
      "Iter 19880 | Time 38.6748(38.3415) | Bit/dim 3.7169(3.7429) | Xent 0.2008(0.2032) | Loss 9.7738(10.5121) | Error 0.0656(0.0683) Steps 1276(1251.11) | Grad Norm 15.6079(29.4484) | Total Time 0.00(0.00)\n",
      "Iter 19890 | Time 38.9715(38.1418) | Bit/dim 3.6912(3.7329) | Xent 0.2369(0.1978) | Loss 9.8203(10.3192) | Error 0.0856(0.0662) Steps 1210(1245.23) | Grad Norm 18.0209(31.2153) | Total Time 0.00(0.00)\n",
      "Iter 19900 | Time 36.0033(37.7809) | Bit/dim 3.7152(3.7255) | Xent 0.1625(0.1945) | Loss 9.7247(10.1610) | Error 0.0589(0.0649) Steps 1204(1236.95) | Grad Norm 51.1422(31.1167) | Total Time 0.00(0.00)\n",
      "Iter 19910 | Time 35.3709(37.6017) | Bit/dim 3.6870(3.7196) | Xent 0.1650(0.1915) | Loss 9.5517(10.0453) | Error 0.0589(0.0642) Steps 1168(1233.47) | Grad Norm 11.0612(28.4438) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 146.7958, Epoch Time 2235.0640(2022.5780), Bit/dim 3.7109(best: 3.5691), Xent 0.7917, Loss 4.1068, Error 0.2107(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19920 | Time 36.4439(37.5340) | Bit/dim 3.7360(3.7172) | Xent 0.1846(0.1896) | Loss 9.8432(10.8294) | Error 0.0633(0.0637) Steps 1246(1235.43) | Grad Norm 24.5898(29.1664) | Total Time 0.00(0.00)\n",
      "Iter 19930 | Time 36.3001(37.4473) | Bit/dim 3.6368(3.7113) | Xent 0.1479(0.1858) | Loss 9.4913(10.5423) | Error 0.0533(0.0625) Steps 1222(1233.11) | Grad Norm 11.6782(30.8844) | Total Time 0.00(0.00)\n",
      "Iter 19940 | Time 36.3412(37.2632) | Bit/dim 3.6529(3.7023) | Xent 0.2021(0.1834) | Loss 9.6778(10.3057) | Error 0.0700(0.0616) Steps 1192(1225.03) | Grad Norm 26.2014(27.2236) | Total Time 0.00(0.00)\n",
      "Iter 19950 | Time 36.5735(37.1835) | Bit/dim 3.6514(3.6998) | Xent 0.1815(0.1803) | Loss 9.6811(10.1457) | Error 0.0611(0.0602) Steps 1204(1220.47) | Grad Norm 67.9285(57.2360) | Total Time 0.00(0.00)\n",
      "Iter 19960 | Time 35.9878(37.1339) | Bit/dim 3.6643(3.6978) | Xent 0.1765(0.1827) | Loss 9.7259(10.0301) | Error 0.0533(0.0606) Steps 1192(1217.90) | Grad Norm 17.1219(92.7172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 148.1068, Epoch Time 2208.7807(2028.1641), Bit/dim 3.7285(best: 3.5691), Xent 0.8477, Loss 4.1523, Error 0.2183(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19970 | Time 37.6747(37.3244) | Bit/dim 3.6883(3.7010) | Xent 0.1694(0.1844) | Loss 9.6333(10.9422) | Error 0.0589(0.0619) Steps 1270(1226.67) | Grad Norm 48.3098(77.1742) | Total Time 0.00(0.00)\n",
      "Iter 19980 | Time 38.7958(37.6225) | Bit/dim 3.7123(3.7076) | Xent 0.1848(0.1888) | Loss 9.6914(10.6544) | Error 0.0678(0.0629) Steps 1240(1235.45) | Grad Norm 13.8998(87.6287) | Total Time 0.00(0.00)\n",
      "Iter 19990 | Time 37.5703(37.6455) | Bit/dim 3.6992(3.7105) | Xent 0.1751(0.1893) | Loss 9.8041(10.4187) | Error 0.0667(0.0628) Steps 1234(1234.12) | Grad Norm 63.7497(74.1640) | Total Time 0.00(0.00)\n",
      "Iter 20000 | Time 35.9874(37.5529) | Bit/dim 3.6959(3.7083) | Xent 0.2012(0.1882) | Loss 9.6817(10.2390) | Error 0.0756(0.0630) Steps 1198(1232.16) | Grad Norm 23.9228(59.5948) | Total Time 0.00(0.00)\n",
      "Iter 20010 | Time 38.1515(37.4902) | Bit/dim 3.6725(3.7064) | Xent 0.1499(0.1851) | Loss 9.7479(10.1090) | Error 0.0556(0.0622) Steps 1222(1232.93) | Grad Norm 17.2516(50.3515) | Total Time 0.00(0.00)\n",
      "Iter 20020 | Time 36.7830(37.5062) | Bit/dim 3.6771(3.7051) | Xent 0.2017(0.1822) | Loss 9.7200(10.0050) | Error 0.0622(0.0612) Steps 1276(1231.21) | Grad Norm 26.9748(61.6652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 147.3044, Epoch Time 2238.8506(2034.4847), Bit/dim 3.7037(best: 3.5691), Xent 0.8034, Loss 4.1054, Error 0.2121(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20030 | Time 35.5059(37.4442) | Bit/dim 3.6804(3.7039) | Xent 0.1845(0.1771) | Loss 9.5330(10.7780) | Error 0.0633(0.0598) Steps 1186(1225.68) | Grad Norm 44.4966(55.4474) | Total Time 0.00(0.00)\n",
      "Iter 20040 | Time 34.8521(37.1349) | Bit/dim 3.6726(3.7008) | Xent 0.1743(0.1735) | Loss 9.6411(10.4908) | Error 0.0544(0.0588) Steps 1204(1220.68) | Grad Norm 43.5981(64.6652) | Total Time 0.00(0.00)\n",
      "Iter 20050 | Time 36.3382(36.7675) | Bit/dim 3.6623(3.6918) | Xent 0.1717(0.1722) | Loss 9.6447(10.2629) | Error 0.0611(0.0584) Steps 1162(1211.71) | Grad Norm 23.1141(127.3571) | Total Time 0.00(0.00)\n",
      "Iter 20060 | Time 36.2864(36.6332) | Bit/dim 3.6880(3.6901) | Xent 0.1749(0.1718) | Loss 9.5611(10.1127) | Error 0.0578(0.0576) Steps 1204(1211.78) | Grad Norm 25.4632(101.6688) | Total Time 0.00(0.00)\n",
      "Iter 20070 | Time 37.2021(36.5769) | Bit/dim 3.6116(3.6840) | Xent 0.1263(0.1684) | Loss 9.6321(9.9808) | Error 0.0433(0.0564) Steps 1228(1210.93) | Grad Norm 13.3406(79.8961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 145.4285, Epoch Time 2165.8316(2038.4251), Bit/dim 3.6812(best: 3.5691), Xent 0.8232, Loss 4.0928, Error 0.2092(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20080 | Time 35.4695(36.5942) | Bit/dim 3.6646(3.6795) | Xent 0.1759(0.1697) | Loss 9.4539(10.8971) | Error 0.0622(0.0567) Steps 1192(1209.02) | Grad Norm 16.3364(62.9560) | Total Time 0.00(0.00)\n",
      "Iter 20090 | Time 35.2256(36.3295) | Bit/dim 3.6734(3.6778) | Xent 0.1614(0.1669) | Loss 9.5628(10.5629) | Error 0.0444(0.0552) Steps 1150(1202.88) | Grad Norm 156.5253(64.6724) | Total Time 0.00(0.00)\n",
      "Iter 20100 | Time 36.7779(36.0645) | Bit/dim 3.6525(3.6728) | Xent 0.1851(0.1664) | Loss 9.6373(10.3050) | Error 0.0622(0.0551) Steps 1222(1198.08) | Grad Norm 1727.9671(147.5498) | Total Time 0.00(0.00)\n",
      "Iter 20110 | Time 27.3527(34.5382) | Bit/dim 3.7766(3.6964) | Xent 0.3005(0.1960) | Loss 9.7540(10.1868) | Error 0.1033(0.0654) Steps 1000(1168.60) | Grad Norm 12.7832(354.1897) | Total Time 0.00(0.00)\n",
      "Iter 20120 | Time 27.6321(32.6033) | Bit/dim 3.6634(3.7045) | Xent 0.2264(0.2173) | Loss 9.6224(10.0679) | Error 0.0833(0.0743) Steps 1042(1125.62) | Grad Norm 9.9433(263.9364) | Total Time 0.00(0.00)\n",
      "Iter 20130 | Time 25.6777(30.8646) | Bit/dim 3.6912(3.7048) | Xent 0.1852(0.2170) | Loss 9.5696(9.9508) | Error 0.0500(0.0738) Steps 970(1088.19) | Grad Norm 8.7235(196.7123) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 112.1327, Epoch Time 1859.8957(2033.0692), Bit/dim 3.6872(best: 3.5691), Xent 0.7789, Loss 4.0766, Error 0.2087(best: 0.2042)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20140 | Time 26.1619(29.7901) | Bit/dim 3.6826(3.6959) | Xent 0.1997(0.2116) | Loss 9.5299(10.6139) | Error 0.0700(0.0722) Steps 1000(1060.20) | Grad Norm 8.8055(146.6854) | Total Time 0.00(0.00)\n",
      "Iter 20150 | Time 25.8919(28.9112) | Bit/dim 3.6587(3.6876) | Xent 0.1827(0.2026) | Loss 9.4741(10.3217) | Error 0.0667(0.0694) Steps 988(1039.39) | Grad Norm 7.0493(110.1077) | Total Time 0.00(0.00)\n",
      "Iter 20160 | Time 25.6243(28.1487) | Bit/dim 3.6624(3.6796) | Xent 0.1590(0.1948) | Loss 9.5007(10.1012) | Error 0.0600(0.0666) Steps 952(1020.94) | Grad Norm 5.8425(83.0859) | Total Time 0.00(0.00)\n",
      "Iter 20170 | Time 25.1607(27.7186) | Bit/dim 3.6151(3.6694) | Xent 0.1589(0.1883) | Loss 9.2605(9.9329) | Error 0.0533(0.0645) Steps 934(1009.89) | Grad Norm 6.9235(63.2621) | Total Time 0.00(0.00)\n",
      "Iter 20180 | Time 26.3838(27.4240) | Bit/dim 3.6758(3.6654) | Xent 0.1819(0.1807) | Loss 9.4931(9.8055) | Error 0.0578(0.0617) Steps 1024(1002.95) | Grad Norm 7.2735(48.5335) | Total Time 0.00(0.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_run3/epoch_235_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
