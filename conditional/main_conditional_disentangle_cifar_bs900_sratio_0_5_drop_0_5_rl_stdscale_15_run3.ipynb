{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        if args.data == \"colormnist\":\n",
      "            # print train images\n",
      "            xall = []\n",
      "            ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "            for i in range(ximg.shape[0]):\n",
      "                xall.append(ximg[i])\n",
      "        \n",
      "            xall = np.hstack(xall)\n",
      "\n",
      "            plt.imshow(xall)\n",
      "            plt.axis('off')\n",
      "            plt.show()\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                if args.data == \"colormnist\":\n",
      "                    # print test images\n",
      "                    xall = []\n",
      "                    ximg = x[0:40].cpu().numpy().transpose((0,2,3,1))\n",
      "                    for i in range(ximg.shape[0]):\n",
      "                        xall.append(ximg[i])\n",
      "\n",
      "                    xall = np.hstack(xall)\n",
      "\n",
      "                    plt.imshow(xall)\n",
      "                    plt.axis('off')\n",
      "                    plt.show()\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3/epoch_150_checkpt.pth', rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=3, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 8260 | Time 19.5607(20.4024) | Bit/dim 3.6051(3.6105) | Xent 0.3080(0.3865) | Loss 8.5728(9.4296) | Error 0.1089(0.1371) Steps 778(800.75) | Grad Norm 3.8216(6.5180) | Total Time 0.00(0.00)\n",
      "Iter 8270 | Time 19.0458(20.1897) | Bit/dim 3.6244(3.6063) | Xent 0.3485(0.3745) | Loss 8.7749(9.2197) | Error 0.1289(0.1324) Steps 766(799.28) | Grad Norm 3.3049(5.6387) | Total Time 0.00(0.00)\n",
      "Iter 8280 | Time 19.6464(20.0354) | Bit/dim 3.5923(3.6028) | Xent 0.3590(0.3664) | Loss 8.5904(9.0608) | Error 0.1222(0.1289) Steps 802(796.50) | Grad Norm 3.0103(5.0323) | Total Time 0.00(0.00)\n",
      "Iter 8290 | Time 19.9164(19.8688) | Bit/dim 3.5872(3.5991) | Xent 0.3109(0.3577) | Loss 8.6649(8.9513) | Error 0.1100(0.1253) Steps 808(795.74) | Grad Norm 3.8416(4.5521) | Total Time 0.00(0.00)\n",
      "Iter 8300 | Time 20.4575(19.8606) | Bit/dim 3.5808(3.5978) | Xent 0.2952(0.3496) | Loss 8.6505(8.8720) | Error 0.1067(0.1234) Steps 790(795.88) | Grad Norm 2.5570(4.2379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 98.2816, Epoch Time 1218.8817(1137.2006), Bit/dim 3.5931(best: inf), Xent 0.6386, Loss 3.9124, Error 0.2037(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 19.8869(19.8739) | Bit/dim 3.5800(3.5944) | Xent 0.3639(0.3471) | Loss 8.7191(9.4675) | Error 0.1178(0.1217) Steps 796(795.33) | Grad Norm 2.8480(4.0626) | Total Time 0.00(0.00)\n",
      "Iter 8320 | Time 19.7214(19.8992) | Bit/dim 3.5727(3.5934) | Xent 0.3184(0.3421) | Loss 8.5325(9.2582) | Error 0.1089(0.1199) Steps 802(797.36) | Grad Norm 3.2780(3.8538) | Total Time 0.00(0.00)\n",
      "Iter 8330 | Time 20.9227(19.9813) | Bit/dim 3.6016(3.5922) | Xent 0.3545(0.3367) | Loss 8.8452(9.1087) | Error 0.1189(0.1177) Steps 826(801.36) | Grad Norm 2.9645(3.7614) | Total Time 0.00(0.00)\n",
      "Iter 8340 | Time 19.3634(19.9398) | Bit/dim 3.5722(3.5893) | Xent 0.3238(0.3323) | Loss 8.5816(8.9854) | Error 0.1144(0.1166) Steps 802(800.38) | Grad Norm 2.9696(3.5879) | Total Time 0.00(0.00)\n",
      "Iter 8350 | Time 20.2075(19.9511) | Bit/dim 3.5670(3.5900) | Xent 0.3042(0.3291) | Loss 8.6357(8.8955) | Error 0.1089(0.1155) Steps 760(799.29) | Grad Norm 4.4431(3.6136) | Total Time 0.00(0.00)\n",
      "Iter 8360 | Time 19.8799(19.8915) | Bit/dim 3.6081(3.5921) | Xent 0.3169(0.3268) | Loss 8.6431(8.8306) | Error 0.1089(0.1144) Steps 802(799.41) | Grad Norm 4.3034(3.5308) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 95.3468, Epoch Time 1209.6971(1139.3755), Bit/dim 3.5946(best: 3.5931), Xent 0.6409, Loss 3.9151, Error 0.2056(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 19.3632(19.9127) | Bit/dim 3.5911(3.5927) | Xent 0.3480(0.3230) | Loss 8.5626(9.3716) | Error 0.1167(0.1119) Steps 802(800.33) | Grad Norm 3.6803(3.4421) | Total Time 0.00(0.00)\n",
      "Iter 8380 | Time 19.3099(19.9227) | Bit/dim 3.6087(3.5924) | Xent 0.3425(0.3231) | Loss 8.6162(9.1758) | Error 0.1267(0.1124) Steps 784(800.32) | Grad Norm 4.8273(3.4633) | Total Time 0.00(0.00)\n",
      "Iter 8390 | Time 20.2329(19.9582) | Bit/dim 3.5801(3.5906) | Xent 0.3035(0.3249) | Loss 8.6229(9.0418) | Error 0.1133(0.1127) Steps 784(799.43) | Grad Norm 3.0211(3.4501) | Total Time 0.00(0.00)\n",
      "Iter 8400 | Time 20.7719(19.9100) | Bit/dim 3.5974(3.5890) | Xent 0.3322(0.3244) | Loss 8.5741(8.9285) | Error 0.1056(0.1126) Steps 838(800.18) | Grad Norm 2.5244(3.4007) | Total Time 0.00(0.00)\n",
      "Iter 8410 | Time 19.3934(19.8713) | Bit/dim 3.5688(3.5889) | Xent 0.3861(0.3255) | Loss 8.6817(8.8586) | Error 0.1333(0.1138) Steps 808(801.01) | Grad Norm 3.2708(3.4253) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 96.6005, Epoch Time 1208.0067(1141.4345), Bit/dim 3.5950(best: 3.5931), Xent 0.6453, Loss 3.9177, Error 0.2064(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 19.9806(19.8392) | Bit/dim 3.6013(3.5894) | Xent 0.3131(0.3237) | Loss 8.5793(9.4716) | Error 0.1078(0.1128) Steps 808(800.87) | Grad Norm 6.1041(3.5587) | Total Time 0.00(0.00)\n",
      "Iter 8430 | Time 19.7499(19.8241) | Bit/dim 3.5502(3.5847) | Xent 0.3260(0.3234) | Loss 8.6273(9.2508) | Error 0.1200(0.1131) Steps 790(800.56) | Grad Norm 5.0112(3.7605) | Total Time 0.00(0.00)\n",
      "Iter 8440 | Time 19.6844(19.8015) | Bit/dim 3.5804(3.5874) | Xent 0.3004(0.3212) | Loss 8.5191(9.0808) | Error 0.1156(0.1124) Steps 814(800.50) | Grad Norm 3.2032(3.7734) | Total Time 0.00(0.00)\n",
      "Iter 8450 | Time 19.8296(19.8090) | Bit/dim 3.6131(3.5914) | Xent 0.3381(0.3207) | Loss 8.8099(8.9847) | Error 0.1244(0.1125) Steps 790(801.22) | Grad Norm 3.2984(3.6439) | Total Time 0.00(0.00)\n",
      "Iter 8460 | Time 20.6066(19.8516) | Bit/dim 3.5551(3.5908) | Xent 0.3377(0.3200) | Loss 8.7505(8.9027) | Error 0.1089(0.1113) Steps 820(798.70) | Grad Norm 2.7020(3.5185) | Total Time 0.00(0.00)\n",
      "Iter 8470 | Time 19.8807(19.9671) | Bit/dim 3.6239(3.5886) | Xent 0.2942(0.3205) | Loss 8.6551(8.8420) | Error 0.1033(0.1117) Steps 826(801.54) | Grad Norm 2.9160(3.4471) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 95.4188, Epoch Time 1208.6509(1143.4510), Bit/dim 3.5929(best: 3.5931), Xent 0.6467, Loss 3.9163, Error 0.2069(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 20.0211(19.9107) | Bit/dim 3.6034(3.5868) | Xent 0.3072(0.3181) | Loss 8.6990(9.3536) | Error 0.1022(0.1109) Steps 784(799.77) | Grad Norm 3.4114(3.4024) | Total Time 0.00(0.00)\n",
      "Iter 8490 | Time 20.5029(19.9959) | Bit/dim 3.6049(3.5876) | Xent 0.3055(0.3180) | Loss 8.6245(9.1678) | Error 0.1089(0.1106) Steps 802(800.31) | Grad Norm 3.0560(3.3920) | Total Time 0.00(0.00)\n",
      "Iter 8500 | Time 20.2815(20.0098) | Bit/dim 3.5749(3.5892) | Xent 0.3231(0.3164) | Loss 8.5369(9.0231) | Error 0.1189(0.1101) Steps 826(802.50) | Grad Norm 2.7019(3.4028) | Total Time 0.00(0.00)\n",
      "Iter 8510 | Time 19.9422(20.0006) | Bit/dim 3.5910(3.5883) | Xent 0.3066(0.3169) | Loss 8.5979(8.9196) | Error 0.1178(0.1108) Steps 784(801.96) | Grad Norm 3.7457(3.4009) | Total Time 0.00(0.00)\n",
      "Iter 8520 | Time 20.2762(20.0354) | Bit/dim 3.5839(3.5866) | Xent 0.3025(0.3176) | Loss 8.5565(8.8380) | Error 0.1011(0.1117) Steps 820(802.19) | Grad Norm 4.1605(3.5429) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 95.7616, Epoch Time 1214.5218(1145.5831), Bit/dim 3.5938(best: 3.5929), Xent 0.6537, Loss 3.9207, Error 0.2055(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 20.5149(20.0353) | Bit/dim 3.6145(3.5875) | Xent 0.3254(0.3156) | Loss 8.7929(9.4666) | Error 0.1122(0.1111) Steps 832(805.88) | Grad Norm 4.2057(3.6914) | Total Time 0.00(0.00)\n",
      "Iter 8540 | Time 20.5066(20.1382) | Bit/dim 3.5764(3.5904) | Xent 0.2941(0.3151) | Loss 8.6314(9.2637) | Error 0.1067(0.1107) Steps 814(804.57) | Grad Norm 2.9674(3.6242) | Total Time 0.00(0.00)\n",
      "Iter 8550 | Time 18.5814(20.0881) | Bit/dim 3.5968(3.5894) | Xent 0.2729(0.3142) | Loss 8.5510(9.0981) | Error 0.0900(0.1101) Steps 790(805.12) | Grad Norm 2.7619(3.5411) | Total Time 0.00(0.00)\n",
      "Iter 8560 | Time 19.8608(20.0650) | Bit/dim 3.5977(3.5896) | Xent 0.3408(0.3157) | Loss 8.7958(8.9838) | Error 0.1133(0.1104) Steps 802(804.32) | Grad Norm 2.9909(3.4420) | Total Time 0.00(0.00)\n",
      "Iter 8570 | Time 20.0683(20.0929) | Bit/dim 3.5810(3.5880) | Xent 0.2867(0.3127) | Loss 8.5591(8.8826) | Error 0.0978(0.1092) Steps 796(801.13) | Grad Norm 3.0261(3.4589) | Total Time 0.00(0.00)\n",
      "Iter 8580 | Time 19.6809(20.0665) | Bit/dim 3.5771(3.5877) | Xent 0.3309(0.3160) | Loss 8.6898(8.8230) | Error 0.1200(0.1103) Steps 802(802.47) | Grad Norm 3.7050(3.5767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 96.6128, Epoch Time 1221.2578(1147.8533), Bit/dim 3.5931(best: 3.5929), Xent 0.6509, Loss 3.9185, Error 0.2070(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 20.0277(20.0081) | Bit/dim 3.5959(3.5846) | Xent 0.3425(0.3159) | Loss 8.7489(9.3567) | Error 0.1222(0.1102) Steps 772(802.02) | Grad Norm 5.0345(3.6612) | Total Time 0.00(0.00)\n",
      "Iter 8600 | Time 20.1423(19.9867) | Bit/dim 3.5476(3.5861) | Xent 0.2615(0.3122) | Loss 8.4264(9.1562) | Error 0.0900(0.1090) Steps 790(803.80) | Grad Norm 3.4779(3.6408) | Total Time 0.00(0.00)\n",
      "Iter 8610 | Time 19.8542(20.0196) | Bit/dim 3.6364(3.5886) | Xent 0.3522(0.3167) | Loss 8.7109(9.0336) | Error 0.1233(0.1098) Steps 778(804.67) | Grad Norm 2.9465(3.5950) | Total Time 0.00(0.00)\n",
      "Iter 8620 | Time 19.8088(20.0295) | Bit/dim 3.5857(3.5887) | Xent 0.3227(0.3159) | Loss 8.7199(8.9338) | Error 0.1189(0.1101) Steps 784(805.91) | Grad Norm 5.3776(3.8136) | Total Time 0.00(0.00)\n",
      "Iter 8630 | Time 20.8942(20.0616) | Bit/dim 3.5863(3.5867) | Xent 0.3171(0.3141) | Loss 8.7469(8.8536) | Error 0.1100(0.1093) Steps 850(807.22) | Grad Norm 2.7907(3.6907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 96.7498, Epoch Time 1216.1309(1149.9017), Bit/dim 3.5924(best: 3.5929), Xent 0.6536, Loss 3.9192, Error 0.2079(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 20.2220(20.0952) | Bit/dim 3.5930(3.5881) | Xent 0.3292(0.3143) | Loss 8.8104(9.4895) | Error 0.1156(0.1093) Steps 832(808.68) | Grad Norm 3.3185(3.5716) | Total Time 0.00(0.00)\n",
      "Iter 8650 | Time 20.3604(20.1589) | Bit/dim 3.5845(3.5889) | Xent 0.3165(0.3136) | Loss 8.5580(9.2774) | Error 0.1100(0.1101) Steps 808(807.40) | Grad Norm 3.2454(3.5792) | Total Time 0.00(0.00)\n",
      "Iter 8660 | Time 20.4471(20.1601) | Bit/dim 3.5595(3.5885) | Xent 0.2750(0.3119) | Loss 8.6054(9.1013) | Error 0.0967(0.1087) Steps 790(808.73) | Grad Norm 3.4664(3.5675) | Total Time 0.00(0.00)\n",
      "Iter 8670 | Time 19.7701(20.1556) | Bit/dim 3.5933(3.5869) | Xent 0.3210(0.3120) | Loss 8.6423(8.9774) | Error 0.1089(0.1077) Steps 796(806.58) | Grad Norm 2.4396(3.5695) | Total Time 0.00(0.00)\n",
      "Iter 8680 | Time 20.3534(20.1217) | Bit/dim 3.5736(3.5867) | Xent 0.3102(0.3137) | Loss 8.6175(8.8972) | Error 0.1122(0.1084) Steps 814(805.01) | Grad Norm 3.9048(3.5761) | Total Time 0.00(0.00)\n",
      "Iter 8690 | Time 19.7974(20.1471) | Bit/dim 3.6089(3.5870) | Xent 0.3027(0.3126) | Loss 8.7315(8.8324) | Error 0.1011(0.1088) Steps 778(802.96) | Grad Norm 3.8604(3.5318) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 97.0403, Epoch Time 1225.4496(1152.1681), Bit/dim 3.5923(best: 3.5924), Xent 0.6645, Loss 3.9245, Error 0.2078(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 19.9991(20.1298) | Bit/dim 3.5759(3.5871) | Xent 0.3056(0.3136) | Loss 8.6800(9.3725) | Error 0.1022(0.1089) Steps 820(806.36) | Grad Norm 3.2271(3.5823) | Total Time 0.00(0.00)\n",
      "Iter 8710 | Time 19.6224(20.0749) | Bit/dim 3.5769(3.5898) | Xent 0.2672(0.3128) | Loss 8.5764(9.1856) | Error 0.0967(0.1083) Steps 784(804.15) | Grad Norm 4.5658(3.7815) | Total Time 0.00(0.00)\n",
      "Iter 8720 | Time 19.6919(20.0918) | Bit/dim 3.6120(3.5880) | Xent 0.3162(0.3150) | Loss 8.7100(9.0428) | Error 0.1133(0.1091) Steps 790(805.32) | Grad Norm 3.7954(3.8428) | Total Time 0.00(0.00)\n",
      "Iter 8730 | Time 19.6084(20.0625) | Bit/dim 3.5969(3.5869) | Xent 0.3161(0.3137) | Loss 8.6086(8.9331) | Error 0.1056(0.1103) Steps 796(806.55) | Grad Norm 4.8010(4.0799) | Total Time 0.00(0.00)\n",
      "Iter 8740 | Time 19.6236(20.0429) | Bit/dim 3.5929(3.5888) | Xent 0.2852(0.3141) | Loss 8.5035(8.8602) | Error 0.1022(0.1108) Steps 778(804.83) | Grad Norm 2.8641(4.1180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 96.5373, Epoch Time 1215.7490(1154.0755), Bit/dim 3.5906(best: 3.5923), Xent 0.6620, Loss 3.9217, Error 0.2055(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 19.7015(20.0188) | Bit/dim 3.5918(3.5894) | Xent 0.3070(0.3145) | Loss 8.7246(9.4945) | Error 0.1044(0.1108) Steps 808(805.44) | Grad Norm 3.6238(4.0130) | Total Time 0.00(0.00)\n",
      "Iter 8760 | Time 19.8170(19.9753) | Bit/dim 3.5715(3.5878) | Xent 0.2857(0.3121) | Loss 8.5560(9.2629) | Error 0.1033(0.1103) Steps 766(803.90) | Grad Norm 4.2690(3.8842) | Total Time 0.00(0.00)\n",
      "Iter 8770 | Time 19.3625(19.9104) | Bit/dim 3.5626(3.5876) | Xent 0.2997(0.3116) | Loss 8.5602(9.0912) | Error 0.1144(0.1097) Steps 784(802.07) | Grad Norm 2.9737(3.8150) | Total Time 0.00(0.00)\n",
      "Iter 8780 | Time 20.4269(19.9864) | Bit/dim 3.6009(3.5886) | Xent 0.3023(0.3080) | Loss 8.6885(8.9744) | Error 0.0956(0.1079) Steps 814(802.21) | Grad Norm 4.7228(3.9833) | Total Time 0.00(0.00)\n",
      "Iter 8790 | Time 19.6361(19.9906) | Bit/dim 3.6008(3.5865) | Xent 0.3295(0.3076) | Loss 8.5879(8.8630) | Error 0.1200(0.1076) Steps 826(802.18) | Grad Norm 4.5977(3.9203) | Total Time 0.00(0.00)\n",
      "Iter 8800 | Time 19.5639(20.0374) | Bit/dim 3.6074(3.5878) | Xent 0.2800(0.3070) | Loss 8.6279(8.8035) | Error 0.0956(0.1074) Steps 832(803.77) | Grad Norm 2.6442(3.8133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 97.5484, Epoch Time 1214.1443(1155.8776), Bit/dim 3.5911(best: 3.5906), Xent 0.6513, Loss 3.9168, Error 0.2052(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 21.5877(20.0749) | Bit/dim 3.5851(3.5885) | Xent 0.3109(0.3073) | Loss 8.6261(9.3385) | Error 0.1178(0.1077) Steps 820(802.70) | Grad Norm 4.1295(3.8340) | Total Time 0.00(0.00)\n",
      "Iter 8820 | Time 19.7115(20.0367) | Bit/dim 3.5965(3.5879) | Xent 0.3294(0.3077) | Loss 8.7813(9.1591) | Error 0.1100(0.1081) Steps 826(805.25) | Grad Norm 3.0574(3.7417) | Total Time 0.00(0.00)\n",
      "Iter 8830 | Time 20.0423(20.0319) | Bit/dim 3.5851(3.5871) | Xent 0.2790(0.3070) | Loss 8.5737(9.0197) | Error 0.0956(0.1073) Steps 796(807.58) | Grad Norm 3.3835(3.7054) | Total Time 0.00(0.00)\n",
      "Iter 8840 | Time 19.8193(20.0108) | Bit/dim 3.6179(3.5873) | Xent 0.3014(0.3080) | Loss 8.5924(8.9170) | Error 0.1078(0.1070) Steps 802(806.01) | Grad Norm 3.6304(3.6764) | Total Time 0.00(0.00)\n",
      "Iter 8850 | Time 20.3704(20.0134) | Bit/dim 3.5502(3.5904) | Xent 0.3262(0.3091) | Loss 8.5964(8.8606) | Error 0.1133(0.1078) Steps 820(806.04) | Grad Norm 5.1844(3.7367) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 96.7234, Epoch Time 1214.0455(1157.6226), Bit/dim 3.5917(best: 3.5906), Xent 0.6561, Loss 3.9197, Error 0.2060(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 18.9844(19.9370) | Bit/dim 3.5700(3.5872) | Xent 0.2944(0.3068) | Loss 8.5675(9.4847) | Error 0.1033(0.1080) Steps 808(804.34) | Grad Norm 5.0689(3.7925) | Total Time 0.00(0.00)\n",
      "Iter 8870 | Time 19.7400(19.9593) | Bit/dim 3.6003(3.5880) | Xent 0.2881(0.3085) | Loss 8.7472(9.2612) | Error 0.0989(0.1079) Steps 838(805.54) | Grad Norm 4.3120(3.7709) | Total Time 0.00(0.00)\n",
      "Iter 8880 | Time 19.5048(19.9490) | Bit/dim 3.5717(3.5865) | Xent 0.2926(0.3082) | Loss 8.6175(9.1044) | Error 0.1067(0.1079) Steps 790(805.79) | Grad Norm 3.3988(3.7345) | Total Time 0.00(0.00)\n",
      "Iter 8890 | Time 20.6994(19.9718) | Bit/dim 3.5722(3.5884) | Xent 0.2945(0.3092) | Loss 8.6659(8.9869) | Error 0.1078(0.1086) Steps 802(806.22) | Grad Norm 5.1609(3.7289) | Total Time 0.00(0.00)\n",
      "Iter 8900 | Time 19.6844(20.0154) | Bit/dim 3.5554(3.5856) | Xent 0.2768(0.3081) | Loss 8.5339(8.8914) | Error 0.0911(0.1079) Steps 796(804.48) | Grad Norm 3.2956(3.8442) | Total Time 0.00(0.00)\n",
      "Iter 8910 | Time 19.8577(20.0488) | Bit/dim 3.6539(3.5894) | Xent 0.3619(0.3090) | Loss 8.8159(8.8336) | Error 0.1278(0.1089) Steps 808(807.82) | Grad Norm 4.3311(3.8835) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 96.7208, Epoch Time 1216.2936(1159.3828), Bit/dim 3.5975(best: 3.5906), Xent 0.6670, Loss 3.9310, Error 0.2059(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 19.5859(20.0324) | Bit/dim 3.5781(3.5888) | Xent 0.3420(0.3095) | Loss 8.5820(9.3741) | Error 0.1233(0.1091) Steps 802(807.65) | Grad Norm 4.1625(3.9527) | Total Time 0.00(0.00)\n",
      "Iter 8930 | Time 20.4317(20.0380) | Bit/dim 3.5691(3.5859) | Xent 0.3100(0.3100) | Loss 8.7412(9.1720) | Error 0.1111(0.1091) Steps 826(807.88) | Grad Norm 3.0085(3.9945) | Total Time 0.00(0.00)\n",
      "Iter 8940 | Time 20.2588(20.0814) | Bit/dim 3.6078(3.5846) | Xent 0.2842(0.3078) | Loss 8.5818(9.0197) | Error 0.1011(0.1078) Steps 820(805.45) | Grad Norm 2.9979(4.1432) | Total Time 0.00(0.00)\n",
      "Iter 8950 | Time 20.5435(20.0492) | Bit/dim 3.5784(3.5867) | Xent 0.3118(0.3071) | Loss 8.6767(8.9152) | Error 0.1100(0.1082) Steps 796(805.89) | Grad Norm 2.5881(3.9606) | Total Time 0.00(0.00)\n",
      "Iter 8960 | Time 19.9463(20.0255) | Bit/dim 3.5910(3.5878) | Xent 0.2922(0.3034) | Loss 8.6328(8.8307) | Error 0.0978(0.1066) Steps 796(805.52) | Grad Norm 4.0154(3.9415) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 95.5002, Epoch Time 1215.8998(1161.0783), Bit/dim 3.5897(best: 3.5906), Xent 0.6591, Loss 3.9192, Error 0.2048(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 19.8409(20.1140) | Bit/dim 3.6001(3.5840) | Xent 0.2559(0.3034) | Loss 8.5929(9.4557) | Error 0.0878(0.1067) Steps 796(806.43) | Grad Norm 3.6598(3.9102) | Total Time 0.00(0.00)\n",
      "Iter 8980 | Time 20.0095(20.1030) | Bit/dim 3.6234(3.5866) | Xent 0.2940(0.3025) | Loss 8.6266(9.2277) | Error 0.1089(0.1060) Steps 802(807.04) | Grad Norm 2.7424(3.7201) | Total Time 0.00(0.00)\n",
      "Iter 8990 | Time 19.7415(20.0611) | Bit/dim 3.5821(3.5888) | Xent 0.3077(0.3008) | Loss 8.5506(9.0711) | Error 0.1022(0.1049) Steps 802(807.18) | Grad Norm 4.0490(3.7170) | Total Time 0.00(0.00)\n",
      "Iter 9000 | Time 20.3003(20.0416) | Bit/dim 3.5578(3.5892) | Xent 0.3049(0.3007) | Loss 8.6244(8.9558) | Error 0.1100(0.1054) Steps 862(811.07) | Grad Norm 4.7612(3.7762) | Total Time 0.00(0.00)\n",
      "Iter 9010 | Time 19.9108(20.0180) | Bit/dim 3.5826(3.5885) | Xent 0.3010(0.3036) | Loss 8.6192(8.8777) | Error 0.1167(0.1065) Steps 814(810.45) | Grad Norm 4.1113(3.9890) | Total Time 0.00(0.00)\n",
      "Iter 9020 | Time 20.7998(20.0725) | Bit/dim 3.6173(3.5876) | Xent 0.3073(0.3031) | Loss 8.6293(8.8035) | Error 0.1011(0.1062) Steps 766(807.44) | Grad Norm 4.6617(4.1163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 96.9642, Epoch Time 1219.1598(1162.8207), Bit/dim 3.5927(best: 3.5897), Xent 0.6739, Loss 3.9297, Error 0.2058(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 19.8617(20.1038) | Bit/dim 3.5561(3.5865) | Xent 0.3520(0.3026) | Loss 8.6461(9.3419) | Error 0.1211(0.1062) Steps 754(804.61) | Grad Norm 3.9491(4.1570) | Total Time 0.00(0.00)\n",
      "Iter 9040 | Time 21.3016(20.1319) | Bit/dim 3.5904(3.5891) | Xent 0.2982(0.3036) | Loss 8.6550(9.1650) | Error 0.1011(0.1060) Steps 784(803.12) | Grad Norm 4.4139(4.1863) | Total Time 0.00(0.00)\n",
      "Iter 9050 | Time 19.7695(20.0593) | Bit/dim 3.5734(3.5882) | Xent 0.2770(0.3035) | Loss 8.5899(9.0220) | Error 0.0922(0.1056) Steps 778(805.10) | Grad Norm 3.5863(4.0378) | Total Time 0.00(0.00)\n",
      "Iter 9060 | Time 20.4785(20.0995) | Bit/dim 3.5953(3.5900) | Xent 0.2987(0.3032) | Loss 8.7285(8.9272) | Error 0.1067(0.1054) Steps 808(807.29) | Grad Norm 7.6479(4.0962) | Total Time 0.00(0.00)\n",
      "Iter 9070 | Time 19.4378(20.0662) | Bit/dim 3.5541(3.5868) | Xent 0.3045(0.3037) | Loss 8.4809(8.8387) | Error 0.1222(0.1065) Steps 790(804.57) | Grad Norm 4.9735(4.3390) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 96.8718, Epoch Time 1219.4228(1164.5188), Bit/dim 3.5946(best: 3.5897), Xent 0.6660, Loss 3.9276, Error 0.2077(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 20.2195(20.1104) | Bit/dim 3.6174(3.5869) | Xent 0.2957(0.3026) | Loss 8.7380(9.4734) | Error 0.1144(0.1062) Steps 796(806.88) | Grad Norm 3.9186(4.1930) | Total Time 0.00(0.00)\n",
      "Iter 9090 | Time 20.5806(20.0770) | Bit/dim 3.5938(3.5856) | Xent 0.2868(0.3026) | Loss 8.6793(9.2507) | Error 0.0922(0.1059) Steps 820(806.40) | Grad Norm 4.0999(4.1239) | Total Time 0.00(0.00)\n",
      "Iter 9100 | Time 20.3134(20.0947) | Bit/dim 3.5881(3.5894) | Xent 0.3023(0.3007) | Loss 8.6229(9.0938) | Error 0.1078(0.1052) Steps 790(806.59) | Grad Norm 3.6992(4.0468) | Total Time 0.00(0.00)\n",
      "Iter 9110 | Time 19.6468(20.0749) | Bit/dim 3.5980(3.5895) | Xent 0.2839(0.3005) | Loss 8.5724(8.9728) | Error 0.0933(0.1050) Steps 784(804.82) | Grad Norm 4.4653(4.1777) | Total Time 0.00(0.00)\n",
      "Iter 9120 | Time 19.4994(20.0656) | Bit/dim 3.6017(3.5878) | Xent 0.2839(0.3052) | Loss 8.3582(8.8696) | Error 0.1067(0.1071) Steps 790(805.85) | Grad Norm 4.8457(4.3694) | Total Time 0.00(0.00)\n",
      "Iter 9130 | Time 20.1530(20.0215) | Bit/dim 3.5806(3.5849) | Xent 0.3226(0.3037) | Loss 8.7559(8.8095) | Error 0.1167(0.1064) Steps 832(804.78) | Grad Norm 3.3340(4.3389) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 96.6732, Epoch Time 1217.5789(1166.1106), Bit/dim 3.5913(best: 3.5897), Xent 0.6698, Loss 3.9263, Error 0.2073(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 20.5997(20.0811) | Bit/dim 3.5608(3.5855) | Xent 0.2978(0.3044) | Loss 8.5987(9.3579) | Error 0.1033(0.1058) Steps 790(805.23) | Grad Norm 3.7992(4.2125) | Total Time 0.00(0.00)\n",
      "Iter 9150 | Time 20.8844(20.1095) | Bit/dim 3.5631(3.5853) | Xent 0.2664(0.3031) | Loss 8.5983(9.1735) | Error 0.0900(0.1066) Steps 838(805.39) | Grad Norm 6.1531(4.2236) | Total Time 0.00(0.00)\n",
      "Iter 9160 | Time 20.2192(20.1325) | Bit/dim 3.5999(3.5868) | Xent 0.3155(0.2995) | Loss 8.5721(9.0222) | Error 0.1189(0.1058) Steps 790(804.66) | Grad Norm 4.9433(4.2751) | Total Time 0.00(0.00)\n",
      "Iter 9170 | Time 21.0327(20.1501) | Bit/dim 3.5997(3.5868) | Xent 0.3236(0.3029) | Loss 8.7581(8.9168) | Error 0.1122(0.1070) Steps 874(804.75) | Grad Norm 4.0683(4.3441) | Total Time 0.00(0.00)\n",
      "Iter 9180 | Time 19.7731(20.1276) | Bit/dim 3.5736(3.5854) | Xent 0.2732(0.3068) | Loss 8.5700(8.8427) | Error 0.0956(0.1095) Steps 832(808.73) | Grad Norm 4.4124(4.2533) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 98.1740, Epoch Time 1226.0510(1167.9088), Bit/dim 3.5920(best: 3.5897), Xent 0.6605, Loss 3.9222, Error 0.2077(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 20.6108(20.1343) | Bit/dim 3.5933(3.5861) | Xent 0.3054(0.3041) | Loss 8.6521(9.4500) | Error 0.1056(0.1083) Steps 832(805.66) | Grad Norm 4.7577(4.3601) | Total Time 0.00(0.00)\n",
      "Iter 9200 | Time 20.6031(20.1648) | Bit/dim 3.6047(3.5851) | Xent 0.3008(0.3032) | Loss 8.6067(9.2394) | Error 0.0967(0.1066) Steps 796(809.08) | Grad Norm 4.3579(4.1592) | Total Time 0.00(0.00)\n",
      "Iter 9210 | Time 20.7408(20.1770) | Bit/dim 3.5833(3.5853) | Xent 0.2918(0.3005) | Loss 8.5831(9.0787) | Error 0.0989(0.1055) Steps 838(810.34) | Grad Norm 3.7941(4.0316) | Total Time 0.00(0.00)\n",
      "Iter 9220 | Time 20.0606(20.1563) | Bit/dim 3.6056(3.5862) | Xent 0.2526(0.3003) | Loss 8.5809(8.9670) | Error 0.0944(0.1050) Steps 796(810.27) | Grad Norm 3.5091(4.0027) | Total Time 0.00(0.00)\n",
      "Iter 9230 | Time 20.1675(20.1294) | Bit/dim 3.6073(3.5841) | Xent 0.3033(0.2992) | Loss 8.7327(8.8637) | Error 0.1033(0.1049) Steps 826(810.28) | Grad Norm 4.8380(3.9667) | Total Time 0.00(0.00)\n",
      "Iter 9240 | Time 19.7968(20.0900) | Bit/dim 3.6161(3.5866) | Xent 0.3072(0.3011) | Loss 8.6880(8.7976) | Error 0.1067(0.1052) Steps 796(808.16) | Grad Norm 5.0601(4.1054) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 97.4316, Epoch Time 1221.3434(1169.5118), Bit/dim 3.5941(best: 3.5897), Xent 0.6742, Loss 3.9312, Error 0.2071(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 18.7771(20.0352) | Bit/dim 3.6040(3.5866) | Xent 0.3703(0.3003) | Loss 8.6255(9.3447) | Error 0.1333(0.1057) Steps 790(805.97) | Grad Norm 5.8234(4.2103) | Total Time 0.00(0.00)\n",
      "Iter 9260 | Time 19.5417(20.0634) | Bit/dim 3.5982(3.5856) | Xent 0.3403(0.2999) | Loss 8.6008(9.1509) | Error 0.1211(0.1055) Steps 796(806.57) | Grad Norm 3.7224(4.2815) | Total Time 0.00(0.00)\n",
      "Iter 9270 | Time 19.7760(20.1112) | Bit/dim 3.5877(3.5845) | Xent 0.3011(0.2984) | Loss 8.7218(9.0205) | Error 0.1089(0.1050) Steps 856(810.58) | Grad Norm 4.2478(4.3185) | Total Time 0.00(0.00)\n",
      "Iter 9280 | Time 20.2895(20.0859) | Bit/dim 3.5828(3.5853) | Xent 0.2817(0.2995) | Loss 8.6595(8.9174) | Error 0.0978(0.1056) Steps 802(810.54) | Grad Norm 3.2493(4.2995) | Total Time 0.00(0.00)\n",
      "Iter 9290 | Time 20.1519(20.0985) | Bit/dim 3.6160(3.5857) | Xent 0.2858(0.3023) | Loss 8.6696(8.8413) | Error 0.1000(0.1065) Steps 826(808.92) | Grad Norm 5.4014(4.3448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 97.9912, Epoch Time 1220.2935(1171.0353), Bit/dim 3.5908(best: 3.5897), Xent 0.6727, Loss 3.9272, Error 0.2110(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 20.2772(20.1149) | Bit/dim 3.5529(3.5867) | Xent 0.2686(0.2965) | Loss 8.5367(9.4551) | Error 0.0922(0.1044) Steps 802(811.24) | Grad Norm 5.0710(4.4398) | Total Time 0.00(0.00)\n",
      "Iter 9310 | Time 21.4220(20.2149) | Bit/dim 3.6046(3.5864) | Xent 0.2703(0.2967) | Loss 8.6661(9.2352) | Error 0.0911(0.1039) Steps 808(808.98) | Grad Norm 2.6622(4.3526) | Total Time 0.00(0.00)\n",
      "Iter 9320 | Time 19.3522(20.2273) | Bit/dim 3.6235(3.5873) | Xent 0.3280(0.2968) | Loss 8.7225(9.0803) | Error 0.1056(0.1040) Steps 808(808.09) | Grad Norm 4.4167(4.3016) | Total Time 0.00(0.00)\n",
      "Iter 9330 | Time 20.3570(20.2633) | Bit/dim 3.5741(3.5863) | Xent 0.2831(0.2965) | Loss 8.4778(8.9633) | Error 0.1044(0.1044) Steps 832(808.81) | Grad Norm 5.2647(4.3179) | Total Time 0.00(0.00)\n",
      "Iter 9340 | Time 20.1108(20.2748) | Bit/dim 3.5632(3.5834) | Xent 0.2589(0.2986) | Loss 8.5606(8.8688) | Error 0.0844(0.1045) Steps 808(809.41) | Grad Norm 4.5578(4.2704) | Total Time 0.00(0.00)\n",
      "Iter 9350 | Time 20.9269(20.2866) | Bit/dim 3.6307(3.5864) | Xent 0.2544(0.3027) | Loss 8.7283(8.8202) | Error 0.0900(0.1048) Steps 862(812.78) | Grad Norm 5.4990(4.3482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 97.1190, Epoch Time 1233.9989(1172.9242), Bit/dim 3.5883(best: 3.5897), Xent 0.6643, Loss 3.9204, Error 0.2078(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 20.0667(20.2792) | Bit/dim 3.5806(3.5850) | Xent 0.3039(0.3004) | Loss 8.5299(9.3490) | Error 0.1122(0.1048) Steps 820(813.42) | Grad Norm 4.3354(4.3075) | Total Time 0.00(0.00)\n",
      "Iter 9370 | Time 20.5485(20.2784) | Bit/dim 3.5733(3.5849) | Xent 0.2555(0.3004) | Loss 8.5449(9.1579) | Error 0.0889(0.1052) Steps 784(811.53) | Grad Norm 2.9607(4.0677) | Total Time 0.00(0.00)\n",
      "Iter 9380 | Time 19.6408(20.2061) | Bit/dim 3.5928(3.5861) | Xent 0.3196(0.2982) | Loss 8.6908(9.0248) | Error 0.1122(0.1038) Steps 814(811.86) | Grad Norm 4.6954(4.2086) | Total Time 0.00(0.00)\n",
      "Iter 9390 | Time 20.0049(20.1601) | Bit/dim 3.5712(3.5848) | Xent 0.2829(0.2957) | Loss 8.5896(8.9144) | Error 0.1022(0.1040) Steps 808(809.65) | Grad Norm 3.3773(4.2409) | Total Time 0.00(0.00)\n",
      "Iter 9400 | Time 19.6145(20.1236) | Bit/dim 3.5781(3.5840) | Xent 0.3154(0.2961) | Loss 8.7252(8.8388) | Error 0.1122(0.1042) Steps 826(810.80) | Grad Norm 4.4058(4.3137) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 97.2047, Epoch Time 1222.4116(1174.4088), Bit/dim 3.5887(best: 3.5883), Xent 0.6691, Loss 3.9233, Error 0.2067(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9410 | Time 20.0599(20.1121) | Bit/dim 3.5906(3.5871) | Xent 0.2626(0.2961) | Loss 8.7320(9.4945) | Error 0.0922(0.1045) Steps 784(808.49) | Grad Norm 4.3499(4.1943) | Total Time 0.00(0.00)\n",
      "Iter 9420 | Time 19.7123(20.0990) | Bit/dim 3.5910(3.5866) | Xent 0.2742(0.2953) | Loss 8.4181(9.2586) | Error 0.0833(0.1041) Steps 814(810.83) | Grad Norm 4.0237(4.2017) | Total Time 0.00(0.00)\n",
      "Iter 9430 | Time 19.6847(20.0832) | Bit/dim 3.5381(3.5845) | Xent 0.3122(0.2942) | Loss 8.6791(9.0877) | Error 0.1044(0.1033) Steps 808(807.93) | Grad Norm 3.1556(4.2210) | Total Time 0.00(0.00)\n",
      "Iter 9440 | Time 20.3234(20.0907) | Bit/dim 3.5800(3.5853) | Xent 0.2798(0.2935) | Loss 8.5241(8.9642) | Error 0.0944(0.1033) Steps 838(811.03) | Grad Norm 5.3369(4.3099) | Total Time 0.00(0.00)\n",
      "Iter 9450 | Time 19.7071(20.0298) | Bit/dim 3.5730(3.5840) | Xent 0.3270(0.2978) | Loss 8.6215(8.8774) | Error 0.1089(0.1045) Steps 826(807.49) | Grad Norm 4.1221(4.5579) | Total Time 0.00(0.00)\n",
      "Iter 9460 | Time 20.1649(20.0905) | Bit/dim 3.6094(3.5826) | Xent 0.3406(0.2994) | Loss 8.7352(8.8175) | Error 0.1267(0.1059) Steps 826(811.68) | Grad Norm 6.6299(4.7040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 97.3093, Epoch Time 1217.6759(1175.7068), Bit/dim 3.5896(best: 3.5883), Xent 0.6717, Loss 3.9255, Error 0.2090(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9470 | Time 20.2511(20.0710) | Bit/dim 3.5937(3.5822) | Xent 0.2931(0.2984) | Loss 8.6060(9.3522) | Error 0.0956(0.1046) Steps 820(810.99) | Grad Norm 4.3846(4.5350) | Total Time 0.00(0.00)\n",
      "Iter 9480 | Time 19.3048(20.1115) | Bit/dim 3.5925(3.5856) | Xent 0.2996(0.2986) | Loss 8.6170(9.1731) | Error 0.1156(0.1047) Steps 772(808.93) | Grad Norm 3.9716(4.4125) | Total Time 0.00(0.00)\n",
      "Iter 9490 | Time 19.5998(20.0436) | Bit/dim 3.5908(3.5844) | Xent 0.2916(0.2977) | Loss 8.5779(9.0145) | Error 0.1067(0.1043) Steps 832(811.47) | Grad Norm 3.8394(4.4498) | Total Time 0.00(0.00)\n",
      "Iter 9500 | Time 20.5132(20.1499) | Bit/dim 3.5959(3.5848) | Xent 0.3063(0.2993) | Loss 8.6625(8.9161) | Error 0.1067(0.1047) Steps 778(812.81) | Grad Norm 3.5069(4.4393) | Total Time 0.00(0.00)\n",
      "Iter 9510 | Time 19.8390(20.1864) | Bit/dim 3.6167(3.5873) | Xent 0.3095(0.3002) | Loss 8.7431(8.8416) | Error 0.1056(0.1050) Steps 808(811.13) | Grad Norm 3.7890(4.3971) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 97.2447, Epoch Time 1225.6111(1177.2040), Bit/dim 3.5911(best: 3.5883), Xent 0.6765, Loss 3.9294, Error 0.2082(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9520 | Time 20.1937(20.1963) | Bit/dim 3.5805(3.5840) | Xent 0.2767(0.3010) | Loss 8.6933(9.4870) | Error 0.0956(0.1054) Steps 814(808.21) | Grad Norm 5.5988(4.4939) | Total Time 0.00(0.00)\n",
      "Iter 9530 | Time 20.0569(20.1844) | Bit/dim 3.5567(3.5818) | Xent 0.2767(0.2990) | Loss 8.6479(9.2630) | Error 0.0956(0.1051) Steps 814(807.92) | Grad Norm 3.6757(4.3262) | Total Time 0.00(0.00)\n",
      "Iter 9540 | Time 19.9034(20.1678) | Bit/dim 3.5983(3.5858) | Xent 0.3053(0.2988) | Loss 8.6941(9.1053) | Error 0.1111(0.1050) Steps 802(807.71) | Grad Norm 4.9042(4.2261) | Total Time 0.00(0.00)\n",
      "Iter 9550 | Time 20.8052(20.2062) | Bit/dim 3.5919(3.5874) | Xent 0.3207(0.2956) | Loss 8.6348(8.9714) | Error 0.1111(0.1026) Steps 802(805.95) | Grad Norm 5.9824(4.2799) | Total Time 0.00(0.00)\n",
      "Iter 9560 | Time 20.2450(20.1433) | Bit/dim 3.6036(3.5863) | Xent 0.2630(0.2981) | Loss 8.5831(8.8722) | Error 0.0978(0.1037) Steps 820(806.69) | Grad Norm 2.8042(4.4397) | Total Time 0.00(0.00)\n",
      "Iter 9570 | Time 20.4939(20.1463) | Bit/dim 3.6027(3.5868) | Xent 0.2970(0.2980) | Loss 8.7069(8.8133) | Error 0.1189(0.1036) Steps 832(808.92) | Grad Norm 4.5478(4.4849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 97.5559, Epoch Time 1222.8104(1178.5721), Bit/dim 3.5867(best: 3.5883), Xent 0.6806, Loss 3.9270, Error 0.2088(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9580 | Time 19.7168(20.0784) | Bit/dim 3.6232(3.5850) | Xent 0.2592(0.2979) | Loss 8.7227(9.3631) | Error 0.0889(0.1039) Steps 784(807.91) | Grad Norm 2.9687(4.3990) | Total Time 0.00(0.00)\n",
      "Iter 9590 | Time 19.7654(20.1293) | Bit/dim 3.5496(3.5866) | Xent 0.2859(0.2969) | Loss 8.4348(9.1707) | Error 0.0956(0.1037) Steps 832(809.24) | Grad Norm 3.4690(4.5127) | Total Time 0.00(0.00)\n",
      "Iter 9600 | Time 20.8860(20.1631) | Bit/dim 3.5657(3.5843) | Xent 0.2532(0.2958) | Loss 8.5510(9.0269) | Error 0.0878(0.1036) Steps 802(809.84) | Grad Norm 3.9709(4.6744) | Total Time 0.00(0.00)\n",
      "Iter 9610 | Time 20.3188(20.1893) | Bit/dim 3.5968(3.5863) | Xent 0.3000(0.2955) | Loss 8.6984(8.9297) | Error 0.0922(0.1037) Steps 826(808.98) | Grad Norm 6.8805(4.7793) | Total Time 0.00(0.00)\n",
      "Iter 9620 | Time 19.8814(20.1019) | Bit/dim 3.5660(3.5861) | Xent 0.2774(0.2970) | Loss 8.4956(8.8532) | Error 0.1000(0.1038) Steps 808(808.49) | Grad Norm 5.5345(5.0264) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 98.5195, Epoch Time 1222.8581(1179.9007), Bit/dim 3.5912(best: 3.5867), Xent 0.7004, Loss 3.9414, Error 0.2138(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9630 | Time 20.5427(20.1738) | Bit/dim 3.5942(3.5889) | Xent 0.3013(0.2997) | Loss 8.6611(9.5236) | Error 0.1156(0.1052) Steps 796(807.23) | Grad Norm 7.4489(5.3991) | Total Time 0.00(0.00)\n",
      "Iter 9640 | Time 19.9825(20.2660) | Bit/dim 3.5829(3.5881) | Xent 0.2856(0.2978) | Loss 8.7364(9.2950) | Error 0.0967(0.1044) Steps 808(808.69) | Grad Norm 5.0795(5.4406) | Total Time 0.00(0.00)\n",
      "Iter 9650 | Time 19.5321(20.2212) | Bit/dim 3.5914(3.5887) | Xent 0.3239(0.3008) | Loss 8.6072(9.1200) | Error 0.1056(0.1059) Steps 796(810.54) | Grad Norm 6.7686(5.4421) | Total Time 0.00(0.00)\n",
      "Iter 9660 | Time 19.5051(20.2001) | Bit/dim 3.6037(3.5869) | Xent 0.2938(0.2972) | Loss 8.6957(8.9872) | Error 0.1111(0.1050) Steps 796(810.37) | Grad Norm 2.9890(5.2176) | Total Time 0.00(0.00)\n",
      "Iter 9670 | Time 20.9126(20.1755) | Bit/dim 3.5513(3.5841) | Xent 0.2407(0.2965) | Loss 8.5321(8.8864) | Error 0.0778(0.1045) Steps 814(809.05) | Grad Norm 3.3962(5.0932) | Total Time 0.00(0.00)\n",
      "Iter 9680 | Time 19.5406(20.1064) | Bit/dim 3.5829(3.5862) | Xent 0.2900(0.2943) | Loss 8.4998(8.8192) | Error 0.1100(0.1033) Steps 796(805.84) | Grad Norm 6.1164(4.9917) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 98.4308, Epoch Time 1227.2933(1181.3225), Bit/dim 3.5877(best: 3.5867), Xent 0.6821, Loss 3.9287, Error 0.2062(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9690 | Time 19.0681(20.0965) | Bit/dim 3.6222(3.5879) | Xent 0.2983(0.2922) | Loss 8.5897(9.3756) | Error 0.1033(0.1021) Steps 778(807.33) | Grad Norm 5.7120(5.2172) | Total Time 0.00(0.00)\n",
      "Iter 9700 | Time 19.6839(20.0791) | Bit/dim 3.5817(3.5870) | Xent 0.3182(0.2940) | Loss 8.5743(9.1777) | Error 0.1178(0.1031) Steps 796(805.82) | Grad Norm 7.6084(5.3003) | Total Time 0.00(0.00)\n",
      "Iter 9710 | Time 19.9942(20.0808) | Bit/dim 3.5771(3.5878) | Xent 0.2748(0.2951) | Loss 8.7102(9.0468) | Error 0.1033(0.1035) Steps 844(806.40) | Grad Norm 3.5967(5.2080) | Total Time 0.00(0.00)\n",
      "Iter 9720 | Time 21.5360(20.1224) | Bit/dim 3.5632(3.5868) | Xent 0.2827(0.2919) | Loss 8.6134(8.9416) | Error 0.1011(0.1028) Steps 790(806.58) | Grad Norm 4.5782(4.9618) | Total Time 0.00(0.00)\n",
      "Iter 9730 | Time 20.3930(20.1565) | Bit/dim 3.5946(3.5852) | Xent 0.3160(0.2946) | Loss 8.7089(8.8647) | Error 0.1189(0.1043) Steps 832(809.01) | Grad Norm 4.5911(4.7780) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 97.7388, Epoch Time 1224.1924(1182.6086), Bit/dim 3.5921(best: 3.5867), Xent 0.6754, Loss 3.9298, Error 0.2053(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9740 | Time 20.3233(20.1878) | Bit/dim 3.5759(3.5851) | Xent 0.3020(0.2898) | Loss 8.4687(9.4863) | Error 0.1078(0.1031) Steps 808(810.84) | Grad Norm 3.0848(4.4435) | Total Time 0.00(0.00)\n",
      "Iter 9750 | Time 20.4065(20.2616) | Bit/dim 3.5683(3.5852) | Xent 0.2559(0.2885) | Loss 8.4409(9.2453) | Error 0.0844(0.1021) Steps 832(810.94) | Grad Norm 3.5253(4.2377) | Total Time 0.00(0.00)\n",
      "Iter 9760 | Time 20.8715(20.2834) | Bit/dim 3.6197(3.5886) | Xent 0.2995(0.2894) | Loss 8.7032(9.0958) | Error 0.1044(0.1022) Steps 796(811.33) | Grad Norm 4.0840(4.1831) | Total Time 0.00(0.00)\n",
      "Iter 9770 | Time 20.0478(20.2259) | Bit/dim 3.5995(3.5873) | Xent 0.3039(0.2877) | Loss 8.6802(8.9711) | Error 0.0978(0.1017) Steps 796(809.45) | Grad Norm 7.4442(4.1860) | Total Time 0.00(0.00)\n",
      "Iter 9780 | Time 20.7694(20.2783) | Bit/dim 3.5537(3.5826) | Xent 0.3384(0.2894) | Loss 8.6705(8.8724) | Error 0.1200(0.1023) Steps 790(808.63) | Grad Norm 4.2264(4.6545) | Total Time 0.00(0.00)\n",
      "Iter 9790 | Time 19.9079(20.1873) | Bit/dim 3.5850(3.5833) | Xent 0.3150(0.2948) | Loss 8.6440(8.8173) | Error 0.1189(0.1040) Steps 802(807.88) | Grad Norm 6.0522(4.8458) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 97.0327, Epoch Time 1228.3789(1183.9817), Bit/dim 3.5918(best: 3.5867), Xent 0.6821, Loss 3.9329, Error 0.2053(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9800 | Time 20.7047(20.1829) | Bit/dim 3.5890(3.5838) | Xent 0.2792(0.2938) | Loss 8.6920(9.3492) | Error 0.0967(0.1037) Steps 856(811.80) | Grad Norm 3.4674(4.9184) | Total Time 0.00(0.00)\n",
      "Iter 9810 | Time 20.0846(20.1537) | Bit/dim 3.5670(3.5844) | Xent 0.3042(0.2912) | Loss 8.5468(9.1549) | Error 0.1067(0.1029) Steps 832(810.33) | Grad Norm 4.8220(5.0851) | Total Time 0.00(0.00)\n",
      "Iter 9820 | Time 20.1526(20.1735) | Bit/dim 3.6138(3.5853) | Xent 0.2807(0.2909) | Loss 8.7762(9.0272) | Error 0.0956(0.1025) Steps 826(808.42) | Grad Norm 4.0664(4.9667) | Total Time 0.00(0.00)\n",
      "Iter 9830 | Time 20.0027(20.1375) | Bit/dim 3.5900(3.5862) | Xent 0.2831(0.2924) | Loss 8.5948(8.9230) | Error 0.1022(0.1029) Steps 784(810.07) | Grad Norm 4.4356(4.6594) | Total Time 0.00(0.00)\n",
      "Iter 9840 | Time 20.6734(20.1407) | Bit/dim 3.5865(3.5858) | Xent 0.3152(0.2924) | Loss 8.6165(8.8421) | Error 0.1144(0.1038) Steps 784(810.11) | Grad Norm 3.9445(4.4515) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 98.3707, Epoch Time 1223.1952(1185.1581), Bit/dim 3.5912(best: 3.5867), Xent 0.6785, Loss 3.9304, Error 0.2080(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9850 | Time 20.6609(20.1692) | Bit/dim 3.5771(3.5848) | Xent 0.3253(0.2886) | Loss 8.6158(9.4530) | Error 0.1178(0.1026) Steps 772(807.89) | Grad Norm 5.0550(4.3387) | Total Time 0.00(0.00)\n",
      "Iter 9860 | Time 19.7226(20.1005) | Bit/dim 3.6385(3.5860) | Xent 0.2665(0.2866) | Loss 8.4824(9.2323) | Error 0.0944(0.1018) Steps 832(808.86) | Grad Norm 3.5520(4.2978) | Total Time 0.00(0.00)\n",
      "Iter 9870 | Time 19.8859(20.0600) | Bit/dim 3.5810(3.5868) | Xent 0.2946(0.2878) | Loss 8.5833(9.0706) | Error 0.0967(0.1024) Steps 832(808.76) | Grad Norm 3.2366(4.1972) | Total Time 0.00(0.00)\n",
      "Iter 9880 | Time 19.8719(20.0701) | Bit/dim 3.5842(3.5848) | Xent 0.2733(0.2879) | Loss 8.4853(8.9459) | Error 0.0978(0.1025) Steps 796(809.52) | Grad Norm 3.7584(4.1145) | Total Time 0.00(0.00)\n",
      "Iter 9890 | Time 19.6728(20.1237) | Bit/dim 3.5751(3.5831) | Xent 0.2546(0.2891) | Loss 8.5738(8.8626) | Error 0.0933(0.1025) Steps 820(809.87) | Grad Norm 5.8149(4.3059) | Total Time 0.00(0.00)\n",
      "Iter 9900 | Time 20.8474(20.1532) | Bit/dim 3.5923(3.5828) | Xent 0.2707(0.2912) | Loss 8.6204(8.8029) | Error 0.1033(0.1030) Steps 814(811.25) | Grad Norm 4.7549(4.6847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 97.3506, Epoch Time 1221.6970(1186.2543), Bit/dim 3.5885(best: 3.5867), Xent 0.6783, Loss 3.9277, Error 0.2078(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9910 | Time 20.2625(20.2512) | Bit/dim 3.5479(3.5811) | Xent 0.2632(0.2911) | Loss 8.4859(9.3582) | Error 0.0878(0.1027) Steps 808(810.51) | Grad Norm 4.1348(4.8165) | Total Time 0.00(0.00)\n",
      "Iter 9920 | Time 20.4832(20.2969) | Bit/dim 3.5607(3.5825) | Xent 0.2368(0.2896) | Loss 8.5004(9.1764) | Error 0.0778(0.1022) Steps 790(811.16) | Grad Norm 7.8402(5.1626) | Total Time 0.00(0.00)\n",
      "Iter 9930 | Time 19.4193(20.2848) | Bit/dim 3.5695(3.5819) | Xent 0.3038(0.2902) | Loss 8.6358(9.0316) | Error 0.1011(0.1020) Steps 802(813.11) | Grad Norm 5.7455(5.1314) | Total Time 0.00(0.00)\n",
      "Iter 9940 | Time 20.0920(20.2361) | Bit/dim 3.6354(3.5835) | Xent 0.2509(0.2854) | Loss 8.6279(8.9111) | Error 0.0811(0.0997) Steps 814(812.26) | Grad Norm 3.6522(4.9546) | Total Time 0.00(0.00)\n",
      "Iter 9950 | Time 20.4344(20.1841) | Bit/dim 3.5604(3.5844) | Xent 0.3177(0.2924) | Loss 8.6593(8.8403) | Error 0.1122(0.1022) Steps 796(811.34) | Grad Norm 5.3183(4.8881) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 97.3738, Epoch Time 1230.9516(1187.5952), Bit/dim 3.5905(best: 3.5867), Xent 0.6868, Loss 3.9339, Error 0.2059(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9960 | Time 20.9618(20.2235) | Bit/dim 3.5849(3.5853) | Xent 0.3050(0.2919) | Loss 8.5905(9.4854) | Error 0.1022(0.1022) Steps 808(807.88) | Grad Norm 4.2182(5.0064) | Total Time 0.00(0.00)\n",
      "Iter 9970 | Time 19.5520(20.1721) | Bit/dim 3.5669(3.5866) | Xent 0.2830(0.2895) | Loss 8.6777(9.2643) | Error 0.0989(0.1016) Steps 808(810.89) | Grad Norm 6.0876(4.9581) | Total Time 0.00(0.00)\n",
      "Iter 9980 | Time 20.1209(20.1274) | Bit/dim 3.5877(3.5867) | Xent 0.2532(0.2860) | Loss 8.7120(9.1019) | Error 0.0956(0.1008) Steps 826(813.00) | Grad Norm 7.6520(5.1160) | Total Time 0.00(0.00)\n",
      "Iter 9990 | Time 20.4039(20.0745) | Bit/dim 3.5766(3.5873) | Xent 0.2955(0.2882) | Loss 8.6841(8.9666) | Error 0.0856(0.1018) Steps 814(812.67) | Grad Norm 4.2090(5.2195) | Total Time 0.00(0.00)\n",
      "Iter 10000 | Time 20.7866(20.1982) | Bit/dim 3.6018(3.5871) | Xent 0.3108(0.2900) | Loss 8.8031(8.8904) | Error 0.1111(0.1024) Steps 832(816.19) | Grad Norm 5.4153(5.1978) | Total Time 0.00(0.00)\n",
      "Iter 10010 | Time 21.3310(20.2151) | Bit/dim 3.5680(3.5859) | Xent 0.2840(0.2893) | Loss 8.6841(8.8230) | Error 0.0956(0.1021) Steps 826(813.01) | Grad Norm 3.9365(5.0363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 98.5598, Epoch Time 1225.6989(1188.7383), Bit/dim 3.5879(best: 3.5867), Xent 0.6930, Loss 3.9344, Error 0.2070(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10020 | Time 20.3273(20.1789) | Bit/dim 3.5555(3.5856) | Xent 0.2689(0.2872) | Loss 8.6046(9.3944) | Error 0.0967(0.1014) Steps 808(813.27) | Grad Norm 3.9318(4.9269) | Total Time 0.00(0.00)\n",
      "Iter 10030 | Time 20.5567(20.2746) | Bit/dim 3.5983(3.5869) | Xent 0.3140(0.2879) | Loss 8.6370(9.1990) | Error 0.1067(0.1011) Steps 802(811.33) | Grad Norm 4.8522(4.7079) | Total Time 0.00(0.00)\n",
      "Iter 10040 | Time 19.4854(20.2325) | Bit/dim 3.5828(3.5869) | Xent 0.3022(0.2909) | Loss 8.5883(9.0481) | Error 0.1011(0.1024) Steps 802(812.99) | Grad Norm 4.0958(4.7048) | Total Time 0.00(0.00)\n",
      "Iter 10050 | Time 20.5700(20.2508) | Bit/dim 3.5787(3.5852) | Xent 0.3437(0.2936) | Loss 8.5727(8.9328) | Error 0.1100(0.1028) Steps 796(811.33) | Grad Norm 8.3553(5.1128) | Total Time 0.00(0.00)\n",
      "Iter 10060 | Time 19.9838(20.2844) | Bit/dim 3.5795(3.5871) | Xent 0.2622(0.2895) | Loss 8.3824(8.8473) | Error 0.0900(0.1016) Steps 826(814.11) | Grad Norm 4.0474(4.9796) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 98.1803, Epoch Time 1231.8642(1190.0321), Bit/dim 3.5912(best: 3.5867), Xent 0.6979, Loss 3.9401, Error 0.2072(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10070 | Time 20.7244(20.2865) | Bit/dim 3.5690(3.5869) | Xent 0.2641(0.2897) | Loss 8.5864(9.4843) | Error 0.0856(0.1021) Steps 796(811.37) | Grad Norm 3.7105(4.7823) | Total Time 0.00(0.00)\n",
      "Iter 10080 | Time 19.5812(20.2293) | Bit/dim 3.5876(3.5869) | Xent 0.2959(0.2916) | Loss 8.5843(9.2518) | Error 0.1000(0.1026) Steps 778(808.29) | Grad Norm 4.2863(4.5968) | Total Time 0.00(0.00)\n",
      "Iter 10090 | Time 21.8676(20.3293) | Bit/dim 3.6002(3.5880) | Xent 0.2737(0.2846) | Loss 8.7358(9.0973) | Error 0.0900(0.1000) Steps 880(812.59) | Grad Norm 4.9836(4.5768) | Total Time 0.00(0.00)\n",
      "Iter 10100 | Time 19.8024(20.2996) | Bit/dim 3.5639(3.5859) | Xent 0.2787(0.2869) | Loss 8.6372(8.9726) | Error 0.0978(0.1006) Steps 802(814.16) | Grad Norm 4.9218(4.5369) | Total Time 0.00(0.00)\n",
      "Iter 10110 | Time 20.8137(20.3738) | Bit/dim 3.5930(3.5859) | Xent 0.2901(0.2887) | Loss 8.6214(8.8826) | Error 0.0967(0.1018) Steps 826(815.21) | Grad Norm 3.6423(4.6084) | Total Time 0.00(0.00)\n",
      "Iter 10120 | Time 20.1529(20.3468) | Bit/dim 3.5469(3.5831) | Xent 0.2811(0.2860) | Loss 8.6287(8.8180) | Error 0.0944(0.1012) Steps 796(814.94) | Grad Norm 4.4751(4.4927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 98.6396, Epoch Time 1234.8365(1191.3762), Bit/dim 3.5912(best: 3.5867), Xent 0.6835, Loss 3.9330, Error 0.2077(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10130 | Time 20.8017(20.3087) | Bit/dim 3.5646(3.5855) | Xent 0.3208(0.2865) | Loss 8.6048(9.3674) | Error 0.1144(0.1009) Steps 838(812.82) | Grad Norm 4.4513(4.5307) | Total Time 0.00(0.00)\n",
      "Iter 10140 | Time 20.9141(20.3539) | Bit/dim 3.6004(3.5887) | Xent 0.2706(0.2853) | Loss 8.6712(9.1747) | Error 0.1044(0.1007) Steps 832(813.36) | Grad Norm 4.9891(4.7118) | Total Time 0.00(0.00)\n",
      "Iter 10150 | Time 19.7112(20.2801) | Bit/dim 3.5848(3.5874) | Xent 0.2552(0.2835) | Loss 8.5672(9.0175) | Error 0.0922(0.1002) Steps 826(810.15) | Grad Norm 4.8579(4.9850) | Total Time 0.00(0.00)\n",
      "Iter 10160 | Time 21.4563(20.3105) | Bit/dim 3.5829(3.5841) | Xent 0.2909(0.2860) | Loss 8.7553(8.9196) | Error 0.0956(0.1011) Steps 814(811.78) | Grad Norm 6.3007(5.0233) | Total Time 0.00(0.00)\n",
      "Iter 10170 | Time 20.6339(20.2636) | Bit/dim 3.6055(3.5828) | Xent 0.3086(0.2873) | Loss 8.7130(8.8279) | Error 0.1111(0.1024) Steps 826(809.05) | Grad Norm 5.6929(4.8627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 98.4516, Epoch Time 1231.3476(1192.5754), Bit/dim 3.5876(best: 3.5867), Xent 0.6927, Loss 3.9339, Error 0.2094(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10180 | Time 19.9990(20.3273) | Bit/dim 3.5918(3.5825) | Xent 0.2805(0.2844) | Loss 8.6225(9.4798) | Error 0.0989(0.1015) Steps 808(809.92) | Grad Norm 3.1879(4.7990) | Total Time 0.00(0.00)\n",
      "Iter 10190 | Time 20.4673(20.3305) | Bit/dim 3.5981(3.5816) | Xent 0.2508(0.2885) | Loss 8.6892(9.2581) | Error 0.0911(0.1018) Steps 844(811.02) | Grad Norm 3.6083(4.8750) | Total Time 0.00(0.00)\n",
      "Iter 10200 | Time 20.7857(20.3869) | Bit/dim 3.5724(3.5837) | Xent 0.2974(0.2893) | Loss 8.5866(9.0953) | Error 0.1011(0.1019) Steps 802(811.44) | Grad Norm 5.1452(5.0733) | Total Time 0.00(0.00)\n",
      "Iter 10210 | Time 20.1491(20.3606) | Bit/dim 3.5729(3.5828) | Xent 0.2926(0.2888) | Loss 8.7181(8.9711) | Error 0.1022(0.1022) Steps 826(813.86) | Grad Norm 3.4234(4.8453) | Total Time 0.00(0.00)\n",
      "Iter 10220 | Time 21.0656(20.4241) | Bit/dim 3.5759(3.5832) | Xent 0.2601(0.2875) | Loss 8.6497(8.8948) | Error 0.0900(0.1013) Steps 802(815.44) | Grad Norm 4.9279(4.7362) | Total Time 0.00(0.00)\n",
      "Iter 10230 | Time 20.5899(20.4628) | Bit/dim 3.5809(3.5840) | Xent 0.2570(0.2875) | Loss 8.5569(8.8211) | Error 0.0878(0.1006) Steps 778(815.34) | Grad Norm 4.2478(4.7618) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 98.4478, Epoch Time 1242.7842(1194.0816), Bit/dim 3.5911(best: 3.5867), Xent 0.6999, Loss 3.9410, Error 0.2082(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10240 | Time 20.9463(20.4828) | Bit/dim 3.5703(3.5840) | Xent 0.2743(0.2847) | Loss 8.5097(9.3613) | Error 0.1000(0.1003) Steps 808(815.13) | Grad Norm 4.0201(4.6300) | Total Time 0.00(0.00)\n",
      "Iter 10250 | Time 20.4882(20.4728) | Bit/dim 3.5996(3.5844) | Xent 0.3134(0.2851) | Loss 8.6510(9.1663) | Error 0.1033(0.1000) Steps 826(813.80) | Grad Norm 5.5979(4.8393) | Total Time 0.00(0.00)\n",
      "Iter 10260 | Time 19.8376(20.4294) | Bit/dim 3.5742(3.5841) | Xent 0.3063(0.2834) | Loss 8.6173(9.0203) | Error 0.1011(0.0990) Steps 826(812.19) | Grad Norm 4.6508(4.7957) | Total Time 0.00(0.00)\n",
      "Iter 10270 | Time 20.6563(20.4714) | Bit/dim 3.5899(3.5822) | Xent 0.2859(0.2835) | Loss 8.6535(8.9146) | Error 0.1067(0.0990) Steps 844(813.33) | Grad Norm 4.6128(4.8124) | Total Time 0.00(0.00)\n",
      "Iter 10280 | Time 20.4882(20.4260) | Bit/dim 3.5730(3.5840) | Xent 0.3093(0.2837) | Loss 8.6728(8.8359) | Error 0.1144(0.0989) Steps 826(814.39) | Grad Norm 7.9829(4.8709) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 98.8359, Epoch Time 1238.2611(1195.4070), Bit/dim 3.5883(best: 3.5867), Xent 0.6997, Loss 3.9382, Error 0.2084(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10290 | Time 19.7101(20.3554) | Bit/dim 3.6119(3.5833) | Xent 0.2922(0.2829) | Loss 8.7300(9.4826) | Error 0.1133(0.0988) Steps 796(813.69) | Grad Norm 3.9199(4.8927) | Total Time 0.00(0.00)\n",
      "Iter 10300 | Time 20.7291(20.3785) | Bit/dim 3.5904(3.5841) | Xent 0.2801(0.2822) | Loss 8.6934(9.2622) | Error 0.0900(0.0978) Steps 862(816.53) | Grad Norm 3.2456(4.8662) | Total Time 0.00(0.00)\n",
      "Iter 10310 | Time 19.6900(20.3649) | Bit/dim 3.5313(3.5828) | Xent 0.2651(0.2817) | Loss 8.3998(9.0836) | Error 0.0978(0.0982) Steps 820(814.18) | Grad Norm 3.2210(4.6553) | Total Time 0.00(0.00)\n",
      "Iter 10320 | Time 20.7106(20.3277) | Bit/dim 3.5866(3.5861) | Xent 0.3179(0.2834) | Loss 8.6130(8.9733) | Error 0.1122(0.0984) Steps 814(815.87) | Grad Norm 4.9782(4.8555) | Total Time 0.00(0.00)\n",
      "Iter 10330 | Time 19.1868(20.3453) | Bit/dim 3.5957(3.5854) | Xent 0.3153(0.2855) | Loss 8.6403(8.8850) | Error 0.1178(0.1005) Steps 802(816.42) | Grad Norm 8.6683(5.0481) | Total Time 0.00(0.00)\n",
      "Iter 10340 | Time 20.1018(20.2812) | Bit/dim 3.5271(3.5826) | Xent 0.2735(0.2875) | Loss 8.4106(8.8023) | Error 0.0967(0.1018) Steps 796(814.29) | Grad Norm 3.5277(4.9583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 98.3457, Epoch Time 1232.5870(1196.5224), Bit/dim 3.5917(best: 3.5867), Xent 0.7045, Loss 3.9440, Error 0.2106(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10350 | Time 19.6783(20.1887) | Bit/dim 3.5726(3.5845) | Xent 0.3126(0.2873) | Loss 8.6151(9.3423) | Error 0.1111(0.1016) Steps 802(812.04) | Grad Norm 4.5545(4.8738) | Total Time 0.00(0.00)\n",
      "Iter 10360 | Time 19.7940(20.2041) | Bit/dim 3.5649(3.5861) | Xent 0.2882(0.2865) | Loss 8.5578(9.1554) | Error 0.0944(0.1015) Steps 808(808.92) | Grad Norm 4.5905(4.6291) | Total Time 0.00(0.00)\n",
      "Iter 10370 | Time 20.9853(20.2340) | Bit/dim 3.5973(3.5863) | Xent 0.2876(0.2831) | Loss 8.5884(9.0118) | Error 0.0978(0.1000) Steps 796(809.04) | Grad Norm 6.3887(4.5961) | Total Time 0.00(0.00)\n",
      "Iter 10380 | Time 19.6650(20.2440) | Bit/dim 3.5805(3.5863) | Xent 0.2844(0.2838) | Loss 8.5673(8.9115) | Error 0.1067(0.1003) Steps 820(809.55) | Grad Norm 6.5629(4.9747) | Total Time 0.00(0.00)\n",
      "Iter 10390 | Time 21.5064(20.3315) | Bit/dim 3.5394(3.5816) | Xent 0.3016(0.2842) | Loss 8.5706(8.8272) | Error 0.1144(0.1010) Steps 844(810.05) | Grad Norm 3.4994(5.2446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 98.1391, Epoch Time 1232.8087(1197.6110), Bit/dim 3.5867(best: 3.5867), Xent 0.6906, Loss 3.9320, Error 0.2104(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10400 | Time 21.2774(20.4639) | Bit/dim 3.5839(3.5829) | Xent 0.2793(0.2826) | Loss 8.6623(9.4893) | Error 0.1022(0.1006) Steps 802(811.88) | Grad Norm 4.1638(5.2308) | Total Time 0.00(0.00)\n",
      "Iter 10410 | Time 20.2952(20.4001) | Bit/dim 3.5788(3.5833) | Xent 0.2465(0.2813) | Loss 8.5979(9.2653) | Error 0.0800(0.0994) Steps 796(810.13) | Grad Norm 3.5419(5.0823) | Total Time 0.00(0.00)\n",
      "Iter 10420 | Time 20.1675(20.3960) | Bit/dim 3.5967(3.5870) | Xent 0.2943(0.2789) | Loss 8.6732(9.1030) | Error 0.1044(0.0984) Steps 832(811.29) | Grad Norm 4.1366(5.0347) | Total Time 0.00(0.00)\n",
      "Iter 10430 | Time 20.5227(20.3286) | Bit/dim 3.5779(3.5858) | Xent 0.2554(0.2791) | Loss 8.6150(8.9751) | Error 0.0900(0.0987) Steps 826(816.09) | Grad Norm 3.9871(5.0215) | Total Time 0.00(0.00)\n",
      "Iter 10440 | Time 20.2943(20.3638) | Bit/dim 3.5733(3.5845) | Xent 0.2679(0.2831) | Loss 8.5771(8.8701) | Error 0.0967(0.1003) Steps 820(817.25) | Grad Norm 6.9224(5.3908) | Total Time 0.00(0.00)\n",
      "Iter 10450 | Time 20.4863(20.3597) | Bit/dim 3.5502(3.5815) | Xent 0.3011(0.2870) | Loss 8.6631(8.8181) | Error 0.1044(0.1018) Steps 832(820.84) | Grad Norm 6.0410(5.5349) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 98.3291, Epoch Time 1236.8389(1198.7878), Bit/dim 3.5926(best: 3.5867), Xent 0.7030, Loss 3.9440, Error 0.2087(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10460 | Time 20.0643(20.2945) | Bit/dim 3.5771(3.5801) | Xent 0.2992(0.2844) | Loss 8.6670(9.3426) | Error 0.1022(0.1008) Steps 820(818.49) | Grad Norm 5.5626(5.4374) | Total Time 0.00(0.00)\n",
      "Iter 10470 | Time 20.5232(20.2447) | Bit/dim 3.6109(3.5837) | Xent 0.2630(0.2835) | Loss 8.8062(9.1667) | Error 0.0900(0.1004) Steps 838(817.20) | Grad Norm 4.3722(5.1819) | Total Time 0.00(0.00)\n",
      "Iter 10480 | Time 20.4459(20.3358) | Bit/dim 3.5459(3.5831) | Xent 0.2468(0.2850) | Loss 8.5998(9.0269) | Error 0.0844(0.1009) Steps 832(818.34) | Grad Norm 3.3630(4.8886) | Total Time 0.00(0.00)\n",
      "Iter 10490 | Time 19.8456(20.3164) | Bit/dim 3.5469(3.5833) | Xent 0.2490(0.2811) | Loss 8.4979(8.9089) | Error 0.0889(0.0993) Steps 832(820.00) | Grad Norm 6.4453(4.7480) | Total Time 0.00(0.00)\n",
      "Iter 10500 | Time 20.4027(20.2542) | Bit/dim 3.5820(3.5821) | Xent 0.2584(0.2809) | Loss 8.6658(8.8276) | Error 0.0856(0.0984) Steps 862(820.72) | Grad Norm 3.9386(4.8003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 99.1213, Epoch Time 1230.0561(1199.7259), Bit/dim 3.5886(best: 3.5867), Xent 0.6871, Loss 3.9322, Error 0.2072(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10510 | Time 20.2296(20.2131) | Bit/dim 3.5740(3.5829) | Xent 0.2310(0.2760) | Loss 8.6134(9.4933) | Error 0.0844(0.0970) Steps 838(820.29) | Grad Norm 3.4493(4.6199) | Total Time 0.00(0.00)\n",
      "Iter 10520 | Time 20.3571(20.2803) | Bit/dim 3.6181(3.5858) | Xent 0.2795(0.2781) | Loss 8.6351(9.2703) | Error 0.0933(0.0977) Steps 838(820.72) | Grad Norm 5.2265(4.6570) | Total Time 0.00(0.00)\n",
      "Iter 10530 | Time 20.3072(20.3520) | Bit/dim 3.5672(3.5857) | Xent 0.2702(0.2781) | Loss 8.5028(9.1000) | Error 0.0967(0.0975) Steps 832(818.73) | Grad Norm 4.2428(4.7237) | Total Time 0.00(0.00)\n",
      "Iter 10540 | Time 20.1781(20.3542) | Bit/dim 3.5917(3.5851) | Xent 0.2368(0.2793) | Loss 8.6636(8.9776) | Error 0.0767(0.0974) Steps 808(818.53) | Grad Norm 3.8300(5.2193) | Total Time 0.00(0.00)\n",
      "Iter 10550 | Time 19.7080(20.3391) | Bit/dim 3.5767(3.5831) | Xent 0.2834(0.2810) | Loss 8.6088(8.8788) | Error 0.1067(0.0980) Steps 796(816.60) | Grad Norm 5.4957(5.4460) | Total Time 0.00(0.00)\n",
      "Iter 10560 | Time 19.5445(20.2826) | Bit/dim 3.6171(3.5855) | Xent 0.2405(0.2825) | Loss 8.5597(8.8127) | Error 0.0767(0.0978) Steps 772(812.98) | Grad Norm 3.9295(5.2817) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 97.8419, Epoch Time 1233.6164(1200.7426), Bit/dim 3.5894(best: 3.5867), Xent 0.7116, Loss 3.9452, Error 0.2119(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10570 | Time 19.9962(20.3540) | Bit/dim 3.5791(3.5834) | Xent 0.2508(0.2825) | Loss 8.6356(9.3541) | Error 0.0922(0.0983) Steps 826(815.51) | Grad Norm 8.1480(5.5760) | Total Time 0.00(0.00)\n",
      "Iter 10580 | Time 20.4100(20.4008) | Bit/dim 3.5899(3.5839) | Xent 0.2548(0.2799) | Loss 8.7159(9.1645) | Error 0.0867(0.0978) Steps 796(813.50) | Grad Norm 7.2381(5.9231) | Total Time 0.00(0.00)\n",
      "Iter 10590 | Time 21.8632(20.4408) | Bit/dim 3.5747(3.5846) | Xent 0.2928(0.2835) | Loss 8.5816(9.0171) | Error 0.0978(0.0991) Steps 820(812.81) | Grad Norm 7.2951(6.6774) | Total Time 0.00(0.00)\n",
      "Iter 10600 | Time 20.3481(20.4987) | Bit/dim 3.6042(3.5865) | Xent 0.2837(0.2855) | Loss 8.5132(8.9179) | Error 0.1000(0.1000) Steps 754(814.60) | Grad Norm 5.4847(6.6447) | Total Time 0.00(0.00)\n",
      "Iter 10610 | Time 20.3014(20.4900) | Bit/dim 3.6005(3.5862) | Xent 0.3022(0.2883) | Loss 8.8162(8.8561) | Error 0.0989(0.0998) Steps 844(815.74) | Grad Norm 6.4343(6.3580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 99.5145, Epoch Time 1247.5992(1202.1483), Bit/dim 3.5907(best: 3.5867), Xent 0.6983, Loss 3.9398, Error 0.2088(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10620 | Time 20.0955(20.4866) | Bit/dim 3.5792(3.5839) | Xent 0.2656(0.2846) | Loss 8.6736(9.5056) | Error 0.1044(0.0998) Steps 838(817.81) | Grad Norm 4.5363(6.0795) | Total Time 0.00(0.00)\n",
      "Iter 10630 | Time 20.8057(20.4746) | Bit/dim 3.6194(3.5852) | Xent 0.2765(0.2816) | Loss 8.5712(9.2777) | Error 0.1000(0.0982) Steps 778(815.63) | Grad Norm 3.3552(5.5620) | Total Time 0.00(0.00)\n",
      "Iter 10640 | Time 20.6939(20.4554) | Bit/dim 3.5807(3.5818) | Xent 0.2942(0.2821) | Loss 8.6264(9.0957) | Error 0.1022(0.0983) Steps 832(816.80) | Grad Norm 4.2720(5.3059) | Total Time 0.00(0.00)\n",
      "Iter 10650 | Time 19.5440(20.3820) | Bit/dim 3.5935(3.5832) | Xent 0.2789(0.2837) | Loss 8.4579(8.9644) | Error 0.1033(0.0992) Steps 820(817.66) | Grad Norm 5.5791(5.5040) | Total Time 0.00(0.00)\n",
      "Iter 10660 | Time 19.6255(20.3020) | Bit/dim 3.6109(3.5827) | Xent 0.2714(0.2794) | Loss 8.7230(8.8776) | Error 0.0956(0.0982) Steps 802(815.51) | Grad Norm 3.4351(5.5597) | Total Time 0.00(0.00)\n",
      "Iter 10670 | Time 19.5837(20.2044) | Bit/dim 3.5677(3.5852) | Xent 0.2768(0.2785) | Loss 8.5586(8.8080) | Error 0.0889(0.0975) Steps 814(812.62) | Grad Norm 6.1779(5.4728) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 97.4283, Epoch Time 1228.7464(1202.9462), Bit/dim 3.5901(best: 3.5867), Xent 0.7027, Loss 3.9414, Error 0.2089(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10680 | Time 21.5331(20.2334) | Bit/dim 3.5924(3.5876) | Xent 0.2860(0.2735) | Loss 8.7757(9.3732) | Error 0.0978(0.0954) Steps 832(813.33) | Grad Norm 5.2901(5.4270) | Total Time 0.00(0.00)\n",
      "Iter 10690 | Time 20.2487(20.2705) | Bit/dim 3.5780(3.5825) | Xent 0.2596(0.2752) | Loss 8.5980(9.1804) | Error 0.0922(0.0968) Steps 820(815.91) | Grad Norm 4.3388(5.5328) | Total Time 0.00(0.00)\n",
      "Iter 10700 | Time 20.6539(20.3282) | Bit/dim 3.6095(3.5839) | Xent 0.3136(0.2761) | Loss 8.7106(9.0341) | Error 0.1044(0.0968) Steps 820(816.87) | Grad Norm 4.5301(5.3607) | Total Time 0.00(0.00)\n",
      "Iter 10710 | Time 20.9356(20.3772) | Bit/dim 3.6056(3.5853) | Xent 0.2924(0.2774) | Loss 8.7321(8.9326) | Error 0.0911(0.0959) Steps 832(817.95) | Grad Norm 5.0241(5.3105) | Total Time 0.00(0.00)\n",
      "Iter 10720 | Time 21.3649(20.4487) | Bit/dim 3.6019(3.5845) | Xent 0.3022(0.2792) | Loss 8.7426(8.8671) | Error 0.1111(0.0961) Steps 820(816.03) | Grad Norm 4.9742(5.1691) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 98.8427, Epoch Time 1241.9452(1204.1162), Bit/dim 3.5892(best: 3.5867), Xent 0.6998, Loss 3.9391, Error 0.2094(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10730 | Time 20.7820(20.4646) | Bit/dim 3.5863(3.5828) | Xent 0.2710(0.2787) | Loss 8.6045(9.5141) | Error 0.1011(0.0963) Steps 814(817.35) | Grad Norm 4.4182(5.1716) | Total Time 0.00(0.00)\n",
      "Iter 10740 | Time 20.4671(20.4564) | Bit/dim 3.6144(3.5841) | Xent 0.2372(0.2769) | Loss 8.6045(9.2669) | Error 0.0800(0.0953) Steps 844(816.99) | Grad Norm 6.2209(5.2834) | Total Time 0.00(0.00)\n",
      "Iter 10750 | Time 21.1507(20.4860) | Bit/dim 3.5829(3.5866) | Xent 0.2795(0.2761) | Loss 8.6573(9.0981) | Error 0.1044(0.0957) Steps 820(818.07) | Grad Norm 8.0301(5.7102) | Total Time 0.00(0.00)\n",
      "Iter 10760 | Time 20.3965(20.4739) | Bit/dim 3.5777(3.5837) | Xent 0.2868(0.2806) | Loss 8.7084(8.9716) | Error 0.1044(0.0977) Steps 844(819.25) | Grad Norm 4.2821(5.8521) | Total Time 0.00(0.00)\n",
      "Iter 10770 | Time 20.8418(20.5087) | Bit/dim 3.5764(3.5851) | Xent 0.2457(0.2761) | Loss 8.6454(8.8824) | Error 0.0822(0.0965) Steps 820(819.07) | Grad Norm 5.1740(5.5891) | Total Time 0.00(0.00)\n",
      "Iter 10780 | Time 19.8126(20.4857) | Bit/dim 3.5670(3.5833) | Xent 0.2788(0.2770) | Loss 8.6103(8.8109) | Error 0.0989(0.0973) Steps 790(816.53) | Grad Norm 4.0756(5.4781) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 98.0016, Epoch Time 1243.8057(1205.3069), Bit/dim 3.5922(best: 3.5867), Xent 0.7027, Loss 3.9435, Error 0.2108(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10790 | Time 20.6927(20.4541) | Bit/dim 3.5744(3.5818) | Xent 0.2524(0.2764) | Loss 8.5167(9.3489) | Error 0.0900(0.0969) Steps 838(814.19) | Grad Norm 5.5819(5.5446) | Total Time 0.00(0.00)\n",
      "Iter 10800 | Time 20.5168(20.4325) | Bit/dim 3.5820(3.5817) | Xent 0.2832(0.2764) | Loss 8.6927(9.1625) | Error 0.0878(0.0968) Steps 796(814.83) | Grad Norm 3.2365(5.4820) | Total Time 0.00(0.00)\n",
      "Iter 10810 | Time 20.5867(20.4647) | Bit/dim 3.5647(3.5849) | Xent 0.2391(0.2760) | Loss 8.4954(9.0274) | Error 0.0811(0.0963) Steps 826(816.39) | Grad Norm 3.7205(5.7282) | Total Time 0.00(0.00)\n",
      "Iter 10820 | Time 20.2638(20.4106) | Bit/dim 3.5602(3.5839) | Xent 0.2920(0.2768) | Loss 8.5749(8.9111) | Error 0.1056(0.0966) Steps 826(815.72) | Grad Norm 5.7273(5.7750) | Total Time 0.00(0.00)\n",
      "Iter 10830 | Time 20.6932(20.4209) | Bit/dim 3.5679(3.5838) | Xent 0.2821(0.2783) | Loss 8.7110(8.8408) | Error 0.0878(0.0967) Steps 808(813.49) | Grad Norm 5.4851(5.8944) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 99.1379, Epoch Time 1239.8055(1206.3419), Bit/dim 3.5873(best: 3.5867), Xent 0.6966, Loss 3.9357, Error 0.2093(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10840 | Time 20.0120(20.4334) | Bit/dim 3.5583(3.5839) | Xent 0.2596(0.2819) | Loss 8.5424(9.4650) | Error 0.0800(0.0982) Steps 832(814.09) | Grad Norm 5.6872(5.7815) | Total Time 0.00(0.00)\n",
      "Iter 10850 | Time 20.3083(20.4807) | Bit/dim 3.6374(3.5824) | Xent 0.2975(0.2827) | Loss 8.7497(9.2484) | Error 0.0989(0.0979) Steps 844(816.24) | Grad Norm 6.9468(5.8767) | Total Time 0.00(0.00)\n",
      "Iter 10860 | Time 20.3079(20.4624) | Bit/dim 3.5821(3.5857) | Xent 0.2710(0.2839) | Loss 8.6047(9.0947) | Error 0.0956(0.0994) Steps 802(818.46) | Grad Norm 4.9195(6.2910) | Total Time 0.00(0.00)\n",
      "Iter 10870 | Time 20.6404(20.4657) | Bit/dim 3.5957(3.5849) | Xent 0.2707(0.2822) | Loss 8.7704(8.9689) | Error 0.1022(0.0995) Steps 844(818.80) | Grad Norm 4.9233(6.2170) | Total Time 0.00(0.00)\n",
      "Iter 10880 | Time 20.7753(20.5883) | Bit/dim 3.6049(3.5866) | Xent 0.2403(0.2822) | Loss 8.6098(8.8901) | Error 0.0844(0.0991) Steps 868(823.83) | Grad Norm 3.9827(6.0160) | Total Time 0.00(0.00)\n",
      "Iter 10890 | Time 19.7650(20.5177) | Bit/dim 3.5849(3.5833) | Xent 0.2574(0.2774) | Loss 8.5728(8.8199) | Error 0.0944(0.0978) Steps 826(823.81) | Grad Norm 4.5700(5.5847) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 98.4993, Epoch Time 1245.3453(1207.5120), Bit/dim 3.5913(best: 3.5867), Xent 0.7112, Loss 3.9469, Error 0.2071(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10900 | Time 19.6535(20.4664) | Bit/dim 3.6023(3.5832) | Xent 0.2393(0.2763) | Loss 8.5643(9.3646) | Error 0.0889(0.0967) Steps 820(821.33) | Grad Norm 3.8905(5.2777) | Total Time 0.00(0.00)\n",
      "Iter 10910 | Time 20.6549(20.4951) | Bit/dim 3.6043(3.5846) | Xent 0.2795(0.2740) | Loss 8.5777(9.1625) | Error 0.1022(0.0956) Steps 820(817.60) | Grad Norm 5.3358(5.1660) | Total Time 0.00(0.00)\n",
      "Iter 10920 | Time 20.4246(20.5138) | Bit/dim 3.5549(3.5850) | Xent 0.2323(0.2731) | Loss 8.5873(9.0295) | Error 0.0822(0.0959) Steps 844(820.25) | Grad Norm 5.7611(4.9922) | Total Time 0.00(0.00)\n",
      "Iter 10930 | Time 20.1979(20.4984) | Bit/dim 3.5871(3.5851) | Xent 0.2597(0.2718) | Loss 8.5340(8.9182) | Error 0.0978(0.0956) Steps 832(820.70) | Grad Norm 4.2741(4.8135) | Total Time 0.00(0.00)\n",
      "Iter 10940 | Time 20.2888(20.5096) | Bit/dim 3.5650(3.5842) | Xent 0.2798(0.2726) | Loss 8.6076(8.8354) | Error 0.0956(0.0951) Steps 808(821.01) | Grad Norm 6.1247(5.0474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 98.7628, Epoch Time 1243.8760(1208.6029), Bit/dim 3.5897(best: 3.5867), Xent 0.6988, Loss 3.9391, Error 0.2090(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 10950 | Time 20.6693(20.4976) | Bit/dim 3.5668(3.5813) | Xent 0.2573(0.2738) | Loss 8.4807(9.4877) | Error 0.0956(0.0959) Steps 796(821.65) | Grad Norm 5.7689(5.4194) | Total Time 0.00(0.00)\n",
      "Iter 10960 | Time 20.2861(20.4647) | Bit/dim 3.5879(3.5812) | Xent 0.2925(0.2719) | Loss 8.6310(9.2544) | Error 0.1022(0.0956) Steps 844(821.46) | Grad Norm 5.8496(5.4144) | Total Time 0.00(0.00)\n",
      "Iter 10970 | Time 20.4376(20.4821) | Bit/dim 3.5735(3.5813) | Xent 0.2078(0.2692) | Loss 8.6052(9.0805) | Error 0.0678(0.0947) Steps 838(821.97) | Grad Norm 3.6199(5.3671) | Total Time 0.00(0.00)\n",
      "Iter 10980 | Time 20.3260(20.4744) | Bit/dim 3.5815(3.5824) | Xent 0.3058(0.2668) | Loss 8.7032(8.9610) | Error 0.1011(0.0939) Steps 814(819.73) | Grad Norm 4.6203(5.0626) | Total Time 0.00(0.00)\n",
      "Iter 10990 | Time 20.6742(20.5505) | Bit/dim 3.5786(3.5847) | Xent 0.2917(0.2697) | Loss 8.6214(8.8752) | Error 0.0922(0.0946) Steps 844(821.94) | Grad Norm 5.0664(4.9982) | Total Time 0.00(0.00)\n",
      "Iter 11000 | Time 19.7175(20.4691) | Bit/dim 3.5601(3.5857) | Xent 0.2782(0.2749) | Loss 8.5298(8.8147) | Error 0.1022(0.0964) Steps 844(819.64) | Grad Norm 4.7270(5.1855) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 97.5830, Epoch Time 1241.1787(1209.5802), Bit/dim 3.5924(best: 3.5867), Xent 0.7128, Loss 3.9488, Error 0.2096(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11010 | Time 20.6925(20.4204) | Bit/dim 3.5804(3.5843) | Xent 0.2805(0.2749) | Loss 8.7977(9.3732) | Error 0.0922(0.0964) Steps 862(819.33) | Grad Norm 4.3709(5.1769) | Total Time 0.00(0.00)\n",
      "Iter 11020 | Time 20.8110(20.4015) | Bit/dim 3.5966(3.5834) | Xent 0.2501(0.2744) | Loss 8.7176(9.1795) | Error 0.0878(0.0961) Steps 826(821.23) | Grad Norm 3.7200(5.1589) | Total Time 0.00(0.00)\n",
      "Iter 11030 | Time 20.5656(20.5148) | Bit/dim 3.5655(3.5830) | Xent 0.2687(0.2728) | Loss 8.5280(9.0337) | Error 0.1033(0.0956) Steps 790(823.95) | Grad Norm 3.7036(4.9895) | Total Time 0.00(0.00)\n",
      "Iter 11040 | Time 20.4504(20.5783) | Bit/dim 3.5850(3.5858) | Xent 0.2460(0.2717) | Loss 8.7300(8.9381) | Error 0.0900(0.0956) Steps 850(823.78) | Grad Norm 4.0095(5.0343) | Total Time 0.00(0.00)\n",
      "Iter 11050 | Time 20.9520(20.5438) | Bit/dim 3.6243(3.5859) | Xent 0.2885(0.2768) | Loss 8.6496(8.8478) | Error 0.0989(0.0969) Steps 838(825.09) | Grad Norm 4.6182(4.8820) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 98.8990, Epoch Time 1246.7078(1210.6940), Bit/dim 3.5914(best: 3.5867), Xent 0.6981, Loss 3.9404, Error 0.2073(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11060 | Time 19.8154(20.5304) | Bit/dim 3.5717(3.5854) | Xent 0.2525(0.2757) | Loss 8.5206(9.5010) | Error 0.0900(0.0964) Steps 796(825.79) | Grad Norm 4.3807(4.7579) | Total Time 0.00(0.00)\n",
      "Iter 11070 | Time 20.2723(20.5295) | Bit/dim 3.6129(3.5853) | Xent 0.3051(0.2781) | Loss 8.7805(9.2731) | Error 0.1000(0.0973) Steps 826(824.04) | Grad Norm 7.8768(4.9491) | Total Time 0.00(0.00)\n",
      "Iter 11080 | Time 20.8092(20.5576) | Bit/dim 3.5568(3.5860) | Xent 0.2700(0.2761) | Loss 8.5118(9.0996) | Error 0.0889(0.0973) Steps 826(820.72) | Grad Norm 4.1801(5.2923) | Total Time 0.00(0.00)\n",
      "Iter 11090 | Time 21.0056(20.5461) | Bit/dim 3.5820(3.5828) | Xent 0.2694(0.2753) | Loss 8.6485(8.9666) | Error 0.0978(0.0969) Steps 850(819.70) | Grad Norm 4.3241(5.2016) | Total Time 0.00(0.00)\n",
      "Iter 11100 | Time 21.2675(20.6382) | Bit/dim 3.5644(3.5807) | Xent 0.2357(0.2754) | Loss 8.6359(8.8919) | Error 0.0789(0.0960) Steps 832(823.58) | Grad Norm 4.5997(5.3691) | Total Time 0.00(0.00)\n",
      "Iter 11110 | Time 20.5068(20.5464) | Bit/dim 3.6144(3.5858) | Xent 0.2914(0.2735) | Loss 8.6485(8.8260) | Error 0.0956(0.0950) Steps 856(824.71) | Grad Norm 8.1887(5.6833) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 98.3845, Epoch Time 1246.4726(1211.7673), Bit/dim 3.5890(best: 3.5867), Xent 0.7206, Loss 3.9493, Error 0.2114(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11120 | Time 20.1281(20.5405) | Bit/dim 3.5708(3.5874) | Xent 0.2548(0.2695) | Loss 8.6782(9.3756) | Error 0.0889(0.0939) Steps 838(824.81) | Grad Norm 5.9746(5.7648) | Total Time 0.00(0.00)\n",
      "Iter 11130 | Time 20.3803(20.5543) | Bit/dim 3.5917(3.5867) | Xent 0.2575(0.2697) | Loss 8.6075(9.1804) | Error 0.0844(0.0941) Steps 838(824.69) | Grad Norm 4.2440(5.6187) | Total Time 0.00(0.00)\n",
      "Iter 11140 | Time 20.0603(20.5422) | Bit/dim 3.6038(3.5872) | Xent 0.3300(0.2719) | Loss 8.7449(9.0306) | Error 0.1233(0.0952) Steps 820(825.81) | Grad Norm 7.7260(5.5460) | Total Time 0.00(0.00)\n",
      "Iter 11150 | Time 21.1046(20.5558) | Bit/dim 3.5860(3.5846) | Xent 0.2502(0.2714) | Loss 8.7214(8.9233) | Error 0.0878(0.0954) Steps 844(826.77) | Grad Norm 4.8178(5.4538) | Total Time 0.00(0.00)\n",
      "Iter 11160 | Time 20.6828(20.5086) | Bit/dim 3.6180(3.5839) | Xent 0.2974(0.2721) | Loss 8.7846(8.8493) | Error 0.1044(0.0957) Steps 808(825.64) | Grad Norm 5.7404(5.3547) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 98.3925, Epoch Time 1243.4946(1212.7192), Bit/dim 3.5906(best: 3.5867), Xent 0.7245, Loss 3.9529, Error 0.2099(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11170 | Time 21.2427(20.4984) | Bit/dim 3.5678(3.5829) | Xent 0.2665(0.2685) | Loss 8.5721(9.4842) | Error 0.0978(0.0947) Steps 802(821.67) | Grad Norm 3.4374(5.0529) | Total Time 0.00(0.00)\n",
      "Iter 11180 | Time 20.3341(20.5332) | Bit/dim 3.6149(3.5884) | Xent 0.3121(0.2692) | Loss 8.8521(9.2762) | Error 0.1189(0.0944) Steps 790(820.21) | Grad Norm 4.2634(5.1999) | Total Time 0.00(0.00)\n",
      "Iter 11190 | Time 20.5963(20.5808) | Bit/dim 3.5758(3.5867) | Xent 0.2873(0.2711) | Loss 8.6797(9.1096) | Error 0.0967(0.0955) Steps 850(821.48) | Grad Norm 3.6267(5.2915) | Total Time 0.00(0.00)\n",
      "Iter 11200 | Time 20.1762(20.6162) | Bit/dim 3.5643(3.5845) | Xent 0.2701(0.2720) | Loss 8.4915(8.9796) | Error 0.0911(0.0960) Steps 796(820.50) | Grad Norm 5.4112(5.4219) | Total Time 0.00(0.00)\n",
      "Iter 11210 | Time 20.9572(20.5762) | Bit/dim 3.6015(3.5827) | Xent 0.2654(0.2732) | Loss 8.7018(8.8955) | Error 0.0933(0.0968) Steps 838(820.67) | Grad Norm 5.2721(5.3956) | Total Time 0.00(0.00)\n",
      "Iter 11220 | Time 20.5777(20.5831) | Bit/dim 3.5527(3.5814) | Xent 0.2570(0.2697) | Loss 8.5372(8.8170) | Error 0.0911(0.0955) Steps 790(819.98) | Grad Norm 5.5365(5.3482) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 98.0970, Epoch Time 1250.8621(1213.8634), Bit/dim 3.5892(best: 3.5867), Xent 0.7121, Loss 3.9453, Error 0.2055(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11230 | Time 20.0003(20.5129) | Bit/dim 3.6006(3.5813) | Xent 0.2661(0.2687) | Loss 8.7116(9.3909) | Error 0.0900(0.0957) Steps 826(821.91) | Grad Norm 5.0482(5.1452) | Total Time 0.00(0.00)\n",
      "Iter 11240 | Time 20.3135(20.4786) | Bit/dim 3.6050(3.5815) | Xent 0.2768(0.2687) | Loss 8.5746(9.1860) | Error 0.0967(0.0953) Steps 784(821.97) | Grad Norm 3.2492(5.1365) | Total Time 0.00(0.00)\n",
      "Iter 11250 | Time 20.4535(20.4236) | Bit/dim 3.5564(3.5819) | Xent 0.2994(0.2678) | Loss 8.7013(9.0417) | Error 0.1022(0.0944) Steps 838(823.44) | Grad Norm 5.3400(5.1378) | Total Time 0.00(0.00)\n",
      "Iter 11260 | Time 20.2036(20.4433) | Bit/dim 3.5669(3.5807) | Xent 0.2617(0.2684) | Loss 8.4829(8.9317) | Error 0.0911(0.0943) Steps 814(820.60) | Grad Norm 2.6246(5.0963) | Total Time 0.00(0.00)\n",
      "Iter 11270 | Time 20.8120(20.5209) | Bit/dim 3.5889(3.5804) | Xent 0.2633(0.2663) | Loss 8.6628(8.8414) | Error 0.0800(0.0940) Steps 844(822.26) | Grad Norm 3.8276(5.1162) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 98.5288, Epoch Time 1242.3758(1214.7188), Bit/dim 3.5905(best: 3.5867), Xent 0.7190, Loss 3.9500, Error 0.2133(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11280 | Time 20.6506(20.5095) | Bit/dim 3.5980(3.5837) | Xent 0.2903(0.2705) | Loss 8.6130(9.4918) | Error 0.1033(0.0948) Steps 802(820.92) | Grad Norm 6.2660(5.2052) | Total Time 0.00(0.00)\n",
      "Iter 11290 | Time 20.6579(20.5008) | Bit/dim 3.5537(3.5830) | Xent 0.2674(0.2706) | Loss 8.6370(9.2728) | Error 0.0867(0.0951) Steps 844(822.33) | Grad Norm 4.8210(5.1900) | Total Time 0.00(0.00)\n",
      "Iter 11300 | Time 19.6792(20.5002) | Bit/dim 3.5876(3.5831) | Xent 0.2874(0.2704) | Loss 8.6198(9.1089) | Error 0.0944(0.0954) Steps 826(823.19) | Grad Norm 6.3520(5.3074) | Total Time 0.00(0.00)\n",
      "Iter 11310 | Time 21.4248(20.5667) | Bit/dim 3.5693(3.5844) | Xent 0.2390(0.2720) | Loss 8.4987(8.9897) | Error 0.0767(0.0951) Steps 814(825.05) | Grad Norm 6.6445(5.6113) | Total Time 0.00(0.00)\n",
      "Iter 11320 | Time 20.6900(20.5437) | Bit/dim 3.6074(3.5838) | Xent 0.2589(0.2699) | Loss 8.7048(8.8888) | Error 0.0856(0.0936) Steps 850(825.33) | Grad Norm 4.6101(5.3884) | Total Time 0.00(0.00)\n",
      "Iter 11330 | Time 20.5350(20.5142) | Bit/dim 3.5634(3.5822) | Xent 0.2114(0.2688) | Loss 8.5654(8.8160) | Error 0.0689(0.0938) Steps 850(825.87) | Grad Norm 6.5912(5.3531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 96.9917, Epoch Time 1242.7017(1215.5583), Bit/dim 3.5900(best: 3.5867), Xent 0.7156, Loss 3.9478, Error 0.2135(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11340 | Time 21.5760(20.5070) | Bit/dim 3.5883(3.5820) | Xent 0.2745(0.2685) | Loss 8.6481(9.3493) | Error 0.0967(0.0939) Steps 874(824.54) | Grad Norm 9.9438(5.7709) | Total Time 0.00(0.00)\n",
      "Iter 11350 | Time 21.2873(20.5351) | Bit/dim 3.5900(3.5840) | Xent 0.2794(0.2686) | Loss 8.6303(9.1648) | Error 0.0944(0.0935) Steps 808(823.46) | Grad Norm 4.4360(5.8529) | Total Time 0.00(0.00)\n",
      "Iter 11360 | Time 20.4218(20.5802) | Bit/dim 3.5795(3.5834) | Xent 0.2954(0.2691) | Loss 8.6387(9.0235) | Error 0.1022(0.0944) Steps 790(821.54) | Grad Norm 5.7486(5.6448) | Total Time 0.00(0.00)\n",
      "Iter 11370 | Time 20.1039(20.5492) | Bit/dim 3.5744(3.5833) | Xent 0.2993(0.2678) | Loss 8.5851(8.9047) | Error 0.1100(0.0938) Steps 820(821.17) | Grad Norm 3.9157(5.2507) | Total Time 0.00(0.00)\n",
      "Iter 11380 | Time 20.6919(20.6203) | Bit/dim 3.5537(3.5802) | Xent 0.2783(0.2708) | Loss 8.6156(8.8369) | Error 0.1000(0.0953) Steps 826(821.20) | Grad Norm 4.9697(5.3933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 99.1917, Epoch Time 1250.5411(1216.6078), Bit/dim 3.5890(best: 3.5867), Xent 0.7092, Loss 3.9436, Error 0.2080(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11390 | Time 20.7737(20.6330) | Bit/dim 3.5647(3.5795) | Xent 0.2682(0.2693) | Loss 8.5738(9.4750) | Error 0.0956(0.0951) Steps 820(822.73) | Grad Norm 4.5937(5.3187) | Total Time 0.00(0.00)\n",
      "Iter 11400 | Time 20.0355(20.5959) | Bit/dim 3.5980(3.5832) | Xent 0.2937(0.2713) | Loss 8.5567(9.2453) | Error 0.1111(0.0951) Steps 814(822.55) | Grad Norm 6.4959(5.6425) | Total Time 0.00(0.00)\n",
      "Iter 11410 | Time 20.6427(20.6604) | Bit/dim 3.5757(3.5803) | Xent 0.2665(0.2714) | Loss 8.6585(9.0843) | Error 0.0978(0.0952) Steps 856(826.41) | Grad Norm 7.0384(5.7081) | Total Time 0.00(0.00)\n",
      "Iter 11420 | Time 20.6479(20.5590) | Bit/dim 3.5706(3.5830) | Xent 0.2964(0.2718) | Loss 8.5667(8.9709) | Error 0.1022(0.0954) Steps 832(827.37) | Grad Norm 4.0972(5.7233) | Total Time 0.00(0.00)\n",
      "Iter 11430 | Time 20.1707(20.5341) | Bit/dim 3.6027(3.5839) | Xent 0.2403(0.2693) | Loss 8.7623(8.8832) | Error 0.0778(0.0948) Steps 856(827.72) | Grad Norm 4.9539(5.4618) | Total Time 0.00(0.00)\n",
      "Iter 11440 | Time 21.1784(20.5356) | Bit/dim 3.5880(3.5852) | Xent 0.2809(0.2698) | Loss 8.6761(8.8263) | Error 0.0933(0.0959) Steps 856(827.24) | Grad Norm 4.2015(5.3122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 99.3776, Epoch Time 1247.2811(1217.5280), Bit/dim 3.5843(best: 3.5867), Xent 0.7107, Loss 3.9397, Error 0.2082(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11450 | Time 21.6310(20.5756) | Bit/dim 3.5927(3.5848) | Xent 0.2540(0.2689) | Loss 8.6090(9.3877) | Error 0.0933(0.0966) Steps 778(822.22) | Grad Norm 4.4895(5.3247) | Total Time 0.00(0.00)\n",
      "Iter 11460 | Time 20.8161(20.5457) | Bit/dim 3.5535(3.5838) | Xent 0.2901(0.2726) | Loss 8.6427(9.1849) | Error 0.1067(0.0982) Steps 826(823.17) | Grad Norm 4.8416(5.6693) | Total Time 0.00(0.00)\n",
      "Iter 11470 | Time 20.5190(20.5189) | Bit/dim 3.5556(3.5823) | Xent 0.2503(0.2652) | Loss 8.5489(9.0206) | Error 0.0833(0.0945) Steps 838(821.24) | Grad Norm 5.5213(5.6797) | Total Time 0.00(0.00)\n",
      "Iter 11480 | Time 20.6650(20.5492) | Bit/dim 3.5569(3.5830) | Xent 0.2383(0.2655) | Loss 8.6230(8.9190) | Error 0.0878(0.0938) Steps 832(821.92) | Grad Norm 5.9953(5.7936) | Total Time 0.00(0.00)\n",
      "Iter 11490 | Time 20.8551(20.5424) | Bit/dim 3.5970(3.5811) | Xent 0.2834(0.2653) | Loss 8.7639(8.8386) | Error 0.1033(0.0933) Steps 862(822.91) | Grad Norm 5.6027(6.1493) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 100.1416, Epoch Time 1248.5737(1218.4594), Bit/dim 3.5882(best: 3.5843), Xent 0.7101, Loss 3.9432, Error 0.2087(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11500 | Time 21.3173(20.6058) | Bit/dim 3.6101(3.5828) | Xent 0.2413(0.2662) | Loss 8.7025(9.4761) | Error 0.0878(0.0932) Steps 826(824.38) | Grad Norm 4.2971(5.9636) | Total Time 0.00(0.00)\n",
      "Iter 11510 | Time 20.2314(20.6099) | Bit/dim 3.5682(3.5845) | Xent 0.3148(0.2671) | Loss 8.6168(9.2650) | Error 0.1244(0.0940) Steps 838(824.89) | Grad Norm 4.8436(5.8497) | Total Time 0.00(0.00)\n",
      "Iter 11520 | Time 21.0554(20.6727) | Bit/dim 3.5767(3.5821) | Xent 0.2177(0.2642) | Loss 8.5988(9.0878) | Error 0.0689(0.0924) Steps 808(823.29) | Grad Norm 6.7205(5.7144) | Total Time 0.00(0.00)\n",
      "Iter 11530 | Time 21.0596(20.6714) | Bit/dim 3.5921(3.5827) | Xent 0.2436(0.2618) | Loss 8.5465(8.9649) | Error 0.0889(0.0914) Steps 796(821.73) | Grad Norm 4.8467(5.3692) | Total Time 0.00(0.00)\n",
      "Iter 11540 | Time 20.5377(20.6424) | Bit/dim 3.5808(3.5860) | Xent 0.2853(0.2629) | Loss 8.6862(8.8749) | Error 0.0956(0.0914) Steps 826(820.09) | Grad Norm 6.6138(5.3934) | Total Time 0.00(0.00)\n",
      "Iter 11550 | Time 20.1437(20.6061) | Bit/dim 3.5639(3.5827) | Xent 0.2578(0.2676) | Loss 8.5537(8.8118) | Error 0.0911(0.0930) Steps 808(821.91) | Grad Norm 4.4709(5.7079) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 99.4495, Epoch Time 1254.3029(1219.5347), Bit/dim 3.5915(best: 3.5843), Xent 0.7474, Loss 3.9652, Error 0.2179(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11560 | Time 20.5141(20.6089) | Bit/dim 3.5810(3.5852) | Xent 0.2649(0.2688) | Loss 8.6301(9.3967) | Error 0.0922(0.0938) Steps 850(822.69) | Grad Norm 4.9229(6.0012) | Total Time 0.00(0.00)\n",
      "Iter 11570 | Time 20.7121(20.6754) | Bit/dim 3.5923(3.5858) | Xent 0.2627(0.2680) | Loss 8.6594(9.1935) | Error 0.0989(0.0938) Steps 850(825.55) | Grad Norm 5.7109(5.8827) | Total Time 0.00(0.00)\n",
      "Iter 11580 | Time 20.3208(20.5786) | Bit/dim 3.5633(3.5838) | Xent 0.3016(0.2665) | Loss 8.5425(9.0456) | Error 0.1122(0.0936) Steps 796(823.97) | Grad Norm 5.8913(5.5407) | Total Time 0.00(0.00)\n",
      "Iter 11590 | Time 20.7244(20.6068) | Bit/dim 3.6105(3.5802) | Xent 0.2578(0.2672) | Loss 8.6433(8.9277) | Error 0.0944(0.0943) Steps 826(823.82) | Grad Norm 5.2393(5.3835) | Total Time 0.00(0.00)\n",
      "Iter 11600 | Time 20.7592(20.6158) | Bit/dim 3.5825(3.5808) | Xent 0.2322(0.2629) | Loss 8.5887(8.8425) | Error 0.0844(0.0932) Steps 826(823.61) | Grad Norm 3.7314(5.1726) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 99.2754, Epoch Time 1250.9902(1220.4783), Bit/dim 3.5905(best: 3.5843), Xent 0.7025, Loss 3.9417, Error 0.2058(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11610 | Time 20.9159(20.6359) | Bit/dim 3.5716(3.5836) | Xent 0.2381(0.2623) | Loss 8.5004(9.5064) | Error 0.0878(0.0935) Steps 814(824.17) | Grad Norm 4.8841(4.9711) | Total Time 0.00(0.00)\n",
      "Iter 11620 | Time 20.0401(20.5934) | Bit/dim 3.5956(3.5819) | Xent 0.2486(0.2639) | Loss 8.5461(9.2598) | Error 0.0833(0.0932) Steps 820(826.30) | Grad Norm 3.3343(4.9265) | Total Time 0.00(0.00)\n",
      "Iter 11630 | Time 20.2757(20.6505) | Bit/dim 3.5991(3.5815) | Xent 0.2706(0.2637) | Loss 8.5852(9.0913) | Error 0.0978(0.0934) Steps 820(825.56) | Grad Norm 3.9036(5.1823) | Total Time 0.00(0.00)\n",
      "Iter 11640 | Time 19.8684(20.6658) | Bit/dim 3.5703(3.5845) | Xent 0.2391(0.2641) | Loss 8.5711(8.9725) | Error 0.0844(0.0929) Steps 814(823.49) | Grad Norm 5.4089(5.3830) | Total Time 0.00(0.00)\n",
      "Iter 11650 | Time 21.3629(20.7420) | Bit/dim 3.5895(3.5831) | Xent 0.2813(0.2637) | Loss 8.7645(8.8816) | Error 0.0989(0.0930) Steps 832(822.66) | Grad Norm 5.1691(5.2563) | Total Time 0.00(0.00)\n",
      "Iter 11660 | Time 20.5936(20.7274) | Bit/dim 3.5362(3.5835) | Xent 0.3044(0.2650) | Loss 8.5914(8.8183) | Error 0.1167(0.0935) Steps 814(823.90) | Grad Norm 5.7813(5.1051) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 99.8069, Epoch Time 1258.8555(1221.6296), Bit/dim 3.5898(best: 3.5843), Xent 0.7293, Loss 3.9545, Error 0.2119(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11670 | Time 20.4770(20.6963) | Bit/dim 3.5837(3.5836) | Xent 0.2967(0.2658) | Loss 8.4828(9.3744) | Error 0.0956(0.0925) Steps 832(822.12) | Grad Norm 3.9127(5.0884) | Total Time 0.00(0.00)\n",
      "Iter 11680 | Time 20.4822(20.7047) | Bit/dim 3.5707(3.5833) | Xent 0.2564(0.2591) | Loss 8.6068(9.1663) | Error 0.0900(0.0904) Steps 820(824.12) | Grad Norm 5.9566(5.3424) | Total Time 0.00(0.00)\n",
      "Iter 11690 | Time 20.6710(20.7918) | Bit/dim 3.5372(3.5832) | Xent 0.2977(0.2609) | Loss 8.5217(9.0289) | Error 0.1133(0.0926) Steps 820(824.18) | Grad Norm 5.0534(5.4296) | Total Time 0.00(0.00)\n",
      "Iter 11700 | Time 21.0482(20.7311) | Bit/dim 3.5914(3.5833) | Xent 0.2633(0.2637) | Loss 8.6661(8.9181) | Error 0.0933(0.0925) Steps 844(825.65) | Grad Norm 4.6239(5.2882) | Total Time 0.00(0.00)\n",
      "Iter 11710 | Time 19.7814(20.7343) | Bit/dim 3.5883(3.5821) | Xent 0.2880(0.2639) | Loss 8.6259(8.8334) | Error 0.0944(0.0915) Steps 820(824.35) | Grad Norm 5.5780(5.3335) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 99.7540, Epoch Time 1259.9640(1222.7797), Bit/dim 3.5884(best: 3.5843), Xent 0.7062, Loss 3.9415, Error 0.2087(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11720 | Time 20.6632(20.7616) | Bit/dim 3.5820(3.5819) | Xent 0.2709(0.2668) | Loss 8.5811(9.5105) | Error 0.0889(0.0927) Steps 784(825.58) | Grad Norm 6.3110(5.5106) | Total Time 0.00(0.00)\n",
      "Iter 11730 | Time 20.3376(20.7271) | Bit/dim 3.5467(3.5818) | Xent 0.3071(0.2714) | Loss 8.5725(9.2836) | Error 0.1089(0.0948) Steps 784(823.79) | Grad Norm 11.1666(5.8321) | Total Time 0.00(0.00)\n",
      "Iter 11740 | Time 20.4021(20.7305) | Bit/dim 3.6074(3.5840) | Xent 0.2250(0.2680) | Loss 8.6417(9.1042) | Error 0.0867(0.0935) Steps 796(823.71) | Grad Norm 5.9338(5.6856) | Total Time 0.00(0.00)\n",
      "Iter 11750 | Time 20.9636(20.6725) | Bit/dim 3.5692(3.5831) | Xent 0.2577(0.2635) | Loss 8.5578(8.9651) | Error 0.0978(0.0932) Steps 838(823.71) | Grad Norm 4.4025(5.3264) | Total Time 0.00(0.00)\n",
      "Iter 11760 | Time 21.5366(20.6644) | Bit/dim 3.5737(3.5817) | Xent 0.2149(0.2607) | Loss 8.6604(8.8837) | Error 0.0744(0.0920) Steps 838(826.04) | Grad Norm 4.3729(5.1521) | Total Time 0.00(0.00)\n",
      "Iter 11770 | Time 20.8107(20.5959) | Bit/dim 3.5896(3.5823) | Xent 0.2773(0.2633) | Loss 8.5281(8.8096) | Error 0.1044(0.0931) Steps 808(826.38) | Grad Norm 5.3581(5.0620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 100.3064, Epoch Time 1251.1950(1223.6321), Bit/dim 3.5894(best: 3.5843), Xent 0.7257, Loss 3.9522, Error 0.2100(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11780 | Time 20.4944(20.6543) | Bit/dim 3.5788(3.5817) | Xent 0.2272(0.2578) | Loss 8.6103(9.3624) | Error 0.0844(0.0910) Steps 862(825.18) | Grad Norm 7.9812(5.1221) | Total Time 0.00(0.00)\n",
      "Iter 11790 | Time 20.5962(20.7184) | Bit/dim 3.6008(3.5827) | Xent 0.2487(0.2602) | Loss 8.6223(9.1855) | Error 0.0844(0.0912) Steps 838(827.85) | Grad Norm 6.7997(5.1884) | Total Time 0.00(0.00)\n",
      "Iter 11800 | Time 20.2615(20.7351) | Bit/dim 3.5815(3.5826) | Xent 0.2990(0.2595) | Loss 8.6401(9.0368) | Error 0.0978(0.0909) Steps 814(826.39) | Grad Norm 4.1130(5.2503) | Total Time 0.00(0.00)\n",
      "Iter 11810 | Time 20.7431(20.7360) | Bit/dim 3.5483(3.5811) | Xent 0.2740(0.2597) | Loss 8.6016(8.9265) | Error 0.0989(0.0909) Steps 856(828.40) | Grad Norm 4.5657(5.1760) | Total Time 0.00(0.00)\n",
      "Iter 11820 | Time 20.7508(20.6728) | Bit/dim 3.5965(3.5835) | Xent 0.2696(0.2560) | Loss 8.7488(8.8558) | Error 0.0922(0.0909) Steps 838(829.66) | Grad Norm 4.3476(5.2194) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 98.6565, Epoch Time 1255.5837(1224.5907), Bit/dim 3.5899(best: 3.5843), Xent 0.7240, Loss 3.9519, Error 0.2098(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11830 | Time 21.8297(20.6964) | Bit/dim 3.6122(3.5812) | Xent 0.2532(0.2601) | Loss 8.6957(9.5058) | Error 0.0789(0.0917) Steps 832(830.03) | Grad Norm 3.8757(5.1493) | Total Time 0.00(0.00)\n",
      "Iter 11840 | Time 20.6273(20.6762) | Bit/dim 3.5516(3.5815) | Xent 0.2649(0.2577) | Loss 8.5647(9.2696) | Error 0.0933(0.0906) Steps 820(832.46) | Grad Norm 8.0145(5.0824) | Total Time 0.00(0.00)\n",
      "Iter 11850 | Time 21.3279(20.6701) | Bit/dim 3.5598(3.5814) | Xent 0.3151(0.2559) | Loss 8.7034(9.0929) | Error 0.1222(0.0904) Steps 844(831.56) | Grad Norm 6.0986(5.3667) | Total Time 0.00(0.00)\n",
      "Iter 11860 | Time 20.6959(20.6873) | Bit/dim 3.5529(3.5810) | Xent 0.2936(0.2598) | Loss 8.5637(8.9684) | Error 0.0978(0.0914) Steps 790(830.92) | Grad Norm 6.1640(5.7818) | Total Time 0.00(0.00)\n",
      "Iter 11870 | Time 20.8191(20.8248) | Bit/dim 3.5634(3.5806) | Xent 0.2929(0.2642) | Loss 8.5455(8.8872) | Error 0.0944(0.0923) Steps 802(830.99) | Grad Norm 11.5516(6.1481) | Total Time 0.00(0.00)\n",
      "Iter 11880 | Time 21.0182(20.8935) | Bit/dim 3.5538(3.5791) | Xent 0.2566(0.2646) | Loss 8.5192(8.8116) | Error 0.0956(0.0931) Steps 832(830.45) | Grad Norm 3.3348(6.0600) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 98.7395, Epoch Time 1265.9040(1225.8301), Bit/dim 3.5904(best: 3.5843), Xent 0.7387, Loss 3.9597, Error 0.2109(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11890 | Time 21.2958(20.9024) | Bit/dim 3.5574(3.5798) | Xent 0.2481(0.2621) | Loss 8.6849(9.3778) | Error 0.0900(0.0925) Steps 814(830.25) | Grad Norm 6.2961(5.9994) | Total Time 0.00(0.00)\n",
      "Iter 11900 | Time 20.5308(20.8294) | Bit/dim 3.6345(3.5790) | Xent 0.2639(0.2634) | Loss 8.7153(9.1816) | Error 0.0933(0.0931) Steps 820(829.75) | Grad Norm 8.1309(5.9713) | Total Time 0.00(0.00)\n",
      "Iter 11910 | Time 20.6837(20.8682) | Bit/dim 3.5780(3.5811) | Xent 0.2572(0.2600) | Loss 8.7611(9.0415) | Error 0.0944(0.0921) Steps 880(832.06) | Grad Norm 5.6215(5.6915) | Total Time 0.00(0.00)\n",
      "Iter 11920 | Time 20.8142(20.7993) | Bit/dim 3.5639(3.5787) | Xent 0.2741(0.2613) | Loss 8.5802(8.9293) | Error 0.0911(0.0918) Steps 844(833.20) | Grad Norm 9.8024(5.9342) | Total Time 0.00(0.00)\n",
      "Iter 11930 | Time 20.5602(20.7583) | Bit/dim 3.5948(3.5836) | Xent 0.2565(0.2618) | Loss 8.7317(8.8569) | Error 0.1011(0.0925) Steps 838(830.81) | Grad Norm 5.3715(6.2996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 99.1046, Epoch Time 1257.9260(1226.7930), Bit/dim 3.5847(best: 3.5843), Xent 0.7333, Loss 3.9514, Error 0.2100(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 11940 | Time 21.1187(20.7643) | Bit/dim 3.6091(3.5843) | Xent 0.2721(0.2611) | Loss 8.7658(9.4966) | Error 0.0900(0.0921) Steps 814(829.47) | Grad Norm 4.0069(6.2271) | Total Time 0.00(0.00)\n",
      "Iter 11950 | Time 20.5279(20.6856) | Bit/dim 3.5664(3.5825) | Xent 0.2429(0.2602) | Loss 8.5832(9.2546) | Error 0.0878(0.0916) Steps 790(828.84) | Grad Norm 6.7601(6.0618) | Total Time 0.00(0.00)\n",
      "Iter 11960 | Time 20.2455(20.7938) | Bit/dim 3.5964(3.5827) | Xent 0.2877(0.2581) | Loss 8.6565(9.0850) | Error 0.1022(0.0911) Steps 790(823.99) | Grad Norm 5.1319(6.0045) | Total Time 0.00(0.00)\n",
      "Iter 11970 | Time 21.0823(20.9116) | Bit/dim 3.5748(3.5862) | Xent 0.2370(0.2592) | Loss 8.5202(8.9558) | Error 0.0811(0.0917) Steps 832(825.89) | Grad Norm 6.0523(6.0919) | Total Time 0.00(0.00)\n",
      "Iter 11980 | Time 20.9793(20.8198) | Bit/dim 3.6015(3.5846) | Xent 0.2406(0.2614) | Loss 8.7076(8.8609) | Error 0.0822(0.0929) Steps 814(827.46) | Grad Norm 6.2708(6.3037) | Total Time 0.00(0.00)\n",
      "Iter 11990 | Time 20.5154(20.8325) | Bit/dim 3.5847(3.5822) | Xent 0.2568(0.2644) | Loss 8.6957(8.7999) | Error 0.0878(0.0934) Steps 832(825.93) | Grad Norm 6.1206(6.2097) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 99.1113, Epoch Time 1263.8948(1227.9060), Bit/dim 3.5910(best: 3.5843), Xent 0.7257, Loss 3.9539, Error 0.2087(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12000 | Time 20.6349(20.7462) | Bit/dim 3.5579(3.5811) | Xent 0.2860(0.2630) | Loss 8.6106(9.3422) | Error 0.0922(0.0927) Steps 850(826.52) | Grad Norm 4.3435(5.9197) | Total Time 0.00(0.00)\n",
      "Iter 12010 | Time 20.3933(20.7241) | Bit/dim 3.6027(3.5825) | Xent 0.2515(0.2614) | Loss 8.5496(9.1527) | Error 0.0933(0.0925) Steps 826(826.18) | Grad Norm 3.0179(5.5366) | Total Time 0.00(0.00)\n",
      "Iter 12020 | Time 20.3894(20.8408) | Bit/dim 3.5779(3.5801) | Xent 0.2479(0.2611) | Loss 8.6283(9.0078) | Error 0.0833(0.0922) Steps 820(826.35) | Grad Norm 4.3411(5.1438) | Total Time 0.00(0.00)\n",
      "Iter 12030 | Time 21.5904(20.8660) | Bit/dim 3.5899(3.5822) | Xent 0.2864(0.2591) | Loss 8.6505(8.9035) | Error 0.0933(0.0913) Steps 868(825.34) | Grad Norm 4.0561(5.0205) | Total Time 0.00(0.00)\n",
      "Iter 12040 | Time 21.0185(20.9097) | Bit/dim 3.5743(3.5839) | Xent 0.2812(0.2621) | Loss 8.7754(8.8406) | Error 0.0978(0.0924) Steps 844(827.09) | Grad Norm 9.1236(5.1467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 100.1835, Epoch Time 1266.7509(1229.0714), Bit/dim 3.5896(best: 3.5843), Xent 0.7287, Loss 3.9540, Error 0.2093(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12050 | Time 20.6002(20.8998) | Bit/dim 3.6187(3.5866) | Xent 0.2571(0.2590) | Loss 8.6639(9.4839) | Error 0.0933(0.0903) Steps 826(831.12) | Grad Norm 6.1930(5.2073) | Total Time 0.00(0.00)\n",
      "Iter 12060 | Time 20.7122(20.8645) | Bit/dim 3.5762(3.5832) | Xent 0.2884(0.2594) | Loss 8.6279(9.2597) | Error 0.1033(0.0905) Steps 814(830.46) | Grad Norm 4.2481(5.1847) | Total Time 0.00(0.00)\n",
      "Iter 12070 | Time 20.5897(20.8361) | Bit/dim 3.6018(3.5835) | Xent 0.2772(0.2583) | Loss 8.6507(9.0911) | Error 0.0900(0.0900) Steps 832(828.62) | Grad Norm 6.5688(5.3800) | Total Time 0.00(0.00)\n",
      "Iter 12080 | Time 20.3786(20.8590) | Bit/dim 3.5863(3.5836) | Xent 0.2682(0.2577) | Loss 8.6624(8.9612) | Error 0.1100(0.0907) Steps 814(829.05) | Grad Norm 6.8500(5.3332) | Total Time 0.00(0.00)\n",
      "Iter 12090 | Time 21.2007(20.8874) | Bit/dim 3.5773(3.5821) | Xent 0.2984(0.2597) | Loss 8.6229(8.8691) | Error 0.1111(0.0916) Steps 814(830.50) | Grad Norm 4.8869(5.3538) | Total Time 0.00(0.00)\n",
      "Iter 12100 | Time 20.8913(20.8281) | Bit/dim 3.5811(3.5826) | Xent 0.2533(0.2617) | Loss 8.7211(8.8103) | Error 0.0967(0.0923) Steps 826(829.66) | Grad Norm 4.1595(5.4307) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 101.6489, Epoch Time 1263.9087(1230.1165), Bit/dim 3.5888(best: 3.5843), Xent 0.7468, Loss 3.9622, Error 0.2089(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12110 | Time 20.5843(20.8329) | Bit/dim 3.6110(3.5865) | Xent 0.2223(0.2600) | Loss 8.5966(9.3881) | Error 0.0867(0.0929) Steps 820(825.51) | Grad Norm 7.0816(5.5311) | Total Time 0.00(0.00)\n",
      "Iter 12120 | Time 20.6850(20.8567) | Bit/dim 3.6002(3.5855) | Xent 0.2347(0.2608) | Loss 8.6004(9.1906) | Error 0.0767(0.0928) Steps 796(826.33) | Grad Norm 6.8916(5.7582) | Total Time 0.00(0.00)\n",
      "Iter 12130 | Time 21.1431(20.8667) | Bit/dim 3.6133(3.5854) | Xent 0.2904(0.2599) | Loss 8.7373(9.0490) | Error 0.1089(0.0920) Steps 838(826.51) | Grad Norm 5.4910(5.6013) | Total Time 0.00(0.00)\n",
      "Iter 12140 | Time 20.1800(20.7881) | Bit/dim 3.4899(3.5797) | Xent 0.2866(0.2612) | Loss 8.4795(8.9262) | Error 0.0978(0.0920) Steps 838(825.99) | Grad Norm 8.4134(5.9132) | Total Time 0.00(0.00)\n",
      "Iter 12150 | Time 20.7715(20.6967) | Bit/dim 3.5898(3.5819) | Xent 0.2931(0.2622) | Loss 8.6308(8.8443) | Error 0.1033(0.0923) Steps 832(824.54) | Grad Norm 6.6708(5.8711) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 99.3785, Epoch Time 1260.3415(1231.0232), Bit/dim 3.5892(best: 3.5843), Xent 0.7430, Loss 3.9607, Error 0.2130(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12160 | Time 20.5952(20.7888) | Bit/dim 3.5925(3.5830) | Xent 0.2642(0.2604) | Loss 8.6304(9.5034) | Error 0.0844(0.0915) Steps 820(828.88) | Grad Norm 5.0251(5.9013) | Total Time 0.00(0.00)\n",
      "Iter 12170 | Time 21.0432(20.7054) | Bit/dim 3.5820(3.5850) | Xent 0.2012(0.2590) | Loss 8.6070(9.2796) | Error 0.0778(0.0910) Steps 820(826.18) | Grad Norm 4.7237(5.6461) | Total Time 0.00(0.00)\n",
      "Iter 12180 | Time 21.0406(20.7306) | Bit/dim 3.5718(3.5824) | Xent 0.2134(0.2580) | Loss 8.5599(9.1015) | Error 0.0767(0.0909) Steps 802(824.72) | Grad Norm 5.0137(5.6635) | Total Time 0.00(0.00)\n",
      "Iter 12190 | Time 20.2579(20.7067) | Bit/dim 3.5969(3.5831) | Xent 0.2576(0.2601) | Loss 8.5548(8.9773) | Error 0.0933(0.0918) Steps 838(826.79) | Grad Norm 4.3331(5.4365) | Total Time 0.00(0.00)\n",
      "Iter 12200 | Time 21.5563(20.7458) | Bit/dim 3.5729(3.5808) | Xent 0.2509(0.2599) | Loss 8.6471(8.8832) | Error 0.0978(0.0912) Steps 838(825.36) | Grad Norm 3.8666(5.4318) | Total Time 0.00(0.00)\n",
      "Iter 12210 | Time 21.4924(20.7728) | Bit/dim 3.5775(3.5831) | Xent 0.2341(0.2576) | Loss 8.6400(8.8231) | Error 0.0744(0.0896) Steps 838(825.89) | Grad Norm 3.9820(5.2283) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 100.7049, Epoch Time 1259.2234(1231.8692), Bit/dim 3.5910(best: 3.5843), Xent 0.7304, Loss 3.9562, Error 0.2097(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12220 | Time 20.5472(20.8178) | Bit/dim 3.5522(3.5839) | Xent 0.2737(0.2566) | Loss 8.4398(9.3845) | Error 0.1000(0.0896) Steps 802(824.43) | Grad Norm 6.6647(5.1968) | Total Time 0.00(0.00)\n",
      "Iter 12230 | Time 21.7501(20.8370) | Bit/dim 3.5751(3.5807) | Xent 0.2395(0.2565) | Loss 8.5601(9.1756) | Error 0.0778(0.0897) Steps 832(826.84) | Grad Norm 3.8411(5.2353) | Total Time 0.00(0.00)\n",
      "Iter 12240 | Time 21.2180(20.8492) | Bit/dim 3.5855(3.5829) | Xent 0.2802(0.2588) | Loss 8.5341(9.0315) | Error 0.1000(0.0903) Steps 862(829.22) | Grad Norm 9.9100(5.6154) | Total Time 0.00(0.00)\n",
      "Iter 12250 | Time 20.7656(20.9004) | Bit/dim 3.5881(3.5855) | Xent 0.2710(0.2617) | Loss 8.4760(8.9284) | Error 0.1011(0.0918) Steps 820(832.12) | Grad Norm 5.3248(6.1208) | Total Time 0.00(0.00)\n",
      "Iter 12260 | Time 21.0962(20.8891) | Bit/dim 3.5698(3.5848) | Xent 0.2532(0.2589) | Loss 8.6034(8.8467) | Error 0.0922(0.0903) Steps 826(832.19) | Grad Norm 6.2566(5.8518) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 99.7995, Epoch Time 1268.4702(1232.9673), Bit/dim 3.5900(best: 3.5843), Xent 0.7442, Loss 3.9621, Error 0.2113(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12270 | Time 20.5777(20.8399) | Bit/dim 3.5389(3.5811) | Xent 0.2425(0.2546) | Loss 8.6030(9.4867) | Error 0.0867(0.0889) Steps 844(831.20) | Grad Norm 4.2231(5.3867) | Total Time 0.00(0.00)\n",
      "Iter 12280 | Time 21.3115(20.8551) | Bit/dim 3.5775(3.5791) | Xent 0.2409(0.2517) | Loss 8.7906(9.2521) | Error 0.0789(0.0881) Steps 850(831.11) | Grad Norm 5.6755(5.3543) | Total Time 0.00(0.00)\n",
      "Iter 12290 | Time 20.8330(20.7848) | Bit/dim 3.5728(3.5796) | Xent 0.2640(0.2538) | Loss 8.6620(9.0850) | Error 0.0911(0.0887) Steps 838(830.97) | Grad Norm 5.2792(5.3317) | Total Time 0.00(0.00)\n",
      "Iter 12300 | Time 20.8300(20.8362) | Bit/dim 3.6190(3.5813) | Xent 0.2376(0.2524) | Loss 8.7015(8.9633) | Error 0.0811(0.0881) Steps 844(830.08) | Grad Norm 4.3344(5.3145) | Total Time 0.00(0.00)\n",
      "Iter 12310 | Time 21.2794(20.8893) | Bit/dim 3.5558(3.5840) | Xent 0.2334(0.2526) | Loss 8.5561(8.8756) | Error 0.0911(0.0882) Steps 820(832.15) | Grad Norm 7.4011(5.3744) | Total Time 0.00(0.00)\n",
      "Iter 12320 | Time 21.2858(20.8355) | Bit/dim 3.5872(3.5860) | Xent 0.2689(0.2557) | Loss 8.5272(8.8086) | Error 0.0944(0.0891) Steps 826(829.29) | Grad Norm 5.6996(5.4994) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 99.0691, Epoch Time 1261.2983(1233.8172), Bit/dim 3.5872(best: 3.5843), Xent 0.7288, Loss 3.9516, Error 0.2081(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12330 | Time 21.5955(20.8900) | Bit/dim 3.5952(3.5859) | Xent 0.2579(0.2571) | Loss 8.6614(9.3582) | Error 0.0922(0.0896) Steps 832(830.01) | Grad Norm 3.8021(5.3118) | Total Time 0.00(0.00)\n",
      "Iter 12340 | Time 21.0532(20.8895) | Bit/dim 3.5936(3.5859) | Xent 0.2462(0.2554) | Loss 8.5274(9.1653) | Error 0.0844(0.0892) Steps 844(829.58) | Grad Norm 6.7581(5.3283) | Total Time 0.00(0.00)\n",
      "Iter 12350 | Time 20.0053(20.9460) | Bit/dim 3.5833(3.5842) | Xent 0.2578(0.2544) | Loss 8.4881(9.0060) | Error 0.0867(0.0891) Steps 814(829.21) | Grad Norm 3.4284(5.2690) | Total Time 0.00(0.00)\n",
      "Iter 12360 | Time 21.2243(20.9609) | Bit/dim 3.5950(3.5836) | Xent 0.2548(0.2557) | Loss 8.6666(8.9039) | Error 0.0867(0.0893) Steps 850(829.98) | Grad Norm 6.4418(5.2171) | Total Time 0.00(0.00)\n",
      "Iter 12370 | Time 20.2495(20.9448) | Bit/dim 3.6039(3.5841) | Xent 0.2385(0.2537) | Loss 8.4744(8.8211) | Error 0.0867(0.0887) Steps 820(829.95) | Grad Norm 8.0822(5.2683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 100.6545, Epoch Time 1273.7689(1235.0157), Bit/dim 3.5884(best: 3.5843), Xent 0.7403, Loss 3.9586, Error 0.2111(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12380 | Time 20.2976(20.9585) | Bit/dim 3.5740(3.5863) | Xent 0.2824(0.2556) | Loss 8.6431(9.4999) | Error 0.1000(0.0894) Steps 826(832.85) | Grad Norm 6.6103(5.5514) | Total Time 0.00(0.00)\n",
      "Iter 12390 | Time 21.2274(20.9944) | Bit/dim 3.5594(3.5846) | Xent 0.2152(0.2576) | Loss 8.6073(9.2777) | Error 0.0800(0.0902) Steps 808(832.96) | Grad Norm 3.8588(5.6179) | Total Time 0.00(0.00)\n",
      "Iter 12400 | Time 21.3061(20.9334) | Bit/dim 3.5769(3.5830) | Xent 0.2899(0.2570) | Loss 8.6219(9.0973) | Error 0.0967(0.0903) Steps 844(831.04) | Grad Norm 3.9289(5.4518) | Total Time 0.00(0.00)\n",
      "Iter 12410 | Time 20.6005(20.9552) | Bit/dim 3.5830(3.5830) | Xent 0.2556(0.2558) | Loss 8.6972(8.9875) | Error 0.0944(0.0894) Steps 838(836.06) | Grad Norm 4.5464(5.5519) | Total Time 0.00(0.00)\n",
      "Iter 12420 | Time 21.2325(20.9198) | Bit/dim 3.6012(3.5864) | Xent 0.2311(0.2576) | Loss 8.5898(8.8939) | Error 0.0767(0.0892) Steps 832(835.90) | Grad Norm 5.1034(5.4998) | Total Time 0.00(0.00)\n",
      "Iter 12430 | Time 21.8263(20.9102) | Bit/dim 3.5779(3.5858) | Xent 0.2495(0.2592) | Loss 8.5250(8.8085) | Error 0.0933(0.0905) Steps 814(834.75) | Grad Norm 5.3308(5.5496) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 100.3981, Epoch Time 1269.0290(1236.0361), Bit/dim 3.5928(best: 3.5843), Xent 0.7413, Loss 3.9634, Error 0.2111(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12440 | Time 20.3953(20.8479) | Bit/dim 3.5920(3.5837) | Xent 0.2979(0.2575) | Loss 8.5608(9.3589) | Error 0.1011(0.0903) Steps 790(834.57) | Grad Norm 5.2940(5.5847) | Total Time 0.00(0.00)\n",
      "Iter 12450 | Time 20.7693(20.7902) | Bit/dim 3.6058(3.5831) | Xent 0.2359(0.2557) | Loss 8.6727(9.1653) | Error 0.0822(0.0900) Steps 808(834.23) | Grad Norm 4.8446(5.4830) | Total Time 0.00(0.00)\n",
      "Iter 12460 | Time 20.9017(20.8300) | Bit/dim 3.5820(3.5857) | Xent 0.2370(0.2532) | Loss 8.5516(9.0234) | Error 0.0822(0.0888) Steps 802(831.30) | Grad Norm 4.6403(5.4637) | Total Time 0.00(0.00)\n",
      "Iter 12470 | Time 21.4941(20.9393) | Bit/dim 3.5716(3.5843) | Xent 0.2495(0.2551) | Loss 8.6038(8.9174) | Error 0.0944(0.0896) Steps 802(830.69) | Grad Norm 5.4347(5.6486) | Total Time 0.00(0.00)\n",
      "Iter 12480 | Time 20.8361(20.9512) | Bit/dim 3.5506(3.5855) | Xent 0.2742(0.2519) | Loss 8.5202(8.8304) | Error 0.0900(0.0886) Steps 826(831.53) | Grad Norm 6.9931(5.6575) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 99.8361, Epoch Time 1265.8007(1236.9291), Bit/dim 3.5901(best: 3.5843), Xent 0.7466, Loss 3.9633, Error 0.2120(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12490 | Time 20.5355(20.8772) | Bit/dim 3.5576(3.5843) | Xent 0.2212(0.2505) | Loss 8.3941(9.5046) | Error 0.0711(0.0879) Steps 826(830.34) | Grad Norm 7.0477(5.6621) | Total Time 0.00(0.00)\n",
      "Iter 12500 | Time 21.2197(20.8747) | Bit/dim 3.5546(3.5835) | Xent 0.2779(0.2547) | Loss 8.6148(9.2721) | Error 0.0944(0.0896) Steps 874(831.31) | Grad Norm 9.7916(5.9800) | Total Time 0.00(0.00)\n",
      "Iter 12510 | Time 21.4260(20.8857) | Bit/dim 3.5779(3.5852) | Xent 0.2133(0.2553) | Loss 8.6042(9.1103) | Error 0.0656(0.0905) Steps 826(830.68) | Grad Norm 7.4207(6.4787) | Total Time 0.00(0.00)\n",
      "Iter 12520 | Time 21.1979(20.8664) | Bit/dim 3.6018(3.5820) | Xent 0.2612(0.2580) | Loss 8.7240(8.9822) | Error 0.0878(0.0908) Steps 880(832.63) | Grad Norm 6.5565(6.8104) | Total Time 0.00(0.00)\n",
      "Iter 12530 | Time 20.0078(20.8522) | Bit/dim 3.5862(3.5824) | Xent 0.2529(0.2551) | Loss 8.5444(8.8814) | Error 0.0911(0.0896) Steps 808(831.58) | Grad Norm 4.9678(6.2144) | Total Time 0.00(0.00)\n",
      "Iter 12540 | Time 21.0974(20.9224) | Bit/dim 3.5820(3.5835) | Xent 0.2373(0.2539) | Loss 8.5306(8.8035) | Error 0.0811(0.0891) Steps 826(833.74) | Grad Norm 4.5601(5.8797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 98.6826, Epoch Time 1265.7709(1237.7943), Bit/dim 3.5884(best: 3.5843), Xent 0.7397, Loss 3.9582, Error 0.2111(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12550 | Time 20.8534(20.9394) | Bit/dim 3.5715(3.5832) | Xent 0.2473(0.2532) | Loss 8.5123(9.3813) | Error 0.0800(0.0882) Steps 844(836.00) | Grad Norm 5.2827(5.7606) | Total Time 0.00(0.00)\n",
      "Iter 12560 | Time 21.1635(20.9276) | Bit/dim 3.5716(3.5823) | Xent 0.2256(0.2532) | Loss 8.5266(9.1812) | Error 0.0756(0.0887) Steps 844(834.15) | Grad Norm 5.2099(5.6968) | Total Time 0.00(0.00)\n",
      "Iter 12570 | Time 20.8372(20.9311) | Bit/dim 3.5981(3.5830) | Xent 0.2530(0.2529) | Loss 8.5689(9.0264) | Error 0.0911(0.0885) Steps 790(830.53) | Grad Norm 5.5199(5.7776) | Total Time 0.00(0.00)\n",
      "Iter 12580 | Time 21.9233(20.9492) | Bit/dim 3.5823(3.5829) | Xent 0.2640(0.2501) | Loss 8.6439(8.9079) | Error 0.0956(0.0883) Steps 874(834.23) | Grad Norm 4.1257(5.7545) | Total Time 0.00(0.00)\n",
      "Iter 12590 | Time 20.2660(20.9762) | Bit/dim 3.5497(3.5821) | Xent 0.2251(0.2514) | Loss 8.5251(8.8314) | Error 0.0811(0.0880) Steps 832(830.60) | Grad Norm 4.5362(5.4860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 99.2516, Epoch Time 1270.4594(1238.7743), Bit/dim 3.5895(best: 3.5843), Xent 0.7422, Loss 3.9606, Error 0.2111(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12600 | Time 21.8705(21.0198) | Bit/dim 3.6073(3.5835) | Xent 0.1858(0.2529) | Loss 8.6010(9.4978) | Error 0.0678(0.0883) Steps 862(829.42) | Grad Norm 4.9711(5.3785) | Total Time 0.00(0.00)\n",
      "Iter 12610 | Time 20.3107(20.9009) | Bit/dim 3.5683(3.5826) | Xent 0.2404(0.2514) | Loss 8.4058(9.2581) | Error 0.0822(0.0883) Steps 802(827.36) | Grad Norm 4.4724(5.2398) | Total Time 0.00(0.00)\n",
      "Iter 12620 | Time 21.1856(20.9363) | Bit/dim 3.5937(3.5834) | Xent 0.2655(0.2529) | Loss 8.6978(9.0950) | Error 0.0967(0.0894) Steps 856(829.72) | Grad Norm 6.3064(5.4865) | Total Time 0.00(0.00)\n",
      "Iter 12630 | Time 21.4873(20.9329) | Bit/dim 3.6129(3.5851) | Xent 0.2609(0.2518) | Loss 8.7182(8.9858) | Error 0.0989(0.0890) Steps 850(832.58) | Grad Norm 5.9085(5.8430) | Total Time 0.00(0.00)\n",
      "Iter 12640 | Time 20.9091(20.9776) | Bit/dim 3.6037(3.5850) | Xent 0.2650(0.2501) | Loss 8.6672(8.8964) | Error 0.0867(0.0876) Steps 820(830.75) | Grad Norm 5.5401(5.7074) | Total Time 0.00(0.00)\n",
      "Iter 12650 | Time 21.4502(20.9788) | Bit/dim 3.5481(3.5836) | Xent 0.2756(0.2535) | Loss 8.6442(8.8299) | Error 0.1022(0.0888) Steps 814(830.04) | Grad Norm 8.9044(6.0018) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 100.7201, Epoch Time 1271.1539(1239.7457), Bit/dim 3.5921(best: 3.5843), Xent 0.7451, Loss 3.9646, Error 0.2091(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12660 | Time 20.7042(20.9061) | Bit/dim 3.5839(3.5857) | Xent 0.2830(0.2521) | Loss 8.7009(9.3709) | Error 0.0978(0.0885) Steps 796(827.82) | Grad Norm 6.3271(6.0529) | Total Time 0.00(0.00)\n",
      "Iter 12670 | Time 20.5369(20.9213) | Bit/dim 3.5826(3.5812) | Xent 0.2554(0.2552) | Loss 8.6910(9.1703) | Error 0.0811(0.0896) Steps 802(825.94) | Grad Norm 5.5869(5.9960) | Total Time 0.00(0.00)\n",
      "Iter 12680 | Time 21.2937(20.9352) | Bit/dim 3.5632(3.5841) | Xent 0.2185(0.2527) | Loss 8.5194(9.0402) | Error 0.0789(0.0886) Steps 850(827.76) | Grad Norm 5.2809(5.9466) | Total Time 0.00(0.00)\n",
      "Iter 12690 | Time 21.0993(20.8496) | Bit/dim 3.5732(3.5847) | Xent 0.2588(0.2499) | Loss 8.5821(8.9269) | Error 0.0956(0.0881) Steps 844(828.87) | Grad Norm 6.0449(6.0652) | Total Time 0.00(0.00)\n",
      "Iter 12700 | Time 20.3682(20.8438) | Bit/dim 3.5539(3.5843) | Xent 0.2253(0.2516) | Loss 8.5616(8.8421) | Error 0.0833(0.0880) Steps 844(828.34) | Grad Norm 6.3182(6.0141) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 100.3206, Epoch Time 1263.0545(1240.4449), Bit/dim 3.5908(best: 3.5843), Xent 0.7557, Loss 3.9686, Error 0.2144(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12710 | Time 20.9836(20.8175) | Bit/dim 3.5997(3.5850) | Xent 0.2373(0.2514) | Loss 8.6924(9.4924) | Error 0.0800(0.0894) Steps 826(827.12) | Grad Norm 7.8102(6.0814) | Total Time 0.00(0.00)\n",
      "Iter 12720 | Time 20.5887(20.8342) | Bit/dim 3.5877(3.5838) | Xent 0.2686(0.2480) | Loss 8.6515(9.2581) | Error 0.1067(0.0881) Steps 826(828.04) | Grad Norm 5.6230(5.9303) | Total Time 0.00(0.00)\n",
      "Iter 12730 | Time 20.1669(20.8109) | Bit/dim 3.5390(3.5824) | Xent 0.2713(0.2489) | Loss 8.5009(9.0829) | Error 0.1044(0.0877) Steps 802(830.81) | Grad Norm 7.6768(5.9398) | Total Time 0.00(0.00)\n",
      "Iter 12740 | Time 21.0124(20.8913) | Bit/dim 3.5976(3.5851) | Xent 0.2671(0.2510) | Loss 8.6647(8.9769) | Error 0.1011(0.0893) Steps 844(829.93) | Grad Norm 7.9949(6.0879) | Total Time 0.00(0.00)\n",
      "Iter 12750 | Time 20.8789(20.8398) | Bit/dim 3.5418(3.5850) | Xent 0.2104(0.2524) | Loss 8.5614(8.8772) | Error 0.0767(0.0898) Steps 862(828.16) | Grad Norm 7.4214(6.3658) | Total Time 0.00(0.00)\n",
      "Iter 12760 | Time 20.7643(20.8570) | Bit/dim 3.5808(3.5845) | Xent 0.2915(0.2545) | Loss 8.6369(8.8084) | Error 0.0922(0.0910) Steps 826(830.36) | Grad Norm 7.1386(6.3874) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 99.4082, Epoch Time 1264.7868(1241.1752), Bit/dim 3.5917(best: 3.5843), Xent 0.7421, Loss 3.9627, Error 0.2122(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12770 | Time 21.1730(20.8307) | Bit/dim 3.6224(3.5860) | Xent 0.2597(0.2548) | Loss 8.7551(9.3645) | Error 0.0933(0.0909) Steps 850(829.79) | Grad Norm 5.7431(6.3733) | Total Time 0.00(0.00)\n",
      "Iter 12780 | Time 21.4876(20.9182) | Bit/dim 3.5942(3.5836) | Xent 0.2219(0.2539) | Loss 8.5014(9.1602) | Error 0.0744(0.0899) Steps 868(830.20) | Grad Norm 5.0972(6.4600) | Total Time 0.00(0.00)\n",
      "Iter 12790 | Time 21.5040(20.9340) | Bit/dim 3.5822(3.5844) | Xent 0.2460(0.2519) | Loss 8.6593(9.0201) | Error 0.0878(0.0889) Steps 820(828.76) | Grad Norm 6.7996(6.6309) | Total Time 0.00(0.00)\n",
      "Iter 12800 | Time 20.3937(20.8572) | Bit/dim 3.6113(3.5835) | Xent 0.2504(0.2524) | Loss 8.5014(8.9101) | Error 0.0833(0.0891) Steps 784(828.05) | Grad Norm 4.9356(6.5537) | Total Time 0.00(0.00)\n",
      "Iter 12810 | Time 20.2542(20.9663) | Bit/dim 3.5781(3.5871) | Xent 0.2590(0.2533) | Loss 8.6035(8.8482) | Error 0.0900(0.0895) Steps 832(831.93) | Grad Norm 5.0972(6.1098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 98.6493, Epoch Time 1270.7719(1242.0631), Bit/dim 3.5885(best: 3.5843), Xent 0.7416, Loss 3.9594, Error 0.2122(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12820 | Time 20.4195(20.9802) | Bit/dim 3.5758(3.5862) | Xent 0.2671(0.2523) | Loss 8.6572(9.5128) | Error 0.0911(0.0892) Steps 826(835.07) | Grad Norm 4.6362(5.6637) | Total Time 0.00(0.00)\n",
      "Iter 12830 | Time 20.4124(20.9860) | Bit/dim 3.5430(3.5876) | Xent 0.2040(0.2494) | Loss 8.3572(9.2736) | Error 0.0733(0.0880) Steps 844(835.39) | Grad Norm 4.3719(5.4760) | Total Time 0.00(0.00)\n",
      "Iter 12840 | Time 21.1028(21.0464) | Bit/dim 3.5866(3.5869) | Xent 0.2833(0.2468) | Loss 8.6671(9.1048) | Error 0.0967(0.0872) Steps 844(837.53) | Grad Norm 4.7865(5.3537) | Total Time 0.00(0.00)\n",
      "Iter 12850 | Time 20.5293(21.1031) | Bit/dim 3.5867(3.5854) | Xent 0.2813(0.2489) | Loss 8.5844(8.9712) | Error 0.0989(0.0870) Steps 832(837.63) | Grad Norm 5.5635(5.4152) | Total Time 0.00(0.00)\n",
      "Iter 12860 | Time 20.5763(21.1019) | Bit/dim 3.6095(3.5852) | Xent 0.2739(0.2510) | Loss 8.6126(8.8852) | Error 0.0967(0.0883) Steps 820(837.85) | Grad Norm 8.4737(5.9791) | Total Time 0.00(0.00)\n",
      "Iter 12870 | Time 21.4854(21.0874) | Bit/dim 3.6262(3.5831) | Xent 0.2361(0.2543) | Loss 8.7270(8.8112) | Error 0.0856(0.0888) Steps 820(835.71) | Grad Norm 7.1326(6.1692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 100.5104, Epoch Time 1278.9375(1243.1693), Bit/dim 3.5895(best: 3.5843), Xent 0.7480, Loss 3.9635, Error 0.2093(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12880 | Time 21.8804(21.0828) | Bit/dim 3.5816(3.5838) | Xent 0.2671(0.2550) | Loss 8.6366(9.3722) | Error 0.0989(0.0892) Steps 808(837.45) | Grad Norm 7.4409(6.2330) | Total Time 0.00(0.00)\n",
      "Iter 12890 | Time 20.7317(21.0302) | Bit/dim 3.5573(3.5834) | Xent 0.2325(0.2523) | Loss 8.5331(9.1721) | Error 0.0867(0.0881) Steps 868(834.73) | Grad Norm 4.0462(6.1499) | Total Time 0.00(0.00)\n",
      "Iter 12900 | Time 20.6151(21.0365) | Bit/dim 3.5806(3.5820) | Xent 0.2875(0.2540) | Loss 8.6428(9.0351) | Error 0.0956(0.0887) Steps 838(837.41) | Grad Norm 3.9725(6.1175) | Total Time 0.00(0.00)\n",
      "Iter 12910 | Time 21.6544(20.9938) | Bit/dim 3.5730(3.5819) | Xent 0.2625(0.2513) | Loss 8.6719(8.9146) | Error 0.0978(0.0880) Steps 850(837.71) | Grad Norm 5.0315(6.1262) | Total Time 0.00(0.00)\n",
      "Iter 12920 | Time 21.6171(21.0893) | Bit/dim 3.5885(3.5827) | Xent 0.2854(0.2533) | Loss 8.7124(8.8392) | Error 0.1000(0.0893) Steps 850(835.38) | Grad Norm 7.0407(6.0966) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 99.7555, Epoch Time 1274.8332(1244.1192), Bit/dim 3.5972(best: 3.5843), Xent 0.7454, Loss 3.9699, Error 0.2101(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12930 | Time 21.0686(21.0750) | Bit/dim 3.5734(3.5838) | Xent 0.2454(0.2516) | Loss 8.5707(9.4928) | Error 0.0844(0.0885) Steps 796(835.11) | Grad Norm 4.2312(6.0619) | Total Time 0.00(0.00)\n",
      "Iter 12940 | Time 20.2847(21.0857) | Bit/dim 3.5767(3.5842) | Xent 0.2889(0.2517) | Loss 8.6324(9.2731) | Error 0.1022(0.0896) Steps 832(837.03) | Grad Norm 7.6720(6.0939) | Total Time 0.00(0.00)\n",
      "Iter 12950 | Time 20.8266(21.1012) | Bit/dim 3.5917(3.5847) | Xent 0.2464(0.2506) | Loss 8.6728(9.1008) | Error 0.0856(0.0891) Steps 838(833.65) | Grad Norm 8.5522(6.1717) | Total Time 0.00(0.00)\n",
      "Iter 12960 | Time 20.8160(21.0579) | Bit/dim 3.6130(3.5855) | Xent 0.2253(0.2526) | Loss 8.6712(8.9859) | Error 0.0800(0.0891) Steps 856(833.60) | Grad Norm 4.8606(6.2007) | Total Time 0.00(0.00)\n",
      "Iter 12970 | Time 21.1564(21.0997) | Bit/dim 3.5584(3.5833) | Xent 0.2789(0.2505) | Loss 8.6051(8.8787) | Error 0.0989(0.0880) Steps 844(836.70) | Grad Norm 4.9474(6.0994) | Total Time 0.00(0.00)\n",
      "Iter 12980 | Time 20.2682(21.0640) | Bit/dim 3.6133(3.5858) | Xent 0.2616(0.2482) | Loss 8.6077(8.8126) | Error 0.0956(0.0870) Steps 838(835.80) | Grad Norm 5.2376(5.8569) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 100.1929, Epoch Time 1278.0498(1245.1372), Bit/dim 3.5949(best: 3.5843), Xent 0.7539, Loss 3.9719, Error 0.2133(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 12990 | Time 21.0271(21.1068) | Bit/dim 3.5677(3.5864) | Xent 0.2386(0.2453) | Loss 8.5784(9.3622) | Error 0.0867(0.0862) Steps 808(837.31) | Grad Norm 4.4300(5.8851) | Total Time 0.00(0.00)\n",
      "Iter 13000 | Time 21.5751(21.0873) | Bit/dim 3.5759(3.5861) | Xent 0.2873(0.2441) | Loss 8.5836(9.1589) | Error 0.0967(0.0858) Steps 802(831.94) | Grad Norm 6.9476(5.8633) | Total Time 0.00(0.00)\n",
      "Iter 13010 | Time 21.0371(21.0905) | Bit/dim 3.5721(3.5863) | Xent 0.2605(0.2480) | Loss 8.5351(9.0139) | Error 0.0889(0.0874) Steps 856(831.41) | Grad Norm 6.1072(5.7864) | Total Time 0.00(0.00)\n",
      "Iter 13020 | Time 21.1410(21.0992) | Bit/dim 3.5903(3.5877) | Xent 0.2497(0.2514) | Loss 8.6753(8.9261) | Error 0.0900(0.0889) Steps 874(834.47) | Grad Norm 9.5756(6.0164) | Total Time 0.00(0.00)\n",
      "Iter 13030 | Time 21.1389(21.0959) | Bit/dim 3.5878(3.5872) | Xent 0.2737(0.2537) | Loss 8.5722(8.8455) | Error 0.0956(0.0891) Steps 862(836.07) | Grad Norm 5.5087(6.0529) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 99.8198, Epoch Time 1278.7346(1246.1451), Bit/dim 3.5943(best: 3.5843), Xent 0.7540, Loss 3.9714, Error 0.2101(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13040 | Time 20.4480(21.1028) | Bit/dim 3.5920(3.5892) | Xent 0.1938(0.2498) | Loss 8.5503(9.5026) | Error 0.0689(0.0886) Steps 844(838.24) | Grad Norm 6.2097(5.9335) | Total Time 0.00(0.00)\n",
      "Iter 13050 | Time 20.6173(21.0782) | Bit/dim 3.6173(3.5926) | Xent 0.2765(0.2481) | Loss 8.6845(9.2771) | Error 0.0967(0.0872) Steps 856(835.01) | Grad Norm 4.3355(5.6751) | Total Time 0.00(0.00)\n",
      "Iter 13060 | Time 21.3140(20.9575) | Bit/dim 3.5620(3.5909) | Xent 0.2683(0.2473) | Loss 8.6937(9.1024) | Error 0.0922(0.0875) Steps 844(834.44) | Grad Norm 6.7548(5.7601) | Total Time 0.00(0.00)\n",
      "Iter 13070 | Time 20.3489(20.9865) | Bit/dim 3.5554(3.5884) | Xent 0.2715(0.2494) | Loss 8.6582(8.9857) | Error 0.0911(0.0881) Steps 820(834.03) | Grad Norm 5.3196(6.0219) | Total Time 0.00(0.00)\n",
      "Iter 13080 | Time 21.8903(20.9771) | Bit/dim 3.5817(3.5877) | Xent 0.2796(0.2500) | Loss 8.6762(8.8901) | Error 0.1022(0.0884) Steps 790(832.21) | Grad Norm 5.4270(5.9364) | Total Time 0.00(0.00)\n",
      "Iter 13090 | Time 19.6827(20.9630) | Bit/dim 3.6009(3.5871) | Xent 0.2741(0.2526) | Loss 8.6125(8.8189) | Error 0.1011(0.0895) Steps 820(831.56) | Grad Norm 7.7016(5.9852) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 99.5885, Epoch Time 1269.5981(1246.8487), Bit/dim 3.5928(best: 3.5843), Xent 0.7505, Loss 3.9681, Error 0.2112(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13100 | Time 20.2653(20.9043) | Bit/dim 3.6334(3.5863) | Xent 0.2182(0.2493) | Loss 8.5205(9.3860) | Error 0.0778(0.0885) Steps 790(830.53) | Grad Norm 5.9655(6.0500) | Total Time 0.00(0.00)\n",
      "Iter 13110 | Time 22.5488(20.9423) | Bit/dim 3.5949(3.5866) | Xent 0.2462(0.2486) | Loss 8.6226(9.1761) | Error 0.0900(0.0887) Steps 808(828.46) | Grad Norm 9.5781(6.1333) | Total Time 0.00(0.00)\n",
      "Iter 13120 | Time 20.2181(20.9198) | Bit/dim 3.5988(3.5865) | Xent 0.2132(0.2447) | Loss 8.6326(9.0228) | Error 0.0756(0.0875) Steps 838(829.85) | Grad Norm 3.7057(6.0065) | Total Time 0.00(0.00)\n",
      "Iter 13130 | Time 20.4356(20.9274) | Bit/dim 3.5700(3.5874) | Xent 0.2577(0.2446) | Loss 8.4776(8.9142) | Error 0.0878(0.0869) Steps 784(828.67) | Grad Norm 4.1997(5.7257) | Total Time 0.00(0.00)\n",
      "Iter 13140 | Time 21.5586(20.9447) | Bit/dim 3.5768(3.5862) | Xent 0.2315(0.2450) | Loss 8.6540(8.8341) | Error 0.0722(0.0864) Steps 808(830.90) | Grad Norm 9.8107(5.8970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 99.3740, Epoch Time 1268.6030(1247.5013), Bit/dim 3.5963(best: 3.5843), Xent 0.7673, Loss 3.9800, Error 0.2133(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13150 | Time 21.4750(20.9894) | Bit/dim 3.5918(3.5880) | Xent 0.2273(0.2430) | Loss 8.6146(9.4974) | Error 0.0867(0.0856) Steps 814(829.33) | Grad Norm 5.8742(6.0270) | Total Time 0.00(0.00)\n",
      "Iter 13160 | Time 20.5195(20.9826) | Bit/dim 3.5818(3.5876) | Xent 0.2773(0.2435) | Loss 8.6866(9.2644) | Error 0.0900(0.0859) Steps 820(831.41) | Grad Norm 6.3827(6.0040) | Total Time 0.00(0.00)\n",
      "Iter 13170 | Time 21.5408(21.0340) | Bit/dim 3.6217(3.5890) | Xent 0.2394(0.2453) | Loss 8.7413(9.0892) | Error 0.0700(0.0859) Steps 838(834.07) | Grad Norm 6.2794(6.0860) | Total Time 0.00(0.00)\n",
      "Iter 13180 | Time 20.0931(20.9466) | Bit/dim 3.6007(3.5889) | Xent 0.2072(0.2444) | Loss 8.6270(8.9671) | Error 0.0711(0.0847) Steps 826(831.94) | Grad Norm 5.8786(6.0488) | Total Time 0.00(0.00)\n",
      "Iter 13190 | Time 20.4614(20.9639) | Bit/dim 3.6051(3.5880) | Xent 0.2457(0.2456) | Loss 8.5054(8.8715) | Error 0.0878(0.0860) Steps 814(831.89) | Grad Norm 6.8467(6.0400) | Total Time 0.00(0.00)\n",
      "Iter 13200 | Time 20.5583(20.9145) | Bit/dim 3.5679(3.5854) | Xent 0.2844(0.2489) | Loss 8.6429(8.8006) | Error 0.0967(0.0868) Steps 832(830.19) | Grad Norm 8.8600(6.0337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 98.9679, Epoch Time 1269.8032(1248.1704), Bit/dim 3.5947(best: 3.5843), Xent 0.7536, Loss 3.9715, Error 0.2108(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13210 | Time 20.8896(20.9274) | Bit/dim 3.6249(3.5885) | Xent 0.2572(0.2469) | Loss 8.6367(9.3736) | Error 0.0956(0.0862) Steps 838(832.25) | Grad Norm 6.9830(5.9304) | Total Time 0.00(0.00)\n",
      "Iter 13220 | Time 20.8228(20.9281) | Bit/dim 3.5845(3.5890) | Xent 0.2771(0.2489) | Loss 8.6937(9.1830) | Error 0.1056(0.0872) Steps 832(831.54) | Grad Norm 8.0111(6.3553) | Total Time 0.00(0.00)\n",
      "Iter 13230 | Time 21.0347(20.9236) | Bit/dim 3.5770(3.5885) | Xent 0.3060(0.2514) | Loss 8.8020(9.0389) | Error 0.1033(0.0877) Steps 856(832.47) | Grad Norm 7.8226(6.6456) | Total Time 0.00(0.00)\n",
      "Iter 13240 | Time 20.3573(20.8685) | Bit/dim 3.5826(3.5882) | Xent 0.2450(0.2530) | Loss 8.6207(8.9330) | Error 0.0878(0.0881) Steps 832(831.53) | Grad Norm 8.6686(6.6308) | Total Time 0.00(0.00)\n",
      "Iter 13250 | Time 21.6743(20.8573) | Bit/dim 3.5860(3.5875) | Xent 0.2249(0.2536) | Loss 8.6390(8.8499) | Error 0.0811(0.0888) Steps 850(829.34) | Grad Norm 6.3098(6.5329) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 99.5854, Epoch Time 1264.6399(1248.6644), Bit/dim 3.5980(best: 3.5843), Xent 0.7508, Loss 3.9734, Error 0.2083(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13260 | Time 20.8143(20.8375) | Bit/dim 3.6313(3.5869) | Xent 0.2360(0.2517) | Loss 8.7193(9.5121) | Error 0.0756(0.0876) Steps 832(830.92) | Grad Norm 4.4586(6.2014) | Total Time 0.00(0.00)\n",
      "Iter 13270 | Time 20.7063(20.8767) | Bit/dim 3.5914(3.5878) | Xent 0.1926(0.2460) | Loss 8.5487(9.2827) | Error 0.0678(0.0861) Steps 796(831.33) | Grad Norm 4.0631(6.0556) | Total Time 0.00(0.00)\n",
      "Iter 13280 | Time 20.2152(20.8387) | Bit/dim 3.5618(3.5911) | Xent 0.2398(0.2475) | Loss 8.5280(9.1185) | Error 0.0822(0.0868) Steps 862(832.33) | Grad Norm 6.2501(5.8542) | Total Time 0.00(0.00)\n",
      "Iter 13290 | Time 20.9492(20.8703) | Bit/dim 3.5843(3.5919) | Xent 0.2125(0.2449) | Loss 8.6968(8.9905) | Error 0.0756(0.0859) Steps 856(833.33) | Grad Norm 5.5822(5.7342) | Total Time 0.00(0.00)\n",
      "Iter 13300 | Time 20.4807(20.8442) | Bit/dim 3.5950(3.5918) | Xent 0.2628(0.2474) | Loss 8.5986(8.8914) | Error 0.0856(0.0869) Steps 832(828.35) | Grad Norm 5.1292(6.0078) | Total Time 0.00(0.00)\n",
      "Iter 13310 | Time 20.4865(20.8579) | Bit/dim 3.5921(3.5899) | Xent 0.2638(0.2489) | Loss 8.6298(8.8175) | Error 0.0856(0.0878) Steps 850(829.38) | Grad Norm 4.5450(6.0466) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 99.7425, Epoch Time 1265.3610(1249.1653), Bit/dim 3.6002(best: 3.5843), Xent 0.7450, Loss 3.9727, Error 0.2123(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13320 | Time 20.8530(20.7983) | Bit/dim 3.5996(3.5890) | Xent 0.2296(0.2488) | Loss 8.5570(9.3751) | Error 0.0800(0.0881) Steps 808(827.72) | Grad Norm 4.3630(6.0921) | Total Time 0.00(0.00)\n",
      "Iter 13330 | Time 21.1445(20.8987) | Bit/dim 3.5935(3.5906) | Xent 0.2472(0.2449) | Loss 8.6547(9.1833) | Error 0.0833(0.0860) Steps 814(830.00) | Grad Norm 5.1440(5.9311) | Total Time 0.00(0.00)\n",
      "Iter 13340 | Time 21.1020(20.8728) | Bit/dim 3.5443(3.5899) | Xent 0.2357(0.2469) | Loss 8.5445(9.0423) | Error 0.0789(0.0865) Steps 844(830.01) | Grad Norm 3.5970(6.0212) | Total Time 0.00(0.00)\n",
      "Iter 13350 | Time 20.0906(20.9615) | Bit/dim 3.6099(3.5942) | Xent 0.2792(0.2475) | Loss 8.5303(8.9401) | Error 0.0956(0.0869) Steps 820(833.45) | Grad Norm 4.6889(5.7825) | Total Time 0.00(0.00)\n",
      "Iter 13360 | Time 20.7173(20.9319) | Bit/dim 3.6059(3.5909) | Xent 0.2565(0.2475) | Loss 8.6140(8.8525) | Error 0.0933(0.0872) Steps 856(834.03) | Grad Norm 8.3485(5.9212) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 99.9469, Epoch Time 1269.9845(1249.7899), Bit/dim 3.5971(best: 3.5843), Xent 0.7654, Loss 3.9798, Error 0.2134(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13370 | Time 20.8449(20.9360) | Bit/dim 3.6087(3.5930) | Xent 0.2532(0.2436) | Loss 8.6767(9.4991) | Error 0.0844(0.0855) Steps 814(832.31) | Grad Norm 6.6477(6.1679) | Total Time 0.00(0.00)\n",
      "Iter 13380 | Time 21.2384(21.0051) | Bit/dim 3.6248(3.5961) | Xent 0.2787(0.2450) | Loss 8.7803(9.2892) | Error 0.0967(0.0860) Steps 850(831.64) | Grad Norm 6.6625(6.2843) | Total Time 0.00(0.00)\n",
      "Iter 13390 | Time 21.8169(21.0561) | Bit/dim 3.6049(3.5948) | Xent 0.2667(0.2464) | Loss 8.7445(9.1292) | Error 0.0922(0.0865) Steps 844(834.12) | Grad Norm 5.1376(6.1777) | Total Time 0.00(0.00)\n",
      "Iter 13400 | Time 20.7247(20.9890) | Bit/dim 3.6056(3.5939) | Xent 0.2598(0.2449) | Loss 8.8022(8.9964) | Error 0.0900(0.0855) Steps 862(834.89) | Grad Norm 4.5952(5.8587) | Total Time 0.00(0.00)\n",
      "Iter 13410 | Time 21.2387(20.9653) | Bit/dim 3.6044(3.5946) | Xent 0.2553(0.2456) | Loss 8.5677(8.9017) | Error 0.0900(0.0869) Steps 820(838.20) | Grad Norm 5.7332(5.6733) | Total Time 0.00(0.00)\n",
      "Iter 13420 | Time 20.4459(21.0624) | Bit/dim 3.6156(3.5950) | Xent 0.2817(0.2437) | Loss 8.5883(8.8297) | Error 0.0956(0.0865) Steps 814(838.67) | Grad Norm 8.0390(5.7822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 100.0715, Epoch Time 1276.7363(1250.5983), Bit/dim 3.6088(best: 3.5843), Xent 0.7785, Loss 3.9980, Error 0.2149(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13430 | Time 21.3658(21.0936) | Bit/dim 3.6001(3.5970) | Xent 0.2456(0.2425) | Loss 8.7144(9.3839) | Error 0.0922(0.0857) Steps 844(838.16) | Grad Norm 5.2805(5.8047) | Total Time 0.00(0.00)\n",
      "Iter 13440 | Time 20.8693(21.0969) | Bit/dim 3.6187(3.6006) | Xent 0.2513(0.2449) | Loss 8.6022(9.2031) | Error 0.0889(0.0861) Steps 784(838.89) | Grad Norm 5.5158(5.9226) | Total Time 0.00(0.00)\n",
      "Iter 13450 | Time 20.8055(21.0414) | Bit/dim 3.6237(3.6015) | Xent 0.2407(0.2453) | Loss 8.6605(9.0530) | Error 0.0844(0.0860) Steps 832(838.81) | Grad Norm 7.0374(6.0637) | Total Time 0.00(0.00)\n",
      "Iter 13460 | Time 20.6298(21.0630) | Bit/dim 3.6001(3.6000) | Xent 0.2593(0.2460) | Loss 8.7944(8.9488) | Error 0.0944(0.0868) Steps 862(839.14) | Grad Norm 6.8481(6.3172) | Total Time 0.00(0.00)\n",
      "Iter 13470 | Time 21.2763(21.1159) | Bit/dim 3.6081(3.5968) | Xent 0.2658(0.2484) | Loss 8.6023(8.8675) | Error 0.0900(0.0877) Steps 838(839.13) | Grad Norm 4.5443(6.4383) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 99.6460, Epoch Time 1278.1909(1251.4261), Bit/dim 3.6105(best: 3.5843), Xent 0.7523, Loss 3.9866, Error 0.2119(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13480 | Time 21.3296(21.0823) | Bit/dim 3.6392(3.6017) | Xent 0.2944(0.2509) | Loss 8.4488(9.5058) | Error 0.1000(0.0878) Steps 796(833.98) | Grad Norm 6.4316(6.8879) | Total Time 0.00(0.00)\n",
      "Iter 13490 | Time 21.1973(21.0661) | Bit/dim 3.6300(3.6049) | Xent 0.2253(0.2519) | Loss 8.8029(9.3010) | Error 0.0800(0.0873) Steps 856(838.34) | Grad Norm 6.1181(6.9701) | Total Time 0.00(0.00)\n",
      "Iter 13500 | Time 20.9563(21.1453) | Bit/dim 3.5906(3.6017) | Xent 0.2258(0.2509) | Loss 8.5288(9.1258) | Error 0.0800(0.0882) Steps 856(839.75) | Grad Norm 5.3255(6.6636) | Total Time 0.00(0.00)\n",
      "Iter 13510 | Time 21.1551(21.2626) | Bit/dim 3.6202(3.6032) | Xent 0.1909(0.2451) | Loss 8.6003(8.9959) | Error 0.0722(0.0862) Steps 826(839.05) | Grad Norm 4.6543(6.6059) | Total Time 0.00(0.00)\n",
      "Iter 13520 | Time 21.8149(21.2101) | Bit/dim 3.5990(3.6053) | Xent 0.2335(0.2429) | Loss 8.7245(8.9081) | Error 0.0867(0.0868) Steps 886(840.24) | Grad Norm 5.7243(6.5701) | Total Time 0.00(0.00)\n",
      "Iter 13530 | Time 21.6496(21.1674) | Bit/dim 3.5474(3.6025) | Xent 0.2611(0.2456) | Loss 8.5800(8.8383) | Error 0.0878(0.0876) Steps 826(837.45) | Grad Norm 6.8226(6.1928) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 101.2037, Epoch Time 1285.0496(1252.4348), Bit/dim 3.6083(best: 3.5843), Xent 0.7784, Loss 3.9975, Error 0.2153(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13540 | Time 20.7662(21.1251) | Bit/dim 3.5726(3.6019) | Xent 0.2653(0.2489) | Loss 8.6053(9.4187) | Error 0.0911(0.0883) Steps 856(838.38) | Grad Norm 7.1708(6.6330) | Total Time 0.00(0.00)\n",
      "Iter 13550 | Time 20.7690(21.1253) | Bit/dim 3.5948(3.6055) | Xent 0.2413(0.2480) | Loss 8.6254(9.2211) | Error 0.0856(0.0883) Steps 832(837.17) | Grad Norm 4.7575(6.3424) | Total Time 0.00(0.00)\n",
      "Iter 13560 | Time 20.9831(21.1299) | Bit/dim 3.6142(3.6042) | Xent 0.2214(0.2464) | Loss 8.6942(9.0676) | Error 0.0889(0.0875) Steps 826(838.23) | Grad Norm 9.5459(6.3275) | Total Time 0.00(0.00)\n",
      "Iter 13570 | Time 21.7149(21.2010) | Bit/dim 3.5940(3.6047) | Xent 0.2085(0.2485) | Loss 8.5738(8.9667) | Error 0.0722(0.0879) Steps 844(839.13) | Grad Norm 6.9006(6.6511) | Total Time 0.00(0.00)\n",
      "Iter 13580 | Time 21.5922(21.2694) | Bit/dim 3.6153(3.6089) | Xent 0.2291(0.2504) | Loss 8.8328(8.8998) | Error 0.0844(0.0880) Steps 844(841.53) | Grad Norm 6.0330(6.6182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 100.9465, Epoch Time 1286.2523(1253.4493), Bit/dim 3.6094(best: 3.5843), Xent 0.7738, Loss 3.9963, Error 0.2155(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13590 | Time 20.8948(21.2531) | Bit/dim 3.6406(3.6070) | Xent 0.2449(0.2541) | Loss 8.7503(9.5632) | Error 0.0811(0.0901) Steps 838(844.21) | Grad Norm 7.6183(6.8702) | Total Time 0.00(0.00)\n",
      "Iter 13600 | Time 20.5779(21.1799) | Bit/dim 3.6100(3.6076) | Xent 0.2349(0.2526) | Loss 8.5688(9.3263) | Error 0.0922(0.0893) Steps 826(840.58) | Grad Norm 7.0404(7.2004) | Total Time 0.00(0.00)\n",
      "Iter 13610 | Time 22.1339(21.2476) | Bit/dim 3.6539(3.6079) | Xent 0.2128(0.2544) | Loss 8.6996(9.1598) | Error 0.0744(0.0902) Steps 850(841.63) | Grad Norm 8.6529(7.3079) | Total Time 0.00(0.00)\n",
      "Iter 13620 | Time 21.2991(21.2676) | Bit/dim 3.5981(3.6072) | Xent 0.2639(0.2568) | Loss 8.5616(9.0232) | Error 0.0833(0.0910) Steps 826(839.74) | Grad Norm 12.3139(7.9205) | Total Time 0.00(0.00)\n",
      "Iter 13630 | Time 21.0707(21.2288) | Bit/dim 3.6066(3.6059) | Xent 0.2263(0.2576) | Loss 8.6326(8.9297) | Error 0.0800(0.0911) Steps 850(838.75) | Grad Norm 4.4977(7.5746) | Total Time 0.00(0.00)\n",
      "Iter 13640 | Time 20.7182(21.1857) | Bit/dim 3.6048(3.6047) | Xent 0.2563(0.2545) | Loss 8.7293(8.8645) | Error 0.0956(0.0906) Steps 814(836.45) | Grad Norm 5.6720(7.3050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 101.0396, Epoch Time 1284.2065(1254.3720), Bit/dim 3.6188(best: 3.5843), Xent 0.7725, Loss 4.0050, Error 0.2173(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13650 | Time 21.5843(21.3054) | Bit/dim 3.6356(3.6093) | Xent 0.2911(0.2522) | Loss 8.8567(9.4206) | Error 0.1067(0.0901) Steps 886(838.07) | Grad Norm 10.2472(7.3181) | Total Time 0.00(0.00)\n",
      "Iter 13660 | Time 21.0150(21.2541) | Bit/dim 3.6087(3.6089) | Xent 0.2680(0.2531) | Loss 8.6474(9.2253) | Error 0.0856(0.0894) Steps 808(837.59) | Grad Norm 7.1649(7.2083) | Total Time 0.00(0.00)\n",
      "Iter 13670 | Time 21.1703(21.2786) | Bit/dim 3.6077(3.6068) | Xent 0.2219(0.2524) | Loss 8.6063(9.0759) | Error 0.0733(0.0891) Steps 880(838.30) | Grad Norm 5.6986(7.0579) | Total Time 0.00(0.00)\n",
      "Iter 13680 | Time 21.6203(21.2011) | Bit/dim 3.5800(3.6043) | Xent 0.2641(0.2559) | Loss 8.6114(8.9664) | Error 0.0967(0.0904) Steps 802(836.02) | Grad Norm 9.7789(7.3323) | Total Time 0.00(0.00)\n",
      "Iter 13690 | Time 21.4430(21.2277) | Bit/dim 3.5901(3.6040) | Xent 0.2646(0.2565) | Loss 8.5370(8.8893) | Error 0.0878(0.0902) Steps 802(838.14) | Grad Norm 4.5918(7.2808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 100.3622, Epoch Time 1287.8413(1255.3761), Bit/dim 3.6091(best: 3.5843), Xent 0.7569, Loss 3.9875, Error 0.2115(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13700 | Time 20.5386(21.1797) | Bit/dim 3.6287(3.6050) | Xent 0.2172(0.2530) | Loss 8.7553(9.5533) | Error 0.0744(0.0893) Steps 820(841.45) | Grad Norm 7.5359(7.1077) | Total Time 0.00(0.00)\n",
      "Iter 13710 | Time 20.8472(21.1650) | Bit/dim 3.6199(3.6030) | Xent 0.2231(0.2503) | Loss 8.6146(9.3242) | Error 0.0844(0.0884) Steps 856(842.94) | Grad Norm 4.1307(6.8586) | Total Time 0.00(0.00)\n",
      "Iter 13720 | Time 21.7669(21.1971) | Bit/dim 3.6277(3.6009) | Xent 0.2426(0.2530) | Loss 8.6695(9.1447) | Error 0.0844(0.0890) Steps 826(840.14) | Grad Norm 6.9991(6.8708) | Total Time 0.00(0.00)\n",
      "Iter 13730 | Time 20.8755(21.1802) | Bit/dim 3.5672(3.6024) | Xent 0.2927(0.2570) | Loss 8.5635(9.0266) | Error 0.0967(0.0903) Steps 838(839.19) | Grad Norm 7.4224(6.7825) | Total Time 0.00(0.00)\n",
      "Iter 13740 | Time 20.6414(21.1118) | Bit/dim 3.6051(3.6012) | Xent 0.2136(0.2526) | Loss 8.6702(8.9266) | Error 0.0656(0.0891) Steps 832(839.37) | Grad Norm 4.9398(6.4852) | Total Time 0.00(0.00)\n",
      "Iter 13750 | Time 20.8282(21.0884) | Bit/dim 3.6130(3.6062) | Xent 0.2674(0.2516) | Loss 8.6617(8.8584) | Error 0.1044(0.0897) Steps 832(840.65) | Grad Norm 9.9011(6.5112) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 100.7331, Epoch Time 1278.6891(1256.0755), Bit/dim 3.6186(best: 3.5843), Xent 0.7665, Loss 4.0019, Error 0.2136(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13760 | Time 20.2647(21.1417) | Bit/dim 3.5753(3.6046) | Xent 0.2128(0.2485) | Loss 8.5623(9.4187) | Error 0.0678(0.0879) Steps 844(841.04) | Grad Norm 6.0362(6.5680) | Total Time 0.00(0.00)\n",
      "Iter 13770 | Time 21.9345(21.1166) | Bit/dim 3.5792(3.6029) | Xent 0.2425(0.2501) | Loss 8.6411(9.2160) | Error 0.0789(0.0881) Steps 808(839.70) | Grad Norm 8.0530(6.7399) | Total Time 0.00(0.00)\n",
      "Iter 13780 | Time 20.8612(21.1794) | Bit/dim 3.6084(3.6064) | Xent 0.2402(0.2527) | Loss 8.7049(9.0819) | Error 0.0900(0.0893) Steps 874(842.12) | Grad Norm 5.3200(6.8205) | Total Time 0.00(0.00)\n",
      "Iter 13790 | Time 21.2557(21.2413) | Bit/dim 3.5786(3.6055) | Xent 0.2618(0.2532) | Loss 8.6320(8.9680) | Error 0.0922(0.0893) Steps 862(842.29) | Grad Norm 7.4115(6.7186) | Total Time 0.00(0.00)\n",
      "Iter 13800 | Time 20.5072(21.1246) | Bit/dim 3.5666(3.6039) | Xent 0.2825(0.2517) | Loss 8.6855(8.8812) | Error 0.0922(0.0885) Steps 832(839.84) | Grad Norm 6.5891(6.6805) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 99.5391, Epoch Time 1281.8021(1256.8473), Bit/dim 3.6121(best: 3.5843), Xent 0.7603, Loss 3.9922, Error 0.2113(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 21.1241(21.1603) | Bit/dim 3.5654(3.6041) | Xent 0.2342(0.2525) | Loss 8.6274(9.5405) | Error 0.0844(0.0889) Steps 850(838.59) | Grad Norm 8.5934(6.5223) | Total Time 0.00(0.00)\n",
      "Iter 13820 | Time 20.9062(21.1707) | Bit/dim 3.6132(3.6049) | Xent 0.2373(0.2514) | Loss 8.8007(9.3064) | Error 0.0867(0.0885) Steps 844(837.23) | Grad Norm 6.5960(6.1396) | Total Time 0.00(0.00)\n",
      "Iter 13830 | Time 20.8285(21.1110) | Bit/dim 3.5734(3.6047) | Xent 0.2085(0.2473) | Loss 8.5402(9.1373) | Error 0.0722(0.0873) Steps 832(837.67) | Grad Norm 4.4374(5.9550) | Total Time 0.00(0.00)\n",
      "Iter 13840 | Time 21.4952(21.1322) | Bit/dim 3.6116(3.6077) | Xent 0.2323(0.2471) | Loss 8.7388(9.0132) | Error 0.0922(0.0877) Steps 820(833.95) | Grad Norm 6.0033(5.8152) | Total Time 0.00(0.00)\n",
      "Iter 13850 | Time 21.7043(21.1295) | Bit/dim 3.5955(3.6056) | Xent 0.2267(0.2474) | Loss 8.6291(8.9152) | Error 0.0889(0.0881) Steps 850(833.51) | Grad Norm 4.5286(5.6305) | Total Time 0.00(0.00)\n",
      "Iter 13860 | Time 20.2925(21.0517) | Bit/dim 3.5986(3.6056) | Xent 0.2306(0.2487) | Loss 8.5910(8.8507) | Error 0.0822(0.0869) Steps 838(831.27) | Grad Norm 5.7220(5.5248) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 99.4765, Epoch Time 1277.3037(1257.4610), Bit/dim 3.6154(best: 3.5843), Xent 0.7676, Loss 3.9992, Error 0.2120(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 21.0756(21.1047) | Bit/dim 3.6089(3.6059) | Xent 0.2237(0.2508) | Loss 8.6608(9.4246) | Error 0.0800(0.0885) Steps 850(833.63) | Grad Norm 6.0712(5.7978) | Total Time 0.00(0.00)\n",
      "Iter 13880 | Time 21.9447(21.1722) | Bit/dim 3.6530(3.6077) | Xent 0.2216(0.2508) | Loss 8.8635(9.2336) | Error 0.0800(0.0887) Steps 832(834.09) | Grad Norm 8.0594(5.9453) | Total Time 0.00(0.00)\n",
      "Iter 13890 | Time 22.1483(21.2066) | Bit/dim 3.6148(3.6076) | Xent 0.2254(0.2481) | Loss 8.7058(9.0850) | Error 0.0922(0.0871) Steps 862(836.59) | Grad Norm 7.7588(5.9204) | Total Time 0.00(0.00)\n",
      "Iter 13900 | Time 20.6006(21.2008) | Bit/dim 3.5937(3.6054) | Xent 0.2696(0.2507) | Loss 8.5206(8.9617) | Error 0.0944(0.0880) Steps 814(832.95) | Grad Norm 7.7264(6.2765) | Total Time 0.00(0.00)\n",
      "Iter 13910 | Time 20.9945(21.2284) | Bit/dim 3.5813(3.6086) | Xent 0.2457(0.2573) | Loss 8.5467(8.9023) | Error 0.0978(0.0904) Steps 838(835.91) | Grad Norm 6.2293(6.7647) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 101.0225, Epoch Time 1289.5932(1258.4250), Bit/dim 3.6317(best: 3.5843), Xent 0.7640, Loss 4.0137, Error 0.2157(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 21.3900(21.2774) | Bit/dim 3.6178(3.6135) | Xent 0.2507(0.2596) | Loss 8.7101(9.5819) | Error 0.0978(0.0920) Steps 862(838.96) | Grad Norm 5.3652(7.0916) | Total Time 0.00(0.00)\n",
      "Iter 13930 | Time 21.5586(21.3235) | Bit/dim 3.6404(3.6153) | Xent 0.2956(0.2619) | Loss 8.8848(9.3597) | Error 0.0989(0.0923) Steps 838(844.74) | Grad Norm 7.7426(6.8878) | Total Time 0.00(0.00)\n",
      "Iter 13940 | Time 22.7617(21.4482) | Bit/dim 3.6466(3.6230) | Xent 0.2851(0.2678) | Loss 8.8617(9.2124) | Error 0.1067(0.0946) Steps 910(849.16) | Grad Norm 6.3606(7.6192) | Total Time 0.00(0.00)\n",
      "Iter 13950 | Time 22.4349(21.5172) | Bit/dim 3.6501(3.6263) | Xent 0.2814(0.2724) | Loss 8.8378(9.0936) | Error 0.1078(0.0960) Steps 880(849.58) | Grad Norm 10.2156(8.7955) | Total Time 0.00(0.00)\n",
      "Iter 13960 | Time 21.2058(21.6990) | Bit/dim 3.6606(3.6275) | Xent 0.2986(0.2733) | Loss 8.6867(9.0121) | Error 0.1078(0.0965) Steps 844(858.19) | Grad Norm 8.5149(9.3104) | Total Time 0.00(0.00)\n",
      "Iter 13970 | Time 22.1851(21.8837) | Bit/dim 3.6766(3.6295) | Xent 0.2719(0.2718) | Loss 8.7912(8.9508) | Error 0.0956(0.0964) Steps 838(862.89) | Grad Norm 14.3826(10.1030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 102.3524, Epoch Time 1323.4261(1260.3750), Bit/dim 3.6306(best: 3.5843), Xent 0.7821, Loss 4.0216, Error 0.2165(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 23.0536(22.0432) | Bit/dim 3.6823(3.6308) | Xent 0.3130(0.2762) | Loss 8.9101(9.5195) | Error 0.1067(0.0982) Steps 874(864.48) | Grad Norm 19.2321(10.7423) | Total Time 0.00(0.00)\n",
      "Iter 13990 | Time 22.5712(22.2296) | Bit/dim 3.6688(3.6431) | Xent 0.3075(0.3001) | Loss 8.9124(9.3748) | Error 0.1144(0.1032) Steps 868(868.22) | Grad Norm 20.0337(19.5316) | Total Time 0.00(0.00)\n",
      "Iter 14000 | Time 22.7877(22.2927) | Bit/dim 3.6199(3.6404) | Xent 0.3245(0.2998) | Loss 8.8784(9.2185) | Error 0.1044(0.1038) Steps 850(862.56) | Grad Norm 10.4146(17.4859) | Total Time 0.00(0.00)\n",
      "Iter 14010 | Time 23.3874(22.3884) | Bit/dim 3.6964(3.6423) | Xent 0.3086(0.2993) | Loss 9.1603(9.1175) | Error 0.1111(0.1038) Steps 892(864.57) | Grad Norm 10.5746(16.5189) | Total Time 0.00(0.00)\n",
      "Iter 14020 | Time 23.2009(22.6747) | Bit/dim 3.7100(3.6605) | Xent 0.3396(0.3107) | Loss 9.0348(9.0869) | Error 0.1211(0.1075) Steps 904(870.73) | Grad Norm 45.1966(26.5563) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 105.4093, Epoch Time 1375.1813(1263.8192), Bit/dim 3.6566(best: 3.5843), Xent 0.7558, Loss 4.0345, Error 0.2186(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 21.8502(22.7097) | Bit/dim 3.6682(3.6581) | Xent 0.3505(0.3133) | Loss 8.8745(9.7443) | Error 0.1200(0.1083) Steps 880(871.01) | Grad Norm 21.2787(25.1697) | Total Time 0.00(0.00)\n",
      "Iter 14040 | Time 23.6520(22.8121) | Bit/dim 3.7264(3.6631) | Xent 0.3504(0.3133) | Loss 9.0150(9.5124) | Error 0.1222(0.1082) Steps 910(875.19) | Grad Norm 41.3308(25.7373) | Total Time 0.00(0.00)\n",
      "Iter 14050 | Time 25.0971(23.1777) | Bit/dim 3.7252(3.6785) | Xent 0.3579(0.3373) | Loss 9.0661(9.4061) | Error 0.1300(0.1145) Steps 946(886.93) | Grad Norm 29.5737(38.4097) | Total Time 0.00(0.00)\n",
      "Iter 14060 | Time 25.0358(23.3365) | Bit/dim 3.6455(3.6729) | Xent 0.3518(0.3386) | Loss 8.9606(9.2745) | Error 0.1244(0.1165) Steps 868(885.78) | Grad Norm 52.0072(36.5985) | Total Time 0.00(0.00)\n",
      "Iter 14070 | Time 23.8378(23.6879) | Bit/dim 3.6831(3.6822) | Xent 0.4196(0.3528) | Loss 9.0586(9.2173) | Error 0.1422(0.1193) Steps 910(892.17) | Grad Norm 107.1687(48.8798) | Total Time 0.00(0.00)\n",
      "Iter 14080 | Time 23.9319(23.7561) | Bit/dim 3.6309(3.6772) | Xent 0.3115(0.3483) | Loss 8.8772(9.1295) | Error 0.0978(0.1178) Steps 892(889.79) | Grad Norm 7.0607(44.2570) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 104.4825, Epoch Time 1432.7677(1268.8876), Bit/dim 3.6471(best: 3.5843), Xent 0.7521, Loss 4.0231, Error 0.2218(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 22.9501(23.6106) | Bit/dim 3.6951(3.6724) | Xent 0.3212(0.3333) | Loss 8.9457(9.6650) | Error 0.1078(0.1135) Steps 880(885.99) | Grad Norm 35.9677(37.4333) | Total Time 0.00(0.00)\n",
      "Iter 14100 | Time 25.1789(24.0012) | Bit/dim 3.7733(3.7083) | Xent 0.5521(0.3976) | Loss 9.2270(9.6107) | Error 0.1867(0.1267) Steps 934(896.51) | Grad Norm 200.8271(72.7465) | Total Time 0.00(0.00)\n",
      "Iter 14110 | Time 23.5533(24.0270) | Bit/dim 3.6508(3.7122) | Xent 0.3818(0.4079) | Loss 8.9884(9.4804) | Error 0.1344(0.1329) Steps 868(893.89) | Grad Norm 12.7811(63.4364) | Total Time 0.00(0.00)\n",
      "Iter 14120 | Time 22.0087(23.5897) | Bit/dim 3.6536(3.6975) | Xent 0.3335(0.3935) | Loss 8.8338(9.3190) | Error 0.1144(0.1307) Steps 838(883.53) | Grad Norm 11.2988(49.5939) | Total Time 0.00(0.00)\n",
      "Iter 14130 | Time 23.1244(23.3518) | Bit/dim 3.6345(3.6835) | Xent 0.3313(0.3709) | Loss 8.8635(9.1751) | Error 0.1156(0.1247) Steps 886(877.25) | Grad Norm 17.5259(39.2098) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 104.4491, Epoch Time 1410.9134(1273.1484), Bit/dim 3.6688(best: 3.5843), Xent 0.7518, Loss 4.0447, Error 0.2145(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 24.3069(23.3342) | Bit/dim 3.7063(3.6772) | Xent 0.3193(0.3527) | Loss 9.0846(9.8111) | Error 0.1078(0.1202) Steps 862(875.49) | Grad Norm 33.4235(34.5761) | Total Time 0.00(0.00)\n",
      "Iter 14150 | Time 26.9442(23.8941) | Bit/dim 3.9269(3.7162) | Xent 0.6417(0.3923) | Loss 9.8074(9.7021) | Error 0.1956(0.1318) Steps 988(891.35) | Grad Norm 258.0198(59.9123) | Total Time 0.00(0.00)\n",
      "Iter 14160 | Time 25.2369(24.2965) | Bit/dim 3.7526(3.7432) | Xent 0.4998(0.4286) | Loss 9.2543(9.6180) | Error 0.1767(0.1421) Steps 964(901.02) | Grad Norm 20.9865(71.6135) | Total Time 0.00(0.00)\n",
      "Iter 14170 | Time 25.4615(24.0844) | Bit/dim 3.6833(3.7264) | Xent 0.3073(0.4153) | Loss 8.9795(9.4409) | Error 0.1011(0.1395) Steps 970(900.59) | Grad Norm 7.1799(57.2046) | Total Time 0.00(0.00)\n",
      "Iter 14180 | Time 24.0525(24.1475) | Bit/dim 3.6697(3.7102) | Xent 0.3220(0.3898) | Loss 8.8850(9.2928) | Error 0.1133(0.1325) Steps 892(895.79) | Grad Norm 30.4342(48.0324) | Total Time 0.00(0.00)\n",
      "Iter 14190 | Time 27.3339(24.5142) | Bit/dim 4.0417(3.7386) | Xent 0.7886(0.4187) | Loss 10.1974(9.3200) | Error 0.2533(0.1410) Steps 1024(908.10) | Grad Norm 356.4506(66.0826) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 119.5829, Epoch Time 1497.5705(1279.8811), Bit/dim 4.0296(best: 3.5843), Xent 1.1455, Loss 4.6024, Error 0.3003(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 24.0857(24.9856) | Bit/dim 3.8674(3.8084) | Xent 0.6568(0.5440) | Loss 9.6487(10.1956) | Error 0.2022(0.1668) Steps 928(922.11) | Grad Norm 17.8644(125.4562) | Total Time 0.00(0.00)\n",
      "Iter 14210 | Time 22.6488(24.5833) | Bit/dim 3.6872(3.7973) | Xent 0.4584(0.5429) | Loss 9.0958(9.9530) | Error 0.1733(0.1723) Steps 910(912.97) | Grad Norm 12.7368(95.4578) | Total Time 0.00(0.00)\n",
      "Iter 14220 | Time 22.3382(24.1019) | Bit/dim 3.6816(3.7671) | Xent 0.4051(0.5017) | Loss 8.9156(9.6867) | Error 0.1433(0.1633) Steps 832(900.02) | Grad Norm 6.2955(72.6167) | Total Time 0.00(0.00)\n",
      "Iter 14230 | Time 23.0846(23.8232) | Bit/dim 3.6849(3.7431) | Xent 0.3354(0.4600) | Loss 8.9246(9.4886) | Error 0.1156(0.1516) Steps 892(896.78) | Grad Norm 6.4572(55.3280) | Total Time 0.00(0.00)\n",
      "Iter 14240 | Time 22.3892(23.6608) | Bit/dim 3.6702(3.7199) | Xent 0.3204(0.4224) | Loss 8.7527(9.3035) | Error 0.1267(0.1428) Steps 862(890.20) | Grad Norm 7.4234(42.9630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 104.8555, Epoch Time 1429.1533(1284.3592), Bit/dim 3.6733(best: 3.5843), Xent 0.6995, Loss 4.0231, Error 0.2151(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 23.2800(23.5808) | Bit/dim 3.7007(3.7081) | Xent 0.2964(0.3927) | Loss 8.9561(9.9085) | Error 0.1011(0.1337) Steps 910(886.85) | Grad Norm 38.5489(36.9045) | Total Time 0.00(0.00)\n",
      "Iter 14260 | Time 25.3006(23.9244) | Bit/dim 3.8129(3.7283) | Xent 0.4268(0.3899) | Loss 9.2995(9.7161) | Error 0.1522(0.1334) Steps 868(900.07) | Grad Norm 31.5623(34.4318) | Total Time 0.00(0.00)\n",
      "Iter 14270 | Time 24.7864(24.4411) | Bit/dim 3.7453(3.7464) | Xent 0.3381(0.3962) | Loss 9.1640(9.5997) | Error 0.1189(0.1351) Steps 934(916.50) | Grad Norm 25.5993(34.7833) | Total Time 0.00(0.00)\n",
      "Iter 14280 | Time 25.3483(24.5410) | Bit/dim 3.6478(3.7334) | Xent 0.3525(0.3875) | Loss 8.8698(9.4368) | Error 0.1311(0.1340) Steps 874(915.85) | Grad Norm 9.4823(34.2289) | Total Time 0.00(0.00)\n",
      "Iter 14290 | Time 24.0703(24.5341) | Bit/dim 3.6349(3.7155) | Xent 0.3378(0.3705) | Loss 8.7799(9.2897) | Error 0.1178(0.1283) Steps 856(909.28) | Grad Norm 10.4493(28.5616) | Total Time 0.00(0.00)\n",
      "Iter 14300 | Time 25.1260(24.4753) | Bit/dim 3.6397(3.7005) | Xent 0.3309(0.3565) | Loss 8.9111(9.1828) | Error 0.1322(0.1250) Steps 916(905.79) | Grad Norm 18.2652(26.4632) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 107.1816, Epoch Time 1485.8635(1290.4044), Bit/dim 3.6704(best: 3.5843), Xent 0.7326, Loss 4.0367, Error 0.2175(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 25.5224(24.7072) | Bit/dim 3.9014(3.7233) | Xent 0.5157(0.3709) | Loss 9.6517(9.8292) | Error 0.1622(0.1297) Steps 988(915.91) | Grad Norm 149.3977(37.2859) | Total Time 0.00(0.00)\n",
      "Iter 14320 | Time 26.3750(25.1658) | Bit/dim 3.8124(3.7726) | Xent 0.4462(0.4422) | Loss 9.3800(9.8060) | Error 0.1656(0.1468) Steps 916(928.87) | Grad Norm 91.7946(98.1049) | Total Time 0.00(0.00)\n",
      "Iter 14330 | Time 25.7547(25.3086) | Bit/dim 3.7039(3.7628) | Xent 0.4226(0.4361) | Loss 9.1137(9.6260) | Error 0.1533(0.1474) Steps 934(923.66) | Grad Norm 25.6630(81.9452) | Total Time 0.00(0.00)\n",
      "Iter 14340 | Time 25.7144(25.1359) | Bit/dim 3.6583(3.7363) | Xent 0.3426(0.4173) | Loss 8.9468(9.4264) | Error 0.1233(0.1424) Steps 922(918.97) | Grad Norm 12.2753(64.4475) | Total Time 0.00(0.00)\n",
      "Iter 14350 | Time 26.2270(25.0200) | Bit/dim 3.6697(3.7097) | Xent 0.3265(0.3896) | Loss 8.9999(9.2613) | Error 0.1178(0.1334) Steps 868(914.68) | Grad Norm 9.1770(50.3221) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 106.1253, Epoch Time 1514.1148(1297.1157), Bit/dim 3.6538(best: 3.5843), Xent 0.7092, Loss 4.0084, Error 0.2140(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 26.0657(24.9164) | Bit/dim 3.6565(3.6935) | Xent 0.3162(0.3672) | Loss 8.9129(9.8771) | Error 0.1189(0.1277) Steps 892(911.50) | Grad Norm 67.6290(46.0986) | Total Time 0.00(0.00)\n",
      "Iter 14370 | Time 27.3750(25.2744) | Bit/dim 3.8633(3.7261) | Xent 0.4797(0.3921) | Loss 9.6007(9.7380) | Error 0.1456(0.1343) Steps 1018(925.51) | Grad Norm 144.1346(65.4490) | Total Time 0.00(0.00)\n",
      "Iter 14380 | Time 25.9233(25.8849) | Bit/dim 3.7962(3.7543) | Xent 0.4356(0.4067) | Loss 9.2429(9.6452) | Error 0.1411(0.1379) Steps 958(940.36) | Grad Norm 56.7877(93.6443) | Total Time 0.00(0.00)\n",
      "Iter 14390 | Time 25.1304(25.9200) | Bit/dim 3.7271(3.7500) | Xent 0.3567(0.3971) | Loss 8.9967(9.4971) | Error 0.1244(0.1353) Steps 940(938.13) | Grad Norm 31.8792(82.1131) | Total Time 0.00(0.00)\n",
      "Iter 14400 | Time 25.2864(25.8880) | Bit/dim 3.6666(3.7304) | Xent 0.3316(0.3767) | Loss 8.9786(9.3493) | Error 0.1100(0.1285) Steps 940(936.51) | Grad Norm 26.7489(68.6313) | Total Time 0.00(0.00)\n",
      "Iter 14410 | Time 27.2379(25.8751) | Bit/dim 3.8096(3.7370) | Xent 0.3645(0.3722) | Loss 9.2445(9.2772) | Error 0.1367(0.1284) Steps 1012(937.13) | Grad Norm 119.6275(65.5149) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 118.2821, Epoch Time 1575.3906(1305.4639), Bit/dim 3.8384(best: 3.5843), Xent 0.7902, Loss 4.2335, Error 0.2303(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 26.0381(26.2845) | Bit/dim 3.7941(3.7627) | Xent 0.4032(0.3849) | Loss 9.2010(9.9377) | Error 0.1200(0.1320) Steps 958(947.61) | Grad Norm 97.1296(94.3652) | Total Time 0.00(0.00)\n",
      "Iter 14430 | Time 27.4424(26.3904) | Bit/dim 3.7143(3.7576) | Xent 0.3884(0.3824) | Loss 9.0356(9.7153) | Error 0.1233(0.1310) Steps 988(954.58) | Grad Norm 48.5434(96.9290) | Total Time 0.00(0.00)\n",
      "Iter 14440 | Time 26.3582(26.3902) | Bit/dim 3.6985(3.7447) | Xent 0.3003(0.3714) | Loss 8.9623(9.5323) | Error 0.1044(0.1271) Steps 928(952.16) | Grad Norm 15.7246(96.0222) | Total Time 0.00(0.00)\n",
      "Iter 14450 | Time 26.4501(26.3398) | Bit/dim 3.6783(3.7320) | Xent 0.3689(0.3604) | Loss 8.9683(9.3874) | Error 0.1200(0.1233) Steps 910(950.36) | Grad Norm 78.0252(86.9523) | Total Time 0.00(0.00)\n",
      "Iter 14460 | Time 29.9146(26.8587) | Bit/dim 4.5058(3.8095) | Xent 1.2609(0.4832) | Loss 11.5899(9.5864) | Error 0.3267(0.1506) Steps 994(958.10) | Grad Norm 1130.0052(209.1439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 120.4661, Epoch Time 1631.3773(1315.2413), Bit/dim 3.9266(best: 3.5843), Xent 0.9100, Loss 4.3816, Error 0.2814(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 25.0403(26.9592) | Bit/dim 3.8290(3.8628) | Xent 0.4590(0.5521) | Loss 9.1413(10.4672) | Error 0.1711(0.1717) Steps 916(965.93) | Grad Norm 44.5307(189.3843) | Total Time 0.00(0.00)\n",
      "Iter 14480 | Time 25.7902(26.4692) | Bit/dim 3.7494(3.8405) | Xent 0.4028(0.5338) | Loss 9.0648(10.1502) | Error 0.1378(0.1706) Steps 844(951.14) | Grad Norm 10.9703(147.0532) | Total Time 0.00(0.00)\n",
      "Iter 14490 | Time 23.6536(25.7869) | Bit/dim 3.6774(3.8010) | Xent 0.3844(0.4979) | Loss 8.9201(9.8384) | Error 0.1344(0.1630) Steps 856(934.37) | Grad Norm 6.3871(112.8360) | Total Time 0.00(0.00)\n",
      "Iter 14500 | Time 24.0827(25.2830) | Bit/dim 3.6415(3.7655) | Xent 0.3652(0.4577) | Loss 8.8226(9.5874) | Error 0.1367(0.1526) Steps 880(917.03) | Grad Norm 30.1812(85.9377) | Total Time 0.00(0.00)\n",
      "Iter 14510 | Time 24.7548(24.8638) | Bit/dim 3.6296(3.7349) | Xent 0.3091(0.4217) | Loss 8.8796(9.3902) | Error 0.1111(0.1430) Steps 910(910.27) | Grad Norm 12.9203(65.4515) | Total Time 0.00(0.00)\n",
      "Iter 14520 | Time 24.7845(24.7231) | Bit/dim 3.6527(3.7119) | Xent 0.3062(0.3906) | Loss 8.8458(9.2371) | Error 0.1144(0.1335) Steps 910(905.15) | Grad Norm 17.6540(51.3069) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 105.2514, Epoch Time 1461.4921(1319.6289), Bit/dim 3.6522(best: 3.5843), Xent 0.6899, Loss 3.9971, Error 0.2108(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 24.3057(24.6610) | Bit/dim 3.6652(3.6956) | Xent 0.3079(0.3651) | Loss 8.8843(9.7463) | Error 0.1189(0.1253) Steps 922(903.95) | Grad Norm 16.4518(42.2161) | Total Time 0.00(0.00)\n",
      "Iter 14540 | Time 24.7301(24.6736) | Bit/dim 3.7564(3.6949) | Xent 0.3796(0.3514) | Loss 9.0365(9.5218) | Error 0.1456(0.1217) Steps 922(907.51) | Grad Norm 42.2076(43.2790) | Total Time 0.00(0.00)\n",
      "Iter 14550 | Time 27.1102(25.2947) | Bit/dim 3.9958(3.7604) | Xent 0.6222(0.3923) | Loss 9.7687(9.5624) | Error 0.1822(0.1330) Steps 1036(924.57) | Grad Norm 302.5911(64.2118) | Total Time 0.00(0.00)\n",
      "Iter 14560 | Time 27.3502(26.0000) | Bit/dim 3.8460(3.7987) | Xent 0.4821(0.4354) | Loss 9.4168(9.5898) | Error 0.1656(0.1467) Steps 946(943.36) | Grad Norm 80.6983(94.1466) | Total Time 0.00(0.00)\n",
      "Iter 14570 | Time 25.8204(26.0547) | Bit/dim 3.7479(3.7925) | Xent 0.4018(0.4343) | Loss 9.1483(9.4911) | Error 0.1533(0.1486) Steps 910(944.48) | Grad Norm 67.8813(82.8396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 110.4296, Epoch Time 1564.0025(1326.9601), Bit/dim 3.6986(best: 3.5843), Xent 0.6890, Loss 4.0431, Error 0.2162(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 25.4731(26.0970) | Bit/dim 3.6807(3.7675) | Xent 0.3673(0.4145) | Loss 9.0168(10.0881) | Error 0.1156(0.1429) Steps 898(940.66) | Grad Norm 22.1702(68.2020) | Total Time 0.00(0.00)\n",
      "Iter 14590 | Time 24.9109(26.1073) | Bit/dim 3.6788(3.7406) | Xent 0.3024(0.3888) | Loss 8.7889(9.7687) | Error 0.1133(0.1349) Steps 928(939.25) | Grad Norm 46.0323(55.8457) | Total Time 0.00(0.00)\n",
      "Iter 14600 | Time 24.8466(25.9548) | Bit/dim 3.6925(3.7227) | Xent 0.2711(0.3679) | Loss 8.7982(9.5319) | Error 0.0989(0.1285) Steps 928(937.88) | Grad Norm 70.0433(51.2441) | Total Time 0.00(0.00)\n",
      "Iter 14610 | Time 27.1542(26.0446) | Bit/dim 4.1516(3.7577) | Xent 0.6773(0.3949) | Loss 10.2999(9.5086) | Error 0.2400(0.1378) Steps 982(942.58) | Grad Norm 215.2825(77.1774) | Total Time 0.00(0.00)\n",
      "Iter 14620 | Time 31.2050(26.8936) | Bit/dim 4.0492(3.8780) | Xent 0.8056(0.5493) | Loss 10.1482(9.8416) | Error 0.2433(0.1754) Steps 988(970.78) | Grad Norm 92.8515(177.9087) | Total Time 0.00(0.00)\n",
      "Iter 14630 | Time 26.8314(27.1563) | Bit/dim 3.8086(3.8757) | Xent 0.4944(0.5518) | Loss 9.4743(9.7725) | Error 0.1678(0.1796) Steps 982(977.96) | Grad Norm 17.8552(149.1092) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 114.0416, Epoch Time 1613.2066(1335.5475), Bit/dim 3.7837(best: 3.5843), Xent 0.7306, Loss 4.1490, Error 0.2354(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 25.4089(26.8487) | Bit/dim 3.6927(3.8366) | Xent 0.4155(0.5244) | Loss 8.9548(10.2046) | Error 0.1533(0.1740) Steps 910(963.90) | Grad Norm 15.8183(121.9182) | Total Time 0.00(0.00)\n",
      "Iter 14650 | Time 25.0058(26.3944) | Bit/dim 3.6984(3.7976) | Xent 0.3298(0.4845) | Loss 8.9387(9.8744) | Error 0.1111(0.1632) Steps 862(944.17) | Grad Norm 34.7250(97.3998) | Total Time 0.00(0.00)\n",
      "Iter 14660 | Time 24.8479(26.2424) | Bit/dim 3.6984(3.7641) | Xent 0.2893(0.4426) | Loss 8.8366(9.6161) | Error 0.1089(0.1507) Steps 892(937.86) | Grad Norm 15.7205(77.1896) | Total Time 0.00(0.00)\n",
      "Iter 14670 | Time 24.7261(26.0556) | Bit/dim 3.6870(3.7346) | Xent 0.3179(0.4067) | Loss 8.7721(9.4111) | Error 0.1133(0.1390) Steps 910(933.57) | Grad Norm 8.3217(61.5717) | Total Time 0.00(0.00)\n",
      "Iter 14680 | Time 25.1225(25.9183) | Bit/dim 3.6683(3.7151) | Xent 0.3473(0.3857) | Loss 8.9413(9.2632) | Error 0.1167(0.1319) Steps 928(929.89) | Grad Norm 22.0983(54.9958) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 110.7219, Epoch Time 1535.4484(1341.5445), Bit/dim 3.6716(best: 3.5843), Xent 0.6997, Loss 4.0215, Error 0.2103(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 25.1262(25.7454) | Bit/dim 3.7042(3.7060) | Xent 0.2574(0.3620) | Loss 8.9319(9.8703) | Error 0.0944(0.1247) Steps 934(928.02) | Grad Norm 101.6011(53.5370) | Total Time 0.00(0.00)\n",
      "Iter 14700 | Time 25.0776(25.6586) | Bit/dim 3.6625(3.6952) | Xent 0.3203(0.3482) | Loss 8.7208(9.6009) | Error 0.1189(0.1202) Steps 910(928.00) | Grad Norm 32.7595(50.9217) | Total Time 0.00(0.00)\n",
      "Iter 14710 | Time 28.7674(25.8989) | Bit/dim 3.9319(3.7132) | Xent 0.4872(0.3537) | Loss 9.5288(9.4642) | Error 0.1622(0.1232) Steps 994(935.16) | Grad Norm 139.4143(57.2655) | Total Time 0.00(0.00)\n",
      "Iter 14720 | Time 29.4913(26.6246) | Bit/dim 4.4947(3.8192) | Xent 2.0529(0.4937) | Loss 12.5902(9.7318) | Error 0.4067(0.1526) Steps 1072(959.74) | Grad Norm 1352.3598(227.6039) | Total Time 0.00(0.00)\n",
      "Iter 14730 | Time 26.7441(27.2464) | Bit/dim 3.9558(3.8934) | Xent 0.6847(0.6000) | Loss 9.9563(9.9162) | Error 0.2289(0.1809) Steps 1000(979.17) | Grad Norm 42.8101(207.0770) | Total Time 0.00(0.00)\n",
      "Iter 14740 | Time 25.7043(27.1608) | Bit/dim 3.8361(3.8824) | Xent 0.4713(0.5831) | Loss 9.3957(9.8012) | Error 0.1633(0.1819) Steps 916(972.42) | Grad Norm 13.1182(158.9447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 113.1398, Epoch Time 1622.9872(1349.9878), Bit/dim 3.7974(best: 3.5843), Xent 0.7243, Loss 4.1596, Error 0.2352(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 24.1362(26.7967) | Bit/dim 3.7633(3.8504) | Xent 0.4370(0.5409) | Loss 9.1650(10.2662) | Error 0.1556(0.1742) Steps 940(964.06) | Grad Norm 23.2348(120.8070) | Total Time 0.00(0.00)\n",
      "Iter 14760 | Time 26.5623(26.6803) | Bit/dim 3.6990(3.8142) | Xent 0.3276(0.4963) | Loss 9.0144(9.9469) | Error 0.1100(0.1617) Steps 874(953.24) | Grad Norm 7.9766(91.8682) | Total Time 0.00(0.00)\n",
      "Iter 14770 | Time 26.0186(26.3921) | Bit/dim 3.6581(3.7805) | Xent 0.3086(0.4570) | Loss 8.8051(9.6859) | Error 0.1144(0.1509) Steps 904(947.76) | Grad Norm 10.2323(70.2002) | Total Time 0.00(0.00)\n",
      "Iter 14780 | Time 25.9520(26.0981) | Bit/dim 3.6495(3.7522) | Xent 0.2788(0.4223) | Loss 8.8987(9.4761) | Error 0.1011(0.1418) Steps 964(943.94) | Grad Norm 5.1725(54.9225) | Total Time 0.00(0.00)\n",
      "Iter 14790 | Time 25.3423(25.9287) | Bit/dim 3.6517(3.7290) | Xent 0.2724(0.3955) | Loss 8.8036(9.3185) | Error 0.0900(0.1341) Steps 886(935.93) | Grad Norm 5.0254(42.5970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 108.3660, Epoch Time 1532.3473(1355.4586), Bit/dim 3.6663(best: 3.5843), Xent 0.6807, Loss 4.0067, Error 0.2111(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 24.6138(25.4752) | Bit/dim 3.6755(3.7136) | Xent 0.3095(0.3708) | Loss 8.9788(9.9165) | Error 0.1022(0.1268) Steps 934(926.28) | Grad Norm 6.3176(33.0003) | Total Time 0.00(0.00)\n",
      "Iter 14810 | Time 25.4558(25.2933) | Bit/dim 3.6612(3.6971) | Xent 0.3287(0.3545) | Loss 8.7673(9.6304) | Error 0.1144(0.1217) Steps 868(920.17) | Grad Norm 7.6874(26.2864) | Total Time 0.00(0.00)\n",
      "Iter 14820 | Time 25.6153(25.1705) | Bit/dim 3.6972(3.6877) | Xent 0.2839(0.3417) | Loss 8.9707(9.4282) | Error 0.0967(0.1176) Steps 928(917.23) | Grad Norm 22.3257(23.3816) | Total Time 0.00(0.00)\n",
      "Iter 14830 | Time 24.2145(25.1870) | Bit/dim 3.6741(3.6828) | Xent 0.2942(0.3304) | Loss 8.9840(9.2824) | Error 0.1111(0.1147) Steps 934(920.56) | Grad Norm 11.0865(20.8988) | Total Time 0.00(0.00)\n",
      "Iter 14840 | Time 25.9528(25.4469) | Bit/dim 3.6774(3.6805) | Xent 0.2900(0.3221) | Loss 8.7880(9.1708) | Error 0.1156(0.1125) Steps 916(928.00) | Grad Norm 28.1442(19.4162) | Total Time 0.00(0.00)\n",
      "Iter 14850 | Time 25.0636(25.6115) | Bit/dim 3.6679(3.6797) | Xent 0.2760(0.3203) | Loss 8.8138(9.0999) | Error 0.0856(0.1118) Steps 916(930.56) | Grad Norm 10.8580(17.6683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 113.5784, Epoch Time 1523.4795(1360.4992), Bit/dim 3.6869(best: 3.5843), Xent 0.7079, Loss 4.0408, Error 0.2154(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 25.7629(25.6635) | Bit/dim 3.6744(3.6812) | Xent 0.2866(0.3158) | Loss 8.7146(9.6952) | Error 0.1011(0.1112) Steps 898(928.26) | Grad Norm 12.8654(16.3933) | Total Time 0.00(0.00)\n",
      "Iter 14870 | Time 25.8606(25.8754) | Bit/dim 3.6635(3.6808) | Xent 0.2848(0.3178) | Loss 8.8644(9.4955) | Error 0.1044(0.1117) Steps 904(932.03) | Grad Norm 9.0454(14.6241) | Total Time 0.00(0.00)\n",
      "Iter 14880 | Time 25.4514(25.9473) | Bit/dim 3.6269(3.6787) | Xent 0.2931(0.3150) | Loss 8.7244(9.3289) | Error 0.1022(0.1111) Steps 910(936.17) | Grad Norm 9.8892(15.7331) | Total Time 0.00(0.00)\n",
      "Iter 14890 | Time 26.2073(25.8880) | Bit/dim 3.6883(3.6745) | Xent 0.3020(0.3078) | Loss 8.9804(9.1933) | Error 0.0989(0.1087) Steps 994(938.16) | Grad Norm 11.8233(16.9269) | Total Time 0.00(0.00)\n",
      "Iter 14900 | Time 28.1867(25.9051) | Bit/dim 3.6223(3.6684) | Xent 0.2880(0.3034) | Loss 8.8135(9.1006) | Error 0.1067(0.1071) Steps 946(935.57) | Grad Norm 11.8441(16.4865) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 112.5653, Epoch Time 1563.3265(1366.5840), Bit/dim 3.6822(best: 3.5843), Xent 0.7233, Loss 4.0439, Error 0.2135(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 25.4154(26.0177) | Bit/dim 3.6667(3.6704) | Xent 0.3420(0.3028) | Loss 8.9516(9.7813) | Error 0.1133(0.1072) Steps 922(941.10) | Grad Norm 26.4263(16.2296) | Total Time 0.00(0.00)\n",
      "Iter 14920 | Time 24.0808(25.9105) | Bit/dim 3.6453(3.6695) | Xent 0.3572(0.3042) | Loss 8.8031(9.5466) | Error 0.1278(0.1068) Steps 916(936.19) | Grad Norm 23.6823(18.1907) | Total Time 0.00(0.00)\n",
      "Iter 14930 | Time 25.0061(25.9356) | Bit/dim 3.6758(3.6714) | Xent 0.2883(0.3013) | Loss 8.8119(9.3682) | Error 0.1011(0.1056) Steps 898(939.89) | Grad Norm 29.5263(17.8021) | Total Time 0.00(0.00)\n",
      "Iter 14940 | Time 25.9533(25.8287) | Bit/dim 3.6886(3.6713) | Xent 0.3179(0.3020) | Loss 8.8401(9.2419) | Error 0.1167(0.1067) Steps 934(938.14) | Grad Norm 46.3365(20.7304) | Total Time 0.00(0.00)\n",
      "Iter 14950 | Time 26.6762(26.0073) | Bit/dim 3.7114(3.6762) | Xent 0.3521(0.3054) | Loss 9.0588(9.1563) | Error 0.1200(0.1085) Steps 1000(941.74) | Grad Norm 41.0252(30.5053) | Total Time 0.00(0.00)\n",
      "Iter 14960 | Time 24.7830(26.2056) | Bit/dim 3.7007(3.6869) | Xent 0.3458(0.3153) | Loss 8.8156(9.1303) | Error 0.1156(0.1113) Steps 928(949.69) | Grad Norm 46.7797(41.9715) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 117.9560, Epoch Time 1572.1325(1372.7505), Bit/dim 3.7339(best: 3.5843), Xent 0.7347, Loss 4.1013, Error 0.2215(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 27.2809(26.3701) | Bit/dim 3.7506(3.6952) | Xent 0.3659(0.3180) | Loss 9.2809(9.7482) | Error 0.1322(0.1121) Steps 964(952.52) | Grad Norm 82.6375(51.4584) | Total Time 0.00(0.00)\n",
      "Iter 14980 | Time 28.0085(26.5584) | Bit/dim 3.7034(3.6994) | Xent 0.3393(0.3210) | Loss 8.9185(9.5465) | Error 0.1289(0.1128) Steps 1024(959.95) | Grad Norm 147.3457(64.8191) | Total Time 0.00(0.00)\n",
      "Iter 14990 | Time 29.4000(26.9963) | Bit/dim 3.8641(3.7187) | Xent 0.4520(0.3335) | Loss 9.5767(9.4665) | Error 0.1467(0.1167) Steps 976(971.84) | Grad Norm 86.4666(88.2571) | Total Time 0.00(0.00)\n",
      "Iter 15000 | Time 29.4435(27.5749) | Bit/dim 3.8307(3.7565) | Xent 0.4105(0.3661) | Loss 9.3568(9.4632) | Error 0.1378(0.1259) Steps 958(985.32) | Grad Norm 188.3243(134.0577) | Total Time 0.00(0.00)\n",
      "Iter 15010 | Time 27.8004(27.7089) | Bit/dim 3.7334(3.7613) | Xent 0.3074(0.3670) | Loss 8.9544(9.3801) | Error 0.1022(0.1258) Steps 958(988.94) | Grad Norm 32.1308(123.7794) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 113.6740, Epoch Time 1664.9840(1381.5175), Bit/dim 3.7125(best: 3.5843), Xent 0.7000, Loss 4.0626, Error 0.2167(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 27.1771(27.6338) | Bit/dim 3.6802(3.7454) | Xent 0.2940(0.3594) | Loss 8.9701(10.0083) | Error 0.1156(0.1240) Steps 958(982.75) | Grad Norm 34.8576(107.6252) | Total Time 0.00(0.00)\n",
      "Iter 15030 | Time 28.2904(27.3947) | Bit/dim 3.6745(3.7285) | Xent 0.3177(0.3461) | Loss 8.9419(9.7178) | Error 0.1122(0.1198) Steps 958(975.92) | Grad Norm 14.6052(85.4365) | Total Time 0.00(0.00)\n",
      "Iter 15040 | Time 26.8432(27.2418) | Bit/dim 3.6427(3.7112) | Xent 0.2859(0.3295) | Loss 8.9506(9.4917) | Error 0.1078(0.1156) Steps 952(972.53) | Grad Norm 30.3062(71.0701) | Total Time 0.00(0.00)\n",
      "Iter 15050 | Time 27.7273(27.1052) | Bit/dim 3.6699(3.7010) | Xent 0.3683(0.3234) | Loss 8.9332(9.3278) | Error 0.1300(0.1131) Steps 1012(968.57) | Grad Norm 141.6013(75.8584) | Total Time 0.00(0.00)\n",
      "Iter 15060 | Time 30.3645(27.2964) | Bit/dim 3.8550(3.7119) | Xent 0.4171(0.3290) | Loss 9.4745(9.2713) | Error 0.1489(0.1165) Steps 1066(973.00) | Grad Norm 127.4860(85.5396) | Total Time 0.00(0.00)\n",
      "Iter 15070 | Time 28.4147(27.9044) | Bit/dim 3.8876(3.7592) | Xent 0.4680(0.3692) | Loss 9.5342(9.3527) | Error 0.1578(0.1265) Steps 1036(985.46) | Grad Norm 104.4360(117.7606) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 124.3681, Epoch Time 1652.9296(1389.6598), Bit/dim 3.8874(best: 3.5843), Xent 0.8050, Loss 4.2899, Error 0.2402(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 28.8225(28.3433) | Bit/dim 3.8359(3.7942) | Xent 0.4053(0.4193) | Loss 9.3220(10.0820) | Error 0.1511(0.1378) Steps 958(993.20) | Grad Norm 22.0987(184.0931) | Total Time 0.00(0.00)\n",
      "Iter 15090 | Time 26.3937(28.3151) | Bit/dim 3.7607(3.7866) | Xent 0.3238(0.4121) | Loss 8.9467(9.8370) | Error 0.1111(0.1379) Steps 934(989.69) | Grad Norm 18.2632(143.3051) | Total Time 0.00(0.00)\n",
      "Iter 15100 | Time 28.0349(28.1857) | Bit/dim 3.7220(3.7657) | Xent 0.3288(0.3970) | Loss 8.9957(9.6336) | Error 0.1122(0.1339) Steps 1018(991.19) | Grad Norm 7.2985(112.6400) | Total Time 0.00(0.00)\n",
      "Iter 15110 | Time 27.3973(27.8689) | Bit/dim 3.6560(3.7456) | Xent 0.3212(0.3754) | Loss 8.8424(9.4491) | Error 0.1189(0.1284) Steps 952(984.15) | Grad Norm 7.4520(85.5864) | Total Time 0.00(0.00)\n",
      "Iter 15120 | Time 26.6459(27.5014) | Bit/dim 3.6535(3.7275) | Xent 0.2963(0.3559) | Loss 8.8285(9.2977) | Error 0.0989(0.1237) Steps 928(971.71) | Grad Norm 65.8789(69.3546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 112.4019, Epoch Time 1651.8301(1397.5249), Bit/dim 3.6806(best: 3.5843), Xent 0.7100, Loss 4.0356, Error 0.2125(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15130 | Time 26.5502(27.2100) | Bit/dim 3.6572(3.7132) | Xent 0.3286(0.3425) | Loss 8.8760(9.9309) | Error 0.1067(0.1180) Steps 922(966.06) | Grad Norm 22.2269(58.5691) | Total Time 0.00(0.00)\n",
      "Iter 15140 | Time 31.2134(27.3643) | Bit/dim 3.7650(3.7121) | Xent 0.3575(0.3383) | Loss 9.1053(9.6798) | Error 0.1333(0.1176) Steps 982(967.61) | Grad Norm 65.1143(59.8377) | Total Time 0.00(0.00)\n",
      "Iter 15150 | Time 28.2270(27.7440) | Bit/dim 3.7587(3.7315) | Xent 0.3431(0.3497) | Loss 9.1240(9.5583) | Error 0.1211(0.1219) Steps 982(973.41) | Grad Norm 242.0320(70.4102) | Total Time 0.00(0.00)\n",
      "Iter 15160 | Time 29.6026(27.9353) | Bit/dim 3.7116(3.7317) | Xent 0.2725(0.3406) | Loss 9.0184(9.4250) | Error 0.1044(0.1185) Steps 1048(980.04) | Grad Norm 159.2635(70.2864) | Total Time 0.00(0.00)\n",
      "Iter 15170 | Time 25.8654(27.7465) | Bit/dim 3.6605(3.7229) | Xent 0.2954(0.3320) | Loss 8.9278(9.3038) | Error 0.1067(0.1161) Steps 952(975.71) | Grad Norm 70.7140(62.7843) | Total Time 0.00(0.00)\n",
      "Iter 15180 | Time 27.4003(27.4861) | Bit/dim 3.6929(3.7083) | Xent 0.2788(0.3235) | Loss 8.9459(9.1919) | Error 0.0967(0.1136) Steps 958(967.77) | Grad Norm 50.6719(61.2822) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 113.3586, Epoch Time 1654.7363(1405.2413), Bit/dim 3.6666(best: 3.5843), Xent 0.6993, Loss 4.0162, Error 0.2144(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15190 | Time 27.0936(27.3030) | Bit/dim 3.7399(3.7027) | Xent 0.3560(0.3194) | Loss 9.1118(9.7293) | Error 0.1200(0.1129) Steps 1006(965.09) | Grad Norm 32.4483(76.7972) | Total Time 0.00(0.00)\n",
      "Iter 15200 | Time 32.0247(27.9889) | Bit/dim 4.1686(3.7849) | Xent 0.9548(0.4196) | Loss 10.5909(9.8218) | Error 0.2622(0.1384) Steps 1000(986.15) | Grad Norm 754.8004(203.3426) | Total Time 0.00(0.00)\n",
      "Iter 15210 | Time 29.1143(28.6235) | Bit/dim 3.8904(3.8464) | Xent 0.5335(0.4733) | Loss 9.5746(9.8689) | Error 0.1933(0.1555) Steps 1012(1006.37) | Grad Norm 45.8117(187.9221) | Total Time 0.00(0.00)\n",
      "Iter 15220 | Time 27.0193(28.5771) | Bit/dim 3.7986(3.8370) | Xent 0.3746(0.4683) | Loss 9.2292(9.7313) | Error 0.1422(0.1570) Steps 1000(1003.76) | Grad Norm 15.5759(146.3532) | Total Time 0.00(0.00)\n",
      "Iter 15230 | Time 28.1021(28.3578) | Bit/dim 3.7156(3.8067) | Xent 0.3855(0.4468) | Loss 9.0189(9.5509) | Error 0.1289(0.1513) Steps 976(996.83) | Grad Norm 12.2645(114.7419) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 114.0144, Epoch Time 1698.8910(1414.0508), Bit/dim 3.6912(best: 3.5843), Xent 0.6898, Loss 4.0361, Error 0.2140(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15240 | Time 27.0239(28.0404) | Bit/dim 3.6827(3.7744) | Xent 0.3219(0.4114) | Loss 8.9364(10.1235) | Error 0.1322(0.1412) Steps 958(983.39) | Grad Norm 11.3208(89.1382) | Total Time 0.00(0.00)\n",
      "Iter 15250 | Time 28.4110(27.8550) | Bit/dim 3.7003(3.7507) | Xent 0.3057(0.3856) | Loss 9.0801(9.8094) | Error 0.1122(0.1330) Steps 994(974.90) | Grad Norm 32.3124(87.2154) | Total Time 0.00(0.00)\n",
      "Iter 15260 | Time 28.2248(27.8575) | Bit/dim 3.7461(3.7446) | Xent 0.3552(0.3733) | Loss 9.0764(9.6009) | Error 0.1178(0.1288) Steps 1024(972.12) | Grad Norm 31.5382(78.7123) | Total Time 0.00(0.00)\n",
      "Iter 15270 | Time 27.9708(28.0351) | Bit/dim 3.7875(3.7495) | Xent 0.4389(0.3710) | Loss 9.3805(9.4868) | Error 0.1433(0.1278) Steps 1012(983.16) | Grad Norm 134.9661(86.1427) | Total Time 0.00(0.00)\n",
      "Iter 15280 | Time 27.4781(27.9515) | Bit/dim 3.7255(3.7467) | Xent 0.3068(0.3629) | Loss 9.0223(9.3694) | Error 0.1078(0.1252) Steps 994(983.22) | Grad Norm 64.7363(84.3756) | Total Time 0.00(0.00)\n",
      "Iter 15290 | Time 27.8395(28.0350) | Bit/dim 3.6999(3.7304) | Xent 0.3445(0.3532) | Loss 9.1284(9.2669) | Error 0.1233(0.1230) Steps 1000(985.20) | Grad Norm 87.9550(78.6754) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 117.6929, Epoch Time 1669.0187(1421.6998), Bit/dim 3.6907(best: 3.5843), Xent 0.7077, Loss 4.0445, Error 0.2132(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15300 | Time 28.7180(27.8495) | Bit/dim 3.6728(3.7166) | Xent 0.2973(0.3388) | Loss 9.0476(9.7888) | Error 0.0967(0.1176) Steps 970(975.88) | Grad Norm 79.8498(79.2442) | Total Time 0.00(0.00)\n",
      "Iter 15310 | Time 30.4069(28.2030) | Bit/dim 4.5005(3.7674) | Xent 1.1884(0.3758) | Loss 11.4925(9.7301) | Error 0.2667(0.1246) Steps 1042(983.25) | Grad Norm 2999.1385(201.1653) | Total Time 0.00(0.00)\n",
      "Iter 15320 | Time 27.9932(28.6717) | Bit/dim 4.0016(4.0689) | Xent 0.9098(0.7896) | Loss 10.1654(10.6246) | Error 0.2889(0.1854) Steps 940(1002.78) | Grad Norm 23.0733(500.8335) | Total Time 0.00(0.00)\n",
      "Iter 15330 | Time 25.3241(28.0861) | Bit/dim 3.8594(4.0264) | Xent 0.6859(0.7733) | Loss 9.7107(10.4025) | Error 0.2433(0.2015) Steps 934(993.76) | Grad Norm 16.0288(375.7470) | Total Time 0.00(0.00)\n",
      "Iter 15340 | Time 25.0831(27.2582) | Bit/dim 3.7817(3.9668) | Xent 0.5170(0.7263) | Loss 9.3264(10.1326) | Error 0.1800(0.2044) Steps 940(971.27) | Grad Norm 8.5814(280.1787) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 105.6336, Epoch Time 1626.5002(1427.8438), Bit/dim 3.7522(best: 3.5843), Xent 0.7165, Loss 4.1104, Error 0.2338(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15350 | Time 25.1753(26.6246) | Bit/dim 3.7323(3.9087) | Xent 0.3939(0.6493) | Loss 9.0742(10.6028) | Error 0.1322(0.1924) Steps 904(953.29) | Grad Norm 5.6109(208.3366) | Total Time 0.00(0.00)\n",
      "Iter 15360 | Time 23.0200(25.9668) | Bit/dim 3.7229(3.8623) | Xent 0.3594(0.5804) | Loss 9.1370(10.1918) | Error 0.1367(0.1785) Steps 862(932.96) | Grad Norm 5.5836(155.1328) | Total Time 0.00(0.00)\n",
      "Iter 15370 | Time 24.7374(25.4609) | Bit/dim 3.7077(3.8203) | Xent 0.4207(0.5255) | Loss 8.9343(9.8627) | Error 0.1489(0.1658) Steps 856(915.15) | Grad Norm 4.9490(115.6938) | Total Time 0.00(0.00)\n",
      "Iter 15380 | Time 23.5351(25.1029) | Bit/dim 3.6593(3.7850) | Xent 0.3613(0.4872) | Loss 8.9118(9.6227) | Error 0.1167(0.1570) Steps 898(907.95) | Grad Norm 4.1422(86.5984) | Total Time 0.00(0.00)\n",
      "Iter 15390 | Time 25.4404(24.9648) | Bit/dim 3.6723(3.7593) | Xent 0.3163(0.4498) | Loss 8.7423(9.4302) | Error 0.1100(0.1476) Steps 910(901.16) | Grad Norm 3.3532(65.4295) | Total Time 0.00(0.00)\n",
      "Iter 15400 | Time 23.2797(24.8933) | Bit/dim 3.6881(3.7363) | Xent 0.3441(0.4236) | Loss 8.8820(9.2960) | Error 0.1300(0.1415) Steps 844(897.56) | Grad Norm 5.2260(50.3439) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 106.1824, Epoch Time 1465.1414(1428.9627), Bit/dim 3.6911(best: 3.5843), Xent 0.6965, Loss 4.0393, Error 0.2150(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15410 | Time 24.3571(24.7595) | Bit/dim 3.6948(3.7217) | Xent 0.3722(0.4002) | Loss 8.9585(9.8075) | Error 0.1289(0.1360) Steps 886(895.01) | Grad Norm 10.1722(40.6005) | Total Time 0.00(0.00)\n",
      "Iter 15420 | Time 24.8357(24.7074) | Bit/dim 3.7012(3.7143) | Xent 0.3119(0.3817) | Loss 8.8390(9.5660) | Error 0.1067(0.1298) Steps 892(896.39) | Grad Norm 8.5272(34.3688) | Total Time 0.00(0.00)\n",
      "Iter 15430 | Time 25.8022(24.8255) | Bit/dim 3.7019(3.7125) | Xent 0.3057(0.3672) | Loss 9.0360(9.4062) | Error 0.1067(0.1257) Steps 892(902.21) | Grad Norm 10.6568(29.3998) | Total Time 0.00(0.00)\n",
      "Iter 15440 | Time 26.8139(25.1499) | Bit/dim 3.7392(3.7109) | Xent 0.3739(0.3615) | Loss 9.1809(9.2960) | Error 0.1222(0.1244) Steps 952(911.15) | Grad Norm 7.0546(24.0027) | Total Time 0.00(0.00)\n",
      "Iter 15450 | Time 26.7700(25.3668) | Bit/dim 3.7104(3.7091) | Xent 0.3300(0.3500) | Loss 9.0308(9.2071) | Error 0.1189(0.1215) Steps 952(919.85) | Grad Norm 32.7692(21.1986) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 111.7376, Epoch Time 1519.7615(1431.6867), Bit/dim 3.7143(best: 3.5843), Xent 0.7142, Loss 4.0713, Error 0.2155(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15460 | Time 24.8742(25.4615) | Bit/dim 3.6996(3.7105) | Xent 0.3003(0.3474) | Loss 8.9898(9.8806) | Error 0.1089(0.1217) Steps 928(926.85) | Grad Norm 10.0769(17.9099) | Total Time 0.00(0.00)\n",
      "Iter 15470 | Time 26.6341(25.7719) | Bit/dim 3.6916(3.7065) | Xent 0.3583(0.3461) | Loss 8.9523(9.6454) | Error 0.1189(0.1216) Steps 898(930.26) | Grad Norm 8.9345(15.5627) | Total Time 0.00(0.00)\n",
      "Iter 15480 | Time 26.6225(25.9325) | Bit/dim 3.6995(3.7078) | Xent 0.2933(0.3396) | Loss 8.9561(9.4697) | Error 0.1089(0.1188) Steps 922(934.76) | Grad Norm 27.2468(14.2078) | Total Time 0.00(0.00)\n",
      "Iter 15490 | Time 26.3347(25.9709) | Bit/dim 3.6688(3.7029) | Xent 0.3625(0.3409) | Loss 8.9114(9.3407) | Error 0.1267(0.1193) Steps 934(934.57) | Grad Norm 8.8813(13.2007) | Total Time 0.00(0.00)\n",
      "Iter 15500 | Time 25.3413(26.0365) | Bit/dim 3.7017(3.6997) | Xent 0.3222(0.3315) | Loss 9.0428(9.2331) | Error 0.1100(0.1158) Steps 958(936.86) | Grad Norm 7.7807(12.1059) | Total Time 0.00(0.00)\n",
      "Iter 15510 | Time 26.9072(26.1550) | Bit/dim 3.7202(3.7021) | Xent 0.3255(0.3299) | Loss 9.0282(9.1684) | Error 0.1156(0.1153) Steps 1000(939.87) | Grad Norm 7.4974(11.6730) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 113.9727, Epoch Time 1578.2877(1436.0847), Bit/dim 3.7088(best: 3.5843), Xent 0.7140, Loss 4.0658, Error 0.2183(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15520 | Time 26.1670(26.3382) | Bit/dim 3.6663(3.6988) | Xent 0.3574(0.3282) | Loss 9.0649(9.7453) | Error 0.1278(0.1151) Steps 934(939.37) | Grad Norm 11.4714(10.8016) | Total Time 0.00(0.00)\n",
      "Iter 15530 | Time 25.0818(26.2485) | Bit/dim 3.6567(3.6963) | Xent 0.2816(0.3259) | Loss 8.7459(9.5229) | Error 0.1100(0.1154) Steps 916(943.02) | Grad Norm 29.2382(11.8372) | Total Time 0.00(0.00)\n",
      "Iter 15540 | Time 27.7942(26.3906) | Bit/dim 3.6819(3.6977) | Xent 0.3191(0.3246) | Loss 8.9246(9.3693) | Error 0.1200(0.1147) Steps 988(944.46) | Grad Norm 8.1586(13.0336) | Total Time 0.00(0.00)\n",
      "Iter 15550 | Time 28.1115(26.4629) | Bit/dim 3.6964(3.6999) | Xent 0.3028(0.3220) | Loss 8.8658(9.2639) | Error 0.1189(0.1133) Steps 988(948.74) | Grad Norm 41.6347(14.0117) | Total Time 0.00(0.00)\n",
      "Iter 15560 | Time 27.1876(26.6279) | Bit/dim 3.7089(3.7002) | Xent 0.2896(0.3243) | Loss 8.9103(9.1851) | Error 0.1056(0.1137) Steps 946(946.28) | Grad Norm 19.6458(14.0832) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 115.0471, Epoch Time 1601.0406(1441.0334), Bit/dim 3.7062(best: 3.5843), Xent 0.7214, Loss 4.0669, Error 0.2150(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15570 | Time 24.5138(26.6133) | Bit/dim 3.6845(3.6996) | Xent 0.3184(0.3207) | Loss 8.8666(9.8537) | Error 0.1000(0.1116) Steps 940(946.41) | Grad Norm 33.5076(13.9721) | Total Time 0.00(0.00)\n",
      "Iter 15580 | Time 27.1929(26.7897) | Bit/dim 3.6808(3.6945) | Xent 0.2488(0.3168) | Loss 8.8762(9.6108) | Error 0.0856(0.1103) Steps 982(950.77) | Grad Norm 13.4209(12.5798) | Total Time 0.00(0.00)\n",
      "Iter 15590 | Time 28.1980(26.8881) | Bit/dim 3.6833(3.6921) | Xent 0.3439(0.3187) | Loss 8.9273(9.4397) | Error 0.1056(0.1104) Steps 970(952.86) | Grad Norm 13.4871(12.5621) | Total Time 0.00(0.00)\n",
      "Iter 15600 | Time 27.5797(26.9279) | Bit/dim 3.6686(3.6903) | Xent 0.2781(0.3115) | Loss 8.9219(9.2946) | Error 0.0978(0.1083) Steps 1000(959.00) | Grad Norm 8.9315(11.3911) | Total Time 0.00(0.00)\n",
      "Iter 15610 | Time 28.2052(26.9738) | Bit/dim 3.6945(3.6865) | Xent 0.2882(0.3127) | Loss 8.9997(9.2052) | Error 0.1022(0.1092) Steps 982(958.39) | Grad Norm 5.8260(12.5056) | Total Time 0.00(0.00)\n",
      "Iter 15620 | Time 27.1998(26.9236) | Bit/dim 3.6920(3.6846) | Xent 0.2786(0.3122) | Loss 8.9167(9.1278) | Error 0.0978(0.1094) Steps 922(954.72) | Grad Norm 10.5539(14.0075) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 114.3837, Epoch Time 1616.8879(1446.3091), Bit/dim 3.6875(best: 3.5843), Xent 0.7255, Loss 4.0502, Error 0.2162(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15630 | Time 27.4649(27.1544) | Bit/dim 3.6667(3.6851) | Xent 0.3222(0.3095) | Loss 8.8883(9.7020) | Error 0.1167(0.1087) Steps 904(956.66) | Grad Norm 16.8664(13.4451) | Total Time 0.00(0.00)\n",
      "Iter 15640 | Time 24.4284(27.0897) | Bit/dim 3.6926(3.6830) | Xent 0.2853(0.3069) | Loss 8.7627(9.4978) | Error 0.0933(0.1078) Steps 916(957.08) | Grad Norm 15.6472(13.2434) | Total Time 0.00(0.00)\n",
      "Iter 15650 | Time 27.6427(27.2593) | Bit/dim 3.7148(3.6811) | Xent 0.3061(0.3080) | Loss 8.7807(9.3435) | Error 0.1100(0.1074) Steps 892(955.60) | Grad Norm 12.4641(13.9014) | Total Time 0.00(0.00)\n",
      "Iter 15660 | Time 27.0217(27.2207) | Bit/dim 3.7056(3.6820) | Xent 0.3221(0.3094) | Loss 9.0394(9.2339) | Error 0.1156(0.1076) Steps 982(956.56) | Grad Norm 13.2065(14.2084) | Total Time 0.00(0.00)\n",
      "Iter 15670 | Time 26.8235(27.1038) | Bit/dim 3.6599(3.6822) | Xent 0.3356(0.3107) | Loss 8.9452(9.1549) | Error 0.1122(0.1081) Steps 970(958.19) | Grad Norm 7.6152(15.7403) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 112.3839, Epoch Time 1629.3740(1451.8010), Bit/dim 3.6754(best: 3.5843), Xent 0.7138, Loss 4.0323, Error 0.2140(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15680 | Time 28.3715(27.1617) | Bit/dim 3.6842(3.6769) | Xent 0.2857(0.3059) | Loss 8.9610(9.8133) | Error 0.1078(0.1066) Steps 994(954.58) | Grad Norm 22.5137(14.8072) | Total Time 0.00(0.00)\n",
      "Iter 15690 | Time 27.3595(26.9857) | Bit/dim 3.6517(3.6762) | Xent 0.2819(0.3025) | Loss 8.9885(9.5662) | Error 0.0967(0.1058) Steps 1018(951.70) | Grad Norm 32.3234(21.3174) | Total Time 0.00(0.00)\n",
      "Iter 15700 | Time 27.7794(27.3681) | Bit/dim 3.7608(3.6922) | Xent 0.3575(0.3125) | Loss 9.2056(9.4532) | Error 0.1200(0.1086) Steps 988(960.72) | Grad Norm 22.4446(28.9338) | Total Time 0.00(0.00)\n",
      "Iter 15710 | Time 30.2888(27.7982) | Bit/dim 3.8232(3.7165) | Xent 0.3792(0.3282) | Loss 9.3975(9.3939) | Error 0.1267(0.1134) Steps 1006(975.64) | Grad Norm 17.1882(27.2439) | Total Time 0.00(0.00)\n",
      "Iter 15720 | Time 29.4076(27.9559) | Bit/dim 3.7407(3.7271) | Xent 0.3485(0.3375) | Loss 9.0206(9.3215) | Error 0.1200(0.1159) Steps 994(979.19) | Grad Norm 14.6892(24.9496) | Total Time 0.00(0.00)\n",
      "Iter 15730 | Time 28.2386(28.0639) | Bit/dim 3.7241(3.7289) | Xent 0.3485(0.3428) | Loss 9.0843(9.2647) | Error 0.1122(0.1166) Steps 994(982.96) | Grad Norm 63.3215(24.7567) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 117.6504, Epoch Time 1678.4366(1458.6001), Bit/dim 3.7290(best: 3.5843), Xent 0.7340, Loss 4.0960, Error 0.2192(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15740 | Time 27.9846(28.0903) | Bit/dim 3.7101(3.7232) | Xent 0.3142(0.3339) | Loss 8.9894(9.8159) | Error 0.1189(0.1145) Steps 1018(983.05) | Grad Norm 27.9576(22.5426) | Total Time 0.00(0.00)\n",
      "Iter 15750 | Time 27.8535(28.0420) | Bit/dim 3.7149(3.7148) | Xent 0.2864(0.3307) | Loss 8.9943(9.5973) | Error 0.1067(0.1138) Steps 1012(982.67) | Grad Norm 15.4819(26.9887) | Total Time 0.00(0.00)\n",
      "Iter 15760 | Time 28.5626(28.0322) | Bit/dim 3.6836(3.7113) | Xent 0.3280(0.3254) | Loss 8.9287(9.4245) | Error 0.1167(0.1127) Steps 1048(983.09) | Grad Norm 16.9695(26.4303) | Total Time 0.00(0.00)\n",
      "Iter 15770 | Time 28.8201(28.2558) | Bit/dim 3.7391(3.7154) | Xent 0.3461(0.3259) | Loss 9.0796(9.3310) | Error 0.1189(0.1133) Steps 1000(987.95) | Grad Norm 19.0285(24.3879) | Total Time 0.00(0.00)\n",
      "Iter 15780 | Time 27.2008(28.3504) | Bit/dim 3.7549(3.7181) | Xent 0.3572(0.3323) | Loss 9.1597(9.2631) | Error 0.1233(0.1153) Steps 982(988.82) | Grad Norm 36.5270(23.0785) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 118.8111, Epoch Time 1694.0459(1465.6634), Bit/dim 3.7239(best: 3.5843), Xent 0.7233, Loss 4.0855, Error 0.2160(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15790 | Time 26.8063(28.2533) | Bit/dim 3.7457(3.7217) | Xent 0.3115(0.3338) | Loss 9.0272(9.9467) | Error 0.1222(0.1160) Steps 976(986.86) | Grad Norm 24.9702(23.0160) | Total Time 0.00(0.00)\n",
      "Iter 15800 | Time 28.2644(28.3478) | Bit/dim 3.6865(3.7140) | Xent 0.2935(0.3235) | Loss 9.0168(9.6860) | Error 0.0967(0.1114) Steps 970(986.54) | Grad Norm 10.7202(21.9807) | Total Time 0.00(0.00)\n",
      "Iter 15810 | Time 29.7088(28.3695) | Bit/dim 3.6755(3.7052) | Xent 0.3043(0.3193) | Loss 8.9156(9.4930) | Error 0.1056(0.1094) Steps 982(990.86) | Grad Norm 13.6733(20.7570) | Total Time 0.00(0.00)\n",
      "Iter 15820 | Time 27.0918(28.3470) | Bit/dim 3.6820(3.6987) | Xent 0.2666(0.3152) | Loss 8.9355(9.3533) | Error 0.0911(0.1089) Steps 916(987.54) | Grad Norm 41.6493(22.7770) | Total Time 0.00(0.00)\n",
      "Iter 15830 | Time 29.1702(28.5694) | Bit/dim 3.7377(3.6968) | Xent 0.3910(0.3163) | Loss 9.0315(9.2574) | Error 0.1300(0.1093) Steps 1078(997.63) | Grad Norm 70.2535(26.9289) | Total Time 0.00(0.00)\n",
      "Iter 15840 | Time 29.5399(28.5219) | Bit/dim 3.7517(3.7105) | Xent 0.3618(0.3199) | Loss 9.2667(9.2031) | Error 0.1278(0.1098) Steps 1078(999.12) | Grad Norm 56.4588(37.8631) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 121.4099, Epoch Time 1707.7695(1472.9266), Bit/dim 3.7664(best: 3.5843), Xent 0.7861, Loss 4.1594, Error 0.2257(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15850 | Time 29.4362(28.6790) | Bit/dim 3.7293(3.7211) | Xent 0.3814(0.3315) | Loss 8.9537(9.8228) | Error 0.1333(0.1138) Steps 1012(1003.07) | Grad Norm 25.6957(41.4767) | Total Time 0.00(0.00)\n",
      "Iter 15860 | Time 27.9332(28.8296) | Bit/dim 3.7756(3.7350) | Xent 0.3727(0.3424) | Loss 9.2391(9.6605) | Error 0.1189(0.1173) Steps 1018(1013.23) | Grad Norm 74.2382(40.0543) | Total Time 0.00(0.00)\n",
      "Iter 15870 | Time 29.6102(29.0497) | Bit/dim 3.7168(3.7389) | Xent 0.4060(0.3493) | Loss 9.0528(9.5216) | Error 0.1200(0.1185) Steps 1000(1014.54) | Grad Norm 28.5553(37.6029) | Total Time 0.00(0.00)\n",
      "Iter 15880 | Time 29.8397(29.3444) | Bit/dim 3.7183(3.7384) | Xent 0.2896(0.3429) | Loss 9.0208(9.4091) | Error 0.0944(0.1175) Steps 1036(1017.86) | Grad Norm 39.4393(35.8074) | Total Time 0.00(0.00)\n",
      "Iter 15890 | Time 30.3859(29.3738) | Bit/dim 3.7232(3.7396) | Xent 0.3438(0.3392) | Loss 9.0456(9.3227) | Error 0.1178(0.1161) Steps 1072(1021.06) | Grad Norm 15.4444(33.8374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 120.6701, Epoch Time 1766.5629(1481.7357), Bit/dim 3.7225(best: 3.5843), Xent 0.7412, Loss 4.0931, Error 0.2218(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15900 | Time 30.5677(29.4269) | Bit/dim 3.7373(3.7369) | Xent 0.3223(0.3364) | Loss 9.2754(10.0382) | Error 0.1100(0.1152) Steps 1114(1023.48) | Grad Norm 99.0976(35.3243) | Total Time 0.00(0.00)\n",
      "Iter 15910 | Time 30.1570(29.4373) | Bit/dim 3.7539(3.7389) | Xent 0.3836(0.3418) | Loss 9.2796(9.8054) | Error 0.1300(0.1170) Steps 994(1024.20) | Grad Norm 99.6298(37.2450) | Total Time 0.00(0.00)\n",
      "Iter 15920 | Time 29.2817(29.6433) | Bit/dim 3.7430(3.7399) | Xent 0.3573(0.3468) | Loss 9.0719(9.6329) | Error 0.1278(0.1194) Steps 988(1026.75) | Grad Norm 80.3973(36.8420) | Total Time 0.00(0.00)\n",
      "Iter 15930 | Time 31.2624(29.7039) | Bit/dim 3.7597(3.7387) | Xent 0.3012(0.3436) | Loss 9.2113(9.4968) | Error 0.1089(0.1189) Steps 1114(1028.98) | Grad Norm 20.6799(34.9683) | Total Time 0.00(0.00)\n",
      "Iter 15940 | Time 30.5227(29.6903) | Bit/dim 3.7234(3.7313) | Xent 0.3412(0.3422) | Loss 9.1284(9.3777) | Error 0.1233(0.1180) Steps 958(1027.44) | Grad Norm 36.4664(32.8250) | Total Time 0.00(0.00)\n",
      "Iter 15950 | Time 29.3795(29.6253) | Bit/dim 3.7211(3.7232) | Xent 0.2943(0.3346) | Loss 8.9772(9.2713) | Error 0.0978(0.1154) Steps 910(1018.73) | Grad Norm 19.2013(31.2937) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 119.7655, Epoch Time 1770.1752(1490.3889), Bit/dim 3.7099(best: 3.5843), Xent 0.7286, Loss 4.0742, Error 0.2179(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15960 | Time 30.1486(29.3771) | Bit/dim 3.7415(3.7181) | Xent 0.3278(0.3251) | Loss 9.2127(9.8137) | Error 0.1067(0.1122) Steps 1066(1012.29) | Grad Norm 30.3362(38.8205) | Total Time 0.00(0.00)\n",
      "Iter 15970 | Time 28.4823(29.2511) | Bit/dim 3.6916(3.7154) | Xent 0.3086(0.3240) | Loss 8.9033(9.6037) | Error 0.0956(0.1124) Steps 1030(1012.55) | Grad Norm 30.9557(38.4424) | Total Time 0.00(0.00)\n",
      "Iter 15980 | Time 27.1902(29.2743) | Bit/dim 3.7142(3.7149) | Xent 0.3015(0.3209) | Loss 8.9361(9.4499) | Error 0.1089(0.1120) Steps 994(1006.11) | Grad Norm 33.8615(53.2590) | Total Time 0.00(0.00)\n",
      "Iter 15990 | Time 29.7881(29.1751) | Bit/dim 3.7042(3.7138) | Xent 0.3222(0.3213) | Loss 9.1273(9.3459) | Error 0.1244(0.1118) Steps 976(1003.82) | Grad Norm 108.4013(58.1410) | Total Time 0.00(0.00)\n",
      "Iter 16000 | Time 31.1379(29.2716) | Bit/dim 3.7414(3.7135) | Xent 0.3650(0.3237) | Loss 9.1004(9.2611) | Error 0.1200(0.1122) Steps 976(1007.13) | Grad Norm 47.1920(58.9242) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 123.9869, Epoch Time 1746.6883(1498.0779), Bit/dim 3.7440(best: 3.5843), Xent 0.7608, Loss 4.1244, Error 0.2214(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16010 | Time 29.0374(29.4194) | Bit/dim 3.7097(3.7187) | Xent 0.3064(0.3252) | Loss 8.9860(9.9740) | Error 0.1100(0.1127) Steps 988(1013.54) | Grad Norm 142.6549(57.2407) | Total Time 0.00(0.00)\n",
      "Iter 16020 | Time 32.1435(29.5765) | Bit/dim 3.7379(3.7267) | Xent 0.3771(0.3285) | Loss 9.1940(9.7500) | Error 0.1300(0.1132) Steps 1126(1017.77) | Grad Norm 42.0919(60.1859) | Total Time 0.00(0.00)\n",
      "Iter 16030 | Time 30.2759(29.9163) | Bit/dim 3.8465(3.7588) | Xent 0.4282(0.3471) | Loss 9.5319(9.6587) | Error 0.1422(0.1191) Steps 1102(1030.24) | Grad Norm 33.0853(64.4920) | Total Time 0.00(0.00)\n",
      "Iter 16040 | Time 31.2986(30.1317) | Bit/dim 3.7596(3.7657) | Xent 0.4375(0.3622) | Loss 9.2765(9.5573) | Error 0.1400(0.1235) Steps 976(1033.17) | Grad Norm 31.9242(61.5180) | Total Time 0.00(0.00)\n",
      "Iter 16050 | Time 31.4513(30.2646) | Bit/dim 3.7998(3.7611) | Xent 0.3606(0.3604) | Loss 9.1235(9.4554) | Error 0.1233(0.1225) Steps 1084(1037.34) | Grad Norm 48.3002(55.9968) | Total Time 0.00(0.00)\n",
      "Iter 16060 | Time 29.9043(30.1048) | Bit/dim 3.7169(3.7494) | Xent 0.3014(0.3474) | Loss 9.0770(9.3492) | Error 0.1089(0.1183) Steps 1072(1038.38) | Grad Norm 95.2054(51.3809) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 119.6485, Epoch Time 1803.9192(1507.2531), Bit/dim 3.7062(best: 3.5843), Xent 0.7224, Loss 4.0674, Error 0.2167(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16070 | Time 30.1717(29.9519) | Bit/dim 3.8371(3.7566) | Xent 0.3661(0.3495) | Loss 9.2978(9.9328) | Error 0.1344(0.1197) Steps 1096(1031.50) | Grad Norm 53.5629(59.9159) | Total Time 0.00(0.00)\n",
      "Iter 16080 | Time 28.1764(30.0998) | Bit/dim 3.7837(3.7631) | Xent 0.3549(0.3594) | Loss 9.0914(9.7573) | Error 0.1189(0.1232) Steps 1054(1034.77) | Grad Norm 30.5583(59.6863) | Total Time 0.00(0.00)\n",
      "Iter 16090 | Time 29.8197(30.2785) | Bit/dim 3.7145(3.7597) | Xent 0.3095(0.3586) | Loss 8.9922(9.6081) | Error 0.1033(0.1222) Steps 1036(1040.52) | Grad Norm 85.9843(60.1136) | Total Time 0.00(0.00)\n",
      "Iter 16100 | Time 31.6665(30.1444) | Bit/dim 3.7143(3.7546) | Xent 0.3474(0.3502) | Loss 9.1858(9.4861) | Error 0.1156(0.1205) Steps 1132(1041.92) | Grad Norm 19.0887(54.1880) | Total Time 0.00(0.00)\n",
      "Iter 16110 | Time 31.1005(30.0584) | Bit/dim 3.7206(3.7449) | Xent 0.3478(0.3472) | Loss 9.2153(9.3827) | Error 0.1222(0.1191) Steps 1054(1035.34) | Grad Norm 117.2418(51.2266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 122.0656, Epoch Time 1789.7497(1515.7280), Bit/dim 3.7298(best: 3.5843), Xent 0.7310, Loss 4.0953, Error 0.2205(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16120 | Time 28.5293(29.9326) | Bit/dim 3.7162(3.7380) | Xent 0.3046(0.3383) | Loss 9.0222(10.0671) | Error 0.1033(0.1161) Steps 1024(1034.33) | Grad Norm 60.0047(50.1813) | Total Time 0.00(0.00)\n",
      "Iter 16130 | Time 31.9560(29.9269) | Bit/dim 3.7435(3.7327) | Xent 0.2854(0.3325) | Loss 9.1001(9.7936) | Error 0.0911(0.1137) Steps 1042(1036.63) | Grad Norm 32.0426(51.9526) | Total Time 0.00(0.00)\n",
      "Iter 16140 | Time 30.0318(29.8340) | Bit/dim 3.7270(3.7268) | Xent 0.3286(0.3292) | Loss 9.0237(9.5859) | Error 0.1067(0.1132) Steps 958(1025.68) | Grad Norm 70.7560(64.1330) | Total Time 0.00(0.00)\n",
      "Iter 16150 | Time 29.8359(30.0985) | Bit/dim 4.0188(3.7804) | Xent 0.4752(0.3686) | Loss 9.7167(9.6019) | Error 0.1656(0.1256) Steps 994(1034.11) | Grad Norm 88.5293(82.3940) | Total Time 0.00(0.00)\n",
      "Iter 16160 | Time 31.5652(30.5415) | Bit/dim 3.8574(3.8248) | Xent 0.4225(0.4071) | Loss 9.5168(9.6474) | Error 0.1356(0.1361) Steps 1126(1040.36) | Grad Norm 148.0496(114.5499) | Total Time 0.00(0.00)\n",
      "Iter 16170 | Time 29.1765(30.6344) | Bit/dim 3.7930(3.8199) | Xent 0.3401(0.4070) | Loss 9.1836(9.5585) | Error 0.1067(0.1369) Steps 1048(1043.43) | Grad Norm 25.2039(105.2422) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 123.7203, Epoch Time 1820.4471(1524.8696), Bit/dim 3.7738(best: 3.5843), Xent 0.7391, Loss 4.1433, Error 0.2244(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16180 | Time 30.7133(30.4585) | Bit/dim 3.7142(3.8009) | Xent 0.3766(0.3899) | Loss 9.1917(10.0850) | Error 0.1356(0.1317) Steps 1066(1037.12) | Grad Norm 35.5665(87.1250) | Total Time 0.00(0.00)\n",
      "Iter 16190 | Time 29.2166(30.1842) | Bit/dim 3.6759(3.7754) | Xent 0.2865(0.3670) | Loss 8.8138(9.7875) | Error 0.0933(0.1244) Steps 976(1029.23) | Grad Norm 6.5212(70.4951) | Total Time 0.00(0.00)\n",
      "Iter 16200 | Time 28.2211(29.7038) | Bit/dim 3.6675(3.7477) | Xent 0.3454(0.3533) | Loss 8.7632(9.5423) | Error 0.1178(0.1204) Steps 922(1019.86) | Grad Norm 24.3812(57.3197) | Total Time 0.00(0.00)\n",
      "Iter 16210 | Time 26.5361(29.5679) | Bit/dim 3.6815(3.7315) | Xent 0.2861(0.3366) | Loss 8.8643(9.3886) | Error 0.1056(0.1153) Steps 922(1012.79) | Grad Norm 36.4216(50.0926) | Total Time 0.00(0.00)\n",
      "Iter 16220 | Time 28.5645(29.2433) | Bit/dim 3.6860(3.7147) | Xent 0.3030(0.3266) | Loss 9.0760(9.2662) | Error 0.1033(0.1126) Steps 952(1006.53) | Grad Norm 20.5614(58.0036) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 113.9634, Epoch Time 1720.4984(1530.7385), Bit/dim 3.6634(best: 3.5843), Xent 0.7011, Loss 4.0139, Error 0.2107(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16230 | Time 26.3531(28.6012) | Bit/dim 3.6745(3.6994) | Xent 0.2777(0.3165) | Loss 8.9443(9.8690) | Error 0.0867(0.1092) Steps 916(986.63) | Grad Norm 14.5839(53.8548) | Total Time 0.00(0.00)\n",
      "Iter 16240 | Time 24.7986(27.6499) | Bit/dim 3.6573(3.6860) | Xent 0.2650(0.3095) | Loss 8.8355(9.5985) | Error 0.1011(0.1071) Steps 904(966.02) | Grad Norm 12.0676(51.9050) | Total Time 0.00(0.00)\n",
      "Iter 16250 | Time 23.0161(26.6408) | Bit/dim 3.6677(3.6780) | Xent 0.3010(0.3024) | Loss 8.8232(9.3969) | Error 0.1011(0.1047) Steps 880(942.10) | Grad Norm 9.9329(41.1662) | Total Time 0.00(0.00)\n",
      "Iter 16260 | Time 22.8694(25.7897) | Bit/dim 3.6235(3.6692) | Xent 0.3004(0.2962) | Loss 8.7888(9.2267) | Error 0.0944(0.1029) Steps 886(925.60) | Grad Norm 18.8354(32.9132) | Total Time 0.00(0.00)\n",
      "Iter 16270 | Time 24.7728(25.4379) | Bit/dim 3.6385(3.6602) | Xent 0.2909(0.2931) | Loss 8.8334(9.1196) | Error 0.1100(0.1024) Steps 886(917.79) | Grad Norm 5.6121(27.7480) | Total Time 0.00(0.00)\n",
      "Iter 16280 | Time 24.8076(25.1094) | Bit/dim 3.6086(3.6523) | Xent 0.2699(0.2891) | Loss 8.8029(9.0132) | Error 0.0800(0.1013) Steps 874(911.21) | Grad Norm 10.5019(23.3978) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 106.4009, Epoch Time 1465.5212(1528.7819), Bit/dim 3.6393(best: 3.5843), Xent 0.7095, Loss 3.9941, Error 0.2125(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16290 | Time 24.9600(24.9146) | Bit/dim 3.6165(3.6469) | Xent 0.2446(0.2798) | Loss 8.6516(9.5655) | Error 0.0900(0.0983) Steps 976(914.74) | Grad Norm 46.9665(25.1141) | Total Time 0.00(0.00)\n",
      "Iter 16300 | Time 26.7027(25.0175) | Bit/dim 3.6513(3.6479) | Xent 0.2957(0.2756) | Loss 8.8428(9.3671) | Error 0.1144(0.0970) Steps 904(914.69) | Grad Norm 10.6659(40.4420) | Total Time 0.00(0.00)\n",
      "Iter 16310 | Time 29.9274(25.8638) | Bit/dim 3.9561(3.6855) | Xent 0.4936(0.2987) | Loss 9.6472(9.3310) | Error 0.1589(0.1052) Steps 1066(937.80) | Grad Norm 150.2812(59.3945) | Total Time 0.00(0.00)\n",
      "Iter 16320 | Time 32.8853(27.5716) | Bit/dim 4.6261(3.8809) | Xent 1.1406(0.4688) | Loss 11.6486(9.8083) | Error 0.3511(0.1542) Steps 1168(978.72) | Grad Norm 117.0069(82.0352) | Total Time 0.00(0.00)\n",
      "Iter 16330 | Time 35.5046(29.0375) | Bit/dim 4.2692(4.0284) | Xent 0.7656(0.6095) | Loss 10.6369(10.1834) | Error 0.2711(0.1977) Steps 1060(1012.45) | Grad Norm 67.1452(114.0477) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 126.2173, Epoch Time 1737.7464(1535.0509), Bit/dim 4.0844(best: 3.5843), Xent 0.8173, Loss 4.4931, Error 0.2743(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16340 | Time 31.3687(29.7194) | Bit/dim 3.9748(4.0456) | Xent 0.5573(0.6331) | Loss 9.8187(10.9724) | Error 0.1967(0.2090) Steps 1120(1026.77) | Grad Norm 120.4449(113.0278) | Total Time 0.00(0.00)\n",
      "Iter 16350 | Time 28.6348(29.7308) | Bit/dim 3.8793(4.0110) | Xent 0.4474(0.5992) | Loss 9.2964(10.6154) | Error 0.1611(0.2011) Steps 988(1026.51) | Grad Norm 19.1181(103.6988) | Total Time 0.00(0.00)\n",
      "Iter 16360 | Time 30.1770(29.5381) | Bit/dim 3.7580(3.9620) | Xent 0.4184(0.5519) | Loss 9.0835(10.2656) | Error 0.1567(0.1871) Steps 982(1014.73) | Grad Norm 30.2772(91.7229) | Total Time 0.00(0.00)\n",
      "Iter 16370 | Time 28.7793(29.3862) | Bit/dim 3.7441(3.9107) | Xent 0.3469(0.5047) | Loss 9.0522(9.9813) | Error 0.1189(0.1719) Steps 970(1007.30) | Grad Norm 39.3568(81.9273) | Total Time 0.00(0.00)\n",
      "Iter 16380 | Time 27.9821(29.0898) | Bit/dim 3.6978(3.8616) | Xent 0.2932(0.4611) | Loss 8.9269(9.7163) | Error 0.1078(0.1586) Steps 946(997.50) | Grad Norm 76.2230(72.0989) | Total Time 0.00(0.00)\n",
      "Iter 16390 | Time 29.4920(28.8340) | Bit/dim 3.6852(3.8207) | Xent 0.3230(0.4290) | Loss 8.9267(9.5253) | Error 0.1089(0.1483) Steps 976(991.72) | Grad Norm 16.5150(62.8001) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 113.9418, Epoch Time 1728.1776(1540.8447), Bit/dim 3.6979(best: 3.5843), Xent 0.6926, Loss 4.0442, Error 0.2121(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16400 | Time 27.9446(28.7094) | Bit/dim 3.6994(3.7888) | Xent 0.3058(0.3977) | Loss 8.8765(9.9954) | Error 0.1222(0.1386) Steps 976(990.64) | Grad Norm 214.0326(67.3933) | Total Time 0.00(0.00)\n",
      "Iter 16410 | Time 31.2638(28.9500) | Bit/dim 3.7999(3.7830) | Xent 0.3486(0.3867) | Loss 9.2222(9.7678) | Error 0.1367(0.1357) Steps 1114(996.07) | Grad Norm 147.4920(78.1028) | Total Time 0.00(0.00)\n",
      "Iter 16420 | Time 31.1048(29.6055) | Bit/dim 3.8921(3.8080) | Xent 0.4146(0.4093) | Loss 9.5158(9.7077) | Error 0.1467(0.1422) Steps 1060(1019.66) | Grad Norm 56.0045(110.5436) | Total Time 0.00(0.00)\n",
      "Iter 16430 | Time 31.4957(30.2050) | Bit/dim 3.8923(3.8311) | Xent 0.4933(0.4309) | Loss 9.5211(9.6838) | Error 0.1600(0.1476) Steps 1096(1029.73) | Grad Norm 259.0111(157.9333) | Total Time 0.00(0.00)\n",
      "Iter 16440 | Time 30.2945(30.4350) | Bit/dim 3.7726(3.8265) | Xent 0.4294(0.4254) | Loss 9.2340(9.5896) | Error 0.1600(0.1468) Steps 1078(1036.57) | Grad Norm 97.5852(148.4363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 123.0065, Epoch Time 1821.1502(1549.2538), Bit/dim 3.7614(best: 3.5843), Xent 0.7034, Loss 4.1131, Error 0.2202(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16450 | Time 31.9789(30.5194) | Bit/dim 3.7327(3.8095) | Xent 0.3394(0.4070) | Loss 8.9548(10.2003) | Error 0.1100(0.1407) Steps 1000(1035.80) | Grad Norm 28.4867(124.8798) | Total Time 0.00(0.00)\n",
      "Iter 16460 | Time 30.0356(30.3488) | Bit/dim 3.7239(3.7853) | Xent 0.3168(0.3847) | Loss 9.0577(9.8940) | Error 0.1078(0.1342) Steps 1006(1032.14) | Grad Norm 30.9382(101.0522) | Total Time 0.00(0.00)\n",
      "Iter 16470 | Time 28.7367(30.0881) | Bit/dim 3.7310(3.7651) | Xent 0.3194(0.3625) | Loss 9.0168(9.6418) | Error 0.1189(0.1274) Steps 1036(1029.52) | Grad Norm 147.8837(100.1699) | Total Time 0.00(0.00)\n",
      "Iter 16480 | Time 31.2951(30.0730) | Bit/dim 4.3394(3.8001) | Xent 1.0257(0.4013) | Loss 10.9757(9.6323) | Error 0.3033(0.1375) Steps 1102(1031.11) | Grad Norm 570.5674(199.5550) | Total Time 0.00(0.00)\n",
      "Iter 16490 | Time 35.4557(31.4178) | Bit/dim 4.8221(4.1200) | Xent 1.5241(0.7707) | Loss 12.5860(10.5870) | Error 0.4244(0.2219) Steps 1168(1071.64) | Grad Norm 390.3364(392.3444) | Total Time 0.00(0.00)\n",
      "Iter 16500 | Time 30.3092(31.8632) | Bit/dim 4.2523(4.2118) | Xent 0.8592(0.8475) | Loss 10.5371(10.7804) | Error 0.2956(0.2544) Steps 1024(1080.91) | Grad Norm 135.0119(343.7428) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 129.5159, Epoch Time 1874.0884(1558.9989), Bit/dim 4.2190(best: 3.5843), Xent 0.8586, Loss 4.6484, Error 0.2995(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16510 | Time 29.8203(31.6366) | Bit/dim 4.0048(4.1769) | Xent 0.5893(0.8018) | Loss 9.9290(11.2540) | Error 0.1978(0.2487) Steps 1012(1068.01) | Grad Norm 46.1583(269.5335) | Total Time 0.00(0.00)\n",
      "Iter 16520 | Time 27.5268(31.0424) | Bit/dim 3.8126(4.0985) | Xent 0.4521(0.7253) | Loss 9.1567(10.7960) | Error 0.1578(0.2317) Steps 928(1054.53) | Grad Norm 87.2761(210.5981) | Total Time 0.00(0.00)\n",
      "Iter 16530 | Time 27.7847(30.4094) | Bit/dim 3.7864(4.0192) | Xent 0.3699(0.6444) | Loss 9.2280(10.3946) | Error 0.1267(0.2085) Steps 958(1037.47) | Grad Norm 9.9787(161.0473) | Total Time 0.00(0.00)\n",
      "Iter 16540 | Time 27.3267(29.9156) | Bit/dim 3.7256(3.9458) | Xent 0.4016(0.5819) | Loss 9.0535(10.0694) | Error 0.1567(0.1912) Steps 994(1026.29) | Grad Norm 28.7078(123.9069) | Total Time 0.00(0.00)\n",
      "Iter 16550 | Time 26.7991(29.3738) | Bit/dim 3.6557(3.8824) | Xent 0.3869(0.5282) | Loss 8.8509(9.7777) | Error 0.1444(0.1763) Steps 940(1012.32) | Grad Norm 20.0043(96.1829) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 115.3363, Epoch Time 1728.5103(1564.0842), Bit/dim 3.7058(best: 3.5843), Xent 0.6784, Loss 4.0450, Error 0.2124(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16560 | Time 28.0054(29.0754) | Bit/dim 3.7133(3.8361) | Xent 0.2840(0.4792) | Loss 8.9785(10.3130) | Error 0.0944(0.1620) Steps 994(1002.36) | Grad Norm 23.0431(75.7609) | Total Time 0.00(0.00)\n",
      "Iter 16570 | Time 27.5673(28.8632) | Bit/dim 3.6988(3.8002) | Xent 0.3429(0.4396) | Loss 9.0043(9.9560) | Error 0.1133(0.1495) Steps 982(994.49) | Grad Norm 157.2426(73.0266) | Total Time 0.00(0.00)\n",
      "Iter 16580 | Time 29.4530(28.9691) | Bit/dim 3.7738(3.7822) | Xent 0.3668(0.4173) | Loss 9.1261(9.7206) | Error 0.1311(0.1446) Steps 1030(995.60) | Grad Norm 89.2345(92.6432) | Total Time 0.00(0.00)\n",
      "Iter 16590 | Time 34.1564(29.7963) | Bit/dim 4.0894(3.8350) | Xent 0.6168(0.4481) | Loss 10.2698(9.7493) | Error 0.2089(0.1552) Steps 1114(1014.04) | Grad Norm 392.8495(126.8164) | Total Time 0.00(0.00)\n",
      "Iter 16600 | Time 31.4366(30.6508) | Bit/dim 3.9818(3.8889) | Xent 0.5194(0.4789) | Loss 9.7177(9.7990) | Error 0.1689(0.1647) Steps 1042(1030.48) | Grad Norm 83.1789(123.2227) | Total Time 0.00(0.00)\n",
      "Iter 16610 | Time 33.7129(31.0772) | Bit/dim 3.8717(3.8958) | Xent 0.4841(0.4816) | Loss 9.5299(9.7595) | Error 0.1733(0.1672) Steps 1066(1039.64) | Grad Norm 283.4313(125.1045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 126.9336, Epoch Time 1834.6655(1572.2017), Bit/dim 3.8683(best: 3.5843), Xent 0.7051, Loss 4.2208, Error 0.2273(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16620 | Time 30.2053(31.0209) | Bit/dim 3.8003(3.8764) | Xent 0.3838(0.4660) | Loss 9.3158(10.3223) | Error 0.1500(0.1629) Steps 1066(1041.38) | Grad Norm 32.7741(121.9364) | Total Time 0.00(0.00)\n",
      "Iter 16630 | Time 31.8409(30.8449) | Bit/dim 3.8234(3.8533) | Xent 0.3592(0.4440) | Loss 9.3781(10.0354) | Error 0.1211(0.1550) Steps 1048(1039.43) | Grad Norm 118.9519(133.9740) | Total Time 0.00(0.00)\n",
      "Iter 16640 | Time 33.1703(31.1016) | Bit/dim 3.9758(3.8641) | Xent 0.4931(0.4461) | Loss 9.8103(9.8956) | Error 0.1678(0.1568) Steps 1132(1050.46) | Grad Norm 400.2669(156.9447) | Total Time 0.00(0.00)\n",
      "Iter 16650 | Time 33.3061(31.5817) | Bit/dim 4.0309(3.9091) | Xent 0.5104(0.4803) | Loss 9.9391(9.9146) | Error 0.1756(0.1660) Steps 1114(1058.08) | Grad Norm 230.7202(182.8583) | Total Time 0.00(0.00)\n",
      "Iter 16660 | Time 32.4351(31.6904) | Bit/dim 3.9491(3.9311) | Xent 0.4739(0.4985) | Loss 9.6622(9.9010) | Error 0.1600(0.1725) Steps 1108(1065.72) | Grad Norm 62.8403(212.7652) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 126.5748, Epoch Time 1881.6122(1581.4840), Bit/dim 3.8842(best: 3.5843), Xent 0.7296, Loss 4.2490, Error 0.2300(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16670 | Time 33.2665(31.6288) | Bit/dim 3.8522(3.9214) | Xent 0.4446(0.4893) | Loss 9.5338(10.5681) | Error 0.1589(0.1695) Steps 1126(1067.59) | Grad Norm 62.2731(168.2195) | Total Time 0.00(0.00)\n",
      "Iter 16680 | Time 28.8939(31.3821) | Bit/dim 3.8105(3.8900) | Xent 0.3630(0.4664) | Loss 9.2194(10.2221) | Error 0.1300(0.1630) Steps 982(1059.52) | Grad Norm 25.9277(136.7017) | Total Time 0.00(0.00)\n",
      "Iter 16690 | Time 30.0233(31.2345) | Bit/dim 3.7354(3.8566) | Xent 0.3300(0.4413) | Loss 8.7616(9.9302) | Error 0.1133(0.1552) Steps 1000(1048.47) | Grad Norm 64.7230(118.8128) | Total Time 0.00(0.00)\n",
      "Iter 16700 | Time 30.8738(30.7992) | Bit/dim 3.7660(3.8308) | Xent 0.3430(0.4178) | Loss 9.1457(9.7139) | Error 0.1189(0.1473) Steps 1036(1039.34) | Grad Norm 57.9496(120.1698) | Total Time 0.00(0.00)\n",
      "Iter 16710 | Time 29.3129(30.7531) | Bit/dim 3.7569(3.8100) | Xent 0.2772(0.3980) | Loss 8.9433(9.5479) | Error 0.0944(0.1398) Steps 982(1034.98) | Grad Norm 85.3590(120.1745) | Total Time 0.00(0.00)\n",
      "Iter 16720 | Time 29.9154(30.6267) | Bit/dim 3.7366(3.7944) | Xent 0.3041(0.3840) | Loss 9.1320(9.4364) | Error 0.1067(0.1353) Steps 1036(1030.24) | Grad Norm 129.3427(144.9689) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 122.9557, Epoch Time 1818.5884(1588.5971), Bit/dim 3.7705(best: 3.5843), Xent 0.7274, Loss 4.1342, Error 0.2275(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16730 | Time 31.4995(30.8137) | Bit/dim 3.7586(3.7931) | Xent 0.3643(0.3857) | Loss 9.0720(10.0328) | Error 0.1133(0.1355) Steps 1096(1039.13) | Grad Norm 188.7967(163.4019) | Total Time 0.00(0.00)\n",
      "Iter 16740 | Time 29.6169(30.8488) | Bit/dim 3.7619(3.7874) | Xent 0.4076(0.3809) | Loss 9.2410(9.8175) | Error 0.1522(0.1342) Steps 1024(1037.58) | Grad Norm 79.1406(145.8888) | Total Time 0.00(0.00)\n",
      "Iter 16750 | Time 32.6182(30.9062) | Bit/dim 3.8845(3.7897) | Xent 0.4311(0.3847) | Loss 9.5931(9.6717) | Error 0.1522(0.1352) Steps 1150(1044.81) | Grad Norm 102.0367(153.5797) | Total Time 0.00(0.00)\n",
      "Iter 16760 | Time 33.8986(31.7458) | Bit/dim 4.4333(3.9044) | Xent 0.8055(0.4875) | Loss 11.1556(9.9152) | Error 0.2600(0.1648) Steps 1162(1071.50) | Grad Norm 375.0374(260.9748) | Total Time 0.00(0.00)\n",
      "Iter 16770 | Time 33.5117(32.5117) | Bit/dim 4.3115(4.0295) | Xent 0.7758(0.5907) | Loss 10.7766(10.2201) | Error 0.2767(0.1948) Steps 1120(1089.19) | Grad Norm 187.6967(271.0159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 133.5795, Epoch Time 1947.7050(1599.3703), Bit/dim 4.0916(best: 3.5843), Xent 0.8238, Loss 4.5035, Error 0.2732(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16780 | Time 31.6251(32.7157) | Bit/dim 3.9778(4.0495) | Xent 0.5818(0.6096) | Loss 9.9295(11.0242) | Error 0.2133(0.2024) Steps 1090(1091.65) | Grad Norm 230.9325(251.4982) | Total Time 0.00(0.00)\n",
      "Iter 16790 | Time 32.0755(32.5607) | Bit/dim 3.8674(4.0105) | Xent 0.4003(0.5757) | Loss 9.3139(10.6438) | Error 0.1378(0.1936) Steps 1060(1080.86) | Grad Norm 48.2995(222.4562) | Total Time 0.00(0.00)\n",
      "Iter 16800 | Time 32.1140(32.2960) | Bit/dim 3.7720(3.9580) | Xent 0.3487(0.5293) | Loss 9.2259(10.2936) | Error 0.1311(0.1792) Steps 1060(1071.47) | Grad Norm 73.9404(185.2781) | Total Time 0.00(0.00)\n",
      "Iter 16810 | Time 30.8522(31.8132) | Bit/dim 3.7511(3.9083) | Xent 0.3748(0.4873) | Loss 8.9926(9.9995) | Error 0.1356(0.1668) Steps 1024(1061.32) | Grad Norm 91.3239(154.4140) | Total Time 0.00(0.00)\n",
      "Iter 16820 | Time 31.7785(31.6569) | Bit/dim 3.8375(3.8738) | Xent 0.4668(0.4597) | Loss 9.3974(9.7883) | Error 0.1789(0.1593) Steps 1060(1060.24) | Grad Norm 206.4433(158.5498) | Total Time 0.00(0.00)\n",
      "Iter 16830 | Time 32.0758(31.6665) | Bit/dim 3.8170(3.8644) | Xent 0.4757(0.4476) | Loss 9.2241(9.6772) | Error 0.1578(0.1552) Steps 1006(1058.57) | Grad Norm 259.7558(152.1800) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 126.0376, Epoch Time 1872.5692(1607.5663), Bit/dim 3.8244(best: 3.5843), Xent 0.7286, Loss 4.1887, Error 0.2239(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16840 | Time 32.5176(31.6632) | Bit/dim 3.7781(3.8488) | Xent 0.3485(0.4288) | Loss 9.1111(10.2352) | Error 0.1156(0.1498) Steps 958(1051.96) | Grad Norm 76.9225(159.0535) | Total Time 0.00(0.00)\n",
      "Iter 16850 | Time 33.1970(31.6780) | Bit/dim 3.8583(3.8387) | Xent 0.4710(0.4246) | Loss 9.4774(9.9818) | Error 0.1689(0.1479) Steps 1096(1053.62) | Grad Norm 162.2526(186.8578) | Total Time 0.00(0.00)\n",
      "Iter 16860 | Time 36.5920(32.6852) | Bit/dim 4.7975(3.9988) | Xent 1.2695(0.5990) | Loss 12.4106(10.3582) | Error 0.3356(0.1864) Steps 1186(1084.78) | Grad Norm 544.6404(402.1641) | Total Time 0.00(0.00)\n",
      "Iter 16870 | Time 33.4772(33.2187) | Bit/dim 4.0921(4.0876) | Xent 0.6982(0.6850) | Loss 10.1839(10.5258) | Error 0.2411(0.2116) Steps 1060(1095.45) | Grad Norm 62.2774(357.2759) | Total Time 0.00(0.00)\n",
      "Iter 16880 | Time 32.5571(33.1321) | Bit/dim 3.8922(4.0570) | Xent 0.4352(0.6529) | Loss 9.5740(10.3418) | Error 0.1567(0.2087) Steps 1096(1094.25) | Grad Norm 36.5918(276.7476) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 126.9535, Epoch Time 1968.5467(1618.3957), Bit/dim 3.8460(best: 3.5843), Xent 0.7157, Loss 4.2038, Error 0.2341(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16890 | Time 30.5629(32.7351) | Bit/dim 3.8263(4.0009) | Xent 0.4153(0.6016) | Loss 9.3148(10.8650) | Error 0.1400(0.1952) Steps 970(1088.90) | Grad Norm 768.2162(233.5211) | Total Time 0.00(0.00)\n",
      "Iter 16900 | Time 33.2954(32.7648) | Bit/dim 3.8950(3.9638) | Xent 0.4843(0.5664) | Loss 9.4953(10.4986) | Error 0.1711(0.1876) Steps 1060(1089.37) | Grad Norm 42.0745(184.7194) | Total Time 0.00(0.00)\n",
      "Iter 16910 | Time 32.8551(32.9194) | Bit/dim 3.8464(3.9422) | Xent 0.4246(0.5376) | Loss 9.4744(10.2406) | Error 0.1444(0.1803) Steps 1096(1083.98) | Grad Norm 57.1315(148.0344) | Total Time 0.00(0.00)\n",
      "Iter 16920 | Time 31.7750(32.8071) | Bit/dim 3.8062(3.9177) | Xent 0.4557(0.5101) | Loss 9.3843(10.0264) | Error 0.1622(0.1735) Steps 994(1081.45) | Grad Norm 70.3731(123.0637) | Total Time 0.00(0.00)\n",
      "Iter 16930 | Time 32.7816(32.4358) | Bit/dim 3.7799(3.8867) | Xent 0.4315(0.4796) | Loss 9.3447(9.8188) | Error 0.1611(0.1645) Steps 1126(1076.23) | Grad Norm 121.9461(114.5505) | Total Time 0.00(0.00)\n",
      "Iter 16940 | Time 32.8487(32.3961) | Bit/dim 3.9791(3.8813) | Xent 0.4888(0.4692) | Loss 9.6639(9.7259) | Error 0.1778(0.1619) Steps 1108(1074.31) | Grad Norm 67.3081(122.6810) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 134.3511, Epoch Time 1931.6001(1627.7919), Bit/dim 4.0062(best: 3.5843), Xent 0.7704, Loss 4.3914, Error 0.2447(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 16950 | Time 33.5150(32.7440) | Bit/dim 4.0848(3.9276) | Xent 0.5957(0.4931) | Loss 10.2312(10.4718) | Error 0.1944(0.1705) Steps 1168(1087.22) | Grad Norm 139.2248(120.7116) | Total Time 0.00(0.00)\n",
      "Iter 16960 | Time 32.2106(32.8416) | Bit/dim 3.9599(3.9469) | Xent 0.5443(0.5062) | Loss 9.6769(10.2995) | Error 0.1944(0.1748) Steps 1006(1088.69) | Grad Norm 232.6982(135.6046) | Total Time 0.00(0.00)\n",
      "Iter 16970 | Time 32.6967(32.8793) | Bit/dim 3.9191(3.9410) | Xent 0.5533(0.5025) | Loss 9.8328(10.1251) | Error 0.1844(0.1734) Steps 1066(1087.44) | Grad Norm 198.6270(157.3893) | Total Time 0.00(0.00)\n",
      "Iter 16980 | Time 34.3265(32.9583) | Bit/dim 3.8922(3.9328) | Xent 0.4659(0.5048) | Loss 9.5589(9.9885) | Error 0.1611(0.1743) Steps 1060(1083.44) | Grad Norm 307.9928(174.4407) | Total Time 0.00(0.00)\n",
      "Iter 16990 | Time 34.3416(33.0570) | Bit/dim 3.8474(3.9166) | Xent 0.4154(0.4846) | Loss 9.4887(9.8503) | Error 0.1478(0.1661) Steps 1156(1090.59) | Grad Norm 85.8268(171.2987) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 124.5571, Epoch Time 1957.6158(1637.6866), Bit/dim 3.8433(best: 3.5843), Xent 0.6972, Loss 4.1919, Error 0.2245(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17000 | Time 33.9369(32.6017) | Bit/dim 3.8321(3.8958) | Xent 0.4752(0.4706) | Loss 9.4868(10.4790) | Error 0.1611(0.1622) Steps 1150(1081.61) | Grad Norm 181.0155(185.8729) | Total Time 0.00(0.00)\n",
      "Iter 17010 | Time 36.2017(32.9532) | Bit/dim 4.3110(3.9351) | Xent 0.7321(0.4992) | Loss 10.8288(10.3545) | Error 0.2100(0.1694) Steps 1162(1091.92) | Grad Norm 237.6308(226.5172) | Total Time 0.00(0.00)\n",
      "Iter 17020 | Time 33.2939(33.7532) | Bit/dim 4.2997(4.0424) | Xent 0.7115(0.5773) | Loss 10.5889(10.4906) | Error 0.2533(0.1921) Steps 1150(1108.62) | Grad Norm 180.2143(244.8127) | Total Time 0.00(0.00)\n",
      "Iter 17030 | Time 33.5582(33.9668) | Bit/dim 4.0617(4.0660) | Xent 0.5795(0.5877) | Loss 9.9872(10.4106) | Error 0.1933(0.1965) Steps 1144(1112.88) | Grad Norm 109.5183(233.6046) | Total Time 0.00(0.00)\n",
      "Iter 17040 | Time 31.8119(33.8772) | Bit/dim 3.9196(4.0377) | Xent 0.4894(0.5679) | Loss 9.7174(10.2347) | Error 0.1822(0.1920) Steps 1102(1111.27) | Grad Norm 98.0940(221.7896) | Total Time 0.00(0.00)\n",
      "Iter 17050 | Time 30.7364(33.2569) | Bit/dim 3.8457(3.9885) | Xent 0.4030(0.5319) | Loss 9.3135(10.0101) | Error 0.1456(0.1813) Steps 1012(1091.51) | Grad Norm 262.8401(227.2599) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 128.1199, Epoch Time 2004.3157(1648.6854), Bit/dim 3.8472(best: 3.5843), Xent 0.7126, Loss 4.2036, Error 0.2255(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17060 | Time 35.8799(33.3826) | Bit/dim 4.3553(4.0070) | Xent 0.7994(0.5623) | Loss 11.0978(10.6951) | Error 0.2656(0.1926) Steps 1204(1095.46) | Grad Norm 605.6893(312.5309) | Total Time 0.00(0.00)\n",
      "Iter 17070 | Time 33.2821(33.8065) | Bit/dim 4.2372(4.0745) | Xent 0.7397(0.6092) | Loss 10.5185(10.6710) | Error 0.2644(0.2089) Steps 1072(1105.89) | Grad Norm 301.9474(294.8917) | Total Time 0.00(0.00)\n",
      "Iter 17080 | Time 35.8429(34.2261) | Bit/dim 4.0452(4.0915) | Xent 0.5895(0.6119) | Loss 9.9479(10.5582) | Error 0.1944(0.2106) Steps 1084(1119.90) | Grad Norm 65.1218(248.3499) | Total Time 0.00(0.00)\n",
      "Iter 17090 | Time 31.2278(34.1354) | Bit/dim 3.9879(4.0712) | Xent 0.4737(0.5926) | Loss 9.8213(10.3827) | Error 0.1678(0.2049) Steps 1066(1114.94) | Grad Norm 104.3431(211.4455) | Total Time 0.00(0.00)\n",
      "Iter 17100 | Time 31.8154(33.9804) | Bit/dim 3.8851(4.0302) | Xent 0.4321(0.5586) | Loss 9.4290(10.1729) | Error 0.1522(0.1932) Steps 1036(1105.31) | Grad Norm 61.6347(189.7047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 128.7311, Epoch Time 2028.9688(1660.0939), Bit/dim 3.8657(best: 3.5843), Xent 0.7029, Loss 4.2172, Error 0.2300(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 33.4222(33.8546) | Bit/dim 3.8598(3.9890) | Xent 0.4385(0.5293) | Loss 9.2289(10.7721) | Error 0.1522(0.1837) Steps 1084(1105.07) | Grad Norm 103.5057(181.8132) | Total Time 0.00(0.00)\n",
      "Iter 17120 | Time 31.4181(33.4376) | Bit/dim 3.9019(3.9532) | Xent 0.4228(0.4956) | Loss 9.6227(10.4162) | Error 0.1367(0.1720) Steps 1090(1099.28) | Grad Norm 58.3562(198.8913) | Total Time 0.00(0.00)\n",
      "Iter 17130 | Time 35.0497(33.6484) | Bit/dim 4.0306(3.9599) | Xent 0.7857(0.5171) | Loss 10.1287(10.2631) | Error 0.2622(0.1792) Steps 1114(1103.79) | Grad Norm 1491.5376(284.7104) | Total Time 0.00(0.00)\n",
      "Iter 17140 | Time 35.8386(33.9935) | Bit/dim 3.9989(3.9834) | Xent 0.4836(0.5413) | Loss 9.8948(10.2054) | Error 0.1756(0.1868) Steps 1192(1116.63) | Grad Norm 117.3612(293.9251) | Total Time 0.00(0.00)\n",
      "Iter 17150 | Time 32.6860(34.0373) | Bit/dim 3.9297(3.9770) | Xent 0.5169(0.5335) | Loss 9.5973(10.0792) | Error 0.1733(0.1847) Steps 1072(1117.24) | Grad Norm 137.4170(251.6311) | Total Time 0.00(0.00)\n",
      "Iter 17160 | Time 31.9690(33.6526) | Bit/dim 3.8665(3.9518) | Xent 0.4273(0.5106) | Loss 9.3712(9.9198) | Error 0.1489(0.1785) Steps 1096(1106.78) | Grad Norm 1153.6734(251.9239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 127.7602, Epoch Time 1997.1408(1670.2054), Bit/dim 3.8462(best: 3.5843), Xent 0.6915, Loss 4.1920, Error 0.2226(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 31.5432(32.9553) | Bit/dim 3.7827(3.9163) | Xent 0.3857(0.4803) | Loss 9.1113(10.3720) | Error 0.1433(0.1685) Steps 1054(1088.96) | Grad Norm 68.4635(204.1601) | Total Time 0.00(0.00)\n",
      "Iter 17180 | Time 27.2337(32.0122) | Bit/dim 3.7551(3.8835) | Xent 0.3590(0.4498) | Loss 9.0457(10.0724) | Error 0.1156(0.1585) Steps 982(1062.57) | Grad Norm 30.2893(178.4938) | Total Time 0.00(0.00)\n",
      "Iter 17190 | Time 27.5364(31.0519) | Bit/dim 3.7782(3.8523) | Xent 0.2874(0.4221) | Loss 9.1401(9.8233) | Error 0.1022(0.1493) Steps 946(1037.44) | Grad Norm 177.5869(173.1021) | Total Time 0.00(0.00)\n",
      "Iter 17200 | Time 27.4291(30.2964) | Bit/dim 3.7304(3.8239) | Xent 0.3159(0.4001) | Loss 9.0254(9.6189) | Error 0.1256(0.1422) Steps 1000(1021.62) | Grad Norm 307.7257(164.3277) | Total Time 0.00(0.00)\n",
      "Iter 17210 | Time 30.3348(29.9950) | Bit/dim 3.7100(3.8015) | Xent 0.3952(0.3961) | Loss 9.0540(9.4681) | Error 0.1322(0.1404) Steps 1048(1009.66) | Grad Norm 52.9194(157.3699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 121.7375, Epoch Time 1748.3263(1672.5490), Bit/dim 3.7560(best: 3.5843), Xent 0.6809, Loss 4.0964, Error 0.2182(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 31.9234(29.8905) | Bit/dim 3.7254(3.7873) | Xent 0.3922(0.3847) | Loss 8.9961(10.0815) | Error 0.1256(0.1361) Steps 1018(1009.21) | Grad Norm 44.3408(142.0072) | Total Time 0.00(0.00)\n",
      "Iter 17230 | Time 31.4679(30.1711) | Bit/dim 3.7922(3.7870) | Xent 0.3737(0.3814) | Loss 9.2067(9.8544) | Error 0.1222(0.1342) Steps 1090(1019.30) | Grad Norm 179.9759(148.4801) | Total Time 0.00(0.00)\n",
      "Iter 17240 | Time 32.0049(30.9691) | Bit/dim 3.9717(3.8243) | Xent 0.5044(0.4091) | Loss 9.7299(9.8122) | Error 0.1722(0.1429) Steps 1072(1035.37) | Grad Norm 87.6372(129.6821) | Total Time 0.00(0.00)\n",
      "Iter 17250 | Time 31.9941(31.5839) | Bit/dim 3.8866(3.8506) | Xent 0.4112(0.4266) | Loss 9.5544(9.7666) | Error 0.1456(0.1498) Steps 1096(1055.23) | Grad Norm 149.2766(130.0785) | Total Time 0.00(0.00)\n",
      "Iter 17260 | Time 33.0125(31.9761) | Bit/dim 3.8296(3.8553) | Xent 0.4075(0.4314) | Loss 9.2917(9.6873) | Error 0.1411(0.1512) Steps 1114(1060.02) | Grad Norm 285.1239(146.8031) | Total Time 0.00(0.00)\n",
      "Iter 17270 | Time 32.2776(31.9217) | Bit/dim 3.7981(3.8447) | Xent 0.4252(0.4215) | Loss 9.3332(9.5846) | Error 0.1456(0.1475) Steps 970(1062.30) | Grad Norm 113.3932(146.3939) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 129.4523, Epoch Time 1919.5299(1679.9584), Bit/dim 3.8107(best: 3.5843), Xent 0.7245, Loss 4.1729, Error 0.2258(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 36.1673(32.2772) | Bit/dim 3.8846(3.8438) | Xent 0.4605(0.4218) | Loss 9.5413(10.2068) | Error 0.1622(0.1486) Steps 1186(1073.27) | Grad Norm 60.4662(178.3000) | Total Time 0.00(0.00)\n",
      "Iter 17290 | Time 35.7144(32.8302) | Bit/dim 4.0488(3.8913) | Xent 0.5365(0.4576) | Loss 9.9555(10.1284) | Error 0.1878(0.1612) Steps 1162(1085.78) | Grad Norm 218.5134(209.0722) | Total Time 0.00(0.00)\n",
      "Iter 17300 | Time 33.6006(33.3124) | Bit/dim 3.9841(3.9230) | Xent 0.5257(0.4816) | Loss 9.7059(10.0668) | Error 0.1833(0.1687) Steps 1048(1087.56) | Grad Norm 160.2305(222.0551) | Total Time 0.00(0.00)\n",
      "Iter 17310 | Time 34.3033(33.4223) | Bit/dim 3.9060(3.9258) | Xent 0.5130(0.4824) | Loss 9.5718(9.9536) | Error 0.1756(0.1686) Steps 1102(1088.63) | Grad Norm 277.3894(209.7726) | Total Time 0.00(0.00)\n",
      "Iter 17320 | Time 29.6092(33.2466) | Bit/dim 3.7729(3.9034) | Xent 0.4131(0.4722) | Loss 9.2797(9.8109) | Error 0.1489(0.1666) Steps 1036(1084.50) | Grad Norm 129.0691(199.1154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 124.9487, Epoch Time 1989.1797(1689.2351), Bit/dim 3.7925(best: 3.5843), Xent 0.6852, Loss 4.1351, Error 0.2182(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 30.5920(32.8680) | Bit/dim 3.7426(3.8742) | Xent 0.3296(0.4488) | Loss 9.0425(10.4277) | Error 0.1311(0.1581) Steps 1066(1077.73) | Grad Norm 134.9366(188.0408) | Total Time 0.00(0.00)\n",
      "Iter 17340 | Time 31.8566(32.3638) | Bit/dim 3.7695(3.8487) | Xent 0.3935(0.4248) | Loss 9.0611(10.0923) | Error 0.1389(0.1506) Steps 1060(1064.64) | Grad Norm 216.1711(237.6754) | Total Time 0.00(0.00)\n",
      "Iter 17350 | Time 33.9601(32.5392) | Bit/dim 4.3105(3.8892) | Xent 0.9257(0.4688) | Loss 10.8013(10.0462) | Error 0.2778(0.1624) Steps 1150(1070.86) | Grad Norm 1296.7307(309.0688) | Total Time 0.00(0.00)\n",
      "Iter 17360 | Time 37.7087(33.9159) | Bit/dim 4.4142(4.0582) | Xent 1.0823(0.6624) | Loss 11.3172(10.4897) | Error 0.3667(0.2155) Steps 1138(1104.15) | Grad Norm 296.2435(406.4775) | Total Time 0.00(0.00)\n",
      "Iter 17370 | Time 32.8784(34.4134) | Bit/dim 4.1169(4.1117) | Xent 0.7024(0.7116) | Loss 10.0873(10.5528) | Error 0.2400(0.2344) Steps 1120(1125.59) | Grad Norm 105.1885(350.8311) | Total Time 0.00(0.00)\n",
      "Iter 17380 | Time 37.4200(34.7449) | Bit/dim 3.9612(4.0906) | Xent 0.4494(0.6818) | Loss 9.7854(10.4107) | Error 0.1622(0.2279) Steps 1180(1128.59) | Grad Norm 42.8615(281.1183) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 134.9915, Epoch Time 2042.3143(1699.8274), Bit/dim 3.9583(best: 3.5843), Xent 0.7371, Loss 4.3269, Error 0.2452(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 34.1346(34.5996) | Bit/dim 3.8720(4.0434) | Xent 0.5253(0.6329) | Loss 9.4428(10.8945) | Error 0.1944(0.2139) Steps 1138(1121.91) | Grad Norm 50.4606(219.0142) | Total Time 0.00(0.00)\n",
      "Iter 17400 | Time 31.8273(34.2121) | Bit/dim 3.8424(3.9907) | Xent 0.4256(0.5792) | Loss 9.3001(10.5007) | Error 0.1544(0.1972) Steps 1084(1108.38) | Grad Norm 84.8431(177.3545) | Total Time 0.00(0.00)\n",
      "Iter 17410 | Time 32.7712(33.8823) | Bit/dim 3.7932(3.9426) | Xent 0.3722(0.5326) | Loss 9.2904(10.1881) | Error 0.1311(0.1828) Steps 1090(1102.64) | Grad Norm 44.0221(149.3231) | Total Time 0.00(0.00)\n",
      "Iter 17420 | Time 30.8823(33.5958) | Bit/dim 3.8199(3.9034) | Xent 0.3687(0.4965) | Loss 9.1559(9.9466) | Error 0.1367(0.1713) Steps 1054(1096.10) | Grad Norm 102.8782(152.9321) | Total Time 0.00(0.00)\n",
      "Iter 17430 | Time 32.2095(33.1340) | Bit/dim 3.8550(3.8752) | Xent 0.4224(0.4704) | Loss 9.4477(9.7753) | Error 0.1456(0.1633) Steps 1120(1085.21) | Grad Norm 124.4458(172.0920) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 133.7172, Epoch Time 1969.1465(1707.9070), Bit/dim 4.0777(best: 3.5843), Xent 0.7942, Loss 4.4748, Error 0.2606(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 38.0150(33.4676) | Bit/dim 4.1188(3.9184) | Xent 0.5825(0.4935) | Loss 10.2988(10.5915) | Error 0.2078(0.1715) Steps 1138(1087.84) | Grad Norm 64.7699(147.6199) | Total Time 0.00(0.00)\n",
      "Iter 17450 | Time 31.3137(33.7081) | Bit/dim 4.0426(3.9611) | Xent 0.5408(0.5117) | Loss 9.8660(10.4364) | Error 0.2044(0.1791) Steps 1084(1093.88) | Grad Norm 140.4716(137.8326) | Total Time 0.00(0.00)\n",
      "Iter 17460 | Time 34.4611(33.7988) | Bit/dim 3.9546(3.9721) | Xent 0.5038(0.5171) | Loss 9.9332(10.2912) | Error 0.1667(0.1803) Steps 1096(1096.02) | Grad Norm 416.8776(171.0452) | Total Time 0.00(0.00)\n",
      "Iter 17470 | Time 32.6640(33.7063) | Bit/dim 3.8834(3.9620) | Xent 0.4520(0.5091) | Loss 9.4384(10.1190) | Error 0.1533(0.1778) Steps 1108(1094.89) | Grad Norm 266.7805(208.2908) | Total Time 0.00(0.00)\n",
      "Iter 17480 | Time 32.3372(33.5959) | Bit/dim 3.9311(3.9441) | Xent 0.4313(0.4982) | Loss 9.4981(9.9581) | Error 0.1456(0.1736) Steps 1108(1090.26) | Grad Norm 147.7512(229.1114) | Total Time 0.00(0.00)\n",
      "Iter 17490 | Time 30.1657(33.1604) | Bit/dim 3.8377(3.9231) | Xent 0.4311(0.4821) | Loss 9.2260(9.8295) | Error 0.1422(0.1682) Steps 1054(1081.89) | Grad Norm 85.5233(191.1843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 124.7108, Epoch Time 1991.6867(1716.4204), Bit/dim 3.8441(best: 3.5843), Xent 0.6875, Loss 4.1879, Error 0.2274(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 31.2353(32.5526) | Bit/dim 3.8086(3.8957) | Xent 0.3695(0.4579) | Loss 9.4476(10.3408) | Error 0.1333(0.1605) Steps 1054(1071.33) | Grad Norm 222.4792(172.9656) | Total Time 0.00(0.00)\n",
      "Iter 17510 | Time 30.1538(31.8075) | Bit/dim 3.7993(3.8687) | Xent 0.3283(0.4360) | Loss 9.1849(10.0411) | Error 0.1122(0.1534) Steps 1012(1057.74) | Grad Norm 283.6099(172.8373) | Total Time 0.00(0.00)\n",
      "Iter 17520 | Time 30.6830(31.3161) | Bit/dim 3.7971(3.8455) | Xent 0.3459(0.4145) | Loss 9.2630(9.8254) | Error 0.1300(0.1467) Steps 1066(1048.02) | Grad Norm 68.2464(157.3708) | Total Time 0.00(0.00)\n",
      "Iter 17530 | Time 31.1901(30.9329) | Bit/dim 3.7939(3.8232) | Xent 0.3458(0.4000) | Loss 9.1717(9.6281) | Error 0.1189(0.1418) Steps 1024(1040.52) | Grad Norm 86.0110(151.7148) | Total Time 0.00(0.00)\n",
      "Iter 17540 | Time 32.2672(31.1077) | Bit/dim 3.8361(3.8153) | Xent 0.3400(0.3890) | Loss 9.2322(9.5215) | Error 0.1211(0.1375) Steps 1084(1045.03) | Grad Norm 142.9485(151.0382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 128.4891, Epoch Time 1827.4135(1719.7502), Bit/dim 3.8873(best: 3.5843), Xent 0.7307, Loss 4.2527, Error 0.2302(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 32.7557(31.5181) | Bit/dim 3.9010(3.8279) | Xent 0.4084(0.3948) | Loss 9.3471(10.2459) | Error 0.1278(0.1385) Steps 1066(1056.37) | Grad Norm 43.8632(130.9207) | Total Time 0.00(0.00)\n",
      "Iter 17560 | Time 32.1311(31.7594) | Bit/dim 3.7934(3.8263) | Xent 0.3709(0.3903) | Loss 9.2291(9.9998) | Error 0.1122(0.1369) Steps 1090(1058.61) | Grad Norm 46.6350(121.6861) | Total Time 0.00(0.00)\n",
      "Iter 17570 | Time 30.5801(31.3998) | Bit/dim 3.7979(3.8202) | Xent 0.3731(0.3852) | Loss 9.2248(9.8028) | Error 0.1311(0.1358) Steps 976(1052.15) | Grad Norm 47.8015(114.9387) | Total Time 0.00(0.00)\n",
      "Iter 17580 | Time 28.6733(31.1229) | Bit/dim 3.7957(3.8198) | Xent 0.3993(0.3800) | Loss 9.1831(9.6568) | Error 0.1356(0.1341) Steps 994(1045.16) | Grad Norm 33.2574(110.3747) | Total Time 0.00(0.00)\n",
      "Iter 17590 | Time 30.0089(31.0727) | Bit/dim 3.8041(3.8172) | Xent 0.3654(0.3783) | Loss 9.0291(9.5433) | Error 0.1278(0.1333) Steps 988(1035.98) | Grad Norm 196.8262(107.7294) | Total Time 0.00(0.00)\n",
      "Iter 17600 | Time 29.3288(31.1077) | Bit/dim 3.8333(3.8118) | Xent 0.4151(0.3799) | Loss 9.2138(9.4546) | Error 0.1422(0.1325) Steps 1006(1036.98) | Grad Norm 392.4380(122.2076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 125.0953, Epoch Time 1861.5993(1724.0057), Bit/dim 3.8027(best: 3.5843), Xent 0.6925, Loss 4.1490, Error 0.2181(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 33.6547(31.4530) | Bit/dim 3.8928(3.8199) | Xent 0.4511(0.3856) | Loss 9.4588(10.0847) | Error 0.1644(0.1349) Steps 1126(1045.72) | Grad Norm 77.9917(152.9151) | Total Time 0.00(0.00)\n",
      "Iter 17620 | Time 35.8834(32.1963) | Bit/dim 4.1428(3.8499) | Xent 0.7421(0.4168) | Loss 10.4423(9.9882) | Error 0.2156(0.1424) Steps 1144(1058.77) | Grad Norm 469.1143(201.9730) | Total Time 0.00(0.00)\n",
      "Iter 17630 | Time 36.5139(33.4529) | Bit/dim 4.9733(4.1926) | Xent 1.3625(0.7939) | Loss 12.8152(10.9396) | Error 0.3567(0.2006) Steps 1174(1093.15) | Grad Norm 435.5211(606.3358) | Total Time 0.00(0.00)\n",
      "Iter 17640 | Time 32.3509(34.1078) | Bit/dim 4.3021(4.2787) | Xent 0.7809(0.8575) | Loss 10.6693(11.0838) | Error 0.2700(0.2353) Steps 1114(1117.74) | Grad Norm 129.7368(492.4493) | Total Time 0.00(0.00)\n",
      "Iter 17650 | Time 35.4374(34.0453) | Bit/dim 4.0880(4.2468) | Xent 0.6588(0.8241) | Loss 10.2810(10.9008) | Error 0.2389(0.2417) Steps 1084(1112.37) | Grad Norm 41.1603(381.3436) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 128.8639, Epoch Time 2046.7945(1733.6893), Bit/dim 4.0033(best: 3.5843), Xent 0.7555, Loss 4.3811, Error 0.2598(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 33.1751(33.8423) | Bit/dim 3.9799(4.1817) | Xent 0.5747(0.7668) | Loss 9.8469(11.3961) | Error 0.1989(0.2352) Steps 1024(1104.39) | Grad Norm 19.4916(288.9256) | Total Time 0.00(0.00)\n",
      "Iter 17670 | Time 32.0245(33.3730) | Bit/dim 3.9233(4.1128) | Xent 0.5045(0.6986) | Loss 9.5256(10.9266) | Error 0.1856(0.2212) Steps 994(1089.22) | Grad Norm 24.7475(218.1018) | Total Time 0.00(0.00)\n",
      "Iter 17680 | Time 32.2944(33.0416) | Bit/dim 3.8618(4.0531) | Xent 0.4888(0.6368) | Loss 9.4579(10.5484) | Error 0.1767(0.2057) Steps 988(1078.80) | Grad Norm 208.2133(174.2969) | Total Time 0.00(0.00)\n",
      "Iter 17690 | Time 30.4129(32.5528) | Bit/dim 3.8599(4.0026) | Xent 0.4713(0.5837) | Loss 9.4038(10.2497) | Error 0.1567(0.1928) Steps 1054(1068.92) | Grad Norm 146.1747(168.9065) | Total Time 0.00(0.00)\n",
      "Iter 17700 | Time 31.5614(32.2641) | Bit/dim 3.9691(3.9845) | Xent 0.5081(0.5557) | Loss 9.7293(10.0704) | Error 0.1811(0.1866) Steps 1054(1064.44) | Grad Norm 36.7494(167.1451) | Total Time 0.00(0.00)\n",
      "Iter 17710 | Time 35.1511(32.7999) | Bit/dim 4.0137(3.9926) | Xent 0.4797(0.5472) | Loss 9.7505(10.0127) | Error 0.1778(0.1874) Steps 1192(1076.07) | Grad Norm 24.4755(135.0643) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 134.7774, Epoch Time 1930.7879(1739.6023), Bit/dim 4.0222(best: 3.5843), Xent 0.7646, Loss 4.4045, Error 0.2459(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 32.6548(32.8587) | Bit/dim 3.9236(3.9889) | Xent 0.4902(0.5402) | Loss 9.6074(10.6476) | Error 0.1678(0.1852) Steps 1066(1079.23) | Grad Norm 119.6333(141.3809) | Total Time 0.00(0.00)\n",
      "Iter 17730 | Time 29.6972(32.6747) | Bit/dim 3.7991(3.9602) | Xent 0.4986(0.5162) | Loss 9.2854(10.3515) | Error 0.1867(0.1783) Steps 982(1075.75) | Grad Norm 56.8886(159.3927) | Total Time 0.00(0.00)\n",
      "Iter 17740 | Time 30.5012(31.9088) | Bit/dim 3.8139(3.9283) | Xent 0.3770(0.4866) | Loss 9.3059(10.0804) | Error 0.1244(0.1677) Steps 1006(1056.71) | Grad Norm 31.0780(152.6520) | Total Time 0.00(0.00)\n",
      "Iter 17750 | Time 28.5982(31.2561) | Bit/dim 3.7724(3.8934) | Xent 0.3461(0.4603) | Loss 9.1688(9.8546) | Error 0.1189(0.1588) Steps 994(1041.10) | Grad Norm 153.5600(148.0769) | Total Time 0.00(0.00)\n",
      "Iter 17760 | Time 28.9139(30.8970) | Bit/dim 3.8205(3.8672) | Xent 0.3465(0.4423) | Loss 9.3700(9.7026) | Error 0.1278(0.1538) Steps 964(1031.16) | Grad Norm 73.3260(131.6036) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 122.3166, Epoch Time 1836.5976(1742.5121), Bit/dim 3.7913(best: 3.5843), Xent 0.6922, Loss 4.1374, Error 0.2203(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 29.5127(30.6201) | Bit/dim 3.7579(3.8445) | Xent 0.3803(0.4222) | Loss 9.1089(10.3210) | Error 0.1244(0.1478) Steps 1012(1027.95) | Grad Norm 70.7505(127.7813) | Total Time 0.00(0.00)\n",
      "Iter 17780 | Time 29.2317(30.5646) | Bit/dim 3.7718(3.8298) | Xent 0.4043(0.4073) | Loss 9.2409(10.0280) | Error 0.1456(0.1429) Steps 928(1028.91) | Grad Norm 357.9931(136.5005) | Total Time 0.00(0.00)\n",
      "Iter 17790 | Time 29.3583(30.4988) | Bit/dim 3.7823(3.8132) | Xent 0.3567(0.3972) | Loss 9.1779(9.8067) | Error 0.1411(0.1394) Steps 1000(1030.57) | Grad Norm 346.9505(132.2378) | Total Time 0.00(0.00)\n",
      "Iter 17800 | Time 30.6700(30.3966) | Bit/dim 3.7662(3.8025) | Xent 0.3436(0.3889) | Loss 9.2380(9.6431) | Error 0.1211(0.1369) Steps 1006(1028.09) | Grad Norm 148.0128(135.0883) | Total Time 0.00(0.00)\n",
      "Iter 17810 | Time 29.1481(30.3020) | Bit/dim 3.7689(3.7938) | Xent 0.4157(0.3901) | Loss 9.2288(9.5286) | Error 0.1400(0.1364) Steps 1042(1025.89) | Grad Norm 20.7031(152.2005) | Total Time 0.00(0.00)\n",
      "Iter 17820 | Time 28.0829(29.7155) | Bit/dim 3.8031(3.7972) | Xent 0.4030(0.3964) | Loss 9.3328(9.4513) | Error 0.1467(0.1381) Steps 1000(1015.15) | Grad Norm 9.3385(130.7328) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 116.1420, Epoch Time 1771.8227(1743.3915), Bit/dim 3.8132(best: 3.5843), Xent 0.6961, Loss 4.1612, Error 0.2249(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 26.1242(28.9028) | Bit/dim 3.8028(3.8011) | Xent 0.4355(0.4029) | Loss 9.2696(10.0370) | Error 0.1500(0.1407) Steps 922(996.35) | Grad Norm 5.9722(100.1005) | Total Time 0.00(0.00)\n",
      "Iter 17840 | Time 26.1129(28.2732) | Bit/dim 3.7829(3.7975) | Xent 0.3737(0.4007) | Loss 9.2685(9.8216) | Error 0.1422(0.1406) Steps 928(985.57) | Grad Norm 6.4262(76.0153) | Total Time 0.00(0.00)\n",
      "Iter 17850 | Time 26.0612(27.6605) | Bit/dim 3.7156(3.7883) | Xent 0.3639(0.3962) | Loss 8.9118(9.6266) | Error 0.1211(0.1382) Steps 934(968.61) | Grad Norm 13.4871(58.3319) | Total Time 0.00(0.00)\n",
      "Iter 17860 | Time 27.5759(27.2953) | Bit/dim 3.7612(3.7795) | Xent 0.3328(0.3828) | Loss 9.0823(9.4822) | Error 0.1278(0.1343) Steps 934(960.68) | Grad Norm 6.7184(45.5442) | Total Time 0.00(0.00)\n",
      "Iter 17870 | Time 25.1296(27.1425) | Bit/dim 3.6989(3.7672) | Xent 0.3695(0.3731) | Loss 9.0250(9.3712) | Error 0.1433(0.1317) Steps 946(961.03) | Grad Norm 10.1005(36.1490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 113.3083, Epoch Time 1586.4062(1738.6819), Bit/dim 3.7380(best: 3.5843), Xent 0.6655, Loss 4.0707, Error 0.2133(best: 0.2037)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 26.5904(27.0253) | Bit/dim 3.7407(3.7564) | Xent 0.3693(0.3657) | Loss 9.0741(10.0277) | Error 0.1256(0.1284) Steps 946(956.03) | Grad Norm 12.7893(33.5089) | Total Time 0.00(0.00)\n",
      "Iter 17890 | Time 27.6249(26.9674) | Bit/dim 3.6996(3.7509) | Xent 0.3573(0.3576) | Loss 9.0308(9.7690) | Error 0.1344(0.1262) Steps 928(954.23) | Grad Norm 4.5454(27.9061) | Total Time 0.00(0.00)\n",
      "Iter 17900 | Time 27.0155(26.9424) | Bit/dim 3.7176(3.7403) | Xent 0.3643(0.3528) | Loss 9.0434(9.5599) | Error 0.1167(0.1237) Steps 1000(954.69) | Grad Norm 26.5154(26.1500) | Total Time 0.00(0.00)\n",
      "Iter 17910 | Time 24.8368(26.6605) | Bit/dim 3.7286(3.7336) | Xent 0.3641(0.3468) | Loss 8.9839(9.4020) | Error 0.1300(0.1215) Steps 892(949.35) | Grad Norm 5.4171(44.2942) | Total Time 0.00(0.00)\n",
      "Iter 17920 | Time 24.2938(26.1713) | Bit/dim 3.7098(3.7304) | Xent 0.3278(0.3426) | Loss 8.8756(9.2824) | Error 0.1256(0.1205) Steps 880(938.26) | Grad Norm 8.0983(35.7897) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_15_run3/epoch_150_checkpt.pth --seed 3 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 15.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
