{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1/epoch_310_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 17060 | Time 25.6898(26.4967) | Bit/dim 3.5222(3.5387) | Xent 0.0836(0.0950) | Loss 3.5640(3.5862) | Error 0.0311(0.0337) Steps 1078(1072.86) | Grad Norm 1.3371(1.9437) | Total Time 14.00(14.00)\n",
      "Iter 17070 | Time 25.9502(26.2921) | Bit/dim 3.5758(3.5389) | Xent 0.0878(0.0939) | Loss 3.6197(3.5859) | Error 0.0322(0.0326) Steps 1066(1072.81) | Grad Norm 1.5708(1.8926) | Total Time 14.00(14.00)\n",
      "Iter 17080 | Time 25.5463(26.1842) | Bit/dim 3.5238(3.5360) | Xent 0.1223(0.0928) | Loss 3.5849(3.5824) | Error 0.0433(0.0322) Steps 1078(1071.40) | Grad Norm 2.0417(1.8542) | Total Time 14.00(14.00)\n",
      "Iter 17090 | Time 26.0876(26.1464) | Bit/dim 3.5507(3.5377) | Xent 0.0754(0.0915) | Loss 3.5884(3.5835) | Error 0.0344(0.0323) Steps 1066(1071.67) | Grad Norm 1.6021(1.8216) | Total Time 14.00(14.00)\n",
      "Iter 17100 | Time 26.6437(26.1332) | Bit/dim 3.5330(3.5374) | Xent 0.0743(0.0916) | Loss 3.5701(3.5832) | Error 0.0244(0.0319) Steps 1084(1072.68) | Grad Norm 2.3703(1.8078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 127.8634, Epoch Time 1589.5270(1572.3378), Bit/dim 3.5474(best: inf), Xent 1.1823, Loss 4.1385, Error 0.2412(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17110 | Time 25.4869(26.0746) | Bit/dim 3.5537(3.5370) | Xent 0.1425(0.0928) | Loss 3.6250(3.5834) | Error 0.0478(0.0319) Steps 1060(1071.69) | Grad Norm 2.8605(1.8782) | Total Time 14.00(14.00)\n",
      "Iter 17120 | Time 26.6259(26.0346) | Bit/dim 3.5299(3.5355) | Xent 0.0842(0.0913) | Loss 3.5720(3.5812) | Error 0.0333(0.0318) Steps 1084(1074.14) | Grad Norm 1.6166(1.8391) | Total Time 14.00(14.00)\n",
      "Iter 17130 | Time 25.7809(26.0523) | Bit/dim 3.5614(3.5358) | Xent 0.0815(0.0882) | Loss 3.6021(3.5798) | Error 0.0278(0.0306) Steps 1078(1075.08) | Grad Norm 1.5518(1.7463) | Total Time 14.00(14.00)\n",
      "Iter 17140 | Time 25.8035(25.9699) | Bit/dim 3.4939(3.5347) | Xent 0.1056(0.0901) | Loss 3.5467(3.5797) | Error 0.0389(0.0315) Steps 1072(1074.87) | Grad Norm 1.9294(1.7791) | Total Time 14.00(14.00)\n",
      "Iter 17150 | Time 26.0800(25.9899) | Bit/dim 3.5392(3.5333) | Xent 0.1065(0.0908) | Loss 3.5924(3.5787) | Error 0.0356(0.0322) Steps 1096(1075.75) | Grad Norm 2.3255(1.7745) | Total Time 14.00(14.00)\n",
      "Iter 17160 | Time 26.3728(25.9901) | Bit/dim 3.5448(3.5363) | Xent 0.0903(0.0909) | Loss 3.5899(3.5817) | Error 0.0333(0.0324) Steps 1078(1076.20) | Grad Norm 2.2095(1.8061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 125.0990, Epoch Time 1568.3529(1572.2182), Bit/dim 3.5461(best: 3.5474), Xent 1.1813, Loss 4.1367, Error 0.2419(best: 0.2412)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17170 | Time 26.9687(26.0227) | Bit/dim 3.5243(3.5358) | Xent 0.0925(0.0907) | Loss 3.5706(3.5811) | Error 0.0300(0.0321) Steps 1066(1077.37) | Grad Norm 2.3411(1.8936) | Total Time 14.00(14.00)\n",
      "Iter 17180 | Time 26.4532(26.0250) | Bit/dim 3.5169(3.5333) | Xent 0.0981(0.0911) | Loss 3.5659(3.5789) | Error 0.0356(0.0316) Steps 1090(1077.56) | Grad Norm 2.2356(1.9395) | Total Time 14.00(14.00)\n",
      "Iter 17190 | Time 26.4077(25.9961) | Bit/dim 3.5478(3.5341) | Xent 0.0899(0.0890) | Loss 3.5928(3.5786) | Error 0.0267(0.0309) Steps 1060(1076.55) | Grad Norm 2.1038(1.9177) | Total Time 14.00(14.00)\n",
      "Iter 17200 | Time 26.0179(25.9790) | Bit/dim 3.5344(3.5368) | Xent 0.0713(0.0874) | Loss 3.5700(3.5804) | Error 0.0211(0.0303) Steps 1054(1075.54) | Grad Norm 1.1827(1.8182) | Total Time 14.00(14.00)\n",
      "Iter 17210 | Time 26.0878(26.0162) | Bit/dim 3.5599(3.5377) | Xent 0.0810(0.0877) | Loss 3.6003(3.5816) | Error 0.0289(0.0303) Steps 1090(1076.15) | Grad Norm 1.2886(1.8352) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 123.7408, Epoch Time 1569.2855(1572.1303), Bit/dim 3.5455(best: 3.5461), Xent 1.1832, Loss 4.1371, Error 0.2387(best: 0.2412)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17220 | Time 26.1803(25.9769) | Bit/dim 3.5429(3.5379) | Xent 0.0872(0.0894) | Loss 3.5864(3.5826) | Error 0.0322(0.0311) Steps 1084(1076.67) | Grad Norm 1.9916(1.9513) | Total Time 14.00(14.00)\n",
      "Iter 17230 | Time 25.6991(25.9552) | Bit/dim 3.5202(3.5379) | Xent 0.0913(0.0895) | Loss 3.5659(3.5827) | Error 0.0322(0.0310) Steps 1072(1077.49) | Grad Norm 1.3729(1.8999) | Total Time 14.00(14.00)\n",
      "Iter 17240 | Time 25.7671(25.9944) | Bit/dim 3.5483(3.5370) | Xent 0.0884(0.0884) | Loss 3.5925(3.5812) | Error 0.0244(0.0310) Steps 1072(1077.86) | Grad Norm 1.3716(1.8629) | Total Time 14.00(14.00)\n",
      "Iter 17250 | Time 26.5822(26.0891) | Bit/dim 3.5088(3.5364) | Xent 0.0812(0.0881) | Loss 3.5494(3.5805) | Error 0.0267(0.0309) Steps 1090(1079.49) | Grad Norm 1.3446(1.8593) | Total Time 14.00(14.00)\n",
      "Iter 17260 | Time 25.6735(26.0412) | Bit/dim 3.5109(3.5359) | Xent 0.0986(0.0866) | Loss 3.5602(3.5792) | Error 0.0289(0.0300) Steps 1084(1078.33) | Grad Norm 1.8434(1.8661) | Total Time 14.00(14.00)\n",
      "Iter 17270 | Time 25.1581(25.9909) | Bit/dim 3.5453(3.5344) | Xent 0.1130(0.0904) | Loss 3.6018(3.5796) | Error 0.0300(0.0310) Steps 1078(1078.13) | Grad Norm 1.8237(1.8896) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 124.1384, Epoch Time 1571.9758(1572.1256), Bit/dim 3.5490(best: 3.5455), Xent 1.2280, Loss 4.1630, Error 0.2361(best: 0.2387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17280 | Time 25.7930(25.9765) | Bit/dim 3.5739(3.5356) | Xent 0.0829(0.0902) | Loss 3.6153(3.5807) | Error 0.0367(0.0319) Steps 1078(1078.55) | Grad Norm 2.0928(1.8973) | Total Time 14.00(14.00)\n",
      "Iter 17290 | Time 26.2352(26.0285) | Bit/dim 3.5384(3.5359) | Xent 0.0674(0.0891) | Loss 3.5721(3.5805) | Error 0.0222(0.0316) Steps 1090(1078.50) | Grad Norm 1.6378(1.8559) | Total Time 14.00(14.00)\n",
      "Iter 17300 | Time 26.0149(26.0453) | Bit/dim 3.5615(3.5363) | Xent 0.0971(0.0913) | Loss 3.6101(3.5820) | Error 0.0367(0.0329) Steps 1078(1078.34) | Grad Norm 1.6113(1.8828) | Total Time 14.00(14.00)\n",
      "Iter 17310 | Time 26.0008(26.0559) | Bit/dim 3.5283(3.5346) | Xent 0.1100(0.0913) | Loss 3.5833(3.5803) | Error 0.0333(0.0326) Steps 1072(1078.41) | Grad Norm 2.7195(1.8522) | Total Time 14.00(14.00)\n",
      "Iter 17320 | Time 26.7514(26.1190) | Bit/dim 3.5680(3.5371) | Xent 0.0942(0.0915) | Loss 3.6150(3.5828) | Error 0.0356(0.0322) Steps 1084(1078.15) | Grad Norm 3.1711(1.9721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 125.4590, Epoch Time 1577.8053(1572.2960), Bit/dim 3.5464(best: 3.5455), Xent 1.1613, Loss 4.1270, Error 0.2401(best: 0.2361)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17330 | Time 25.4432(26.0329) | Bit/dim 3.5525(3.5354) | Xent 0.0955(0.0933) | Loss 3.6002(3.5821) | Error 0.0367(0.0329) Steps 1084(1076.89) | Grad Norm 2.0117(1.9919) | Total Time 14.00(14.00)\n",
      "Iter 17340 | Time 25.8807(26.0088) | Bit/dim 3.5602(3.5351) | Xent 0.0879(0.0917) | Loss 3.6041(3.5809) | Error 0.0244(0.0323) Steps 1084(1077.73) | Grad Norm 1.9478(1.9647) | Total Time 14.00(14.00)\n",
      "Iter 17350 | Time 26.0686(25.9641) | Bit/dim 3.5523(3.5344) | Xent 0.0978(0.0910) | Loss 3.6012(3.5799) | Error 0.0311(0.0316) Steps 1072(1077.96) | Grad Norm 1.5545(1.8989) | Total Time 14.00(14.00)\n",
      "Iter 17360 | Time 25.9516(25.9475) | Bit/dim 3.5216(3.5326) | Xent 0.0886(0.0911) | Loss 3.5659(3.5782) | Error 0.0322(0.0321) Steps 1078(1076.07) | Grad Norm 2.0234(1.9485) | Total Time 14.00(14.00)\n",
      "Iter 17370 | Time 26.2710(25.9897) | Bit/dim 3.4995(3.5328) | Xent 0.1061(0.0932) | Loss 3.5526(3.5794) | Error 0.0400(0.0329) Steps 1060(1077.05) | Grad Norm 3.0894(2.0543) | Total Time 14.00(14.00)\n",
      "Iter 17380 | Time 26.1344(26.0150) | Bit/dim 3.5494(3.5362) | Xent 0.1174(0.0930) | Loss 3.6081(3.5827) | Error 0.0411(0.0333) Steps 1084(1077.15) | Grad Norm 1.7671(2.0458) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 123.0624, Epoch Time 1565.7387(1572.0993), Bit/dim 3.5476(best: 3.5455), Xent 1.1818, Loss 4.1385, Error 0.2382(best: 0.2361)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17390 | Time 25.4203(25.9671) | Bit/dim 3.5452(3.5355) | Xent 0.1006(0.0926) | Loss 3.5955(3.5818) | Error 0.0378(0.0328) Steps 1060(1076.24) | Grad Norm 1.5277(1.9723) | Total Time 14.00(14.00)\n",
      "Iter 17400 | Time 26.9053(26.0000) | Bit/dim 3.5233(3.5359) | Xent 0.0757(0.0915) | Loss 3.5611(3.5817) | Error 0.0244(0.0329) Steps 1090(1077.98) | Grad Norm 1.8110(1.9796) | Total Time 14.00(14.00)\n",
      "Iter 17410 | Time 25.9604(26.0418) | Bit/dim 3.5285(3.5360) | Xent 0.0863(0.0906) | Loss 3.5716(3.5813) | Error 0.0278(0.0327) Steps 1060(1077.16) | Grad Norm 1.5328(1.8747) | Total Time 14.00(14.00)\n",
      "Iter 17420 | Time 25.3668(25.9863) | Bit/dim 3.5360(3.5365) | Xent 0.0861(0.0893) | Loss 3.5790(3.5811) | Error 0.0244(0.0319) Steps 1096(1076.96) | Grad Norm 1.5889(1.8594) | Total Time 14.00(14.00)\n",
      "Iter 17430 | Time 25.8582(26.0187) | Bit/dim 3.5274(3.5348) | Xent 0.1123(0.0910) | Loss 3.5836(3.5803) | Error 0.0400(0.0323) Steps 1072(1077.25) | Grad Norm 2.6018(1.9670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 123.8353, Epoch Time 1568.7226(1571.9980), Bit/dim 3.5454(best: 3.5455), Xent 1.2031, Loss 4.1469, Error 0.2386(best: 0.2361)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17440 | Time 25.6939(25.9261) | Bit/dim 3.5104(3.5376) | Xent 0.0885(0.0885) | Loss 3.5546(3.5818) | Error 0.0311(0.0313) Steps 1048(1076.07) | Grad Norm 1.4275(1.8945) | Total Time 14.00(14.00)\n",
      "Iter 17450 | Time 25.7117(25.9226) | Bit/dim 3.5400(3.5373) | Xent 0.0851(0.0900) | Loss 3.5826(3.5824) | Error 0.0311(0.0321) Steps 1078(1076.47) | Grad Norm 1.9760(1.8662) | Total Time 14.00(14.00)\n",
      "Iter 17460 | Time 25.8123(25.9251) | Bit/dim 3.5445(3.5395) | Xent 0.0889(0.0891) | Loss 3.5890(3.5841) | Error 0.0322(0.0319) Steps 1078(1075.33) | Grad Norm 1.9749(1.8428) | Total Time 14.00(14.00)\n",
      "Iter 17470 | Time 25.3760(25.8928) | Bit/dim 3.5004(3.5352) | Xent 0.0666(0.0891) | Loss 3.5337(3.5798) | Error 0.0200(0.0314) Steps 1072(1075.01) | Grad Norm 2.1627(1.9480) | Total Time 14.00(14.00)\n",
      "Iter 17480 | Time 25.9401(25.8404) | Bit/dim 3.5351(3.5326) | Xent 0.1285(0.0902) | Loss 3.5993(3.5777) | Error 0.0478(0.0317) Steps 1078(1075.70) | Grad Norm 2.8304(2.0352) | Total Time 14.00(14.00)\n",
      "Iter 17490 | Time 25.7795(25.7532) | Bit/dim 3.5354(3.5343) | Xent 0.0869(0.0922) | Loss 3.5788(3.5804) | Error 0.0256(0.0329) Steps 1054(1075.64) | Grad Norm 1.5247(2.1039) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 123.2527, Epoch Time 1556.4028(1571.5301), Bit/dim 3.5461(best: 3.5454), Xent 1.1945, Loss 4.1433, Error 0.2384(best: 0.2361)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17500 | Time 26.1282(25.7930) | Bit/dim 3.5587(3.5393) | Xent 0.0741(0.0921) | Loss 3.5957(3.5854) | Error 0.0233(0.0331) Steps 1078(1075.28) | Grad Norm 2.2248(2.1526) | Total Time 14.00(14.00)\n",
      "Iter 17510 | Time 25.3963(25.7993) | Bit/dim 3.5384(3.5372) | Xent 0.0967(0.0927) | Loss 3.5867(3.5835) | Error 0.0256(0.0328) Steps 1084(1074.07) | Grad Norm 2.0121(2.1556) | Total Time 14.00(14.00)\n",
      "Iter 17520 | Time 25.3134(25.8368) | Bit/dim 3.5219(3.5368) | Xent 0.0647(0.0899) | Loss 3.5542(3.5817) | Error 0.0222(0.0319) Steps 1078(1074.74) | Grad Norm 1.2964(2.1010) | Total Time 14.00(14.00)\n",
      "Iter 17530 | Time 26.0649(25.8089) | Bit/dim 3.5195(3.5348) | Xent 0.0731(0.0881) | Loss 3.5561(3.5788) | Error 0.0244(0.0309) Steps 1084(1077.16) | Grad Norm 1.6026(2.0635) | Total Time 14.00(14.00)\n",
      "Iter 17540 | Time 26.1410(25.8769) | Bit/dim 3.5520(3.5324) | Xent 0.0700(0.0893) | Loss 3.5870(3.5771) | Error 0.0233(0.0313) Steps 1090(1078.04) | Grad Norm 1.4771(2.0015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 122.1545, Epoch Time 1564.4399(1571.3174), Bit/dim 3.5455(best: 3.5454), Xent 1.1931, Loss 4.1420, Error 0.2353(best: 0.2361)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17550 | Time 26.3950(25.9345) | Bit/dim 3.5336(3.5341) | Xent 0.0932(0.0872) | Loss 3.5802(3.5777) | Error 0.0289(0.0307) Steps 1072(1076.84) | Grad Norm 1.5454(1.9027) | Total Time 14.00(14.00)\n",
      "Iter 17560 | Time 25.8088(25.9485) | Bit/dim 3.5597(3.5373) | Xent 0.0774(0.0870) | Loss 3.5984(3.5808) | Error 0.0267(0.0304) Steps 1078(1075.80) | Grad Norm 1.7145(1.9026) | Total Time 14.00(14.00)\n",
      "Iter 17570 | Time 25.3058(25.9449) | Bit/dim 3.5128(3.5336) | Xent 0.1140(0.0893) | Loss 3.5698(3.5783) | Error 0.0456(0.0314) Steps 1072(1075.62) | Grad Norm 3.6993(2.0801) | Total Time 14.00(14.00)\n",
      "Iter 17580 | Time 26.1660(25.9681) | Bit/dim 3.5392(3.5325) | Xent 0.0776(0.0883) | Loss 3.5780(3.5766) | Error 0.0256(0.0316) Steps 1072(1074.66) | Grad Norm 2.0648(2.1305) | Total Time 14.00(14.00)\n",
      "Iter 17590 | Time 26.4996(25.9165) | Bit/dim 3.5515(3.5336) | Xent 0.1090(0.0879) | Loss 3.6060(3.5776) | Error 0.0378(0.0316) Steps 1066(1074.62) | Grad Norm 2.4541(2.0853) | Total Time 14.00(14.00)\n",
      "Iter 17600 | Time 25.8970(25.9024) | Bit/dim 3.5611(3.5351) | Xent 0.0710(0.0879) | Loss 3.5966(3.5791) | Error 0.0244(0.0310) Steps 1084(1076.66) | Grad Norm 1.0816(1.9617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 123.4546, Epoch Time 1565.1315(1571.1319), Bit/dim 3.5452(best: 3.5454), Xent 1.1878, Loss 4.1391, Error 0.2413(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17610 | Time 25.2311(25.8849) | Bit/dim 3.5685(3.5352) | Xent 0.0735(0.0876) | Loss 3.6052(3.5789) | Error 0.0256(0.0309) Steps 1066(1075.36) | Grad Norm 1.5955(1.8868) | Total Time 14.00(14.00)\n",
      "Iter 17620 | Time 25.0808(25.8438) | Bit/dim 3.5370(3.5363) | Xent 0.1019(0.0882) | Loss 3.5880(3.5804) | Error 0.0322(0.0310) Steps 1066(1075.04) | Grad Norm 3.5110(1.8855) | Total Time 14.00(14.00)\n",
      "Iter 17630 | Time 25.9733(25.8460) | Bit/dim 3.5055(3.5358) | Xent 0.0839(0.0880) | Loss 3.5474(3.5798) | Error 0.0267(0.0310) Steps 1090(1074.51) | Grad Norm 1.6751(1.8768) | Total Time 14.00(14.00)\n",
      "Iter 17640 | Time 25.8814(25.9128) | Bit/dim 3.5308(3.5371) | Xent 0.0935(0.0872) | Loss 3.5775(3.5807) | Error 0.0378(0.0308) Steps 1066(1074.95) | Grad Norm 1.7023(1.8453) | Total Time 14.00(14.00)\n",
      "Iter 17650 | Time 25.6576(25.8897) | Bit/dim 3.5384(3.5335) | Xent 0.0765(0.0869) | Loss 3.5766(3.5769) | Error 0.0233(0.0308) Steps 1078(1074.80) | Grad Norm 1.8099(1.9031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 123.0187, Epoch Time 1564.6400(1570.9371), Bit/dim 3.5452(best: 3.5452), Xent 1.2033, Loss 4.1468, Error 0.2391(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17660 | Time 26.3021(25.9720) | Bit/dim 3.5377(3.5327) | Xent 0.0849(0.0871) | Loss 3.5802(3.5763) | Error 0.0300(0.0306) Steps 1078(1074.21) | Grad Norm 2.0033(1.9009) | Total Time 14.00(14.00)\n",
      "Iter 17670 | Time 26.0780(25.9281) | Bit/dim 3.5012(3.5334) | Xent 0.0967(0.0877) | Loss 3.5495(3.5772) | Error 0.0367(0.0312) Steps 1072(1073.86) | Grad Norm 2.0931(1.9325) | Total Time 14.00(14.00)\n",
      "Iter 17680 | Time 26.1556(25.9560) | Bit/dim 3.5454(3.5334) | Xent 0.0845(0.0890) | Loss 3.5876(3.5779) | Error 0.0322(0.0318) Steps 1066(1073.14) | Grad Norm 2.0346(2.0568) | Total Time 14.00(14.00)\n",
      "Iter 17690 | Time 26.6928(25.9666) | Bit/dim 3.5343(3.5363) | Xent 0.0929(0.0869) | Loss 3.5807(3.5797) | Error 0.0389(0.0315) Steps 1090(1074.13) | Grad Norm 2.4529(2.0490) | Total Time 14.00(14.00)\n",
      "Iter 17700 | Time 26.5951(26.0053) | Bit/dim 3.5354(3.5369) | Xent 0.0825(0.0857) | Loss 3.5767(3.5797) | Error 0.0333(0.0306) Steps 1054(1072.32) | Grad Norm 1.8388(2.0056) | Total Time 14.00(14.00)\n",
      "Iter 17710 | Time 25.3749(26.0047) | Bit/dim 3.5125(3.5351) | Xent 0.1016(0.0867) | Loss 3.5633(3.5785) | Error 0.0400(0.0305) Steps 1084(1073.19) | Grad Norm 1.3689(1.9345) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 124.2752, Epoch Time 1570.5659(1570.9260), Bit/dim 3.5462(best: 3.5452), Xent 1.2099, Loss 4.1511, Error 0.2387(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17720 | Time 26.1199(25.9978) | Bit/dim 3.5384(3.5356) | Xent 0.0835(0.0849) | Loss 3.5802(3.5780) | Error 0.0278(0.0293) Steps 1078(1073.38) | Grad Norm 1.8116(1.8595) | Total Time 14.00(14.00)\n",
      "Iter 17730 | Time 25.3787(25.9528) | Bit/dim 3.5551(3.5372) | Xent 0.1012(0.0855) | Loss 3.6057(3.5800) | Error 0.0400(0.0297) Steps 1066(1073.05) | Grad Norm 2.1012(1.8378) | Total Time 14.00(14.00)\n",
      "Iter 17740 | Time 26.5972(25.9064) | Bit/dim 3.5285(3.5346) | Xent 0.0881(0.0871) | Loss 3.5725(3.5782) | Error 0.0311(0.0304) Steps 1072(1072.66) | Grad Norm 2.6165(1.8386) | Total Time 14.00(14.00)\n",
      "Iter 17750 | Time 25.8056(25.9384) | Bit/dim 3.5005(3.5351) | Xent 0.0992(0.0869) | Loss 3.5501(3.5786) | Error 0.0367(0.0304) Steps 1072(1073.45) | Grad Norm 2.2456(1.8964) | Total Time 14.00(14.00)\n",
      "Iter 17760 | Time 26.1239(25.9330) | Bit/dim 3.5428(3.5350) | Xent 0.0653(0.0868) | Loss 3.5754(3.5784) | Error 0.0222(0.0297) Steps 1060(1073.03) | Grad Norm 1.6860(2.0071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 124.2948, Epoch Time 1565.2543(1570.7558), Bit/dim 3.5475(best: 3.5452), Xent 1.2417, Loss 4.1684, Error 0.2443(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17770 | Time 25.5750(25.9268) | Bit/dim 3.4902(3.5363) | Xent 0.0908(0.0863) | Loss 3.5356(3.5794) | Error 0.0300(0.0297) Steps 1078(1073.84) | Grad Norm 1.3329(2.1070) | Total Time 14.00(14.00)\n",
      "Iter 17780 | Time 26.1136(25.8853) | Bit/dim 3.5481(3.5370) | Xent 0.0759(0.0864) | Loss 3.5861(3.5802) | Error 0.0256(0.0303) Steps 1072(1072.31) | Grad Norm 1.8927(2.0435) | Total Time 14.00(14.00)\n",
      "Iter 17790 | Time 26.6340(25.9283) | Bit/dim 3.5306(3.5357) | Xent 0.0950(0.0880) | Loss 3.5781(3.5797) | Error 0.0322(0.0310) Steps 1096(1074.72) | Grad Norm 2.2611(2.0640) | Total Time 14.00(14.00)\n",
      "Iter 17800 | Time 25.7676(25.8187) | Bit/dim 3.5326(3.5360) | Xent 0.0985(0.0889) | Loss 3.5819(3.5805) | Error 0.0333(0.0312) Steps 1072(1075.25) | Grad Norm 1.7272(2.0896) | Total Time 14.00(14.00)\n",
      "Iter 17810 | Time 25.3596(25.7957) | Bit/dim 3.5001(3.5318) | Xent 0.0895(0.0902) | Loss 3.5449(3.5769) | Error 0.0322(0.0313) Steps 1078(1076.68) | Grad Norm 1.8337(2.0990) | Total Time 14.00(14.00)\n",
      "Iter 17820 | Time 25.7889(25.7772) | Bit/dim 3.5452(3.5341) | Xent 0.0953(0.0888) | Loss 3.5929(3.5785) | Error 0.0289(0.0311) Steps 1072(1076.17) | Grad Norm 1.9396(2.0394) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 123.4760, Epoch Time 1556.8141(1570.3376), Bit/dim 3.5466(best: 3.5452), Xent 1.2319, Loss 4.1625, Error 0.2401(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17830 | Time 26.0568(25.7137) | Bit/dim 3.5057(3.5332) | Xent 0.0995(0.0892) | Loss 3.5555(3.5778) | Error 0.0344(0.0309) Steps 1054(1075.30) | Grad Norm 2.3293(2.0657) | Total Time 14.00(14.00)\n",
      "Iter 17840 | Time 26.3102(25.7304) | Bit/dim 3.5185(3.5354) | Xent 0.0788(0.0870) | Loss 3.5579(3.5789) | Error 0.0222(0.0298) Steps 1060(1074.95) | Grad Norm 1.6387(1.9945) | Total Time 14.00(14.00)\n",
      "Iter 17850 | Time 26.3515(25.8112) | Bit/dim 3.5371(3.5352) | Xent 0.0984(0.0867) | Loss 3.5863(3.5785) | Error 0.0311(0.0294) Steps 1066(1074.70) | Grad Norm 1.4506(1.8691) | Total Time 14.00(14.00)\n",
      "Iter 17860 | Time 26.1459(25.8494) | Bit/dim 3.5527(3.5336) | Xent 0.0904(0.0870) | Loss 3.5979(3.5771) | Error 0.0333(0.0298) Steps 1090(1074.33) | Grad Norm 1.8111(1.8575) | Total Time 14.00(14.00)\n",
      "Iter 17870 | Time 26.0272(25.9494) | Bit/dim 3.5286(3.5331) | Xent 0.0614(0.0874) | Loss 3.5593(3.5768) | Error 0.0200(0.0302) Steps 1084(1075.58) | Grad Norm 1.9143(1.9178) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 124.2613, Epoch Time 1564.8524(1570.1730), Bit/dim 3.5457(best: 3.5452), Xent 1.2340, Loss 4.1627, Error 0.2413(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17880 | Time 25.6820(25.8994) | Bit/dim 3.5377(3.5342) | Xent 0.0820(0.0893) | Loss 3.5787(3.5789) | Error 0.0322(0.0311) Steps 1084(1075.26) | Grad Norm 2.0973(1.9472) | Total Time 14.00(14.00)\n",
      "Iter 17890 | Time 25.6585(25.9022) | Bit/dim 3.4911(3.5303) | Xent 0.1026(0.0896) | Loss 3.5424(3.5752) | Error 0.0367(0.0313) Steps 1072(1076.16) | Grad Norm 1.4687(1.8833) | Total Time 14.00(14.00)\n",
      "Iter 17900 | Time 25.8697(25.8253) | Bit/dim 3.5283(3.5329) | Xent 0.0840(0.0892) | Loss 3.5703(3.5775) | Error 0.0300(0.0312) Steps 1072(1074.89) | Grad Norm 2.7300(2.0002) | Total Time 14.00(14.00)\n",
      "Iter 17910 | Time 25.5113(25.8016) | Bit/dim 3.5337(3.5314) | Xent 0.0940(0.0909) | Loss 3.5807(3.5769) | Error 0.0267(0.0320) Steps 1066(1072.83) | Grad Norm 1.6617(2.1173) | Total Time 14.00(14.00)\n",
      "Iter 17920 | Time 26.2220(25.8086) | Bit/dim 3.5617(3.5343) | Xent 0.0831(0.0912) | Loss 3.6033(3.5799) | Error 0.0322(0.0322) Steps 1090(1073.44) | Grad Norm 2.5651(2.1970) | Total Time 14.00(14.00)\n",
      "Iter 17930 | Time 25.3894(25.7320) | Bit/dim 3.5655(3.5375) | Xent 0.0969(0.0900) | Loss 3.6140(3.5825) | Error 0.0356(0.0319) Steps 1060(1073.06) | Grad Norm 2.8525(2.2599) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 124.1939, Epoch Time 1554.9470(1569.7162), Bit/dim 3.5454(best: 3.5452), Xent 1.1956, Loss 4.1432, Error 0.2397(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17940 | Time 25.5997(25.7251) | Bit/dim 3.5113(3.5351) | Xent 0.0965(0.0928) | Loss 3.5596(3.5815) | Error 0.0322(0.0324) Steps 1066(1073.81) | Grad Norm 1.7638(2.3146) | Total Time 14.00(14.00)\n",
      "Iter 17950 | Time 25.1256(25.7635) | Bit/dim 3.5530(3.5357) | Xent 0.1127(0.0917) | Loss 3.6093(3.5816) | Error 0.0444(0.0320) Steps 1060(1075.31) | Grad Norm 1.9003(2.2089) | Total Time 14.00(14.00)\n",
      "Iter 17960 | Time 26.0214(25.7147) | Bit/dim 3.4968(3.5358) | Xent 0.0969(0.0901) | Loss 3.5452(3.5809) | Error 0.0344(0.0315) Steps 1072(1075.71) | Grad Norm 2.1878(2.1583) | Total Time 14.00(14.00)\n",
      "Iter 17970 | Time 25.4288(25.7484) | Bit/dim 3.5466(3.5375) | Xent 0.0974(0.0916) | Loss 3.5953(3.5833) | Error 0.0367(0.0324) Steps 1078(1075.35) | Grad Norm 2.8964(2.2433) | Total Time 14.00(14.00)\n",
      "Iter 17980 | Time 26.2352(25.7554) | Bit/dim 3.5286(3.5363) | Xent 0.0911(0.0906) | Loss 3.5741(3.5816) | Error 0.0333(0.0326) Steps 1060(1075.04) | Grad Norm 2.5849(2.2938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 122.2219, Epoch Time 1555.0536(1569.2763), Bit/dim 3.5474(best: 3.5452), Xent 1.2551, Loss 4.1749, Error 0.2389(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 17990 | Time 26.2270(25.7341) | Bit/dim 3.5136(3.5356) | Xent 0.0963(0.0895) | Loss 3.5617(3.5803) | Error 0.0344(0.0321) Steps 1078(1075.62) | Grad Norm 2.1244(2.2425) | Total Time 14.00(14.00)\n",
      "Iter 18000 | Time 26.3401(25.8243) | Bit/dim 3.5154(3.5350) | Xent 0.0860(0.0883) | Loss 3.5584(3.5792) | Error 0.0300(0.0321) Steps 1066(1076.20) | Grad Norm 2.2783(2.1880) | Total Time 14.00(14.00)\n",
      "Iter 18010 | Time 25.8798(25.7448) | Bit/dim 3.5280(3.5349) | Xent 0.0848(0.0869) | Loss 3.5704(3.5783) | Error 0.0311(0.0309) Steps 1066(1073.12) | Grad Norm 1.8692(2.1117) | Total Time 14.00(14.00)\n",
      "Iter 18020 | Time 26.6199(25.7810) | Bit/dim 3.5247(3.5319) | Xent 0.0911(0.0861) | Loss 3.5703(3.5750) | Error 0.0300(0.0303) Steps 1078(1074.42) | Grad Norm 1.8298(2.0469) | Total Time 14.00(14.00)\n",
      "Iter 18030 | Time 26.0552(25.8091) | Bit/dim 3.5516(3.5321) | Xent 0.0937(0.0870) | Loss 3.5985(3.5756) | Error 0.0300(0.0310) Steps 1090(1076.59) | Grad Norm 1.8462(2.0556) | Total Time 14.00(14.00)\n",
      "Iter 18040 | Time 25.7716(25.7700) | Bit/dim 3.5624(3.5362) | Xent 0.0898(0.0866) | Loss 3.6073(3.5795) | Error 0.0344(0.0311) Steps 1078(1076.20) | Grad Norm 1.8988(2.0589) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 122.3896, Epoch Time 1556.9436(1568.9064), Bit/dim 3.5467(best: 3.5452), Xent 1.2178, Loss 4.1555, Error 0.2402(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18050 | Time 26.4100(25.7834) | Bit/dim 3.5413(3.5361) | Xent 0.0820(0.0867) | Loss 3.5823(3.5795) | Error 0.0311(0.0307) Steps 1084(1075.92) | Grad Norm 1.9513(2.1046) | Total Time 14.00(14.00)\n",
      "Iter 18060 | Time 25.6902(25.7603) | Bit/dim 3.5285(3.5372) | Xent 0.1139(0.0885) | Loss 3.5855(3.5814) | Error 0.0411(0.0315) Steps 1054(1074.15) | Grad Norm 3.7642(2.2194) | Total Time 14.00(14.00)\n",
      "Iter 18070 | Time 25.3594(25.7088) | Bit/dim 3.5280(3.5360) | Xent 0.0735(0.0882) | Loss 3.5647(3.5801) | Error 0.0300(0.0314) Steps 1066(1074.48) | Grad Norm 1.7552(2.2025) | Total Time 14.00(14.00)\n",
      "Iter 18080 | Time 25.4146(25.6671) | Bit/dim 3.5119(3.5350) | Xent 0.0928(0.0881) | Loss 3.5583(3.5790) | Error 0.0344(0.0312) Steps 1060(1075.15) | Grad Norm 1.4306(2.1559) | Total Time 14.00(14.00)\n",
      "Iter 18090 | Time 26.0060(25.7794) | Bit/dim 3.5073(3.5320) | Xent 0.0783(0.0864) | Loss 3.5465(3.5752) | Error 0.0256(0.0301) Steps 1048(1074.78) | Grad Norm 1.8301(2.0331) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 124.0557, Epoch Time 1558.1403(1568.5834), Bit/dim 3.5457(best: 3.5452), Xent 1.2554, Loss 4.1734, Error 0.2443(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18100 | Time 25.9814(25.8102) | Bit/dim 3.5001(3.5324) | Xent 0.0729(0.0860) | Loss 3.5365(3.5754) | Error 0.0256(0.0302) Steps 1102(1076.60) | Grad Norm 1.7733(2.0596) | Total Time 14.00(14.00)\n",
      "Iter 18110 | Time 25.5884(25.8084) | Bit/dim 3.5744(3.5317) | Xent 0.0986(0.0867) | Loss 3.6236(3.5750) | Error 0.0389(0.0306) Steps 1066(1076.83) | Grad Norm 3.9002(2.1038) | Total Time 14.00(14.00)\n",
      "Iter 18120 | Time 24.9629(25.7400) | Bit/dim 3.5647(3.5349) | Xent 0.0836(0.0879) | Loss 3.6065(3.5788) | Error 0.0378(0.0307) Steps 1066(1074.77) | Grad Norm 1.4419(2.0450) | Total Time 14.00(14.00)\n",
      "Iter 18130 | Time 25.5075(25.6666) | Bit/dim 3.5643(3.5347) | Xent 0.1056(0.0879) | Loss 3.6171(3.5786) | Error 0.0400(0.0310) Steps 1066(1072.96) | Grad Norm 2.7035(2.0292) | Total Time 14.00(14.00)\n",
      "Iter 18140 | Time 25.8997(25.7027) | Bit/dim 3.5093(3.5341) | Xent 0.0696(0.0869) | Loss 3.5441(3.5776) | Error 0.0256(0.0304) Steps 1072(1071.17) | Grad Norm 2.1042(2.0092) | Total Time 14.00(14.00)\n",
      "Iter 18150 | Time 25.8407(25.7214) | Bit/dim 3.5355(3.5330) | Xent 0.0969(0.0865) | Loss 3.5840(3.5762) | Error 0.0278(0.0306) Steps 1078(1072.96) | Grad Norm 2.0046(2.0995) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 121.8069, Epoch Time 1550.4723(1568.0400), Bit/dim 3.5429(best: 3.5452), Xent 1.1874, Loss 4.1366, Error 0.2385(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18160 | Time 25.9192(25.7402) | Bit/dim 3.5412(3.5336) | Xent 0.0964(0.0849) | Loss 3.5894(3.5761) | Error 0.0344(0.0302) Steps 1066(1072.18) | Grad Norm 1.9861(2.0668) | Total Time 14.00(14.00)\n",
      "Iter 18170 | Time 25.8306(25.7328) | Bit/dim 3.5460(3.5340) | Xent 0.0830(0.0858) | Loss 3.5875(3.5769) | Error 0.0289(0.0305) Steps 1084(1074.09) | Grad Norm 1.9407(2.0652) | Total Time 14.00(14.00)\n",
      "Iter 18180 | Time 26.0626(25.7353) | Bit/dim 3.5548(3.5333) | Xent 0.1132(0.0887) | Loss 3.6114(3.5776) | Error 0.0444(0.0318) Steps 1072(1074.14) | Grad Norm 4.8622(2.2041) | Total Time 14.00(14.00)\n",
      "Iter 18190 | Time 25.9734(25.8437) | Bit/dim 3.5255(3.5328) | Xent 0.0922(0.0894) | Loss 3.5716(3.5775) | Error 0.0244(0.0319) Steps 1084(1075.81) | Grad Norm 1.9774(2.3063) | Total Time 14.00(14.00)\n",
      "Iter 18200 | Time 25.3838(25.8949) | Bit/dim 3.5557(3.5329) | Xent 0.0893(0.0901) | Loss 3.6004(3.5779) | Error 0.0278(0.0321) Steps 1078(1076.55) | Grad Norm 2.2673(2.2460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 123.4062, Epoch Time 1563.5141(1567.9043), Bit/dim 3.5467(best: 3.5429), Xent 1.2495, Loss 4.1714, Error 0.2413(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18210 | Time 25.7748(25.9033) | Bit/dim 3.5422(3.5342) | Xent 0.0760(0.0869) | Loss 3.5802(3.5776) | Error 0.0244(0.0309) Steps 1084(1076.72) | Grad Norm 2.1147(2.1831) | Total Time 14.00(14.00)\n",
      "Iter 18220 | Time 26.1675(25.8005) | Bit/dim 3.5008(3.5335) | Xent 0.0735(0.0849) | Loss 3.5376(3.5759) | Error 0.0211(0.0298) Steps 1084(1076.25) | Grad Norm 1.6779(2.1001) | Total Time 14.00(14.00)\n",
      "Iter 18230 | Time 25.2423(25.7275) | Bit/dim 3.5323(3.5334) | Xent 0.0630(0.0830) | Loss 3.5638(3.5749) | Error 0.0300(0.0296) Steps 1066(1074.92) | Grad Norm 1.2894(1.9934) | Total Time 14.00(14.00)\n",
      "Iter 18240 | Time 26.1689(25.7222) | Bit/dim 3.5138(3.5327) | Xent 0.0733(0.0841) | Loss 3.5505(3.5747) | Error 0.0200(0.0295) Steps 1066(1074.58) | Grad Norm 1.4975(2.0214) | Total Time 14.00(14.00)\n",
      "Iter 18250 | Time 25.3541(25.6996) | Bit/dim 3.5288(3.5349) | Xent 0.0736(0.0847) | Loss 3.5656(3.5772) | Error 0.0256(0.0295) Steps 1072(1075.11) | Grad Norm 1.7278(2.0382) | Total Time 14.00(14.00)\n",
      "Iter 18260 | Time 25.7661(25.7318) | Bit/dim 3.4986(3.5331) | Xent 0.0683(0.0869) | Loss 3.5328(3.5765) | Error 0.0244(0.0302) Steps 1066(1075.55) | Grad Norm 1.8734(2.0618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 122.8076, Epoch Time 1550.5077(1567.3824), Bit/dim 3.5460(best: 3.5429), Xent 1.2320, Loss 4.1620, Error 0.2366(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18270 | Time 25.6936(25.7217) | Bit/dim 3.5117(3.5324) | Xent 0.0753(0.0856) | Loss 3.5493(3.5752) | Error 0.0289(0.0297) Steps 1072(1074.72) | Grad Norm 1.9090(2.1222) | Total Time 14.00(14.00)\n",
      "Iter 18280 | Time 25.9047(25.7190) | Bit/dim 3.5055(3.5322) | Xent 0.0874(0.0856) | Loss 3.5493(3.5750) | Error 0.0333(0.0299) Steps 1078(1073.52) | Grad Norm 1.7852(2.1170) | Total Time 14.00(14.00)\n",
      "Iter 18290 | Time 24.7677(25.6427) | Bit/dim 3.5541(3.5337) | Xent 0.0908(0.0847) | Loss 3.5994(3.5760) | Error 0.0300(0.0298) Steps 1072(1073.12) | Grad Norm 2.7047(2.1530) | Total Time 14.00(14.00)\n",
      "Iter 18300 | Time 25.7748(25.6078) | Bit/dim 3.5583(3.5342) | Xent 0.0755(0.0846) | Loss 3.5960(3.5765) | Error 0.0289(0.0296) Steps 1078(1073.16) | Grad Norm 1.5206(2.1710) | Total Time 14.00(14.00)\n",
      "Iter 18310 | Time 25.3649(25.5720) | Bit/dim 3.5466(3.5341) | Xent 0.0960(0.0863) | Loss 3.5946(3.5772) | Error 0.0333(0.0306) Steps 1072(1073.00) | Grad Norm 2.5082(2.1878) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 123.1540, Epoch Time 1544.8133(1566.7053), Bit/dim 3.5449(best: 3.5429), Xent 1.2662, Loss 4.1780, Error 0.2397(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18320 | Time 25.2499(25.5589) | Bit/dim 3.5267(3.5349) | Xent 0.0867(0.0860) | Loss 3.5701(3.5779) | Error 0.0300(0.0297) Steps 1066(1073.60) | Grad Norm 1.9173(2.0667) | Total Time 14.00(14.00)\n",
      "Iter 18330 | Time 26.2466(25.5848) | Bit/dim 3.5001(3.5319) | Xent 0.0848(0.0851) | Loss 3.5426(3.5744) | Error 0.0300(0.0296) Steps 1090(1074.65) | Grad Norm 1.8747(2.0160) | Total Time 14.00(14.00)\n",
      "Iter 18340 | Time 25.6881(25.7127) | Bit/dim 3.5263(3.5334) | Xent 0.0866(0.0853) | Loss 3.5696(3.5761) | Error 0.0333(0.0303) Steps 1072(1073.61) | Grad Norm 2.5062(2.0541) | Total Time 14.00(14.00)\n",
      "Iter 18350 | Time 25.6708(25.6709) | Bit/dim 3.5625(3.5318) | Xent 0.0743(0.0844) | Loss 3.5997(3.5740) | Error 0.0267(0.0299) Steps 1078(1073.37) | Grad Norm 2.2070(2.0935) | Total Time 14.00(14.00)\n",
      "Iter 18360 | Time 25.7891(25.6781) | Bit/dim 3.5583(3.5337) | Xent 0.0889(0.0846) | Loss 3.6027(3.5760) | Error 0.0278(0.0298) Steps 1084(1072.87) | Grad Norm 2.2708(2.1597) | Total Time 14.00(14.00)\n",
      "Iter 18370 | Time 25.5983(25.6639) | Bit/dim 3.5387(3.5336) | Xent 0.0832(0.0859) | Loss 3.5803(3.5766) | Error 0.0267(0.0305) Steps 1072(1072.08) | Grad Norm 1.6121(2.1368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 122.1599, Epoch Time 1552.5929(1566.2819), Bit/dim 3.5463(best: 3.5429), Xent 1.2782, Loss 4.1854, Error 0.2440(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18380 | Time 25.9661(25.6840) | Bit/dim 3.5295(3.5303) | Xent 0.1060(0.0872) | Loss 3.5825(3.5740) | Error 0.0444(0.0314) Steps 1084(1072.86) | Grad Norm 2.3638(2.1347) | Total Time 14.00(14.00)\n",
      "Iter 18390 | Time 25.8358(25.7257) | Bit/dim 3.5584(3.5329) | Xent 0.0766(0.0877) | Loss 3.5967(3.5768) | Error 0.0267(0.0317) Steps 1084(1074.21) | Grad Norm 1.9317(2.1309) | Total Time 14.00(14.00)\n",
      "Iter 18400 | Time 26.0078(25.7091) | Bit/dim 3.5582(3.5341) | Xent 0.0797(0.0875) | Loss 3.5981(3.5778) | Error 0.0300(0.0314) Steps 1072(1073.43) | Grad Norm 1.8400(2.1415) | Total Time 14.00(14.00)\n",
      "Iter 18410 | Time 25.7014(25.7043) | Bit/dim 3.5559(3.5336) | Xent 0.0675(0.0874) | Loss 3.5896(3.5773) | Error 0.0267(0.0314) Steps 1078(1074.39) | Grad Norm 1.3765(2.1384) | Total Time 14.00(14.00)\n",
      "Iter 18420 | Time 26.0142(25.7927) | Bit/dim 3.5188(3.5345) | Xent 0.0801(0.0870) | Loss 3.5588(3.5780) | Error 0.0322(0.0314) Steps 1066(1075.91) | Grad Norm 3.1408(2.2341) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 123.1924, Epoch Time 1557.6317(1566.0224), Bit/dim 3.5454(best: 3.5429), Xent 1.2622, Loss 4.1765, Error 0.2418(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18430 | Time 26.1649(25.7524) | Bit/dim 3.5551(3.5357) | Xent 0.0778(0.0856) | Loss 3.5940(3.5785) | Error 0.0278(0.0306) Steps 1078(1075.58) | Grad Norm 1.9626(2.1476) | Total Time 14.00(14.00)\n",
      "Iter 18440 | Time 24.8630(25.7871) | Bit/dim 3.5664(3.5366) | Xent 0.0799(0.0842) | Loss 3.6064(3.5787) | Error 0.0278(0.0304) Steps 1072(1074.51) | Grad Norm 2.2541(2.1413) | Total Time 14.00(14.00)\n",
      "Iter 18450 | Time 25.5512(25.7467) | Bit/dim 3.5457(3.5379) | Xent 0.0649(0.0840) | Loss 3.5781(3.5800) | Error 0.0211(0.0304) Steps 1066(1073.66) | Grad Norm 1.7161(2.0395) | Total Time 14.00(14.00)\n",
      "Iter 18460 | Time 25.4404(25.7597) | Bit/dim 3.5217(3.5350) | Xent 0.0779(0.0821) | Loss 3.5606(3.5761) | Error 0.0267(0.0298) Steps 1060(1072.48) | Grad Norm 1.7291(1.9316) | Total Time 14.00(14.00)\n",
      "Iter 18470 | Time 26.0560(25.7564) | Bit/dim 3.4908(3.5317) | Xent 0.0763(0.0828) | Loss 3.5289(3.5731) | Error 0.0222(0.0299) Steps 1078(1073.31) | Grad Norm 1.4033(1.9967) | Total Time 14.00(14.00)\n",
      "Iter 18480 | Time 25.5214(25.7444) | Bit/dim 3.5094(3.5317) | Xent 0.1021(0.0851) | Loss 3.5605(3.5742) | Error 0.0389(0.0306) Steps 1072(1074.69) | Grad Norm 2.3889(1.9994) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 124.0814, Epoch Time 1558.1311(1565.7857), Bit/dim 3.5461(best: 3.5429), Xent 1.2730, Loss 4.1826, Error 0.2413(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18490 | Time 26.0520(25.7438) | Bit/dim 3.5481(3.5310) | Xent 0.0833(0.0863) | Loss 3.5897(3.5741) | Error 0.0322(0.0315) Steps 1078(1073.35) | Grad Norm 2.4893(2.0724) | Total Time 14.00(14.00)\n",
      "Iter 18500 | Time 24.9729(25.7775) | Bit/dim 3.5504(3.5324) | Xent 0.0758(0.0874) | Loss 3.5883(3.5761) | Error 0.0289(0.0312) Steps 1066(1073.83) | Grad Norm 1.8270(2.1192) | Total Time 14.00(14.00)\n",
      "Iter 18510 | Time 25.5676(25.7535) | Bit/dim 3.5170(3.5307) | Xent 0.0879(0.0874) | Loss 3.5610(3.5744) | Error 0.0300(0.0313) Steps 1072(1073.48) | Grad Norm 1.9730(2.1553) | Total Time 14.00(14.00)\n",
      "Iter 18520 | Time 25.5476(25.7682) | Bit/dim 3.5436(3.5329) | Xent 0.0858(0.0872) | Loss 3.5865(3.5765) | Error 0.0311(0.0312) Steps 1060(1073.24) | Grad Norm 2.1404(2.1864) | Total Time 14.00(14.00)\n",
      "Iter 18530 | Time 25.5320(25.7649) | Bit/dim 3.5446(3.5336) | Xent 0.1001(0.0888) | Loss 3.5946(3.5780) | Error 0.0344(0.0320) Steps 1078(1075.40) | Grad Norm 2.7179(2.3029) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 123.1817, Epoch Time 1557.2453(1565.5295), Bit/dim 3.5440(best: 3.5429), Xent 1.2211, Loss 4.1545, Error 0.2350(best: 0.2353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18540 | Time 26.6740(25.7870) | Bit/dim 3.5451(3.5353) | Xent 0.0881(0.0881) | Loss 3.5892(3.5793) | Error 0.0300(0.0315) Steps 1090(1074.97) | Grad Norm 1.5440(2.2809) | Total Time 14.00(14.00)\n",
      "Iter 18550 | Time 25.4349(25.7275) | Bit/dim 3.5500(3.5370) | Xent 0.0721(0.0866) | Loss 3.5860(3.5803) | Error 0.0233(0.0307) Steps 1066(1074.05) | Grad Norm 1.4731(2.2575) | Total Time 14.00(14.00)\n",
      "Iter 18560 | Time 25.4710(25.6982) | Bit/dim 3.5467(3.5340) | Xent 0.0891(0.0877) | Loss 3.5913(3.5779) | Error 0.0311(0.0308) Steps 1054(1072.92) | Grad Norm 2.7967(2.2568) | Total Time 14.00(14.00)\n",
      "Iter 18570 | Time 26.0206(25.7039) | Bit/dim 3.5306(3.5370) | Xent 0.0812(0.0872) | Loss 3.5712(3.5806) | Error 0.0267(0.0311) Steps 1054(1072.21) | Grad Norm 2.2959(2.2885) | Total Time 14.00(14.00)\n",
      "Iter 18580 | Time 25.9242(25.7205) | Bit/dim 3.5345(3.5355) | Xent 0.0942(0.0867) | Loss 3.5815(3.5788) | Error 0.0300(0.0306) Steps 1066(1071.84) | Grad Norm 2.0536(2.2644) | Total Time 14.00(14.00)\n",
      "Iter 18590 | Time 25.6101(25.6456) | Bit/dim 3.5124(3.5328) | Xent 0.0771(0.0857) | Loss 3.5509(3.5756) | Error 0.0244(0.0306) Steps 1090(1072.78) | Grad Norm 2.2184(2.1967) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 123.0892, Epoch Time 1550.1234(1565.0673), Bit/dim 3.5433(best: 3.5429), Xent 1.2302, Loss 4.1584, Error 0.2406(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18600 | Time 25.2207(25.6624) | Bit/dim 3.5716(3.5343) | Xent 0.0990(0.0865) | Loss 3.6211(3.5776) | Error 0.0322(0.0310) Steps 1078(1072.10) | Grad Norm 2.5215(2.1961) | Total Time 14.00(14.00)\n",
      "Iter 18610 | Time 26.1888(25.6762) | Bit/dim 3.5412(3.5342) | Xent 0.0678(0.0856) | Loss 3.5751(3.5770) | Error 0.0267(0.0309) Steps 1078(1072.62) | Grad Norm 1.6876(2.2406) | Total Time 14.00(14.00)\n",
      "Iter 18620 | Time 25.3954(25.6770) | Bit/dim 3.5242(3.5331) | Xent 0.0760(0.0879) | Loss 3.5622(3.5770) | Error 0.0256(0.0311) Steps 1066(1073.00) | Grad Norm 1.6941(2.2362) | Total Time 14.00(14.00)\n",
      "Iter 18630 | Time 25.7922(25.6853) | Bit/dim 3.5442(3.5325) | Xent 0.0805(0.0877) | Loss 3.5845(3.5763) | Error 0.0289(0.0309) Steps 1078(1073.56) | Grad Norm 1.6239(2.1753) | Total Time 14.00(14.00)\n",
      "Iter 18640 | Time 26.1111(25.7259) | Bit/dim 3.5160(3.5321) | Xent 0.0811(0.0889) | Loss 3.5565(3.5766) | Error 0.0278(0.0313) Steps 1078(1073.23) | Grad Norm 1.7139(2.0958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 123.2572, Epoch Time 1555.0815(1564.7677), Bit/dim 3.5426(best: 3.5429), Xent 1.2057, Loss 4.1455, Error 0.2390(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18650 | Time 26.4336(25.7137) | Bit/dim 3.5414(3.5316) | Xent 0.0819(0.0861) | Loss 3.5823(3.5746) | Error 0.0311(0.0305) Steps 1048(1071.75) | Grad Norm 2.0248(2.0188) | Total Time 14.00(14.00)\n",
      "Iter 18660 | Time 25.3181(25.7268) | Bit/dim 3.5656(3.5329) | Xent 0.0817(0.0862) | Loss 3.6064(3.5760) | Error 0.0267(0.0306) Steps 1072(1071.72) | Grad Norm 2.6264(2.0861) | Total Time 14.00(14.00)\n",
      "Iter 18670 | Time 25.6508(25.7276) | Bit/dim 3.5566(3.5329) | Xent 0.1173(0.0876) | Loss 3.6152(3.5767) | Error 0.0444(0.0309) Steps 1084(1071.83) | Grad Norm 2.4137(2.0617) | Total Time 14.00(14.00)\n",
      "Iter 18680 | Time 25.6375(25.7095) | Bit/dim 3.5311(3.5317) | Xent 0.0856(0.0871) | Loss 3.5739(3.5752) | Error 0.0322(0.0307) Steps 1072(1072.15) | Grad Norm 1.7194(2.0303) | Total Time 14.00(14.00)\n",
      "Iter 18690 | Time 25.5305(25.7740) | Bit/dim 3.5649(3.5340) | Xent 0.0922(0.0877) | Loss 3.6111(3.5778) | Error 0.0300(0.0310) Steps 1072(1072.64) | Grad Norm 2.4028(2.1537) | Total Time 14.00(14.00)\n",
      "Iter 18700 | Time 27.2233(25.8154) | Bit/dim 3.5102(3.5334) | Xent 0.0996(0.0867) | Loss 3.5600(3.5768) | Error 0.0322(0.0304) Steps 1090(1073.29) | Grad Norm 2.8166(2.2097) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 123.9265, Epoch Time 1558.5498(1564.5812), Bit/dim 3.5443(best: 3.5426), Xent 1.2480, Loss 4.1683, Error 0.2401(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18710 | Time 25.6681(25.7799) | Bit/dim 3.5389(3.5328) | Xent 0.0737(0.0855) | Loss 3.5758(3.5756) | Error 0.0289(0.0303) Steps 1072(1074.38) | Grad Norm 1.7633(2.1606) | Total Time 14.00(14.00)\n",
      "Iter 18720 | Time 25.2528(25.7704) | Bit/dim 3.5499(3.5368) | Xent 0.0947(0.0850) | Loss 3.5972(3.5793) | Error 0.0356(0.0300) Steps 1066(1074.33) | Grad Norm 2.1612(2.1489) | Total Time 14.00(14.00)\n",
      "Iter 18730 | Time 26.1584(25.7459) | Bit/dim 3.5306(3.5369) | Xent 0.0915(0.0837) | Loss 3.5763(3.5787) | Error 0.0322(0.0295) Steps 1060(1074.00) | Grad Norm 1.9784(2.1732) | Total Time 14.00(14.00)\n",
      "Iter 18740 | Time 25.1128(25.7451) | Bit/dim 3.4976(3.5340) | Xent 0.0860(0.0861) | Loss 3.5406(3.5771) | Error 0.0344(0.0302) Steps 1072(1074.76) | Grad Norm 2.1220(2.1874) | Total Time 14.00(14.00)\n",
      "Iter 18750 | Time 25.4675(25.7588) | Bit/dim 3.5332(3.5306) | Xent 0.0592(0.0838) | Loss 3.5628(3.5725) | Error 0.0200(0.0299) Steps 1078(1074.28) | Grad Norm 2.0356(2.1653) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 123.7250, Epoch Time 1554.6850(1564.2843), Bit/dim 3.5437(best: 3.5426), Xent 1.2674, Loss 4.1774, Error 0.2411(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18760 | Time 25.1904(25.7439) | Bit/dim 3.5542(3.5332) | Xent 0.0946(0.0848) | Loss 3.6016(3.5756) | Error 0.0378(0.0307) Steps 1060(1074.56) | Grad Norm 2.3702(2.1869) | Total Time 14.00(14.00)\n",
      "Iter 18770 | Time 25.4200(25.7443) | Bit/dim 3.5201(3.5317) | Xent 0.0857(0.0831) | Loss 3.5629(3.5733) | Error 0.0322(0.0298) Steps 1066(1073.74) | Grad Norm 1.7188(2.1372) | Total Time 14.00(14.00)\n",
      "Iter 18780 | Time 26.0965(25.6808) | Bit/dim 3.5760(3.5340) | Xent 0.0795(0.0830) | Loss 3.6157(3.5755) | Error 0.0322(0.0298) Steps 1084(1073.92) | Grad Norm 1.7811(2.1022) | Total Time 14.00(14.00)\n",
      "Iter 18790 | Time 25.0672(25.6373) | Bit/dim 3.5248(3.5334) | Xent 0.0937(0.0839) | Loss 3.5716(3.5753) | Error 0.0367(0.0304) Steps 1072(1072.88) | Grad Norm 2.9221(2.2217) | Total Time 14.00(14.00)\n",
      "Iter 18800 | Time 26.3991(25.6349) | Bit/dim 3.5500(3.5347) | Xent 0.1173(0.0860) | Loss 3.6086(3.5777) | Error 0.0422(0.0308) Steps 1072(1071.82) | Grad Norm 3.9272(2.3832) | Total Time 14.00(14.00)\n",
      "Iter 18810 | Time 25.6932(25.6475) | Bit/dim 3.5074(3.5319) | Xent 0.0808(0.0849) | Loss 3.5478(3.5743) | Error 0.0311(0.0306) Steps 1084(1072.17) | Grad Norm 2.5539(2.2932) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 123.8501, Epoch Time 1549.8164(1563.8503), Bit/dim 3.5436(best: 3.5426), Xent 1.2628, Loss 4.1750, Error 0.2388(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18820 | Time 25.6429(25.7533) | Bit/dim 3.5326(3.5318) | Xent 0.0963(0.0851) | Loss 3.5807(3.5743) | Error 0.0378(0.0308) Steps 1078(1072.79) | Grad Norm 1.8154(2.2708) | Total Time 14.00(14.00)\n",
      "Iter 18830 | Time 25.7493(25.8108) | Bit/dim 3.5197(3.5302) | Xent 0.0764(0.0820) | Loss 3.5579(3.5712) | Error 0.0233(0.0293) Steps 1084(1072.73) | Grad Norm 1.6225(2.1317) | Total Time 14.00(14.00)\n",
      "Iter 18840 | Time 26.8626(25.8086) | Bit/dim 3.5310(3.5287) | Xent 0.0929(0.0814) | Loss 3.5775(3.5694) | Error 0.0322(0.0291) Steps 1066(1071.82) | Grad Norm 1.8050(2.0410) | Total Time 14.00(14.00)\n",
      "Iter 18850 | Time 25.3509(25.7345) | Bit/dim 3.5111(3.5288) | Xent 0.0740(0.0804) | Loss 3.5481(3.5690) | Error 0.0233(0.0281) Steps 1084(1071.89) | Grad Norm 1.9493(1.9775) | Total Time 14.00(14.00)\n",
      "Iter 18860 | Time 25.3196(25.7989) | Bit/dim 3.5526(3.5319) | Xent 0.0728(0.0810) | Loss 3.5890(3.5725) | Error 0.0256(0.0287) Steps 1072(1073.00) | Grad Norm 2.2018(1.9382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 124.1279, Epoch Time 1562.7982(1563.8187), Bit/dim 3.5436(best: 3.5426), Xent 1.2623, Loss 4.1748, Error 0.2411(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18870 | Time 25.6245(25.7974) | Bit/dim 3.5515(3.5319) | Xent 0.0765(0.0813) | Loss 3.5898(3.5726) | Error 0.0289(0.0287) Steps 1084(1073.70) | Grad Norm 1.9714(1.9684) | Total Time 14.00(14.00)\n",
      "Iter 18880 | Time 25.1793(25.7549) | Bit/dim 3.5365(3.5320) | Xent 0.0917(0.0813) | Loss 3.5824(3.5727) | Error 0.0300(0.0284) Steps 1072(1072.42) | Grad Norm 1.7537(1.9191) | Total Time 14.00(14.00)\n",
      "Iter 18890 | Time 25.2276(25.7417) | Bit/dim 3.5291(3.5327) | Xent 0.0684(0.0827) | Loss 3.5633(3.5741) | Error 0.0200(0.0293) Steps 1078(1073.37) | Grad Norm 1.6726(1.8884) | Total Time 14.00(14.00)\n",
      "Iter 18900 | Time 25.3248(25.7717) | Bit/dim 3.5276(3.5336) | Xent 0.0809(0.0823) | Loss 3.5681(3.5747) | Error 0.0300(0.0292) Steps 1066(1073.60) | Grad Norm 2.1515(1.9246) | Total Time 14.00(14.00)\n",
      "Iter 18910 | Time 25.6406(25.7097) | Bit/dim 3.5343(3.5333) | Xent 0.0713(0.0832) | Loss 3.5700(3.5749) | Error 0.0233(0.0292) Steps 1072(1073.52) | Grad Norm 1.8805(1.9139) | Total Time 14.00(14.00)\n",
      "Iter 18920 | Time 25.5405(25.7729) | Bit/dim 3.5148(3.5323) | Xent 0.0883(0.0825) | Loss 3.5590(3.5735) | Error 0.0311(0.0289) Steps 1060(1072.96) | Grad Norm 1.3957(1.8410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 123.2221, Epoch Time 1555.4456(1563.5675), Bit/dim 3.5427(best: 3.5426), Xent 1.2346, Loss 4.1600, Error 0.2386(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18930 | Time 26.1415(25.7819) | Bit/dim 3.5625(3.5313) | Xent 0.0735(0.0822) | Loss 3.5993(3.5724) | Error 0.0211(0.0288) Steps 1066(1073.49) | Grad Norm 1.5593(1.8113) | Total Time 14.00(14.00)\n",
      "Iter 18940 | Time 26.0325(25.7216) | Bit/dim 3.5362(3.5326) | Xent 0.0825(0.0817) | Loss 3.5775(3.5735) | Error 0.0289(0.0288) Steps 1060(1073.93) | Grad Norm 2.0472(1.8772) | Total Time 14.00(14.00)\n",
      "Iter 18950 | Time 25.2943(25.7130) | Bit/dim 3.5323(3.5316) | Xent 0.0850(0.0826) | Loss 3.5748(3.5730) | Error 0.0311(0.0294) Steps 1084(1074.44) | Grad Norm 2.0590(1.9214) | Total Time 14.00(14.00)\n",
      "Iter 18960 | Time 25.6050(25.7287) | Bit/dim 3.5128(3.5302) | Xent 0.0795(0.0820) | Loss 3.5525(3.5712) | Error 0.0311(0.0293) Steps 1078(1075.21) | Grad Norm 1.5401(1.9015) | Total Time 14.00(14.00)\n",
      "Iter 18970 | Time 26.2192(25.7259) | Bit/dim 3.5490(3.5334) | Xent 0.0869(0.0824) | Loss 3.5925(3.5746) | Error 0.0311(0.0295) Steps 1078(1074.82) | Grad Norm 2.6233(1.9428) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 124.0985, Epoch Time 1553.8428(1563.2758), Bit/dim 3.5442(best: 3.5426), Xent 1.2882, Loss 4.1883, Error 0.2428(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 18980 | Time 25.9023(25.7508) | Bit/dim 3.5502(3.5329) | Xent 0.0790(0.0821) | Loss 3.5898(3.5739) | Error 0.0267(0.0293) Steps 1072(1074.58) | Grad Norm 2.1667(2.0775) | Total Time 14.00(14.00)\n",
      "Iter 18990 | Time 25.2980(25.7633) | Bit/dim 3.5462(3.5306) | Xent 0.0867(0.0836) | Loss 3.5895(3.5724) | Error 0.0267(0.0293) Steps 1066(1074.09) | Grad Norm 2.2858(2.0339) | Total Time 14.00(14.00)\n",
      "Iter 19000 | Time 26.1019(25.7362) | Bit/dim 3.5541(3.5329) | Xent 0.0798(0.0836) | Loss 3.5940(3.5747) | Error 0.0289(0.0293) Steps 1072(1074.25) | Grad Norm 2.6118(2.0749) | Total Time 14.00(14.00)\n",
      "Iter 19010 | Time 25.2514(25.7071) | Bit/dim 3.5475(3.5345) | Xent 0.1018(0.0853) | Loss 3.5984(3.5772) | Error 0.0344(0.0300) Steps 1066(1074.61) | Grad Norm 2.8411(2.1442) | Total Time 14.00(14.00)\n",
      "Iter 19020 | Time 25.2406(25.6627) | Bit/dim 3.5182(3.5328) | Xent 0.1003(0.0878) | Loss 3.5683(3.5767) | Error 0.0333(0.0307) Steps 1066(1073.03) | Grad Norm 2.0603(2.2582) | Total Time 14.00(14.00)\n",
      "Iter 19030 | Time 25.3420(25.6617) | Bit/dim 3.5716(3.5331) | Xent 0.0733(0.0863) | Loss 3.6083(3.5762) | Error 0.0289(0.0304) Steps 1078(1072.01) | Grad Norm 1.4974(2.1231) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 123.8562, Epoch Time 1553.0497(1562.9690), Bit/dim 3.5444(best: 3.5426), Xent 1.2470, Loss 4.1679, Error 0.2425(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19040 | Time 26.6038(25.6613) | Bit/dim 3.5090(3.5326) | Xent 0.0925(0.0851) | Loss 3.5552(3.5752) | Error 0.0322(0.0300) Steps 1066(1071.62) | Grad Norm 1.7692(2.0026) | Total Time 14.00(14.00)\n",
      "Iter 19050 | Time 25.2886(25.6605) | Bit/dim 3.5404(3.5320) | Xent 0.0845(0.0845) | Loss 3.5827(3.5743) | Error 0.0278(0.0297) Steps 1066(1071.10) | Grad Norm 2.6833(2.0633) | Total Time 14.00(14.00)\n",
      "Iter 19060 | Time 25.2354(25.6463) | Bit/dim 3.5462(3.5300) | Xent 0.0867(0.0870) | Loss 3.5896(3.5735) | Error 0.0300(0.0312) Steps 1060(1072.33) | Grad Norm 1.9749(2.0682) | Total Time 14.00(14.00)\n",
      "Iter 19070 | Time 25.1858(25.6688) | Bit/dim 3.4911(3.5300) | Xent 0.0780(0.0865) | Loss 3.5301(3.5733) | Error 0.0300(0.0311) Steps 1078(1072.55) | Grad Norm 2.5352(2.1479) | Total Time 14.00(14.00)\n",
      "Iter 19080 | Time 25.2277(25.6453) | Bit/dim 3.5228(3.5311) | Xent 0.1035(0.0862) | Loss 3.5745(3.5742) | Error 0.0433(0.0313) Steps 1084(1073.81) | Grad Norm 2.8768(2.1674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 123.8480, Epoch Time 1549.4315(1562.5629), Bit/dim 3.5451(best: 3.5426), Xent 1.2899, Loss 4.1900, Error 0.2418(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19090 | Time 25.2665(25.6361) | Bit/dim 3.5413(3.5333) | Xent 0.0802(0.0853) | Loss 3.5814(3.5759) | Error 0.0233(0.0309) Steps 1054(1072.54) | Grad Norm 3.0543(2.2699) | Total Time 14.00(14.00)\n",
      "Iter 19100 | Time 25.9719(25.7320) | Bit/dim 3.5637(3.5348) | Xent 0.0669(0.0872) | Loss 3.5972(3.5783) | Error 0.0189(0.0315) Steps 1066(1072.63) | Grad Norm 1.7332(2.4582) | Total Time 14.00(14.00)\n",
      "Iter 19110 | Time 25.5541(25.6927) | Bit/dim 3.5636(3.5359) | Xent 0.1102(0.0883) | Loss 3.6187(3.5801) | Error 0.0344(0.0318) Steps 1084(1072.99) | Grad Norm 2.3430(2.3970) | Total Time 14.00(14.00)\n",
      "Iter 19120 | Time 25.6817(25.6950) | Bit/dim 3.5023(3.5335) | Xent 0.1143(0.0893) | Loss 3.5595(3.5781) | Error 0.0500(0.0325) Steps 1060(1071.56) | Grad Norm 3.8114(2.4013) | Total Time 14.00(14.00)\n",
      "Iter 19130 | Time 25.2153(25.6547) | Bit/dim 3.5268(3.5329) | Xent 0.0827(0.0886) | Loss 3.5681(3.5772) | Error 0.0311(0.0323) Steps 1054(1070.58) | Grad Norm 2.6538(2.3650) | Total Time 14.00(14.00)\n",
      "Iter 19140 | Time 25.8247(25.6644) | Bit/dim 3.5280(3.5311) | Xent 0.0893(0.0873) | Loss 3.5727(3.5747) | Error 0.0256(0.0311) Steps 1078(1070.45) | Grad Norm 2.2877(2.3337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 123.3932, Epoch Time 1553.7441(1562.2983), Bit/dim 3.5441(best: 3.5426), Xent 1.2925, Loss 4.1903, Error 0.2422(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19150 | Time 25.2023(25.7238) | Bit/dim 3.5585(3.5329) | Xent 0.0718(0.0871) | Loss 3.5944(3.5765) | Error 0.0244(0.0312) Steps 1054(1071.22) | Grad Norm 1.8140(2.3467) | Total Time 14.00(14.00)\n",
      "Iter 19160 | Time 25.5538(25.6753) | Bit/dim 3.5534(3.5352) | Xent 0.0723(0.0846) | Loss 3.5896(3.5775) | Error 0.0211(0.0303) Steps 1066(1069.82) | Grad Norm 2.6252(2.2609) | Total Time 14.00(14.00)\n",
      "Iter 19170 | Time 25.7301(25.7470) | Bit/dim 3.5724(3.5375) | Xent 0.0868(0.0852) | Loss 3.6158(3.5802) | Error 0.0300(0.0303) Steps 1060(1070.44) | Grad Norm 2.0406(2.2875) | Total Time 14.00(14.00)\n",
      "Iter 19180 | Time 26.3615(25.7412) | Bit/dim 3.5298(3.5341) | Xent 0.0919(0.0860) | Loss 3.5758(3.5771) | Error 0.0344(0.0305) Steps 1096(1072.48) | Grad Norm 1.6188(2.1799) | Total Time 14.00(14.00)\n",
      "Iter 19190 | Time 25.5749(25.6802) | Bit/dim 3.5317(3.5311) | Xent 0.0839(0.0863) | Loss 3.5737(3.5743) | Error 0.0311(0.0306) Steps 1084(1072.18) | Grad Norm 1.8624(2.2035) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 123.3691, Epoch Time 1555.0223(1562.0800), Bit/dim 3.5428(best: 3.5426), Xent 1.2926, Loss 4.1891, Error 0.2414(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19200 | Time 25.5362(25.7382) | Bit/dim 3.5343(3.5302) | Xent 0.0683(0.0839) | Loss 3.5685(3.5722) | Error 0.0244(0.0300) Steps 1096(1072.07) | Grad Norm 1.4383(2.2129) | Total Time 14.00(14.00)\n",
      "Iter 19210 | Time 26.0668(25.7463) | Bit/dim 3.5444(3.5312) | Xent 0.0903(0.0844) | Loss 3.5895(3.5735) | Error 0.0344(0.0302) Steps 1084(1072.70) | Grad Norm 1.9664(2.1957) | Total Time 14.00(14.00)\n",
      "Iter 19220 | Time 26.4018(25.7129) | Bit/dim 3.5234(3.5322) | Xent 0.1150(0.0841) | Loss 3.5809(3.5743) | Error 0.0467(0.0302) Steps 1078(1072.64) | Grad Norm 2.1953(2.1796) | Total Time 14.00(14.00)\n",
      "Iter 19230 | Time 25.2890(25.6800) | Bit/dim 3.5328(3.5301) | Xent 0.0622(0.0830) | Loss 3.5639(3.5717) | Error 0.0211(0.0298) Steps 1078(1072.23) | Grad Norm 1.2719(2.0533) | Total Time 14.00(14.00)\n",
      "Iter 19240 | Time 25.4803(25.6964) | Bit/dim 3.5117(3.5312) | Xent 0.0848(0.0827) | Loss 3.5541(3.5726) | Error 0.0289(0.0294) Steps 1066(1071.35) | Grad Norm 4.1044(2.0905) | Total Time 14.00(14.00)\n",
      "Iter 19250 | Time 25.6503(25.7036) | Bit/dim 3.5195(3.5311) | Xent 0.0909(0.0848) | Loss 3.5650(3.5735) | Error 0.0322(0.0302) Steps 1066(1071.49) | Grad Norm 1.9915(2.1488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 123.2374, Epoch Time 1553.7595(1561.8304), Bit/dim 3.5449(best: 3.5426), Xent 1.3120, Loss 4.2009, Error 0.2412(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19260 | Time 25.7559(25.7065) | Bit/dim 3.5113(3.5308) | Xent 0.0759(0.0839) | Loss 3.5492(3.5728) | Error 0.0300(0.0302) Steps 1072(1069.82) | Grad Norm 2.0215(2.1218) | Total Time 14.00(14.00)\n",
      "Iter 19270 | Time 25.1114(25.6726) | Bit/dim 3.5335(3.5351) | Xent 0.0744(0.0849) | Loss 3.5707(3.5775) | Error 0.0289(0.0304) Steps 1060(1071.26) | Grad Norm 2.7543(2.1271) | Total Time 14.00(14.00)\n",
      "Iter 19280 | Time 25.3181(25.6481) | Bit/dim 3.5132(3.5335) | Xent 0.0744(0.0837) | Loss 3.5504(3.5753) | Error 0.0211(0.0295) Steps 1084(1071.48) | Grad Norm 2.0693(2.0964) | Total Time 14.00(14.00)\n",
      "Iter 19290 | Time 26.4001(25.6710) | Bit/dim 3.5352(3.5319) | Xent 0.0736(0.0812) | Loss 3.5720(3.5725) | Error 0.0256(0.0284) Steps 1042(1069.87) | Grad Norm 2.6739(2.0486) | Total Time 14.00(14.00)\n",
      "Iter 19300 | Time 26.0176(25.6556) | Bit/dim 3.5560(3.5319) | Xent 0.0777(0.0832) | Loss 3.5948(3.5735) | Error 0.0289(0.0287) Steps 1078(1070.07) | Grad Norm 1.8003(2.1430) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 123.9197, Epoch Time 1550.5739(1561.4927), Bit/dim 3.5425(best: 3.5426), Xent 1.3108, Loss 4.1979, Error 0.2442(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19310 | Time 25.8402(25.6622) | Bit/dim 3.5067(3.5299) | Xent 0.0758(0.0811) | Loss 3.5445(3.5705) | Error 0.0300(0.0281) Steps 1072(1072.03) | Grad Norm 1.9929(2.0070) | Total Time 14.00(14.00)\n",
      "Iter 19320 | Time 26.3603(25.6611) | Bit/dim 3.5626(3.5306) | Xent 0.0932(0.0808) | Loss 3.6092(3.5710) | Error 0.0333(0.0283) Steps 1078(1071.47) | Grad Norm 2.1931(1.9714) | Total Time 14.00(14.00)\n",
      "Iter 19330 | Time 25.2844(25.6164) | Bit/dim 3.5375(3.5310) | Xent 0.0787(0.0814) | Loss 3.5769(3.5717) | Error 0.0300(0.0285) Steps 1090(1071.15) | Grad Norm 2.6682(2.0373) | Total Time 14.00(14.00)\n",
      "Iter 19340 | Time 25.6469(25.5689) | Bit/dim 3.5260(3.5294) | Xent 0.0626(0.0821) | Loss 3.5573(3.5705) | Error 0.0211(0.0287) Steps 1066(1070.77) | Grad Norm 1.5363(2.0617) | Total Time 14.00(14.00)\n",
      "Iter 19350 | Time 25.3990(25.5726) | Bit/dim 3.5409(3.5306) | Xent 0.0841(0.0822) | Loss 3.5829(3.5717) | Error 0.0256(0.0282) Steps 1066(1069.53) | Grad Norm 1.6284(2.0492) | Total Time 14.00(14.00)\n",
      "Iter 19360 | Time 25.2240(25.5261) | Bit/dim 3.5603(3.5324) | Xent 0.0689(0.0816) | Loss 3.5948(3.5732) | Error 0.0233(0.0280) Steps 1072(1070.24) | Grad Norm 1.4793(2.0315) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 124.3735, Epoch Time 1545.0193(1560.9985), Bit/dim 3.5429(best: 3.5425), Xent 1.2666, Loss 4.1762, Error 0.2422(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19370 | Time 26.2835(25.5890) | Bit/dim 3.5144(3.5313) | Xent 0.0770(0.0802) | Loss 3.5529(3.5714) | Error 0.0322(0.0274) Steps 1078(1070.93) | Grad Norm 1.5228(1.9637) | Total Time 14.00(14.00)\n",
      "Iter 19380 | Time 26.0313(25.6416) | Bit/dim 3.4961(3.5301) | Xent 0.1187(0.0808) | Loss 3.5555(3.5705) | Error 0.0378(0.0282) Steps 1048(1070.32) | Grad Norm 4.5358(2.1144) | Total Time 14.00(14.00)\n",
      "Iter 19390 | Time 25.8496(25.6731) | Bit/dim 3.5344(3.5303) | Xent 0.0877(0.0822) | Loss 3.5783(3.5714) | Error 0.0289(0.0289) Steps 1072(1071.79) | Grad Norm 1.6664(2.1114) | Total Time 14.00(14.00)\n",
      "Iter 19400 | Time 25.5583(25.6645) | Bit/dim 3.5243(3.5325) | Xent 0.0741(0.0825) | Loss 3.5614(3.5737) | Error 0.0289(0.0293) Steps 1072(1070.65) | Grad Norm 1.7028(2.1639) | Total Time 14.00(14.00)\n",
      "Iter 19410 | Time 25.5996(25.6925) | Bit/dim 3.5395(3.5341) | Xent 0.0812(0.0838) | Loss 3.5801(3.5760) | Error 0.0322(0.0300) Steps 1078(1071.03) | Grad Norm 1.9800(2.2531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 123.0009, Epoch Time 1553.8978(1560.7855), Bit/dim 3.5432(best: 3.5425), Xent 1.2800, Loss 4.1832, Error 0.2441(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19420 | Time 25.3157(25.6793) | Bit/dim 3.5105(3.5352) | Xent 0.0657(0.0848) | Loss 3.5434(3.5776) | Error 0.0289(0.0307) Steps 1072(1070.39) | Grad Norm 3.2190(2.3230) | Total Time 14.00(14.00)\n",
      "Iter 19430 | Time 25.0208(25.6932) | Bit/dim 3.5195(3.5320) | Xent 0.0917(0.0840) | Loss 3.5653(3.5740) | Error 0.0378(0.0303) Steps 1072(1071.11) | Grad Norm 2.9962(2.3509) | Total Time 14.00(14.00)\n",
      "Iter 19440 | Time 25.1079(25.6972) | Bit/dim 3.5162(3.5344) | Xent 0.0879(0.0849) | Loss 3.5602(3.5768) | Error 0.0300(0.0304) Steps 1060(1070.79) | Grad Norm 2.5861(2.3437) | Total Time 14.00(14.00)\n",
      "Iter 19450 | Time 25.6657(25.7343) | Bit/dim 3.5243(3.5325) | Xent 0.0891(0.0856) | Loss 3.5688(3.5753) | Error 0.0333(0.0304) Steps 1060(1069.92) | Grad Norm 1.9637(2.2134) | Total Time 14.00(14.00)\n",
      "Iter 19460 | Time 25.5187(25.6799) | Bit/dim 3.5105(3.5323) | Xent 0.0866(0.0821) | Loss 3.5538(3.5734) | Error 0.0278(0.0289) Steps 1054(1069.92) | Grad Norm 1.5552(2.0849) | Total Time 14.00(14.00)\n",
      "Iter 19470 | Time 25.8783(25.6782) | Bit/dim 3.5314(3.5299) | Xent 0.0650(0.0811) | Loss 3.5639(3.5705) | Error 0.0233(0.0289) Steps 1078(1070.49) | Grad Norm 1.3552(1.9999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 123.7451, Epoch Time 1553.8903(1560.5786), Bit/dim 3.5432(best: 3.5425), Xent 1.3127, Loss 4.1995, Error 0.2466(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19480 | Time 25.3856(25.5764) | Bit/dim 3.5571(3.5336) | Xent 0.0832(0.0808) | Loss 3.5987(3.5740) | Error 0.0367(0.0291) Steps 1066(1071.55) | Grad Norm 2.0421(2.0058) | Total Time 14.00(14.00)\n",
      "Iter 19490 | Time 24.7098(25.5771) | Bit/dim 3.5337(3.5334) | Xent 0.0920(0.0815) | Loss 3.5797(3.5741) | Error 0.0333(0.0293) Steps 1072(1070.33) | Grad Norm 2.4276(2.0287) | Total Time 14.00(14.00)\n",
      "Iter 19500 | Time 25.4358(25.5820) | Bit/dim 3.5664(3.5322) | Xent 0.0702(0.0799) | Loss 3.6015(3.5721) | Error 0.0267(0.0286) Steps 1066(1070.45) | Grad Norm 1.2807(1.9605) | Total Time 14.00(14.00)\n",
      "Iter 19510 | Time 25.6458(25.5286) | Bit/dim 3.5056(3.5306) | Xent 0.0871(0.0797) | Loss 3.5491(3.5704) | Error 0.0311(0.0284) Steps 1078(1070.69) | Grad Norm 2.0391(1.9215) | Total Time 14.00(14.00)\n",
      "Iter 19520 | Time 26.6219(25.6181) | Bit/dim 3.5117(3.5302) | Xent 0.0858(0.0811) | Loss 3.5546(3.5707) | Error 0.0256(0.0288) Steps 1084(1071.41) | Grad Norm 1.9208(1.9722) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 121.8614, Epoch Time 1543.7568(1560.0740), Bit/dim 3.5411(best: 3.5425), Xent 1.2806, Loss 4.1814, Error 0.2418(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19530 | Time 25.8763(25.6029) | Bit/dim 3.5353(3.5287) | Xent 0.0647(0.0813) | Loss 3.5677(3.5693) | Error 0.0200(0.0288) Steps 1078(1071.32) | Grad Norm 1.9695(2.0134) | Total Time 14.00(14.00)\n",
      "Iter 19540 | Time 26.0133(25.6581) | Bit/dim 3.5591(3.5298) | Xent 0.0809(0.0821) | Loss 3.5996(3.5708) | Error 0.0267(0.0292) Steps 1096(1072.88) | Grad Norm 1.8154(2.1245) | Total Time 14.00(14.00)\n",
      "Iter 19550 | Time 25.2737(25.6567) | Bit/dim 3.5444(3.5309) | Xent 0.0894(0.0811) | Loss 3.5891(3.5715) | Error 0.0333(0.0287) Steps 1066(1071.97) | Grad Norm 2.6163(2.1300) | Total Time 14.00(14.00)\n",
      "Iter 19560 | Time 25.5088(25.6528) | Bit/dim 3.5309(3.5287) | Xent 0.0852(0.0807) | Loss 3.5735(3.5690) | Error 0.0311(0.0287) Steps 1066(1072.71) | Grad Norm 2.6760(2.1802) | Total Time 14.00(14.00)\n",
      "Iter 19570 | Time 26.1583(25.6943) | Bit/dim 3.5251(3.5310) | Xent 0.0754(0.0818) | Loss 3.5628(3.5719) | Error 0.0267(0.0293) Steps 1054(1072.64) | Grad Norm 1.4946(2.1294) | Total Time 14.00(14.00)\n",
      "Iter 19580 | Time 25.9084(25.7081) | Bit/dim 3.5185(3.5305) | Xent 0.0883(0.0818) | Loss 3.5627(3.5714) | Error 0.0378(0.0291) Steps 1072(1072.12) | Grad Norm 2.5594(2.1124) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 122.2660, Epoch Time 1552.4556(1559.8454), Bit/dim 3.5428(best: 3.5411), Xent 1.3014, Loss 4.1935, Error 0.2424(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19590 | Time 25.4797(25.6090) | Bit/dim 3.5350(3.5262) | Xent 0.0778(0.0820) | Loss 3.5739(3.5672) | Error 0.0333(0.0291) Steps 1060(1071.21) | Grad Norm 2.1105(2.0550) | Total Time 14.00(14.00)\n",
      "Iter 19600 | Time 25.1221(25.5663) | Bit/dim 3.5393(3.5311) | Xent 0.0780(0.0814) | Loss 3.5783(3.5718) | Error 0.0267(0.0291) Steps 1072(1070.94) | Grad Norm 2.3704(2.0331) | Total Time 14.00(14.00)\n",
      "Iter 19610 | Time 25.9160(25.5528) | Bit/dim 3.5357(3.5309) | Xent 0.0748(0.0822) | Loss 3.5731(3.5719) | Error 0.0267(0.0288) Steps 1060(1070.76) | Grad Norm 1.3952(2.0333) | Total Time 14.00(14.00)\n",
      "Iter 19620 | Time 26.3223(25.6047) | Bit/dim 3.5297(3.5298) | Xent 0.0647(0.0809) | Loss 3.5621(3.5703) | Error 0.0211(0.0287) Steps 1060(1070.73) | Grad Norm 1.6141(1.9713) | Total Time 14.00(14.00)\n",
      "Iter 19630 | Time 25.3861(25.5767) | Bit/dim 3.5362(3.5307) | Xent 0.0802(0.0809) | Loss 3.5763(3.5712) | Error 0.0222(0.0285) Steps 1060(1070.48) | Grad Norm 2.0385(1.9679) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 123.1228, Epoch Time 1543.3891(1559.3517), Bit/dim 3.5425(best: 3.5411), Xent 1.2756, Loss 4.1803, Error 0.2408(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19640 | Time 25.9923(25.6122) | Bit/dim 3.5009(3.5298) | Xent 0.0805(0.0820) | Loss 3.5411(3.5708) | Error 0.0267(0.0291) Steps 1078(1071.18) | Grad Norm 2.0922(2.1266) | Total Time 14.00(14.00)\n",
      "Iter 19650 | Time 25.9044(25.7296) | Bit/dim 3.5180(3.5306) | Xent 0.0943(0.0819) | Loss 3.5651(3.5715) | Error 0.0344(0.0294) Steps 1084(1072.35) | Grad Norm 1.9723(2.1338) | Total Time 14.00(14.00)\n",
      "Iter 19660 | Time 26.3826(25.7631) | Bit/dim 3.5484(3.5327) | Xent 0.1117(0.0833) | Loss 3.6042(3.5743) | Error 0.0400(0.0298) Steps 1078(1071.48) | Grad Norm 2.4876(2.1256) | Total Time 14.00(14.00)\n",
      "Iter 19670 | Time 26.3834(25.7862) | Bit/dim 3.5264(3.5323) | Xent 0.1024(0.0830) | Loss 3.5776(3.5738) | Error 0.0378(0.0300) Steps 1048(1070.81) | Grad Norm 2.2533(2.1776) | Total Time 14.00(14.00)\n",
      "Iter 19680 | Time 25.4574(25.7707) | Bit/dim 3.5127(3.5304) | Xent 0.0719(0.0830) | Loss 3.5487(3.5718) | Error 0.0244(0.0297) Steps 1066(1069.48) | Grad Norm 2.4479(2.1897) | Total Time 14.00(14.00)\n",
      "Iter 19690 | Time 25.5283(25.7463) | Bit/dim 3.5172(3.5312) | Xent 0.0785(0.0835) | Loss 3.5564(3.5729) | Error 0.0278(0.0298) Steps 1084(1069.62) | Grad Norm 3.1183(2.1933) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 124.1644, Epoch Time 1560.7122(1559.3925), Bit/dim 3.5410(best: 3.5411), Xent 1.2488, Loss 4.1654, Error 0.2399(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19700 | Time 25.2340(25.7316) | Bit/dim 3.5338(3.5313) | Xent 0.0752(0.0818) | Loss 3.5714(3.5721) | Error 0.0244(0.0290) Steps 1078(1070.74) | Grad Norm 1.6362(2.1194) | Total Time 14.00(14.00)\n",
      "Iter 19710 | Time 25.2253(25.7589) | Bit/dim 3.5885(3.5333) | Xent 0.0884(0.0804) | Loss 3.6327(3.5735) | Error 0.0322(0.0286) Steps 1066(1071.42) | Grad Norm 2.5355(2.1346) | Total Time 14.00(14.00)\n",
      "Iter 19720 | Time 26.0260(25.8087) | Bit/dim 3.5570(3.5323) | Xent 0.1026(0.0818) | Loss 3.6084(3.5732) | Error 0.0422(0.0288) Steps 1066(1071.80) | Grad Norm 2.2223(2.2001) | Total Time 14.00(14.00)\n",
      "Iter 19730 | Time 25.9921(25.8038) | Bit/dim 3.5400(3.5313) | Xent 0.0882(0.0837) | Loss 3.5841(3.5732) | Error 0.0333(0.0296) Steps 1066(1071.74) | Grad Norm 4.7278(2.4401) | Total Time 14.00(14.00)\n",
      "Iter 19740 | Time 26.5367(25.8347) | Bit/dim 3.5264(3.5300) | Xent 0.0881(0.0840) | Loss 3.5704(3.5720) | Error 0.0322(0.0296) Steps 1090(1071.56) | Grad Norm 2.1687(2.4276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 124.0688, Epoch Time 1562.4857(1559.4853), Bit/dim 3.5406(best: 3.5410), Xent 1.2660, Loss 4.1736, Error 0.2421(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19750 | Time 25.3961(25.8214) | Bit/dim 3.5337(3.5283) | Xent 0.0701(0.0838) | Loss 3.5687(3.5702) | Error 0.0278(0.0296) Steps 1060(1072.08) | Grad Norm 1.6118(2.2629) | Total Time 14.00(14.00)\n",
      "Iter 19760 | Time 25.5677(25.8098) | Bit/dim 3.4769(3.5263) | Xent 0.0732(0.0798) | Loss 3.5135(3.5662) | Error 0.0233(0.0286) Steps 1084(1073.40) | Grad Norm 2.0035(2.1256) | Total Time 14.00(14.00)\n",
      "Iter 19770 | Time 25.9585(25.7924) | Bit/dim 3.5093(3.5256) | Xent 0.1012(0.0787) | Loss 3.5599(3.5649) | Error 0.0378(0.0284) Steps 1078(1073.37) | Grad Norm 1.9975(2.0489) | Total Time 14.00(14.00)\n",
      "Iter 19780 | Time 25.6904(25.8214) | Bit/dim 3.5186(3.5288) | Xent 0.0917(0.0782) | Loss 3.5644(3.5679) | Error 0.0289(0.0280) Steps 1078(1074.04) | Grad Norm 2.6276(2.0480) | Total Time 14.00(14.00)\n",
      "Iter 19790 | Time 25.0357(25.7368) | Bit/dim 3.5442(3.5337) | Xent 0.0797(0.0824) | Loss 3.5841(3.5748) | Error 0.0322(0.0299) Steps 1054(1072.37) | Grad Norm 2.2375(2.2407) | Total Time 14.00(14.00)\n",
      "Iter 19800 | Time 24.9762(25.7175) | Bit/dim 3.5492(3.5334) | Xent 0.1029(0.0850) | Loss 3.6007(3.5759) | Error 0.0367(0.0309) Steps 1060(1070.78) | Grad Norm 2.1733(2.2697) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 123.8070, Epoch Time 1554.1142(1559.3242), Bit/dim 3.5421(best: 3.5406), Xent 1.2811, Loss 4.1826, Error 0.2414(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19810 | Time 25.7737(25.7740) | Bit/dim 3.5252(3.5332) | Xent 0.0826(0.0836) | Loss 3.5665(3.5749) | Error 0.0333(0.0303) Steps 1078(1071.52) | Grad Norm 1.7290(2.2284) | Total Time 14.00(14.00)\n",
      "Iter 19820 | Time 25.7624(25.7608) | Bit/dim 3.5133(3.5335) | Xent 0.0768(0.0828) | Loss 3.5517(3.5749) | Error 0.0267(0.0298) Steps 1066(1070.97) | Grad Norm 2.2401(2.1770) | Total Time 14.00(14.00)\n",
      "Iter 19830 | Time 25.1617(25.7181) | Bit/dim 3.5549(3.5294) | Xent 0.0704(0.0815) | Loss 3.5900(3.5701) | Error 0.0200(0.0292) Steps 1066(1069.84) | Grad Norm 2.2360(2.1234) | Total Time 14.00(14.00)\n",
      "Iter 19840 | Time 25.9114(25.7054) | Bit/dim 3.5386(3.5297) | Xent 0.0789(0.0825) | Loss 3.5780(3.5710) | Error 0.0311(0.0299) Steps 1054(1069.34) | Grad Norm 1.5952(2.1757) | Total Time 14.00(14.00)\n",
      "Iter 19850 | Time 25.7314(25.6522) | Bit/dim 3.5157(3.5305) | Xent 0.0867(0.0818) | Loss 3.5591(3.5714) | Error 0.0333(0.0298) Steps 1084(1070.82) | Grad Norm 2.1965(2.1851) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 122.4829, Epoch Time 1550.3929(1559.0563), Bit/dim 3.5432(best: 3.5406), Xent 1.3357, Loss 4.2111, Error 0.2407(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19860 | Time 25.2765(25.5731) | Bit/dim 3.5106(3.5293) | Xent 0.0759(0.0817) | Loss 3.5486(3.5701) | Error 0.0178(0.0293) Steps 1054(1071.13) | Grad Norm 1.6589(2.1863) | Total Time 14.00(14.00)\n",
      "Iter 19870 | Time 25.2963(25.6142) | Bit/dim 3.5413(3.5289) | Xent 0.0771(0.0803) | Loss 3.5799(3.5690) | Error 0.0222(0.0283) Steps 1078(1071.26) | Grad Norm 1.5872(2.0941) | Total Time 14.00(14.00)\n",
      "Iter 19880 | Time 25.5012(25.5859) | Bit/dim 3.4949(3.5287) | Xent 0.0735(0.0816) | Loss 3.5316(3.5695) | Error 0.0222(0.0288) Steps 1078(1072.21) | Grad Norm 2.4872(2.1935) | Total Time 14.00(14.00)\n",
      "Iter 19890 | Time 25.0542(25.4742) | Bit/dim 3.5284(3.5303) | Xent 0.0900(0.0844) | Loss 3.5734(3.5725) | Error 0.0378(0.0300) Steps 1072(1071.15) | Grad Norm 3.9179(2.3322) | Total Time 14.00(14.00)\n",
      "Iter 19900 | Time 25.8399(25.4803) | Bit/dim 3.5143(3.5306) | Xent 0.0828(0.0845) | Loss 3.5557(3.5729) | Error 0.0311(0.0303) Steps 1072(1070.99) | Grad Norm 2.0900(2.3735) | Total Time 14.00(14.00)\n",
      "Iter 19910 | Time 25.9653(25.5267) | Bit/dim 3.5248(3.5336) | Xent 0.0700(0.0838) | Loss 3.5598(3.5755) | Error 0.0178(0.0297) Steps 1078(1070.24) | Grad Norm 1.5308(2.3191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 123.6476, Epoch Time 1541.4804(1558.5290), Bit/dim 3.5436(best: 3.5406), Xent 1.3055, Loss 4.1963, Error 0.2392(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19920 | Time 24.9763(25.5558) | Bit/dim 3.5316(3.5338) | Xent 0.0900(0.0830) | Loss 3.5766(3.5753) | Error 0.0300(0.0295) Steps 1066(1070.72) | Grad Norm 2.7585(2.2938) | Total Time 14.00(14.00)\n",
      "Iter 19930 | Time 25.3283(25.6091) | Bit/dim 3.5312(3.5312) | Xent 0.0750(0.0815) | Loss 3.5687(3.5719) | Error 0.0311(0.0292) Steps 1066(1070.49) | Grad Norm 1.9678(2.2141) | Total Time 14.00(14.00)\n",
      "Iter 19940 | Time 25.8557(25.6593) | Bit/dim 3.5225(3.5309) | Xent 0.0611(0.0807) | Loss 3.5530(3.5712) | Error 0.0244(0.0288) Steps 1066(1070.28) | Grad Norm 1.9706(2.2338) | Total Time 14.00(14.00)\n",
      "Iter 19950 | Time 26.1588(25.7088) | Bit/dim 3.5293(3.5311) | Xent 0.0676(0.0812) | Loss 3.5631(3.5717) | Error 0.0244(0.0289) Steps 1072(1070.75) | Grad Norm 2.3416(2.2524) | Total Time 14.00(14.00)\n",
      "Iter 19960 | Time 25.5146(25.7011) | Bit/dim 3.5390(3.5314) | Xent 0.1053(0.0818) | Loss 3.5917(3.5723) | Error 0.0367(0.0292) Steps 1066(1071.21) | Grad Norm 2.5349(2.2958) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 123.4239, Epoch Time 1554.9706(1558.4222), Bit/dim 3.5452(best: 3.5406), Xent 1.3562, Loss 4.2233, Error 0.2411(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 19970 | Time 25.7018(25.6714) | Bit/dim 3.5463(3.5299) | Xent 0.0634(0.0803) | Loss 3.5780(3.5700) | Error 0.0189(0.0289) Steps 1090(1070.73) | Grad Norm 1.5346(2.4216) | Total Time 14.00(14.00)\n",
      "Iter 19980 | Time 26.0150(25.7329) | Bit/dim 3.5349(3.5327) | Xent 0.0653(0.0784) | Loss 3.5675(3.5719) | Error 0.0178(0.0275) Steps 1060(1072.02) | Grad Norm 1.8341(2.2863) | Total Time 14.00(14.00)\n",
      "Iter 19990 | Time 25.2652(25.6743) | Bit/dim 3.5085(3.5332) | Xent 0.0664(0.0782) | Loss 3.5417(3.5723) | Error 0.0178(0.0273) Steps 1066(1070.29) | Grad Norm 1.5294(2.1809) | Total Time 14.00(14.00)\n",
      "Iter 20000 | Time 25.8085(25.6450) | Bit/dim 3.5085(3.5343) | Xent 0.0614(0.0769) | Loss 3.5393(3.5727) | Error 0.0233(0.0272) Steps 1078(1070.86) | Grad Norm 1.4855(2.1122) | Total Time 14.00(14.00)\n",
      "Iter 20010 | Time 25.6158(25.6717) | Bit/dim 3.5383(3.5300) | Xent 0.0856(0.0789) | Loss 3.5811(3.5694) | Error 0.0300(0.0281) Steps 1078(1071.95) | Grad Norm 2.3563(2.1334) | Total Time 14.00(14.00)\n",
      "Iter 20020 | Time 26.1493(25.6795) | Bit/dim 3.5298(3.5294) | Xent 0.0757(0.0800) | Loss 3.5677(3.5694) | Error 0.0300(0.0287) Steps 1060(1069.83) | Grad Norm 3.0167(2.2137) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 122.5844, Epoch Time 1550.7583(1558.1923), Bit/dim 3.5421(best: 3.5406), Xent 1.3017, Loss 4.1929, Error 0.2394(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20030 | Time 25.7184(25.6979) | Bit/dim 3.5058(3.5271) | Xent 0.0658(0.0798) | Loss 3.5387(3.5670) | Error 0.0211(0.0282) Steps 1090(1070.85) | Grad Norm 1.6483(2.1450) | Total Time 14.00(14.00)\n",
      "Iter 20040 | Time 25.6219(25.6872) | Bit/dim 3.5034(3.5270) | Xent 0.0980(0.0789) | Loss 3.5524(3.5664) | Error 0.0356(0.0279) Steps 1072(1070.64) | Grad Norm 2.3325(2.0384) | Total Time 14.00(14.00)\n",
      "Iter 20050 | Time 25.2210(25.6875) | Bit/dim 3.5300(3.5276) | Xent 0.0835(0.0802) | Loss 3.5717(3.5677) | Error 0.0322(0.0285) Steps 1084(1071.76) | Grad Norm 3.0007(2.0400) | Total Time 14.00(14.00)\n",
      "Iter 20060 | Time 25.5611(25.6663) | Bit/dim 3.5625(3.5326) | Xent 0.0711(0.0786) | Loss 3.5980(3.5719) | Error 0.0244(0.0278) Steps 1078(1071.99) | Grad Norm 1.9659(2.0292) | Total Time 14.00(14.00)\n",
      "Iter 20070 | Time 25.6612(25.6902) | Bit/dim 3.5296(3.5308) | Xent 0.0818(0.0795) | Loss 3.5705(3.5706) | Error 0.0322(0.0282) Steps 1072(1071.64) | Grad Norm 1.7690(1.9899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 124.2254, Epoch Time 1553.5965(1558.0544), Bit/dim 3.5416(best: 3.5406), Xent 1.3129, Loss 4.1980, Error 0.2424(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20080 | Time 25.7431(25.6769) | Bit/dim 3.5640(3.5318) | Xent 0.0943(0.0804) | Loss 3.6112(3.5720) | Error 0.0367(0.0287) Steps 1066(1072.21) | Grad Norm 4.4807(2.0758) | Total Time 14.00(14.00)\n",
      "Iter 20090 | Time 25.8101(25.7215) | Bit/dim 3.5306(3.5311) | Xent 0.0797(0.0785) | Loss 3.5704(3.5703) | Error 0.0300(0.0281) Steps 1072(1072.16) | Grad Norm 2.6611(2.1709) | Total Time 14.00(14.00)\n",
      "Iter 20100 | Time 26.0805(25.7208) | Bit/dim 3.5594(3.5320) | Xent 0.0817(0.0777) | Loss 3.6003(3.5708) | Error 0.0256(0.0276) Steps 1072(1073.28) | Grad Norm 1.5464(2.1174) | Total Time 14.00(14.00)\n",
      "Iter 20110 | Time 25.2126(25.7915) | Bit/dim 3.5007(3.5300) | Xent 0.0622(0.0770) | Loss 3.5318(3.5685) | Error 0.0233(0.0276) Steps 1078(1073.41) | Grad Norm 1.2848(2.0019) | Total Time 14.00(14.00)\n",
      "Iter 20120 | Time 25.5563(25.7752) | Bit/dim 3.5328(3.5287) | Xent 0.0680(0.0794) | Loss 3.5668(3.5685) | Error 0.0278(0.0287) Steps 1084(1073.01) | Grad Norm 1.7474(2.0481) | Total Time 14.00(14.00)\n",
      "Iter 20130 | Time 25.6697(25.7796) | Bit/dim 3.5444(3.5301) | Xent 0.0791(0.0791) | Loss 3.5839(3.5696) | Error 0.0300(0.0285) Steps 1084(1073.45) | Grad Norm 2.6133(2.1681) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 124.0313, Epoch Time 1559.5782(1558.1002), Bit/dim 3.5416(best: 3.5406), Xent 1.2819, Loss 4.1826, Error 0.2406(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20140 | Time 25.7336(25.7216) | Bit/dim 3.4739(3.5285) | Xent 0.0582(0.0793) | Loss 3.5030(3.5682) | Error 0.0211(0.0288) Steps 1078(1073.31) | Grad Norm 1.8887(2.1335) | Total Time 14.00(14.00)\n",
      "Iter 20150 | Time 25.5603(25.6845) | Bit/dim 3.5292(3.5285) | Xent 0.0943(0.0792) | Loss 3.5763(3.5681) | Error 0.0367(0.0286) Steps 1060(1071.43) | Grad Norm 2.1710(2.2304) | Total Time 14.00(14.00)\n",
      "Iter 20160 | Time 25.9408(25.7381) | Bit/dim 3.5516(3.5306) | Xent 0.0930(0.0802) | Loss 3.5981(3.5708) | Error 0.0356(0.0291) Steps 1078(1073.37) | Grad Norm 2.6361(2.3938) | Total Time 14.00(14.00)\n",
      "Iter 20170 | Time 25.3682(25.7789) | Bit/dim 3.5710(3.5317) | Xent 0.0860(0.0783) | Loss 3.6140(3.5709) | Error 0.0289(0.0279) Steps 1072(1073.87) | Grad Norm 2.5744(2.3411) | Total Time 14.00(14.00)\n",
      "Iter 20180 | Time 24.8631(25.7078) | Bit/dim 3.5050(3.5294) | Xent 0.1003(0.0809) | Loss 3.5552(3.5698) | Error 0.0344(0.0286) Steps 1060(1072.86) | Grad Norm 4.0225(2.4324) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 122.2878, Epoch Time 1551.6224(1557.9058), Bit/dim 3.5420(best: 3.5406), Xent 1.3226, Loss 4.2033, Error 0.2407(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20190 | Time 26.1502(25.7026) | Bit/dim 3.5366(3.5329) | Xent 0.0775(0.0814) | Loss 3.5754(3.5736) | Error 0.0244(0.0290) Steps 1066(1071.72) | Grad Norm 2.3242(2.5462) | Total Time 14.00(14.00)\n",
      "Iter 20200 | Time 25.0442(25.6745) | Bit/dim 3.5314(3.5359) | Xent 0.0923(0.0814) | Loss 3.5775(3.5766) | Error 0.0289(0.0289) Steps 1072(1073.51) | Grad Norm 2.5029(2.6074) | Total Time 14.00(14.00)\n",
      "Iter 20210 | Time 25.9460(25.7006) | Bit/dim 3.5378(3.5334) | Xent 0.0773(0.0834) | Loss 3.5765(3.5751) | Error 0.0300(0.0295) Steps 1078(1073.43) | Grad Norm 2.3060(2.5989) | Total Time 14.00(14.00)\n",
      "Iter 20220 | Time 26.3980(25.6908) | Bit/dim 3.4745(3.5309) | Xent 0.1009(0.0826) | Loss 3.5249(3.5723) | Error 0.0356(0.0290) Steps 1078(1072.55) | Grad Norm 2.3788(2.4589) | Total Time 14.00(14.00)\n",
      "Iter 20230 | Time 25.4481(25.7110) | Bit/dim 3.4953(3.5298) | Xent 0.0751(0.0801) | Loss 3.5328(3.5698) | Error 0.0267(0.0281) Steps 1066(1070.61) | Grad Norm 1.4772(2.2952) | Total Time 14.00(14.00)\n",
      "Iter 20240 | Time 25.5789(25.6730) | Bit/dim 3.5185(3.5284) | Xent 0.0680(0.0792) | Loss 3.5525(3.5680) | Error 0.0200(0.0277) Steps 1072(1070.80) | Grad Norm 2.1766(2.2447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 123.3317, Epoch Time 1552.4441(1557.7420), Bit/dim 3.5397(best: 3.5406), Xent 1.2872, Loss 4.1833, Error 0.2430(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20250 | Time 25.7440(25.6819) | Bit/dim 3.5216(3.5279) | Xent 0.0655(0.0795) | Loss 3.5544(3.5676) | Error 0.0211(0.0284) Steps 1084(1071.29) | Grad Norm 2.1680(2.2546) | Total Time 14.00(14.00)\n",
      "Iter 20260 | Time 25.4872(25.6777) | Bit/dim 3.5406(3.5280) | Xent 0.0825(0.0813) | Loss 3.5819(3.5687) | Error 0.0322(0.0295) Steps 1072(1070.87) | Grad Norm 1.6864(2.2906) | Total Time 14.00(14.00)\n",
      "Iter 20270 | Time 25.8155(25.6715) | Bit/dim 3.5071(3.5303) | Xent 0.0584(0.0799) | Loss 3.5363(3.5703) | Error 0.0211(0.0290) Steps 1066(1070.75) | Grad Norm 1.2500(2.1801) | Total Time 14.00(14.00)\n",
      "Iter 20280 | Time 25.8599(25.6866) | Bit/dim 3.4881(3.5286) | Xent 0.0884(0.0776) | Loss 3.5323(3.5674) | Error 0.0333(0.0283) Steps 1072(1072.03) | Grad Norm 1.7803(2.0894) | Total Time 14.00(14.00)\n",
      "Iter 20290 | Time 24.8022(25.5898) | Bit/dim 3.5135(3.5286) | Xent 0.0721(0.0781) | Loss 3.5496(3.5676) | Error 0.0289(0.0284) Steps 1060(1071.54) | Grad Norm 1.3928(2.0601) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 121.9532, Epoch Time 1546.1703(1557.3948), Bit/dim 3.5420(best: 3.5397), Xent 1.3609, Loss 4.2224, Error 0.2428(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20300 | Time 26.2396(25.5688) | Bit/dim 3.5310(3.5304) | Xent 0.0732(0.0782) | Loss 3.5676(3.5695) | Error 0.0267(0.0285) Steps 1072(1073.20) | Grad Norm 2.2522(2.0703) | Total Time 14.00(14.00)\n",
      "Iter 20310 | Time 25.5488(25.6570) | Bit/dim 3.5248(3.5297) | Xent 0.0694(0.0785) | Loss 3.5595(3.5689) | Error 0.0267(0.0288) Steps 1066(1072.33) | Grad Norm 2.4400(2.0606) | Total Time 14.00(14.00)\n",
      "Iter 20320 | Time 25.7123(25.7028) | Bit/dim 3.5574(3.5279) | Xent 0.0898(0.0788) | Loss 3.6023(3.5672) | Error 0.0311(0.0281) Steps 1078(1072.71) | Grad Norm 2.7603(2.0702) | Total Time 14.00(14.00)\n",
      "Iter 20330 | Time 25.7068(25.7138) | Bit/dim 3.5084(3.5286) | Xent 0.0952(0.0798) | Loss 3.5560(3.5685) | Error 0.0333(0.0282) Steps 1078(1073.78) | Grad Norm 2.6540(2.0981) | Total Time 14.00(14.00)\n",
      "Iter 20340 | Time 26.3059(25.7670) | Bit/dim 3.5433(3.5284) | Xent 0.0624(0.0785) | Loss 3.5745(3.5676) | Error 0.0222(0.0282) Steps 1084(1073.90) | Grad Norm 1.6943(2.0633) | Total Time 14.00(14.00)\n",
      "Iter 20350 | Time 25.7701(25.6978) | Bit/dim 3.5407(3.5301) | Xent 0.0872(0.0806) | Loss 3.5843(3.5704) | Error 0.0367(0.0293) Steps 1048(1073.31) | Grad Norm 2.5065(2.1464) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 121.8329, Epoch Time 1554.8563(1557.3187), Bit/dim 3.5426(best: 3.5397), Xent 1.3429, Loss 4.2141, Error 0.2435(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20360 | Time 25.6273(25.6904) | Bit/dim 3.5110(3.5314) | Xent 0.0917(0.0793) | Loss 3.5569(3.5710) | Error 0.0300(0.0287) Steps 1084(1073.43) | Grad Norm 2.8764(2.2137) | Total Time 14.00(14.00)\n",
      "Iter 20370 | Time 25.3255(25.7137) | Bit/dim 3.5554(3.5321) | Xent 0.0719(0.0794) | Loss 3.5914(3.5718) | Error 0.0256(0.0287) Steps 1054(1073.71) | Grad Norm 1.8812(2.1805) | Total Time 14.00(14.00)\n",
      "Iter 20380 | Time 24.2101(25.6373) | Bit/dim 3.5114(3.5312) | Xent 0.0780(0.0797) | Loss 3.5504(3.5710) | Error 0.0278(0.0285) Steps 1084(1074.23) | Grad Norm 1.9390(2.1942) | Total Time 14.00(14.00)\n",
      "Iter 20390 | Time 25.6003(25.6041) | Bit/dim 3.5497(3.5303) | Xent 0.0720(0.0799) | Loss 3.5857(3.5702) | Error 0.0311(0.0290) Steps 1066(1074.32) | Grad Norm 1.7068(2.1591) | Total Time 14.00(14.00)\n",
      "Iter 20400 | Time 25.7428(25.5635) | Bit/dim 3.5022(3.5288) | Xent 0.0944(0.0795) | Loss 3.5494(3.5685) | Error 0.0344(0.0287) Steps 1072(1073.72) | Grad Norm 2.0675(2.1163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 124.4211, Epoch Time 1547.7615(1557.0319), Bit/dim 3.5434(best: 3.5397), Xent 1.3794, Loss 4.2332, Error 0.2457(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20410 | Time 26.3893(25.6122) | Bit/dim 3.5121(3.5284) | Xent 0.0885(0.0806) | Loss 3.5563(3.5687) | Error 0.0311(0.0289) Steps 1072(1073.42) | Grad Norm 1.8866(2.1910) | Total Time 14.00(14.00)\n",
      "Iter 20420 | Time 25.3671(25.5729) | Bit/dim 3.5288(3.5289) | Xent 0.0548(0.0779) | Loss 3.5562(3.5678) | Error 0.0189(0.0282) Steps 1060(1071.54) | Grad Norm 1.4741(2.1436) | Total Time 14.00(14.00)\n",
      "Iter 20430 | Time 25.9426(25.6031) | Bit/dim 3.5274(3.5297) | Xent 0.0756(0.0778) | Loss 3.5652(3.5686) | Error 0.0244(0.0281) Steps 1072(1072.57) | Grad Norm 1.6774(2.0622) | Total Time 14.00(14.00)\n",
      "Iter 20440 | Time 25.9540(25.6007) | Bit/dim 3.5178(3.5279) | Xent 0.0649(0.0770) | Loss 3.5503(3.5665) | Error 0.0267(0.0279) Steps 1090(1073.42) | Grad Norm 1.4791(1.9988) | Total Time 14.00(14.00)\n",
      "Iter 20450 | Time 25.0007(25.5625) | Bit/dim 3.5384(3.5299) | Xent 0.0670(0.0768) | Loss 3.5719(3.5683) | Error 0.0211(0.0276) Steps 1066(1072.76) | Grad Norm 1.7156(1.9898) | Total Time 14.00(14.00)\n",
      "Iter 20460 | Time 25.2978(25.5542) | Bit/dim 3.5398(3.5310) | Xent 0.0789(0.0780) | Loss 3.5792(3.5700) | Error 0.0289(0.0279) Steps 1060(1072.41) | Grad Norm 1.5858(2.0109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 122.3585, Epoch Time 1544.7922(1556.6648), Bit/dim 3.5404(best: 3.5397), Xent 1.3201, Loss 4.2004, Error 0.2423(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20470 | Time 25.9527(25.6230) | Bit/dim 3.5291(3.5293) | Xent 0.0632(0.0777) | Loss 3.5607(3.5681) | Error 0.0267(0.0275) Steps 1072(1071.91) | Grad Norm 1.4753(1.9920) | Total Time 14.00(14.00)\n",
      "Iter 20480 | Time 25.6688(25.6335) | Bit/dim 3.5037(3.5277) | Xent 0.0804(0.0778) | Loss 3.5439(3.5666) | Error 0.0322(0.0274) Steps 1108(1074.18) | Grad Norm 1.9140(1.9954) | Total Time 14.00(14.00)\n",
      "Iter 20490 | Time 25.4215(25.6010) | Bit/dim 3.5633(3.5314) | Xent 0.0931(0.0780) | Loss 3.6098(3.5704) | Error 0.0300(0.0278) Steps 1066(1073.94) | Grad Norm 2.1464(2.0129) | Total Time 14.00(14.00)\n",
      "Iter 20500 | Time 25.2104(25.6195) | Bit/dim 3.5539(3.5310) | Xent 0.0596(0.0759) | Loss 3.5837(3.5689) | Error 0.0200(0.0269) Steps 1066(1073.71) | Grad Norm 1.5460(2.0084) | Total Time 14.00(14.00)\n",
      "Iter 20510 | Time 26.0234(25.7105) | Bit/dim 3.5336(3.5299) | Xent 0.0802(0.0783) | Loss 3.5737(3.5691) | Error 0.0267(0.0279) Steps 1084(1074.24) | Grad Norm 1.9312(2.0159) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 121.6242, Epoch Time 1552.0218(1556.5255), Bit/dim 3.5415(best: 3.5397), Xent 1.3354, Loss 4.2092, Error 0.2422(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20520 | Time 25.9525(25.7116) | Bit/dim 3.5025(3.5294) | Xent 0.0751(0.0784) | Loss 3.5400(3.5686) | Error 0.0233(0.0281) Steps 1078(1073.34) | Grad Norm 1.7343(2.0029) | Total Time 14.00(14.00)\n",
      "Iter 20530 | Time 26.0973(25.6534) | Bit/dim 3.5322(3.5283) | Xent 0.0927(0.0771) | Loss 3.5785(3.5669) | Error 0.0356(0.0275) Steps 1072(1074.28) | Grad Norm 2.1161(2.0052) | Total Time 14.00(14.00)\n",
      "Iter 20540 | Time 25.6425(25.6172) | Bit/dim 3.5486(3.5298) | Xent 0.0569(0.0775) | Loss 3.5771(3.5686) | Error 0.0211(0.0278) Steps 1078(1074.62) | Grad Norm 1.7085(2.0224) | Total Time 14.00(14.00)\n",
      "Iter 20550 | Time 26.1200(25.6035) | Bit/dim 3.5225(3.5307) | Xent 0.0781(0.0766) | Loss 3.5615(3.5690) | Error 0.0267(0.0279) Steps 1102(1073.96) | Grad Norm 1.3368(1.9737) | Total Time 14.00(14.00)\n",
      "Iter 20560 | Time 25.5108(25.6432) | Bit/dim 3.5220(3.5272) | Xent 0.0817(0.0767) | Loss 3.5629(3.5655) | Error 0.0333(0.0272) Steps 1072(1073.51) | Grad Norm 2.1654(1.9610) | Total Time 14.00(14.00)\n",
      "Iter 20570 | Time 26.1521(25.6432) | Bit/dim 3.5354(3.5294) | Xent 0.0862(0.0779) | Loss 3.5785(3.5684) | Error 0.0300(0.0278) Steps 1078(1073.20) | Grad Norm 2.2312(2.0918) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 122.2523, Epoch Time 1547.1702(1556.2448), Bit/dim 3.5392(best: 3.5397), Xent 1.2910, Loss 4.1847, Error 0.2399(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20580 | Time 25.5981(25.6293) | Bit/dim 3.5572(3.5305) | Xent 0.0706(0.0799) | Loss 3.5925(3.5705) | Error 0.0222(0.0288) Steps 1072(1072.56) | Grad Norm 2.2076(2.1333) | Total Time 14.00(14.00)\n",
      "Iter 20590 | Time 26.2489(25.6655) | Bit/dim 3.5460(3.5311) | Xent 0.0769(0.0810) | Loss 3.5844(3.5716) | Error 0.0267(0.0294) Steps 1072(1072.55) | Grad Norm 3.4634(2.3267) | Total Time 14.00(14.00)\n",
      "Iter 20600 | Time 24.4241(25.6206) | Bit/dim 3.5464(3.5285) | Xent 0.0648(0.0815) | Loss 3.5788(3.5693) | Error 0.0222(0.0294) Steps 1072(1071.70) | Grad Norm 1.5507(2.3678) | Total Time 14.00(14.00)\n",
      "Iter 20610 | Time 25.9157(25.6572) | Bit/dim 3.5242(3.5279) | Xent 0.0955(0.0815) | Loss 3.5720(3.5687) | Error 0.0411(0.0299) Steps 1084(1074.44) | Grad Norm 3.7588(2.5047) | Total Time 14.00(14.00)\n",
      "Iter 20620 | Time 25.7722(25.7042) | Bit/dim 3.5330(3.5310) | Xent 0.0815(0.0806) | Loss 3.5738(3.5713) | Error 0.0289(0.0297) Steps 1090(1074.67) | Grad Norm 2.3344(2.5580) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 121.3489, Epoch Time 1550.0138(1556.0579), Bit/dim 3.5410(best: 3.5392), Xent 1.3222, Loss 4.2021, Error 0.2411(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20630 | Time 26.1311(25.7057) | Bit/dim 3.5325(3.5297) | Xent 0.0596(0.0805) | Loss 3.5623(3.5699) | Error 0.0211(0.0292) Steps 1072(1074.10) | Grad Norm 1.5124(2.5183) | Total Time 14.00(14.00)\n",
      "Iter 20640 | Time 26.1478(25.6806) | Bit/dim 3.4977(3.5277) | Xent 0.0698(0.0807) | Loss 3.5327(3.5681) | Error 0.0311(0.0293) Steps 1078(1074.24) | Grad Norm 1.8866(2.5040) | Total Time 14.00(14.00)\n",
      "Iter 20650 | Time 25.0478(25.7024) | Bit/dim 3.5219(3.5314) | Xent 0.0750(0.0827) | Loss 3.5594(3.5727) | Error 0.0256(0.0300) Steps 1090(1074.97) | Grad Norm 2.0149(2.5278) | Total Time 14.00(14.00)\n",
      "Iter 20660 | Time 25.5363(25.7055) | Bit/dim 3.5210(3.5294) | Xent 0.0752(0.0818) | Loss 3.5586(3.5703) | Error 0.0311(0.0295) Steps 1084(1075.30) | Grad Norm 2.5553(2.4597) | Total Time 14.00(14.00)\n",
      "Iter 20670 | Time 25.2046(25.6670) | Bit/dim 3.5466(3.5315) | Xent 0.0699(0.0815) | Loss 3.5815(3.5723) | Error 0.0244(0.0292) Steps 1060(1074.70) | Grad Norm 2.3219(2.4110) | Total Time 14.00(14.00)\n",
      "Iter 20680 | Time 25.1323(25.6200) | Bit/dim 3.4971(3.5303) | Xent 0.0867(0.0815) | Loss 3.5405(3.5710) | Error 0.0344(0.0295) Steps 1060(1075.71) | Grad Norm 2.8148(2.4743) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 121.7864, Epoch Time 1548.1362(1555.8202), Bit/dim 3.5411(best: 3.5392), Xent 1.3259, Loss 4.2040, Error 0.2407(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20690 | Time 25.3655(25.6571) | Bit/dim 3.5389(3.5313) | Xent 0.0692(0.0804) | Loss 3.5735(3.5715) | Error 0.0267(0.0292) Steps 1078(1075.43) | Grad Norm 1.9305(2.4485) | Total Time 14.00(14.00)\n",
      "Iter 20700 | Time 24.7811(25.6324) | Bit/dim 3.5491(3.5301) | Xent 0.1001(0.0779) | Loss 3.5992(3.5690) | Error 0.0356(0.0279) Steps 1078(1075.49) | Grad Norm 1.9265(2.2604) | Total Time 14.00(14.00)\n",
      "Iter 20710 | Time 25.3820(25.6521) | Bit/dim 3.5390(3.5291) | Xent 0.0659(0.0786) | Loss 3.5720(3.5683) | Error 0.0256(0.0283) Steps 1072(1076.27) | Grad Norm 1.9859(2.2271) | Total Time 14.00(14.00)\n",
      "Iter 20720 | Time 24.9553(25.6478) | Bit/dim 3.4951(3.5289) | Xent 0.0647(0.0775) | Loss 3.5274(3.5677) | Error 0.0222(0.0283) Steps 1078(1076.53) | Grad Norm 2.5659(2.2210) | Total Time 14.00(14.00)\n",
      "Iter 20730 | Time 26.2944(25.7482) | Bit/dim 3.5356(3.5279) | Xent 0.0773(0.0777) | Loss 3.5743(3.5667) | Error 0.0256(0.0285) Steps 1084(1077.19) | Grad Norm 1.8584(2.2173) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 121.7378, Epoch Time 1552.5146(1555.7211), Bit/dim 3.5419(best: 3.5392), Xent 1.3308, Loss 4.2073, Error 0.2421(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20740 | Time 25.5188(25.7482) | Bit/dim 3.5439(3.5306) | Xent 0.0796(0.0782) | Loss 3.5837(3.5697) | Error 0.0344(0.0286) Steps 1084(1076.01) | Grad Norm 1.8350(2.2557) | Total Time 14.00(14.00)\n",
      "Iter 20750 | Time 25.8740(25.7638) | Bit/dim 3.5440(3.5309) | Xent 0.0634(0.0782) | Loss 3.5757(3.5701) | Error 0.0178(0.0278) Steps 1066(1075.23) | Grad Norm 1.9009(2.1614) | Total Time 14.00(14.00)\n",
      "Iter 20760 | Time 25.4796(25.7216) | Bit/dim 3.5640(3.5297) | Xent 0.0793(0.0770) | Loss 3.6037(3.5682) | Error 0.0311(0.0276) Steps 1078(1075.36) | Grad Norm 1.7655(2.1576) | Total Time 14.00(14.00)\n",
      "Iter 20770 | Time 25.9128(25.6090) | Bit/dim 3.4872(3.5288) | Xent 0.0751(0.0769) | Loss 3.5248(3.5672) | Error 0.0267(0.0275) Steps 1060(1073.45) | Grad Norm 2.8671(2.1223) | Total Time 14.00(14.00)\n",
      "Iter 20780 | Time 25.3193(25.6536) | Bit/dim 3.5058(3.5294) | Xent 0.0751(0.0773) | Loss 3.5434(3.5681) | Error 0.0289(0.0281) Steps 1078(1073.73) | Grad Norm 1.5619(2.1022) | Total Time 14.00(14.00)\n",
      "Iter 20790 | Time 26.0173(25.6851) | Bit/dim 3.5507(3.5287) | Xent 0.0956(0.0792) | Loss 3.5985(3.5683) | Error 0.0378(0.0286) Steps 1072(1073.93) | Grad Norm 2.6223(2.2135) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 122.4837, Epoch Time 1551.3118(1555.5888), Bit/dim 3.5395(best: 3.5392), Xent 1.3198, Loss 4.1994, Error 0.2446(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20800 | Time 25.7265(25.7080) | Bit/dim 3.5228(3.5282) | Xent 0.0765(0.0770) | Loss 3.5610(3.5667) | Error 0.0333(0.0274) Steps 1072(1073.59) | Grad Norm 2.4410(2.1905) | Total Time 14.00(14.00)\n",
      "Iter 20810 | Time 26.4266(25.7302) | Bit/dim 3.5437(3.5293) | Xent 0.0726(0.0764) | Loss 3.5800(3.5675) | Error 0.0267(0.0272) Steps 1072(1073.21) | Grad Norm 2.2672(2.1206) | Total Time 14.00(14.00)\n",
      "Iter 20820 | Time 25.8174(25.6959) | Bit/dim 3.5212(3.5285) | Xent 0.0997(0.0780) | Loss 3.5711(3.5675) | Error 0.0322(0.0279) Steps 1066(1073.94) | Grad Norm 2.7038(2.1688) | Total Time 14.00(14.00)\n",
      "Iter 20830 | Time 25.3025(25.6382) | Bit/dim 3.5100(3.5275) | Xent 0.0854(0.0800) | Loss 3.5527(3.5675) | Error 0.0344(0.0288) Steps 1090(1074.23) | Grad Norm 2.0339(2.3238) | Total Time 14.00(14.00)\n",
      "Iter 20840 | Time 25.1810(25.5835) | Bit/dim 3.5292(3.5288) | Xent 0.0833(0.0824) | Loss 3.5708(3.5700) | Error 0.0256(0.0291) Steps 1066(1074.61) | Grad Norm 2.8285(2.4613) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 121.4781, Epoch Time 1547.8577(1555.3568), Bit/dim 3.5400(best: 3.5392), Xent 1.3181, Loss 4.1991, Error 0.2436(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20850 | Time 26.4947(25.6056) | Bit/dim 3.5320(3.5282) | Xent 0.0699(0.0829) | Loss 3.5669(3.5697) | Error 0.0278(0.0296) Steps 1078(1073.73) | Grad Norm 2.4886(2.4461) | Total Time 14.00(14.00)\n",
      "Iter 20860 | Time 25.8028(25.6530) | Bit/dim 3.5029(3.5290) | Xent 0.0922(0.0819) | Loss 3.5490(3.5700) | Error 0.0322(0.0291) Steps 1066(1073.67) | Grad Norm 3.0530(2.4199) | Total Time 14.00(14.00)\n",
      "Iter 20870 | Time 26.0119(25.6537) | Bit/dim 3.4951(3.5300) | Xent 0.0853(0.0811) | Loss 3.5378(3.5706) | Error 0.0233(0.0290) Steps 1066(1073.70) | Grad Norm 1.9922(2.4067) | Total Time 14.00(14.00)\n",
      "Iter 20880 | Time 24.8224(25.5950) | Bit/dim 3.5328(3.5285) | Xent 0.0625(0.0823) | Loss 3.5641(3.5697) | Error 0.0233(0.0291) Steps 1066(1073.61) | Grad Norm 1.9925(2.3554) | Total Time 14.00(14.00)\n",
      "Iter 20890 | Time 25.3744(25.6310) | Bit/dim 3.5325(3.5302) | Xent 0.0746(0.0808) | Loss 3.5698(3.5706) | Error 0.0233(0.0286) Steps 1084(1073.89) | Grad Norm 1.7956(2.2844) | Total Time 14.00(14.00)\n",
      "Iter 20900 | Time 25.0925(25.5763) | Bit/dim 3.5033(3.5280) | Xent 0.0727(0.0793) | Loss 3.5397(3.5676) | Error 0.0256(0.0280) Steps 1078(1073.52) | Grad Norm 1.5520(2.1572) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 122.6704, Epoch Time 1546.6059(1555.0943), Bit/dim 3.5411(best: 3.5392), Xent 1.3339, Loss 4.2080, Error 0.2432(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20910 | Time 26.1003(25.5919) | Bit/dim 3.5370(3.5288) | Xent 0.0727(0.0789) | Loss 3.5734(3.5682) | Error 0.0256(0.0279) Steps 1072(1073.90) | Grad Norm 2.4562(2.1319) | Total Time 14.00(14.00)\n",
      "Iter 20920 | Time 26.4464(25.6892) | Bit/dim 3.5939(3.5296) | Xent 0.0711(0.0771) | Loss 3.6294(3.5681) | Error 0.0267(0.0275) Steps 1072(1074.55) | Grad Norm 1.8478(2.0913) | Total Time 14.00(14.00)\n",
      "Iter 20930 | Time 25.3336(25.6788) | Bit/dim 3.4933(3.5277) | Xent 0.0565(0.0755) | Loss 3.5215(3.5654) | Error 0.0200(0.0271) Steps 1066(1074.37) | Grad Norm 1.6538(2.0352) | Total Time 14.00(14.00)\n",
      "Iter 20940 | Time 25.2707(25.6290) | Bit/dim 3.5198(3.5289) | Xent 0.0999(0.0749) | Loss 3.5698(3.5664) | Error 0.0389(0.0267) Steps 1084(1075.42) | Grad Norm 2.6777(1.9929) | Total Time 14.00(14.00)\n",
      "Iter 20950 | Time 25.5442(25.6356) | Bit/dim 3.5177(3.5283) | Xent 0.0688(0.0747) | Loss 3.5521(3.5657) | Error 0.0256(0.0264) Steps 1072(1074.70) | Grad Norm 1.8834(2.0201) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 121.6446, Epoch Time 1548.8286(1554.9063), Bit/dim 3.5410(best: 3.5392), Xent 1.3191, Loss 4.2005, Error 0.2430(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 20960 | Time 25.1447(25.5544) | Bit/dim 3.5242(3.5261) | Xent 0.0744(0.0764) | Loss 3.5614(3.5642) | Error 0.0300(0.0271) Steps 1084(1073.91) | Grad Norm 2.5601(2.0823) | Total Time 14.00(14.00)\n",
      "Iter 20970 | Time 25.5234(25.6076) | Bit/dim 3.5507(3.5294) | Xent 0.0659(0.0756) | Loss 3.5837(3.5672) | Error 0.0189(0.0270) Steps 1072(1073.25) | Grad Norm 1.4011(2.0619) | Total Time 14.00(14.00)\n",
      "Iter 20980 | Time 25.1111(25.5892) | Bit/dim 3.5454(3.5280) | Xent 0.0639(0.0754) | Loss 3.5774(3.5657) | Error 0.0244(0.0269) Steps 1090(1073.56) | Grad Norm 1.7734(2.1321) | Total Time 14.00(14.00)\n",
      "Iter 20990 | Time 25.3429(25.6179) | Bit/dim 3.5472(3.5300) | Xent 0.0856(0.0760) | Loss 3.5900(3.5680) | Error 0.0344(0.0274) Steps 1090(1074.28) | Grad Norm 2.5412(2.2578) | Total Time 14.00(14.00)\n",
      "Iter 21000 | Time 25.4316(25.6079) | Bit/dim 3.5272(3.5290) | Xent 0.0877(0.0790) | Loss 3.5711(3.5685) | Error 0.0278(0.0282) Steps 1084(1076.03) | Grad Norm 2.3583(2.3025) | Total Time 14.00(14.00)\n",
      "Iter 21010 | Time 25.7927(25.5645) | Bit/dim 3.5060(3.5290) | Xent 0.0980(0.0802) | Loss 3.5550(3.5692) | Error 0.0367(0.0288) Steps 1078(1074.30) | Grad Norm 4.3648(2.5246) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 123.3598, Epoch Time 1546.1042(1554.6423), Bit/dim 3.5418(best: 3.5392), Xent 1.3626, Loss 4.2231, Error 0.2406(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21020 | Time 26.0912(25.6025) | Bit/dim 3.5616(3.5316) | Xent 0.0925(0.0804) | Loss 3.6078(3.5718) | Error 0.0333(0.0287) Steps 1108(1075.82) | Grad Norm 2.6127(2.5313) | Total Time 14.00(14.00)\n",
      "Iter 21030 | Time 25.4258(25.6298) | Bit/dim 3.5412(3.5283) | Xent 0.0741(0.0798) | Loss 3.5783(3.5682) | Error 0.0244(0.0281) Steps 1084(1075.59) | Grad Norm 2.9556(2.4220) | Total Time 14.00(14.00)\n",
      "Iter 21040 | Time 25.3754(25.5463) | Bit/dim 3.5316(3.5284) | Xent 0.0785(0.0792) | Loss 3.5709(3.5680) | Error 0.0289(0.0282) Steps 1066(1075.26) | Grad Norm 2.2682(2.3722) | Total Time 14.00(14.00)\n",
      "Iter 21050 | Time 25.5493(25.5440) | Bit/dim 3.5405(3.5251) | Xent 0.0893(0.0793) | Loss 3.5852(3.5647) | Error 0.0333(0.0282) Steps 1066(1075.93) | Grad Norm 2.5017(2.3710) | Total Time 14.00(14.00)\n",
      "Iter 21060 | Time 25.4467(25.5852) | Bit/dim 3.5493(3.5282) | Xent 0.0692(0.0787) | Loss 3.5840(3.5675) | Error 0.0244(0.0278) Steps 1060(1076.18) | Grad Norm 2.7302(2.3296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 121.4357, Epoch Time 1543.9380(1554.3212), Bit/dim 3.5407(best: 3.5392), Xent 1.3709, Loss 4.2261, Error 0.2434(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21070 | Time 25.1401(25.5925) | Bit/dim 3.5278(3.5301) | Xent 0.1018(0.0812) | Loss 3.5787(3.5707) | Error 0.0378(0.0290) Steps 1066(1074.30) | Grad Norm 5.0297(2.5101) | Total Time 14.00(14.00)\n",
      "Iter 21080 | Time 25.1036(25.6569) | Bit/dim 3.5170(3.5313) | Xent 0.0650(0.0782) | Loss 3.5495(3.5704) | Error 0.0211(0.0280) Steps 1060(1073.42) | Grad Norm 2.1980(2.4913) | Total Time 14.00(14.00)\n",
      "Iter 21090 | Time 26.5983(25.6795) | Bit/dim 3.5299(3.5290) | Xent 0.0829(0.0782) | Loss 3.5713(3.5681) | Error 0.0233(0.0278) Steps 1078(1074.09) | Grad Norm 2.0856(2.3650) | Total Time 14.00(14.00)\n",
      "Iter 21100 | Time 26.0082(25.7535) | Bit/dim 3.5069(3.5311) | Xent 0.0674(0.0769) | Loss 3.5405(3.5695) | Error 0.0233(0.0272) Steps 1066(1073.85) | Grad Norm 1.6441(2.2850) | Total Time 14.00(14.00)\n",
      "Iter 21110 | Time 25.7324(25.6551) | Bit/dim 3.5098(3.5288) | Xent 0.0933(0.0767) | Loss 3.5565(3.5672) | Error 0.0356(0.0272) Steps 1072(1074.48) | Grad Norm 2.3124(2.2179) | Total Time 14.00(14.00)\n",
      "Iter 21120 | Time 25.7814(25.6727) | Bit/dim 3.5257(3.5277) | Xent 0.0750(0.0765) | Loss 3.5631(3.5659) | Error 0.0322(0.0272) Steps 1078(1074.06) | Grad Norm 1.7210(2.1512) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 122.6872, Epoch Time 1554.5925(1554.3293), Bit/dim 3.5415(best: 3.5392), Xent 1.4017, Loss 4.2423, Error 0.2438(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21130 | Time 25.3111(25.7016) | Bit/dim 3.5231(3.5271) | Xent 0.0794(0.0767) | Loss 3.5629(3.5655) | Error 0.0233(0.0269) Steps 1084(1075.73) | Grad Norm 1.6080(2.1501) | Total Time 14.00(14.00)\n",
      "Iter 21140 | Time 25.0543(25.6078) | Bit/dim 3.5366(3.5250) | Xent 0.0781(0.0765) | Loss 3.5756(3.5633) | Error 0.0244(0.0272) Steps 1060(1075.95) | Grad Norm 2.3410(2.1800) | Total Time 14.00(14.00)\n",
      "Iter 21150 | Time 26.1128(25.6108) | Bit/dim 3.5235(3.5275) | Xent 0.0741(0.0770) | Loss 3.5605(3.5660) | Error 0.0256(0.0274) Steps 1084(1076.64) | Grad Norm 2.6969(2.1790) | Total Time 14.00(14.00)\n",
      "Iter 21160 | Time 25.7341(25.6231) | Bit/dim 3.5571(3.5282) | Xent 0.0684(0.0742) | Loss 3.5913(3.5653) | Error 0.0211(0.0262) Steps 1084(1076.80) | Grad Norm 1.5465(2.0732) | Total Time 14.00(14.00)\n",
      "Iter 21170 | Time 25.8714(25.5975) | Bit/dim 3.5304(3.5304) | Xent 0.0524(0.0742) | Loss 3.5566(3.5676) | Error 0.0189(0.0270) Steps 1078(1076.90) | Grad Norm 2.1417(2.0818) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 121.0667, Epoch Time 1543.5792(1554.0068), Bit/dim 3.5413(best: 3.5392), Xent 1.3971, Loss 4.2398, Error 0.2412(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21180 | Time 26.1398(25.5636) | Bit/dim 3.5040(3.5285) | Xent 0.0556(0.0730) | Loss 3.5318(3.5650) | Error 0.0156(0.0263) Steps 1060(1075.95) | Grad Norm 1.7502(2.0382) | Total Time 14.00(14.00)\n",
      "Iter 21190 | Time 24.9770(25.5741) | Bit/dim 3.5693(3.5291) | Xent 0.0712(0.0748) | Loss 3.6050(3.5665) | Error 0.0311(0.0277) Steps 1084(1076.74) | Grad Norm 2.7625(2.1878) | Total Time 14.00(14.00)\n",
      "Iter 21200 | Time 25.9916(25.5922) | Bit/dim 3.4972(3.5274) | Xent 0.0820(0.0779) | Loss 3.5382(3.5663) | Error 0.0278(0.0288) Steps 1108(1078.40) | Grad Norm 2.8266(2.2247) | Total Time 14.00(14.00)\n",
      "Iter 21210 | Time 25.5917(25.5249) | Bit/dim 3.5252(3.5279) | Xent 0.0607(0.0776) | Loss 3.5555(3.5667) | Error 0.0222(0.0286) Steps 1072(1076.98) | Grad Norm 1.5743(2.2092) | Total Time 14.00(14.00)\n",
      "Iter 21220 | Time 25.9541(25.5499) | Bit/dim 3.5599(3.5270) | Xent 0.0567(0.0760) | Loss 3.5882(3.5650) | Error 0.0211(0.0282) Steps 1084(1076.69) | Grad Norm 1.7064(2.1472) | Total Time 14.00(14.00)\n",
      "Iter 21230 | Time 25.4696(25.5786) | Bit/dim 3.5308(3.5289) | Xent 0.0958(0.0789) | Loss 3.5787(3.5683) | Error 0.0322(0.0295) Steps 1078(1076.15) | Grad Norm 2.3316(2.1887) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 121.9886, Epoch Time 1544.3629(1553.7175), Bit/dim 3.5412(best: 3.5392), Xent 1.3308, Loss 4.2066, Error 0.2391(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21240 | Time 25.6703(25.5338) | Bit/dim 3.5362(3.5277) | Xent 0.0744(0.0768) | Loss 3.5734(3.5661) | Error 0.0233(0.0280) Steps 1078(1075.28) | Grad Norm 2.6615(2.1985) | Total Time 14.00(14.00)\n",
      "Iter 21250 | Time 25.3503(25.5094) | Bit/dim 3.5337(3.5273) | Xent 0.0712(0.0768) | Loss 3.5693(3.5657) | Error 0.0233(0.0278) Steps 1072(1075.02) | Grad Norm 1.8880(2.1564) | Total Time 14.00(14.00)\n",
      "Iter 21260 | Time 25.5716(25.5618) | Bit/dim 3.5275(3.5286) | Xent 0.0777(0.0774) | Loss 3.5663(3.5673) | Error 0.0278(0.0278) Steps 1066(1074.96) | Grad Norm 2.4777(2.2507) | Total Time 14.00(14.00)\n",
      "Iter 21270 | Time 25.6178(25.5727) | Bit/dim 3.4942(3.5272) | Xent 0.0712(0.0768) | Loss 3.5298(3.5656) | Error 0.0289(0.0276) Steps 1078(1076.07) | Grad Norm 1.8188(2.2238) | Total Time 14.00(14.00)\n",
      "Iter 21280 | Time 25.8094(25.5858) | Bit/dim 3.5315(3.5283) | Xent 0.0685(0.0746) | Loss 3.5658(3.5657) | Error 0.0256(0.0266) Steps 1078(1076.26) | Grad Norm 2.0343(2.1346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 123.2764, Epoch Time 1545.3394(1553.4661), Bit/dim 3.5404(best: 3.5392), Xent 1.3917, Loss 4.2362, Error 0.2433(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21290 | Time 25.7352(25.6068) | Bit/dim 3.5463(3.5296) | Xent 0.0689(0.0741) | Loss 3.5807(3.5666) | Error 0.0267(0.0262) Steps 1084(1076.99) | Grad Norm 2.9225(2.1151) | Total Time 14.00(14.00)\n",
      "Iter 21300 | Time 25.4045(25.6255) | Bit/dim 3.5561(3.5296) | Xent 0.0861(0.0750) | Loss 3.5992(3.5672) | Error 0.0311(0.0264) Steps 1078(1077.04) | Grad Norm 2.8694(2.1850) | Total Time 14.00(14.00)\n",
      "Iter 21310 | Time 26.0374(25.6751) | Bit/dim 3.5015(3.5262) | Xent 0.0584(0.0751) | Loss 3.5307(3.5637) | Error 0.0233(0.0266) Steps 1078(1077.74) | Grad Norm 1.5453(2.1197) | Total Time 14.00(14.00)\n",
      "Iter 21320 | Time 25.9712(25.7008) | Bit/dim 3.5293(3.5294) | Xent 0.0614(0.0730) | Loss 3.5600(3.5659) | Error 0.0267(0.0262) Steps 1084(1078.20) | Grad Norm 1.4898(2.0049) | Total Time 14.00(14.00)\n",
      "Iter 21330 | Time 26.3007(25.7325) | Bit/dim 3.5362(3.5290) | Xent 0.0724(0.0738) | Loss 3.5724(3.5659) | Error 0.0233(0.0266) Steps 1066(1076.12) | Grad Norm 1.5446(1.9685) | Total Time 14.00(14.00)\n",
      "Iter 21340 | Time 25.7240(25.6856) | Bit/dim 3.5351(3.5285) | Xent 0.0558(0.0744) | Loss 3.5630(3.5658) | Error 0.0200(0.0269) Steps 1066(1076.04) | Grad Norm 1.5172(1.9723) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 121.9329, Epoch Time 1552.7930(1553.4459), Bit/dim 3.5394(best: 3.5392), Xent 1.3568, Loss 4.2178, Error 0.2442(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21350 | Time 25.5804(25.6817) | Bit/dim 3.5217(3.5268) | Xent 0.0816(0.0742) | Loss 3.5625(3.5639) | Error 0.0367(0.0267) Steps 1072(1075.78) | Grad Norm 2.3917(1.9974) | Total Time 14.00(14.00)\n",
      "Iter 21360 | Time 25.1596(25.6539) | Bit/dim 3.5173(3.5247) | Xent 0.1025(0.0755) | Loss 3.5686(3.5624) | Error 0.0322(0.0272) Steps 1072(1075.59) | Grad Norm 2.7780(2.1503) | Total Time 14.00(14.00)\n",
      "Iter 21370 | Time 25.9197(25.6184) | Bit/dim 3.5202(3.5284) | Xent 0.0652(0.0765) | Loss 3.5529(3.5667) | Error 0.0222(0.0275) Steps 1084(1076.64) | Grad Norm 3.0945(2.3683) | Total Time 14.00(14.00)\n",
      "Iter 21380 | Time 25.3332(25.5792) | Bit/dim 3.5334(3.5280) | Xent 0.0587(0.0769) | Loss 3.5628(3.5665) | Error 0.0267(0.0278) Steps 1084(1076.66) | Grad Norm 1.6317(2.4144) | Total Time 14.00(14.00)\n",
      "Iter 21390 | Time 26.1622(25.6386) | Bit/dim 3.5267(3.5296) | Xent 0.0705(0.0764) | Loss 3.5619(3.5678) | Error 0.0256(0.0274) Steps 1072(1076.43) | Grad Norm 2.4826(2.3937) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 121.9865, Epoch Time 1546.8839(1553.2491), Bit/dim 3.5395(best: 3.5392), Xent 1.3262, Loss 4.2026, Error 0.2410(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21400 | Time 25.4704(25.6425) | Bit/dim 3.5312(3.5293) | Xent 0.0717(0.0767) | Loss 3.5671(3.5677) | Error 0.0233(0.0274) Steps 1078(1076.59) | Grad Norm 2.6761(2.3333) | Total Time 14.00(14.00)\n",
      "Iter 21410 | Time 24.6517(25.6305) | Bit/dim 3.5380(3.5299) | Xent 0.0866(0.0755) | Loss 3.5814(3.5677) | Error 0.0367(0.0270) Steps 1072(1075.04) | Grad Norm 2.1646(2.3258) | Total Time 14.00(14.00)\n",
      "Iter 21420 | Time 25.9292(25.6491) | Bit/dim 3.5109(3.5277) | Xent 0.0676(0.0753) | Loss 3.5447(3.5653) | Error 0.0222(0.0269) Steps 1096(1075.70) | Grad Norm 1.9933(2.3251) | Total Time 14.00(14.00)\n",
      "Iter 21430 | Time 25.4890(25.6260) | Bit/dim 3.5553(3.5273) | Xent 0.0570(0.0751) | Loss 3.5838(3.5649) | Error 0.0222(0.0275) Steps 1054(1074.44) | Grad Norm 1.7104(2.2806) | Total Time 14.00(14.00)\n",
      "Iter 21440 | Time 25.9772(25.5845) | Bit/dim 3.5092(3.5267) | Xent 0.0715(0.0770) | Loss 3.5450(3.5652) | Error 0.0278(0.0286) Steps 1078(1076.14) | Grad Norm 2.2772(2.4686) | Total Time 14.00(14.00)\n",
      "Iter 21450 | Time 25.4149(25.6024) | Bit/dim 3.5712(3.5315) | Xent 0.1098(0.0803) | Loss 3.6260(3.5716) | Error 0.0344(0.0295) Steps 1072(1075.05) | Grad Norm 2.4407(2.6335) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 122.0222, Epoch Time 1546.1382(1553.0358), Bit/dim 3.5406(best: 3.5392), Xent 1.3596, Loss 4.2204, Error 0.2432(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21460 | Time 25.7448(25.5846) | Bit/dim 3.4839(3.5294) | Xent 0.0765(0.0778) | Loss 3.5221(3.5683) | Error 0.0278(0.0285) Steps 1066(1075.78) | Grad Norm 1.9695(2.5790) | Total Time 14.00(14.00)\n",
      "Iter 21470 | Time 26.8145(25.6803) | Bit/dim 3.5215(3.5297) | Xent 0.0546(0.0788) | Loss 3.5488(3.5691) | Error 0.0189(0.0289) Steps 1102(1076.97) | Grad Norm 1.5395(2.4908) | Total Time 14.00(14.00)\n",
      "Iter 21480 | Time 26.0628(25.7374) | Bit/dim 3.5029(3.5273) | Xent 0.0715(0.0780) | Loss 3.5387(3.5663) | Error 0.0267(0.0283) Steps 1084(1076.48) | Grad Norm 2.6261(2.4032) | Total Time 14.00(14.00)\n",
      "Iter 21490 | Time 25.5020(25.7222) | Bit/dim 3.5586(3.5284) | Xent 0.0908(0.0782) | Loss 3.6040(3.5675) | Error 0.0300(0.0281) Steps 1066(1076.94) | Grad Norm 2.6597(2.4330) | Total Time 14.00(14.00)\n",
      "Iter 21500 | Time 26.1273(25.7117) | Bit/dim 3.4981(3.5300) | Xent 0.0797(0.0786) | Loss 3.5379(3.5693) | Error 0.0289(0.0285) Steps 1090(1077.27) | Grad Norm 2.4985(2.3670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 122.9711, Epoch Time 1555.1768(1553.1000), Bit/dim 3.5393(best: 3.5392), Xent 1.3405, Loss 4.2096, Error 0.2431(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21510 | Time 26.1114(25.7456) | Bit/dim 3.5435(3.5320) | Xent 0.0716(0.0768) | Loss 3.5793(3.5704) | Error 0.0211(0.0276) Steps 1078(1077.97) | Grad Norm 2.5136(2.3124) | Total Time 14.00(14.00)\n",
      "Iter 21520 | Time 26.6050(25.7736) | Bit/dim 3.5298(3.5317) | Xent 0.0725(0.0766) | Loss 3.5660(3.5700) | Error 0.0256(0.0274) Steps 1072(1077.48) | Grad Norm 1.9670(2.2697) | Total Time 14.00(14.00)\n",
      "Iter 21530 | Time 25.3583(25.7788) | Bit/dim 3.5571(3.5311) | Xent 0.0840(0.0770) | Loss 3.5991(3.5696) | Error 0.0333(0.0276) Steps 1084(1077.32) | Grad Norm 1.7189(2.2325) | Total Time 14.00(14.00)\n",
      "Iter 21540 | Time 25.7338(25.7405) | Bit/dim 3.5236(3.5279) | Xent 0.0675(0.0769) | Loss 3.5574(3.5663) | Error 0.0256(0.0276) Steps 1072(1076.38) | Grad Norm 1.9961(2.2389) | Total Time 14.00(14.00)\n",
      "Iter 21550 | Time 25.7057(25.6511) | Bit/dim 3.5216(3.5277) | Xent 0.0767(0.0775) | Loss 3.5600(3.5665) | Error 0.0222(0.0276) Steps 1090(1076.87) | Grad Norm 3.2358(2.3205) | Total Time 14.00(14.00)\n",
      "Iter 21560 | Time 25.0348(25.6179) | Bit/dim 3.5007(3.5255) | Xent 0.0558(0.0783) | Loss 3.5286(3.5647) | Error 0.0189(0.0281) Steps 1078(1077.30) | Grad Norm 1.8914(2.3343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 121.2925, Epoch Time 1548.9570(1552.9757), Bit/dim 3.5411(best: 3.5392), Xent 1.4077, Loss 4.2450, Error 0.2436(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21570 | Time 25.1424(25.5615) | Bit/dim 3.5207(3.5247) | Xent 0.0762(0.0762) | Loss 3.5588(3.5628) | Error 0.0278(0.0274) Steps 1072(1076.35) | Grad Norm 2.2069(2.2850) | Total Time 14.00(14.00)\n",
      "Iter 21580 | Time 26.0708(25.5481) | Bit/dim 3.5214(3.5275) | Xent 0.0758(0.0754) | Loss 3.5593(3.5652) | Error 0.0289(0.0266) Steps 1084(1075.19) | Grad Norm 1.6025(2.2605) | Total Time 14.00(14.00)\n",
      "Iter 21590 | Time 25.3646(25.5108) | Bit/dim 3.5002(3.5262) | Xent 0.0828(0.0761) | Loss 3.5416(3.5643) | Error 0.0300(0.0274) Steps 1078(1075.53) | Grad Norm 1.8012(2.2375) | Total Time 14.00(14.00)\n",
      "Iter 21600 | Time 25.1113(25.4541) | Bit/dim 3.5321(3.5250) | Xent 0.0920(0.0777) | Loss 3.5781(3.5638) | Error 0.0333(0.0281) Steps 1084(1075.70) | Grad Norm 3.0645(2.2508) | Total Time 14.00(14.00)\n",
      "Iter 21610 | Time 26.1431(25.4444) | Bit/dim 3.5268(3.5269) | Xent 0.0664(0.0753) | Loss 3.5600(3.5646) | Error 0.0233(0.0265) Steps 1078(1075.36) | Grad Norm 1.6614(2.1882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 121.8531, Epoch Time 1536.7071(1552.4876), Bit/dim 3.5382(best: 3.5392), Xent 1.3450, Loss 4.2107, Error 0.2417(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21620 | Time 26.3260(25.4778) | Bit/dim 3.5137(3.5276) | Xent 0.0800(0.0737) | Loss 3.5537(3.5645) | Error 0.0244(0.0262) Steps 1084(1075.14) | Grad Norm 1.7138(2.0548) | Total Time 14.00(14.00)\n",
      "Iter 21630 | Time 26.1299(25.5443) | Bit/dim 3.5486(3.5287) | Xent 0.0703(0.0737) | Loss 3.5837(3.5655) | Error 0.0211(0.0260) Steps 1066(1077.28) | Grad Norm 1.9694(2.0574) | Total Time 14.00(14.00)\n",
      "Iter 21640 | Time 25.4855(25.4942) | Bit/dim 3.5151(3.5293) | Xent 0.0511(0.0759) | Loss 3.5406(3.5672) | Error 0.0144(0.0269) Steps 1060(1076.23) | Grad Norm 2.0028(2.2237) | Total Time 14.00(14.00)\n",
      "Iter 21650 | Time 26.1381(25.6073) | Bit/dim 3.4987(3.5283) | Xent 0.0934(0.0763) | Loss 3.5454(3.5665) | Error 0.0400(0.0272) Steps 1084(1076.62) | Grad Norm 2.3194(2.2220) | Total Time 14.00(14.00)\n",
      "Iter 21660 | Time 26.1432(25.6239) | Bit/dim 3.5301(3.5278) | Xent 0.0706(0.0766) | Loss 3.5654(3.5661) | Error 0.0200(0.0275) Steps 1090(1075.92) | Grad Norm 2.3139(2.2014) | Total Time 14.00(14.00)\n",
      "Iter 21670 | Time 25.9039(25.6088) | Bit/dim 3.5252(3.5276) | Xent 0.0703(0.0760) | Loss 3.5603(3.5656) | Error 0.0267(0.0271) Steps 1084(1075.14) | Grad Norm 2.1274(2.1656) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 122.6548, Epoch Time 1548.4787(1552.3674), Bit/dim 3.5401(best: 3.5382), Xent 1.3778, Loss 4.2290, Error 0.2419(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21680 | Time 25.8706(25.6187) | Bit/dim 3.5169(3.5279) | Xent 0.0717(0.0754) | Loss 3.5527(3.5656) | Error 0.0222(0.0264) Steps 1084(1073.96) | Grad Norm 2.6822(2.2317) | Total Time 14.00(14.00)\n",
      "Iter 21690 | Time 26.0341(25.5858) | Bit/dim 3.5158(3.5262) | Xent 0.0745(0.0766) | Loss 3.5531(3.5645) | Error 0.0233(0.0267) Steps 1072(1073.64) | Grad Norm 2.0669(2.2808) | Total Time 14.00(14.00)\n",
      "Iter 21700 | Time 25.6224(25.5585) | Bit/dim 3.5064(3.5255) | Xent 0.0475(0.0751) | Loss 3.5302(3.5630) | Error 0.0167(0.0262) Steps 1078(1073.54) | Grad Norm 1.6870(2.2173) | Total Time 14.00(14.00)\n",
      "Iter 21710 | Time 25.6759(25.6495) | Bit/dim 3.5033(3.5236) | Xent 0.0847(0.0758) | Loss 3.5456(3.5615) | Error 0.0311(0.0264) Steps 1066(1074.40) | Grad Norm 2.4495(2.1672) | Total Time 14.00(14.00)\n",
      "Iter 21720 | Time 25.0343(25.5941) | Bit/dim 3.5489(3.5266) | Xent 0.0759(0.0730) | Loss 3.5869(3.5631) | Error 0.0244(0.0253) Steps 1060(1073.68) | Grad Norm 1.9758(2.1106) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 121.1622, Epoch Time 1543.6085(1552.1046), Bit/dim 3.5387(best: 3.5382), Xent 1.3345, Loss 4.2060, Error 0.2416(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21730 | Time 25.9155(25.5419) | Bit/dim 3.5481(3.5324) | Xent 0.0588(0.0728) | Loss 3.5776(3.5688) | Error 0.0189(0.0254) Steps 1078(1074.81) | Grad Norm 1.4587(2.1082) | Total Time 14.00(14.00)\n",
      "Iter 21740 | Time 25.8914(25.5199) | Bit/dim 3.5253(3.5300) | Xent 0.0737(0.0739) | Loss 3.5621(3.5670) | Error 0.0233(0.0262) Steps 1084(1074.14) | Grad Norm 1.9827(2.1662) | Total Time 14.00(14.00)\n",
      "Iter 21750 | Time 25.3658(25.5695) | Bit/dim 3.5311(3.5286) | Xent 0.0739(0.0735) | Loss 3.5680(3.5654) | Error 0.0256(0.0264) Steps 1090(1074.33) | Grad Norm 1.9116(2.1055) | Total Time 14.00(14.00)\n",
      "Iter 21760 | Time 25.4524(25.5452) | Bit/dim 3.5265(3.5280) | Xent 0.0810(0.0719) | Loss 3.5670(3.5639) | Error 0.0256(0.0257) Steps 1078(1074.53) | Grad Norm 1.8829(2.0751) | Total Time 14.00(14.00)\n",
      "Iter 21770 | Time 25.6637(25.5523) | Bit/dim 3.5141(3.5265) | Xent 0.0679(0.0722) | Loss 3.5481(3.5626) | Error 0.0200(0.0253) Steps 1072(1075.57) | Grad Norm 1.4946(2.0676) | Total Time 14.00(14.00)\n",
      "Iter 21780 | Time 25.5022(25.5784) | Bit/dim 3.5082(3.5278) | Xent 0.0878(0.0745) | Loss 3.5521(3.5651) | Error 0.0333(0.0259) Steps 1072(1074.02) | Grad Norm 2.6096(2.1133) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 122.9482, Epoch Time 1544.9929(1551.8912), Bit/dim 3.5400(best: 3.5382), Xent 1.4003, Loss 4.2402, Error 0.2418(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21790 | Time 25.6804(25.5624) | Bit/dim 3.5497(3.5260) | Xent 0.0797(0.0770) | Loss 3.5896(3.5645) | Error 0.0233(0.0271) Steps 1084(1074.57) | Grad Norm 2.2751(2.3378) | Total Time 14.00(14.00)\n",
      "Iter 21800 | Time 25.3507(25.6110) | Bit/dim 3.5221(3.5258) | Xent 0.0500(0.0773) | Loss 3.5471(3.5644) | Error 0.0200(0.0275) Steps 1078(1073.76) | Grad Norm 1.9621(2.4619) | Total Time 14.00(14.00)\n",
      "Iter 21810 | Time 26.0101(25.5237) | Bit/dim 3.5374(3.5288) | Xent 0.0552(0.0772) | Loss 3.5650(3.5674) | Error 0.0156(0.0272) Steps 1072(1072.08) | Grad Norm 2.2044(2.4516) | Total Time 14.00(14.00)\n",
      "Iter 21820 | Time 25.5793(25.5542) | Bit/dim 3.5415(3.5303) | Xent 0.0585(0.0777) | Loss 3.5707(3.5691) | Error 0.0222(0.0274) Steps 1066(1072.47) | Grad Norm 1.5810(2.4763) | Total Time 14.00(14.00)\n",
      "Iter 21830 | Time 25.5213(25.5502) | Bit/dim 3.5274(3.5284) | Xent 0.0812(0.0758) | Loss 3.5680(3.5663) | Error 0.0289(0.0267) Steps 1078(1074.13) | Grad Norm 2.9504(2.4110) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 122.0567, Epoch Time 1544.4267(1551.6673), Bit/dim 3.5394(best: 3.5382), Xent 1.3724, Loss 4.2256, Error 0.2433(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21840 | Time 25.6129(25.5374) | Bit/dim 3.5464(3.5289) | Xent 0.0515(0.0757) | Loss 3.5722(3.5668) | Error 0.0167(0.0268) Steps 1096(1074.23) | Grad Norm 2.0996(2.4410) | Total Time 14.00(14.00)\n",
      "Iter 21850 | Time 25.6200(25.6184) | Bit/dim 3.5269(3.5291) | Xent 0.0790(0.0778) | Loss 3.5665(3.5680) | Error 0.0300(0.0275) Steps 1096(1074.76) | Grad Norm 3.1114(2.5441) | Total Time 14.00(14.00)\n",
      "Iter 21860 | Time 25.4220(25.5861) | Bit/dim 3.5323(3.5261) | Xent 0.0698(0.0772) | Loss 3.5672(3.5647) | Error 0.0211(0.0278) Steps 1084(1075.91) | Grad Norm 2.4544(2.4535) | Total Time 14.00(14.00)\n",
      "Iter 21870 | Time 26.1628(25.6617) | Bit/dim 3.5248(3.5270) | Xent 0.0559(0.0763) | Loss 3.5528(3.5651) | Error 0.0200(0.0270) Steps 1084(1077.30) | Grad Norm 1.6461(2.3381) | Total Time 14.00(14.00)\n",
      "Iter 21880 | Time 26.0713(25.6620) | Bit/dim 3.5562(3.5285) | Xent 0.0622(0.0744) | Loss 3.5872(3.5657) | Error 0.0244(0.0269) Steps 1072(1077.81) | Grad Norm 1.8949(2.2172) | Total Time 14.00(14.00)\n",
      "Iter 21890 | Time 25.5307(25.6577) | Bit/dim 3.5215(3.5275) | Xent 0.0785(0.0751) | Loss 3.5607(3.5651) | Error 0.0289(0.0271) Steps 1072(1077.13) | Grad Norm 2.7017(2.3651) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 123.3905, Epoch Time 1551.2423(1551.6546), Bit/dim 3.5381(best: 3.5382), Xent 1.3171, Loss 4.1967, Error 0.2380(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21900 | Time 25.4617(25.6143) | Bit/dim 3.5268(3.5286) | Xent 0.0759(0.0746) | Loss 3.5647(3.5658) | Error 0.0278(0.0266) Steps 1096(1076.99) | Grad Norm 2.9113(2.3457) | Total Time 14.00(14.00)\n",
      "Iter 21910 | Time 26.0149(25.6661) | Bit/dim 3.5600(3.5293) | Xent 0.0780(0.0745) | Loss 3.5990(3.5665) | Error 0.0278(0.0269) Steps 1072(1077.14) | Grad Norm 1.6691(2.2994) | Total Time 14.00(14.00)\n",
      "Iter 21920 | Time 25.1281(25.6372) | Bit/dim 3.5471(3.5299) | Xent 0.0530(0.0749) | Loss 3.5736(3.5674) | Error 0.0211(0.0272) Steps 1090(1078.26) | Grad Norm 2.2727(2.3254) | Total Time 14.00(14.00)\n",
      "Iter 21930 | Time 25.5197(25.5956) | Bit/dim 3.5168(3.5292) | Xent 0.0782(0.0760) | Loss 3.5559(3.5672) | Error 0.0300(0.0272) Steps 1084(1076.79) | Grad Norm 2.2008(2.5116) | Total Time 14.00(14.00)\n",
      "Iter 21940 | Time 24.5976(25.5617) | Bit/dim 3.5626(3.5267) | Xent 0.0617(0.0760) | Loss 3.5935(3.5647) | Error 0.0211(0.0271) Steps 1060(1075.63) | Grad Norm 1.6270(2.4638) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 122.0611, Epoch Time 1544.2334(1551.4319), Bit/dim 3.5397(best: 3.5381), Xent 1.3553, Loss 4.2173, Error 0.2440(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 21950 | Time 25.6803(25.5745) | Bit/dim 3.5368(3.5268) | Xent 0.0822(0.0748) | Loss 3.5779(3.5642) | Error 0.0311(0.0269) Steps 1078(1077.28) | Grad Norm 2.7911(2.4012) | Total Time 14.00(14.00)\n",
      "Iter 21960 | Time 25.9435(25.5952) | Bit/dim 3.5019(3.5254) | Xent 0.0843(0.0737) | Loss 3.5441(3.5623) | Error 0.0289(0.0264) Steps 1072(1077.69) | Grad Norm 2.0674(2.2479) | Total Time 14.00(14.00)\n",
      "Iter 21970 | Time 24.9146(25.5779) | Bit/dim 3.5053(3.5270) | Xent 0.0798(0.0732) | Loss 3.5452(3.5636) | Error 0.0322(0.0264) Steps 1066(1076.94) | Grad Norm 2.7014(2.2060) | Total Time 14.00(14.00)\n",
      "Iter 21980 | Time 26.0153(25.6138) | Bit/dim 3.5558(3.5267) | Xent 0.0713(0.0740) | Loss 3.5914(3.5637) | Error 0.0267(0.0268) Steps 1078(1075.93) | Grad Norm 2.3881(2.2505) | Total Time 14.00(14.00)\n",
      "Iter 21990 | Time 25.1862(25.6286) | Bit/dim 3.5222(3.5258) | Xent 0.0708(0.0736) | Loss 3.5576(3.5626) | Error 0.0211(0.0271) Steps 1072(1076.19) | Grad Norm 2.4100(2.2885) | Total Time 14.00(14.00)\n",
      "Iter 22000 | Time 25.6913(25.6645) | Bit/dim 3.4936(3.5279) | Xent 0.0838(0.0744) | Loss 3.5355(3.5651) | Error 0.0300(0.0269) Steps 1072(1076.00) | Grad Norm 3.2262(2.2558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 122.6882, Epoch Time 1550.3184(1551.3985), Bit/dim 3.5387(best: 3.5381), Xent 1.3733, Loss 4.2253, Error 0.2404(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22010 | Time 24.8453(25.5669) | Bit/dim 3.4717(3.5248) | Xent 0.0857(0.0746) | Loss 3.5145(3.5621) | Error 0.0322(0.0268) Steps 1078(1076.39) | Grad Norm 2.9290(2.2421) | Total Time 14.00(14.00)\n",
      "Iter 22020 | Time 25.0274(25.5722) | Bit/dim 3.5458(3.5252) | Xent 0.0831(0.0755) | Loss 3.5874(3.5630) | Error 0.0289(0.0270) Steps 1072(1076.31) | Grad Norm 2.9839(2.3857) | Total Time 14.00(14.00)\n",
      "Iter 22030 | Time 25.3075(25.6096) | Bit/dim 3.4997(3.5252) | Xent 0.0824(0.0749) | Loss 3.5409(3.5627) | Error 0.0300(0.0265) Steps 1078(1077.59) | Grad Norm 1.9940(2.4039) | Total Time 14.00(14.00)\n",
      "Iter 22040 | Time 25.9897(25.6388) | Bit/dim 3.4929(3.5256) | Xent 0.0784(0.0752) | Loss 3.5321(3.5632) | Error 0.0256(0.0267) Steps 1072(1077.98) | Grad Norm 1.7927(2.3673) | Total Time 14.00(14.00)\n",
      "Iter 22050 | Time 25.5407(25.7152) | Bit/dim 3.5787(3.5283) | Xent 0.0614(0.0729) | Loss 3.6094(3.5647) | Error 0.0222(0.0260) Steps 1072(1077.15) | Grad Norm 1.9573(2.3015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 122.2374, Epoch Time 1548.7963(1551.3205), Bit/dim 3.5381(best: 3.5381), Xent 1.3349, Loss 4.2056, Error 0.2377(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22060 | Time 26.1643(25.7069) | Bit/dim 3.4989(3.5285) | Xent 0.0742(0.0737) | Loss 3.5360(3.5653) | Error 0.0300(0.0264) Steps 1072(1076.89) | Grad Norm 2.4662(2.3127) | Total Time 14.00(14.00)\n",
      "Iter 22070 | Time 24.8527(25.6693) | Bit/dim 3.5153(3.5284) | Xent 0.0877(0.0757) | Loss 3.5592(3.5663) | Error 0.0300(0.0271) Steps 1072(1075.76) | Grad Norm 1.8249(2.3652) | Total Time 14.00(14.00)\n",
      "Iter 22080 | Time 25.6362(25.7574) | Bit/dim 3.5060(3.5264) | Xent 0.0837(0.0773) | Loss 3.5479(3.5651) | Error 0.0344(0.0278) Steps 1096(1076.28) | Grad Norm 2.3916(2.4676) | Total Time 14.00(14.00)\n",
      "Iter 22090 | Time 25.0168(25.7482) | Bit/dim 3.5039(3.5259) | Xent 0.1020(0.0784) | Loss 3.5549(3.5651) | Error 0.0411(0.0283) Steps 1072(1075.94) | Grad Norm 4.2677(2.5965) | Total Time 14.00(14.00)\n",
      "Iter 22100 | Time 25.1572(25.7030) | Bit/dim 3.5725(3.5294) | Xent 0.0738(0.0788) | Loss 3.6094(3.5688) | Error 0.0289(0.0285) Steps 1072(1073.88) | Grad Norm 2.6279(2.6550) | Total Time 14.00(14.00)\n",
      "Iter 22110 | Time 25.2661(25.7004) | Bit/dim 3.5522(3.5291) | Xent 0.0846(0.0782) | Loss 3.5945(3.5682) | Error 0.0322(0.0284) Steps 1066(1075.24) | Grad Norm 3.6805(2.8205) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 121.1560, Epoch Time 1552.1438(1551.3452), Bit/dim 3.5440(best: 3.5381), Xent 1.3906, Loss 4.2392, Error 0.2402(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22120 | Time 24.8985(25.6478) | Bit/dim 3.4975(3.5266) | Xent 0.0901(0.0777) | Loss 3.5426(3.5655) | Error 0.0333(0.0284) Steps 1078(1073.95) | Grad Norm 2.9346(2.8619) | Total Time 14.00(14.00)\n",
      "Iter 22130 | Time 25.7154(25.6153) | Bit/dim 3.5450(3.5265) | Xent 0.0646(0.0776) | Loss 3.5773(3.5652) | Error 0.0256(0.0279) Steps 1078(1073.89) | Grad Norm 2.1419(2.7553) | Total Time 14.00(14.00)\n",
      "Iter 22140 | Time 25.5551(25.5950) | Bit/dim 3.5365(3.5282) | Xent 0.0985(0.0768) | Loss 3.5857(3.5666) | Error 0.0322(0.0274) Steps 1072(1073.75) | Grad Norm 2.6381(2.5761) | Total Time 14.00(14.00)\n",
      "Iter 22150 | Time 25.5859(25.6721) | Bit/dim 3.5430(3.5275) | Xent 0.0770(0.0770) | Loss 3.5815(3.5660) | Error 0.0300(0.0274) Steps 1072(1074.24) | Grad Norm 1.9238(2.4789) | Total Time 14.00(14.00)\n",
      "Iter 22160 | Time 26.4159(25.7211) | Bit/dim 3.5632(3.5290) | Xent 0.0521(0.0745) | Loss 3.5893(3.5663) | Error 0.0189(0.0267) Steps 1078(1074.63) | Grad Norm 1.7135(2.3437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 122.4394, Epoch Time 1550.6665(1551.3248), Bit/dim 3.5378(best: 3.5381), Xent 1.3971, Loss 4.2363, Error 0.2459(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22170 | Time 26.2580(25.7487) | Bit/dim 3.5308(3.5301) | Xent 0.0765(0.0729) | Loss 3.5691(3.5665) | Error 0.0289(0.0265) Steps 1072(1074.65) | Grad Norm 2.4508(2.2894) | Total Time 14.00(14.00)\n",
      "Iter 22180 | Time 25.1167(25.8202) | Bit/dim 3.5333(3.5269) | Xent 0.0747(0.0741) | Loss 3.5706(3.5640) | Error 0.0244(0.0262) Steps 1060(1074.61) | Grad Norm 2.5504(2.2752) | Total Time 14.00(14.00)\n",
      "Iter 22190 | Time 25.7345(25.7479) | Bit/dim 3.5149(3.5264) | Xent 0.0729(0.0740) | Loss 3.5514(3.5634) | Error 0.0267(0.0268) Steps 1066(1074.65) | Grad Norm 3.3368(2.3746) | Total Time 14.00(14.00)\n",
      "Iter 22200 | Time 26.1113(25.6449) | Bit/dim 3.5367(3.5289) | Xent 0.0744(0.0767) | Loss 3.5739(3.5672) | Error 0.0211(0.0274) Steps 1096(1073.02) | Grad Norm 2.6890(2.5726) | Total Time 14.00(14.00)\n",
      "Iter 22210 | Time 26.2126(25.7297) | Bit/dim 3.5378(3.5267) | Xent 0.0873(0.0776) | Loss 3.5815(3.5654) | Error 0.0322(0.0280) Steps 1078(1073.62) | Grad Norm 3.0850(2.5728) | Total Time 14.00(14.00)\n",
      "Iter 22220 | Time 25.8792(25.7162) | Bit/dim 3.5557(3.5294) | Xent 0.0786(0.0783) | Loss 3.5950(3.5685) | Error 0.0311(0.0283) Steps 1084(1073.27) | Grad Norm 2.6792(2.6098) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 121.9788, Epoch Time 1552.4434(1551.3584), Bit/dim 3.5401(best: 3.5378), Xent 1.3720, Loss 4.2261, Error 0.2420(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22230 | Time 25.3243(25.6910) | Bit/dim 3.5082(3.5325) | Xent 0.0699(0.0751) | Loss 3.5432(3.5700) | Error 0.0256(0.0269) Steps 1060(1072.23) | Grad Norm 2.5617(2.4289) | Total Time 14.00(14.00)\n",
      "Iter 22240 | Time 25.8165(25.6534) | Bit/dim 3.5055(3.5315) | Xent 0.0573(0.0737) | Loss 3.5341(3.5683) | Error 0.0178(0.0260) Steps 1078(1072.32) | Grad Norm 1.5132(2.3773) | Total Time 14.00(14.00)\n",
      "Iter 22250 | Time 24.9756(25.6050) | Bit/dim 3.5437(3.5267) | Xent 0.0837(0.0748) | Loss 3.5855(3.5641) | Error 0.0300(0.0267) Steps 1078(1072.94) | Grad Norm 2.5266(2.3309) | Total Time 14.00(14.00)\n",
      "Iter 22260 | Time 25.3054(25.6075) | Bit/dim 3.5223(3.5276) | Xent 0.0847(0.0746) | Loss 3.5646(3.5650) | Error 0.0244(0.0270) Steps 1072(1074.59) | Grad Norm 2.0263(2.3369) | Total Time 14.00(14.00)\n",
      "Iter 22270 | Time 25.2451(25.5261) | Bit/dim 3.5475(3.5265) | Xent 0.0543(0.0749) | Loss 3.5747(3.5639) | Error 0.0178(0.0272) Steps 1078(1073.98) | Grad Norm 1.5929(2.3882) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 122.6117, Epoch Time 1542.2489(1551.0851), Bit/dim 3.5388(best: 3.5378), Xent 1.3927, Loss 4.2351, Error 0.2458(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22280 | Time 25.9492(25.5763) | Bit/dim 3.5158(3.5230) | Xent 0.0940(0.0758) | Loss 3.5628(3.5609) | Error 0.0278(0.0275) Steps 1090(1074.32) | Grad Norm 2.6800(2.4304) | Total Time 14.00(14.00)\n",
      "Iter 22290 | Time 26.5812(25.6886) | Bit/dim 3.5303(3.5264) | Xent 0.0920(0.0743) | Loss 3.5763(3.5636) | Error 0.0278(0.0267) Steps 1072(1074.19) | Grad Norm 3.3181(2.4532) | Total Time 14.00(14.00)\n",
      "Iter 22300 | Time 25.6272(25.7037) | Bit/dim 3.5141(3.5265) | Xent 0.0750(0.0754) | Loss 3.5516(3.5642) | Error 0.0267(0.0271) Steps 1084(1075.74) | Grad Norm 1.5576(2.3801) | Total Time 14.00(14.00)\n",
      "Iter 22310 | Time 25.4665(25.7388) | Bit/dim 3.5250(3.5258) | Xent 0.0833(0.0753) | Loss 3.5666(3.5634) | Error 0.0344(0.0269) Steps 1084(1075.80) | Grad Norm 3.1209(2.3088) | Total Time 14.00(14.00)\n",
      "Iter 22320 | Time 26.1856(25.7751) | Bit/dim 3.5179(3.5258) | Xent 0.0658(0.0738) | Loss 3.5507(3.5628) | Error 0.0244(0.0265) Steps 1078(1076.35) | Grad Norm 2.4518(2.3061) | Total Time 14.00(14.00)\n",
      "Iter 22330 | Time 25.9212(25.7856) | Bit/dim 3.5255(3.5258) | Xent 0.0969(0.0739) | Loss 3.5739(3.5628) | Error 0.0356(0.0265) Steps 1078(1076.73) | Grad Norm 3.5397(2.3205) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0406 | Time 123.1390, Epoch Time 1560.8562(1551.3782), Bit/dim 3.5385(best: 3.5378), Xent 1.4019, Loss 4.2395, Error 0.2409(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22340 | Time 24.9525(25.6567) | Bit/dim 3.5496(3.5275) | Xent 0.0767(0.0728) | Loss 3.5880(3.5638) | Error 0.0222(0.0257) Steps 1066(1075.76) | Grad Norm 2.0028(2.2386) | Total Time 14.00(14.00)\n",
      "Iter 22350 | Time 25.4101(25.7558) | Bit/dim 3.5325(3.5280) | Xent 0.0507(0.0708) | Loss 3.5579(3.5634) | Error 0.0178(0.0250) Steps 1090(1076.21) | Grad Norm 1.6259(2.1532) | Total Time 14.00(14.00)\n",
      "Iter 22360 | Time 25.9827(25.8425) | Bit/dim 3.5195(3.5256) | Xent 0.0932(0.0708) | Loss 3.5661(3.5610) | Error 0.0367(0.0250) Steps 1084(1076.67) | Grad Norm 2.2430(2.0859) | Total Time 14.00(14.00)\n",
      "Iter 22370 | Time 26.0228(25.7590) | Bit/dim 3.5191(3.5283) | Xent 0.0824(0.0707) | Loss 3.5603(3.5637) | Error 0.0256(0.0247) Steps 1066(1075.51) | Grad Norm 1.9604(2.0211) | Total Time 14.00(14.00)\n",
      "Iter 22380 | Time 26.0905(25.7522) | Bit/dim 3.5281(3.5269) | Xent 0.0686(0.0701) | Loss 3.5624(3.5619) | Error 0.0256(0.0248) Steps 1072(1076.14) | Grad Norm 1.3840(2.0445) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0407 | Time 122.4523, Epoch Time 1554.7271(1551.4787), Bit/dim 3.5389(best: 3.5378), Xent 1.3996, Loss 4.2387, Error 0.2463(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22390 | Time 26.0382(25.8373) | Bit/dim 3.5059(3.5255) | Xent 0.0612(0.0700) | Loss 3.5365(3.5606) | Error 0.0222(0.0248) Steps 1096(1077.74) | Grad Norm 1.8355(1.9911) | Total Time 14.00(14.00)\n",
      "Iter 22400 | Time 25.7662(25.7637) | Bit/dim 3.4821(3.5273) | Xent 0.0713(0.0701) | Loss 3.5178(3.5624) | Error 0.0267(0.0248) Steps 1084(1076.81) | Grad Norm 2.0115(2.0412) | Total Time 14.00(14.00)\n",
      "Iter 22410 | Time 25.7854(25.7972) | Bit/dim 3.5315(3.5276) | Xent 0.0788(0.0715) | Loss 3.5709(3.5633) | Error 0.0322(0.0258) Steps 1066(1075.59) | Grad Norm 2.4998(2.0708) | Total Time 14.00(14.00)\n",
      "Iter 22420 | Time 25.8717(25.8046) | Bit/dim 3.5319(3.5275) | Xent 0.0742(0.0712) | Loss 3.5690(3.5631) | Error 0.0311(0.0259) Steps 1072(1075.94) | Grad Norm 2.8380(2.1640) | Total Time 14.00(14.00)\n",
      "Iter 22430 | Time 26.2110(25.8280) | Bit/dim 3.5157(3.5259) | Xent 0.0739(0.0713) | Loss 3.5527(3.5616) | Error 0.0256(0.0257) Steps 1078(1077.22) | Grad Norm 1.7512(2.1052) | Total Time 14.00(14.00)\n",
      "Iter 22440 | Time 26.5987(25.8265) | Bit/dim 3.5150(3.5251) | Xent 0.0461(0.0702) | Loss 3.5380(3.5602) | Error 0.0144(0.0249) Steps 1078(1076.40) | Grad Norm 2.3399(2.1792) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0408 | Time 122.1287, Epoch Time 1558.9916(1551.7041), Bit/dim 3.5357(best: 3.5378), Xent 1.3665, Loss 4.2189, Error 0.2429(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22450 | Time 25.1676(25.7966) | Bit/dim 3.5497(3.5287) | Xent 0.0638(0.0713) | Loss 3.5816(3.5644) | Error 0.0233(0.0253) Steps 1072(1076.75) | Grad Norm 2.1636(2.1021) | Total Time 14.00(14.00)\n",
      "Iter 22460 | Time 25.9317(25.7800) | Bit/dim 3.5580(3.5283) | Xent 0.0630(0.0714) | Loss 3.5895(3.5639) | Error 0.0200(0.0252) Steps 1060(1076.98) | Grad Norm 1.6282(2.0800) | Total Time 14.00(14.00)\n",
      "Iter 22470 | Time 26.2846(25.7780) | Bit/dim 3.5087(3.5278) | Xent 0.0836(0.0714) | Loss 3.5504(3.5635) | Error 0.0300(0.0251) Steps 1084(1076.46) | Grad Norm 2.9324(2.1041) | Total Time 14.00(14.00)\n",
      "Iter 22480 | Time 25.6758(25.7158) | Bit/dim 3.5270(3.5263) | Xent 0.0682(0.0705) | Loss 3.5611(3.5615) | Error 0.0211(0.0247) Steps 1066(1076.22) | Grad Norm 2.1016(2.1348) | Total Time 14.00(14.00)\n",
      "Iter 22490 | Time 25.5773(25.6384) | Bit/dim 3.4980(3.5239) | Xent 0.0612(0.0721) | Loss 3.5287(3.5599) | Error 0.0189(0.0256) Steps 1078(1075.86) | Grad Norm 1.8281(2.2226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0409 | Time 122.1892, Epoch Time 1547.8793(1551.5893), Bit/dim 3.5382(best: 3.5357), Xent 1.3926, Loss 4.2345, Error 0.2401(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22500 | Time 25.6804(25.6362) | Bit/dim 3.5273(3.5240) | Xent 0.0763(0.0722) | Loss 3.5655(3.5601) | Error 0.0300(0.0256) Steps 1096(1076.91) | Grad Norm 2.4454(2.2052) | Total Time 14.00(14.00)\n",
      "Iter 22510 | Time 25.3593(25.6472) | Bit/dim 3.5115(3.5221) | Xent 0.0729(0.0745) | Loss 3.5479(3.5593) | Error 0.0256(0.0266) Steps 1072(1077.57) | Grad Norm 2.3799(2.3278) | Total Time 14.00(14.00)\n",
      "Iter 22520 | Time 26.0447(25.5700) | Bit/dim 3.5416(3.5253) | Xent 0.0694(0.0745) | Loss 3.5763(3.5625) | Error 0.0256(0.0271) Steps 1090(1076.29) | Grad Norm 3.0470(2.4719) | Total Time 14.00(14.00)\n",
      "Iter 22530 | Time 25.7162(25.5259) | Bit/dim 3.5289(3.5277) | Xent 0.0711(0.0730) | Loss 3.5644(3.5642) | Error 0.0233(0.0261) Steps 1090(1076.21) | Grad Norm 2.0898(2.4912) | Total Time 14.00(14.00)\n",
      "Iter 22540 | Time 25.5101(25.5404) | Bit/dim 3.5108(3.5285) | Xent 0.0691(0.0736) | Loss 3.5453(3.5653) | Error 0.0222(0.0265) Steps 1084(1077.02) | Grad Norm 2.1897(2.4025) | Total Time 14.00(14.00)\n",
      "Iter 22550 | Time 26.1764(25.6087) | Bit/dim 3.4927(3.5252) | Xent 0.0759(0.0717) | Loss 3.5307(3.5611) | Error 0.0244(0.0258) Steps 1072(1076.37) | Grad Norm 1.6537(2.3319) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0410 | Time 121.9443, Epoch Time 1544.5173(1551.3772), Bit/dim 3.5385(best: 3.5357), Xent 1.3808, Loss 4.2289, Error 0.2442(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22560 | Time 24.8550(25.6130) | Bit/dim 3.5354(3.5288) | Xent 0.0930(0.0710) | Loss 3.5819(3.5642) | Error 0.0267(0.0257) Steps 1078(1077.01) | Grad Norm 2.4804(2.2706) | Total Time 14.00(14.00)\n",
      "Iter 22570 | Time 24.9142(25.6294) | Bit/dim 3.5504(3.5266) | Xent 0.0742(0.0712) | Loss 3.5875(3.5622) | Error 0.0256(0.0256) Steps 1078(1077.99) | Grad Norm 2.2011(2.3896) | Total Time 14.00(14.00)\n",
      "Iter 22580 | Time 25.6945(25.6109) | Bit/dim 3.5369(3.5288) | Xent 0.0721(0.0719) | Loss 3.5730(3.5648) | Error 0.0289(0.0259) Steps 1072(1077.19) | Grad Norm 2.3394(2.4264) | Total Time 14.00(14.00)\n",
      "Iter 22590 | Time 25.0276(25.6781) | Bit/dim 3.5008(3.5235) | Xent 0.0657(0.0725) | Loss 3.5336(3.5597) | Error 0.0200(0.0259) Steps 1066(1075.75) | Grad Norm 1.5661(2.3514) | Total Time 14.00(14.00)\n",
      "Iter 22600 | Time 25.6717(25.6192) | Bit/dim 3.4979(3.5246) | Xent 0.0954(0.0747) | Loss 3.5456(3.5620) | Error 0.0289(0.0266) Steps 1060(1074.86) | Grad Norm 2.0532(2.3356) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0411 | Time 121.9333, Epoch Time 1547.7231(1551.2675), Bit/dim 3.5377(best: 3.5357), Xent 1.3893, Loss 4.2323, Error 0.2416(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22610 | Time 25.1867(25.6130) | Bit/dim 3.5316(3.5240) | Xent 0.0850(0.0749) | Loss 3.5741(3.5614) | Error 0.0278(0.0266) Steps 1078(1075.66) | Grad Norm 3.4601(2.3042) | Total Time 14.00(14.00)\n",
      "Iter 22620 | Time 25.5679(25.5775) | Bit/dim 3.5546(3.5239) | Xent 0.0524(0.0746) | Loss 3.5808(3.5612) | Error 0.0211(0.0266) Steps 1096(1076.21) | Grad Norm 2.1083(2.2827) | Total Time 14.00(14.00)\n",
      "Iter 22630 | Time 26.0506(25.6654) | Bit/dim 3.5066(3.5214) | Xent 0.0834(0.0738) | Loss 3.5483(3.5583) | Error 0.0244(0.0260) Steps 1084(1076.66) | Grad Norm 2.1184(2.2692) | Total Time 14.00(14.00)\n",
      "Iter 22640 | Time 25.7394(25.6434) | Bit/dim 3.5580(3.5242) | Xent 0.0888(0.0748) | Loss 3.6025(3.5616) | Error 0.0311(0.0267) Steps 1066(1077.58) | Grad Norm 2.8377(2.2818) | Total Time 14.00(14.00)\n",
      "Iter 22650 | Time 26.6514(25.7551) | Bit/dim 3.5041(3.5295) | Xent 0.0599(0.0726) | Loss 3.5340(3.5658) | Error 0.0200(0.0260) Steps 1072(1078.03) | Grad Norm 2.0185(2.2395) | Total Time 14.00(14.00)\n",
      "Iter 22660 | Time 25.6647(25.6657) | Bit/dim 3.5664(3.5278) | Xent 0.0775(0.0741) | Loss 3.6051(3.5648) | Error 0.0300(0.0263) Steps 1066(1076.93) | Grad Norm 1.9335(2.2288) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0412 | Time 121.5950, Epoch Time 1551.0417(1551.2608), Bit/dim 3.5379(best: 3.5357), Xent 1.4322, Loss 4.2540, Error 0.2444(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22670 | Time 25.3529(25.6853) | Bit/dim 3.5342(3.5284) | Xent 0.0701(0.0725) | Loss 3.5693(3.5647) | Error 0.0267(0.0257) Steps 1072(1076.07) | Grad Norm 2.9006(2.2986) | Total Time 14.00(14.00)\n",
      "Iter 22680 | Time 26.1701(25.6475) | Bit/dim 3.5279(3.5272) | Xent 0.0669(0.0708) | Loss 3.5614(3.5626) | Error 0.0256(0.0255) Steps 1078(1077.46) | Grad Norm 1.6107(2.1879) | Total Time 14.00(14.00)\n",
      "Iter 22690 | Time 25.6391(25.6491) | Bit/dim 3.5458(3.5281) | Xent 0.0533(0.0693) | Loss 3.5725(3.5628) | Error 0.0167(0.0249) Steps 1072(1078.06) | Grad Norm 1.7656(2.1226) | Total Time 14.00(14.00)\n",
      "Iter 22700 | Time 25.5823(25.6248) | Bit/dim 3.5361(3.5275) | Xent 0.0686(0.0712) | Loss 3.5703(3.5631) | Error 0.0300(0.0257) Steps 1072(1076.51) | Grad Norm 4.0296(2.2796) | Total Time 14.00(14.00)\n",
      "Iter 22710 | Time 25.7568(25.5964) | Bit/dim 3.5373(3.5239) | Xent 0.0719(0.0719) | Loss 3.5732(3.5599) | Error 0.0267(0.0259) Steps 1096(1075.95) | Grad Norm 3.3414(2.3343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0413 | Time 122.1555, Epoch Time 1549.9244(1551.2207), Bit/dim 3.5357(best: 3.5357), Xent 1.3828, Loss 4.2271, Error 0.2449(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22720 | Time 26.0210(25.6927) | Bit/dim 3.4954(3.5251) | Xent 0.0688(0.0738) | Loss 3.5299(3.5620) | Error 0.0267(0.0267) Steps 1084(1075.86) | Grad Norm 2.2326(2.3387) | Total Time 14.00(14.00)\n",
      "Iter 22730 | Time 26.3812(25.7166) | Bit/dim 3.5316(3.5246) | Xent 0.0549(0.0717) | Loss 3.5591(3.5605) | Error 0.0211(0.0259) Steps 1078(1077.48) | Grad Norm 1.8109(2.3026) | Total Time 14.00(14.00)\n",
      "Iter 22740 | Time 25.9284(25.7156) | Bit/dim 3.4913(3.5253) | Xent 0.0654(0.0720) | Loss 3.5240(3.5613) | Error 0.0278(0.0260) Steps 1066(1077.34) | Grad Norm 1.5697(2.1930) | Total Time 14.00(14.00)\n",
      "Iter 22750 | Time 25.1004(25.6828) | Bit/dim 3.4970(3.5252) | Xent 0.0723(0.0724) | Loss 3.5332(3.5614) | Error 0.0244(0.0264) Steps 1072(1076.71) | Grad Norm 3.0256(2.2777) | Total Time 14.00(14.00)\n",
      "Iter 22760 | Time 25.8770(25.6523) | Bit/dim 3.5409(3.5245) | Xent 0.0630(0.0721) | Loss 3.5724(3.5605) | Error 0.0222(0.0264) Steps 1084(1077.07) | Grad Norm 2.1667(2.1800) | Total Time 14.00(14.00)\n",
      "Iter 22770 | Time 24.7504(25.6888) | Bit/dim 3.5629(3.5262) | Xent 0.0599(0.0714) | Loss 3.5929(3.5619) | Error 0.0200(0.0254) Steps 1078(1077.26) | Grad Norm 1.5230(2.1692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0414 | Time 123.4550, Epoch Time 1552.4459(1551.2574), Bit/dim 3.5367(best: 3.5357), Xent 1.3963, Loss 4.2349, Error 0.2436(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22780 | Time 25.1574(25.6036) | Bit/dim 3.5611(3.5266) | Xent 0.0608(0.0703) | Loss 3.5915(3.5617) | Error 0.0189(0.0251) Steps 1066(1077.24) | Grad Norm 1.8831(2.0880) | Total Time 14.00(14.00)\n",
      "Iter 22790 | Time 25.3390(25.5816) | Bit/dim 3.4857(3.5242) | Xent 0.0632(0.0703) | Loss 3.5173(3.5593) | Error 0.0167(0.0248) Steps 1072(1077.34) | Grad Norm 2.3504(2.0594) | Total Time 14.00(14.00)\n",
      "Iter 22800 | Time 25.3134(25.5995) | Bit/dim 3.5094(3.5233) | Xent 0.0665(0.0707) | Loss 3.5427(3.5587) | Error 0.0200(0.0250) Steps 1084(1077.65) | Grad Norm 1.8035(2.0793) | Total Time 14.00(14.00)\n",
      "Iter 22810 | Time 26.0344(25.6089) | Bit/dim 3.5367(3.5254) | Xent 0.0662(0.0718) | Loss 3.5698(3.5613) | Error 0.0244(0.0251) Steps 1066(1076.12) | Grad Norm 2.1725(2.1759) | Total Time 14.00(14.00)\n",
      "Iter 22820 | Time 25.2082(25.6254) | Bit/dim 3.5170(3.5243) | Xent 0.0831(0.0728) | Loss 3.5586(3.5607) | Error 0.0300(0.0260) Steps 1054(1076.22) | Grad Norm 2.0937(2.2234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0415 | Time 122.1387, Epoch Time 1543.3833(1551.0212), Bit/dim 3.5363(best: 3.5357), Xent 1.3838, Loss 4.2282, Error 0.2459(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22830 | Time 24.5909(25.5671) | Bit/dim 3.5457(3.5270) | Xent 0.0795(0.0739) | Loss 3.5855(3.5639) | Error 0.0256(0.0264) Steps 1060(1076.57) | Grad Norm 1.8420(2.4137) | Total Time 14.00(14.00)\n",
      "Iter 22840 | Time 26.0214(25.6096) | Bit/dim 3.5176(3.5264) | Xent 0.0703(0.0735) | Loss 3.5527(3.5631) | Error 0.0256(0.0262) Steps 1072(1075.70) | Grad Norm 2.5884(2.4540) | Total Time 14.00(14.00)\n",
      "Iter 22850 | Time 24.7082(25.6057) | Bit/dim 3.5270(3.5275) | Xent 0.0803(0.0745) | Loss 3.5672(3.5648) | Error 0.0311(0.0265) Steps 1066(1075.35) | Grad Norm 3.6698(2.6380) | Total Time 14.00(14.00)\n",
      "Iter 22860 | Time 25.4800(25.6050) | Bit/dim 3.5160(3.5275) | Xent 0.0695(0.0738) | Loss 3.5508(3.5644) | Error 0.0244(0.0257) Steps 1066(1073.99) | Grad Norm 2.8686(2.6393) | Total Time 14.00(14.00)\n",
      "Iter 22870 | Time 25.9159(25.6198) | Bit/dim 3.5127(3.5252) | Xent 0.0630(0.0760) | Loss 3.5441(3.5632) | Error 0.0256(0.0269) Steps 1072(1073.02) | Grad Norm 2.7957(2.6978) | Total Time 14.00(14.00)\n",
      "Iter 22880 | Time 24.7504(25.4862) | Bit/dim 3.5317(3.5243) | Xent 0.0827(0.0759) | Loss 3.5730(3.5622) | Error 0.0322(0.0272) Steps 1066(1072.83) | Grad Norm 3.0210(2.6283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0416 | Time 123.1783, Epoch Time 1543.3006(1550.7896), Bit/dim 3.5379(best: 3.5357), Xent 1.4020, Loss 4.2389, Error 0.2424(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22890 | Time 25.5619(25.5681) | Bit/dim 3.5458(3.5247) | Xent 0.0754(0.0737) | Loss 3.5835(3.5615) | Error 0.0300(0.0263) Steps 1072(1073.66) | Grad Norm 2.2707(2.4592) | Total Time 14.00(14.00)\n",
      "Iter 22900 | Time 25.9197(25.5804) | Bit/dim 3.5596(3.5238) | Xent 0.0676(0.0737) | Loss 3.5935(3.5606) | Error 0.0256(0.0271) Steps 1072(1075.03) | Grad Norm 1.5410(2.3233) | Total Time 14.00(14.00)\n",
      "Iter 22910 | Time 25.6100(25.6278) | Bit/dim 3.5538(3.5270) | Xent 0.0663(0.0729) | Loss 3.5869(3.5635) | Error 0.0267(0.0265) Steps 1066(1075.33) | Grad Norm 3.2658(2.2355) | Total Time 14.00(14.00)\n",
      "Iter 22920 | Time 26.0661(25.5963) | Bit/dim 3.5075(3.5260) | Xent 0.0874(0.0717) | Loss 3.5511(3.5619) | Error 0.0256(0.0261) Steps 1072(1074.86) | Grad Norm 3.5662(2.2456) | Total Time 14.00(14.00)\n",
      "Iter 22930 | Time 25.8172(25.5854) | Bit/dim 3.5592(3.5254) | Xent 0.0628(0.0725) | Loss 3.5907(3.5617) | Error 0.0233(0.0268) Steps 1072(1075.24) | Grad Norm 2.1032(2.2859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0417 | Time 123.9280, Epoch Time 1552.2625(1550.8338), Bit/dim 3.5386(best: 3.5357), Xent 1.4247, Loss 4.2510, Error 0.2410(best: 0.2350)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 22940 | Time 25.5762(25.6701) | Bit/dim 3.5389(3.5274) | Xent 0.0679(0.0720) | Loss 3.5729(3.5634) | Error 0.0200(0.0262) Steps 1072(1074.95) | Grad Norm 1.4570(2.2330) | Total Time 14.00(14.00)\n",
      "Iter 22950 | Time 25.3435(25.6753) | Bit/dim 3.5112(3.5255) | Xent 0.0596(0.0720) | Loss 3.5410(3.5615) | Error 0.0233(0.0261) Steps 1090(1077.08) | Grad Norm 1.3976(2.1650) | Total Time 14.00(14.00)\n",
      "Iter 22960 | Time 26.2474(25.6667) | Bit/dim 3.5218(3.5262) | Xent 0.0824(0.0729) | Loss 3.5630(3.5626) | Error 0.0289(0.0263) Steps 1102(1076.19) | Grad Norm 2.3179(2.1808) | Total Time 14.00(14.00)\n",
      "Iter 22970 | Time 25.4636(25.6805) | Bit/dim 3.5479(3.5248) | Xent 0.0694(0.0749) | Loss 3.5826(3.5622) | Error 0.0222(0.0269) Steps 1066(1077.56) | Grad Norm 2.6302(2.2912) | Total Time 14.00(14.00)\n",
      "Iter 22980 | Time 25.1557(25.6460) | Bit/dim 3.5295(3.5250) | Xent 0.0565(0.0749) | Loss 3.5577(3.5625) | Error 0.0233(0.0270) Steps 1066(1077.27) | Grad Norm 1.8552(2.3255) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_run1/epoch_310_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
