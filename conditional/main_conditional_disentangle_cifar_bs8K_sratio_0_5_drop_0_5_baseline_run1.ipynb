{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_run1/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 1501 | Time 131.9712(74.6377) | Bit/dim 3.9756(3.9994) | Xent 1.2007(1.2558) | Loss 4.5759(4.6273) | Error 0.4343(0.4482) Steps 802(806.73) | Grad Norm 2.6350(5.9918) | Total Time 14.00(14.00)\n",
      "Iter 1502 | Time 73.9040(74.6157) | Bit/dim 3.9652(3.9984) | Xent 1.2111(1.2545) | Loss 4.5707(4.6256) | Error 0.4271(0.4475) Steps 802(806.59) | Grad Norm 2.1972(5.8780) | Total Time 14.00(14.00)\n",
      "Iter 1503 | Time 75.1091(74.6305) | Bit/dim 3.9694(3.9975) | Xent 1.2026(1.2529) | Loss 4.5706(4.6240) | Error 0.4391(0.4473) Steps 802(806.45) | Grad Norm 1.2939(5.7405) | Total Time 14.00(14.00)\n",
      "Iter 1504 | Time 73.6952(74.6025) | Bit/dim 3.9608(3.9964) | Xent 1.2172(1.2519) | Loss 4.5694(4.6223) | Error 0.4376(0.4470) Steps 802(806.32) | Grad Norm 1.1691(5.6033) | Total Time 14.00(14.00)\n",
      "Iter 1505 | Time 73.3825(74.5659) | Bit/dim 3.9646(3.9955) | Xent 1.1979(1.2502) | Loss 4.5635(4.6206) | Error 0.4277(0.4464) Steps 802(806.19) | Grad Norm 1.5671(5.4822) | Total Time 14.00(14.00)\n",
      "Iter 1506 | Time 75.0738(74.5811) | Bit/dim 3.9609(3.9944) | Xent 1.1949(1.2486) | Loss 4.5584(4.6187) | Error 0.4280(0.4459) Steps 802(806.06) | Grad Norm 1.6396(5.3669) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 44.4445, Epoch Time 563.7716(483.4405), Bit/dim 3.9643(best: inf), Xent 1.1648, Loss 4.5467, Error 0.4185(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 85.4549(74.9073) | Bit/dim 3.9700(3.9937) | Xent 1.2256(1.2479) | Loss 4.5828(4.6176) | Error 0.4417(0.4457) Steps 802(805.94) | Grad Norm 1.7809(5.2594) | Total Time 14.00(14.00)\n",
      "Iter 1508 | Time 74.9357(74.9082) | Bit/dim 3.9597(3.9927) | Xent 1.2012(1.2465) | Loss 4.5603(4.6159) | Error 0.4293(0.4452) Steps 802(805.82) | Grad Norm 1.6745(5.1518) | Total Time 14.00(14.00)\n",
      "Iter 1509 | Time 74.8477(74.9063) | Bit/dim 3.9613(3.9917) | Xent 1.1886(1.2447) | Loss 4.5556(4.6141) | Error 0.4247(0.4446) Steps 802(805.71) | Grad Norm 0.8626(5.0231) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 73.3480(74.8596) | Bit/dim 3.9675(3.9910) | Xent 1.2149(1.2438) | Loss 4.5750(4.6129) | Error 0.4315(0.4442) Steps 802(805.60) | Grad Norm 1.1527(4.9070) | Total Time 14.00(14.00)\n",
      "Iter 1511 | Time 75.1261(74.8676) | Bit/dim 3.9601(3.9901) | Xent 1.2076(1.2428) | Loss 4.5639(4.6115) | Error 0.4273(0.4437) Steps 802(805.49) | Grad Norm 1.5948(4.8077) | Total Time 14.00(14.00)\n",
      "Iter 1512 | Time 78.8578(74.9873) | Bit/dim 3.9588(3.9891) | Xent 1.1779(1.2408) | Loss 4.5478(4.6096) | Error 0.4220(0.4431) Steps 796(805.20) | Grad Norm 1.9535(4.7220) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 29.5578, Epoch Time 507.9170(484.1748), Bit/dim 3.9624(best: 3.9643), Xent 1.1588, Loss 4.5418, Error 0.4165(best: 0.4185)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 77.5078(75.0629) | Bit/dim 3.9730(3.9887) | Xent 1.1799(1.2390) | Loss 4.5629(4.6082) | Error 0.4230(0.4425) Steps 802(805.11) | Grad Norm 1.4593(4.6242) | Total Time 14.00(14.00)\n",
      "Iter 1514 | Time 76.2133(75.0974) | Bit/dim 3.9670(3.9880) | Xent 1.2157(1.2383) | Loss 4.5749(4.6072) | Error 0.4326(0.4422) Steps 802(805.01) | Grad Norm 1.1299(4.5193) | Total Time 14.00(14.00)\n",
      "Iter 1515 | Time 73.5181(75.0500) | Bit/dim 3.9536(3.9870) | Xent 1.2052(1.2373) | Loss 4.5562(4.6056) | Error 0.4343(0.4419) Steps 802(804.92) | Grad Norm 0.8799(4.4101) | Total Time 14.00(14.00)\n",
      "Iter 1516 | Time 75.8426(75.0738) | Bit/dim 3.9536(3.9860) | Xent 1.2051(1.2363) | Loss 4.5561(4.6041) | Error 0.4353(0.4417) Steps 802(804.84) | Grad Norm 1.1503(4.3123) | Total Time 14.00(14.00)\n",
      "Iter 1517 | Time 75.5811(75.0890) | Bit/dim 3.9563(3.9851) | Xent 1.1802(1.2346) | Loss 4.5464(4.6024) | Error 0.4250(0.4412) Steps 802(804.75) | Grad Norm 1.2726(4.2212) | Total Time 14.00(14.00)\n",
      "Iter 1518 | Time 75.5954(75.1042) | Bit/dim 3.9690(3.9846) | Xent 1.1754(1.2329) | Loss 4.5567(4.6010) | Error 0.4181(0.4405) Steps 802(804.67) | Grad Norm 1.3756(4.1358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 29.1612, Epoch Time 499.4035(484.6317), Bit/dim 3.9618(best: 3.9624), Xent 1.1582, Loss 4.5409, Error 0.4194(best: 0.4165)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 71.8184(75.0057) | Bit/dim 3.9565(3.9838) | Xent 1.1818(1.2313) | Loss 4.5473(4.5994) | Error 0.4215(0.4400) Steps 802(804.59) | Grad Norm 0.9926(4.0415) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 74.6171(74.9940) | Bit/dim 3.9680(3.9833) | Xent 1.1874(1.2300) | Loss 4.5617(4.5983) | Error 0.4300(0.4397) Steps 802(804.51) | Grad Norm 0.7353(3.9423) | Total Time 14.00(14.00)\n",
      "Iter 1521 | Time 72.6339(74.9232) | Bit/dim 3.9708(3.9829) | Xent 1.2099(1.2294) | Loss 4.5757(4.5976) | Error 0.4389(0.4396) Steps 802(804.44) | Grad Norm 0.9281(3.8519) | Total Time 14.00(14.00)\n",
      "Iter 1522 | Time 73.1551(74.8701) | Bit/dim 3.9491(3.9819) | Xent 1.2013(1.2286) | Loss 4.5497(4.5962) | Error 0.4241(0.4392) Steps 802(804.36) | Grad Norm 1.7266(3.7881) | Total Time 14.00(14.00)\n",
      "Iter 1523 | Time 73.5225(74.8297) | Bit/dim 3.9651(3.9814) | Xent 1.1834(1.2272) | Loss 4.5568(4.5950) | Error 0.4230(0.4387) Steps 802(804.29) | Grad Norm 1.8766(3.7308) | Total Time 14.00(14.00)\n",
      "Iter 1524 | Time 74.4859(74.8194) | Bit/dim 3.9564(3.9806) | Xent 1.2034(1.2265) | Loss 4.5581(4.5939) | Error 0.4319(0.4385) Steps 802(804.22) | Grad Norm 1.0660(3.6508) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 29.2168, Epoch Time 485.0525(484.6443), Bit/dim 3.9626(best: 3.9618), Xent 1.1547, Loss 4.5399, Error 0.4166(best: 0.4165)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 73.3582(74.7756) | Bit/dim 3.9552(3.9799) | Xent 1.2047(1.2258) | Loss 4.5576(4.5928) | Error 0.4327(0.4383) Steps 802(804.16) | Grad Norm 1.2489(3.5788) | Total Time 14.00(14.00)\n",
      "Iter 1526 | Time 74.6734(74.7725) | Bit/dim 3.9550(3.9791) | Xent 1.2004(1.2251) | Loss 4.5552(4.5917) | Error 0.4233(0.4379) Steps 802(804.09) | Grad Norm 1.2436(3.5087) | Total Time 14.00(14.00)\n",
      "Iter 1527 | Time 73.0635(74.7212) | Bit/dim 3.9646(3.9787) | Xent 1.1931(1.2241) | Loss 4.5612(4.5908) | Error 0.4286(0.4376) Steps 802(804.03) | Grad Norm 1.4672(3.4475) | Total Time 14.00(14.00)\n",
      "Iter 1528 | Time 74.7640(74.7225) | Bit/dim 3.9642(3.9783) | Xent 1.2024(1.2235) | Loss 4.5654(4.5900) | Error 0.4361(0.4375) Steps 802(803.97) | Grad Norm 1.3048(3.3832) | Total Time 14.00(14.00)\n",
      "Iter 1529 | Time 73.0155(74.6713) | Bit/dim 3.9617(3.9778) | Xent 1.1905(1.2225) | Loss 4.5570(4.5890) | Error 0.4225(0.4371) Steps 802(803.91) | Grad Norm 1.2542(3.3193) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 77.3541(74.7518) | Bit/dim 3.9544(3.9771) | Xent 1.1980(1.2217) | Loss 4.5534(4.5879) | Error 0.4300(0.4369) Steps 802(803.85) | Grad Norm 1.1511(3.2543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 28.8405, Epoch Time 490.7752(484.8282), Bit/dim 3.9609(best: 3.9618), Xent 1.1526, Loss 4.5372, Error 0.4146(best: 0.4165)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 74.2564(74.7369) | Bit/dim 3.9587(3.9765) | Xent 1.1916(1.2208) | Loss 4.5545(4.5869) | Error 0.4319(0.4367) Steps 802(803.80) | Grad Norm 0.9155(3.1841) | Total Time 14.00(14.00)\n",
      "Iter 1532 | Time 76.3081(74.7841) | Bit/dim 3.9636(3.9761) | Xent 1.1718(1.2194) | Loss 4.5495(4.5858) | Error 0.4209(0.4363) Steps 802(803.74) | Grad Norm 0.9212(3.1162) | Total Time 14.00(14.00)\n",
      "Iter 1533 | Time 76.2575(74.8283) | Bit/dim 3.9545(3.9755) | Xent 1.1716(1.2179) | Loss 4.5403(4.5844) | Error 0.4173(0.4357) Steps 808(803.87) | Grad Norm 1.2250(3.0595) | Total Time 14.00(14.00)\n",
      "Iter 1534 | Time 72.9342(74.7714) | Bit/dim 3.9602(3.9750) | Xent 1.2117(1.2178) | Loss 4.5661(4.5839) | Error 0.4374(0.4357) Steps 802(803.81) | Grad Norm 0.8951(2.9946) | Total Time 14.00(14.00)\n",
      "Iter 1535 | Time 75.7237(74.8000) | Bit/dim 3.9717(3.9749) | Xent 1.1930(1.2170) | Loss 4.5682(4.5834) | Error 0.4266(0.4355) Steps 802(803.76) | Grad Norm 0.8829(2.9312) | Total Time 14.00(14.00)\n",
      "Iter 1536 | Time 73.6143(74.7644) | Bit/dim 3.9556(3.9743) | Xent 1.2064(1.2167) | Loss 4.5588(4.5827) | Error 0.4260(0.4352) Steps 802(803.71) | Grad Norm 0.9367(2.8714) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 28.9601, Epoch Time 493.7057(485.0946), Bit/dim 3.9608(best: 3.9609), Xent 1.1523, Loss 4.5370, Error 0.4154(best: 0.4146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 74.8751(74.7678) | Bit/dim 3.9595(3.9739) | Xent 1.2076(1.2164) | Loss 4.5632(4.5821) | Error 0.4384(0.4353) Steps 802(803.66) | Grad Norm 0.6182(2.8038) | Total Time 14.00(14.00)\n",
      "Iter 1538 | Time 75.8949(74.8016) | Bit/dim 3.9561(3.9734) | Xent 1.1842(1.2155) | Loss 4.5482(4.5811) | Error 0.4301(0.4351) Steps 802(803.61) | Grad Norm 1.1743(2.7549) | Total Time 14.00(14.00)\n",
      "Iter 1539 | Time 76.2686(74.8456) | Bit/dim 3.9685(3.9732) | Xent 1.1699(1.2141) | Loss 4.5535(4.5803) | Error 0.4191(0.4346) Steps 802(803.56) | Grad Norm 0.8508(2.6978) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 77.2597(74.9180) | Bit/dim 3.9411(3.9723) | Xent 1.2032(1.2138) | Loss 4.5427(4.5791) | Error 0.4317(0.4346) Steps 802(803.51) | Grad Norm 0.6466(2.6362) | Total Time 14.00(14.00)\n",
      "Iter 1541 | Time 73.3475(74.8709) | Bit/dim 3.9639(3.9720) | Xent 1.1866(1.2129) | Loss 4.5572(4.5785) | Error 0.4224(0.4342) Steps 802(803.47) | Grad Norm 1.4730(2.6013) | Total Time 14.00(14.00)\n",
      "Iter 1542 | Time 77.1647(74.9397) | Bit/dim 3.9640(3.9718) | Xent 1.1802(1.2120) | Loss 4.5541(4.5777) | Error 0.4177(0.4337) Steps 802(803.42) | Grad Norm 0.6091(2.5416) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 29.0531, Epoch Time 499.7100(485.5330), Bit/dim 3.9592(best: 3.9608), Xent 1.1509, Loss 4.5347, Error 0.4116(best: 0.4146)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 71.5789(74.8389) | Bit/dim 3.9549(3.9713) | Xent 1.1889(1.2113) | Loss 4.5494(4.5769) | Error 0.4219(0.4333) Steps 802(803.38) | Grad Norm 0.6356(2.4844) | Total Time 14.00(14.00)\n",
      "Iter 1544 | Time 75.7353(74.8658) | Bit/dim 3.9551(3.9708) | Xent 1.2116(1.2113) | Loss 4.5610(4.5764) | Error 0.4326(0.4333) Steps 802(803.34) | Grad Norm 0.6840(2.4304) | Total Time 14.00(14.00)\n",
      "Iter 1545 | Time 75.8112(74.8941) | Bit/dim 3.9634(3.9706) | Xent 1.1903(1.2106) | Loss 4.5585(4.5759) | Error 0.4347(0.4334) Steps 802(803.30) | Grad Norm 0.8808(2.3839) | Total Time 14.00(14.00)\n",
      "Iter 1546 | Time 75.1459(74.9017) | Bit/dim 3.9613(3.9703) | Xent 1.1860(1.2099) | Loss 4.5543(4.5752) | Error 0.4225(0.4330) Steps 802(803.26) | Grad Norm 0.6040(2.3305) | Total Time 14.00(14.00)\n",
      "Iter 1547 | Time 74.4035(74.8867) | Bit/dim 3.9550(3.9698) | Xent 1.1774(1.2089) | Loss 4.5437(4.5743) | Error 0.4223(0.4327) Steps 802(803.22) | Grad Norm 0.8472(2.2860) | Total Time 14.00(14.00)\n",
      "Iter 1548 | Time 76.5017(74.9352) | Bit/dim 3.9615(3.9696) | Xent 1.1905(1.2084) | Loss 4.5567(4.5738) | Error 0.4223(0.4324) Steps 802(803.18) | Grad Norm 0.8082(2.2417) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 28.6998, Epoch Time 493.4155(485.7695), Bit/dim 3.9587(best: 3.9592), Xent 1.1500, Loss 4.5338, Error 0.4142(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 74.1476(74.9116) | Bit/dim 3.9516(3.9690) | Xent 1.2052(1.2083) | Loss 4.5542(4.5732) | Error 0.4284(0.4323) Steps 802(803.15) | Grad Norm 0.6341(2.1934) | Total Time 14.00(14.00)\n",
      "Iter 1550 | Time 77.1599(74.9790) | Bit/dim 3.9607(3.9688) | Xent 1.1781(1.2074) | Loss 4.5498(4.5725) | Error 0.4203(0.4319) Steps 802(803.11) | Grad Norm 0.8853(2.1542) | Total Time 14.00(14.00)\n",
      "Iter 1551 | Time 75.6888(75.0003) | Bit/dim 3.9553(3.9684) | Xent 1.1760(1.2064) | Loss 4.5433(4.5716) | Error 0.4175(0.4315) Steps 802(803.08) | Grad Norm 0.5763(2.1069) | Total Time 14.00(14.00)\n",
      "Iter 1552 | Time 74.7517(74.9929) | Bit/dim 3.9599(3.9681) | Xent 1.1741(1.2055) | Loss 4.5470(4.5709) | Error 0.4226(0.4312) Steps 802(803.05) | Grad Norm 0.7064(2.0648) | Total Time 14.00(14.00)\n",
      "Iter 1553 | Time 75.9200(75.0207) | Bit/dim 3.9658(3.9681) | Xent 1.2119(1.2057) | Loss 4.5717(4.5709) | Error 0.4375(0.4314) Steps 802(803.02) | Grad Norm 0.8001(2.0269) | Total Time 14.00(14.00)\n",
      "Iter 1554 | Time 72.8026(74.9541) | Bit/dim 3.9595(3.9678) | Xent 1.1868(1.2051) | Loss 4.5529(4.5703) | Error 0.4243(0.4312) Steps 802(802.99) | Grad Norm 0.7244(1.9878) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 29.0433, Epoch Time 495.0239(486.0471), Bit/dim 3.9586(best: 3.9587), Xent 1.1495, Loss 4.5333, Error 0.4159(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 73.7882(74.9191) | Bit/dim 3.9611(3.9676) | Xent 1.1948(1.2048) | Loss 4.5585(4.5700) | Error 0.4300(0.4312) Steps 802(802.96) | Grad Norm 0.7665(1.9512) | Total Time 14.00(14.00)\n",
      "Iter 1556 | Time 75.2716(74.9297) | Bit/dim 3.9660(3.9675) | Xent 1.2034(1.2047) | Loss 4.5677(4.5699) | Error 0.4236(0.4309) Steps 802(802.93) | Grad Norm 0.8023(1.9167) | Total Time 14.00(14.00)\n",
      "Iter 1557 | Time 75.4236(74.9445) | Bit/dim 3.9512(3.9671) | Xent 1.1640(1.2035) | Loss 4.5332(4.5688) | Error 0.4189(0.4306) Steps 802(802.90) | Grad Norm 0.8244(1.8839) | Total Time 14.00(14.00)\n",
      "Iter 1558 | Time 72.9261(74.8840) | Bit/dim 3.9592(3.9668) | Xent 1.1958(1.2033) | Loss 4.5571(4.5685) | Error 0.4311(0.4306) Steps 802(802.87) | Grad Norm 0.8452(1.8528) | Total Time 14.00(14.00)\n",
      "Iter 1559 | Time 76.7100(74.9388) | Bit/dim 3.9492(3.9663) | Xent 1.1945(1.2030) | Loss 4.5465(4.5678) | Error 0.4267(0.4305) Steps 802(802.85) | Grad Norm 1.0784(1.8296) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 75.7974(74.9645) | Bit/dim 3.9560(3.9660) | Xent 1.1745(1.2022) | Loss 4.5432(4.5671) | Error 0.4153(0.4300) Steps 802(802.82) | Grad Norm 1.1793(1.8100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 29.2642, Epoch Time 494.9243(486.3134), Bit/dim 3.9582(best: 3.9586), Xent 1.1463, Loss 4.5314, Error 0.4119(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 73.2660(74.9136) | Bit/dim 3.9452(3.9654) | Xent 1.1993(1.2021) | Loss 4.5449(4.5664) | Error 0.4217(0.4298) Steps 802(802.80) | Grad Norm 0.8839(1.7823) | Total Time 14.00(14.00)\n",
      "Iter 1562 | Time 73.5234(74.8719) | Bit/dim 3.9628(3.9653) | Xent 1.1860(1.2016) | Loss 4.5559(4.5661) | Error 0.4209(0.4295) Steps 802(802.77) | Grad Norm 0.6114(1.7471) | Total Time 14.00(14.00)\n",
      "Iter 1563 | Time 75.7799(74.8991) | Bit/dim 3.9700(3.9654) | Xent 1.1646(1.2005) | Loss 4.5523(4.5657) | Error 0.4201(0.4292) Steps 802(802.75) | Grad Norm 1.1997(1.7307) | Total Time 14.00(14.00)\n",
      "Iter 1564 | Time 73.2192(74.8487) | Bit/dim 3.9539(3.9651) | Xent 1.1812(1.1999) | Loss 4.5445(4.5650) | Error 0.4203(0.4289) Steps 802(802.73) | Grad Norm 1.1385(1.7129) | Total Time 14.00(14.00)\n",
      "Iter 1565 | Time 75.4675(74.8673) | Bit/dim 3.9606(3.9649) | Xent 1.1817(1.1994) | Loss 4.5514(4.5646) | Error 0.4169(0.4286) Steps 808(802.89) | Grad Norm 0.5979(1.6795) | Total Time 14.00(14.00)\n",
      "Iter 1566 | Time 74.1937(74.8471) | Bit/dim 3.9452(3.9643) | Xent 1.1840(1.1989) | Loss 4.5372(4.5638) | Error 0.4225(0.4284) Steps 802(802.86) | Grad Norm 0.8308(1.6540) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 29.1198, Epoch Time 490.2072(486.4303), Bit/dim 3.9578(best: 3.9582), Xent 1.1487, Loss 4.5321, Error 0.4150(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 75.2154(74.8581) | Bit/dim 3.9427(3.9637) | Xent 1.1773(1.1983) | Loss 4.5314(4.5628) | Error 0.4193(0.4281) Steps 808(803.01) | Grad Norm 0.6326(1.6234) | Total Time 14.00(14.00)\n",
      "Iter 1568 | Time 73.7133(74.8238) | Bit/dim 3.9564(3.9635) | Xent 1.1763(1.1976) | Loss 4.5445(4.5623) | Error 0.4239(0.4280) Steps 802(802.98) | Grad Norm 0.7820(1.5981) | Total Time 14.00(14.00)\n",
      "Iter 1569 | Time 76.7618(74.8819) | Bit/dim 3.9485(3.9630) | Xent 1.1693(1.1968) | Loss 4.5332(4.5614) | Error 0.4211(0.4278) Steps 802(802.95) | Grad Norm 0.8490(1.5757) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 76.3251(74.9252) | Bit/dim 3.9595(3.9629) | Xent 1.1876(1.1965) | Loss 4.5532(4.5612) | Error 0.4210(0.4276) Steps 802(802.92) | Grad Norm 1.0951(1.5613) | Total Time 14.00(14.00)\n",
      "Iter 1571 | Time 74.6623(74.9173) | Bit/dim 3.9653(3.9630) | Xent 1.1737(1.1958) | Loss 4.5521(4.5609) | Error 0.4209(0.4274) Steps 802(802.90) | Grad Norm 0.9383(1.5426) | Total Time 14.00(14.00)\n",
      "Iter 1572 | Time 71.6350(74.8188) | Bit/dim 3.9638(3.9630) | Xent 1.1993(1.1959) | Loss 4.5634(4.5610) | Error 0.4223(0.4272) Steps 802(802.87) | Grad Norm 0.6233(1.5150) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 28.7544, Epoch Time 492.6203(486.6160), Bit/dim 3.9574(best: 3.9578), Xent 1.1443, Loss 4.5296, Error 0.4128(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 75.5513(74.8408) | Bit/dim 3.9591(3.9629) | Xent 1.1708(1.1951) | Loss 4.5445(4.5605) | Error 0.4171(0.4269) Steps 802(802.84) | Grad Norm 0.6638(1.4895) | Total Time 14.00(14.00)\n",
      "Iter 1574 | Time 75.2975(74.8545) | Bit/dim 3.9546(3.9627) | Xent 1.1548(1.1939) | Loss 4.5321(4.5596) | Error 0.4141(0.4265) Steps 802(802.82) | Grad Norm 0.9907(1.4745) | Total Time 14.00(14.00)\n",
      "Iter 1575 | Time 75.3923(74.8707) | Bit/dim 3.9617(3.9626) | Xent 1.1770(1.1934) | Loss 4.5502(4.5593) | Error 0.4250(0.4265) Steps 802(802.79) | Grad Norm 0.6155(1.4487) | Total Time 14.00(14.00)\n",
      "Iter 1576 | Time 76.0258(74.9053) | Bit/dim 3.9576(3.9625) | Xent 1.1955(1.1935) | Loss 4.5553(4.5592) | Error 0.4263(0.4265) Steps 802(802.77) | Grad Norm 0.6598(1.4251) | Total Time 14.00(14.00)\n",
      "Iter 1577 | Time 71.7924(74.8119) | Bit/dim 3.9501(3.9621) | Xent 1.2087(1.1939) | Loss 4.5545(4.5591) | Error 0.4353(0.4268) Steps 802(802.75) | Grad Norm 0.7826(1.4058) | Total Time 14.00(14.00)\n",
      "Iter 1578 | Time 73.3648(74.7685) | Bit/dim 3.9593(3.9620) | Xent 1.1855(1.1937) | Loss 4.5521(4.5589) | Error 0.4264(0.4267) Steps 802(802.72) | Grad Norm 0.7006(1.3846) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 29.4824, Epoch Time 492.6408(486.7967), Bit/dim 3.9574(best: 3.9574), Xent 1.1454, Loss 4.5301, Error 0.4140(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 75.0280(74.7763) | Bit/dim 3.9617(3.9620) | Xent 1.1923(1.1937) | Loss 4.5579(4.5588) | Error 0.4213(0.4266) Steps 802(802.70) | Grad Norm 0.7454(1.3654) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 74.6051(74.7712) | Bit/dim 3.9471(3.9616) | Xent 1.1719(1.1930) | Loss 4.5331(4.5581) | Error 0.4166(0.4263) Steps 802(802.68) | Grad Norm 0.7978(1.3484) | Total Time 14.00(14.00)\n",
      "Iter 1581 | Time 76.6473(74.8274) | Bit/dim 3.9594(3.9615) | Xent 1.1772(1.1925) | Loss 4.5480(4.5578) | Error 0.4194(0.4261) Steps 802(802.66) | Grad Norm 0.5519(1.3245) | Total Time 14.00(14.00)\n",
      "Iter 1582 | Time 72.9225(74.7703) | Bit/dim 3.9446(3.9610) | Xent 1.1778(1.1921) | Loss 4.5334(4.5570) | Error 0.4147(0.4257) Steps 802(802.64) | Grad Norm 0.6161(1.3033) | Total Time 14.00(14.00)\n",
      "Iter 1583 | Time 72.2393(74.6944) | Bit/dim 3.9597(3.9610) | Xent 1.1816(1.1918) | Loss 4.5505(4.5568) | Error 0.4225(0.4256) Steps 802(802.62) | Grad Norm 0.8579(1.2899) | Total Time 14.00(14.00)\n",
      "Iter 1584 | Time 72.9886(74.6432) | Bit/dim 3.9563(3.9608) | Xent 1.1840(1.1915) | Loss 4.5483(4.5566) | Error 0.4215(0.4255) Steps 802(802.60) | Grad Norm 0.7366(1.2733) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 29.1189, Epoch Time 489.1842(486.8683), Bit/dim 3.9568(best: 3.9574), Xent 1.1446, Loss 4.5291, Error 0.4111(best: 0.4116)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 74.8244(74.6486) | Bit/dim 3.9584(3.9607) | Xent 1.1995(1.1918) | Loss 4.5581(4.5566) | Error 0.4207(0.4254) Steps 802(802.59) | Grad Norm 1.2146(1.2715) | Total Time 14.00(14.00)\n",
      "Iter 1586 | Time 73.2279(74.6060) | Bit/dim 3.9630(3.9608) | Xent 1.1751(1.1913) | Loss 4.5506(4.5564) | Error 0.4164(0.4251) Steps 802(802.57) | Grad Norm 1.0679(1.2654) | Total Time 14.00(14.00)\n",
      "Iter 1587 | Time 73.2167(74.5643) | Bit/dim 3.9547(3.9606) | Xent 1.1967(1.1914) | Loss 4.5530(4.5563) | Error 0.4259(0.4251) Steps 802(802.55) | Grad Norm 0.9048(1.2546) | Total Time 14.00(14.00)\n",
      "Iter 1588 | Time 71.7370(74.4795) | Bit/dim 3.9532(3.9604) | Xent 1.1902(1.1914) | Loss 4.5483(4.5561) | Error 0.4230(0.4251) Steps 802(802.53) | Grad Norm 0.5540(1.2336) | Total Time 14.00(14.00)\n",
      "Iter 1589 | Time 73.1019(74.4382) | Bit/dim 3.9584(3.9603) | Xent 1.1520(1.1902) | Loss 4.5344(4.5554) | Error 0.4116(0.4247) Steps 802(802.52) | Grad Norm 0.7213(1.2182) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 73.2330(74.4020) | Bit/dim 3.9472(3.9599) | Xent 1.1827(1.1900) | Loss 4.5385(4.5549) | Error 0.4239(0.4246) Steps 802(802.50) | Grad Norm 1.2578(1.2194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 29.2465, Epoch Time 484.3543(486.7929), Bit/dim 3.9561(best: 3.9568), Xent 1.1418, Loss 4.5270, Error 0.4115(best: 0.4111)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 73.6916(74.3807) | Bit/dim 3.9526(3.9597) | Xent 1.1870(1.1899) | Loss 4.5461(4.5547) | Error 0.4239(0.4246) Steps 802(802.49) | Grad Norm 0.8575(1.2086) | Total Time 14.00(14.00)\n",
      "Iter 1592 | Time 77.4177(74.4718) | Bit/dim 3.9528(3.9595) | Xent 1.1891(1.1899) | Loss 4.5474(4.5545) | Error 0.4313(0.4248) Steps 802(802.47) | Grad Norm 0.6873(1.1929) | Total Time 14.00(14.00)\n",
      "Iter 1593 | Time 75.3796(74.4991) | Bit/dim 3.9546(3.9594) | Xent 1.1872(1.1898) | Loss 4.5482(4.5543) | Error 0.4243(0.4248) Steps 802(802.46) | Grad Norm 0.5661(1.1741) | Total Time 14.00(14.00)\n",
      "Iter 1594 | Time 75.3892(74.5258) | Bit/dim 3.9701(3.9597) | Xent 1.1976(1.1900) | Loss 4.5689(4.5547) | Error 0.4224(0.4247) Steps 802(802.45) | Grad Norm 0.6696(1.1590) | Total Time 14.00(14.00)\n",
      "Iter 1595 | Time 76.4802(74.5844) | Bit/dim 3.9441(3.9592) | Xent 1.1720(1.1895) | Loss 4.5301(4.5540) | Error 0.4189(0.4245) Steps 802(802.43) | Grad Norm 0.5285(1.1401) | Total Time 14.00(14.00)\n",
      "Iter 1596 | Time 73.9657(74.5658) | Bit/dim 3.9500(3.9589) | Xent 1.1769(1.1891) | Loss 4.5384(4.5535) | Error 0.4237(0.4245) Steps 802(802.42) | Grad Norm 0.9601(1.1347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 29.3758, Epoch Time 497.5441(487.1154), Bit/dim 3.9559(best: 3.9561), Xent 1.1435, Loss 4.5277, Error 0.4121(best: 0.4111)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 75.0277(74.5797) | Bit/dim 3.9551(3.9588) | Xent 1.1891(1.1891) | Loss 4.5496(4.5534) | Error 0.4229(0.4245) Steps 802(802.41) | Grad Norm 0.6359(1.1197) | Total Time 14.00(14.00)\n",
      "Iter 1598 | Time 74.8641(74.5882) | Bit/dim 3.9478(3.9585) | Xent 1.1908(1.1892) | Loss 4.5432(4.5531) | Error 0.4254(0.4245) Steps 802(802.39) | Grad Norm 0.5057(1.1013) | Total Time 14.00(14.00)\n",
      "Iter 1599 | Time 75.0424(74.6018) | Bit/dim 3.9584(3.9585) | Xent 1.1708(1.1886) | Loss 4.5438(4.5528) | Error 0.4191(0.4243) Steps 802(802.38) | Grad Norm 0.6236(1.0870) | Total Time 14.00(14.00)\n",
      "Iter 1600 | Time 74.0661(74.5858) | Bit/dim 3.9504(3.9583) | Xent 1.1573(1.1877) | Loss 4.5291(4.5521) | Error 0.4117(0.4240) Steps 802(802.37) | Grad Norm 0.5113(1.0697) | Total Time 14.00(14.00)\n",
      "Iter 1601 | Time 73.4273(74.5510) | Bit/dim 3.9626(3.9584) | Xent 1.1903(1.1878) | Loss 4.5577(4.5523) | Error 0.4200(0.4238) Steps 802(802.36) | Grad Norm 0.9727(1.0668) | Total Time 14.00(14.00)\n",
      "Iter 1602 | Time 76.0337(74.5955) | Bit/dim 3.9527(3.9582) | Xent 1.1608(1.1869) | Loss 4.5331(4.5517) | Error 0.4214(0.4238) Steps 808(802.53) | Grad Norm 0.9596(1.0636) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 29.5012, Epoch Time 493.3116(487.3013), Bit/dim 3.9554(best: 3.9559), Xent 1.1397, Loss 4.5253, Error 0.4103(best: 0.4111)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 72.9951(74.5475) | Bit/dim 3.9517(3.9580) | Xent 1.1829(1.1868) | Loss 4.5431(4.5514) | Error 0.4195(0.4236) Steps 802(802.51) | Grad Norm 0.7237(1.0534) | Total Time 14.00(14.00)\n",
      "Iter 1604 | Time 76.5991(74.6090) | Bit/dim 3.9479(3.9577) | Xent 1.1793(1.1866) | Loss 4.5375(4.5510) | Error 0.4179(0.4235) Steps 802(802.50) | Grad Norm 0.6114(1.0401) | Total Time 14.00(14.00)\n",
      "Iter 1605 | Time 74.0649(74.5927) | Bit/dim 3.9568(3.9577) | Xent 1.1871(1.1866) | Loss 4.5504(4.5510) | Error 0.4233(0.4235) Steps 802(802.48) | Grad Norm 1.0754(1.0412) | Total Time 14.00(14.00)\n",
      "Iter 1606 | Time 74.3283(74.5848) | Bit/dim 3.9478(3.9574) | Xent 1.1532(1.1856) | Loss 4.5244(4.5502) | Error 0.4126(0.4231) Steps 802(802.47) | Grad Norm 0.8057(1.0341) | Total Time 14.00(14.00)\n",
      "Iter 1607 | Time 74.1668(74.5722) | Bit/dim 3.9567(3.9574) | Xent 1.1803(1.1855) | Loss 4.5468(4.5501) | Error 0.4215(0.4231) Steps 802(802.45) | Grad Norm 1.4082(1.0453) | Total Time 14.00(14.00)\n",
      "Iter 1608 | Time 75.5626(74.6020) | Bit/dim 3.9640(3.9576) | Xent 1.1861(1.1855) | Loss 4.5571(4.5503) | Error 0.4260(0.4232) Steps 808(802.62) | Grad Norm 0.8726(1.0401) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 29.1981, Epoch Time 492.3349(487.4523), Bit/dim 3.9544(best: 3.9554), Xent 1.1406, Loss 4.5247, Error 0.4101(best: 0.4103)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 73.8014(74.5779) | Bit/dim 3.9663(3.9578) | Xent 1.1745(1.1851) | Loss 4.5536(4.5504) | Error 0.4161(0.4230) Steps 802(802.60) | Grad Norm 1.3047(1.0481) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 74.3899(74.5723) | Bit/dim 3.9388(3.9573) | Xent 1.1690(1.1847) | Loss 4.5233(4.5496) | Error 0.4209(0.4229) Steps 802(802.58) | Grad Norm 0.7588(1.0394) | Total Time 14.00(14.00)\n",
      "Iter 1611 | Time 74.8205(74.5797) | Bit/dim 3.9435(3.9568) | Xent 1.1804(1.1845) | Loss 4.5337(4.5491) | Error 0.4204(0.4228) Steps 802(802.57) | Grad Norm 0.8492(1.0337) | Total Time 14.00(14.00)\n",
      "Iter 1612 | Time 75.1204(74.5960) | Bit/dim 3.9524(3.9567) | Xent 1.1612(1.1838) | Loss 4.5330(4.5486) | Error 0.4179(0.4227) Steps 802(802.55) | Grad Norm 0.6311(1.0216) | Total Time 14.00(14.00)\n",
      "Iter 1613 | Time 73.6359(74.5672) | Bit/dim 3.9671(3.9570) | Xent 1.1813(1.1838) | Loss 4.5577(4.5489) | Error 0.4195(0.4226) Steps 802(802.53) | Grad Norm 0.8418(1.0162) | Total Time 14.00(14.00)\n",
      "Iter 1614 | Time 75.4361(74.5932) | Bit/dim 3.9554(3.9570) | Xent 1.1705(1.1834) | Loss 4.5406(4.5487) | Error 0.4164(0.4224) Steps 808(802.70) | Grad Norm 0.6726(1.0059) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 29.3071, Epoch Time 491.9117(487.5861), Bit/dim 3.9542(best: 3.9544), Xent 1.1374, Loss 4.5229, Error 0.4085(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 74.3123(74.5848) | Bit/dim 3.9586(3.9570) | Xent 1.1981(1.1838) | Loss 4.5577(4.5489) | Error 0.4324(0.4227) Steps 808(802.86) | Grad Norm 0.6866(0.9963) | Total Time 14.00(14.00)\n",
      "Iter 1616 | Time 74.1789(74.5726) | Bit/dim 3.9539(3.9569) | Xent 1.1580(1.1830) | Loss 4.5329(4.5484) | Error 0.4125(0.4224) Steps 802(802.83) | Grad Norm 0.5777(0.9838) | Total Time 14.00(14.00)\n",
      "Iter 1617 | Time 73.7507(74.5480) | Bit/dim 3.9499(3.9567) | Xent 1.1610(1.1824) | Loss 4.5304(4.5479) | Error 0.4149(0.4222) Steps 808(802.99) | Grad Norm 0.8361(0.9793) | Total Time 14.00(14.00)\n",
      "Iter 1618 | Time 74.5112(74.5469) | Bit/dim 3.9491(3.9565) | Xent 1.1751(1.1821) | Loss 4.5366(4.5476) | Error 0.4240(0.4222) Steps 802(802.96) | Grad Norm 1.0417(0.9812) | Total Time 14.00(14.00)\n",
      "Iter 1619 | Time 75.0498(74.5619) | Bit/dim 3.9564(3.9565) | Xent 1.1674(1.1817) | Loss 4.5401(4.5473) | Error 0.4187(0.4221) Steps 802(802.93) | Grad Norm 0.7743(0.9750) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 75.0906(74.5778) | Bit/dim 3.9525(3.9564) | Xent 1.1887(1.1819) | Loss 4.5469(4.5473) | Error 0.4265(0.4222) Steps 802(802.90) | Grad Norm 0.6649(0.9657) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 29.2361, Epoch Time 491.8512(487.7141), Bit/dim 3.9549(best: 3.9542), Xent 1.1383, Loss 4.5240, Error 0.4082(best: 0.4085)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 75.3519(74.6010) | Bit/dim 3.9610(3.9565) | Xent 1.1785(1.1818) | Loss 4.5502(4.5474) | Error 0.4239(0.4223) Steps 802(802.87) | Grad Norm 0.6839(0.9573) | Total Time 14.00(14.00)\n",
      "Iter 1622 | Time 72.7475(74.5454) | Bit/dim 3.9466(3.9562) | Xent 1.1790(1.1817) | Loss 4.5361(4.5471) | Error 0.4187(0.4222) Steps 808(803.03) | Grad Norm 0.7749(0.9518) | Total Time 14.00(14.00)\n",
      "Iter 1623 | Time 74.5974(74.5470) | Bit/dim 3.9530(3.9561) | Xent 1.1622(1.1811) | Loss 4.5341(4.5467) | Error 0.4137(0.4219) Steps 808(803.18) | Grad Norm 0.6269(0.9420) | Total Time 14.00(14.00)\n",
      "Iter 1624 | Time 73.3884(74.5122) | Bit/dim 3.9520(3.9560) | Xent 1.1629(1.1806) | Loss 4.5335(4.5463) | Error 0.4177(0.4218) Steps 808(803.32) | Grad Norm 0.6360(0.9329) | Total Time 14.00(14.00)\n",
      "Iter 1625 | Time 74.8950(74.5237) | Bit/dim 3.9565(3.9560) | Xent 1.1918(1.1809) | Loss 4.5524(4.5465) | Error 0.4209(0.4218) Steps 808(803.46) | Grad Norm 1.0193(0.9354) | Total Time 14.00(14.00)\n",
      "Iter 1626 | Time 76.2547(74.5756) | Bit/dim 3.9475(3.9558) | Xent 1.1600(1.1803) | Loss 4.5275(4.5459) | Error 0.4090(0.4214) Steps 808(803.60) | Grad Norm 0.7311(0.9293) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 29.4398, Epoch Time 492.1391(487.8468), Bit/dim 3.9539(best: 3.9542), Xent 1.1355, Loss 4.5216, Error 0.4062(best: 0.4082)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 73.9171(74.5559) | Bit/dim 3.9603(3.9559) | Xent 1.1747(1.1801) | Loss 4.5476(4.5460) | Error 0.4183(0.4213) Steps 802(803.55) | Grad Norm 0.6154(0.9199) | Total Time 14.00(14.00)\n",
      "Iter 1628 | Time 71.4696(74.4633) | Bit/dim 3.9571(3.9559) | Xent 1.1863(1.1803) | Loss 4.5503(4.5461) | Error 0.4289(0.4215) Steps 808(803.68) | Grad Norm 1.0210(0.9229) | Total Time 14.00(14.00)\n",
      "Iter 1629 | Time 74.8048(74.4735) | Bit/dim 3.9448(3.9556) | Xent 1.1645(1.1799) | Loss 4.5270(4.5455) | Error 0.4157(0.4214) Steps 808(803.81) | Grad Norm 0.7364(0.9173) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 75.0790(74.4917) | Bit/dim 3.9589(3.9557) | Xent 1.1819(1.1799) | Loss 4.5499(4.5456) | Error 0.4187(0.4213) Steps 802(803.76) | Grad Norm 0.7420(0.9121) | Total Time 14.00(14.00)\n",
      "Iter 1631 | Time 71.5716(74.4041) | Bit/dim 3.9515(3.9556) | Xent 1.1673(1.1795) | Loss 4.5351(4.5453) | Error 0.4129(0.4210) Steps 802(803.70) | Grad Norm 0.9119(0.9121) | Total Time 14.00(14.00)\n",
      "Iter 1632 | Time 74.9432(74.4203) | Bit/dim 3.9503(3.9554) | Xent 1.1778(1.1795) | Loss 4.5392(4.5451) | Error 0.4261(0.4212) Steps 808(803.83) | Grad Norm 0.5870(0.9023) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 29.3966, Epoch Time 486.7242(487.8131), Bit/dim 3.9535(best: 3.9539), Xent 1.1361, Loss 4.5215, Error 0.4084(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 73.9312(74.4056) | Bit/dim 3.9570(3.9555) | Xent 1.1749(1.1793) | Loss 4.5445(4.5451) | Error 0.4177(0.4211) Steps 808(803.96) | Grad Norm 0.7777(0.8986) | Total Time 14.00(14.00)\n",
      "Iter 1634 | Time 74.4946(74.4083) | Bit/dim 3.9541(3.9554) | Xent 1.1621(1.1788) | Loss 4.5351(4.5448) | Error 0.4183(0.4210) Steps 802(803.90) | Grad Norm 0.5387(0.8878) | Total Time 14.00(14.00)\n",
      "Iter 1635 | Time 73.8264(74.3908) | Bit/dim 3.9561(3.9554) | Xent 1.1764(1.1788) | Loss 4.5443(4.5448) | Error 0.4203(0.4210) Steps 808(804.02) | Grad Norm 0.9846(0.8907) | Total Time 14.00(14.00)\n",
      "Iter 1636 | Time 74.4588(74.3929) | Bit/dim 3.9564(3.9555) | Xent 1.1601(1.1782) | Loss 4.5365(4.5446) | Error 0.4130(0.4207) Steps 808(804.14) | Grad Norm 0.8247(0.8887) | Total Time 14.00(14.00)\n",
      "Iter 1637 | Time 74.7631(74.4040) | Bit/dim 3.9476(3.9552) | Xent 1.1425(1.1771) | Loss 4.5189(4.5438) | Error 0.4044(0.4202) Steps 808(804.26) | Grad Norm 0.6481(0.8815) | Total Time 14.00(14.00)\n",
      "Iter 1638 | Time 74.8850(74.4184) | Bit/dim 3.9432(3.9549) | Xent 1.1732(1.1770) | Loss 4.5298(4.5434) | Error 0.4181(0.4202) Steps 802(804.19) | Grad Norm 0.5903(0.8728) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 29.1765, Epoch Time 490.9752(487.9080), Bit/dim 3.9534(best: 3.9535), Xent 1.1349, Loss 4.5209, Error 0.4090(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1639 | Time 73.9822(74.4053) | Bit/dim 3.9414(3.9545) | Xent 1.1648(1.1766) | Loss 4.5238(4.5428) | Error 0.4231(0.4203) Steps 802(804.12) | Grad Norm 0.9226(0.8742) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 72.3628(74.3440) | Bit/dim 3.9469(3.9542) | Xent 1.1821(1.1768) | Loss 4.5380(4.5426) | Error 0.4181(0.4202) Steps 802(804.06) | Grad Norm 0.7669(0.8710) | Total Time 14.00(14.00)\n",
      "Iter 1641 | Time 74.7059(74.3549) | Bit/dim 3.9452(3.9540) | Xent 1.1711(1.1766) | Loss 4.5308(4.5423) | Error 0.4267(0.4204) Steps 802(804.00) | Grad Norm 0.6180(0.8634) | Total Time 14.00(14.00)\n",
      "Iter 1642 | Time 76.3461(74.4146) | Bit/dim 3.9618(3.9542) | Xent 1.1812(1.1768) | Loss 4.5524(4.5426) | Error 0.4183(0.4203) Steps 802(803.94) | Grad Norm 0.6702(0.8576) | Total Time 14.00(14.00)\n",
      "Iter 1643 | Time 73.8425(74.3975) | Bit/dim 3.9530(3.9542) | Xent 1.1478(1.1759) | Loss 4.5269(4.5421) | Error 0.4107(0.4200) Steps 808(804.06) | Grad Norm 0.7455(0.8543) | Total Time 14.00(14.00)\n",
      "Iter 1644 | Time 71.8844(74.3221) | Bit/dim 3.9572(3.9543) | Xent 1.1510(1.1752) | Loss 4.5328(4.5418) | Error 0.4167(0.4199) Steps 808(804.18) | Grad Norm 1.3570(0.8694) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 29.0506, Epoch Time 487.7851(487.9043), Bit/dim 3.9538(best: 3.9534), Xent 1.1340, Loss 4.5208, Error 0.4094(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1645 | Time 74.2298(74.3193) | Bit/dim 3.9539(3.9542) | Xent 1.1621(1.1748) | Loss 4.5350(4.5416) | Error 0.4183(0.4199) Steps 802(804.11) | Grad Norm 1.0974(0.8762) | Total Time 14.00(14.00)\n",
      "Iter 1646 | Time 74.3201(74.3193) | Bit/dim 3.9483(3.9541) | Xent 1.1841(1.1750) | Loss 4.5403(4.5416) | Error 0.4230(0.4200) Steps 802(804.05) | Grad Norm 0.6738(0.8701) | Total Time 14.00(14.00)\n",
      "Iter 1647 | Time 77.1517(74.4043) | Bit/dim 3.9506(3.9540) | Xent 1.1642(1.1747) | Loss 4.5328(4.5413) | Error 0.4187(0.4200) Steps 802(803.99) | Grad Norm 0.9640(0.8729) | Total Time 14.00(14.00)\n",
      "Iter 1648 | Time 74.9159(74.4197) | Bit/dim 3.9521(3.9539) | Xent 1.1669(1.1745) | Loss 4.5355(4.5411) | Error 0.4145(0.4198) Steps 802(803.93) | Grad Norm 0.5505(0.8633) | Total Time 14.00(14.00)\n",
      "Iter 1649 | Time 74.6392(74.4262) | Bit/dim 3.9530(3.9539) | Xent 1.1705(1.1744) | Loss 4.5383(4.5411) | Error 0.4091(0.4195) Steps 808(804.05) | Grad Norm 0.8126(0.8617) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 74.1461(74.4178) | Bit/dim 3.9557(3.9539) | Xent 1.1715(1.1743) | Loss 4.5414(4.5411) | Error 0.4166(0.4194) Steps 802(803.99) | Grad Norm 0.9626(0.8648) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 29.2491, Epoch Time 494.0753(488.0895), Bit/dim 3.9538(best: 3.9534), Xent 1.1331, Loss 4.5204, Error 0.4088(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1651 | Time 75.6226(74.4540) | Bit/dim 3.9488(3.9538) | Xent 1.1585(1.1738) | Loss 4.5280(4.5407) | Error 0.4087(0.4191) Steps 808(804.11) | Grad Norm 0.7839(0.8623) | Total Time 14.00(14.00)\n",
      "Iter 1652 | Time 74.9737(74.4696) | Bit/dim 3.9437(3.9535) | Xent 1.1607(1.1734) | Loss 4.5241(4.5402) | Error 0.4114(0.4188) Steps 802(804.05) | Grad Norm 0.6676(0.8565) | Total Time 14.00(14.00)\n",
      "Iter 1653 | Time 74.9585(74.4842) | Bit/dim 3.9638(3.9538) | Xent 1.1661(1.1732) | Loss 4.5468(4.5404) | Error 0.4133(0.4187) Steps 802(803.99) | Grad Norm 0.7321(0.8528) | Total Time 14.00(14.00)\n",
      "Iter 1654 | Time 72.8336(74.4347) | Bit/dim 3.9488(3.9536) | Xent 1.1712(1.1731) | Loss 4.5344(4.5402) | Error 0.4210(0.4187) Steps 802(803.93) | Grad Norm 0.5730(0.8444) | Total Time 14.00(14.00)\n",
      "Iter 1655 | Time 71.4987(74.3466) | Bit/dim 3.9413(3.9533) | Xent 1.1465(1.1723) | Loss 4.5146(4.5394) | Error 0.4140(0.4186) Steps 808(804.05) | Grad Norm 0.7796(0.8424) | Total Time 14.00(14.00)\n",
      "Iter 1656 | Time 76.3537(74.4069) | Bit/dim 3.9666(3.9537) | Xent 1.1811(1.1726) | Loss 4.5571(4.5400) | Error 0.4240(0.4188) Steps 808(804.17) | Grad Norm 0.7010(0.8382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 28.9741, Epoch Time 490.7385(488.1689), Bit/dim 3.9521(best: 3.9534), Xent 1.1324, Loss 4.5183, Error 0.4080(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1657 | Time 72.1455(74.3390) | Bit/dim 3.9554(3.9537) | Xent 1.1723(1.1726) | Loss 4.5416(4.5400) | Error 0.4266(0.4190) Steps 802(804.10) | Grad Norm 0.6424(0.8323) | Total Time 14.00(14.00)\n",
      "Iter 1658 | Time 73.3339(74.3089) | Bit/dim 3.9588(3.9539) | Xent 1.1594(1.1722) | Loss 4.5385(4.5400) | Error 0.4167(0.4189) Steps 808(804.22) | Grad Norm 0.5959(0.8252) | Total Time 14.00(14.00)\n",
      "Iter 1659 | Time 74.1340(74.3036) | Bit/dim 3.9397(3.9534) | Xent 1.1645(1.1720) | Loss 4.5220(4.5394) | Error 0.4100(0.4187) Steps 808(804.33) | Grad Norm 0.5028(0.8156) | Total Time 14.00(14.00)\n",
      "Iter 1660 | Time 72.6119(74.2529) | Bit/dim 3.9514(3.9534) | Xent 1.1596(1.1716) | Loss 4.5312(4.5392) | Error 0.4113(0.4184) Steps 808(804.44) | Grad Norm 0.6099(0.8094) | Total Time 14.00(14.00)\n",
      "Iter 1661 | Time 71.9858(74.1848) | Bit/dim 3.9561(3.9535) | Xent 1.1591(1.1712) | Loss 4.5357(4.5391) | Error 0.4146(0.4183) Steps 802(804.37) | Grad Norm 0.6489(0.8046) | Total Time 14.00(14.00)\n",
      "Iter 1662 | Time 77.1099(74.2726) | Bit/dim 3.9420(3.9531) | Xent 1.1691(1.1712) | Loss 4.5266(4.5387) | Error 0.4145(0.4182) Steps 802(804.30) | Grad Norm 0.6024(0.7985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 29.0227, Epoch Time 486.0953(488.1067), Bit/dim 3.9511(best: 3.9521), Xent 1.1319, Loss 4.5171, Error 0.4115(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1663 | Time 74.9955(74.2943) | Bit/dim 3.9563(3.9532) | Xent 1.1548(1.1707) | Loss 4.5337(4.5386) | Error 0.4104(0.4180) Steps 808(804.41) | Grad Norm 0.7013(0.7956) | Total Time 14.00(14.00)\n",
      "Iter 1664 | Time 71.4812(74.2099) | Bit/dim 3.9566(3.9533) | Xent 1.1813(1.1710) | Loss 4.5473(4.5388) | Error 0.4225(0.4181) Steps 802(804.34) | Grad Norm 0.5906(0.7894) | Total Time 14.00(14.00)\n",
      "Iter 1665 | Time 74.8991(74.2306) | Bit/dim 3.9382(3.9529) | Xent 1.1664(1.1708) | Loss 4.5214(4.5383) | Error 0.4134(0.4180) Steps 802(804.27) | Grad Norm 0.6326(0.7847) | Total Time 14.00(14.00)\n",
      "Iter 1666 | Time 73.6554(74.2133) | Bit/dim 3.9447(3.9526) | Xent 1.1634(1.1706) | Loss 4.5264(4.5379) | Error 0.4154(0.4179) Steps 808(804.38) | Grad Norm 0.8286(0.7861) | Total Time 14.00(14.00)\n",
      "Iter 1667 | Time 75.1447(74.2413) | Bit/dim 3.9591(3.9528) | Xent 1.1608(1.1703) | Loss 4.5395(4.5380) | Error 0.4213(0.4180) Steps 802(804.31) | Grad Norm 0.6646(0.7824) | Total Time 14.00(14.00)\n",
      "Iter 1668 | Time 74.7420(74.2563) | Bit/dim 3.9465(3.9526) | Xent 1.1529(1.1698) | Loss 4.5229(4.5375) | Error 0.4094(0.4177) Steps 802(804.24) | Grad Norm 1.0064(0.7891) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 29.3788, Epoch Time 489.6687(488.1536), Bit/dim 3.9518(best: 3.9511), Xent 1.1298, Loss 4.5167, Error 0.4047(best: 0.4062)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1669 | Time 72.3986(74.2006) | Bit/dim 3.9579(3.9528) | Xent 1.1815(1.1702) | Loss 4.5486(4.5379) | Error 0.4229(0.4179) Steps 802(804.17) | Grad Norm 0.6103(0.7838) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 74.7634(74.2174) | Bit/dim 3.9501(3.9527) | Xent 1.1540(1.1697) | Loss 4.5272(4.5375) | Error 0.4134(0.4177) Steps 802(804.11) | Grad Norm 0.7283(0.7821) | Total Time 14.00(14.00)\n",
      "Iter 1671 | Time 76.6683(74.2910) | Bit/dim 3.9527(3.9527) | Xent 1.1596(1.1694) | Loss 4.5325(4.5374) | Error 0.4161(0.4177) Steps 802(804.04) | Grad Norm 0.8309(0.7836) | Total Time 14.00(14.00)\n",
      "Iter 1672 | Time 74.0729(74.2844) | Bit/dim 3.9420(3.9524) | Xent 1.1548(1.1689) | Loss 4.5194(4.5369) | Error 0.4069(0.4174) Steps 802(803.98) | Grad Norm 0.8492(0.7855) | Total Time 14.00(14.00)\n",
      "Iter 1673 | Time 77.2978(74.3748) | Bit/dim 3.9494(3.9523) | Xent 1.1571(1.1686) | Loss 4.5279(4.5366) | Error 0.4086(0.4171) Steps 808(804.10) | Grad Norm 1.0777(0.7943) | Total Time 14.00(14.00)\n",
      "Iter 1674 | Time 75.3851(74.4051) | Bit/dim 3.9543(3.9524) | Xent 1.1706(1.1686) | Loss 4.5396(4.5367) | Error 0.4191(0.4172) Steps 808(804.22) | Grad Norm 0.7149(0.7919) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 29.1445, Epoch Time 495.1460(488.3633), Bit/dim 3.9514(best: 3.9511), Xent 1.1308, Loss 4.5168, Error 0.4096(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1675 | Time 75.1079(74.4262) | Bit/dim 3.9540(3.9524) | Xent 1.1803(1.1690) | Loss 4.5442(4.5369) | Error 0.4131(0.4171) Steps 808(804.33) | Grad Norm 0.5595(0.7849) | Total Time 14.00(14.00)\n",
      "Iter 1676 | Time 71.7330(74.3454) | Bit/dim 3.9536(3.9524) | Xent 1.1389(1.1681) | Loss 4.5230(4.5365) | Error 0.4034(0.4166) Steps 802(804.26) | Grad Norm 0.9316(0.7893) | Total Time 14.00(14.00)\n",
      "Iter 1677 | Time 74.7203(74.3567) | Bit/dim 3.9386(3.9520) | Xent 1.1549(1.1677) | Loss 4.5160(4.5359) | Error 0.4086(0.4164) Steps 802(804.19) | Grad Norm 0.9105(0.7930) | Total Time 14.00(14.00)\n",
      "Iter 1678 | Time 73.7564(74.3387) | Bit/dim 3.9566(3.9522) | Xent 1.1571(1.1674) | Loss 4.5352(4.5359) | Error 0.4079(0.4161) Steps 808(804.31) | Grad Norm 0.8390(0.7944) | Total Time 14.00(14.00)\n",
      "Iter 1679 | Time 74.0408(74.3297) | Bit/dim 3.9537(3.9522) | Xent 1.1574(1.1671) | Loss 4.5323(4.5357) | Error 0.4147(0.4161) Steps 808(804.42) | Grad Norm 0.6045(0.7887) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 75.0371(74.3509) | Bit/dim 3.9445(3.9520) | Xent 1.1521(1.1666) | Loss 4.5206(4.5353) | Error 0.4180(0.4162) Steps 802(804.35) | Grad Norm 0.8863(0.7916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 28.9248, Epoch Time 489.0431(488.3837), Bit/dim 3.9511(best: 3.9511), Xent 1.1279, Loss 4.5151, Error 0.4075(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1681 | Time 74.8895(74.3671) | Bit/dim 3.9565(3.9521) | Xent 1.1454(1.1660) | Loss 4.5292(4.5351) | Error 0.4019(0.4157) Steps 808(804.46) | Grad Norm 0.6887(0.7885) | Total Time 14.00(14.00)\n",
      "Iter 1682 | Time 75.6883(74.4067) | Bit/dim 3.9480(3.9520) | Xent 1.1578(1.1657) | Loss 4.5269(4.5349) | Error 0.4097(0.4156) Steps 802(804.38) | Grad Norm 1.3945(0.8067) | Total Time 14.00(14.00)\n",
      "Iter 1683 | Time 77.1648(74.4895) | Bit/dim 3.9388(3.9516) | Xent 1.1584(1.1655) | Loss 4.5180(4.5344) | Error 0.4205(0.4157) Steps 802(804.31) | Grad Norm 0.7959(0.8064) | Total Time 14.00(14.00)\n",
      "Iter 1684 | Time 74.3433(74.4851) | Bit/dim 3.9533(3.9516) | Xent 1.1634(1.1655) | Loss 4.5350(4.5344) | Error 0.4204(0.4158) Steps 802(804.24) | Grad Norm 1.0675(0.8142) | Total Time 14.00(14.00)\n",
      "Iter 1685 | Time 75.0668(74.5025) | Bit/dim 3.9507(3.9516) | Xent 1.1571(1.1652) | Loss 4.5293(4.5342) | Error 0.4101(0.4157) Steps 802(804.17) | Grad Norm 0.7624(0.8126) | Total Time 14.00(14.00)\n",
      "Iter 1686 | Time 73.7542(74.4801) | Bit/dim 3.9511(3.9516) | Xent 1.1654(1.1652) | Loss 4.5338(4.5342) | Error 0.4169(0.4157) Steps 802(804.11) | Grad Norm 1.1783(0.8236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 29.2580, Epoch Time 495.5274(488.5980), Bit/dim 3.9497(best: 3.9511), Xent 1.1291, Loss 4.5143, Error 0.4077(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1687 | Time 73.9735(74.4649) | Bit/dim 3.9458(3.9514) | Xent 1.1443(1.1646) | Loss 4.5179(4.5337) | Error 0.4114(0.4156) Steps 808(804.23) | Grad Norm 0.5838(0.8164) | Total Time 14.00(14.00)\n",
      "Iter 1688 | Time 73.8350(74.4460) | Bit/dim 3.9605(3.9517) | Xent 1.1616(1.1645) | Loss 4.5413(4.5339) | Error 0.4216(0.4158) Steps 808(804.34) | Grad Norm 0.6828(0.8124) | Total Time 14.00(14.00)\n",
      "Iter 1689 | Time 73.1498(74.4071) | Bit/dim 3.9538(3.9518) | Xent 1.1512(1.1641) | Loss 4.5294(4.5338) | Error 0.4179(0.4158) Steps 808(804.45) | Grad Norm 0.6816(0.8085) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 75.1230(74.4286) | Bit/dim 3.9494(3.9517) | Xent 1.1623(1.1640) | Loss 4.5305(4.5337) | Error 0.4199(0.4159) Steps 802(804.38) | Grad Norm 0.7371(0.8063) | Total Time 14.00(14.00)\n",
      "Iter 1691 | Time 73.5613(74.4026) | Bit/dim 3.9407(3.9514) | Xent 1.1683(1.1642) | Loss 4.5249(4.5334) | Error 0.4219(0.4161) Steps 802(804.30) | Grad Norm 1.0833(0.8147) | Total Time 14.00(14.00)\n",
      "Iter 1692 | Time 74.9361(74.4186) | Bit/dim 3.9498(3.9513) | Xent 1.2020(1.1653) | Loss 4.5508(4.5340) | Error 0.4259(0.4164) Steps 808(804.42) | Grad Norm 1.2040(0.8263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 29.5174, Epoch Time 489.5396(488.6263), Bit/dim 3.9505(best: 3.9497), Xent 1.1260, Loss 4.5135, Error 0.4040(best: 0.4047)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1693 | Time 73.8290(74.4009) | Bit/dim 3.9536(3.9514) | Xent 1.1714(1.1655) | Loss 4.5393(4.5341) | Error 0.4156(0.4164) Steps 802(804.34) | Grad Norm 0.5533(0.8181) | Total Time 14.00(14.00)\n",
      "Iter 1694 | Time 74.8351(74.4139) | Bit/dim 3.9488(3.9513) | Xent 1.1486(1.1650) | Loss 4.5231(4.5338) | Error 0.4109(0.4162) Steps 808(804.45) | Grad Norm 0.9877(0.8232) | Total Time 14.00(14.00)\n",
      "Iter 1695 | Time 74.6845(74.4220) | Bit/dim 3.9494(3.9512) | Xent 1.1795(1.1654) | Loss 4.5391(4.5340) | Error 0.4230(0.4164) Steps 802(804.38) | Grad Norm 1.9747(0.8578) | Total Time 14.00(14.00)\n",
      "Iter 1696 | Time 74.7242(74.4311) | Bit/dim 3.9518(3.9513) | Xent 1.1519(1.1650) | Loss 4.5278(4.5338) | Error 0.4058(0.4161) Steps 802(804.31) | Grad Norm 0.9472(0.8605) | Total Time 14.00(14.00)\n",
      "Iter 1697 | Time 72.5891(74.3758) | Bit/dim 3.9534(3.9513) | Xent 1.1568(1.1648) | Loss 4.5318(4.5337) | Error 0.4131(0.4160) Steps 802(804.24) | Grad Norm 1.0135(0.8650) | Total Time 14.00(14.00)\n",
      "Iter 1698 | Time 77.9168(74.4821) | Bit/dim 3.9407(3.9510) | Xent 1.1704(1.1649) | Loss 4.5259(4.5335) | Error 0.4197(0.4161) Steps 802(804.17) | Grad Norm 0.9128(0.8665) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 28.9774, Epoch Time 493.0382(488.7587), Bit/dim 3.9507(best: 3.9497), Xent 1.1260, Loss 4.5137, Error 0.4029(best: 0.4040)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1699 | Time 76.2991(74.5366) | Bit/dim 3.9565(3.9512) | Xent 1.1725(1.1652) | Loss 4.5427(4.5338) | Error 0.4233(0.4163) Steps 802(804.11) | Grad Norm 0.9955(0.8704) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 74.9440(74.5488) | Bit/dim 3.9515(3.9512) | Xent 1.1559(1.1649) | Loss 4.5295(4.5336) | Error 0.4084(0.4161) Steps 808(804.22) | Grad Norm 1.0002(0.8742) | Total Time 14.00(14.00)\n",
      "Iter 1701 | Time 75.6898(74.5830) | Bit/dim 3.9519(3.9512) | Xent 1.1424(1.1642) | Loss 4.5231(4.5333) | Error 0.4089(0.4159) Steps 808(804.34) | Grad Norm 1.3169(0.8875) | Total Time 14.00(14.00)\n",
      "Iter 1702 | Time 76.5067(74.6407) | Bit/dim 3.9481(3.9511) | Xent 1.1746(1.1645) | Loss 4.5354(4.5334) | Error 0.4177(0.4159) Steps 820(804.81) | Grad Norm 0.8294(0.8858) | Total Time 14.00(14.00)\n",
      "Iter 1703 | Time 76.4036(74.6936) | Bit/dim 3.9480(3.9510) | Xent 1.1782(1.1649) | Loss 4.5371(4.5335) | Error 0.4260(0.4162) Steps 802(804.72) | Grad Norm 1.0808(0.8916) | Total Time 14.00(14.00)\n",
      "Iter 1704 | Time 73.5224(74.6585) | Bit/dim 3.9447(3.9508) | Xent 1.1594(1.1648) | Loss 4.5244(4.5332) | Error 0.4125(0.4161) Steps 808(804.82) | Grad Norm 1.2789(0.9033) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 29.5445, Epoch Time 498.3544(489.0465), Bit/dim 3.9502(best: 3.9497), Xent 1.1280, Loss 4.5142, Error 0.4078(best: 0.4029)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1705 | Time 75.2851(74.6773) | Bit/dim 3.9491(3.9508) | Xent 1.1640(1.1647) | Loss 4.5311(4.5332) | Error 0.4097(0.4159) Steps 802(804.74) | Grad Norm 1.3821(0.9176) | Total Time 14.00(14.00)\n",
      "Iter 1706 | Time 73.9143(74.6544) | Bit/dim 3.9510(3.9508) | Xent 1.1598(1.1646) | Loss 4.5309(4.5331) | Error 0.4126(0.4158) Steps 802(804.65) | Grad Norm 1.1199(0.9237) | Total Time 14.00(14.00)\n",
      "Iter 1707 | Time 75.7368(74.6869) | Bit/dim 3.9413(3.9505) | Xent 1.1499(1.1642) | Loss 4.5162(4.5326) | Error 0.4096(0.4157) Steps 808(804.75) | Grad Norm 0.6783(0.9163) | Total Time 14.00(14.00)\n",
      "Iter 1708 | Time 72.7047(74.6274) | Bit/dim 3.9523(3.9506) | Xent 1.1713(1.1644) | Loss 4.5380(4.5327) | Error 0.4159(0.4157) Steps 808(804.85) | Grad Norm 0.6624(0.9087) | Total Time 14.00(14.00)\n",
      "Iter 1709 | Time 75.3970(74.6505) | Bit/dim 3.9540(3.9507) | Xent 1.1715(1.1646) | Loss 4.5398(4.5330) | Error 0.4189(0.4158) Steps 808(804.95) | Grad Norm 1.0378(0.9126) | Total Time 14.00(14.00)\n",
      "Iter 1710 | Time 72.2275(74.5778) | Bit/dim 3.9402(3.9503) | Xent 1.1634(1.1645) | Loss 4.5219(4.5326) | Error 0.4137(0.4157) Steps 808(805.04) | Grad Norm 0.7199(0.9068) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 29.4133, Epoch Time 489.9464(489.0735), Bit/dim 3.9497(best: 3.9497), Xent 1.1232, Loss 4.5113, Error 0.4026(best: 0.4029)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1711 | Time 72.7555(74.5231) | Bit/dim 3.9420(3.9501) | Xent 1.1652(1.1646) | Loss 4.5247(4.5324) | Error 0.4139(0.4156) Steps 802(804.95) | Grad Norm 1.2969(0.9185) | Total Time 14.00(14.00)\n",
      "Iter 1712 | Time 74.5334(74.5234) | Bit/dim 3.9441(3.9499) | Xent 1.1681(1.1647) | Loss 4.5281(4.5323) | Error 0.4159(0.4156) Steps 808(805.04) | Grad Norm 0.6654(0.9109) | Total Time 14.00(14.00)\n",
      "Iter 1713 | Time 74.2855(74.5163) | Bit/dim 3.9548(3.9501) | Xent 1.1631(1.1646) | Loss 4.5364(4.5324) | Error 0.4143(0.4156) Steps 808(805.13) | Grad Norm 0.9146(0.9110) | Total Time 14.00(14.00)\n",
      "Iter 1714 | Time 72.9485(74.4693) | Bit/dim 3.9528(3.9501) | Xent 1.1569(1.1644) | Loss 4.5312(4.5323) | Error 0.4153(0.4156) Steps 808(805.21) | Grad Norm 1.1065(0.9169) | Total Time 14.00(14.00)\n",
      "Iter 1715 | Time 74.0236(74.4559) | Bit/dim 3.9482(3.9501) | Xent 1.1608(1.1643) | Loss 4.5286(4.5322) | Error 0.4149(0.4156) Steps 802(805.12) | Grad Norm 0.8219(0.9140) | Total Time 14.00(14.00)\n",
      "Iter 1716 | Time 76.1946(74.5081) | Bit/dim 3.9486(3.9500) | Xent 1.1584(1.1641) | Loss 4.5278(4.5321) | Error 0.4129(0.4155) Steps 808(805.20) | Grad Norm 0.9831(0.9161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 29.2069, Epoch Time 489.6865(489.0919), Bit/dim 3.9504(best: 3.9497), Xent 1.1218, Loss 4.5113, Error 0.4016(best: 0.4026)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1717 | Time 72.7508(74.4553) | Bit/dim 3.9566(3.9502) | Xent 1.1430(1.1635) | Loss 4.5281(4.5320) | Error 0.4052(0.4152) Steps 802(805.11) | Grad Norm 0.8006(0.9126) | Total Time 14.00(14.00)\n",
      "Iter 1718 | Time 74.4954(74.4565) | Bit/dim 3.9518(3.9503) | Xent 1.1642(1.1635) | Loss 4.5338(4.5320) | Error 0.4131(0.4151) Steps 802(805.01) | Grad Norm 1.4688(0.9293) | Total Time 14.00(14.00)\n",
      "Iter 1719 | Time 73.4651(74.4268) | Bit/dim 3.9420(3.9500) | Xent 1.1449(1.1629) | Loss 4.5144(4.5315) | Error 0.4100(0.4150) Steps 802(804.92) | Grad Norm 1.2320(0.9384) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 72.8141(74.3784) | Bit/dim 3.9432(3.9498) | Xent 1.1713(1.1632) | Loss 4.5289(4.5314) | Error 0.4180(0.4151) Steps 802(804.84) | Grad Norm 0.6082(0.9285) | Total Time 14.00(14.00)\n",
      "Iter 1721 | Time 72.6497(74.3266) | Bit/dim 3.9503(3.9498) | Xent 1.1643(1.1632) | Loss 4.5324(4.5315) | Error 0.4164(0.4151) Steps 808(804.93) | Grad Norm 1.1425(0.9349) | Total Time 14.00(14.00)\n",
      "Iter 1722 | Time 75.7189(74.3683) | Bit/dim 3.9426(3.9496) | Xent 1.1484(1.1628) | Loss 4.5168(4.5310) | Error 0.4051(0.4148) Steps 808(805.02) | Grad Norm 0.9756(0.9361) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 29.6100, Epoch Time 487.2114(489.0355), Bit/dim 3.9493(best: 3.9497), Xent 1.1233, Loss 4.5109, Error 0.4039(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1723 | Time 73.8644(74.3532) | Bit/dim 3.9456(3.9495) | Xent 1.1706(1.1630) | Loss 4.5309(4.5310) | Error 0.4116(0.4147) Steps 802(804.93) | Grad Norm 1.4032(0.9501) | Total Time 14.00(14.00)\n",
      "Iter 1724 | Time 74.5906(74.3603) | Bit/dim 3.9509(3.9496) | Xent 1.1586(1.1629) | Loss 4.5302(4.5310) | Error 0.4175(0.4148) Steps 808(805.02) | Grad Norm 0.6907(0.9424) | Total Time 14.00(14.00)\n",
      "Iter 1725 | Time 75.2652(74.3875) | Bit/dim 3.9398(3.9493) | Xent 1.1665(1.1630) | Loss 4.5230(4.5308) | Error 0.4211(0.4150) Steps 808(805.11) | Grad Norm 0.6067(0.9323) | Total Time 14.00(14.00)\n",
      "Iter 1726 | Time 76.3026(74.4449) | Bit/dim 3.9529(3.9494) | Xent 1.1435(1.1624) | Loss 4.5246(4.5306) | Error 0.4085(0.4148) Steps 808(805.20) | Grad Norm 1.0103(0.9346) | Total Time 14.00(14.00)\n",
      "Iter 1727 | Time 74.9117(74.4589) | Bit/dim 3.9479(3.9493) | Xent 1.1548(1.1622) | Loss 4.5253(4.5304) | Error 0.4084(0.4146) Steps 808(805.28) | Grad Norm 1.6840(0.9571) | Total Time 14.00(14.00)\n",
      "Iter 1728 | Time 73.9466(74.4436) | Bit/dim 3.9505(3.9494) | Xent 1.1588(1.1621) | Loss 4.5299(4.5304) | Error 0.4074(0.4144) Steps 802(805.19) | Grad Norm 0.9476(0.9568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 29.3977, Epoch Time 493.7045(489.1756), Bit/dim 3.9490(best: 3.9493), Xent 1.1208, Loss 4.5094, Error 0.4036(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1729 | Time 72.5510(74.3868) | Bit/dim 3.9444(3.9492) | Xent 1.1522(1.1618) | Loss 4.5205(4.5301) | Error 0.4077(0.4142) Steps 808(805.27) | Grad Norm 0.7673(0.9511) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 72.6440(74.3345) | Bit/dim 3.9501(3.9492) | Xent 1.1559(1.1616) | Loss 4.5281(4.5300) | Error 0.4094(0.4140) Steps 808(805.35) | Grad Norm 1.4675(0.9666) | Total Time 14.00(14.00)\n",
      "Iter 1731 | Time 74.5132(74.3399) | Bit/dim 3.9475(3.9492) | Xent 1.1659(1.1617) | Loss 4.5305(4.5301) | Error 0.4150(0.4141) Steps 802(805.25) | Grad Norm 1.0254(0.9684) | Total Time 14.00(14.00)\n",
      "Iter 1732 | Time 77.5484(74.4361) | Bit/dim 3.9467(3.9491) | Xent 1.1535(1.1615) | Loss 4.5234(4.5299) | Error 0.4139(0.4141) Steps 802(805.15) | Grad Norm 0.9284(0.9672) | Total Time 14.00(14.00)\n",
      "Iter 1733 | Time 78.7289(74.5649) | Bit/dim 3.9470(3.9490) | Xent 1.1618(1.1615) | Loss 4.5279(4.5298) | Error 0.4145(0.4141) Steps 802(805.06) | Grad Norm 0.6416(0.9574) | Total Time 14.00(14.00)\n",
      "Iter 1734 | Time 74.9967(74.5779) | Bit/dim 3.9596(3.9494) | Xent 1.1417(1.1609) | Loss 4.5304(4.5298) | Error 0.4083(0.4139) Steps 808(805.15) | Grad Norm 1.1204(0.9623) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 29.5745, Epoch Time 496.1214(489.3839), Bit/dim 3.9495(best: 3.9490), Xent 1.1221, Loss 4.5105, Error 0.4039(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 73.5317(74.5465) | Bit/dim 3.9517(3.9494) | Xent 1.1717(1.1612) | Loss 4.5376(4.5300) | Error 0.4147(0.4139) Steps 802(805.05) | Grad Norm 2.0501(0.9950) | Total Time 14.00(14.00)\n",
      "Iter 1736 | Time 72.0690(74.4722) | Bit/dim 3.9484(3.9494) | Xent 1.1605(1.1612) | Loss 4.5286(4.5300) | Error 0.4136(0.4139) Steps 808(805.14) | Grad Norm 1.0120(0.9955) | Total Time 14.00(14.00)\n",
      "Iter 1737 | Time 76.9224(74.5457) | Bit/dim 3.9427(3.9492) | Xent 1.1387(1.1605) | Loss 4.5120(4.5295) | Error 0.4089(0.4138) Steps 808(805.23) | Grad Norm 0.9321(0.9936) | Total Time 14.00(14.00)\n",
      "Iter 1738 | Time 75.1969(74.5652) | Bit/dim 3.9451(3.9491) | Xent 1.1520(1.1603) | Loss 4.5211(4.5292) | Error 0.4091(0.4136) Steps 802(805.13) | Grad Norm 1.4765(1.0081) | Total Time 14.00(14.00)\n",
      "Iter 1739 | Time 76.2941(74.6171) | Bit/dim 3.9491(3.9491) | Xent 1.1674(1.1605) | Loss 4.5328(4.5293) | Error 0.4179(0.4138) Steps 808(805.22) | Grad Norm 1.1847(1.0134) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 73.7155(74.5900) | Bit/dim 3.9471(3.9490) | Xent 1.1499(1.1602) | Loss 4.5221(4.5291) | Error 0.4123(0.4137) Steps 802(805.12) | Grad Norm 1.3799(1.0243) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 29.4109, Epoch Time 492.8479(489.4879), Bit/dim 3.9479(best: 3.9490), Xent 1.1200, Loss 4.5078, Error 0.4037(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 71.5878(74.4999) | Bit/dim 3.9456(3.9489) | Xent 1.1619(1.1602) | Loss 4.5265(4.5290) | Error 0.4151(0.4137) Steps 802(805.03) | Grad Norm 0.8572(1.0193) | Total Time 14.00(14.00)\n",
      "Iter 1742 | Time 74.9684(74.5140) | Bit/dim 3.9455(3.9488) | Xent 1.1623(1.1603) | Loss 4.5267(4.5290) | Error 0.4123(0.4137) Steps 802(804.94) | Grad Norm 1.0980(1.0217) | Total Time 14.00(14.00)\n",
      "Iter 1743 | Time 74.9748(74.5278) | Bit/dim 3.9432(3.9486) | Xent 1.1547(1.1601) | Loss 4.5205(4.5287) | Error 0.4149(0.4137) Steps 808(805.03) | Grad Norm 1.6483(1.0405) | Total Time 14.00(14.00)\n",
      "Iter 1744 | Time 73.5901(74.4997) | Bit/dim 3.9439(3.9485) | Xent 1.1499(1.1598) | Loss 4.5188(4.5284) | Error 0.4106(0.4136) Steps 808(805.12) | Grad Norm 1.3219(1.0489) | Total Time 14.00(14.00)\n",
      "Iter 1745 | Time 74.6455(74.5041) | Bit/dim 3.9510(3.9486) | Xent 1.1518(1.1596) | Loss 4.5269(4.5284) | Error 0.4103(0.4135) Steps 808(805.20) | Grad Norm 0.6388(1.0366) | Total Time 14.00(14.00)\n",
      "Iter 1746 | Time 76.3072(74.5582) | Bit/dim 3.9486(3.9486) | Xent 1.1775(1.1601) | Loss 4.5373(4.5286) | Error 0.4219(0.4138) Steps 808(805.29) | Grad Norm 1.1794(1.0409) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 29.3412, Epoch Time 491.0061(489.5334), Bit/dim 3.9480(best: 3.9479), Xent 1.1211, Loss 4.5085, Error 0.4055(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 74.6449(74.5608) | Bit/dim 3.9492(3.9486) | Xent 1.1494(1.1598) | Loss 4.5239(4.5285) | Error 0.4077(0.4136) Steps 808(805.37) | Grad Norm 1.3860(1.0513) | Total Time 14.00(14.00)\n",
      "Iter 1748 | Time 73.5669(74.5309) | Bit/dim 3.9617(3.9490) | Xent 1.1701(1.1601) | Loss 4.5467(4.5290) | Error 0.4166(0.4137) Steps 808(805.45) | Grad Norm 1.8103(1.0740) | Total Time 14.00(14.00)\n",
      "Iter 1749 | Time 76.7142(74.5964) | Bit/dim 3.9409(3.9487) | Xent 1.1575(1.1600) | Loss 4.5197(4.5288) | Error 0.4194(0.4139) Steps 808(805.52) | Grad Norm 0.7942(1.0656) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 74.3644(74.5895) | Bit/dim 3.9395(3.9485) | Xent 1.1502(1.1597) | Loss 4.5146(4.5283) | Error 0.4185(0.4140) Steps 808(805.60) | Grad Norm 0.9208(1.0613) | Total Time 14.00(14.00)\n",
      "Iter 1751 | Time 74.7320(74.5938) | Bit/dim 3.9443(3.9483) | Xent 1.1432(1.1592) | Loss 4.5159(4.5280) | Error 0.4090(0.4139) Steps 802(805.49) | Grad Norm 1.9395(1.0876) | Total Time 14.00(14.00)\n",
      "Iter 1752 | Time 75.1685(74.6110) | Bit/dim 3.9440(3.9482) | Xent 1.1682(1.1595) | Loss 4.5281(4.5280) | Error 0.4193(0.4140) Steps 808(805.57) | Grad Norm 1.4808(1.0994) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 28.9313, Epoch Time 493.5778(489.6547), Bit/dim 3.9479(best: 3.9479), Xent 1.1192, Loss 4.5075, Error 0.4027(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 75.4674(74.6367) | Bit/dim 3.9552(3.9484) | Xent 1.1574(1.1594) | Loss 4.5339(4.5281) | Error 0.4115(0.4139) Steps 808(805.64) | Grad Norm 0.6418(1.0857) | Total Time 14.00(14.00)\n",
      "Iter 1754 | Time 73.7004(74.6086) | Bit/dim 3.9493(3.9484) | Xent 1.1624(1.1595) | Loss 4.5305(4.5282) | Error 0.4179(0.4141) Steps 802(805.53) | Grad Norm 0.8189(1.0777) | Total Time 14.00(14.00)\n",
      "Iter 1755 | Time 74.1157(74.5938) | Bit/dim 3.9444(3.9483) | Xent 1.1575(1.1595) | Loss 4.5231(4.5281) | Error 0.4151(0.4141) Steps 808(805.60) | Grad Norm 0.8704(1.0715) | Total Time 14.00(14.00)\n",
      "Iter 1756 | Time 75.0038(74.6061) | Bit/dim 3.9372(3.9480) | Xent 1.1431(1.1590) | Loss 4.5087(4.5275) | Error 0.4096(0.4140) Steps 808(805.68) | Grad Norm 1.6414(1.0886) | Total Time 14.00(14.00)\n",
      "Iter 1757 | Time 75.2161(74.6244) | Bit/dim 3.9467(3.9480) | Xent 1.1514(1.1587) | Loss 4.5224(4.5273) | Error 0.4076(0.4138) Steps 808(805.75) | Grad Norm 0.5872(1.0735) | Total Time 14.00(14.00)\n",
      "Iter 1758 | Time 76.3743(74.6769) | Bit/dim 3.9444(3.9478) | Xent 1.1588(1.1587) | Loss 4.5238(4.5272) | Error 0.4080(0.4136) Steps 808(805.81) | Grad Norm 0.7059(1.0625) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 29.2636, Epoch Time 494.6594(489.8049), Bit/dim 3.9477(best: 3.9479), Xent 1.1162, Loss 4.5058, Error 0.4012(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 74.2483(74.6640) | Bit/dim 3.9488(3.9479) | Xent 1.1537(1.1586) | Loss 4.5256(4.5272) | Error 0.4117(0.4135) Steps 808(805.88) | Grad Norm 0.7686(1.0537) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 73.9422(74.6424) | Bit/dim 3.9465(3.9478) | Xent 1.1681(1.1589) | Loss 4.5305(4.5273) | Error 0.4103(0.4134) Steps 808(805.94) | Grad Norm 0.6674(1.0421) | Total Time 14.00(14.00)\n",
      "Iter 1761 | Time 76.5579(74.6999) | Bit/dim 3.9510(3.9479) | Xent 1.1512(1.1587) | Loss 4.5266(4.5272) | Error 0.4041(0.4132) Steps 808(806.00) | Grad Norm 0.8949(1.0377) | Total Time 14.00(14.00)\n",
      "Iter 1762 | Time 74.4922(74.6936) | Bit/dim 3.9475(3.9479) | Xent 1.1503(1.1584) | Loss 4.5227(4.5271) | Error 0.4137(0.4132) Steps 808(806.06) | Grad Norm 1.4119(1.0489) | Total Time 14.00(14.00)\n",
      "Iter 1763 | Time 75.6847(74.7234) | Bit/dim 3.9368(3.9476) | Xent 1.1608(1.1585) | Loss 4.5172(4.5268) | Error 0.4171(0.4133) Steps 802(805.94) | Grad Norm 0.8520(1.0430) | Total Time 14.00(14.00)\n",
      "Iter 1764 | Time 75.2900(74.7404) | Bit/dim 3.9525(3.9477) | Xent 1.1358(1.1578) | Loss 4.5204(4.5266) | Error 0.3998(0.4129) Steps 820(806.36) | Grad Norm 0.7330(1.0337) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 29.2770, Epoch Time 494.9299(489.9586), Bit/dim 3.9462(best: 3.9477), Xent 1.1184, Loss 4.5054, Error 0.4026(best: 0.4012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 75.1435(74.7525) | Bit/dim 3.9456(3.9477) | Xent 1.1403(1.1573) | Loss 4.5157(4.5263) | Error 0.4000(0.4125) Steps 808(806.41) | Grad Norm 1.1896(1.0384) | Total Time 14.00(14.00)\n",
      "Iter 1766 | Time 73.3701(74.7110) | Bit/dim 3.9548(3.9479) | Xent 1.1665(1.1575) | Loss 4.5381(4.5266) | Error 0.4154(0.4126) Steps 808(806.46) | Grad Norm 1.1723(1.0424) | Total Time 14.00(14.00)\n",
      "Iter 1767 | Time 75.8356(74.7447) | Bit/dim 3.9461(3.9478) | Xent 1.1521(1.1574) | Loss 4.5221(4.5265) | Error 0.4175(0.4127) Steps 802(806.33) | Grad Norm 0.6617(1.0310) | Total Time 14.00(14.00)\n",
      "Iter 1768 | Time 74.2613(74.7302) | Bit/dim 3.9384(3.9475) | Xent 1.1456(1.1570) | Loss 4.5113(4.5261) | Error 0.4066(0.4126) Steps 802(806.20) | Grad Norm 0.5762(1.0173) | Total Time 14.00(14.00)\n",
      "Iter 1769 | Time 75.1000(74.7413) | Bit/dim 3.9512(3.9476) | Xent 1.1639(1.1572) | Loss 4.5332(4.5263) | Error 0.4166(0.4127) Steps 808(806.25) | Grad Norm 1.0584(1.0186) | Total Time 14.00(14.00)\n",
      "Iter 1770 | Time 74.5740(74.7363) | Bit/dim 3.9410(3.9475) | Xent 1.1540(1.1571) | Loss 4.5180(4.5260) | Error 0.4166(0.4128) Steps 808(806.30) | Grad Norm 0.7803(1.0114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 29.4391, Epoch Time 493.2606(490.0577), Bit/dim 3.9471(best: 3.9462), Xent 1.1162, Loss 4.5052, Error 0.3997(best: 0.4012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 71.9844(74.6537) | Bit/dim 3.9406(3.9472) | Xent 1.1534(1.1570) | Loss 4.5174(4.5258) | Error 0.4116(0.4128) Steps 808(806.35) | Grad Norm 0.8283(1.0059) | Total Time 14.00(14.00)\n",
      "Iter 1772 | Time 72.5801(74.5915) | Bit/dim 3.9418(3.9471) | Xent 1.1352(1.1564) | Loss 4.5094(4.5253) | Error 0.4019(0.4124) Steps 808(806.40) | Grad Norm 0.8508(1.0013) | Total Time 14.00(14.00)\n",
      "Iter 1773 | Time 73.3171(74.5533) | Bit/dim 3.9468(3.9471) | Xent 1.1732(1.1569) | Loss 4.5334(4.5255) | Error 0.4219(0.4127) Steps 808(806.45) | Grad Norm 0.7703(0.9943) | Total Time 14.00(14.00)\n",
      "Iter 1774 | Time 75.9632(74.5956) | Bit/dim 3.9513(3.9472) | Xent 1.1676(1.1572) | Loss 4.5352(4.5258) | Error 0.4204(0.4129) Steps 802(806.32) | Grad Norm 0.6883(0.9852) | Total Time 14.00(14.00)\n",
      "Iter 1775 | Time 72.7962(74.5416) | Bit/dim 3.9438(3.9471) | Xent 1.1685(1.1575) | Loss 4.5281(4.5259) | Error 0.4125(0.4129) Steps 808(806.37) | Grad Norm 0.7871(0.9792) | Total Time 14.00(14.00)\n",
      "Iter 1776 | Time 75.4639(74.5693) | Bit/dim 3.9499(3.9472) | Xent 1.1417(1.1571) | Loss 4.5207(4.5257) | Error 0.4094(0.4128) Steps 808(806.42) | Grad Norm 0.7354(0.9719) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 29.4510, Epoch Time 487.1568(489.9707), Bit/dim 3.9468(best: 3.9462), Xent 1.1157, Loss 4.5046, Error 0.4028(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 73.4499(74.5357) | Bit/dim 3.9558(3.9474) | Xent 1.1738(1.1576) | Loss 4.5427(4.5262) | Error 0.4167(0.4129) Steps 802(806.28) | Grad Norm 0.8970(0.9697) | Total Time 14.00(14.00)\n",
      "Iter 1778 | Time 76.5259(74.5954) | Bit/dim 3.9399(3.9472) | Xent 1.1507(1.1574) | Loss 4.5152(4.5259) | Error 0.4073(0.4128) Steps 808(806.34) | Grad Norm 0.8137(0.9650) | Total Time 14.00(14.00)\n",
      "Iter 1779 | Time 74.5188(74.5931) | Bit/dim 3.9510(3.9473) | Xent 1.1587(1.1574) | Loss 4.5304(4.5260) | Error 0.4166(0.4129) Steps 820(806.75) | Grad Norm 0.9984(0.9660) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 77.1065(74.6685) | Bit/dim 3.9443(3.9472) | Xent 1.1629(1.1576) | Loss 4.5258(4.5260) | Error 0.4200(0.4131) Steps 802(806.60) | Grad Norm 0.6836(0.9575) | Total Time 14.00(14.00)\n",
      "Iter 1781 | Time 73.7617(74.6413) | Bit/dim 3.9412(3.9471) | Xent 1.1289(1.1567) | Loss 4.5056(4.5254) | Error 0.4054(0.4129) Steps 808(806.65) | Grad Norm 0.7664(0.9518) | Total Time 14.00(14.00)\n",
      "Iter 1782 | Time 73.4321(74.6050) | Bit/dim 3.9405(3.9469) | Xent 1.1320(1.1560) | Loss 4.5065(4.5248) | Error 0.4024(0.4126) Steps 808(806.69) | Grad Norm 0.6419(0.9425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 29.7171, Epoch Time 493.9485(490.0900), Bit/dim 3.9462(best: 3.9462), Xent 1.1151, Loss 4.5038, Error 0.4002(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 75.7381(74.6390) | Bit/dim 3.9444(3.9468) | Xent 1.1473(1.1557) | Loss 4.5181(4.5246) | Error 0.4099(0.4125) Steps 808(806.73) | Grad Norm 0.9966(0.9441) | Total Time 14.00(14.00)\n",
      "Iter 1784 | Time 75.2375(74.6570) | Bit/dim 3.9362(3.9465) | Xent 1.1410(1.1553) | Loss 4.5067(4.5241) | Error 0.4085(0.4124) Steps 802(806.58) | Grad Norm 0.8637(0.9417) | Total Time 14.00(14.00)\n",
      "Iter 1785 | Time 74.1926(74.6430) | Bit/dim 3.9499(3.9466) | Xent 1.1558(1.1553) | Loss 4.5278(4.5242) | Error 0.4117(0.4123) Steps 808(806.63) | Grad Norm 1.0520(0.9450) | Total Time 14.00(14.00)\n",
      "Iter 1786 | Time 74.7420(74.6460) | Bit/dim 3.9502(3.9467) | Xent 1.1575(1.1553) | Loss 4.5289(4.5244) | Error 0.4121(0.4123) Steps 802(806.49) | Grad Norm 0.9584(0.9454) | Total Time 14.00(14.00)\n",
      "Iter 1787 | Time 74.9645(74.6556) | Bit/dim 3.9380(3.9464) | Xent 1.1539(1.1553) | Loss 4.5150(4.5241) | Error 0.4131(0.4124) Steps 802(806.35) | Grad Norm 0.7624(0.9399) | Total Time 14.00(14.00)\n",
      "Iter 1788 | Time 76.5502(74.7124) | Bit/dim 3.9463(3.9464) | Xent 1.1460(1.1550) | Loss 4.5193(4.5239) | Error 0.4100(0.4123) Steps 808(806.40) | Grad Norm 0.6841(0.9322) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 28.7882, Epoch Time 495.6746(490.2575), Bit/dim 3.9446(best: 3.9462), Xent 1.1150, Loss 4.5021, Error 0.4008(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 77.5808(74.7985) | Bit/dim 3.9457(3.9464) | Xent 1.1377(1.1545) | Loss 4.5146(4.5236) | Error 0.4030(0.4120) Steps 808(806.45) | Grad Norm 0.5443(0.9206) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 76.9263(74.8623) | Bit/dim 3.9427(3.9463) | Xent 1.1586(1.1546) | Loss 4.5221(4.5236) | Error 0.4164(0.4121) Steps 814(806.68) | Grad Norm 0.6140(0.9114) | Total Time 14.00(14.00)\n",
      "Iter 1791 | Time 75.5685(74.8835) | Bit/dim 3.9445(3.9462) | Xent 1.1614(1.1548) | Loss 4.5252(4.5236) | Error 0.4127(0.4122) Steps 808(806.72) | Grad Norm 0.7759(0.9073) | Total Time 14.00(14.00)\n",
      "Iter 1792 | Time 74.4202(74.8696) | Bit/dim 3.9477(3.9463) | Xent 1.1357(1.1543) | Loss 4.5155(4.5234) | Error 0.4051(0.4119) Steps 802(806.57) | Grad Norm 0.7032(0.9012) | Total Time 14.00(14.00)\n",
      "Iter 1793 | Time 76.3780(74.9148) | Bit/dim 3.9414(3.9461) | Xent 1.1705(1.1547) | Loss 4.5266(4.5235) | Error 0.4119(0.4119) Steps 808(806.62) | Grad Norm 0.5853(0.8917) | Total Time 14.00(14.00)\n",
      "Iter 1794 | Time 72.4185(74.8399) | Bit/dim 3.9450(3.9461) | Xent 1.1632(1.1550) | Loss 4.5266(4.5236) | Error 0.4226(0.4123) Steps 802(806.48) | Grad Norm 0.7511(0.8875) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 29.4635, Epoch Time 498.0987(490.4928), Bit/dim 3.9462(best: 3.9446), Xent 1.1137, Loss 4.5030, Error 0.4031(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 75.6899(74.8654) | Bit/dim 3.9464(3.9461) | Xent 1.1434(1.1547) | Loss 4.5181(4.5234) | Error 0.4018(0.4119) Steps 808(806.52) | Grad Norm 0.7055(0.8821) | Total Time 14.00(14.00)\n",
      "Iter 1796 | Time 74.8956(74.8663) | Bit/dim 3.9359(3.9458) | Xent 1.1549(1.1547) | Loss 4.5133(4.5231) | Error 0.4137(0.4120) Steps 808(806.57) | Grad Norm 0.6888(0.8763) | Total Time 14.00(14.00)\n",
      "Iter 1797 | Time 75.1191(74.8739) | Bit/dim 3.9547(3.9461) | Xent 1.1354(1.1541) | Loss 4.5224(4.5231) | Error 0.4062(0.4118) Steps 808(806.61) | Grad Norm 0.9101(0.8773) | Total Time 14.00(14.00)\n",
      "Iter 1798 | Time 75.9665(74.9067) | Bit/dim 3.9453(3.9460) | Xent 1.1527(1.1540) | Loss 4.5216(4.5231) | Error 0.4061(0.4117) Steps 802(806.47) | Grad Norm 0.8989(0.8779) | Total Time 14.00(14.00)\n",
      "Iter 1799 | Time 75.2996(74.9185) | Bit/dim 3.9343(3.9457) | Xent 1.1610(1.1542) | Loss 4.5148(4.5228) | Error 0.4151(0.4118) Steps 808(806.52) | Grad Norm 0.8123(0.8760) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 73.7179(74.8825) | Bit/dim 3.9510(3.9458) | Xent 1.1358(1.1537) | Loss 4.5189(4.5227) | Error 0.4069(0.4116) Steps 808(806.56) | Grad Norm 1.7798(0.9031) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 29.2551, Epoch Time 495.2696(490.6361), Bit/dim 3.9462(best: 3.9446), Xent 1.1146, Loss 4.5035, Error 0.4027(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 74.2345(74.8630) | Bit/dim 3.9469(3.9459) | Xent 1.1615(1.1539) | Loss 4.5276(4.5228) | Error 0.4113(0.4116) Steps 808(806.61) | Grad Norm 0.8697(0.9021) | Total Time 14.00(14.00)\n",
      "Iter 1802 | Time 75.4035(74.8793) | Bit/dim 3.9449(3.9458) | Xent 1.1647(1.1543) | Loss 4.5272(4.5230) | Error 0.4175(0.4118) Steps 802(806.47) | Grad Norm 1.1401(0.9092) | Total Time 14.00(14.00)\n",
      "Iter 1803 | Time 72.6205(74.8115) | Bit/dim 3.9496(3.9460) | Xent 1.1375(1.1537) | Loss 4.5184(4.5228) | Error 0.4009(0.4115) Steps 808(806.51) | Grad Norm 0.7934(0.9057) | Total Time 14.00(14.00)\n",
      "Iter 1804 | Time 77.5678(74.8942) | Bit/dim 3.9367(3.9457) | Xent 1.1316(1.1531) | Loss 4.5025(4.5222) | Error 0.4074(0.4113) Steps 820(806.92) | Grad Norm 1.6678(0.9286) | Total Time 14.00(14.00)\n",
      "Iter 1805 | Time 73.7609(74.8602) | Bit/dim 3.9377(3.9454) | Xent 1.1634(1.1534) | Loss 4.5194(4.5221) | Error 0.4239(0.4117) Steps 808(806.95) | Grad Norm 1.3023(0.9398) | Total Time 14.00(14.00)\n",
      "Iter 1806 | Time 75.6735(74.8846) | Bit/dim 3.9525(3.9457) | Xent 1.1370(1.1529) | Loss 4.5210(4.5221) | Error 0.4006(0.4114) Steps 808(806.98) | Grad Norm 1.0941(0.9444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 29.4965, Epoch Time 494.2861(490.7456), Bit/dim 3.9451(best: 3.9446), Xent 1.1106, Loss 4.5004, Error 0.4023(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 76.5558(74.9347) | Bit/dim 3.9515(3.9458) | Xent 1.1556(1.1530) | Loss 4.5292(4.5223) | Error 0.4161(0.4115) Steps 808(807.01) | Grad Norm 1.1238(0.9498) | Total Time 14.00(14.00)\n",
      "Iter 1808 | Time 75.8707(74.9628) | Bit/dim 3.9424(3.9457) | Xent 1.1606(1.1532) | Loss 4.5227(4.5223) | Error 0.4120(0.4115) Steps 808(807.04) | Grad Norm 1.7739(0.9745) | Total Time 14.00(14.00)\n",
      "Iter 1809 | Time 75.5849(74.9815) | Bit/dim 3.9442(3.9457) | Xent 1.1431(1.1529) | Loss 4.5158(4.5221) | Error 0.4089(0.4115) Steps 808(807.07) | Grad Norm 1.7853(0.9989) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 77.2073(75.0482) | Bit/dim 3.9397(3.9455) | Xent 1.1497(1.1528) | Loss 4.5145(4.5219) | Error 0.4062(0.4113) Steps 808(807.10) | Grad Norm 0.5824(0.9864) | Total Time 14.00(14.00)\n",
      "Iter 1811 | Time 74.3273(75.0266) | Bit/dim 3.9356(3.9452) | Xent 1.1421(1.1525) | Loss 4.5066(4.5214) | Error 0.4096(0.4112) Steps 808(807.13) | Grad Norm 1.2191(0.9934) | Total Time 14.00(14.00)\n",
      "Iter 1812 | Time 75.7156(75.0473) | Bit/dim 3.9509(3.9454) | Xent 1.1427(1.1522) | Loss 4.5223(4.5215) | Error 0.4120(0.4113) Steps 808(807.15) | Grad Norm 1.8223(1.0182) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 29.1841, Epoch Time 500.0724(491.0254), Bit/dim 3.9443(best: 3.9446), Xent 1.1112, Loss 4.4999, Error 0.4000(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 76.2836(75.0844) | Bit/dim 3.9405(3.9452) | Xent 1.1473(1.1521) | Loss 4.5142(4.5213) | Error 0.4095(0.4112) Steps 820(807.54) | Grad Norm 0.9732(1.0169) | Total Time 14.00(14.00)\n",
      "Iter 1814 | Time 74.9675(75.0809) | Bit/dim 3.9567(3.9456) | Xent 1.1459(1.1519) | Loss 4.5296(4.5215) | Error 0.4073(0.4111) Steps 802(807.37) | Grad Norm 0.7101(1.0077) | Total Time 14.00(14.00)\n",
      "Iter 1815 | Time 74.9774(75.0778) | Bit/dim 3.9393(3.9454) | Xent 1.1407(1.1515) | Loss 4.5097(4.5211) | Error 0.4099(0.4111) Steps 820(807.75) | Grad Norm 0.7509(1.0000) | Total Time 14.00(14.00)\n",
      "Iter 1816 | Time 75.7830(75.0989) | Bit/dim 3.9448(3.9454) | Xent 1.1519(1.1515) | Loss 4.5208(4.5211) | Error 0.4127(0.4111) Steps 808(807.76) | Grad Norm 1.2886(1.0086) | Total Time 14.00(14.00)\n",
      "Iter 1817 | Time 75.6146(75.1144) | Bit/dim 3.9401(3.9452) | Xent 1.1485(1.1515) | Loss 4.5144(4.5209) | Error 0.4095(0.4111) Steps 808(807.77) | Grad Norm 1.5643(1.0253) | Total Time 14.00(14.00)\n",
      "Iter 1818 | Time 75.5857(75.1285) | Bit/dim 3.9396(3.9450) | Xent 1.1383(1.1511) | Loss 4.5088(4.5206) | Error 0.4135(0.4111) Steps 802(807.59) | Grad Norm 0.8243(1.0193) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 29.3071, Epoch Time 497.9634(491.2335), Bit/dim 3.9440(best: 3.9443), Xent 1.1111, Loss 4.4996, Error 0.4024(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 76.9088(75.1819) | Bit/dim 3.9477(3.9451) | Xent 1.1625(1.1514) | Loss 4.5290(4.5208) | Error 0.4165(0.4113) Steps 808(807.61) | Grad Norm 0.9068(1.0159) | Total Time 14.00(14.00)\n",
      "Iter 1820 | Time 73.8864(75.1431) | Bit/dim 3.9515(3.9453) | Xent 1.1466(1.1513) | Loss 4.5248(4.5209) | Error 0.4096(0.4112) Steps 808(807.62) | Grad Norm 1.6444(1.0348) | Total Time 14.00(14.00)\n",
      "Iter 1821 | Time 75.3723(75.1499) | Bit/dim 3.9403(3.9452) | Xent 1.1182(1.1503) | Loss 4.4994(4.5203) | Error 0.4034(0.4110) Steps 808(807.63) | Grad Norm 1.3623(1.0446) | Total Time 14.00(14.00)\n",
      "Iter 1822 | Time 74.3655(75.1264) | Bit/dim 3.9384(3.9450) | Xent 1.1425(1.1500) | Loss 4.5096(4.5200) | Error 0.4077(0.4109) Steps 808(807.64) | Grad Norm 0.7189(1.0348) | Total Time 14.00(14.00)\n",
      "Iter 1823 | Time 77.6184(75.2012) | Bit/dim 3.9401(3.9448) | Xent 1.1501(1.1500) | Loss 4.5152(4.5198) | Error 0.4121(0.4109) Steps 808(807.65) | Grad Norm 0.9099(1.0311) | Total Time 14.00(14.00)\n",
      "Iter 1824 | Time 75.7334(75.2171) | Bit/dim 3.9414(3.9447) | Xent 1.1501(1.1500) | Loss 4.5164(4.5197) | Error 0.4110(0.4110) Steps 808(807.66) | Grad Norm 1.4614(1.0440) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 29.5086, Epoch Time 498.9991(491.4665), Bit/dim 3.9444(best: 3.9440), Xent 1.1101, Loss 4.4994, Error 0.4004(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 74.0092(75.1809) | Bit/dim 3.9419(3.9446) | Xent 1.1467(1.1499) | Loss 4.5152(4.5196) | Error 0.4065(0.4108) Steps 808(807.67) | Grad Norm 0.8521(1.0382) | Total Time 14.00(14.00)\n",
      "Iter 1826 | Time 77.6472(75.2549) | Bit/dim 3.9379(3.9444) | Xent 1.1492(1.1499) | Loss 4.5125(4.5194) | Error 0.4086(0.4108) Steps 802(807.50) | Grad Norm 0.9351(1.0351) | Total Time 14.00(14.00)\n",
      "Iter 1827 | Time 74.9582(75.2460) | Bit/dim 3.9491(3.9446) | Xent 1.1558(1.1501) | Loss 4.5270(4.5196) | Error 0.4123(0.4108) Steps 808(807.52) | Grad Norm 0.8420(1.0293) | Total Time 14.00(14.00)\n",
      "Iter 1828 | Time 76.7094(75.2899) | Bit/dim 3.9468(3.9446) | Xent 1.1307(1.1495) | Loss 4.5121(4.5194) | Error 0.4020(0.4105) Steps 802(807.35) | Grad Norm 0.7912(1.0222) | Total Time 14.00(14.00)\n",
      "Iter 1829 | Time 73.5978(75.2391) | Bit/dim 3.9417(3.9445) | Xent 1.1362(1.1491) | Loss 4.5098(4.5191) | Error 0.4058(0.4104) Steps 820(807.73) | Grad Norm 1.4240(1.0342) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 75.6103(75.2503) | Bit/dim 3.9424(3.9445) | Xent 1.1451(1.1490) | Loss 4.5150(4.5190) | Error 0.4136(0.4105) Steps 814(807.92) | Grad Norm 0.5613(1.0201) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 29.3276, Epoch Time 497.2608(491.6403), Bit/dim 3.9444(best: 3.9440), Xent 1.1093, Loss 4.4991, Error 0.3981(best: 0.3997)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1831 | Time 76.6644(75.2927) | Bit/dim 3.9434(3.9444) | Xent 1.1522(1.1491) | Loss 4.5195(4.5190) | Error 0.4127(0.4106) Steps 826(808.46) | Grad Norm 0.9824(1.0189) | Total Time 14.00(14.00)\n",
      "Iter 1832 | Time 76.9981(75.3438) | Bit/dim 3.9437(3.9444) | Xent 1.1576(1.1493) | Loss 4.5224(4.5191) | Error 0.4115(0.4106) Steps 808(808.45) | Grad Norm 1.0479(1.0198) | Total Time 14.00(14.00)\n",
      "Iter 1833 | Time 77.3937(75.4053) | Bit/dim 3.9364(3.9442) | Xent 1.1434(1.1492) | Loss 4.5081(4.5188) | Error 0.4062(0.4105) Steps 808(808.43) | Grad Norm 1.2549(1.0268) | Total Time 14.00(14.00)\n",
      "Iter 1834 | Time 76.9650(75.4521) | Bit/dim 3.9422(3.9441) | Xent 1.1465(1.1491) | Loss 4.5155(4.5187) | Error 0.4081(0.4104) Steps 814(808.60) | Grad Norm 0.7667(1.0190) | Total Time 14.00(14.00)\n",
      "Iter 1835 | Time 77.2115(75.5049) | Bit/dim 3.9475(3.9442) | Xent 1.1293(1.1485) | Loss 4.5121(4.5185) | Error 0.3972(0.4100) Steps 820(808.94) | Grad Norm 0.8641(1.0144) | Total Time 14.00(14.00)\n",
      "Iter 1836 | Time 74.2938(75.4686) | Bit/dim 3.9404(3.9441) | Xent 1.1445(1.1484) | Loss 4.5127(4.5183) | Error 0.4136(0.4101) Steps 814(809.09) | Grad Norm 2.3808(1.0554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 29.4257, Epoch Time 504.5409(492.0273), Bit/dim 3.9440(best: 3.9440), Xent 1.1074, Loss 4.4977, Error 0.4004(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1837 | Time 76.3999(75.4965) | Bit/dim 3.9416(3.9440) | Xent 1.1535(1.1485) | Loss 4.5183(4.5183) | Error 0.4115(0.4101) Steps 808(809.06) | Grad Norm 1.3899(1.0654) | Total Time 14.00(14.00)\n",
      "Iter 1838 | Time 76.5031(75.5267) | Bit/dim 3.9422(3.9440) | Xent 1.1343(1.1481) | Loss 4.5094(4.5180) | Error 0.4050(0.4100) Steps 808(809.03) | Grad Norm 1.1019(1.0665) | Total Time 14.00(14.00)\n",
      "Iter 1839 | Time 73.3098(75.4602) | Bit/dim 3.9457(3.9440) | Xent 1.1410(1.1479) | Loss 4.5162(4.5180) | Error 0.4031(0.4098) Steps 808(809.00) | Grad Norm 2.3708(1.1056) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 74.9850(75.4460) | Bit/dim 3.9371(3.9438) | Xent 1.1322(1.1474) | Loss 4.5032(4.5175) | Error 0.4060(0.4097) Steps 808(808.97) | Grad Norm 1.6997(1.1235) | Total Time 14.00(14.00)\n",
      "Iter 1841 | Time 78.8143(75.5470) | Bit/dim 3.9348(3.9436) | Xent 1.1389(1.1472) | Loss 4.5042(4.5171) | Error 0.4131(0.4098) Steps 820(809.30) | Grad Norm 0.8128(1.1141) | Total Time 14.00(14.00)\n",
      "Iter 1842 | Time 76.4662(75.5746) | Bit/dim 3.9527(3.9438) | Xent 1.1646(1.1477) | Loss 4.5350(4.5177) | Error 0.4159(0.4100) Steps 814(809.44) | Grad Norm 1.2222(1.1174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 29.2478, Epoch Time 501.0008(492.2965), Bit/dim 3.9436(best: 3.9440), Xent 1.1071, Loss 4.4971, Error 0.4004(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1843 | Time 73.6174(75.5159) | Bit/dim 3.9438(3.9438) | Xent 1.1496(1.1477) | Loss 4.5186(4.5177) | Error 0.4059(0.4098) Steps 808(809.40) | Grad Norm 1.4683(1.1279) | Total Time 14.00(14.00)\n",
      "Iter 1844 | Time 75.8420(75.5256) | Bit/dim 3.9434(3.9438) | Xent 1.1414(1.1475) | Loss 4.5141(4.5176) | Error 0.4065(0.4097) Steps 814(809.54) | Grad Norm 1.0383(1.1252) | Total Time 14.00(14.00)\n",
      "Iter 1845 | Time 72.9753(75.4491) | Bit/dim 3.9461(3.9439) | Xent 1.1485(1.1476) | Loss 4.5204(4.5177) | Error 0.4060(0.4096) Steps 808(809.49) | Grad Norm 0.7584(1.1142) | Total Time 14.00(14.00)\n",
      "Iter 1846 | Time 74.7207(75.4273) | Bit/dim 3.9474(3.9440) | Xent 1.1432(1.1474) | Loss 4.5190(4.5177) | Error 0.4114(0.4097) Steps 808(809.44) | Grad Norm 0.9323(1.1088) | Total Time 14.00(14.00)\n",
      "Iter 1847 | Time 75.2362(75.4215) | Bit/dim 3.9440(3.9440) | Xent 1.1326(1.1470) | Loss 4.5103(4.5175) | Error 0.4100(0.4097) Steps 808(809.40) | Grad Norm 1.2798(1.1139) | Total Time 14.00(14.00)\n",
      "Iter 1848 | Time 75.9811(75.4383) | Bit/dim 3.9305(3.9436) | Xent 1.1278(1.1464) | Loss 4.4944(4.5168) | Error 0.3989(0.4094) Steps 820(809.72) | Grad Norm 1.3848(1.1220) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 29.4610, Epoch Time 493.3095(492.3269), Bit/dim 3.9437(best: 3.9436), Xent 1.1071, Loss 4.4973, Error 0.3963(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1849 | Time 76.9842(75.4847) | Bit/dim 3.9474(3.9437) | Xent 1.1409(1.1463) | Loss 4.5178(4.5168) | Error 0.4064(0.4093) Steps 814(809.85) | Grad Norm 0.6599(1.1082) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 75.6715(75.4903) | Bit/dim 3.9529(3.9440) | Xent 1.1297(1.1458) | Loss 4.5178(4.5169) | Error 0.4059(0.4092) Steps 802(809.61) | Grad Norm 1.0964(1.1078) | Total Time 14.00(14.00)\n",
      "Iter 1851 | Time 77.7937(75.5594) | Bit/dim 3.9230(3.9433) | Xent 1.1357(1.1455) | Loss 4.4908(4.5161) | Error 0.4038(0.4090) Steps 808(809.56) | Grad Norm 1.1785(1.1099) | Total Time 14.00(14.00)\n",
      "Iter 1852 | Time 75.2315(75.5496) | Bit/dim 3.9371(3.9432) | Xent 1.1467(1.1455) | Loss 4.5105(4.5159) | Error 0.4087(0.4090) Steps 808(809.52) | Grad Norm 1.2015(1.1127) | Total Time 14.00(14.00)\n",
      "Iter 1853 | Time 77.9368(75.6212) | Bit/dim 3.9374(3.9430) | Xent 1.1362(1.1452) | Loss 4.5055(4.5156) | Error 0.4048(0.4089) Steps 808(809.47) | Grad Norm 0.5660(1.0963) | Total Time 14.00(14.00)\n",
      "Iter 1854 | Time 75.2509(75.6101) | Bit/dim 3.9500(3.9432) | Xent 1.1522(1.1454) | Loss 4.5261(4.5159) | Error 0.4126(0.4090) Steps 808(809.43) | Grad Norm 0.8520(1.0889) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 29.4210, Epoch Time 503.8686(492.6732), Bit/dim 3.9436(best: 3.9436), Xent 1.1053, Loss 4.4962, Error 0.3966(best: 0.3963)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1855 | Time 74.6475(75.5812) | Bit/dim 3.9403(3.9431) | Xent 1.1380(1.1452) | Loss 4.5093(4.5157) | Error 0.4030(0.4088) Steps 808(809.38) | Grad Norm 1.2947(1.0951) | Total Time 14.00(14.00)\n",
      "Iter 1856 | Time 74.8501(75.5593) | Bit/dim 3.9354(3.9429) | Xent 1.1226(1.1445) | Loss 4.4967(4.5151) | Error 0.4031(0.4086) Steps 820(809.70) | Grad Norm 0.8927(1.0890) | Total Time 14.00(14.00)\n",
      "Iter 1857 | Time 76.4514(75.5860) | Bit/dim 3.9365(3.9427) | Xent 1.1194(1.1438) | Loss 4.4962(4.5146) | Error 0.4039(0.4085) Steps 808(809.65) | Grad Norm 0.5672(1.0734) | Total Time 14.00(14.00)\n",
      "Iter 1858 | Time 75.3575(75.5792) | Bit/dim 3.9433(3.9427) | Xent 1.1369(1.1436) | Loss 4.5117(4.5145) | Error 0.4026(0.4083) Steps 808(809.60) | Grad Norm 0.8831(1.0677) | Total Time 14.00(14.00)\n",
      "Iter 1859 | Time 75.9278(75.5896) | Bit/dim 3.9517(3.9430) | Xent 1.1566(1.1440) | Loss 4.5300(4.5150) | Error 0.4116(0.4084) Steps 808(809.55) | Grad Norm 1.0313(1.0666) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 81.0953(75.7548) | Bit/dim 3.9412(3.9429) | Xent 1.1441(1.1440) | Loss 4.5132(4.5149) | Error 0.4026(0.4082) Steps 826(810.05) | Grad Norm 1.3116(1.0739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 29.1701, Epoch Time 502.7274(492.9748), Bit/dim 3.9434(best: 3.9436), Xent 1.1054, Loss 4.4961, Error 0.3990(best: 0.3963)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1861 | Time 75.5520(75.7487) | Bit/dim 3.9467(3.9430) | Xent 1.1375(1.1438) | Loss 4.5155(4.5149) | Error 0.4061(0.4082) Steps 808(809.99) | Grad Norm 0.8167(1.0662) | Total Time 14.00(14.00)\n",
      "Iter 1862 | Time 79.4852(75.8608) | Bit/dim 3.9374(3.9429) | Xent 1.1366(1.1436) | Loss 4.5057(4.5146) | Error 0.4028(0.4080) Steps 808(809.93) | Grad Norm 1.1656(1.0692) | Total Time 14.00(14.00)\n",
      "Iter 1863 | Time 76.4431(75.8783) | Bit/dim 3.9480(3.9430) | Xent 1.1517(1.1438) | Loss 4.5239(4.5149) | Error 0.4129(0.4082) Steps 808(809.87) | Grad Norm 1.4422(1.0804) | Total Time 14.00(14.00)\n",
      "Iter 1864 | Time 75.2325(75.8589) | Bit/dim 3.9474(3.9432) | Xent 1.1477(1.1439) | Loss 4.5213(4.5151) | Error 0.4054(0.4081) Steps 820(810.17) | Grad Norm 0.5915(1.0657) | Total Time 14.00(14.00)\n",
      "Iter 1865 | Time 76.1585(75.8679) | Bit/dim 3.9390(3.9430) | Xent 1.1186(1.1432) | Loss 4.4983(4.5146) | Error 0.4060(0.4080) Steps 802(809.93) | Grad Norm 0.6979(1.0547) | Total Time 14.00(14.00)\n",
      "Iter 1866 | Time 78.1342(75.9359) | Bit/dim 3.9391(3.9429) | Xent 1.1503(1.1434) | Loss 4.5143(4.5146) | Error 0.4065(0.4080) Steps 820(810.23) | Grad Norm 0.7152(1.0445) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 29.1604, Epoch Time 505.6399(493.3548), Bit/dim 3.9417(best: 3.9434), Xent 1.1067, Loss 4.4950, Error 0.3982(best: 0.3963)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1867 | Time 74.7808(75.9012) | Bit/dim 3.9424(3.9429) | Xent 1.1328(1.1431) | Loss 4.5088(4.5144) | Error 0.4066(0.4079) Steps 820(810.52) | Grad Norm 0.6524(1.0327) | Total Time 14.00(14.00)\n",
      "Iter 1868 | Time 75.5242(75.8899) | Bit/dim 3.9427(3.9429) | Xent 1.1318(1.1427) | Loss 4.5086(4.5142) | Error 0.4051(0.4078) Steps 802(810.27) | Grad Norm 0.6881(1.0224) | Total Time 14.00(14.00)\n",
      "Iter 1869 | Time 74.5802(75.8506) | Bit/dim 3.9393(3.9428) | Xent 1.1489(1.1429) | Loss 4.5137(4.5142) | Error 0.4058(0.4078) Steps 808(810.20) | Grad Norm 0.9314(1.0197) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 76.6863(75.8757) | Bit/dim 3.9415(3.9427) | Xent 1.1278(1.1424) | Loss 4.5054(4.5140) | Error 0.4029(0.4076) Steps 826(810.67) | Grad Norm 0.8011(1.0131) | Total Time 14.00(14.00)\n",
      "Iter 1871 | Time 73.4695(75.8035) | Bit/dim 3.9376(3.9426) | Xent 1.1569(1.1429) | Loss 4.5160(4.5140) | Error 0.4110(0.4077) Steps 820(810.95) | Grad Norm 0.6222(1.0014) | Total Time 14.00(14.00)\n",
      "Iter 1872 | Time 77.0372(75.8405) | Bit/dim 3.9432(3.9426) | Xent 1.1396(1.1428) | Loss 4.5130(4.5140) | Error 0.4123(0.4079) Steps 820(811.22) | Grad Norm 0.7576(0.9941) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 29.4220, Epoch Time 496.8265(493.4589), Bit/dim 3.9421(best: 3.9417), Xent 1.1050, Loss 4.4946, Error 0.3982(best: 0.3963)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1873 | Time 77.6730(75.8955) | Bit/dim 3.9397(3.9425) | Xent 1.1369(1.1426) | Loss 4.5082(4.5138) | Error 0.4009(0.4077) Steps 808(811.13) | Grad Norm 0.9162(0.9917) | Total Time 14.00(14.00)\n",
      "Iter 1874 | Time 76.6607(75.9185) | Bit/dim 3.9489(3.9427) | Xent 1.1454(1.1427) | Loss 4.5216(4.5141) | Error 0.4154(0.4079) Steps 826(811.57) | Grad Norm 0.9484(0.9904) | Total Time 14.00(14.00)\n",
      "Iter 1875 | Time 73.8824(75.8574) | Bit/dim 3.9412(3.9427) | Xent 1.1414(1.1427) | Loss 4.5119(4.5140) | Error 0.4044(0.4078) Steps 820(811.83) | Grad Norm 1.2927(0.9995) | Total Time 14.00(14.00)\n",
      "Iter 1876 | Time 77.8497(75.9171) | Bit/dim 3.9431(3.9427) | Xent 1.1310(1.1423) | Loss 4.5086(4.5138) | Error 0.4056(0.4077) Steps 808(811.71) | Grad Norm 0.8052(0.9937) | Total Time 14.00(14.00)\n",
      "Iter 1877 | Time 76.6721(75.9398) | Bit/dim 3.9333(3.9424) | Xent 1.1397(1.1422) | Loss 4.5032(4.5135) | Error 0.4025(0.4076) Steps 802(811.42) | Grad Norm 0.9009(0.9909) | Total Time 14.00(14.00)\n",
      "Iter 1878 | Time 74.7609(75.9044) | Bit/dim 3.9348(3.9422) | Xent 1.1312(1.1419) | Loss 4.5004(4.5131) | Error 0.4081(0.4076) Steps 802(811.14) | Grad Norm 1.3892(1.0028) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 29.4804, Epoch Time 502.5346(493.7312), Bit/dim 3.9411(best: 3.9417), Xent 1.1032, Loss 4.4927, Error 0.3951(best: 0.3963)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1879 | Time 76.0541(75.9089) | Bit/dim 3.9389(3.9421) | Xent 1.1402(1.1418) | Loss 4.5090(4.5130) | Error 0.4040(0.4075) Steps 808(811.04) | Grad Norm 1.0687(1.0048) | Total Time 14.00(14.00)\n",
      "Iter 1880 | Time 77.5981(75.9596) | Bit/dim 3.9440(3.9421) | Xent 1.1531(1.1422) | Loss 4.5205(4.5132) | Error 0.4080(0.4075) Steps 814(811.13) | Grad Norm 0.5680(0.9917) | Total Time 14.00(14.00)\n",
      "Iter 1881 | Time 78.7671(76.0438) | Bit/dim 3.9364(3.9420) | Xent 1.1406(1.1421) | Loss 4.5067(4.5130) | Error 0.4119(0.4076) Steps 820(811.40) | Grad Norm 0.7826(0.9854) | Total Time 14.00(14.00)\n",
      "Iter 1882 | Time 76.7584(76.0653) | Bit/dim 3.9335(3.9417) | Xent 1.1306(1.1418) | Loss 4.4988(4.5126) | Error 0.4039(0.4075) Steps 832(812.02) | Grad Norm 1.7311(1.0078) | Total Time 14.00(14.00)\n",
      "Iter 1883 | Time 77.1114(76.0966) | Bit/dim 3.9390(3.9416) | Xent 1.1380(1.1417) | Loss 4.5080(4.5125) | Error 0.4000(0.4073) Steps 826(812.44) | Grad Norm 1.8813(1.0340) | Total Time 14.00(14.00)\n",
      "Iter 1884 | Time 77.9285(76.1516) | Bit/dim 3.9517(3.9419) | Xent 1.1390(1.1416) | Loss 4.5212(4.5127) | Error 0.4061(0.4072) Steps 814(812.48) | Grad Norm 0.6658(1.0230) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 29.1608, Epoch Time 509.0223(494.1899), Bit/dim 3.9419(best: 3.9411), Xent 1.1020, Loss 4.4929, Error 0.3951(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1885 | Time 74.6309(76.1060) | Bit/dim 3.9492(3.9421) | Xent 1.1464(1.1417) | Loss 4.5223(4.5130) | Error 0.4141(0.4075) Steps 814(812.53) | Grad Norm 1.8630(1.0482) | Total Time 14.00(14.00)\n",
      "Iter 1886 | Time 75.3714(76.0839) | Bit/dim 3.9395(3.9421) | Xent 1.1489(1.1420) | Loss 4.5139(4.5130) | Error 0.4089(0.4075) Steps 814(812.57) | Grad Norm 0.9181(1.0443) | Total Time 14.00(14.00)\n",
      "Iter 1887 | Time 78.2761(76.1497) | Bit/dim 3.9430(3.9421) | Xent 1.1373(1.1418) | Loss 4.5117(4.5130) | Error 0.4009(0.4073) Steps 826(812.98) | Grad Norm 1.4891(1.0576) | Total Time 14.00(14.00)\n",
      "Iter 1888 | Time 77.1513(76.1798) | Bit/dim 3.9392(3.9420) | Xent 1.1204(1.1412) | Loss 4.4994(4.5126) | Error 0.3949(0.4069) Steps 814(813.01) | Grad Norm 1.1182(1.0594) | Total Time 14.00(14.00)\n",
      "Iter 1889 | Time 76.1854(76.1799) | Bit/dim 3.9320(3.9417) | Xent 1.1562(1.1416) | Loss 4.5101(4.5125) | Error 0.4149(0.4072) Steps 820(813.22) | Grad Norm 0.7996(1.0516) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 76.3317(76.1845) | Bit/dim 3.9433(3.9417) | Xent 1.1194(1.1410) | Loss 4.5031(4.5122) | Error 0.3940(0.4068) Steps 826(813.60) | Grad Norm 0.8199(1.0447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 29.1112, Epoch Time 502.7148(494.4457), Bit/dim 3.9416(best: 3.9411), Xent 1.1019, Loss 4.4926, Error 0.3977(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1891 | Time 77.4691(76.2230) | Bit/dim 3.9363(3.9416) | Xent 1.1332(1.1407) | Loss 4.5028(4.5119) | Error 0.4015(0.4066) Steps 814(813.61) | Grad Norm 1.4970(1.0582) | Total Time 14.00(14.00)\n",
      "Iter 1892 | Time 78.9413(76.3046) | Bit/dim 3.9498(3.9418) | Xent 1.1437(1.1408) | Loss 4.5217(4.5122) | Error 0.4020(0.4065) Steps 814(813.62) | Grad Norm 1.0059(1.0567) | Total Time 14.00(14.00)\n",
      "Iter 1893 | Time 78.2955(76.3643) | Bit/dim 3.9418(3.9418) | Xent 1.1119(1.1399) | Loss 4.4978(4.5118) | Error 0.3999(0.4063) Steps 832(814.17) | Grad Norm 0.8454(1.0503) | Total Time 14.00(14.00)\n",
      "Iter 1894 | Time 76.9644(76.3823) | Bit/dim 3.9419(3.9418) | Xent 1.1405(1.1400) | Loss 4.5121(4.5118) | Error 0.4064(0.4063) Steps 808(813.99) | Grad Norm 1.0304(1.0497) | Total Time 14.00(14.00)\n",
      "Iter 1895 | Time 76.3820(76.3823) | Bit/dim 3.9376(3.9417) | Xent 1.1333(1.1398) | Loss 4.5043(4.5116) | Error 0.4018(0.4061) Steps 820(814.17) | Grad Norm 1.0257(1.0490) | Total Time 14.00(14.00)\n",
      "Iter 1896 | Time 75.6964(76.3617) | Bit/dim 3.9365(3.9415) | Xent 1.1509(1.1401) | Loss 4.5119(4.5116) | Error 0.4101(0.4063) Steps 802(813.80) | Grad Norm 1.5118(1.0629) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 29.1761, Epoch Time 508.3773(494.8636), Bit/dim 3.9416(best: 3.9411), Xent 1.1001, Loss 4.4917, Error 0.3976(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1897 | Time 78.6185(76.4294) | Bit/dim 3.9356(3.9414) | Xent 1.1351(1.1399) | Loss 4.5031(4.5113) | Error 0.4095(0.4064) Steps 820(813.99) | Grad Norm 1.0771(1.0633) | Total Time 14.00(14.00)\n",
      "Iter 1898 | Time 78.2720(76.4847) | Bit/dim 3.9515(3.9417) | Xent 1.1354(1.1398) | Loss 4.5192(4.5116) | Error 0.4067(0.4064) Steps 808(813.81) | Grad Norm 1.1345(1.0655) | Total Time 14.00(14.00)\n",
      "Iter 1899 | Time 75.0247(76.4409) | Bit/dim 3.9411(3.9417) | Xent 1.1388(1.1398) | Loss 4.5105(4.5115) | Error 0.4085(0.4064) Steps 820(814.00) | Grad Norm 0.8080(1.0577) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 76.9162(76.4551) | Bit/dim 3.9385(3.9416) | Xent 1.1252(1.1393) | Loss 4.5011(4.5112) | Error 0.4008(0.4063) Steps 814(814.00) | Grad Norm 1.4538(1.0696) | Total Time 14.00(14.00)\n",
      "Iter 1901 | Time 75.4527(76.4251) | Bit/dim 3.9398(3.9415) | Xent 1.1222(1.1388) | Loss 4.5009(4.5109) | Error 0.4021(0.4061) Steps 820(814.18) | Grad Norm 1.9215(1.0952) | Total Time 14.00(14.00)\n",
      "Iter 1902 | Time 78.3255(76.4821) | Bit/dim 3.9346(3.9413) | Xent 1.1243(1.1384) | Loss 4.4968(4.5105) | Error 0.4020(0.4060) Steps 808(813.99) | Grad Norm 0.6360(1.0814) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 29.3752, Epoch Time 507.4364(495.2408), Bit/dim 3.9406(best: 3.9411), Xent 1.1006, Loss 4.4909, Error 0.3979(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1903 | Time 75.3108(76.4469) | Bit/dim 3.9423(3.9413) | Xent 1.1221(1.1379) | Loss 4.5034(4.5103) | Error 0.3979(0.4058) Steps 814(813.99) | Grad Norm 1.8382(1.1041) | Total Time 14.00(14.00)\n",
      "Iter 1904 | Time 75.1891(76.4092) | Bit/dim 3.9412(3.9413) | Xent 1.1401(1.1380) | Loss 4.5112(4.5103) | Error 0.4010(0.4056) Steps 820(814.17) | Grad Norm 1.1802(1.1064) | Total Time 14.00(14.00)\n",
      "Iter 1905 | Time 81.2274(76.5538) | Bit/dim 3.9419(3.9413) | Xent 1.1342(1.1379) | Loss 4.5090(4.5103) | Error 0.4020(0.4055) Steps 808(813.99) | Grad Norm 1.7088(1.1245) | Total Time 14.00(14.00)\n",
      "Iter 1906 | Time 78.9271(76.6250) | Bit/dim 3.9302(3.9410) | Xent 1.1052(1.1369) | Loss 4.4829(4.5094) | Error 0.4020(0.4054) Steps 826(814.35) | Grad Norm 1.0228(1.1214) | Total Time 14.00(14.00)\n",
      "Iter 1907 | Time 79.8235(76.7209) | Bit/dim 3.9334(3.9408) | Xent 1.1469(1.1372) | Loss 4.5068(4.5094) | Error 0.4087(0.4055) Steps 808(814.16) | Grad Norm 1.5638(1.1347) | Total Time 14.00(14.00)\n",
      "Iter 1908 | Time 75.4908(76.6840) | Bit/dim 3.9511(3.9411) | Xent 1.1478(1.1375) | Loss 4.5250(4.5098) | Error 0.4134(0.4057) Steps 826(814.51) | Grad Norm 1.8346(1.1557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 29.7352, Epoch Time 511.0776(495.7159), Bit/dim 3.9405(best: 3.9406), Xent 1.0992, Loss 4.4901, Error 0.3975(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1909 | Time 79.9747(76.7827) | Bit/dim 3.9464(3.9412) | Xent 1.1308(1.1373) | Loss 4.5118(4.5099) | Error 0.4036(0.4057) Steps 820(814.68) | Grad Norm 1.0735(1.1532) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 79.2482(76.8567) | Bit/dim 3.9349(3.9411) | Xent 1.1457(1.1376) | Loss 4.5078(4.5098) | Error 0.4117(0.4059) Steps 814(814.66) | Grad Norm 0.6699(1.1387) | Total Time 14.00(14.00)\n",
      "Iter 1911 | Time 79.4580(76.9347) | Bit/dim 3.9383(3.9410) | Xent 1.1544(1.1381) | Loss 4.5155(4.5100) | Error 0.4071(0.4059) Steps 808(814.46) | Grad Norm 1.9888(1.1642) | Total Time 14.00(14.00)\n",
      "Iter 1912 | Time 74.4017(76.8587) | Bit/dim 3.9437(3.9411) | Xent 1.1557(1.1386) | Loss 4.5215(4.5104) | Error 0.4134(0.4061) Steps 808(814.26) | Grad Norm 1.5370(1.1754) | Total Time 14.00(14.00)\n",
      "Iter 1913 | Time 76.1672(76.8380) | Bit/dim 3.9308(3.9408) | Xent 1.1103(1.1377) | Loss 4.4860(4.5096) | Error 0.3948(0.4058) Steps 820(814.43) | Grad Norm 1.6478(1.1896) | Total Time 14.00(14.00)\n",
      "Iter 1914 | Time 76.6342(76.8319) | Bit/dim 3.9420(3.9408) | Xent 1.1214(1.1372) | Loss 4.5027(4.5094) | Error 0.4029(0.4057) Steps 826(814.78) | Grad Norm 1.8020(1.2080) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 29.3791, Epoch Time 510.8429(496.1697), Bit/dim 3.9405(best: 3.9405), Xent 1.1008, Loss 4.4909, Error 0.3974(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1915 | Time 77.3026(76.8460) | Bit/dim 3.9444(3.9409) | Xent 1.1239(1.1368) | Loss 4.5063(4.5093) | Error 0.3990(0.4055) Steps 820(814.94) | Grad Norm 2.0672(1.2337) | Total Time 14.00(14.00)\n",
      "Iter 1916 | Time 74.7110(76.7820) | Bit/dim 3.9266(3.9405) | Xent 1.1413(1.1370) | Loss 4.4972(4.5090) | Error 0.4095(0.4056) Steps 826(815.27) | Grad Norm 1.0330(1.2277) | Total Time 14.00(14.00)\n",
      "Iter 1917 | Time 79.2682(76.8565) | Bit/dim 3.9437(3.9406) | Xent 1.1432(1.1372) | Loss 4.5152(4.5091) | Error 0.4091(0.4057) Steps 820(815.41) | Grad Norm 0.8069(1.2151) | Total Time 14.00(14.00)\n",
      "Iter 1918 | Time 79.6315(76.9398) | Bit/dim 3.9425(3.9406) | Xent 1.1267(1.1368) | Loss 4.5058(4.5090) | Error 0.3968(0.4055) Steps 814(815.37) | Grad Norm 0.7755(1.2019) | Total Time 14.00(14.00)\n",
      "Iter 1919 | Time 78.4430(76.9849) | Bit/dim 3.9430(3.9407) | Xent 1.1435(1.1370) | Loss 4.5148(4.5092) | Error 0.4060(0.4055) Steps 814(815.33) | Grad Norm 1.2833(1.2043) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 75.9751(76.9546) | Bit/dim 3.9383(3.9406) | Xent 1.1406(1.1372) | Loss 4.5086(4.5092) | Error 0.4048(0.4055) Steps 802(814.93) | Grad Norm 0.9439(1.1965) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 29.1927, Epoch Time 510.0290(496.5855), Bit/dim 3.9403(best: 3.9405), Xent 1.0977, Loss 4.4891, Error 0.3967(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1921 | Time 75.7106(76.9173) | Bit/dim 3.9411(3.9406) | Xent 1.1378(1.1372) | Loss 4.5100(4.5092) | Error 0.4089(0.4056) Steps 808(814.72) | Grad Norm 0.6716(1.1808) | Total Time 14.00(14.00)\n",
      "Iter 1922 | Time 74.9927(76.8595) | Bit/dim 3.9493(3.9409) | Xent 1.1233(1.1368) | Loss 4.5110(4.5093) | Error 0.4025(0.4055) Steps 814(814.70) | Grad Norm 1.2645(1.1833) | Total Time 14.00(14.00)\n",
      "Iter 1923 | Time 77.9251(76.8915) | Bit/dim 3.9401(3.9409) | Xent 1.1393(1.1368) | Loss 4.5098(4.5093) | Error 0.4040(0.4054) Steps 814(814.68) | Grad Norm 1.0027(1.1779) | Total Time 14.00(14.00)\n",
      "Iter 1924 | Time 78.3577(76.9355) | Bit/dim 3.9313(3.9406) | Xent 1.1429(1.1370) | Loss 4.5027(4.5091) | Error 0.4160(0.4057) Steps 814(814.66) | Grad Norm 0.7198(1.1641) | Total Time 14.00(14.00)\n",
      "Iter 1925 | Time 76.5231(76.9231) | Bit/dim 3.9459(3.9407) | Xent 1.1200(1.1365) | Loss 4.5059(4.5090) | Error 0.4000(0.4056) Steps 826(815.00) | Grad Norm 1.4385(1.1724) | Total Time 14.00(14.00)\n",
      "Iter 1926 | Time 76.0041(76.8955) | Bit/dim 3.9280(3.9404) | Xent 1.1293(1.1363) | Loss 4.4927(4.5085) | Error 0.4054(0.4056) Steps 814(814.97) | Grad Norm 0.8374(1.1623) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 29.8093, Epoch Time 504.8227(496.8326), Bit/dim 3.9398(best: 3.9403), Xent 1.0967, Loss 4.4881, Error 0.3938(best: 0.3951)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1927 | Time 76.2052(76.8748) | Bit/dim 3.9398(3.9403) | Xent 1.1454(1.1366) | Loss 4.5124(4.5086) | Error 0.4051(0.4055) Steps 826(815.30) | Grad Norm 1.2607(1.1653) | Total Time 14.00(14.00)\n",
      "Iter 1928 | Time 79.4583(76.9523) | Bit/dim 3.9402(3.9403) | Xent 1.1180(1.1360) | Loss 4.4992(4.5083) | Error 0.4012(0.4054) Steps 814(815.26) | Grad Norm 0.7402(1.1525) | Total Time 14.00(14.00)\n",
      "Iter 1929 | Time 77.8556(76.9794) | Bit/dim 3.9376(3.9403) | Xent 1.1304(1.1358) | Loss 4.5028(4.5082) | Error 0.3938(0.4051) Steps 820(815.40) | Grad Norm 0.6603(1.1377) | Total Time 14.00(14.00)\n",
      "Iter 1930 | Time 79.1086(77.0433) | Bit/dim 3.9324(3.9400) | Xent 1.1254(1.1355) | Loss 4.4950(4.5078) | Error 0.4026(0.4050) Steps 826(815.72) | Grad Norm 0.7592(1.1264) | Total Time 14.00(14.00)\n",
      "Iter 1931 | Time 77.0846(77.0445) | Bit/dim 3.9416(3.9401) | Xent 1.1420(1.1357) | Loss 4.5126(4.5079) | Error 0.4074(0.4051) Steps 802(815.31) | Grad Norm 0.8883(1.1192) | Total Time 14.00(14.00)\n",
      "Iter 1932 | Time 78.3040(77.0823) | Bit/dim 3.9410(3.9401) | Xent 1.1245(1.1354) | Loss 4.5033(4.5078) | Error 0.3958(0.4048) Steps 838(815.99) | Grad Norm 0.7564(1.1084) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 29.3658, Epoch Time 513.0070(497.3178), Bit/dim 3.9394(best: 3.9398), Xent 1.0962, Loss 4.4875, Error 0.3963(best: 0.3938)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1933 | Time 75.1790(77.0252) | Bit/dim 3.9367(3.9400) | Xent 1.1370(1.1354) | Loss 4.5052(4.5077) | Error 0.4070(0.4049) Steps 826(816.29) | Grad Norm 0.5867(1.0927) | Total Time 14.00(14.00)\n",
      "Iter 1934 | Time 80.7852(77.1380) | Bit/dim 3.9374(3.9399) | Xent 1.1308(1.1353) | Loss 4.5029(4.5076) | Error 0.3998(0.4047) Steps 826(816.58) | Grad Norm 1.1668(1.0949) | Total Time 14.00(14.00)\n",
      "Iter 1935 | Time 77.7459(77.1563) | Bit/dim 3.9381(3.9399) | Xent 1.1076(1.1345) | Loss 4.4919(4.5071) | Error 0.3981(0.4045) Steps 808(816.32) | Grad Norm 0.7646(1.0850) | Total Time 14.00(14.00)\n",
      "Iter 1936 | Time 76.0917(77.1243) | Bit/dim 3.9392(3.9398) | Xent 1.1136(1.1338) | Loss 4.4960(4.5068) | Error 0.3938(0.4042) Steps 808(816.07) | Grad Norm 0.8002(1.0765) | Total Time 14.00(14.00)\n",
      "Iter 1937 | Time 76.2005(77.0966) | Bit/dim 3.9391(3.9398) | Xent 1.1244(1.1335) | Loss 4.5013(4.5066) | Error 0.4018(0.4041) Steps 820(816.19) | Grad Norm 1.0651(1.0761) | Total Time 14.00(14.00)\n",
      "Iter 1938 | Time 79.5854(77.1713) | Bit/dim 3.9363(3.9397) | Xent 1.1193(1.1331) | Loss 4.4959(4.5063) | Error 0.4012(0.4040) Steps 820(816.31) | Grad Norm 1.1149(1.0773) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 29.1263, Epoch Time 510.3862(497.7099), Bit/dim 3.9397(best: 3.9394), Xent 1.0979, Loss 4.4887, Error 0.3962(best: 0.3938)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1939 | Time 77.4106(77.1785) | Bit/dim 3.9433(3.9398) | Xent 1.1440(1.1334) | Loss 4.5153(4.5065) | Error 0.4107(0.4042) Steps 808(816.06) | Grad Norm 0.8998(1.0720) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 78.8953(77.2300) | Bit/dim 3.9254(3.9394) | Xent 1.1490(1.1339) | Loss 4.4998(4.5063) | Error 0.4090(0.4044) Steps 826(816.36) | Grad Norm 0.8963(1.0667) | Total Time 14.00(14.00)\n",
      "Iter 1941 | Time 80.3208(77.3227) | Bit/dim 3.9446(3.9395) | Xent 1.1167(1.1334) | Loss 4.5030(4.5062) | Error 0.3989(0.4042) Steps 832(816.82) | Grad Norm 0.7567(1.0574) | Total Time 14.00(14.00)\n",
      "Iter 1942 | Time 80.4824(77.4175) | Bit/dim 3.9325(3.9393) | Xent 1.1366(1.1335) | Loss 4.5008(4.5061) | Error 0.4059(0.4043) Steps 814(816.74) | Grad Norm 0.7863(1.0493) | Total Time 14.00(14.00)\n",
      "Iter 1943 | Time 77.3260(77.4147) | Bit/dim 3.9452(3.9395) | Xent 1.1134(1.1329) | Loss 4.5019(4.5060) | Error 0.3942(0.4040) Steps 808(816.48) | Grad Norm 0.6753(1.0381) | Total Time 14.00(14.00)\n",
      "Iter 1944 | Time 77.1211(77.4059) | Bit/dim 3.9378(3.9395) | Xent 1.1187(1.1325) | Loss 4.4972(4.5057) | Error 0.3955(0.4037) Steps 814(816.40) | Grad Norm 0.7736(1.0301) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 29.5251, Epoch Time 516.5141(498.2740), Bit/dim 3.9381(best: 3.9394), Xent 1.0956, Loss 4.4859, Error 0.3932(best: 0.3938)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1945 | Time 77.6725(77.4139) | Bit/dim 3.9299(3.9392) | Xent 1.1200(1.1321) | Loss 4.4899(4.5052) | Error 0.3995(0.4036) Steps 820(816.51) | Grad Norm 0.9622(1.0281) | Total Time 14.00(14.00)\n",
      "Iter 1946 | Time 78.8329(77.4565) | Bit/dim 3.9445(3.9393) | Xent 1.1359(1.1322) | Loss 4.5124(4.5054) | Error 0.4115(0.4038) Steps 808(816.26) | Grad Norm 1.1746(1.0325) | Total Time 14.00(14.00)\n",
      "Iter 1947 | Time 76.3265(77.4226) | Bit/dim 3.9511(3.9397) | Xent 1.1228(1.1319) | Loss 4.5125(4.5057) | Error 0.4012(0.4037) Steps 820(816.37) | Grad Norm 0.6799(1.0219) | Total Time 14.00(14.00)\n",
      "Iter 1948 | Time 73.9145(77.3173) | Bit/dim 3.9328(3.9395) | Xent 1.1336(1.1320) | Loss 4.4996(4.5055) | Error 0.4096(0.4039) Steps 820(816.48) | Grad Norm 1.4981(1.0362) | Total Time 14.00(14.00)\n",
      "Iter 1949 | Time 77.3127(77.3172) | Bit/dim 3.9402(3.9395) | Xent 1.1241(1.1317) | Loss 4.5023(4.5054) | Error 0.4022(0.4039) Steps 832(816.94) | Grad Norm 0.7654(1.0281) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 78.6171(77.3562) | Bit/dim 3.9321(3.9393) | Xent 1.1227(1.1315) | Loss 4.4934(4.5050) | Error 0.4004(0.4038) Steps 820(817.03) | Grad Norm 1.3857(1.0388) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 29.3490, Epoch Time 507.6104(498.5541), Bit/dim 3.9386(best: 3.9381), Xent 1.0961, Loss 4.4867, Error 0.3916(best: 0.3932)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1951 | Time 77.3248(77.3553) | Bit/dim 3.9385(3.9393) | Xent 1.1208(1.1311) | Loss 4.4990(4.5048) | Error 0.3945(0.4035) Steps 808(816.76) | Grad Norm 1.4052(1.0498) | Total Time 14.00(14.00)\n",
      "Iter 1952 | Time 79.0831(77.4071) | Bit/dim 3.9371(3.9392) | Xent 1.1137(1.1306) | Loss 4.4939(4.5045) | Error 0.3951(0.4032) Steps 826(817.04) | Grad Norm 1.2502(1.0558) | Total Time 14.00(14.00)\n",
      "Iter 1953 | Time 75.5179(77.3504) | Bit/dim 3.9358(3.9391) | Xent 1.1274(1.1305) | Loss 4.4995(4.5044) | Error 0.3959(0.4030) Steps 826(817.31) | Grad Norm 1.5204(1.0697) | Total Time 14.00(14.00)\n",
      "Iter 1954 | Time 76.5797(77.3273) | Bit/dim 3.9314(3.9389) | Xent 1.1272(1.1304) | Loss 4.4950(4.5041) | Error 0.4075(0.4031) Steps 808(817.03) | Grad Norm 0.8827(1.0641) | Total Time 14.00(14.00)\n",
      "Iter 1955 | Time 77.3772(77.3288) | Bit/dim 3.9360(3.9388) | Xent 1.1471(1.1309) | Loss 4.5095(4.5042) | Error 0.4115(0.4034) Steps 808(816.76) | Grad Norm 1.6779(1.0825) | Total Time 14.00(14.00)\n",
      "Iter 1956 | Time 76.8208(77.3136) | Bit/dim 3.9467(3.9390) | Xent 1.1326(1.1310) | Loss 4.5130(4.5045) | Error 0.4036(0.4034) Steps 814(816.68) | Grad Norm 1.6580(1.0998) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 29.2813, Epoch Time 507.3033(498.8166), Bit/dim 3.9386(best: 3.9381), Xent 1.0928, Loss 4.4850, Error 0.3949(best: 0.3916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1957 | Time 78.3820(77.3456) | Bit/dim 3.9349(3.9389) | Xent 1.1259(1.1308) | Loss 4.4978(4.5043) | Error 0.4020(0.4034) Steps 820(816.78) | Grad Norm 1.1497(1.1013) | Total Time 14.00(14.00)\n",
      "Iter 1958 | Time 77.2807(77.3437) | Bit/dim 3.9459(3.9391) | Xent 1.1312(1.1308) | Loss 4.5115(4.5045) | Error 0.4032(0.4034) Steps 808(816.51) | Grad Norm 1.2692(1.1063) | Total Time 14.00(14.00)\n",
      "Iter 1959 | Time 75.7601(77.2962) | Bit/dim 3.9367(3.9390) | Xent 1.1457(1.1313) | Loss 4.5096(4.5047) | Error 0.4061(0.4034) Steps 826(816.80) | Grad Norm 1.7740(1.1264) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 78.2296(77.3242) | Bit/dim 3.9259(3.9386) | Xent 1.1357(1.1314) | Loss 4.4938(4.5043) | Error 0.4103(0.4036) Steps 826(817.07) | Grad Norm 1.4624(1.1365) | Total Time 14.00(14.00)\n",
      "Iter 1961 | Time 77.0487(77.3159) | Bit/dim 3.9415(3.9387) | Xent 1.1200(1.1311) | Loss 4.5015(4.5043) | Error 0.3974(0.4035) Steps 814(816.98) | Grad Norm 0.7368(1.1245) | Total Time 14.00(14.00)\n",
      "Iter 1962 | Time 77.9001(77.3334) | Bit/dim 3.9379(3.9387) | Xent 1.1169(1.1306) | Loss 4.4963(4.5040) | Error 0.4051(0.4035) Steps 826(817.25) | Grad Norm 1.1602(1.1255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 29.5630, Epoch Time 509.6875(499.1427), Bit/dim 3.9378(best: 3.9381), Xent 1.0934, Loss 4.4845, Error 0.3925(best: 0.3916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1963 | Time 80.8319(77.4384) | Bit/dim 3.9287(3.9384) | Xent 1.1278(1.1306) | Loss 4.4926(4.5037) | Error 0.4015(0.4034) Steps 832(817.69) | Grad Norm 1.9615(1.1506) | Total Time 14.00(14.00)\n",
      "Iter 1964 | Time 79.2746(77.4935) | Bit/dim 3.9415(3.9385) | Xent 1.1155(1.1301) | Loss 4.4992(4.5035) | Error 0.3962(0.4032) Steps 832(818.12) | Grad Norm 0.8059(1.1403) | Total Time 14.00(14.00)\n",
      "Iter 1965 | Time 78.7057(77.5298) | Bit/dim 3.9422(3.9386) | Xent 1.1139(1.1296) | Loss 4.4992(4.5034) | Error 0.3990(0.4031) Steps 814(818.00) | Grad Norm 1.4815(1.1505) | Total Time 14.00(14.00)\n",
      "Iter 1966 | Time 77.4459(77.5273) | Bit/dim 3.9285(3.9383) | Xent 1.1451(1.1301) | Loss 4.5011(4.5033) | Error 0.4069(0.4032) Steps 820(818.06) | Grad Norm 0.7573(1.1387) | Total Time 14.00(14.00)\n",
      "Iter 1967 | Time 78.7077(77.5627) | Bit/dim 3.9318(3.9381) | Xent 1.1200(1.1298) | Loss 4.4918(4.5030) | Error 0.3970(0.4030) Steps 820(818.12) | Grad Norm 1.0915(1.1373) | Total Time 14.00(14.00)\n",
      "Iter 1968 | Time 79.1944(77.6117) | Bit/dim 3.9465(3.9384) | Xent 1.1391(1.1301) | Loss 4.5160(4.5034) | Error 0.4034(0.4030) Steps 826(818.35) | Grad Norm 0.9079(1.1304) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 29.6572, Epoch Time 519.3521(499.7490), Bit/dim 3.9378(best: 3.9378), Xent 1.0919, Loss 4.4837, Error 0.3916(best: 0.3916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1969 | Time 77.7125(77.6147) | Bit/dim 3.9309(3.9381) | Xent 1.1400(1.1304) | Loss 4.5009(4.5033) | Error 0.4095(0.4032) Steps 826(818.58) | Grad Norm 0.7054(1.1177) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 79.6205(77.6749) | Bit/dim 3.9381(3.9381) | Xent 1.1052(1.1296) | Loss 4.4907(4.5029) | Error 0.3918(0.4029) Steps 808(818.27) | Grad Norm 0.8489(1.1096) | Total Time 14.00(14.00)\n",
      "Iter 1971 | Time 78.7670(77.7076) | Bit/dim 3.9239(3.9377) | Xent 1.1192(1.1293) | Loss 4.4835(4.5023) | Error 0.3955(0.4027) Steps 820(818.32) | Grad Norm 0.7835(1.0998) | Total Time 14.00(14.00)\n",
      "Iter 1972 | Time 77.2940(77.6952) | Bit/dim 3.9329(3.9376) | Xent 1.1410(1.1296) | Loss 4.5033(4.5024) | Error 0.4100(0.4029) Steps 832(818.73) | Grad Norm 0.7106(1.0881) | Total Time 14.00(14.00)\n",
      "Iter 1973 | Time 79.1174(77.7379) | Bit/dim 3.9468(3.9378) | Xent 1.1199(1.1294) | Loss 4.5068(4.5025) | Error 0.3991(0.4028) Steps 820(818.77) | Grad Norm 0.7833(1.0790) | Total Time 14.00(14.00)\n",
      "Iter 1974 | Time 78.6289(77.7646) | Bit/dim 3.9458(3.9381) | Xent 1.1272(1.1293) | Loss 4.5094(4.5027) | Error 0.3976(0.4026) Steps 820(818.80) | Grad Norm 1.2108(1.0829) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 29.1770, Epoch Time 516.4035(500.2486), Bit/dim 3.9376(best: 3.9378), Xent 1.0929, Loss 4.4840, Error 0.3898(best: 0.3916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1975 | Time 76.8355(77.7367) | Bit/dim 3.9386(3.9381) | Xent 1.1199(1.1290) | Loss 4.4985(4.5026) | Error 0.4005(0.4026) Steps 820(818.84) | Grad Norm 0.7114(1.0718) | Total Time 14.00(14.00)\n",
      "Iter 1976 | Time 75.9326(77.6826) | Bit/dim 3.9321(3.9379) | Xent 1.1229(1.1288) | Loss 4.4936(4.5023) | Error 0.3976(0.4024) Steps 832(819.23) | Grad Norm 1.8598(1.0954) | Total Time 14.00(14.00)\n",
      "Iter 1977 | Time 77.6372(77.6813) | Bit/dim 3.9380(3.9379) | Xent 1.1347(1.1290) | Loss 4.5053(4.5024) | Error 0.4058(0.4025) Steps 832(819.62) | Grad Norm 1.2117(1.0989) | Total Time 14.00(14.00)\n",
      "Iter 1978 | Time 75.9100(77.6281) | Bit/dim 3.9470(3.9382) | Xent 1.1237(1.1288) | Loss 4.5088(4.5026) | Error 0.4022(0.4025) Steps 832(819.99) | Grad Norm 0.8224(1.0906) | Total Time 14.00(14.00)\n",
      "Iter 1979 | Time 78.2723(77.6474) | Bit/dim 3.9343(3.9381) | Xent 1.1228(1.1287) | Loss 4.4957(4.5024) | Error 0.3996(0.4024) Steps 820(819.99) | Grad Norm 1.9178(1.1154) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 78.0840(77.6605) | Bit/dim 3.9318(3.9379) | Xent 1.1297(1.1287) | Loss 4.4966(4.5022) | Error 0.4054(0.4025) Steps 820(819.99) | Grad Norm 1.1250(1.1157) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 29.1918, Epoch Time 507.1977(500.4571), Bit/dim 3.9382(best: 3.9376), Xent 1.0915, Loss 4.4840, Error 0.3909(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1981 | Time 77.0127(77.6411) | Bit/dim 3.9406(3.9380) | Xent 1.1108(1.1282) | Loss 4.4960(4.5020) | Error 0.4005(0.4024) Steps 808(819.63) | Grad Norm 0.6005(1.1003) | Total Time 14.00(14.00)\n",
      "Iter 1982 | Time 76.7981(77.6158) | Bit/dim 3.9458(3.9382) | Xent 1.1370(1.1284) | Loss 4.5143(4.5024) | Error 0.4101(0.4027) Steps 820(819.64) | Grad Norm 0.7751(1.0905) | Total Time 14.00(14.00)\n",
      "Iter 1983 | Time 77.5993(77.6153) | Bit/dim 3.9215(3.9377) | Xent 1.1139(1.1280) | Loss 4.4785(4.5017) | Error 0.3988(0.4026) Steps 832(820.01) | Grad Norm 1.8121(1.1122) | Total Time 14.00(14.00)\n",
      "Iter 1984 | Time 78.5616(77.6437) | Bit/dim 3.9380(3.9377) | Xent 1.1260(1.1279) | Loss 4.5010(4.5017) | Error 0.4000(0.4025) Steps 826(820.19) | Grad Norm 0.7987(1.1028) | Total Time 14.00(14.00)\n",
      "Iter 1985 | Time 79.6048(77.7025) | Bit/dim 3.9322(3.9375) | Xent 1.1187(1.1276) | Loss 4.4916(4.5014) | Error 0.3960(0.4023) Steps 826(820.37) | Grad Norm 1.0032(1.0998) | Total Time 14.00(14.00)\n",
      "Iter 1986 | Time 77.8399(77.7067) | Bit/dim 3.9370(3.9375) | Xent 1.1399(1.1280) | Loss 4.5069(4.5015) | Error 0.4052(0.4024) Steps 826(820.53) | Grad Norm 1.4919(1.1115) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 29.2214, Epoch Time 512.2036(500.8095), Bit/dim 3.9377(best: 3.9376), Xent 1.0909, Loss 4.4832, Error 0.3924(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 80.8859(77.8020) | Bit/dim 3.9373(3.9375) | Xent 1.1332(1.1282) | Loss 4.5039(4.5016) | Error 0.4071(0.4025) Steps 826(820.70) | Grad Norm 1.7634(1.1311) | Total Time 14.00(14.00)\n",
      "Iter 1988 | Time 76.3445(77.7583) | Bit/dim 3.9425(3.9377) | Xent 1.1174(1.1278) | Loss 4.5012(4.5016) | Error 0.4028(0.4025) Steps 808(820.32) | Grad Norm 0.6793(1.1175) | Total Time 14.00(14.00)\n",
      "Iter 1989 | Time 75.3107(77.6849) | Bit/dim 3.9269(3.9373) | Xent 1.1295(1.1279) | Loss 4.4917(4.5013) | Error 0.4067(0.4026) Steps 832(820.67) | Grad Norm 1.1173(1.1175) | Total Time 14.00(14.00)\n",
      "Iter 1990 | Time 77.6352(77.6834) | Bit/dim 3.9404(3.9374) | Xent 1.1169(1.1276) | Loss 4.4989(4.5012) | Error 0.4040(0.4027) Steps 820(820.65) | Grad Norm 2.0489(1.1455) | Total Time 14.00(14.00)\n",
      "Iter 1991 | Time 79.7791(77.7463) | Bit/dim 3.9380(3.9375) | Xent 1.1464(1.1281) | Loss 4.5112(4.5015) | Error 0.4073(0.4028) Steps 838(821.17) | Grad Norm 1.1221(1.1448) | Total Time 14.00(14.00)\n",
      "Iter 1992 | Time 78.3062(77.7631) | Bit/dim 3.9337(3.9373) | Xent 1.1313(1.1282) | Loss 4.4994(4.5015) | Error 0.4044(0.4029) Steps 826(821.31) | Grad Norm 2.1553(1.1751) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 29.7987, Epoch Time 513.6418(501.1945), Bit/dim 3.9371(best: 3.9376), Xent 1.0900, Loss 4.4821, Error 0.3915(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 79.8011(77.8242) | Bit/dim 3.9358(3.9373) | Xent 1.1339(1.1284) | Loss 4.5027(4.5015) | Error 0.4041(0.4029) Steps 826(821.45) | Grad Norm 1.2110(1.1762) | Total Time 14.00(14.00)\n",
      "Iter 1994 | Time 77.5371(77.8156) | Bit/dim 3.9435(3.9375) | Xent 1.1092(1.1278) | Loss 4.4981(4.5014) | Error 0.3872(0.4024) Steps 826(821.59) | Grad Norm 0.9456(1.1693) | Total Time 14.00(14.00)\n",
      "Iter 1995 | Time 78.7405(77.8433) | Bit/dim 3.9350(3.9374) | Xent 1.1442(1.1283) | Loss 4.5072(4.5016) | Error 0.4123(0.4027) Steps 820(821.54) | Grad Norm 1.2881(1.1728) | Total Time 14.00(14.00)\n",
      "Iter 1996 | Time 79.1334(77.8820) | Bit/dim 3.9324(3.9373) | Xent 1.1201(1.1281) | Loss 4.4924(4.5013) | Error 0.4008(0.4027) Steps 826(821.68) | Grad Norm 1.1039(1.1707) | Total Time 14.00(14.00)\n",
      "Iter 1997 | Time 77.8990(77.8826) | Bit/dim 3.9284(3.9370) | Xent 1.1267(1.1280) | Loss 4.4917(4.5010) | Error 0.4040(0.4027) Steps 832(821.99) | Grad Norm 1.4028(1.1777) | Total Time 14.00(14.00)\n",
      "Iter 1998 | Time 79.6146(77.9345) | Bit/dim 3.9420(3.9371) | Xent 1.1025(1.1273) | Loss 4.4932(4.5008) | Error 0.3922(0.4024) Steps 832(822.29) | Grad Norm 1.3255(1.1821) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 29.5226, Epoch Time 517.9188(501.6962), Bit/dim 3.9376(best: 3.9371), Xent 1.0907, Loss 4.4829, Error 0.3904(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 77.1027(77.9096) | Bit/dim 3.9391(3.9372) | Xent 1.1141(1.1269) | Loss 4.4961(4.5006) | Error 0.3972(0.4022) Steps 832(822.58) | Grad Norm 1.0037(1.1768) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 76.8691(77.8783) | Bit/dim 3.9305(3.9370) | Xent 1.1268(1.1269) | Loss 4.4939(4.5004) | Error 0.4024(0.4023) Steps 820(822.50) | Grad Norm 1.0115(1.1718) | Total Time 14.00(14.00)\n",
      "Iter 2001 | Time 78.7961(77.9059) | Bit/dim 3.9254(3.9367) | Xent 1.1313(1.1270) | Loss 4.4911(4.5001) | Error 0.4009(0.4022) Steps 832(822.79) | Grad Norm 0.8272(1.1615) | Total Time 14.00(14.00)\n",
      "Iter 2002 | Time 78.2036(77.9148) | Bit/dim 3.9365(3.9366) | Xent 1.1166(1.1267) | Loss 4.4948(4.5000) | Error 0.4018(0.4022) Steps 826(822.88) | Grad Norm 1.2270(1.1635) | Total Time 14.00(14.00)\n",
      "Iter 2003 | Time 80.1027(77.9804) | Bit/dim 3.9447(3.9369) | Xent 1.1255(1.1267) | Loss 4.5074(4.5002) | Error 0.3892(0.4018) Steps 832(823.16) | Grad Norm 1.5922(1.1763) | Total Time 14.00(14.00)\n",
      "Iter 2004 | Time 77.6836(77.9715) | Bit/dim 3.9401(3.9370) | Xent 1.1300(1.1268) | Loss 4.5051(4.5004) | Error 0.4040(0.4019) Steps 832(823.42) | Grad Norm 0.9766(1.1703) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 28.9508, Epoch Time 513.1551(502.0400), Bit/dim 3.9359(best: 3.9371), Xent 1.0879, Loss 4.4799, Error 0.3920(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 78.9507(78.0009) | Bit/dim 3.9389(3.9370) | Xent 1.1550(1.1276) | Loss 4.5164(4.5008) | Error 0.4123(0.4022) Steps 832(823.68) | Grad Norm 1.4655(1.1792) | Total Time 14.00(14.00)\n",
      "Iter 2006 | Time 80.4910(78.0756) | Bit/dim 3.9302(3.9368) | Xent 1.1258(1.1275) | Loss 4.4931(4.5006) | Error 0.4022(0.4022) Steps 826(823.75) | Grad Norm 1.2679(1.1818) | Total Time 14.00(14.00)\n",
      "Iter 2007 | Time 77.5622(78.0602) | Bit/dim 3.9357(3.9368) | Xent 1.1163(1.1272) | Loss 4.4938(4.5004) | Error 0.3990(0.4021) Steps 832(824.00) | Grad Norm 0.7071(1.1676) | Total Time 14.00(14.00)\n",
      "Iter 2008 | Time 76.7692(78.0215) | Bit/dim 3.9347(3.9367) | Xent 1.1086(1.1266) | Loss 4.4890(4.5001) | Error 0.3910(0.4018) Steps 814(823.70) | Grad Norm 1.0930(1.1654) | Total Time 14.00(14.00)\n",
      "Iter 2009 | Time 79.4351(78.0639) | Bit/dim 3.9367(3.9367) | Xent 1.1101(1.1262) | Loss 4.4918(4.4998) | Error 0.3959(0.4016) Steps 820(823.58) | Grad Norm 0.7370(1.1525) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 79.1352(78.0960) | Bit/dim 3.9320(3.9366) | Xent 1.1147(1.1258) | Loss 4.4893(4.4995) | Error 0.3932(0.4013) Steps 826(823.66) | Grad Norm 1.1323(1.1519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 29.2737, Epoch Time 516.9623(502.4876), Bit/dim 3.9360(best: 3.9359), Xent 1.0874, Loss 4.4797, Error 0.3910(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 78.1962(78.0990) | Bit/dim 3.9358(3.9366) | Xent 1.1130(1.1254) | Loss 4.4924(4.4993) | Error 0.3938(0.4011) Steps 826(823.73) | Grad Norm 0.8894(1.1440) | Total Time 14.00(14.00)\n",
      "Iter 2012 | Time 77.3966(78.0780) | Bit/dim 3.9223(3.9361) | Xent 1.1285(1.1255) | Loss 4.4866(4.4989) | Error 0.4040(0.4012) Steps 802(823.08) | Grad Norm 0.9796(1.1391) | Total Time 14.00(14.00)\n",
      "Iter 2013 | Time 78.8737(78.1018) | Bit/dim 3.9405(3.9363) | Xent 1.1356(1.1258) | Loss 4.5083(4.4992) | Error 0.4042(0.4013) Steps 820(822.98) | Grad Norm 0.9237(1.1326) | Total Time 14.00(14.00)\n",
      "Iter 2014 | Time 75.0463(78.0102) | Bit/dim 3.9377(3.9363) | Xent 1.1083(1.1253) | Loss 4.4919(4.4990) | Error 0.4014(0.4013) Steps 832(823.25) | Grad Norm 1.9124(1.1560) | Total Time 14.00(14.00)\n",
      "Iter 2015 | Time 77.8563(78.0056) | Bit/dim 3.9355(3.9363) | Xent 1.1301(1.1254) | Loss 4.5006(4.4990) | Error 0.4011(0.4013) Steps 826(823.34) | Grad Norm 1.3788(1.1627) | Total Time 14.00(14.00)\n",
      "Iter 2016 | Time 80.4056(78.0776) | Bit/dim 3.9315(3.9362) | Xent 1.1154(1.1251) | Loss 4.4892(4.4987) | Error 0.3998(0.4012) Steps 826(823.42) | Grad Norm 1.0974(1.1607) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 29.5528, Epoch Time 513.0931(502.8058), Bit/dim 3.9361(best: 3.9359), Xent 1.0886, Loss 4.4804, Error 0.3918(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 77.4654(78.0592) | Bit/dim 3.9289(3.9359) | Xent 1.1119(1.1247) | Loss 4.4849(4.4983) | Error 0.3971(0.4011) Steps 814(823.13) | Grad Norm 1.8892(1.1826) | Total Time 14.00(14.00)\n",
      "Iter 2018 | Time 79.7805(78.1108) | Bit/dim 3.9429(3.9361) | Xent 1.1366(1.1251) | Loss 4.5112(4.4987) | Error 0.4090(0.4013) Steps 826(823.22) | Grad Norm 1.1264(1.1809) | Total Time 14.00(14.00)\n",
      "Iter 2019 | Time 76.3668(78.0585) | Bit/dim 3.9312(3.9360) | Xent 1.1114(1.1247) | Loss 4.4869(4.4983) | Error 0.4000(0.4013) Steps 826(823.30) | Grad Norm 0.7306(1.1674) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 78.3551(78.0674) | Bit/dim 3.9322(3.9359) | Xent 1.1131(1.1243) | Loss 4.4888(4.4981) | Error 0.3914(0.4010) Steps 808(822.84) | Grad Norm 1.5721(1.1795) | Total Time 14.00(14.00)\n",
      "Iter 2021 | Time 78.8845(78.0919) | Bit/dim 3.9302(3.9357) | Xent 1.1323(1.1246) | Loss 4.4963(4.4980) | Error 0.4096(0.4013) Steps 820(822.76) | Grad Norm 1.3466(1.1846) | Total Time 14.00(14.00)\n",
      "Iter 2022 | Time 75.7370(78.0213) | Bit/dim 3.9449(3.9360) | Xent 1.1131(1.1242) | Loss 4.5015(4.4981) | Error 0.3998(0.4012) Steps 814(822.50) | Grad Norm 0.5981(1.1670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 29.2620, Epoch Time 511.1142(503.0550), Bit/dim 3.9361(best: 3.9359), Xent 1.0869, Loss 4.4796, Error 0.3898(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 79.7382(78.0728) | Bit/dim 3.9346(3.9359) | Xent 1.1010(1.1235) | Loss 4.4851(4.4977) | Error 0.3936(0.4010) Steps 814(822.24) | Grad Norm 1.0917(1.1647) | Total Time 14.00(14.00)\n",
      "Iter 2024 | Time 78.6767(78.0909) | Bit/dim 3.9323(3.9358) | Xent 1.1109(1.1232) | Loss 4.4878(4.4974) | Error 0.4034(0.4011) Steps 820(822.17) | Grad Norm 1.0426(1.1610) | Total Time 14.00(14.00)\n",
      "Iter 2025 | Time 75.9435(78.0265) | Bit/dim 3.9363(3.9359) | Xent 1.1259(1.1232) | Loss 4.4993(4.4975) | Error 0.4004(0.4010) Steps 820(822.11) | Grad Norm 0.7253(1.1480) | Total Time 14.00(14.00)\n",
      "Iter 2026 | Time 76.6438(77.9850) | Bit/dim 3.9346(3.9358) | Xent 1.0995(1.1225) | Loss 4.4844(4.4971) | Error 0.3920(0.4008) Steps 820(822.05) | Grad Norm 1.0105(1.1438) | Total Time 14.00(14.00)\n",
      "Iter 2027 | Time 74.7914(77.8892) | Bit/dim 3.9311(3.9357) | Xent 1.1263(1.1226) | Loss 4.4942(4.4970) | Error 0.3996(0.4007) Steps 826(822.16) | Grad Norm 1.8094(1.1638) | Total Time 14.00(14.00)\n",
      "Iter 2028 | Time 80.8793(77.9789) | Bit/dim 3.9381(3.9357) | Xent 1.1172(1.1225) | Loss 4.4967(4.4970) | Error 0.3970(0.4006) Steps 808(821.74) | Grad Norm 1.2351(1.1660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 28.8411, Epoch Time 511.2339(503.3004), Bit/dim 3.9352(best: 3.9359), Xent 1.0837, Loss 4.4771, Error 0.3875(best: 0.3898)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 79.8245(78.0343) | Bit/dim 3.9289(3.9355) | Xent 1.1136(1.1222) | Loss 4.4857(4.4966) | Error 0.3986(0.4006) Steps 820(821.69) | Grad Norm 1.0540(1.1626) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 78.5192(78.0488) | Bit/dim 3.9274(3.9353) | Xent 1.1330(1.1225) | Loss 4.4939(4.4966) | Error 0.4030(0.4006) Steps 832(822.00) | Grad Norm 1.2370(1.1648) | Total Time 14.00(14.00)\n",
      "Iter 2031 | Time 78.7961(78.0712) | Bit/dim 3.9451(3.9356) | Xent 1.1347(1.1229) | Loss 4.5125(4.4970) | Error 0.4080(0.4009) Steps 814(821.76) | Grad Norm 1.2326(1.1669) | Total Time 14.00(14.00)\n",
      "Iter 2032 | Time 79.6314(78.1180) | Bit/dim 3.9441(3.9358) | Xent 1.1193(1.1228) | Loss 4.5037(4.4972) | Error 0.3996(0.4008) Steps 808(821.34) | Grad Norm 0.9472(1.1603) | Total Time 14.00(14.00)\n",
      "Iter 2033 | Time 77.0768(78.0868) | Bit/dim 3.9187(3.9353) | Xent 1.1295(1.1230) | Loss 4.4835(4.4968) | Error 0.4016(0.4008) Steps 820(821.30) | Grad Norm 1.1447(1.1598) | Total Time 14.00(14.00)\n",
      "Iter 2034 | Time 77.5568(78.0709) | Bit/dim 3.9385(3.9354) | Xent 1.1004(1.1223) | Loss 4.4887(4.4966) | Error 0.3952(0.4007) Steps 832(821.62) | Grad Norm 0.7935(1.1488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 29.4284, Epoch Time 516.2213(503.6880), Bit/dim 3.9357(best: 3.9352), Xent 1.0847, Loss 4.4780, Error 0.3933(best: 0.3875)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 77.9009(78.0658) | Bit/dim 3.9236(3.9351) | Xent 1.1178(1.1222) | Loss 4.4825(4.4962) | Error 0.4083(0.4009) Steps 832(821.94) | Grad Norm 1.2311(1.1513) | Total Time 14.00(14.00)\n",
      "Iter 2036 | Time 80.1443(78.1282) | Bit/dim 3.9307(3.9349) | Xent 1.1156(1.1220) | Loss 4.4885(4.4959) | Error 0.3911(0.4006) Steps 826(822.06) | Grad Norm 1.5295(1.1626) | Total Time 14.00(14.00)\n",
      "Iter 2037 | Time 77.0243(78.0950) | Bit/dim 3.9362(3.9350) | Xent 1.1220(1.1220) | Loss 4.4972(4.4960) | Error 0.3954(0.4005) Steps 826(822.18) | Grad Norm 0.7406(1.1500) | Total Time 14.00(14.00)\n",
      "Iter 2038 | Time 80.0050(78.1523) | Bit/dim 3.9469(3.9353) | Xent 1.1141(1.1218) | Loss 4.5039(4.4962) | Error 0.4012(0.4005) Steps 802(821.57) | Grad Norm 0.8077(1.1397) | Total Time 14.00(14.00)\n",
      "Iter 2039 | Time 78.2720(78.1559) | Bit/dim 3.9321(3.9352) | Xent 1.1384(1.1222) | Loss 4.5012(4.4964) | Error 0.4067(0.4007) Steps 802(820.98) | Grad Norm 1.0884(1.1382) | Total Time 14.00(14.00)\n",
      "Iter 2040 | Time 79.6074(78.1995) | Bit/dim 3.9353(3.9352) | Xent 1.1141(1.1220) | Loss 4.4924(4.4962) | Error 0.3948(0.4005) Steps 814(820.77) | Grad Norm 0.9777(1.1333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 29.4289, Epoch Time 517.9307(504.1153), Bit/dim 3.9348(best: 3.9352), Xent 1.0826, Loss 4.4761, Error 0.3865(best: 0.3875)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 77.3391(78.1737) | Bit/dim 3.9359(3.9353) | Xent 1.1167(1.1218) | Loss 4.4942(4.4962) | Error 0.3990(0.4004) Steps 832(821.11) | Grad Norm 1.5950(1.1472) | Total Time 14.00(14.00)\n",
      "Iter 2042 | Time 78.6248(78.1872) | Bit/dim 3.9179(3.9347) | Xent 1.1150(1.1216) | Loss 4.4755(4.4956) | Error 0.3979(0.4004) Steps 820(821.08) | Grad Norm 1.0730(1.1450) | Total Time 14.00(14.00)\n",
      "Iter 2043 | Time 80.3080(78.2508) | Bit/dim 3.9428(3.9350) | Xent 1.1266(1.1218) | Loss 4.5061(4.4959) | Error 0.4011(0.4004) Steps 820(821.05) | Grad Norm 0.9563(1.1393) | Total Time 14.00(14.00)\n",
      "Iter 2044 | Time 78.5861(78.2609) | Bit/dim 3.9279(3.9348) | Xent 1.1310(1.1221) | Loss 4.4933(4.4958) | Error 0.3974(0.4003) Steps 832(821.37) | Grad Norm 1.3199(1.1447) | Total Time 14.00(14.00)\n",
      "Iter 2045 | Time 79.7309(78.3050) | Bit/dim 3.9397(3.9349) | Xent 1.1170(1.1219) | Loss 4.4982(4.4959) | Error 0.3960(0.4002) Steps 820(821.33) | Grad Norm 0.9686(1.1394) | Total Time 14.00(14.00)\n",
      "Iter 2046 | Time 77.8178(78.2904) | Bit/dim 3.9328(3.9349) | Xent 1.1192(1.1218) | Loss 4.4924(4.4958) | Error 0.3985(0.4001) Steps 826(821.47) | Grad Norm 0.8216(1.1299) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 29.5910, Epoch Time 517.6763(504.5221), Bit/dim 3.9354(best: 3.9348), Xent 1.0849, Loss 4.4778, Error 0.3878(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 75.9854(78.2212) | Bit/dim 3.9344(3.9348) | Xent 1.1031(1.1213) | Loss 4.4859(4.4955) | Error 0.3899(0.3998) Steps 826(821.61) | Grad Norm 1.3520(1.1366) | Total Time 14.00(14.00)\n",
      "Iter 2048 | Time 79.2081(78.2508) | Bit/dim 3.9375(3.9349) | Xent 1.1275(1.1215) | Loss 4.5012(4.4956) | Error 0.3989(0.3998) Steps 832(821.92) | Grad Norm 1.1216(1.1361) | Total Time 14.00(14.00)\n",
      "Iter 2049 | Time 78.8992(78.2703) | Bit/dim 3.9297(3.9348) | Xent 1.1058(1.1210) | Loss 4.4826(4.4953) | Error 0.3932(0.3996) Steps 832(822.22) | Grad Norm 0.8460(1.1274) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 79.5805(78.3096) | Bit/dim 3.9391(3.9349) | Xent 1.1198(1.1210) | Loss 4.4990(4.4954) | Error 0.3978(0.3995) Steps 814(821.98) | Grad Norm 0.9117(1.1209) | Total Time 14.00(14.00)\n",
      "Iter 2051 | Time 78.4423(78.3136) | Bit/dim 3.9334(3.9348) | Xent 1.1194(1.1209) | Loss 4.4931(4.4953) | Error 0.4014(0.3996) Steps 832(822.28) | Grad Norm 1.0666(1.1193) | Total Time 14.00(14.00)\n",
      "Iter 2052 | Time 80.2578(78.3719) | Bit/dim 3.9285(3.9347) | Xent 1.1084(1.1205) | Loss 4.4828(4.4949) | Error 0.3929(0.3994) Steps 832(822.57) | Grad Norm 0.9967(1.1156) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 29.3854, Epoch Time 517.2226(504.9032), Bit/dim 3.9341(best: 3.9348), Xent 1.0810, Loss 4.4746, Error 0.3851(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 80.8584(78.4465) | Bit/dim 3.9251(3.9344) | Xent 1.1108(1.1202) | Loss 4.4805(4.4945) | Error 0.4021(0.3995) Steps 832(822.85) | Grad Norm 1.1438(1.1165) | Total Time 14.00(14.00)\n",
      "Iter 2054 | Time 79.2998(78.4721) | Bit/dim 3.9320(3.9343) | Xent 1.1131(1.1200) | Loss 4.4885(4.4943) | Error 0.3971(0.3994) Steps 832(823.13) | Grad Norm 0.7860(1.1066) | Total Time 14.00(14.00)\n",
      "Iter 2055 | Time 78.3975(78.4698) | Bit/dim 3.9444(3.9346) | Xent 1.1153(1.1199) | Loss 4.5021(4.4945) | Error 0.3995(0.3994) Steps 826(823.21) | Grad Norm 1.3001(1.1124) | Total Time 14.00(14.00)\n",
      "Iter 2056 | Time 77.5549(78.4424) | Bit/dim 3.9358(3.9346) | Xent 1.1331(1.1203) | Loss 4.5023(4.4948) | Error 0.4026(0.3995) Steps 826(823.30) | Grad Norm 0.6469(1.0984) | Total Time 14.00(14.00)\n",
      "Iter 2057 | Time 78.8155(78.4536) | Bit/dim 3.9284(3.9345) | Xent 1.1241(1.1204) | Loss 4.4905(4.4947) | Error 0.4062(0.3997) Steps 832(823.56) | Grad Norm 1.0891(1.0981) | Total Time 14.00(14.00)\n",
      "Iter 2058 | Time 82.5965(78.5779) | Bit/dim 3.9307(3.9343) | Xent 1.1312(1.1207) | Loss 4.4963(4.4947) | Error 0.4073(0.3999) Steps 802(822.91) | Grad Norm 1.2231(1.1019) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 29.5083, Epoch Time 522.4437(505.4294), Bit/dim 3.9331(best: 3.9341), Xent 1.0809, Loss 4.4736, Error 0.3842(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 78.0177(78.5611) | Bit/dim 3.9217(3.9340) | Xent 1.1213(1.1207) | Loss 4.4824(4.4943) | Error 0.3959(0.3998) Steps 832(823.18) | Grad Norm 0.7617(1.0917) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 77.3186(78.5238) | Bit/dim 3.9339(3.9340) | Xent 1.1333(1.1211) | Loss 4.5005(4.4945) | Error 0.4002(0.3998) Steps 832(823.45) | Grad Norm 1.0322(1.0899) | Total Time 14.00(14.00)\n",
      "Iter 2061 | Time 77.7420(78.5003) | Bit/dim 3.9192(3.9335) | Xent 1.0899(1.1202) | Loss 4.4641(4.4936) | Error 0.3880(0.3995) Steps 814(823.16) | Grad Norm 0.7757(1.0805) | Total Time 14.00(14.00)\n",
      "Iter 2062 | Time 79.1450(78.5197) | Bit/dim 3.9317(3.9335) | Xent 1.1115(1.1199) | Loss 4.4875(4.4934) | Error 0.3936(0.3993) Steps 832(823.43) | Grad Norm 1.3511(1.0886) | Total Time 14.00(14.00)\n",
      "Iter 2063 | Time 78.8742(78.5303) | Bit/dim 3.9477(3.9339) | Xent 1.1041(1.1194) | Loss 4.4997(4.4936) | Error 0.3971(0.3992) Steps 814(823.15) | Grad Norm 0.9154(1.0834) | Total Time 14.00(14.00)\n",
      "Iter 2064 | Time 80.6992(78.5954) | Bit/dim 3.9344(3.9339) | Xent 1.1249(1.1196) | Loss 4.4968(4.4937) | Error 0.4005(0.3993) Steps 832(823.41) | Grad Norm 1.0730(1.0831) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 29.2594, Epoch Time 516.7948(505.7703), Bit/dim 3.9329(best: 3.9331), Xent 1.0794, Loss 4.4726, Error 0.3859(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 80.0128(78.6379) | Bit/dim 3.9355(3.9340) | Xent 1.1169(1.1195) | Loss 4.4939(4.4937) | Error 0.3998(0.3993) Steps 832(823.67) | Grad Norm 0.6766(1.0709) | Total Time 14.00(14.00)\n",
      "Iter 2066 | Time 78.8964(78.6457) | Bit/dim 3.9245(3.9337) | Xent 1.1248(1.1197) | Loss 4.4869(4.4935) | Error 0.4051(0.3995) Steps 826(823.74) | Grad Norm 1.4425(1.0820) | Total Time 14.00(14.00)\n",
      "Iter 2067 | Time 76.2845(78.5748) | Bit/dim 3.9325(3.9336) | Xent 1.1037(1.1192) | Loss 4.4843(4.4932) | Error 0.3949(0.3993) Steps 832(823.99) | Grad Norm 0.9551(1.0782) | Total Time 14.00(14.00)\n",
      "Iter 2068 | Time 78.1568(78.5623) | Bit/dim 3.9376(3.9337) | Xent 1.1263(1.1194) | Loss 4.5008(4.4935) | Error 0.4024(0.3994) Steps 820(823.87) | Grad Norm 0.8833(1.0724) | Total Time 14.00(14.00)\n",
      "Iter 2069 | Time 80.3908(78.6171) | Bit/dim 3.9324(3.9337) | Xent 1.1072(1.1191) | Loss 4.4860(4.4932) | Error 0.3989(0.3994) Steps 832(824.11) | Grad Norm 0.9170(1.0677) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 79.6567(78.6483) | Bit/dim 3.9304(3.9336) | Xent 1.1035(1.1186) | Loss 4.4821(4.4929) | Error 0.3924(0.3992) Steps 814(823.81) | Grad Norm 1.2647(1.0736) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 29.2334, Epoch Time 518.3599(506.1480), Bit/dim 3.9328(best: 3.9329), Xent 1.0809, Loss 4.4732, Error 0.3864(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 77.9397(78.6271) | Bit/dim 3.9333(3.9336) | Xent 1.1170(1.1185) | Loss 4.4918(4.4929) | Error 0.4042(0.3993) Steps 826(823.87) | Grad Norm 0.8241(1.0661) | Total Time 14.00(14.00)\n",
      "Iter 2072 | Time 78.8194(78.6328) | Bit/dim 3.9344(3.9336) | Xent 1.1075(1.1182) | Loss 4.4882(4.4927) | Error 0.3939(0.3992) Steps 808(823.40) | Grad Norm 1.6322(1.0831) | Total Time 14.00(14.00)\n",
      "Iter 2073 | Time 76.9522(78.5824) | Bit/dim 3.9313(3.9336) | Xent 1.1035(1.1178) | Loss 4.4830(4.4924) | Error 0.3982(0.3991) Steps 826(823.48) | Grad Norm 0.8473(1.0760) | Total Time 14.00(14.00)\n",
      "Iter 2074 | Time 75.9662(78.5039) | Bit/dim 3.9263(3.9333) | Xent 1.1178(1.1178) | Loss 4.4852(4.4922) | Error 0.3978(0.3991) Steps 826(823.55) | Grad Norm 0.9055(1.0709) | Total Time 14.00(14.00)\n",
      "Iter 2075 | Time 78.8559(78.5145) | Bit/dim 3.9383(3.9335) | Xent 1.1231(1.1179) | Loss 4.4999(4.4924) | Error 0.3995(0.3991) Steps 814(823.26) | Grad Norm 1.4148(1.0812) | Total Time 14.00(14.00)\n",
      "Iter 2076 | Time 79.8841(78.5556) | Bit/dim 3.9269(3.9333) | Xent 1.1154(1.1179) | Loss 4.4846(4.4922) | Error 0.3991(0.3991) Steps 808(822.81) | Grad Norm 1.1957(1.0847) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 29.1362, Epoch Time 512.9498(506.3521), Bit/dim 3.9328(best: 3.9328), Xent 1.0788, Loss 4.4722, Error 0.3850(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 78.0958(78.5418) | Bit/dim 3.9456(3.9337) | Xent 1.1238(1.1180) | Loss 4.5075(4.4927) | Error 0.4020(0.3992) Steps 826(822.90) | Grad Norm 0.8278(1.0770) | Total Time 14.00(14.00)\n",
      "Iter 2078 | Time 77.0119(78.4959) | Bit/dim 3.9250(3.9334) | Xent 1.1243(1.1182) | Loss 4.4871(4.4925) | Error 0.3981(0.3992) Steps 826(823.00) | Grad Norm 2.0384(1.1058) | Total Time 14.00(14.00)\n",
      "Iter 2079 | Time 78.6840(78.5015) | Bit/dim 3.9228(3.9331) | Xent 1.1061(1.1179) | Loss 4.4759(4.4920) | Error 0.4002(0.3992) Steps 814(822.73) | Grad Norm 1.7003(1.1237) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 81.0946(78.5793) | Bit/dim 3.9331(3.9331) | Xent 1.1100(1.1176) | Loss 4.4881(4.4919) | Error 0.3888(0.3989) Steps 826(822.82) | Grad Norm 1.2155(1.1264) | Total Time 14.00(14.00)\n",
      "Iter 2081 | Time 76.8051(78.5261) | Bit/dim 3.9352(3.9331) | Xent 1.1129(1.1175) | Loss 4.4916(4.4919) | Error 0.3958(0.3988) Steps 832(823.10) | Grad Norm 1.8088(1.1469) | Total Time 14.00(14.00)\n",
      "Iter 2082 | Time 78.5871(78.5279) | Bit/dim 3.9351(3.9332) | Xent 1.1156(1.1174) | Loss 4.4929(4.4919) | Error 0.3976(0.3988) Steps 808(822.65) | Grad Norm 2.1185(1.1760) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 29.2633, Epoch Time 515.0773(506.6138), Bit/dim 3.9333(best: 3.9328), Xent 1.0783, Loss 4.4725, Error 0.3855(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 78.5886(78.5298) | Bit/dim 3.9357(3.9333) | Xent 1.1057(1.1171) | Loss 4.4886(4.4918) | Error 0.4000(0.3988) Steps 826(822.75) | Grad Norm 0.8160(1.1652) | Total Time 14.00(14.00)\n",
      "Iter 2084 | Time 79.5700(78.5610) | Bit/dim 3.9389(3.9334) | Xent 1.1233(1.1173) | Loss 4.5006(4.4921) | Error 0.4026(0.3989) Steps 832(823.02) | Grad Norm 2.0477(1.1917) | Total Time 14.00(14.00)\n",
      "Iter 2085 | Time 79.1274(78.5780) | Bit/dim 3.9255(3.9332) | Xent 1.1011(1.1168) | Loss 4.4760(4.4916) | Error 0.3915(0.3987) Steps 832(823.29) | Grad Norm 1.2892(1.1946) | Total Time 14.00(14.00)\n",
      "Iter 2086 | Time 79.0186(78.5912) | Bit/dim 3.9311(3.9331) | Xent 1.1275(1.1171) | Loss 4.4948(4.4917) | Error 0.4030(0.3988) Steps 826(823.37) | Grad Norm 1.4288(1.2017) | Total Time 14.00(14.00)\n",
      "Iter 2087 | Time 78.1641(78.5784) | Bit/dim 3.9302(3.9331) | Xent 1.1035(1.1167) | Loss 4.4820(4.4914) | Error 0.3891(0.3985) Steps 820(823.27) | Grad Norm 1.3947(1.2074) | Total Time 14.00(14.00)\n",
      "Iter 2088 | Time 81.7048(78.6722) | Bit/dim 3.9292(3.9329) | Xent 1.1208(1.1168) | Loss 4.4896(4.4913) | Error 0.4031(0.3987) Steps 808(822.82) | Grad Norm 1.2506(1.2087) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 29.3623, Epoch Time 520.9569(507.0441), Bit/dim 3.9327(best: 3.9328), Xent 1.0773, Loss 4.4713, Error 0.3870(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 77.5278(78.6378) | Bit/dim 3.9274(3.9328) | Xent 1.1096(1.1166) | Loss 4.4822(4.4911) | Error 0.3946(0.3985) Steps 826(822.91) | Grad Norm 0.8470(1.1979) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 80.9426(78.7070) | Bit/dim 3.9272(3.9326) | Xent 1.1036(1.1162) | Loss 4.4790(4.4907) | Error 0.3910(0.3983) Steps 832(823.18) | Grad Norm 1.6543(1.2116) | Total Time 14.00(14.00)\n",
      "Iter 2091 | Time 78.9749(78.7150) | Bit/dim 3.9465(3.9330) | Xent 1.1059(1.1159) | Loss 4.4994(4.4910) | Error 0.3931(0.3982) Steps 814(822.91) | Grad Norm 1.2026(1.2113) | Total Time 14.00(14.00)\n",
      "Iter 2092 | Time 81.0883(78.7862) | Bit/dim 3.9304(3.9329) | Xent 1.1385(1.1166) | Loss 4.4996(4.4912) | Error 0.4040(0.3983) Steps 832(823.18) | Grad Norm 1.2707(1.2131) | Total Time 14.00(14.00)\n",
      "Iter 2093 | Time 79.2479(78.8001) | Bit/dim 3.9234(3.9327) | Xent 1.0987(1.1160) | Loss 4.4727(4.4907) | Error 0.3885(0.3980) Steps 826(823.27) | Grad Norm 1.1929(1.2125) | Total Time 14.00(14.00)\n",
      "Iter 2094 | Time 76.5730(78.7332) | Bit/dim 3.9329(3.9327) | Xent 1.1175(1.1161) | Loss 4.4916(4.4907) | Error 0.4032(0.3982) Steps 820(823.17) | Grad Norm 1.3636(1.2170) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 29.5145, Epoch Time 519.2418(507.4101), Bit/dim 3.9328(best: 3.9327), Xent 1.0781, Loss 4.4718, Error 0.3846(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 79.3928(78.7530) | Bit/dim 3.9307(3.9326) | Xent 1.1093(1.1159) | Loss 4.4853(4.4905) | Error 0.3921(0.3980) Steps 832(823.43) | Grad Norm 1.3586(1.2213) | Total Time 14.00(14.00)\n",
      "Iter 2096 | Time 81.3179(78.8300) | Bit/dim 3.9325(3.9326) | Xent 1.0815(1.1148) | Loss 4.4733(4.4900) | Error 0.3860(0.3977) Steps 826(823.51) | Grad Norm 0.9074(1.2118) | Total Time 14.00(14.00)\n",
      "Iter 2097 | Time 79.4405(78.8483) | Bit/dim 3.9253(3.9324) | Xent 1.1013(1.1144) | Loss 4.4759(4.4896) | Error 0.3909(0.3975) Steps 808(823.04) | Grad Norm 1.6992(1.2265) | Total Time 14.00(14.00)\n",
      "Iter 2098 | Time 79.3365(78.8629) | Bit/dim 3.9334(3.9324) | Xent 1.1242(1.1147) | Loss 4.4955(4.4898) | Error 0.4052(0.3977) Steps 832(823.31) | Grad Norm 2.8392(1.2749) | Total Time 14.00(14.00)\n",
      "Iter 2099 | Time 81.6931(78.9478) | Bit/dim 3.9376(3.9326) | Xent 1.1149(1.1147) | Loss 4.4950(4.4899) | Error 0.3979(0.3977) Steps 814(823.03) | Grad Norm 1.4711(1.2807) | Total Time 14.00(14.00)\n",
      "Iter 2100 | Time 77.3213(78.8990) | Bit/dim 3.9276(3.9324) | Xent 1.1152(1.1147) | Loss 4.4852(4.4898) | Error 0.4004(0.3978) Steps 826(823.12) | Grad Norm 2.6138(1.3207) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 29.5386, Epoch Time 523.5622(507.8946), Bit/dim 3.9325(best: 3.9327), Xent 1.0806, Loss 4.4729, Error 0.3843(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 77.0543(78.8437) | Bit/dim 3.9284(3.9323) | Xent 1.1158(1.1148) | Loss 4.4863(4.4897) | Error 0.4008(0.3979) Steps 826(823.21) | Grad Norm 2.4563(1.3548) | Total Time 14.00(14.00)\n",
      "Iter 2102 | Time 79.8403(78.8736) | Bit/dim 3.9286(3.9322) | Xent 1.0979(1.1143) | Loss 4.4775(4.4893) | Error 0.3935(0.3977) Steps 820(823.11) | Grad Norm 1.6123(1.3625) | Total Time 14.00(14.00)\n",
      "Iter 2103 | Time 79.9210(78.9050) | Bit/dim 3.9283(3.9321) | Xent 1.0984(1.1138) | Loss 4.4776(4.4890) | Error 0.3878(0.3974) Steps 826(823.20) | Grad Norm 1.4506(1.3652) | Total Time 14.00(14.00)\n",
      "Iter 2104 | Time 78.7009(78.8989) | Bit/dim 3.9471(3.9325) | Xent 1.1011(1.1134) | Loss 4.4977(4.4892) | Error 0.3909(0.3972) Steps 832(823.46) | Grad Norm 2.0104(1.3845) | Total Time 14.00(14.00)\n",
      "Iter 2105 | Time 80.8805(78.9583) | Bit/dim 3.9185(3.9321) | Xent 1.1216(1.1137) | Loss 4.4793(4.4889) | Error 0.4041(0.3974) Steps 814(823.18) | Grad Norm 2.3714(1.4141) | Total Time 14.00(14.00)\n",
      "Iter 2106 | Time 78.5200(78.9452) | Bit/dim 3.9331(3.9321) | Xent 1.1169(1.1138) | Loss 4.4915(4.4890) | Error 0.4019(0.3976) Steps 832(823.44) | Grad Norm 1.0988(1.4047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 29.6346, Epoch Time 520.1307(508.2617), Bit/dim 3.9318(best: 3.9325), Xent 1.0771, Loss 4.4703, Error 0.3855(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 78.4911(78.9316) | Bit/dim 3.9360(3.9323) | Xent 1.1263(1.1141) | Loss 4.4992(4.4893) | Error 0.4020(0.3977) Steps 820(823.34) | Grad Norm 1.9309(1.4205) | Total Time 14.00(14.00)\n",
      "Iter 2108 | Time 75.5522(78.8302) | Bit/dim 3.9260(3.9321) | Xent 1.1127(1.1141) | Loss 4.4823(4.4891) | Error 0.3981(0.3977) Steps 826(823.42) | Grad Norm 2.0447(1.4392) | Total Time 14.00(14.00)\n",
      "Iter 2109 | Time 75.9077(78.7425) | Bit/dim 3.9272(3.9319) | Xent 1.0967(1.1136) | Loss 4.4755(4.4887) | Error 0.3886(0.3974) Steps 826(823.50) | Grad Norm 1.8573(1.4517) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 78.8253(78.7450) | Bit/dim 3.9313(3.9319) | Xent 1.1028(1.1133) | Loss 4.4827(4.4885) | Error 0.3892(0.3972) Steps 808(823.03) | Grad Norm 1.0939(1.4410) | Total Time 14.00(14.00)\n",
      "Iter 2111 | Time 79.0743(78.7549) | Bit/dim 3.9258(3.9317) | Xent 1.1020(1.1129) | Loss 4.4768(4.4882) | Error 0.3970(0.3972) Steps 826(823.12) | Grad Norm 0.9744(1.4270) | Total Time 14.00(14.00)\n",
      "Iter 2112 | Time 79.2209(78.7689) | Bit/dim 3.9316(3.9317) | Xent 1.1151(1.1130) | Loss 4.4892(4.4882) | Error 0.3975(0.3972) Steps 832(823.39) | Grad Norm 1.1505(1.4187) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 29.2248, Epoch Time 511.8953(508.3707), Bit/dim 3.9309(best: 3.9318), Xent 1.0754, Loss 4.4685, Error 0.3844(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 77.2704(78.7239) | Bit/dim 3.9307(3.9317) | Xent 1.1251(1.1133) | Loss 4.4933(4.4884) | Error 0.3952(0.3971) Steps 832(823.65) | Grad Norm 1.0732(1.4083) | Total Time 14.00(14.00)\n",
      "Iter 2114 | Time 76.9683(78.6712) | Bit/dim 3.9374(3.9319) | Xent 1.1020(1.1130) | Loss 4.4884(4.4884) | Error 0.3914(0.3970) Steps 808(823.18) | Grad Norm 1.6801(1.4165) | Total Time 14.00(14.00)\n",
      "Iter 2115 | Time 79.5374(78.6972) | Bit/dim 3.9212(3.9315) | Xent 1.1158(1.1131) | Loss 4.4790(4.4881) | Error 0.3960(0.3969) Steps 820(823.08) | Grad Norm 0.9054(1.4012) | Total Time 14.00(14.00)\n",
      "Iter 2116 | Time 76.6137(78.6347) | Bit/dim 3.9314(3.9315) | Xent 1.0887(1.1124) | Loss 4.4758(4.4877) | Error 0.3899(0.3967) Steps 826(823.17) | Grad Norm 0.7362(1.3812) | Total Time 14.00(14.00)\n",
      "Iter 2117 | Time 78.5892(78.6334) | Bit/dim 3.9352(3.9316) | Xent 1.1078(1.1122) | Loss 4.4890(4.4877) | Error 0.3956(0.3967) Steps 820(823.07) | Grad Norm 1.1299(1.3737) | Total Time 14.00(14.00)\n",
      "Iter 2118 | Time 79.1864(78.6499) | Bit/dim 3.9216(3.9313) | Xent 1.0929(1.1116) | Loss 4.4681(4.4872) | Error 0.3908(0.3965) Steps 832(823.34) | Grad Norm 1.4293(1.3753) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 29.5771, Epoch Time 513.2208(508.5162), Bit/dim 3.9303(best: 3.9309), Xent 1.0739, Loss 4.4672, Error 0.3829(best: 0.3842)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 75.6646(78.5604) | Bit/dim 3.9341(3.9314) | Xent 1.1035(1.1114) | Loss 4.4859(4.4871) | Error 0.3906(0.3963) Steps 832(823.60) | Grad Norm 0.9229(1.3618) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 78.1020(78.5466) | Bit/dim 3.9324(3.9315) | Xent 1.1173(1.1116) | Loss 4.4911(4.4872) | Error 0.3939(0.3963) Steps 808(823.13) | Grad Norm 0.9041(1.3480) | Total Time 14.00(14.00)\n",
      "Iter 2121 | Time 77.7649(78.5232) | Bit/dim 3.9307(3.9314) | Xent 1.1023(1.1113) | Loss 4.4818(4.4871) | Error 0.3922(0.3961) Steps 826(823.22) | Grad Norm 1.9299(1.3655) | Total Time 14.00(14.00)\n",
      "Iter 2122 | Time 80.2765(78.5758) | Bit/dim 3.9206(3.9311) | Xent 1.0934(1.1108) | Loss 4.4673(4.4865) | Error 0.3874(0.3959) Steps 832(823.48) | Grad Norm 1.1467(1.3589) | Total Time 14.00(14.00)\n",
      "Iter 2123 | Time 78.4267(78.5713) | Bit/dim 3.9267(3.9310) | Xent 1.1160(1.1109) | Loss 4.4847(4.4864) | Error 0.4012(0.3960) Steps 832(823.74) | Grad Norm 0.8571(1.3439) | Total Time 14.00(14.00)\n",
      "Iter 2124 | Time 76.9025(78.5212) | Bit/dim 3.9267(3.9308) | Xent 1.1092(1.1109) | Loss 4.4813(4.4863) | Error 0.3949(0.3960) Steps 808(823.27) | Grad Norm 1.4028(1.3456) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 29.2968, Epoch Time 511.8885(508.6174), Bit/dim 3.9306(best: 3.9303), Xent 1.0733, Loss 4.4673, Error 0.3799(best: 0.3829)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 78.5968(78.5235) | Bit/dim 3.9220(3.9306) | Xent 1.1107(1.1109) | Loss 4.4774(4.4860) | Error 0.3920(0.3959) Steps 808(822.81) | Grad Norm 1.7781(1.3586) | Total Time 14.00(14.00)\n",
      "Iter 2126 | Time 80.3774(78.5791) | Bit/dim 3.9257(3.9304) | Xent 1.1033(1.1106) | Loss 4.4773(4.4857) | Error 0.3931(0.3958) Steps 808(822.36) | Grad Norm 1.3701(1.3590) | Total Time 14.00(14.00)\n",
      "Iter 2127 | Time 77.1154(78.5352) | Bit/dim 3.9264(3.9303) | Xent 1.1019(1.1104) | Loss 4.4774(4.4855) | Error 0.3900(0.3956) Steps 826(822.47) | Grad Norm 1.7913(1.3719) | Total Time 14.00(14.00)\n",
      "Iter 2128 | Time 77.3509(78.4997) | Bit/dim 3.9348(3.9304) | Xent 1.0942(1.1099) | Loss 4.4819(4.4854) | Error 0.3884(0.3954) Steps 838(822.94) | Grad Norm 1.6678(1.3808) | Total Time 14.00(14.00)\n",
      "Iter 2129 | Time 79.7014(78.5357) | Bit/dim 3.9397(3.9307) | Xent 1.1069(1.1098) | Loss 4.4932(4.4856) | Error 0.3989(0.3955) Steps 808(822.49) | Grad Norm 1.3264(1.3792) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 79.5743(78.5669) | Bit/dim 3.9249(3.9306) | Xent 1.1078(1.1097) | Loss 4.4788(4.4854) | Error 0.3981(0.3956) Steps 814(822.24) | Grad Norm 0.8894(1.3645) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 29.4363, Epoch Time 517.4530(508.8825), Bit/dim 3.9311(best: 3.9303), Xent 1.0719, Loss 4.4670, Error 0.3855(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 78.8967(78.5768) | Bit/dim 3.9224(3.9303) | Xent 1.0973(1.1094) | Loss 4.4711(4.4850) | Error 0.3974(0.3957) Steps 832(822.53) | Grad Norm 0.9931(1.3533) | Total Time 14.00(14.00)\n",
      "Iter 2132 | Time 77.9603(78.5583) | Bit/dim 3.9401(3.9306) | Xent 1.1156(1.1095) | Loss 4.4979(4.4854) | Error 0.3960(0.3957) Steps 820(822.45) | Grad Norm 1.5513(1.3593) | Total Time 14.00(14.00)\n",
      "Iter 2133 | Time 81.1270(78.6354) | Bit/dim 3.9322(3.9307) | Xent 1.0955(1.1091) | Loss 4.4799(4.4852) | Error 0.3849(0.3953) Steps 826(822.56) | Grad Norm 1.5436(1.3648) | Total Time 14.00(14.00)\n",
      "Iter 2134 | Time 78.6103(78.6346) | Bit/dim 3.9269(3.9305) | Xent 1.1152(1.1093) | Loss 4.4845(4.4852) | Error 0.3972(0.3954) Steps 826(822.66) | Grad Norm 0.9115(1.3512) | Total Time 14.00(14.00)\n",
      "Iter 2135 | Time 80.4212(78.6882) | Bit/dim 3.9309(3.9305) | Xent 1.0957(1.1089) | Loss 4.4787(4.4850) | Error 0.3906(0.3953) Steps 832(822.94) | Grad Norm 1.9445(1.3690) | Total Time 14.00(14.00)\n",
      "Iter 2136 | Time 82.1660(78.7925) | Bit/dim 3.9271(3.9304) | Xent 1.1254(1.1094) | Loss 4.4899(4.4851) | Error 0.3985(0.3953) Steps 808(822.49) | Grad Norm 1.8384(1.3831) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 29.4538, Epoch Time 524.0411(509.3372), Bit/dim 3.9301(best: 3.9303), Xent 1.0729, Loss 4.4665, Error 0.3857(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 77.6005(78.7568) | Bit/dim 3.9292(3.9304) | Xent 1.0914(1.1089) | Loss 4.4749(4.4848) | Error 0.3951(0.3953) Steps 802(821.88) | Grad Norm 0.9917(1.3713) | Total Time 14.00(14.00)\n",
      "Iter 2138 | Time 79.6512(78.7836) | Bit/dim 3.9362(3.9306) | Xent 1.1098(1.1089) | Loss 4.4910(4.4850) | Error 0.3914(0.3952) Steps 826(822.00) | Grad Norm 1.6389(1.3794) | Total Time 14.00(14.00)\n",
      "Iter 2139 | Time 78.8484(78.7855) | Bit/dim 3.9234(3.9304) | Xent 1.1287(1.1095) | Loss 4.4877(4.4851) | Error 0.4012(0.3954) Steps 826(822.12) | Grad Norm 2.0788(1.4004) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 79.0652(78.7939) | Bit/dim 3.9164(3.9299) | Xent 1.1108(1.1095) | Loss 4.4718(4.4847) | Error 0.3956(0.3954) Steps 826(822.24) | Grad Norm 1.3271(1.3982) | Total Time 14.00(14.00)\n",
      "Iter 2141 | Time 74.7738(78.6733) | Bit/dim 3.9335(3.9301) | Xent 1.1144(1.1097) | Loss 4.4907(4.4849) | Error 0.3970(0.3955) Steps 820(822.17) | Grad Norm 1.1393(1.3904) | Total Time 14.00(14.00)\n",
      "Iter 2142 | Time 80.6195(78.7317) | Bit/dim 3.9329(3.9301) | Xent 1.0905(1.1091) | Loss 4.4782(4.4847) | Error 0.3868(0.3952) Steps 808(821.75) | Grad Norm 1.1866(1.3843) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 29.2204, Epoch Time 515.3478(509.5175), Bit/dim 3.9304(best: 3.9301), Xent 1.0716, Loss 4.4662, Error 0.3839(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 78.5943(78.7276) | Bit/dim 3.9303(3.9301) | Xent 1.1111(1.1091) | Loss 4.4858(4.4847) | Error 0.3964(0.3952) Steps 838(822.23) | Grad Norm 1.5539(1.3894) | Total Time 14.00(14.00)\n",
      "Iter 2144 | Time 78.6205(78.7244) | Bit/dim 3.9344(3.9303) | Xent 1.1047(1.1090) | Loss 4.4867(4.4848) | Error 0.3896(0.3951) Steps 832(822.53) | Grad Norm 0.8616(1.3735) | Total Time 14.00(14.00)\n",
      "Iter 2145 | Time 79.6487(78.7521) | Bit/dim 3.9298(3.9303) | Xent 1.0987(1.1087) | Loss 4.4792(4.4846) | Error 0.3880(0.3949) Steps 826(822.63) | Grad Norm 1.3454(1.3727) | Total Time 14.00(14.00)\n",
      "Iter 2146 | Time 80.3337(78.7996) | Bit/dim 3.9161(3.9298) | Xent 1.1144(1.1089) | Loss 4.4733(4.4843) | Error 0.3966(0.3949) Steps 808(822.19) | Grad Norm 1.8277(1.3863) | Total Time 14.00(14.00)\n",
      "Iter 2147 | Time 77.7216(78.7672) | Bit/dim 3.9297(3.9298) | Xent 1.1018(1.1087) | Loss 4.4806(4.4842) | Error 0.3999(0.3951) Steps 826(822.31) | Grad Norm 1.0665(1.3767) | Total Time 14.00(14.00)\n",
      "Iter 2148 | Time 78.7001(78.7652) | Bit/dim 3.9342(3.9300) | Xent 1.1099(1.1087) | Loss 4.4892(4.4843) | Error 0.3965(0.3951) Steps 808(821.88) | Grad Norm 0.7343(1.3575) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 29.3562, Epoch Time 518.3825(509.7835), Bit/dim 3.9296(best: 3.9301), Xent 1.0705, Loss 4.4648, Error 0.3819(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 78.5008(78.7573) | Bit/dim 3.9173(3.9296) | Xent 1.1099(1.1087) | Loss 4.4723(4.4840) | Error 0.3904(0.3950) Steps 808(821.46) | Grad Norm 1.1558(1.3514) | Total Time 14.00(14.00)\n",
      "Iter 2150 | Time 78.3699(78.7457) | Bit/dim 3.9183(3.9292) | Xent 1.1089(1.1087) | Loss 4.4728(4.4836) | Error 0.3958(0.3950) Steps 826(821.60) | Grad Norm 0.9441(1.3392) | Total Time 14.00(14.00)\n",
      "Iter 2151 | Time 79.5944(78.7711) | Bit/dim 3.9311(3.9293) | Xent 1.1020(1.1085) | Loss 4.4821(4.4836) | Error 0.3941(0.3950) Steps 832(821.91) | Grad Norm 1.4680(1.3431) | Total Time 14.00(14.00)\n",
      "Iter 2152 | Time 78.2312(78.7549) | Bit/dim 3.9356(3.9295) | Xent 1.1136(1.1087) | Loss 4.4924(4.4838) | Error 0.3936(0.3949) Steps 808(821.49) | Grad Norm 0.6443(1.3221) | Total Time 14.00(14.00)\n",
      "Iter 2153 | Time 77.8801(78.7287) | Bit/dim 3.9345(3.9296) | Xent 1.1011(1.1085) | Loss 4.4850(4.4839) | Error 0.3895(0.3948) Steps 826(821.63) | Grad Norm 1.4446(1.3258) | Total Time 14.00(14.00)\n",
      "Iter 2154 | Time 78.7556(78.7295) | Bit/dim 3.9272(3.9296) | Xent 1.0997(1.1082) | Loss 4.4771(4.4837) | Error 0.3980(0.3948) Steps 832(821.94) | Grad Norm 1.3047(1.3251) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 29.5172, Epoch Time 516.2607(509.9778), Bit/dim 3.9293(best: 3.9296), Xent 1.0684, Loss 4.4635, Error 0.3812(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 78.3673(78.7186) | Bit/dim 3.9332(3.9297) | Xent 1.0988(1.1079) | Loss 4.4825(4.4836) | Error 0.3906(0.3947) Steps 826(822.06) | Grad Norm 0.6238(1.3041) | Total Time 14.00(14.00)\n",
      "Iter 2156 | Time 77.6261(78.6858) | Bit/dim 3.9288(3.9296) | Xent 1.1050(1.1078) | Loss 4.4813(4.4836) | Error 0.3920(0.3946) Steps 826(822.18) | Grad Norm 0.7212(1.2866) | Total Time 14.00(14.00)\n",
      "Iter 2157 | Time 82.0177(78.7858) | Bit/dim 3.9281(3.9296) | Xent 1.1010(1.1076) | Loss 4.4787(4.4834) | Error 0.3956(0.3947) Steps 820(822.11) | Grad Norm 0.9354(1.2761) | Total Time 14.00(14.00)\n",
      "Iter 2158 | Time 80.8552(78.8479) | Bit/dim 3.9313(3.9297) | Xent 1.1040(1.1075) | Loss 4.4833(4.4834) | Error 0.3900(0.3945) Steps 832(822.41) | Grad Norm 1.3880(1.2794) | Total Time 14.00(14.00)\n",
      "Iter 2159 | Time 77.9893(78.8221) | Bit/dim 3.9195(3.9294) | Xent 1.1110(1.1076) | Loss 4.4750(4.4832) | Error 0.3956(0.3946) Steps 826(822.52) | Grad Norm 0.6891(1.2617) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 80.3190(78.8670) | Bit/dim 3.9241(3.9292) | Xent 1.1015(1.1074) | Loss 4.4749(4.4829) | Error 0.3971(0.3946) Steps 826(822.62) | Grad Norm 0.8708(1.2500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 29.5431, Epoch Time 522.2699(510.3466), Bit/dim 3.9285(best: 3.9293), Xent 1.0693, Loss 4.4632, Error 0.3812(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 78.7577(78.8637) | Bit/dim 3.9329(3.9293) | Xent 1.1071(1.1074) | Loss 4.4864(4.4830) | Error 0.3916(0.3945) Steps 826(822.72) | Grad Norm 1.6592(1.2623) | Total Time 14.00(14.00)\n",
      "Iter 2162 | Time 79.2841(78.8764) | Bit/dim 3.9220(3.9291) | Xent 1.0996(1.1072) | Loss 4.4718(4.4827) | Error 0.3866(0.3943) Steps 826(822.82) | Grad Norm 1.2358(1.2615) | Total Time 14.00(14.00)\n",
      "Iter 2163 | Time 76.8973(78.8170) | Bit/dim 3.9325(3.9292) | Xent 1.0918(1.1067) | Loss 4.4784(4.4826) | Error 0.3929(0.3943) Steps 808(822.38) | Grad Norm 1.1080(1.2569) | Total Time 14.00(14.00)\n",
      "Iter 2164 | Time 78.2200(78.7991) | Bit/dim 3.9289(3.9292) | Xent 1.1113(1.1069) | Loss 4.4845(4.4826) | Error 0.3916(0.3942) Steps 832(822.67) | Grad Norm 1.1105(1.2525) | Total Time 14.00(14.00)\n",
      "Iter 2165 | Time 77.9196(78.7727) | Bit/dim 3.9279(3.9291) | Xent 1.0959(1.1065) | Loss 4.4758(4.4824) | Error 0.3876(0.3940) Steps 826(822.77) | Grad Norm 1.4945(1.2597) | Total Time 14.00(14.00)\n",
      "Iter 2166 | Time 78.7518(78.7721) | Bit/dim 3.9199(3.9289) | Xent 1.0949(1.1062) | Loss 4.4674(4.4820) | Error 0.3912(0.3939) Steps 826(822.86) | Grad Norm 0.7577(1.2447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 29.7578, Epoch Time 515.1212(510.4898), Bit/dim 3.9292(best: 3.9285), Xent 1.0693, Loss 4.4638, Error 0.3800(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 77.4539(78.7325) | Bit/dim 3.9197(3.9286) | Xent 1.1137(1.1064) | Loss 4.4765(4.4818) | Error 0.3974(0.3940) Steps 826(822.96) | Grad Norm 1.3998(1.2493) | Total Time 14.00(14.00)\n",
      "Iter 2168 | Time 76.6339(78.6696) | Bit/dim 3.9288(3.9286) | Xent 1.0918(1.1060) | Loss 4.4747(4.4816) | Error 0.3885(0.3938) Steps 826(823.05) | Grad Norm 1.6712(1.2620) | Total Time 14.00(14.00)\n",
      "Iter 2169 | Time 77.8673(78.6455) | Bit/dim 3.9264(3.9285) | Xent 1.0938(1.1056) | Loss 4.4733(4.4813) | Error 0.3858(0.3936) Steps 820(822.96) | Grad Norm 0.8266(1.2489) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 76.2402(78.5733) | Bit/dim 3.9386(3.9288) | Xent 1.0935(1.1052) | Loss 4.4853(4.4815) | Error 0.3845(0.3933) Steps 826(823.05) | Grad Norm 0.9637(1.2404) | Total Time 14.00(14.00)\n",
      "Iter 2171 | Time 78.2674(78.5642) | Bit/dim 3.9212(3.9286) | Xent 1.1041(1.1052) | Loss 4.4732(4.4812) | Error 0.3928(0.3933) Steps 826(823.14) | Grad Norm 1.7962(1.2570) | Total Time 14.00(14.00)\n",
      "Iter 2172 | Time 79.2416(78.5845) | Bit/dim 3.9362(3.9288) | Xent 1.1331(1.1061) | Loss 4.5028(4.4819) | Error 0.4107(0.3938) Steps 832(823.40) | Grad Norm 1.2391(1.2565) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 29.5427, Epoch Time 511.1271(510.5089), Bit/dim 3.9289(best: 3.9285), Xent 1.0673, Loss 4.4626, Error 0.3817(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 76.6476(78.5264) | Bit/dim 3.9233(3.9287) | Xent 1.1051(1.1060) | Loss 4.4759(4.4817) | Error 0.3919(0.3938) Steps 826(823.48) | Grad Norm 0.7034(1.2399) | Total Time 14.00(14.00)\n",
      "Iter 2174 | Time 78.0038(78.5107) | Bit/dim 3.9228(3.9285) | Xent 1.0974(1.1058) | Loss 4.4716(4.4814) | Error 0.3878(0.3936) Steps 832(823.74) | Grad Norm 1.3880(1.2444) | Total Time 14.00(14.00)\n",
      "Iter 2175 | Time 79.9643(78.5543) | Bit/dim 3.9306(3.9286) | Xent 1.1206(1.1062) | Loss 4.4909(4.4817) | Error 0.3985(0.3937) Steps 832(823.98) | Grad Norm 1.1749(1.2423) | Total Time 14.00(14.00)\n",
      "Iter 2176 | Time 79.1334(78.5717) | Bit/dim 3.9285(3.9285) | Xent 1.1322(1.1070) | Loss 4.4946(4.4820) | Error 0.4026(0.3940) Steps 826(824.04) | Grad Norm 1.4352(1.2481) | Total Time 14.00(14.00)\n",
      "Iter 2177 | Time 77.5664(78.5415) | Bit/dim 3.9310(3.9286) | Xent 1.1038(1.1069) | Loss 4.4829(4.4821) | Error 0.3959(0.3941) Steps 826(824.10) | Grad Norm 1.2629(1.2485) | Total Time 14.00(14.00)\n",
      "Iter 2178 | Time 79.2663(78.5633) | Bit/dim 3.9248(3.9285) | Xent 1.0760(1.1060) | Loss 4.4628(4.4815) | Error 0.3832(0.3937) Steps 826(824.16) | Grad Norm 0.8152(1.2355) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 29.6284, Epoch Time 515.6547(510.6633), Bit/dim 3.9287(best: 3.9285), Xent 1.0666, Loss 4.4620, Error 0.3784(best: 0.3799)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 76.7897(78.5101) | Bit/dim 3.9209(3.9283) | Xent 1.1112(1.1061) | Loss 4.4764(4.4813) | Error 0.3999(0.3939) Steps 826(824.22) | Grad Norm 1.0081(1.2287) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 77.1710(78.4699) | Bit/dim 3.9226(3.9281) | Xent 1.0960(1.1058) | Loss 4.4706(4.4810) | Error 0.3881(0.3938) Steps 820(824.09) | Grad Norm 0.8274(1.2166) | Total Time 14.00(14.00)\n",
      "Iter 2181 | Time 78.2344(78.4628) | Bit/dim 3.9305(3.9282) | Xent 1.1171(1.1062) | Loss 4.4891(4.4813) | Error 0.3940(0.3938) Steps 820(823.97) | Grad Norm 1.4558(1.2238) | Total Time 14.00(14.00)\n",
      "Iter 2182 | Time 76.8971(78.4158) | Bit/dim 3.9185(3.9279) | Xent 1.0904(1.1057) | Loss 4.4637(4.4807) | Error 0.3862(0.3935) Steps 826(824.03) | Grad Norm 0.8717(1.2133) | Total Time 14.00(14.00)\n",
      "Iter 2183 | Time 77.0423(78.3746) | Bit/dim 3.9381(3.9282) | Xent 1.0916(1.1053) | Loss 4.4839(4.4808) | Error 0.3864(0.3933) Steps 826(824.09) | Grad Norm 1.6970(1.2278) | Total Time 14.00(14.00)\n",
      "Iter 2184 | Time 78.6859(78.3840) | Bit/dim 3.9315(3.9283) | Xent 1.1066(1.1053) | Loss 4.4847(4.4809) | Error 0.3945(0.3934) Steps 826(824.14) | Grad Norm 1.8066(1.2451) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 29.5922, Epoch Time 510.3914(510.6551), Bit/dim 3.9277(best: 3.9285), Xent 1.0668, Loss 4.4611, Error 0.3809(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 80.5457(78.4488) | Bit/dim 3.9341(3.9285) | Xent 1.0871(1.1048) | Loss 4.4776(4.4808) | Error 0.3856(0.3931) Steps 826(824.20) | Grad Norm 0.7300(1.2297) | Total Time 14.00(14.00)\n",
      "Iter 2186 | Time 78.6919(78.4561) | Bit/dim 3.9307(3.9285) | Xent 1.1223(1.1053) | Loss 4.4919(4.4812) | Error 0.3996(0.3933) Steps 832(824.43) | Grad Norm 2.0122(1.2532) | Total Time 14.00(14.00)\n",
      "Iter 2187 | Time 75.5174(78.3680) | Bit/dim 3.9270(3.9285) | Xent 1.1023(1.1052) | Loss 4.4781(4.4811) | Error 0.3966(0.3934) Steps 826(824.48) | Grad Norm 1.7258(1.2673) | Total Time 14.00(14.00)\n",
      "Iter 2188 | Time 79.3200(78.3965) | Bit/dim 3.9266(3.9284) | Xent 1.1009(1.1051) | Loss 4.4770(4.4810) | Error 0.3931(0.3934) Steps 832(824.71) | Grad Norm 0.7596(1.2521) | Total Time 14.00(14.00)\n",
      "Iter 2189 | Time 77.2586(78.3624) | Bit/dim 3.9258(3.9284) | Xent 1.1133(1.1053) | Loss 4.4824(4.4810) | Error 0.3951(0.3935) Steps 826(824.74) | Grad Norm 1.8868(1.2711) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 77.7395(78.3437) | Bit/dim 3.9182(3.9280) | Xent 1.0989(1.1051) | Loss 4.4676(4.4806) | Error 0.3908(0.3934) Steps 826(824.78) | Grad Norm 2.5424(1.3093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 29.7647, Epoch Time 514.4106(510.7678), Bit/dim 3.9286(best: 3.9277), Xent 1.0652, Loss 4.4612, Error 0.3793(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 79.4151(78.3758) | Bit/dim 3.9239(3.9279) | Xent 1.1091(1.1052) | Loss 4.4784(4.4805) | Error 0.3919(0.3933) Steps 826(824.82) | Grad Norm 0.7640(1.2929) | Total Time 14.00(14.00)\n",
      "Iter 2192 | Time 79.7256(78.4163) | Bit/dim 3.9206(3.9277) | Xent 1.0963(1.1050) | Loss 4.4687(4.4802) | Error 0.3861(0.3931) Steps 814(824.49) | Grad Norm 2.8688(1.3402) | Total Time 14.00(14.00)\n",
      "Iter 2193 | Time 78.4919(78.4186) | Bit/dim 3.9234(3.9276) | Xent 1.1101(1.1051) | Loss 4.4785(4.4801) | Error 0.4009(0.3934) Steps 814(824.18) | Grad Norm 2.8734(1.3862) | Total Time 14.00(14.00)\n",
      "Iter 2194 | Time 76.6846(78.3666) | Bit/dim 3.9248(3.9275) | Xent 1.1018(1.1050) | Loss 4.4757(4.4800) | Error 0.3840(0.3931) Steps 826(824.23) | Grad Norm 1.3216(1.3843) | Total Time 14.00(14.00)\n",
      "Iter 2195 | Time 77.9403(78.3538) | Bit/dim 3.9310(3.9276) | Xent 1.1123(1.1052) | Loss 4.4872(4.4802) | Error 0.3954(0.3931) Steps 820(824.11) | Grad Norm 2.6018(1.4208) | Total Time 14.00(14.00)\n",
      "Iter 2196 | Time 79.2772(78.3815) | Bit/dim 3.9341(3.9278) | Xent 1.1024(1.1052) | Loss 4.4853(4.4804) | Error 0.3964(0.3932) Steps 832(824.34) | Grad Norm 2.4250(1.4509) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 29.3101, Epoch Time 516.1901(510.9305), Bit/dim 3.9275(best: 3.9277), Xent 1.0638, Loss 4.4594, Error 0.3790(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 78.9698(78.3991) | Bit/dim 3.9194(3.9275) | Xent 1.0975(1.1049) | Loss 4.4682(4.4800) | Error 0.3860(0.3930) Steps 826(824.39) | Grad Norm 0.8620(1.4332) | Total Time 14.00(14.00)\n",
      "Iter 2198 | Time 77.4906(78.3719) | Bit/dim 3.9263(3.9275) | Xent 1.0921(1.1045) | Loss 4.4723(4.4798) | Error 0.3901(0.3929) Steps 814(824.08) | Grad Norm 2.2072(1.4565) | Total Time 14.00(14.00)\n",
      "Iter 2199 | Time 78.8108(78.3851) | Bit/dim 3.9263(3.9275) | Xent 1.1165(1.1049) | Loss 4.4846(4.4799) | Error 0.4004(0.3932) Steps 820(823.96) | Grad Norm 1.8955(1.4696) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 76.3814(78.3249) | Bit/dim 3.9294(3.9275) | Xent 1.0799(1.1042) | Loss 4.4693(4.4796) | Error 0.3819(0.3928) Steps 826(824.02) | Grad Norm 1.0861(1.4581) | Total Time 14.00(14.00)\n",
      "Iter 2201 | Time 77.4820(78.2997) | Bit/dim 3.9242(3.9274) | Xent 1.1105(1.1043) | Loss 4.4795(4.4796) | Error 0.3969(0.3929) Steps 826(824.08) | Grad Norm 1.0001(1.4444) | Total Time 14.00(14.00)\n",
      "Iter 2202 | Time 77.7899(78.2844) | Bit/dim 3.9321(3.9276) | Xent 1.0842(1.1037) | Loss 4.4742(4.4794) | Error 0.3881(0.3928) Steps 832(824.32) | Grad Norm 2.3868(1.4727) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 29.7319, Epoch Time 512.3442(510.9729), Bit/dim 3.9271(best: 3.9275), Xent 1.0634, Loss 4.4588, Error 0.3793(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 78.3686(78.2869) | Bit/dim 3.9257(3.9275) | Xent 1.1177(1.1042) | Loss 4.4845(4.4796) | Error 0.3964(0.3929) Steps 832(824.55) | Grad Norm 1.3050(1.4676) | Total Time 14.00(14.00)\n",
      "Iter 2204 | Time 77.8898(78.2750) | Bit/dim 3.9159(3.9272) | Xent 1.1025(1.1041) | Loss 4.4672(4.4792) | Error 0.3898(0.3928) Steps 820(824.41) | Grad Norm 1.3286(1.4635) | Total Time 14.00(14.00)\n",
      "Iter 2205 | Time 81.7905(78.3804) | Bit/dim 3.9310(3.9273) | Xent 1.0953(1.1038) | Loss 4.4787(4.4792) | Error 0.3881(0.3927) Steps 826(824.46) | Grad Norm 2.1809(1.4850) | Total Time 14.00(14.00)\n",
      "Iter 2206 | Time 79.6631(78.4189) | Bit/dim 3.9262(3.9272) | Xent 1.0902(1.1034) | Loss 4.4713(4.4790) | Error 0.3848(0.3924) Steps 826(824.51) | Grad Norm 2.0257(1.5012) | Total Time 14.00(14.00)\n",
      "Iter 2207 | Time 80.3598(78.4771) | Bit/dim 3.9240(3.9271) | Xent 1.0941(1.1032) | Loss 4.4710(4.4787) | Error 0.3918(0.3924) Steps 826(824.55) | Grad Norm 1.2707(1.4943) | Total Time 14.00(14.00)\n",
      "Iter 2208 | Time 79.1560(78.4975) | Bit/dim 3.9312(3.9273) | Xent 1.0822(1.1025) | Loss 4.4723(4.4785) | Error 0.3831(0.3921) Steps 826(824.59) | Grad Norm 2.8767(1.5358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 29.2441, Epoch Time 522.1708(511.3088), Bit/dim 3.9269(best: 3.9271), Xent 1.0650, Loss 4.4594, Error 0.3791(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 80.9052(78.5697) | Bit/dim 3.9179(3.9270) | Xent 1.0874(1.1021) | Loss 4.4616(4.4780) | Error 0.3898(0.3921) Steps 826(824.64) | Grad Norm 1.2445(1.5270) | Total Time 14.00(14.00)\n",
      "Iter 2210 | Time 76.6646(78.5126) | Bit/dim 3.9192(3.9268) | Xent 1.1141(1.1024) | Loss 4.4763(4.4780) | Error 0.3979(0.3922) Steps 826(824.68) | Grad Norm 1.6387(1.5304) | Total Time 14.00(14.00)\n",
      "Iter 2211 | Time 78.2051(78.5034) | Bit/dim 3.9232(3.9267) | Xent 1.1015(1.1024) | Loss 4.4740(4.4779) | Error 0.3896(0.3922) Steps 826(824.72) | Grad Norm 2.0131(1.5449) | Total Time 14.00(14.00)\n",
      "Iter 2212 | Time 77.2094(78.4645) | Bit/dim 3.9315(3.9268) | Xent 1.1105(1.1026) | Loss 4.4868(4.4781) | Error 0.4021(0.3925) Steps 826(824.76) | Grad Norm 1.5471(1.5449) | Total Time 14.00(14.00)\n",
      "Iter 2213 | Time 77.6271(78.4394) | Bit/dim 3.9237(3.9267) | Xent 1.0915(1.1023) | Loss 4.4694(4.4779) | Error 0.3816(0.3921) Steps 826(824.79) | Grad Norm 1.4355(1.5416) | Total Time 14.00(14.00)\n",
      "Iter 2214 | Time 78.1073(78.4295) | Bit/dim 3.9356(3.9270) | Xent 1.0916(1.1020) | Loss 4.4814(4.4780) | Error 0.3832(0.3919) Steps 820(824.65) | Grad Norm 1.9083(1.5526) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 29.4242, Epoch Time 513.8016(511.3836), Bit/dim 3.9258(best: 3.9269), Xent 1.0645, Loss 4.4580, Error 0.3791(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 77.9626(78.4155) | Bit/dim 3.9243(3.9269) | Xent 1.0879(1.1016) | Loss 4.4682(4.4777) | Error 0.3852(0.3917) Steps 826(824.69) | Grad Norm 1.5481(1.5525) | Total Time 14.00(14.00)\n",
      "Iter 2216 | Time 78.2713(78.4111) | Bit/dim 3.9233(3.9268) | Xent 1.1149(1.1020) | Loss 4.4808(4.4778) | Error 0.3965(0.3918) Steps 826(824.73) | Grad Norm 1.5141(1.5513) | Total Time 14.00(14.00)\n",
      "Iter 2217 | Time 76.7930(78.3626) | Bit/dim 3.9251(3.9267) | Xent 1.0877(1.1015) | Loss 4.4689(4.4775) | Error 0.3902(0.3918) Steps 826(824.77) | Grad Norm 1.9057(1.5620) | Total Time 14.00(14.00)\n",
      "Iter 2218 | Time 78.4341(78.3647) | Bit/dim 3.9425(3.9272) | Xent 1.0863(1.1011) | Loss 4.4856(4.4777) | Error 0.3894(0.3917) Steps 826(824.80) | Grad Norm 1.6436(1.5644) | Total Time 14.00(14.00)\n",
      "Iter 2219 | Time 78.9180(78.3813) | Bit/dim 3.9169(3.9269) | Xent 1.1096(1.1013) | Loss 4.4717(4.4776) | Error 0.3971(0.3919) Steps 826(824.84) | Grad Norm 2.2997(1.5865) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 76.8631(78.3358) | Bit/dim 3.9184(3.9266) | Xent 1.0815(1.1007) | Loss 4.4591(4.4770) | Error 0.3885(0.3918) Steps 808(824.33) | Grad Norm 1.0910(1.5716) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 29.3967, Epoch Time 512.3479(511.4125), Bit/dim 3.9260(best: 3.9258), Xent 1.0631, Loss 4.4575, Error 0.3800(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 79.5929(78.3735) | Bit/dim 3.9240(3.9266) | Xent 1.0920(1.1005) | Loss 4.4700(4.4768) | Error 0.3855(0.3916) Steps 826(824.38) | Grad Norm 1.4957(1.5693) | Total Time 14.00(14.00)\n",
      "Iter 2222 | Time 77.4176(78.3448) | Bit/dim 3.9175(3.9263) | Xent 1.0897(1.1002) | Loss 4.4623(4.4764) | Error 0.3869(0.3914) Steps 826(824.43) | Grad Norm 1.6009(1.5703) | Total Time 14.00(14.00)\n",
      "Iter 2223 | Time 78.4658(78.3484) | Bit/dim 3.9260(3.9263) | Xent 1.0812(1.0996) | Loss 4.4666(4.4761) | Error 0.3848(0.3912) Steps 820(824.30) | Grad Norm 1.0021(1.5532) | Total Time 14.00(14.00)\n",
      "Iter 2224 | Time 80.6232(78.4167) | Bit/dim 3.9296(3.9264) | Xent 1.1137(1.1000) | Loss 4.4864(4.4764) | Error 0.3981(0.3914) Steps 826(824.35) | Grad Norm 1.0342(1.5377) | Total Time 14.00(14.00)\n",
      "Iter 2225 | Time 78.8000(78.4282) | Bit/dim 3.9284(3.9264) | Xent 1.0915(1.0998) | Loss 4.4742(4.4763) | Error 0.3876(0.3913) Steps 826(824.40) | Grad Norm 1.1163(1.5250) | Total Time 14.00(14.00)\n",
      "Iter 2226 | Time 78.1353(78.4194) | Bit/dim 3.9238(3.9264) | Xent 1.0819(1.0992) | Loss 4.4648(4.4760) | Error 0.3822(0.3910) Steps 826(824.45) | Grad Norm 0.9897(1.5090) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 29.4491, Epoch Time 518.2823(511.6186), Bit/dim 3.9260(best: 3.9258), Xent 1.0592, Loss 4.4556, Error 0.3776(best: 0.3784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 77.7519(78.3994) | Bit/dim 3.9342(3.9266) | Xent 1.0843(1.0988) | Loss 4.4763(4.4760) | Error 0.3792(0.3907) Steps 826(824.49) | Grad Norm 0.7768(1.4870) | Total Time 14.00(14.00)\n",
      "Iter 2228 | Time 76.3069(78.3366) | Bit/dim 3.9159(3.9263) | Xent 1.0881(1.0985) | Loss 4.4599(4.4755) | Error 0.3904(0.3907) Steps 808(824.00) | Grad Norm 1.6610(1.4922) | Total Time 14.00(14.00)\n",
      "Iter 2229 | Time 75.2679(78.2445) | Bit/dim 3.9242(3.9262) | Xent 1.0973(1.0984) | Loss 4.4729(4.4754) | Error 0.3928(0.3907) Steps 826(824.06) | Grad Norm 0.9632(1.4764) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 79.1189(78.2708) | Bit/dim 3.9251(3.9262) | Xent 1.0873(1.0981) | Loss 4.4687(4.4752) | Error 0.3822(0.3905) Steps 826(824.12) | Grad Norm 1.4230(1.4748) | Total Time 14.00(14.00)\n",
      "Iter 2231 | Time 77.7492(78.2551) | Bit/dim 3.9217(3.9260) | Xent 1.0980(1.0981) | Loss 4.4707(4.4751) | Error 0.3909(0.3905) Steps 826(824.17) | Grad Norm 1.7137(1.4819) | Total Time 14.00(14.00)\n",
      "Iter 2232 | Time 77.8817(78.2439) | Bit/dim 3.9247(3.9260) | Xent 1.0812(1.0976) | Loss 4.4653(4.4748) | Error 0.3820(0.3902) Steps 826(824.23) | Grad Norm 0.9049(1.4646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 29.3485, Epoch Time 509.0746(511.5423), Bit/dim 3.9257(best: 3.9258), Xent 1.0625, Loss 4.4569, Error 0.3777(best: 0.3776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2233 | Time 79.2111(78.2729) | Bit/dim 3.9276(3.9261) | Xent 1.1000(1.0976) | Loss 4.4776(4.4749) | Error 0.3949(0.3904) Steps 826(824.28) | Grad Norm 1.0213(1.4513) | Total Time 14.00(14.00)\n",
      "Iter 2234 | Time 79.1282(78.2986) | Bit/dim 3.9258(3.9260) | Xent 1.0858(1.0973) | Loss 4.4687(4.4747) | Error 0.3899(0.3904) Steps 826(824.33) | Grad Norm 2.4571(1.4815) | Total Time 14.00(14.00)\n",
      "Iter 2235 | Time 78.1411(78.2939) | Bit/dim 3.9285(3.9261) | Xent 1.0971(1.0973) | Loss 4.4770(4.4748) | Error 0.3940(0.3905) Steps 826(824.38) | Grad Norm 0.7355(1.4591) | Total Time 14.00(14.00)\n",
      "Iter 2236 | Time 79.1057(78.3182) | Bit/dim 3.9206(3.9260) | Xent 1.0920(1.0971) | Loss 4.4666(4.4745) | Error 0.3828(0.3902) Steps 826(824.43) | Grad Norm 0.9421(1.4436) | Total Time 14.00(14.00)\n",
      "Iter 2237 | Time 77.3137(78.2881) | Bit/dim 3.9230(3.9259) | Xent 1.1076(1.0974) | Loss 4.4768(4.4746) | Error 0.3912(0.3903) Steps 826(824.48) | Grad Norm 1.5524(1.4469) | Total Time 14.00(14.00)\n",
      "Iter 2238 | Time 79.9571(78.3382) | Bit/dim 3.9202(3.9257) | Xent 1.0769(1.0968) | Loss 4.4587(4.4741) | Error 0.3818(0.3900) Steps 826(824.53) | Grad Norm 1.7390(1.4556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 29.4097, Epoch Time 517.7695(511.7291), Bit/dim 3.9265(best: 3.9257), Xent 1.0593, Loss 4.4562, Error 0.3765(best: 0.3776)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2239 | Time 77.9729(78.3272) | Bit/dim 3.9373(3.9260) | Xent 1.0927(1.0967) | Loss 4.4837(4.4744) | Error 0.3871(0.3899) Steps 814(824.21) | Grad Norm 1.2289(1.4488) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 79.0041(78.3475) | Bit/dim 3.9205(3.9259) | Xent 1.0998(1.0968) | Loss 4.4704(4.4743) | Error 0.3942(0.3901) Steps 796(823.36) | Grad Norm 1.1312(1.4393) | Total Time 14.00(14.00)\n",
      "Iter 2241 | Time 77.1300(78.3110) | Bit/dim 3.9193(3.9257) | Xent 1.0780(1.0962) | Loss 4.4583(4.4738) | Error 0.3852(0.3899) Steps 808(822.90) | Grad Norm 1.6347(1.4452) | Total Time 14.00(14.00)\n",
      "Iter 2242 | Time 78.7523(78.3242) | Bit/dim 3.9235(3.9256) | Xent 1.1005(1.0964) | Loss 4.4737(4.4738) | Error 0.3934(0.3900) Steps 820(822.82) | Grad Norm 1.7310(1.4537) | Total Time 14.00(14.00)\n",
      "Iter 2243 | Time 78.6848(78.3350) | Bit/dim 3.9241(3.9256) | Xent 1.1156(1.0969) | Loss 4.4819(4.4740) | Error 0.3958(0.3902) Steps 820(822.73) | Grad Norm 1.1029(1.4432) | Total Time 14.00(14.00)\n",
      "Iter 2244 | Time 78.2791(78.3334) | Bit/dim 3.9284(3.9257) | Xent 1.0813(1.0965) | Loss 4.4691(4.4739) | Error 0.3798(0.3899) Steps 814(822.47) | Grad Norm 1.3197(1.4395) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 29.2701, Epoch Time 514.4901(511.8120), Bit/dim 3.9258(best: 3.9257), Xent 1.0576, Loss 4.4547, Error 0.3753(best: 0.3765)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2245 | Time 77.2357(78.3004) | Bit/dim 3.9351(3.9259) | Xent 1.0929(1.0964) | Loss 4.4816(4.4741) | Error 0.3938(0.3900) Steps 826(822.57) | Grad Norm 0.6308(1.4152) | Total Time 14.00(14.00)\n",
      "Iter 2246 | Time 77.8234(78.2861) | Bit/dim 3.9268(3.9260) | Xent 1.0873(1.0961) | Loss 4.4705(4.4740) | Error 0.3890(0.3900) Steps 826(822.68) | Grad Norm 1.6871(1.4234) | Total Time 14.00(14.00)\n",
      "Iter 2247 | Time 77.3878(78.2592) | Bit/dim 3.9123(3.9256) | Xent 1.0632(1.0951) | Loss 4.4439(4.4731) | Error 0.3790(0.3896) Steps 820(822.60) | Grad Norm 1.9022(1.4378) | Total Time 14.00(14.00)\n",
      "Iter 2248 | Time 77.4876(78.2360) | Bit/dim 3.9262(3.9256) | Xent 1.0877(1.0949) | Loss 4.4700(4.4730) | Error 0.3882(0.3896) Steps 832(822.88) | Grad Norm 0.6373(1.4137) | Total Time 14.00(14.00)\n",
      "Iter 2249 | Time 77.2310(78.2059) | Bit/dim 3.9228(3.9255) | Xent 1.0893(1.0947) | Loss 4.4675(4.4728) | Error 0.3926(0.3897) Steps 826(822.97) | Grad Norm 2.2377(1.4385) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 78.4378(78.2128) | Bit/dim 3.9214(3.9254) | Xent 1.1011(1.0949) | Loss 4.4719(4.4728) | Error 0.3914(0.3897) Steps 814(822.70) | Grad Norm 1.7141(1.4467) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 29.2605, Epoch Time 510.1647(511.7625), Bit/dim 3.9244(best: 3.9257), Xent 1.0583, Loss 4.4536, Error 0.3775(best: 0.3753)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2251 | Time 79.3397(78.2466) | Bit/dim 3.9289(3.9255) | Xent 1.0522(1.0936) | Loss 4.4549(4.4723) | Error 0.3741(0.3893) Steps 826(822.80) | Grad Norm 1.3168(1.4428) | Total Time 14.00(14.00)\n",
      "Iter 2252 | Time 77.7481(78.2317) | Bit/dim 3.9195(3.9253) | Xent 1.0941(1.0936) | Loss 4.4666(4.4721) | Error 0.3916(0.3893) Steps 826(822.90) | Grad Norm 1.2237(1.4363) | Total Time 14.00(14.00)\n",
      "Iter 2253 | Time 78.0567(78.2264) | Bit/dim 3.9301(3.9254) | Xent 1.0894(1.0935) | Loss 4.4748(4.4722) | Error 0.3890(0.3893) Steps 832(823.17) | Grad Norm 1.3057(1.4323) | Total Time 14.00(14.00)\n",
      "Iter 2254 | Time 78.2790(78.2280) | Bit/dim 3.9125(3.9250) | Xent 1.1027(1.0938) | Loss 4.4638(4.4719) | Error 0.3974(0.3896) Steps 826(823.26) | Grad Norm 0.8116(1.4137) | Total Time 14.00(14.00)\n",
      "Iter 2255 | Time 78.2119(78.2275) | Bit/dim 3.9212(3.9249) | Xent 1.1044(1.0941) | Loss 4.4734(4.4720) | Error 0.3968(0.3898) Steps 808(822.80) | Grad Norm 1.8782(1.4277) | Total Time 14.00(14.00)\n",
      "Iter 2256 | Time 79.4144(78.2631) | Bit/dim 3.9328(3.9252) | Xent 1.1028(1.0944) | Loss 4.4842(4.4724) | Error 0.3942(0.3899) Steps 826(822.89) | Grad Norm 1.7015(1.4359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 29.6487, Epoch Time 516.0106(511.8900), Bit/dim 3.9244(best: 3.9244), Xent 1.0571, Loss 4.4529, Error 0.3757(best: 0.3753)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2257 | Time 76.5770(78.2125) | Bit/dim 3.9219(3.9251) | Xent 1.0800(1.0939) | Loss 4.4619(4.4720) | Error 0.3916(0.3900) Steps 832(823.17) | Grad Norm 1.1328(1.4268) | Total Time 14.00(14.00)\n",
      "Iter 2258 | Time 78.8064(78.2304) | Bit/dim 3.9180(3.9249) | Xent 1.0900(1.0938) | Loss 4.4630(4.4718) | Error 0.3889(0.3899) Steps 826(823.25) | Grad Norm 1.6451(1.4333) | Total Time 14.00(14.00)\n",
      "Iter 2259 | Time 80.8710(78.3096) | Bit/dim 3.9166(3.9246) | Xent 1.0949(1.0938) | Loss 4.4640(4.4715) | Error 0.3854(0.3898) Steps 826(823.34) | Grad Norm 0.8298(1.4152) | Total Time 14.00(14.00)\n",
      "Iter 2260 | Time 76.9372(78.2684) | Bit/dim 3.9372(3.9250) | Xent 1.0941(1.0939) | Loss 4.4843(4.4719) | Error 0.3884(0.3898) Steps 832(823.60) | Grad Norm 1.3893(1.4144) | Total Time 14.00(14.00)\n",
      "Iter 2261 | Time 75.9934(78.2002) | Bit/dim 3.9174(3.9248) | Xent 1.0802(1.0934) | Loss 4.4576(4.4715) | Error 0.3814(0.3895) Steps 826(823.67) | Grad Norm 0.9290(1.3999) | Total Time 14.00(14.00)\n",
      "Iter 2262 | Time 77.7165(78.1856) | Bit/dim 3.9326(3.9250) | Xent 1.0829(1.0931) | Loss 4.4740(4.4716) | Error 0.3841(0.3893) Steps 826(823.74) | Grad Norm 2.4218(1.4305) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 29.3049, Epoch Time 511.6152(511.8817), Bit/dim 3.9244(best: 3.9244), Xent 1.0557, Loss 4.4522, Error 0.3744(best: 0.3753)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2263 | Time 76.0554(78.1217) | Bit/dim 3.9229(3.9249) | Xent 1.1107(1.0937) | Loss 4.4783(4.4718) | Error 0.3952(0.3895) Steps 832(823.99) | Grad Norm 1.0909(1.4204) | Total Time 14.00(14.00)\n",
      "Iter 2264 | Time 78.2697(78.1262) | Bit/dim 3.9121(3.9246) | Xent 1.0648(1.0928) | Loss 4.4446(4.4709) | Error 0.3832(0.3893) Steps 814(823.69) | Grad Norm 0.5889(1.3954) | Total Time 14.00(14.00)\n",
      "Iter 2265 | Time 77.9962(78.1223) | Bit/dim 3.9244(3.9245) | Xent 1.0928(1.0928) | Loss 4.4708(4.4709) | Error 0.3904(0.3894) Steps 826(823.76) | Grad Norm 1.5009(1.3986) | Total Time 14.00(14.00)\n",
      "Iter 2266 | Time 79.1865(78.1542) | Bit/dim 3.9264(3.9246) | Xent 1.0951(1.0929) | Loss 4.4740(4.4710) | Error 0.3904(0.3894) Steps 826(823.82) | Grad Norm 1.0641(1.3885) | Total Time 14.00(14.00)\n",
      "Iter 2267 | Time 76.2608(78.0974) | Bit/dim 3.9204(3.9245) | Xent 1.0926(1.0929) | Loss 4.4667(4.4709) | Error 0.3908(0.3894) Steps 826(823.89) | Grad Norm 0.6117(1.3652) | Total Time 14.00(14.00)\n",
      "Iter 2268 | Time 76.0051(78.0346) | Bit/dim 3.9325(3.9247) | Xent 1.0741(1.0923) | Loss 4.4696(4.4709) | Error 0.3799(0.3892) Steps 826(823.95) | Grad Norm 1.3148(1.3637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 29.5478, Epoch Time 508.8996(511.7923), Bit/dim 3.9243(best: 3.9244), Xent 1.0557, Loss 4.4522, Error 0.3732(best: 0.3744)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2269 | Time 81.3609(78.1344) | Bit/dim 3.9324(3.9249) | Xent 1.1155(1.0930) | Loss 4.4901(4.4714) | Error 0.3965(0.3894) Steps 826(824.01) | Grad Norm 0.9331(1.3508) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 80.4185(78.2029) | Bit/dim 3.9201(3.9248) | Xent 1.0798(1.0926) | Loss 4.4600(4.4711) | Error 0.3915(0.3894) Steps 826(824.07) | Grad Norm 0.9027(1.3374) | Total Time 14.00(14.00)\n",
      "Iter 2271 | Time 77.6910(78.1876) | Bit/dim 3.9180(3.9246) | Xent 1.0818(1.0923) | Loss 4.4589(4.4707) | Error 0.3872(0.3894) Steps 826(824.13) | Grad Norm 1.1125(1.3306) | Total Time 14.00(14.00)\n",
      "Iter 2272 | Time 78.0274(78.1828) | Bit/dim 3.9260(3.9246) | Xent 1.0625(1.0914) | Loss 4.4573(4.4703) | Error 0.3858(0.3893) Steps 826(824.19) | Grad Norm 0.7645(1.3136) | Total Time 14.00(14.00)\n",
      "Iter 2273 | Time 77.9381(78.1754) | Bit/dim 3.9173(3.9244) | Xent 1.0790(1.0910) | Loss 4.4568(4.4699) | Error 0.3831(0.3891) Steps 826(824.24) | Grad Norm 0.9134(1.3016) | Total Time 14.00(14.00)\n",
      "Iter 2274 | Time 78.9325(78.1982) | Bit/dim 3.9215(3.9243) | Xent 1.0892(1.0910) | Loss 4.4661(4.4698) | Error 0.3836(0.3889) Steps 832(824.47) | Grad Norm 1.1378(1.2967) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 29.6272, Epoch Time 519.3404(512.0187), Bit/dim 3.9239(best: 3.9243), Xent 1.0555, Loss 4.4517, Error 0.3746(best: 0.3732)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2275 | Time 78.0186(78.1928) | Bit/dim 3.9218(3.9243) | Xent 1.0919(1.0910) | Loss 4.4677(4.4697) | Error 0.3910(0.3890) Steps 832(824.70) | Grad Norm 1.2481(1.2953) | Total Time 14.00(14.00)\n",
      "Iter 2276 | Time 79.2939(78.2258) | Bit/dim 3.9140(3.9239) | Xent 1.1042(1.0914) | Loss 4.4662(4.4696) | Error 0.3901(0.3890) Steps 826(824.74) | Grad Norm 2.0421(1.3177) | Total Time 14.00(14.00)\n",
      "Iter 2277 | Time 77.4228(78.2017) | Bit/dim 3.9215(3.9239) | Xent 1.0995(1.0916) | Loss 4.4713(4.4697) | Error 0.3909(0.3891) Steps 820(824.60) | Grad Norm 1.5149(1.3236) | Total Time 14.00(14.00)\n",
      "Iter 2278 | Time 78.5161(78.2111) | Bit/dim 3.9278(3.9240) | Xent 1.0636(1.0908) | Loss 4.4596(4.4694) | Error 0.3791(0.3888) Steps 826(824.64) | Grad Norm 1.2275(1.3207) | Total Time 14.00(14.00)\n",
      "Iter 2279 | Time 77.8512(78.2003) | Bit/dim 3.9236(3.9240) | Xent 1.1012(1.0911) | Loss 4.4742(4.4695) | Error 0.3894(0.3888) Steps 826(824.68) | Grad Norm 1.4117(1.3234) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 77.5012(78.1794) | Bit/dim 3.9259(3.9240) | Xent 1.0700(1.0905) | Loss 4.4609(4.4693) | Error 0.3804(0.3885) Steps 826(824.72) | Grad Norm 0.8042(1.3078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 29.3269, Epoch Time 513.2767(512.0565), Bit/dim 3.9230(best: 3.9239), Xent 1.0537, Loss 4.4498, Error 0.3744(best: 0.3732)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2281 | Time 79.7465(78.2264) | Bit/dim 3.9166(3.9238) | Xent 1.1022(1.0908) | Loss 4.4677(4.4692) | Error 0.3935(0.3887) Steps 832(824.94) | Grad Norm 1.6074(1.3168) | Total Time 14.00(14.00)\n",
      "Iter 2282 | Time 83.1604(78.3744) | Bit/dim 3.9247(3.9238) | Xent 1.0970(1.0910) | Loss 4.4732(4.4693) | Error 0.3919(0.3888) Steps 826(824.97) | Grad Norm 1.2493(1.3148) | Total Time 14.00(14.00)\n",
      "Iter 2283 | Time 78.0822(78.3656) | Bit/dim 3.9276(3.9240) | Xent 1.0850(1.0908) | Loss 4.4701(4.4694) | Error 0.3860(0.3887) Steps 826(825.00) | Grad Norm 1.3111(1.3147) | Total Time 14.00(14.00)\n",
      "Iter 2284 | Time 78.2687(78.3627) | Bit/dim 3.9191(3.9238) | Xent 1.0767(1.0904) | Loss 4.4574(4.4690) | Error 0.3850(0.3886) Steps 826(825.03) | Grad Norm 1.1945(1.3111) | Total Time 14.00(14.00)\n",
      "Iter 2285 | Time 78.1040(78.3550) | Bit/dim 3.9215(3.9237) | Xent 1.0715(1.0898) | Loss 4.4573(4.4687) | Error 0.3866(0.3885) Steps 826(825.06) | Grad Norm 1.0760(1.3040) | Total Time 14.00(14.00)\n",
      "Iter 2286 | Time 78.6026(78.3624) | Bit/dim 3.9220(3.9237) | Xent 1.0760(1.0894) | Loss 4.4600(4.4684) | Error 0.3794(0.3883) Steps 826(825.09) | Grad Norm 1.1205(1.2985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 29.0341, Epoch Time 520.4012(512.3068), Bit/dim 3.9231(best: 3.9230), Xent 1.0537, Loss 4.4500, Error 0.3747(best: 0.3732)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2287 | Time 76.8799(78.3179) | Bit/dim 3.9230(3.9237) | Xent 1.0629(1.0886) | Loss 4.4545(4.4680) | Error 0.3724(0.3878) Steps 814(824.75) | Grad Norm 0.8518(1.2851) | Total Time 14.00(14.00)\n",
      "Iter 2288 | Time 75.4159(78.2309) | Bit/dim 3.9260(3.9237) | Xent 1.0972(1.0889) | Loss 4.4746(4.4682) | Error 0.3925(0.3879) Steps 826(824.79) | Grad Norm 1.0355(1.2776) | Total Time 14.00(14.00)\n",
      "Iter 2289 | Time 78.7850(78.2475) | Bit/dim 3.9275(3.9239) | Xent 1.1010(1.0892) | Loss 4.4780(4.4685) | Error 0.3866(0.3879) Steps 826(824.83) | Grad Norm 1.0851(1.2719) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 78.6712(78.2602) | Bit/dim 3.9141(3.9236) | Xent 1.0966(1.0895) | Loss 4.4624(4.4683) | Error 0.3842(0.3878) Steps 826(824.86) | Grad Norm 0.9080(1.2609) | Total Time 14.00(14.00)\n",
      "Iter 2291 | Time 75.8733(78.1886) | Bit/dim 3.9161(3.9233) | Xent 1.0626(1.0887) | Loss 4.4474(4.4677) | Error 0.3751(0.3874) Steps 826(824.90) | Grad Norm 1.5623(1.2700) | Total Time 14.00(14.00)\n",
      "Iter 2292 | Time 77.8875(78.1796) | Bit/dim 3.9219(3.9233) | Xent 1.0959(1.0889) | Loss 4.4698(4.4677) | Error 0.3885(0.3874) Steps 826(824.93) | Grad Norm 1.1948(1.2677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 29.7671, Epoch Time 508.5910(512.1953), Bit/dim 3.9234(best: 3.9230), Xent 1.0523, Loss 4.4495, Error 0.3747(best: 0.3732)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2293 | Time 77.9166(78.1717) | Bit/dim 3.9124(3.9230) | Xent 1.0754(1.0885) | Loss 4.4501(4.4672) | Error 0.3831(0.3873) Steps 826(824.96) | Grad Norm 1.5362(1.2758) | Total Time 14.00(14.00)\n",
      "Iter 2294 | Time 76.8494(78.1320) | Bit/dim 3.9227(3.9230) | Xent 1.0619(1.0877) | Loss 4.4537(4.4668) | Error 0.3764(0.3870) Steps 826(824.99) | Grad Norm 1.1748(1.2728) | Total Time 14.00(14.00)\n",
      "Iter 2295 | Time 80.1593(78.1928) | Bit/dim 3.9170(3.9228) | Xent 1.0963(1.0879) | Loss 4.4652(4.4667) | Error 0.3944(0.3872) Steps 826(825.02) | Grad Norm 1.0854(1.2671) | Total Time 14.00(14.00)\n",
      "Iter 2296 | Time 76.0095(78.1273) | Bit/dim 3.9309(3.9230) | Xent 1.0723(1.0875) | Loss 4.4671(4.4668) | Error 0.3844(0.3871) Steps 826(825.05) | Grad Norm 2.0925(1.2919) | Total Time 14.00(14.00)\n",
      "Iter 2297 | Time 79.1657(78.1585) | Bit/dim 3.9248(3.9231) | Xent 1.0814(1.0873) | Loss 4.4655(4.4667) | Error 0.3901(0.3872) Steps 826(825.08) | Grad Norm 1.4313(1.2961) | Total Time 14.00(14.00)\n",
      "Iter 2298 | Time 78.2966(78.1626) | Bit/dim 3.9216(3.9230) | Xent 1.1016(1.0877) | Loss 4.4724(4.4669) | Error 0.3849(0.3871) Steps 826(825.11) | Grad Norm 1.4614(1.3010) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 29.4596, Epoch Time 513.4833(512.2340), Bit/dim 3.9218(best: 3.9230), Xent 1.0521, Loss 4.4479, Error 0.3694(best: 0.3732)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2299 | Time 76.7172(78.1193) | Bit/dim 3.9182(3.9229) | Xent 1.0822(1.0875) | Loss 4.4593(4.4667) | Error 0.3860(0.3871) Steps 826(825.14) | Grad Norm 1.9206(1.3196) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 76.2399(78.0629) | Bit/dim 3.9360(3.9233) | Xent 1.0900(1.0876) | Loss 4.4810(4.4671) | Error 0.3925(0.3873) Steps 826(825.16) | Grad Norm 1.2519(1.3176) | Total Time 14.00(14.00)\n",
      "Iter 2301 | Time 76.5343(78.0170) | Bit/dim 3.9181(3.9231) | Xent 1.0766(1.0873) | Loss 4.4564(4.4668) | Error 0.3821(0.3871) Steps 826(825.19) | Grad Norm 1.9625(1.3369) | Total Time 14.00(14.00)\n",
      "Iter 2302 | Time 77.3123(77.9959) | Bit/dim 3.9105(3.9227) | Xent 1.0793(1.0870) | Loss 4.4501(4.4663) | Error 0.3869(0.3871) Steps 820(825.03) | Grad Norm 1.4565(1.3405) | Total Time 14.00(14.00)\n",
      "Iter 2303 | Time 80.9407(78.0842) | Bit/dim 3.9222(3.9227) | Xent 1.0872(1.0871) | Loss 4.4658(4.4663) | Error 0.3918(0.3872) Steps 826(825.06) | Grad Norm 1.4586(1.3441) | Total Time 14.00(14.00)\n",
      "Iter 2304 | Time 76.0873(78.0243) | Bit/dim 3.9170(3.9226) | Xent 1.0766(1.0867) | Loss 4.4554(4.4659) | Error 0.3822(0.3871) Steps 826(825.09) | Grad Norm 1.1175(1.3373) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 29.1448, Epoch Time 508.5615(512.1238), Bit/dim 3.9232(best: 3.9218), Xent 1.0517, Loss 4.4490, Error 0.3750(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2305 | Time 76.4797(77.9780) | Bit/dim 3.9206(3.9225) | Xent 1.0923(1.0869) | Loss 4.4667(4.4660) | Error 0.3894(0.3872) Steps 826(825.12) | Grad Norm 1.1790(1.3325) | Total Time 14.00(14.00)\n",
      "Iter 2306 | Time 78.4789(77.9930) | Bit/dim 3.9116(3.9222) | Xent 1.0850(1.0869) | Loss 4.4541(4.4656) | Error 0.3762(0.3868) Steps 826(825.14) | Grad Norm 1.2547(1.3302) | Total Time 14.00(14.00)\n",
      "Iter 2307 | Time 77.9402(77.9914) | Bit/dim 3.9209(3.9221) | Xent 1.0934(1.0870) | Loss 4.4676(4.4657) | Error 0.3899(0.3869) Steps 826(825.17) | Grad Norm 1.1578(1.3250) | Total Time 14.00(14.00)\n",
      "Iter 2308 | Time 78.1979(77.9976) | Bit/dim 3.9311(3.9224) | Xent 1.0857(1.0870) | Loss 4.4739(4.4659) | Error 0.3952(0.3872) Steps 826(825.19) | Grad Norm 1.3653(1.3262) | Total Time 14.00(14.00)\n",
      "Iter 2309 | Time 76.7756(77.9609) | Bit/dim 3.9135(3.9221) | Xent 1.0581(1.0861) | Loss 4.4425(4.4652) | Error 0.3700(0.3867) Steps 826(825.22) | Grad Norm 1.4041(1.3286) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 80.5770(78.0394) | Bit/dim 3.9275(3.9223) | Xent 1.0887(1.0862) | Loss 4.4718(4.4654) | Error 0.3891(0.3867) Steps 826(825.24) | Grad Norm 1.0887(1.3214) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 29.8592, Epoch Time 513.8322(512.1750), Bit/dim 3.9227(best: 3.9218), Xent 1.0501, Loss 4.4477, Error 0.3730(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2311 | Time 79.4975(78.0832) | Bit/dim 3.9305(3.9225) | Xent 1.1002(1.0866) | Loss 4.4806(4.4659) | Error 0.3906(0.3868) Steps 826(825.26) | Grad Norm 0.7817(1.3052) | Total Time 14.00(14.00)\n",
      "Iter 2312 | Time 76.4765(78.0350) | Bit/dim 3.9171(3.9224) | Xent 1.0731(1.0862) | Loss 4.4536(4.4655) | Error 0.3774(0.3866) Steps 826(825.29) | Grad Norm 1.0134(1.2964) | Total Time 14.00(14.00)\n",
      "Iter 2313 | Time 79.1433(78.0682) | Bit/dim 3.9267(3.9225) | Xent 1.1030(1.0867) | Loss 4.4782(4.4659) | Error 0.3944(0.3868) Steps 808(824.77) | Grad Norm 0.8157(1.2820) | Total Time 14.00(14.00)\n",
      "Iter 2314 | Time 75.8832(78.0027) | Bit/dim 3.9058(3.9220) | Xent 1.0865(1.0867) | Loss 4.4490(4.4654) | Error 0.3821(0.3867) Steps 826(824.80) | Grad Norm 0.7194(1.2651) | Total Time 14.00(14.00)\n",
      "Iter 2315 | Time 77.8210(77.9972) | Bit/dim 3.9269(3.9222) | Xent 1.0723(1.0863) | Loss 4.4631(4.4653) | Error 0.3818(0.3865) Steps 814(824.48) | Grad Norm 1.4385(1.2703) | Total Time 14.00(14.00)\n",
      "Iter 2316 | Time 77.3373(77.9774) | Bit/dim 3.9204(3.9221) | Xent 1.0640(1.0856) | Loss 4.4524(4.4649) | Error 0.3731(0.3861) Steps 820(824.35) | Grad Norm 0.9823(1.2617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 29.4426, Epoch Time 511.5626(512.1567), Bit/dim 3.9215(best: 3.9218), Xent 1.0511, Loss 4.4471, Error 0.3746(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2317 | Time 80.0035(78.0382) | Bit/dim 3.9267(3.9222) | Xent 1.0802(1.0855) | Loss 4.4668(4.4650) | Error 0.3831(0.3860) Steps 826(824.40) | Grad Norm 1.0894(1.2565) | Total Time 14.00(14.00)\n",
      "Iter 2318 | Time 78.0578(78.0388) | Bit/dim 3.9160(3.9221) | Xent 1.0712(1.0850) | Loss 4.4515(4.4646) | Error 0.3810(0.3859) Steps 826(824.44) | Grad Norm 1.2170(1.2553) | Total Time 14.00(14.00)\n",
      "Iter 2319 | Time 76.6119(77.9960) | Bit/dim 3.9146(3.9218) | Xent 1.0791(1.0849) | Loss 4.4542(4.4643) | Error 0.3845(0.3858) Steps 826(824.49) | Grad Norm 0.8635(1.2436) | Total Time 14.00(14.00)\n",
      "Iter 2320 | Time 77.5627(77.9830) | Bit/dim 3.9260(3.9220) | Xent 1.0695(1.0844) | Loss 4.4607(4.4641) | Error 0.3820(0.3857) Steps 826(824.54) | Grad Norm 1.0105(1.2366) | Total Time 14.00(14.00)\n",
      "Iter 2321 | Time 79.2014(78.0195) | Bit/dim 3.9229(3.9220) | Xent 1.0963(1.0847) | Loss 4.4711(4.4644) | Error 0.3868(0.3857) Steps 832(824.76) | Grad Norm 1.2194(1.2361) | Total Time 14.00(14.00)\n",
      "Iter 2322 | Time 76.1273(77.9628) | Bit/dim 3.9230(3.9220) | Xent 1.0804(1.0846) | Loss 4.4632(4.4643) | Error 0.3845(0.3857) Steps 826(824.80) | Grad Norm 0.8917(1.2257) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 29.1903, Epoch Time 512.2078(512.1582), Bit/dim 3.9212(best: 3.9215), Xent 1.0485, Loss 4.4455, Error 0.3724(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2323 | Time 76.9485(77.9323) | Bit/dim 3.9277(3.9222) | Xent 1.0769(1.0844) | Loss 4.4662(4.4644) | Error 0.3836(0.3856) Steps 826(824.83) | Grad Norm 1.1030(1.2221) | Total Time 14.00(14.00)\n",
      "Iter 2324 | Time 77.3824(77.9158) | Bit/dim 3.9230(3.9222) | Xent 1.1013(1.0849) | Loss 4.4737(4.4647) | Error 0.3951(0.3859) Steps 820(824.69) | Grad Norm 1.6524(1.2350) | Total Time 14.00(14.00)\n",
      "Iter 2325 | Time 78.1774(77.9237) | Bit/dim 3.9204(3.9222) | Xent 1.0767(1.0846) | Loss 4.4587(4.4645) | Error 0.3828(0.3858) Steps 826(824.73) | Grad Norm 1.6534(1.2475) | Total Time 14.00(14.00)\n",
      "Iter 2326 | Time 77.0877(77.8986) | Bit/dim 3.9136(3.9219) | Xent 1.0764(1.0844) | Loss 4.4518(4.4641) | Error 0.3805(0.3857) Steps 826(824.77) | Grad Norm 0.8789(1.2365) | Total Time 14.00(14.00)\n",
      "Iter 2327 | Time 76.4257(77.8544) | Bit/dim 3.9176(3.9218) | Xent 1.0939(1.0847) | Loss 4.4646(4.4641) | Error 0.3889(0.3858) Steps 826(824.80) | Grad Norm 2.1621(1.2642) | Total Time 14.00(14.00)\n",
      "Iter 2328 | Time 79.3664(77.8998) | Bit/dim 3.9106(3.9214) | Xent 1.0815(1.0846) | Loss 4.4514(4.4637) | Error 0.3880(0.3858) Steps 826(824.84) | Grad Norm 2.2403(1.2935) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 29.4701, Epoch Time 510.0377(512.0946), Bit/dim 3.9207(best: 3.9212), Xent 1.0477, Loss 4.4445, Error 0.3719(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2329 | Time 76.9223(77.8705) | Bit/dim 3.9257(3.9216) | Xent 1.0702(1.0842) | Loss 4.4609(4.4636) | Error 0.3799(0.3857) Steps 832(825.05) | Grad Norm 0.8718(1.2809) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 78.9540(77.9030) | Bit/dim 3.9268(3.9217) | Xent 1.0754(1.0839) | Loss 4.4645(4.4637) | Error 0.3814(0.3855) Steps 826(825.08) | Grad Norm 2.3927(1.3142) | Total Time 14.00(14.00)\n",
      "Iter 2331 | Time 78.2958(77.9148) | Bit/dim 3.9161(3.9216) | Xent 1.0832(1.0839) | Loss 4.4577(4.4635) | Error 0.3871(0.3856) Steps 832(825.29) | Grad Norm 2.0958(1.3377) | Total Time 14.00(14.00)\n",
      "Iter 2332 | Time 77.5228(77.9030) | Bit/dim 3.9184(3.9215) | Xent 1.0869(1.0840) | Loss 4.4619(4.4634) | Error 0.3851(0.3856) Steps 826(825.31) | Grad Norm 0.8286(1.3224) | Total Time 14.00(14.00)\n",
      "Iter 2333 | Time 78.3497(77.9164) | Bit/dim 3.9120(3.9212) | Xent 1.0967(1.0843) | Loss 4.4604(4.4633) | Error 0.3934(0.3858) Steps 826(825.33) | Grad Norm 3.2921(1.3815) | Total Time 14.00(14.00)\n",
      "Iter 2334 | Time 78.3133(77.9283) | Bit/dim 3.9165(3.9210) | Xent 1.0700(1.0839) | Loss 4.4515(4.4630) | Error 0.3801(0.3856) Steps 820(825.17) | Grad Norm 1.4406(1.3833) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 29.1817, Epoch Time 513.0807(512.1242), Bit/dim 3.9209(best: 3.9207), Xent 1.0501, Loss 4.4459, Error 0.3728(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2335 | Time 77.5754(77.9177) | Bit/dim 3.9199(3.9210) | Xent 1.0847(1.0839) | Loss 4.4623(4.4630) | Error 0.3862(0.3856) Steps 826(825.20) | Grad Norm 1.3877(1.3834) | Total Time 14.00(14.00)\n",
      "Iter 2336 | Time 77.8125(77.9146) | Bit/dim 3.9149(3.9208) | Xent 1.0686(1.0835) | Loss 4.4492(4.4626) | Error 0.3786(0.3854) Steps 826(825.22) | Grad Norm 3.2819(1.4403) | Total Time 14.00(14.00)\n",
      "Iter 2337 | Time 80.5640(77.9940) | Bit/dim 3.9173(3.9207) | Xent 1.0865(1.0836) | Loss 4.4606(4.4625) | Error 0.3889(0.3855) Steps 826(825.24) | Grad Norm 2.5532(1.4737) | Total Time 14.00(14.00)\n",
      "Iter 2338 | Time 78.4372(78.0073) | Bit/dim 3.9273(3.9209) | Xent 1.0787(1.0834) | Loss 4.4666(4.4626) | Error 0.3891(0.3856) Steps 820(825.09) | Grad Norm 2.3933(1.5013) | Total Time 14.00(14.00)\n",
      "Iter 2339 | Time 77.1791(77.9825) | Bit/dim 3.9207(3.9209) | Xent 1.0848(1.0835) | Loss 4.4631(4.4626) | Error 0.3864(0.3857) Steps 826(825.11) | Grad Norm 2.8893(1.5430) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 80.8583(78.0688) | Bit/dim 3.9168(3.9208) | Xent 1.1010(1.0840) | Loss 4.4673(4.4628) | Error 0.3941(0.3859) Steps 826(825.14) | Grad Norm 3.0900(1.5894) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 29.5137, Epoch Time 517.4076(512.2827), Bit/dim 3.9203(best: 3.9207), Xent 1.0485, Loss 4.4446, Error 0.3744(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2341 | Time 80.3747(78.1379) | Bit/dim 3.9187(3.9207) | Xent 1.0801(1.0839) | Loss 4.4588(4.4627) | Error 0.3851(0.3859) Steps 814(824.81) | Grad Norm 2.1665(1.6067) | Total Time 14.00(14.00)\n",
      "Iter 2342 | Time 77.7327(78.1258) | Bit/dim 3.9146(3.9205) | Xent 1.0732(1.0836) | Loss 4.4512(4.4623) | Error 0.3848(0.3859) Steps 820(824.66) | Grad Norm 2.4583(1.6322) | Total Time 14.00(14.00)\n",
      "Iter 2343 | Time 77.3277(78.1018) | Bit/dim 3.9131(3.9203) | Xent 1.0697(1.0831) | Loss 4.4480(4.4619) | Error 0.3826(0.3858) Steps 826(824.70) | Grad Norm 1.5085(1.6285) | Total Time 14.00(14.00)\n",
      "Iter 2344 | Time 77.4672(78.0828) | Bit/dim 3.9247(3.9204) | Xent 1.0721(1.0828) | Loss 4.4607(4.4618) | Error 0.3869(0.3858) Steps 820(824.56) | Grad Norm 3.1040(1.6728) | Total Time 14.00(14.00)\n",
      "Iter 2345 | Time 76.3988(78.0323) | Bit/dim 3.9266(3.9206) | Xent 1.0718(1.0825) | Loss 4.4625(4.4619) | Error 0.3851(0.3858) Steps 826(824.60) | Grad Norm 2.2167(1.6891) | Total Time 14.00(14.00)\n",
      "Iter 2346 | Time 78.8929(78.0581) | Bit/dim 3.9176(3.9205) | Xent 1.1095(1.0833) | Loss 4.4724(4.4622) | Error 0.3960(0.3861) Steps 826(824.65) | Grad Norm 1.1813(1.6739) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 29.1738, Epoch Time 512.8444(512.2995), Bit/dim 3.9205(best: 3.9203), Xent 1.0461, Loss 4.4436, Error 0.3705(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2347 | Time 78.6224(78.0750) | Bit/dim 3.9213(3.9206) | Xent 1.0865(1.0834) | Loss 4.4646(4.4623) | Error 0.3859(0.3861) Steps 826(824.69) | Grad Norm 2.8208(1.7083) | Total Time 14.00(14.00)\n",
      "Iter 2348 | Time 78.3717(78.0839) | Bit/dim 3.9194(3.9205) | Xent 1.0679(1.0829) | Loss 4.4534(4.4620) | Error 0.3774(0.3858) Steps 814(824.37) | Grad Norm 1.5936(1.7048) | Total Time 14.00(14.00)\n",
      "Iter 2349 | Time 76.2210(78.0280) | Bit/dim 3.9150(3.9204) | Xent 1.0951(1.0833) | Loss 4.4626(4.4620) | Error 0.3866(0.3858) Steps 832(824.59) | Grad Norm 1.3057(1.6929) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 77.1549(78.0018) | Bit/dim 3.9176(3.9203) | Xent 1.0837(1.0833) | Loss 4.4595(4.4619) | Error 0.3899(0.3860) Steps 808(824.10) | Grad Norm 1.8252(1.6968) | Total Time 14.00(14.00)\n",
      "Iter 2351 | Time 78.7759(78.0251) | Bit/dim 3.9242(3.9204) | Xent 1.0632(1.0827) | Loss 4.4558(4.4617) | Error 0.3710(0.3855) Steps 826(824.15) | Grad Norm 1.8645(1.7019) | Total Time 14.00(14.00)\n",
      "Iter 2352 | Time 75.5198(77.9499) | Bit/dim 3.9158(3.9203) | Xent 1.0718(1.0824) | Loss 4.4517(4.4614) | Error 0.3792(0.3853) Steps 826(824.21) | Grad Norm 1.0353(1.6819) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 29.3846, Epoch Time 509.4730(512.2147), Bit/dim 3.9199(best: 3.9203), Xent 1.0472, Loss 4.4435, Error 0.3714(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2353 | Time 77.8237(77.9461) | Bit/dim 3.9209(3.9203) | Xent 1.0666(1.0819) | Loss 4.4542(4.4612) | Error 0.3809(0.3852) Steps 820(824.08) | Grad Norm 2.0458(1.6928) | Total Time 14.00(14.00)\n",
      "Iter 2354 | Time 80.0905(78.0105) | Bit/dim 3.9153(3.9201) | Xent 1.0892(1.0821) | Loss 4.4599(4.4612) | Error 0.3861(0.3852) Steps 826(824.14) | Grad Norm 1.1201(1.6756) | Total Time 14.00(14.00)\n",
      "Iter 2355 | Time 75.7618(77.9430) | Bit/dim 3.9141(3.9200) | Xent 1.0659(1.0816) | Loss 4.4471(4.4608) | Error 0.3762(0.3850) Steps 826(824.20) | Grad Norm 1.1389(1.6595) | Total Time 14.00(14.00)\n",
      "Iter 2356 | Time 78.4402(77.9579) | Bit/dim 3.9210(3.9200) | Xent 1.0678(1.0812) | Loss 4.4549(4.4606) | Error 0.3835(0.3849) Steps 832(824.43) | Grad Norm 0.9794(1.6391) | Total Time 14.00(14.00)\n",
      "Iter 2357 | Time 77.9150(77.9566) | Bit/dim 3.9163(3.9199) | Xent 1.0782(1.0811) | Loss 4.4554(4.4604) | Error 0.3855(0.3849) Steps 826(824.48) | Grad Norm 1.4545(1.6336) | Total Time 14.00(14.00)\n",
      "Iter 2358 | Time 78.5977(77.9759) | Bit/dim 3.9267(3.9201) | Xent 1.0688(1.0807) | Loss 4.4611(4.4605) | Error 0.3801(0.3848) Steps 826(824.52) | Grad Norm 0.9609(1.6134) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 29.8236, Epoch Time 513.8934(512.2651), Bit/dim 3.9200(best: 3.9199), Xent 1.0465, Loss 4.4432, Error 0.3718(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2359 | Time 76.7546(77.9392) | Bit/dim 3.9126(3.9199) | Xent 1.0667(1.0803) | Loss 4.4459(4.4600) | Error 0.3831(0.3847) Steps 826(824.57) | Grad Norm 1.4195(1.6076) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 75.5030(77.8661) | Bit/dim 3.9174(3.9198) | Xent 1.0831(1.0804) | Loss 4.4589(4.4600) | Error 0.3832(0.3847) Steps 826(824.61) | Grad Norm 1.1736(1.5945) | Total Time 14.00(14.00)\n",
      "Iter 2361 | Time 79.0210(77.9008) | Bit/dim 3.9198(3.9198) | Xent 1.0869(1.0806) | Loss 4.4633(4.4601) | Error 0.3834(0.3846) Steps 826(824.65) | Grad Norm 0.9135(1.5741) | Total Time 14.00(14.00)\n",
      "Iter 2362 | Time 80.3856(77.9753) | Bit/dim 3.9250(3.9199) | Xent 1.0725(1.0804) | Loss 4.4612(4.4601) | Error 0.3850(0.3847) Steps 826(824.69) | Grad Norm 0.7465(1.5493) | Total Time 14.00(14.00)\n",
      "Iter 2363 | Time 78.7508(77.9986) | Bit/dim 3.9127(3.9197) | Xent 1.1055(1.0811) | Loss 4.4654(4.4603) | Error 0.3939(0.3849) Steps 826(824.73) | Grad Norm 1.9256(1.5606) | Total Time 14.00(14.00)\n",
      "Iter 2364 | Time 78.0818(78.0011) | Bit/dim 3.9229(3.9198) | Xent 1.0678(1.0807) | Loss 4.4568(4.4602) | Error 0.3778(0.3847) Steps 814(824.41) | Grad Norm 0.8045(1.5379) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 29.2998, Epoch Time 513.4223(512.2998), Bit/dim 3.9193(best: 3.9199), Xent 1.0435, Loss 4.4411, Error 0.3717(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2365 | Time 77.9323(77.9990) | Bit/dim 3.9167(3.9197) | Xent 1.0659(1.0803) | Loss 4.4497(4.4599) | Error 0.3775(0.3845) Steps 826(824.46) | Grad Norm 1.1733(1.5270) | Total Time 14.00(14.00)\n",
      "Iter 2366 | Time 77.0123(77.9694) | Bit/dim 3.9131(3.9195) | Xent 1.0639(1.0798) | Loss 4.4450(4.4594) | Error 0.3731(0.3842) Steps 826(824.50) | Grad Norm 1.1934(1.5169) | Total Time 14.00(14.00)\n",
      "Iter 2367 | Time 77.3514(77.9509) | Bit/dim 3.9197(3.9195) | Xent 1.0839(1.0799) | Loss 4.4617(4.4595) | Error 0.3865(0.3842) Steps 832(824.73) | Grad Norm 1.3563(1.5121) | Total Time 14.00(14.00)\n",
      "Iter 2368 | Time 78.2411(77.9596) | Bit/dim 3.9171(3.9195) | Xent 1.0853(1.0801) | Loss 4.4598(4.4595) | Error 0.3859(0.3843) Steps 832(824.95) | Grad Norm 1.5635(1.5137) | Total Time 14.00(14.00)\n",
      "Iter 2369 | Time 76.2980(77.9097) | Bit/dim 3.9187(3.9194) | Xent 1.0863(1.0803) | Loss 4.4619(4.4596) | Error 0.3841(0.3843) Steps 826(824.98) | Grad Norm 1.4434(1.5116) | Total Time 14.00(14.00)\n",
      "Iter 2370 | Time 80.0618(77.9743) | Bit/dim 3.9222(3.9195) | Xent 1.0629(1.0797) | Loss 4.4537(4.4594) | Error 0.3839(0.3843) Steps 838(825.37) | Grad Norm 1.5998(1.5142) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 29.4899, Epoch Time 512.4304(512.3037), Bit/dim 3.9191(best: 3.9193), Xent 1.0420, Loss 4.4401, Error 0.3710(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2371 | Time 75.8080(77.9093) | Bit/dim 3.9156(3.9194) | Xent 1.0766(1.0796) | Loss 4.4539(4.4592) | Error 0.3809(0.3842) Steps 826(825.39) | Grad Norm 1.8467(1.5242) | Total Time 14.00(14.00)\n",
      "Iter 2372 | Time 75.3029(77.8311) | Bit/dim 3.9281(3.9197) | Xent 1.0752(1.0795) | Loss 4.4657(4.4594) | Error 0.3844(0.3842) Steps 826(825.41) | Grad Norm 2.4439(1.5518) | Total Time 14.00(14.00)\n",
      "Iter 2373 | Time 76.1938(77.7820) | Bit/dim 3.9291(3.9199) | Xent 1.0670(1.0791) | Loss 4.4625(4.4595) | Error 0.3829(0.3841) Steps 826(825.42) | Grad Norm 2.0335(1.5662) | Total Time 14.00(14.00)\n",
      "Iter 2374 | Time 77.2604(77.7664) | Bit/dim 3.9114(3.9197) | Xent 1.0851(1.0793) | Loss 4.4540(4.4593) | Error 0.3874(0.3842) Steps 832(825.62) | Grad Norm 1.8068(1.5734) | Total Time 14.00(14.00)\n",
      "Iter 2375 | Time 77.8081(77.7676) | Bit/dim 3.9022(3.9192) | Xent 1.0608(1.0788) | Loss 4.4326(4.4585) | Error 0.3804(0.3841) Steps 820(825.45) | Grad Norm 1.1670(1.5613) | Total Time 14.00(14.00)\n",
      "Iter 2376 | Time 76.1606(77.7194) | Bit/dim 3.9195(3.9192) | Xent 1.0768(1.0787) | Loss 4.4579(4.4585) | Error 0.3865(0.3842) Steps 826(825.47) | Grad Norm 1.2196(1.5510) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 29.6315, Epoch Time 503.6813(512.0451), Bit/dim 3.9178(best: 3.9191), Xent 1.0424, Loss 4.4390, Error 0.3703(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2377 | Time 75.3903(77.6495) | Bit/dim 3.9081(3.9188) | Xent 1.0735(1.0785) | Loss 4.4449(4.4581) | Error 0.3815(0.3841) Steps 826(825.49) | Grad Norm 1.4356(1.5475) | Total Time 14.00(14.00)\n",
      "Iter 2378 | Time 77.9782(77.6594) | Bit/dim 3.9144(3.9187) | Xent 1.0683(1.0782) | Loss 4.4485(4.4578) | Error 0.3834(0.3841) Steps 826(825.50) | Grad Norm 1.3364(1.5412) | Total Time 14.00(14.00)\n",
      "Iter 2379 | Time 78.5935(77.6874) | Bit/dim 3.9183(3.9187) | Xent 1.0858(1.0785) | Loss 4.4612(4.4579) | Error 0.3926(0.3843) Steps 814(825.16) | Grad Norm 1.6163(1.5435) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 79.0526(77.7284) | Bit/dim 3.9269(3.9189) | Xent 1.0740(1.0783) | Loss 4.4639(4.4581) | Error 0.3851(0.3844) Steps 826(825.18) | Grad Norm 1.5126(1.5425) | Total Time 14.00(14.00)\n",
      "Iter 2381 | Time 77.4251(77.7193) | Bit/dim 3.9237(3.9191) | Xent 1.0644(1.0779) | Loss 4.4559(4.4580) | Error 0.3765(0.3841) Steps 826(825.21) | Grad Norm 2.5265(1.5721) | Total Time 14.00(14.00)\n",
      "Iter 2382 | Time 77.7375(77.7198) | Bit/dim 3.9162(3.9190) | Xent 1.0746(1.0778) | Loss 4.4535(4.4579) | Error 0.3829(0.3841) Steps 826(825.23) | Grad Norm 0.9227(1.5526) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0397 | Time 29.4084, Epoch Time 511.4877(512.0283), Bit/dim 3.9185(best: 3.9178), Xent 1.0437, Loss 4.4404, Error 0.3698(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2383 | Time 77.0897(77.7009) | Bit/dim 3.9163(3.9189) | Xent 1.0834(1.0780) | Loss 4.4580(4.4579) | Error 0.3861(0.3841) Steps 826(825.25) | Grad Norm 1.9566(1.5647) | Total Time 14.00(14.00)\n",
      "Iter 2384 | Time 77.6754(77.7001) | Bit/dim 3.9137(3.9188) | Xent 1.0723(1.0778) | Loss 4.4498(4.4577) | Error 0.3759(0.3839) Steps 814(824.91) | Grad Norm 1.9795(1.5771) | Total Time 14.00(14.00)\n",
      "Iter 2385 | Time 77.8723(77.7053) | Bit/dim 3.9175(3.9187) | Xent 1.0633(1.0774) | Loss 4.4491(4.4574) | Error 0.3739(0.3836) Steps 826(824.95) | Grad Norm 0.8730(1.5560) | Total Time 14.00(14.00)\n",
      "Iter 2386 | Time 77.8647(77.7101) | Bit/dim 3.9118(3.9185) | Xent 1.0702(1.0772) | Loss 4.4469(4.4571) | Error 0.3775(0.3834) Steps 826(824.98) | Grad Norm 2.3260(1.5791) | Total Time 14.00(14.00)\n",
      "Iter 2387 | Time 77.6097(77.7071) | Bit/dim 3.9217(3.9186) | Xent 1.0659(1.0768) | Loss 4.4546(4.4570) | Error 0.3805(0.3833) Steps 826(825.01) | Grad Norm 1.3883(1.5734) | Total Time 14.00(14.00)\n",
      "Iter 2388 | Time 78.9969(77.7458) | Bit/dim 3.9221(3.9187) | Xent 1.0707(1.0766) | Loss 4.4574(4.4570) | Error 0.3761(0.3831) Steps 826(825.04) | Grad Norm 1.6103(1.5745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0398 | Time 29.5519, Epoch Time 512.2528(512.0351), Bit/dim 3.9186(best: 3.9178), Xent 1.0435, Loss 4.4404, Error 0.3695(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2389 | Time 78.3059(77.7626) | Bit/dim 3.9266(3.9189) | Xent 1.0846(1.0769) | Loss 4.4689(4.4574) | Error 0.3852(0.3832) Steps 832(825.25) | Grad Norm 2.5421(1.6035) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 75.5011(77.6947) | Bit/dim 3.9191(3.9190) | Xent 1.0427(1.0758) | Loss 4.4405(4.4569) | Error 0.3749(0.3829) Steps 826(825.27) | Grad Norm 1.1410(1.5896) | Total Time 14.00(14.00)\n",
      "Iter 2391 | Time 76.1375(77.6480) | Bit/dim 3.9121(3.9187) | Xent 1.0591(1.0753) | Loss 4.4416(4.4564) | Error 0.3745(0.3827) Steps 826(825.29) | Grad Norm 2.3102(1.6113) | Total Time 14.00(14.00)\n",
      "Iter 2392 | Time 76.3200(77.6082) | Bit/dim 3.9141(3.9186) | Xent 1.1027(1.0762) | Loss 4.4654(4.4567) | Error 0.3922(0.3830) Steps 820(825.13) | Grad Norm 2.6028(1.6410) | Total Time 14.00(14.00)\n",
      "Iter 2393 | Time 79.8102(77.6742) | Bit/dim 3.9257(3.9188) | Xent 1.0898(1.0766) | Loss 4.4705(4.4571) | Error 0.3841(0.3830) Steps 826(825.16) | Grad Norm 1.3281(1.6316) | Total Time 14.00(14.00)\n",
      "Iter 2394 | Time 81.2122(77.7804) | Bit/dim 3.9093(3.9185) | Xent 1.0580(1.0760) | Loss 4.4383(4.4565) | Error 0.3798(0.3829) Steps 814(824.83) | Grad Norm 2.2447(1.6500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0399 | Time 29.6239, Epoch Time 512.4374(512.0471), Bit/dim 3.9174(best: 3.9178), Xent 1.0397, Loss 4.4373, Error 0.3710(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2395 | Time 77.7350(77.7790) | Bit/dim 3.9155(3.9184) | Xent 1.0893(1.0764) | Loss 4.4602(4.4566) | Error 0.3891(0.3831) Steps 814(824.50) | Grad Norm 2.8641(1.6864) | Total Time 14.00(14.00)\n",
      "Iter 2396 | Time 77.5926(77.7734) | Bit/dim 3.9047(3.9180) | Xent 1.0699(1.0762) | Loss 4.4396(4.4561) | Error 0.3815(0.3830) Steps 832(824.73) | Grad Norm 1.3624(1.6767) | Total Time 14.00(14.00)\n",
      "Iter 2397 | Time 79.0212(77.8109) | Bit/dim 3.9169(3.9180) | Xent 1.0552(1.0756) | Loss 4.4445(4.4558) | Error 0.3771(0.3829) Steps 820(824.58) | Grad Norm 2.0537(1.6880) | Total Time 14.00(14.00)\n",
      "Iter 2398 | Time 76.9838(77.7860) | Bit/dim 3.9106(3.9178) | Xent 1.0640(1.0752) | Loss 4.4426(4.4554) | Error 0.3791(0.3827) Steps 826(824.63) | Grad Norm 1.4798(1.6818) | Total Time 14.00(14.00)\n",
      "Iter 2399 | Time 77.9286(77.7903) | Bit/dim 3.9273(3.9181) | Xent 1.0621(1.0748) | Loss 4.4583(4.4555) | Error 0.3794(0.3826) Steps 826(824.67) | Grad Norm 2.3707(1.7024) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 76.6544(77.7562) | Bit/dim 3.9248(3.9183) | Xent 1.0819(1.0751) | Loss 4.4657(4.4558) | Error 0.3792(0.3825) Steps 826(824.71) | Grad Norm 1.1129(1.6848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0400 | Time 29.3063, Epoch Time 510.7649(512.0087), Bit/dim 3.9174(best: 3.9174), Xent 1.0398, Loss 4.4373, Error 0.3713(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2401 | Time 78.0156(77.7640) | Bit/dim 3.9120(3.9181) | Xent 1.0622(1.0747) | Loss 4.4432(4.4554) | Error 0.3799(0.3825) Steps 808(824.21) | Grad Norm 1.5612(1.6811) | Total Time 14.00(14.00)\n",
      "Iter 2402 | Time 77.8039(77.7652) | Bit/dim 3.9266(3.9183) | Xent 1.0727(1.0746) | Loss 4.4629(4.4556) | Error 0.3845(0.3825) Steps 832(824.44) | Grad Norm 1.0414(1.6619) | Total Time 14.00(14.00)\n",
      "Iter 2403 | Time 78.7000(77.7933) | Bit/dim 3.9172(3.9183) | Xent 1.0759(1.0747) | Loss 4.4552(4.4556) | Error 0.3844(0.3826) Steps 826(824.49) | Grad Norm 1.4010(1.6540) | Total Time 14.00(14.00)\n",
      "Iter 2404 | Time 77.4273(77.7823) | Bit/dim 3.9151(3.9182) | Xent 1.0757(1.0747) | Loss 4.4529(4.4555) | Error 0.3885(0.3828) Steps 826(824.53) | Grad Norm 1.5326(1.6504) | Total Time 14.00(14.00)\n",
      "Iter 2405 | Time 77.8082(77.7831) | Bit/dim 3.9119(3.9180) | Xent 1.0537(1.0741) | Loss 4.4387(4.4550) | Error 0.3756(0.3825) Steps 826(824.58) | Grad Norm 1.3344(1.6409) | Total Time 14.00(14.00)\n",
      "Iter 2406 | Time 78.3615(77.8004) | Bit/dim 3.9112(3.9178) | Xent 1.0613(1.0737) | Loss 4.4419(4.4546) | Error 0.3729(0.3823) Steps 826(824.62) | Grad Norm 1.2278(1.6285) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0401 | Time 29.4434, Epoch Time 512.9626(512.0373), Bit/dim 3.9180(best: 3.9174), Xent 1.0393, Loss 4.4376, Error 0.3679(best: 0.3694)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2407 | Time 76.7557(77.7691) | Bit/dim 3.9145(3.9177) | Xent 1.0784(1.0738) | Loss 4.4537(4.4546) | Error 0.3846(0.3823) Steps 826(824.66) | Grad Norm 1.9211(1.6373) | Total Time 14.00(14.00)\n",
      "Iter 2408 | Time 79.1256(77.8098) | Bit/dim 3.9160(3.9177) | Xent 1.0680(1.0736) | Loss 4.4500(4.4545) | Error 0.3834(0.3824) Steps 826(824.70) | Grad Norm 1.3456(1.6285) | Total Time 14.00(14.00)\n",
      "Iter 2409 | Time 76.5139(77.7709) | Bit/dim 3.9110(3.9175) | Xent 1.0645(1.0734) | Loss 4.4433(4.4541) | Error 0.3841(0.3824) Steps 832(824.92) | Grad Norm 1.2524(1.6173) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 77.9564(77.7765) | Bit/dim 3.9153(3.9174) | Xent 1.0721(1.0733) | Loss 4.4514(4.4541) | Error 0.3812(0.3824) Steps 832(825.13) | Grad Norm 1.8942(1.6256) | Total Time 14.00(14.00)\n",
      "Iter 2411 | Time 78.2019(77.7892) | Bit/dim 3.9250(3.9176) | Xent 1.0564(1.0728) | Loss 4.4532(4.4540) | Error 0.3748(0.3821) Steps 832(825.34) | Grad Norm 1.6620(1.6267) | Total Time 14.00(14.00)\n",
      "Iter 2412 | Time 77.6604(77.7854) | Bit/dim 3.9122(3.9175) | Xent 1.0638(1.0725) | Loss 4.4440(4.4537) | Error 0.3702(0.3818) Steps 826(825.36) | Grad Norm 1.8321(1.6328) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0402 | Time 29.5390, Epoch Time 511.2333(512.0132), Bit/dim 3.9169(best: 3.9174), Xent 1.0390, Loss 4.4364, Error 0.3676(best: 0.3679)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2413 | Time 78.1312(77.7957) | Bit/dim 3.9191(3.9175) | Xent 1.0589(1.0721) | Loss 4.4485(4.4536) | Error 0.3741(0.3816) Steps 814(825.02) | Grad Norm 1.7693(1.6369) | Total Time 14.00(14.00)\n",
      "Iter 2414 | Time 77.7874(77.7955) | Bit/dim 3.9126(3.9174) | Xent 1.0824(1.0724) | Loss 4.4538(4.4536) | Error 0.3825(0.3816) Steps 826(825.05) | Grad Norm 1.6214(1.6365) | Total Time 14.00(14.00)\n",
      "Iter 2415 | Time 77.0080(77.7719) | Bit/dim 3.9029(3.9169) | Xent 1.0510(1.0718) | Loss 4.4284(4.4528) | Error 0.3709(0.3813) Steps 814(824.72) | Grad Norm 2.4566(1.6611) | Total Time 14.00(14.00)\n",
      "Iter 2416 | Time 80.0531(77.8403) | Bit/dim 3.9093(3.9167) | Xent 1.0784(1.0720) | Loss 4.4485(4.4527) | Error 0.3868(0.3814) Steps 808(824.21) | Grad Norm 1.5412(1.6575) | Total Time 14.00(14.00)\n",
      "Iter 2417 | Time 79.1892(77.8808) | Bit/dim 3.9275(3.9170) | Xent 1.0728(1.0720) | Loss 4.4639(4.4530) | Error 0.3798(0.3814) Steps 832(824.45) | Grad Norm 2.3111(1.6771) | Total Time 14.00(14.00)\n",
      "Iter 2418 | Time 78.2277(77.8912) | Bit/dim 3.9224(3.9172) | Xent 1.0688(1.0719) | Loss 4.4568(4.4531) | Error 0.3825(0.3814) Steps 826(824.49) | Grad Norm 2.0138(1.6872) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0403 | Time 29.4107, Epoch Time 515.2894(512.1114), Bit/dim 3.9175(best: 3.9169), Xent 1.0395, Loss 4.4372, Error 0.3683(best: 0.3676)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2419 | Time 78.9828(77.9239) | Bit/dim 3.9147(3.9171) | Xent 1.0765(1.0721) | Loss 4.4530(4.4531) | Error 0.3801(0.3814) Steps 832(824.72) | Grad Norm 2.4720(1.7107) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 78.3170(77.9357) | Bit/dim 3.9109(3.9169) | Xent 1.0799(1.0723) | Loss 4.4509(4.4531) | Error 0.3890(0.3816) Steps 826(824.76) | Grad Norm 1.5909(1.7071) | Total Time 14.00(14.00)\n",
      "Iter 2421 | Time 77.2826(77.9161) | Bit/dim 3.9259(3.9172) | Xent 1.0747(1.0724) | Loss 4.4633(4.4534) | Error 0.3799(0.3816) Steps 814(824.43) | Grad Norm 2.6001(1.7339) | Total Time 14.00(14.00)\n",
      "Iter 2422 | Time 78.4024(77.9307) | Bit/dim 3.9131(3.9171) | Xent 1.0706(1.0723) | Loss 4.4484(4.4532) | Error 0.3749(0.3814) Steps 814(824.12) | Grad Norm 2.1516(1.7464) | Total Time 14.00(14.00)\n",
      "Iter 2423 | Time 77.9469(77.9312) | Bit/dim 3.9107(3.9169) | Xent 1.0609(1.0720) | Loss 4.4412(4.4529) | Error 0.3776(0.3812) Steps 832(824.36) | Grad Norm 2.5064(1.7692) | Total Time 14.00(14.00)\n",
      "Iter 2424 | Time 79.6951(77.9841) | Bit/dim 3.9133(3.9168) | Xent 1.0589(1.0716) | Loss 4.4427(4.4526) | Error 0.3759(0.3811) Steps 826(824.41) | Grad Norm 2.1905(1.7819) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0404 | Time 29.5189, Epoch Time 515.5835(512.2156), Bit/dim 3.9170(best: 3.9169), Xent 1.0415, Loss 4.4378, Error 0.3692(best: 0.3676)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2425 | Time 76.6637(77.9445) | Bit/dim 3.9108(3.9166) | Xent 1.0718(1.0716) | Loss 4.4467(4.4524) | Error 0.3854(0.3812) Steps 826(824.46) | Grad Norm 2.0499(1.7899) | Total Time 14.00(14.00)\n",
      "Iter 2426 | Time 77.6318(77.9351) | Bit/dim 3.9173(3.9166) | Xent 1.0746(1.0717) | Loss 4.4546(4.4525) | Error 0.3748(0.3810) Steps 826(824.50) | Grad Norm 0.8942(1.7631) | Total Time 14.00(14.00)\n",
      "Iter 2427 | Time 77.0817(77.9095) | Bit/dim 3.9116(3.9165) | Xent 1.0648(1.0715) | Loss 4.4440(4.4522) | Error 0.3778(0.3809) Steps 826(824.55) | Grad Norm 3.3522(1.8107) | Total Time 14.00(14.00)\n",
      "Iter 2428 | Time 76.6953(77.8731) | Bit/dim 3.9177(3.9165) | Xent 1.0584(1.0711) | Loss 4.4469(4.4520) | Error 0.3676(0.3805) Steps 808(824.05) | Grad Norm 1.4246(1.7991) | Total Time 14.00(14.00)\n",
      "Iter 2429 | Time 78.3241(77.8866) | Bit/dim 3.9167(3.9165) | Xent 1.0521(1.0705) | Loss 4.4427(4.4518) | Error 0.3814(0.3805) Steps 820(823.93) | Grad Norm 2.1610(1.8100) | Total Time 14.00(14.00)\n",
      "Iter 2430 | Time 77.2633(77.8679) | Bit/dim 3.9191(3.9166) | Xent 1.0861(1.0710) | Loss 4.4622(4.4521) | Error 0.3825(0.3806) Steps 826(823.99) | Grad Norm 2.1337(1.8197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0405 | Time 29.4302, Epoch Time 508.5304(512.1051), Bit/dim 3.9162(best: 3.9169), Xent 1.0355, Loss 4.4339, Error 0.3708(best: 0.3676)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2431 | Time 76.4123(77.8243) | Bit/dim 3.9180(3.9166) | Xent 1.0569(1.0706) | Loss 4.4464(4.4519) | Error 0.3764(0.3805) Steps 826(824.05) | Grad Norm 1.3715(1.8063) | Total Time 14.00(14.00)\n",
      "Iter 2432 | Time 79.9054(77.8867) | Bit/dim 3.9147(3.9166) | Xent 1.0580(1.0702) | Loss 4.4437(4.4517) | Error 0.3781(0.3804) Steps 826(824.11) | Grad Norm 1.7593(1.8049) | Total Time 14.00(14.00)\n",
      "Iter 2433 | Time 76.6947(77.8509) | Bit/dim 3.9141(3.9165) | Xent 1.0721(1.0702) | Loss 4.4501(4.4516) | Error 0.3816(0.3804) Steps 826(824.17) | Grad Norm 1.9360(1.8088) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_baseline_run1/epoch_250_checkpt.pth --seed 1 --conditional True --controlled_tol False --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-5  --rtol 1e-5 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
