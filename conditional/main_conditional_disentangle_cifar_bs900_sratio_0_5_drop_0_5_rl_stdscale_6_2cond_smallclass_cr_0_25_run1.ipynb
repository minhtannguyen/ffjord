{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_2cond.py\n",
      "from __future__ import print_function\n",
      "\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"`MNIST <http://yann.lecun.com/exdb/mnist/>`_ Dataset.\n",
      "\n",
      "    Args:\n",
      "        root (string): Root directory of dataset where ``processed/training.pt``\n",
      "            and  ``processed/test.pt`` exist.\n",
      "        train (bool, optional): If True, creates dataset from ``training.pt``,\n",
      "            otherwise from ``test.pt``.\n",
      "        download (bool, optional): If true, downloads the dataset from the internet and\n",
      "            puts it in root directory. If dataset is already downloaded, it is not\n",
      "            downloaded again.\n",
      "        transform (callable, optional): A function/transform that  takes in an PIL image\n",
      "            and returns a transformed version. E.g, ``transforms.RandomCrop``\n",
      "        target_transform (callable, optional): A function/transform that takes in the\n",
      "            target and transforms it.\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index], self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index], self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, 10)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    if args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - 2 * args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.25, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_25_drop_0_5_rl_stdscale_6_2cond_small_classification_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.005)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1176, bias=True)\n",
      "  (project_class): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=588, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 951104\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 7.0602(22.2207) | Bit/dim 25.4501(27.3296) | Xent 2.2876(2.3009) | Xent Color 2.3003(2.3023) | Loss 46.5657(49.7544) | Error 0.8744(0.8883) | Error Color 0.9067(0.8900) |Steps 308(293.70) | Grad Norm 260.8154(274.9497) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 7.3876(18.2929) | Bit/dim 20.0892(26.0732) | Xent 2.2541(2.2933) | Xent Color 2.2916(2.3002) | Loss 37.1561(47.5457) | Error 0.8656(0.8873) | Error Color 0.8900(0.8912) |Steps 314(297.34) | Grad Norm 216.3574(264.7723) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 7.5100(15.4359) | Bit/dim 13.8624(23.5605) | Xent 2.2035(2.2753) | Xent Color 2.2717(2.2949) | Loss 25.6689(43.0626) | Error 0.5922(0.8397) | Error Color 0.8156(0.8796) |Steps 326(300.40) | Grad Norm 159.0940(243.5705) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 8.6137(13.5151) | Bit/dim 8.9813(20.2251) | Xent 2.1329(2.2453) | Xent Color 2.2448(2.2853) | Loss 17.0016(37.0795) | Error 0.3878(0.7413) | Error Color 0.8078(0.8610) |Steps 356(311.60) | Grad Norm 95.3527(211.8060) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 8.9328(12.2891) | Bit/dim 6.6886(16.8743) | Xent 2.0631(2.2039) | Xent Color 2.2403(2.2729) | Loss 12.8963(31.0888) | Error 0.3267(0.6288) | Error Color 0.8700(0.8520) |Steps 356(327.30) | Grad Norm 33.7714(171.5080) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 9.3732(11.5123) | Bit/dim 6.0946(14.1043) | Xent 2.0071(2.1564) | Xent Color 2.2436(2.2659) | Loss 11.9488(26.1490) | Error 0.3844(0.5536) | Error Color 0.9133(0.8630) |Steps 398(344.98) | Grad Norm 24.7106(132.4498) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 57.9705, Epoch Time 638.3523(638.3523), Bit/dim 5.7383(best: inf), Xent 1.9370, Xent Color 2.2167. Loss 5.7487, Error 0.2647(best: inf), Error Color 0.8966(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 9.2734(10.9176) | Bit/dim 4.3163(11.7710) | Xent 2.0177(2.1137) | Xent Color 2.2215(2.2575) | Loss 8.6407(22.4449) | Error 0.3356(0.4953) | Error Color 0.8400(0.8666) |Steps 362(352.84) | Grad Norm 16.3984(103.3720) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 8.8326(10.3938) | Bit/dim 3.9643(9.7670) | Xent 1.9573(2.0774) | Xent Color 2.2147(2.2477) | Loss 7.9531(18.7297) | Error 0.3356(0.4522) | Error Color 0.7967(0.8555) |Steps 386(360.71) | Grad Norm 12.2622(79.6430) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 9.3270(10.0405) | Bit/dim 3.7379(8.2214) | Xent 1.9178(2.0402) | Xent Color 2.1785(2.2352) | Loss 7.6937(15.8654) | Error 0.3133(0.4209) | Error Color 0.8144(0.8439) |Steps 410(364.36) | Grad Norm 9.7632(61.7034) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 8.9425(9.7536) | Bit/dim 3.5059(7.0141) | Xent 1.8787(2.0024) | Xent Color 2.1838(2.2205) | Loss 7.0551(13.6263) | Error 0.3311(0.3973) | Error Color 0.8311(0.8369) |Steps 350(367.33) | Grad Norm 7.6621(47.4836) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 8.8631(9.5831) | Bit/dim 3.2387(6.0545) | Xent 1.8550(1.9673) | Xent Color 2.1667(2.2046) | Loss 6.6558(11.8637) | Error 0.3144(0.3796) | Error Color 0.8378(0.8320) |Steps 362(372.01) | Grad Norm 6.6987(36.9024) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 9.2652(9.4558) | Bit/dim 2.9919(5.2802) | Xent 1.8620(1.9377) | Xent Color 2.1357(2.1876) | Loss 6.2612(10.4305) | Error 0.3789(0.3690) | Error Color 0.8189(0.8262) |Steps 374(373.75) | Grad Norm 6.7735(29.0190) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 9.0702(9.3487) | Bit/dim 2.7549(4.6449) | Xent 1.8542(1.9147) | Xent Color 2.1158(2.1710) | Loss 5.6687(9.2649) | Error 0.3444(0.3671) | Error Color 0.8067(0.8204) |Steps 356(376.48) | Grad Norm 5.6329(23.0191) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 54.7127, Epoch Time 676.5501(639.4982), Bit/dim 2.6855(best: 5.7383), Xent 1.8471, Xent Color 2.0984. Loss 2.6953, Error 0.3135(best: 0.2647), Error Color 0.7919(best: 0.8966)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 9.4262(9.3388) | Bit/dim 2.1523(4.0283) | Xent 1.9651(1.9256) | Xent Color 2.1675(2.1667) | Loss 4.7666(8.5694) | Error 0.4544(0.3885) | Error Color 0.8189(0.8193) |Steps 392(381.80) | Grad Norm 4.2116(18.3632) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 9.8839(9.3312) | Bit/dim 2.0455(3.5195) | Xent 2.0034(1.9429) | Xent Color 2.1539(2.1641) | Loss 4.5917(7.5356) | Error 0.4678(0.4092) | Error Color 0.8167(0.8193) |Steps 410(385.52) | Grad Norm 3.9005(14.5404) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 9.6984(9.4289) | Bit/dim 1.9452(3.1180) | Xent 2.0335(1.9643) | Xent Color 2.1649(2.1631) | Loss 4.4459(6.7362) | Error 0.4900(0.4325) | Error Color 0.8233(0.8223) |Steps 416(391.27) | Grad Norm 3.1344(11.6424) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 9.6040(9.4662) | Bit/dim 1.8811(2.8010) | Xent 2.0725(1.9884) | Xent Color 2.1406(2.1616) | Loss 4.2425(6.1043) | Error 0.5544(0.4601) | Error Color 0.8122(0.8234) |Steps 386(394.13) | Grad Norm 2.8717(9.3981) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 9.4649(9.4902) | Bit/dim 1.8059(2.5484) | Xent 2.0791(2.0135) | Xent Color 2.1829(2.1603) | Loss 4.1981(5.6065) | Error 0.5767(0.4867) | Error Color 0.8356(0.8240) |Steps 410(396.36) | Grad Norm 2.4986(7.5921) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 9.8610(9.5205) | Bit/dim 1.7651(2.3465) | Xent 2.1143(2.0374) | Xent Color 2.1732(2.1606) | Loss 4.0979(5.2056) | Error 0.5944(0.5131) | Error Color 0.8289(0.8260) |Steps 392(397.23) | Grad Norm 2.0625(6.1625) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 57.1118, Epoch Time 709.7832(641.6068), Bit/dim 1.7230(best: 2.6855), Xent 2.1058, Xent Color 2.1607. Loss 1.7337, Error 0.5139(best: 0.2647), Error Color 0.8239(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 9.4583(9.5752) | Bit/dim 1.4678(2.1702) | Xent 2.1525(2.0602) | Xent Color 2.2398(2.1663) | Loss 3.5695(5.3885) | Error 0.6344(0.5381) | Error Color 0.8511(0.8283) |Steps 422(400.56) | Grad Norm 2.1307(5.0222) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 9.7867(9.5953) | Bit/dim 1.4311(1.9802) | Xent 2.1594(2.0887) | Xent Color 2.2024(2.1807) | Loss 3.4712(4.8860) | Error 0.6489(0.5686) | Error Color 0.8456(0.8333) |Steps 410(400.45) | Grad Norm 1.2416(4.0895) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 9.5758(9.6264) | Bit/dim 1.4152(1.8349) | Xent 2.1423(2.1063) | Xent Color 2.2133(2.1912) | Loss 3.3269(4.5065) | Error 0.6256(0.5892) | Error Color 0.8556(0.8384) |Steps 386(402.31) | Grad Norm 1.4276(3.3206) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 10.0656(9.6193) | Bit/dim 1.3977(1.7228) | Xent 2.1348(2.1143) | Xent Color 2.2242(2.1992) | Loss 3.4335(4.2211) | Error 0.6289(0.5989) | Error Color 0.8544(0.8423) |Steps 422(403.72) | Grad Norm 0.8390(2.7365) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 9.7919(9.6342) | Bit/dim 1.4012(1.6366) | Xent 2.1288(2.1193) | Xent Color 2.2455(2.2057) | Loss 3.4419(4.0039) | Error 0.6367(0.6074) | Error Color 0.8633(0.8451) |Steps 410(403.63) | Grad Norm 0.8348(2.2456) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 9.5451(9.6237) | Bit/dim 1.3638(1.5692) | Xent 2.1332(2.1180) | Xent Color 2.2231(2.2109) | Loss 3.3394(3.8364) | Error 0.6222(0.6074) | Error Color 0.8333(0.8466) |Steps 410(403.59) | Grad Norm 0.5792(1.8895) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 9.3955(9.5945) | Bit/dim 1.3627(1.5151) | Xent 2.1056(2.1155) | Xent Color 2.2287(2.2148) | Loss 3.2726(3.7020) | Error 0.6033(0.6079) | Error Color 0.8556(0.8492) |Steps 398(402.03) | Grad Norm 0.6793(1.5705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 56.7510, Epoch Time 715.3313(643.8185), Bit/dim 1.3597(best: 1.7230), Xent 2.0779, Xent Color 2.2252. Loss 1.3705, Error 0.5025(best: 0.2647), Error Color 0.8505(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 9.0298(9.5223) | Bit/dim 1.1594(1.4394) | Xent 2.1418(2.1190) | Xent Color 2.2718(2.2242) | Loss 2.9068(3.9945) | Error 0.6589(0.6163) | Error Color 0.8878(0.8535) |Steps 380(401.10) | Grad Norm 0.5015(1.3310) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 9.3472(9.4808) | Bit/dim 1.1254(1.3605) | Xent 2.1297(2.1242) | Xent Color 2.2611(2.2335) | Loss 2.9615(3.7153) | Error 0.6344(0.6255) | Error Color 0.8867(0.8570) |Steps 392(400.17) | Grad Norm 0.8315(1.1581) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 9.3561(9.4235) | Bit/dim 1.1228(1.2995) | Xent 2.1271(2.1237) | Xent Color 2.2546(2.2395) | Loss 2.8885(3.4971) | Error 0.6311(0.6279) | Error Color 0.8767(0.8599) |Steps 386(397.33) | Grad Norm 0.4280(0.9963) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 9.0128(9.3266) | Bit/dim 1.1270(1.2540) | Xent 2.0808(2.1185) | Xent Color 2.2483(2.2435) | Loss 2.8526(3.3287) | Error 0.6111(0.6278) | Error Color 0.8589(0.8627) |Steps 386(393.59) | Grad Norm 1.2295(0.9413) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 9.1482(9.2892) | Bit/dim 1.1219(1.2197) | Xent 2.0724(2.1086) | Xent Color 2.2555(2.2471) | Loss 2.8529(3.2126) | Error 0.6078(0.6240) | Error Color 0.8478(0.8637) |Steps 392(392.90) | Grad Norm 1.1766(0.9458) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 9.1072(9.2528) | Bit/dim 1.1115(1.1939) | Xent 2.0663(2.0975) | Xent Color 2.2456(2.2484) | Loss 2.8485(3.1244) | Error 0.5978(0.6177) | Error Color 0.8478(0.8617) |Steps 404(392.10) | Grad Norm 0.7961(0.9042) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 9.2316(9.2354) | Bit/dim 1.1151(1.1731) | Xent 2.0307(2.0839) | Xent Color 2.2553(2.2519) | Loss 2.8042(3.0553) | Error 0.5856(0.6116) | Error Color 0.8711(0.8627) |Steps 368(391.56) | Grad Norm 0.3039(0.7727) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 54.9129, Epoch Time 685.5474(645.0704), Bit/dim 1.1068(best: 1.3597), Xent 2.0197, Xent Color 2.2514. Loss 1.1175, Error 0.5083(best: 0.2647), Error Color 0.8544(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 9.5891(9.2344) | Bit/dim 0.9288(1.1106) | Xent 2.1408(2.1012) | Xent Color 2.2801(2.2594) | Loss 2.6087(3.3430) | Error 0.6722(0.6335) | Error Color 0.8589(0.8656) |Steps 398(392.18) | Grad Norm 0.6597(0.7333) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 9.3525(9.2115) | Bit/dim 0.9229(1.0626) | Xent 2.1209(2.1083) | Xent Color 2.2907(2.2656) | Loss 2.5656(3.1337) | Error 0.6778(0.6472) | Error Color 0.8756(0.8707) |Steps 398(392.08) | Grad Norm 0.8309(0.6981) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 8.8633(9.1749) | Bit/dim 0.9117(1.0267) | Xent 2.1173(2.1095) | Xent Color 2.2853(2.2687) | Loss 2.4381(2.9746) | Error 0.6911(0.6541) | Error Color 0.8800(0.8715) |Steps 368(391.58) | Grad Norm 0.3056(0.6394) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 8.8921(9.1511) | Bit/dim 0.9229(0.9992) | Xent 2.0858(2.1030) | Xent Color 2.2761(2.2711) | Loss 2.5309(2.8576) | Error 0.6611(0.6568) | Error Color 0.8811(0.8738) |Steps 386(393.05) | Grad Norm 0.4094(0.5901) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 8.9300(9.1160) | Bit/dim 0.9181(0.9787) | Xent 2.0470(2.0926) | Xent Color 2.2748(2.2728) | Loss 2.4496(2.7662) | Error 0.6256(0.6541) | Error Color 0.8622(0.8738) |Steps 374(391.93) | Grad Norm 0.7380(0.5745) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 9.3130(9.1296) | Bit/dim 0.9331(0.9629) | Xent 2.0173(2.0781) | Xent Color 2.2787(2.2742) | Loss 2.5444(2.6996) | Error 0.6322(0.6497) | Error Color 0.8856(0.8749) |Steps 410(391.31) | Grad Norm 0.6880(0.5735) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 55.6412, Epoch Time 681.6387(646.1674), Bit/dim 0.9082(best: 1.1068), Xent 2.0029, Xent Color 2.2769. Loss 0.9189, Error 0.5618(best: 0.2647), Error Color 0.8769(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 9.0563(9.1220) | Bit/dim 0.7822(0.9359) | Xent 2.1914(2.0826) | Xent Color 2.2982(2.2772) | Loss 2.2767(3.0950) | Error 0.7567(0.6613) | Error Color 0.9056(0.8783) |Steps 410(392.25) | Grad Norm 2.3521(0.7591) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 9.1672(9.1571) | Bit/dim 0.7890(0.8955) | Xent 2.1725(2.1083) | Xent Color 2.3033(2.2820) | Loss 2.3232(2.8832) | Error 0.7622(0.6880) | Error Color 0.9000(0.8829) |Steps 410(392.74) | Grad Norm 0.5724(0.8942) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 9.2997(9.1890) | Bit/dim 0.7824(0.8653) | Xent 2.1388(2.1195) | Xent Color 2.2962(2.2849) | Loss 2.2220(2.7234) | Error 0.7278(0.7024) | Error Color 0.8944(0.8855) |Steps 404(392.58) | Grad Norm 0.8358(0.9047) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 9.2420(9.2154) | Bit/dim 0.7791(0.8416) | Xent 2.1382(2.1232) | Xent Color 2.2898(2.2868) | Loss 2.3477(2.6074) | Error 0.7489(0.7097) | Error Color 0.8844(0.8858) |Steps 404(393.98) | Grad Norm 0.8265(0.8020) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 9.5471(9.2388) | Bit/dim 0.7707(0.8233) | Xent 2.1231(2.1269) | Xent Color 2.2997(2.2892) | Loss 2.2447(2.5187) | Error 0.7200(0.7161) | Error Color 0.8833(0.8872) |Steps 416(395.73) | Grad Norm 1.9602(0.8307) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 9.3874(9.2724) | Bit/dim 0.7756(0.8093) | Xent 2.1063(2.1226) | Xent Color 2.2878(2.2915) | Loss 2.2679(2.4571) | Error 0.7111(0.7180) | Error Color 0.8900(0.8885) |Steps 404(396.83) | Grad Norm 0.8935(0.9929) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 9.4824(9.3042) | Bit/dim 0.7686(0.7987) | Xent 2.0953(2.1173) | Xent Color 2.2978(2.2923) | Loss 2.2924(2.4099) | Error 0.7144(0.7174) | Error Color 0.9011(0.8890) |Steps 392(397.64) | Grad Norm 2.2234(1.2095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 56.7272, Epoch Time 693.0160(647.5729), Bit/dim 0.7633(best: 0.9082), Xent 2.0747, Xent Color 2.2925. Loss 0.7742, Error 0.6749(best: 0.2647), Error Color 0.8911(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 9.1997(9.3132) | Bit/dim 0.6862(0.7738) | Xent 2.2613(2.1470) | Xent Color 2.2981(2.2943) | Loss 2.1540(2.7822) | Error 0.8233(0.7408) | Error Color 0.8911(0.8896) |Steps 398(397.85) | Grad Norm 3.3331(1.7068) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 9.1442(9.3086) | Bit/dim 0.6827(0.7503) | Xent 2.2456(2.1768) | Xent Color 2.3055(2.2970) | Loss 2.1215(2.6006) | Error 0.8089(0.7637) | Error Color 0.9022(0.8921) |Steps 404(398.42) | Grad Norm 3.3755(2.1486) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 9.0690(9.3155) | Bit/dim 0.6841(0.7343) | Xent 2.2471(2.1985) | Xent Color 2.2957(2.2996) | Loss 2.0030(2.4765) | Error 0.8278(0.7815) | Error Color 0.8900(0.8930) |Steps 392(399.54) | Grad Norm 5.8296(4.6529) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 8.8957(9.2973) | Bit/dim 0.6834(0.7210) | Xent 2.2177(2.2103) | Xent Color 2.3141(2.3015) | Loss 2.1146(2.3814) | Error 0.7956(0.7899) | Error Color 0.9133(0.8947) |Steps 374(398.25) | Grad Norm 5.0765(5.1368) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 9.3611(9.3114) | Bit/dim 0.6750(0.7101) | Xent 2.2368(2.2195) | Xent Color 2.3098(2.3024) | Loss 2.1044(2.3032) | Error 0.8122(0.7973) | Error Color 0.8978(0.8941) |Steps 392(395.46) | Grad Norm 3.7238(4.7559) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 8.9325(9.2604) | Bit/dim 0.6781(0.7019) | Xent 2.2395(2.2257) | Xent Color 2.3114(2.3027) | Loss 2.0101(2.2499) | Error 0.8167(0.8025) | Error Color 0.9011(0.8949) |Steps 380(396.58) | Grad Norm 3.2904(4.2912) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 56.6781, Epoch Time 692.9133(648.9331), Bit/dim 0.6710(best: 0.7633), Xent 2.2157, Xent Color 2.2996. Loss 0.6823, Error 0.7974(best: 0.2647), Error Color 0.8917(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 9.4988(9.3074) | Bit/dim 0.6364(0.6928) | Xent 2.3141(2.2324) | Xent Color 2.3074(2.3028) | Loss 1.9206(2.7285) | Error 0.8589(0.8073) | Error Color 0.8878(0.8940) |Steps 386(396.68) | Grad Norm 2.4910(3.8849) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 9.7173(9.3369) | Bit/dim 0.6307(0.6770) | Xent 2.3299(2.2578) | Xent Color 2.3046(2.3042) | Loss 2.0598(2.5386) | Error 0.8700(0.8245) | Error Color 0.8978(0.8944) |Steps 380(395.25) | Grad Norm 3.0881(3.4149) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 9.3111(9.2831) | Bit/dim 0.6755(0.6788) | Xent 2.3559(2.2759) | Xent Color 2.3253(2.3093) | Loss 2.0735(2.4212) | Error 0.9011(0.8374) | Error Color 0.9000(0.8961) |Steps 386(392.59) | Grad Norm 21.3296(9.6956) | Total Time 0.00(0.00)\n",
      "Iter 0560 | Time 9.4950(9.3437) | Bit/dim 0.6431(0.6721) | Xent 2.3371(2.2941) | Xent Color 2.3078(2.3097) | Loss 2.0483(2.3285) | Error 0.8822(0.8505) | Error Color 0.8822(0.8959) |Steps 386(395.37) | Grad Norm 6.6239(10.2510) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 9.6834(9.3889) | Bit/dim 0.6337(0.6642) | Xent 2.3368(2.3041) | Xent Color 2.3010(2.3087) | Loss 2.0902(2.2550) | Error 0.8711(0.8580) | Error Color 0.8856(0.8948) |Steps 416(395.71) | Grad Norm 3.3120(8.9952) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 9.4599(9.4149) | Bit/dim 0.6339(0.6570) | Xent 2.3294(2.3102) | Xent Color 2.3042(2.3078) | Loss 1.9965(2.1922) | Error 0.8778(0.8634) | Error Color 0.8900(0.8947) |Steps 386(397.16) | Grad Norm 3.2538(7.5732) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 9.1741(9.3807) | Bit/dim 0.6331(0.6506) | Xent 2.3096(2.3141) | Xent Color 2.2995(2.3072) | Loss 2.0790(2.1514) | Error 0.8667(0.8670) | Error Color 0.8922(0.8951) |Steps 410(398.21) | Grad Norm 1.4752(6.1418) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 56.8654, Epoch Time 700.6770(650.4854), Bit/dim 0.6270(best: 0.6710), Xent 2.2988, Xent Color 2.3022. Loss 0.6386, Error 0.8701(best: 0.2647), Error Color 0.8966(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 9.1786(9.3787) | Bit/dim 0.6059(0.6417) | Xent 2.3239(2.3171) | Xent Color 2.3078(2.3065) | Loss 2.0263(2.6093) | Error 0.8589(0.8710) | Error Color 0.8833(0.8947) |Steps 386(400.55) | Grad Norm 0.8027(4.8738) | Total Time 0.00(0.00)\n",
      "Iter 0610 | Time 9.4083(9.4091) | Bit/dim 0.6069(0.6327) | Xent 2.3427(2.3224) | Xent Color 2.3040(2.3067) | Loss 1.9770(2.4523) | Error 0.9111(0.8755) | Error Color 0.9044(0.8952) |Steps 368(397.85) | Grad Norm 0.5251(3.8224) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 9.5420(9.4310) | Bit/dim 0.6055(0.6255) | Xent 2.3275(2.3257) | Xent Color 2.3118(2.3073) | Loss 2.0271(2.3408) | Error 0.8900(0.8791) | Error Color 0.9111(0.8972) |Steps 380(398.76) | Grad Norm 0.5396(2.9821) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 9.6975(9.4233) | Bit/dim 0.6038(0.6201) | Xent 2.3321(2.3269) | Xent Color 2.3082(2.3075) | Loss 2.0588(2.2532) | Error 0.8844(0.8808) | Error Color 0.9044(0.8969) |Steps 392(398.64) | Grad Norm 1.2161(2.4052) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 9.5745(9.4214) | Bit/dim 0.6046(0.6163) | Xent 2.3259(2.3267) | Xent Color 2.3119(2.3079) | Loss 2.0164(2.1855) | Error 0.8856(0.8834) | Error Color 0.9067(0.8979) |Steps 422(396.87) | Grad Norm 1.7629(2.1676) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 9.2573(9.4101) | Bit/dim 0.6048(0.6125) | Xent 2.3171(2.3267) | Xent Color 2.3138(2.3079) | Loss 1.9917(2.1358) | Error 0.8789(0.8841) | Error Color 0.9089(0.8978) |Steps 410(395.59) | Grad Norm 0.3082(1.9213) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 9.6584(9.3894) | Bit/dim 0.6484(0.6201) | Xent 2.3402(2.3281) | Xent Color 2.3099(2.3081) | Loss 2.1135(2.1188) | Error 0.9033(0.8852) | Error Color 0.9100(0.8988) |Steps 434(398.97) | Grad Norm 24.2553(6.6562) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 55.5628, Epoch Time 699.5441(651.9572), Bit/dim 0.6878(best: 0.6270), Xent 2.3089, Xent Color 2.3089. Loss 0.6993, Error 0.8853(best: 0.2647), Error Color 0.9027(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 9.5919(9.4525) | Bit/dim 0.6257(0.6278) | Xent 2.3283(2.3299) | Xent Color 2.3057(2.3086) | Loss 1.9779(2.5193) | Error 0.9000(0.8877) | Error Color 0.8811(0.8978) |Steps 398(401.61) | Grad Norm 7.0886(9.7253) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 9.2130(9.4871) | Bit/dim 0.6168(0.6257) | Xent 2.3324(2.3311) | Xent Color 2.3124(2.3086) | Loss 1.9662(2.3936) | Error 0.9011(0.8912) | Error Color 0.9056(0.8973) |Steps 374(403.91) | Grad Norm 6.1804(9.4382) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 9.4684(9.4466) | Bit/dim 0.6073(0.6211) | Xent 2.3354(2.3311) | Xent Color 2.3153(2.3089) | Loss 2.0224(2.2931) | Error 0.9022(0.8931) | Error Color 0.9033(0.8982) |Steps 422(405.62) | Grad Norm 3.7264(8.0134) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 9.6223(9.4242) | Bit/dim 0.5971(0.6156) | Xent 2.3286(2.3301) | Xent Color 2.3156(2.3091) | Loss 2.0849(2.2134) | Error 0.8978(0.8955) | Error Color 0.9078(0.8983) |Steps 386(401.18) | Grad Norm 1.6940(6.5547) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 9.2554(9.4358) | Bit/dim 0.5937(0.6103) | Xent 2.3145(2.3291) | Xent Color 2.3100(2.3089) | Loss 1.9756(2.1631) | Error 0.8956(0.8955) | Error Color 0.9056(0.8991) |Steps 422(403.16) | Grad Norm 1.4861(5.2692) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 9.2546(9.4186) | Bit/dim 0.5916(0.6054) | Xent 2.3255(2.3277) | Xent Color 2.3045(2.3088) | Loss 1.9647(2.1207) | Error 0.8900(0.8944) | Error Color 0.8900(0.8998) |Steps 392(400.19) | Grad Norm 0.8020(4.2287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 57.7700, Epoch Time 703.4713(653.5026), Bit/dim 0.5900(best: 0.6270), Xent 2.3132, Xent Color 2.3054. Loss 0.6016, Error 0.8923(best: 0.2647), Error Color 0.9018(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 9.6101(9.4055) | Bit/dim 0.5837(0.6011) | Xent 2.3331(2.3266) | Xent Color 2.3067(2.3084) | Loss 2.0311(2.6141) | Error 0.9078(0.8936) | Error Color 0.9000(0.8992) |Steps 392(400.00) | Grad Norm 0.5325(3.3439) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 9.4135(9.3958) | Bit/dim 0.5835(0.5967) | Xent 2.3167(2.3258) | Xent Color 2.3137(2.3089) | Loss 2.0531(2.4470) | Error 0.8989(0.8946) | Error Color 0.8956(0.8993) |Steps 410(399.95) | Grad Norm 0.3329(2.6157) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 9.4771(9.4090) | Bit/dim 0.5825(0.5931) | Xent 2.3331(2.3250) | Xent Color 2.3043(2.3091) | Loss 1.9884(2.3268) | Error 0.8956(0.8940) | Error Color 0.8933(0.9000) |Steps 386(399.79) | Grad Norm 1.4417(2.1546) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 10.0546(9.4231) | Bit/dim 0.5807(0.5900) | Xent 2.3155(2.3249) | Xent Color 2.3121(2.3091) | Loss 2.0507(2.2334) | Error 0.9033(0.8948) | Error Color 0.9033(0.8993) |Steps 428(400.02) | Grad Norm 1.2522(1.8165) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 9.4238(9.4862) | Bit/dim 0.5806(0.5874) | Xent 2.3222(2.3238) | Xent Color 2.3106(2.3090) | Loss 2.0529(2.1748) | Error 0.8989(0.8958) | Error Color 0.9167(0.9001) |Steps 422(401.01) | Grad Norm 1.3866(1.6225) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 9.3808(9.4615) | Bit/dim 0.5790(0.5860) | Xent 2.3215(2.3219) | Xent Color 2.3066(2.3088) | Loss 1.9949(2.1215) | Error 0.8867(0.8945) | Error Color 0.8911(0.8997) |Steps 416(401.94) | Grad Norm 1.4033(2.7466) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 9.4171(9.4553) | Bit/dim 0.5782(0.5839) | Xent 2.3093(2.3215) | Xent Color 2.3029(2.3084) | Loss 1.9496(2.0761) | Error 0.8867(0.8948) | Error Color 0.8811(0.8982) |Steps 398(400.39) | Grad Norm 3.0902(3.0086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 57.3303, Epoch Time 705.4330(655.0605), Bit/dim 0.5767(best: 0.5900), Xent 2.3087, Xent Color 2.3054. Loss 0.5882, Error 0.8914(best: 0.2647), Error Color 0.8979(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 8.7663(9.4446) | Bit/dim 0.5710(0.5814) | Xent 2.3274(2.3206) | Xent Color 2.3025(2.3087) | Loss 1.8517(2.5036) | Error 0.8978(0.8941) | Error Color 0.8878(0.8980) |Steps 386(399.30) | Grad Norm 1.1747(2.9491) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 9.5245(9.4189) | Bit/dim 0.5749(0.5795) | Xent 2.3120(2.3199) | Xent Color 2.3186(2.3096) | Loss 1.9337(2.3536) | Error 0.8933(0.8947) | Error Color 0.9144(0.8992) |Steps 398(399.35) | Grad Norm 6.2130(3.6578) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 9.0391(9.3724) | Bit/dim 0.5763(0.5773) | Xent 2.3230(2.3194) | Xent Color 2.3048(2.3095) | Loss 1.9450(2.2440) | Error 0.8944(0.8933) | Error Color 0.8811(0.8989) |Steps 410(396.61) | Grad Norm 9.5194(4.0112) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 8.9941(9.3266) | Bit/dim 0.5687(0.5749) | Xent 2.3054(2.3182) | Xent Color 2.3089(2.3091) | Loss 1.8846(2.1600) | Error 0.9067(0.8938) | Error Color 0.9044(0.8985) |Steps 374(395.01) | Grad Norm 7.1246(4.2703) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 8.9455(9.2955) | Bit/dim 0.5615(0.5718) | Xent 2.3133(2.3177) | Xent Color 2.3099(2.3093) | Loss 1.8728(2.0946) | Error 0.9144(0.8957) | Error Color 0.9056(0.8999) |Steps 380(393.27) | Grad Norm 2.1990(4.0192) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 9.3683(9.2774) | Bit/dim 0.5871(0.5707) | Xent 2.3203(2.3171) | Xent Color 2.3153(2.3093) | Loss 1.9568(2.0472) | Error 0.9022(0.8954) | Error Color 0.9044(0.8994) |Steps 386(393.72) | Grad Norm 21.6118(5.0208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 55.8092, Epoch Time 692.5269(656.1845), Bit/dim 0.8989(best: 0.5767), Xent 2.3077, Xent Color 2.3062. Loss 0.9104, Error 0.8912(best: 0.2647), Error Color 0.8985(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 9.9013(9.3572) | Bit/dim 0.6419(0.6165) | Xent 2.3137(2.3173) | Xent Color 2.3149(2.3101) | Loss 2.0436(2.6174) | Error 0.8856(0.8956) | Error Color 0.9144(0.9001) |Steps 422(398.02) | Grad Norm 16.1483(13.5696) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 9.2151(9.3805) | Bit/dim 0.5858(0.6283) | Xent 2.3173(2.3170) | Xent Color 2.3080(2.3100) | Loss 1.9440(2.4901) | Error 0.9056(0.8968) | Error Color 0.8967(0.8983) |Steps 392(397.61) | Grad Norm 3.3198(14.2428) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 8.9265(9.3442) | Bit/dim 0.5736(0.6180) | Xent 2.3177(2.3170) | Xent Color 2.3068(2.3096) | Loss 1.9202(2.3600) | Error 0.8811(0.8978) | Error Color 0.9089(0.8986) |Steps 404(396.35) | Grad Norm 6.5428(12.6466) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 9.3056(9.2729) | Bit/dim 0.5379(0.6003) | Xent 2.3132(2.3165) | Xent Color 2.3058(2.3092) | Loss 1.8829(2.2445) | Error 0.9033(0.8991) | Error Color 0.9033(0.9006) |Steps 398(395.63) | Grad Norm 4.7976(10.4515) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 8.6211(9.2336) | Bit/dim 0.5013(0.5784) | Xent 2.3154(2.3154) | Xent Color 2.3105(2.3093) | Loss 1.8851(2.1437) | Error 0.8989(0.8988) | Error Color 0.9044(0.9011) |Steps 404(395.87) | Grad Norm 1.1040(8.4069) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 9.1366(9.2197) | Bit/dim 0.4718(0.5541) | Xent 2.3189(2.3152) | Xent Color 2.3073(2.3084) | Loss 1.8145(2.0652) | Error 0.8911(0.8987) | Error Color 0.9033(0.9003) |Steps 374(395.28) | Grad Norm 1.8758(6.7085) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 9.1003(9.2571) | Bit/dim 0.4571(0.5304) | Xent 2.3157(2.3137) | Xent Color 2.3045(2.3085) | Loss 1.7075(1.9979) | Error 0.8889(0.8966) | Error Color 0.8989(0.8999) |Steps 386(397.23) | Grad Norm 5.4981(6.0026) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 59.7719, Epoch Time 694.1675(657.3240), Bit/dim 0.4578(best: 0.5767), Xent 2.3103, Xent Color 2.3057. Loss 0.4693, Error 0.9008(best: 0.2647), Error Color 0.9022(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 9.0243(9.2019) | Bit/dim 0.4546(0.5106) | Xent 2.3146(2.3143) | Xent Color 2.3120(2.3083) | Loss 1.7127(2.4512) | Error 0.9022(0.8978) | Error Color 0.9111(0.8998) |Steps 380(397.89) | Grad Norm 10.2515(6.5370) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 9.5294(9.2326) | Bit/dim 0.4666(0.4940) | Xent 2.3093(2.3140) | Xent Color 2.3040(2.3087) | Loss 1.8373(2.2756) | Error 0.8789(0.8961) | Error Color 0.8900(0.9010) |Steps 428(401.40) | Grad Norm 19.0192(6.9595) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 9.2311(9.2126) | Bit/dim 0.4379(0.4814) | Xent 2.3149(2.3142) | Xent Color 2.3075(2.3088) | Loss 1.7628(2.1391) | Error 0.8811(0.8966) | Error Color 0.9133(0.9021) |Steps 380(397.81) | Grad Norm 6.2172(7.7670) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 9.2506(9.2368) | Bit/dim 0.4281(0.4683) | Xent 2.3132(2.3147) | Xent Color 2.3051(2.3091) | Loss 1.6825(2.0296) | Error 0.8922(0.8965) | Error Color 0.9133(0.9026) |Steps 374(395.23) | Grad Norm 2.2413(6.8809) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 9.3082(9.2718) | Bit/dim 0.4313(0.4594) | Xent 2.3220(2.3152) | Xent Color 2.3076(2.3090) | Loss 1.6868(1.9496) | Error 0.9144(0.8975) | Error Color 0.8989(0.9016) |Steps 380(391.57) | Grad Norm 9.1189(7.6280) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 9.5617(9.2802) | Bit/dim 0.4285(0.4518) | Xent 2.3186(2.3151) | Xent Color 2.3069(2.3084) | Loss 1.8037(1.8935) | Error 0.9133(0.8973) | Error Color 0.8867(0.8999) |Steps 398(395.76) | Grad Norm 12.1097(8.2962) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 9.4195(9.2894) | Bit/dim 0.4154(0.4440) | Xent 2.3021(2.3140) | Xent Color 2.3078(2.3083) | Loss 1.6682(1.8410) | Error 0.8989(0.8973) | Error Color 0.9122(0.9003) |Steps 362(394.16) | Grad Norm 8.7437(8.2916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 57.9298, Epoch Time 692.5007(658.3793), Bit/dim 0.4132(best: 0.4578), Xent 2.3077, Xent Color 2.3038. Loss 0.4248, Error 0.8940(best: 0.2647), Error Color 0.8990(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 8.6396(9.2724) | Bit/dim 0.4033(0.4353) | Xent 2.3148(2.3137) | Xent Color 2.3104(2.3084) | Loss 1.6529(2.2350) | Error 0.8911(0.8985) | Error Color 0.9033(0.9006) |Steps 392(395.88) | Grad Norm 3.3427(7.7635) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 8.3508(9.2090) | Bit/dim 0.5620(0.4476) | Xent 2.3151(2.3147) | Xent Color 2.3033(2.3080) | Loss 1.7329(2.1040) | Error 0.8978(0.8993) | Error Color 0.8978(0.8996) |Steps 356(391.09) | Grad Norm 15.7998(11.5151) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 9.0141(9.1389) | Bit/dim 0.4290(0.4635) | Xent 2.3196(2.3149) | Xent Color 2.3096(2.3081) | Loss 1.6344(2.0049) | Error 0.9144(0.9004) | Error Color 0.8878(0.8990) |Steps 404(387.74) | Grad Norm 5.6938(11.1586) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 9.2524(9.1243) | Bit/dim 0.4034(0.4508) | Xent 2.3037(2.3142) | Xent Color 2.3092(2.3082) | Loss 1.6457(1.9062) | Error 0.8867(0.8993) | Error Color 0.9111(0.9001) |Steps 350(387.56) | Grad Norm 3.2875(9.2853) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 9.1318(9.1145) | Bit/dim 0.3673(0.4323) | Xent 2.3029(2.3135) | Xent Color 2.3070(2.3081) | Loss 1.5996(1.8265) | Error 0.8967(0.8986) | Error Color 0.8944(0.9006) |Steps 398(390.62) | Grad Norm 3.0115(7.5865) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 9.0073(9.1215) | Bit/dim 0.3364(0.4098) | Xent 2.3108(2.3131) | Xent Color 2.3142(2.3087) | Loss 1.5679(1.7520) | Error 0.9078(0.8986) | Error Color 0.9189(0.9006) |Steps 410(392.63) | Grad Norm 6.7724(6.4868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 59.6863, Epoch Time 683.6712(659.1381), Bit/dim 0.6789(best: 0.4132), Xent 2.3098, Xent Color 2.3066. Loss 0.6904, Error 0.9003(best: 0.2647), Error Color 0.8980(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 9.1988(9.2049) | Bit/dim 0.4139(0.4330) | Xent 2.3069(2.3133) | Xent Color 2.3082(2.3085) | Loss 1.6749(2.2957) | Error 0.8844(0.8984) | Error Color 0.9067(0.8994) |Steps 416(394.72) | Grad Norm 10.3704(12.6551) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 9.7354(9.4051) | Bit/dim 0.3829(0.4302) | Xent 2.3098(2.3125) | Xent Color 2.3056(2.3086) | Loss 1.7800(2.1664) | Error 0.8900(0.8962) | Error Color 0.9044(0.8993) |Steps 458(409.25) | Grad Norm 6.9666(12.8514) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 9.1320(9.5085) | Bit/dim 0.3529(0.4127) | Xent 2.3138(2.3121) | Xent Color 2.3034(2.3086) | Loss 1.6151(2.0353) | Error 0.8833(0.8955) | Error Color 0.9000(0.8996) |Steps 410(415.79) | Grad Norm 5.5763(11.1222) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 9.0230(9.4652) | Bit/dim 0.3165(0.3903) | Xent 2.3041(2.3120) | Xent Color 2.3079(2.3085) | Loss 1.4659(1.9075) | Error 0.8911(0.8961) | Error Color 0.9067(0.8998) |Steps 404(413.94) | Grad Norm 5.6573(9.2691) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 9.5493(9.4397) | Bit/dim 0.2972(0.3679) | Xent 2.3090(2.3117) | Xent Color 2.3141(2.3085) | Loss 1.5263(1.7976) | Error 0.9056(0.8964) | Error Color 0.9100(0.8996) |Steps 398(409.72) | Grad Norm 3.1452(7.5591) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 9.5332(9.4358) | Bit/dim 0.2778(0.3463) | Xent 2.3136(2.3108) | Xent Color 2.3065(2.3085) | Loss 1.4912(1.7177) | Error 0.9056(0.8963) | Error Color 0.8933(0.8999) |Steps 404(408.02) | Grad Norm 1.3888(6.1106) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 9.3485(9.4302) | Bit/dim 0.2597(0.3253) | Xent 2.3124(2.3110) | Xent Color 2.3044(2.3080) | Loss 1.3689(1.6512) | Error 0.9111(0.8962) | Error Color 0.8956(0.8997) |Steps 392(412.91) | Grad Norm 1.2304(4.8959) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 62.5911, Epoch Time 716.3498(660.8544), Bit/dim 0.2530(best: 0.4132), Xent 2.3065, Xent Color 2.3048. Loss 0.2645, Error 0.8923(best: 0.2647), Error Color 0.8982(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 8.8687(9.4422) | Bit/dim 0.2846(0.3072) | Xent 2.3232(2.3112) | Xent Color 2.3118(2.3084) | Loss 1.4693(2.0750) | Error 0.9011(0.8959) | Error Color 0.9022(0.9005) |Steps 386(410.96) | Grad Norm 23.2792(5.3784) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 9.4926(9.4415) | Bit/dim 0.3300(0.3214) | Xent 2.3237(2.3115) | Xent Color 2.3057(2.3084) | Loss 1.5271(1.9582) | Error 0.9056(0.8958) | Error Color 0.9022(0.9003) |Steps 362(410.11) | Grad Norm 24.0937(10.3716) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 9.5886(9.4611) | Bit/dim 0.2715(0.3165) | Xent 2.3158(2.3113) | Xent Color 2.3110(2.3080) | Loss 1.4184(1.8369) | Error 0.9000(0.8961) | Error Color 0.9056(0.8999) |Steps 416(409.60) | Grad Norm 8.1230(10.6265) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 9.2363(9.4154) | Bit/dim 0.2458(0.3003) | Xent 2.3103(2.3112) | Xent Color 2.3078(2.3082) | Loss 1.3902(1.7207) | Error 0.8922(0.8964) | Error Color 0.9122(0.9013) |Steps 428(409.51) | Grad Norm 7.2590(9.7228) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 9.3057(9.3667) | Bit/dim 0.2159(0.2808) | Xent 2.3062(2.3114) | Xent Color 2.3078(2.3087) | Loss 1.3243(1.6228) | Error 0.8867(0.8965) | Error Color 0.8956(0.9022) |Steps 440(410.39) | Grad Norm 4.4903(8.2661) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 9.2221(9.3566) | Bit/dim 0.2013(0.2616) | Xent 2.3128(2.3111) | Xent Color 2.3039(2.3082) | Loss 1.3307(1.5502) | Error 0.9211(0.8983) | Error Color 0.8956(0.9019) |Steps 392(407.75) | Grad Norm 3.2381(6.9577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 61.0603, Epoch Time 702.9951(662.1186), Bit/dim 0.1867(best: 0.2530), Xent 2.3070, Xent Color 2.3046. Loss 0.1982, Error 0.8967(best: 0.2647), Error Color 0.9045(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 9.8535(9.3495) | Bit/dim 0.1854(0.2433) | Xent 2.3102(2.3114) | Xent Color 2.3077(2.3076) | Loss 1.3730(2.0469) | Error 0.8922(0.8986) | Error Color 0.8956(0.9008) |Steps 452(406.34) | Grad Norm 2.8477(5.7336) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 9.5882(9.3497) | Bit/dim 0.1745(0.2266) | Xent 2.3168(2.3115) | Xent Color 2.3027(2.3072) | Loss 1.2632(1.8458) | Error 0.9056(0.8984) | Error Color 0.9022(0.8992) |Steps 410(404.16) | Grad Norm 2.4218(4.8407) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 9.0405(9.3154) | Bit/dim 0.1995(0.2132) | Xent 2.3126(2.3109) | Xent Color 2.3048(2.3071) | Loss 1.2367(1.6865) | Error 0.8978(0.8971) | Error Color 0.8944(0.8984) |Steps 392(402.67) | Grad Norm 18.3577(4.8919) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 9.5399(9.3087) | Bit/dim 0.2224(0.2343) | Xent 2.3062(2.3105) | Xent Color 2.3070(2.3070) | Loss 1.4311(1.6275) | Error 0.8933(0.8975) | Error Color 0.9022(0.8977) |Steps 428(402.62) | Grad Norm 10.5452(9.7107) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 9.2886(9.2638) | Bit/dim 0.2213(0.2332) | Xent 2.3138(2.3102) | Xent Color 2.3080(2.3069) | Loss 1.4181(1.5460) | Error 0.8911(0.8964) | Error Color 0.9044(0.8984) |Steps 410(400.97) | Grad Norm 11.0855(10.2101) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 8.6330(9.2213) | Bit/dim 0.1827(0.2229) | Xent 2.3071(2.3100) | Xent Color 2.3135(2.3071) | Loss 1.2230(1.4684) | Error 0.8844(0.8965) | Error Color 0.9122(0.8994) |Steps 362(398.14) | Grad Norm 5.6289(8.9279) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 9.5257(9.2183) | Bit/dim 0.1729(0.2108) | Xent 2.3152(2.3104) | Xent Color 2.3080(2.3073) | Loss 1.2445(1.4077) | Error 0.8911(0.8956) | Error Color 0.9011(0.9003) |Steps 404(398.76) | Grad Norm 4.5703(7.6437) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 60.5737, Epoch Time 692.3810(663.0265), Bit/dim 0.1643(best: 0.1867), Xent 2.3050, Xent Color 2.3044. Loss 0.1758, Error 0.8911(best: 0.2647), Error Color 0.8993(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 9.3621(9.2280) | Bit/dim 0.1608(0.1986) | Xent 2.3065(2.3102) | Xent Color 2.3118(2.3074) | Loss 1.1989(1.8453) | Error 0.8989(0.8961) | Error Color 0.9156(0.9014) |Steps 398(397.17) | Grad Norm 1.0030(6.3124) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 9.3693(9.2104) | Bit/dim 0.1509(0.1873) | Xent 2.3133(2.3103) | Xent Color 2.3069(2.3072) | Loss 1.1880(1.6700) | Error 0.9167(0.8973) | Error Color 0.8967(0.9003) |Steps 404(396.21) | Grad Norm 1.8565(5.1838) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 9.1882(9.2883) | Bit/dim 0.1462(0.1775) | Xent 2.3036(2.3099) | Xent Color 2.3082(2.3070) | Loss 1.1535(1.5459) | Error 0.8944(0.8966) | Error Color 0.9044(0.9015) |Steps 410(399.09) | Grad Norm 0.9178(4.2183) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 9.2759(9.3109) | Bit/dim 0.1438(0.1689) | Xent 2.3034(2.3096) | Xent Color 2.3042(2.3066) | Loss 1.1025(1.4486) | Error 0.8867(0.8974) | Error Color 0.8922(0.9012) |Steps 392(400.23) | Grad Norm 2.1109(3.4464) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 9.2592(9.3342) | Bit/dim 0.1729(0.1644) | Xent 2.3098(2.3096) | Xent Color 2.3099(2.3069) | Loss 1.1822(1.3837) | Error 0.8967(0.8969) | Error Color 0.8911(0.8998) |Steps 416(399.99) | Grad Norm 15.8812(4.6522) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 9.5217(9.3239) | Bit/dim 0.2660(0.1697) | Xent 2.3089(2.3099) | Xent Color 2.3071(2.3070) | Loss 1.3992(1.3486) | Error 0.9000(0.8981) | Error Color 0.8922(0.8996) |Steps 374(400.56) | Grad Norm 32.0372(7.6776) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 9.7636(9.3492) | Bit/dim 0.2801(0.1894) | Xent 2.3115(2.3101) | Xent Color 2.3066(2.3066) | Loss 1.4323(1.3511) | Error 0.9011(0.8987) | Error Color 0.8956(0.8979) |Steps 374(397.91) | Grad Norm 7.6055(10.4819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 58.8193, Epoch Time 698.8270(664.1005), Bit/dim 0.2745(best: 0.1643), Xent 2.3052, Xent Color 2.3043. Loss 0.2860, Error 0.8967(best: 0.2647), Error Color 0.8967(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 9.2048(9.2784) | Bit/dim 0.2714(0.2128) | Xent 2.3089(2.3097) | Xent Color 2.3076(2.3066) | Loss 1.4936(1.8205) | Error 0.9144(0.8978) | Error Color 0.8844(0.8974) |Steps 416(398.32) | Grad Norm 6.0787(9.0881) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 8.8867(9.2455) | Bit/dim 0.2454(0.2230) | Xent 2.3029(2.3092) | Xent Color 2.3098(2.3063) | Loss 1.3597(1.7080) | Error 0.8867(0.8973) | Error Color 0.9011(0.8978) |Steps 392(399.83) | Grad Norm 4.6323(7.7465) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 8.9637(9.2198) | Bit/dim 0.2255(0.2263) | Xent 2.3111(2.3098) | Xent Color 2.3029(2.3060) | Loss 1.2890(1.6143) | Error 0.8922(0.8980) | Error Color 0.8867(0.8984) |Steps 392(397.05) | Grad Norm 2.6409(6.7469) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 9.4549(9.1670) | Bit/dim 0.2106(0.2238) | Xent 2.3028(2.3093) | Xent Color 2.3094(2.3062) | Loss 1.2713(1.5271) | Error 0.8844(0.8966) | Error Color 0.8989(0.8999) |Steps 398(393.16) | Grad Norm 7.3391(6.1165) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 9.2228(9.1208) | Bit/dim 0.1968(0.2181) | Xent 2.3091(2.3096) | Xent Color 2.3033(2.3062) | Loss 1.2509(1.4481) | Error 0.9011(0.8964) | Error Color 0.8911(0.9006) |Steps 386(389.50) | Grad Norm 7.9903(6.1929) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 9.6381(9.1759) | Bit/dim 0.1929(0.2101) | Xent 2.3172(2.3097) | Xent Color 2.3055(2.3065) | Loss 1.2224(1.3856) | Error 0.9178(0.8972) | Error Color 0.8867(0.9002) |Steps 422(387.55) | Grad Norm 14.6979(6.2651) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 56.0423, Epoch Time 682.9374(664.6656), Bit/dim 0.2509(best: 0.1643), Xent 2.3056, Xent Color 2.3038. Loss 0.2624, Error 0.8934(best: 0.2647), Error Color 0.8952(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 8.7272(9.2040) | Bit/dim 0.3220(0.2272) | Xent 2.3087(2.3101) | Xent Color 2.3095(2.3063) | Loss 1.4075(1.8545) | Error 0.8900(0.8983) | Error Color 0.8922(0.8987) |Steps 380(388.67) | Grad Norm 10.9055(10.6643) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 9.2875(9.1869) | Bit/dim 0.2181(0.2306) | Xent 2.3043(2.3094) | Xent Color 2.3036(2.3066) | Loss 1.2642(1.7124) | Error 0.8911(0.8968) | Error Color 0.8889(0.8990) |Steps 410(389.86) | Grad Norm 9.4182(10.9576) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 9.9502(9.3054) | Bit/dim 0.1799(0.2196) | Xent 2.3140(2.3099) | Xent Color 2.3075(2.3064) | Loss 1.3607(1.6012) | Error 0.8967(0.8988) | Error Color 0.9122(0.8994) |Steps 422(396.73) | Grad Norm 7.1521(10.0272) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 9.7187(9.3239) | Bit/dim 0.1594(0.2053) | Xent 2.3100(2.3091) | Xent Color 2.3022(2.3058) | Loss 1.2949(1.5211) | Error 0.8856(0.8975) | Error Color 0.8856(0.8985) |Steps 428(401.78) | Grad Norm 2.6807(8.5043) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 10.1987(9.4394) | Bit/dim 0.1502(0.1915) | Xent 2.3036(2.3087) | Xent Color 2.3094(2.3060) | Loss 1.3559(1.4688) | Error 0.8900(0.8978) | Error Color 0.8944(0.8989) |Steps 428(407.12) | Grad Norm 3.5193(7.1710) | Total Time 0.00(0.00)\n",
      "Iter 1440 | Time 9.3756(9.4555) | Bit/dim 0.1384(0.1789) | Xent 2.3115(2.3095) | Xent Color 2.3056(2.3062) | Loss 1.2370(1.4125) | Error 0.9167(0.8992) | Error Color 0.8911(0.8996) |Steps 416(406.29) | Grad Norm 5.1798(6.3509) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 9.5629(9.4856) | Bit/dim 0.3696(0.1778) | Xent 2.3288(2.3095) | Xent Color 2.3119(2.3062) | Loss 1.5585(1.3855) | Error 0.9156(0.8989) | Error Color 0.8900(0.8995) |Steps 392(408.04) | Grad Norm 80.0580(9.1462) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 61.1793, Epoch Time 710.6392(666.0448), Bit/dim 0.7726(best: 0.1643), Xent 2.3042, Xent Color 2.3041. Loss 0.7841, Error 0.8924(best: 0.2647), Error Color 0.9008(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 10.3196(9.5901) | Bit/dim 0.5811(0.3092) | Xent 2.3123(2.3090) | Xent Color 2.3007(2.3059) | Loss 2.0494(2.0952) | Error 0.9033(0.8974) | Error Color 0.8822(0.8989) |Steps 440(412.58) | Grad Norm 6.8751(11.7287) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 8.6972(9.5342) | Bit/dim 0.4925(0.3616) | Xent 2.3074(2.3088) | Xent Color 2.3080(2.3059) | Loss 1.7139(2.0344) | Error 0.8878(0.8957) | Error Color 0.9078(0.8983) |Steps 356(409.91) | Grad Norm 2.2945(9.7413) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 9.3304(9.4526) | Bit/dim 0.4465(0.3883) | Xent 2.3111(2.3091) | Xent Color 2.3047(2.3061) | Loss 1.7329(1.9540) | Error 0.9056(0.8959) | Error Color 0.8778(0.8989) |Steps 404(404.34) | Grad Norm 1.2183(7.7166) | Total Time 0.00(0.00)\n",
      "Iter 1490 | Time 9.8281(9.4320) | Bit/dim 0.4325(0.4017) | Xent 2.3155(2.3085) | Xent Color 2.3066(2.3059) | Loss 1.7791(1.9119) | Error 0.9100(0.8953) | Error Color 0.9200(0.9004) |Steps 392(402.53) | Grad Norm 1.6172(6.0723) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 8.6144(9.3190) | Bit/dim 0.4265(0.4084) | Xent 2.3101(2.3085) | Xent Color 2.3082(2.3054) | Loss 1.6273(1.8609) | Error 0.8978(0.8942) | Error Color 0.9144(0.8987) |Steps 374(399.94) | Grad Norm 7.4748(5.3710) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 8.8202(9.2404) | Bit/dim 0.4148(0.4116) | Xent 2.3177(2.3086) | Xent Color 2.3091(2.3056) | Loss 1.6628(1.8113) | Error 0.8956(0.8940) | Error Color 0.9100(0.8988) |Steps 410(397.80) | Grad Norm 9.6320(5.7213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 58.1661, Epoch Time 692.3106(666.8328), Bit/dim 0.3932(best: 0.1643), Xent 2.3038, Xent Color 2.3040. Loss 0.4047, Error 0.8939(best: 0.2647), Error Color 0.9019(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 9.2727(9.1897) | Bit/dim 0.3863(0.4086) | Xent 2.3137(2.3092) | Xent Color 2.3066(2.3053) | Loss 1.6011(2.3003) | Error 0.8878(0.8942) | Error Color 0.8944(0.8976) |Steps 386(395.22) | Grad Norm 7.7905(6.3312) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 9.1348(9.1353) | Bit/dim 0.3716(0.4000) | Xent 2.3116(2.3093) | Xent Color 2.3049(2.3054) | Loss 1.6144(2.1138) | Error 0.9078(0.8954) | Error Color 0.9133(0.8991) |Steps 410(394.57) | Grad Norm 2.2981(5.5829) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 8.8885(9.0726) | Bit/dim 0.3454(0.3886) | Xent 2.3095(2.3091) | Xent Color 2.3050(2.3054) | Loss 1.4882(1.9578) | Error 0.8978(0.8965) | Error Color 0.9067(0.8991) |Steps 362(389.68) | Grad Norm 2.5180(4.7082) | Total Time 0.00(0.00)\n",
      "Iter 1550 | Time 9.1220(9.0164) | Bit/dim 0.3614(0.3801) | Xent 2.2967(2.3083) | Xent Color 2.3057(2.3057) | Loss 1.5343(1.8459) | Error 0.8744(0.8954) | Error Color 0.8833(0.8982) |Steps 410(389.93) | Grad Norm 12.5285(6.0729) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 9.3653(8.9593) | Bit/dim 0.3949(0.3999) | Xent 2.3121(2.3087) | Xent Color 2.3049(2.3057) | Loss 1.5933(1.8071) | Error 0.9089(0.8959) | Error Color 0.9044(0.8988) |Steps 386(388.56) | Grad Norm 9.4963(8.2756) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 9.2224(8.9876) | Bit/dim 0.3289(0.3895) | Xent 2.3093(2.3090) | Xent Color 2.3092(2.3058) | Loss 1.4612(1.7373) | Error 0.9078(0.8964) | Error Color 0.8889(0.8979) |Steps 404(389.92) | Grad Norm 5.4190(8.5661) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 9.3735(8.9601) | Bit/dim 0.3036(0.3698) | Xent 2.3182(2.3087) | Xent Color 2.3075(2.3060) | Loss 1.4746(1.6554) | Error 0.9011(0.8954) | Error Color 0.8978(0.8981) |Steps 374(385.91) | Grad Norm 3.7636(7.5697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 57.8521, Epoch Time 670.2641(666.9357), Bit/dim 0.2934(best: 0.1643), Xent 2.3039, Xent Color 2.3033. Loss 0.3050, Error 0.8922(best: 0.2647), Error Color 0.8988(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 8.9962(8.9469) | Bit/dim 0.2804(0.3489) | Xent 2.3117(2.3090) | Xent Color 2.3037(2.3059) | Loss 1.4201(2.0562) | Error 0.9033(0.8967) | Error Color 0.8867(0.8991) |Steps 392(383.23) | Grad Norm 1.7367(6.5003) | Total Time 0.00(0.00)\n",
      "Iter 1600 | Time 9.1440(8.9735) | Bit/dim 0.2632(0.3280) | Xent 2.3069(2.3090) | Xent Color 2.3069(2.3059) | Loss 1.3118(1.8785) | Error 0.8922(0.8974) | Error Color 0.8922(0.8993) |Steps 380(386.63) | Grad Norm 1.5724(5.3634) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 8.9969(9.0129) | Bit/dim 0.2487(0.3083) | Xent 2.3103(2.3087) | Xent Color 2.3078(2.3061) | Loss 1.2781(1.7349) | Error 0.9056(0.8970) | Error Color 0.9056(0.9005) |Steps 380(386.63) | Grad Norm 1.5494(4.3967) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 9.2622(9.0599) | Bit/dim 0.2477(0.2907) | Xent 2.3064(2.3091) | Xent Color 2.3031(2.3061) | Loss 1.3324(1.6211) | Error 0.8967(0.8969) | Error Color 0.8967(0.9006) |Steps 398(386.62) | Grad Norm 9.0705(4.0697) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 9.1466(9.0661) | Bit/dim 0.2868(0.2859) | Xent 2.3169(2.3097) | Xent Color 2.3021(2.3062) | Loss 1.3483(1.5444) | Error 0.9044(0.8974) | Error Color 0.8944(0.9001) |Steps 410(386.08) | Grad Norm 23.1874(7.2105) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 8.8296(9.0779) | Bit/dim 0.2265(0.2760) | Xent 2.2976(2.3088) | Xent Color 2.3065(2.3067) | Loss 1.2651(1.4788) | Error 0.8811(0.8949) | Error Color 0.9144(0.9008) |Steps 404(388.06) | Grad Norm 7.6920(8.4189) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 9.6560(9.1226) | Bit/dim 0.2095(0.2607) | Xent 2.3014(2.3083) | Xent Color 2.3074(2.3063) | Loss 1.2399(1.4219) | Error 0.8744(0.8937) | Error Color 0.8989(0.8997) |Steps 380(390.48) | Grad Norm 6.9408(8.2122) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 58.2627, Epoch Time 682.6688(667.4077), Bit/dim 0.2002(best: 0.1643), Xent 2.3054, Xent Color 2.3038. Loss 0.2117, Error 0.8943(best: 0.2647), Error Color 0.8978(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 8.9443(9.1925) | Bit/dim 0.1882(0.2432) | Xent 2.2996(2.3075) | Xent Color 2.3118(2.3062) | Loss 1.1870(1.7764) | Error 0.8933(0.8940) | Error Color 0.9133(0.8991) |Steps 398(392.29) | Grad Norm 3.3162(7.0983) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 9.6215(9.3027) | Bit/dim 0.1691(0.2262) | Xent 2.3060(2.3080) | Xent Color 2.3105(2.3061) | Loss 1.1821(1.6294) | Error 0.8900(0.8958) | Error Color 0.9044(0.8995) |Steps 416(393.62) | Grad Norm 2.3900(6.0254) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 9.3771(9.3394) | Bit/dim 0.1614(0.2105) | Xent 2.3078(2.3086) | Xent Color 2.3078(2.3061) | Loss 1.1366(1.5101) | Error 0.9022(0.8980) | Error Color 0.8889(0.8997) |Steps 392(392.38) | Grad Norm 4.0707(5.3140) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 9.4412(9.3405) | Bit/dim 0.2038(0.2009) | Xent 2.3110(2.3086) | Xent Color 2.3047(2.3060) | Loss 1.2701(1.4277) | Error 0.9078(0.8983) | Error Color 0.8900(0.9001) |Steps 416(394.90) | Grad Norm 22.5787(7.6416) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 9.4395(9.3693) | Bit/dim 0.2171(0.1974) | Xent 2.3008(2.3083) | Xent Color 2.3056(2.3056) | Loss 1.3031(1.3686) | Error 0.8844(0.8971) | Error Color 0.8844(0.8995) |Steps 380(395.35) | Grad Norm 17.3653(9.9445) | Total Time 0.00(0.00)\n",
      "Iter 1710 | Time 9.1320(9.3723) | Bit/dim 0.1906(0.1937) | Xent 2.3034(2.3083) | Xent Color 2.3026(2.3055) | Loss 1.1909(1.3259) | Error 0.8756(0.8966) | Error Color 0.8944(0.8997) |Steps 392(394.69) | Grad Norm 11.6943(10.9887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 58.8876, Epoch Time 705.5214(668.5511), Bit/dim 0.1631(best: 0.1643), Xent 2.3025, Xent Color 2.3047. Loss 0.1746, Error 0.8977(best: 0.2647), Error Color 0.9012(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 9.6479(9.4176) | Bit/dim 0.1598(0.1878) | Xent 2.3082(2.3079) | Xent Color 2.3039(2.3054) | Loss 1.1590(1.7673) | Error 0.8889(0.8950) | Error Color 0.8967(0.8996) |Steps 410(395.92) | Grad Norm 9.4876(11.1790) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 9.2867(9.3973) | Bit/dim 0.1514(0.1785) | Xent 2.3093(2.3079) | Xent Color 2.3046(2.3056) | Loss 1.1137(1.5988) | Error 0.9089(0.8950) | Error Color 0.8889(0.8997) |Steps 386(394.35) | Grad Norm 8.5186(10.4649) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 9.7073(9.4635) | Bit/dim 0.1333(0.1689) | Xent 2.3046(2.3078) | Xent Color 2.3040(2.3058) | Loss 1.1457(1.4832) | Error 0.8756(0.8955) | Error Color 0.8889(0.8991) |Steps 422(398.13) | Grad Norm 4.6240(9.4213) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 9.8337(9.5096) | Bit/dim 0.1302(0.1594) | Xent 2.3073(2.3079) | Xent Color 2.3055(2.3055) | Loss 1.1618(1.3916) | Error 0.9011(0.8957) | Error Color 0.9178(0.8989) |Steps 410(401.47) | Grad Norm 4.0680(8.0015) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 9.8363(9.6010) | Bit/dim 0.1341(0.1521) | Xent 2.3037(2.3078) | Xent Color 2.3086(2.3053) | Loss 1.0936(1.3273) | Error 0.8867(0.8965) | Error Color 0.9000(0.8992) |Steps 422(404.31) | Grad Norm 9.0955(7.6199) | Total Time 0.00(0.00)\n",
      "Iter 1770 | Time 10.0932(9.6669) | Bit/dim 0.1237(0.1452) | Xent 2.3100(2.3085) | Xent Color 2.3028(2.3049) | Loss 1.1092(1.2740) | Error 0.8789(0.8974) | Error Color 0.8989(0.8981) |Steps 410(405.37) | Grad Norm 6.1431(6.9603) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 9.8525(9.7571) | Bit/dim 0.1301(0.1399) | Xent 2.3071(2.3081) | Xent Color 2.3052(2.3051) | Loss 1.1185(1.2359) | Error 0.8900(0.8963) | Error Color 0.9078(0.8992) |Steps 398(407.18) | Grad Norm 11.6619(6.7513) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 60.2034, Epoch Time 724.8111(670.2389), Bit/dim 0.1319(best: 0.1631), Xent 2.3041, Xent Color 2.3047. Loss 0.1434, Error 0.8939(best: 0.2647), Error Color 0.9023(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 9.8226(9.7950) | Bit/dim 0.1342(0.1391) | Xent 2.3113(2.3078) | Xent Color 2.3048(2.3053) | Loss 1.1503(1.6536) | Error 0.9033(0.8956) | Error Color 0.9000(0.9005) |Steps 428(408.28) | Grad Norm 13.3440(8.6768) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 9.6462(9.8419) | Bit/dim 0.1171(0.1378) | Xent 2.3073(2.3077) | Xent Color 2.3084(2.3052) | Loss 1.1197(1.5213) | Error 0.9022(0.8963) | Error Color 0.9111(0.9007) |Steps 416(409.35) | Grad Norm 3.5046(9.6293) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 9.7557(9.8029) | Bit/dim 0.1329(0.1345) | Xent 2.3069(2.3075) | Xent Color 2.3025(2.3050) | Loss 1.0685(1.4163) | Error 0.9067(0.8957) | Error Color 0.8811(0.8988) |Steps 398(410.30) | Grad Norm 13.7319(9.7262) | Total Time 0.00(0.00)\n",
      "Iter 1820 | Time 9.9351(9.8568) | Bit/dim 0.1280(0.1332) | Xent 2.3065(2.3075) | Xent Color 2.3055(2.3051) | Loss 1.1002(1.3439) | Error 0.9078(0.8959) | Error Color 0.8967(0.8984) |Steps 392(409.78) | Grad Norm 12.4399(10.2113) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 9.9714(9.8790) | Bit/dim 0.1212(0.1304) | Xent 2.2989(2.3072) | Xent Color 2.3078(2.3053) | Loss 1.1373(1.2877) | Error 0.8833(0.8961) | Error Color 0.9078(0.9014) |Steps 404(411.31) | Grad Norm 5.6535(10.1812) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 10.2581(9.9023) | Bit/dim 0.1438(0.1282) | Xent 2.3035(2.3069) | Xent Color 2.3056(2.3049) | Loss 1.1942(1.2458) | Error 0.8756(0.8956) | Error Color 0.8856(0.9002) |Steps 392(412.57) | Grad Norm 17.1065(10.3129) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 60.5106, Epoch Time 736.8746(672.2380), Bit/dim 0.1197(best: 0.1319), Xent 2.3028, Xent Color 2.3035. Loss 0.1313, Error 0.8883(best: 0.2647), Error Color 0.9009(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 9.9946(9.8913) | Bit/dim 0.1296(0.1274) | Xent 2.3009(2.3066) | Xent Color 2.3062(2.3049) | Loss 1.1240(1.7509) | Error 0.8789(0.8960) | Error Color 0.9033(0.9010) |Steps 416(411.77) | Grad Norm 16.1472(11.0100) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 9.8077(9.8347) | Bit/dim 0.2936(0.1506) | Xent 2.2996(2.3069) | Xent Color 2.3026(2.3050) | Loss 1.3162(1.6206) | Error 0.8800(0.8960) | Error Color 0.9067(0.9015) |Steps 362(408.54) | Grad Norm 36.4137(14.1192) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 9.7068(9.7469) | Bit/dim 0.1526(0.1572) | Xent 2.3119(2.3074) | Xent Color 2.3032(2.3051) | Loss 1.1795(1.4978) | Error 0.9033(0.8965) | Error Color 0.8956(0.9011) |Steps 386(405.10) | Grad Norm 7.0035(13.4464) | Total Time 0.00(0.00)\n",
      "Iter 1880 | Time 9.3150(9.7181) | Bit/dim 0.1200(0.1515) | Xent 2.3107(2.3076) | Xent Color 2.3037(2.3053) | Loss 1.0419(1.3954) | Error 0.9033(0.8961) | Error Color 0.8856(0.9010) |Steps 392(403.25) | Grad Norm 3.9277(12.0694) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 9.5604(9.6639) | Bit/dim 0.1128(0.1434) | Xent 2.3028(2.3072) | Xent Color 2.3096(2.3055) | Loss 1.0967(1.3172) | Error 0.8889(0.8965) | Error Color 0.9056(0.9001) |Steps 392(400.95) | Grad Norm 2.8005(10.5527) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 9.5055(9.6415) | Bit/dim 0.1126(0.1354) | Xent 2.3108(2.3071) | Xent Color 2.3058(2.3052) | Loss 1.0333(1.2605) | Error 0.9000(0.8959) | Error Color 0.9056(0.8993) |Steps 410(403.50) | Grad Norm 7.7737(9.1080) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 9.6166(9.6599) | Bit/dim 0.1371(0.1297) | Xent 2.3123(2.3069) | Xent Color 2.3022(2.3053) | Loss 1.2341(1.2198) | Error 0.8900(0.8943) | Error Color 0.9011(0.8994) |Steps 416(404.89) | Grad Norm 12.4370(8.5473) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 58.9308, Epoch Time 717.6758(673.6012), Bit/dim 0.1544(best: 0.1197), Xent 2.3047, Xent Color 2.3037. Loss 0.1659, Error 0.8957(best: 0.2647), Error Color 0.8953(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 9.5050(9.6915) | Bit/dim 0.1343(0.1373) | Xent 2.3149(2.3072) | Xent Color 2.3053(2.3053) | Loss 1.1483(1.6294) | Error 0.8967(0.8946) | Error Color 0.9067(0.8991) |Steps 392(404.90) | Grad Norm 12.2586(10.7140) | Total Time 0.00(0.00)\n",
      "Iter 1930 | Time 9.5723(9.6847) | Bit/dim 0.1377(0.1437) | Xent 2.3074(2.3075) | Xent Color 2.2997(2.3049) | Loss 1.0910(1.5048) | Error 0.8878(0.8950) | Error Color 0.8867(0.8983) |Steps 398(403.76) | Grad Norm 8.2151(11.9728) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 9.6537(9.6891) | Bit/dim 0.1331(0.1420) | Xent 2.3053(2.3074) | Xent Color 2.3006(2.3048) | Loss 1.0276(1.3945) | Error 0.8878(0.8953) | Error Color 0.8889(0.8979) |Steps 386(403.08) | Grad Norm 10.4766(11.6655) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 9.6865(9.6989) | Bit/dim 0.1165(0.1355) | Xent 2.3054(2.3072) | Xent Color 2.3048(2.3048) | Loss 1.0372(1.3185) | Error 0.9056(0.8967) | Error Color 0.8878(0.8979) |Steps 404(404.18) | Grad Norm 9.8148(10.9488) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 9.8243(9.7249) | Bit/dim 0.1036(0.1274) | Xent 2.3049(2.3072) | Xent Color 2.3046(2.3048) | Loss 1.0619(1.2619) | Error 0.8789(0.8956) | Error Color 0.9000(0.8996) |Steps 422(405.36) | Grad Norm 5.9419(9.4378) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 9.9117(9.7191) | Bit/dim 0.0959(0.1196) | Xent 2.3056(2.3067) | Xent Color 2.3048(2.3046) | Loss 1.1169(1.2164) | Error 0.9111(0.8963) | Error Color 0.9089(0.8982) |Steps 422(407.59) | Grad Norm 3.5939(7.9052) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 9.8470(9.7089) | Bit/dim 0.0912(0.1132) | Xent 2.3144(2.3067) | Xent Color 2.3012(2.3045) | Loss 0.9900(1.1719) | Error 0.9122(0.8957) | Error Color 0.9011(0.8982) |Steps 416(407.99) | Grad Norm 7.5718(7.1589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 59.5735, Epoch Time 724.2327(675.1201), Bit/dim 0.0962(best: 0.1197), Xent 2.3031, Xent Color 2.3041. Loss 0.1077, Error 0.8888(best: 0.2647), Error Color 0.8981(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 9.3840(9.7344) | Bit/dim 0.1561(0.1112) | Xent 2.3147(2.3073) | Xent Color 2.3064(2.3045) | Loss 1.1926(1.5665) | Error 0.9022(0.8970) | Error Color 0.9022(0.8983) |Steps 416(406.49) | Grad Norm 17.9259(8.0406) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 9.5499(9.7661) | Bit/dim 0.2541(0.1334) | Xent 2.3055(2.3072) | Xent Color 2.3056(2.3049) | Loss 1.3401(1.4807) | Error 0.8933(0.8957) | Error Color 0.8878(0.8999) |Steps 392(405.78) | Grad Norm 14.7587(11.2188) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 9.4573(9.7498) | Bit/dim 0.2020(0.1531) | Xent 2.3004(2.3069) | Xent Color 2.3006(2.3047) | Loss 1.2314(1.4136) | Error 0.8544(0.8933) | Error Color 0.8944(0.9000) |Steps 398(404.51) | Grad Norm 26.3945(13.0861) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 9.4130(9.6838) | Bit/dim 0.1334(0.1495) | Xent 2.3117(2.3069) | Xent Color 2.3052(2.3048) | Loss 1.0423(1.3306) | Error 0.8933(0.8936) | Error Color 0.9144(0.9009) |Steps 392(402.67) | Grad Norm 11.7387(12.6170) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 9.4435(9.6745) | Bit/dim 0.1116(0.1402) | Xent 2.3052(2.3070) | Xent Color 2.3049(2.3047) | Loss 1.1180(1.2730) | Error 0.8844(0.8931) | Error Color 0.8956(0.9009) |Steps 404(403.94) | Grad Norm 6.0067(11.2099) | Total Time 0.00(0.00)\n",
      "Iter 2040 | Time 9.3941(9.6424) | Bit/dim 0.0978(0.1303) | Xent 2.3061(2.3070) | Xent Color 2.3077(2.3049) | Loss 1.0789(1.2196) | Error 0.9011(0.8943) | Error Color 0.9178(0.9010) |Steps 410(404.24) | Grad Norm 4.6127(9.4052) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 59.7819, Epoch Time 721.9452(676.5248), Bit/dim 0.0952(best: 0.0962), Xent 2.3041, Xent Color 2.3035. Loss 0.1068, Error 0.8891(best: 0.2647), Error Color 0.9047(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 9.5531(9.6322) | Bit/dim 0.0921(0.1213) | Xent 2.3043(2.3074) | Xent Color 2.3069(2.3051) | Loss 1.0778(1.6584) | Error 0.8756(0.8953) | Error Color 0.9111(0.9020) |Steps 428(403.25) | Grad Norm 2.7974(7.8697) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 9.3263(9.5740) | Bit/dim 0.0868(0.1134) | Xent 2.3059(2.3074) | Xent Color 2.3049(2.3050) | Loss 0.9556(1.4902) | Error 0.8811(0.8951) | Error Color 0.9100(0.9015) |Steps 386(398.18) | Grad Norm 2.2530(6.6260) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 9.5426(9.5570) | Bit/dim 0.0870(0.1070) | Xent 2.3057(2.3071) | Xent Color 2.3048(2.3049) | Loss 0.9389(1.3634) | Error 0.8933(0.8953) | Error Color 0.9000(0.9003) |Steps 386(397.43) | Grad Norm 2.2936(5.9358) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 9.4184(9.5675) | Bit/dim 0.0886(0.1023) | Xent 2.3016(2.3070) | Xent Color 2.3015(2.3048) | Loss 0.9877(1.2746) | Error 0.8811(0.8938) | Error Color 0.8844(0.8997) |Steps 398(399.00) | Grad Norm 5.5713(5.9360) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 9.9809(9.5740) | Bit/dim 0.0939(0.0983) | Xent 2.3019(2.3064) | Xent Color 2.3071(2.3048) | Loss 1.0554(1.2092) | Error 0.8856(0.8936) | Error Color 0.9044(0.8983) |Steps 416(398.42) | Grad Norm 12.1309(5.6829) | Total Time 0.00(0.00)\n",
      "Iter 2100 | Time 9.2586(9.5798) | Bit/dim 0.0849(0.0959) | Xent 2.3031(2.3064) | Xent Color 2.3065(2.3048) | Loss 1.0195(1.1644) | Error 0.8878(0.8942) | Error Color 0.9011(0.8988) |Steps 374(398.17) | Grad Norm 7.8725(6.1675) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 9.8727(9.6093) | Bit/dim 0.1266(0.1039) | Xent 2.3054(2.3058) | Xent Color 2.3039(2.3048) | Loss 1.1252(1.1470) | Error 0.8956(0.8934) | Error Color 0.8989(0.8986) |Steps 422(399.18) | Grad Norm 8.9475(7.8854) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 61.5700, Epoch Time 717.1853(677.7447), Bit/dim 0.3724(best: 0.0952), Xent 2.3052, Xent Color 2.3033. Loss 0.3839, Error 0.8889(best: 0.2647), Error Color 0.8978(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 9.4034(9.6621) | Bit/dim 0.3222(0.1619) | Xent 2.3029(2.3061) | Xent Color 2.3040(2.3049) | Loss 1.4201(1.6897) | Error 0.9044(0.8947) | Error Color 0.9089(0.9008) |Steps 386(402.83) | Grad Norm 11.1073(10.8396) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 9.4898(9.7061) | Bit/dim 0.2404(0.1887) | Xent 2.3051(2.3057) | Xent Color 2.3042(2.3048) | Loss 1.2418(1.5836) | Error 0.8811(0.8941) | Error Color 0.8956(0.9003) |Steps 380(405.64) | Grad Norm 5.0976(9.6788) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 10.1232(9.6476) | Bit/dim 0.1874(0.1937) | Xent 2.3093(2.3062) | Xent Color 2.3067(2.3048) | Loss 1.1867(1.4831) | Error 0.8944(0.8942) | Error Color 0.9000(0.9008) |Steps 410(399.32) | Grad Norm 3.7999(8.2653) | Total Time 0.00(0.00)\n",
      "Iter 2150 | Time 9.1276(9.4883) | Bit/dim 0.1428(0.1847) | Xent 2.3059(2.3067) | Xent Color 2.3073(2.3051) | Loss 1.0712(1.3736) | Error 0.8878(0.8947) | Error Color 0.9167(0.9035) |Steps 380(392.16) | Grad Norm 3.4677(7.0060) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 9.0874(9.4103) | Bit/dim 0.1201(0.1703) | Xent 2.3050(2.3064) | Xent Color 2.3027(2.3050) | Loss 0.9281(1.2674) | Error 0.8978(0.8946) | Error Color 0.8989(0.9039) |Steps 350(382.24) | Grad Norm 2.0915(5.8006) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 9.1049(9.3473) | Bit/dim 0.1098(0.1564) | Xent 2.3076(2.3062) | Xent Color 2.3047(2.3049) | Loss 0.9257(1.1837) | Error 0.8922(0.8949) | Error Color 0.8889(0.9027) |Steps 350(378.10) | Grad Norm 5.5899(5.9948) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 53.1299, Epoch Time 696.2782(678.3007), Bit/dim 0.1101(best: 0.0952), Xent 2.3038, Xent Color 2.3034. Loss 0.1216, Error 0.8926(best: 0.2647), Error Color 0.8999(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 9.7671(9.3503) | Bit/dim 0.1151(0.1449) | Xent 2.3121(2.3070) | Xent Color 2.3058(2.3047) | Loss 0.9694(1.5511) | Error 0.9100(0.8952) | Error Color 0.8867(0.9007) |Steps 380(376.85) | Grad Norm 11.0263(6.5022) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 9.2719(9.2827) | Bit/dim 0.1375(0.1420) | Xent 2.3133(2.3071) | Xent Color 2.3032(2.3046) | Loss 0.9632(1.4020) | Error 0.8989(0.8959) | Error Color 0.8933(0.9013) |Steps 356(372.66) | Grad Norm 20.2840(8.5269) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 9.6739(9.3158) | Bit/dim 0.1066(0.1356) | Xent 2.3072(2.3070) | Xent Color 2.3052(2.3046) | Loss 1.0311(1.3023) | Error 0.9022(0.8968) | Error Color 0.8878(0.9007) |Steps 374(374.07) | Grad Norm 7.6579(9.2629) | Total Time 0.00(0.00)\n",
      "Iter 2210 | Time 9.5092(9.3432) | Bit/dim 0.1109(0.1285) | Xent 2.3053(2.3071) | Xent Color 2.3022(2.3042) | Loss 1.0067(1.2205) | Error 0.8944(0.8955) | Error Color 0.8922(0.9004) |Steps 386(374.19) | Grad Norm 7.2980(9.1601) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 9.4497(9.3699) | Bit/dim 0.0912(0.1205) | Xent 2.3002(2.3066) | Xent Color 2.3065(2.3043) | Loss 1.0101(1.1509) | Error 0.8833(0.8945) | Error Color 0.9000(0.8994) |Steps 392(378.85) | Grad Norm 3.8742(8.3898) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 9.5052(9.3830) | Bit/dim 0.0904(0.1129) | Xent 2.3041(2.3070) | Xent Color 2.3049(2.3041) | Loss 0.9551(1.0933) | Error 0.8833(0.8949) | Error Color 0.8978(0.8990) |Steps 374(378.76) | Grad Norm 4.2230(7.2960) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 9.4396(9.4238) | Bit/dim 0.0849(0.1068) | Xent 2.3044(2.3062) | Xent Color 2.3019(2.3039) | Loss 0.9072(1.0506) | Error 0.8878(0.8935) | Error Color 0.8833(0.8988) |Steps 386(379.80) | Grad Norm 2.4020(6.0537) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 55.7237, Epoch Time 699.2948(678.9305), Bit/dim 0.0853(best: 0.0952), Xent 2.3033, Xent Color 2.3040. Loss 0.0968, Error 0.8875(best: 0.2647), Error Color 0.8990(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 9.2194(9.4712) | Bit/dim 0.0868(0.1012) | Xent 2.3111(2.3061) | Xent Color 2.3062(2.3040) | Loss 0.9034(1.4460) | Error 0.9189(0.8935) | Error Color 0.9011(0.8990) |Steps 380(380.38) | Grad Norm 2.3953(5.0333) | Total Time 0.00(0.00)\n",
      "Iter 2260 | Time 9.7172(9.5025) | Bit/dim 0.0838(0.0967) | Xent 2.3015(2.3059) | Xent Color 2.3010(2.3038) | Loss 0.9700(1.3133) | Error 0.8833(0.8922) | Error Color 0.8900(0.8982) |Steps 386(382.23) | Grad Norm 2.5761(4.4185) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 9.4784(9.5268) | Bit/dim 0.1340(0.1003) | Xent 2.3009(2.3057) | Xent Color 2.3055(2.3039) | Loss 1.0756(1.2308) | Error 0.8800(0.8929) | Error Color 0.9033(0.8980) |Steps 392(383.52) | Grad Norm 15.5928(6.9807) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 9.9258(9.5508) | Bit/dim 0.2792(0.1221) | Xent 2.3047(2.3066) | Xent Color 2.3052(2.3040) | Loss 1.2716(1.2145) | Error 0.8856(0.8957) | Error Color 0.8933(0.8986) |Steps 374(384.70) | Grad Norm 32.7550(9.8059) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 9.3271(9.5039) | Bit/dim 0.2157(0.1531) | Xent 2.2965(2.3059) | Xent Color 2.3028(2.3039) | Loss 1.1144(1.2112) | Error 0.8722(0.8932) | Error Color 0.9022(0.8990) |Steps 356(382.84) | Grad Norm 9.7520(9.5891) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 10.0879(9.5348) | Bit/dim 0.1358(0.1524) | Xent 2.3063(2.3058) | Xent Color 2.3066(2.3040) | Loss 1.0403(1.1739) | Error 0.8933(0.8930) | Error Color 0.9111(0.8982) |Steps 392(385.08) | Grad Norm 18.2786(9.6765) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 9.1570(9.5013) | Bit/dim 0.0977(0.1409) | Xent 2.3110(2.3060) | Xent Color 2.3059(2.3041) | Loss 0.9020(1.1184) | Error 0.9022(0.8939) | Error Color 0.9122(0.8984) |Steps 386(384.71) | Grad Norm 6.2662(9.3030) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 54.5220, Epoch Time 707.8860(679.7992), Bit/dim 0.1007(best: 0.0853), Xent 2.3029, Xent Color 2.3029. Loss 0.1122, Error 0.8976(best: 0.2647), Error Color 0.9011(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 9.3392(9.4915) | Bit/dim 0.0927(0.1290) | Xent 2.3049(2.3057) | Xent Color 2.3020(2.3040) | Loss 0.9669(1.4218) | Error 0.9067(0.8936) | Error Color 0.9100(0.8995) |Steps 386(382.42) | Grad Norm 4.2632(8.3230) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 9.7124(9.4895) | Bit/dim 0.0811(0.1177) | Xent 2.3081(2.3061) | Xent Color 2.3058(2.3041) | Loss 0.9180(1.2873) | Error 0.9022(0.8942) | Error Color 0.9000(0.8993) |Steps 404(382.49) | Grad Norm 1.7970(7.0824) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 9.7386(9.4279) | Bit/dim 0.0826(0.1087) | Xent 2.3064(2.3058) | Xent Color 2.3036(2.3044) | Loss 0.9064(1.1837) | Error 0.9011(0.8931) | Error Color 0.8989(0.9010) |Steps 380(380.03) | Grad Norm 4.4646(6.3330) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 9.7116(9.4250) | Bit/dim 0.0811(0.1017) | Xent 2.3041(2.3054) | Xent Color 2.3018(2.3042) | Loss 0.8913(1.1078) | Error 0.8967(0.8922) | Error Color 0.8878(0.8994) |Steps 386(381.03) | Grad Norm 2.2454(5.7748) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 9.3153(9.3979) | Bit/dim 0.0819(0.0958) | Xent 2.3036(2.3054) | Xent Color 2.3003(2.3040) | Loss 0.8704(1.0475) | Error 0.8833(0.8922) | Error Color 0.8844(0.8988) |Steps 398(381.29) | Grad Norm 7.1628(5.4452) | Total Time 0.00(0.00)\n",
      "Iter 2370 | Time 9.5763(9.4191) | Bit/dim 0.0763(0.0916) | Xent 2.3077(2.3058) | Xent Color 2.3068(2.3042) | Loss 0.8702(1.0077) | Error 0.8933(0.8928) | Error Color 0.9167(0.9006) |Steps 356(380.37) | Grad Norm 1.2451(5.9585) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 54.9591, Epoch Time 700.2596(680.4130), Bit/dim 0.0969(best: 0.0853), Xent 2.3029, Xent Color 2.3039. Loss 0.1084, Error 0.8936(best: 0.2647), Error Color 0.9018(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 8.7101(9.4223) | Bit/dim 0.1697(0.0991) | Xent 2.3040(2.3059) | Xent Color 2.3043(2.3044) | Loss 1.0647(1.4149) | Error 0.8811(0.8940) | Error Color 0.9156(0.9012) |Steps 374(381.38) | Grad Norm 16.6181(7.7508) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 11.0517(9.5916) | Bit/dim 0.5075(0.1973) | Xent 2.3102(2.3057) | Xent Color 2.3038(2.3046) | Loss 1.9510(1.5081) | Error 0.8956(0.8924) | Error Color 0.8978(0.9010) |Steps 476(394.09) | Grad Norm 13.8437(12.6661) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 12.5105(10.4993) | Bit/dim 0.3576(0.2532) | Xent 2.3066(2.3060) | Xent Color 2.3041(2.3041) | Loss 1.6955(1.6050) | Error 0.8900(0.8918) | Error Color 0.8978(0.8980) |Steps 548(443.77) | Grad Norm 4.5242(11.0260) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 11.3971(10.7943) | Bit/dim 0.2936(0.2699) | Xent 2.3070(2.3058) | Xent Color 2.3039(2.3041) | Loss 1.4312(1.5845) | Error 0.9089(0.8937) | Error Color 0.9022(0.8979) |Steps 518(459.81) | Grad Norm 2.4170(8.9940) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 10.8881(10.8576) | Bit/dim 0.2622(0.2712) | Xent 2.3031(2.3053) | Xent Color 2.3056(2.3042) | Loss 1.3383(1.5281) | Error 0.9078(0.8935) | Error Color 0.9133(0.8994) |Steps 470(460.82) | Grad Norm 0.9046(7.1175) | Total Time 0.00(0.00)\n",
      "Iter 2430 | Time 10.5438(10.7685) | Bit/dim 0.2403(0.2653) | Xent 2.3019(2.3049) | Xent Color 2.3048(2.3041) | Loss 1.2777(1.4678) | Error 0.8967(0.8927) | Error Color 0.9022(0.8995) |Steps 464(457.66) | Grad Norm 1.0026(5.6238) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 10.6520(10.6924) | Bit/dim 0.2722(0.2639) | Xent 2.3055(2.3053) | Xent Color 2.3070(2.3039) | Loss 1.3825(1.4363) | Error 0.8922(0.8938) | Error Color 0.9044(0.8988) |Steps 458(456.40) | Grad Norm 10.8346(6.9964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 62.6937, Epoch Time 810.7083(684.3218), Bit/dim 0.2332(best: 0.0853), Xent 2.3036, Xent Color 2.3033. Loss 0.2447, Error 0.8951(best: 0.2647), Error Color 0.9004(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2450 | Time 10.6556(10.6016) | Bit/dim 0.2215(0.2562) | Xent 2.3115(2.3055) | Xent Color 2.3028(2.3040) | Loss 1.2421(1.8271) | Error 0.9089(0.8929) | Error Color 0.9056(0.8988) |Steps 428(452.32) | Grad Norm 4.3218(7.1664) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 10.4804(10.4822) | Bit/dim 0.2040(0.2452) | Xent 2.2983(2.3059) | Xent Color 2.3024(2.3040) | Loss 1.2673(1.6759) | Error 0.8911(0.8936) | Error Color 0.8956(0.8991) |Steps 440(448.27) | Grad Norm 2.9704(6.6677) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 10.1958(10.3761) | Bit/dim 0.1916(0.2332) | Xent 2.3108(2.3057) | Xent Color 2.3039(2.3038) | Loss 1.2218(1.5542) | Error 0.9100(0.8934) | Error Color 0.8922(0.8998) |Steps 428(442.75) | Grad Norm 5.0990(6.3258) | Total Time 0.00(0.00)\n",
      "Iter 2480 | Time 9.8136(10.3092) | Bit/dim 0.1804(0.2210) | Xent 2.3088(2.3055) | Xent Color 2.3035(2.3037) | Loss 1.1462(1.4650) | Error 0.9067(0.8931) | Error Color 0.9089(0.9000) |Steps 428(440.68) | Grad Norm 5.3739(6.1495) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 10.4204(10.2644) | Bit/dim 0.1737(0.2093) | Xent 2.3035(2.3055) | Xent Color 2.3049(2.3039) | Loss 1.2060(1.3875) | Error 0.8833(0.8931) | Error Color 0.8900(0.8988) |Steps 452(437.60) | Grad Norm 9.5629(6.2205) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 10.0967(10.2346) | Bit/dim 0.1813(0.2021) | Xent 2.2988(2.3055) | Xent Color 2.3031(2.3039) | Loss 1.1629(1.3373) | Error 0.8856(0.8940) | Error Color 0.9022(0.9001) |Steps 416(436.45) | Grad Norm 10.1163(7.6260) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 64.4537, Epoch Time 757.7969(686.5261), Bit/dim 0.1830(best: 0.0853), Xent 2.3044, Xent Color 2.3032. Loss 0.1945, Error 0.8928(best: 0.2647), Error Color 0.8974(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2510 | Time 10.9324(10.2471) | Bit/dim 0.1801(0.1946) | Xent 2.3061(2.3053) | Xent Color 2.3033(2.3038) | Loss 1.1954(1.8175) | Error 0.9022(0.8938) | Error Color 0.9022(0.9000) |Steps 452(434.17) | Grad Norm 14.5300(8.4713) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 9.9493(10.2110) | Bit/dim 0.1541(0.1856) | Xent 2.3036(2.3050) | Xent Color 2.3054(2.3038) | Loss 1.1196(1.6372) | Error 0.8933(0.8938) | Error Color 0.9111(0.9011) |Steps 434(432.10) | Grad Norm 10.2394(8.7787) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 10.2421(10.1806) | Bit/dim 0.1361(0.1748) | Xent 2.3063(2.3051) | Xent Color 2.3019(2.3038) | Loss 1.1188(1.5026) | Error 0.8978(0.8941) | Error Color 0.8867(0.9007) |Steps 410(430.17) | Grad Norm 3.0377(7.9733) | Total Time 0.00(0.00)\n",
      "Iter 2540 | Time 10.3536(10.1524) | Bit/dim 0.1833(0.1668) | Xent 2.3022(2.3050) | Xent Color 2.3044(2.3039) | Loss 1.2046(1.3998) | Error 0.8911(0.8936) | Error Color 0.9056(0.8998) |Steps 434(429.30) | Grad Norm 34.8562(8.7636) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 9.9350(10.1834) | Bit/dim 0.2975(0.2182) | Xent 2.3026(2.3050) | Xent Color 2.3042(2.3039) | Loss 1.4009(1.4481) | Error 0.8978(0.8942) | Error Color 0.8956(0.8978) |Steps 416(431.66) | Grad Norm 6.6338(10.1344) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 9.9730(10.2417) | Bit/dim 0.1935(0.2237) | Xent 2.3056(2.3051) | Xent Color 2.3016(2.3040) | Loss 1.1343(1.4038) | Error 0.8933(0.8951) | Error Color 0.8978(0.8999) |Steps 434(433.31) | Grad Norm 11.0409(11.2177) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 10.6713(10.2183) | Bit/dim 0.1582(0.2158) | Xent 2.3049(2.3050) | Xent Color 2.3053(2.3039) | Loss 1.1350(1.3438) | Error 0.9000(0.8951) | Error Color 0.8989(0.8995) |Steps 428(430.04) | Grad Norm 6.4219(10.8467) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 59.9685, Epoch Time 757.4635(688.6542), Bit/dim 0.1570(best: 0.0853), Xent 2.3022, Xent Color 2.3034. Loss 0.1685, Error 0.8899(best: 0.2647), Error Color 0.8951(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2580 | Time 9.9178(10.2300) | Bit/dim 0.1415(0.1984) | Xent 2.3097(2.3050) | Xent Color 2.3042(2.3040) | Loss 1.0835(1.7176) | Error 0.9022(0.8950) | Error Color 0.9178(0.9002) |Steps 440(430.00) | Grad Norm 5.6067(9.7101) | Total Time 0.00(0.00)\n",
      "Iter 2590 | Time 9.9872(10.2312) | Bit/dim 0.1234(0.1808) | Xent 2.2997(2.3047) | Xent Color 2.3023(2.3040) | Loss 0.9708(1.5392) | Error 0.9022(0.8949) | Error Color 0.8900(0.8991) |Steps 416(428.01) | Grad Norm 2.0364(8.2480) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 9.9809(10.1582) | Bit/dim 0.1137(0.1649) | Xent 2.3057(2.3047) | Xent Color 2.3037(2.3040) | Loss 1.0206(1.4028) | Error 0.8911(0.8944) | Error Color 0.9089(0.8996) |Steps 422(425.74) | Grad Norm 1.7586(6.8445) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 10.2616(10.1277) | Bit/dim 0.1097(0.1514) | Xent 2.3068(2.3048) | Xent Color 2.3027(2.3038) | Loss 0.9923(1.3007) | Error 0.8856(0.8946) | Error Color 0.8989(0.8987) |Steps 410(423.40) | Grad Norm 1.6940(5.5726) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 10.0524(10.1052) | Bit/dim 0.1036(0.1400) | Xent 2.3042(2.3047) | Xent Color 2.3036(2.3041) | Loss 1.0518(1.2262) | Error 0.9011(0.8953) | Error Color 0.8889(0.8989) |Steps 440(420.08) | Grad Norm 1.0228(4.4544) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 10.3849(10.0866) | Bit/dim 0.1095(0.1308) | Xent 2.3061(2.3047) | Xent Color 2.3040(2.3039) | Loss 1.0274(1.1716) | Error 0.8944(0.8953) | Error Color 0.9033(0.8990) |Steps 428(421.63) | Grad Norm 6.6924(3.9837) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 10.1595(10.0854) | Bit/dim 0.1195(0.1249) | Xent 2.3002(2.3045) | Xent Color 2.3046(2.3040) | Loss 1.0511(1.1332) | Error 0.8800(0.8934) | Error Color 0.9167(0.8992) |Steps 428(422.58) | Grad Norm 11.2831(4.6789) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 61.5899, Epoch Time 749.3174(690.4741), Bit/dim 0.1004(best: 0.0853), Xent 2.3028, Xent Color 2.3026. Loss 0.1119, Error 0.8914(best: 0.2647), Error Color 0.9006(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2650 | Time 9.6370(10.0503) | Bit/dim 0.1803(0.1355) | Xent 2.3090(2.3049) | Xent Color 2.3027(2.3040) | Loss 1.1381(1.5186) | Error 0.8944(0.8935) | Error Color 0.8967(0.9004) |Steps 434(422.27) | Grad Norm 22.2513(7.7919) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 10.7987(10.1424) | Bit/dim 0.1426(0.1376) | Xent 2.3032(2.3051) | Xent Color 2.3058(2.3039) | Loss 1.1545(1.4176) | Error 0.8989(0.8947) | Error Color 0.9044(0.8999) |Steps 410(427.04) | Grad Norm 16.5426(8.7301) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 10.0063(10.1384) | Bit/dim 0.1151(0.1321) | Xent 2.3042(2.3046) | Xent Color 2.3070(2.3040) | Loss 0.9741(1.3129) | Error 0.8678(0.8936) | Error Color 0.9000(0.8996) |Steps 404(422.88) | Grad Norm 7.3931(8.3515) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 10.3293(10.1741) | Bit/dim 0.0979(0.1247) | Xent 2.3061(2.3050) | Xent Color 2.3037(2.3039) | Loss 0.9853(1.2276) | Error 0.9011(0.8949) | Error Color 0.9022(0.8992) |Steps 428(422.28) | Grad Norm 4.4003(7.3053) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 10.4248(10.1314) | Bit/dim 0.0937(0.1174) | Xent 2.3038(2.3053) | Xent Color 2.3082(2.3041) | Loss 0.9981(1.1633) | Error 0.9000(0.8966) | Error Color 0.9211(0.9004) |Steps 434(422.61) | Grad Norm 3.5725(6.3006) | Total Time 0.00(0.00)\n",
      "Iter 2700 | Time 10.0653(10.1389) | Bit/dim 0.0919(0.1110) | Xent 2.3023(2.3051) | Xent Color 2.3032(2.3041) | Loss 0.9729(1.1166) | Error 0.8989(0.8967) | Error Color 0.8867(0.9000) |Steps 410(423.02) | Grad Norm 1.9175(5.6374) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 62.2156, Epoch Time 755.2989(692.4188), Bit/dim 0.0900(best: 0.0853), Xent 2.3031, Xent Color 2.3037. Loss 0.1015, Error 0.8925(best: 0.2647), Error Color 0.9038(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2710 | Time 10.2108(10.1332) | Bit/dim 0.0876(0.1056) | Xent 2.3101(2.3053) | Xent Color 2.3044(2.3042) | Loss 0.9980(1.5592) | Error 0.8978(0.8978) | Error Color 0.8856(0.8987) |Steps 416(423.36) | Grad Norm 0.8793(5.0988) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 9.9649(10.0846) | Bit/dim 0.1043(0.1023) | Xent 2.3084(2.3050) | Xent Color 2.3033(2.3040) | Loss 1.0044(1.4076) | Error 0.9100(0.8967) | Error Color 0.8922(0.8999) |Steps 428(422.23) | Grad Norm 18.4402(5.4171) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 10.2387(10.0866) | Bit/dim 0.1247(0.1113) | Xent 2.3013(2.3048) | Xent Color 2.3045(2.3039) | Loss 1.0287(1.3194) | Error 0.8700(0.8966) | Error Color 0.9011(0.8988) |Steps 428(423.67) | Grad Norm 4.7445(5.9326) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 9.3673(10.0619) | Bit/dim 0.0926(0.1091) | Xent 2.2964(2.3043) | Xent Color 2.3039(2.3037) | Loss 0.9217(1.2294) | Error 0.8667(0.8940) | Error Color 0.9122(0.9000) |Steps 422(425.62) | Grad Norm 1.2611(5.8642) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 10.0065(10.0800) | Bit/dim 0.0910(0.1047) | Xent 2.3128(2.3047) | Xent Color 2.3045(2.3040) | Loss 0.9458(1.1643) | Error 0.8778(0.8942) | Error Color 0.8911(0.9006) |Steps 398(425.73) | Grad Norm 6.9129(5.9220) | Total Time 0.00(0.00)\n",
      "Iter 2760 | Time 10.7497(10.1100) | Bit/dim 0.2615(0.1094) | Xent 2.3047(2.3050) | Xent Color 2.3045(2.3038) | Loss 1.4182(1.1308) | Error 0.8956(0.8959) | Error Color 0.9022(0.8990) |Steps 470(426.22) | Grad Norm 17.0715(7.6457) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 9.7511(10.0437) | Bit/dim 0.2687(0.1556) | Xent 2.3023(2.3049) | Xent Color 2.3034(2.3036) | Loss 1.2829(1.1915) | Error 0.9067(0.8954) | Error Color 0.8989(0.8967) |Steps 404(426.00) | Grad Norm 4.6593(10.1832) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 58.0952, Epoch Time 744.3326(693.9763), Bit/dim 0.2371(best: 0.0853), Xent 2.3022, Xent Color 2.3036. Loss 0.2486, Error 0.8866(best: 0.2647), Error Color 0.9041(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2780 | Time 10.5990(10.0280) | Bit/dim 0.1676(0.1667) | Xent 2.3038(2.3047) | Xent Color 2.3037(2.3037) | Loss 1.1820(1.6037) | Error 0.9011(0.8955) | Error Color 0.9044(0.8976) |Steps 464(424.74) | Grad Norm 13.1005(10.7856) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 10.0174(10.0682) | Bit/dim 0.1090(0.1573) | Xent 2.2999(2.3047) | Xent Color 2.3023(2.3037) | Loss 0.9867(1.4669) | Error 0.8833(0.8952) | Error Color 0.9000(0.8978) |Steps 428(426.67) | Grad Norm 4.7494(10.2217) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 9.8391(10.0413) | Bit/dim 0.0976(0.1440) | Xent 2.3067(2.3050) | Xent Color 2.3040(2.3039) | Loss 0.9501(1.3396) | Error 0.8978(0.8960) | Error Color 0.8956(0.8987) |Steps 404(424.32) | Grad Norm 4.3411(9.1123) | Total Time 0.00(0.00)\n",
      "Iter 2810 | Time 10.1130(10.0222) | Bit/dim 0.0878(0.1301) | Xent 2.3061(2.3048) | Xent Color 2.3051(2.3038) | Loss 0.9401(1.2349) | Error 0.8944(0.8956) | Error Color 0.9022(0.8982) |Steps 422(422.70) | Grad Norm 3.5915(7.7155) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 9.8205(9.9680) | Bit/dim 0.0817(0.1180) | Xent 2.3036(2.3044) | Xent Color 2.3052(2.3039) | Loss 0.9259(1.1537) | Error 0.8933(0.8947) | Error Color 0.9044(0.8997) |Steps 410(418.94) | Grad Norm 1.8276(6.4111) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 9.7706(9.9434) | Bit/dim 0.0803(0.1082) | Xent 2.3034(2.3045) | Xent Color 2.3029(2.3038) | Loss 0.9233(1.0937) | Error 0.8933(0.8941) | Error Color 0.8911(0.8992) |Steps 416(416.06) | Grad Norm 1.8422(5.4629) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 58.5922, Epoch Time 737.9219(695.2946), Bit/dim 0.0763(best: 0.0853), Xent 2.3029, Xent Color 2.3035. Loss 0.0878, Error 0.8927(best: 0.2647), Error Color 0.9032(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2840 | Time 9.8658(9.9047) | Bit/dim 0.0806(0.1002) | Xent 2.3023(2.3042) | Xent Color 2.3045(2.3036) | Loss 0.9545(1.5034) | Error 0.8911(0.8954) | Error Color 0.8933(0.8988) |Steps 404(411.02) | Grad Norm 3.0279(4.7963) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 10.2778(9.8461) | Bit/dim 0.0709(0.0936) | Xent 2.3030(2.3039) | Xent Color 2.3047(2.3038) | Loss 0.8660(1.3481) | Error 0.8867(0.8941) | Error Color 0.9000(0.8988) |Steps 416(406.98) | Grad Norm 1.0550(3.8181) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 9.9130(9.8209) | Bit/dim 0.0938(0.0903) | Xent 2.3098(2.3041) | Xent Color 2.3027(2.3038) | Loss 0.9629(1.2372) | Error 0.9056(0.8944) | Error Color 0.8889(0.8989) |Steps 398(406.89) | Grad Norm 22.3548(5.2288) | Total Time 0.00(0.00)\n",
      "Iter 2870 | Time 10.4026(9.8935) | Bit/dim 0.2734(0.1269) | Xent 2.3027(2.3039) | Xent Color 2.3048(2.3039) | Loss 1.5049(1.2470) | Error 0.8989(0.8940) | Error Color 0.9178(0.9015) |Steps 482(413.31) | Grad Norm 10.7102(8.6148) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 10.1541(10.0653) | Bit/dim 0.1984(0.1507) | Xent 2.3006(2.3039) | Xent Color 2.3029(2.3039) | Loss 1.1589(1.2621) | Error 0.8889(0.8928) | Error Color 0.8978(0.9017) |Steps 440(424.46) | Grad Norm 5.1361(8.4367) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 10.9369(10.1435) | Bit/dim 0.1694(0.1594) | Xent 2.3092(2.3038) | Xent Color 2.3059(2.3038) | Loss 1.1758(1.2472) | Error 0.9211(0.8924) | Error Color 0.9200(0.9016) |Steps 452(428.78) | Grad Norm 2.3279(7.7041) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 10.1572(10.2425) | Bit/dim 0.1522(0.1587) | Xent 2.3056(2.3039) | Xent Color 2.3015(2.3035) | Loss 1.0583(1.2239) | Error 0.8911(0.8923) | Error Color 0.8833(0.8995) |Steps 428(432.33) | Grad Norm 3.6195(6.6973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 63.1860, Epoch Time 757.4908(697.1605), Bit/dim 0.1448(best: 0.0763), Xent 2.3023, Xent Color 2.3027. Loss 0.1563, Error 0.8844(best: 0.2647), Error Color 0.8907(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2910 | Time 10.1019(10.2546) | Bit/dim 0.1410(0.1543) | Xent 2.3061(2.3041) | Xent Color 2.3043(2.3034) | Loss 1.0934(1.6476) | Error 0.9044(0.8923) | Error Color 0.8978(0.8994) |Steps 440(435.80) | Grad Norm 3.7731(5.6436) | Total Time 0.00(0.00)\n",
      "Iter 2920 | Time 10.5289(10.2507) | Bit/dim 0.1261(0.1483) | Xent 2.2994(2.3038) | Xent Color 2.3043(2.3034) | Loss 1.0685(1.5014) | Error 0.8878(0.8918) | Error Color 0.9133(0.8997) |Steps 428(436.49) | Grad Norm 1.1059(4.6506) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 10.1252(10.2499) | Bit/dim 0.1166(0.1414) | Xent 2.3005(2.3045) | Xent Color 2.3060(2.3036) | Loss 1.0991(1.3885) | Error 0.8767(0.8924) | Error Color 0.9056(0.8997) |Steps 434(437.45) | Grad Norm 2.7424(3.7865) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 10.5847(10.2491) | Bit/dim 0.1202(0.1358) | Xent 2.3025(2.3045) | Xent Color 2.3046(2.3037) | Loss 1.1476(1.3094) | Error 0.8922(0.8926) | Error Color 0.8978(0.9003) |Steps 458(440.63) | Grad Norm 5.6924(3.8589) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 10.4317(10.1662) | Bit/dim 0.1160(0.1303) | Xent 2.3020(2.3044) | Xent Color 2.3066(2.3039) | Loss 1.0968(1.2470) | Error 0.8833(0.8924) | Error Color 0.9044(0.9010) |Steps 410(437.99) | Grad Norm 6.0792(4.1057) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 10.0621(10.1389) | Bit/dim 0.1656(0.1283) | Xent 2.2995(2.3040) | Xent Color 2.3033(2.3037) | Loss 1.2277(1.2041) | Error 0.8911(0.8929) | Error Color 0.9056(0.9005) |Steps 440(438.58) | Grad Norm 14.4226(5.6141) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 10.8920(10.2884) | Bit/dim 0.3478(0.1646) | Xent 2.3038(2.3042) | Xent Color 2.3019(2.3035) | Loss 1.7626(1.2862) | Error 0.8878(0.8928) | Error Color 0.8789(0.8982) |Steps 476(445.55) | Grad Norm 17.0134(8.4729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 66.8346, Epoch Time 764.9137(699.1931), Bit/dim 0.3243(best: 0.0763), Xent 2.3028, Xent Color 2.3027. Loss 0.3358, Error 0.8892(best: 0.2647), Error Color 0.8950(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2980 | Time 9.6347(10.2598) | Bit/dim 0.4085(0.2105) | Xent 2.3045(2.3042) | Xent Color 2.3017(2.3035) | Loss 1.6545(1.8645) | Error 0.8989(0.8929) | Error Color 0.8844(0.8978) |Steps 416(448.18) | Grad Norm 6.1734(9.8090) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 9.6854(10.1019) | Bit/dim 0.3090(0.2512) | Xent 2.3125(2.3047) | Xent Color 2.3033(2.3035) | Loss 1.4144(1.7804) | Error 0.8956(0.8938) | Error Color 0.9044(0.8996) |Steps 392(442.00) | Grad Norm 3.7404(9.0332) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 9.8161(10.0009) | Bit/dim 0.2768(0.2607) | Xent 2.3050(2.3046) | Xent Color 2.3012(2.3033) | Loss 1.3026(1.6719) | Error 0.8878(0.8934) | Error Color 0.8978(0.8989) |Steps 416(434.97) | Grad Norm 18.3656(8.8724) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 10.0497(9.9581) | Bit/dim 0.2446(0.2586) | Xent 2.3006(2.3043) | Xent Color 2.3041(2.3034) | Loss 1.3027(1.5737) | Error 0.8811(0.8937) | Error Color 0.9111(0.8991) |Steps 404(429.55) | Grad Norm 3.9455(8.1391) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 9.4498(9.8359) | Bit/dim 0.2231(0.2519) | Xent 2.2995(2.3043) | Xent Color 2.3033(2.3035) | Loss 1.2236(1.4832) | Error 0.8833(0.8936) | Error Color 0.8833(0.8991) |Steps 422(423.44) | Grad Norm 2.1292(6.8631) | Total Time 0.00(0.00)\n",
      "Iter 3030 | Time 9.5972(9.7832) | Bit/dim 0.2025(0.2421) | Xent 2.3089(2.3048) | Xent Color 2.3014(2.3036) | Loss 1.1635(1.4043) | Error 0.9133(0.8941) | Error Color 0.9033(0.9013) |Steps 398(417.24) | Grad Norm 2.5868(5.4999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 58.6001, Epoch Time 724.3033(699.9464), Bit/dim 0.1970(best: 0.0763), Xent 2.3034, Xent Color 2.3035. Loss 0.2085, Error 0.8928(best: 0.2647), Error Color 0.9064(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3040 | Time 9.6939(9.7338) | Bit/dim 0.1880(0.2317) | Xent 2.3051(2.3048) | Xent Color 2.3053(2.3037) | Loss 1.1869(1.8025) | Error 0.9000(0.8948) | Error Color 0.9011(0.9016) |Steps 404(412.79) | Grad Norm 3.1515(5.2638) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 9.9763(9.7473) | Bit/dim 0.1917(0.2283) | Xent 2.3048(2.3045) | Xent Color 2.3025(2.3037) | Loss 1.2345(1.6611) | Error 0.8933(0.8945) | Error Color 0.8922(0.9012) |Steps 464(416.00) | Grad Norm 6.5383(6.7600) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 9.5969(9.7838) | Bit/dim 0.1653(0.2156) | Xent 2.3053(2.3046) | Xent Color 2.3038(2.3036) | Loss 1.0860(1.5307) | Error 0.8978(0.8953) | Error Color 0.9022(0.9007) |Steps 410(414.54) | Grad Norm 5.5508(6.6016) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 8.9985(9.6957) | Bit/dim 0.1645(0.2021) | Xent 2.3032(2.3048) | Xent Color 2.3030(2.3037) | Loss 1.0519(1.4184) | Error 0.8922(0.8959) | Error Color 0.9100(0.9014) |Steps 386(410.96) | Grad Norm 7.7481(6.2178) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 9.3667(9.6378) | Bit/dim 0.1885(0.1930) | Xent 2.3074(2.3046) | Xent Color 2.3030(2.3035) | Loss 1.2135(1.3440) | Error 0.8978(0.8957) | Error Color 0.9133(0.9005) |Steps 416(407.72) | Grad Norm 16.1727(7.2369) | Total Time 0.00(0.00)\n",
      "Iter 3090 | Time 9.3711(9.5902) | Bit/dim 0.1495(0.1877) | Xent 2.3002(2.3041) | Xent Color 2.3004(2.3033) | Loss 1.0164(1.2823) | Error 0.8867(0.8951) | Error Color 0.8822(0.8995) |Steps 374(403.53) | Grad Norm 7.6317(8.6488) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 9.1275(9.5746) | Bit/dim 0.2213(0.1918) | Xent 2.3026(2.3042) | Xent Color 2.3033(2.3035) | Loss 1.1966(1.2593) | Error 0.8856(0.8952) | Error Color 0.9133(0.9006) |Steps 398(403.79) | Grad Norm 8.9139(10.0620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 60.2055, Epoch Time 718.3428(700.4983), Bit/dim 0.1550(best: 0.0763), Xent 2.3023, Xent Color 2.3026. Loss 0.1666, Error 0.8895(best: 0.2647), Error Color 0.8996(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3110 | Time 10.0555(9.6157) | Bit/dim 0.1820(0.1886) | Xent 2.3081(2.3041) | Xent Color 2.3000(2.3033) | Loss 1.1578(1.6628) | Error 0.8911(0.8942) | Error Color 0.8867(0.8986) |Steps 404(404.58) | Grad Norm 21.2308(10.5719) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 9.4985(9.6180) | Bit/dim 0.1499(0.1772) | Xent 2.3037(2.3040) | Xent Color 2.3037(2.3033) | Loss 1.0671(1.5170) | Error 0.8989(0.8947) | Error Color 0.9044(0.8996) |Steps 374(406.81) | Grad Norm 9.5040(10.0025) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 9.5347(9.6156) | Bit/dim 0.1788(0.1686) | Xent 2.3055(2.3042) | Xent Color 2.3024(2.3033) | Loss 1.1633(1.4038) | Error 0.9067(0.8942) | Error Color 0.8978(0.8988) |Steps 416(406.12) | Grad Norm 20.4095(9.8034) | Total Time 0.00(0.00)\n",
      "Iter 3140 | Time 10.1875(9.7292) | Bit/dim 0.1594(0.1753) | Xent 2.3052(2.3042) | Xent Color 2.3027(2.3032) | Loss 1.1869(1.3637) | Error 0.8967(0.8939) | Error Color 0.8911(0.8988) |Steps 434(412.46) | Grad Norm 8.9915(10.5777) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 9.6093(9.6849) | Bit/dim 0.1342(0.1692) | Xent 2.3033(2.3039) | Xent Color 2.3020(2.3034) | Loss 0.9986(1.2840) | Error 0.8867(0.8930) | Error Color 0.9033(0.8986) |Steps 392(405.68) | Grad Norm 4.4630(9.8131) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 9.1331(9.6253) | Bit/dim 0.1266(0.1597) | Xent 2.3040(2.3038) | Xent Color 2.3028(2.3036) | Loss 0.9572(1.2083) | Error 0.8833(0.8931) | Error Color 0.8956(0.9000) |Steps 374(400.03) | Grad Norm 4.5867(8.5724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 55.1021, Epoch Time 714.2181(700.9099), Bit/dim 0.1160(best: 0.0763), Xent 2.3023, Xent Color 2.3032. Loss 0.1276, Error 0.8894(best: 0.2647), Error Color 0.9005(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3170 | Time 9.9077(9.6156) | Bit/dim 0.1126(0.1490) | Xent 2.3037(2.3040) | Xent Color 2.3034(2.3036) | Loss 0.9741(1.6097) | Error 0.8822(0.8940) | Error Color 0.8978(0.8997) |Steps 416(397.96) | Grad Norm 3.2105(7.1203) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 9.6816(9.6067) | Bit/dim 0.1006(0.1390) | Xent 2.3060(2.3039) | Xent Color 2.3056(2.3036) | Loss 0.9439(1.4379) | Error 0.8956(0.8926) | Error Color 0.9289(0.9008) |Steps 392(394.68) | Grad Norm 5.1704(6.5483) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 9.9078(9.6319) | Bit/dim 0.1542(0.1465) | Xent 2.3010(2.3038) | Xent Color 2.3051(2.3037) | Loss 1.0991(1.3491) | Error 0.8956(0.8935) | Error Color 0.9011(0.9009) |Steps 428(395.67) | Grad Norm 17.5424(8.2423) | Total Time 0.00(0.00)\n",
      "Iter 3200 | Time 9.5624(9.6948) | Bit/dim 0.1421(0.1431) | Xent 2.3062(2.3042) | Xent Color 2.3038(2.3039) | Loss 1.0163(1.2690) | Error 0.9011(0.8942) | Error Color 0.8922(0.9013) |Steps 380(400.51) | Grad Norm 10.3955(8.4147) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 9.4790(9.7007) | Bit/dim 0.1137(0.1363) | Xent 2.3018(2.3041) | Xent Color 2.3044(2.3037) | Loss 0.9496(1.1909) | Error 0.8878(0.8947) | Error Color 0.8833(0.9009) |Steps 374(399.32) | Grad Norm 6.5127(8.0974) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 9.3891(9.6612) | Bit/dim 0.1162(0.1304) | Xent 2.3018(2.3042) | Xent Color 2.3068(2.3037) | Loss 0.9552(1.1279) | Error 0.8922(0.8953) | Error Color 0.9167(0.9002) |Steps 392(395.63) | Grad Norm 5.0094(7.4741) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 9.8298(9.6668) | Bit/dim 0.0993(0.1246) | Xent 2.3060(2.3041) | Xent Color 2.3027(2.3037) | Loss 0.9129(1.0806) | Error 0.8967(0.8962) | Error Color 0.9033(0.9006) |Steps 392(395.02) | Grad Norm 5.5951(7.4175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 55.3943, Epoch Time 719.2850(701.4611), Bit/dim 0.1561(best: 0.0763), Xent 2.3025, Xent Color 2.3031. Loss 0.1676, Error 0.8876(best: 0.2647), Error Color 0.8986(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3240 | Time 9.5572(9.6791) | Bit/dim 0.1260(0.1304) | Xent 2.3055(2.3043) | Xent Color 2.3049(2.3037) | Loss 0.9820(1.4857) | Error 0.8867(0.8963) | Error Color 0.9078(0.9014) |Steps 374(396.12) | Grad Norm 9.1590(9.2140) | Total Time 0.00(0.00)\n",
      "Iter 3250 | Time 9.4004(9.6369) | Bit/dim 0.1105(0.1273) | Xent 2.3038(2.3040) | Xent Color 2.3021(2.3036) | Loss 0.9824(1.3536) | Error 0.8967(0.8953) | Error Color 0.8922(0.9009) |Steps 386(393.20) | Grad Norm 10.1853(9.0330) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 9.1918(9.5859) | Bit/dim 0.0925(0.1207) | Xent 2.3032(2.3041) | Xent Color 2.3045(2.3033) | Loss 0.9263(1.2431) | Error 0.8867(0.8948) | Error Color 0.8911(0.8986) |Steps 386(391.08) | Grad Norm 5.9095(7.9741) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 10.3912(9.5634) | Bit/dim 0.1096(0.1162) | Xent 2.3017(2.3037) | Xent Color 2.3045(2.3033) | Loss 1.0475(1.1627) | Error 0.9033(0.8935) | Error Color 0.9067(0.8981) |Steps 428(390.68) | Grad Norm 10.1887(7.5080) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 9.6629(9.6655) | Bit/dim 0.1906(0.1375) | Xent 2.3108(2.3042) | Xent Color 2.3061(2.3033) | Loss 1.1293(1.1682) | Error 0.9133(0.8950) | Error Color 0.9100(0.8976) |Steps 410(396.12) | Grad Norm 16.2031(8.9881) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 10.1129(9.7508) | Bit/dim 0.1126(0.1380) | Xent 2.3075(2.3044) | Xent Color 2.3038(2.3032) | Loss 0.9950(1.1378) | Error 0.9067(0.8952) | Error Color 0.9122(0.8976) |Steps 392(399.96) | Grad Norm 4.1605(9.4646) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 9.3280(9.7301) | Bit/dim 0.1142(0.1307) | Xent 2.3064(2.3040) | Xent Color 2.3036(2.3032) | Loss 0.9437(1.0929) | Error 0.8856(0.8945) | Error Color 0.8978(0.8979) |Steps 362(397.43) | Grad Norm 7.5334(8.6154) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 56.2026, Epoch Time 717.9896(701.9570), Bit/dim 0.1041(best: 0.0763), Xent 2.3024, Xent Color 2.3034. Loss 0.1156, Error 0.8960(best: 0.2647), Error Color 0.9015(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3310 | Time 9.5498(9.6937) | Bit/dim 0.0967(0.1228) | Xent 2.2974(2.3036) | Xent Color 2.3036(2.3034) | Loss 0.8993(1.4110) | Error 0.8722(0.8931) | Error Color 0.9067(0.8995) |Steps 380(394.12) | Grad Norm 6.2387(7.4378) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 9.0236(9.6404) | Bit/dim 0.0982(0.1153) | Xent 2.3072(2.3037) | Xent Color 2.3036(2.3033) | Loss 0.9416(1.2838) | Error 0.9200(0.8941) | Error Color 0.9133(0.8991) |Steps 368(392.85) | Grad Norm 2.5807(6.4182) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 9.4361(9.6236) | Bit/dim 0.0962(0.1091) | Xent 2.3036(2.3035) | Xent Color 2.3057(2.3036) | Loss 0.9092(1.1884) | Error 0.8833(0.8937) | Error Color 0.9078(0.9007) |Steps 398(392.36) | Grad Norm 2.1031(5.8150) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 9.6446(9.5972) | Bit/dim 0.1512(0.1112) | Xent 2.3031(2.3034) | Xent Color 2.3021(2.3033) | Loss 1.0752(1.1328) | Error 0.8889(0.8935) | Error Color 0.8933(0.8996) |Steps 392(389.60) | Grad Norm 14.8479(7.2406) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 9.1484(9.5757) | Bit/dim 0.3294(0.1646) | Xent 2.3030(2.3036) | Xent Color 2.3050(2.3034) | Loss 1.3326(1.2021) | Error 0.9000(0.8938) | Error Color 0.9056(0.8991) |Steps 386(391.99) | Grad Norm 24.2288(10.4064) | Total Time 0.00(0.00)\n",
      "Iter 3360 | Time 9.9676(9.7143) | Bit/dim 0.2614(0.1975) | Xent 2.3089(2.3041) | Xent Color 2.3031(2.3032) | Loss 1.2960(1.2451) | Error 0.8967(0.8943) | Error Color 0.9011(0.8977) |Steps 392(397.85) | Grad Norm 7.3065(10.0808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 64.0499, Epoch Time 727.4775(702.7226), Bit/dim 0.2336(best: 0.0763), Xent 2.3019, Xent Color 2.3025. Loss 0.2451, Error 0.8952(best: 0.2647), Error Color 0.9016(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3370 | Time 10.0836(9.8774) | Bit/dim 0.2132(0.2078) | Xent 2.3041(2.3042) | Xent Color 2.3029(2.3033) | Loss 1.2471(1.8265) | Error 0.8856(0.8942) | Error Color 0.9056(0.8983) |Steps 410(405.62) | Grad Norm 1.4770(8.4573) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 10.5309(9.8975) | Bit/dim 0.1784(0.2039) | Xent 2.3042(2.3041) | Xent Color 2.3048(2.3033) | Loss 1.0618(1.6476) | Error 0.8856(0.8941) | Error Color 0.9044(0.8982) |Steps 398(405.24) | Grad Norm 4.2048(7.1515) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 9.3614(9.8491) | Bit/dim 0.1448(0.1906) | Xent 2.3018(2.3039) | Xent Color 2.3059(2.3036) | Loss 1.0275(1.4842) | Error 0.8822(0.8940) | Error Color 0.9022(0.8984) |Steps 374(398.98) | Grad Norm 2.3303(5.8419) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 10.3665(9.7975) | Bit/dim 0.1429(0.1778) | Xent 2.3040(2.3041) | Xent Color 2.3058(2.3036) | Loss 1.1478(1.3617) | Error 0.9011(0.8952) | Error Color 0.9178(0.8990) |Steps 446(396.19) | Grad Norm 9.5424(6.1512) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 10.2253(9.8961) | Bit/dim 0.1772(0.1741) | Xent 2.2969(2.3036) | Xent Color 2.3016(2.3037) | Loss 1.2473(1.2989) | Error 0.8833(0.8937) | Error Color 0.8867(0.8997) |Steps 428(397.22) | Grad Norm 12.9753(8.8157) | Total Time 0.00(0.00)\n",
      "Iter 3420 | Time 9.5527(9.8121) | Bit/dim 0.1377(0.1684) | Xent 2.3036(2.3039) | Xent Color 2.3041(2.3036) | Loss 1.0704(1.2353) | Error 0.8944(0.8937) | Error Color 0.9278(0.9004) |Steps 404(396.22) | Grad Norm 6.8427(9.7527) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 9.5236(9.7021) | Bit/dim 0.1209(0.1589) | Xent 2.3071(2.3036) | Xent Color 2.3040(2.3037) | Loss 0.9640(1.1663) | Error 0.9011(0.8932) | Error Color 0.9056(0.9001) |Steps 350(389.04) | Grad Norm 5.9491(9.3068) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 54.1656, Epoch Time 721.7938(703.2948), Bit/dim 0.1179(best: 0.0763), Xent 2.3021, Xent Color 2.3034. Loss 0.1294, Error 0.8893(best: 0.2647), Error Color 0.9031(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3440 | Time 9.2832(9.5895) | Bit/dim 0.1139(0.1483) | Xent 2.3022(2.3035) | Xent Color 2.3046(2.3037) | Loss 0.9334(1.4737) | Error 0.8678(0.8918) | Error Color 0.9089(0.8999) |Steps 380(384.75) | Grad Norm 3.9746(8.3230) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 9.4803(9.5668) | Bit/dim 0.1065(0.1388) | Xent 2.3042(2.3034) | Xent Color 2.3056(2.3037) | Loss 0.9175(1.3306) | Error 0.9078(0.8927) | Error Color 0.9044(0.9011) |Steps 404(383.54) | Grad Norm 5.1459(7.2812) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 9.9040(9.5751) | Bit/dim 0.1061(0.1299) | Xent 2.3018(2.3036) | Xent Color 2.3021(2.3035) | Loss 1.0127(1.2258) | Error 0.8800(0.8930) | Error Color 0.9167(0.9007) |Steps 428(382.52) | Grad Norm 4.4210(6.5120) | Total Time 0.00(0.00)\n",
      "Iter 3470 | Time 8.9039(9.5199) | Bit/dim 0.1087(0.1246) | Xent 2.3077(2.3039) | Xent Color 2.3051(2.3036) | Loss 0.8980(1.1489) | Error 0.9000(0.8936) | Error Color 0.9111(0.9012) |Steps 356(382.32) | Grad Norm 6.4330(6.4223) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 9.9437(9.5345) | Bit/dim 0.0986(0.1213) | Xent 2.3074(2.3038) | Xent Color 2.3038(2.3035) | Loss 0.9623(1.0985) | Error 0.8978(0.8937) | Error Color 0.9056(0.9017) |Steps 404(381.79) | Grad Norm 6.0885(7.3192) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 9.8829(9.5777) | Bit/dim 0.1311(0.1196) | Xent 2.3062(2.3039) | Xent Color 2.3036(2.3034) | Loss 0.9654(1.0618) | Error 0.8844(0.8930) | Error Color 0.8656(0.9000) |Steps 386(383.10) | Grad Norm 22.3762(8.4084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 54.3925, Epoch Time 704.5907(703.3336), Bit/dim 0.1359(best: 0.0763), Xent 2.3013, Xent Color 2.3032. Loss 0.1474, Error 0.8857(best: 0.2647), Error Color 0.8993(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3500 | Time 8.7252(9.5385) | Bit/dim 0.1778(0.1244) | Xent 2.3020(2.3037) | Xent Color 2.3030(2.3033) | Loss 1.0695(1.5065) | Error 0.8967(0.8934) | Error Color 0.9089(0.9003) |Steps 350(381.63) | Grad Norm 10.8308(10.0583) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 9.9618(9.4634) | Bit/dim 0.1633(0.1332) | Xent 2.2997(2.3032) | Xent Color 2.3031(2.3034) | Loss 1.1523(1.3870) | Error 0.8800(0.8923) | Error Color 0.8878(0.9004) |Steps 404(378.93) | Grad Norm 27.1492(11.3600) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 9.5047(9.4609) | Bit/dim 0.1173(0.1297) | Xent 2.2979(2.3030) | Xent Color 2.3035(2.3034) | Loss 0.9504(1.2801) | Error 0.8711(0.8914) | Error Color 0.9144(0.9001) |Steps 380(380.24) | Grad Norm 11.3049(10.8841) | Total Time 0.00(0.00)\n",
      "Iter 3530 | Time 10.2925(9.5328) | Bit/dim 0.1007(0.1231) | Xent 2.3060(2.3034) | Xent Color 2.3040(2.3035) | Loss 0.9367(1.1895) | Error 0.9000(0.8926) | Error Color 0.8844(0.9011) |Steps 392(380.95) | Grad Norm 6.9997(10.0931) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 10.5756(9.6485) | Bit/dim 0.1038(0.1188) | Xent 2.3038(2.3032) | Xent Color 2.3052(2.3034) | Loss 0.9442(1.1273) | Error 0.8944(0.8919) | Error Color 0.9133(0.9011) |Steps 404(386.03) | Grad Norm 10.2253(9.7989) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 11.1807(9.9190) | Bit/dim 0.3769(0.1789) | Xent 2.3062(2.3036) | Xent Color 2.3009(2.3034) | Loss 2.0077(1.2655) | Error 0.9089(0.8938) | Error Color 0.8900(0.9001) |Steps 506(403.14) | Grad Norm 9.3116(12.5909) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 10.6163(10.2160) | Bit/dim 0.3209(0.2223) | Xent 2.3021(2.3035) | Xent Color 2.3023(2.3032) | Loss 1.4974(1.3819) | Error 0.8933(0.8940) | Error Color 0.8989(0.8996) |Steps 410(423.21) | Grad Norm 3.0643(10.4724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 61.5104, Epoch Time 747.0637(704.6455), Bit/dim 0.3004(best: 0.0763), Xent 2.3024, Xent Color 2.3023. Loss 0.3119, Error 0.8900(best: 0.2647), Error Color 0.8947(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3570 | Time 10.4223(10.3656) | Bit/dim 0.2817(0.2424) | Xent 2.3077(2.3037) | Xent Color 2.3042(2.3031) | Loss 1.3242(1.9042) | Error 0.8978(0.8940) | Error Color 0.8933(0.8988) |Steps 458(431.28) | Grad Norm 2.7168(8.4685) | Total Time 0.00(0.00)\n",
      "Iter 3580 | Time 10.3571(10.3421) | Bit/dim 0.2374(0.2454) | Xent 2.3028(2.3032) | Xent Color 2.3043(2.3032) | Loss 1.2337(1.7340) | Error 0.8956(0.8941) | Error Color 0.8967(0.8996) |Steps 422(426.82) | Grad Norm 1.7197(6.7317) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 9.9316(10.1392) | Bit/dim 0.2045(0.2390) | Xent 2.3096(2.3037) | Xent Color 2.3047(2.3033) | Loss 1.1982(1.5852) | Error 0.9222(0.8955) | Error Color 0.8900(0.8987) |Steps 386(415.72) | Grad Norm 1.2646(5.3477) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 9.4544(9.9962) | Bit/dim 0.1590(0.2232) | Xent 2.3046(2.3038) | Xent Color 2.3040(2.3033) | Loss 1.0192(1.4574) | Error 0.8878(0.8941) | Error Color 0.8956(0.8987) |Steps 368(408.27) | Grad Norm 1.9096(4.2995) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 10.3823(10.0129) | Bit/dim 0.1946(0.2120) | Xent 2.3046(2.3038) | Xent Color 2.3033(2.3034) | Loss 1.2805(1.3770) | Error 0.9111(0.8937) | Error Color 0.8978(0.8993) |Steps 440(407.88) | Grad Norm 11.1510(5.4433) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 11.5341(10.2610) | Bit/dim 0.1529(0.1997) | Xent 2.3051(2.3037) | Xent Color 2.3063(2.3036) | Loss 1.3048(1.3587) | Error 0.9111(0.8943) | Error Color 0.9111(0.9021) |Steps 476(417.92) | Grad Norm 10.6681(6.8722) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 10.4708(10.3524) | Bit/dim 0.1120(0.1802) | Xent 2.3007(2.3036) | Xent Color 2.3036(2.3037) | Loss 1.0798(1.3065) | Error 0.8878(0.8937) | Error Color 0.9078(0.9025) |Steps 440(423.00) | Grad Norm 5.4666(6.9090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 59.0041, Epoch Time 757.9322(706.2441), Bit/dim 0.1105(best: 0.0763), Xent 2.3021, Xent Color 2.3024. Loss 0.1220, Error 0.8958(best: 0.2647), Error Color 0.9024(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3640 | Time 10.0094(10.2654) | Bit/dim 0.0941(0.1599) | Xent 2.3020(2.3034) | Xent Color 2.3026(2.3035) | Loss 1.0311(1.6384) | Error 0.8911(0.8931) | Error Color 0.9000(0.9023) |Steps 428(419.27) | Grad Norm 3.4650(6.4995) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 10.0590(10.2292) | Bit/dim 0.0825(0.1414) | Xent 2.3085(2.3036) | Xent Color 2.3037(2.3034) | Loss 0.9418(1.4565) | Error 0.9000(0.8930) | Error Color 0.8900(0.9009) |Steps 398(414.94) | Grad Norm 5.6056(5.8942) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 10.3320(10.1872) | Bit/dim 0.3216(0.1420) | Xent 2.2976(2.3034) | Xent Color 2.3023(2.3033) | Loss 1.5840(1.3593) | Error 0.8744(0.8928) | Error Color 0.8867(0.9006) |Steps 416(412.97) | Grad Norm 10.9835(7.5550) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 11.6298(10.4080) | Bit/dim 0.2568(0.1832) | Xent 2.3039(2.3035) | Xent Color 2.3021(2.3033) | Loss 1.6261(1.4336) | Error 0.9000(0.8937) | Error Color 0.9000(0.8998) |Steps 506(428.45) | Grad Norm 4.5767(8.5949) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 11.0714(10.4835) | Bit/dim 0.1763(0.1867) | Xent 2.3070(2.3039) | Xent Color 2.3055(2.3034) | Loss 1.2102(1.3804) | Error 0.8956(0.8934) | Error Color 0.9167(0.8999) |Steps 476(433.49) | Grad Norm 8.5647(7.9605) | Total Time 0.00(0.00)\n",
      "Iter 3690 | Time 11.4418(10.7113) | Bit/dim 0.1310(0.1752) | Xent 2.3029(2.3036) | Xent Color 2.3039(2.3034) | Loss 1.1669(1.3263) | Error 0.9056(0.8934) | Error Color 0.8989(0.9008) |Steps 458(440.65) | Grad Norm 5.0617(7.2325) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 57.4033, Epoch Time 777.4213(708.3794), Bit/dim 0.1148(best: 0.0763), Xent 2.3017, Xent Color 2.3032. Loss 0.1264, Error 0.8868(best: 0.2647), Error Color 0.9046(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3700 | Time 9.0845(10.5987) | Bit/dim 0.1141(0.1602) | Xent 2.3014(2.3036) | Xent Color 2.3037(2.3033) | Loss 0.9215(1.7270) | Error 0.8944(0.8932) | Error Color 0.8900(0.9004) |Steps 368(435.39) | Grad Norm 2.6261(6.1188) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 10.3866(10.5116) | Bit/dim 0.0987(0.1456) | Xent 2.3042(2.3039) | Xent Color 2.3019(2.3032) | Loss 0.9904(1.5381) | Error 0.9111(0.8944) | Error Color 0.8811(0.8979) |Steps 404(430.04) | Grad Norm 2.3552(5.1905) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 10.1692(10.3720) | Bit/dim 0.0884(0.1325) | Xent 2.2993(2.3037) | Xent Color 2.3030(2.3032) | Loss 0.9620(1.3853) | Error 0.8844(0.8942) | Error Color 0.8856(0.8991) |Steps 380(418.89) | Grad Norm 1.3924(4.4859) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 9.9712(10.2819) | Bit/dim 0.0883(0.1216) | Xent 2.3002(2.3035) | Xent Color 2.3027(2.3032) | Loss 0.9055(1.2661) | Error 0.8789(0.8938) | Error Color 0.8933(0.8994) |Steps 386(412.89) | Grad Norm 4.8558(4.3024) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 10.1169(10.1969) | Bit/dim 0.0875(0.1124) | Xent 2.2996(2.3033) | Xent Color 2.3038(2.3032) | Loss 0.9458(1.1775) | Error 0.8878(0.8936) | Error Color 0.9056(0.8997) |Steps 422(409.13) | Grad Norm 7.1400(4.7227) | Total Time 0.00(0.00)\n",
      "Iter 3750 | Time 9.8790(10.0937) | Bit/dim 0.0847(0.1050) | Xent 2.3076(2.3033) | Xent Color 2.3041(2.3032) | Loss 0.9085(1.1113) | Error 0.8978(0.8929) | Error Color 0.8989(0.8998) |Steps 368(404.12) | Grad Norm 9.6981(5.4258) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 10.0176(10.0944) | Bit/dim 0.2099(0.1092) | Xent 2.3029(2.3030) | Xent Color 2.3029(2.3034) | Loss 1.1603(1.0832) | Error 0.8911(0.8932) | Error Color 0.9011(0.9001) |Steps 410(404.23) | Grad Norm 67.1675(9.5239) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 69.1506, Epoch Time 752.6216(709.7067), Bit/dim 0.3954(best: 0.0763), Xent 2.3017, Xent Color 2.3027. Loss 0.4070, Error 0.8914(best: 0.2647), Error Color 0.8961(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3770 | Time 11.2379(10.2604) | Bit/dim 0.2836(0.1636) | Xent 2.3035(2.3031) | Xent Color 2.3037(2.3034) | Loss 1.5816(1.7781) | Error 0.8800(0.8939) | Error Color 0.8867(0.8996) |Steps 506(422.31) | Grad Norm 7.7661(11.1473) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 10.8131(10.3165) | Bit/dim 0.1979(0.1800) | Xent 2.3022(2.3033) | Xent Color 2.3042(2.3033) | Loss 1.2447(1.6587) | Error 0.8844(0.8938) | Error Color 0.9067(0.8991) |Steps 446(429.62) | Grad Norm 4.7925(10.0406) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 10.4162(10.3155) | Bit/dim 0.1665(0.1814) | Xent 2.3036(2.3032) | Xent Color 2.3028(2.3033) | Loss 1.1094(1.5352) | Error 0.9011(0.8944) | Error Color 0.8878(0.8998) |Steps 428(431.99) | Grad Norm 4.1510(8.9989) | Total Time 0.00(0.00)\n",
      "Iter 3800 | Time 9.9526(10.3064) | Bit/dim 0.1448(0.1745) | Xent 2.3015(2.3031) | Xent Color 2.3039(2.3033) | Loss 1.1047(1.4260) | Error 0.8911(0.8935) | Error Color 0.9011(0.8995) |Steps 434(432.82) | Grad Norm 4.0161(7.7861) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 10.1930(10.2559) | Bit/dim 0.1291(0.1640) | Xent 2.3071(2.3032) | Xent Color 2.3027(2.3033) | Loss 1.0624(1.3361) | Error 0.8978(0.8920) | Error Color 0.9189(0.9001) |Steps 416(431.61) | Grad Norm 3.9429(6.8953) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 10.5284(10.2616) | Bit/dim 0.1467(0.1635) | Xent 2.3043(2.3036) | Xent Color 2.3031(2.3033) | Loss 1.0843(1.2873) | Error 0.8978(0.8939) | Error Color 0.9111(0.8999) |Steps 416(430.00) | Grad Norm 8.5860(8.8796) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 59.1372, Epoch Time 762.3345(711.2855), Bit/dim 0.1392(best: 0.0763), Xent 2.3024, Xent Color 2.3030. Loss 0.1507, Error 0.8928(best: 0.2647), Error Color 0.9011(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3830 | Time 10.4283(10.1762) | Bit/dim 0.1663(0.1615) | Xent 2.3029(2.3041) | Xent Color 2.3016(2.3033) | Loss 1.2468(1.7697) | Error 0.8878(0.8951) | Error Color 0.8978(0.9000) |Steps 452(425.73) | Grad Norm 18.6943(10.3654) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 10.3591(10.1859) | Bit/dim 0.2002(0.1619) | Xent 2.3059(2.3041) | Xent Color 2.3028(2.3033) | Loss 1.2654(1.6179) | Error 0.8989(0.8954) | Error Color 0.8978(0.9002) |Steps 440(427.07) | Grad Norm 10.3218(11.3059) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 9.8033(10.0970) | Bit/dim 0.1284(0.1573) | Xent 2.3049(2.3040) | Xent Color 2.3034(2.3032) | Loss 1.0304(1.4788) | Error 0.8978(0.8959) | Error Color 0.8933(0.8990) |Steps 428(423.73) | Grad Norm 5.1774(10.5555) | Total Time 0.00(0.00)\n",
      "Iter 3860 | Time 9.3691(9.9839) | Bit/dim 0.1137(0.1475) | Xent 2.3018(2.3037) | Xent Color 2.3025(2.3032) | Loss 1.0198(1.3623) | Error 0.8967(0.8942) | Error Color 0.8911(0.8986) |Steps 380(417.47) | Grad Norm 5.4547(9.2660) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 9.3446(9.8241) | Bit/dim 0.1042(0.1377) | Xent 2.3040(2.3036) | Xent Color 2.3022(2.3031) | Loss 0.9460(1.2606) | Error 0.8878(0.8929) | Error Color 0.9011(0.8985) |Steps 392(411.43) | Grad Norm 4.2195(8.1493) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 9.1581(9.6988) | Bit/dim 0.1102(0.1293) | Xent 2.3043(2.3034) | Xent Color 2.3019(2.3030) | Loss 0.9294(1.1829) | Error 0.8944(0.8919) | Error Color 0.8956(0.8978) |Steps 380(406.29) | Grad Norm 5.3894(6.9847) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 9.3707(9.6420) | Bit/dim 0.0951(0.1212) | Xent 2.3082(2.3035) | Xent Color 2.3029(2.3030) | Loss 0.9250(1.1202) | Error 0.8967(0.8930) | Error Color 0.8978(0.8974) |Steps 392(401.55) | Grad Norm 4.6371(6.1379) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 56.7336, Epoch Time 718.5278(711.5028), Bit/dim 0.1004(best: 0.0763), Xent 2.3019, Xent Color 2.3031. Loss 0.1119, Error 0.8886(best: 0.2647), Error Color 0.8999(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3900 | Time 9.6973(9.6310) | Bit/dim 0.0983(0.1149) | Xent 2.2997(2.3035) | Xent Color 2.3025(2.3030) | Loss 0.9503(1.4920) | Error 0.8844(0.8933) | Error Color 0.8978(0.8971) |Steps 386(398.62) | Grad Norm 3.3290(6.2116) | Total Time 0.00(0.00)\n",
      "Iter 3910 | Time 9.3461(9.5790) | Bit/dim 0.1032(0.1121) | Xent 2.3027(2.3031) | Xent Color 2.3029(2.3030) | Loss 0.9841(1.3522) | Error 0.8878(0.8921) | Error Color 0.8956(0.8974) |Steps 398(395.62) | Grad Norm 7.6243(7.3509) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 9.4593(9.5367) | Bit/dim 0.1007(0.1089) | Xent 2.3039(2.3031) | Xent Color 2.3031(2.3031) | Loss 0.9247(1.2443) | Error 0.9011(0.8923) | Error Color 0.9167(0.8983) |Steps 398(393.31) | Grad Norm 5.7650(7.4412) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 9.4934(9.5520) | Bit/dim 0.1597(0.1098) | Xent 2.3030(2.3032) | Xent Color 2.3062(2.3031) | Loss 1.1302(1.1764) | Error 0.8889(0.8921) | Error Color 0.9322(0.9002) |Steps 404(392.20) | Grad Norm 18.9680(8.7625) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 11.2793(9.7228) | Bit/dim 0.3586(0.1718) | Xent 2.3018(2.3030) | Xent Color 2.3014(2.3030) | Loss 1.8690(1.3113) | Error 0.8844(0.8915) | Error Color 0.8922(0.8995) |Steps 506(406.98) | Grad Norm 7.4324(11.3180) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 11.9109(10.2208) | Bit/dim 0.2948(0.2084) | Xent 2.3026(2.3032) | Xent Color 2.3029(2.3030) | Loss 1.6988(1.4354) | Error 0.8911(0.8920) | Error Color 0.9056(0.8987) |Steps 548(437.71) | Grad Norm 6.4122(10.8091) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 10.7999(10.5206) | Bit/dim 0.2039(0.2147) | Xent 2.3058(2.3034) | Xent Color 2.3024(2.3031) | Loss 1.2906(1.4240) | Error 0.8944(0.8918) | Error Color 0.9000(0.8997) |Steps 464(449.63) | Grad Norm 5.5319(9.2731) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 61.8615, Epoch Time 758.4849(712.9123), Bit/dim 0.2004(best: 0.0763), Xent 2.3018, Xent Color 2.3028. Loss 0.2119, Error 0.8894(best: 0.2647), Error Color 0.9004(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 3970 | Time 9.9512(10.4303) | Bit/dim 0.1851(0.2094) | Xent 2.3061(2.3034) | Xent Color 2.3027(2.3031) | Loss 1.1581(1.7783) | Error 0.8956(0.8905) | Error Color 0.8911(0.8999) |Steps 404(442.06) | Grad Norm 3.8301(7.9934) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 10.1150(10.3212) | Bit/dim 0.1576(0.1980) | Xent 2.2996(2.3032) | Xent Color 2.3038(2.3034) | Loss 1.0973(1.6037) | Error 0.8856(0.8906) | Error Color 0.9144(0.9006) |Steps 422(433.26) | Grad Norm 1.7138(6.6997) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 9.8303(10.1843) | Bit/dim 0.1375(0.1839) | Xent 2.3027(2.3035) | Xent Color 2.3007(2.3033) | Loss 1.0279(1.4600) | Error 0.8678(0.8908) | Error Color 0.8733(0.9001) |Steps 392(424.31) | Grad Norm 1.9085(5.5049) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 9.9334(10.0455) | Bit/dim 0.1181(0.1693) | Xent 2.3047(2.3034) | Xent Color 2.3031(2.3032) | Loss 1.0327(1.3453) | Error 0.9111(0.8908) | Error Color 0.9056(0.8997) |Steps 404(417.49) | Grad Norm 5.0066(4.8930) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 10.3021(10.0002) | Bit/dim 0.1133(0.1551) | Xent 2.3073(2.3034) | Xent Color 2.3038(2.3030) | Loss 1.0334(1.2524) | Error 0.9156(0.8912) | Error Color 0.8956(0.8984) |Steps 410(410.97) | Grad Norm 2.3173(4.5975) | Total Time 0.00(0.00)\n",
      "Iter 4020 | Time 10.2374(9.9783) | Bit/dim 0.1128(0.1434) | Xent 2.3033(2.3029) | Xent Color 2.3026(2.3028) | Loss 1.0699(1.1839) | Error 0.8867(0.8901) | Error Color 0.8944(0.8988) |Steps 428(406.37) | Grad Norm 5.1554(4.5621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 58.9825, Epoch Time 734.9370(713.5730), Bit/dim 0.1175(best: 0.0763), Xent 2.3015, Xent Color 2.3026. Loss 0.1290, Error 0.8923(best: 0.2647), Error Color 0.8988(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4030 | Time 10.0399(9.9356) | Bit/dim 0.1131(0.1349) | Xent 2.3038(2.3026) | Xent Color 2.3041(2.3030) | Loss 0.9933(1.6034) | Error 0.8989(0.8907) | Error Color 0.9189(0.9003) |Steps 386(401.99) | Grad Norm 7.1197(5.4325) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 10.7322(9.9112) | Bit/dim 0.1875(0.1485) | Xent 2.3018(2.3029) | Xent Color 2.3016(2.3030) | Loss 1.1769(1.4898) | Error 0.8956(0.8910) | Error Color 0.8978(0.9004) |Steps 428(403.38) | Grad Norm 10.7338(8.7326) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 9.6969(9.9841) | Bit/dim 0.1267(0.1491) | Xent 2.3044(2.3030) | Xent Color 2.3037(2.3031) | Loss 1.0083(1.3840) | Error 0.8978(0.8905) | Error Color 0.8956(0.8999) |Steps 386(406.78) | Grad Norm 3.9391(8.6916) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 10.1458(9.9314) | Bit/dim 0.1156(0.1415) | Xent 2.3083(2.3031) | Xent Color 2.3008(2.3030) | Loss 0.9923(1.2798) | Error 0.9167(0.8915) | Error Color 0.8989(0.9003) |Steps 398(404.14) | Grad Norm 3.5430(7.7847) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 9.7058(9.8671) | Bit/dim 0.0999(0.1319) | Xent 2.3051(2.3029) | Xent Color 2.3011(2.3032) | Loss 0.9319(1.1912) | Error 0.9200(0.8922) | Error Color 0.8867(0.9020) |Steps 374(400.20) | Grad Norm 3.2001(6.5677) | Total Time 0.00(0.00)\n",
      "Iter 4080 | Time 9.5952(9.8233) | Bit/dim 0.0938(0.1228) | Xent 2.3040(2.3032) | Xent Color 2.3034(2.3030) | Loss 0.9055(1.1235) | Error 0.8911(0.8921) | Error Color 0.8967(0.9008) |Steps 386(397.06) | Grad Norm 3.8604(5.7584) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 9.8028(9.8099) | Bit/dim 0.0865(0.1152) | Xent 2.3008(2.3030) | Xent Color 2.3033(2.3031) | Loss 0.8921(1.0667) | Error 0.8856(0.8911) | Error Color 0.9056(0.9007) |Steps 386(394.45) | Grad Norm 2.6041(5.0970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 56.3047, Epoch Time 728.2185(714.0124), Bit/dim 0.0915(best: 0.0763), Xent 2.3017, Xent Color 2.3033. Loss 0.1030, Error 0.8915(best: 0.2647), Error Color 0.9004(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4100 | Time 9.8001(9.8072) | Bit/dim 0.0930(0.1092) | Xent 2.3031(2.3030) | Xent Color 2.3045(2.3032) | Loss 0.9601(1.4123) | Error 0.8822(0.8931) | Error Color 0.9089(0.9014) |Steps 410(394.82) | Grad Norm 4.2503(4.7577) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 9.5944(9.8055) | Bit/dim 0.1235(0.1068) | Xent 2.3029(2.3028) | Xent Color 2.3019(2.3033) | Loss 0.9533(1.2850) | Error 0.8822(0.8930) | Error Color 0.9022(0.9017) |Steps 386(394.06) | Grad Norm 16.2695(5.9768) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 11.3537(9.9549) | Bit/dim 0.4416(0.1406) | Xent 2.3052(2.3031) | Xent Color 2.3026(2.3031) | Loss 1.7815(1.2833) | Error 0.9000(0.8936) | Error Color 0.8956(0.9013) |Steps 494(403.58) | Grad Norm 15.5898(10.1287) | Total Time 0.00(0.00)\n",
      "Iter 4130 | Time 11.2503(10.3568) | Bit/dim 0.2100(0.1729) | Xent 2.3015(2.3031) | Xent Color 2.3044(2.3031) | Loss 1.4140(1.3531) | Error 0.8967(0.8943) | Error Color 0.9033(0.9006) |Steps 470(428.36) | Grad Norm 5.7388(9.7894) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 11.2551(10.6001) | Bit/dim 0.1700(0.1774) | Xent 2.3067(2.3034) | Xent Color 2.3017(2.3030) | Loss 1.2485(1.3407) | Error 0.9100(0.8958) | Error Color 0.8922(0.9000) |Steps 470(444.11) | Grad Norm 3.2546(8.4226) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 10.9243(10.7082) | Bit/dim 0.1483(0.1725) | Xent 2.3004(2.3032) | Xent Color 2.3009(2.3028) | Loss 1.1984(1.3077) | Error 0.8956(0.8957) | Error Color 0.8789(0.8998) |Steps 470(452.09) | Grad Norm 4.2963(7.1468) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 62.7567, Epoch Time 788.7654(716.2550), Bit/dim 0.1352(best: 0.0763), Xent 2.3015, Xent Color 2.3027. Loss 0.1467, Error 0.8900(best: 0.2647), Error Color 0.8961(best: 0.7919)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 4160 | Time 10.2576(10.6766) | Bit/dim 0.1347(0.1640) | Xent 2.3064(2.3030) | Xent Color 2.3038(2.3030) | Loss 1.0514(1.7936) | Error 0.9044(0.8950) | Error Color 0.8967(0.9004) |Steps 440(450.45) | Grad Norm 3.3192(6.0160) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 10.9525(10.7034) | Bit/dim 0.1252(0.1545) | Xent 2.3075(2.3033) | Xent Color 2.3034(2.3030) | Loss 1.0837(1.6078) | Error 0.8922(0.8948) | Error Color 0.8944(0.9006) |Steps 440(449.51) | Grad Norm 3.9749(5.3586) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 10.4788(10.6519) | Bit/dim 0.1173(0.1457) | Xent 2.3006(2.3028) | Xent Color 2.3030(2.3030) | Loss 1.0851(1.4708) | Error 0.8933(0.8931) | Error Color 0.9044(0.9011) |Steps 428(447.57) | Grad Norm 7.1144(5.0394) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_2cond.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_25_drop_0_5_rl_stdscale_6_2cond_small_classification_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.005 --condition_ratio 0.25 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
