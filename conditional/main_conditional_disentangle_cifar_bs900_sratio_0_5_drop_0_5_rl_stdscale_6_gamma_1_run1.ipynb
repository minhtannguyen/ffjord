{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.95, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_gamma_0_95_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.5393(29.6735) | Bit/dim 8.6907(8.9519) | Xent 2.2806(2.3001) | Loss 18.7241(19.3405) | Error 0.8044(0.8607) Steps 0(0.00) | Grad Norm 19.8229(25.6435) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 11.8133(25.0309) | Bit/dim 8.4870(8.8630) | Xent 2.2267(2.2874) | Loss 18.5248(19.1593) | Error 0.7256(0.8329) Steps 0(0.00) | Grad Norm 8.3320(22.2027) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 11.5905(21.5817) | Bit/dim 8.3921(8.7514) | Xent 2.1741(2.2634) | Loss 17.9171(18.9301) | Error 0.7633(0.8096) Steps 0(0.00) | Grad Norm 8.1127(18.2325) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.1794(19.1036) | Bit/dim 8.1926(8.6233) | Xent 2.1138(2.2347) | Loss 17.9613(18.6818) | Error 0.7267(0.7914) Steps 0(0.00) | Grad Norm 5.1801(14.9650) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 11.9126(17.3050) | Bit/dim 7.9751(8.4738) | Xent 2.1012(2.2021) | Loss 17.3910(18.3777) | Error 0.7067(0.7757) Steps 0(0.00) | Grad Norm 5.0245(12.4287) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 72.2978, Epoch Time 778.6192(778.6192), Bit/dim 7.7700(best: inf), Xent 2.0785, Loss 8.8093, Error 0.7009(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 13.1495(16.0364) | Bit/dim 7.6554(8.2923) | Xent 2.0894(2.1718) | Loss 16.8035(18.5026) | Error 0.7078(0.7593) Steps 0(0.00) | Grad Norm 4.6231(10.4759) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 12.4557(15.1425) | Bit/dim 7.3664(8.0808) | Xent 2.0789(2.1454) | Loss 16.1478(17.9617) | Error 0.6978(0.7420) Steps 0(0.00) | Grad Norm 3.9558(8.8551) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 12.5962(14.5239) | Bit/dim 7.1818(7.8642) | Xent 2.0827(2.1249) | Loss 15.9673(17.4703) | Error 0.6711(0.7267) Steps 0(0.00) | Grad Norm 2.8827(7.3988) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.1580(14.0838) | Bit/dim 7.0766(7.6684) | Xent 2.0836(2.1141) | Loss 15.8108(17.0325) | Error 0.6867(0.7192) Steps 0(0.00) | Grad Norm 2.2192(6.0766) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.3953(13.8538) | Bit/dim 7.0201(7.5058) | Xent 2.0817(2.1033) | Loss 15.5902(16.6771) | Error 0.7056(0.7145) Steps 0(0.00) | Grad Norm 2.4319(5.0102) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.0459(13.7363) | Bit/dim 6.9997(7.3757) | Xent 2.0660(2.0948) | Loss 15.6811(16.4175) | Error 0.7233(0.7139) Steps 0(0.00) | Grad Norm 5.8756(4.5369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 72.5243, Epoch Time 801.9341(779.3186), Bit/dim 6.9919(best: 7.7700), Xent 2.0576, Loss 8.0207, Error 0.6956(best: 0.7009)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 13.5650(13.6879) | Bit/dim 6.9378(7.2706) | Xent 2.0470(2.0852) | Loss 15.5173(16.6430) | Error 0.6811(0.7119) Steps 0(0.00) | Grad Norm 3.3622(4.4490) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 13.4500(13.6821) | Bit/dim 6.9079(7.1832) | Xent 2.0653(2.0761) | Loss 15.4834(16.3449) | Error 0.7167(0.7093) Steps 0(0.00) | Grad Norm 4.5784(4.1149) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 14.4220(13.7408) | Bit/dim 6.8742(7.1068) | Xent 2.0530(2.0704) | Loss 15.3774(16.0942) | Error 0.6933(0.7056) Steps 0(0.00) | Grad Norm 13.3149(4.7362) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 14.5792(13.7837) | Bit/dim 6.8473(7.0376) | Xent 1.9909(2.0587) | Loss 15.4387(15.8832) | Error 0.6756(0.7007) Steps 0(0.00) | Grad Norm 1.9537(4.8755) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 13.9267(13.8665) | Bit/dim 6.7209(6.9634) | Xent 1.9767(2.0479) | Loss 15.0774(15.6811) | Error 0.6844(0.6961) Steps 0(0.00) | Grad Norm 12.5357(5.6915) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 74.5833, Epoch Time 852.8107(781.5234), Bit/dim 6.6384(best: 6.9919), Xent 2.0144, Loss 7.6456, Error 0.6808(best: 0.6956)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 13.6945(13.8691) | Bit/dim 6.5883(6.8815) | Xent 2.0402(2.0457) | Loss 14.8391(16.0064) | Error 0.6900(0.6979) Steps 0(0.00) | Grad Norm 32.9904(11.8964) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.0503(13.8087) | Bit/dim 6.4199(6.7772) | Xent 2.0290(2.0419) | Loss 14.6009(15.6531) | Error 0.7111(0.7004) Steps 0(0.00) | Grad Norm 32.0401(18.3032) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 14.1206(13.8641) | Bit/dim 6.2364(6.6566) | Xent 2.0343(2.0506) | Loss 14.2684(15.3324) | Error 0.7167(0.7102) Steps 0(0.00) | Grad Norm 35.3551(31.6548) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.9509(13.8460) | Bit/dim 6.1354(6.5270) | Xent 2.0392(2.0628) | Loss 13.8782(15.0071) | Error 0.7367(0.7222) Steps 0(0.00) | Grad Norm 73.0726(42.7745) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.4610(13.7440) | Bit/dim 5.9383(6.3872) | Xent 2.0426(2.0663) | Loss 13.5848(14.6847) | Error 0.7300(0.7231) Steps 0(0.00) | Grad Norm 56.1742(42.9680) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 13.8069(13.7256) | Bit/dim 5.8213(6.2516) | Xent 2.0175(2.0600) | Loss 13.4224(14.3819) | Error 0.6844(0.7182) Steps 0(0.00) | Grad Norm 37.6911(41.6282) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 75.3282, Epoch Time 848.6643(783.5376), Bit/dim 5.8143(best: 6.6384), Xent 2.0178, Loss 6.8232, Error 0.6819(best: 0.6808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.4578(13.7033) | Bit/dim 5.8047(6.1304) | Xent 1.9881(2.0468) | Loss 13.2259(14.5744) | Error 0.6744(0.7102) Steps 0(0.00) | Grad Norm 63.4937(40.6668) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.7868(13.6727) | Bit/dim 5.7045(6.0305) | Xent 1.9669(2.0360) | Loss 13.2128(14.2477) | Error 0.6189(0.7037) Steps 0(0.00) | Grad Norm 28.1355(42.3924) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.7789(13.5992) | Bit/dim 5.6517(5.9410) | Xent 1.9229(2.0231) | Loss 13.0006(13.9448) | Error 0.6689(0.6979) Steps 0(0.00) | Grad Norm 16.6417(38.6778) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.5065(13.6305) | Bit/dim 5.6836(5.8665) | Xent 2.0566(2.0138) | Loss 13.0165(13.7215) | Error 0.7100(0.6943) Steps 0(0.00) | Grad Norm 42.5391(37.6975) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.2802(13.6497) | Bit/dim 5.6417(5.8076) | Xent 2.0825(2.0105) | Loss 13.1870(13.5555) | Error 0.7667(0.6939) Steps 0(0.00) | Grad Norm 89.9692(38.5277) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 75.3115, Epoch Time 841.1182(785.2650), Bit/dim 5.6171(best: 5.8143), Xent 1.9628, Loss 6.5985, Error 0.6730(best: 0.6808)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.5322(13.5708) | Bit/dim 5.6133(5.7604) | Xent 2.0293(2.0090) | Loss 13.0690(13.9600) | Error 0.7367(0.6989) Steps 0(0.00) | Grad Norm 59.7158(40.8291) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.6424(13.5461) | Bit/dim 5.5708(5.7143) | Xent 1.9335(2.0040) | Loss 12.9301(13.6934) | Error 0.6800(0.6956) Steps 0(0.00) | Grad Norm 24.4153(38.1625) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 12.4817(13.4607) | Bit/dim 5.5734(5.6720) | Xent 1.9216(1.9883) | Loss 12.6733(13.4738) | Error 0.6633(0.6883) Steps 0(0.00) | Grad Norm 7.4925(32.2499) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 12.7373(13.4652) | Bit/dim 5.5521(5.6379) | Xent 1.9408(1.9738) | Loss 12.8830(13.3059) | Error 0.6567(0.6814) Steps 0(0.00) | Grad Norm 37.0339(29.2525) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.3684(13.4724) | Bit/dim 5.5626(5.6106) | Xent 2.0111(1.9737) | Loss 12.8530(13.1816) | Error 0.7122(0.6826) Steps 0(0.00) | Grad Norm 52.9432(32.6402) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 14.0376(13.5222) | Bit/dim 5.5379(5.5882) | Xent 2.0970(2.0071) | Loss 13.0784(13.1334) | Error 0.7556(0.6992) Steps 0(0.00) | Grad Norm 30.6587(36.7985) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 74.2523, Epoch Time 831.9473(786.6655), Bit/dim 5.4804(best: 5.6171), Xent 2.0375, Loss 6.4992, Error 0.7135(best: 0.6730)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.5956(13.5138) | Bit/dim 5.4353(5.5551) | Xent 1.9938(2.0102) | Loss 12.7384(13.4903) | Error 0.6933(0.6995) Steps 0(0.00) | Grad Norm 9.2174(31.3813) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.7523(13.5406) | Bit/dim 5.3635(5.5205) | Xent 1.9728(1.9995) | Loss 12.6216(13.2716) | Error 0.6622(0.6943) Steps 0(0.00) | Grad Norm 6.0733(25.3806) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 13.7808(13.5943) | Bit/dim 5.3392(5.4776) | Xent 1.9431(1.9804) | Loss 12.5676(13.0742) | Error 0.6544(0.6851) Steps 0(0.00) | Grad Norm 16.3810(21.2549) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.3126(13.7252) | Bit/dim 5.3292(5.4472) | Xent 1.9219(1.9680) | Loss 12.4685(12.9399) | Error 0.6478(0.6825) Steps 0(0.00) | Grad Norm 17.4123(23.5868) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.9551(13.8535) | Bit/dim 5.3455(5.4097) | Xent 1.8834(1.9545) | Loss 12.5494(12.8197) | Error 0.6633(0.6774) Steps 0(0.00) | Grad Norm 21.5142(24.4946) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 77.9640, Epoch Time 857.5314(788.7915), Bit/dim 5.2571(best: 5.4804), Xent 1.8826, Loss 6.1984, Error 0.6381(best: 0.6730)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.7736(13.9588) | Bit/dim 5.2917(5.3726) | Xent 1.9399(1.9389) | Loss 12.4072(13.2557) | Error 0.6600(0.6736) Steps 0(0.00) | Grad Norm 16.1541(23.6427) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 13.7744(14.0040) | Bit/dim 5.2381(5.3397) | Xent 1.9203(1.9310) | Loss 12.2126(13.0044) | Error 0.6622(0.6705) Steps 0(0.00) | Grad Norm 22.9975(25.2236) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 14.3333(14.0567) | Bit/dim 5.1874(5.3022) | Xent 1.9363(1.9244) | Loss 12.1267(12.8008) | Error 0.6811(0.6688) Steps 0(0.00) | Grad Norm 24.5025(25.5243) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 13.8481(14.0691) | Bit/dim 5.1639(5.2718) | Xent 1.9506(1.9170) | Loss 12.1933(12.6340) | Error 0.6833(0.6680) Steps 0(0.00) | Grad Norm 32.1194(24.8553) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.0938(14.1160) | Bit/dim 5.1380(5.2396) | Xent 1.9240(1.9225) | Loss 12.1540(12.5147) | Error 0.6756(0.6721) Steps 0(0.00) | Grad Norm 26.7165(25.3022) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 14.2744(14.1771) | Bit/dim 5.1135(5.2085) | Xent 1.8754(1.9147) | Loss 12.0358(12.3850) | Error 0.6522(0.6703) Steps 0(0.00) | Grad Norm 25.5558(25.5522) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 79.4120, Epoch Time 879.8091(791.5220), Bit/dim 5.0982(best: 5.2571), Xent 1.8493, Loss 6.0228, Error 0.6278(best: 0.6381)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.7220(14.2930) | Bit/dim 5.0781(5.1790) | Xent 1.8852(1.9055) | Loss 12.0836(12.7523) | Error 0.6722(0.6666) Steps 0(0.00) | Grad Norm 37.1048(25.1406) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.3943(14.4760) | Bit/dim 5.0579(5.1488) | Xent 1.8858(1.9093) | Loss 11.9115(12.5623) | Error 0.6600(0.6691) Steps 0(0.00) | Grad Norm 17.2773(26.6406) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 15.0253(14.6166) | Bit/dim 5.0499(5.1152) | Xent 1.8931(1.9004) | Loss 11.9772(12.3778) | Error 0.6456(0.6656) Steps 0(0.00) | Grad Norm 19.7205(23.3650) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 14.0576(14.6677) | Bit/dim 5.0629(5.0901) | Xent 1.9136(1.8925) | Loss 12.0309(12.2379) | Error 0.6667(0.6625) Steps 0(0.00) | Grad Norm 54.1196(26.0517) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 14.3937(14.6758) | Bit/dim 4.9767(5.0659) | Xent 1.9210(1.9071) | Loss 11.7912(12.1442) | Error 0.6644(0.6669) Steps 0(0.00) | Grad Norm 28.7211(31.8843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 79.6497, Epoch Time 915.0854(795.2289), Bit/dim 4.9915(best: 5.0982), Xent 1.8922, Loss 5.9376, Error 0.6510(best: 0.6278)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.0573(14.7552) | Bit/dim 4.9107(5.0365) | Xent 1.8693(1.9079) | Loss 11.6454(12.5926) | Error 0.6722(0.6694) Steps 0(0.00) | Grad Norm 17.9051(30.2419) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 15.0851(14.8044) | Bit/dim 4.8947(5.0080) | Xent 1.8478(1.9000) | Loss 11.5401(12.3536) | Error 0.6278(0.6662) Steps 0(0.00) | Grad Norm 17.9690(28.8872) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.8405(14.8037) | Bit/dim 4.9139(4.9804) | Xent 1.8708(1.8888) | Loss 11.6903(12.1573) | Error 0.6567(0.6609) Steps 0(0.00) | Grad Norm 39.0680(27.9049) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 15.2249(14.8678) | Bit/dim 4.8535(4.9564) | Xent 1.8265(1.8770) | Loss 11.3508(12.0067) | Error 0.6667(0.6585) Steps 0(0.00) | Grad Norm 13.0540(28.5067) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.3638(14.9540) | Bit/dim 4.8972(4.9341) | Xent 1.9499(1.8676) | Loss 11.7287(11.8923) | Error 0.6800(0.6552) Steps 0(0.00) | Grad Norm 69.2495(31.2229) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 15.5465(14.9867) | Bit/dim 4.8624(4.9271) | Xent 1.9391(1.8834) | Loss 11.6252(11.8428) | Error 0.6867(0.6610) Steps 0(0.00) | Grad Norm 27.2292(34.1746) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 80.2116, Epoch Time 922.7568(799.0547), Bit/dim 4.8418(best: 4.9915), Xent 1.8716, Loss 5.7776, Error 0.6614(best: 0.6278)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 15.9535(15.0751) | Bit/dim 4.8394(4.9048) | Xent 1.8430(1.8850) | Loss 11.5093(12.2383) | Error 0.6700(0.6624) Steps 0(0.00) | Grad Norm 11.4952(30.2801) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 16.3730(15.1475) | Bit/dim 4.9220(4.8926) | Xent 1.9514(1.8822) | Loss 11.7732(12.0725) | Error 0.6889(0.6613) Steps 0(0.00) | Grad Norm 61.8669(30.7264) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 14.9358(15.1477) | Bit/dim 4.8588(4.8822) | Xent 1.8357(1.8745) | Loss 11.4680(11.9242) | Error 0.6489(0.6590) Steps 0(0.00) | Grad Norm 23.1721(28.6021) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.1850(15.2024) | Bit/dim 4.7319(4.8598) | Xent 1.8417(1.8567) | Loss 11.2326(11.7823) | Error 0.6567(0.6535) Steps 0(0.00) | Grad Norm 23.4730(26.4324) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 15.9135(15.2814) | Bit/dim 4.7515(4.8367) | Xent 1.8136(1.8414) | Loss 11.3875(11.6641) | Error 0.6456(0.6501) Steps 0(0.00) | Grad Norm 27.2616(26.7369) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 80.8838, Epoch Time 943.3793(803.3845), Bit/dim 4.7386(best: 4.8418), Xent 1.7133, Loss 5.5953, Error 0.6060(best: 0.6278)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 15.8248(15.3683) | Bit/dim 4.7779(4.8138) | Xent 1.7442(1.8234) | Loss 11.4081(12.1127) | Error 0.6267(0.6444) Steps 0(0.00) | Grad Norm 21.7106(25.6522) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 16.0757(15.4389) | Bit/dim 4.8257(4.8278) | Xent 1.9381(1.8346) | Loss 11.6337(11.9820) | Error 0.7000(0.6486) Steps 0(0.00) | Grad Norm 30.5206(30.6506) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 14.8913(15.4192) | Bit/dim 4.7919(4.8287) | Xent 1.7905(1.8395) | Loss 11.4302(11.8584) | Error 0.6433(0.6503) Steps 0(0.00) | Grad Norm 13.0299(28.2768) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 15.5159(15.4796) | Bit/dim 4.7281(4.8094) | Xent 1.8441(1.8359) | Loss 11.4518(11.7220) | Error 0.6533(0.6492) Steps 0(0.00) | Grad Norm 9.3773(23.8500) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.4500(15.5239) | Bit/dim 4.6979(4.7814) | Xent 1.7622(1.8139) | Loss 11.1608(11.5792) | Error 0.6222(0.6427) Steps 0(0.00) | Grad Norm 14.4655(20.2819) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.4435(15.5612) | Bit/dim 4.6852(4.7574) | Xent 1.7543(1.7947) | Loss 11.2238(11.4736) | Error 0.6233(0.6360) Steps 0(0.00) | Grad Norm 15.4322(20.9262) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 83.1647, Epoch Time 959.2023(808.0590), Bit/dim 4.6781(best: 4.7386), Xent 1.6688, Loss 5.5125, Error 0.5822(best: 0.6060)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 16.2076(15.6416) | Bit/dim 4.6866(4.7339) | Xent 1.6706(1.7684) | Loss 11.0599(11.8349) | Error 0.6022(0.6278) Steps 0(0.00) | Grad Norm 28.6683(21.1533) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.0811(15.6180) | Bit/dim 4.7475(4.7301) | Xent 1.7377(1.7640) | Loss 11.1896(11.6752) | Error 0.6156(0.6268) Steps 0(0.00) | Grad Norm 31.6997(25.6405) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.2705(15.6486) | Bit/dim 4.6517(4.7148) | Xent 1.9028(1.7807) | Loss 11.1407(11.5543) | Error 0.6778(0.6346) Steps 0(0.00) | Grad Norm 38.4811(27.3126) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.1209(15.6600) | Bit/dim 4.6749(4.7001) | Xent 1.7556(1.7753) | Loss 11.2584(11.4472) | Error 0.6489(0.6335) Steps 0(0.00) | Grad Norm 23.2789(26.2284) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 16.0401(15.6708) | Bit/dim 4.5964(4.6809) | Xent 1.6804(1.7591) | Loss 11.0149(11.3235) | Error 0.5933(0.6275) Steps 0(0.00) | Grad Norm 14.0648(23.5641) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 83.5392, Epoch Time 966.0257(812.7980), Bit/dim 4.6620(best: 4.6781), Xent 1.6303, Loss 5.4772, Error 0.5876(best: 0.5822)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.5059(15.7082) | Bit/dim 4.6581(4.6719) | Xent 1.8473(1.7539) | Loss 11.4018(11.8265) | Error 0.6622(0.6268) Steps 0(0.00) | Grad Norm 41.8372(25.3581) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 15.9040(15.7447) | Bit/dim 4.6103(4.6578) | Xent 1.7119(1.7471) | Loss 11.0387(11.6137) | Error 0.6233(0.6250) Steps 0(0.00) | Grad Norm 11.6593(24.0989) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.8236(15.7861) | Bit/dim 4.5897(4.6423) | Xent 1.7931(1.7408) | Loss 11.0338(11.4452) | Error 0.6444(0.6220) Steps 0(0.00) | Grad Norm 34.6902(23.1206) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.2020(15.8217) | Bit/dim 4.5657(4.6324) | Xent 1.7338(1.7377) | Loss 10.9479(11.3332) | Error 0.6244(0.6208) Steps 0(0.00) | Grad Norm 16.3316(25.8217) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 15.7695(15.9275) | Bit/dim 4.6130(4.6169) | Xent 1.7622(1.7280) | Loss 11.1440(11.2348) | Error 0.6256(0.6172) Steps 0(0.00) | Grad Norm 33.2961(24.0928) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 16.0964(15.9293) | Bit/dim 4.6387(4.6136) | Xent 1.7186(1.7239) | Loss 11.1092(11.1774) | Error 0.6056(0.6168) Steps 0(0.00) | Grad Norm 36.5323(24.9149) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 83.9031, Epoch Time 979.0726(817.7863), Bit/dim 4.5602(best: 4.6620), Xent 1.6049, Loss 5.3626, Error 0.5710(best: 0.5822)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 15.2440(15.8792) | Bit/dim 4.5246(4.5985) | Xent 1.6008(1.7000) | Loss 10.6103(11.5723) | Error 0.5756(0.6080) Steps 0(0.00) | Grad Norm 20.6716(22.3683) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 16.1769(15.9031) | Bit/dim 4.5371(4.5808) | Xent 1.6646(1.6905) | Loss 10.8062(11.3745) | Error 0.5944(0.6048) Steps 0(0.00) | Grad Norm 31.5870(21.5823) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 16.5787(15.9559) | Bit/dim 4.5450(4.5667) | Xent 1.7591(1.6922) | Loss 10.9288(11.2361) | Error 0.6389(0.6067) Steps 0(0.00) | Grad Norm 34.9622(23.3632) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.7703(16.0211) | Bit/dim 4.5077(4.5550) | Xent 1.6136(1.6880) | Loss 10.7344(11.1257) | Error 0.5611(0.6060) Steps 0(0.00) | Grad Norm 13.6476(22.7845) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 15.5020(16.0612) | Bit/dim 4.5259(4.5455) | Xent 1.6810(1.6769) | Loss 10.8492(11.0439) | Error 0.6044(0.6015) Steps 0(0.00) | Grad Norm 18.2035(21.0698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 85.9833, Epoch Time 985.5904(822.8204), Bit/dim 4.4824(best: 4.5602), Xent 1.5420, Loss 5.2533, Error 0.5494(best: 0.5710)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 15.7445(16.0207) | Bit/dim 4.5015(4.5294) | Xent 1.6082(1.6590) | Loss 10.5897(11.5392) | Error 0.5744(0.5972) Steps 0(0.00) | Grad Norm 15.2371(19.0956) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 16.2377(16.0505) | Bit/dim 4.4678(4.5190) | Xent 1.6241(1.6524) | Loss 10.8050(11.3288) | Error 0.5800(0.5938) Steps 0(0.00) | Grad Norm 25.7683(20.3950) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 17.0616(16.0807) | Bit/dim 4.5513(4.5110) | Xent 1.7147(1.6548) | Loss 11.0056(11.1754) | Error 0.6122(0.5969) Steps 0(0.00) | Grad Norm 54.8916(22.4247) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 16.4563(16.0855) | Bit/dim 4.4461(4.5095) | Xent 1.6028(1.6621) | Loss 10.5936(11.0709) | Error 0.5600(0.5972) Steps 0(0.00) | Grad Norm 17.4162(23.0800) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.1435(16.0361) | Bit/dim 4.4795(4.4990) | Xent 1.6345(1.6552) | Loss 10.7405(10.9720) | Error 0.5911(0.5950) Steps 0(0.00) | Grad Norm 20.2140(22.0980) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 15.6711(16.0442) | Bit/dim 4.4648(4.4857) | Xent 1.6109(1.6496) | Loss 10.6135(10.8740) | Error 0.6000(0.5938) Steps 0(0.00) | Grad Norm 27.9398(20.9981) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 85.4275, Epoch Time 986.4918(827.7305), Bit/dim 4.4745(best: 4.4824), Xent 1.5288, Loss 5.2388, Error 0.5470(best: 0.5494)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.3720(16.0880) | Bit/dim 4.4515(4.4723) | Xent 1.6252(1.6347) | Loss 10.7342(11.2937) | Error 0.6100(0.5884) Steps 0(0.00) | Grad Norm 10.6615(18.8379) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 15.7667(16.0965) | Bit/dim 4.3957(4.4560) | Xent 1.5924(1.6268) | Loss 10.3496(11.0992) | Error 0.5489(0.5857) Steps 0(0.00) | Grad Norm 17.8062(18.0743) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 16.0742(16.1772) | Bit/dim 4.4061(4.4494) | Xent 1.6203(1.6259) | Loss 10.4408(10.9689) | Error 0.5833(0.5844) Steps 0(0.00) | Grad Norm 26.0849(19.4693) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.3676(16.2346) | Bit/dim 4.4169(4.4398) | Xent 1.6066(1.6289) | Loss 10.5692(10.8632) | Error 0.5822(0.5877) Steps 0(0.00) | Grad Norm 21.0049(19.5054) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 16.0751(16.2451) | Bit/dim 4.4086(4.4269) | Xent 1.5902(1.6269) | Loss 10.5358(10.7795) | Error 0.5889(0.5876) Steps 0(0.00) | Grad Norm 10.4788(18.5885) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 83.0664, Epoch Time 994.4334(832.7316), Bit/dim 4.3851(best: 4.4745), Xent 1.5303, Loss 5.1502, Error 0.5459(best: 0.5470)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.8688(16.2641) | Bit/dim 4.4536(4.4162) | Xent 1.6133(1.6178) | Loss 10.6107(11.2702) | Error 0.5722(0.5826) Steps 0(0.00) | Grad Norm 27.7824(19.2806) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 15.9358(16.2279) | Bit/dim 4.3825(4.4064) | Xent 1.5478(1.6083) | Loss 10.4641(11.0580) | Error 0.5589(0.5803) Steps 0(0.00) | Grad Norm 15.0412(18.1525) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.5047(16.1877) | Bit/dim 4.3678(4.3955) | Xent 1.6239(1.6057) | Loss 10.4368(10.8942) | Error 0.5733(0.5798) Steps 0(0.00) | Grad Norm 25.3033(18.7702) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.4756(16.1532) | Bit/dim 4.3765(4.3832) | Xent 1.6753(1.6081) | Loss 10.4608(10.7690) | Error 0.6111(0.5830) Steps 0(0.00) | Grad Norm 24.8128(18.9508) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.6903(16.1863) | Bit/dim 4.3342(4.3749) | Xent 1.5755(1.6035) | Loss 10.4080(10.6722) | Error 0.5778(0.5809) Steps 0(0.00) | Grad Norm 14.6435(19.2066) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 16.6472(16.1946) | Bit/dim 4.3196(4.3656) | Xent 1.5480(1.5935) | Loss 10.2755(10.5830) | Error 0.5678(0.5772) Steps 0(0.00) | Grad Norm 10.8548(18.0640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 83.8753, Epoch Time 992.9740(837.5389), Bit/dim 4.3161(best: 4.3851), Xent 1.4687, Loss 5.0504, Error 0.5295(best: 0.5459)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.5436(16.2016) | Bit/dim 4.2908(4.3499) | Xent 1.5799(1.5783) | Loss 10.2798(10.9738) | Error 0.5678(0.5706) Steps 0(0.00) | Grad Norm 8.2351(16.6486) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 17.1275(16.3423) | Bit/dim 4.2946(4.3340) | Xent 1.5524(1.5659) | Loss 10.2393(10.7770) | Error 0.5600(0.5671) Steps 0(0.00) | Grad Norm 17.4305(16.1075) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 16.6792(16.3239) | Bit/dim 4.2482(4.3206) | Xent 1.5351(1.5579) | Loss 10.0995(10.6350) | Error 0.5600(0.5640) Steps 0(0.00) | Grad Norm 15.8433(15.3141) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.3911(16.2990) | Bit/dim 4.3067(4.3127) | Xent 1.6690(1.5574) | Loss 10.3277(10.5317) | Error 0.6067(0.5649) Steps 0(0.00) | Grad Norm 20.6350(16.3842) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 15.5802(16.2661) | Bit/dim 4.2803(4.3011) | Xent 1.5354(1.5509) | Loss 10.1673(10.4427) | Error 0.5622(0.5622) Steps 0(0.00) | Grad Norm 15.8130(16.6365) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 84.0730, Epoch Time 1000.0607(842.4145), Bit/dim 4.2526(best: 4.3161), Xent 1.4335, Loss 4.9694, Error 0.5155(best: 0.5295)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 16.5385(16.2697) | Bit/dim 4.2189(4.2905) | Xent 1.4645(1.5421) | Loss 10.0192(10.9111) | Error 0.5322(0.5598) Steps 0(0.00) | Grad Norm 5.7048(16.0409) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 17.2625(16.3351) | Bit/dim 4.2627(4.2798) | Xent 1.5171(1.5235) | Loss 10.1974(10.7046) | Error 0.5733(0.5539) Steps 0(0.00) | Grad Norm 26.5743(15.4608) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 15.9872(16.3148) | Bit/dim 4.2147(4.2680) | Xent 1.5588(1.5262) | Loss 10.0390(10.5545) | Error 0.5767(0.5540) Steps 0(0.00) | Grad Norm 17.5640(16.3649) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 16.0901(16.2519) | Bit/dim 4.2500(4.2607) | Xent 1.4999(1.5197) | Loss 10.0920(10.4282) | Error 0.5300(0.5507) Steps 0(0.00) | Grad Norm 17.1234(16.1052) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 17.5104(16.3630) | Bit/dim 4.2187(4.2546) | Xent 1.5574(1.5280) | Loss 10.2254(10.3673) | Error 0.5667(0.5542) Steps 0(0.00) | Grad Norm 18.9132(16.6864) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 15.9585(16.3365) | Bit/dim 4.1835(4.2439) | Xent 1.4232(1.5115) | Loss 9.8448(10.2718) | Error 0.5233(0.5491) Steps 0(0.00) | Grad Norm 10.3570(15.5558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 83.7647, Epoch Time 999.8804(847.1385), Bit/dim 4.2433(best: 4.2526), Xent 1.4066, Loss 4.9465, Error 0.5129(best: 0.5155)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 17.1591(16.2745) | Bit/dim 4.1871(4.2338) | Xent 1.4674(1.4996) | Loss 10.0696(10.6842) | Error 0.5356(0.5435) Steps 0(0.00) | Grad Norm 9.5499(14.7271) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.8814(16.2462) | Bit/dim 4.1965(4.2237) | Xent 1.4135(1.4873) | Loss 9.8640(10.4892) | Error 0.5211(0.5392) Steps 0(0.00) | Grad Norm 8.7642(13.6784) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 16.1451(16.2182) | Bit/dim 4.1633(4.2122) | Xent 1.6139(1.4761) | Loss 10.1047(10.3481) | Error 0.5689(0.5366) Steps 0(0.00) | Grad Norm 35.0013(14.4949) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 16.0361(16.2005) | Bit/dim 4.2011(4.2053) | Xent 1.4260(1.4719) | Loss 9.9361(10.2457) | Error 0.5067(0.5345) Steps 0(0.00) | Grad Norm 11.6136(14.4553) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 16.2086(16.1219) | Bit/dim 4.1538(4.2000) | Xent 1.4230(1.4665) | Loss 9.9047(10.1586) | Error 0.5289(0.5313) Steps 0(0.00) | Grad Norm 8.2370(14.3734) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 82.8149, Epoch Time 986.6178(851.3229), Bit/dim 4.1680(best: 4.2433), Xent 1.3805, Loss 4.8583, Error 0.4976(best: 0.5129)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.0308(16.1488) | Bit/dim 4.1635(4.1923) | Xent 1.4502(1.4638) | Loss 9.8927(10.6543) | Error 0.5178(0.5305) Steps 0(0.00) | Grad Norm 23.3119(14.5486) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.6412(16.1919) | Bit/dim 4.1477(4.1865) | Xent 1.4325(1.4514) | Loss 9.8016(10.4533) | Error 0.5122(0.5253) Steps 0(0.00) | Grad Norm 15.9516(13.9685) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.6917(16.1624) | Bit/dim 4.1543(4.1773) | Xent 1.4040(1.4427) | Loss 9.8881(10.3051) | Error 0.4767(0.5209) Steps 0(0.00) | Grad Norm 14.7560(14.1031) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 16.4911(16.1953) | Bit/dim 4.1524(4.1674) | Xent 1.4420(1.4374) | Loss 9.9169(10.1873) | Error 0.5244(0.5195) Steps 0(0.00) | Grad Norm 11.8806(13.6211) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 15.5142(16.0481) | Bit/dim 4.1109(4.1541) | Xent 1.4477(1.4300) | Loss 9.7430(10.0721) | Error 0.5311(0.5176) Steps 0(0.00) | Grad Norm 9.6679(13.8302) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 16.0522(16.0494) | Bit/dim 4.1632(4.1499) | Xent 1.4426(1.4353) | Loss 9.8574(10.0093) | Error 0.5244(0.5195) Steps 0(0.00) | Grad Norm 14.6892(14.1353) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 82.7479, Epoch Time 985.8741(855.3594), Bit/dim 4.1315(best: 4.1680), Xent 1.3201, Loss 4.7915, Error 0.4774(best: 0.4976)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 15.7201(15.9287) | Bit/dim 4.1428(4.1429) | Xent 1.3807(1.4201) | Loss 9.8472(10.4129) | Error 0.5033(0.5152) Steps 0(0.00) | Grad Norm 8.1398(12.6819) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 14.8808(15.8061) | Bit/dim 4.1203(4.1384) | Xent 1.3521(1.4189) | Loss 9.7006(10.2448) | Error 0.4967(0.5138) Steps 0(0.00) | Grad Norm 6.6446(13.9114) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.2733(15.6847) | Bit/dim 4.1032(4.1293) | Xent 1.3469(1.4105) | Loss 9.7157(10.1032) | Error 0.4933(0.5104) Steps 0(0.00) | Grad Norm 12.7114(12.7188) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 15.8793(15.7515) | Bit/dim 4.1346(4.1227) | Xent 1.3020(1.3979) | Loss 9.5881(10.0021) | Error 0.4756(0.5071) Steps 0(0.00) | Grad Norm 10.5447(12.1139) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 15.8734(15.7846) | Bit/dim 4.1048(4.1151) | Xent 1.3479(1.3856) | Loss 9.7598(9.9281) | Error 0.4856(0.5031) Steps 0(0.00) | Grad Norm 8.7339(11.7761) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 80.0980, Epoch Time 958.3274(858.4485), Bit/dim 4.0812(best: 4.1315), Xent 1.2997, Loss 4.7310, Error 0.4703(best: 0.4774)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 15.6358(15.7303) | Bit/dim 4.0825(4.1083) | Xent 1.4254(1.3830) | Loss 9.7637(10.4239) | Error 0.5133(0.5029) Steps 0(0.00) | Grad Norm 15.8857(12.0886) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 15.2833(15.7353) | Bit/dim 4.0791(4.1018) | Xent 1.3365(1.3790) | Loss 9.5654(10.2269) | Error 0.4900(0.5028) Steps 0(0.00) | Grad Norm 12.9252(12.6102) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.9324(15.7477) | Bit/dim 4.1008(4.0985) | Xent 1.3334(1.3706) | Loss 9.6794(10.0869) | Error 0.4867(0.4972) Steps 0(0.00) | Grad Norm 10.4150(12.2585) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 15.9061(15.6709) | Bit/dim 4.0725(4.0911) | Xent 1.2973(1.3643) | Loss 9.6050(9.9573) | Error 0.4844(0.4948) Steps 0(0.00) | Grad Norm 6.7768(11.6392) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.5997(15.6980) | Bit/dim 4.1006(4.0844) | Xent 1.4608(1.3635) | Loss 9.8015(9.8694) | Error 0.5233(0.4947) Steps 0(0.00) | Grad Norm 21.5846(12.1984) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 15.5868(15.7514) | Bit/dim 4.0820(4.0801) | Xent 1.3512(1.3595) | Loss 9.7650(9.8106) | Error 0.4600(0.4931) Steps 0(0.00) | Grad Norm 18.8650(12.7151) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 80.1793, Epoch Time 962.7354(861.5771), Bit/dim 4.0660(best: 4.0812), Xent 1.2762, Loss 4.7041, Error 0.4605(best: 0.4703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 14.9055(15.7249) | Bit/dim 4.0555(4.0743) | Xent 1.3728(1.3516) | Loss 9.5889(10.2151) | Error 0.4811(0.4896) Steps 0(0.00) | Grad Norm 18.0645(13.1220) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.0452(15.6608) | Bit/dim 4.0314(4.0658) | Xent 1.3869(1.3488) | Loss 9.6681(10.0401) | Error 0.5100(0.4886) Steps 0(0.00) | Grad Norm 6.7874(12.5906) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 15.6971(15.6204) | Bit/dim 4.0563(4.0667) | Xent 1.3540(1.3428) | Loss 9.5684(9.9287) | Error 0.4900(0.4852) Steps 0(0.00) | Grad Norm 6.1295(12.7618) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 14.7028(15.5248) | Bit/dim 4.0404(4.0592) | Xent 1.3578(1.3418) | Loss 9.5235(9.8273) | Error 0.4844(0.4834) Steps 0(0.00) | Grad Norm 12.2114(12.4529) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 15.5869(15.5015) | Bit/dim 4.0227(4.0568) | Xent 1.3347(1.3322) | Loss 9.5799(9.7480) | Error 0.4856(0.4816) Steps 0(0.00) | Grad Norm 8.4237(12.1620) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 81.0070, Epoch Time 952.4330(864.3028), Bit/dim 4.0581(best: 4.0660), Xent 1.2377, Loss 4.6769, Error 0.4510(best: 0.4605)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.4222(15.6030) | Bit/dim 4.0112(4.0489) | Xent 1.2139(1.3239) | Loss 9.3702(10.2393) | Error 0.4456(0.4799) Steps 0(0.00) | Grad Norm 10.5394(12.3599) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 15.0969(15.6173) | Bit/dim 4.0157(4.0452) | Xent 1.2917(1.3188) | Loss 9.4364(10.0440) | Error 0.4689(0.4764) Steps 0(0.00) | Grad Norm 8.5302(12.3488) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 15.7977(15.6258) | Bit/dim 4.0339(4.0426) | Xent 1.3602(1.3179) | Loss 9.6230(9.9085) | Error 0.4856(0.4768) Steps 0(0.00) | Grad Norm 13.4192(12.6096) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 15.3940(15.5894) | Bit/dim 4.0688(4.0389) | Xent 1.3104(1.3169) | Loss 9.5732(9.8026) | Error 0.4756(0.4766) Steps 0(0.00) | Grad Norm 17.4383(12.6694) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 15.5570(15.5760) | Bit/dim 3.9797(4.0346) | Xent 1.3091(1.3135) | Loss 9.4091(9.7156) | Error 0.4667(0.4744) Steps 0(0.00) | Grad Norm 9.9402(12.8090) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 14.6811(15.5199) | Bit/dim 4.0159(4.0275) | Xent 1.2736(1.3194) | Loss 9.3472(9.6485) | Error 0.4667(0.4758) Steps 0(0.00) | Grad Norm 8.9397(13.3842) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 80.3676, Epoch Time 954.6338(867.0127), Bit/dim 4.0136(best: 4.0581), Xent 1.2164, Loss 4.6218, Error 0.4372(best: 0.4510)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.4617(15.5120) | Bit/dim 4.0077(4.0224) | Xent 1.2740(1.3076) | Loss 9.5778(10.0660) | Error 0.4389(0.4711) Steps 0(0.00) | Grad Norm 7.6976(12.0193) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.2643(15.5411) | Bit/dim 4.0238(4.0157) | Xent 1.3222(1.2952) | Loss 9.5486(9.8994) | Error 0.5022(0.4674) Steps 0(0.00) | Grad Norm 9.8401(10.8305) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 15.1947(15.5097) | Bit/dim 3.9772(4.0112) | Xent 1.2946(1.2940) | Loss 9.3215(9.7687) | Error 0.4511(0.4664) Steps 0(0.00) | Grad Norm 20.7338(12.0424) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 16.0141(15.4806) | Bit/dim 3.9857(4.0087) | Xent 1.3100(1.2920) | Loss 9.4336(9.6745) | Error 0.4900(0.4668) Steps 0(0.00) | Grad Norm 10.1138(11.8675) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 14.9809(15.4058) | Bit/dim 4.0113(4.0042) | Xent 1.3720(1.2859) | Loss 9.4831(9.5989) | Error 0.5056(0.4654) Steps 0(0.00) | Grad Norm 9.9128(10.8361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 80.9493, Epoch Time 948.7627(869.4652), Bit/dim 3.9850(best: 4.0136), Xent 1.1865, Loss 4.5783, Error 0.4257(best: 0.4372)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 15.5537(15.4730) | Bit/dim 3.9450(4.0004) | Xent 1.2111(1.2722) | Loss 9.2578(10.0794) | Error 0.4422(0.4608) Steps 0(0.00) | Grad Norm 10.4614(10.3636) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 16.0881(15.4824) | Bit/dim 3.9930(3.9967) | Xent 1.1961(1.2657) | Loss 9.3518(9.8932) | Error 0.4200(0.4577) Steps 0(0.00) | Grad Norm 5.2838(10.3046) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 15.3395(15.4430) | Bit/dim 4.0223(3.9934) | Xent 1.2319(1.2615) | Loss 9.3620(9.7513) | Error 0.4567(0.4553) Steps 0(0.00) | Grad Norm 11.2321(10.0621) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 14.3127(15.4284) | Bit/dim 3.9795(3.9895) | Xent 1.2753(1.2583) | Loss 9.3597(9.6476) | Error 0.4544(0.4542) Steps 0(0.00) | Grad Norm 6.6703(10.0328) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.1319(15.3746) | Bit/dim 3.9760(3.9848) | Xent 1.2371(1.2538) | Loss 9.4324(9.5635) | Error 0.4478(0.4525) Steps 0(0.00) | Grad Norm 11.0126(9.6093) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.1279(15.3678) | Bit/dim 3.9468(3.9788) | Xent 1.2169(1.2497) | Loss 9.1117(9.4919) | Error 0.4433(0.4506) Steps 0(0.00) | Grad Norm 8.0597(9.6630) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 79.5802, Epoch Time 943.5287(871.6871), Bit/dim 3.9603(best: 3.9850), Xent 1.1772, Loss 4.5489, Error 0.4228(best: 0.4257)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.2148(15.4497) | Bit/dim 3.9678(3.9778) | Xent 1.1811(1.2486) | Loss 9.3104(9.9141) | Error 0.4356(0.4515) Steps 0(0.00) | Grad Norm 7.1009(11.1328) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.6682(15.5220) | Bit/dim 3.9415(3.9763) | Xent 1.2429(1.2419) | Loss 9.1964(9.7548) | Error 0.4411(0.4483) Steps 0(0.00) | Grad Norm 7.2036(11.3186) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.1888(15.4974) | Bit/dim 3.9672(3.9707) | Xent 1.2258(1.2334) | Loss 9.2038(9.6300) | Error 0.4444(0.4453) Steps 0(0.00) | Grad Norm 10.4414(10.7842) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 16.0529(15.5150) | Bit/dim 3.9531(3.9687) | Xent 1.2733(1.2341) | Loss 9.3907(9.5516) | Error 0.4611(0.4452) Steps 0(0.00) | Grad Norm 9.4102(11.1090) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.5921(15.4969) | Bit/dim 3.9729(3.9661) | Xent 1.2533(1.2287) | Loss 9.2496(9.4792) | Error 0.4478(0.4438) Steps 0(0.00) | Grad Norm 17.0490(10.8703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 80.4604, Epoch Time 953.4134(874.1389), Bit/dim 3.9479(best: 3.9603), Xent 1.1445, Loss 4.5201, Error 0.4076(best: 0.4228)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.2833(15.4162) | Bit/dim 3.9699(3.9633) | Xent 1.1996(1.2200) | Loss 9.2880(9.9527) | Error 0.4256(0.4408) Steps 0(0.00) | Grad Norm 6.4730(10.5760) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 15.1797(15.4200) | Bit/dim 3.9443(3.9571) | Xent 1.2078(1.2113) | Loss 9.2301(9.7587) | Error 0.4356(0.4358) Steps 0(0.00) | Grad Norm 12.7690(9.9325) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 14.9540(15.4237) | Bit/dim 3.9556(3.9535) | Xent 1.1249(1.2051) | Loss 9.1686(9.6203) | Error 0.4067(0.4345) Steps 0(0.00) | Grad Norm 7.9725(9.7002) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.6787(15.4500) | Bit/dim 3.9165(3.9483) | Xent 1.2294(1.2000) | Loss 9.2591(9.5151) | Error 0.4489(0.4326) Steps 0(0.00) | Grad Norm 8.1469(10.2783) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 14.8989(15.4257) | Bit/dim 3.9522(3.9511) | Xent 1.1953(1.2082) | Loss 9.2850(9.4444) | Error 0.4344(0.4335) Steps 0(0.00) | Grad Norm 9.1865(11.3693) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.2375(15.3692) | Bit/dim 3.9480(3.9472) | Xent 1.2263(1.2148) | Loss 9.3459(9.3958) | Error 0.4367(0.4376) Steps 0(0.00) | Grad Norm 10.7545(11.5961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 80.9873, Epoch Time 944.4013(876.2468), Bit/dim 3.9406(best: 3.9479), Xent 1.1582, Loss 4.5198, Error 0.4128(best: 0.4076)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 15.2813(15.3815) | Bit/dim 3.9451(3.9454) | Xent 1.1733(1.2075) | Loss 9.2280(9.7977) | Error 0.4356(0.4356) Steps 0(0.00) | Grad Norm 8.6520(11.2365) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 16.0380(15.3639) | Bit/dim 3.9512(3.9441) | Xent 1.1701(1.2011) | Loss 9.2600(9.6384) | Error 0.4200(0.4314) Steps 0(0.00) | Grad Norm 8.4965(11.7923) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.2974(15.3387) | Bit/dim 3.9163(3.9411) | Xent 1.1534(1.1930) | Loss 9.0871(9.5035) | Error 0.4011(0.4284) Steps 0(0.00) | Grad Norm 9.6369(10.9729) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 14.6144(15.4086) | Bit/dim 3.9245(3.9385) | Xent 1.1571(1.1901) | Loss 9.0350(9.4233) | Error 0.4167(0.4273) Steps 0(0.00) | Grad Norm 8.9357(11.0328) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 14.6441(15.4007) | Bit/dim 3.9552(3.9341) | Xent 1.1908(1.1867) | Loss 9.2874(9.3566) | Error 0.4289(0.4257) Steps 0(0.00) | Grad Norm 11.0373(11.1023) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 80.6534, Epoch Time 946.2735(878.3476), Bit/dim 3.9185(best: 3.9406), Xent 1.0776, Loss 4.4573, Error 0.3877(best: 0.4076)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.2167(15.4007) | Bit/dim 3.9387(3.9299) | Xent 1.1511(1.1776) | Loss 9.1990(9.8466) | Error 0.4144(0.4237) Steps 0(0.00) | Grad Norm 12.5331(10.2269) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.0781(15.4306) | Bit/dim 3.9405(3.9280) | Xent 1.0846(1.1639) | Loss 9.0881(9.6525) | Error 0.3800(0.4185) Steps 0(0.00) | Grad Norm 8.8151(10.4009) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 16.1214(15.4804) | Bit/dim 3.9136(3.9233) | Xent 1.0815(1.1526) | Loss 9.0870(9.5195) | Error 0.3867(0.4154) Steps 0(0.00) | Grad Norm 12.4525(10.2638) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.0807(15.4663) | Bit/dim 3.8992(3.9209) | Xent 1.2036(1.1575) | Loss 9.1733(9.4319) | Error 0.4489(0.4161) Steps 0(0.00) | Grad Norm 23.6746(11.0139) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.3579(15.4528) | Bit/dim 3.9142(3.9200) | Xent 1.1378(1.1530) | Loss 9.0693(9.3570) | Error 0.3956(0.4140) Steps 0(0.00) | Grad Norm 12.0359(11.2406) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 15.2838(15.4431) | Bit/dim 3.9148(3.9181) | Xent 1.1411(1.1490) | Loss 8.9604(9.2875) | Error 0.4056(0.4127) Steps 0(0.00) | Grad Norm 9.3665(10.9384) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 80.0261, Epoch Time 947.8378(880.4323), Bit/dim 3.9073(best: 3.9185), Xent 1.0734, Loss 4.4440, Error 0.3819(best: 0.3877)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 15.5061(15.4122) | Bit/dim 3.9315(3.9195) | Xent 1.1543(1.1426) | Loss 9.1617(9.7012) | Error 0.3956(0.4098) Steps 0(0.00) | Grad Norm 9.8349(10.6066) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 15.8386(15.4788) | Bit/dim 3.8811(3.9128) | Xent 1.1334(1.1345) | Loss 9.0826(9.5342) | Error 0.3900(0.4072) Steps 0(0.00) | Grad Norm 7.4611(9.9454) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.6351(15.4678) | Bit/dim 3.8664(3.9083) | Xent 1.1152(1.1356) | Loss 9.0298(9.4176) | Error 0.4156(0.4082) Steps 0(0.00) | Grad Norm 9.5090(10.3597) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 14.9176(15.3853) | Bit/dim 3.8852(3.9080) | Xent 1.1034(1.1412) | Loss 9.0610(9.3381) | Error 0.3944(0.4102) Steps 0(0.00) | Grad Norm 16.9878(11.6755) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.0327(15.3864) | Bit/dim 3.9055(3.9066) | Xent 1.1171(1.1365) | Loss 9.0200(9.2726) | Error 0.3789(0.4085) Steps 0(0.00) | Grad Norm 8.4659(10.7897) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 79.0722, Epoch Time 942.8068(882.3035), Bit/dim 3.9044(best: 3.9073), Xent 1.0732, Loss 4.4410, Error 0.3816(best: 0.3819)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.2627(15.3506) | Bit/dim 3.9187(3.9071) | Xent 1.0372(1.1295) | Loss 9.0023(9.7347) | Error 0.3600(0.4060) Steps 0(0.00) | Grad Norm 14.6297(10.7703) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 15.8277(15.3612) | Bit/dim 3.8721(3.9056) | Xent 1.0757(1.1188) | Loss 9.0584(9.5588) | Error 0.3911(0.4007) Steps 0(0.00) | Grad Norm 10.4221(10.0680) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 15.2931(15.3771) | Bit/dim 3.8835(3.9013) | Xent 1.1056(1.1250) | Loss 8.9327(9.4307) | Error 0.3667(0.4006) Steps 0(0.00) | Grad Norm 14.0558(10.4984) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 15.4205(15.4488) | Bit/dim 3.9014(3.8989) | Xent 1.1442(1.1281) | Loss 9.0659(9.3398) | Error 0.4133(0.4023) Steps 0(0.00) | Grad Norm 9.1743(10.8566) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.8633(15.4951) | Bit/dim 3.8631(3.8950) | Xent 1.1214(1.1219) | Loss 9.0221(9.2599) | Error 0.4067(0.4009) Steps 0(0.00) | Grad Norm 6.2308(10.4804) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 14.7961(15.5026) | Bit/dim 3.8699(3.8932) | Xent 1.1397(1.1149) | Loss 9.0868(9.1996) | Error 0.4011(0.3976) Steps 0(0.00) | Grad Norm 7.7407(10.7699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 80.0110, Epoch Time 950.6315(884.3533), Bit/dim 3.8905(best: 3.9044), Xent 1.0895, Loss 4.4353, Error 0.3897(best: 0.3816)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 15.3796(15.4609) | Bit/dim 3.8922(3.8929) | Xent 1.1994(1.1149) | Loss 9.2018(9.5998) | Error 0.4311(0.3989) Steps 0(0.00) | Grad Norm 14.7568(11.3473) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.6004(15.4681) | Bit/dim 3.8531(3.8872) | Xent 1.0126(1.1034) | Loss 8.8677(9.4327) | Error 0.3633(0.3952) Steps 0(0.00) | Grad Norm 10.5161(10.6179) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 14.8907(15.4512) | Bit/dim 3.8759(3.8870) | Xent 1.1313(1.0984) | Loss 8.9296(9.3178) | Error 0.3911(0.3936) Steps 0(0.00) | Grad Norm 16.5804(10.4237) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.4789(15.4608) | Bit/dim 3.8974(3.8842) | Xent 1.0805(1.0987) | Loss 8.9669(9.2360) | Error 0.3822(0.3945) Steps 0(0.00) | Grad Norm 15.6035(11.0357) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 15.0582(15.4188) | Bit/dim 3.8923(3.8863) | Xent 1.2013(1.1103) | Loss 9.0743(9.1902) | Error 0.4211(0.3961) Steps 0(0.00) | Grad Norm 7.9227(11.4267) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 79.4818, Epoch Time 944.0567(886.1444), Bit/dim 3.8769(best: 3.8905), Xent 1.0242, Loss 4.3890, Error 0.3645(best: 0.3816)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 14.4989(15.3485) | Bit/dim 3.8559(3.8857) | Xent 1.0865(1.1034) | Loss 8.9645(9.6630) | Error 0.3800(0.3938) Steps 0(0.00) | Grad Norm 5.5887(10.7062) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.5974(15.3753) | Bit/dim 3.8654(3.8842) | Xent 1.1239(1.0898) | Loss 8.9661(9.4712) | Error 0.3933(0.3888) Steps 0(0.00) | Grad Norm 10.5562(10.5504) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 16.1412(15.4429) | Bit/dim 3.8756(3.8817) | Xent 1.0575(1.0870) | Loss 9.0272(9.3411) | Error 0.3744(0.3863) Steps 0(0.00) | Grad Norm 11.4560(10.6600) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 15.3240(15.4051) | Bit/dim 3.8651(3.8789) | Xent 1.1263(1.0895) | Loss 9.0318(9.2604) | Error 0.3944(0.3889) Steps 0(0.00) | Grad Norm 6.8970(10.2412) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 15.7776(15.4875) | Bit/dim 3.8345(3.8749) | Xent 1.0166(1.0760) | Loss 8.8877(9.1830) | Error 0.3611(0.3847) Steps 0(0.00) | Grad Norm 7.5708(10.5697) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.1581(15.4911) | Bit/dim 3.8346(3.8713) | Xent 1.0715(1.0818) | Loss 8.9680(9.1360) | Error 0.3778(0.3853) Steps 0(0.00) | Grad Norm 9.7464(10.4586) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 79.1796, Epoch Time 948.4933(888.0149), Bit/dim 3.8758(best: 3.8769), Xent 1.0651, Loss 4.4083, Error 0.3810(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 15.5810(15.4659) | Bit/dim 3.8816(3.8698) | Xent 1.0819(1.0794) | Loss 9.0367(9.5563) | Error 0.3744(0.3854) Steps 0(0.00) | Grad Norm 14.9397(10.7099) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 15.5335(15.4378) | Bit/dim 3.8754(3.8660) | Xent 1.0247(1.0729) | Loss 9.0475(9.3959) | Error 0.3744(0.3838) Steps 0(0.00) | Grad Norm 6.6830(10.1346) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.5217(15.4418) | Bit/dim 3.9122(3.8667) | Xent 1.0475(1.0599) | Loss 9.0412(9.2751) | Error 0.3678(0.3777) Steps 0(0.00) | Grad Norm 8.1918(9.5865) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 14.9614(15.4080) | Bit/dim 3.8405(3.8638) | Xent 1.0587(1.0511) | Loss 8.8616(9.1759) | Error 0.3800(0.3747) Steps 0(0.00) | Grad Norm 7.3303(8.8641) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.8742(15.4419) | Bit/dim 3.8935(3.8651) | Xent 1.1037(1.0545) | Loss 9.1206(9.1258) | Error 0.3967(0.3762) Steps 0(0.00) | Grad Norm 16.5298(9.1268) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 80.8539, Epoch Time 945.9884(889.7541), Bit/dim 3.9038(best: 3.8758), Xent 1.1091, Loss 4.4584, Error 0.4020(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 15.8733(15.4464) | Bit/dim 3.8592(3.8675) | Xent 1.1124(1.0758) | Loss 9.0198(9.6418) | Error 0.3789(0.3836) Steps 0(0.00) | Grad Norm 7.4541(10.3949) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 16.4008(15.5174) | Bit/dim 3.8618(3.8642) | Xent 1.0283(1.0697) | Loss 9.0884(9.4647) | Error 0.3356(0.3810) Steps 0(0.00) | Grad Norm 9.1265(9.9943) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.1012(15.4427) | Bit/dim 3.8732(3.8641) | Xent 1.0759(1.0586) | Loss 9.0975(9.3294) | Error 0.3789(0.3767) Steps 0(0.00) | Grad Norm 12.5363(9.8205) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.7126(15.4737) | Bit/dim 3.8358(3.8591) | Xent 1.0235(1.0523) | Loss 8.8168(9.2176) | Error 0.3722(0.3746) Steps 0(0.00) | Grad Norm 6.4166(8.8865) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.3504(15.3783) | Bit/dim 3.8650(3.8560) | Xent 1.0082(1.0494) | Loss 8.8172(9.1239) | Error 0.3711(0.3732) Steps 0(0.00) | Grad Norm 9.4345(8.7761) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.5486(15.4513) | Bit/dim 3.8494(3.8516) | Xent 1.0585(1.0423) | Loss 8.9185(9.0645) | Error 0.4000(0.3719) Steps 0(0.00) | Grad Norm 7.6491(8.1182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 80.6636, Epoch Time 948.8341(891.5265), Bit/dim 3.8507(best: 3.8758), Xent 0.9971, Loss 4.3492, Error 0.3576(best: 0.3645)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.6552(15.4850) | Bit/dim 3.8727(3.8544) | Xent 1.0131(1.0353) | Loss 8.9676(9.4823) | Error 0.3556(0.3695) Steps 0(0.00) | Grad Norm 11.3384(8.8362) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 16.1111(15.4512) | Bit/dim 3.8643(3.8513) | Xent 1.0410(1.0341) | Loss 8.9755(9.3321) | Error 0.3933(0.3700) Steps 0(0.00) | Grad Norm 5.1711(9.3669) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 14.9108(15.4113) | Bit/dim 3.8369(3.8489) | Xent 1.0336(1.0307) | Loss 8.8360(9.2123) | Error 0.3567(0.3677) Steps 0(0.00) | Grad Norm 10.0603(9.1184) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.3152(15.4051) | Bit/dim 3.8252(3.8441) | Xent 0.9884(1.0294) | Loss 8.7910(9.1244) | Error 0.3578(0.3670) Steps 0(0.00) | Grad Norm 8.2769(8.7124) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.9394(15.3876) | Bit/dim 3.8475(3.8436) | Xent 1.0610(1.0352) | Loss 8.8430(9.0630) | Error 0.3878(0.3697) Steps 0(0.00) | Grad Norm 18.0702(10.4347) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 79.4393, Epoch Time 942.8615(893.0666), Bit/dim 3.8450(best: 3.8507), Xent 0.9923, Loss 4.3411, Error 0.3569(best: 0.3576)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.2498(15.3369) | Bit/dim 3.8334(3.8452) | Xent 1.0374(1.0320) | Loss 8.8619(9.5241) | Error 0.3667(0.3698) Steps 0(0.00) | Grad Norm 7.9275(10.4585) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.5835(15.3364) | Bit/dim 3.8612(3.8493) | Xent 1.0166(1.0326) | Loss 8.7933(9.3647) | Error 0.3656(0.3705) Steps 0(0.00) | Grad Norm 5.7370(10.5916) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 15.0717(15.3085) | Bit/dim 3.8596(3.8474) | Xent 0.9817(1.0273) | Loss 8.9664(9.2432) | Error 0.3644(0.3689) Steps 0(0.00) | Grad Norm 10.8208(10.2773) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.1865(15.3531) | Bit/dim 3.8524(3.8462) | Xent 0.9847(1.0243) | Loss 8.8183(9.1565) | Error 0.3467(0.3671) Steps 0(0.00) | Grad Norm 8.7611(10.1068) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 16.3210(15.3834) | Bit/dim 3.8328(3.8435) | Xent 1.0046(1.0189) | Loss 8.9679(9.0804) | Error 0.3744(0.3651) Steps 0(0.00) | Grad Norm 6.4836(9.9831) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.4595(15.4933) | Bit/dim 3.8022(3.8395) | Xent 1.0869(1.0242) | Loss 8.9507(9.0329) | Error 0.3800(0.3660) Steps 0(0.00) | Grad Norm 12.0082(9.9396) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 81.3167, Epoch Time 948.4283(894.7274), Bit/dim 3.8459(best: 3.8450), Xent 1.0386, Loss 4.3652, Error 0.3717(best: 0.3569)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.6864(15.5642) | Bit/dim 3.8261(3.8410) | Xent 0.9975(1.0354) | Loss 8.8427(9.4667) | Error 0.3567(0.3699) Steps 0(0.00) | Grad Norm 5.4245(10.6325) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.5941(15.5349) | Bit/dim 3.8344(3.8402) | Xent 1.0963(1.0319) | Loss 8.9801(9.3069) | Error 0.4122(0.3700) Steps 0(0.00) | Grad Norm 15.5302(11.0563) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.5404(15.5445) | Bit/dim 3.8456(3.8393) | Xent 0.9632(1.0344) | Loss 8.8995(9.2065) | Error 0.3478(0.3700) Steps 0(0.00) | Grad Norm 5.2555(11.1018) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.6016(15.4972) | Bit/dim 3.7910(3.8369) | Xent 1.0296(1.0238) | Loss 8.7148(9.1049) | Error 0.3622(0.3658) Steps 0(0.00) | Grad Norm 10.4595(10.1865) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.1687(15.5047) | Bit/dim 3.8229(3.8325) | Xent 1.0134(1.0196) | Loss 8.8910(9.0425) | Error 0.3700(0.3638) Steps 0(0.00) | Grad Norm 8.3086(10.1983) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 80.5118, Epoch Time 954.8238(896.5303), Bit/dim 3.8305(best: 3.8450), Xent 0.9683, Loss 4.3147, Error 0.3450(best: 0.3569)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.2444(15.4787) | Bit/dim 3.8573(3.8334) | Xent 0.9557(1.0138) | Loss 8.8810(9.5275) | Error 0.3456(0.3626) Steps 0(0.00) | Grad Norm 6.5552(10.3168) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.7294(15.5140) | Bit/dim 3.8538(3.8320) | Xent 1.0282(1.0126) | Loss 8.9652(9.3424) | Error 0.3533(0.3622) Steps 0(0.00) | Grad Norm 12.8953(10.2685) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 15.5649(15.4642) | Bit/dim 3.8439(3.8278) | Xent 1.0204(1.0170) | Loss 8.9437(9.2143) | Error 0.3622(0.3632) Steps 0(0.00) | Grad Norm 11.9274(10.8641) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.7682(15.4386) | Bit/dim 3.8448(3.8274) | Xent 1.0560(1.0112) | Loss 9.0362(9.1128) | Error 0.3678(0.3614) Steps 0(0.00) | Grad Norm 12.6364(10.8840) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 15.0568(15.4334) | Bit/dim 3.8094(3.8270) | Xent 1.0602(1.0281) | Loss 8.8484(9.0625) | Error 0.3667(0.3659) Steps 0(0.00) | Grad Norm 10.4712(11.1548) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 15.2905(15.3795) | Bit/dim 3.8421(3.8300) | Xent 1.0613(1.0250) | Loss 8.9218(9.0161) | Error 0.3811(0.3646) Steps 0(0.00) | Grad Norm 10.6938(10.5894) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 79.5019, Epoch Time 941.9729(897.8936), Bit/dim 3.8296(best: 3.8305), Xent 1.0097, Loss 4.3344, Error 0.3588(best: 0.3450)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 15.0161(15.3976) | Bit/dim 3.8160(3.8255) | Xent 0.9803(1.0148) | Loss 8.7915(9.4019) | Error 0.3356(0.3623) Steps 0(0.00) | Grad Norm 4.7012(9.9859) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 15.6171(15.4099) | Bit/dim 3.8138(3.8246) | Xent 0.9812(1.0030) | Loss 8.7444(9.2405) | Error 0.3389(0.3567) Steps 0(0.00) | Grad Norm 4.7295(9.2755) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.0601(15.4265) | Bit/dim 3.7904(3.8227) | Xent 0.9423(0.9914) | Loss 8.8116(9.1271) | Error 0.3400(0.3531) Steps 0(0.00) | Grad Norm 8.7376(8.9218) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 14.7718(15.4155) | Bit/dim 3.8288(3.8226) | Xent 1.0358(0.9968) | Loss 8.8143(9.0524) | Error 0.3589(0.3541) Steps 0(0.00) | Grad Norm 16.9132(9.4450) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.2645(15.4231) | Bit/dim 3.8244(3.8181) | Xent 1.0759(1.0070) | Loss 8.8553(8.9927) | Error 0.3756(0.3576) Steps 0(0.00) | Grad Norm 11.0654(9.7690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 80.4180, Epoch Time 947.6137(899.3852), Bit/dim 3.8302(best: 3.8296), Xent 0.9594, Loss 4.3099, Error 0.3407(best: 0.3450)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.6371(15.4448) | Bit/dim 3.8041(3.8219) | Xent 0.9716(1.0129) | Loss 8.7399(9.5106) | Error 0.3456(0.3595) Steps 0(0.00) | Grad Norm 5.3426(10.7643) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.6962(15.4130) | Bit/dim 3.8216(3.8194) | Xent 1.0210(1.0066) | Loss 8.8641(9.3249) | Error 0.3700(0.3587) Steps 0(0.00) | Grad Norm 14.1939(10.2851) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 14.9031(15.3389) | Bit/dim 3.8252(3.8180) | Xent 0.9514(0.9978) | Loss 8.8857(9.1887) | Error 0.3500(0.3563) Steps 0(0.00) | Grad Norm 7.8277(9.6295) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 15.6750(15.3962) | Bit/dim 3.8259(3.8199) | Xent 1.0069(0.9909) | Loss 8.8419(9.0954) | Error 0.3689(0.3545) Steps 0(0.00) | Grad Norm 5.6983(9.5021) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.1775(15.4091) | Bit/dim 3.8005(3.8133) | Xent 1.0088(0.9860) | Loss 8.8317(9.0109) | Error 0.3422(0.3518) Steps 0(0.00) | Grad Norm 9.5344(8.9263) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.5607(15.4553) | Bit/dim 3.8221(3.8129) | Xent 1.0400(0.9851) | Loss 8.8802(8.9662) | Error 0.3767(0.3516) Steps 0(0.00) | Grad Norm 9.4225(9.2519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 80.1775, Epoch Time 946.6352(900.8027), Bit/dim 3.8155(best: 3.8296), Xent 0.9451, Loss 4.2881, Error 0.3354(best: 0.3407)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 15.2794(15.4406) | Bit/dim 3.8059(3.8140) | Xent 0.9483(0.9733) | Loss 8.7736(9.3667) | Error 0.3422(0.3492) Steps 0(0.00) | Grad Norm 6.1606(8.9403) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 15.7694(15.4431) | Bit/dim 3.8370(3.8132) | Xent 0.9795(0.9659) | Loss 8.8327(9.2080) | Error 0.3500(0.3461) Steps 0(0.00) | Grad Norm 8.5111(8.6956) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 15.6006(15.4776) | Bit/dim 3.8216(3.8102) | Xent 0.9796(0.9689) | Loss 8.7859(9.1040) | Error 0.3511(0.3464) Steps 0(0.00) | Grad Norm 8.1502(9.0449) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 15.4382(15.4803) | Bit/dim 3.8379(3.8087) | Xent 1.0777(0.9788) | Loss 8.9363(9.0218) | Error 0.4011(0.3516) Steps 0(0.00) | Grad Norm 9.4067(9.9907) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 16.0089(15.4405) | Bit/dim 3.8049(3.8123) | Xent 1.1138(0.9932) | Loss 8.9354(8.9739) | Error 0.4144(0.3562) Steps 0(0.00) | Grad Norm 14.2767(11.1172) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 81.9017, Epoch Time 950.9401(902.3068), Bit/dim 3.8044(best: 3.8155), Xent 0.9981, Loss 4.3034, Error 0.3564(best: 0.3354)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 15.4726(15.4564) | Bit/dim 3.7993(3.8120) | Xent 0.9340(0.9945) | Loss 8.7855(9.5064) | Error 0.3400(0.3554) Steps 0(0.00) | Grad Norm 6.0052(10.4948) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.4089(15.3938) | Bit/dim 3.8370(3.8123) | Xent 0.9697(0.9847) | Loss 8.8974(9.3146) | Error 0.3478(0.3529) Steps 0(0.00) | Grad Norm 9.1398(10.2580) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.5062(15.4677) | Bit/dim 3.8086(3.8130) | Xent 0.9866(0.9821) | Loss 8.7542(9.1864) | Error 0.3511(0.3529) Steps 0(0.00) | Grad Norm 7.3860(10.5371) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 14.9426(15.4323) | Bit/dim 3.7845(3.8089) | Xent 0.9770(0.9739) | Loss 8.6887(9.0664) | Error 0.3589(0.3493) Steps 0(0.00) | Grad Norm 14.0018(10.1644) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.3530(15.4020) | Bit/dim 3.7315(3.8062) | Xent 1.0101(0.9728) | Loss 8.6035(8.9814) | Error 0.3589(0.3478) Steps 0(0.00) | Grad Norm 9.7079(10.2013) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 14.8746(15.3587) | Bit/dim 3.8091(3.8072) | Xent 0.9814(0.9716) | Loss 8.7479(8.9332) | Error 0.3578(0.3477) Steps 0(0.00) | Grad Norm 11.6689(10.2860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 80.1971, Epoch Time 942.2914(903.5063), Bit/dim 3.8026(best: 3.8044), Xent 0.9828, Loss 4.2941, Error 0.3525(best: 0.3354)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.4993(15.3745) | Bit/dim 3.8223(3.8055) | Xent 0.9014(0.9668) | Loss 8.6515(9.3367) | Error 0.3144(0.3461) Steps 0(0.00) | Grad Norm 9.0014(10.0345) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 15.4259(15.3186) | Bit/dim 3.7746(3.8036) | Xent 0.8952(0.9600) | Loss 8.6303(9.1749) | Error 0.3256(0.3445) Steps 0(0.00) | Grad Norm 10.6418(9.6183) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.6887(15.3821) | Bit/dim 3.7991(3.8015) | Xent 0.9207(0.9567) | Loss 8.6566(9.0622) | Error 0.3411(0.3433) Steps 0(0.00) | Grad Norm 11.5886(9.7876) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 16.0131(15.4257) | Bit/dim 3.7919(3.7994) | Xent 0.8538(0.9535) | Loss 8.7989(8.9812) | Error 0.3222(0.3407) Steps 0(0.00) | Grad Norm 9.4688(9.4596) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.0623(15.4275) | Bit/dim 3.7759(3.7977) | Xent 0.8613(0.9463) | Loss 8.6469(8.9053) | Error 0.3144(0.3371) Steps 0(0.00) | Grad Norm 6.9156(9.3274) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 81.1447, Epoch Time 947.1826(904.8166), Bit/dim 3.7976(best: 3.8026), Xent 0.9052, Loss 4.2501, Error 0.3188(best: 0.3354)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 14.9858(15.3968) | Bit/dim 3.7755(3.7953) | Xent 0.9510(0.9416) | Loss 8.6309(9.3961) | Error 0.3367(0.3359) Steps 0(0.00) | Grad Norm 10.1594(9.2278) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 15.3492(15.3604) | Bit/dim 3.8161(3.7936) | Xent 0.9306(0.9383) | Loss 8.6791(9.2101) | Error 0.3144(0.3325) Steps 0(0.00) | Grad Norm 7.2890(9.0565) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 14.9579(15.3300) | Bit/dim 3.7752(3.7915) | Xent 0.9015(0.9341) | Loss 8.5586(9.0746) | Error 0.3178(0.3313) Steps 0(0.00) | Grad Norm 8.4184(8.9368) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 15.7912(15.3417) | Bit/dim 3.8165(3.7917) | Xent 0.9599(0.9379) | Loss 8.8116(8.9884) | Error 0.3522(0.3346) Steps 0(0.00) | Grad Norm 5.9375(9.2803) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 14.9046(15.3541) | Bit/dim 3.7928(3.7937) | Xent 1.0569(0.9472) | Loss 8.8374(8.9302) | Error 0.3856(0.3373) Steps 0(0.00) | Grad Norm 12.7707(9.5076) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 15.3673(15.3675) | Bit/dim 3.7727(3.7940) | Xent 0.9585(0.9597) | Loss 8.6823(8.8897) | Error 0.3389(0.3423) Steps 0(0.00) | Grad Norm 8.2053(10.2019) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 80.1265, Epoch Time 941.1502(905.9066), Bit/dim 3.8102(best: 3.7976), Xent 0.9214, Loss 4.2709, Error 0.3298(best: 0.3188)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.4345(15.3451) | Bit/dim 3.7556(3.7935) | Xent 0.9359(0.9528) | Loss 8.6810(9.3103) | Error 0.3411(0.3395) Steps 0(0.00) | Grad Norm 11.0441(10.4129) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.9403(15.3550) | Bit/dim 3.7930(3.7940) | Xent 0.8964(0.9431) | Loss 8.6790(9.1582) | Error 0.3144(0.3361) Steps 0(0.00) | Grad Norm 6.0695(9.5470) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 15.6275(15.3353) | Bit/dim 3.8028(3.7913) | Xent 0.9069(0.9371) | Loss 8.7696(9.0443) | Error 0.3167(0.3333) Steps 0(0.00) | Grad Norm 8.4999(9.0532) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 15.1646(15.3625) | Bit/dim 3.7747(3.7926) | Xent 0.8885(0.9313) | Loss 8.6733(8.9673) | Error 0.3089(0.3311) Steps 0(0.00) | Grad Norm 9.2589(9.1306) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 14.7244(15.3271) | Bit/dim 3.8251(3.7923) | Xent 0.9271(0.9300) | Loss 8.7892(8.9067) | Error 0.3144(0.3297) Steps 0(0.00) | Grad Norm 8.2317(9.0941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 80.5085, Epoch Time 941.2277(906.9663), Bit/dim 3.7780(best: 3.7976), Xent 0.9279, Loss 4.2420, Error 0.3347(best: 0.3188)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.1934(15.2921) | Bit/dim 3.8114(3.7884) | Xent 0.8860(0.9251) | Loss 8.7602(9.3837) | Error 0.3156(0.3279) Steps 0(0.00) | Grad Norm 8.0179(9.2127) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.5674(15.3315) | Bit/dim 3.7928(3.7884) | Xent 0.9279(0.9287) | Loss 8.7908(9.2194) | Error 0.3233(0.3289) Steps 0(0.00) | Grad Norm 8.8786(9.4364) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 15.6794(15.3687) | Bit/dim 3.7723(3.7858) | Xent 0.9684(0.9296) | Loss 8.7867(9.0867) | Error 0.3400(0.3313) Steps 0(0.00) | Grad Norm 10.2314(9.6131) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 14.9388(15.2837) | Bit/dim 3.7979(3.7881) | Xent 1.0196(0.9440) | Loss 8.7692(9.0052) | Error 0.3667(0.3363) Steps 0(0.00) | Grad Norm 13.6950(10.2408) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 15.6038(15.3451) | Bit/dim 3.7473(3.7851) | Xent 0.9101(0.9380) | Loss 8.6464(8.9211) | Error 0.3489(0.3357) Steps 0(0.00) | Grad Norm 10.1063(9.8926) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 14.9683(15.3516) | Bit/dim 3.7542(3.7820) | Xent 0.8795(0.9296) | Loss 8.6791(8.8631) | Error 0.3256(0.3324) Steps 0(0.00) | Grad Norm 8.4149(9.2230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 80.1594, Epoch Time 941.9909(908.0170), Bit/dim 3.7836(best: 3.7780), Xent 0.8769, Loss 4.2221, Error 0.3117(best: 0.3188)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 14.9017(15.3419) | Bit/dim 3.7654(3.7802) | Xent 0.8826(0.9119) | Loss 8.6144(9.2805) | Error 0.3078(0.3264) Steps 0(0.00) | Grad Norm 5.9578(8.6553) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.5489(15.3361) | Bit/dim 3.7850(3.7759) | Xent 0.8916(0.9064) | Loss 8.6338(9.1169) | Error 0.3122(0.3239) Steps 0(0.00) | Grad Norm 6.2685(8.2064) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 15.1086(15.4167) | Bit/dim 3.7824(3.7760) | Xent 0.8787(0.9094) | Loss 8.6707(9.0030) | Error 0.2967(0.3249) Steps 0(0.00) | Grad Norm 9.7751(8.3110) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.5173(15.4359) | Bit/dim 3.7720(3.7789) | Xent 0.9647(0.9180) | Loss 8.8108(8.9374) | Error 0.3389(0.3268) Steps 0(0.00) | Grad Norm 11.4243(9.4992) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.2668(15.4279) | Bit/dim 3.7595(3.7786) | Xent 0.8591(0.9122) | Loss 8.7349(8.8716) | Error 0.3078(0.3249) Steps 0(0.00) | Grad Norm 8.8148(9.1970) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 80.5979, Epoch Time 948.6191(909.2351), Bit/dim 3.7774(best: 3.7780), Xent 0.8714, Loss 4.2131, Error 0.3057(best: 0.3117)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.3168(15.4771) | Bit/dim 3.7620(3.7784) | Xent 0.8652(0.9028) | Loss 8.5659(9.3607) | Error 0.3000(0.3200) Steps 0(0.00) | Grad Norm 9.4096(9.0639) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.6221(15.4447) | Bit/dim 3.7630(3.7766) | Xent 0.8231(0.9010) | Loss 8.4879(9.1763) | Error 0.2989(0.3203) Steps 0(0.00) | Grad Norm 9.5976(8.8029) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.2140(15.3911) | Bit/dim 3.7582(3.7758) | Xent 0.9511(0.9063) | Loss 8.5424(9.0418) | Error 0.3400(0.3223) Steps 0(0.00) | Grad Norm 9.1563(9.7481) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.2902(15.3757) | Bit/dim 3.7912(3.7779) | Xent 0.8767(0.9040) | Loss 8.6848(8.9560) | Error 0.3367(0.3217) Steps 0(0.00) | Grad Norm 5.7170(9.4825) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 15.7893(15.4135) | Bit/dim 3.7592(3.7776) | Xent 0.9072(0.9147) | Loss 8.6348(8.8925) | Error 0.3078(0.3257) Steps 0(0.00) | Grad Norm 7.4718(9.9538) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 14.8103(15.3323) | Bit/dim 3.7548(3.7736) | Xent 0.8525(0.9157) | Loss 8.5535(8.8298) | Error 0.3089(0.3266) Steps 0(0.00) | Grad Norm 5.8652(9.4737) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 80.8129, Epoch Time 941.8141(910.2124), Bit/dim 3.7709(best: 3.7774), Xent 0.8783, Loss 4.2101, Error 0.3110(best: 0.3057)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.6918(15.3630) | Bit/dim 3.7589(3.7724) | Xent 0.9371(0.9088) | Loss 8.6845(9.2463) | Error 0.3311(0.3246) Steps 0(0.00) | Grad Norm 9.4667(8.9696) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 14.3862(15.3520) | Bit/dim 3.7759(3.7730) | Xent 0.9401(0.9143) | Loss 8.5716(9.0992) | Error 0.3278(0.3268) Steps 0(0.00) | Grad Norm 7.1446(8.8885) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.0948(15.3046) | Bit/dim 3.7473(3.7714) | Xent 0.9199(0.9115) | Loss 8.6719(8.9785) | Error 0.3278(0.3258) Steps 0(0.00) | Grad Norm 5.5436(8.8408) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 14.9494(15.3079) | Bit/dim 3.7582(3.7693) | Xent 0.8267(0.8996) | Loss 8.5674(8.8906) | Error 0.2967(0.3232) Steps 0(0.00) | Grad Norm 13.1341(8.8360) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.0147(15.3335) | Bit/dim 3.7791(3.7693) | Xent 0.9150(0.9036) | Loss 8.7170(8.8366) | Error 0.3244(0.3234) Steps 0(0.00) | Grad Norm 9.7421(9.0133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 80.9262, Epoch Time 942.9398(911.1943), Bit/dim 3.7679(best: 3.7709), Xent 0.8949, Loss 4.2154, Error 0.3124(best: 0.3057)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.2288(15.3333) | Bit/dim 3.7352(3.7675) | Xent 0.8736(0.8953) | Loss 8.4736(9.3223) | Error 0.3089(0.3209) Steps 0(0.00) | Grad Norm 9.7997(9.3445) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 15.8186(15.3943) | Bit/dim 3.7101(3.7657) | Xent 0.7997(0.8881) | Loss 8.4938(9.1373) | Error 0.2756(0.3167) Steps 0(0.00) | Grad Norm 9.2412(9.0588) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.4431(15.3405) | Bit/dim 3.7237(3.7652) | Xent 0.7849(0.8777) | Loss 8.6162(9.0031) | Error 0.2878(0.3133) Steps 0(0.00) | Grad Norm 6.8509(8.5054) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.1139(15.3875) | Bit/dim 3.7556(3.7643) | Xent 0.9031(0.8826) | Loss 8.6813(8.9070) | Error 0.3278(0.3152) Steps 0(0.00) | Grad Norm 11.1814(8.8037) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.0112(15.4152) | Bit/dim 3.7739(3.7642) | Xent 0.9668(0.8923) | Loss 8.8017(8.8619) | Error 0.3411(0.3178) Steps 0(0.00) | Grad Norm 8.8728(9.2747) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.8773(15.4244) | Bit/dim 3.7402(3.7639) | Xent 0.9701(0.8926) | Loss 8.6369(8.8034) | Error 0.3356(0.3181) Steps 0(0.00) | Grad Norm 11.5619(9.1691) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 80.0795, Epoch Time 946.7377(912.2606), Bit/dim 3.7654(best: 3.7679), Xent 0.8799, Loss 4.2053, Error 0.3118(best: 0.3057)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 15.6160(15.4401) | Bit/dim 3.7944(3.7644) | Xent 0.8704(0.8938) | Loss 8.7967(9.2245) | Error 0.3122(0.3182) Steps 0(0.00) | Grad Norm 7.3404(9.2483) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.1421(15.3988) | Bit/dim 3.7665(3.7659) | Xent 0.8822(0.8917) | Loss 8.6048(9.0727) | Error 0.3022(0.3166) Steps 0(0.00) | Grad Norm 5.5970(9.4226) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.9475(15.3800) | Bit/dim 3.7973(3.7640) | Xent 0.8699(0.8932) | Loss 8.6592(8.9615) | Error 0.3056(0.3177) Steps 0(0.00) | Grad Norm 10.6921(9.3682) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.2588(15.3614) | Bit/dim 3.7758(3.7643) | Xent 0.8399(0.8948) | Loss 8.6328(8.8844) | Error 0.3122(0.3190) Steps 0(0.00) | Grad Norm 7.8500(9.3910) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.1010(15.4022) | Bit/dim 3.7644(3.7613) | Xent 0.9439(0.8905) | Loss 8.8201(8.8234) | Error 0.3267(0.3172) Steps 0(0.00) | Grad Norm 10.2538(9.0550) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 83.1829, Epoch Time 946.4300(913.2856), Bit/dim 3.7707(best: 3.7654), Xent 0.8561, Loss 4.1988, Error 0.3033(best: 0.3057)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.8767(15.3668) | Bit/dim 3.7504(3.7646) | Xent 0.8254(0.8811) | Loss 8.5582(9.3440) | Error 0.2944(0.3141) Steps 0(0.00) | Grad Norm 7.8717(8.7220) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.4802(15.3161) | Bit/dim 3.7593(3.7648) | Xent 0.9187(0.8738) | Loss 8.7303(9.1601) | Error 0.3222(0.3114) Steps 0(0.00) | Grad Norm 8.1503(8.4388) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.1649(15.3170) | Bit/dim 3.7314(3.7602) | Xent 0.9262(0.8787) | Loss 8.6601(9.0263) | Error 0.3178(0.3123) Steps 0(0.00) | Grad Norm 6.6033(8.8638) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.7665(15.3988) | Bit/dim 3.7589(3.7597) | Xent 0.9038(0.8822) | Loss 8.6522(8.9238) | Error 0.3189(0.3144) Steps 0(0.00) | Grad Norm 8.1135(8.9872) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 15.6480(15.3867) | Bit/dim 3.7602(3.7576) | Xent 0.8799(0.8789) | Loss 8.5516(8.8377) | Error 0.3211(0.3141) Steps 0(0.00) | Grad Norm 6.9328(8.7547) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 15.2515(15.3541) | Bit/dim 3.7512(3.7552) | Xent 0.8655(0.8703) | Loss 8.6021(8.7727) | Error 0.2956(0.3105) Steps 0(0.00) | Grad Norm 7.5967(8.4873) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 82.3738, Epoch Time 944.0373(914.2082), Bit/dim 3.7542(best: 3.7654), Xent 0.8352, Loss 4.1718, Error 0.2962(best: 0.3033)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 14.8025(15.3304) | Bit/dim 3.7438(3.7574) | Xent 0.8028(0.8598) | Loss 8.4521(9.1960) | Error 0.2878(0.3079) Steps 0(0.00) | Grad Norm 6.7529(8.4249) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.1168(15.3065) | Bit/dim 3.7307(3.7573) | Xent 0.8385(0.8667) | Loss 8.5586(9.0522) | Error 0.3067(0.3104) Steps 0(0.00) | Grad Norm 9.8752(8.9920) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.4310(15.3204) | Bit/dim 3.7701(3.7567) | Xent 0.8465(0.8667) | Loss 8.5023(8.9236) | Error 0.3100(0.3096) Steps 0(0.00) | Grad Norm 4.4039(8.7271) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 15.4882(15.3108) | Bit/dim 3.7089(3.7516) | Xent 0.9004(0.8655) | Loss 8.4739(8.8311) | Error 0.3167(0.3092) Steps 0(0.00) | Grad Norm 6.5705(8.8576) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.6381(15.3131) | Bit/dim 3.7711(3.7507) | Xent 0.9062(0.8722) | Loss 8.7925(8.7855) | Error 0.3178(0.3113) Steps 0(0.00) | Grad Norm 12.4003(9.0274) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 83.0008, Epoch Time 943.0173(915.0725), Bit/dim 3.7640(best: 3.7542), Xent 0.8675, Loss 4.1977, Error 0.3045(best: 0.2962)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.4167(15.3295) | Bit/dim 3.7535(3.7505) | Xent 0.8174(0.8652) | Loss 8.5009(9.2807) | Error 0.2900(0.3096) Steps 0(0.00) | Grad Norm 15.0456(9.1466) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 14.9254(15.3128) | Bit/dim 3.7597(3.7510) | Xent 0.7451(0.8540) | Loss 8.5824(9.0997) | Error 0.2689(0.3048) Steps 0(0.00) | Grad Norm 7.2513(9.0231) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 14.8792(15.3248) | Bit/dim 3.7534(3.7507) | Xent 0.8942(0.8508) | Loss 8.6456(8.9701) | Error 0.3067(0.3044) Steps 0(0.00) | Grad Norm 10.9754(9.0963) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.0856(15.3162) | Bit/dim 3.7469(3.7483) | Xent 0.8462(0.8488) | Loss 8.5881(8.8716) | Error 0.2933(0.3025) Steps 0(0.00) | Grad Norm 9.5488(9.0290) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 14.5919(15.2207) | Bit/dim 3.7663(3.7499) | Xent 0.8796(0.8534) | Loss 8.6381(8.8038) | Error 0.3156(0.3039) Steps 0(0.00) | Grad Norm 7.2286(8.8876) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.7775(15.2158) | Bit/dim 3.7320(3.7461) | Xent 0.8114(0.8558) | Loss 8.5535(8.7417) | Error 0.2800(0.3038) Steps 0(0.00) | Grad Norm 4.6228(8.9943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 82.1069, Epoch Time 937.4311(915.7432), Bit/dim 3.7533(best: 3.7542), Xent 0.8257, Loss 4.1662, Error 0.2924(best: 0.2962)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 14.6509(15.2883) | Bit/dim 3.7414(3.7467) | Xent 0.7839(0.8472) | Loss 8.4550(9.1763) | Error 0.2744(0.3023) Steps 0(0.00) | Grad Norm 5.5944(8.8587) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.2114(15.2994) | Bit/dim 3.7626(3.7458) | Xent 0.8243(0.8512) | Loss 8.7078(9.0239) | Error 0.2978(0.3038) Steps 0(0.00) | Grad Norm 10.4508(9.1085) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.7055(15.3027) | Bit/dim 3.7351(3.7459) | Xent 0.8782(0.8508) | Loss 8.7492(8.9102) | Error 0.3111(0.3044) Steps 0(0.00) | Grad Norm 7.4648(8.9902) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.5611(15.3448) | Bit/dim 3.7418(3.7457) | Xent 0.8562(0.8457) | Loss 8.6587(8.8297) | Error 0.3033(0.3032) Steps 0(0.00) | Grad Norm 11.0874(8.7822) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.1543(15.2937) | Bit/dim 3.7631(3.7485) | Xent 0.8238(0.8457) | Loss 8.5876(8.7701) | Error 0.2900(0.3030) Steps 0(0.00) | Grad Norm 6.4449(8.6381) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 82.7404, Epoch Time 943.7598(916.5837), Bit/dim 3.7398(best: 3.7533), Xent 0.8702, Loss 4.1749, Error 0.3146(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.2800(15.2963) | Bit/dim 3.7428(3.7480) | Xent 0.9792(0.8585) | Loss 8.6243(9.2971) | Error 0.3356(0.3062) Steps 0(0.00) | Grad Norm 25.7756(9.4439) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.5727(15.3092) | Bit/dim 3.7533(3.7546) | Xent 0.8660(0.8639) | Loss 8.5779(9.1282) | Error 0.3189(0.3080) Steps 0(0.00) | Grad Norm 9.7465(10.0522) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 15.8543(15.2688) | Bit/dim 3.7311(3.7547) | Xent 0.8515(0.8582) | Loss 8.5456(8.9915) | Error 0.3144(0.3055) Steps 0(0.00) | Grad Norm 7.2543(9.7275) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 14.4106(15.2143) | Bit/dim 3.7671(3.7527) | Xent 0.8493(0.8484) | Loss 8.6297(8.8765) | Error 0.3211(0.3029) Steps 0(0.00) | Grad Norm 4.5835(8.9441) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 15.1834(15.2009) | Bit/dim 3.6940(3.7461) | Xent 0.8627(0.8454) | Loss 8.5734(8.7921) | Error 0.2956(0.3012) Steps 0(0.00) | Grad Norm 10.4725(9.0897) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.3564(15.1650) | Bit/dim 3.7309(3.7433) | Xent 0.8376(0.8450) | Loss 8.4700(8.7310) | Error 0.2989(0.2994) Steps 0(0.00) | Grad Norm 6.5622(9.2943) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 82.8502, Epoch Time 936.0767(917.1685), Bit/dim 3.7468(best: 3.7398), Xent 0.8512, Loss 4.1725, Error 0.3053(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 14.7224(15.2171) | Bit/dim 3.7410(3.7423) | Xent 0.8809(0.8402) | Loss 8.6885(9.1785) | Error 0.3144(0.2978) Steps 0(0.00) | Grad Norm 10.8389(9.3388) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 14.9127(15.1898) | Bit/dim 3.7406(3.7400) | Xent 0.8222(0.8358) | Loss 8.5875(9.0249) | Error 0.2856(0.2973) Steps 0(0.00) | Grad Norm 9.9932(9.3055) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 16.6154(15.2723) | Bit/dim 3.7786(3.7454) | Xent 0.8279(0.8358) | Loss 8.6259(8.9109) | Error 0.2967(0.2982) Steps 0(0.00) | Grad Norm 8.8031(9.1544) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.0790(15.2386) | Bit/dim 3.7421(3.7436) | Xent 0.8396(0.8318) | Loss 8.5625(8.8269) | Error 0.2989(0.2969) Steps 0(0.00) | Grad Norm 7.1816(8.4249) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.4628(15.1741) | Bit/dim 3.7201(3.7420) | Xent 0.7755(0.8240) | Loss 8.5503(8.7532) | Error 0.2589(0.2938) Steps 0(0.00) | Grad Norm 3.9887(7.8245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 83.0611, Epoch Time 936.9866(917.7631), Bit/dim 3.7371(best: 3.7398), Xent 0.8571, Loss 4.1656, Error 0.3013(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.4884(15.1425) | Bit/dim 3.7239(3.7386) | Xent 0.8053(0.8164) | Loss 8.4955(9.2485) | Error 0.2800(0.2913) Steps 0(0.00) | Grad Norm 4.6746(7.6075) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 14.6811(15.1209) | Bit/dim 3.7200(3.7381) | Xent 0.8398(0.8119) | Loss 8.5787(9.0710) | Error 0.3011(0.2904) Steps 0(0.00) | Grad Norm 10.4141(7.8119) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 14.8718(15.1519) | Bit/dim 3.7159(3.7370) | Xent 0.8868(0.8178) | Loss 8.6426(8.9478) | Error 0.3156(0.2924) Steps 0(0.00) | Grad Norm 8.8851(8.0584) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 14.7850(15.1870) | Bit/dim 3.7342(3.7353) | Xent 0.8640(0.8223) | Loss 8.6178(8.8513) | Error 0.3144(0.2932) Steps 0(0.00) | Grad Norm 9.4503(8.6266) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 15.2050(15.1492) | Bit/dim 3.7065(3.7342) | Xent 0.8831(0.8277) | Loss 8.6178(8.7838) | Error 0.3211(0.2954) Steps 0(0.00) | Grad Norm 6.6576(8.7397) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.1028(15.1605) | Bit/dim 3.7821(3.7366) | Xent 0.7613(0.8255) | Loss 8.5965(8.7341) | Error 0.2844(0.2947) Steps 0(0.00) | Grad Norm 6.0015(8.7230) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 83.4583, Epoch Time 935.2196(918.2868), Bit/dim 3.7359(best: 3.7371), Xent 0.8571, Loss 4.1644, Error 0.3056(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.7689(15.1650) | Bit/dim 3.7090(3.7359) | Xent 0.8164(0.8288) | Loss 8.5136(9.1831) | Error 0.2822(0.2962) Steps 0(0.00) | Grad Norm 12.1408(8.9955) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.0850(15.1495) | Bit/dim 3.7519(3.7355) | Xent 0.7849(0.8250) | Loss 8.6072(9.0164) | Error 0.2856(0.2950) Steps 0(0.00) | Grad Norm 12.1167(9.0716) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 14.7315(15.2319) | Bit/dim 3.7494(3.7336) | Xent 0.8204(0.8248) | Loss 8.5230(8.8940) | Error 0.2922(0.2952) Steps 0(0.00) | Grad Norm 5.4925(9.0056) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 14.8341(15.2267) | Bit/dim 3.6895(3.7328) | Xent 0.8453(0.8246) | Loss 8.5632(8.8081) | Error 0.3044(0.2947) Steps 0(0.00) | Grad Norm 6.7311(9.0123) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.3065(15.2097) | Bit/dim 3.7462(3.7315) | Xent 0.8421(0.8205) | Loss 8.5025(8.7375) | Error 0.2967(0.2924) Steps 0(0.00) | Grad Norm 7.7861(8.6502) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 84.0721, Epoch Time 937.5484(918.8646), Bit/dim 3.7283(best: 3.7359), Xent 0.8201, Loss 4.1383, Error 0.2915(best: 0.2924)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.2535(15.1779) | Bit/dim 3.7464(3.7350) | Xent 0.7809(0.8191) | Loss 8.4759(9.2441) | Error 0.2778(0.2921) Steps 0(0.00) | Grad Norm 4.8361(8.8168) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 14.9218(15.1847) | Bit/dim 3.7158(3.7341) | Xent 0.7481(0.8113) | Loss 8.4566(9.0485) | Error 0.2656(0.2897) Steps 0(0.00) | Grad Norm 6.5849(8.4882) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.1843(15.1607) | Bit/dim 3.7071(3.7311) | Xent 0.7939(0.8045) | Loss 8.3590(8.9031) | Error 0.2956(0.2875) Steps 0(0.00) | Grad Norm 5.9773(8.3878) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 14.9698(15.2100) | Bit/dim 3.7651(3.7290) | Xent 0.8303(0.8101) | Loss 8.6426(8.8032) | Error 0.2944(0.2904) Steps 0(0.00) | Grad Norm 7.6771(8.5195) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.6316(15.1713) | Bit/dim 3.7469(3.7326) | Xent 0.8061(0.8073) | Loss 8.5698(8.7368) | Error 0.2844(0.2888) Steps 0(0.00) | Grad Norm 6.2768(8.1652) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 14.7545(15.2208) | Bit/dim 3.7391(3.7319) | Xent 0.8334(0.8083) | Loss 8.6386(8.6889) | Error 0.2922(0.2878) Steps 0(0.00) | Grad Norm 15.6400(8.6636) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 84.4796, Epoch Time 939.1312(919.4726), Bit/dim 3.7288(best: 3.7283), Xent 0.8544, Loss 4.1559, Error 0.3030(best: 0.2915)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 14.5687(15.1951) | Bit/dim 3.7574(3.7317) | Xent 0.8238(0.8055) | Loss 8.5434(9.1446) | Error 0.3056(0.2867) Steps 0(0.00) | Grad Norm 6.2422(8.5132) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 15.7689(15.2239) | Bit/dim 3.7670(3.7298) | Xent 0.8318(0.8109) | Loss 8.6288(8.9958) | Error 0.2933(0.2889) Steps 0(0.00) | Grad Norm 12.9804(8.9755) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.0883(15.2714) | Bit/dim 3.7664(3.7313) | Xent 0.8649(0.8122) | Loss 8.5127(8.8804) | Error 0.3156(0.2894) Steps 0(0.00) | Grad Norm 11.1391(9.1987) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.3692(15.2936) | Bit/dim 3.7593(3.7327) | Xent 0.8672(0.8131) | Loss 8.6866(8.8048) | Error 0.3111(0.2893) Steps 0(0.00) | Grad Norm 7.8942(8.8935) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.4040(15.3158) | Bit/dim 3.7229(3.7296) | Xent 0.7979(0.8096) | Loss 8.5422(8.7296) | Error 0.2922(0.2895) Steps 0(0.00) | Grad Norm 8.8942(8.6145) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 82.6932, Epoch Time 940.6766(920.1087), Bit/dim 3.7245(best: 3.7283), Xent 0.8652, Loss 4.1571, Error 0.3046(best: 0.2915)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.1290(15.2342) | Bit/dim 3.6755(3.7319) | Xent 0.9627(0.8148) | Loss 8.6463(9.2334) | Error 0.3233(0.2902) Steps 0(0.00) | Grad Norm 16.7978(9.6199) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 15.7516(15.2359) | Bit/dim 3.7223(3.7319) | Xent 0.8350(0.8218) | Loss 8.5318(9.0647) | Error 0.3033(0.2933) Steps 0(0.00) | Grad Norm 8.4491(9.3853) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.3156(15.2429) | Bit/dim 3.7408(3.7291) | Xent 0.8051(0.8118) | Loss 8.5211(8.9136) | Error 0.2744(0.2901) Steps 0(0.00) | Grad Norm 4.9565(8.8801) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.3952(15.2231) | Bit/dim 3.7145(3.7281) | Xent 0.7944(0.8097) | Loss 8.4404(8.8066) | Error 0.2700(0.2885) Steps 0(0.00) | Grad Norm 8.7905(8.6771) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 14.3621(15.1378) | Bit/dim 3.7430(3.7298) | Xent 0.7527(0.7981) | Loss 8.4085(8.7186) | Error 0.2811(0.2852) Steps 0(0.00) | Grad Norm 4.6610(8.3448) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.0724(15.1962) | Bit/dim 3.7326(3.7263) | Xent 0.7353(0.8011) | Loss 8.4784(8.6676) | Error 0.2644(0.2859) Steps 0(0.00) | Grad Norm 4.5750(8.1149) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 82.2398, Epoch Time 935.0153(920.5559), Bit/dim 3.7227(best: 3.7245), Xent 0.8441, Loss 4.1447, Error 0.2987(best: 0.2915)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 15.4224(15.2510) | Bit/dim 3.7260(3.7222) | Xent 0.7583(0.7957) | Loss 8.4827(9.0800) | Error 0.2778(0.2856) Steps 0(0.00) | Grad Norm 8.5708(7.9385) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.3235(15.2384) | Bit/dim 3.7238(3.7234) | Xent 0.7578(0.7925) | Loss 8.5336(8.9317) | Error 0.2800(0.2857) Steps 0(0.00) | Grad Norm 6.8771(8.2886) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 15.2446(15.2193) | Bit/dim 3.7081(3.7219) | Xent 0.8546(0.7961) | Loss 8.5497(8.8156) | Error 0.3133(0.2864) Steps 0(0.00) | Grad Norm 9.2276(8.5474) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.1043(15.2628) | Bit/dim 3.7049(3.7230) | Xent 0.7888(0.7930) | Loss 8.5727(8.7397) | Error 0.2900(0.2856) Steps 0(0.00) | Grad Norm 6.3599(8.3237) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 15.3073(15.3121) | Bit/dim 3.7359(3.7248) | Xent 0.8020(0.7951) | Loss 8.6003(8.6996) | Error 0.2789(0.2846) Steps 0(0.00) | Grad Norm 11.7968(8.6543) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 83.8540, Epoch Time 946.6596(921.3390), Bit/dim 3.7246(best: 3.7227), Xent 0.8183, Loss 4.1337, Error 0.2900(best: 0.2915)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.0786(15.3403) | Bit/dim 3.7010(3.7272) | Xent 0.7306(0.7914) | Loss 8.4157(9.2373) | Error 0.2578(0.2846) Steps 0(0.00) | Grad Norm 12.5504(8.9191) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 14.7411(15.3083) | Bit/dim 3.7267(3.7263) | Xent 0.7752(0.7801) | Loss 8.5871(9.0418) | Error 0.2822(0.2788) Steps 0(0.00) | Grad Norm 6.9822(8.3855) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 15.0663(15.3254) | Bit/dim 3.7396(3.7230) | Xent 0.7211(0.7780) | Loss 8.4878(8.8983) | Error 0.2644(0.2788) Steps 0(0.00) | Grad Norm 5.1053(7.9080) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 14.9883(15.2748) | Bit/dim 3.7243(3.7210) | Xent 0.7528(0.7742) | Loss 8.5175(8.7856) | Error 0.2811(0.2778) Steps 0(0.00) | Grad Norm 9.6789(7.7233) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 15.6616(15.2911) | Bit/dim 3.7192(3.7210) | Xent 0.8527(0.7769) | Loss 8.5997(8.7068) | Error 0.3189(0.2779) Steps 0(0.00) | Grad Norm 6.6212(7.7611) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 15.4778(15.2775) | Bit/dim 3.7338(3.7208) | Xent 0.8411(0.7804) | Loss 8.6194(8.6514) | Error 0.3000(0.2797) Steps 0(0.00) | Grad Norm 13.6618(7.8732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 84.9671, Epoch Time 941.8154(921.9533), Bit/dim 3.7228(best: 3.7227), Xent 0.7883, Loss 4.1169, Error 0.2806(best: 0.2900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.3770(15.2504) | Bit/dim 3.7290(3.7202) | Xent 0.7063(0.7693) | Loss 8.4167(9.0688) | Error 0.2622(0.2749) Steps 0(0.00) | Grad Norm 4.9248(7.3198) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 16.2486(15.2890) | Bit/dim 3.7388(3.7186) | Xent 0.8196(0.7661) | Loss 8.4804(8.9100) | Error 0.2778(0.2740) Steps 0(0.00) | Grad Norm 14.9455(8.0114) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 14.6937(15.2525) | Bit/dim 3.7316(3.7200) | Xent 0.8369(0.7691) | Loss 8.5176(8.8019) | Error 0.2789(0.2737) Steps 0(0.00) | Grad Norm 15.1743(8.4001) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 14.8022(15.2226) | Bit/dim 3.6979(3.7195) | Xent 0.7940(0.7733) | Loss 8.5639(8.7258) | Error 0.2789(0.2742) Steps 0(0.00) | Grad Norm 6.5972(8.5716) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 15.4726(15.2254) | Bit/dim 3.7028(3.7177) | Xent 0.7452(0.7737) | Loss 8.4952(8.6632) | Error 0.2767(0.2761) Steps 0(0.00) | Grad Norm 5.3257(8.9039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 85.8499, Epoch Time 940.0343(922.4958), Bit/dim 3.7178(best: 3.7227), Xent 0.8049, Loss 4.1202, Error 0.2864(best: 0.2806)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.0493(15.2447) | Bit/dim 3.7389(3.7186) | Xent 0.7557(0.7685) | Loss 8.5717(9.1961) | Error 0.2789(0.2739) Steps 0(0.00) | Grad Norm 5.0766(8.2474) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 14.8573(15.3094) | Bit/dim 3.7034(3.7166) | Xent 0.7349(0.7670) | Loss 8.3978(9.0185) | Error 0.2700(0.2733) Steps 0(0.00) | Grad Norm 6.1899(7.8242) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.0327(15.2400) | Bit/dim 3.7162(3.7139) | Xent 0.7552(0.7616) | Loss 8.5485(8.8763) | Error 0.2689(0.2719) Steps 0(0.00) | Grad Norm 7.2891(7.6760) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 15.6263(15.2393) | Bit/dim 3.7237(3.7122) | Xent 0.7636(0.7604) | Loss 8.5930(8.7721) | Error 0.2633(0.2708) Steps 0(0.00) | Grad Norm 8.1304(7.5854) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 14.2259(15.2054) | Bit/dim 3.7114(3.7116) | Xent 0.7788(0.7587) | Loss 8.4914(8.6995) | Error 0.2989(0.2702) Steps 0(0.00) | Grad Norm 6.5084(7.4023) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 15.6187(15.2364) | Bit/dim 3.7456(3.7118) | Xent 0.7932(0.7655) | Loss 8.6238(8.6635) | Error 0.2811(0.2736) Steps 0(0.00) | Grad Norm 18.4098(8.6703) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 86.4341, Epoch Time 943.9315(923.1388), Bit/dim 3.7141(best: 3.7178), Xent 0.8030, Loss 4.1156, Error 0.2857(best: 0.2806)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 15.6770(15.2800) | Bit/dim 3.6638(3.7132) | Xent 0.6988(0.7625) | Loss 8.3906(9.1054) | Error 0.2500(0.2729) Steps 0(0.00) | Grad Norm 6.6792(8.5711) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 16.0558(15.2166) | Bit/dim 3.6758(3.7130) | Xent 0.7461(0.7566) | Loss 8.5673(8.9387) | Error 0.2600(0.2718) Steps 0(0.00) | Grad Norm 6.1072(8.6679) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.2492(15.2780) | Bit/dim 3.7326(3.7134) | Xent 0.7990(0.7579) | Loss 8.5755(8.8255) | Error 0.2922(0.2730) Steps 0(0.00) | Grad Norm 7.6603(8.2179) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 14.3889(15.2290) | Bit/dim 3.6887(3.7128) | Xent 0.7519(0.7623) | Loss 8.4048(8.7332) | Error 0.2722(0.2739) Steps 0(0.00) | Grad Norm 5.5055(8.4357) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.8254(15.3134) | Bit/dim 3.7243(3.7104) | Xent 0.7389(0.7659) | Loss 8.5273(8.6696) | Error 0.2789(0.2747) Steps 0(0.00) | Grad Norm 10.8835(8.7299) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 84.9697, Epoch Time 946.2195(923.8312), Bit/dim 3.7257(best: 3.7141), Xent 0.8066, Loss 4.1290, Error 0.2851(best: 0.2806)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 15.4865(15.3942) | Bit/dim 3.7451(3.7123) | Xent 0.7752(0.7680) | Loss 8.5941(9.2049) | Error 0.2878(0.2748) Steps 0(0.00) | Grad Norm 7.8661(9.1096) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 15.0682(15.3755) | Bit/dim 3.7076(3.7110) | Xent 0.7445(0.7612) | Loss 8.5978(9.0173) | Error 0.2833(0.2723) Steps 0(0.00) | Grad Norm 6.8583(8.7745) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 15.4564(15.4110) | Bit/dim 3.7067(3.7116) | Xent 0.7105(0.7529) | Loss 8.4887(8.8710) | Error 0.2667(0.2713) Steps 0(0.00) | Grad Norm 6.7117(8.2033) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 15.9871(15.3623) | Bit/dim 3.7183(3.7139) | Xent 0.7958(0.7553) | Loss 8.5511(8.7770) | Error 0.2756(0.2706) Steps 0(0.00) | Grad Norm 13.4074(8.3226) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 14.9420(15.3315) | Bit/dim 3.7116(3.7123) | Xent 0.7585(0.7577) | Loss 8.4918(8.7019) | Error 0.2889(0.2718) Steps 0(0.00) | Grad Norm 4.9218(8.5366) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 14.5851(15.3020) | Bit/dim 3.7002(3.7082) | Xent 0.7689(0.7607) | Loss 8.4183(8.6355) | Error 0.2678(0.2729) Steps 0(0.00) | Grad Norm 8.9485(8.8951) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 86.1901, Epoch Time 947.2897(924.5350), Bit/dim 3.7101(best: 3.7141), Xent 0.8154, Loss 4.1178, Error 0.2859(best: 0.2806)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 14.4389(15.2677) | Bit/dim 3.6989(3.7105) | Xent 0.7663(0.7581) | Loss 8.5158(9.1010) | Error 0.2844(0.2725) Steps 0(0.00) | Grad Norm 6.3150(8.7746) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 14.8142(15.3217) | Bit/dim 3.6990(3.7081) | Xent 0.7277(0.7501) | Loss 8.3716(8.9335) | Error 0.2500(0.2683) Steps 0(0.00) | Grad Norm 10.6548(8.4342) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 15.2670(15.3192) | Bit/dim 3.7280(3.7071) | Xent 0.6697(0.7451) | Loss 8.5683(8.8075) | Error 0.2511(0.2671) Steps 0(0.00) | Grad Norm 3.8618(7.5198) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.7034(15.3446) | Bit/dim 3.6801(3.7025) | Xent 0.7399(0.7485) | Loss 8.5301(8.7161) | Error 0.2689(0.2674) Steps 0(0.00) | Grad Norm 9.6646(7.6278) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 15.1911(15.3926) | Bit/dim 3.7282(3.7032) | Xent 0.7475(0.7510) | Loss 8.4817(8.6528) | Error 0.2556(0.2675) Steps 0(0.00) | Grad Norm 10.7661(8.1698) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 86.6736, Epoch Time 950.2012(925.3050), Bit/dim 3.7133(best: 3.7101), Xent 0.7913, Loss 4.1090, Error 0.2761(best: 0.2806)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 15.8146(15.3765) | Bit/dim 3.7126(3.7072) | Xent 0.6765(0.7460) | Loss 8.5373(9.1803) | Error 0.2544(0.2664) Steps 0(0.00) | Grad Norm 9.5643(8.3535) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 15.6732(15.4036) | Bit/dim 3.7336(3.7057) | Xent 0.7295(0.7415) | Loss 8.5418(8.9933) | Error 0.2689(0.2652) Steps 0(0.00) | Grad Norm 11.1675(8.0052) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 15.5098(15.4180) | Bit/dim 3.6838(3.7037) | Xent 0.7859(0.7441) | Loss 8.5684(8.8564) | Error 0.2833(0.2659) Steps 0(0.00) | Grad Norm 12.0369(8.1893) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.3608(15.3781) | Bit/dim 3.7154(3.7052) | Xent 0.7379(0.7410) | Loss 8.5526(8.7611) | Error 0.2500(0.2654) Steps 0(0.00) | Grad Norm 10.9474(8.3316) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 14.6385(15.3919) | Bit/dim 3.6893(3.7049) | Xent 0.7256(0.7445) | Loss 8.4011(8.6846) | Error 0.2567(0.2675) Steps 0(0.00) | Grad Norm 6.6117(8.4062) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 15.7321(15.3479) | Bit/dim 3.7175(3.7065) | Xent 0.7408(0.7484) | Loss 8.4246(8.6246) | Error 0.2700(0.2679) Steps 0(0.00) | Grad Norm 7.5426(8.5894) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 85.2567, Epoch Time 948.7954(926.0097), Bit/dim 3.7121(best: 3.7101), Xent 0.7735, Loss 4.0989, Error 0.2757(best: 0.2761)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 15.8282(15.3579) | Bit/dim 3.7334(3.7062) | Xent 0.7404(0.7427) | Loss 8.6081(9.0678) | Error 0.2722(0.2665) Steps 0(0.00) | Grad Norm 13.2649(8.7789) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.1553(15.3478) | Bit/dim 3.7243(3.7101) | Xent 0.7159(0.7382) | Loss 8.4550(8.9199) | Error 0.2522(0.2645) Steps 0(0.00) | Grad Norm 7.1034(8.4617) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 15.2499(15.3157) | Bit/dim 3.7165(3.7099) | Xent 0.6567(0.7285) | Loss 8.4583(8.7945) | Error 0.2278(0.2605) Steps 0(0.00) | Grad Norm 5.2746(7.9403) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.5547(15.3634) | Bit/dim 3.6561(3.7075) | Xent 0.6881(0.7352) | Loss 8.3913(8.6987) | Error 0.2311(0.2615) Steps 0(0.00) | Grad Norm 6.3229(8.3627) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 14.8657(15.3649) | Bit/dim 3.7073(3.7081) | Xent 0.7293(0.7332) | Loss 8.5376(8.6377) | Error 0.2589(0.2615) Steps 0(0.00) | Grad Norm 11.5437(8.9622) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 86.8337, Epoch Time 950.8484(926.7549), Bit/dim 3.6997(best: 3.7101), Xent 0.8132, Loss 4.1064, Error 0.2817(best: 0.2757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 14.9471(15.3797) | Bit/dim 3.6961(3.7087) | Xent 0.7212(0.7384) | Loss 8.4088(9.1924) | Error 0.2567(0.2635) Steps 0(0.00) | Grad Norm 9.0584(8.7173) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 15.3495(15.3601) | Bit/dim 3.7220(3.7099) | Xent 0.7207(0.7397) | Loss 8.4745(9.0037) | Error 0.2600(0.2646) Steps 0(0.00) | Grad Norm 9.6794(8.8242) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 15.9374(15.2793) | Bit/dim 3.7038(3.7071) | Xent 0.8017(0.7319) | Loss 8.4777(8.8531) | Error 0.2911(0.2610) Steps 0(0.00) | Grad Norm 8.7994(8.4852) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 15.6686(15.3233) | Bit/dim 3.7275(3.7045) | Xent 0.7213(0.7353) | Loss 8.5207(8.7535) | Error 0.2556(0.2623) Steps 0(0.00) | Grad Norm 9.1389(8.4842) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 14.9395(15.2864) | Bit/dim 3.7307(3.7049) | Xent 0.7244(0.7346) | Loss 8.4248(8.6800) | Error 0.2667(0.2620) Steps 0(0.00) | Grad Norm 7.0566(8.1967) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.3036(15.3095) | Bit/dim 3.7635(3.7073) | Xent 0.8412(0.7439) | Loss 8.6900(8.6308) | Error 0.2856(0.2646) Steps 0(0.00) | Grad Norm 11.8790(8.7676) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 85.9698, Epoch Time 943.6363(927.2613), Bit/dim 3.7146(best: 3.6997), Xent 0.7633, Loss 4.0962, Error 0.2703(best: 0.2757)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 15.2069(15.2774) | Bit/dim 3.6725(3.7058) | Xent 0.7109(0.7365) | Loss 8.3694(9.0791) | Error 0.2422(0.2616) Steps 0(0.00) | Grad Norm 5.5496(8.7566) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 15.2529(15.2676) | Bit/dim 3.6817(3.7031) | Xent 0.6909(0.7297) | Loss 8.3201(8.9067) | Error 0.2544(0.2586) Steps 0(0.00) | Grad Norm 6.3330(7.9879) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.2965(15.3285) | Bit/dim 3.7289(3.7030) | Xent 0.7292(0.7291) | Loss 8.4670(8.7962) | Error 0.2678(0.2590) Steps 0(0.00) | Grad Norm 9.3392(8.3472) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 14.9822(15.2759) | Bit/dim 3.7170(3.7030) | Xent 0.7144(0.7259) | Loss 8.4895(8.7067) | Error 0.2478(0.2576) Steps 0(0.00) | Grad Norm 4.8857(7.8087) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 16.4112(15.3248) | Bit/dim 3.7118(3.7030) | Xent 0.7066(0.7252) | Loss 8.4176(8.6347) | Error 0.2511(0.2576) Steps 0(0.00) | Grad Norm 5.7649(7.2416) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 86.7389, Epoch Time 947.7090(927.8747), Bit/dim 3.6947(best: 3.6997), Xent 0.7382, Loss 4.0638, Error 0.2595(best: 0.2703)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 14.7170(15.3861) | Bit/dim 3.7168(3.7002) | Xent 0.7043(0.7205) | Loss 8.5051(9.1733) | Error 0.2533(0.2576) Steps 0(0.00) | Grad Norm 8.2833(7.0436) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 16.2771(15.3896) | Bit/dim 3.7045(3.6984) | Xent 0.7410(0.7151) | Loss 8.3717(8.9710) | Error 0.2644(0.2550) Steps 0(0.00) | Grad Norm 6.6351(7.1580) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.1833(15.4486) | Bit/dim 3.6887(3.6977) | Xent 0.7681(0.7218) | Loss 8.4978(8.8376) | Error 0.2844(0.2588) Steps 0(0.00) | Grad Norm 8.0980(7.7639) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 14.7578(15.4027) | Bit/dim 3.6737(3.6973) | Xent 0.7167(0.7186) | Loss 8.4064(8.7345) | Error 0.2578(0.2569) Steps 0(0.00) | Grad Norm 6.1123(7.4971) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.6809(15.4201) | Bit/dim 3.6717(3.6968) | Xent 0.6630(0.7123) | Loss 8.3584(8.6519) | Error 0.2500(0.2557) Steps 0(0.00) | Grad Norm 5.7420(7.0249) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 15.3726(15.4418) | Bit/dim 3.7247(3.6961) | Xent 0.6824(0.7129) | Loss 8.5213(8.5947) | Error 0.2456(0.2556) Steps 0(0.00) | Grad Norm 7.7161(7.1090) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 88.0344, Epoch Time 956.7182(928.7400), Bit/dim 3.6935(best: 3.6947), Xent 0.7579, Loss 4.0725, Error 0.2673(best: 0.2595)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 16.4127(15.4278) | Bit/dim 3.7207(3.6976) | Xent 0.7757(0.7129) | Loss 8.5960(9.0503) | Error 0.2811(0.2558) Steps 0(0.00) | Grad Norm 8.3805(7.6952) | Total Time 0.00(0.00)\n",
      "Iter 4310 | Time 16.0682(15.4040) | Bit/dim 3.6858(3.6976) | Xent 0.7153(0.7181) | Loss 8.2956(8.8845) | Error 0.2444(0.2568) Steps 0(0.00) | Grad Norm 11.0146(8.4046) | Total Time 0.00(0.00)\n",
      "Iter 4320 | Time 16.1399(15.4618) | Bit/dim 3.7540(3.6986) | Xent 0.7980(0.7218) | Loss 8.6157(8.7731) | Error 0.2622(0.2586) Steps 0(0.00) | Grad Norm 11.3131(8.4289) | Total Time 0.00(0.00)\n",
      "Iter 4330 | Time 15.5693(15.4220) | Bit/dim 3.6940(3.6962) | Xent 0.7338(0.7267) | Loss 8.3741(8.6799) | Error 0.2667(0.2601) Steps 0(0.00) | Grad Norm 7.2608(8.4201) | Total Time 0.00(0.00)\n",
      "Iter 4340 | Time 15.6556(15.4460) | Bit/dim 3.7029(3.6990) | Xent 0.6978(0.7185) | Loss 8.4864(8.6155) | Error 0.2411(0.2574) Steps 0(0.00) | Grad Norm 8.4869(8.2936) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 88.7858, Epoch Time 955.6577(929.5476), Bit/dim 3.6999(best: 3.6935), Xent 0.7498, Loss 4.0748, Error 0.2607(best: 0.2595)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 15.5929(15.4853) | Bit/dim 3.6456(3.6962) | Xent 0.7104(0.7173) | Loss 8.3649(9.1583) | Error 0.2478(0.2559) Steps 0(0.00) | Grad Norm 8.3443(8.4179) | Total Time 0.00(0.00)\n",
      "Iter 4360 | Time 17.0903(15.5753) | Bit/dim 3.6933(3.6965) | Xent 0.7454(0.7048) | Loss 8.5044(8.9623) | Error 0.2589(0.2519) Steps 0(0.00) | Grad Norm 9.6486(7.9975) | Total Time 0.00(0.00)\n",
      "Iter 4370 | Time 15.5254(15.6285) | Bit/dim 3.7305(3.6968) | Xent 0.7370(0.6977) | Loss 8.4254(8.8150) | Error 0.2722(0.2497) Steps 0(0.00) | Grad Norm 6.8719(7.8177) | Total Time 0.00(0.00)\n",
      "Iter 4380 | Time 15.2827(15.5969) | Bit/dim 3.6862(3.6942) | Xent 0.7331(0.6988) | Loss 8.4641(8.6987) | Error 0.2478(0.2505) Steps 0(0.00) | Grad Norm 7.9024(7.1108) | Total Time 0.00(0.00)\n",
      "Iter 4390 | Time 15.9741(15.5996) | Bit/dim 3.7128(3.6931) | Xent 0.6764(0.6975) | Loss 8.4696(8.6353) | Error 0.2267(0.2485) Steps 0(0.00) | Grad Norm 5.9746(7.1707) | Total Time 0.00(0.00)\n",
      "Iter 4400 | Time 15.1258(15.6464) | Bit/dim 3.6866(3.6921) | Xent 0.6476(0.7012) | Loss 8.3264(8.5678) | Error 0.2333(0.2505) Steps 0(0.00) | Grad Norm 3.8999(7.4047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 88.2083, Epoch Time 969.3426(930.7414), Bit/dim 3.6882(best: 3.6935), Xent 0.7577, Loss 4.0670, Error 0.2693(best: 0.2595)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_gamma_0_95_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --gamma 0.95\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
