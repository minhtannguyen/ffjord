{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn1', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=1.0, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_1_0_run1', scale=1.0, scale_fac=1.0, scale_std=6.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateI(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450732\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0010 | Time 12.8208(31.2233) | Bit/dim 8.6914(8.9522) | Xent 2.2806(2.3001) | Loss 1184.4175(1230.1095) | Error 0.8000(0.8602) Steps 0(0.00) | Grad Norm 1199.8421(1564.9506) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 12.6403(26.3272) | Bit/dim 8.4870(8.8632) | Xent 2.2265(2.2874) | Loss 1187.7446(1218.4676) | Error 0.7200(0.8324) Steps 0(0.00) | Grad Norm 511.0942(1354.5099) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 12.7920(22.6699) | Bit/dim 8.3923(8.7515) | Xent 2.1748(2.2634) | Loss 1125.9677(1204.1575) | Error 0.7578(0.8091) Steps 0(0.00) | Grad Norm 487.4314(1112.0508) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 12.7885(20.0094) | Bit/dim 8.1928(8.6234) | Xent 2.1140(2.2346) | Loss 1156.0286(1189.7822) | Error 0.7222(0.7907) Steps 0(0.00) | Grad Norm 316.8129(912.7752) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 12.2540(18.0091) | Bit/dim 7.9745(8.4738) | Xent 2.1014(2.2021) | Loss 1108.3744(1171.0290) | Error 0.7100(0.7753) Steps 0(0.00) | Grad Norm 304.9591(757.8841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 73.1679, Epoch Time 807.0850(807.0850), Bit/dim 7.7697(best: inf), Xent 2.0784, Loss 8.8089, Error 0.7004(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.4662(16.6013) | Bit/dim 7.6554(8.2922) | Xent 2.0905(2.1718) | Loss 1077.5732(1206.8597) | Error 0.7078(0.7591) Steps 0(0.00) | Grad Norm 281.4340(638.4436) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 13.1070(15.6927) | Bit/dim 7.3663(8.0808) | Xent 2.0765(2.1453) | Loss 1028.8480(1166.3008) | Error 0.7067(0.7423) Steps 0(0.00) | Grad Norm 239.6435(539.5246) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 13.1061(15.0280) | Bit/dim 7.1817(7.8642) | Xent 2.0832(2.1249) | Loss 1027.5187(1131.7390) | Error 0.6789(0.7264) Steps 0(0.00) | Grad Norm 176.4954(450.9350) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 13.1031(14.5822) | Bit/dim 7.0760(7.6684) | Xent 2.0832(2.1140) | Loss 1019.8286(1100.8046) | Error 0.6867(0.7185) Steps 0(0.00) | Grad Norm 134.9792(370.3270) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 13.7864(14.3290) | Bit/dim 7.0194(7.5058) | Xent 2.0800(2.1033) | Loss 997.2030(1076.2052) | Error 0.7133(0.7147) Steps 0(0.00) | Grad Norm 143.3174(305.0828) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 13.4620(14.1396) | Bit/dim 6.9995(7.3757) | Xent 2.0665(2.0947) | Loss 1013.8358(1059.4884) | Error 0.7267(0.7149) Steps 0(0.00) | Grad Norm 365.0465(277.0135) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 73.1479, Epoch Time 823.0580(807.5641), Bit/dim 6.9918(best: 7.7697), Xent 2.0575, Loss 8.0205, Error 0.6952(best: 0.7004)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 14.3988(14.1050) | Bit/dim 6.9382(7.2706) | Xent 2.0467(2.0852) | Loss 998.1518(1096.3955) | Error 0.6789(0.7116) Steps 0(0.00) | Grad Norm 212.1296(273.1291) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 14.2239(14.0889) | Bit/dim 6.9079(7.1831) | Xent 2.0643(2.0759) | Loss 997.7079(1071.0331) | Error 0.7244(0.7085) Steps 0(0.00) | Grad Norm 282.0025(252.6417) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 15.1180(14.1054) | Bit/dim 6.8745(7.1068) | Xent 2.0526(2.0701) | Loss 988.0668(1049.7567) | Error 0.6956(0.7054) Steps 0(0.00) | Grad Norm 802.8652(289.1638) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 14.5002(14.1736) | Bit/dim 6.8472(7.0375) | Xent 1.9917(2.0585) | Loss 1006.0488(1032.6154) | Error 0.6711(0.7002) Steps 0(0.00) | Grad Norm 115.7590(297.0432) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 14.6066(14.2719) | Bit/dim 6.7212(6.9634) | Xent 1.9788(2.0479) | Loss 976.4684(1016.8621) | Error 0.6789(0.6952) Steps 0(0.00) | Grad Norm 845.1803(351.7208) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 75.9203, Epoch Time 877.4234(809.6599), Bit/dim 6.6425(best: 6.9918), Xent 2.0361, Loss 7.6606, Error 0.7031(best: 0.6952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 14.8526(14.3260) | Bit/dim 6.5805(6.8812) | Xent 2.0154(2.0446) | Loss 953.1902(1062.6531) | Error 0.6767(0.6971) Steps 0(0.00) | Grad Norm 370.2945(665.8088) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 13.8320(14.3075) | Bit/dim 6.4576(6.7801) | Xent 2.2598(2.0502) | Loss 965.1896(1033.5392) | Error 0.8133(0.7040) Steps 0(0.00) | Grad Norm 6521.2182(1310.8770) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 14.3881(14.2355) | Bit/dim 6.2789(6.6632) | Xent 2.0857(2.0677) | Loss 936.9789(1009.2250) | Error 0.7478(0.7194) Steps 0(0.00) | Grad Norm 2130.4918(1849.8526) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 13.3412(14.1262) | Bit/dim 6.1449(6.5425) | Xent 2.1130(2.0730) | Loss 893.3572(984.2454) | Error 0.7711(0.7242) Steps 0(0.00) | Grad Norm 4381.1195(2371.1156) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 13.6297(14.0992) | Bit/dim 5.9393(6.4087) | Xent 2.0551(2.0716) | Loss 872.1132(960.7611) | Error 0.7200(0.7234) Steps 0(0.00) | Grad Norm 1354.0875(2330.8641) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 14.4041(14.0724) | Bit/dim 5.8801(6.2706) | Xent 2.2288(2.0669) | Loss 891.0735(939.5670) | Error 0.8022(0.7188) Steps 0(0.00) | Grad Norm 9676.7796(2415.6941) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 75.5832, Epoch Time 868.5050(811.4253), Bit/dim 6.1573(best: 6.6425), Xent 2.3475, Loss 7.3311, Error 0.8007(best: 0.6952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 13.5616(14.0820) | Bit/dim 5.8384(6.1731) | Xent 2.0132(2.0725) | Loss 855.7343(978.8754) | Error 0.6600(0.7215) Steps 0(0.00) | Grad Norm 1113.1522(2885.7630) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 13.3792(13.8925) | Bit/dim 5.7249(6.0696) | Xent 2.0110(2.0612) | Loss 866.1855(951.0085) | Error 0.6811(0.7122) Steps 0(0.00) | Grad Norm 983.6582(2413.6509) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 13.9949(13.7448) | Bit/dim 5.6764(5.9762) | Xent 1.9641(2.0479) | Loss 847.3233(924.1645) | Error 0.6778(0.7046) Steps 0(0.00) | Grad Norm 1940.8657(2136.6882) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 12.9023(13.6344) | Bit/dim 5.7111(5.8963) | Xent 2.0993(2.0392) | Loss 839.3045(905.4184) | Error 0.7456(0.7022) Steps 0(0.00) | Grad Norm 3411.6737(2059.3950) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 13.3375(13.6282) | Bit/dim 5.6400(5.8313) | Xent 1.9957(2.0290) | Loss 853.3674(890.8425) | Error 0.6944(0.6964) Steps 0(0.00) | Grad Norm 1404.8540(1849.1953) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 75.1363, Epoch Time 837.4012(812.2046), Bit/dim 5.6206(best: 6.1573), Xent 1.9697, Loss 6.6055, Error 0.6779(best: 0.6952)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 13.7595(13.5418) | Bit/dim 5.6222(5.7784) | Xent 1.9780(2.0182) | Loss 845.5244(940.1689) | Error 0.6978(0.6959) Steps 0(0.00) | Grad Norm 1423.1874(1791.6496) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 13.2969(13.5181) | Bit/dim 5.5782(5.7283) | Xent 1.9173(2.0035) | Loss 840.3362(913.1017) | Error 0.6367(0.6873) Steps 0(0.00) | Grad Norm 1136.3182(1528.9791) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 13.2283(13.5093) | Bit/dim 5.5895(5.6881) | Xent 1.9464(1.9963) | Loss 810.3398(893.3058) | Error 0.6700(0.6881) Steps 0(0.00) | Grad Norm 2478.6419(1908.6201) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 13.4142(13.5341) | Bit/dim 5.5591(5.6535) | Xent 1.9413(1.9850) | Loss 839.2072(877.9003) | Error 0.6656(0.6840) Steps 0(0.00) | Grad Norm 708.0221(1783.3955) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 13.4610(13.5227) | Bit/dim 5.5357(5.6218) | Xent 2.0165(1.9778) | Loss 829.1005(865.3079) | Error 0.7189(0.6822) Steps 0(0.00) | Grad Norm 3052.6075(1688.5731) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 13.8838(13.5481) | Bit/dim 5.5191(5.5907) | Xent 1.9042(1.9730) | Loss 842.0065(857.3456) | Error 0.6356(0.6784) Steps 0(0.00) | Grad Norm 1013.6242(1572.8705) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 74.2281, Epoch Time 835.3216(812.8981), Bit/dim 5.4765(best: 5.6206), Xent 1.9382, Loss 6.4456, Error 0.6686(best: 0.6779)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 13.9256(13.6003) | Bit/dim 5.4506(5.5581) | Xent 1.9175(1.9658) | Loss 828.6193(901.1722) | Error 0.6567(0.6762) Steps 0(0.00) | Grad Norm 1195.8341(1470.3632) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 13.5644(13.6402) | Bit/dim 5.3879(5.5284) | Xent 1.9644(1.9586) | Loss 826.4052(880.7154) | Error 0.6744(0.6751) Steps 0(0.00) | Grad Norm 2990.0021(1674.2220) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 14.2516(13.7163) | Bit/dim 5.3581(5.4876) | Xent 1.8892(1.9421) | Loss 820.1937(863.4092) | Error 0.6467(0.6676) Steps 0(0.00) | Grad Norm 1266.7914(1496.3469) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 14.1530(13.8433) | Bit/dim 5.3506(5.4594) | Xent 2.0031(1.9471) | Loss 818.3286(852.6421) | Error 0.6922(0.6724) Steps 0(0.00) | Grad Norm 2141.7094(1706.6782) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 13.7687(13.9066) | Bit/dim 5.3533(5.4232) | Xent 1.8819(1.9421) | Loss 821.1828(844.0167) | Error 0.6267(0.6712) Steps 0(0.00) | Grad Norm 1805.0322(1658.1397) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 76.8508, Epoch Time 864.0569(814.4328), Bit/dim 5.3091(best: 5.4765), Xent 1.9718, Loss 6.2950, Error 0.7008(best: 0.6686)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 14.4329(14.0200) | Bit/dim 5.3806(5.3951) | Xent 2.0230(1.9402) | Loss 820.5095(901.2281) | Error 0.7311(0.6754) Steps 0(0.00) | Grad Norm 3856.0844(1956.4881) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 14.6070(14.0646) | Bit/dim 5.2454(5.3616) | Xent 1.9240(1.9395) | Loss 796.3048(876.3091) | Error 0.6533(0.6760) Steps 0(0.00) | Grad Norm 474.5351(1884.6046) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 13.9062(14.0948) | Bit/dim 5.2164(5.3248) | Xent 1.9419(1.9335) | Loss 793.8922(856.9900) | Error 0.6711(0.6725) Steps 0(0.00) | Grad Norm 1470.2380(1763.3030) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 14.0873(14.1594) | Bit/dim 5.1852(5.2938) | Xent 1.9159(1.9217) | Loss 796.2525(840.6399) | Error 0.6533(0.6681) Steps 0(0.00) | Grad Norm 1335.0436(1552.4977) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 14.1728(14.2303) | Bit/dim 5.1428(5.2586) | Xent 1.8998(1.9177) | Loss 795.6320(829.4689) | Error 0.6633(0.6673) Steps 0(0.00) | Grad Norm 1346.4417(1431.4994) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 13.9463(14.2689) | Bit/dim 5.1275(5.2267) | Xent 1.8461(1.9056) | Loss 787.3940(817.9407) | Error 0.6556(0.6649) Steps 0(0.00) | Grad Norm 1090.9492(1282.0164) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 79.0267, Epoch Time 883.9067(816.5170), Bit/dim 5.1214(best: 5.3091), Xent 1.8309, Loss 6.0368, Error 0.6245(best: 0.6686)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 14.5960(14.3811) | Bit/dim 5.0958(5.1993) | Xent 1.8578(1.8977) | Loss 799.5400(865.9832) | Error 0.6733(0.6626) Steps 0(0.00) | Grad Norm 1170.5357(1409.9381) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 14.3003(14.4741) | Bit/dim 5.0923(5.1808) | Xent 1.8703(1.9177) | Loss 783.0089(848.7491) | Error 0.6456(0.6717) Steps 0(0.00) | Grad Norm 518.5697(1673.9971) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 14.4936(14.5454) | Bit/dim 5.0990(5.1540) | Xent 1.9273(1.9219) | Loss 789.9984(832.1467) | Error 0.6622(0.6723) Steps 0(0.00) | Grad Norm 1099.7584(1477.1709) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 14.7458(14.6363) | Bit/dim 5.0680(5.1240) | Xent 1.9152(1.9163) | Loss 788.6420(818.0336) | Error 0.6767(0.6711) Steps 0(0.00) | Grad Norm 1338.8339(1306.3572) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 15.0514(14.7223) | Bit/dim 5.0370(5.0989) | Xent 1.9227(1.9109) | Loss 775.1378(807.4074) | Error 0.6878(0.6689) Steps 0(0.00) | Grad Norm 2134.1482(1420.3669) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 78.8002, Epoch Time 913.2683(819.4196), Bit/dim 4.9740(best: 5.1214), Xent 1.8256, Loss 5.8868, Error 0.6336(best: 0.6245)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 15.3309(14.8473) | Bit/dim 4.9321(5.0693) | Xent 1.8176(1.9021) | Loss 759.1325(861.2120) | Error 0.6378(0.6664) Steps 0(0.00) | Grad Norm 662.7660(1423.3427) | Total Time 0.00(0.00)\n",
      "Iter 0510 | Time 15.2026(14.8920) | Bit/dim 4.9194(5.0361) | Xent 1.8297(1.8858) | Loss 752.7083(835.7120) | Error 0.6178(0.6604) Steps 0(0.00) | Grad Norm 831.8131(1286.5232) | Total Time 0.00(0.00)\n",
      "Iter 0520 | Time 14.4476(14.9523) | Bit/dim 5.0348(5.0110) | Xent 1.9767(1.8804) | Loss 788.1869(817.1834) | Error 0.7144(0.6580) Steps 0(0.00) | Grad Norm 4769.7240(1456.4756) | Total Time 0.00(0.00)\n",
      "Iter 0530 | Time 14.9916(14.9850) | Bit/dim 4.9492(5.0005) | Xent 1.8593(1.8818) | Loss 750.4333(805.2921) | Error 0.6678(0.6606) Steps 0(0.00) | Grad Norm 1585.1099(1662.7169) | Total Time 0.00(0.00)\n",
      "Iter 0540 | Time 15.4923(15.1052) | Bit/dim 4.9067(4.9750) | Xent 1.8646(1.8761) | Loss 768.0908(794.5048) | Error 0.6667(0.6592) Steps 0(0.00) | Grad Norm 946.4406(1597.5102) | Total Time 0.00(0.00)\n",
      "Iter 0550 | Time 14.7544(15.1637) | Bit/dim 4.8147(4.9439) | Xent 1.7806(1.8588) | Loss 747.5251(784.0525) | Error 0.6511(0.6536) Steps 0(0.00) | Grad Norm 1011.0453(1374.5299) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 80.9639, Epoch Time 935.7162(822.9085), Bit/dim 4.8396(best: 4.9740), Xent 1.7672, Loss 5.7232, Error 0.6158(best: 0.6245)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 16.0880(15.2268) | Bit/dim 4.8520(4.9234) | Xent 1.8028(1.8591) | Loss 756.6877(832.8544) | Error 0.6656(0.6550) Steps 0(0.00) | Grad Norm 1113.8148(1723.2523) | Total Time 0.00(0.00)\n",
      "Iter 0570 | Time 15.4221(15.3059) | Bit/dim 4.7877(4.8952) | Xent 1.9113(1.8627) | Loss 755.4554(813.4470) | Error 0.6956(0.6592) Steps 0(0.00) | Grad Norm 948.0454(1547.2825) | Total Time 0.00(0.00)\n",
      "Iter 0580 | Time 15.4393(15.3887) | Bit/dim 4.8521(4.8924) | Xent 1.9038(1.8913) | Loss 756.0394(801.1776) | Error 0.6644(0.6691) Steps 0(0.00) | Grad Norm 1154.9313(1885.7775) | Total Time 0.00(0.00)\n",
      "Iter 0590 | Time 15.5105(15.3718) | Bit/dim 4.7471(4.8697) | Xent 1.8937(1.8940) | Loss 738.8604(788.7267) | Error 0.6633(0.6700) Steps 0(0.00) | Grad Norm 733.7891(1632.9396) | Total Time 0.00(0.00)\n",
      "Iter 0600 | Time 15.5144(15.4112) | Bit/dim 4.7409(4.8453) | Xent 1.8516(1.8812) | Loss 754.6725(777.7961) | Error 0.6600(0.6642) Steps 0(0.00) | Grad Norm 341.9758(1398.1806) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 81.2134, Epoch Time 951.7313(826.7732), Bit/dim 4.7460(best: 4.8396), Xent 1.7296, Loss 5.6108, Error 0.5983(best: 0.6158)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 16.3050(15.4733) | Bit/dim 4.8917(4.8249) | Xent 1.8030(1.8550) | Loss 773.8191(832.8474) | Error 0.6422(0.6554) Steps 0(0.00) | Grad Norm 3666.8365(1316.9221) | Total Time 0.00(0.00)\n",
      "Iter 0620 | Time 15.2747(15.4354) | Bit/dim 4.7880(4.8552) | Xent 1.8790(1.8734) | Loss 760.2449(817.9342) | Error 0.6633(0.6602) Steps 0(0.00) | Grad Norm 846.4100(1592.3421) | Total Time 0.00(0.00)\n",
      "Iter 0630 | Time 16.3256(15.5352) | Bit/dim 4.7925(4.8454) | Xent 1.7947(1.8765) | Loss 761.0711(802.3104) | Error 0.6311(0.6620) Steps 0(0.00) | Grad Norm 858.8595(1365.8003) | Total Time 0.00(0.00)\n",
      "Iter 0640 | Time 16.0334(15.6506) | Bit/dim 4.7475(4.8193) | Xent 1.9055(1.8754) | Loss 767.2020(787.4738) | Error 0.6656(0.6624) Steps 0(0.00) | Grad Norm 1229.2628(1163.2421) | Total Time 0.00(0.00)\n",
      "Iter 0650 | Time 15.7728(15.6730) | Bit/dim 4.8702(4.8017) | Xent 1.9091(1.8647) | Loss 762.3959(776.4121) | Error 0.6922(0.6596) Steps 0(0.00) | Grad Norm 2609.6300(1296.4633) | Total Time 0.00(0.00)\n",
      "Iter 0660 | Time 15.3499(15.6891) | Bit/dim 4.7107(4.7844) | Xent 1.8251(1.8560) | Loss 745.4061(768.3797) | Error 0.6467(0.6561) Steps 0(0.00) | Grad Norm 480.2545(1295.8704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 81.7546, Epoch Time 964.6979(830.9109), Bit/dim 4.7075(best: 4.7460), Xent 1.7513, Loss 5.5832, Error 0.6195(best: 0.5983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 15.8790(15.7668) | Bit/dim 4.7046(4.7607) | Xent 1.7290(1.8292) | Loss 733.1332(812.6910) | Error 0.5989(0.6468) Steps 0(0.00) | Grad Norm 805.7920(1168.9190) | Total Time 0.00(0.00)\n",
      "Iter 0680 | Time 15.3531(15.7480) | Bit/dim 4.6747(4.7371) | Xent 1.7493(1.8054) | Loss 723.4375(791.3007) | Error 0.6067(0.6392) Steps 0(0.00) | Grad Norm 1003.2452(1119.4198) | Total Time 0.00(0.00)\n",
      "Iter 0690 | Time 15.7884(15.7716) | Bit/dim 4.6233(4.7132) | Xent 1.7404(1.7871) | Loss 715.5400(775.4279) | Error 0.6244(0.6325) Steps 0(0.00) | Grad Norm 761.6568(1102.6176) | Total Time 0.00(0.00)\n",
      "Iter 0700 | Time 15.8996(15.9492) | Bit/dim 4.7103(4.6995) | Xent 1.8664(1.7820) | Loss 758.7712(765.5364) | Error 0.6722(0.6317) Steps 0(0.00) | Grad Norm 2174.2386(1303.0105) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 15.8236(15.9501) | Bit/dim 4.6210(4.6831) | Xent 1.7441(1.7798) | Loss 740.4128(755.6837) | Error 0.6056(0.6312) Steps 0(0.00) | Grad Norm 1401.2851(1378.6524) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 84.1446, Epoch Time 981.3701(835.4247), Bit/dim 4.6374(best: 4.7075), Xent 1.6945, Loss 5.4847, Error 0.6094(best: 0.5983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 16.7418(15.9258) | Bit/dim 4.6013(4.6669) | Xent 1.7393(1.7692) | Loss 749.0186(815.1692) | Error 0.6178(0.6272) Steps 0(0.00) | Grad Norm 2181.3325(1442.0841) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 16.2278(15.9426) | Bit/dim 4.6245(4.6532) | Xent 1.7083(1.7502) | Loss 734.0535(791.6957) | Error 0.6200(0.6220) Steps 0(0.00) | Grad Norm 1337.2051(1457.0748) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 15.7065(15.9937) | Bit/dim 4.6000(4.6366) | Xent 1.7513(1.7378) | Loss 728.8032(773.3538) | Error 0.6289(0.6176) Steps 0(0.00) | Grad Norm 1676.7689(1334.5850) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 16.8133(16.1261) | Bit/dim 4.7054(4.6354) | Xent 1.9060(1.7421) | Loss 750.8120(763.2205) | Error 0.6856(0.6178) Steps 0(0.00) | Grad Norm 3904.5373(1563.0383) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 16.4484(16.2242) | Bit/dim 4.5808(4.6297) | Xent 1.7270(1.7481) | Loss 734.1530(755.7725) | Error 0.6267(0.6221) Steps 0(0.00) | Grad Norm 1147.6412(1548.5295) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 15.6548(16.2187) | Bit/dim 4.5958(4.6162) | Xent 1.7173(1.7453) | Loss 731.4813(748.0902) | Error 0.6156(0.6236) Steps 0(0.00) | Grad Norm 340.2967(1412.1857) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 83.8818, Epoch Time 994.3196(840.1915), Bit/dim 4.5732(best: 4.6374), Xent 1.6214, Loss 5.3839, Error 0.5735(best: 0.5983)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 15.2338(16.2067) | Bit/dim 4.5323(4.5999) | Xent 1.6393(1.7213) | Loss 696.5336(796.1442) | Error 0.5922(0.6155) Steps 0(0.00) | Grad Norm 766.4218(1272.0254) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 16.3202(16.1483) | Bit/dim 4.5437(4.5850) | Xent 1.6247(1.7136) | Loss 709.5851(775.4636) | Error 0.5889(0.6116) Steps 0(0.00) | Grad Norm 803.4244(1301.8518) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 16.9105(16.2166) | Bit/dim 4.5066(4.5683) | Xent 1.6367(1.6992) | Loss 707.8329(759.1848) | Error 0.6067(0.6077) Steps 0(0.00) | Grad Norm 241.5618(1175.4595) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 15.4366(16.2177) | Bit/dim 4.5054(4.5552) | Xent 1.6466(1.6923) | Loss 713.1904(747.5097) | Error 0.5844(0.6066) Steps 0(0.00) | Grad Norm 1354.6313(1247.3901) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 16.2576(16.3147) | Bit/dim 4.5069(4.5428) | Xent 1.6335(1.6771) | Loss 714.5152(739.2823) | Error 0.5956(0.6016) Steps 0(0.00) | Grad Norm 299.9452(1067.6849) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 84.6043, Epoch Time 998.3105(844.9351), Bit/dim 4.6235(best: 4.5732), Xent 1.7531, Loss 5.5000, Error 0.6278(best: 0.5735)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 16.0822(16.3092) | Bit/dim 4.6726(4.5465) | Xent 1.7526(1.6850) | Loss 719.4441(803.8105) | Error 0.6300(0.6038) Steps 0(0.00) | Grad Norm 2190.2416(1280.0110) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 17.0177(16.3870) | Bit/dim 4.5179(4.5414) | Xent 1.6732(1.6984) | Loss 731.9141(782.2608) | Error 0.5933(0.6084) Steps 0(0.00) | Grad Norm 431.4261(1217.8511) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 16.6651(16.3947) | Bit/dim 4.4547(4.5264) | Xent 1.6566(1.6862) | Loss 713.5222(762.7456) | Error 0.5822(0.6043) Steps 0(0.00) | Grad Norm 431.3316(1013.5357) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 15.7424(16.3697) | Bit/dim 4.4500(4.5092) | Xent 1.5699(1.6716) | Loss 701.5909(747.0551) | Error 0.5556(0.5987) Steps 0(0.00) | Grad Norm 821.4677(1002.5821) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 16.3875(16.4023) | Bit/dim 4.4873(4.5022) | Xent 1.7247(1.6773) | Loss 723.1232(739.0091) | Error 0.6367(0.6007) Steps 0(0.00) | Grad Norm 1625.9151(1213.2343) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 16.5971(16.4130) | Bit/dim 4.4136(4.4872) | Xent 1.5991(1.6675) | Loss 693.1870(729.2305) | Error 0.5922(0.5980) Steps 0(0.00) | Grad Norm 391.1760(1165.8337) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 86.5067, Epoch Time 1009.0568(849.8588), Bit/dim 4.4422(best: 4.5732), Xent 1.6172, Loss 5.2508, Error 0.5798(best: 0.5735)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 16.4387(16.4328) | Bit/dim 4.4634(4.4758) | Xent 1.5992(1.6619) | Loss 714.6509(781.4272) | Error 0.5767(0.5961) Steps 0(0.00) | Grad Norm 868.7894(1293.3364) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 15.9090(16.4453) | Bit/dim 4.3932(4.4581) | Xent 1.5969(1.6541) | Loss 676.2339(760.3057) | Error 0.5556(0.5942) Steps 0(0.00) | Grad Norm 711.3834(1192.0296) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 16.4614(16.5171) | Bit/dim 4.4778(4.4504) | Xent 1.5886(1.6434) | Loss 700.0831(745.2063) | Error 0.5667(0.5914) Steps 0(0.00) | Grad Norm 1633.5640(1221.9594) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 16.1672(16.4967) | Bit/dim 4.4326(4.4570) | Xent 1.6043(1.6481) | Loss 697.8780(735.3678) | Error 0.5878(0.5946) Steps 0(0.00) | Grad Norm 813.6439(1279.3106) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 17.4886(16.5502) | Bit/dim 4.4322(4.4490) | Xent 1.6476(1.6481) | Loss 706.3943(727.0697) | Error 0.6044(0.5946) Steps 0(0.00) | Grad Norm 559.4304(1182.3266) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 85.8269, Epoch Time 1015.3213(854.8226), Bit/dim 4.3801(best: 4.4422), Xent 1.5236, Loss 5.1418, Error 0.5527(best: 0.5735)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 16.5872(16.5373) | Bit/dim 4.3870(4.4293) | Xent 1.5540(1.6317) | Loss 691.0277(784.2918) | Error 0.5422(0.5902) Steps 0(0.00) | Grad Norm 398.6383(998.8177) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 17.1521(16.5614) | Bit/dim 4.3732(4.4137) | Xent 1.5849(1.6203) | Loss 701.0170(760.8822) | Error 0.5722(0.5858) Steps 0(0.00) | Grad Norm 815.8229(975.1458) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 16.0567(16.6662) | Bit/dim 4.3499(4.3946) | Xent 1.5828(1.6062) | Loss 689.9583(742.5331) | Error 0.5456(0.5795) Steps 0(0.00) | Grad Norm 927.0649(912.4336) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 16.0032(16.7902) | Bit/dim 4.3195(4.3780) | Xent 1.6485(1.5990) | Loss 678.8159(729.2184) | Error 0.5833(0.5795) Steps 0(0.00) | Grad Norm 1408.7953(923.2569) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 16.6910(16.8693) | Bit/dim 4.3520(4.3685) | Xent 1.5772(1.5911) | Loss 692.5766(718.7947) | Error 0.5689(0.5759) Steps 0(0.00) | Grad Norm 1085.7356(976.7154) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 17.7081(16.8830) | Bit/dim 4.3311(4.3570) | Xent 1.6101(1.5903) | Loss 681.1981(709.7885) | Error 0.5756(0.5764) Steps 0(0.00) | Grad Norm 1338.7752(1043.9821) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 86.2789, Epoch Time 1034.8826(860.2244), Bit/dim 4.3196(best: 4.3801), Xent 1.4841, Loss 5.0616, Error 0.5383(best: 0.5527)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 16.7464(16.8466) | Bit/dim 4.3069(4.3437) | Xent 1.5861(1.5782) | Loss 689.1296(759.8922) | Error 0.5722(0.5706) Steps 0(0.00) | Grad Norm 1022.7561(955.2925) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 17.3913(16.8168) | Bit/dim 4.2920(4.3339) | Xent 1.5993(1.5692) | Loss 682.3281(739.4169) | Error 0.5622(0.5683) Steps 0(0.00) | Grad Norm 1512.4964(990.9486) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 16.1301(16.8020) | Bit/dim 4.2587(4.3243) | Xent 1.5049(1.5630) | Loss 664.3151(724.6146) | Error 0.5422(0.5661) Steps 0(0.00) | Grad Norm 524.3714(991.8520) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 16.5184(16.8329) | Bit/dim 4.3004(4.3136) | Xent 1.6286(1.5534) | Loss 674.9979(712.2463) | Error 0.5756(0.5630) Steps 0(0.00) | Grad Norm 1631.7517(941.4869) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 16.3302(16.7886) | Bit/dim 4.3545(4.3263) | Xent 1.7310(1.5836) | Loss 685.2741(707.7175) | Error 0.6289(0.5713) Steps 0(0.00) | Grad Norm 1496.7753(1151.5944) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 82.4671, Epoch Time 1021.1205(865.0513), Bit/dim 4.3654(best: 4.3196), Xent 1.5190, Loss 5.1249, Error 0.5455(best: 0.5383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 16.6695(16.6871) | Bit/dim 4.2866(4.3277) | Xent 1.6351(1.5850) | Loss 677.1934(765.4920) | Error 0.6100(0.5721) Steps 0(0.00) | Grad Norm 1088.6339(1098.0061) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 15.9265(16.5729) | Bit/dim 4.2657(4.3156) | Xent 1.5084(1.5680) | Loss 674.9575(741.5369) | Error 0.5400(0.5671) Steps 0(0.00) | Grad Norm 356.5306(976.5957) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 16.3590(16.4394) | Bit/dim 4.2121(4.2958) | Xent 1.5164(1.5545) | Loss 659.8829(722.6272) | Error 0.5667(0.5634) Steps 0(0.00) | Grad Norm 470.7325(844.1970) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 16.2260(16.3850) | Bit/dim 4.2396(4.2786) | Xent 1.5059(1.5342) | Loss 665.6784(706.9336) | Error 0.5444(0.5553) Steps 0(0.00) | Grad Norm 318.4482(741.7944) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 16.5891(16.4386) | Bit/dim 4.1878(4.2675) | Xent 1.5294(1.5361) | Loss 679.9100(700.1247) | Error 0.5656(0.5549) Steps 0(0.00) | Grad Norm 571.0787(827.5575) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 16.1342(16.4693) | Bit/dim 4.2099(4.2578) | Xent 1.4605(1.5364) | Loss 660.1784(693.2245) | Error 0.5233(0.5559) Steps 0(0.00) | Grad Norm 572.8970(882.9870) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 86.1377, Epoch Time 1003.5109(869.2051), Bit/dim 4.2221(best: 4.3196), Xent 1.4351, Loss 4.9397, Error 0.5187(best: 0.5383)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 15.9067(16.4457) | Bit/dim 4.1982(4.2449) | Xent 1.5080(1.5243) | Loss 685.1751(743.8922) | Error 0.5544(0.5501) Steps 0(0.00) | Grad Norm 500.1337(831.4323) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 15.9472(16.3785) | Bit/dim 4.2176(4.2345) | Xent 1.5402(1.5143) | Loss 660.4803(722.2485) | Error 0.5667(0.5470) Steps 0(0.00) | Grad Norm 1407.9677(844.2673) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 16.7012(16.3552) | Bit/dim 4.1690(4.2223) | Xent 1.4249(1.4935) | Loss 662.2789(706.7072) | Error 0.5222(0.5415) Steps 0(0.00) | Grad Norm 1366.2327(874.6960) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 16.0099(16.3300) | Bit/dim 4.1960(4.2127) | Xent 1.4348(1.4835) | Loss 662.1674(695.3528) | Error 0.4967(0.5379) Steps 0(0.00) | Grad Norm 535.6441(845.2198) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 17.1488(16.3176) | Bit/dim 4.1428(4.2025) | Xent 1.4321(1.4732) | Loss 661.5265(685.4232) | Error 0.5400(0.5342) Steps 0(0.00) | Grad Norm 613.6249(785.7063) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 83.9287, Epoch Time 995.3808(872.9904), Bit/dim 4.1747(best: 4.2221), Xent 1.3666, Loss 4.8580, Error 0.4944(best: 0.5187)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 16.0870(16.2818) | Bit/dim 4.1822(4.1991) | Xent 1.4307(1.4715) | Loss 661.8214(744.1239) | Error 0.5200(0.5327) Steps 0(0.00) | Grad Norm 613.9872(850.0884) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 15.9087(16.2616) | Bit/dim 4.1757(4.2039) | Xent 1.4160(1.4674) | Loss 655.6729(723.5610) | Error 0.5311(0.5297) Steps 0(0.00) | Grad Norm 615.8408(879.7180) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 16.7261(16.3013) | Bit/dim 4.1641(4.1952) | Xent 1.3914(1.4625) | Loss 660.8173(707.4170) | Error 0.4933(0.5280) Steps 0(0.00) | Grad Norm 533.7167(891.9925) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 16.5230(16.2622) | Bit/dim 4.1474(4.1816) | Xent 1.4300(1.4522) | Loss 656.9378(693.7907) | Error 0.5256(0.5246) Steps 0(0.00) | Grad Norm 384.9712(765.4992) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 16.2866(16.2557) | Bit/dim 4.1195(4.1650) | Xent 1.4484(1.4387) | Loss 645.3906(682.0797) | Error 0.5233(0.5202) Steps 0(0.00) | Grad Norm 901.3569(712.0630) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 16.1662(16.1789) | Bit/dim 4.1983(4.1591) | Xent 1.5878(1.4483) | Loss 671.1587(675.3904) | Error 0.5667(0.5244) Steps 0(0.00) | Grad Norm 2373.9223(822.9961) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 84.4687, Epoch Time 995.2095(876.6569), Bit/dim 4.2261(best: 4.1747), Xent 1.6564, Loss 5.0543, Error 0.6016(best: 0.4944)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 16.6116(16.2320) | Bit/dim 4.1926(4.1686) | Xent 1.4108(1.4652) | Loss 667.6985(728.7935) | Error 0.4900(0.5316) Steps 0(0.00) | Grad Norm 717.6610(918.8795) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 15.7021(16.1362) | Bit/dim 4.1374(4.1628) | Xent 1.4040(1.4530) | Loss 649.1817(708.3306) | Error 0.5222(0.5272) Steps 0(0.00) | Grad Norm 682.3704(874.0882) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 15.5971(16.0730) | Bit/dim 4.1145(4.1506) | Xent 1.3413(1.4390) | Loss 652.3115(691.6407) | Error 0.4800(0.5218) Steps 0(0.00) | Grad Norm 381.3997(777.7147) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 16.0883(16.1894) | Bit/dim 4.1544(4.1399) | Xent 1.3617(1.4249) | Loss 634.7178(680.6326) | Error 0.4944(0.5170) Steps 0(0.00) | Grad Norm 911.5622(698.9724) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 16.0036(16.1693) | Bit/dim 4.1149(4.1307) | Xent 1.4186(1.4110) | Loss 659.9963(673.3187) | Error 0.5133(0.5112) Steps 0(0.00) | Grad Norm 1067.2120(696.0435) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 81.7323, Epoch Time 989.0227(880.0279), Bit/dim 4.0933(best: 4.1747), Xent 1.3078, Loss 4.7472, Error 0.4734(best: 0.4944)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 16.3048(16.1230) | Bit/dim 4.1044(4.1216) | Xent 1.4316(1.4040) | Loss 653.3689(731.2872) | Error 0.5089(0.5092) Steps 0(0.00) | Grad Norm 1187.9165(702.6724) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 16.0044(16.1263) | Bit/dim 4.0826(4.1130) | Xent 1.3773(1.3987) | Loss 631.7273(708.9023) | Error 0.5022(0.5088) Steps 0(0.00) | Grad Norm 676.5617(721.0897) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 15.8605(16.1628) | Bit/dim 4.1149(4.1080) | Xent 1.3553(1.3887) | Loss 651.4722(692.8602) | Error 0.4800(0.5049) Steps 0(0.00) | Grad Norm 509.7555(680.3912) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 16.7706(16.1671) | Bit/dim 4.0901(4.1004) | Xent 1.3372(1.3859) | Loss 649.9930(679.5324) | Error 0.5056(0.5026) Steps 0(0.00) | Grad Norm 897.9887(687.2098) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 15.2492(16.2042) | Bit/dim 4.0883(4.0917) | Xent 1.4458(1.3782) | Loss 653.1187(669.4713) | Error 0.5167(0.4979) Steps 0(0.00) | Grad Norm 616.4796(661.6541) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 16.4029(16.1643) | Bit/dim 4.0872(4.0877) | Xent 1.3574(1.3715) | Loss 657.2493(663.2218) | Error 0.4844(0.4951) Steps 0(0.00) | Grad Norm 834.6134(704.9916) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 82.7478, Epoch Time 990.3146(883.3365), Bit/dim 4.0738(best: 4.0933), Xent 1.3112, Loss 4.7294, Error 0.4774(best: 0.4734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 15.6244(16.1506) | Bit/dim 4.0567(4.0808) | Xent 1.3459(1.3563) | Loss 634.5435(711.3104) | Error 0.4867(0.4894) Steps 0(0.00) | Grad Norm 637.6186(703.2072) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 16.3169(16.1729) | Bit/dim 4.0445(4.0711) | Xent 1.4066(1.3521) | Loss 643.5431(691.1052) | Error 0.5233(0.4872) Steps 0(0.00) | Grad Norm 903.0592(682.1481) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 16.0218(16.1065) | Bit/dim 4.0611(4.0706) | Xent 1.3540(1.3422) | Loss 637.3280(677.6093) | Error 0.4900(0.4844) Steps 0(0.00) | Grad Norm 393.3042(669.2649) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 16.4423(16.1232) | Bit/dim 4.0460(4.0627) | Xent 1.3466(1.3421) | Loss 637.0071(666.5732) | Error 0.4767(0.4843) Steps 0(0.00) | Grad Norm 1004.3485(696.5069) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 15.5289(16.1344) | Bit/dim 4.0333(4.0593) | Xent 1.3771(1.3405) | Loss 640.3840(658.0762) | Error 0.4956(0.4850) Steps 0(0.00) | Grad Norm 704.5050(679.2701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 81.7618, Epoch Time 985.8951(886.4133), Bit/dim 4.0634(best: 4.0738), Xent 1.2473, Loss 4.6870, Error 0.4512(best: 0.4734)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 15.9602(16.0963) | Bit/dim 4.0219(4.0524) | Xent 1.2561(1.3368) | Loss 627.9823(717.0889) | Error 0.4633(0.4855) Steps 0(0.00) | Grad Norm 1017.3795(725.7862) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 15.9440(16.0032) | Bit/dim 4.0238(4.0497) | Xent 1.3372(1.3376) | Loss 625.5956(694.9664) | Error 0.4711(0.4852) Steps 0(0.00) | Grad Norm 480.3734(771.6457) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 16.6020(16.0711) | Bit/dim 4.0380(4.0462) | Xent 1.3555(1.3337) | Loss 642.6145(679.2721) | Error 0.4811(0.4836) Steps 0(0.00) | Grad Norm 572.2703(742.3850) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 15.5143(16.0643) | Bit/dim 4.0548(4.0425) | Xent 1.3115(1.3323) | Loss 634.1697(667.1986) | Error 0.4778(0.4831) Steps 0(0.00) | Grad Norm 419.8358(728.2308) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 16.3989(16.0006) | Bit/dim 3.9840(4.0364) | Xent 1.2862(1.3276) | Loss 623.5342(657.0142) | Error 0.4511(0.4810) Steps 0(0.00) | Grad Norm 593.1786(727.7376) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 15.9538(16.0008) | Bit/dim 4.0319(4.0295) | Xent 1.2901(1.3283) | Loss 620.2357(649.4816) | Error 0.4589(0.4812) Steps 0(0.00) | Grad Norm 877.3681(747.2015) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 83.4052, Epoch Time 980.7599(889.2437), Bit/dim 4.0085(best: 4.0634), Xent 1.2456, Loss 4.6313, Error 0.4537(best: 0.4512)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 16.4353(16.0174) | Bit/dim 4.0105(4.0250) | Xent 1.2761(1.3209) | Loss 646.9475(699.4352) | Error 0.4611(0.4786) Steps 0(0.00) | Grad Norm 711.9977(728.7820) | Total Time 0.00(0.00)\n",
      "Iter 1450 | Time 16.8131(16.0333) | Bit/dim 4.0250(4.0182) | Xent 1.2980(1.3093) | Loss 636.6925(681.3798) | Error 0.4744(0.4746) Steps 0(0.00) | Grad Norm 364.1131(656.5706) | Total Time 0.00(0.00)\n",
      "Iter 1460 | Time 15.3946(15.9743) | Bit/dim 3.9740(4.0127) | Xent 1.2747(1.3051) | Loss 619.6602(666.5716) | Error 0.4422(0.4711) Steps 0(0.00) | Grad Norm 681.9224(698.6285) | Total Time 0.00(0.00)\n",
      "Iter 1470 | Time 16.0184(15.9967) | Bit/dim 3.9892(4.0095) | Xent 1.2974(1.3015) | Loss 628.3319(656.0983) | Error 0.4822(0.4699) Steps 0(0.00) | Grad Norm 375.9522(666.7773) | Total Time 0.00(0.00)\n",
      "Iter 1480 | Time 15.3629(16.0320) | Bit/dim 4.0150(4.0055) | Xent 1.4057(1.3019) | Loss 633.0989(648.5133) | Error 0.5144(0.4717) Steps 0(0.00) | Grad Norm 627.0808(644.3405) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 82.7343, Epoch Time 981.3083(892.0056), Bit/dim 3.9882(best: 4.0085), Xent 1.2082, Loss 4.5923, Error 0.4386(best: 0.4512)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 15.9415(15.9868) | Bit/dim 3.9574(4.0036) | Xent 1.2191(1.2931) | Loss 618.2944(706.6233) | Error 0.4378(0.4682) Steps 0(0.00) | Grad Norm 987.2817(660.7032) | Total Time 0.00(0.00)\n",
      "Iter 1500 | Time 15.9082(15.8794) | Bit/dim 3.9951(4.0005) | Xent 1.2719(1.2919) | Loss 627.5444(685.5596) | Error 0.4478(0.4665) Steps 0(0.00) | Grad Norm 986.6512(715.7623) | Total Time 0.00(0.00)\n",
      "Iter 1510 | Time 16.1068(15.8265) | Bit/dim 4.0315(3.9990) | Xent 1.2342(1.2870) | Loss 621.0488(669.2315) | Error 0.4489(0.4654) Steps 0(0.00) | Grad Norm 856.9313(735.2825) | Total Time 0.00(0.00)\n",
      "Iter 1520 | Time 15.9490(15.9241) | Bit/dim 3.9916(3.9961) | Xent 1.3040(1.2817) | Loss 627.7233(657.8096) | Error 0.4678(0.4635) Steps 0(0.00) | Grad Norm 522.7711(703.6929) | Total Time 0.00(0.00)\n",
      "Iter 1530 | Time 16.1801(15.9305) | Bit/dim 3.9696(3.9910) | Xent 1.2840(1.2771) | Loss 635.5705(648.3623) | Error 0.4689(0.4622) Steps 0(0.00) | Grad Norm 412.8500(633.3077) | Total Time 0.00(0.00)\n",
      "Iter 1540 | Time 15.5272(15.8867) | Bit/dim 3.9423(3.9832) | Xent 1.2768(1.2770) | Loss 601.3602(640.4929) | Error 0.4622(0.4624) Steps 0(0.00) | Grad Norm 275.0887(602.3700) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 81.6453, Epoch Time 971.2018(894.3815), Bit/dim 3.9612(best: 3.9882), Xent 1.2133, Loss 4.5679, Error 0.4418(best: 0.4386)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 15.6928(15.9141) | Bit/dim 4.0129(3.9828) | Xent 1.2912(1.2751) | Loss 631.8481(691.7237) | Error 0.4656(0.4607) Steps 0(0.00) | Grad Norm 1388.5374(683.7214) | Total Time 0.00(0.00)\n",
      "Iter 1560 | Time 15.7371(15.9667) | Bit/dim 3.9518(3.9818) | Xent 1.2706(1.2694) | Loss 614.0922(673.1965) | Error 0.4489(0.4595) Steps 0(0.00) | Grad Norm 917.5199(708.1065) | Total Time 0.00(0.00)\n",
      "Iter 1570 | Time 15.2867(15.9245) | Bit/dim 3.9798(3.9763) | Xent 1.2203(1.2656) | Loss 606.5886(659.6666) | Error 0.4311(0.4577) Steps 0(0.00) | Grad Norm 769.9489(714.2449) | Total Time 0.00(0.00)\n",
      "Iter 1580 | Time 16.2580(15.9700) | Bit/dim 3.9533(3.9741) | Xent 1.2888(1.2673) | Loss 629.6682(650.5396) | Error 0.4678(0.4590) Steps 0(0.00) | Grad Norm 359.0573(766.7211) | Total Time 0.00(0.00)\n",
      "Iter 1590 | Time 15.6535(15.8858) | Bit/dim 3.9690(3.9709) | Xent 1.2570(1.2680) | Loss 607.2839(642.6114) | Error 0.4422(0.4578) Steps 0(0.00) | Grad Norm 569.9705(701.4675) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 80.9785, Epoch Time 976.0885(896.8327), Bit/dim 3.9573(best: 3.9612), Xent 1.1813, Loss 4.5480, Error 0.4314(best: 0.4386)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 15.4521(15.8606) | Bit/dim 3.9681(3.9679) | Xent 1.2544(1.2582) | Loss 622.8444(697.4576) | Error 0.4556(0.4553) Steps 0(0.00) | Grad Norm 415.1260(661.7459) | Total Time 0.00(0.00)\n",
      "Iter 1610 | Time 16.2430(15.8541) | Bit/dim 3.9448(3.9611) | Xent 1.2389(1.2498) | Loss 618.0535(676.2802) | Error 0.4356(0.4514) Steps 0(0.00) | Grad Norm 503.4028(606.2730) | Total Time 0.00(0.00)\n",
      "Iter 1620 | Time 15.5578(15.8964) | Bit/dim 3.9576(3.9565) | Xent 1.1720(1.2465) | Loss 609.5417(660.8814) | Error 0.4167(0.4509) Steps 0(0.00) | Grad Norm 483.5304(610.7829) | Total Time 0.00(0.00)\n",
      "Iter 1630 | Time 15.6526(15.8842) | Bit/dim 3.9145(3.9503) | Xent 1.2608(1.2369) | Loss 624.3552(648.9726) | Error 0.4600(0.4470) Steps 0(0.00) | Grad Norm 201.3496(575.1740) | Total Time 0.00(0.00)\n",
      "Iter 1640 | Time 15.5800(15.9019) | Bit/dim 3.9491(3.9510) | Xent 1.2189(1.2342) | Loss 619.4045(638.9329) | Error 0.4289(0.4444) Steps 0(0.00) | Grad Norm 449.3867(586.4974) | Total Time 0.00(0.00)\n",
      "Iter 1650 | Time 15.8160(15.8519) | Bit/dim 3.9595(3.9458) | Xent 1.3125(1.2424) | Loss 627.1927(633.1068) | Error 0.4711(0.4490) Steps 0(0.00) | Grad Norm 1015.5623(618.2875) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 81.9048, Epoch Time 972.9459(899.1161), Bit/dim 3.9343(best: 3.9573), Xent 1.1711, Loss 4.5199, Error 0.4244(best: 0.4314)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 15.0378(15.8401) | Bit/dim 3.9465(3.9449) | Xent 1.2097(1.2360) | Loss 620.7704(680.0552) | Error 0.4422(0.4473) Steps 0(0.00) | Grad Norm 1118.0658(624.9895) | Total Time 0.00(0.00)\n",
      "Iter 1670 | Time 15.8885(15.8679) | Bit/dim 3.9530(3.9441) | Xent 1.2403(1.2287) | Loss 618.8988(662.1222) | Error 0.4311(0.4429) Steps 0(0.00) | Grad Norm 1212.1557(692.0994) | Total Time 0.00(0.00)\n",
      "Iter 1680 | Time 15.0065(15.7898) | Bit/dim 3.9237(3.9409) | Xent 1.3075(1.2295) | Loss 612.6207(647.5790) | Error 0.4633(0.4426) Steps 0(0.00) | Grad Norm 1118.5486(676.6293) | Total Time 0.00(0.00)\n",
      "Iter 1690 | Time 15.4796(15.7812) | Bit/dim 3.9248(3.9387) | Xent 1.2190(1.2351) | Loss 594.9371(639.0837) | Error 0.4522(0.4462) Steps 0(0.00) | Grad Norm 295.2522(650.3488) | Total Time 0.00(0.00)\n",
      "Iter 1700 | Time 15.8082(15.7697) | Bit/dim 3.9610(3.9343) | Xent 1.2490(1.2314) | Loss 623.7007(631.8195) | Error 0.4367(0.4446) Steps 0(0.00) | Grad Norm 790.5246(639.2767) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 81.1615, Epoch Time 965.7534(901.1152), Bit/dim 3.9252(best: 3.9343), Xent 1.1405, Loss 4.4955, Error 0.4122(best: 0.4244)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 15.9155(15.7819) | Bit/dim 3.9391(3.9308) | Xent 1.2232(1.2252) | Loss 624.7701(690.6384) | Error 0.4322(0.4420) Steps 0(0.00) | Grad Norm 1185.4737(617.4594) | Total Time 0.00(0.00)\n",
      "Iter 1720 | Time 16.6382(15.7619) | Bit/dim 3.9665(3.9311) | Xent 1.1797(1.2226) | Loss 607.0927(669.7705) | Error 0.4300(0.4408) Steps 0(0.00) | Grad Norm 1032.5722(708.7575) | Total Time 0.00(0.00)\n",
      "Iter 1730 | Time 15.0665(15.8403) | Bit/dim 3.9185(3.9278) | Xent 1.1725(1.2175) | Loss 610.7188(655.9707) | Error 0.4244(0.4398) Steps 0(0.00) | Grad Norm 973.1903(730.4562) | Total Time 0.00(0.00)\n",
      "Iter 1740 | Time 15.5231(15.7843) | Bit/dim 3.9101(3.9256) | Xent 1.2783(1.2270) | Loss 614.2672(646.6790) | Error 0.4767(0.4420) Steps 0(0.00) | Grad Norm 1298.0514(763.8124) | Total Time 0.00(0.00)\n",
      "Iter 1750 | Time 15.4139(15.7395) | Bit/dim 3.9122(3.9238) | Xent 1.1824(1.2217) | Loss 603.2809(637.7123) | Error 0.4422(0.4411) Steps 0(0.00) | Grad Norm 659.9280(729.9290) | Total Time 0.00(0.00)\n",
      "Iter 1760 | Time 16.8226(15.7065) | Bit/dim 3.9154(3.9220) | Xent 1.1849(1.2158) | Loss 594.7921(629.5356) | Error 0.4244(0.4388) Steps 0(0.00) | Grad Norm 509.6754(681.0163) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 80.4161, Epoch Time 965.6436(903.0511), Bit/dim 3.9069(best: 3.9252), Xent 1.1589, Loss 4.4864, Error 0.4117(best: 0.4122)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 16.4140(15.6782) | Bit/dim 3.9446(3.9232) | Xent 1.2256(1.2078) | Loss 616.8210(678.6452) | Error 0.4256(0.4348) Steps 0(0.00) | Grad Norm 1024.4012(672.3623) | Total Time 0.00(0.00)\n",
      "Iter 1780 | Time 16.4519(15.7013) | Bit/dim 3.8912(3.9160) | Xent 1.1878(1.1990) | Loss 615.5645(660.2757) | Error 0.4322(0.4317) Steps 0(0.00) | Grad Norm 565.0697(620.6983) | Total Time 0.00(0.00)\n",
      "Iter 1790 | Time 15.6545(15.6844) | Bit/dim 3.8665(3.9105) | Xent 1.2176(1.2060) | Loss 607.8226(647.8352) | Error 0.4478(0.4345) Steps 0(0.00) | Grad Norm 480.8323(657.2834) | Total Time 0.00(0.00)\n",
      "Iter 1800 | Time 15.1596(15.6925) | Bit/dim 3.8621(3.9077) | Xent 1.1201(1.2023) | Loss 599.6454(637.4147) | Error 0.4144(0.4349) Steps 0(0.00) | Grad Norm 641.4858(649.6789) | Total Time 0.00(0.00)\n",
      "Iter 1810 | Time 15.2654(15.7056) | Bit/dim 3.9088(3.9064) | Xent 1.2016(1.1964) | Loss 604.4885(630.1522) | Error 0.4300(0.4318) Steps 0(0.00) | Grad Norm 912.2422(646.5213) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 81.4335, Epoch Time 962.3662(904.8305), Bit/dim 3.8985(best: 3.9069), Xent 1.1537, Loss 4.4754, Error 0.4164(best: 0.4117)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 15.7516(15.7280) | Bit/dim 3.9183(3.9069) | Xent 1.1052(1.1915) | Loss 601.4456(684.8561) | Error 0.3878(0.4294) Steps 0(0.00) | Grad Norm 549.1992(637.7736) | Total Time 0.00(0.00)\n",
      "Iter 1830 | Time 16.2329(15.8123) | Bit/dim 3.8694(3.9053) | Xent 1.1950(1.1897) | Loss 613.8727(665.4013) | Error 0.4267(0.4300) Steps 0(0.00) | Grad Norm 652.7941(663.7539) | Total Time 0.00(0.00)\n",
      "Iter 1840 | Time 16.3112(15.7734) | Bit/dim 3.8877(3.9033) | Xent 1.1785(1.1916) | Loss 595.5410(650.4500) | Error 0.4233(0.4294) Steps 0(0.00) | Grad Norm 910.2785(672.5573) | Total Time 0.00(0.00)\n",
      "Iter 1850 | Time 15.8290(15.8934) | Bit/dim 3.9137(3.9020) | Xent 1.2758(1.1968) | Loss 604.9159(639.4208) | Error 0.4911(0.4315) Steps 0(0.00) | Grad Norm 504.2458(689.3461) | Total Time 0.00(0.00)\n",
      "Iter 1860 | Time 15.8732(15.9336) | Bit/dim 3.8709(3.8984) | Xent 1.1802(1.1948) | Loss 601.1249(630.9276) | Error 0.4322(0.4321) Steps 0(0.00) | Grad Norm 631.2583(671.9411) | Total Time 0.00(0.00)\n",
      "Iter 1870 | Time 16.6141(15.9223) | Bit/dim 3.8750(3.8957) | Xent 1.2056(1.1904) | Loss 612.2548(624.3381) | Error 0.4278(0.4292) Steps 0(0.00) | Grad Norm 592.5250(676.3663) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 81.2016, Epoch Time 977.8302(907.0205), Bit/dim 3.8911(best: 3.8985), Xent 1.1490, Loss 4.4656, Error 0.4181(best: 0.4117)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 15.9974(15.8988) | Bit/dim 3.8930(3.8952) | Xent 1.2287(1.1908) | Loss 621.8580(671.4986) | Error 0.4444(0.4285) Steps 0(0.00) | Grad Norm 1198.4081(723.7238) | Total Time 0.00(0.00)\n",
      "Iter 1890 | Time 15.4033(15.8076) | Bit/dim 3.8488(3.8911) | Xent 1.1275(1.1851) | Loss 596.8657(653.7091) | Error 0.3978(0.4246) Steps 0(0.00) | Grad Norm 368.4973(695.1079) | Total Time 0.00(0.00)\n",
      "Iter 1900 | Time 16.1163(15.7900) | Bit/dim 3.8892(3.8914) | Xent 1.1885(1.1773) | Loss 592.0609(640.1188) | Error 0.4356(0.4226) Steps 0(0.00) | Grad Norm 524.8481(665.2309) | Total Time 0.00(0.00)\n",
      "Iter 1910 | Time 15.5020(15.7712) | Bit/dim 3.8906(3.8888) | Xent 1.1581(1.1766) | Loss 598.3585(631.0170) | Error 0.4300(0.4234) Steps 0(0.00) | Grad Norm 782.0567(700.7272) | Total Time 0.00(0.00)\n",
      "Iter 1920 | Time 15.1551(15.7320) | Bit/dim 3.8886(3.8917) | Xent 1.2882(1.1829) | Loss 604.3571(625.1534) | Error 0.4511(0.4233) Steps 0(0.00) | Grad Norm 494.2201(685.5473) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 81.2123, Epoch Time 961.6069(908.6581), Bit/dim 3.8771(best: 3.8911), Xent 1.1003, Loss 4.4273, Error 0.4017(best: 0.4117)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 15.1976(15.6711) | Bit/dim 3.8578(3.8899) | Xent 1.1407(1.1749) | Loss 605.7497(680.8065) | Error 0.4167(0.4212) Steps 0(0.00) | Grad Norm 435.6412(635.2292) | Total Time 0.00(0.00)\n",
      "Iter 1940 | Time 15.9055(15.7576) | Bit/dim 3.8762(3.8885) | Xent 1.1752(1.1703) | Loss 600.9343(660.0489) | Error 0.4111(0.4203) Steps 0(0.00) | Grad Norm 735.1965(670.6395) | Total Time 0.00(0.00)\n",
      "Iter 1950 | Time 16.1588(15.8353) | Bit/dim 3.8746(3.8862) | Xent 1.1116(1.1611) | Loss 611.1542(644.6845) | Error 0.4056(0.4172) Steps 0(0.00) | Grad Norm 501.0978(654.5769) | Total Time 0.00(0.00)\n",
      "Iter 1960 | Time 16.0309(15.7883) | Bit/dim 3.8691(3.8838) | Xent 1.2277(1.1608) | Loss 611.2424(635.1347) | Error 0.4267(0.4171) Steps 0(0.00) | Grad Norm 1443.0635(658.9615) | Total Time 0.00(0.00)\n",
      "Iter 1970 | Time 16.0001(15.8298) | Bit/dim 3.8423(3.8824) | Xent 1.1671(1.1614) | Loss 603.3298(628.3509) | Error 0.4056(0.4163) Steps 0(0.00) | Grad Norm 466.0402(721.8050) | Total Time 0.00(0.00)\n",
      "Iter 1980 | Time 15.8601(15.8295) | Bit/dim 3.8465(3.8812) | Xent 1.1559(1.1590) | Loss 615.3547(623.3128) | Error 0.4044(0.4155) Steps 0(0.00) | Grad Norm 608.7715(719.5202) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 81.7510, Epoch Time 972.2287(910.5652), Bit/dim 3.8809(best: 3.8771), Xent 1.1048, Loss 4.4333, Error 0.4020(best: 0.4017)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 16.2169(15.8951) | Bit/dim 3.8748(3.8782) | Xent 1.1028(1.1454) | Loss 606.1141(672.1533) | Error 0.4000(0.4116) Steps 0(0.00) | Grad Norm 485.6510(657.4113) | Total Time 0.00(0.00)\n",
      "Iter 2000 | Time 15.4919(15.8648) | Bit/dim 3.8763(3.8737) | Xent 1.0704(1.1334) | Loss 608.4019(653.8924) | Error 0.3900(0.4069) Steps 0(0.00) | Grad Norm 375.7698(619.5720) | Total Time 0.00(0.00)\n",
      "Iter 2010 | Time 15.4134(15.8067) | Bit/dim 3.9146(3.8738) | Xent 1.0730(1.1242) | Loss 604.0950(639.9088) | Error 0.3700(0.4025) Steps 0(0.00) | Grad Norm 495.7077(606.4606) | Total Time 0.00(0.00)\n",
      "Iter 2020 | Time 15.5234(15.8411) | Bit/dim 3.8502(3.8710) | Xent 1.1372(1.1171) | Loss 598.2131(629.0058) | Error 0.4111(0.4012) Steps 0(0.00) | Grad Norm 402.5066(602.3304) | Total Time 0.00(0.00)\n",
      "Iter 2030 | Time 15.4873(15.8251) | Bit/dim 3.8937(3.8722) | Xent 1.1786(1.1190) | Loss 611.6910(622.5882) | Error 0.4300(0.4031) Steps 0(0.00) | Grad Norm 1340.3681(612.0182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 81.8240, Epoch Time 971.2100(912.3846), Bit/dim 3.8685(best: 3.8771), Xent 1.0903, Loss 4.4136, Error 0.3905(best: 0.4017)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 16.5433(15.8274) | Bit/dim 3.8718(3.8728) | Xent 1.1563(1.1264) | Loss 609.2902(682.1699) | Error 0.4144(0.4046) Steps 0(0.00) | Grad Norm 654.4179(658.1774) | Total Time 0.00(0.00)\n",
      "Iter 2050 | Time 15.9553(15.8141) | Bit/dim 3.8676(3.8696) | Xent 1.0885(1.1246) | Loss 620.8956(662.0129) | Error 0.3922(0.4046) Steps 0(0.00) | Grad Norm 539.4441(648.1026) | Total Time 0.00(0.00)\n",
      "Iter 2060 | Time 15.8062(15.8050) | Bit/dim 3.8850(3.8699) | Xent 1.0942(1.1124) | Loss 614.3026(646.2766) | Error 0.3878(0.3992) Steps 0(0.00) | Grad Norm 627.2486(621.5012) | Total Time 0.00(0.00)\n",
      "Iter 2070 | Time 15.3611(15.7474) | Bit/dim 3.8419(3.8653) | Xent 1.0354(1.1014) | Loss 595.8583(634.0061) | Error 0.3667(0.3947) Steps 0(0.00) | Grad Norm 226.9435(573.5083) | Total Time 0.00(0.00)\n",
      "Iter 2080 | Time 14.9464(15.6430) | Bit/dim 3.8763(3.8619) | Xent 1.0589(1.0998) | Loss 591.3218(623.3014) | Error 0.3700(0.3942) Steps 0(0.00) | Grad Norm 933.1647(557.5553) | Total Time 0.00(0.00)\n",
      "Iter 2090 | Time 15.0875(15.6442) | Bit/dim 3.8564(3.8569) | Xent 1.1181(1.1001) | Loss 599.2380(617.4580) | Error 0.4044(0.3940) Steps 0(0.00) | Grad Norm 601.7463(554.6418) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 81.9318, Epoch Time 961.3819(913.8545), Bit/dim 3.8606(best: 3.8685), Xent 1.0431, Loss 4.3822, Error 0.3729(best: 0.3905)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 15.3763(15.7769) | Bit/dim 3.8772(3.8599) | Xent 1.0660(1.0933) | Loss 600.9144(666.6367) | Error 0.3900(0.3912) Steps 0(0.00) | Grad Norm 512.0996(601.4284) | Total Time 0.00(0.00)\n",
      "Iter 2110 | Time 16.6293(15.8155) | Bit/dim 3.8731(3.8570) | Xent 1.1273(1.0942) | Loss 608.1058(649.0775) | Error 0.4033(0.3919) Steps 0(0.00) | Grad Norm 648.1069(627.6206) | Total Time 0.00(0.00)\n",
      "Iter 2120 | Time 14.7488(15.7923) | Bit/dim 3.8453(3.8551) | Xent 1.0734(1.0859) | Loss 593.4495(635.5790) | Error 0.3722(0.3877) Steps 0(0.00) | Grad Norm 445.2561(607.0139) | Total Time 0.00(0.00)\n",
      "Iter 2130 | Time 15.3937(15.7451) | Bit/dim 3.8342(3.8516) | Xent 1.0041(1.0807) | Loss 583.0313(625.8896) | Error 0.3578(0.3851) Steps 0(0.00) | Grad Norm 247.1725(570.7630) | Total Time 0.00(0.00)\n",
      "Iter 2140 | Time 15.7508(15.7174) | Bit/dim 3.8353(3.8492) | Xent 1.0707(1.0768) | Loss 588.9523(617.6267) | Error 0.3711(0.3834) Steps 0(0.00) | Grad Norm 559.0544(582.4933) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 80.6291, Epoch Time 967.4126(915.4612), Bit/dim 3.8390(best: 3.8606), Xent 1.0458, Loss 4.3619, Error 0.3720(best: 0.3729)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 15.9120(15.6907) | Bit/dim 3.8343(3.8479) | Xent 1.0739(1.0753) | Loss 597.1473(671.7023) | Error 0.4044(0.3829) Steps 0(0.00) | Grad Norm 461.1357(578.2293) | Total Time 0.00(0.00)\n",
      "Iter 2160 | Time 15.4490(15.6902) | Bit/dim 3.8681(3.8508) | Xent 1.0971(1.0720) | Loss 584.2567(651.5901) | Error 0.3689(0.3805) Steps 0(0.00) | Grad Norm 1120.3086(607.0606) | Total Time 0.00(0.00)\n",
      "Iter 2170 | Time 15.8322(15.6899) | Bit/dim 3.8617(3.8497) | Xent 1.0304(1.0662) | Loss 603.0950(637.7638) | Error 0.3833(0.3793) Steps 0(0.00) | Grad Norm 576.9234(635.5153) | Total Time 0.00(0.00)\n",
      "Iter 2180 | Time 15.8092(15.7512) | Bit/dim 3.8511(3.8481) | Xent 1.0268(1.0568) | Loss 596.3739(627.5239) | Error 0.3522(0.3756) Steps 0(0.00) | Grad Norm 615.7886(607.6286) | Total Time 0.00(0.00)\n",
      "Iter 2190 | Time 16.3923(15.7790) | Bit/dim 3.8404(3.8463) | Xent 1.1130(1.0544) | Loss 612.8724(619.7538) | Error 0.3967(0.3744) Steps 0(0.00) | Grad Norm 722.5996(605.0518) | Total Time 0.00(0.00)\n",
      "Iter 2200 | Time 15.7026(15.8332) | Bit/dim 3.8066(3.8419) | Xent 1.0657(1.0610) | Loss 608.7086(614.2470) | Error 0.3878(0.3773) Steps 0(0.00) | Grad Norm 642.8302(628.1996) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 83.0391, Epoch Time 971.5424(917.1437), Bit/dim 3.8480(best: 3.8390), Xent 1.0136, Loss 4.3549, Error 0.3673(best: 0.3720)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 15.7419(15.8253) | Bit/dim 3.8228(3.8430) | Xent 1.0371(1.0598) | Loss 596.0574(664.2702) | Error 0.3822(0.3795) Steps 0(0.00) | Grad Norm 523.7593(631.7140) | Total Time 0.00(0.00)\n",
      "Iter 2220 | Time 15.4660(15.7481) | Bit/dim 3.8402(3.8416) | Xent 1.1167(1.0590) | Loss 609.3316(646.0312) | Error 0.4211(0.3795) Steps 0(0.00) | Grad Norm 967.3507(695.0863) | Total Time 0.00(0.00)\n",
      "Iter 2230 | Time 15.5297(15.7527) | Bit/dim 3.8488(3.8405) | Xent 0.9956(1.0579) | Loss 597.6663(633.4324) | Error 0.3556(0.3794) Steps 0(0.00) | Grad Norm 238.1754(662.3630) | Total Time 0.00(0.00)\n",
      "Iter 2240 | Time 15.4893(15.7326) | Bit/dim 3.7915(3.8385) | Xent 1.0742(1.0565) | Loss 582.2717(622.8175) | Error 0.3711(0.3773) Steps 0(0.00) | Grad Norm 487.7887(637.0199) | Total Time 0.00(0.00)\n",
      "Iter 2250 | Time 15.9569(15.7678) | Bit/dim 3.8156(3.8337) | Xent 1.0538(1.0543) | Loss 597.9235(616.0951) | Error 0.3689(0.3762) Steps 0(0.00) | Grad Norm 940.0281(659.9240) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 81.4515, Epoch Time 963.0349(918.5204), Bit/dim 3.8231(best: 3.8390), Xent 0.9900, Loss 4.3182, Error 0.3550(best: 0.3673)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 15.4592(15.6719) | Bit/dim 3.8565(3.8331) | Xent 0.9598(1.0483) | Loss 595.2698(671.5970) | Error 0.3467(0.3736) Steps 0(0.00) | Grad Norm 608.7475(626.9707) | Total Time 0.00(0.00)\n",
      "Iter 2270 | Time 15.7255(15.6401) | Bit/dim 3.8514(3.8321) | Xent 1.0707(1.0460) | Loss 604.7610(650.4785) | Error 0.3711(0.3732) Steps 0(0.00) | Grad Norm 759.6872(621.8437) | Total Time 0.00(0.00)\n",
      "Iter 2280 | Time 15.3767(15.6471) | Bit/dim 3.8692(3.8292) | Xent 1.0199(1.0469) | Loss 603.3806(635.9028) | Error 0.3800(0.3744) Steps 0(0.00) | Grad Norm 873.9969(645.1894) | Total Time 0.00(0.00)\n",
      "Iter 2290 | Time 15.9094(15.6758) | Bit/dim 3.8457(3.8304) | Xent 1.0317(1.0423) | Loss 608.1363(624.5475) | Error 0.3600(0.3724) Steps 0(0.00) | Grad Norm 565.6129(657.2138) | Total Time 0.00(0.00)\n",
      "Iter 2300 | Time 16.2234(15.6597) | Bit/dim 3.8301(3.8314) | Xent 1.0823(1.0537) | Loss 596.8302(617.3436) | Error 0.3933(0.3761) Steps 0(0.00) | Grad Norm 733.2853(644.3116) | Total Time 0.00(0.00)\n",
      "Iter 2310 | Time 16.2956(15.6508) | Bit/dim 3.8558(3.8332) | Xent 1.0988(1.0536) | Loss 595.3622(611.7794) | Error 0.4133(0.3766) Steps 0(0.00) | Grad Norm 818.9912(633.4859) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 81.6677, Epoch Time 959.4724(919.7490), Bit/dim 3.8248(best: 3.8231), Xent 1.0013, Loss 4.3254, Error 0.3554(best: 0.3550)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 16.0621(15.7365) | Bit/dim 3.8218(3.8277) | Xent 1.0378(1.0390) | Loss 590.7160(658.2405) | Error 0.3622(0.3719) Steps 0(0.00) | Grad Norm 595.1343(577.1721) | Total Time 0.00(0.00)\n",
      "Iter 2330 | Time 15.1324(15.7156) | Bit/dim 3.8167(3.8278) | Xent 1.0180(1.0270) | Loss 577.8099(640.3757) | Error 0.3467(0.3654) Steps 0(0.00) | Grad Norm 478.1359(541.6887) | Total Time 0.00(0.00)\n",
      "Iter 2340 | Time 15.7471(15.6969) | Bit/dim 3.7911(3.8261) | Xent 1.0054(1.0211) | Loss 598.8566(628.1907) | Error 0.3633(0.3642) Steps 0(0.00) | Grad Norm 644.0781(570.5125) | Total Time 0.00(0.00)\n",
      "Iter 2350 | Time 15.7655(15.6885) | Bit/dim 3.8372(3.8257) | Xent 1.0530(1.0238) | Loss 589.7938(619.0626) | Error 0.3700(0.3656) Steps 0(0.00) | Grad Norm 948.1926(588.8183) | Total Time 0.00(0.00)\n",
      "Iter 2360 | Time 15.1671(15.6227) | Bit/dim 3.8344(3.8233) | Xent 1.0650(1.0346) | Loss 588.1207(611.6129) | Error 0.3878(0.3690) Steps 0(0.00) | Grad Norm 998.7829(659.3590) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 82.3145, Epoch Time 962.5009(921.0315), Bit/dim 3.8364(best: 3.8231), Xent 1.0829, Loss 4.3779, Error 0.3923(best: 0.3550)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 15.6230(15.6309) | Bit/dim 3.8064(3.8267) | Xent 0.9935(1.0391) | Loss 586.2658(671.7668) | Error 0.3400(0.3712) Steps 0(0.00) | Grad Norm 497.1707(674.0925) | Total Time 0.00(0.00)\n",
      "Iter 2380 | Time 15.8572(15.6931) | Bit/dim 3.8208(3.8243) | Xent 1.0255(1.0299) | Loss 594.0804(650.5730) | Error 0.3656(0.3682) Steps 0(0.00) | Grad Norm 439.3531(642.4018) | Total Time 0.00(0.00)\n",
      "Iter 2390 | Time 15.3055(15.6336) | Bit/dim 3.8308(3.8220) | Xent 0.9583(1.0190) | Loss 598.2686(634.8939) | Error 0.3422(0.3650) Steps 0(0.00) | Grad Norm 266.6510(560.3267) | Total Time 0.00(0.00)\n",
      "Iter 2400 | Time 16.1583(15.6788) | Bit/dim 3.8322(3.8230) | Xent 1.0262(1.0112) | Loss 591.7299(623.6674) | Error 0.3700(0.3619) Steps 0(0.00) | Grad Norm 444.7486(535.4750) | Total Time 0.00(0.00)\n",
      "Iter 2410 | Time 15.1693(15.6381) | Bit/dim 3.8016(3.8158) | Xent 1.0196(1.0027) | Loss 596.3939(614.5456) | Error 0.3433(0.3587) Steps 0(0.00) | Grad Norm 612.2190(504.4800) | Total Time 0.00(0.00)\n",
      "Iter 2420 | Time 15.4648(15.6824) | Bit/dim 3.8247(3.8149) | Xent 1.0470(1.0042) | Loss 594.5691(609.0217) | Error 0.3633(0.3585) Steps 0(0.00) | Grad Norm 462.3818(514.5667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 81.7376, Epoch Time 962.7712(922.2837), Bit/dim 3.8138(best: 3.8231), Xent 0.9467, Loss 4.2871, Error 0.3387(best: 0.3550)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 16.5651(15.6794) | Bit/dim 3.8108(3.8165) | Xent 0.9943(0.9959) | Loss 591.9049(657.4038) | Error 0.3689(0.3572) Steps 0(0.00) | Grad Norm 561.0782(526.9287) | Total Time 0.00(0.00)\n",
      "Iter 2440 | Time 16.3964(15.6954) | Bit/dim 3.8416(3.8185) | Xent 0.9855(0.9943) | Loss 597.4216(640.1566) | Error 0.3467(0.3553) Steps 0(0.00) | Grad Norm 284.1232(575.3878) | Total Time 0.00(0.00)\n",
      "Iter 2450 | Time 16.4500(15.7508) | Bit/dim 3.8269(3.8153) | Xent 0.9888(0.9955) | Loss 583.2474(627.8133) | Error 0.3456(0.3561) Steps 0(0.00) | Grad Norm 552.8883(582.2499) | Total Time 0.00(0.00)\n",
      "Iter 2460 | Time 16.3569(15.8062) | Bit/dim 3.8384(3.8120) | Xent 1.0512(0.9981) | Loss 601.6156(617.3672) | Error 0.3922(0.3567) Steps 0(0.00) | Grad Norm 712.6300(640.0348) | Total Time 0.00(0.00)\n",
      "Iter 2470 | Time 15.3713(15.7736) | Bit/dim 3.7990(3.8136) | Xent 1.0815(1.0080) | Loss 595.6110(610.0736) | Error 0.4033(0.3593) Steps 0(0.00) | Grad Norm 822.2937(659.1174) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 82.0916, Epoch Time 966.5403(923.6114), Bit/dim 3.8103(best: 3.8138), Xent 0.9428, Loss 4.2817, Error 0.3371(best: 0.3387)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 16.1078(15.7215) | Bit/dim 3.7969(3.8116) | Xent 1.0107(1.0041) | Loss 597.7609(670.7326) | Error 0.3756(0.3578) Steps 0(0.00) | Grad Norm 644.5839(619.6243) | Total Time 0.00(0.00)\n",
      "Iter 2490 | Time 15.4310(15.6240) | Bit/dim 3.8203(3.8095) | Xent 1.0094(0.9964) | Loss 595.9675(648.9003) | Error 0.3678(0.3552) Steps 0(0.00) | Grad Norm 421.1195(590.4201) | Total Time 0.00(0.00)\n",
      "Iter 2500 | Time 15.5841(15.6284) | Bit/dim 3.8083(3.8090) | Xent 0.9614(0.9875) | Loss 581.1829(633.3476) | Error 0.3311(0.3529) Steps 0(0.00) | Grad Norm 526.1917(558.3018) | Total Time 0.00(0.00)\n",
      "Iter 2510 | Time 15.5060(15.6389) | Bit/dim 3.7934(3.8067) | Xent 0.9869(0.9810) | Loss 581.7388(621.0148) | Error 0.3600(0.3506) Steps 0(0.00) | Grad Norm 953.6943(564.2677) | Total Time 0.00(0.00)\n",
      "Iter 2520 | Time 15.4622(15.6061) | Bit/dim 3.7270(3.8064) | Xent 1.0406(0.9865) | Loss 573.9271(612.5313) | Error 0.3678(0.3508) Steps 0(0.00) | Grad Norm 445.1402(614.9116) | Total Time 0.00(0.00)\n",
      "Iter 2530 | Time 15.9547(15.6548) | Bit/dim 3.8092(3.8069) | Xent 0.9869(0.9823) | Loss 584.7635(606.7839) | Error 0.3278(0.3492) Steps 0(0.00) | Grad Norm 836.1884(596.8126) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 81.6602, Epoch Time 958.6729(924.6633), Bit/dim 3.7968(best: 3.8103), Xent 0.9347, Loss 4.2641, Error 0.3351(best: 0.3371)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 15.7445(15.6348) | Bit/dim 3.8183(3.8047) | Xent 0.9311(0.9797) | Loss 576.3682(654.7139) | Error 0.3433(0.3491) Steps 0(0.00) | Grad Norm 547.4933(573.9868) | Total Time 0.00(0.00)\n",
      "Iter 2550 | Time 14.9217(15.6103) | Bit/dim 3.7772(3.8046) | Xent 0.9296(0.9732) | Loss 583.1405(636.8391) | Error 0.3489(0.3480) Steps 0(0.00) | Grad Norm 665.6552(597.6548) | Total Time 0.00(0.00)\n",
      "Iter 2560 | Time 15.3653(15.6429) | Bit/dim 3.7978(3.8025) | Xent 0.9139(0.9700) | Loss 582.6058(624.3468) | Error 0.3433(0.3470) Steps 0(0.00) | Grad Norm 352.1214(595.0066) | Total Time 0.00(0.00)\n",
      "Iter 2570 | Time 14.7144(15.6645) | Bit/dim 3.7988(3.8011) | Xent 0.9216(0.9722) | Loss 596.7212(615.7273) | Error 0.3278(0.3468) Steps 0(0.00) | Grad Norm 708.4909(588.2991) | Total Time 0.00(0.00)\n",
      "Iter 2580 | Time 15.1270(15.6536) | Bit/dim 3.7826(3.8002) | Xent 0.8588(0.9678) | Loss 589.6386(607.9072) | Error 0.2989(0.3450) Steps 0(0.00) | Grad Norm 429.7743(578.2039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 83.5664, Epoch Time 962.9777(925.8127), Bit/dim 3.7918(best: 3.7968), Xent 0.9469, Loss 4.2653, Error 0.3374(best: 0.3351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 16.0587(15.6717) | Bit/dim 3.7710(3.7978) | Xent 0.9855(0.9660) | Loss 581.8539(666.8812) | Error 0.3478(0.3449) Steps 0(0.00) | Grad Norm 728.7383(563.2432) | Total Time 0.00(0.00)\n",
      "Iter 2600 | Time 16.3687(15.6993) | Bit/dim 3.8216(3.7973) | Xent 0.9614(0.9733) | Loss 587.9249(646.2552) | Error 0.3322(0.3472) Steps 0(0.00) | Grad Norm 411.5501(605.4521) | Total Time 0.00(0.00)\n",
      "Iter 2610 | Time 15.6716(15.7143) | Bit/dim 3.7804(3.7964) | Xent 0.9173(0.9690) | Loss 573.2861(630.8828) | Error 0.3211(0.3461) Steps 0(0.00) | Grad Norm 449.1823(611.3359) | Total Time 0.00(0.00)\n",
      "Iter 2620 | Time 15.4826(15.6922) | Bit/dim 3.8284(3.7965) | Xent 0.9845(0.9673) | Loss 592.1725(619.9995) | Error 0.3367(0.3452) Steps 0(0.00) | Grad Norm 645.1138(598.1880) | Total Time 0.00(0.00)\n",
      "Iter 2630 | Time 16.1479(15.7597) | Bit/dim 3.7917(3.7982) | Xent 1.0730(0.9751) | Loss 598.7958(612.4952) | Error 0.3811(0.3487) Steps 0(0.00) | Grad Norm 675.1205(601.0391) | Total Time 0.00(0.00)\n",
      "Iter 2640 | Time 16.2854(15.8084) | Bit/dim 3.7641(3.7974) | Xent 1.0222(0.9854) | Loss 587.3102(606.5063) | Error 0.3778(0.3532) Steps 0(0.00) | Grad Norm 722.0033(660.8409) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 83.8843, Epoch Time 971.6106(927.1866), Bit/dim 3.8005(best: 3.7918), Xent 0.9421, Loss 4.2715, Error 0.3342(best: 0.3351)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 15.8115(15.7565) | Bit/dim 3.7599(3.7960) | Xent 0.9227(0.9818) | Loss 584.4714(658.0004) | Error 0.3244(0.3504) Steps 0(0.00) | Grad Norm 550.4649(672.7306) | Total Time 0.00(0.00)\n",
      "Iter 2660 | Time 14.9027(15.7275) | Bit/dim 3.7961(3.7962) | Xent 0.9209(0.9692) | Loss 588.0615(640.4029) | Error 0.3200(0.3451) Steps 0(0.00) | Grad Norm 446.3875(602.2771) | Total Time 0.00(0.00)\n",
      "Iter 2670 | Time 16.2444(15.7226) | Bit/dim 3.8021(3.7937) | Xent 0.9067(0.9600) | Loss 591.1306(627.5599) | Error 0.3400(0.3440) Steps 0(0.00) | Grad Norm 317.7046(560.3841) | Total Time 0.00(0.00)\n",
      "Iter 2680 | Time 16.2532(15.7298) | Bit/dim 3.7727(3.7946) | Xent 0.9481(0.9524) | Loss 591.0308(618.7084) | Error 0.3133(0.3395) Steps 0(0.00) | Grad Norm 804.6127(558.6942) | Total Time 0.00(0.00)\n",
      "Iter 2690 | Time 14.9493(15.6884) | Bit/dim 3.8261(3.7936) | Xent 0.9450(0.9517) | Loss 595.6779(611.7936) | Error 0.3422(0.3397) Steps 0(0.00) | Grad Norm 450.3260(549.5683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 84.4576, Epoch Time 965.1191(928.3246), Bit/dim 3.7874(best: 3.7918), Xent 0.9192, Loss 4.2470, Error 0.3285(best: 0.3342)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 15.0466(15.6557) | Bit/dim 3.8108(3.7898) | Xent 0.9224(0.9491) | Loss 593.9442(669.2376) | Error 0.3222(0.3389) Steps 0(0.00) | Grad Norm 663.6085(549.6443) | Total Time 0.00(0.00)\n",
      "Iter 2710 | Time 15.4004(15.6766) | Bit/dim 3.7931(3.7897) | Xent 0.9717(0.9497) | Loss 591.1495(648.9162) | Error 0.3344(0.3385) Steps 0(0.00) | Grad Norm 846.8715(597.4866) | Total Time 0.00(0.00)\n",
      "Iter 2720 | Time 16.0033(15.6453) | Bit/dim 3.7761(3.7862) | Xent 0.9933(0.9416) | Loss 600.5599(632.6811) | Error 0.3344(0.3354) Steps 0(0.00) | Grad Norm 1026.9992(578.9016) | Total Time 0.00(0.00)\n",
      "Iter 2730 | Time 15.0205(15.6412) | Bit/dim 3.7936(3.7887) | Xent 1.0023(0.9568) | Loss 588.0780(622.1251) | Error 0.3544(0.3411) Steps 0(0.00) | Grad Norm 493.6979(617.7376) | Total Time 0.00(0.00)\n",
      "Iter 2740 | Time 16.2468(15.7751) | Bit/dim 3.7615(3.7878) | Xent 0.9801(0.9604) | Loss 590.8594(613.4924) | Error 0.3633(0.3432) Steps 0(0.00) | Grad Norm 730.5928(627.5334) | Total Time 0.00(0.00)\n",
      "Iter 2750 | Time 15.6638(15.7051) | Bit/dim 3.7618(3.7873) | Xent 0.9209(0.9615) | Loss 587.6476(607.6583) | Error 0.3344(0.3431) Steps 0(0.00) | Grad Norm 546.6341(619.8964) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 84.1443, Epoch Time 964.8330(929.4198), Bit/dim 3.7866(best: 3.7874), Xent 0.8999, Loss 4.2365, Error 0.3245(best: 0.3285)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 15.3872(15.6510) | Bit/dim 3.7785(3.7862) | Xent 0.8966(0.9443) | Loss 580.8972(658.5842) | Error 0.3167(0.3364) Steps 0(0.00) | Grad Norm 481.3463(577.1445) | Total Time 0.00(0.00)\n",
      "Iter 2770 | Time 15.8205(15.7329) | Bit/dim 3.7840(3.7824) | Xent 0.9223(0.9439) | Loss 579.7265(640.3325) | Error 0.3067(0.3357) Steps 0(0.00) | Grad Norm 531.0406(583.0535) | Total Time 0.00(0.00)\n",
      "Iter 2780 | Time 16.0788(15.7752) | Bit/dim 3.7993(3.7826) | Xent 0.9423(0.9471) | Loss 588.9551(626.2885) | Error 0.3278(0.3388) Steps 0(0.00) | Grad Norm 570.5127(581.4126) | Total Time 0.00(0.00)\n",
      "Iter 2790 | Time 15.8767(15.7477) | Bit/dim 3.7709(3.7840) | Xent 0.9791(0.9474) | Loss 598.1057(616.8425) | Error 0.3522(0.3397) Steps 0(0.00) | Grad Norm 491.2811(583.2000) | Total Time 0.00(0.00)\n",
      "Iter 2800 | Time 16.3616(15.7872) | Bit/dim 3.7545(3.7827) | Xent 0.8885(0.9456) | Loss 593.2773(609.6269) | Error 0.3167(0.3380) Steps 0(0.00) | Grad Norm 449.0993(573.3654) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 83.8377, Epoch Time 969.9935(930.6371), Bit/dim 3.7812(best: 3.7866), Xent 0.8918, Loss 4.2271, Error 0.3210(best: 0.3245)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 15.9433(15.7589) | Bit/dim 3.7580(3.7810) | Xent 0.8838(0.9348) | Loss 577.5467(667.9919) | Error 0.3089(0.3335) Steps 0(0.00) | Grad Norm 373.7067(557.4840) | Total Time 0.00(0.00)\n",
      "Iter 2820 | Time 15.0103(15.7347) | Bit/dim 3.7697(3.7793) | Xent 0.8329(0.9251) | Loss 565.5602(645.7493) | Error 0.2789(0.3304) Steps 0(0.00) | Grad Norm 381.4114(526.8864) | Total Time 0.00(0.00)\n",
      "Iter 2830 | Time 15.3682(15.6606) | Bit/dim 3.7569(3.7782) | Xent 0.9502(0.9228) | Loss 570.8089(629.2809) | Error 0.3500(0.3297) Steps 0(0.00) | Grad Norm 541.0793(555.1895) | Total Time 0.00(0.00)\n",
      "Iter 2840 | Time 15.4975(15.6632) | Bit/dim 3.7973(3.7799) | Xent 0.8773(0.9165) | Loss 592.4192(618.7992) | Error 0.3167(0.3277) Steps 0(0.00) | Grad Norm 598.5175(533.9933) | Total Time 0.00(0.00)\n",
      "Iter 2850 | Time 16.2561(15.7068) | Bit/dim 3.7704(3.7807) | Xent 1.0076(0.9421) | Loss 589.6272(611.6364) | Error 0.3667(0.3377) Steps 0(0.00) | Grad Norm 724.9523(653.4069) | Total Time 0.00(0.00)\n",
      "Iter 2860 | Time 15.7531(15.7243) | Bit/dim 3.7615(3.7764) | Xent 0.8946(0.9463) | Loss 578.5502(604.5363) | Error 0.3344(0.3399) Steps 0(0.00) | Grad Norm 463.9467(618.2919) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 82.5084, Epoch Time 962.8533(931.6035), Bit/dim 3.7795(best: 3.7812), Xent 0.9034, Loss 4.2312, Error 0.3211(best: 0.3210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 15.9849(15.7403) | Bit/dim 3.7607(3.7747) | Xent 0.9522(0.9433) | Loss 583.2111(653.2753) | Error 0.3311(0.3365) Steps 0(0.00) | Grad Norm 716.9121(599.2066) | Total Time 0.00(0.00)\n",
      "Iter 2880 | Time 15.2407(15.7072) | Bit/dim 3.7838(3.7767) | Xent 0.9090(0.9392) | Loss 565.5936(634.9903) | Error 0.3211(0.3357) Steps 0(0.00) | Grad Norm 384.7417(595.0180) | Total Time 0.00(0.00)\n",
      "Iter 2890 | Time 15.4097(15.6380) | Bit/dim 3.7521(3.7742) | Xent 0.9290(0.9330) | Loss 584.5596(620.9635) | Error 0.3233(0.3337) Steps 0(0.00) | Grad Norm 457.1238(564.2787) | Total Time 0.00(0.00)\n",
      "Iter 2900 | Time 15.4213(15.6203) | Bit/dim 3.7629(3.7719) | Xent 0.7950(0.9166) | Loss 574.0898(611.1357) | Error 0.2778(0.3284) Steps 0(0.00) | Grad Norm 778.0194(549.3586) | Total Time 0.00(0.00)\n",
      "Iter 2910 | Time 15.5196(15.5986) | Bit/dim 3.7773(3.7710) | Xent 0.9590(0.9179) | Loss 586.0589(604.4082) | Error 0.3689(0.3310) Steps 0(0.00) | Grad Norm 550.8335(547.4887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 82.9957, Epoch Time 960.2880(932.4641), Bit/dim 3.7674(best: 3.7795), Xent 0.9199, Loss 4.2274, Error 0.3245(best: 0.3210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 15.5611(15.6156) | Bit/dim 3.7312(3.7688) | Xent 0.8648(0.9099) | Loss 564.4649(662.8591) | Error 0.3111(0.3267) Steps 0(0.00) | Grad Norm 473.0308(566.4520) | Total Time 0.00(0.00)\n",
      "Iter 2930 | Time 14.6295(15.5896) | Bit/dim 3.7147(3.7663) | Xent 0.8351(0.9050) | Loss 579.0767(641.3900) | Error 0.2844(0.3234) Steps 0(0.00) | Grad Norm 477.4468(553.8951) | Total Time 0.00(0.00)\n",
      "Iter 2940 | Time 15.8768(15.5652) | Bit/dim 3.7230(3.7666) | Xent 0.8007(0.9038) | Loss 591.1944(626.2972) | Error 0.2900(0.3228) Steps 0(0.00) | Grad Norm 334.3106(572.4864) | Total Time 0.00(0.00)\n",
      "Iter 2950 | Time 15.4911(15.5457) | Bit/dim 3.7688(3.7680) | Xent 0.9309(0.9102) | Loss 586.9425(615.7464) | Error 0.3378(0.3244) Steps 0(0.00) | Grad Norm 683.6702(597.4386) | Total Time 0.00(0.00)\n",
      "Iter 2960 | Time 15.7477(15.5904) | Bit/dim 3.7932(3.7701) | Xent 0.9655(0.9159) | Loss 597.8138(609.7745) | Error 0.3333(0.3252) Steps 0(0.00) | Grad Norm 823.2786(660.1797) | Total Time 0.00(0.00)\n",
      "Iter 2970 | Time 15.6036(15.6008) | Bit/dim 3.7494(3.7715) | Xent 0.9645(0.9127) | Loss 579.1617(603.8562) | Error 0.3489(0.3248) Steps 0(0.00) | Grad Norm 511.4883(635.3014) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 83.3599, Epoch Time 958.2095(933.2364), Bit/dim 3.7680(best: 3.7674), Xent 0.9038, Loss 4.2200, Error 0.3212(best: 0.3210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 16.7983(15.6434) | Bit/dim 3.7914(3.7704) | Xent 0.8733(0.9114) | Loss 596.5266(653.8035) | Error 0.3078(0.3259) Steps 0(0.00) | Grad Norm 435.4637(611.5807) | Total Time 0.00(0.00)\n",
      "Iter 2990 | Time 15.1851(15.6247) | Bit/dim 3.7686(3.7693) | Xent 0.9036(0.9121) | Loss 572.5284(635.0254) | Error 0.3167(0.3251) Steps 0(0.00) | Grad Norm 510.6155(609.8852) | Total Time 0.00(0.00)\n",
      "Iter 3000 | Time 14.7223(15.5735) | Bit/dim 3.7992(3.7672) | Xent 0.9309(0.9094) | Loss 587.7784(621.1804) | Error 0.3378(0.3247) Steps 0(0.00) | Grad Norm 655.6229(595.6626) | Total Time 0.00(0.00)\n",
      "Iter 3010 | Time 15.0422(15.5459) | Bit/dim 3.7734(3.7656) | Xent 0.9055(0.9108) | Loss 586.7481(611.8231) | Error 0.3289(0.3250) Steps 0(0.00) | Grad Norm 518.7028(567.3126) | Total Time 0.00(0.00)\n",
      "Iter 3020 | Time 15.5534(15.5521) | Bit/dim 3.7654(3.7609) | Xent 0.9381(0.9061) | Loss 595.8521(603.9371) | Error 0.3433(0.3224) Steps 0(0.00) | Grad Norm 644.7442(557.0057) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 83.7097, Epoch Time 958.1192(933.9829), Bit/dim 3.7677(best: 3.7674), Xent 0.8653, Loss 4.2004, Error 0.3120(best: 0.3210)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 15.2479(15.5548) | Bit/dim 3.7546(3.7638) | Xent 0.8592(0.8947) | Loss 579.4999(663.4447) | Error 0.3122(0.3185) Steps 0(0.00) | Grad Norm 493.5107(508.4101) | Total Time 0.00(0.00)\n",
      "Iter 3040 | Time 15.9000(15.5196) | Bit/dim 3.7638(3.7637) | Xent 0.8929(0.8848) | Loss 592.5662(642.7329) | Error 0.3244(0.3151) Steps 0(0.00) | Grad Norm 284.9598(491.5807) | Total Time 0.00(0.00)\n",
      "Iter 3050 | Time 15.3349(15.5162) | Bit/dim 3.7358(3.7595) | Xent 0.9104(0.8853) | Loss 583.6544(627.2054) | Error 0.3211(0.3158) Steps 0(0.00) | Grad Norm 574.5109(511.1776) | Total Time 0.00(0.00)\n",
      "Iter 3060 | Time 15.7020(15.4714) | Bit/dim 3.7628(3.7603) | Xent 0.9661(0.8904) | Loss 588.6390(615.3148) | Error 0.3333(0.3179) Steps 0(0.00) | Grad Norm 974.8763(555.9316) | Total Time 0.00(0.00)\n",
      "Iter 3070 | Time 15.5003(15.4974) | Bit/dim 3.7636(3.7586) | Xent 0.8710(0.8934) | Loss 575.6068(606.3278) | Error 0.3167(0.3178) Steps 0(0.00) | Grad Norm 527.1716(556.5048) | Total Time 0.00(0.00)\n",
      "Iter 3080 | Time 14.8876(15.4934) | Bit/dim 3.7552(3.7562) | Xent 0.8569(0.8871) | Loss 579.1389(599.6280) | Error 0.3089(0.3164) Steps 0(0.00) | Grad Norm 549.7150(533.2055) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 84.9636, Epoch Time 954.2777(934.5918), Bit/dim 3.7551(best: 3.7674), Xent 0.8549, Loss 4.1826, Error 0.3025(best: 0.3120)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 15.2803(15.5195) | Bit/dim 3.7491(3.7589) | Xent 0.7788(0.8736) | Loss 565.7921(651.2441) | Error 0.2744(0.3113) Steps 0(0.00) | Grad Norm 416.7711(519.7684) | Total Time 0.00(0.00)\n",
      "Iter 3100 | Time 15.3784(15.5047) | Bit/dim 3.7338(3.7583) | Xent 0.8534(0.8789) | Loss 579.8956(633.3779) | Error 0.3056(0.3118) Steps 0(0.00) | Grad Norm 738.9003(529.5538) | Total Time 0.00(0.00)\n",
      "Iter 3110 | Time 15.4614(15.4503) | Bit/dim 3.7709(3.7561) | Xent 0.8635(0.8780) | Loss 569.3350(617.6490) | Error 0.3133(0.3106) Steps 0(0.00) | Grad Norm 291.0531(513.0278) | Total Time 0.00(0.00)\n",
      "Iter 3120 | Time 14.6729(15.4337) | Bit/dim 3.7163(3.7516) | Xent 0.8778(0.8664) | Loss 565.9902(606.8601) | Error 0.3144(0.3078) Steps 0(0.00) | Grad Norm 414.4102(502.5051) | Total Time 0.00(0.00)\n",
      "Iter 3130 | Time 15.8212(15.4861) | Bit/dim 3.7780(3.7520) | Xent 1.1085(0.8883) | Loss 610.1680(602.3969) | Error 0.3811(0.3144) Steps 0(0.00) | Grad Norm 1506.6043(591.5175) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 83.8963, Epoch Time 953.5229(935.1597), Bit/dim 3.7682(best: 3.7551), Xent 0.8880, Loss 4.2123, Error 0.3156(best: 0.3025)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 15.6352(15.5069) | Bit/dim 3.7691(3.7555) | Xent 0.9449(0.9039) | Loss 580.5472(662.8150) | Error 0.3278(0.3204) Steps 0(0.00) | Grad Norm 615.0879(606.3900) | Total Time 0.00(0.00)\n",
      "Iter 3150 | Time 15.3756(15.5389) | Bit/dim 3.7698(3.7576) | Xent 0.7787(0.8938) | Loss 585.2638(642.1862) | Error 0.2911(0.3172) Steps 0(0.00) | Grad Norm 461.3129(570.0177) | Total Time 0.00(0.00)\n",
      "Iter 3160 | Time 15.3425(15.5965) | Bit/dim 3.7619(3.7572) | Xent 0.8651(0.8910) | Loss 581.4942(627.5876) | Error 0.2833(0.3157) Steps 0(0.00) | Grad Norm 310.2822(540.3371) | Total Time 0.00(0.00)\n",
      "Iter 3170 | Time 15.5695(15.6306) | Bit/dim 3.7426(3.7545) | Xent 0.9258(0.8836) | Loss 582.5488(615.6793) | Error 0.3189(0.3145) Steps 0(0.00) | Grad Norm 772.8754(529.2666) | Total Time 0.00(0.00)\n",
      "Iter 3180 | Time 15.1946(15.6693) | Bit/dim 3.7750(3.7555) | Xent 0.8689(0.8810) | Loss 577.1000(606.2079) | Error 0.2878(0.3133) Steps 0(0.00) | Grad Norm 357.3212(534.8581) | Total Time 0.00(0.00)\n",
      "Iter 3190 | Time 15.5425(15.6918) | Bit/dim 3.7456(3.7509) | Xent 0.8720(0.8840) | Loss 580.7051(598.8960) | Error 0.3311(0.3146) Steps 0(0.00) | Grad Norm 681.2241(574.7142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 83.6743, Epoch Time 966.8990(936.1119), Bit/dim 3.7504(best: 3.7551), Xent 0.8438, Loss 4.1723, Error 0.2970(best: 0.3025)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 16.5751(15.6867) | Bit/dim 3.7478(3.7512) | Xent 0.8545(0.8758) | Loss 571.6235(651.3945) | Error 0.3222(0.3122) Steps 0(0.00) | Grad Norm 778.3734(581.2954) | Total Time 0.00(0.00)\n",
      "Iter 3210 | Time 16.1997(15.6905) | Bit/dim 3.7576(3.7499) | Xent 0.8220(0.8767) | Loss 590.1491(632.8814) | Error 0.2922(0.3113) Steps 0(0.00) | Grad Norm 589.3032(567.9573) | Total Time 0.00(0.00)\n",
      "Iter 3220 | Time 15.8297(15.6454) | Bit/dim 3.7347(3.7470) | Xent 0.8177(0.8680) | Loss 591.9464(618.1410) | Error 0.2789(0.3080) Steps 0(0.00) | Grad Norm 298.9063(555.4745) | Total Time 0.00(0.00)\n",
      "Iter 3230 | Time 15.6444(15.6605) | Bit/dim 3.7505(3.7466) | Xent 0.8734(0.8620) | Loss 588.4153(608.6879) | Error 0.3089(0.3055) Steps 0(0.00) | Grad Norm 795.3284(567.6984) | Total Time 0.00(0.00)\n",
      "Iter 3240 | Time 15.1654(15.6032) | Bit/dim 3.7604(3.7483) | Xent 0.8543(0.8624) | Loss 580.2532(601.3212) | Error 0.2911(0.3072) Steps 0(0.00) | Grad Norm 387.4178(526.9816) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 84.2132, Epoch Time 959.4133(936.8109), Bit/dim 3.7416(best: 3.7504), Xent 0.9012, Loss 4.1921, Error 0.3198(best: 0.2970)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 15.6977(15.5804) | Bit/dim 3.7268(3.7465) | Xent 0.9447(0.8723) | Loss 572.0677(661.2195) | Error 0.3211(0.3092) Steps 0(0.00) | Grad Norm 694.0013(537.2608) | Total Time 0.00(0.00)\n",
      "Iter 3260 | Time 15.9478(15.6117) | Bit/dim 3.7403(3.7520) | Xent 0.8558(0.8709) | Loss 573.2588(639.9991) | Error 0.3056(0.3080) Steps 0(0.00) | Grad Norm 440.1989(553.3925) | Total Time 0.00(0.00)\n",
      "Iter 3270 | Time 16.5041(15.6199) | Bit/dim 3.7381(3.7520) | Xent 0.8471(0.8691) | Loss 580.1844(624.3973) | Error 0.2989(0.3084) Steps 0(0.00) | Grad Norm 541.3657(557.1174) | Total Time 0.00(0.00)\n",
      "Iter 3280 | Time 14.7732(15.5889) | Bit/dim 3.7678(3.7511) | Xent 0.8586(0.8611) | Loss 584.5242(611.7204) | Error 0.3078(0.3057) Steps 0(0.00) | Grad Norm 445.6515(541.3224) | Total Time 0.00(0.00)\n",
      "Iter 3290 | Time 16.0422(15.6263) | Bit/dim 3.6976(3.7456) | Xent 0.8656(0.8609) | Loss 583.7087(603.6561) | Error 0.2989(0.3060) Steps 0(0.00) | Grad Norm 743.7923(569.0097) | Total Time 0.00(0.00)\n",
      "Iter 3300 | Time 15.4862(15.6038) | Bit/dim 3.7271(3.7427) | Xent 0.8829(0.8613) | Loss 568.4126(597.0222) | Error 0.3167(0.3050) Steps 0(0.00) | Grad Norm 490.2120(581.9456) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 83.7388, Epoch Time 961.9467(937.5650), Bit/dim 3.7418(best: 3.7416), Xent 0.8431, Loss 4.1633, Error 0.3015(best: 0.2970)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 15.4559(15.5368) | Bit/dim 3.7304(3.7411) | Xent 0.9464(0.8597) | Loss 591.9499(648.6189) | Error 0.3311(0.3045) Steps 0(0.00) | Grad Norm 1207.0910(614.3420) | Total Time 0.00(0.00)\n",
      "Iter 3320 | Time 15.8671(15.5812) | Bit/dim 3.7370(3.7386) | Xent 0.8302(0.8573) | Loss 586.0010(631.2939) | Error 0.2833(0.3052) Steps 0(0.00) | Grad Norm 369.8576(584.1590) | Total Time 0.00(0.00)\n",
      "Iter 3330 | Time 14.8991(15.5676) | Bit/dim 3.7787(3.7446) | Xent 0.8562(0.8500) | Loss 578.6370(617.4227) | Error 0.2889(0.3023) Steps 0(0.00) | Grad Norm 428.9923(558.6796) | Total Time 0.00(0.00)\n",
      "Iter 3340 | Time 15.7548(15.6186) | Bit/dim 3.7463(3.7441) | Xent 0.8512(0.8510) | Loss 581.9849(608.5728) | Error 0.3222(0.3046) Steps 0(0.00) | Grad Norm 455.3595(542.0910) | Total Time 0.00(0.00)\n",
      "Iter 3350 | Time 15.5247(15.6869) | Bit/dim 3.7188(3.7427) | Xent 0.8335(0.8455) | Loss 582.0226(601.2615) | Error 0.2811(0.3020) Steps 0(0.00) | Grad Norm 204.8729(514.1073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 85.0040, Epoch Time 963.0877(938.3307), Bit/dim 3.7361(best: 3.7416), Xent 0.8586, Loss 4.1654, Error 0.3034(best: 0.2970)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 15.4042(15.6233) | Bit/dim 3.7273(3.7394) | Xent 0.8313(0.8409) | Loss 571.7728(660.6211) | Error 0.2956(0.3011) Steps 0(0.00) | Grad Norm 432.8837(502.5918) | Total Time 0.00(0.00)\n",
      "Iter 3370 | Time 15.2865(15.6592) | Bit/dim 3.7159(3.7389) | Xent 0.8561(0.8388) | Loss 583.3825(640.0295) | Error 0.3089(0.3000) Steps 0(0.00) | Grad Norm 405.6011(494.6053) | Total Time 0.00(0.00)\n",
      "Iter 3380 | Time 16.2219(15.7972) | Bit/dim 3.7142(3.7370) | Xent 0.8492(0.8358) | Loss 590.8259(624.8238) | Error 0.3122(0.2993) Steps 0(0.00) | Grad Norm 378.0770(464.2743) | Total Time 0.00(0.00)\n",
      "Iter 3390 | Time 15.2651(15.8109) | Bit/dim 3.7436(3.7356) | Xent 0.9029(0.8392) | Loss 589.6126(613.7196) | Error 0.3256(0.3009) Steps 0(0.00) | Grad Norm 711.8578(518.5603) | Total Time 0.00(0.00)\n",
      "Iter 3400 | Time 16.1471(15.8788) | Bit/dim 3.7117(3.7349) | Xent 0.8799(0.8495) | Loss 581.7496(605.9597) | Error 0.3100(0.3021) Steps 0(0.00) | Grad Norm 478.6808(520.1879) | Total Time 0.00(0.00)\n",
      "Iter 3410 | Time 15.2108(15.8109) | Bit/dim 3.7824(3.7372) | Xent 0.7812(0.8421) | Loss 579.0220(599.5971) | Error 0.2944(0.3021) Steps 0(0.00) | Grad Norm 335.1198(499.7876) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 86.1307, Epoch Time 976.0325(939.4617), Bit/dim 3.7368(best: 3.7361), Xent 0.8596, Loss 4.1666, Error 0.3058(best: 0.2970)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 15.9349(15.8104) | Bit/dim 3.7151(3.7361) | Xent 0.8601(0.8423) | Loss 577.8718(650.3735) | Error 0.2967(0.3022) Steps 0(0.00) | Grad Norm 558.6840(488.4400) | Total Time 0.00(0.00)\n",
      "Iter 3430 | Time 16.5469(15.8473) | Bit/dim 3.7450(3.7359) | Xent 0.7785(0.8337) | Loss 580.8815(631.3965) | Error 0.2911(0.3001) Steps 0(0.00) | Grad Norm 504.5421(500.5091) | Total Time 0.00(0.00)\n",
      "Iter 3440 | Time 15.6821(15.8642) | Bit/dim 3.7432(3.7334) | Xent 0.8300(0.8335) | Loss 570.6411(617.4457) | Error 0.2978(0.2984) Steps 0(0.00) | Grad Norm 316.6927(497.4740) | Total Time 0.00(0.00)\n",
      "Iter 3450 | Time 16.3465(15.8051) | Bit/dim 3.6905(3.7326) | Xent 0.8664(0.8357) | Loss 580.2411(607.5972) | Error 0.3144(0.3000) Steps 0(0.00) | Grad Norm 456.7874(509.4584) | Total Time 0.00(0.00)\n",
      "Iter 3460 | Time 15.4335(15.7563) | Bit/dim 3.7527(3.7319) | Xent 0.9180(0.8347) | Loss 572.7762(599.6987) | Error 0.3267(0.2993) Steps 0(0.00) | Grad Norm 726.9517(521.2997) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 85.7573, Epoch Time 971.6536(940.4275), Bit/dim 3.7266(best: 3.7361), Xent 0.8009, Loss 4.1271, Error 0.2816(best: 0.2970)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 15.7540(15.7068) | Bit/dim 3.7443(3.7338) | Xent 0.7907(0.8275) | Loss 572.9377(659.4782) | Error 0.2756(0.2966) Steps 0(0.00) | Grad Norm 365.6339(488.3037) | Total Time 0.00(0.00)\n",
      "Iter 3480 | Time 16.2084(15.7537) | Bit/dim 3.7109(3.7327) | Xent 0.7908(0.8204) | Loss 573.7031(636.6805) | Error 0.2911(0.2943) Steps 0(0.00) | Grad Norm 574.5143(483.7558) | Total Time 0.00(0.00)\n",
      "Iter 3490 | Time 15.8859(15.7492) | Bit/dim 3.7102(3.7308) | Xent 0.7654(0.8150) | Loss 563.2586(620.6107) | Error 0.2722(0.2921) Steps 0(0.00) | Grad Norm 347.3036(505.4068) | Total Time 0.00(0.00)\n",
      "Iter 3500 | Time 16.2664(15.7928) | Bit/dim 3.7621(3.7300) | Xent 0.9791(0.8292) | Loss 592.6099(609.4943) | Error 0.3522(0.2965) Steps 0(0.00) | Grad Norm 717.2038(542.8465) | Total Time 0.00(0.00)\n",
      "Iter 3510 | Time 15.8052(15.7737) | Bit/dim 3.7519(3.7343) | Xent 0.8195(0.8346) | Loss 578.8456(601.6137) | Error 0.2811(0.2985) Steps 0(0.00) | Grad Norm 535.8921(550.1761) | Total Time 0.00(0.00)\n",
      "Iter 3520 | Time 15.3359(15.8121) | Bit/dim 3.7597(3.7364) | Xent 0.8461(0.8413) | Loss 589.5870(596.3025) | Error 0.2956(0.2995) Steps 0(0.00) | Grad Norm 874.3032(628.4039) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 84.9736, Epoch Time 971.9192(941.3722), Bit/dim 3.7359(best: 3.7266), Xent 0.8093, Loss 4.1406, Error 0.2859(best: 0.2816)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 15.8649(15.8673) | Bit/dim 3.7601(3.7363) | Xent 0.8585(0.8340) | Loss 580.7989(650.7083) | Error 0.3111(0.2970) Steps 0(0.00) | Grad Norm 500.6697(613.4047) | Total Time 0.00(0.00)\n",
      "Iter 3540 | Time 16.1923(15.9684) | Bit/dim 3.7541(3.7334) | Xent 0.7773(0.8292) | Loss 575.4161(631.8279) | Error 0.2711(0.2953) Steps 0(0.00) | Grad Norm 463.8751(567.8776) | Total Time 0.00(0.00)\n",
      "Iter 3550 | Time 16.0179(16.0113) | Bit/dim 3.7553(3.7323) | Xent 0.9102(0.8358) | Loss 568.1656(617.9314) | Error 0.3311(0.2971) Steps 0(0.00) | Grad Norm 565.5761(573.8353) | Total Time 0.00(0.00)\n",
      "Iter 3560 | Time 15.6352(15.9702) | Bit/dim 3.7580(3.7325) | Xent 0.8586(0.8308) | Loss 587.6407(607.9818) | Error 0.3011(0.2966) Steps 0(0.00) | Grad Norm 352.1949(544.2701) | Total Time 0.00(0.00)\n",
      "Iter 3570 | Time 15.8395(15.9274) | Bit/dim 3.7272(3.7299) | Xent 0.8150(0.8284) | Loss 574.6960(599.8933) | Error 0.2856(0.2958) Steps 0(0.00) | Grad Norm 698.2831(549.4259) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 85.4816, Epoch Time 983.8678(942.6471), Bit/dim 3.7238(best: 3.7266), Xent 0.8034, Loss 4.1256, Error 0.2848(best: 0.2816)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 15.6508(15.8841) | Bit/dim 3.6687(3.7302) | Xent 0.8446(0.8238) | Loss 578.8968(659.9837) | Error 0.3056(0.2942) Steps 0(0.00) | Grad Norm 503.0011(551.7333) | Total Time 0.00(0.00)\n",
      "Iter 3590 | Time 16.2944(15.8533) | Bit/dim 3.7213(3.7302) | Xent 0.7775(0.8182) | Loss 568.3340(637.8955) | Error 0.2789(0.2922) Steps 0(0.00) | Grad Norm 473.4962(539.7191) | Total Time 0.00(0.00)\n",
      "Iter 3600 | Time 15.8174(15.9047) | Bit/dim 3.7384(3.7272) | Xent 0.8483(0.8138) | Loss 576.2181(620.9235) | Error 0.2922(0.2901) Steps 0(0.00) | Grad Norm 449.0377(508.0299) | Total Time 0.00(0.00)\n",
      "Iter 3610 | Time 15.5549(15.8839) | Bit/dim 3.7158(3.7266) | Xent 0.8229(0.8153) | Loss 570.7188(609.1733) | Error 0.2922(0.2906) Steps 0(0.00) | Grad Norm 667.6074(526.4905) | Total Time 0.00(0.00)\n",
      "Iter 3620 | Time 15.7294(15.8470) | Bit/dim 3.7435(3.7287) | Xent 0.7454(0.8042) | Loss 561.5541(599.1209) | Error 0.2689(0.2855) Steps 0(0.00) | Grad Norm 388.9082(510.1864) | Total Time 0.00(0.00)\n",
      "Iter 3630 | Time 15.5019(15.9149) | Bit/dim 3.7401(3.7257) | Xent 0.7136(0.8031) | Loss 570.7041(592.4750) | Error 0.2567(0.2853) Steps 0(0.00) | Grad Norm 636.3973(493.1245) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 84.5083, Epoch Time 976.0936(943.6505), Bit/dim 3.7182(best: 3.7238), Xent 0.7849, Loss 4.1107, Error 0.2786(best: 0.2816)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 16.6518(15.8357) | Bit/dim 3.7303(3.7217) | Xent 0.7285(0.7995) | Loss 570.8081(641.4550) | Error 0.2789(0.2853) Steps 0(0.00) | Grad Norm 549.0969(498.5703) | Total Time 0.00(0.00)\n",
      "Iter 3650 | Time 15.9980(15.9497) | Bit/dim 3.7239(3.7227) | Xent 0.7727(0.7950) | Loss 581.6808(624.0645) | Error 0.2744(0.2834) Steps 0(0.00) | Grad Norm 506.0230(482.7984) | Total Time 0.00(0.00)\n",
      "Iter 3660 | Time 16.4051(15.9009) | Bit/dim 3.7032(3.7204) | Xent 0.8855(0.7970) | Loss 575.7458(609.6087) | Error 0.3144(0.2845) Steps 0(0.00) | Grad Norm 557.3170(485.9174) | Total Time 0.00(0.00)\n",
      "Iter 3670 | Time 16.6770(15.9703) | Bit/dim 3.6992(3.7210) | Xent 0.8269(0.7963) | Loss 582.5368(600.7315) | Error 0.3033(0.2853) Steps 0(0.00) | Grad Norm 649.4408(513.6804) | Total Time 0.00(0.00)\n",
      "Iter 3680 | Time 16.4568(16.0578) | Bit/dim 3.7404(3.7234) | Xent 0.8441(0.8001) | Loss 583.1481(595.2538) | Error 0.2811(0.2839) Steps 0(0.00) | Grad Norm 969.1047(535.9133) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 86.0323, Epoch Time 986.5819(944.9384), Bit/dim 3.7323(best: 3.7182), Xent 0.8436, Loss 4.1541, Error 0.2950(best: 0.2786)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 15.5342(16.0676) | Bit/dim 3.6901(3.7257) | Xent 0.7406(0.7979) | Loss 570.6896(658.1069) | Error 0.2811(0.2842) Steps 0(0.00) | Grad Norm 609.8275(537.9776) | Total Time 0.00(0.00)\n",
      "Iter 3700 | Time 15.6609(16.0376) | Bit/dim 3.7322(3.7252) | Xent 0.7977(0.7897) | Loss 580.8746(636.0425) | Error 0.2878(0.2806) Steps 0(0.00) | Grad Norm 607.6521(522.0710) | Total Time 0.00(0.00)\n",
      "Iter 3710 | Time 16.3262(16.0518) | Bit/dim 3.7529(3.7231) | Xent 0.8095(0.7949) | Loss 579.0097(620.1986) | Error 0.2733(0.2833) Steps 0(0.00) | Grad Norm 791.0968(532.9735) | Total Time 0.00(0.00)\n",
      "Iter 3720 | Time 16.6745(16.0728) | Bit/dim 3.7206(3.7217) | Xent 0.7968(0.7948) | Loss 577.5954(607.6386) | Error 0.2967(0.2832) Steps 0(0.00) | Grad Norm 537.1439(523.3618) | Total Time 0.00(0.00)\n",
      "Iter 3730 | Time 16.1055(16.0058) | Bit/dim 3.7121(3.7210) | Xent 0.8569(0.7963) | Loss 585.0644(598.2097) | Error 0.3033(0.2835) Steps 0(0.00) | Grad Norm 410.8972(513.3857) | Total Time 0.00(0.00)\n",
      "Iter 3740 | Time 16.0911(16.0380) | Bit/dim 3.7250(3.7199) | Xent 0.9210(0.8008) | Loss 585.7880(591.7398) | Error 0.3344(0.2852) Steps 0(0.00) | Grad Norm 633.0924(503.1738) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 85.7210, Epoch Time 985.6055(946.1585), Bit/dim 3.7141(best: 3.7182), Xent 0.8040, Loss 4.1161, Error 0.2842(best: 0.2786)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 15.3389(15.9179) | Bit/dim 3.7305(3.7203) | Xent 0.7266(0.7971) | Loss 569.3039(640.7983) | Error 0.2700(0.2844) Steps 0(0.00) | Grad Norm 310.8638(518.2104) | Total Time 0.00(0.00)\n",
      "Iter 3760 | Time 16.4837(15.9433) | Bit/dim 3.7291(3.7189) | Xent 0.8615(0.7950) | Loss 568.9715(622.5005) | Error 0.2944(0.2825) Steps 0(0.00) | Grad Norm 611.2898(538.3491) | Total Time 0.00(0.00)\n",
      "Iter 3770 | Time 15.3485(15.9186) | Bit/dim 3.7200(3.7192) | Xent 0.7945(0.7947) | Loss 567.0244(609.2791) | Error 0.2811(0.2822) Steps 0(0.00) | Grad Norm 359.4458(527.4678) | Total Time 0.00(0.00)\n",
      "Iter 3780 | Time 16.2113(15.8870) | Bit/dim 3.6882(3.7165) | Xent 0.8761(0.7901) | Loss 583.3157(599.6069) | Error 0.3156(0.2804) Steps 0(0.00) | Grad Norm 958.1264(516.6957) | Total Time 0.00(0.00)\n",
      "Iter 3790 | Time 16.4549(15.9445) | Bit/dim 3.7162(3.7154) | Xent 0.7937(0.8049) | Loss 578.5724(593.6014) | Error 0.2867(0.2859) Steps 0(0.00) | Grad Norm 418.8257(559.5697) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 86.6529, Epoch Time 979.3668(947.1547), Bit/dim 3.7212(best: 3.7141), Xent 0.7886, Loss 4.1155, Error 0.2781(best: 0.2786)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 16.2013(15.9635) | Bit/dim 3.7520(3.7192) | Xent 0.7957(0.8012) | Loss 573.7897(656.9998) | Error 0.2911(0.2845) Steps 0(0.00) | Grad Norm 566.0906(528.3378) | Total Time 0.00(0.00)\n",
      "Iter 3810 | Time 15.4253(16.0421) | Bit/dim 3.7084(3.7184) | Xent 0.7373(0.7973) | Loss 567.3405(636.2357) | Error 0.2644(0.2849) Steps 0(0.00) | Grad Norm 516.6436(511.0931) | Total Time 0.00(0.00)\n",
      "Iter 3820 | Time 15.8055(16.0109) | Bit/dim 3.7177(3.7167) | Xent 0.7840(0.7969) | Loss 580.9938(620.7077) | Error 0.2889(0.2854) Steps 0(0.00) | Grad Norm 438.2050(517.2385) | Total Time 0.00(0.00)\n",
      "Iter 3830 | Time 17.0334(16.0748) | Bit/dim 3.7313(3.7156) | Xent 0.7757(0.7940) | Loss 586.4921(608.7138) | Error 0.2744(0.2841) Steps 0(0.00) | Grad Norm 305.0854(494.7352) | Total Time 0.00(0.00)\n",
      "Iter 3840 | Time 15.1926(15.9847) | Bit/dim 3.7082(3.7146) | Xent 0.7908(0.7898) | Loss 574.6989(599.9007) | Error 0.3022(0.2841) Steps 0(0.00) | Grad Norm 681.6068(500.9447) | Total Time 0.00(0.00)\n",
      "Iter 3850 | Time 16.3328(16.0037) | Bit/dim 3.7310(3.7143) | Xent 0.7523(0.7880) | Loss 585.1308(594.8778) | Error 0.2656(0.2837) Steps 0(0.00) | Grad Norm 589.8996(512.5696) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 86.3927, Epoch Time 986.0422(948.3213), Bit/dim 3.7132(best: 3.7141), Xent 0.8211, Loss 4.1237, Error 0.2862(best: 0.2781)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 16.9479(16.0536) | Bit/dim 3.6619(3.7138) | Xent 0.7743(0.7801) | Loss 575.2987(642.7115) | Error 0.2833(0.2812) Steps 0(0.00) | Grad Norm 623.7443(493.2479) | Total Time 0.00(0.00)\n",
      "Iter 3870 | Time 17.2357(16.0469) | Bit/dim 3.6770(3.7139) | Xent 0.7752(0.7734) | Loss 583.1884(624.1945) | Error 0.2733(0.2770) Steps 0(0.00) | Grad Norm 674.2293(518.0530) | Total Time 0.00(0.00)\n",
      "Iter 3880 | Time 16.6986(16.0953) | Bit/dim 3.7364(3.7139) | Xent 0.8117(0.7738) | Loss 572.4075(611.2692) | Error 0.3011(0.2786) Steps 0(0.00) | Grad Norm 480.0179(520.9505) | Total Time 0.00(0.00)\n",
      "Iter 3890 | Time 15.5471(16.1159) | Bit/dim 3.6923(3.7142) | Xent 0.8158(0.7787) | Loss 572.8835(601.1100) | Error 0.2767(0.2788) Steps 0(0.00) | Grad Norm 640.0397(523.8885) | Total Time 0.00(0.00)\n",
      "Iter 3900 | Time 15.7740(16.0684) | Bit/dim 3.7158(3.7104) | Xent 0.7762(0.7793) | Loss 581.0682(593.9107) | Error 0.2844(0.2795) Steps 0(0.00) | Grad Norm 316.5659(525.4144) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 86.3742, Epoch Time 992.7454(949.6541), Bit/dim 3.7168(best: 3.7132), Xent 0.7858, Loss 4.1097, Error 0.2784(best: 0.2781)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 16.5202(16.1399) | Bit/dim 3.7474(3.7111) | Xent 0.7993(0.7792) | Loss 587.9142(655.8042) | Error 0.2889(0.2795) Steps 0(0.00) | Grad Norm 594.2330(517.6486) | Total Time 0.00(0.00)\n",
      "Iter 3920 | Time 16.6071(16.1107) | Bit/dim 3.7085(3.7089) | Xent 0.7731(0.7751) | Loss 584.8388(634.4464) | Error 0.2733(0.2771) Steps 0(0.00) | Grad Norm 474.0172(508.4479) | Total Time 0.00(0.00)\n",
      "Iter 3930 | Time 16.4273(16.0944) | Bit/dim 3.7042(3.7092) | Xent 0.7362(0.7734) | Loss 573.1066(618.0705) | Error 0.2678(0.2767) Steps 0(0.00) | Grad Norm 425.6965(530.8833) | Total Time 0.00(0.00)\n",
      "Iter 3940 | Time 16.6730(16.0573) | Bit/dim 3.7224(3.7129) | Xent 0.7914(0.7751) | Loss 576.5203(606.6243) | Error 0.3000(0.2768) Steps 0(0.00) | Grad Norm 567.9351(537.0218) | Total Time 0.00(0.00)\n",
      "Iter 3950 | Time 15.9519(16.0052) | Bit/dim 3.7167(3.7114) | Xent 0.7634(0.7733) | Loss 570.5144(597.5620) | Error 0.3011(0.2770) Steps 0(0.00) | Grad Norm 343.4427(520.6842) | Total Time 0.00(0.00)\n",
      "Iter 3960 | Time 15.1761(15.9192) | Bit/dim 3.7030(3.7069) | Xent 0.7783(0.7736) | Loss 561.0649(589.5147) | Error 0.2678(0.2766) Steps 0(0.00) | Grad Norm 455.0161(520.4990) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 86.2774, Epoch Time 980.5627(950.5813), Bit/dim 3.7146(best: 3.7132), Xent 0.7985, Loss 4.1139, Error 0.2809(best: 0.2781)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 15.2570(15.9453) | Bit/dim 3.6998(3.7098) | Xent 0.8150(0.7694) | Loss 576.2880(642.4171) | Error 0.2889(0.2757) Steps 0(0.00) | Grad Norm 767.8472(537.9266) | Total Time 0.00(0.00)\n",
      "Iter 3980 | Time 15.5899(15.9673) | Bit/dim 3.7044(3.7075) | Xent 0.7659(0.7605) | Loss 564.2466(623.8103) | Error 0.2678(0.2715) Steps 0(0.00) | Grad Norm 943.0354(511.6878) | Total Time 0.00(0.00)\n",
      "Iter 3990 | Time 16.2321(16.0631) | Bit/dim 3.7253(3.7072) | Xent 0.6877(0.7600) | Loss 581.4493(610.4971) | Error 0.2322(0.2715) Steps 0(0.00) | Grad Norm 295.7670(490.1383) | Total Time 0.00(0.00)\n",
      "Iter 4000 | Time 15.6523(16.0511) | Bit/dim 3.6811(3.7038) | Xent 0.7951(0.7603) | Loss 583.3629(600.9209) | Error 0.3022(0.2701) Steps 0(0.00) | Grad Norm 712.0709(490.1618) | Total Time 0.00(0.00)\n",
      "Iter 4010 | Time 16.6881(16.1235) | Bit/dim 3.7300(3.7045) | Xent 0.7751(0.7642) | Loss 567.1662(593.2372) | Error 0.2644(0.2702) Steps 0(0.00) | Grad Norm 747.6361(542.9409) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 87.1880, Epoch Time 991.6606(951.8137), Bit/dim 3.7081(best: 3.7132), Xent 0.7940, Loss 4.1051, Error 0.2767(best: 0.2781)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 16.2678(16.0622) | Bit/dim 3.7098(3.7079) | Xent 0.6752(0.7596) | Loss 588.3461(653.1447) | Error 0.2467(0.2688) Steps 0(0.00) | Grad Norm 357.7686(551.3063) | Total Time 0.00(0.00)\n",
      "Iter 4030 | Time 16.7087(16.0602) | Bit/dim 3.7382(3.7069) | Xent 0.7709(0.7597) | Loss 580.0999(632.0420) | Error 0.2700(0.2698) Steps 0(0.00) | Grad Norm 657.1561(541.9925) | Total Time 0.00(0.00)\n",
      "Iter 4040 | Time 17.3870(16.1286) | Bit/dim 3.6792(3.7041) | Xent 0.7851(0.7676) | Loss 584.4606(616.9385) | Error 0.2867(0.2728) Steps 0(0.00) | Grad Norm 651.0405(559.5585) | Total Time 0.00(0.00)\n",
      "Iter 4050 | Time 15.6800(16.1098) | Bit/dim 3.7154(3.7052) | Xent 0.7604(0.7620) | Loss 580.3266(605.8970) | Error 0.2689(0.2725) Steps 0(0.00) | Grad Norm 346.4085(533.4873) | Total Time 0.00(0.00)\n",
      "Iter 4060 | Time 15.9216(16.1341) | Bit/dim 3.6852(3.7045) | Xent 0.7075(0.7602) | Loss 566.0916(596.7883) | Error 0.2456(0.2721) Steps 0(0.00) | Grad Norm 329.6200(502.6818) | Total Time 0.00(0.00)\n",
      "Iter 4070 | Time 16.0867(16.1477) | Bit/dim 3.7121(3.7050) | Xent 0.7246(0.7577) | Loss 567.8975(589.7002) | Error 0.2611(0.2707) Steps 0(0.00) | Grad Norm 354.2044(510.6006) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 87.4886, Epoch Time 994.3934(953.0911), Bit/dim 3.7134(best: 3.7081), Xent 0.7826, Loss 4.1047, Error 0.2765(best: 0.2767)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 16.3799(16.1646) | Bit/dim 3.7241(3.7037) | Xent 0.8445(0.7544) | Loss 592.8748(641.4912) | Error 0.2978(0.2696) Steps 0(0.00) | Grad Norm 729.0430(513.5582) | Total Time 0.00(0.00)\n",
      "Iter 4090 | Time 15.5330(16.1609) | Bit/dim 3.7250(3.7050) | Xent 0.7596(0.7599) | Loss 569.6301(624.2007) | Error 0.2722(0.2717) Steps 0(0.00) | Grad Norm 524.4297(505.3248) | Total Time 0.00(0.00)\n",
      "Iter 4100 | Time 16.0638(16.1288) | Bit/dim 3.7147(3.7053) | Xent 0.7229(0.7489) | Loss 577.5526(610.0018) | Error 0.2622(0.2685) Steps 0(0.00) | Grad Norm 362.3677(472.5410) | Total Time 0.00(0.00)\n",
      "Iter 4110 | Time 15.9896(16.2211) | Bit/dim 3.6559(3.7034) | Xent 0.7325(0.7461) | Loss 575.1137(598.4933) | Error 0.2478(0.2668) Steps 0(0.00) | Grad Norm 312.0133(465.4310) | Total Time 0.00(0.00)\n",
      "Iter 4120 | Time 16.4268(16.2225) | Bit/dim 3.6960(3.7033) | Xent 0.6793(0.7414) | Loss 574.7831(591.0228) | Error 0.2456(0.2654) Steps 0(0.00) | Grad Norm 297.3140(483.6107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 86.9183, Epoch Time 997.4627(954.4222), Bit/dim 3.6943(best: 3.7081), Xent 0.7550, Loss 4.0718, Error 0.2658(best: 0.2765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 16.3060(16.1931) | Bit/dim 3.6886(3.7031) | Xent 0.6771(0.7392) | Loss 563.8526(653.2376) | Error 0.2400(0.2647) Steps 0(0.00) | Grad Norm 421.7821(486.4329) | Total Time 0.00(0.00)\n",
      "Iter 4140 | Time 16.7624(16.1798) | Bit/dim 3.7149(3.7043) | Xent 0.7105(0.7390) | Loss 564.2232(631.0059) | Error 0.2411(0.2638) Steps 0(0.00) | Grad Norm 285.1158(501.5236) | Total Time 0.00(0.00)\n",
      "Iter 4150 | Time 16.5095(16.1406) | Bit/dim 3.6964(3.7002) | Xent 0.7881(0.7311) | Loss 570.8452(614.5213) | Error 0.2956(0.2617) Steps 0(0.00) | Grad Norm 559.9252(473.6505) | Total Time 0.00(0.00)\n",
      "Iter 4160 | Time 16.4340(16.1619) | Bit/dim 3.7264(3.6990) | Xent 0.7621(0.7406) | Loss 575.5015(603.4513) | Error 0.2678(0.2644) Steps 0(0.00) | Grad Norm 782.6791(501.4363) | Total Time 0.00(0.00)\n",
      "Iter 4170 | Time 15.9525(16.0971) | Bit/dim 3.7314(3.6995) | Xent 0.7893(0.7441) | Loss 567.2498(595.1037) | Error 0.2844(0.2653) Steps 0(0.00) | Grad Norm 318.8842(490.9302) | Total Time 0.00(0.00)\n",
      "Iter 4180 | Time 15.8092(16.0842) | Bit/dim 3.7459(3.7003) | Xent 0.7976(0.7407) | Loss 584.0359(588.2767) | Error 0.2689(0.2637) Steps 0(0.00) | Grad Norm 482.6667(470.5272) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 86.4449, Epoch Time 989.4005(955.4716), Bit/dim 3.6977(best: 3.6943), Xent 0.7525, Loss 4.0739, Error 0.2636(best: 0.2658)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 16.7420(16.0714) | Bit/dim 3.6635(3.6969) | Xent 0.7187(0.7373) | Loss 564.9191(640.8682) | Error 0.2511(0.2623) Steps 0(0.00) | Grad Norm 306.3093(479.3153) | Total Time 0.00(0.00)\n",
      "Iter 4200 | Time 16.3076(16.1109) | Bit/dim 3.6766(3.6962) | Xent 0.7290(0.7322) | Loss 562.9159(621.9528) | Error 0.2700(0.2591) Steps 0(0.00) | Grad Norm 421.2891(465.5893) | Total Time 0.00(0.00)\n",
      "Iter 4210 | Time 15.6621(16.1053) | Bit/dim 3.7363(3.6981) | Xent 0.7217(0.7373) | Loss 570.5186(609.6670) | Error 0.2544(0.2612) Steps 0(0.00) | Grad Norm 607.7239(515.8399) | Total Time 0.00(0.00)\n",
      "Iter 4220 | Time 17.0112(16.2066) | Bit/dim 3.7138(3.6981) | Xent 0.7701(0.7439) | Loss 581.8187(600.2414) | Error 0.2689(0.2646) Steps 0(0.00) | Grad Norm 425.7044(508.5327) | Total Time 0.00(0.00)\n",
      "Iter 4230 | Time 16.2300(16.1548) | Bit/dim 3.7079(3.6991) | Xent 0.7090(0.7486) | Loss 561.8950(592.2442) | Error 0.2544(0.2666) Steps 0(0.00) | Grad Norm 447.3016(502.2841) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 87.3874, Epoch Time 996.5899(956.7051), Bit/dim 3.6981(best: 3.6943), Xent 0.7673, Loss 4.0817, Error 0.2676(best: 0.2636)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 15.9574(16.1965) | Bit/dim 3.7167(3.6983) | Xent 0.7266(0.7421) | Loss 577.3992(654.6316) | Error 0.2567(0.2647) Steps 0(0.00) | Grad Norm 530.4908(504.9128) | Total Time 0.00(0.00)\n",
      "Iter 4250 | Time 16.3577(16.1642) | Bit/dim 3.6973(3.6970) | Xent 0.6957(0.7322) | Loss 552.0345(631.5636) | Error 0.2533(0.2613) Steps 0(0.00) | Grad Norm 291.7176(499.0387) | Total Time 0.00(0.00)\n",
      "Iter 4260 | Time 15.3380(16.1553) | Bit/dim 3.6832(3.6954) | Xent 0.7880(0.7302) | Loss 576.5656(615.7955) | Error 0.2733(0.2622) Steps 0(0.00) | Grad Norm 270.2992(465.2481) | Total Time 0.00(0.00)\n",
      "Iter 4270 | Time 15.1355(16.1407) | Bit/dim 3.6733(3.6959) | Xent 0.7170(0.7239) | Loss 573.1281(604.5370) | Error 0.2567(0.2586) Steps 0(0.00) | Grad Norm 416.0522(446.1293) | Total Time 0.00(0.00)\n",
      "Iter 4280 | Time 15.5838(16.0946) | Bit/dim 3.6737(3.6948) | Xent 0.7111(0.7248) | Loss 566.7490(595.5694) | Error 0.2633(0.2593) Steps 0(0.00) | Grad Norm 504.6053(446.8444) | Total Time 0.00(0.00)\n",
      "Iter 4290 | Time 16.0372(16.0894) | Bit/dim 3.7245(3.6961) | Xent 0.6795(0.7271) | Loss 582.3639(589.9483) | Error 0.2611(0.2600) Steps 0(0.00) | Grad Norm 409.2359(464.5810) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 87.9452, Epoch Time 991.8282(957.7588), Bit/dim 3.6924(best: 3.6943), Xent 0.7683, Loss 4.0766, Error 0.2695(best: 0.2636)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_rl_stdscale_6_rlw_1_0_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --scale_std 6.0 --rl-weight 1.0\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
