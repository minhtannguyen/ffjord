{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn100_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=100, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0001 | Time 99.0963(99.0963) | Bit/dim 11.0250(11.0250) | Xent 2.3026(2.3026) | Loss 12.1763(12.1763) | Error 0.8990(0.8990) Steps 574(574.00) | Grad Norm 68.4504(68.4504) | Total Time 14.00(14.00)\n",
      "Iter 0002 | Time 50.2130(97.6298) | Bit/dim 10.5594(11.0110) | Xent 2.2925(2.3023) | Loss 11.7056(12.1622) | Error 0.7660(0.8950) Steps 574(574.00) | Grad Norm 57.9504(68.1354) | Total Time 14.00(14.00)\n",
      "Iter 0003 | Time 48.5630(96.1578) | Bit/dim 10.0549(10.9823) | Xent 2.2796(2.3016) | Loss 11.1947(12.1331) | Error 0.7666(0.8912) Steps 574(574.00) | Grad Norm 45.0256(67.4421) | Total Time 14.00(14.00)\n",
      "Iter 0004 | Time 48.1978(94.7190) | Bit/dim 9.5929(10.9407) | Xent 2.2610(2.3004) | Loss 10.7234(12.0909) | Error 0.7599(0.8872) Steps 574(574.00) | Grad Norm 31.3391(66.3590) | Total Time 14.00(14.00)\n",
      "Iter 0005 | Time 47.6226(93.3061) | Bit/dim 9.1776(10.8878) | Xent 2.2417(2.2986) | Loss 10.2984(12.0371) | Error 0.7589(0.8834) Steps 574(574.00) | Grad Norm 19.2379(64.9453) | Total Time 14.00(14.00)\n",
      "Iter 0006 | Time 48.8640(91.9728) | Bit/dim 8.9902(10.8308) | Xent 2.2225(2.2963) | Loss 10.1015(11.9790) | Error 0.7681(0.8799) Steps 574(574.00) | Grad Norm 19.1205(63.5706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 45.8370, Epoch Time 395.4028(395.4028), Bit/dim 8.8737(best: inf), Xent 2.1994, Loss 9.9733, Error 0.7600(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 60.3773(91.0250) | Bit/dim 8.8951(10.7728) | Xent 2.2046(2.2936) | Loss 9.9974(11.9196) | Error 0.7720(0.8767) Steps 574(574.00) | Grad Norm 25.0430(62.4148) | Total Time 14.00(14.00)\n",
      "Iter 0008 | Time 49.8311(89.7891) | Bit/dim 8.8697(10.7157) | Xent 2.1868(2.2904) | Loss 9.9631(11.8609) | Error 0.7690(0.8734) Steps 574(574.00) | Grad Norm 30.2164(61.4488) | Total Time 14.00(14.00)\n",
      "Iter 0009 | Time 49.0718(88.5676) | Bit/dim 8.8944(10.6610) | Xent 2.1634(2.2866) | Loss 9.9761(11.8043) | Error 0.7629(0.8701) Steps 574(574.00) | Grad Norm 31.3616(60.5462) | Total Time 14.00(14.00)\n",
      "Iter 0010 | Time 49.1245(87.3843) | Bit/dim 8.8301(10.6061) | Xent 2.1556(2.2826) | Loss 9.9079(11.7474) | Error 0.7479(0.8665) Steps 574(574.00) | Grad Norm 29.2093(59.6061) | Total Time 14.00(14.00)\n",
      "Iter 0011 | Time 48.0988(86.2058) | Bit/dim 8.7226(10.5496) | Xent 2.1342(2.2782) | Loss 9.7897(11.6887) | Error 0.7312(0.8624) Steps 574(574.00) | Grad Norm 23.9485(58.5364) | Total Time 14.00(14.00)\n",
      "Iter 0012 | Time 48.2817(85.0680) | Bit/dim 8.6132(10.4915) | Xent 2.1257(2.2736) | Loss 9.6760(11.6283) | Error 0.7248(0.8583) Steps 574(574.00) | Grad Norm 19.1268(57.3541) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 26.0964, Epoch Time 346.6875(393.9413), Bit/dim 8.4983(best: 8.8737), Xent 2.1092, Loss 9.5529, Error 0.7147(best: 0.7600)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 47.9653(83.9550) | Bit/dim 8.5002(10.4318) | Xent 2.1261(2.2692) | Loss 9.5633(11.5664) | Error 0.7381(0.8547) Steps 574(574.00) | Grad Norm 17.1344(56.1475) | Total Time 14.00(14.00)\n",
      "Iter 0014 | Time 48.3804(82.8877) | Bit/dim 8.3881(10.3705) | Xent 2.1125(2.2645) | Loss 9.4444(11.5027) | Error 0.7376(0.8512) Steps 574(574.00) | Grad Norm 17.5241(54.9888) | Total Time 14.00(14.00)\n",
      "Iter 0015 | Time 48.7689(81.8642) | Bit/dim 8.2981(10.3083) | Xent 2.1011(2.2596) | Loss 9.3487(11.4381) | Error 0.7340(0.8476) Steps 574(574.00) | Grad Norm 17.8558(53.8748) | Total Time 14.00(14.00)\n",
      "Iter 0016 | Time 48.1995(80.8542) | Bit/dim 8.1741(10.2443) | Xent 2.0919(2.2546) | Loss 9.2201(11.3715) | Error 0.7241(0.8439) Steps 574(574.00) | Grad Norm 16.2065(52.7448) | Total Time 14.00(14.00)\n",
      "Iter 0017 | Time 47.8709(79.8647) | Bit/dim 8.0709(10.1791) | Xent 2.0718(2.2491) | Loss 9.1068(11.3036) | Error 0.7003(0.8396) Steps 574(574.00) | Grad Norm 14.1776(51.5877) | Total Time 14.00(14.00)\n",
      "Iter 0018 | Time 48.3911(78.9205) | Bit/dim 7.9498(10.1122) | Xent 2.0436(2.2429) | Loss 8.9716(11.2336) | Error 0.6863(0.8350) Steps 574(574.00) | Grad Norm 12.2668(50.4081) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 25.9152, Epoch Time 331.2071(392.0593), Bit/dim 7.8390(best: 8.4983), Xent 2.0412, Loss 8.8596, Error 0.6807(best: 0.7147)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 48.1992(77.9989) | Bit/dim 7.8443(10.0442) | Xent 2.0482(2.2371) | Loss 8.8684(11.1627) | Error 0.6883(0.8306) Steps 574(574.00) | Grad Norm 11.4795(49.2403) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 47.8458(77.0943) | Bit/dim 7.7406(9.9750) | Xent 2.0492(2.2314) | Loss 8.7652(11.0908) | Error 0.6880(0.8263) Steps 574(574.00) | Grad Norm 10.7696(48.0861) | Total Time 14.00(14.00)\n",
      "Iter 0021 | Time 48.2658(76.2294) | Bit/dim 7.6377(9.9049) | Xent 2.0360(2.2256) | Loss 8.6557(11.0177) | Error 0.6909(0.8223) Steps 574(574.00) | Grad Norm 9.3305(46.9235) | Total Time 14.00(14.00)\n",
      "Iter 0022 | Time 47.1773(75.3579) | Bit/dim 7.5639(9.8347) | Xent 2.0443(2.2201) | Loss 8.5861(10.9448) | Error 0.7020(0.8187) Steps 574(574.00) | Grad Norm 8.1747(45.7610) | Total Time 14.00(14.00)\n",
      "Iter 0023 | Time 47.2774(74.5155) | Bit/dim 7.5085(9.7649) | Xent 2.0429(2.2148) | Loss 8.5299(10.8723) | Error 0.6955(0.8150) Steps 574(574.00) | Grad Norm 8.4422(44.6414) | Total Time 14.00(14.00)\n",
      "Iter 0024 | Time 47.9977(73.7199) | Bit/dim 7.4725(9.6961) | Xent 2.0601(2.2102) | Loss 8.5025(10.8012) | Error 0.7114(0.8119) Steps 574(574.00) | Grad Norm 9.3323(43.5822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 25.6839, Epoch Time 328.1771(390.1428), Bit/dim 7.4307(best: 7.8390), Xent 2.0496, Loss 8.4555, Error 0.6973(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 47.0334(72.9193) | Bit/dim 7.4374(9.6284) | Xent 2.0532(2.2055) | Loss 8.4640(10.7311) | Error 0.7074(0.8087) Steps 574(574.00) | Grad Norm 9.1651(42.5497) | Total Time 14.00(14.00)\n",
      "Iter 0026 | Time 47.9500(72.1702) | Bit/dim 7.3730(9.5607) | Xent 2.0569(2.2010) | Loss 8.4015(10.6612) | Error 0.7092(0.8057) Steps 574(574.00) | Grad Norm 8.1078(41.5164) | Total Time 14.00(14.00)\n",
      "Iter 0027 | Time 48.3914(71.4569) | Bit/dim 7.3089(9.4932) | Xent 2.0630(2.1969) | Loss 8.3404(10.5916) | Error 0.6987(0.8025) Steps 574(574.00) | Grad Norm 6.9729(40.4801) | Total Time 14.00(14.00)\n",
      "Iter 0028 | Time 47.4206(70.7358) | Bit/dim 7.2435(9.4257) | Xent 2.0721(2.1931) | Loss 8.2796(10.5222) | Error 0.7074(0.7997) Steps 574(574.00) | Grad Norm 6.3301(39.4556) | Total Time 14.00(14.00)\n",
      "Iter 0029 | Time 47.4235(70.0364) | Bit/dim 7.1827(9.3584) | Xent 2.0653(2.1893) | Loss 8.2154(10.4530) | Error 0.6994(0.7967) Steps 574(574.00) | Grad Norm 5.4065(38.4341) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 49.9223(69.4330) | Bit/dim 7.1464(9.2920) | Xent 2.0637(2.1855) | Loss 8.1783(10.3848) | Error 0.7039(0.7939) Steps 580(574.18) | Grad Norm 4.6667(37.4211) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 26.5261, Epoch Time 330.4576(388.3523), Bit/dim 7.1277(best: 7.4307), Xent 2.0661, Loss 8.1608, Error 0.7098(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 50.1600(68.8548) | Bit/dim 7.1295(9.2271) | Xent 2.0667(2.1820) | Loss 8.1628(10.3181) | Error 0.7117(0.7914) Steps 580(574.35) | Grad Norm 4.7446(36.4408) | Total Time 14.00(14.00)\n",
      "Iter 0032 | Time 49.2263(68.2660) | Bit/dim 7.1111(9.1637) | Xent 2.0721(2.1787) | Loss 8.1471(10.2530) | Error 0.7350(0.7897) Steps 580(574.52) | Grad Norm 4.8190(35.4921) | Total Time 14.00(14.00)\n",
      "Iter 0033 | Time 49.9772(67.7173) | Bit/dim 7.1037(9.1019) | Xent 2.0765(2.1756) | Loss 8.1420(10.1897) | Error 0.7372(0.7882) Steps 580(574.69) | Grad Norm 4.9547(34.5760) | Total Time 14.00(14.00)\n",
      "Iter 0034 | Time 49.9724(67.1849) | Bit/dim 7.0903(9.0415) | Xent 2.0738(2.1725) | Loss 8.1272(10.1278) | Error 0.7419(0.7868) Steps 580(574.85) | Grad Norm 4.8372(33.6839) | Total Time 14.00(14.00)\n",
      "Iter 0035 | Time 49.6294(66.6583) | Bit/dim 7.0874(8.9829) | Xent 2.0599(2.1692) | Loss 8.1173(10.0675) | Error 0.7232(0.7849) Steps 580(575.00) | Grad Norm 5.2600(32.8311) | Total Time 14.00(14.00)\n",
      "Iter 0036 | Time 50.0533(66.1601) | Bit/dim 7.0583(8.9252) | Xent 2.0422(2.1654) | Loss 8.0794(10.0078) | Error 0.7139(0.7827) Steps 580(575.15) | Grad Norm 5.2329(32.0032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 25.9536, Epoch Time 341.1624(386.9366), Bit/dim 7.0339(best: 7.1277), Xent 2.0374, Loss 8.0526, Error 0.6987(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 49.0244(65.6461) | Bit/dim 7.0314(8.8683) | Xent 2.0495(2.1619) | Loss 8.0562(9.9493) | Error 0.7064(0.7804) Steps 580(575.30) | Grad Norm 8.0523(31.2847) | Total Time 14.00(14.00)\n",
      "Iter 0038 | Time 49.1996(65.1527) | Bit/dim 7.0089(8.8126) | Xent 2.0730(2.1592) | Loss 8.0453(9.8922) | Error 0.7388(0.7792) Steps 580(575.44) | Grad Norm 14.3346(30.7762) | Total Time 14.00(14.00)\n",
      "Iter 0039 | Time 50.0255(64.6989) | Bit/dim 7.0113(8.7585) | Xent 2.1145(2.1579) | Loss 8.0686(9.8375) | Error 0.7719(0.7790) Steps 580(575.58) | Grad Norm 22.3465(30.5233) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 49.6667(64.2479) | Bit/dim 7.0084(8.7060) | Xent 2.1273(2.1570) | Loss 8.0720(9.7845) | Error 0.7700(0.7787) Steps 580(575.71) | Grad Norm 24.9790(30.3569) | Total Time 14.00(14.00)\n",
      "Iter 0041 | Time 48.9236(63.7882) | Bit/dim 6.9966(8.6547) | Xent 2.0696(2.1543) | Loss 8.0314(9.7319) | Error 0.7439(0.7777) Steps 580(575.84) | Grad Norm 13.8897(29.8629) | Total Time 14.00(14.00)\n",
      "Iter 0042 | Time 49.5383(63.3607) | Bit/dim 6.9816(8.6045) | Xent 2.0344(2.1507) | Loss 7.9988(9.6799) | Error 0.7175(0.7759) Steps 580(575.96) | Grad Norm 5.7412(29.1393) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 26.0217, Epoch Time 338.4179(385.4810), Bit/dim 6.9827(best: 7.0339), Xent 2.0764, Loss 8.0209, Error 0.7605(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 51.7775(63.0132) | Bit/dim 6.9799(8.5558) | Xent 2.0941(2.1490) | Loss 8.0269(9.6303) | Error 0.7720(0.7757) Steps 586(576.26) | Grad Norm 20.9831(28.8946) | Total Time 14.00(14.00)\n",
      "Iter 0044 | Time 50.8049(62.6469) | Bit/dim 6.9730(8.5083) | Xent 2.1274(2.1484) | Loss 8.0367(9.5825) | Error 0.7711(0.7756) Steps 586(576.56) | Grad Norm 25.4039(28.7899) | Total Time 14.00(14.00)\n",
      "Iter 0045 | Time 51.0999(62.3005) | Bit/dim 6.9569(8.4618) | Xent 2.0461(2.1453) | Loss 7.9800(9.5344) | Error 0.7388(0.7745) Steps 586(576.84) | Grad Norm 14.8948(28.3730) | Total Time 14.00(14.00)\n",
      "Iter 0046 | Time 52.4870(62.0061) | Bit/dim 6.9355(8.4160) | Xent 2.0225(2.1416) | Loss 7.9468(9.4868) | Error 0.7047(0.7724) Steps 592(577.29) | Grad Norm 3.3282(27.6217) | Total Time 14.00(14.00)\n",
      "Iter 0047 | Time 50.8916(61.6727) | Bit/dim 6.9317(8.3715) | Xent 2.0360(2.1385) | Loss 7.9498(9.4407) | Error 0.7061(0.7704) Steps 592(577.73) | Grad Norm 13.2310(27.1900) | Total Time 14.00(14.00)\n",
      "Iter 0048 | Time 53.3276(61.4223) | Bit/dim 6.9113(8.3277) | Xent 2.0277(2.1351) | Loss 7.9251(9.3952) | Error 0.7220(0.7690) Steps 598(578.34) | Grad Norm 11.5213(26.7199) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 26.9168, Epoch Time 353.4102(384.5189), Bit/dim 6.9091(best: 6.9827), Xent 1.9950, Loss 7.9067, Error 0.6795(best: 0.6807)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 53.4094(61.1819) | Bit/dim 6.9093(8.2851) | Xent 2.0019(2.1311) | Loss 7.9102(9.3507) | Error 0.6830(0.7664) Steps 598(578.93) | Grad Norm 2.2724(25.9865) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 53.0715(60.9386) | Bit/dim 6.8965(8.2434) | Xent 2.0178(2.1277) | Loss 7.9054(9.3073) | Error 0.6945(0.7642) Steps 598(579.50) | Grad Norm 8.9539(25.4755) | Total Time 14.00(14.00)\n",
      "Iter 0051 | Time 53.7706(60.7236) | Bit/dim 6.8928(8.2029) | Xent 2.0316(2.1249) | Loss 7.9086(9.2654) | Error 0.7218(0.7630) Steps 598(580.06) | Grad Norm 12.8734(25.0974) | Total Time 14.00(14.00)\n",
      "Iter 0052 | Time 52.9681(60.4909) | Bit/dim 6.8780(8.1632) | Xent 2.0055(2.1213) | Loss 7.8807(9.2238) | Error 0.6778(0.7604) Steps 598(580.60) | Grad Norm 7.4281(24.5673) | Total Time 14.00(14.00)\n",
      "Iter 0053 | Time 52.7158(60.2577) | Bit/dim 6.8666(8.1243) | Xent 2.0048(2.1178) | Loss 7.8690(9.1832) | Error 0.6744(0.7578) Steps 598(581.12) | Grad Norm 5.2981(23.9893) | Total Time 14.00(14.00)\n",
      "Iter 0054 | Time 52.8610(60.0358) | Bit/dim 6.8549(8.0862) | Xent 2.0285(2.1151) | Loss 7.8692(9.1438) | Error 0.7175(0.7566) Steps 598(581.63) | Grad Norm 14.9418(23.7178) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 26.8456, Epoch Time 361.5515(383.8299), Bit/dim 6.8433(best: 6.9091), Xent 2.0310, Loss 7.8588, Error 0.7035(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 52.2531(59.8023) | Bit/dim 6.8397(8.0488) | Xent 2.0393(2.1128) | Loss 7.8593(9.1052) | Error 0.7172(0.7554) Steps 598(582.12) | Grad Norm 17.5311(23.5322) | Total Time 14.00(14.00)\n",
      "Iter 0056 | Time 52.6061(59.5864) | Bit/dim 6.8146(8.0118) | Xent 2.0187(2.1100) | Loss 7.8239(9.0668) | Error 0.7215(0.7544) Steps 598(582.59) | Grad Norm 12.5823(23.2037) | Total Time 14.00(14.00)\n",
      "Iter 0057 | Time 52.8725(59.3850) | Bit/dim 6.8085(7.9757) | Xent 1.9938(2.1065) | Loss 7.8054(9.0289) | Error 0.6623(0.7516) Steps 598(583.06) | Grad Norm 3.2422(22.6049) | Total Time 14.00(14.00)\n",
      "Iter 0058 | Time 52.4576(59.1772) | Bit/dim 6.7911(7.9401) | Xent 1.9937(2.1031) | Loss 7.7880(8.9917) | Error 0.6636(0.7490) Steps 598(583.50) | Grad Norm 7.2686(22.1448) | Total Time 14.00(14.00)\n",
      "Iter 0059 | Time 53.0460(58.9932) | Bit/dim 6.7764(7.9052) | Xent 2.0054(2.1002) | Loss 7.7791(8.9553) | Error 0.7134(0.7479) Steps 598(583.94) | Grad Norm 14.5932(21.9183) | Total Time 14.00(14.00)\n",
      "Iter 0060 | Time 52.7523(58.8060) | Bit/dim 6.7584(7.8708) | Xent 2.0141(2.0976) | Loss 7.7654(8.9196) | Error 0.7035(0.7466) Steps 598(584.36) | Grad Norm 19.5931(21.8485) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 26.7050, Epoch Time 358.7326(383.0770), Bit/dim 6.7427(best: 6.8433), Xent 2.0575, Loss 7.7715, Error 0.7326(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 52.6306(58.6207) | Bit/dim 6.7409(7.8369) | Xent 2.0747(2.0969) | Loss 7.7782(8.8854) | Error 0.7335(0.7462) Steps 598(584.77) | Grad Norm 25.5240(21.9588) | Total Time 14.00(14.00)\n",
      "Iter 0062 | Time 53.2221(58.4588) | Bit/dim 6.7194(7.8034) | Xent 2.1427(2.0983) | Loss 7.7908(8.8526) | Error 0.7868(0.7474) Steps 598(585.17) | Grad Norm 32.6175(22.2785) | Total Time 14.00(14.00)\n",
      "Iter 0063 | Time 51.9634(58.2639) | Bit/dim 6.7204(7.7709) | Xent 2.1376(2.0995) | Loss 7.7892(8.8207) | Error 0.7609(0.7478) Steps 598(585.55) | Grad Norm 32.5980(22.5881) | Total Time 14.00(14.00)\n",
      "Iter 0064 | Time 51.9478(58.0744) | Bit/dim 6.6581(7.7375) | Xent 2.0135(2.0969) | Loss 7.6649(8.7860) | Error 0.6990(0.7464) Steps 592(585.75) | Grad Norm 15.2177(22.3670) | Total Time 14.00(14.00)\n",
      "Iter 0065 | Time 52.3817(57.9036) | Bit/dim 6.6351(7.7045) | Xent 1.9984(2.0940) | Loss 7.6342(8.7514) | Error 0.6813(0.7444) Steps 598(586.11) | Grad Norm 10.6523(22.0156) | Total Time 14.00(14.00)\n",
      "Iter 0066 | Time 52.3549(57.7372) | Bit/dim 6.6203(7.6719) | Xent 2.0375(2.0923) | Loss 7.6390(8.7181) | Error 0.7350(0.7441) Steps 598(586.47) | Grad Norm 18.1019(21.8982) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 26.5119, Epoch Time 357.0174(382.2952), Bit/dim 6.5851(best: 6.7427), Xent 2.0520, Loss 7.6111, Error 0.7315(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 53.5753(57.6123) | Bit/dim 6.5797(7.6392) | Xent 2.0713(2.0916) | Loss 7.6154(8.6850) | Error 0.7406(0.7440) Steps 598(586.82) | Grad Norm 22.1931(21.9070) | Total Time 14.00(14.00)\n",
      "Iter 0068 | Time 53.0159(57.4744) | Bit/dim 6.7535(7.6126) | Xent 2.5741(2.1061) | Loss 8.0405(8.6656) | Error 0.8080(0.7459) Steps 586(586.79) | Grad Norm 63.5196(23.1554) | Total Time 14.00(14.00)\n",
      "Iter 0069 | Time 52.2750(57.3184) | Bit/dim 6.8800(7.5906) | Xent 3.2622(2.1408) | Loss 8.5111(8.6610) | Error 0.8119(0.7479) Steps 586(586.77) | Grad Norm 72.3014(24.6298) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 52.2586(57.1667) | Bit/dim 6.4998(7.5579) | Xent 2.0714(2.1387) | Loss 7.5355(8.6272) | Error 0.7263(0.7473) Steps 586(586.74) | Grad Norm 14.0467(24.3123) | Total Time 14.00(14.00)\n",
      "Iter 0071 | Time 51.9044(57.0088) | Bit/dim 6.7538(7.5338) | Xent 2.6723(2.1547) | Loss 8.0900(8.6111) | Error 0.8742(0.7511) Steps 574(586.36) | Grad Norm 52.2680(25.1509) | Total Time 14.00(14.00)\n",
      "Iter 0072 | Time 50.6951(56.8194) | Bit/dim 6.5530(7.5043) | Xent 2.0425(2.1513) | Loss 7.5743(8.5800) | Error 0.7032(0.7496) Steps 580(586.17) | Grad Norm 19.4040(24.9785) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 25.8656, Epoch Time 355.5656(381.4933), Bit/dim 6.5156(best: 6.5851), Xent 2.0882, Loss 7.5597, Error 0.7371(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 50.9918(56.6445) | Bit/dim 6.5096(7.4745) | Xent 2.0962(2.1497) | Loss 7.5577(8.5493) | Error 0.7423(0.7494) Steps 580(585.99) | Grad Norm 12.4863(24.6038) | Total Time 14.00(14.00)\n",
      "Iter 0074 | Time 51.5292(56.4911) | Bit/dim 6.6415(7.4495) | Xent 2.0905(2.1479) | Loss 7.6868(8.5235) | Error 0.7541(0.7496) Steps 580(585.81) | Grad Norm 25.2981(24.6246) | Total Time 14.00(14.00)\n",
      "Iter 0075 | Time 50.5952(56.3142) | Bit/dim 6.4720(7.4202) | Xent 2.1046(2.1466) | Loss 7.5243(8.4935) | Error 0.7586(0.7498) Steps 580(585.63) | Grad Norm 11.0715(24.2180) | Total Time 14.00(14.00)\n",
      "Iter 0076 | Time 50.1471(56.1292) | Bit/dim 6.4330(7.3906) | Xent 2.1092(2.1455) | Loss 7.4876(8.4633) | Error 0.7559(0.7500) Steps 580(585.46) | Grad Norm 17.7643(24.0244) | Total Time 14.00(14.00)\n",
      "Iter 0077 | Time 49.8190(55.9399) | Bit/dim 6.3883(7.3605) | Xent 2.0764(2.1434) | Loss 7.4265(8.4322) | Error 0.7332(0.7495) Steps 574(585.12) | Grad Norm 6.8415(23.5089) | Total Time 14.00(14.00)\n",
      "Iter 0078 | Time 51.5312(55.8076) | Bit/dim 6.3800(7.3311) | Xent 2.1462(2.1435) | Loss 7.4532(8.4028) | Error 0.7873(0.7506) Steps 580(584.97) | Grad Norm 17.1752(23.3189) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 25.5231, Epoch Time 346.0518(380.4300), Bit/dim 6.3375(best: 6.5156), Xent 2.0701, Loss 7.3726, Error 0.7243(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 49.8792(55.6298) | Bit/dim 6.3390(7.3013) | Xent 2.0894(2.1419) | Loss 7.3836(8.3723) | Error 0.7498(0.7506) Steps 574(584.64) | Grad Norm 10.6527(22.9389) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 48.7431(55.4232) | Bit/dim 6.2582(7.2700) | Xent 2.0613(2.1395) | Loss 7.2889(8.3398) | Error 0.7243(0.7498) Steps 574(584.32) | Grad Norm 11.8652(22.6067) | Total Time 14.00(14.00)\n",
      "Iter 0081 | Time 49.2777(55.2388) | Bit/dim 6.2309(7.2389) | Xent 2.1285(2.1391) | Loss 7.2952(8.3084) | Error 0.7669(0.7503) Steps 574(584.01) | Grad Norm 23.0540(22.6201) | Total Time 14.00(14.00)\n",
      "Iter 0082 | Time 49.5493(55.0681) | Bit/dim 6.1971(7.2076) | Xent 2.1043(2.1381) | Loss 7.2492(8.2767) | Error 0.7721(0.7510) Steps 574(583.71) | Grad Norm 19.8464(22.5369) | Total Time 14.00(14.00)\n",
      "Iter 0083 | Time 50.4584(54.9298) | Bit/dim 6.1531(7.1760) | Xent 2.0553(2.1356) | Loss 7.1808(8.2438) | Error 0.7300(0.7504) Steps 574(583.42) | Grad Norm 9.8104(22.1551) | Total Time 14.00(14.00)\n",
      "Iter 0084 | Time 49.8274(54.7768) | Bit/dim 6.0949(7.1435) | Xent 2.0747(2.1338) | Loss 7.1323(8.2104) | Error 0.7328(0.7498) Steps 574(583.13) | Grad Norm 8.3099(21.7398) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 25.8685, Epoch Time 339.2640(379.1951), Bit/dim 6.0402(best: 6.3375), Xent 2.0524, Loss 7.0664, Error 0.7190(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 50.2387(54.6406) | Bit/dim 6.0446(7.1106) | Xent 2.0571(2.1315) | Loss 7.0732(8.1763) | Error 0.7166(0.7488) Steps 574(582.86) | Grad Norm 14.0375(21.5087) | Total Time 14.00(14.00)\n",
      "Iter 0086 | Time 49.2140(54.4778) | Bit/dim 6.0006(7.0773) | Xent 2.2185(2.1341) | Loss 7.1098(8.1443) | Error 0.8109(0.7507) Steps 568(582.41) | Grad Norm 38.4430(22.0167) | Total Time 14.00(14.00)\n",
      "Iter 0087 | Time 48.8775(54.3098) | Bit/dim 6.3305(7.0549) | Xent 2.6992(2.1510) | Loss 7.6801(8.1304) | Error 0.8556(0.7538) Steps 568(581.98) | Grad Norm 91.8523(24.1118) | Total Time 14.00(14.00)\n",
      "Iter 0088 | Time 51.9578(54.2393) | Bit/dim 7.0622(7.0551) | Xent 2.7494(2.1690) | Loss 8.4369(8.1396) | Error 0.8370(0.7563) Steps 580(581.92) | Grad Norm 125.7956(27.1623) | Total Time 14.00(14.00)\n",
      "Iter 0089 | Time 49.7126(54.1035) | Bit/dim 6.0210(7.0241) | Xent 2.2119(2.1703) | Loss 7.1269(8.1092) | Error 0.8016(0.7577) Steps 580(581.86) | Grad Norm 22.4725(27.0216) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 49.0053(53.9505) | Bit/dim 6.4236(7.0061) | Xent 2.5834(2.1827) | Loss 7.7153(8.0974) | Error 0.8107(0.7593) Steps 574(581.63) | Grad Norm 55.8155(27.8854) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 25.3335, Epoch Time 340.4277(378.0320), Bit/dim 6.1024(best: 6.0402), Xent 2.6125, Loss 7.4086, Error 0.8629(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 49.6762(53.8223) | Bit/dim 6.1003(6.9789) | Xent 2.6128(2.1956) | Loss 7.4067(8.0767) | Error 0.8672(0.7625) Steps 574(581.40) | Grad Norm 25.6579(27.8186) | Total Time 14.00(14.00)\n",
      "Iter 0092 | Time 49.5500(53.6941) | Bit/dim 6.0935(6.9523) | Xent 2.2280(2.1966) | Loss 7.2075(8.0506) | Error 0.7812(0.7631) Steps 574(581.18) | Grad Norm 16.2551(27.4717) | Total Time 14.00(14.00)\n",
      "Iter 0093 | Time 50.0431(53.5846) | Bit/dim 6.2051(6.9299) | Xent 2.4147(2.2031) | Loss 7.4125(8.0315) | Error 0.8542(0.7658) Steps 580(581.14) | Grad Norm 20.7695(27.2706) | Total Time 14.00(14.00)\n",
      "Iter 0094 | Time 48.7283(53.4389) | Bit/dim 6.1464(6.9064) | Xent 2.2536(2.2046) | Loss 7.2732(8.0087) | Error 0.8164(0.7673) Steps 568(580.75) | Grad Norm 12.3237(26.8222) | Total Time 14.00(14.00)\n",
      "Iter 0095 | Time 48.3705(53.2868) | Bit/dim 6.0829(6.8817) | Xent 2.1718(2.2036) | Loss 7.1688(7.9835) | Error 0.7865(0.7679) Steps 568(580.37) | Grad Norm 9.4796(26.3019) | Total Time 14.00(14.00)\n",
      "Iter 0096 | Time 47.8399(53.1234) | Bit/dim 6.0174(6.8558) | Xent 2.2641(2.2054) | Loss 7.1495(7.9585) | Error 0.8100(0.7692) Steps 568(579.99) | Grad Norm 13.4178(25.9154) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 25.4587, Epoch Time 335.9006(376.7681), Bit/dim 5.9303(best: 6.0402), Xent 2.2094, Loss 7.0350, Error 0.7968(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 47.4328(52.9527) | Bit/dim 5.9301(6.8280) | Xent 2.2280(2.2061) | Loss 7.0441(7.9311) | Error 0.8009(0.7701) Steps 574(579.82) | Grad Norm 9.7896(25.4316) | Total Time 14.00(14.00)\n",
      "Iter 0098 | Time 47.7150(52.7956) | Bit/dim 5.8864(6.7997) | Xent 2.1587(2.2047) | Loss 6.9658(7.9021) | Error 0.7820(0.7705) Steps 568(579.46) | Grad Norm 8.0589(24.9105) | Total Time 14.00(14.00)\n",
      "Iter 0099 | Time 48.3654(52.6627) | Bit/dim 5.8809(6.7722) | Xent 2.1496(2.2030) | Loss 6.9557(7.8737) | Error 0.7742(0.7706) Steps 574(579.30) | Grad Norm 5.7047(24.3343) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 48.9001(52.5498) | Bit/dim 5.8646(6.7450) | Xent 2.2057(2.2031) | Loss 6.9674(7.8465) | Error 0.7931(0.7713) Steps 568(578.96) | Grad Norm 7.6369(23.8334) | Total Time 14.00(14.00)\n",
      "Iter 0101 | Time 47.6762(52.4036) | Bit/dim 5.8736(6.7188) | Xent 2.1963(2.2029) | Loss 6.9717(7.8203) | Error 0.7851(0.7717) Steps 568(578.63) | Grad Norm 7.3718(23.3395) | Total Time 14.00(14.00)\n",
      "Iter 0102 | Time 47.4097(52.2538) | Bit/dim 5.8324(6.6922) | Xent 2.1553(2.2015) | Loss 6.9100(7.7930) | Error 0.7940(0.7724) Steps 568(578.31) | Grad Norm 7.8948(22.8762) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 26.0824, Epoch Time 329.3662(375.3460), Bit/dim 5.7903(best: 5.9303), Xent 2.1086, Loss 6.8446, Error 0.7715(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 47.1380(52.1003) | Bit/dim 5.7896(6.6651) | Xent 2.1279(2.1993) | Loss 6.8535(7.7648) | Error 0.7768(0.7725) Steps 568(578.00) | Grad Norm 7.3307(22.4098) | Total Time 14.00(14.00)\n",
      "Iter 0104 | Time 46.9314(51.9452) | Bit/dim 5.7432(6.6375) | Xent 2.1011(2.1963) | Loss 6.7938(7.7357) | Error 0.7386(0.7715) Steps 556(577.34) | Grad Norm 4.1015(21.8606) | Total Time 14.00(14.00)\n",
      "Iter 0105 | Time 46.5663(51.7839) | Bit/dim 5.7708(6.6115) | Xent 2.1121(2.1938) | Loss 6.8268(7.7084) | Error 0.7345(0.7704) Steps 544(576.34) | Grad Norm 8.0400(21.4459) | Total Time 14.00(14.00)\n",
      "Iter 0106 | Time 46.2659(51.6183) | Bit/dim 5.7496(6.5856) | Xent 2.0839(2.1905) | Loss 6.7916(7.6809) | Error 0.7328(0.7692) Steps 538(575.19) | Grad Norm 4.2566(20.9303) | Total Time 14.00(14.00)\n",
      "Iter 0107 | Time 45.9135(51.4472) | Bit/dim 5.7502(6.5606) | Xent 2.0940(2.1876) | Loss 6.7972(7.6544) | Error 0.7532(0.7688) Steps 538(574.07) | Grad Norm 6.1801(20.4878) | Total Time 14.00(14.00)\n",
      "Iter 0108 | Time 46.1363(51.2879) | Bit/dim 5.7346(6.5358) | Xent 2.0865(2.1846) | Loss 6.7778(7.6281) | Error 0.7306(0.7676) Steps 550(573.35) | Grad Norm 3.6867(19.9837) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 25.5774, Epoch Time 321.2304(373.7226), Bit/dim 5.7072(best: 5.7903), Xent 2.0611, Loss 6.7378, Error 0.7321(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 45.6533(51.1188) | Bit/dim 5.7125(6.5111) | Xent 2.0680(2.1811) | Loss 6.7465(7.6016) | Error 0.7371(0.7667) Steps 550(572.65) | Grad Norm 5.1837(19.5397) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 46.0569(50.9670) | Bit/dim 5.6858(6.4863) | Xent 2.0605(2.1775) | Loss 6.7161(7.5751) | Error 0.7169(0.7652) Steps 550(571.97) | Grad Norm 3.5014(19.0586) | Total Time 14.00(14.00)\n",
      "Iter 0111 | Time 46.7242(50.8397) | Bit/dim 5.6790(6.4621) | Xent 2.0587(2.1739) | Loss 6.7083(7.5491) | Error 0.7204(0.7639) Steps 550(571.31) | Grad Norm 4.5290(18.6227) | Total Time 14.00(14.00)\n",
      "Iter 0112 | Time 46.1970(50.7004) | Bit/dim 5.6554(6.4379) | Xent 2.0461(2.1701) | Loss 6.6784(7.5229) | Error 0.7014(0.7620) Steps 544(570.49) | Grad Norm 3.8659(18.1800) | Total Time 14.00(14.00)\n",
      "Iter 0113 | Time 46.0809(50.5618) | Bit/dim 5.6569(6.4145) | Xent 2.0555(2.1666) | Loss 6.6847(7.4978) | Error 0.7130(0.7605) Steps 538(569.52) | Grad Norm 3.8360(17.7497) | Total Time 14.00(14.00)\n",
      "Iter 0114 | Time 46.6441(50.4443) | Bit/dim 5.6383(6.3912) | Xent 2.0444(2.1630) | Loss 6.6605(7.4727) | Error 0.7124(0.7591) Steps 532(568.39) | Grad Norm 5.3572(17.3779) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 24.8575, Epoch Time 318.1770(372.0562), Bit/dim 5.6247(best: 5.7072), Xent 2.0077, Loss 6.6285, Error 0.6816(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 45.2899(50.2896) | Bit/dim 5.6074(6.3677) | Xent 2.0196(2.1587) | Loss 6.6172(7.4470) | Error 0.7054(0.7575) Steps 538(567.48) | Grad Norm 1.9750(16.9158) | Total Time 14.00(14.00)\n",
      "Iter 0116 | Time 46.6381(50.1801) | Bit/dim 5.6183(6.3452) | Xent 2.0478(2.1553) | Loss 6.6422(7.4229) | Error 0.7125(0.7561) Steps 550(566.96) | Grad Norm 7.4504(16.6318) | Total Time 14.00(14.00)\n",
      "Iter 0117 | Time 45.8198(50.0493) | Bit/dim 5.6051(6.3230) | Xent 2.0317(2.1516) | Loss 6.6210(7.3988) | Error 0.7095(0.7547) Steps 538(566.09) | Grad Norm 6.5028(16.3280) | Total Time 14.00(14.00)\n",
      "Iter 0118 | Time 46.5744(49.9450) | Bit/dim 5.5920(6.3011) | Xent 2.0114(2.1474) | Loss 6.5978(7.3748) | Error 0.6858(0.7526) Steps 538(565.25) | Grad Norm 1.4711(15.8823) | Total Time 14.00(14.00)\n",
      "Iter 0119 | Time 46.5772(49.8440) | Bit/dim 5.5730(6.2792) | Xent 2.0256(2.1438) | Loss 6.5858(7.3511) | Error 0.7034(0.7512) Steps 544(564.61) | Grad Norm 8.0404(15.6470) | Total Time 14.00(14.00)\n",
      "Iter 0120 | Time 44.9646(49.6976) | Bit/dim 5.5793(6.2582) | Xent 2.0517(2.1410) | Loss 6.6052(7.3287) | Error 0.7231(0.7503) Steps 532(563.63) | Grad Norm 14.3319(15.6076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 24.3334, Epoch Time 316.0549(370.3762), Bit/dim 5.5580(best: 5.6247), Xent 2.0586, Loss 6.5872, Error 0.7535(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 46.2456(49.5941) | Bit/dim 5.5447(6.2368) | Xent 2.0629(2.1387) | Loss 6.5762(7.3062) | Error 0.7456(0.7502) Steps 544(563.04) | Grad Norm 18.4786(15.6937) | Total Time 14.00(14.00)\n",
      "Iter 0122 | Time 46.0295(49.4871) | Bit/dim 5.5729(6.2169) | Xent 2.1290(2.1384) | Loss 6.6374(7.2861) | Error 0.7575(0.7504) Steps 532(562.11) | Grad Norm 20.7981(15.8468) | Total Time 14.00(14.00)\n",
      "Iter 0123 | Time 46.1753(49.3878) | Bit/dim 5.5300(6.1963) | Xent 2.0485(2.1357) | Loss 6.5543(7.2641) | Error 0.7401(0.7501) Steps 538(561.39) | Grad Norm 15.2364(15.8285) | Total Time 14.00(14.00)\n",
      "Iter 0124 | Time 46.5546(49.3028) | Bit/dim 5.5065(6.1756) | Xent 1.9906(2.1313) | Loss 6.5018(7.2413) | Error 0.6900(0.7483) Steps 538(560.69) | Grad Norm 5.9079(15.5309) | Total Time 14.00(14.00)\n",
      "Iter 0125 | Time 46.9638(49.2326) | Bit/dim 5.5015(6.1554) | Xent 2.0049(2.1275) | Loss 6.5040(7.2191) | Error 0.6897(0.7465) Steps 538(560.00) | Grad Norm 4.2697(15.1931) | Total Time 14.00(14.00)\n",
      "Iter 0126 | Time 45.7984(49.1296) | Bit/dim 5.4764(6.1350) | Xent 2.0294(2.1246) | Loss 6.4911(7.1973) | Error 0.7206(0.7458) Steps 538(559.34) | Grad Norm 10.8989(15.0642) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 24.6474, Epoch Time 318.5523(368.8214), Bit/dim 5.4722(best: 5.5580), Xent 1.9933, Loss 6.4688, Error 0.6850(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 46.4815(49.0501) | Bit/dim 5.4795(6.1153) | Xent 2.0130(2.1212) | Loss 6.4860(7.1760) | Error 0.7045(0.7445) Steps 538(558.70) | Grad Norm 10.0579(14.9140) | Total Time 14.00(14.00)\n",
      "Iter 0128 | Time 46.3949(48.9705) | Bit/dim 5.4563(6.0956) | Xent 1.9937(2.1174) | Loss 6.4532(7.1543) | Error 0.6818(0.7426) Steps 538(558.08) | Grad Norm 1.8329(14.5216) | Total Time 14.00(14.00)\n",
      "Iter 0129 | Time 46.5788(48.8987) | Bit/dim 5.4417(6.0760) | Xent 2.0261(2.1147) | Loss 6.4548(7.1333) | Error 0.7115(0.7417) Steps 538(557.48) | Grad Norm 9.5861(14.3735) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 45.4377(48.7949) | Bit/dim 5.4298(6.0566) | Xent 2.0050(2.1114) | Loss 6.4323(7.1123) | Error 0.6994(0.7404) Steps 538(556.90) | Grad Norm 8.3871(14.1939) | Total Time 14.00(14.00)\n",
      "Iter 0131 | Time 46.2137(48.7175) | Bit/dim 5.4151(6.0373) | Xent 1.9677(2.1071) | Loss 6.3989(7.0909) | Error 0.6758(0.7385) Steps 538(556.33) | Grad Norm 2.3745(13.8394) | Total Time 14.00(14.00)\n",
      "Iter 0132 | Time 46.8640(48.6619) | Bit/dim 5.4085(6.0185) | Xent 2.0158(2.1043) | Loss 6.4164(7.0706) | Error 0.7090(0.7376) Steps 538(555.78) | Grad Norm 10.7430(13.7465) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 24.4138, Epoch Time 318.6505(367.3163), Bit/dim 5.3951(best: 5.4722), Xent 1.9712, Loss 6.3807, Error 0.6713(best: 0.6795)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 46.3413(48.5922) | Bit/dim 5.3977(5.9998) | Xent 1.9873(2.1008) | Loss 6.3914(7.0503) | Error 0.6847(0.7360) Steps 538(555.25) | Grad Norm 8.8334(13.5991) | Total Time 14.00(14.00)\n",
      "Iter 0134 | Time 46.1909(48.5202) | Bit/dim 5.3797(5.9812) | Xent 1.9587(2.0966) | Loss 6.3591(7.0295) | Error 0.6769(0.7342) Steps 544(554.91) | Grad Norm 3.2190(13.2877) | Total Time 14.00(14.00)\n",
      "Iter 0135 | Time 45.6438(48.4339) | Bit/dim 5.3602(5.9626) | Xent 2.0203(2.0943) | Loss 6.3703(7.0097) | Error 0.7224(0.7339) Steps 538(554.40) | Grad Norm 11.9193(13.2466) | Total Time 14.00(14.00)\n",
      "Iter 0136 | Time 46.6845(48.3814) | Bit/dim 5.3518(5.9443) | Xent 1.9954(2.0913) | Loss 6.3495(6.9899) | Error 0.6889(0.7325) Steps 538(553.91) | Grad Norm 9.7186(13.1408) | Total Time 14.00(14.00)\n",
      "Iter 0137 | Time 46.9179(48.3375) | Bit/dim 5.3377(5.9261) | Xent 1.9677(2.0876) | Loss 6.3216(6.9699) | Error 0.6724(0.7307) Steps 544(553.61) | Grad Norm 2.0357(12.8076) | Total Time 14.00(14.00)\n",
      "Iter 0138 | Time 45.2566(48.2451) | Bit/dim 5.3218(5.9080) | Xent 1.9927(2.0848) | Loss 6.3181(6.9503) | Error 0.6981(0.7298) Steps 538(553.14) | Grad Norm 10.9109(12.7507) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 24.3132, Epoch Time 317.3680(365.8179), Bit/dim 5.3177(best: 5.3951), Xent 1.9594, Loss 6.2974, Error 0.6593(best: 0.6713)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 47.0336(48.2087) | Bit/dim 5.3187(5.8903) | Xent 1.9702(2.0813) | Loss 6.3038(6.9309) | Error 0.6786(0.7282) Steps 544(552.87) | Grad Norm 9.6243(12.6569) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 47.1188(48.1761) | Bit/dim 5.2838(5.8721) | Xent 1.9459(2.0773) | Loss 6.2567(6.9107) | Error 0.6598(0.7262) Steps 544(552.60) | Grad Norm 2.9605(12.3660) | Total Time 14.00(14.00)\n",
      "Iter 0141 | Time 47.6159(48.1592) | Bit/dim 5.3235(5.8556) | Xent 2.0082(2.0752) | Loss 6.3276(6.8932) | Error 0.7065(0.7256) Steps 544(552.35) | Grad Norm 13.8502(12.4106) | Total Time 14.00(14.00)\n",
      "Iter 0142 | Time 46.8010(48.1185) | Bit/dim 5.3171(5.8395) | Xent 1.9998(2.0729) | Loss 6.3170(6.8759) | Error 0.7014(0.7249) Steps 544(552.09) | Grad Norm 13.8684(12.4543) | Total Time 14.00(14.00)\n",
      "Iter 0143 | Time 48.7495(48.1374) | Bit/dim 5.2915(5.8230) | Xent 1.9614(2.0696) | Loss 6.2722(6.8578) | Error 0.6706(0.7232) Steps 550(552.03) | Grad Norm 5.7223(12.2523) | Total Time 14.00(14.00)\n",
      "Iter 0144 | Time 47.3577(48.1140) | Bit/dim 5.2780(5.8067) | Xent 2.0505(2.0690) | Loss 6.3032(6.8412) | Error 0.7389(0.7237) Steps 544(551.79) | Grad Norm 20.5443(12.5011) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 24.6062, Epoch Time 330.4276(364.7562), Bit/dim 5.2814(best: 5.3177), Xent 1.9883, Loss 6.2756, Error 0.6884(best: 0.6593)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 48.5671(48.1276) | Bit/dim 5.2860(5.7911) | Xent 2.0101(2.0672) | Loss 6.2911(6.8247) | Error 0.7084(0.7232) Steps 544(551.56) | Grad Norm 13.8666(12.5421) | Total Time 14.00(14.00)\n",
      "Iter 0146 | Time 48.3017(48.1328) | Bit/dim 5.2521(5.7749) | Xent 1.9688(2.0643) | Loss 6.2364(6.8070) | Error 0.6886(0.7222) Steps 544(551.33) | Grad Norm 9.3052(12.4450) | Total Time 14.00(14.00)\n",
      "Iter 0147 | Time 49.0821(48.1613) | Bit/dim 5.2989(5.7606) | Xent 2.0137(2.0628) | Loss 6.3057(6.7920) | Error 0.7151(0.7220) Steps 550(551.29) | Grad Norm 15.3607(12.5324) | Total Time 14.00(14.00)\n",
      "Iter 0148 | Time 47.4556(48.1402) | Bit/dim 5.2592(5.7456) | Xent 1.9765(2.0602) | Loss 6.2474(6.7757) | Error 0.6885(0.7210) Steps 544(551.07) | Grad Norm 13.8632(12.5724) | Total Time 14.00(14.00)\n",
      "Iter 0149 | Time 49.5942(48.1838) | Bit/dim 5.2691(5.7313) | Xent 2.0666(2.0604) | Loss 6.3025(6.7615) | Error 0.7270(0.7212) Steps 538(550.68) | Grad Norm 26.2116(12.9815) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 47.1244(48.1520) | Bit/dim 5.2556(5.7170) | Xent 2.0354(2.0596) | Loss 6.2733(6.7468) | Error 0.7323(0.7215) Steps 544(550.48) | Grad Norm 24.0304(13.3130) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 24.2209, Epoch Time 330.4876(363.7281), Bit/dim 5.2575(best: 5.2814), Xent 1.9897, Loss 6.2524, Error 0.7045(best: 0.6593)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 47.3341(48.1275) | Bit/dim 5.2730(5.7037) | Xent 2.0127(2.0582) | Loss 6.2794(6.7328) | Error 0.7070(0.7211) Steps 550(550.46) | Grad Norm 18.3581(13.4644) | Total Time 14.00(14.00)\n",
      "Iter 0152 | Time 45.4932(48.0484) | Bit/dim 5.2384(5.6897) | Xent 1.9489(2.0549) | Loss 6.2129(6.7172) | Error 0.6713(0.7196) Steps 538(550.09) | Grad Norm 6.9754(13.2697) | Total Time 14.00(14.00)\n",
      "Iter 0153 | Time 46.0727(47.9892) | Bit/dim 5.2383(5.6762) | Xent 1.9327(2.0513) | Loss 6.2047(6.7018) | Error 0.6605(0.7178) Steps 544(549.91) | Grad Norm 7.0370(13.0827) | Total Time 14.00(14.00)\n",
      "Iter 0154 | Time 48.3361(47.9996) | Bit/dim 5.1878(5.6615) | Xent 1.9621(2.0486) | Loss 6.1688(6.6858) | Error 0.6771(0.7166) Steps 556(550.09) | Grad Norm 6.2270(12.8770) | Total Time 14.00(14.00)\n",
      "Iter 0155 | Time 48.5450(48.0159) | Bit/dim 5.2071(5.6479) | Xent 1.9821(2.0466) | Loss 6.1982(6.6712) | Error 0.6810(0.7155) Steps 550(550.09) | Grad Norm 8.0401(12.7319) | Total Time 14.00(14.00)\n",
      "Iter 0156 | Time 45.6786(47.9458) | Bit/dim 5.2019(5.6345) | Xent 1.9461(2.0436) | Loss 6.1750(6.6563) | Error 0.6721(0.7142) Steps 544(549.91) | Grad Norm 7.0726(12.5621) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 24.6447, Epoch Time 321.8112(362.4706), Bit/dim 5.1948(best: 5.2575), Xent 1.9336, Loss 6.1616, Error 0.6591(best: 0.6593)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 47.4273(47.9303) | Bit/dim 5.1954(5.6213) | Xent 1.9374(2.0404) | Loss 6.1641(6.6415) | Error 0.6663(0.7128) Steps 544(549.73) | Grad Norm 5.8892(12.3620) | Total Time 14.00(14.00)\n",
      "Iter 0158 | Time 46.8653(47.8983) | Bit/dim 5.1654(5.6077) | Xent 1.9683(2.0382) | Loss 6.1495(6.6268) | Error 0.6789(0.7117) Steps 544(549.56) | Grad Norm 6.3161(12.1806) | Total Time 14.00(14.00)\n",
      "Iter 0159 | Time 48.6675(47.9214) | Bit/dim 5.1610(5.5943) | Xent 1.9543(2.0357) | Loss 6.1382(6.6121) | Error 0.6686(0.7105) Steps 544(549.39) | Grad Norm 4.1138(11.9386) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 47.1970(47.8997) | Bit/dim 5.1740(5.5817) | Xent 1.9381(2.0328) | Loss 6.1430(6.5981) | Error 0.6651(0.7091) Steps 544(549.23) | Grad Norm 5.0283(11.7313) | Total Time 14.00(14.00)\n",
      "Iter 0161 | Time 47.3658(47.8836) | Bit/dim 5.1664(5.5692) | Xent 1.9564(2.0305) | Loss 6.1446(6.5844) | Error 0.6709(0.7079) Steps 544(549.07) | Grad Norm 3.4612(11.4832) | Total Time 14.00(14.00)\n",
      "Iter 0162 | Time 47.1470(47.8615) | Bit/dim 5.1541(5.5567) | Xent 1.9454(2.0279) | Loss 6.1268(6.5707) | Error 0.6838(0.7072) Steps 544(548.92) | Grad Norm 5.8117(11.3130) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 24.1078, Epoch Time 325.1151(361.3499), Bit/dim 5.1442(best: 5.1948), Xent 1.9225, Loss 6.1055, Error 0.6536(best: 0.6591)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 47.1012(47.8387) | Bit/dim 5.1419(5.5443) | Xent 1.9346(2.0251) | Loss 6.1092(6.5569) | Error 0.6700(0.7061) Steps 544(548.77) | Grad Norm 5.0631(11.1255) | Total Time 14.00(14.00)\n",
      "Iter 0164 | Time 46.8623(47.8094) | Bit/dim 5.1288(5.5318) | Xent 1.9265(2.0222) | Loss 6.0920(6.5429) | Error 0.6635(0.7048) Steps 544(548.63) | Grad Norm 2.1610(10.8566) | Total Time 14.00(14.00)\n",
      "Iter 0165 | Time 47.4829(47.7996) | Bit/dim 5.1177(5.5194) | Xent 1.9449(2.0199) | Loss 6.0902(6.5293) | Error 0.6825(0.7042) Steps 544(548.49) | Grad Norm 5.1780(10.6862) | Total Time 14.00(14.00)\n",
      "Iter 0166 | Time 48.5687(47.8227) | Bit/dim 5.1315(5.5078) | Xent 1.9421(2.0175) | Loss 6.1025(6.5165) | Error 0.6680(0.7031) Steps 550(548.53) | Grad Norm 3.9328(10.4836) | Total Time 14.00(14.00)\n",
      "Iter 0167 | Time 47.7591(47.8208) | Bit/dim 5.1178(5.4961) | Xent 1.9217(2.0147) | Loss 6.0787(6.5034) | Error 0.6545(0.7016) Steps 550(548.58) | Grad Norm 2.8377(10.2542) | Total Time 14.00(14.00)\n",
      "Iter 0168 | Time 48.2037(47.8323) | Bit/dim 5.1035(5.4843) | Xent 1.9455(2.0126) | Loss 6.0762(6.4906) | Error 0.6846(0.7011) Steps 556(548.80) | Grad Norm 8.6307(10.2055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 24.8826, Epoch Time 326.6086(360.3077), Bit/dim 5.1154(best: 5.1442), Xent 1.9195, Loss 6.0751, Error 0.6545(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 47.8270(47.8321) | Bit/dim 5.1134(5.4732) | Xent 1.9511(2.0107) | Loss 6.0890(6.4785) | Error 0.6745(0.7003) Steps 556(549.02) | Grad Norm 10.1642(10.2043) | Total Time 14.00(14.00)\n",
      "Iter 0170 | Time 48.5377(47.8533) | Bit/dim 5.1078(5.4622) | Xent 1.9388(2.0086) | Loss 6.0772(6.4665) | Error 0.6714(0.6994) Steps 550(549.05) | Grad Norm 5.8512(10.0737) | Total Time 14.00(14.00)\n",
      "Iter 0171 | Time 49.7027(47.9088) | Bit/dim 5.1129(5.4517) | Xent 1.9029(2.0054) | Loss 6.0644(6.4544) | Error 0.6472(0.6979) Steps 550(549.08) | Grad Norm 2.5293(9.8474) | Total Time 14.00(14.00)\n",
      "Iter 0172 | Time 47.9293(47.9094) | Bit/dim 5.0808(5.4406) | Xent 1.8978(2.0022) | Loss 6.0297(6.4417) | Error 0.6505(0.6965) Steps 556(549.28) | Grad Norm 3.6736(9.6622) | Total Time 14.00(14.00)\n",
      "Iter 0173 | Time 49.6996(47.9631) | Bit/dim 5.0857(5.4300) | Xent 1.9302(2.0000) | Loss 6.0508(6.4300) | Error 0.6679(0.6956) Steps 556(549.48) | Grad Norm 8.7075(9.6335) | Total Time 14.00(14.00)\n",
      "Iter 0174 | Time 45.7981(47.8981) | Bit/dim 5.1244(5.4208) | Xent 1.9405(1.9982) | Loss 6.0946(6.4199) | Error 0.6796(0.6951) Steps 538(549.14) | Grad Norm 13.5608(9.7513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 24.9703, Epoch Time 330.1900(359.4042), Bit/dim 5.0825(best: 5.1154), Xent 1.9647, Loss 6.0649, Error 0.7034(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 48.8570(47.9269) | Bit/dim 5.0756(5.4104) | Xent 1.9736(1.9975) | Loss 6.0624(6.4092) | Error 0.7057(0.6954) Steps 556(549.35) | Grad Norm 14.6241(9.8975) | Total Time 14.00(14.00)\n",
      "Iter 0176 | Time 49.2793(47.9675) | Bit/dim 5.1025(5.4012) | Xent 1.9856(1.9971) | Loss 6.0953(6.3998) | Error 0.6963(0.6955) Steps 556(549.55) | Grad Norm 15.4918(10.0654) | Total Time 14.00(14.00)\n",
      "Iter 0177 | Time 48.8037(47.9926) | Bit/dim 5.0862(5.3917) | Xent 1.9309(1.9952) | Loss 6.0517(6.3893) | Error 0.6784(0.6949) Steps 562(549.92) | Grad Norm 14.8849(10.2099) | Total Time 14.00(14.00)\n",
      "Iter 0178 | Time 48.9744(48.0220) | Bit/dim 5.0836(5.3825) | Xent 1.9650(1.9942) | Loss 6.0661(6.3796) | Error 0.6838(0.6946) Steps 568(550.46) | Grad Norm 15.5912(10.3714) | Total Time 14.00(14.00)\n",
      "Iter 0179 | Time 46.9631(47.9903) | Bit/dim 5.1627(5.3759) | Xent 1.9193(1.9920) | Loss 6.1224(6.3719) | Error 0.6699(0.6939) Steps 550(550.45) | Grad Norm 12.6564(10.4399) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 48.8749(48.0168) | Bit/dim 5.0475(5.3661) | Xent 1.9072(1.9895) | Loss 6.0011(6.3608) | Error 0.6690(0.6931) Steps 562(550.79) | Grad Norm 5.0468(10.2781) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 24.9670, Epoch Time 332.6207(358.6007), Bit/dim 5.1483(best: 5.0825), Xent 1.9226, Loss 6.1097, Error 0.6711(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 49.3863(48.0579) | Bit/dim 5.1517(5.3596) | Xent 1.9563(1.9885) | Loss 6.1299(6.3539) | Error 0.6877(0.6930) Steps 562(551.13) | Grad Norm 21.0133(10.6002) | Total Time 14.00(14.00)\n",
      "Iter 0182 | Time 47.4824(48.0406) | Bit/dim 5.2945(5.3577) | Xent 1.9702(1.9879) | Loss 6.2796(6.3516) | Error 0.7037(0.6933) Steps 556(551.28) | Grad Norm 13.4982(10.6871) | Total Time 14.00(14.00)\n",
      "Iter 0183 | Time 47.2320(48.0164) | Bit/dim 5.3384(5.3571) | Xent 1.9332(1.9863) | Loss 6.3050(6.3502) | Error 0.6654(0.6924) Steps 550(551.24) | Grad Norm 11.0942(10.6993) | Total Time 14.00(14.00)\n",
      "Iter 0184 | Time 48.0866(48.0185) | Bit/dim 5.1193(5.3500) | Xent 1.9105(1.9840) | Loss 6.0746(6.3420) | Error 0.6679(0.6917) Steps 562(551.56) | Grad Norm 4.7078(10.5196) | Total Time 14.00(14.00)\n",
      "Iter 0185 | Time 48.9772(48.0472) | Bit/dim 5.2887(5.3481) | Xent 2.0016(1.9845) | Loss 6.2895(6.3404) | Error 0.7072(0.6922) Steps 568(552.05) | Grad Norm 23.0207(10.8946) | Total Time 14.00(14.00)\n",
      "Iter 0186 | Time 47.9455(48.0442) | Bit/dim 5.1765(5.3430) | Xent 1.9243(1.9827) | Loss 6.1387(6.3343) | Error 0.6686(0.6915) Steps 562(552.35) | Grad Norm 6.9020(10.7748) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 25.3162, Epoch Time 330.2855(357.7512), Bit/dim 5.3352(best: 5.0825), Xent 1.9158, Loss 6.2931, Error 0.6631(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 45.5714(47.9700) | Bit/dim 5.3346(5.3427) | Xent 1.9329(1.9812) | Loss 6.3011(6.3333) | Error 0.6733(0.6909) Steps 550(552.28) | Grad Norm 8.7200(10.7132) | Total Time 14.00(14.00)\n",
      "Iter 0188 | Time 46.7702(47.9340) | Bit/dim 5.2688(5.3405) | Xent 1.9016(1.9788) | Loss 6.2196(6.3299) | Error 0.6649(0.6901) Steps 550(552.21) | Grad Norm 6.4676(10.5858) | Total Time 14.00(14.00)\n",
      "Iter 0189 | Time 47.4074(47.9182) | Bit/dim 5.1124(5.3337) | Xent 1.9224(1.9771) | Loss 6.0736(6.3222) | Error 0.6583(0.6892) Steps 562(552.51) | Grad Norm 6.4565(10.4620) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 48.7908(47.9444) | Bit/dim 5.1955(5.3295) | Xent 1.9940(1.9777) | Loss 6.1925(6.3183) | Error 0.6981(0.6895) Steps 568(552.97) | Grad Norm 18.4716(10.7022) | Total Time 14.00(14.00)\n",
      "Iter 0191 | Time 48.1567(47.9508) | Bit/dim 5.1718(5.3248) | Xent 2.2151(1.9848) | Loss 6.2794(6.3172) | Error 0.7716(0.6919) Steps 556(553.06) | Grad Norm 35.6480(11.4506) | Total Time 14.00(14.00)\n",
      "Iter 0192 | Time 47.1576(47.9270) | Bit/dim 5.3003(5.3241) | Xent 2.5216(2.0009) | Loss 6.5611(6.3245) | Error 0.8047(0.6953) Steps 550(552.97) | Grad Norm 44.8762(12.4534) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 25.4169, Epoch Time 325.0065(356.7689), Bit/dim 5.2426(best: 5.0825), Xent 1.9636, Loss 6.2244, Error 0.6887(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 46.7673(47.8922) | Bit/dim 5.2477(5.3218) | Xent 1.9661(1.9998) | Loss 6.2307(6.3217) | Error 0.6903(0.6952) Steps 556(553.06) | Grad Norm 9.4073(12.3620) | Total Time 14.00(14.00)\n",
      "Iter 0194 | Time 45.5089(47.8207) | Bit/dim 5.2285(5.3190) | Xent 2.0828(2.0023) | Loss 6.2699(6.3201) | Error 0.7441(0.6966) Steps 544(552.79) | Grad Norm 14.5727(12.4283) | Total Time 14.00(14.00)\n",
      "Iter 0195 | Time 49.0965(47.8589) | Bit/dim 5.0957(5.3123) | Xent 2.0715(2.0044) | Loss 6.1314(6.3145) | Error 0.7475(0.6981) Steps 562(553.07) | Grad Norm 5.9644(12.2344) | Total Time 14.00(14.00)\n",
      "Iter 0196 | Time 48.7510(47.8857) | Bit/dim 5.1303(5.3068) | Xent 2.0284(2.0051) | Loss 6.1445(6.3094) | Error 0.7205(0.6988) Steps 562(553.33) | Grad Norm 9.2906(12.1461) | Total Time 14.00(14.00)\n",
      "Iter 0197 | Time 46.9822(47.8586) | Bit/dim 5.1200(5.3012) | Xent 1.9933(2.0048) | Loss 6.1166(6.3036) | Error 0.7000(0.6989) Steps 550(553.23) | Grad Norm 5.0640(11.9336) | Total Time 14.00(14.00)\n",
      "Iter 0198 | Time 46.6261(47.8216) | Bit/dim 5.1688(5.2972) | Xent 2.0249(2.0054) | Loss 6.1812(6.2999) | Error 0.7219(0.6995) Steps 544(552.96) | Grad Norm 10.6364(11.8947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 24.8964, Epoch Time 324.4640(355.7997), Bit/dim 5.1524(best: 5.0825), Xent 2.0312, Loss 6.1680, Error 0.7230(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 46.0718(47.7691) | Bit/dim 5.1482(5.2928) | Xent 2.0380(2.0064) | Loss 6.1672(6.2959) | Error 0.7232(0.7003) Steps 544(552.69) | Grad Norm 11.6304(11.8868) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 46.8870(47.7427) | Bit/dim 5.1044(5.2871) | Xent 2.0786(2.0085) | Loss 6.1437(6.2914) | Error 0.7264(0.7010) Steps 556(552.79) | Grad Norm 11.6605(11.8800) | Total Time 14.00(14.00)\n",
      "Iter 0201 | Time 47.9774(47.7497) | Bit/dim 5.0776(5.2808) | Xent 2.1365(2.0124) | Loss 6.1459(6.2870) | Error 0.7605(0.7028) Steps 562(553.06) | Grad Norm 14.3969(11.9555) | Total Time 14.00(14.00)\n",
      "Iter 0202 | Time 47.7391(47.7494) | Bit/dim 5.1083(5.2756) | Xent 2.1838(2.0175) | Loss 6.2001(6.2844) | Error 0.7574(0.7045) Steps 562(553.33) | Grad Norm 16.0276(12.0777) | Total Time 14.00(14.00)\n",
      "Iter 0203 | Time 50.3010(47.8259) | Bit/dim 5.0657(5.2693) | Xent 2.0197(2.0176) | Loss 6.0755(6.2781) | Error 0.7189(0.7049) Steps 562(553.59) | Grad Norm 4.4586(11.8491) | Total Time 14.00(14.00)\n",
      "Iter 0204 | Time 48.3272(47.8410) | Bit/dim 5.0826(5.2637) | Xent 2.0305(2.0180) | Loss 6.0979(6.2727) | Error 0.7274(0.7056) Steps 550(553.48) | Grad Norm 7.4579(11.7174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 24.9111, Epoch Time 328.2015(354.9718), Bit/dim 5.0504(best: 5.0825), Xent 2.0552, Loss 6.0780, Error 0.7310(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 48.7537(47.8684) | Bit/dim 5.0533(5.2574) | Xent 2.0839(2.0199) | Loss 6.0953(6.2674) | Error 0.7321(0.7064) Steps 562(553.74) | Grad Norm 5.8951(11.5427) | Total Time 14.00(14.00)\n",
      "Iter 0206 | Time 48.4399(47.8855) | Bit/dim 5.0417(5.2510) | Xent 1.9803(2.0187) | Loss 6.0318(6.2603) | Error 0.6986(0.7061) Steps 562(553.99) | Grad Norm 4.0994(11.3194) | Total Time 14.00(14.00)\n",
      "Iter 0207 | Time 47.1872(47.8646) | Bit/dim 5.0448(5.2448) | Xent 2.1190(2.0218) | Loss 6.1043(6.2557) | Error 0.7534(0.7075) Steps 544(553.69) | Grad Norm 8.7947(11.2437) | Total Time 14.00(14.00)\n",
      "Iter 0208 | Time 49.9655(47.9276) | Bit/dim 5.0368(5.2385) | Xent 2.0962(2.0240) | Loss 6.0849(6.2505) | Error 0.7498(0.7088) Steps 550(553.58) | Grad Norm 6.9615(11.1152) | Total Time 14.00(14.00)\n",
      "Iter 0209 | Time 48.3135(47.9392) | Bit/dim 5.0038(5.2315) | Xent 2.0294(2.0241) | Loss 6.0185(6.2436) | Error 0.7251(0.7093) Steps 550(553.47) | Grad Norm 4.4265(10.9145) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 46.3532(47.8916) | Bit/dim 5.0327(5.2255) | Xent 2.0770(2.0257) | Loss 6.0712(6.2384) | Error 0.7365(0.7101) Steps 526(552.65) | Grad Norm 10.1892(10.8928) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 24.5161, Epoch Time 329.1268(354.1964), Bit/dim 5.0300(best: 5.0504), Xent 1.9431, Loss 6.0016, Error 0.6821(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 48.2909(47.9036) | Bit/dim 5.0404(5.2200) | Xent 1.9539(2.0236) | Loss 6.0174(6.2318) | Error 0.6951(0.7097) Steps 538(552.21) | Grad Norm 5.7352(10.7380) | Total Time 14.00(14.00)\n",
      "Iter 0212 | Time 47.6389(47.8956) | Bit/dim 5.0062(5.2136) | Xent 2.0063(2.0231) | Loss 6.0093(6.2251) | Error 0.7092(0.7097) Steps 538(551.78) | Grad Norm 4.8213(10.5605) | Total Time 14.00(14.00)\n",
      "Iter 0213 | Time 49.1991(47.9347) | Bit/dim 5.0010(5.2072) | Xent 1.9552(2.0210) | Loss 5.9786(6.2177) | Error 0.6936(0.7092) Steps 538(551.37) | Grad Norm 6.9662(10.4527) | Total Time 14.00(14.00)\n",
      "Iter 0214 | Time 47.0059(47.9069) | Bit/dim 4.9916(5.2007) | Xent 1.9563(2.0191) | Loss 5.9697(6.2103) | Error 0.6905(0.7086) Steps 526(550.61) | Grad Norm 4.3397(10.2693) | Total Time 14.00(14.00)\n",
      "Iter 0215 | Time 47.1661(47.8846) | Bit/dim 4.9759(5.1940) | Xent 1.9698(2.0176) | Loss 5.9608(6.2028) | Error 0.6904(0.7081) Steps 520(549.69) | Grad Norm 4.2504(10.0888) | Total Time 14.00(14.00)\n",
      "Iter 0216 | Time 48.9129(47.9155) | Bit/dim 4.9871(5.1878) | Xent 1.9484(2.0155) | Loss 5.9612(6.1955) | Error 0.6885(0.7075) Steps 532(549.16) | Grad Norm 7.0839(9.9986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 23.9020, Epoch Time 327.7284(353.4024), Bit/dim 4.9802(best: 5.0300), Xent 1.9502, Loss 5.9553, Error 0.6825(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 45.3935(47.8398) | Bit/dim 4.9889(5.1818) | Xent 1.9779(2.0144) | Loss 5.9778(6.1890) | Error 0.7010(0.7073) Steps 514(548.10) | Grad Norm 6.3110(9.8880) | Total Time 14.00(14.00)\n",
      "Iter 0218 | Time 47.5539(47.8313) | Bit/dim 4.9751(5.1756) | Xent 1.9349(2.0120) | Loss 5.9426(6.1816) | Error 0.6786(0.7064) Steps 520(547.26) | Grad Norm 3.4621(9.6952) | Total Time 14.00(14.00)\n",
      "Iter 0219 | Time 47.1909(47.8120) | Bit/dim 4.9715(5.1695) | Xent 1.9379(2.0098) | Loss 5.9405(6.1744) | Error 0.6713(0.7054) Steps 526(546.62) | Grad Norm 8.0086(9.6446) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 47.7910(47.8114) | Bit/dim 4.9461(5.1628) | Xent 1.9408(2.0077) | Loss 5.9165(6.1666) | Error 0.6925(0.7050) Steps 526(546.00) | Grad Norm 7.8944(9.5921) | Total Time 14.00(14.00)\n",
      "Iter 0221 | Time 47.1293(47.7910) | Bit/dim 4.9303(5.1558) | Xent 1.9974(2.0074) | Loss 5.9290(6.1595) | Error 0.7116(0.7052) Steps 520(545.22) | Grad Norm 11.8247(9.6591) | Total Time 14.00(14.00)\n",
      "Iter 0222 | Time 48.5931(47.8150) | Bit/dim 4.9499(5.1496) | Xent 2.0822(2.0097) | Loss 5.9910(6.1545) | Error 0.7406(0.7062) Steps 520(544.47) | Grad Norm 14.4789(9.8037) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 23.9149, Epoch Time 323.6156(352.5088), Bit/dim 4.9689(best: 4.9802), Xent 2.0058, Loss 5.9718, Error 0.7147(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 46.8267(47.7854) | Bit/dim 4.9777(5.1445) | Xent 2.0271(2.0102) | Loss 5.9913(6.1496) | Error 0.7105(0.7064) Steps 520(543.73) | Grad Norm 19.6597(10.0994) | Total Time 14.00(14.00)\n",
      "Iter 0224 | Time 43.7703(47.6649) | Bit/dim 5.0610(5.1420) | Xent 1.9977(2.0098) | Loss 6.0599(6.1469) | Error 0.7224(0.7069) Steps 496(542.30) | Grad Norm 13.9980(10.2163) | Total Time 14.00(14.00)\n",
      "Iter 0225 | Time 45.6694(47.6051) | Bit/dim 4.9665(5.1367) | Xent 1.9362(2.0076) | Loss 5.9346(6.1405) | Error 0.6738(0.7059) Steps 508(541.27) | Grad Norm 7.8091(10.1441) | Total Time 14.00(14.00)\n",
      "Iter 0226 | Time 46.3156(47.5664) | Bit/dim 5.0019(5.1327) | Xent 1.9964(2.0073) | Loss 6.0000(6.1363) | Error 0.7076(0.7059) Steps 514(540.45) | Grad Norm 15.4786(10.3041) | Total Time 14.00(14.00)\n",
      "Iter 0227 | Time 45.1960(47.4953) | Bit/dim 5.0038(5.1288) | Xent 1.9394(2.0052) | Loss 5.9735(6.1314) | Error 0.6794(0.7051) Steps 520(539.84) | Grad Norm 5.3227(10.1547) | Total Time 14.00(14.00)\n",
      "Iter 0228 | Time 46.4459(47.4638) | Bit/dim 4.9878(5.1246) | Xent 1.9620(2.0039) | Loss 5.9688(6.1265) | Error 0.6935(0.7048) Steps 520(539.24) | Grad Norm 7.5787(10.0774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 23.7535, Epoch Time 313.9505(351.3520), Bit/dim 4.9652(best: 4.9689), Xent 1.9584, Loss 5.9444, Error 0.6897(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 46.4109(47.4322) | Bit/dim 4.9657(5.1198) | Xent 1.9933(2.0036) | Loss 5.9623(6.1216) | Error 0.7194(0.7052) Steps 526(538.85) | Grad Norm 7.8531(10.0107) | Total Time 14.00(14.00)\n",
      "Iter 0230 | Time 45.9726(47.3884) | Bit/dim 4.9249(5.1139) | Xent 1.9659(2.0025) | Loss 5.9079(6.1152) | Error 0.6915(0.7048) Steps 514(538.10) | Grad Norm 3.6146(9.8188) | Total Time 14.00(14.00)\n",
      "Iter 0231 | Time 45.9987(47.3467) | Bit/dim 4.9379(5.1087) | Xent 1.9565(2.0011) | Loss 5.9161(6.1092) | Error 0.6809(0.7041) Steps 508(537.20) | Grad Norm 4.5637(9.6611) | Total Time 14.00(14.00)\n",
      "Iter 0232 | Time 46.6435(47.3256) | Bit/dim 4.9240(5.1031) | Xent 1.9355(1.9991) | Loss 5.8917(6.1027) | Error 0.6849(0.7035) Steps 520(536.68) | Grad Norm 4.8577(9.5170) | Total Time 14.00(14.00)\n",
      "Iter 0233 | Time 47.3127(47.3252) | Bit/dim 4.9008(5.0971) | Xent 1.9527(1.9977) | Loss 5.8772(6.0959) | Error 0.6873(0.7030) Steps 526(536.36) | Grad Norm 3.4483(9.3350) | Total Time 14.00(14.00)\n",
      "Iter 0234 | Time 47.3845(47.3270) | Bit/dim 4.8852(5.0907) | Xent 1.9353(1.9959) | Loss 5.8528(6.0886) | Error 0.6824(0.7024) Steps 526(536.05) | Grad Norm 3.9865(9.1745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 23.5012, Epoch Time 319.2554(350.3891), Bit/dim 4.8852(best: 4.9652), Xent 1.9228, Loss 5.8466, Error 0.6710(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 49.6517(47.3967) | Bit/dim 4.8883(5.0846) | Xent 1.9439(1.9943) | Loss 5.8602(6.0818) | Error 0.6863(0.7019) Steps 526(535.75) | Grad Norm 4.0431(9.0206) | Total Time 14.00(14.00)\n",
      "Iter 0236 | Time 48.7930(47.4386) | Bit/dim 4.8693(5.0782) | Xent 1.9207(1.9921) | Loss 5.8296(6.0742) | Error 0.6759(0.7011) Steps 526(535.46) | Grad Norm 3.8588(8.8657) | Total Time 14.00(14.00)\n",
      "Iter 0237 | Time 48.0724(47.4576) | Bit/dim 4.8606(5.0716) | Xent 1.9115(1.9897) | Loss 5.8163(6.0665) | Error 0.6689(0.7002) Steps 526(535.17) | Grad Norm 2.4054(8.6719) | Total Time 14.00(14.00)\n",
      "Iter 0238 | Time 47.0029(47.4440) | Bit/dim 4.8565(5.0652) | Xent 1.9210(1.9876) | Loss 5.8170(6.0590) | Error 0.6700(0.6993) Steps 520(534.72) | Grad Norm 4.3087(8.5410) | Total Time 14.00(14.00)\n",
      "Iter 0239 | Time 45.5496(47.3872) | Bit/dim 4.8787(5.0596) | Xent 1.9118(1.9853) | Loss 5.8346(6.0523) | Error 0.6754(0.6985) Steps 514(534.10) | Grad Norm 4.1346(8.4088) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 48.5077(47.4208) | Bit/dim 4.8525(5.0534) | Xent 1.8907(1.9825) | Loss 5.7979(6.0446) | Error 0.6611(0.6974) Steps 520(533.67) | Grad Norm 2.3116(8.2259) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 23.5522, Epoch Time 327.3598(349.6982), Bit/dim 4.8313(best: 4.8852), Xent 1.8804, Loss 5.7715, Error 0.6496(best: 0.6536)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 46.8954(47.4050) | Bit/dim 4.8285(5.0466) | Xent 1.9005(1.9800) | Loss 5.7788(6.0367) | Error 0.6723(0.6967) Steps 508(532.90) | Grad Norm 3.8503(8.0946) | Total Time 14.00(14.00)\n",
      "Iter 0242 | Time 48.1371(47.4270) | Bit/dim 4.8475(5.0407) | Xent 1.9029(1.9777) | Loss 5.7989(6.0295) | Error 0.6700(0.6959) Steps 508(532.16) | Grad Norm 6.4060(8.0440) | Total Time 14.00(14.00)\n",
      "Iter 0243 | Time 45.5572(47.3709) | Bit/dim 4.8690(5.0355) | Xent 1.9132(1.9758) | Loss 5.8256(6.0234) | Error 0.6751(0.6952) Steps 514(531.61) | Grad Norm 8.4882(8.0573) | Total Time 14.00(14.00)\n",
      "Iter 0244 | Time 47.2662(47.3678) | Bit/dim 4.8863(5.0310) | Xent 1.8980(1.9735) | Loss 5.8353(6.0178) | Error 0.6665(0.6944) Steps 520(531.26) | Grad Norm 13.9400(8.2338) | Total Time 14.00(14.00)\n",
      "Iter 0245 | Time 43.8729(47.2629) | Bit/dim 5.0745(5.0323) | Xent 1.9102(1.9716) | Loss 6.0296(6.0181) | Error 0.6598(0.6933) Steps 508(530.57) | Grad Norm 13.5046(8.3919) | Total Time 14.00(14.00)\n",
      "Iter 0246 | Time 44.5299(47.1809) | Bit/dim 4.8927(5.0281) | Xent 1.8861(1.9690) | Loss 5.8358(6.0126) | Error 0.6500(0.6920) Steps 508(529.89) | Grad Norm 5.7960(8.3140) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 23.7488, Epoch Time 316.0835(348.6898), Bit/dim 5.0445(best: 4.8313), Xent 1.9246, Loss 6.0068, Error 0.6733(best: 0.6496)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 45.6447(47.1348) | Bit/dim 5.0230(5.0280) | Xent 1.9675(1.9690) | Loss 6.0068(6.0125) | Error 0.6785(0.6916) Steps 514(529.41) | Grad Norm 20.0217(8.6653) | Total Time 14.00(14.00)\n",
      "Iter 0248 | Time 46.4870(47.1154) | Bit/dim 5.0671(5.0292) | Xent 1.9047(1.9670) | Loss 6.0195(6.0127) | Error 0.6623(0.6908) Steps 520(529.13) | Grad Norm 8.9009(8.6723) | Total Time 14.00(14.00)\n",
      "Iter 0249 | Time 45.3524(47.0625) | Bit/dim 5.1845(5.0338) | Xent 1.9251(1.9658) | Loss 6.1471(6.0167) | Error 0.6695(0.6901) Steps 520(528.86) | Grad Norm 8.7649(8.6751) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 44.8287(46.9955) | Bit/dim 5.0060(5.0330) | Xent 1.8883(1.9634) | Loss 5.9502(6.0147) | Error 0.6526(0.6890) Steps 508(528.23) | Grad Norm 5.0643(8.5668) | Total Time 14.00(14.00)\n",
      "Iter 0251 | Time 47.6768(47.0159) | Bit/dim 5.0869(5.0346) | Xent 1.9345(1.9626) | Loss 6.0542(6.0159) | Error 0.6771(0.6886) Steps 508(527.62) | Grad Norm 13.7406(8.7220) | Total Time 14.00(14.00)\n",
      "Iter 0252 | Time 44.1579(46.9302) | Bit/dim 4.9627(5.0325) | Xent 1.8913(1.9604) | Loss 5.9084(6.0127) | Error 0.6618(0.6878) Steps 508(527.03) | Grad Norm 5.7452(8.6327) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 23.7736, Epoch Time 313.7984(347.6431), Bit/dim 5.0321(best: 4.8313), Xent 1.8710, Loss 5.9676, Error 0.6359(best: 0.6496)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 44.6452(46.8616) | Bit/dim 5.0330(5.0325) | Xent 1.9123(1.9590) | Loss 5.9891(6.0120) | Error 0.6599(0.6870) Steps 520(526.82) | Grad Norm 9.6112(8.6621) | Total Time 14.00(14.00)\n",
      "Iter 0254 | Time 44.5101(46.7911) | Bit/dim 4.9649(5.0304) | Xent 1.9172(1.9577) | Loss 5.9235(6.0093) | Error 0.6695(0.6865) Steps 508(526.26) | Grad Norm 7.6508(8.6317) | Total Time 14.00(14.00)\n",
      "Iter 0255 | Time 47.5086(46.8126) | Bit/dim 4.9844(5.0291) | Xent 1.9266(1.9568) | Loss 5.9477(6.0075) | Error 0.6765(0.6862) Steps 514(525.89) | Grad Norm 9.7554(8.6654) | Total Time 14.00(14.00)\n",
      "Iter 0256 | Time 45.8249(46.7830) | Bit/dim 4.9066(5.0254) | Xent 1.8857(1.9547) | Loss 5.8494(6.0027) | Error 0.6566(0.6853) Steps 508(525.35) | Grad Norm 6.7869(8.6091) | Total Time 14.00(14.00)\n",
      "Iter 0257 | Time 46.5104(46.7748) | Bit/dim 4.8957(5.0215) | Xent 1.8859(1.9526) | Loss 5.8387(5.9978) | Error 0.6586(0.6845) Steps 514(525.01) | Grad Norm 4.8116(8.4951) | Total Time 14.00(14.00)\n",
      "Iter 0258 | Time 46.2563(46.7593) | Bit/dim 4.8793(5.0172) | Xent 1.8884(1.9507) | Loss 5.8235(5.9926) | Error 0.6680(0.6840) Steps 514(524.68) | Grad Norm 9.1390(8.5145) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 23.3823, Epoch Time 314.9289(346.6616), Bit/dim 4.9058(best: 4.8313), Xent 1.8802, Loss 5.8459, Error 0.6551(best: 0.6359)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 48.3665(46.8075) | Bit/dim 4.9029(5.0138) | Xent 1.8988(1.9491) | Loss 5.8523(5.9884) | Error 0.6607(0.6833) Steps 508(524.18) | Grad Norm 13.1860(8.6546) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 47.7879(46.8369) | Bit/dim 4.8771(5.0097) | Xent 1.8774(1.9470) | Loss 5.8159(5.9832) | Error 0.6532(0.6824) Steps 514(523.88) | Grad Norm 8.2003(8.6410) | Total Time 14.00(14.00)\n",
      "Iter 0261 | Time 48.2254(46.8785) | Bit/dim 4.8356(5.0045) | Xent 1.8265(1.9434) | Loss 5.7489(5.9762) | Error 0.6360(0.6810) Steps 520(523.76) | Grad Norm 4.6341(8.5208) | Total Time 14.00(14.00)\n",
      "Iter 0262 | Time 47.8434(46.9075) | Bit/dim 4.8563(5.0000) | Xent 1.8697(1.9412) | Loss 5.7912(5.9706) | Error 0.6524(0.6801) Steps 514(523.47) | Grad Norm 7.7267(8.4969) | Total Time 14.00(14.00)\n",
      "Iter 0263 | Time 48.4919(46.9550) | Bit/dim 4.8490(4.9955) | Xent 1.8381(1.9381) | Loss 5.7681(5.9645) | Error 0.6407(0.6790) Steps 514(523.18) | Grad Norm 6.2778(8.4304) | Total Time 14.00(14.00)\n",
      "Iter 0264 | Time 47.9552(46.9850) | Bit/dim 4.8179(4.9902) | Xent 1.8323(1.9349) | Loss 5.7340(5.9576) | Error 0.6452(0.6779) Steps 514(522.91) | Grad Norm 4.1969(8.3034) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.9569, Epoch Time 328.7136(346.1232), Bit/dim 4.8098(best: 4.8313), Xent 1.8030, Loss 5.7113, Error 0.6264(best: 0.6359)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 48.4679(47.0295) | Bit/dim 4.8157(4.9849) | Xent 1.8380(1.9320) | Loss 5.7346(5.9509) | Error 0.6415(0.6769) Steps 520(522.82) | Grad Norm 4.8514(8.1998) | Total Time 14.00(14.00)\n",
      "Iter 0266 | Time 47.2711(47.0368) | Bit/dim 4.8146(4.9798) | Xent 1.8176(1.9285) | Loss 5.7234(5.9441) | Error 0.6336(0.6756) Steps 514(522.56) | Grad Norm 4.6850(8.0944) | Total Time 14.00(14.00)\n",
      "Iter 0267 | Time 48.0133(47.0661) | Bit/dim 4.8004(4.9744) | Xent 1.8209(1.9253) | Loss 5.7108(5.9371) | Error 0.6398(0.6745) Steps 514(522.30) | Grad Norm 4.3258(7.9813) | Total Time 14.00(14.00)\n",
      "Iter 0268 | Time 48.7693(47.1171) | Bit/dim 4.7822(4.9687) | Xent 1.8244(1.9223) | Loss 5.6945(5.9298) | Error 0.6334(0.6732) Steps 520(522.23) | Grad Norm 4.4958(7.8767) | Total Time 14.00(14.00)\n",
      "Iter 0269 | Time 47.9465(47.1420) | Bit/dim 4.7868(4.9632) | Xent 1.8407(1.9198) | Loss 5.7071(5.9231) | Error 0.6460(0.6724) Steps 520(522.16) | Grad Norm 8.8643(7.9064) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 48.0064(47.1680) | Bit/dim 4.8114(4.9587) | Xent 2.0129(1.9226) | Loss 5.8179(5.9200) | Error 0.7090(0.6735) Steps 520(522.10) | Grad Norm 23.1614(8.3640) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 25.3561, Epoch Time 329.7023(345.6306), Bit/dim 5.1387(best: 4.8098), Xent 2.5799, Loss 6.4286, Error 0.7574(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 48.5859(47.2105) | Bit/dim 5.1314(4.9639) | Xent 2.5907(1.9427) | Loss 6.4267(5.9352) | Error 0.7478(0.6758) Steps 544(522.76) | Grad Norm 52.2751(9.6814) | Total Time 14.00(14.00)\n",
      "Iter 0272 | Time 45.1056(47.1473) | Bit/dim 5.3090(4.9742) | Xent 2.0937(1.9472) | Loss 6.3558(5.9478) | Error 0.7390(0.6777) Steps 514(522.49) | Grad Norm 21.1834(10.0264) | Total Time 14.00(14.00)\n",
      "Iter 0273 | Time 44.8984(47.0799) | Bit/dim 5.4495(4.9885) | Xent 2.1386(1.9530) | Loss 6.5188(5.9649) | Error 0.7677(0.6804) Steps 520(522.42) | Grad Norm 12.4544(10.0993) | Total Time 14.00(14.00)\n",
      "Iter 0274 | Time 46.6320(47.0664) | Bit/dim 5.2744(4.9970) | Xent 2.0993(1.9573) | Loss 6.3241(5.9757) | Error 0.7516(0.6825) Steps 520(522.35) | Grad Norm 10.0811(10.0987) | Total Time 14.00(14.00)\n",
      "Iter 0275 | Time 48.4971(47.1094) | Bit/dim 5.2158(5.0036) | Xent 2.4170(1.9711) | Loss 6.4243(5.9892) | Error 0.7526(0.6846) Steps 544(523.00) | Grad Norm 28.2863(10.6443) | Total Time 14.00(14.00)\n",
      "Iter 0276 | Time 46.9773(47.1054) | Bit/dim 5.1148(5.0069) | Xent 2.2093(1.9783) | Loss 6.2195(5.9961) | Error 0.7848(0.6876) Steps 532(523.27) | Grad Norm 12.8324(10.7100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 23.9699, Epoch Time 320.8760(344.8879), Bit/dim 5.2145(best: 4.8098), Xent 2.1145, Loss 6.2718, Error 0.7777(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 45.5173(47.0578) | Bit/dim 5.2203(5.0133) | Xent 2.1548(1.9836) | Loss 6.2977(6.0051) | Error 0.7823(0.6904) Steps 532(523.53) | Grad Norm 9.8579(10.6844) | Total Time 14.00(14.00)\n",
      "Iter 0278 | Time 47.0696(47.0581) | Bit/dim 5.1932(5.0187) | Xent 2.1368(1.9882) | Loss 6.2616(6.0128) | Error 0.7704(0.6928) Steps 550(524.32) | Grad Norm 8.2518(10.6114) | Total Time 14.00(14.00)\n",
      "Iter 0279 | Time 47.0885(47.0590) | Bit/dim 5.0808(5.0206) | Xent 2.1663(1.9935) | Loss 6.1639(6.0174) | Error 0.7492(0.6945) Steps 544(524.91) | Grad Norm 6.8752(10.4993) | Total Time 14.00(14.00)\n",
      "Iter 0280 | Time 47.3302(47.0672) | Bit/dim 5.1128(5.0234) | Xent 2.1617(1.9986) | Loss 6.1936(6.0226) | Error 0.7625(0.6966) Steps 544(525.48) | Grad Norm 13.4663(10.5884) | Total Time 14.00(14.00)\n",
      "Iter 0281 | Time 45.5503(47.0217) | Bit/dim 5.1204(5.0263) | Xent 2.1872(2.0042) | Loss 6.2140(6.0284) | Error 0.7695(0.6988) Steps 544(526.04) | Grad Norm 8.6515(10.5303) | Total Time 14.00(14.00)\n",
      "Iter 0282 | Time 45.1118(46.9644) | Bit/dim 5.1280(5.0293) | Xent 2.1609(2.0089) | Loss 6.2085(6.0338) | Error 0.7630(0.7007) Steps 538(526.40) | Grad Norm 9.9504(10.5129) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 24.1031, Epoch Time 317.9067(344.0785), Bit/dim 5.0433(best: 4.8098), Xent 2.0852, Loss 6.0859, Error 0.7607(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 47.2450(46.9728) | Bit/dim 5.0430(5.0297) | Xent 2.1244(2.0124) | Loss 6.1052(6.0359) | Error 0.7648(0.7026) Steps 532(526.57) | Grad Norm 8.8583(10.4632) | Total Time 14.00(14.00)\n",
      "Iter 0284 | Time 47.2753(46.9819) | Bit/dim 5.0213(5.0295) | Xent 2.0789(2.0144) | Loss 6.0607(6.0367) | Error 0.7366(0.7036) Steps 526(526.55) | Grad Norm 6.3555(10.3400) | Total Time 14.00(14.00)\n",
      "Iter 0285 | Time 45.6558(46.9421) | Bit/dim 4.9732(5.0278) | Xent 2.0934(2.0168) | Loss 6.0199(6.0362) | Error 0.7444(0.7048) Steps 514(526.17) | Grad Norm 7.3178(10.2493) | Total Time 14.00(14.00)\n",
      "Iter 0286 | Time 45.9735(46.9130) | Bit/dim 4.9721(5.0261) | Xent 2.0339(2.0173) | Loss 5.9890(6.0348) | Error 0.7266(0.7055) Steps 508(525.63) | Grad Norm 5.0202(10.0924) | Total Time 14.00(14.00)\n",
      "Iter 0287 | Time 45.9649(46.8846) | Bit/dim 4.9659(5.0243) | Xent 2.0553(2.0184) | Loss 5.9935(6.0335) | Error 0.7281(0.7062) Steps 508(525.10) | Grad Norm 5.4978(9.9546) | Total Time 14.00(14.00)\n",
      "Iter 0288 | Time 45.5108(46.8434) | Bit/dim 4.9522(5.0222) | Xent 2.0228(2.0185) | Loss 5.9636(6.0314) | Error 0.7179(0.7065) Steps 496(524.23) | Grad Norm 7.4911(9.8807) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 23.5097, Epoch Time 317.0113(343.2665), Bit/dim 4.9219(best: 4.8098), Xent 2.0154, Loss 5.9296, Error 0.7240(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 45.0297(46.7889) | Bit/dim 4.9328(5.0195) | Xent 2.0381(2.0191) | Loss 5.9518(6.0290) | Error 0.7259(0.7071) Steps 502(523.56) | Grad Norm 9.9003(9.8813) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 43.8480(46.7007) | Bit/dim 4.9382(5.0170) | Xent 2.1224(2.0222) | Loss 5.9994(6.0281) | Error 0.7569(0.7086) Steps 490(522.55) | Grad Norm 13.8205(9.9995) | Total Time 14.00(14.00)\n",
      "Iter 0291 | Time 47.7189(46.7313) | Bit/dim 4.9595(5.0153) | Xent 2.1757(2.0268) | Loss 6.0473(6.0287) | Error 0.7631(0.7102) Steps 538(523.02) | Grad Norm 18.0329(10.2405) | Total Time 14.00(14.00)\n",
      "Iter 0292 | Time 43.1915(46.6251) | Bit/dim 4.9742(5.0141) | Xent 2.0977(2.0290) | Loss 6.0231(6.0286) | Error 0.7475(0.7114) Steps 484(521.85) | Grad Norm 10.9786(10.2626) | Total Time 14.00(14.00)\n",
      "Iter 0293 | Time 44.6614(46.5662) | Bit/dim 4.9082(5.0109) | Xent 1.9967(2.0280) | Loss 5.9066(6.0249) | Error 0.7116(0.7114) Steps 502(521.25) | Grad Norm 5.1959(10.1106) | Total Time 14.00(14.00)\n",
      "Iter 0294 | Time 47.5323(46.5951) | Bit/dim 4.8868(5.0072) | Xent 2.0279(2.0280) | Loss 5.9007(6.0212) | Error 0.7169(0.7115) Steps 520(521.21) | Grad Norm 6.4550(10.0009) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.5944, Epoch Time 311.5735(342.3157), Bit/dim 4.8792(best: 4.8098), Xent 2.0167, Loss 5.8875, Error 0.7187(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 45.4605(46.5611) | Bit/dim 4.8773(5.0033) | Xent 2.0608(2.0290) | Loss 5.9078(6.0178) | Error 0.7312(0.7121) Steps 514(521.00) | Grad Norm 8.4767(9.9552) | Total Time 14.00(14.00)\n",
      "Iter 0296 | Time 46.3946(46.5561) | Bit/dim 4.8744(4.9994) | Xent 2.0964(2.0310) | Loss 5.9226(6.0149) | Error 0.7435(0.7131) Steps 526(521.15) | Grad Norm 14.8501(10.1021) | Total Time 14.00(14.00)\n",
      "Iter 0297 | Time 49.6740(46.6496) | Bit/dim 4.9011(4.9965) | Xent 2.5510(2.0466) | Loss 6.1766(6.0198) | Error 0.8130(0.7161) Steps 544(521.83) | Grad Norm 29.3539(10.6796) | Total Time 14.00(14.00)\n",
      "Iter 0298 | Time 48.3046(46.6993) | Bit/dim 4.9255(4.9943) | Xent 2.6765(2.0655) | Loss 6.2637(6.0271) | Error 0.7964(0.7185) Steps 520(521.78) | Grad Norm 39.5368(11.5453) | Total Time 14.00(14.00)\n",
      "Iter 0299 | Time 46.3426(46.6886) | Bit/dim 5.0356(4.9956) | Xent 2.2328(2.0705) | Loss 6.1520(6.0308) | Error 0.7621(0.7198) Steps 520(521.72) | Grad Norm 15.2790(11.6573) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 45.5652(46.6549) | Bit/dim 5.0283(4.9966) | Xent 2.1931(2.0742) | Loss 6.1248(6.0336) | Error 0.7548(0.7208) Steps 538(522.21) | Grad Norm 10.3665(11.6186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 23.4893, Epoch Time 321.1837(341.6817), Bit/dim 4.9389(best: 4.8098), Xent 2.0349, Loss 5.9564, Error 0.7209(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 46.7546(46.6579) | Bit/dim 4.9328(4.9946) | Xent 2.0521(2.0735) | Loss 5.9589(6.0314) | Error 0.7365(0.7213) Steps 532(522.51) | Grad Norm 5.5219(11.4357) | Total Time 14.00(14.00)\n",
      "Iter 0302 | Time 47.5070(46.6834) | Bit/dim 4.9573(4.9935) | Xent 2.0627(2.0732) | Loss 5.9886(6.0301) | Error 0.7418(0.7219) Steps 538(522.97) | Grad Norm 7.9862(11.3322) | Total Time 14.00(14.00)\n",
      "Iter 0303 | Time 45.1142(46.6363) | Bit/dim 4.9345(4.9918) | Xent 2.0291(2.0719) | Loss 5.9491(6.0277) | Error 0.7283(0.7221) Steps 520(522.88) | Grad Norm 4.1418(11.1165) | Total Time 14.00(14.00)\n",
      "Iter 0304 | Time 43.7724(46.5504) | Bit/dim 4.9439(4.9903) | Xent 2.0585(2.0715) | Loss 5.9732(6.0261) | Error 0.7429(0.7227) Steps 508(522.44) | Grad Norm 4.6093(10.9213) | Total Time 14.00(14.00)\n",
      "Iter 0305 | Time 49.2660(46.6318) | Bit/dim 4.8682(4.9867) | Xent 2.0502(2.0708) | Loss 5.8932(6.0221) | Error 0.7445(0.7234) Steps 532(522.72) | Grad Norm 3.6478(10.7031) | Total Time 14.00(14.00)\n",
      "Iter 0306 | Time 46.6217(46.6315) | Bit/dim 4.8394(4.9822) | Xent 2.0248(2.0695) | Loss 5.8518(6.0170) | Error 0.7156(0.7231) Steps 526(522.82) | Grad Norm 3.1781(10.4773) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 24.2988, Epoch Time 319.4929(341.0161), Bit/dim 4.8333(best: 4.8098), Xent 2.0497, Loss 5.8582, Error 0.7392(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 46.7165(46.6341) | Bit/dim 4.8309(4.9777) | Xent 2.0625(2.0692) | Loss 5.8621(6.0123) | Error 0.7370(0.7236) Steps 532(523.10) | Grad Norm 4.9073(10.3102) | Total Time 14.00(14.00)\n",
      "Iter 0308 | Time 48.5196(46.6906) | Bit/dim 4.8272(4.9732) | Xent 1.9950(2.0670) | Loss 5.8247(6.0067) | Error 0.7011(0.7229) Steps 526(523.18) | Grad Norm 2.9121(10.0883) | Total Time 14.00(14.00)\n",
      "Iter 0309 | Time 49.3944(46.7718) | Bit/dim 4.8084(4.9682) | Xent 2.0507(2.0665) | Loss 5.8338(6.0015) | Error 0.7344(0.7232) Steps 514(522.91) | Grad Norm 6.1194(9.9692) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 47.5876(46.7962) | Bit/dim 4.7910(4.9629) | Xent 1.9989(2.0645) | Loss 5.7905(5.9952) | Error 0.7044(0.7227) Steps 508(522.46) | Grad Norm 2.3476(9.7406) | Total Time 14.00(14.00)\n",
      "Iter 0311 | Time 47.4746(46.8166) | Bit/dim 4.7932(4.9578) | Xent 2.0268(2.0634) | Loss 5.8067(5.9895) | Error 0.7249(0.7227) Steps 514(522.21) | Grad Norm 7.8038(9.6825) | Total Time 14.00(14.00)\n",
      "Iter 0312 | Time 47.9067(46.8493) | Bit/dim 4.8003(4.9531) | Xent 2.0172(2.0620) | Loss 5.8089(5.9841) | Error 0.7230(0.7227) Steps 508(521.78) | Grad Norm 6.8838(9.5985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 23.2481, Epoch Time 326.8464(340.5910), Bit/dim 4.7740(best: 4.8098), Xent 1.9384, Loss 5.7432, Error 0.6799(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 49.4162(46.9263) | Bit/dim 4.7806(4.9479) | Xent 1.9731(2.0593) | Loss 5.7672(5.9776) | Error 0.6957(0.7219) Steps 514(521.55) | Grad Norm 4.0731(9.4328) | Total Time 14.00(14.00)\n",
      "Iter 0314 | Time 48.2445(46.9658) | Bit/dim 4.7740(4.9427) | Xent 2.0265(2.0583) | Loss 5.7872(5.9719) | Error 0.7077(0.7215) Steps 520(521.50) | Grad Norm 9.2377(9.4269) | Total Time 14.00(14.00)\n",
      "Iter 0315 | Time 46.4609(46.9507) | Bit/dim 4.7954(4.9383) | Xent 1.9566(2.0553) | Loss 5.7737(5.9659) | Error 0.6901(0.7206) Steps 514(521.28) | Grad Norm 9.5519(9.4307) | Total Time 14.00(14.00)\n",
      "Iter 0316 | Time 49.2932(47.0210) | Bit/dim 4.7697(4.9332) | Xent 1.9474(2.0520) | Loss 5.7434(5.9593) | Error 0.6861(0.7195) Steps 526(521.42) | Grad Norm 4.2977(9.2767) | Total Time 14.00(14.00)\n",
      "Iter 0317 | Time 47.2547(47.0280) | Bit/dim 4.7361(4.9273) | Xent 1.9363(2.0486) | Loss 5.7042(5.9516) | Error 0.6755(0.7182) Steps 508(521.01) | Grad Norm 4.3083(9.1276) | Total Time 14.00(14.00)\n",
      "Iter 0318 | Time 47.1857(47.0327) | Bit/dim 4.7263(4.9213) | Xent 1.9244(2.0448) | Loss 5.6885(5.9437) | Error 0.6766(0.7170) Steps 496(520.26) | Grad Norm 3.5053(8.9590) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 23.1666, Epoch Time 326.8948(340.1801), Bit/dim 4.7315(best: 4.7740), Xent 1.8842, Loss 5.6736, Error 0.6515(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 46.9260(47.0295) | Bit/dim 4.7192(4.9152) | Xent 1.9246(2.0412) | Loss 5.6815(5.9358) | Error 0.6629(0.7153) Steps 502(519.72) | Grad Norm 4.2254(8.8169) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 46.0818(47.0011) | Bit/dim 4.7207(4.9094) | Xent 1.9076(2.0372) | Loss 5.6745(5.9280) | Error 0.6629(0.7138) Steps 496(519.00) | Grad Norm 3.5880(8.6601) | Total Time 14.00(14.00)\n",
      "Iter 0321 | Time 47.0483(47.0025) | Bit/dim 4.7117(4.9035) | Xent 1.9208(2.0337) | Loss 5.6722(5.9203) | Error 0.6758(0.7126) Steps 502(518.49) | Grad Norm 5.0907(8.5530) | Total Time 14.00(14.00)\n",
      "Iter 0322 | Time 47.9614(47.0313) | Bit/dim 4.7038(4.8975) | Xent 1.9351(2.0308) | Loss 5.6713(5.9129) | Error 0.6841(0.7118) Steps 508(518.18) | Grad Norm 6.6119(8.4948) | Total Time 14.00(14.00)\n",
      "Iter 0323 | Time 47.3363(47.0404) | Bit/dim 4.7027(4.8916) | Xent 1.9096(2.0271) | Loss 5.6574(5.9052) | Error 0.6690(0.7105) Steps 502(517.69) | Grad Norm 6.6372(8.4390) | Total Time 14.00(14.00)\n",
      "Iter 0324 | Time 46.7830(47.0327) | Bit/dim 4.7646(4.8878) | Xent 1.9106(2.0236) | Loss 5.7199(5.8996) | Error 0.6700(0.7093) Steps 508(517.40) | Grad Norm 9.9912(8.4856) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 22.9115, Epoch Time 321.0759(339.6070), Bit/dim 4.7359(best: 4.7315), Xent 1.9740, Loss 5.7228, Error 0.7057(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 45.9971(47.0016) | Bit/dim 4.7420(4.8834) | Xent 1.9996(2.0229) | Loss 5.7418(5.8949) | Error 0.7027(0.7091) Steps 496(516.76) | Grad Norm 14.9166(8.6785) | Total Time 14.00(14.00)\n",
      "Iter 0326 | Time 46.2991(46.9805) | Bit/dim 4.8069(4.8811) | Xent 2.1377(2.0264) | Loss 5.8758(5.8943) | Error 0.7430(0.7101) Steps 484(515.78) | Grad Norm 17.6806(8.9486) | Total Time 14.00(14.00)\n",
      "Iter 0327 | Time 46.2244(46.9579) | Bit/dim 4.7078(4.8759) | Xent 1.9046(2.0227) | Loss 5.6601(5.8873) | Error 0.6791(0.7092) Steps 502(515.37) | Grad Norm 4.1293(8.8040) | Total Time 14.00(14.00)\n",
      "Iter 0328 | Time 46.5193(46.9447) | Bit/dim 4.7548(4.8723) | Xent 2.0317(2.0230) | Loss 5.7707(5.8838) | Error 0.7115(0.7092) Steps 502(514.96) | Grad Norm 13.5736(8.9471) | Total Time 14.00(14.00)\n",
      "Iter 0329 | Time 45.2730(46.8946) | Bit/dim 4.7331(4.8681) | Xent 1.9466(2.0207) | Loss 5.7064(5.8785) | Error 0.6932(0.7088) Steps 502(514.58) | Grad Norm 6.5206(8.8743) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 48.0859(46.9303) | Bit/dim 4.7163(4.8636) | Xent 1.9970(2.0200) | Loss 5.7148(5.8736) | Error 0.7037(0.7086) Steps 514(514.56) | Grad Norm 7.0276(8.8189) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 23.2048, Epoch Time 317.5655(338.9457), Bit/dim 4.7216(best: 4.7315), Xent 1.9320, Loss 5.6876, Error 0.6767(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 46.2480(46.9098) | Bit/dim 4.7239(4.8594) | Xent 1.9706(2.0185) | Loss 5.7092(5.8686) | Error 0.6961(0.7082) Steps 490(513.82) | Grad Norm 7.4861(8.7789) | Total Time 14.00(14.00)\n",
      "Iter 0332 | Time 46.7717(46.9057) | Bit/dim 4.7533(4.8562) | Xent 1.9335(2.0159) | Loss 5.7201(5.8642) | Error 0.6826(0.7075) Steps 478(512.75) | Grad Norm 5.7716(8.6887) | Total Time 14.00(14.00)\n",
      "Iter 0333 | Time 46.3328(46.8885) | Bit/dim 4.7246(4.8523) | Xent 1.9999(2.0155) | Loss 5.7245(5.8600) | Error 0.7221(0.7079) Steps 502(512.42) | Grad Norm 7.2508(8.6456) | Total Time 14.00(14.00)\n",
      "Iter 0334 | Time 46.5871(46.8795) | Bit/dim 4.6801(4.8471) | Xent 1.9411(2.0132) | Loss 5.6506(5.8537) | Error 0.6767(0.7070) Steps 496(511.93) | Grad Norm 5.2085(8.5425) | Total Time 14.00(14.00)\n",
      "Iter 0335 | Time 46.8266(46.8779) | Bit/dim 4.6791(4.8421) | Xent 1.9382(2.0110) | Loss 5.6481(5.8475) | Error 0.6856(0.7063) Steps 496(511.45) | Grad Norm 4.2137(8.4126) | Total Time 14.00(14.00)\n",
      "Iter 0336 | Time 46.0738(46.8537) | Bit/dim 4.7050(4.8379) | Xent 1.9089(2.0079) | Loss 5.6595(5.8419) | Error 0.6761(0.7054) Steps 502(511.17) | Grad Norm 4.6123(8.2986) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 22.7485, Epoch Time 317.4671(338.3014), Bit/dim 4.6674(best: 4.7216), Xent 1.8804, Loss 5.6076, Error 0.6593(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 46.2632(46.8360) | Bit/dim 4.6625(4.8327) | Xent 1.8921(2.0044) | Loss 5.6085(5.8349) | Error 0.6705(0.7044) Steps 496(510.71) | Grad Norm 3.1400(8.1438) | Total Time 14.00(14.00)\n",
      "Iter 0338 | Time 46.9631(46.8398) | Bit/dim 4.6666(4.8277) | Xent 1.8819(2.0008) | Loss 5.6075(5.8281) | Error 0.6647(0.7032) Steps 508(510.63) | Grad Norm 3.7793(8.0129) | Total Time 14.00(14.00)\n",
      "Iter 0339 | Time 46.6803(46.8351) | Bit/dim 4.6813(4.8233) | Xent 1.8798(1.9971) | Loss 5.6212(5.8219) | Error 0.6615(0.7019) Steps 502(510.37) | Grad Norm 4.1502(7.8970) | Total Time 14.00(14.00)\n",
      "Iter 0340 | Time 47.1149(46.8435) | Bit/dim 4.6505(4.8181) | Xent 1.8997(1.9942) | Loss 5.6003(5.8152) | Error 0.6707(0.7010) Steps 496(509.94) | Grad Norm 5.5716(7.8272) | Total Time 14.00(14.00)\n",
      "Iter 0341 | Time 46.6762(46.8384) | Bit/dim 4.6362(4.8127) | Xent 1.8732(1.9906) | Loss 5.5729(5.8080) | Error 0.6603(0.6998) Steps 496(509.52) | Grad Norm 2.0678(7.6545) | Total Time 14.00(14.00)\n",
      "Iter 0342 | Time 46.0813(46.8157) | Bit/dim 4.6454(4.8076) | Xent 1.8900(1.9876) | Loss 5.5903(5.8014) | Error 0.6647(0.6987) Steps 496(509.12) | Grad Norm 5.6970(7.5957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 23.7718, Epoch Time 319.4223(337.7350), Bit/dim 4.6250(best: 4.6674), Xent 1.8491, Loss 5.5495, Error 0.6412(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 47.6466(46.8406) | Bit/dim 4.6251(4.8022) | Xent 1.8495(1.9834) | Loss 5.5499(5.7939) | Error 0.6516(0.6973) Steps 508(509.09) | Grad Norm 3.6782(7.4782) | Total Time 14.00(14.00)\n",
      "Iter 0344 | Time 48.3478(46.8859) | Bit/dim 4.6126(4.7965) | Xent 1.8896(1.9806) | Loss 5.5574(5.7868) | Error 0.6685(0.6964) Steps 508(509.05) | Grad Norm 3.7505(7.3664) | Total Time 14.00(14.00)\n",
      "Iter 0345 | Time 51.3354(47.0193) | Bit/dim 4.6630(4.7925) | Xent 1.8578(1.9769) | Loss 5.5919(5.7809) | Error 0.6558(0.6952) Steps 544(510.10) | Grad Norm 3.9143(7.2628) | Total Time 14.00(14.00)\n",
      "Iter 0346 | Time 48.9512(47.0773) | Bit/dim 4.6288(4.7876) | Xent 1.8642(1.9735) | Loss 5.5609(5.7743) | Error 0.6476(0.6938) Steps 520(510.40) | Grad Norm 6.0546(7.2266) | Total Time 14.00(14.00)\n",
      "Iter 0347 | Time 50.0289(47.1659) | Bit/dim 4.6214(4.7826) | Xent 1.8893(1.9710) | Loss 5.5661(5.7681) | Error 0.6664(0.6930) Steps 538(511.23) | Grad Norm 7.5879(7.2374) | Total Time 14.00(14.00)\n",
      "Iter 0348 | Time 46.9655(47.1598) | Bit/dim 4.6063(4.7773) | Xent 1.8847(1.9684) | Loss 5.5487(5.7615) | Error 0.6655(0.6922) Steps 520(511.49) | Grad Norm 5.1586(7.1750) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.9672, Epoch Time 333.0727(337.5951), Bit/dim 4.6146(best: 4.6250), Xent 1.8313, Loss 5.5303, Error 0.6322(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 49.9872(47.2447) | Bit/dim 4.6045(4.7721) | Xent 1.8529(1.9650) | Loss 5.5309(5.7546) | Error 0.6498(0.6909) Steps 544(512.47) | Grad Norm 3.1747(7.0550) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 50.0644(47.3293) | Bit/dim 4.6044(4.7671) | Xent 1.8488(1.9615) | Loss 5.5288(5.7478) | Error 0.6486(0.6896) Steps 550(513.59) | Grad Norm 4.1258(6.9672) | Total Time 14.00(14.00)\n",
      "Iter 0351 | Time 49.3789(47.3907) | Bit/dim 4.6136(4.7625) | Xent 1.8445(1.9580) | Loss 5.5358(5.7415) | Error 0.6498(0.6884) Steps 550(514.68) | Grad Norm 4.7628(6.9010) | Total Time 14.00(14.00)\n",
      "Iter 0352 | Time 48.6840(47.4295) | Bit/dim 4.5994(4.7576) | Xent 1.8678(1.9553) | Loss 5.5333(5.7352) | Error 0.6583(0.6875) Steps 526(515.02) | Grad Norm 7.9002(6.9310) | Total Time 14.00(14.00)\n",
      "Iter 0353 | Time 51.7670(47.5597) | Bit/dim 4.6483(4.7543) | Xent 1.8599(1.9524) | Loss 5.5782(5.7305) | Error 0.6482(0.6863) Steps 550(516.07) | Grad Norm 9.6182(7.0116) | Total Time 14.00(14.00)\n",
      "Iter 0354 | Time 48.9490(47.6013) | Bit/dim 4.6413(4.7509) | Xent 1.8429(1.9491) | Loss 5.5628(5.7255) | Error 0.6446(0.6851) Steps 514(516.01) | Grad Norm 11.0901(7.1340) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 24.2644, Epoch Time 339.1956(337.6431), Bit/dim 4.7002(best: 4.6146), Xent 1.8243, Loss 5.6123, Error 0.6255(best: 0.6264)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 50.0719(47.6755) | Bit/dim 4.7028(4.7495) | Xent 1.8394(1.9458) | Loss 5.6225(5.7224) | Error 0.6441(0.6839) Steps 550(517.03) | Grad Norm 9.9529(7.2185) | Total Time 14.00(14.00)\n",
      "Iter 0356 | Time 47.4493(47.6687) | Bit/dim 4.6396(4.7462) | Xent 1.8404(1.9427) | Loss 5.5598(5.7175) | Error 0.6412(0.6826) Steps 520(517.12) | Grad Norm 7.7495(7.2345) | Total Time 14.00(14.00)\n",
      "Iter 0357 | Time 48.8529(47.7042) | Bit/dim 4.5986(4.7417) | Xent 1.8200(1.9390) | Loss 5.5086(5.7112) | Error 0.6412(0.6813) Steps 520(517.21) | Grad Norm 4.0299(7.1383) | Total Time 14.00(14.00)\n",
      "Iter 0358 | Time 49.4118(47.7554) | Bit/dim 4.6087(4.7378) | Xent 1.8616(1.9367) | Loss 5.5396(5.7061) | Error 0.6567(0.6806) Steps 550(518.19) | Grad Norm 6.9969(7.1341) | Total Time 14.00(14.00)\n",
      "Iter 0359 | Time 47.1321(47.7367) | Bit/dim 4.5737(4.7328) | Xent 1.8218(1.9332) | Loss 5.4846(5.6994) | Error 0.6459(0.6796) Steps 520(518.24) | Grad Norm 3.7510(7.0326) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 48.2565(47.7523) | Bit/dim 4.5877(4.7285) | Xent 1.8395(1.9304) | Loss 5.5075(5.6937) | Error 0.6567(0.6789) Steps 520(518.30) | Grad Norm 7.3703(7.0427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 24.2639, Epoch Time 331.4047(337.4560), Bit/dim 4.6265(best: 4.6146), Xent 1.8309, Loss 5.5420, Error 0.6446(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 50.1628(47.8246) | Bit/dim 4.6239(4.7253) | Xent 1.8456(1.9279) | Loss 5.5467(5.6893) | Error 0.6545(0.6781) Steps 556(519.43) | Grad Norm 10.3784(7.1428) | Total Time 14.00(14.00)\n",
      "Iter 0362 | Time 46.8460(47.7953) | Bit/dim 4.6928(4.7244) | Xent 1.8736(1.9262) | Loss 5.6296(5.6875) | Error 0.6549(0.6774) Steps 514(519.26) | Grad Norm 14.9123(7.3759) | Total Time 14.00(14.00)\n",
      "Iter 0363 | Time 45.5939(47.7292) | Bit/dim 4.7857(4.7262) | Xent 1.9130(1.9258) | Loss 5.7422(5.6891) | Error 0.6835(0.6776) Steps 532(519.65) | Grad Norm 12.7356(7.5367) | Total Time 14.00(14.00)\n",
      "Iter 0364 | Time 50.3616(47.8082) | Bit/dim 4.6310(4.7233) | Xent 1.8894(1.9247) | Loss 5.5757(5.6857) | Error 0.6663(0.6773) Steps 550(520.56) | Grad Norm 9.0106(7.5809) | Total Time 14.00(14.00)\n",
      "Iter 0365 | Time 48.8057(47.8381) | Bit/dim 4.9728(4.7308) | Xent 1.9570(1.9257) | Loss 5.9514(5.6937) | Error 0.6683(0.6770) Steps 520(520.54) | Grad Norm 23.4068(8.0557) | Total Time 14.00(14.00)\n",
      "Iter 0366 | Time 44.1611(47.7278) | Bit/dim 5.3769(4.7502) | Xent 1.8677(1.9240) | Loss 6.3107(5.7122) | Error 0.6546(0.6763) Steps 526(520.70) | Grad Norm 7.8297(8.0489) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 23.0624, Epoch Time 325.0040(337.0824), Bit/dim 5.8090(best: 4.6146), Xent 1.8765, Loss 6.7473, Error 0.6518(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 45.4653(47.6599) | Bit/dim 5.8094(4.7820) | Xent 1.8790(1.9226) | Loss 6.7489(5.7433) | Error 0.6510(0.6756) Steps 532(521.04) | Grad Norm 4.8220(7.9521) | Total Time 14.00(14.00)\n",
      "Iter 0368 | Time 45.9864(47.6097) | Bit/dim 5.8789(4.8149) | Xent 1.9090(1.9222) | Loss 6.8334(5.7760) | Error 0.6760(0.6756) Steps 532(521.37) | Grad Norm 4.7937(7.8573) | Total Time 14.00(14.00)\n",
      "Iter 0369 | Time 43.4175(47.4840) | Bit/dim 5.7852(4.8440) | Xent 1.8720(1.9207) | Loss 6.7213(5.8044) | Error 0.6547(0.6750) Steps 502(520.79) | Grad Norm 3.7555(7.7343) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 42.6784(47.3398) | Bit/dim 5.5747(4.8659) | Xent 1.8506(1.9186) | Loss 6.5000(5.8252) | Error 0.6445(0.6741) Steps 490(519.87) | Grad Norm 3.8149(7.6167) | Total Time 14.00(14.00)\n",
      "Iter 0371 | Time 42.6178(47.1981) | Bit/dim 5.3358(4.8800) | Xent 1.8947(1.9179) | Loss 6.2832(5.8390) | Error 0.6739(0.6740) Steps 478(518.61) | Grad Norm 3.7596(7.5010) | Total Time 14.00(14.00)\n",
      "Iter 0372 | Time 45.5212(47.1478) | Bit/dim 5.1752(4.8889) | Xent 1.9207(1.9180) | Loss 6.1355(5.8479) | Error 0.6929(0.6746) Steps 490(517.75) | Grad Norm 10.4864(7.5906) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.9733, Epoch Time 305.5297(336.1358), Bit/dim 5.0780(best: 4.6146), Xent 1.9918, Loss 6.0739, Error 0.7135(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 45.5531(47.1000) | Bit/dim 5.0828(4.8947) | Xent 2.0334(1.9214) | Loss 6.0995(5.8554) | Error 0.7212(0.6760) Steps 508(517.46) | Grad Norm 22.3905(8.0346) | Total Time 14.00(14.00)\n",
      "Iter 0374 | Time 46.5834(47.0845) | Bit/dim 4.9967(4.8978) | Xent 2.1100(1.9271) | Loss 6.0517(5.8613) | Error 0.7661(0.6787) Steps 526(517.72) | Grad Norm 18.0312(8.3345) | Total Time 14.00(14.00)\n",
      "Iter 0375 | Time 43.4953(46.9768) | Bit/dim 4.9863(4.9004) | Xent 1.9178(1.9268) | Loss 5.9452(5.8638) | Error 0.6753(0.6786) Steps 508(517.42) | Grad Norm 5.8458(8.2598) | Total Time 14.00(14.00)\n",
      "Iter 0376 | Time 44.7240(46.9092) | Bit/dim 5.0789(4.9058) | Xent 2.0867(1.9316) | Loss 6.1223(5.8716) | Error 0.7682(0.6813) Steps 508(517.14) | Grad Norm 11.7108(8.3633) | Total Time 14.00(14.00)\n",
      "Iter 0377 | Time 47.6205(46.9306) | Bit/dim 5.0616(4.9104) | Xent 1.9618(1.9325) | Loss 6.0425(5.8767) | Error 0.6976(0.6818) Steps 526(517.41) | Grad Norm 10.1369(8.4165) | Total Time 14.00(14.00)\n",
      "Iter 0378 | Time 48.7192(46.9842) | Bit/dim 4.9827(4.9126) | Xent 1.9533(1.9331) | Loss 5.9594(5.8792) | Error 0.6897(0.6820) Steps 562(518.75) | Grad Norm 8.1203(8.4076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 24.4624, Epoch Time 316.6297(335.5507), Bit/dim 4.9294(best: 4.6146), Xent 1.9043, Loss 5.8815, Error 0.6720(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 47.7536(47.0073) | Bit/dim 4.9299(4.9131) | Xent 1.9138(1.9326) | Loss 5.8868(5.8794) | Error 0.6754(0.6818) Steps 556(519.86) | Grad Norm 5.7929(8.3292) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 49.2366(47.0742) | Bit/dim 4.8892(4.9124) | Xent 1.9009(1.9316) | Loss 5.8397(5.8782) | Error 0.6681(0.6814) Steps 544(520.59) | Grad Norm 4.7711(8.2225) | Total Time 14.00(14.00)\n",
      "Iter 0381 | Time 48.1926(47.1077) | Bit/dim 4.8754(4.9113) | Xent 1.9217(1.9313) | Loss 5.8363(5.8770) | Error 0.6813(0.6814) Steps 550(521.47) | Grad Norm 9.5743(8.2630) | Total Time 14.00(14.00)\n",
      "Iter 0382 | Time 48.5597(47.1513) | Bit/dim 4.9075(4.9112) | Xent 1.9439(1.9317) | Loss 5.8795(5.8770) | Error 0.6924(0.6817) Steps 550(522.33) | Grad Norm 14.7013(8.4562) | Total Time 14.00(14.00)\n",
      "Iter 0383 | Time 49.8297(47.2317) | Bit/dim 4.8886(4.9105) | Xent 2.0727(1.9359) | Loss 5.9250(5.8785) | Error 0.7328(0.6833) Steps 550(523.16) | Grad Norm 19.7529(8.7951) | Total Time 14.00(14.00)\n",
      "Iter 0384 | Time 48.6134(47.2731) | Bit/dim 4.9221(4.9109) | Xent 2.0158(1.9383) | Loss 5.9300(5.8800) | Error 0.7278(0.6846) Steps 550(523.96) | Grad Norm 19.3033(9.1103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 24.1507, Epoch Time 332.4980(335.4591), Bit/dim 4.8183(best: 4.6146), Xent 1.8768, Loss 5.7567, Error 0.6568(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 49.9187(47.3525) | Bit/dim 4.8272(4.9083) | Xent 1.9087(1.9374) | Loss 5.7815(5.8771) | Error 0.6800(0.6845) Steps 532(524.20) | Grad Norm 5.1817(8.9924) | Total Time 14.00(14.00)\n",
      "Iter 0386 | Time 49.8992(47.4289) | Bit/dim 4.7981(4.9050) | Xent 1.9210(1.9369) | Loss 5.7586(5.8735) | Error 0.6713(0.6841) Steps 532(524.44) | Grad Norm 6.4556(8.9163) | Total Time 14.00(14.00)\n",
      "Iter 0387 | Time 49.6621(47.4959) | Bit/dim 4.7821(4.9014) | Xent 1.8934(1.9356) | Loss 5.7288(5.8692) | Error 0.6564(0.6832) Steps 532(524.66) | Grad Norm 4.1715(8.7740) | Total Time 14.00(14.00)\n",
      "Iter 0388 | Time 47.5995(47.4990) | Bit/dim 4.7672(4.8973) | Xent 1.9159(1.9350) | Loss 5.7252(5.8648) | Error 0.6713(0.6829) Steps 520(524.52) | Grad Norm 3.4651(8.6147) | Total Time 14.00(14.00)\n",
      "Iter 0389 | Time 47.3718(47.4952) | Bit/dim 4.7556(4.8931) | Xent 1.9128(1.9344) | Loss 5.7120(5.8603) | Error 0.6700(0.6825) Steps 514(524.21) | Grad Norm 3.4139(8.4587) | Total Time 14.00(14.00)\n",
      "Iter 0390 | Time 47.5374(47.4964) | Bit/dim 4.7360(4.8884) | Xent 1.9126(1.9337) | Loss 5.6923(5.8552) | Error 0.6795(0.6824) Steps 514(523.90) | Grad Norm 3.8372(8.3201) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 23.8764, Epoch Time 331.5210(335.3409), Bit/dim 4.7198(best: 4.6146), Xent 1.8733, Loss 5.6564, Error 0.6565(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 46.7281(47.4734) | Bit/dim 4.7108(4.8830) | Xent 1.8993(1.9327) | Loss 5.6605(5.8494) | Error 0.6761(0.6822) Steps 496(523.06) | Grad Norm 2.6746(8.1507) | Total Time 14.00(14.00)\n",
      "Iter 0392 | Time 47.5659(47.4762) | Bit/dim 4.7134(4.8779) | Xent 1.8870(1.9313) | Loss 5.6569(5.8436) | Error 0.6647(0.6817) Steps 502(522.43) | Grad Norm 3.4916(8.0109) | Total Time 14.00(14.00)\n",
      "Iter 0393 | Time 46.4341(47.4449) | Bit/dim 4.7015(4.8727) | Xent 1.8779(1.9297) | Loss 5.6404(5.8375) | Error 0.6655(0.6812) Steps 496(521.64) | Grad Norm 3.3889(7.8723) | Total Time 14.00(14.00)\n",
      "Iter 0394 | Time 47.3503(47.4421) | Bit/dim 4.6880(4.8671) | Xent 1.8700(1.9279) | Loss 5.6230(5.8311) | Error 0.6616(0.6806) Steps 496(520.87) | Grad Norm 2.6408(7.7153) | Total Time 14.00(14.00)\n",
      "Iter 0395 | Time 47.6783(47.4491) | Bit/dim 4.6790(4.8615) | Xent 1.8691(1.9262) | Loss 5.6135(5.8245) | Error 0.6555(0.6799) Steps 508(520.48) | Grad Norm 2.4897(7.5586) | Total Time 14.00(14.00)\n",
      "Iter 0396 | Time 49.7948(47.5195) | Bit/dim 4.6470(4.8550) | Xent 1.8703(1.9245) | Loss 5.5822(5.8173) | Error 0.6614(0.6793) Steps 502(519.93) | Grad Norm 2.4965(7.4067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 23.8335, Epoch Time 325.6075(335.0489), Bit/dim 4.6490(best: 4.6146), Xent 1.8236, Loss 5.5609, Error 0.6381(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 47.6903(47.5246) | Bit/dim 4.6530(4.8490) | Xent 1.8455(1.9221) | Loss 5.5758(5.8100) | Error 0.6524(0.6785) Steps 508(519.57) | Grad Norm 3.0420(7.2758) | Total Time 14.00(14.00)\n",
      "Iter 0398 | Time 46.5786(47.4963) | Bit/dim 4.6451(4.8429) | Xent 1.8378(1.9196) | Loss 5.5639(5.8027) | Error 0.6475(0.6776) Steps 502(519.04) | Grad Norm 2.3590(7.1282) | Total Time 14.00(14.00)\n",
      "Iter 0399 | Time 48.3437(47.5217) | Bit/dim 4.6265(4.8364) | Xent 1.8327(1.9170) | Loss 5.5429(5.7949) | Error 0.6464(0.6766) Steps 514(518.89) | Grad Norm 2.0756(6.9767) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 50.0604(47.5978) | Bit/dim 4.6211(4.8299) | Xent 1.8286(1.9143) | Loss 5.5354(5.7871) | Error 0.6435(0.6756) Steps 514(518.75) | Grad Norm 2.9029(6.8545) | Total Time 14.00(14.00)\n",
      "Iter 0401 | Time 49.2142(47.6463) | Bit/dim 4.6129(4.8234) | Xent 1.8083(1.9111) | Loss 5.5170(5.7790) | Error 0.6345(0.6744) Steps 538(519.32) | Grad Norm 3.6854(6.7594) | Total Time 14.00(14.00)\n",
      "Iter 0402 | Time 47.2426(47.6342) | Bit/dim 4.6059(4.8169) | Xent 1.8332(1.9088) | Loss 5.5225(5.7713) | Error 0.6421(0.6734) Steps 508(518.98) | Grad Norm 5.3762(6.7179) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 23.8175, Epoch Time 328.9228(334.8651), Bit/dim 4.6106(best: 4.6146), Xent 1.7747, Loss 5.4979, Error 0.6113(best: 0.6255)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 51.1981(47.7411) | Bit/dim 4.6052(4.8105) | Xent 1.7868(1.9052) | Loss 5.4986(5.7631) | Error 0.6306(0.6722) Steps 556(520.09) | Grad Norm 6.7688(6.7194) | Total Time 14.00(14.00)\n",
      "Iter 0404 | Time 50.1560(47.8136) | Bit/dim 4.6263(4.8050) | Xent 1.8202(1.9026) | Loss 5.5364(5.7563) | Error 0.6350(0.6710) Steps 526(520.27) | Grad Norm 8.7192(6.7794) | Total Time 14.00(14.00)\n",
      "Iter 0405 | Time 51.0996(47.9122) | Bit/dim 4.6424(4.8001) | Xent 1.8287(1.9004) | Loss 5.5567(5.7503) | Error 0.6424(0.6702) Steps 556(521.34) | Grad Norm 10.4501(6.8895) | Total Time 14.00(14.00)\n",
      "Iter 0406 | Time 49.1242(47.9485) | Bit/dim 4.6126(4.7945) | Xent 1.8580(1.8991) | Loss 5.5416(5.7440) | Error 0.6615(0.6699) Steps 532(521.66) | Grad Norm 11.1445(7.0172) | Total Time 14.00(14.00)\n",
      "Iter 0407 | Time 50.3553(48.0207) | Bit/dim 4.5728(4.7878) | Xent 1.8587(1.8979) | Loss 5.5022(5.7368) | Error 0.6603(0.6696) Steps 556(522.69) | Grad Norm 7.4897(7.0314) | Total Time 14.00(14.00)\n",
      "Iter 0408 | Time 49.8667(48.0761) | Bit/dim 4.5675(4.7812) | Xent 1.8222(1.8956) | Loss 5.4785(5.7290) | Error 0.6499(0.6690) Steps 550(523.51) | Grad Norm 5.8443(6.9957) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.7850, Epoch Time 341.6999(335.0702), Bit/dim 4.5756(best: 4.6106), Xent 1.8237, Loss 5.4875, Error 0.6544(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 49.0868(48.1064) | Bit/dim 4.5673(4.7748) | Xent 1.8534(1.8944) | Loss 5.4940(5.7220) | Error 0.6603(0.6688) Steps 532(523.77) | Grad Norm 9.0220(7.0565) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 51.8619(48.2191) | Bit/dim 4.5740(4.7688) | Xent 1.7945(1.8914) | Loss 5.4712(5.7145) | Error 0.6370(0.6678) Steps 562(524.91) | Grad Norm 7.0099(7.0551) | Total Time 14.00(14.00)\n",
      "Iter 0411 | Time 51.1657(48.3075) | Bit/dim 4.5859(4.7633) | Xent 1.8096(1.8889) | Loss 5.4907(5.7078) | Error 0.6351(0.6668) Steps 562(526.03) | Grad Norm 9.8880(7.1401) | Total Time 14.00(14.00)\n",
      "Iter 0412 | Time 49.2153(48.3347) | Bit/dim 4.6400(4.7596) | Xent 1.8490(1.8877) | Loss 5.5645(5.7035) | Error 0.6542(0.6665) Steps 532(526.21) | Grad Norm 13.6335(7.3349) | Total Time 14.00(14.00)\n",
      "Iter 0413 | Time 50.8457(48.4101) | Bit/dim 4.5965(4.7547) | Xent 1.8649(1.8870) | Loss 5.5289(5.6982) | Error 0.6601(0.6663) Steps 556(527.10) | Grad Norm 11.8293(7.4697) | Total Time 14.00(14.00)\n",
      "Iter 0414 | Time 50.8541(48.4834) | Bit/dim 4.5463(4.7485) | Xent 1.7764(1.8837) | Loss 5.4345(5.6903) | Error 0.6265(0.6651) Steps 538(527.43) | Grad Norm 3.6551(7.3553) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 24.0937, Epoch Time 343.1349(335.3121), Bit/dim 4.5507(best: 4.5756), Xent 1.8071, Loss 5.4542, Error 0.6394(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 47.4203(48.4515) | Bit/dim 4.5528(4.7426) | Xent 1.8255(1.8820) | Loss 5.4656(5.6836) | Error 0.6485(0.6646) Steps 520(527.20) | Grad Norm 6.1758(7.3199) | Total Time 14.00(14.00)\n",
      "Iter 0416 | Time 49.1150(48.4714) | Bit/dim 4.5485(4.7368) | Xent 1.8046(1.8796) | Loss 5.4508(5.6766) | Error 0.6441(0.6640) Steps 544(527.71) | Grad Norm 4.8448(7.2457) | Total Time 14.00(14.00)\n",
      "Iter 0417 | Time 48.3728(48.4684) | Bit/dim 4.5420(4.7309) | Xent 1.8560(1.8789) | Loss 5.4700(5.6704) | Error 0.6615(0.6639) Steps 526(527.66) | Grad Norm 9.7797(7.3217) | Total Time 14.00(14.00)\n",
      "Iter 0418 | Time 49.3664(48.4954) | Bit/dim 4.6342(4.7280) | Xent 1.8493(1.8780) | Loss 5.5588(5.6670) | Error 0.6565(0.6637) Steps 550(528.33) | Grad Norm 14.6567(7.5417) | Total Time 14.00(14.00)\n",
      "Iter 0419 | Time 48.7225(48.5022) | Bit/dim 4.6537(4.7258) | Xent 1.9153(1.8792) | Loss 5.6114(5.6654) | Error 0.6776(0.6641) Steps 532(528.44) | Grad Norm 16.8094(7.8198) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 48.4525(48.5007) | Bit/dim 4.6008(4.7220) | Xent 1.8845(1.8793) | Loss 5.5431(5.6617) | Error 0.6714(0.6643) Steps 526(528.36) | Grad Norm 10.2646(7.8931) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.9244, Epoch Time 331.3331(335.1928), Bit/dim 4.5537(best: 4.5507), Xent 1.7841, Loss 5.4457, Error 0.6214(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 47.4183(48.4682) | Bit/dim 4.5578(4.7171) | Xent 1.8152(1.8774) | Loss 5.4654(5.6558) | Error 0.6426(0.6637) Steps 508(527.75) | Grad Norm 5.0792(7.8087) | Total Time 14.00(14.00)\n",
      "Iter 0422 | Time 46.8539(48.4198) | Bit/dim 4.5710(4.7127) | Xent 1.8836(1.8776) | Loss 5.5128(5.6515) | Error 0.6678(0.6638) Steps 508(527.16) | Grad Norm 8.4976(7.8294) | Total Time 14.00(14.00)\n",
      "Iter 0423 | Time 47.5924(48.3950) | Bit/dim 4.5706(4.7085) | Xent 1.8087(1.8755) | Loss 5.4750(5.6462) | Error 0.6396(0.6631) Steps 514(526.77) | Grad Norm 3.8050(7.7086) | Total Time 14.00(14.00)\n",
      "Iter 0424 | Time 46.9831(48.3526) | Bit/dim 4.5371(4.7033) | Xent 1.8527(1.8748) | Loss 5.4635(5.6407) | Error 0.6614(0.6630) Steps 514(526.38) | Grad Norm 5.1451(7.6317) | Total Time 14.00(14.00)\n",
      "Iter 0425 | Time 47.9141(48.3395) | Bit/dim 4.5174(4.6977) | Xent 1.7966(1.8725) | Loss 5.4157(5.6340) | Error 0.6362(0.6622) Steps 514(526.01) | Grad Norm 2.9306(7.4907) | Total Time 14.00(14.00)\n",
      "Iter 0426 | Time 47.7934(48.3231) | Bit/dim 4.5163(4.6923) | Xent 1.8254(1.8711) | Loss 5.4290(5.6278) | Error 0.6432(0.6616) Steps 514(525.65) | Grad Norm 4.1886(7.3916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 24.5578, Epoch Time 325.0659(334.8890), Bit/dim 4.5150(best: 4.5507), Xent 1.7613, Loss 5.3956, Error 0.6195(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 47.5787(48.3007) | Bit/dim 4.5115(4.6869) | Xent 1.7930(1.8687) | Loss 5.4079(5.6212) | Error 0.6376(0.6609) Steps 526(525.66) | Grad Norm 3.4356(7.2730) | Total Time 14.00(14.00)\n",
      "Iter 0428 | Time 46.5577(48.2485) | Bit/dim 4.5128(4.6817) | Xent 1.7741(1.8659) | Loss 5.3998(5.6146) | Error 0.6182(0.6596) Steps 514(525.31) | Grad Norm 2.6341(7.1338) | Total Time 14.00(14.00)\n",
      "Iter 0429 | Time 47.7222(48.2327) | Bit/dim 4.5131(4.6766) | Xent 1.7614(1.8628) | Loss 5.3938(5.6080) | Error 0.6176(0.6584) Steps 526(525.33) | Grad Norm 4.3622(7.0506) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 49.8909(48.2824) | Bit/dim 4.5088(4.6716) | Xent 1.7788(1.8602) | Loss 5.3981(5.6017) | Error 0.6325(0.6576) Steps 520(525.17) | Grad Norm 8.5388(7.0953) | Total Time 14.00(14.00)\n",
      "Iter 0431 | Time 49.0009(48.3040) | Bit/dim 4.6520(4.6710) | Xent 1.9973(1.8644) | Loss 5.6506(5.6032) | Error 0.7031(0.6590) Steps 532(525.38) | Grad Norm 29.7214(7.7741) | Total Time 14.00(14.00)\n",
      "Iter 0432 | Time 51.0983(48.3878) | Bit/dim 5.4727(4.6950) | Xent 2.8019(1.8925) | Loss 6.8737(5.6413) | Error 0.7710(0.6623) Steps 568(526.66) | Grad Norm 41.6508(8.7904) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 22.5770, Epoch Time 330.1526(334.7469), Bit/dim 5.3297(best: 4.5150), Xent 2.0191, Loss 6.3392, Error 0.7077(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 42.1920(48.2019) | Bit/dim 5.3272(4.7140) | Xent 2.0358(1.8968) | Loss 6.3451(5.6624) | Error 0.7092(0.6637) Steps 484(525.38) | Grad Norm 6.9292(8.7345) | Total Time 14.00(14.00)\n",
      "Iter 0434 | Time 43.7973(48.0698) | Bit/dim 5.7970(4.7465) | Xent 2.0163(1.9004) | Loss 6.8052(5.6967) | Error 0.7103(0.6651) Steps 508(524.85) | Grad Norm 11.0164(8.8030) | Total Time 14.00(14.00)\n",
      "Iter 0435 | Time 42.9423(47.9160) | Bit/dim 5.9183(4.7816) | Xent 2.2293(1.9102) | Loss 7.0329(5.7368) | Error 0.7953(0.6690) Steps 508(524.35) | Grad Norm 11.9705(8.8980) | Total Time 14.00(14.00)\n",
      "Iter 0436 | Time 45.2978(47.8374) | Bit/dim 5.4457(4.8016) | Xent 2.1757(1.9182) | Loss 6.5336(5.7607) | Error 0.7622(0.6718) Steps 538(524.76) | Grad Norm 8.2547(8.8787) | Total Time 14.00(14.00)\n",
      "Iter 0437 | Time 47.0573(47.8140) | Bit/dim 5.3565(4.8182) | Xent 2.1092(1.9239) | Loss 6.4112(5.7802) | Error 0.7419(0.6739) Steps 544(525.34) | Grad Norm 6.7995(8.8163) | Total Time 14.00(14.00)\n",
      "Iter 0438 | Time 48.6576(47.8393) | Bit/dim 5.3008(4.8327) | Xent 2.1348(1.9303) | Loss 6.3681(5.7978) | Error 0.7315(0.6757) Steps 556(526.26) | Grad Norm 12.3230(8.9215) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 25.3130, Epoch Time 311.0401(334.0357), Bit/dim 5.3405(best: 4.5150), Xent 2.6475, Loss 6.6642, Error 0.8393(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 45.7225(47.7758) | Bit/dim 5.3439(4.8480) | Xent 2.7034(1.9534) | Loss 6.6956(5.8247) | Error 0.8325(0.6804) Steps 538(526.61) | Grad Norm 37.7891(9.7876) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 49.0312(47.8135) | Bit/dim 6.7894(4.9063) | Xent 5.7171(2.0664) | Loss 9.6480(5.9394) | Error 0.8269(0.6848) Steps 568(527.85) | Grad Norm 45.6876(10.8646) | Total Time 14.00(14.00)\n",
      "Iter 0441 | Time 50.5545(47.8957) | Bit/dim 6.8637(4.9650) | Xent 2.9310(2.0923) | Loss 8.3292(6.0111) | Error 0.8579(0.6900) Steps 628(530.85) | Grad Norm 33.3441(11.5390) | Total Time 14.00(14.00)\n",
      "Iter 0442 | Time 52.3832(48.0303) | Bit/dim 11.1086(5.1493) | Xent 3.8787(2.1459) | Loss 13.0480(6.2222) | Error 0.8684(0.6953) Steps 640(534.13) | Grad Norm 73.6839(13.4033) | Total Time 14.00(14.00)\n",
      "Iter 0443 | Time 50.9466(48.1178) | Bit/dim 7.8303(5.2297) | Xent 3.6638(2.1914) | Loss 9.6622(6.3254) | Error 0.8918(0.7012) Steps 616(536.58) | Grad Norm 42.9828(14.2907) | Total Time 14.00(14.00)\n",
      "Iter 0444 | Time 47.2367(48.0914) | Bit/dim 6.9567(5.2815) | Xent 6.9159(2.3332) | Loss 10.4146(6.4481) | Error 0.8992(0.7071) Steps 556(537.17) | Grad Norm 40.4303(15.0749) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 26.1548, Epoch Time 338.1570(334.1593), Bit/dim 6.9175(best: 4.5150), Xent 3.7553, Loss 8.7952, Error 0.8910(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 51.1335(48.1827) | Bit/dim 6.9225(5.3308) | Xent 3.7748(2.3764) | Loss 8.8099(6.5190) | Error 0.8872(0.7125) Steps 592(538.81) | Grad Norm 29.4106(15.5049) | Total Time 14.00(14.00)\n",
      "Iter 0446 | Time 51.8582(48.2929) | Bit/dim 7.1567(5.3855) | Xent 10.7022(2.6262) | Loss 12.5078(6.6986) | Error 0.8978(0.7181) Steps 610(540.95) | Grad Norm 44.2273(16.3666) | Total Time 14.00(14.00)\n",
      "Iter 0447 | Time 54.2696(48.4722) | Bit/dim 6.7502(5.4265) | Xent 5.9719(2.7266) | Loss 9.7361(6.7898) | Error 0.8960(0.7234) Steps 652(544.28) | Grad Norm 24.0047(16.5958) | Total Time 14.00(14.00)\n",
      "Iter 0448 | Time 53.8711(48.6342) | Bit/dim 6.7529(5.4663) | Xent 7.9709(2.8839) | Loss 10.7384(6.9082) | Error 0.8998(0.7287) Steps 658(547.69) | Grad Norm 40.0442(17.2992) | Total Time 14.00(14.00)\n",
      "Iter 0449 | Time 51.9357(48.7332) | Bit/dim 6.5158(5.4978) | Xent 2.4601(2.8712) | Loss 7.7458(6.9333) | Error 0.7943(0.7307) Steps 598(549.20) | Grad Norm 4.6837(16.9207) | Total Time 14.00(14.00)\n",
      "Iter 0450 | Time 49.1851(48.7468) | Bit/dim 6.5136(5.5282) | Xent 3.4610(2.8889) | Loss 8.2441(6.9727) | Error 0.8618(0.7346) Steps 586(550.30) | Grad Norm 15.1190(16.8667) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 26.1118, Epoch Time 354.0613(334.7564), Bit/dim 6.5101(best: 4.5150), Xent 3.0749, Loss 8.0475, Error 0.7854(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 49.5565(48.7711) | Bit/dim 6.5093(5.5577) | Xent 3.1325(2.8962) | Loss 8.0755(7.0058) | Error 0.7804(0.7360) Steps 574(551.02) | Grad Norm 24.4324(17.0937) | Total Time 14.00(14.00)\n",
      "Iter 0452 | Time 51.0811(48.8404) | Bit/dim 6.3752(5.5822) | Xent 2.6533(2.8889) | Loss 7.7019(7.0266) | Error 0.7550(0.7366) Steps 586(552.06) | Grad Norm 14.9282(17.0287) | Total Time 14.00(14.00)\n",
      "Iter 0453 | Time 48.9206(48.8428) | Bit/dim 6.5961(5.6126) | Xent 5.6909(2.9730) | Loss 9.4415(7.0991) | Error 0.8944(0.7413) Steps 580(552.90) | Grad Norm 84.1467(19.0422) | Total Time 14.00(14.00)\n",
      "Iter 0454 | Time 52.2044(48.9436) | Bit/dim 6.6552(5.6439) | Xent 5.2290(3.0406) | Loss 9.2697(7.1642) | Error 0.8359(0.7441) Steps 622(554.98) | Grad Norm 43.1326(19.7650) | Total Time 14.00(14.00)\n",
      "Iter 0455 | Time 53.6926(49.0861) | Bit/dim 6.7371(5.6767) | Xent 5.1002(3.1024) | Loss 9.2872(7.2279) | Error 0.7991(0.7458) Steps 634(557.35) | Grad Norm 33.8232(20.1867) | Total Time 14.00(14.00)\n",
      "Iter 0456 | Time 57.6769(49.3438) | Bit/dim 6.6728(5.7066) | Xent 4.1093(3.1326) | Loss 8.7274(7.2729) | Error 0.8370(0.7485) Steps 640(559.83) | Grad Norm 14.8965(20.0280) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 26.5074, Epoch Time 355.2460(335.3711), Bit/dim 6.7667(best: 4.5150), Xent 4.1757, Loss 8.8546, Error 0.8300(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 56.8424(49.5688) | Bit/dim 6.7664(5.7384) | Xent 4.2864(3.1672) | Loss 8.9096(7.3220) | Error 0.8371(0.7512) Steps 622(561.69) | Grad Norm 7.1601(19.6420) | Total Time 14.00(14.00)\n",
      "Iter 0458 | Time 54.7439(49.7240) | Bit/dim 6.8536(5.7718) | Xent 3.8083(3.1865) | Loss 8.7577(7.3651) | Error 0.8282(0.7535) Steps 610(563.14) | Grad Norm 7.1621(19.2676) | Total Time 14.00(14.00)\n",
      "Iter 0459 | Time 56.1895(49.9180) | Bit/dim 6.8490(5.8041) | Xent 3.0815(3.1833) | Loss 8.3897(7.3958) | Error 0.8059(0.7551) Steps 622(564.91) | Grad Norm 4.2923(18.8183) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 56.7559(50.1231) | Bit/dim 6.8237(5.8347) | Xent 2.4943(3.1626) | Loss 8.0708(7.4160) | Error 0.8029(0.7565) Steps 634(566.98) | Grad Norm 3.3964(18.3556) | Total Time 14.00(14.00)\n",
      "Iter 0461 | Time 56.5352(50.3155) | Bit/dim 6.7726(5.8629) | Xent 2.6674(3.1478) | Loss 8.1063(7.4368) | Error 0.8156(0.7583) Steps 634(568.99) | Grad Norm 21.5045(18.4501) | Total Time 14.00(14.00)\n",
      "Iter 0462 | Time 56.2129(50.4924) | Bit/dim 6.7430(5.8893) | Xent 3.9666(3.1724) | Loss 8.7263(7.4754) | Error 0.8788(0.7619) Steps 640(571.12) | Grad Norm 33.7050(18.9078) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 26.9667, Epoch Time 379.9098(336.7072), Bit/dim 6.6340(best: 4.5150), Xent 2.3678, Loss 7.8179, Error 0.7981(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 55.0861(50.6302) | Bit/dim 6.6291(5.9115) | Xent 2.4038(3.1493) | Loss 7.8310(7.4861) | Error 0.7996(0.7630) Steps 616(572.47) | Grad Norm 2.8350(18.4256) | Total Time 14.00(14.00)\n",
      "Iter 0464 | Time 56.2506(50.7988) | Bit/dim 6.6292(5.9330) | Xent 3.9343(3.1728) | Loss 8.5964(7.5194) | Error 0.8674(0.7661) Steps 622(573.95) | Grad Norm 50.3361(19.3829) | Total Time 14.00(14.00)\n",
      "Iter 0465 | Time 55.8161(50.9494) | Bit/dim 6.9177(5.9625) | Xent 4.4474(3.2111) | Loss 9.1414(7.5681) | Error 0.8476(0.7686) Steps 604(574.85) | Grad Norm 31.8072(19.7556) | Total Time 14.00(14.00)\n",
      "Iter 0466 | Time 55.6735(51.0911) | Bit/dim 6.9274(5.9915) | Xent 4.2209(3.2414) | Loss 9.0378(7.6122) | Error 0.8382(0.7707) Steps 604(575.73) | Grad Norm 18.2273(19.7098) | Total Time 14.00(14.00)\n",
      "Iter 0467 | Time 54.2940(51.1872) | Bit/dim 6.6296(6.0106) | Xent 3.7968(3.2580) | Loss 8.5280(7.6396) | Error 0.8491(0.7730) Steps 604(576.58) | Grad Norm 6.6676(19.3185) | Total Time 14.00(14.00)\n",
      "Iter 0468 | Time 54.7609(51.2944) | Bit/dim 6.5665(6.0273) | Xent 3.6090(3.2686) | Loss 8.3710(7.6616) | Error 0.8624(0.7757) Steps 610(577.58) | Grad Norm 6.7196(18.9405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 25.8030, Epoch Time 373.2736(337.8042), Bit/dim 6.5743(best: 4.5150), Xent 3.2789, Loss 8.2138, Error 0.8628(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 53.4915(51.3603) | Bit/dim 6.5748(6.0437) | Xent 3.3217(3.2702) | Loss 8.2357(7.6788) | Error 0.8598(0.7782) Steps 598(578.19) | Grad Norm 8.7390(18.6345) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 55.0369(51.4706) | Bit/dim 6.5589(6.0592) | Xent 2.8778(3.2584) | Loss 7.9978(7.6884) | Error 0.8385(0.7800) Steps 598(578.79) | Grad Norm 7.8022(18.3095) | Total Time 14.00(14.00)\n",
      "Iter 0471 | Time 55.6523(51.5960) | Bit/dim 6.5589(6.0742) | Xent 2.4252(3.2334) | Loss 7.7715(7.6909) | Error 0.8297(0.7815) Steps 598(579.36) | Grad Norm 3.4521(17.8638) | Total Time 14.00(14.00)\n",
      "Iter 0472 | Time 54.1165(51.6717) | Bit/dim 6.5565(6.0886) | Xent 2.4746(3.2106) | Loss 7.7938(7.6940) | Error 0.8488(0.7836) Steps 568(579.02) | Grad Norm 4.4061(17.4601) | Total Time 14.00(14.00)\n",
      "Iter 0473 | Time 50.1332(51.6255) | Bit/dim 6.5306(6.1019) | Xent 2.6978(3.1953) | Loss 7.8795(7.6995) | Error 0.8468(0.7854) Steps 556(578.33) | Grad Norm 7.0525(17.1478) | Total Time 14.00(14.00)\n",
      "Iter 0474 | Time 49.1817(51.5522) | Bit/dim 6.5010(6.1139) | Xent 2.5814(3.1768) | Loss 7.7916(7.7023) | Error 0.8521(0.7874) Steps 550(577.48) | Grad Norm 5.4560(16.7971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 25.3782, Epoch Time 358.5817(338.4275), Bit/dim 6.4453(best: 4.5150), Xent 2.2527, Loss 7.5716, Error 0.8060(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 47.9810(51.4451) | Bit/dim 6.4470(6.1239) | Xent 2.2994(3.1505) | Loss 7.5967(7.6991) | Error 0.8159(0.7883) Steps 544(576.48) | Grad Norm 1.7720(16.3463) | Total Time 14.00(14.00)\n",
      "Iter 0476 | Time 48.7287(51.3636) | Bit/dim 6.3826(6.1316) | Xent 2.2696(3.1241) | Loss 7.5174(7.6937) | Error 0.7840(0.7882) Steps 538(575.32) | Grad Norm 3.2377(15.9531) | Total Time 14.00(14.00)\n",
      "Iter 0477 | Time 49.7429(51.3149) | Bit/dim 6.3377(6.1378) | Xent 2.4126(3.1027) | Loss 7.5440(7.6892) | Error 0.7997(0.7885) Steps 544(574.38) | Grad Norm 5.3692(15.6356) | Total Time 14.00(14.00)\n",
      "Iter 0478 | Time 48.9234(51.2432) | Bit/dim 6.2465(6.1411) | Xent 2.4603(3.0835) | Loss 7.4767(7.6828) | Error 0.8086(0.7891) Steps 550(573.65) | Grad Norm 5.8552(15.3422) | Total Time 14.00(14.00)\n",
      "Iter 0479 | Time 49.7100(51.1972) | Bit/dim 6.1134(6.1402) | Xent 2.3576(3.0617) | Loss 7.2922(7.6711) | Error 0.7894(0.7891) Steps 538(572.58) | Grad Norm 3.8309(14.9968) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 51.9289(51.2192) | Bit/dim 5.9759(6.1353) | Xent 2.2946(3.0387) | Loss 7.1232(7.6546) | Error 0.7936(0.7893) Steps 550(571.90) | Grad Norm 2.4702(14.6210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 26.0037, Epoch Time 338.6467(338.4341), Bit/dim 5.8992(best: 4.5150), Xent 2.2442, Loss 7.0213, Error 0.7784(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 50.8895(51.2093) | Bit/dim 5.8937(6.1281) | Xent 2.2852(3.0161) | Loss 7.0363(7.6361) | Error 0.7908(0.7893) Steps 568(571.79) | Grad Norm 4.9829(14.3319) | Total Time 14.00(14.00)\n",
      "Iter 0482 | Time 50.6827(51.1935) | Bit/dim 5.8421(6.1195) | Xent 2.3110(2.9949) | Loss 6.9976(7.6169) | Error 0.7815(0.7891) Steps 574(571.85) | Grad Norm 5.4692(14.0660) | Total Time 14.00(14.00)\n",
      "Iter 0483 | Time 52.9296(51.2456) | Bit/dim 5.8101(6.1102) | Xent 2.3293(2.9750) | Loss 6.9748(7.5977) | Error 0.7812(0.7888) Steps 580(572.10) | Grad Norm 5.7739(13.8172) | Total Time 14.00(14.00)\n",
      "Iter 0484 | Time 52.5782(51.2855) | Bit/dim 5.7321(6.0989) | Xent 2.2412(2.9529) | Loss 6.8527(7.5753) | Error 0.7716(0.7883) Steps 568(571.97) | Grad Norm 5.1046(13.5559) | Total Time 14.00(14.00)\n",
      "Iter 0485 | Time 50.6192(51.2655) | Bit/dim 5.6616(6.0857) | Xent 2.1782(2.9297) | Loss 6.7507(7.5506) | Error 0.7715(0.7878) Steps 544(571.14) | Grad Norm 3.7442(13.2615) | Total Time 14.00(14.00)\n",
      "Iter 0486 | Time 53.6365(51.3367) | Bit/dim 5.6166(6.0717) | Xent 2.2846(2.9103) | Loss 6.7589(7.5268) | Error 0.8161(0.7887) Steps 556(570.68) | Grad Norm 7.5809(13.0911) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 25.2260, Epoch Time 352.2667(338.8491), Bit/dim 5.6041(best: 4.5150), Xent 2.1565, Loss 6.6824, Error 0.7645(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 52.8547(51.3822) | Bit/dim 5.6129(6.0579) | Xent 2.1939(2.8888) | Loss 6.7099(7.5023) | Error 0.7682(0.7881) Steps 556(570.24) | Grad Norm 6.7026(12.8994) | Total Time 14.00(14.00)\n",
      "Iter 0488 | Time 47.5126(51.2661) | Bit/dim 5.5472(6.0426) | Xent 2.1232(2.8659) | Loss 6.6088(7.4755) | Error 0.7464(0.7868) Steps 532(569.09) | Grad Norm 4.7816(12.6559) | Total Time 14.00(14.00)\n",
      "Iter 0489 | Time 46.4770(51.1224) | Bit/dim 5.5100(6.0266) | Xent 2.1180(2.8434) | Loss 6.5690(7.4483) | Error 0.7529(0.7858) Steps 526(567.80) | Grad Norm 8.5309(12.5321) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 51.1203(51.1224) | Bit/dim 5.4404(6.0090) | Xent 2.1279(2.8220) | Loss 6.5044(7.4200) | Error 0.7370(0.7843) Steps 544(567.09) | Grad Norm 2.9811(12.2456) | Total Time 14.00(14.00)\n",
      "Iter 0491 | Time 54.1782(51.2141) | Bit/dim 5.3777(5.9901) | Xent 2.2048(2.8035) | Loss 6.4801(7.3918) | Error 0.7837(0.7843) Steps 550(566.57) | Grad Norm 10.6147(12.1967) | Total Time 14.00(14.00)\n",
      "Iter 0492 | Time 53.7352(51.2897) | Bit/dim 5.3460(5.9708) | Xent 2.1626(2.7842) | Loss 6.4273(7.3629) | Error 0.7744(0.7840) Steps 538(565.72) | Grad Norm 11.9790(12.1902) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 25.1645, Epoch Time 346.9283(339.0915), Bit/dim 5.2963(best: 4.5150), Xent 2.1002, Loss 6.3464, Error 0.7573(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 51.2642(51.2889) | Bit/dim 5.2929(5.9504) | Xent 2.1371(2.7648) | Loss 6.3614(7.3328) | Error 0.7618(0.7833) Steps 532(564.71) | Grad Norm 8.6099(12.0828) | Total Time 14.00(14.00)\n",
      "Iter 0494 | Time 49.1867(51.2259) | Bit/dim 5.2702(5.9300) | Xent 2.0956(2.7448) | Loss 6.3180(7.3024) | Error 0.7380(0.7820) Steps 526(563.54) | Grad Norm 1.9466(11.7787) | Total Time 14.00(14.00)\n",
      "Iter 0495 | Time 50.1688(51.1941) | Bit/dim 5.2629(5.9100) | Xent 2.1306(2.7263) | Loss 6.3282(7.2732) | Error 0.7731(0.7817) Steps 526(562.42) | Grad Norm 5.8287(11.6002) | Total Time 14.00(14.00)\n",
      "Iter 0496 | Time 50.5928(51.1761) | Bit/dim 5.2265(5.8895) | Xent 2.1156(2.7080) | Loss 6.2843(7.2435) | Error 0.7450(0.7806) Steps 526(561.33) | Grad Norm 7.1186(11.4657) | Total Time 14.00(14.00)\n",
      "Iter 0497 | Time 50.7596(51.1636) | Bit/dim 5.1979(5.8687) | Xent 2.1061(2.6899) | Loss 6.2509(7.2137) | Error 0.7435(0.7795) Steps 544(560.81) | Grad Norm 3.2662(11.2197) | Total Time 14.00(14.00)\n",
      "Iter 0498 | Time 52.5363(51.2048) | Bit/dim 5.1702(5.8478) | Xent 2.0826(2.6717) | Loss 6.2115(7.1836) | Error 0.7339(0.7781) Steps 556(560.66) | Grad Norm 2.8829(10.9696) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 25.2302, Epoch Time 345.4627(339.2826), Bit/dim 5.1535(best: 4.5150), Xent 2.1050, Loss 6.2060, Error 0.7420(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 53.3740(51.2699) | Bit/dim 5.1519(5.8269) | Xent 2.1292(2.6555) | Loss 6.2165(7.1546) | Error 0.7489(0.7773) Steps 562(560.70) | Grad Norm 9.0646(10.9125) | Total Time 14.00(14.00)\n",
      "Iter 0500 | Time 53.2462(51.3292) | Bit/dim 5.1677(5.8071) | Xent 2.1685(2.6408) | Loss 6.2519(7.1276) | Error 0.7809(0.7774) Steps 556(560.56) | Grad Norm 16.5308(11.0810) | Total Time 14.00(14.00)\n",
      "Iter 0501 | Time 53.8480(51.4047) | Bit/dim 5.1833(5.7884) | Xent 2.3386(2.6318) | Loss 6.3525(7.1043) | Error 0.8215(0.7787) Steps 556(560.42) | Grad Norm 25.9846(11.5281) | Total Time 14.00(14.00)\n",
      "Iter 0502 | Time 53.3206(51.4622) | Bit/dim 5.2436(5.7721) | Xent 2.3528(2.6234) | Loss 6.4200(7.0838) | Error 0.8135(0.7797) Steps 562(560.47) | Grad Norm 27.2058(11.9985) | Total Time 14.00(14.00)\n",
      "Iter 0503 | Time 51.8249(51.4731) | Bit/dim 5.1314(5.7529) | Xent 2.0963(2.6076) | Loss 6.1796(7.0567) | Error 0.7270(0.7781) Steps 568(560.70) | Grad Norm 6.1820(11.8240) | Total Time 14.00(14.00)\n",
      "Iter 0504 | Time 51.2370(51.4660) | Bit/dim 5.1781(5.7356) | Xent 2.1724(2.5945) | Loss 6.2643(7.0329) | Error 0.7736(0.7780) Steps 544(560.20) | Grad Norm 14.5902(11.9070) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 25.9393, Epoch Time 358.5446(339.8605), Bit/dim 5.0834(best: 4.5150), Xent 2.0945, Loss 6.1307, Error 0.7575(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 55.6974(51.5929) | Bit/dim 5.0905(5.7163) | Xent 2.1232(2.5804) | Loss 6.1521(7.0065) | Error 0.7625(0.7775) Steps 556(560.07) | Grad Norm 5.4104(11.7121) | Total Time 14.00(14.00)\n",
      "Iter 0506 | Time 57.2798(51.7635) | Bit/dim 5.1023(5.6978) | Xent 2.1753(2.5682) | Loss 6.1900(6.9820) | Error 0.7820(0.7777) Steps 574(560.49) | Grad Norm 11.9210(11.7183) | Total Time 14.00(14.00)\n",
      "Iter 0507 | Time 55.7186(51.8822) | Bit/dim 5.0376(5.6780) | Xent 2.0711(2.5533) | Loss 6.0731(6.9547) | Error 0.7264(0.7761) Steps 574(560.89) | Grad Norm 1.7780(11.4201) | Total Time 14.00(14.00)\n",
      "Iter 0508 | Time 55.6999(51.9967) | Bit/dim 5.0695(5.6598) | Xent 2.1575(2.5415) | Loss 6.1482(6.9305) | Error 0.7691(0.7759) Steps 574(561.29) | Grad Norm 11.2458(11.4149) | Total Time 14.00(14.00)\n",
      "Iter 0509 | Time 56.6089(52.1351) | Bit/dim 5.0214(5.6406) | Xent 2.0800(2.5276) | Loss 6.0614(6.9044) | Error 0.7321(0.7746) Steps 580(561.85) | Grad Norm 2.1633(11.1373) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 54.6202(52.2096) | Bit/dim 5.0372(5.6225) | Xent 2.1024(2.5149) | Loss 6.0884(6.8799) | Error 0.7469(0.7738) Steps 556(561.67) | Grad Norm 7.8011(11.0373) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 24.1389, Epoch Time 375.3578(340.9254), Bit/dim 5.0180(best: 4.5150), Xent 2.0478, Loss 6.0419, Error 0.7294(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0511 | Time 53.3775(52.2447) | Bit/dim 5.0201(5.6044) | Xent 2.0738(2.5016) | Loss 6.0570(6.8553) | Error 0.7395(0.7728) Steps 550(561.32) | Grad Norm 5.4560(10.8698) | Total Time 14.00(14.00)\n",
      "Iter 0512 | Time 53.8465(52.2927) | Bit/dim 5.0041(5.5864) | Xent 2.0633(2.4885) | Loss 6.0358(6.8307) | Error 0.7214(0.7712) Steps 550(560.98) | Grad Norm 2.6245(10.6225) | Total Time 14.00(14.00)\n",
      "Iter 0513 | Time 52.9830(52.3134) | Bit/dim 5.0243(5.5696) | Xent 2.0848(2.4764) | Loss 6.0667(6.8078) | Error 0.7340(0.7701) Steps 550(560.65) | Grad Norm 6.3044(10.4929) | Total Time 14.00(14.00)\n",
      "Iter 0514 | Time 53.5840(52.3516) | Bit/dim 4.9680(5.5515) | Xent 2.0642(2.4640) | Loss 6.0001(6.7835) | Error 0.7209(0.7686) Steps 556(560.51) | Grad Norm 2.9574(10.2669) | Total Time 14.00(14.00)\n",
      "Iter 0515 | Time 53.4966(52.3859) | Bit/dim 4.9832(5.5345) | Xent 2.0648(2.4520) | Loss 6.0156(6.7605) | Error 0.7265(0.7674) Steps 556(560.38) | Grad Norm 2.8990(10.0458) | Total Time 14.00(14.00)\n",
      "Iter 0516 | Time 54.1300(52.4382) | Bit/dim 4.9708(5.5176) | Xent 2.0555(2.4401) | Loss 5.9985(6.7376) | Error 0.7224(0.7660) Steps 562(560.43) | Grad Norm 4.7387(9.8866) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 25.4628, Epoch Time 362.7979(341.5816), Bit/dim 4.9477(best: 4.5150), Xent 2.0353, Loss 5.9654, Error 0.7075(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0517 | Time 52.6571(52.4448) | Bit/dim 4.9380(5.5002) | Xent 2.0473(2.4283) | Loss 5.9616(6.7143) | Error 0.7144(0.7645) Steps 562(560.47) | Grad Norm 2.1664(9.6550) | Total Time 14.00(14.00)\n",
      "Iter 0518 | Time 53.4696(52.4755) | Bit/dim 4.9352(5.4832) | Xent 2.0692(2.4176) | Loss 5.9698(6.6920) | Error 0.7244(0.7633) Steps 562(560.52) | Grad Norm 2.7163(9.4468) | Total Time 14.00(14.00)\n",
      "Iter 0519 | Time 55.1959(52.5572) | Bit/dim 4.9417(5.4670) | Xent 2.0557(2.4067) | Loss 5.9696(6.6703) | Error 0.7114(0.7617) Steps 556(560.38) | Grad Norm 3.2354(9.2605) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 54.5919(52.6182) | Bit/dim 4.9157(5.4504) | Xent 2.0566(2.3962) | Loss 5.9440(6.6485) | Error 0.7210(0.7605) Steps 568(560.61) | Grad Norm 1.2338(9.0197) | Total Time 14.00(14.00)\n",
      "Iter 0521 | Time 54.1987(52.6656) | Bit/dim 4.9127(5.4343) | Xent 2.0534(2.3859) | Loss 5.9394(6.6273) | Error 0.7249(0.7594) Steps 574(561.01) | Grad Norm 1.9779(8.8084) | Total Time 14.00(14.00)\n",
      "Iter 0522 | Time 56.1531(52.7702) | Bit/dim 4.9045(5.4184) | Xent 2.0661(2.3763) | Loss 5.9376(6.6066) | Error 0.7275(0.7585) Steps 580(561.58) | Grad Norm 2.8414(8.6294) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 25.5692, Epoch Time 367.6378(342.3632), Bit/dim 4.8888(best: 4.5150), Xent 2.0278, Loss 5.9027, Error 0.6996(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0523 | Time 56.6790(52.8875) | Bit/dim 4.8876(5.4025) | Xent 2.0569(2.3667) | Loss 5.9160(6.5859) | Error 0.7149(0.7571) Steps 574(561.96) | Grad Norm 1.1966(8.4064) | Total Time 14.00(14.00)\n",
      "Iter 0524 | Time 56.8337(53.0059) | Bit/dim 4.8775(5.3867) | Xent 2.0401(2.3569) | Loss 5.8975(6.5652) | Error 0.7140(0.7559) Steps 574(562.32) | Grad Norm 2.0727(8.2164) | Total Time 14.00(14.00)\n",
      "Iter 0525 | Time 56.2067(53.1019) | Bit/dim 4.8756(5.3714) | Xent 2.0446(2.3476) | Loss 5.8979(6.5452) | Error 0.7076(0.7544) Steps 574(562.67) | Grad Norm 2.2768(8.0382) | Total Time 14.00(14.00)\n",
      "Iter 0526 | Time 53.8795(53.1252) | Bit/dim 4.8751(5.3565) | Xent 2.0328(2.3381) | Loss 5.8915(6.5256) | Error 0.7164(0.7533) Steps 568(562.83) | Grad Norm 0.9916(7.8268) | Total Time 14.00(14.00)\n",
      "Iter 0527 | Time 54.6797(53.1719) | Bit/dim 4.8563(5.3415) | Xent 2.0401(2.3292) | Loss 5.8763(6.5061) | Error 0.7241(0.7524) Steps 568(562.98) | Grad Norm 2.3523(7.6626) | Total Time 14.00(14.00)\n",
      "Iter 0528 | Time 54.0508(53.1982) | Bit/dim 4.8556(5.3269) | Xent 2.0386(2.3205) | Loss 5.8749(6.4872) | Error 0.7134(0.7512) Steps 574(563.31) | Grad Norm 1.8935(7.4895) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 25.6062, Epoch Time 373.8528(343.3079), Bit/dim 4.8473(best: 4.5150), Xent 2.0075, Loss 5.8510, Error 0.6901(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0529 | Time 55.0218(53.2529) | Bit/dim 4.8368(5.3122) | Xent 2.0343(2.3119) | Loss 5.8539(6.4682) | Error 0.7054(0.7498) Steps 574(563.63) | Grad Norm 1.3466(7.3052) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 54.8002(53.2994) | Bit/dim 4.8301(5.2978) | Xent 2.0298(2.3034) | Loss 5.8450(6.4495) | Error 0.7186(0.7489) Steps 574(563.95) | Grad Norm 1.9860(7.1457) | Total Time 14.00(14.00)\n",
      "Iter 0531 | Time 54.9173(53.3479) | Bit/dim 4.8394(5.2840) | Xent 2.0357(2.2954) | Loss 5.8572(6.4317) | Error 0.7105(0.7478) Steps 568(564.07) | Grad Norm 1.2871(6.9699) | Total Time 14.00(14.00)\n",
      "Iter 0532 | Time 54.2768(53.3758) | Bit/dim 4.8339(5.2705) | Xent 2.0107(2.2869) | Loss 5.8393(6.4139) | Error 0.6964(0.7462) Steps 574(564.37) | Grad Norm 0.9306(6.7887) | Total Time 14.00(14.00)\n",
      "Iter 0533 | Time 54.7171(53.4160) | Bit/dim 4.8302(5.2573) | Xent 2.0302(2.2792) | Loss 5.8453(6.3969) | Error 0.7086(0.7451) Steps 574(564.65) | Grad Norm 1.9808(6.6445) | Total Time 14.00(14.00)\n",
      "Iter 0534 | Time 54.8722(53.4597) | Bit/dim 4.8169(5.2441) | Xent 2.0257(2.2716) | Loss 5.8298(6.3799) | Error 0.7096(0.7440) Steps 574(564.93) | Grad Norm 1.2988(6.4841) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 25.2894, Epoch Time 369.8660(344.1047), Bit/dim 4.8152(best: 4.5150), Xent 1.9917, Loss 5.8110, Error 0.6889(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0535 | Time 54.4327(53.4889) | Bit/dim 4.8143(5.2312) | Xent 2.0142(2.2638) | Loss 5.8214(6.3631) | Error 0.7004(0.7427) Steps 568(565.03) | Grad Norm 1.5551(6.3363) | Total Time 14.00(14.00)\n",
      "Iter 0536 | Time 54.2699(53.5123) | Bit/dim 4.8082(5.2185) | Xent 2.0173(2.2564) | Loss 5.8168(6.3467) | Error 0.7109(0.7418) Steps 538(564.22) | Grad Norm 2.1382(6.2103) | Total Time 14.00(14.00)\n",
      "Iter 0537 | Time 54.8760(53.5532) | Bit/dim 4.7994(5.2059) | Xent 2.0024(2.2488) | Loss 5.8006(6.3303) | Error 0.6939(0.7403) Steps 568(564.33) | Grad Norm 0.8316(6.0489) | Total Time 14.00(14.00)\n",
      "Iter 0538 | Time 55.1890(53.6023) | Bit/dim 4.8022(5.1938) | Xent 2.0073(2.2416) | Loss 5.8059(6.3146) | Error 0.7027(0.7392) Steps 574(564.62) | Grad Norm 1.9381(5.9256) | Total Time 14.00(14.00)\n",
      "Iter 0539 | Time 55.7032(53.6653) | Bit/dim 4.7895(5.1817) | Xent 2.0196(2.2349) | Loss 5.7992(6.2991) | Error 0.6955(0.7379) Steps 574(564.90) | Grad Norm 0.8611(5.7737) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 54.8637(53.7013) | Bit/dim 4.7752(5.1695) | Xent 2.0047(2.2280) | Loss 5.7776(6.2835) | Error 0.7015(0.7368) Steps 574(565.17) | Grad Norm 1.2853(5.6390) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 25.1065, Epoch Time 370.1399(344.8857), Bit/dim 4.7805(best: 4.5150), Xent 1.9746, Loss 5.7678, Error 0.6770(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0541 | Time 54.5663(53.7272) | Bit/dim 4.7736(5.1576) | Xent 2.0000(2.2212) | Loss 5.7736(6.2682) | Error 0.6977(0.7356) Steps 568(565.26) | Grad Norm 1.7181(5.5214) | Total Time 14.00(14.00)\n",
      "Iter 0542 | Time 55.2174(53.7719) | Bit/dim 4.7745(5.1461) | Xent 1.9897(2.2142) | Loss 5.7693(6.2532) | Error 0.6969(0.7345) Steps 568(565.34) | Grad Norm 1.2244(5.3925) | Total Time 14.00(14.00)\n",
      "Iter 0543 | Time 53.8263(53.7736) | Bit/dim 4.7779(5.1351) | Xent 1.9977(2.2077) | Loss 5.7768(6.2389) | Error 0.7045(0.7336) Steps 568(565.42) | Grad Norm 2.0126(5.2911) | Total Time 14.00(14.00)\n",
      "Iter 0544 | Time 54.9657(53.8093) | Bit/dim 4.7662(5.1240) | Xent 2.0125(2.2019) | Loss 5.7724(6.2249) | Error 0.6989(0.7325) Steps 574(565.68) | Grad Norm 1.6249(5.1811) | Total Time 14.00(14.00)\n",
      "Iter 0545 | Time 54.9210(53.8427) | Bit/dim 4.7511(5.1128) | Xent 2.0116(2.1962) | Loss 5.7569(6.2109) | Error 0.7021(0.7316) Steps 556(565.39) | Grad Norm 1.4863(5.0703) | Total Time 14.00(14.00)\n",
      "Iter 0546 | Time 53.4492(53.8309) | Bit/dim 4.7594(5.1022) | Xent 1.9960(2.1902) | Loss 5.7574(6.1973) | Error 0.7011(0.7307) Steps 538(564.57) | Grad Norm 0.9842(4.9477) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 25.3392, Epoch Time 368.1465(345.5836), Bit/dim 4.7511(best: 4.5150), Xent 1.9619, Loss 5.7320, Error 0.6757(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 50.6629(53.7358) | Bit/dim 4.7463(5.0915) | Xent 1.9811(2.1839) | Loss 5.7369(6.1835) | Error 0.6949(0.7296) Steps 538(563.77) | Grad Norm 1.3690(4.8403) | Total Time 14.00(14.00)\n",
      "Iter 0548 | Time 51.1434(53.6581) | Bit/dim 4.7422(5.0811) | Xent 1.9931(2.1782) | Loss 5.7387(6.1701) | Error 0.6975(0.7287) Steps 556(563.54) | Grad Norm 0.7699(4.7182) | Total Time 14.00(14.00)\n",
      "Iter 0549 | Time 51.0928(53.5811) | Bit/dim 4.7504(5.0711) | Xent 1.9830(2.1723) | Loss 5.7419(6.1573) | Error 0.6865(0.7274) Steps 550(563.13) | Grad Norm 2.3955(4.6485) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 50.9897(53.5034) | Bit/dim 4.7363(5.0611) | Xent 1.9881(2.1668) | Loss 5.7303(6.1445) | Error 0.6917(0.7263) Steps 550(562.74) | Grad Norm 1.8878(4.5657) | Total Time 14.00(14.00)\n",
      "Iter 0551 | Time 49.9402(53.3965) | Bit/dim 4.7401(5.0515) | Xent 1.9665(2.1608) | Loss 5.7233(6.1318) | Error 0.6889(0.7252) Steps 550(562.35) | Grad Norm 0.9153(4.4562) | Total Time 14.00(14.00)\n",
      "Iter 0552 | Time 51.6838(53.3451) | Bit/dim 4.7286(5.0418) | Xent 2.0036(2.1561) | Loss 5.7304(6.1198) | Error 0.6838(0.7240) Steps 562(562.34) | Grad Norm 1.0667(4.3545) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 25.2903, Epoch Time 346.3872(345.6077), Bit/dim 4.7247(best: 4.5150), Xent 1.9464, Loss 5.6978, Error 0.6707(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 50.4586(53.2585) | Bit/dim 4.7194(5.0321) | Xent 1.9828(2.1509) | Loss 5.7108(6.1075) | Error 0.6921(0.7230) Steps 538(561.61) | Grad Norm 1.2127(4.2603) | Total Time 14.00(14.00)\n",
      "Iter 0554 | Time 50.3326(53.1707) | Bit/dim 4.7291(5.0230) | Xent 1.9849(2.1459) | Loss 5.7216(6.0960) | Error 0.6930(0.7221) Steps 532(560.72) | Grad Norm 0.7497(4.1549) | Total Time 14.00(14.00)\n",
      "Iter 0555 | Time 49.0505(53.0471) | Bit/dim 4.7186(5.0139) | Xent 1.9698(2.1406) | Loss 5.7035(6.0842) | Error 0.6886(0.7211) Steps 532(559.86) | Grad Norm 1.3313(4.0702) | Total Time 14.00(14.00)\n",
      "Iter 0556 | Time 50.6212(52.9743) | Bit/dim 4.7180(5.0050) | Xent 1.9687(2.1354) | Loss 5.7024(6.0727) | Error 0.6866(0.7201) Steps 556(559.75) | Grad Norm 1.4312(3.9911) | Total Time 14.00(14.00)\n",
      "Iter 0557 | Time 50.7673(52.9081) | Bit/dim 4.7034(4.9960) | Xent 1.9646(2.1303) | Loss 5.6856(6.0611) | Error 0.6870(0.7191) Steps 544(559.27) | Grad Norm 1.1536(3.9059) | Total Time 14.00(14.00)\n",
      "Iter 0558 | Time 50.6486(52.8403) | Bit/dim 4.7257(4.9879) | Xent 1.9680(2.1254) | Loss 5.7097(6.0506) | Error 0.6759(0.7178) Steps 544(558.82) | Grad Norm 1.5451(3.8351) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 25.5207, Epoch Time 343.2862(345.5380), Bit/dim 4.7040(best: 4.5150), Xent 1.9350, Loss 5.6715, Error 0.6699(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 50.3106(52.7644) | Bit/dim 4.7037(4.9793) | Xent 1.9563(2.1204) | Loss 5.6818(6.0395) | Error 0.6900(0.7169) Steps 544(558.37) | Grad Norm 2.1997(3.7861) | Total Time 14.00(14.00)\n",
      "Iter 0560 | Time 50.3712(52.6926) | Bit/dim 4.6910(4.9707) | Xent 1.9605(2.1156) | Loss 5.6713(6.0285) | Error 0.6764(0.7157) Steps 544(557.94) | Grad Norm 2.1270(3.7363) | Total Time 14.00(14.00)\n",
      "Iter 0561 | Time 49.3033(52.5910) | Bit/dim 4.6986(4.9625) | Xent 1.9769(2.1114) | Loss 5.6870(6.0182) | Error 0.6854(0.7148) Steps 532(557.16) | Grad Norm 0.6006(3.6422) | Total Time 14.00(14.00)\n",
      "Iter 0562 | Time 49.5834(52.5007) | Bit/dim 4.7006(4.9547) | Xent 1.9581(2.1068) | Loss 5.6796(6.0081) | Error 0.6781(0.7137) Steps 532(556.41) | Grad Norm 1.4236(3.5757) | Total Time 14.00(14.00)\n",
      "Iter 0563 | Time 49.3566(52.4064) | Bit/dim 4.6908(4.9467) | Xent 1.9553(2.1023) | Loss 5.6685(5.9979) | Error 0.6805(0.7127) Steps 532(555.68) | Grad Norm 1.0577(3.5001) | Total Time 14.00(14.00)\n",
      "Iter 0564 | Time 49.0454(52.3056) | Bit/dim 4.6872(4.9390) | Xent 1.9481(2.0976) | Loss 5.6613(5.9878) | Error 0.6864(0.7119) Steps 532(554.97) | Grad Norm 0.8189(3.4197) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 25.5081, Epoch Time 339.2472(345.3493), Bit/dim 4.6812(best: 4.5150), Xent 1.9181, Loss 5.6402, Error 0.6611(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 49.9196(52.2340) | Bit/dim 4.6748(4.9310) | Xent 1.9616(2.0936) | Loss 5.6556(5.9778) | Error 0.6815(0.7110) Steps 544(554.64) | Grad Norm 1.2609(3.3549) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 49.8614(52.1628) | Bit/dim 4.6771(4.9234) | Xent 1.9708(2.0899) | Loss 5.6625(5.9684) | Error 0.6818(0.7101) Steps 544(554.32) | Grad Norm 0.4420(3.2675) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 49.7783(52.0913) | Bit/dim 4.6585(4.9155) | Xent 1.9323(2.0852) | Loss 5.6246(5.9580) | Error 0.6703(0.7089) Steps 532(553.65) | Grad Norm 1.8488(3.2250) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 48.6091(51.9868) | Bit/dim 4.6763(4.9083) | Xent 1.9382(2.0807) | Loss 5.6454(5.9487) | Error 0.6763(0.7080) Steps 532(553.00) | Grad Norm 3.2218(3.2249) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 51.2350(51.9643) | Bit/dim 4.7172(4.9026) | Xent 1.9327(2.0763) | Loss 5.6836(5.9407) | Error 0.6733(0.7069) Steps 544(552.73) | Grad Norm 3.0999(3.2211) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 49.0427(51.8766) | Bit/dim 4.6753(4.8957) | Xent 1.9258(2.0718) | Loss 5.6382(5.9316) | Error 0.6673(0.7057) Steps 532(552.11) | Grad Norm 2.8644(3.2104) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 25.5008, Epoch Time 339.7278(345.1807), Bit/dim 4.6690(best: 4.5150), Xent 1.9205, Loss 5.6293, Error 0.6665(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 50.9559(51.8490) | Bit/dim 4.6699(4.8890) | Xent 1.9534(2.0682) | Loss 5.6466(5.9231) | Error 0.6915(0.7053) Steps 544(551.86) | Grad Norm 6.2466(3.3015) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 48.5846(51.7511) | Bit/dim 4.6826(4.8828) | Xent 1.9870(2.0658) | Loss 5.6761(5.9157) | Error 0.6979(0.7051) Steps 526(551.09) | Grad Norm 10.0155(3.5029) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 49.7680(51.6916) | Bit/dim 4.6944(4.8771) | Xent 2.0257(2.0646) | Loss 5.7072(5.9094) | Error 0.7166(0.7054) Steps 544(550.87) | Grad Norm 12.1348(3.7619) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 48.6870(51.6014) | Bit/dim 4.7107(4.8721) | Xent 1.9663(2.0616) | Loss 5.6939(5.9030) | Error 0.6917(0.7050) Steps 526(550.13) | Grad Norm 10.2379(3.9562) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 51.4324(51.5964) | Bit/dim 4.7167(4.8675) | Xent 1.9264(2.0576) | Loss 5.6799(5.8963) | Error 0.6671(0.7039) Steps 544(549.94) | Grad Norm 4.5003(3.9725) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 49.6451(51.5378) | Bit/dim 4.6613(4.8613) | Xent 1.9525(2.0544) | Loss 5.6376(5.8885) | Error 0.6860(0.7033) Steps 532(549.41) | Grad Norm 6.7589(4.0561) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 25.2299, Epoch Time 340.0453(345.0266), Bit/dim 4.6737(best: 4.5150), Xent 1.9122, Loss 5.6298, Error 0.6643(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 50.8696(51.5178) | Bit/dim 4.6755(4.8557) | Xent 1.9444(2.0511) | Loss 5.6477(5.8813) | Error 0.6776(0.7026) Steps 532(548.88) | Grad Norm 6.7659(4.1374) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 49.3641(51.4532) | Bit/dim 4.6631(4.8499) | Xent 1.9233(2.0473) | Loss 5.6248(5.8736) | Error 0.6644(0.7014) Steps 532(548.38) | Grad Norm 2.4666(4.0873) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 48.9719(51.3787) | Bit/dim 4.6569(4.8441) | Xent 1.9517(2.0444) | Loss 5.6328(5.8664) | Error 0.6881(0.7010) Steps 532(547.89) | Grad Norm 6.7036(4.1657) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 50.9098(51.3647) | Bit/dim 4.6632(4.8387) | Xent 1.9291(2.0410) | Loss 5.6278(5.8592) | Error 0.6735(0.7002) Steps 532(547.41) | Grad Norm 4.8039(4.1849) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 49.2434(51.3010) | Bit/dim 4.6511(4.8331) | Xent 1.9183(2.0373) | Loss 5.6102(5.8517) | Error 0.6654(0.6992) Steps 526(546.77) | Grad Norm 3.8537(4.1750) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 51.4177(51.3045) | Bit/dim 4.6392(4.8273) | Xent 1.9165(2.0337) | Loss 5.5974(5.8441) | Error 0.6673(0.6982) Steps 532(546.32) | Grad Norm 1.8156(4.1042) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 25.6373, Epoch Time 342.1919(344.9415), Bit/dim 4.6313(best: 4.5150), Xent 1.8852, Loss 5.5739, Error 0.6483(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 52.2657(51.3334) | Bit/dim 4.6311(4.8214) | Xent 1.9211(2.0303) | Loss 5.5917(5.8365) | Error 0.6638(0.6972) Steps 532(545.89) | Grad Norm 1.9486(4.0395) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 50.9015(51.3204) | Bit/dim 4.6306(4.8157) | Xent 1.9163(2.0269) | Loss 5.5887(5.8291) | Error 0.6656(0.6962) Steps 526(545.30) | Grad Norm 2.6262(3.9971) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 49.6171(51.2693) | Bit/dim 4.6387(4.8104) | Xent 1.9035(2.0232) | Loss 5.5905(5.8219) | Error 0.6693(0.6954) Steps 526(544.72) | Grad Norm 3.5348(3.9832) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 51.9196(51.2888) | Bit/dim 4.6231(4.8047) | Xent 1.9409(2.0207) | Loss 5.5935(5.8151) | Error 0.6890(0.6952) Steps 538(544.52) | Grad Norm 3.6375(3.9729) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 49.6555(51.2398) | Bit/dim 4.6343(4.7996) | Xent 1.9005(2.0171) | Loss 5.5845(5.8082) | Error 0.6646(0.6943) Steps 526(543.96) | Grad Norm 2.1568(3.9184) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 50.7446(51.2250) | Bit/dim 4.6120(4.7940) | Xent 1.9041(2.0137) | Loss 5.5640(5.8008) | Error 0.6684(0.6935) Steps 532(543.60) | Grad Norm 3.0269(3.8916) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 24.7410, Epoch Time 345.4665(344.9573), Bit/dim 4.6187(best: 4.5150), Xent 1.8747, Loss 5.5561, Error 0.6452(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 51.0247(51.2190) | Bit/dim 4.6201(4.7888) | Xent 1.9097(2.0106) | Loss 5.5750(5.7941) | Error 0.6669(0.6927) Steps 520(542.89) | Grad Norm 2.5924(3.8527) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 50.2854(51.1909) | Bit/dim 4.6193(4.7837) | Xent 1.9021(2.0073) | Loss 5.5704(5.7874) | Error 0.6669(0.6919) Steps 532(542.57) | Grad Norm 2.9394(3.8253) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 50.2134(51.1616) | Bit/dim 4.6468(4.7796) | Xent 1.8989(2.0041) | Loss 5.5962(5.7816) | Error 0.6625(0.6911) Steps 526(542.07) | Grad Norm 4.5457(3.8469) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 49.3661(51.1078) | Bit/dim 4.6307(4.7751) | Xent 1.9168(2.0015) | Loss 5.5892(5.7759) | Error 0.6751(0.6906) Steps 526(541.59) | Grad Norm 6.0658(3.9134) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 51.2690(51.1126) | Bit/dim 4.6900(4.7726) | Xent 1.9625(2.0003) | Loss 5.6712(5.7727) | Error 0.6919(0.6906) Steps 538(541.48) | Grad Norm 8.2426(4.0433) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 49.8000(51.0732) | Bit/dim 4.6431(4.7687) | Xent 1.9058(1.9975) | Loss 5.5960(5.7674) | Error 0.6674(0.6899) Steps 514(540.66) | Grad Norm 6.4550(4.1157) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 24.1443, Epoch Time 341.9881(344.8682), Bit/dim 4.6258(best: 4.5150), Xent 1.8490, Loss 5.5504, Error 0.6350(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 50.7464(51.0634) | Bit/dim 4.6269(4.7644) | Xent 1.8822(1.9940) | Loss 5.5680(5.7614) | Error 0.6575(0.6890) Steps 520(540.04) | Grad Norm 1.7820(4.0457) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 51.3421(51.0718) | Bit/dim 4.6121(4.7599) | Xent 1.8997(1.9912) | Loss 5.5619(5.7554) | Error 0.6634(0.6882) Steps 532(539.80) | Grad Norm 5.1087(4.0775) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 50.4445(51.0529) | Bit/dim 4.6077(4.7553) | Xent 1.8755(1.9877) | Loss 5.5454(5.7491) | Error 0.6452(0.6869) Steps 526(539.38) | Grad Norm 3.0212(4.0459) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 49.4062(51.0035) | Bit/dim 4.6172(4.7512) | Xent 1.8892(1.9847) | Loss 5.5618(5.7435) | Error 0.6542(0.6859) Steps 520(538.80) | Grad Norm 1.7960(3.9784) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 50.4434(50.9867) | Bit/dim 4.6033(4.7467) | Xent 1.8850(1.9818) | Loss 5.5458(5.7376) | Error 0.6639(0.6853) Steps 532(538.60) | Grad Norm 2.5192(3.9346) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 51.1234(50.9908) | Bit/dim 4.5951(4.7422) | Xent 1.8817(1.9788) | Loss 5.5360(5.7315) | Error 0.6524(0.6843) Steps 526(538.22) | Grad Norm 2.6138(3.8950) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 24.4895, Epoch Time 343.6839(344.8327), Bit/dim 4.5955(best: 4.5150), Xent 1.8468, Loss 5.5189, Error 0.6391(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 50.9091(50.9884) | Bit/dim 4.5980(4.7378) | Xent 1.8975(1.9763) | Loss 5.5468(5.7260) | Error 0.6601(0.6835) Steps 526(537.85) | Grad Norm 3.8149(3.8926) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 51.0091(50.9890) | Bit/dim 4.5922(4.7335) | Xent 1.8795(1.9734) | Loss 5.5320(5.7202) | Error 0.6540(0.6827) Steps 532(537.68) | Grad Norm 4.8001(3.9198) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 50.7369(50.9814) | Bit/dim 4.6006(4.7295) | Xent 1.8880(1.9708) | Loss 5.5446(5.7149) | Error 0.6575(0.6819) Steps 526(537.33) | Grad Norm 6.4168(3.9947) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 49.8730(50.9482) | Bit/dim 4.6097(4.7259) | Xent 1.9065(1.9689) | Loss 5.5630(5.7104) | Error 0.6756(0.6817) Steps 520(536.81) | Grad Norm 8.5630(4.1317) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 50.3565(50.9304) | Bit/dim 4.6010(4.7221) | Xent 1.9474(1.9683) | Loss 5.5747(5.7063) | Error 0.6836(0.6818) Steps 526(536.48) | Grad Norm 10.6672(4.3278) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 51.2999(50.9415) | Bit/dim 4.6083(4.7187) | Xent 1.9404(1.9674) | Loss 5.5785(5.7025) | Error 0.6906(0.6820) Steps 532(536.35) | Grad Norm 9.5819(4.4854) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 24.2946, Epoch Time 344.0964(344.8106), Bit/dim 4.5746(best: 4.5150), Xent 1.8528, Loss 5.5010, Error 0.6481(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 50.1838(50.9188) | Bit/dim 4.5860(4.7148) | Xent 1.8942(1.9652) | Loss 5.5331(5.6974) | Error 0.6634(0.6815) Steps 532(536.22) | Grad Norm 4.6479(4.4903) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 50.6673(50.9113) | Bit/dim 4.5880(4.7110) | Xent 1.8795(1.9627) | Loss 5.5278(5.6923) | Error 0.6535(0.6806) Steps 520(535.73) | Grad Norm 4.4494(4.4891) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 50.7126(50.9053) | Bit/dim 4.6194(4.7082) | Xent 1.9411(1.9620) | Loss 5.5900(5.6892) | Error 0.6946(0.6811) Steps 520(535.26) | Grad Norm 7.4425(4.5777) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 50.7626(50.9010) | Bit/dim 4.6181(4.7055) | Xent 1.9121(1.9605) | Loss 5.5741(5.6858) | Error 0.6763(0.6809) Steps 520(534.80) | Grad Norm 10.1958(4.7462) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 51.7306(50.9259) | Bit/dim 4.7037(4.7054) | Xent 2.0607(1.9635) | Loss 5.7341(5.6872) | Error 0.7212(0.6821) Steps 532(534.72) | Grad Norm 13.4577(5.0076) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 50.1371(50.9022) | Bit/dim 4.6044(4.7024) | Xent 1.8923(1.9614) | Loss 5.5505(5.6831) | Error 0.6721(0.6818) Steps 514(534.10) | Grad Norm 5.8839(5.0339) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 23.6760, Epoch Time 344.0310(344.7872), Bit/dim 4.6517(best: 4.5150), Xent 2.0177, Loss 5.6605, Error 0.7164(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 50.2664(50.8832) | Bit/dim 4.6538(4.7010) | Xent 2.0845(1.9651) | Loss 5.6960(5.6835) | Error 0.7321(0.6833) Steps 514(533.49) | Grad Norm 14.3644(5.3138) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 51.3041(50.8958) | Bit/dim 4.7613(4.7028) | Xent 2.1537(1.9707) | Loss 5.8381(5.6881) | Error 0.7504(0.6853) Steps 532(533.45) | Grad Norm 18.5999(5.7124) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 47.2710(50.7870) | Bit/dim 4.7421(4.7039) | Xent 2.1180(1.9752) | Loss 5.8011(5.6915) | Error 0.7480(0.6872) Steps 508(532.68) | Grad Norm 10.9291(5.8689) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 49.6772(50.7537) | Bit/dim 4.6550(4.7025) | Xent 1.9773(1.9752) | Loss 5.6437(5.6901) | Error 0.6941(0.6874) Steps 502(531.76) | Grad Norm 9.3717(5.9739) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 52.7638(50.8141) | Bit/dim 4.6773(4.7017) | Xent 2.1348(1.9800) | Loss 5.7447(5.6917) | Error 0.7419(0.6891) Steps 514(531.23) | Grad Norm 7.5658(6.0217) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 50.9648(50.8186) | Bit/dim 4.7797(4.7041) | Xent 1.9484(1.9791) | Loss 5.7539(5.6936) | Error 0.7007(0.6894) Steps 532(531.25) | Grad Norm 6.1978(6.0270) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 24.0484, Epoch Time 342.1397(344.7078), Bit/dim 4.6607(best: 4.5150), Xent 2.0141, Loss 5.6677, Error 0.7174(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 51.2562(50.8317) | Bit/dim 4.6711(4.7031) | Xent 2.0545(1.9813) | Loss 5.6983(5.6937) | Error 0.7259(0.6905) Steps 532(531.28) | Grad Norm 6.3668(6.0372) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 50.6001(50.8248) | Bit/dim 4.6844(4.7025) | Xent 2.1353(1.9859) | Loss 5.7520(5.6955) | Error 0.7522(0.6924) Steps 532(531.30) | Grad Norm 9.9055(6.1532) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 52.4562(50.8737) | Bit/dim 4.7401(4.7036) | Xent 1.9489(1.9848) | Loss 5.7145(5.6961) | Error 0.6754(0.6919) Steps 550(531.86) | Grad Norm 4.8838(6.1151) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 53.7944(50.9613) | Bit/dim 4.6863(4.7031) | Xent 2.0446(1.9866) | Loss 5.7086(5.6964) | Error 0.7188(0.6927) Steps 556(532.58) | Grad Norm 5.4625(6.0956) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 51.7628(50.9854) | Bit/dim 4.7801(4.7054) | Xent 2.0919(1.9898) | Loss 5.8260(5.7003) | Error 0.7351(0.6939) Steps 514(532.03) | Grad Norm 8.1451(6.1571) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 49.7752(50.9491) | Bit/dim 4.8878(4.7109) | Xent 1.9847(1.9896) | Loss 5.8802(5.7057) | Error 0.6964(0.6940) Steps 538(532.21) | Grad Norm 5.1250(6.1261) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 25.1396, Epoch Time 350.5957(344.8844), Bit/dim 4.9886(best: 4.5150), Xent 1.9206, Loss 5.9489, Error 0.6451(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 49.6193(50.9092) | Bit/dim 4.9872(4.7192) | Xent 1.9298(1.9878) | Loss 5.9521(5.7131) | Error 0.6561(0.6929) Steps 544(532.56) | Grad Norm 4.0411(6.0635) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 51.4867(50.9265) | Bit/dim 4.8535(4.7232) | Xent 1.9761(1.9875) | Loss 5.8415(5.7170) | Error 0.7085(0.6933) Steps 532(532.54) | Grad Norm 4.0907(6.0044) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 50.5317(50.9146) | Bit/dim 4.8839(4.7280) | Xent 2.0606(1.9897) | Loss 5.9142(5.7229) | Error 0.7372(0.6947) Steps 514(531.99) | Grad Norm 8.6190(6.0828) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 53.1990(50.9832) | Bit/dim 4.8539(4.7318) | Xent 1.9729(1.9892) | Loss 5.8404(5.7264) | Error 0.6873(0.6944) Steps 562(532.89) | Grad Norm 7.4851(6.1249) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 49.4994(50.9387) | Bit/dim 4.8584(4.7356) | Xent 1.9637(1.9884) | Loss 5.8402(5.7298) | Error 0.6975(0.6945) Steps 544(533.22) | Grad Norm 5.9516(6.1197) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 47.1712(50.8256) | Bit/dim 4.8050(4.7377) | Xent 1.9476(1.9872) | Loss 5.7788(5.7313) | Error 0.6833(0.6942) Steps 484(531.74) | Grad Norm 3.0670(6.0281) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 22.3250, Epoch Time 339.5217(344.7235), Bit/dim 4.8755(best: 4.5150), Xent 1.9114, Loss 5.8312, Error 0.6637(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 47.7607(50.7337) | Bit/dim 4.8779(4.7419) | Xent 1.9572(1.9863) | Loss 5.8565(5.7350) | Error 0.6937(0.6942) Steps 490(530.49) | Grad Norm 5.7461(6.0196) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 46.2346(50.5987) | Bit/dim 4.7603(4.7425) | Xent 1.9202(1.9843) | Loss 5.7204(5.7346) | Error 0.6749(0.6936) Steps 484(529.10) | Grad Norm 4.1376(5.9632) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 49.3245(50.5605) | Bit/dim 4.7362(4.7423) | Xent 1.9357(1.9828) | Loss 5.7041(5.7337) | Error 0.6873(0.6934) Steps 514(528.64) | Grad Norm 2.5786(5.8616) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 53.1660(50.6387) | Bit/dim 4.7247(4.7417) | Xent 1.9460(1.9817) | Loss 5.6977(5.7326) | Error 0.6954(0.6935) Steps 526(528.56) | Grad Norm 2.7282(5.7676) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 55.6414(50.7887) | Bit/dim 4.7694(4.7426) | Xent 1.9578(1.9810) | Loss 5.7483(5.7331) | Error 0.6970(0.6936) Steps 556(529.39) | Grad Norm 4.6556(5.7343) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 51.2071(50.8013) | Bit/dim 4.7002(4.7413) | Xent 1.9047(1.9787) | Loss 5.6525(5.7307) | Error 0.6613(0.6926) Steps 538(529.65) | Grad Norm 3.1590(5.6570) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 23.6110, Epoch Time 342.9474(344.6703), Bit/dim 4.6686(best: 4.5150), Xent 1.8883, Loss 5.6127, Error 0.6505(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 46.7776(50.6806) | Bit/dim 4.6663(4.7390) | Xent 1.9032(1.9765) | Loss 5.6179(5.7273) | Error 0.6485(0.6913) Steps 502(528.82) | Grad Norm 2.1203(5.5509) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 45.8652(50.5361) | Bit/dim 4.6605(4.7367) | Xent 1.9211(1.9748) | Loss 5.6211(5.7241) | Error 0.6784(0.6909) Steps 484(527.47) | Grad Norm 3.2887(5.4830) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 46.0630(50.4019) | Bit/dim 4.6848(4.7351) | Xent 1.9100(1.9729) | Loss 5.6398(5.7216) | Error 0.6680(0.6902) Steps 484(526.17) | Grad Norm 3.4139(5.4210) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 48.0429(50.3312) | Bit/dim 4.6516(4.7326) | Xent 1.8870(1.9703) | Loss 5.5951(5.7178) | Error 0.6561(0.6892) Steps 490(525.08) | Grad Norm 2.9919(5.3481) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 48.9335(50.2892) | Bit/dim 4.6447(4.7300) | Xent 1.9009(1.9682) | Loss 5.5951(5.7141) | Error 0.6685(0.6886) Steps 496(524.21) | Grad Norm 3.3556(5.2883) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 48.9206(50.2482) | Bit/dim 4.6181(4.7266) | Xent 1.8782(1.9655) | Loss 5.5572(5.7094) | Error 0.6659(0.6879) Steps 490(523.18) | Grad Norm 2.2233(5.1964) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 22.6753, Epoch Time 322.8963(344.0170), Bit/dim 4.6288(best: 4.5150), Xent 1.8327, Loss 5.5451, Error 0.6386(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 48.2670(50.1887) | Bit/dim 4.6422(4.7241) | Xent 1.8595(1.9623) | Loss 5.5720(5.7053) | Error 0.6549(0.6869) Steps 496(522.37) | Grad Norm 2.8213(5.1251) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 48.4993(50.1380) | Bit/dim 4.6355(4.7214) | Xent 1.8578(1.9592) | Loss 5.5644(5.7010) | Error 0.6467(0.6857) Steps 502(521.76) | Grad Norm 2.7546(5.0540) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 47.5365(50.0600) | Bit/dim 4.6049(4.7179) | Xent 1.8595(1.9562) | Loss 5.5346(5.6960) | Error 0.6462(0.6845) Steps 496(520.98) | Grad Norm 1.5864(4.9500) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 47.1261(49.9720) | Bit/dim 4.6138(4.7148) | Xent 1.8515(1.9531) | Loss 5.5396(5.6914) | Error 0.6469(0.6834) Steps 490(520.05) | Grad Norm 3.9851(4.9210) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 48.5562(49.9295) | Bit/dim 4.6026(4.7115) | Xent 1.8522(1.9500) | Loss 5.5287(5.6865) | Error 0.6435(0.6822) Steps 502(519.51) | Grad Norm 3.3602(4.8742) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 49.0902(49.9043) | Bit/dim 4.6036(4.7082) | Xent 1.8350(1.9466) | Loss 5.5211(5.6815) | Error 0.6346(0.6808) Steps 502(518.99) | Grad Norm 3.0238(4.8187) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 22.3100, Epoch Time 327.1741(343.5118), Bit/dim 4.5998(best: 4.5150), Xent 1.7963, Loss 5.4979, Error 0.6288(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 45.7795(49.7806) | Bit/dim 4.5953(4.7048) | Xent 1.8412(1.9434) | Loss 5.5159(5.6765) | Error 0.6581(0.6801) Steps 484(517.94) | Grad Norm 5.4192(4.8367) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 48.7459(49.7495) | Bit/dim 4.5907(4.7014) | Xent 1.8413(1.9404) | Loss 5.5113(5.6716) | Error 0.6389(0.6788) Steps 496(517.28) | Grad Norm 3.9510(4.8101) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 48.6342(49.7161) | Bit/dim 4.5874(4.6980) | Xent 1.8375(1.9373) | Loss 5.5061(5.6666) | Error 0.6375(0.6776) Steps 496(516.64) | Grad Norm 2.3577(4.7366) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 46.3132(49.6140) | Bit/dim 4.5754(4.6943) | Xent 1.8277(1.9340) | Loss 5.4893(5.6613) | Error 0.6409(0.6765) Steps 484(515.66) | Grad Norm 5.4786(4.7588) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 48.0111(49.5659) | Bit/dim 4.5708(4.6906) | Xent 1.7964(1.9299) | Loss 5.4690(5.6555) | Error 0.6215(0.6748) Steps 496(515.07) | Grad Norm 2.5984(4.6940) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 48.2450(49.5263) | Bit/dim 4.5566(4.6866) | Xent 1.8222(1.9266) | Loss 5.4677(5.6499) | Error 0.6366(0.6737) Steps 496(514.50) | Grad Norm 2.9980(4.6431) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 22.5521, Epoch Time 324.3533(342.9370), Bit/dim 4.5701(best: 4.5150), Xent 1.7702, Loss 5.4552, Error 0.6194(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 48.5410(49.4967) | Bit/dim 4.5641(4.6829) | Xent 1.8155(1.9233) | Loss 5.4719(5.6446) | Error 0.6390(0.6727) Steps 484(513.59) | Grad Norm 4.1670(4.6288) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 48.6488(49.4713) | Bit/dim 4.5534(4.6790) | Xent 1.7876(1.9192) | Loss 5.4472(5.6386) | Error 0.6285(0.6713) Steps 496(513.06) | Grad Norm 2.4636(4.5639) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 47.3192(49.4067) | Bit/dim 4.5521(4.6752) | Xent 1.7946(1.9155) | Loss 5.4494(5.6330) | Error 0.6294(0.6701) Steps 490(512.37) | Grad Norm 2.0258(4.4877) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 48.3839(49.3760) | Bit/dim 4.5403(4.6712) | Xent 1.8114(1.9124) | Loss 5.4460(5.6273) | Error 0.6415(0.6692) Steps 490(511.70) | Grad Norm 3.7285(4.4650) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 48.2882(49.3434) | Bit/dim 4.5354(4.6671) | Xent 1.7928(1.9088) | Loss 5.4318(5.6215) | Error 0.6318(0.6681) Steps 496(511.22) | Grad Norm 2.7286(4.4129) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 47.9876(49.3027) | Bit/dim 4.5371(4.6632) | Xent 1.7800(1.9049) | Loss 5.4271(5.6156) | Error 0.6224(0.6667) Steps 496(510.77) | Grad Norm 2.3856(4.3521) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 22.2754, Epoch Time 327.3069(342.4681), Bit/dim 4.5319(best: 4.5150), Xent 1.7401, Loss 5.4019, Error 0.6012(best: 0.6113)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 47.8225(49.2583) | Bit/dim 4.5300(4.6592) | Xent 1.7779(1.9011) | Loss 5.4190(5.6097) | Error 0.6249(0.6655) Steps 490(510.14) | Grad Norm 2.0001(4.2815) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 48.6462(49.2400) | Bit/dim 4.5339(4.6554) | Xent 1.7849(1.8976) | Loss 5.4264(5.6042) | Error 0.6275(0.6643) Steps 490(509.54) | Grad Norm 1.6807(4.2035) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 47.9998(49.2028) | Bit/dim 4.5200(4.6514) | Xent 1.7783(1.8940) | Loss 5.4092(5.5984) | Error 0.6159(0.6629) Steps 490(508.95) | Grad Norm 1.1951(4.1132) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 47.7210(49.1583) | Bit/dim 4.5137(4.6472) | Xent 1.7866(1.8908) | Loss 5.4070(5.5927) | Error 0.6287(0.6618) Steps 484(508.21) | Grad Norm 2.1715(4.0550) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 49.0665(49.1556) | Bit/dim 4.5109(4.6432) | Xent 1.7853(1.8876) | Loss 5.4036(5.5870) | Error 0.6258(0.6608) Steps 496(507.84) | Grad Norm 3.3756(4.0346) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 48.3136(49.1303) | Bit/dim 4.5213(4.6395) | Xent 1.7806(1.8844) | Loss 5.4116(5.5817) | Error 0.6280(0.6598) Steps 484(507.12) | Grad Norm 3.6921(4.0243) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 22.5010, Epoch Time 327.8201(342.0287), Bit/dim 4.5285(best: 4.5150), Xent 1.7494, Loss 5.4032, Error 0.6059(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 48.6087(49.1146) | Bit/dim 4.5239(4.6360) | Xent 1.7939(1.8817) | Loss 5.4208(5.5769) | Error 0.6295(0.6589) Steps 496(506.79) | Grad Norm 6.8747(4.1098) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 47.8719(49.0774) | Bit/dim 4.5724(4.6341) | Xent 1.8984(1.8822) | Loss 5.5216(5.5752) | Error 0.6669(0.6591) Steps 484(506.11) | Grad Norm 15.2579(4.4443) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 47.8244(49.0398) | Bit/dim 4.6876(4.6357) | Xent 2.3053(1.8949) | Loss 5.8402(5.5832) | Error 0.7522(0.6619) Steps 496(505.80) | Grad Norm 32.1404(5.2752) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 46.7863(48.9722) | Bit/dim 5.1018(4.6497) | Xent 2.8308(1.9230) | Loss 6.5173(5.6112) | Error 0.7920(0.6658) Steps 508(505.87) | Grad Norm 25.9782(5.8962) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 46.7549(48.9056) | Bit/dim 4.9638(4.6591) | Xent 1.9322(1.9233) | Loss 5.9299(5.6208) | Error 0.6798(0.6662) Steps 508(505.93) | Grad Norm 8.8646(5.9853) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 47.5878(48.8661) | Bit/dim 4.7923(4.6631) | Xent 1.8672(1.9216) | Loss 5.7259(5.6239) | Error 0.6518(0.6658) Steps 508(506.00) | Grad Norm 4.4245(5.9385) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 23.4260, Epoch Time 324.7402(341.5100), Bit/dim 4.8769(best: 4.5150), Xent 1.9471, Loss 5.8504, Error 0.6869(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 48.8590(48.8659) | Bit/dim 4.8788(4.6696) | Xent 2.0146(1.9244) | Loss 5.8861(5.6318) | Error 0.6972(0.6667) Steps 496(505.70) | Grad Norm 13.6696(6.1704) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 49.3975(48.8818) | Bit/dim 4.9000(4.6765) | Xent 1.9304(1.9246) | Loss 5.8652(5.6388) | Error 0.6801(0.6671) Steps 544(506.84) | Grad Norm 8.4311(6.2382) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 48.4268(48.8682) | Bit/dim 4.8328(4.6812) | Xent 1.9236(1.9245) | Loss 5.7946(5.6435) | Error 0.6665(0.6671) Steps 544(507.96) | Grad Norm 7.1393(6.2653) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 49.6605(48.8920) | Bit/dim 4.6605(4.6806) | Xent 1.9796(1.9262) | Loss 5.6503(5.6437) | Error 0.7163(0.6686) Steps 526(508.50) | Grad Norm 3.4987(6.1823) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 47.9077(48.8624) | Bit/dim 4.7463(4.6826) | Xent 2.0307(1.9293) | Loss 5.7616(5.6472) | Error 0.7141(0.6700) Steps 496(508.13) | Grad Norm 8.2822(6.2453) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 48.9296(48.8645) | Bit/dim 4.7486(4.6845) | Xent 1.9710(1.9306) | Loss 5.7341(5.6498) | Error 0.7030(0.6710) Steps 526(508.66) | Grad Norm 3.9171(6.1754) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 25.3926, Epoch Time 334.1893(341.2904), Bit/dim 4.7837(best: 4.5150), Xent 2.0059, Loss 5.7867, Error 0.7122(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 50.2913(48.9073) | Bit/dim 4.7860(4.6876) | Xent 2.0245(1.9334) | Loss 5.7983(5.6543) | Error 0.7151(0.6723) Steps 544(509.72) | Grad Norm 4.6005(6.1282) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 51.7937(48.9939) | Bit/dim 4.7013(4.6880) | Xent 2.0212(1.9360) | Loss 5.7119(5.6560) | Error 0.7210(0.6737) Steps 550(510.93) | Grad Norm 2.9717(6.0335) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 52.3416(49.0943) | Bit/dim 4.6845(4.6879) | Xent 1.9689(1.9370) | Loss 5.6690(5.6564) | Error 0.7030(0.6746) Steps 544(511.92) | Grad Norm 4.2957(5.9813) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 51.8986(49.1784) | Bit/dim 4.6659(4.6872) | Xent 1.9679(1.9379) | Loss 5.6499(5.6562) | Error 0.6996(0.6754) Steps 556(513.24) | Grad Norm 2.7899(5.8856) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 51.9209(49.2607) | Bit/dim 4.6972(4.6875) | Xent 1.9787(1.9392) | Loss 5.6866(5.6571) | Error 0.7099(0.6764) Steps 556(514.53) | Grad Norm 2.6598(5.7888) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 51.4004(49.3249) | Bit/dim 4.6736(4.6871) | Xent 1.9601(1.9398) | Loss 5.6537(5.6570) | Error 0.6970(0.6770) Steps 550(515.59) | Grad Norm 2.1665(5.6801) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 24.5027, Epoch Time 349.8663(341.5477), Bit/dim 4.6327(best: 4.5150), Xent 1.9295, Loss 5.5975, Error 0.6800(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 51.7835(49.3986) | Bit/dim 4.6260(4.6853) | Xent 1.9549(1.9402) | Loss 5.6034(5.6554) | Error 0.6881(0.6774) Steps 550(516.62) | Grad Norm 1.9675(5.5688) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 51.6857(49.4672) | Bit/dim 4.6166(4.6832) | Xent 1.9449(1.9404) | Loss 5.5891(5.6534) | Error 0.6847(0.6776) Steps 550(517.62) | Grad Norm 1.9700(5.4608) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 50.0361(49.4843) | Bit/dim 4.6154(4.6812) | Xent 1.9540(1.9408) | Loss 5.5924(5.6516) | Error 0.6836(0.6778) Steps 526(517.88) | Grad Norm 2.4491(5.3705) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 49.5443(49.4861) | Bit/dim 4.6174(4.6793) | Xent 1.9392(1.9407) | Loss 5.5870(5.6496) | Error 0.6831(0.6779) Steps 532(518.30) | Grad Norm 1.8038(5.2635) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 48.3436(49.4518) | Bit/dim 4.6061(4.6771) | Xent 1.9127(1.9399) | Loss 5.5624(5.6470) | Error 0.6721(0.6777) Steps 520(518.35) | Grad Norm 2.2486(5.1730) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 47.1228(49.3820) | Bit/dim 4.5780(4.6741) | Xent 1.9080(1.9389) | Loss 5.5320(5.6436) | Error 0.6699(0.6775) Steps 514(518.22) | Grad Norm 1.4980(5.0628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 24.4403, Epoch Time 338.6892(341.4619), Bit/dim 4.5723(best: 4.5150), Xent 1.8770, Loss 5.5108, Error 0.6513(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 48.3497(49.3510) | Bit/dim 4.5710(4.6710) | Xent 1.9078(1.9380) | Loss 5.5249(5.6400) | Error 0.6711(0.6773) Steps 526(518.45) | Grad Norm 2.3734(4.9821) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 48.7205(49.3321) | Bit/dim 4.5576(4.6676) | Xent 1.9224(1.9375) | Loss 5.5188(5.6364) | Error 0.6761(0.6773) Steps 520(518.50) | Grad Norm 1.5760(4.8799) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 47.5425(49.2784) | Bit/dim 4.5669(4.6646) | Xent 1.9152(1.9369) | Loss 5.5245(5.6330) | Error 0.6795(0.6773) Steps 514(518.37) | Grad Norm 1.8294(4.7884) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 48.8717(49.2662) | Bit/dim 4.5578(4.6614) | Xent 1.8890(1.9354) | Loss 5.5023(5.6291) | Error 0.6706(0.6771) Steps 508(518.05) | Grad Norm 1.4676(4.6888) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 49.1330(49.2622) | Bit/dim 4.5291(4.6574) | Xent 1.8923(1.9341) | Loss 5.4753(5.6245) | Error 0.6646(0.6768) Steps 514(517.93) | Grad Norm 1.3110(4.5874) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 47.4360(49.2074) | Bit/dim 4.5415(4.6539) | Xent 1.8777(1.9324) | Loss 5.4803(5.6202) | Error 0.6621(0.6763) Steps 508(517.63) | Grad Norm 2.1749(4.5150) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 22.8289, Epoch Time 328.7035(341.0792), Bit/dim 4.5298(best: 4.5150), Xent 1.8417, Loss 5.4507, Error 0.6418(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 48.3512(49.1817) | Bit/dim 4.5279(4.6501) | Xent 1.8579(1.9302) | Loss 5.4568(5.6153) | Error 0.6535(0.6756) Steps 514(517.53) | Grad Norm 1.3689(4.4207) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 47.1089(49.1195) | Bit/dim 4.5179(4.6462) | Xent 1.8893(1.9290) | Loss 5.4626(5.6107) | Error 0.6619(0.6752) Steps 508(517.24) | Grad Norm 2.1422(4.3523) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 48.4844(49.1005) | Bit/dim 4.5118(4.6421) | Xent 1.8737(1.9273) | Loss 5.4486(5.6058) | Error 0.6663(0.6750) Steps 520(517.32) | Grad Norm 1.3193(4.2613) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 47.3976(49.0494) | Bit/dim 4.5095(4.6382) | Xent 1.8457(1.9249) | Loss 5.4324(5.6006) | Error 0.6542(0.6743) Steps 508(517.04) | Grad Norm 1.2435(4.1708) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 47.2983(48.9969) | Bit/dim 4.5112(4.6344) | Xent 1.8651(1.9231) | Loss 5.4438(5.5959) | Error 0.6544(0.6737) Steps 496(516.41) | Grad Norm 1.5023(4.0907) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 47.9569(48.9657) | Bit/dim 4.5064(4.6305) | Xent 1.8562(1.9211) | Loss 5.4344(5.5911) | Error 0.6510(0.6731) Steps 508(516.16) | Grad Norm 1.5113(4.0133) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 23.6802, Epoch Time 326.0102(340.6271), Bit/dim 4.4999(best: 4.5150), Xent 1.8190, Loss 5.4094, Error 0.6355(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 48.2759(48.9450) | Bit/dim 4.4954(4.6265) | Xent 1.8526(1.9190) | Loss 5.4217(5.5860) | Error 0.6511(0.6724) Steps 526(516.45) | Grad Norm 1.6637(3.9429) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 47.0868(48.8892) | Bit/dim 4.4916(4.6224) | Xent 1.8563(1.9171) | Loss 5.4198(5.5810) | Error 0.6587(0.6720) Steps 502(516.02) | Grad Norm 1.4878(3.8692) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 47.6887(48.8532) | Bit/dim 4.4903(4.6185) | Xent 1.8447(1.9150) | Loss 5.4127(5.5759) | Error 0.6472(0.6713) Steps 514(515.96) | Grad Norm 2.2735(3.8213) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 47.6367(48.8167) | Bit/dim 4.4944(4.6147) | Xent 1.8487(1.9130) | Loss 5.4188(5.5712) | Error 0.6490(0.6706) Steps 508(515.72) | Grad Norm 3.1383(3.8008) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 47.3405(48.7724) | Bit/dim 4.4847(4.6108) | Xent 1.8360(1.9107) | Loss 5.4027(5.5662) | Error 0.6485(0.6699) Steps 514(515.67) | Grad Norm 2.6090(3.7651) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 47.6409(48.7385) | Bit/dim 4.4753(4.6068) | Xent 1.8478(1.9088) | Loss 5.3992(5.5612) | Error 0.6469(0.6692) Steps 514(515.62) | Grad Norm 2.6055(3.7303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Iter 0710 | Time 46.8412(48.6735) | Bit/dim 4.4523(4.5983) | Xent 1.8306(1.9048) | Loss 5.3676(5.5507) | Error 0.6501(0.6681) Steps 508(515.52) | Grad Norm 2.2301(3.6613) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 47.2710(48.6314) | Bit/dim 4.4674(4.5944) | Xent 1.8169(1.9021) | Loss 5.3758(5.5455) | Error 0.6356(0.6671) Steps 514(515.47) | Grad Norm 1.4443(3.5948) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 47.8106(48.6068) | Bit/dim 4.4648(4.5905) | Xent 1.8077(1.8993) | Loss 5.3686(5.5402) | Error 0.6338(0.6661) Steps 502(515.07) | Grad Norm 0.7894(3.5106) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 47.4947(48.5734) | Bit/dim 4.4648(4.5867) | Xent 1.8157(1.8968) | Loss 5.3726(5.5351) | Error 0.6422(0.6654) Steps 502(514.68) | Grad Norm 1.4802(3.4497) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 48.3148(48.5657) | Bit/dim 4.4508(4.5827) | Xent 1.8381(1.8950) | Loss 5.3699(5.5302) | Error 0.6529(0.6650) Steps 526(515.02) | Grad Norm 2.6769(3.4265) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 22.8706, Epoch Time 324.8319(339.7088), Bit/dim 4.4542(best: 4.4721), Xent 1.7923, Loss 5.3504, Error 0.6294(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 46.8301(48.5136) | Bit/dim 4.4509(4.5787) | Xent 1.8241(1.8929) | Loss 5.3629(5.5252) | Error 0.6426(0.6644) Steps 508(514.81) | Grad Norm 3.5507(3.4302) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 47.2540(48.4758) | Bit/dim 4.4550(4.5750) | Xent 1.8150(1.8906) | Loss 5.3625(5.5203) | Error 0.6374(0.6636) Steps 508(514.60) | Grad Norm 4.2479(3.4548) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 47.2641(48.4395) | Bit/dim 4.4561(4.5714) | Xent 1.8300(1.8887) | Loss 5.3712(5.5158) | Error 0.6404(0.6629) Steps 502(514.22) | Grad Norm 5.4327(3.5141) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 48.4619(48.4401) | Bit/dim 4.4699(4.5684) | Xent 1.8228(1.8868) | Loss 5.3813(5.5118) | Error 0.6349(0.6620) Steps 526(514.58) | Grad Norm 6.3154(3.5981) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 46.6788(48.3873) | Bit/dim 4.4570(4.5650) | Xent 1.8492(1.8856) | Loss 5.3816(5.5079) | Error 0.6500(0.6617) Steps 502(514.20) | Grad Norm 6.2756(3.6785) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 47.8959(48.3726) | Bit/dim 4.4596(4.5619) | Xent 1.8347(1.8841) | Loss 5.3769(5.5039) | Error 0.6462(0.6612) Steps 526(514.55) | Grad Norm 5.1929(3.7239) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 22.8916, Epoch Time 322.9785(339.2069), Bit/dim 4.4458(best: 4.4542), Xent 1.7614, Loss 5.3265, Error 0.6127(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 46.7920(48.3251) | Bit/dim 4.4537(4.5586) | Xent 1.7962(1.8815) | Loss 5.3518(5.4994) | Error 0.6307(0.6603) Steps 502(514.18) | Grad Norm 2.9525(3.7008) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 48.5863(48.3330) | Bit/dim 4.4303(4.5548) | Xent 1.8133(1.8794) | Loss 5.3369(5.4945) | Error 0.6346(0.6595) Steps 514(514.17) | Grad Norm 3.5953(3.6976) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 47.7643(48.3159) | Bit/dim 4.4444(4.5515) | Xent 1.8050(1.8772) | Loss 5.3469(5.4901) | Error 0.6382(0.6589) Steps 526(514.53) | Grad Norm 5.5879(3.7543) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 46.2858(48.2550) | Bit/dim 4.4359(4.5480) | Xent 1.8536(1.8765) | Loss 5.3627(5.4862) | Error 0.6574(0.6588) Steps 502(514.15) | Grad Norm 6.4051(3.8338) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 48.3319(48.2573) | Bit/dim 4.4356(4.5446) | Xent 1.8173(1.8747) | Loss 5.3443(5.4820) | Error 0.6429(0.6584) Steps 526(514.51) | Grad Norm 4.2869(3.8474) | Total Time 14.00(14.00)\n",
      "Iter 0727 | Time 46.8310(48.1724) | Bit/dim 4.4308(4.5374) | Xent 1.8062(1.8705) | Loss 5.3340(5.4726) | Error 0.6381(0.6566) Steps 496(513.76) | Grad Norm 4.7311(3.7946) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 48.2118(48.1736) | Bit/dim 4.4301(4.5341) | Xent 1.8346(1.8694) | Loss 5.3474(5.4688) | Error 0.6407(0.6562) Steps 526(514.13) | Grad Norm 6.6318(3.8797) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 47.1994(48.1444) | Bit/dim 4.4158(4.5306) | Xent 1.8508(1.8689) | Loss 5.3412(5.4650) | Error 0.6512(0.6560) Steps 502(513.77) | Grad Norm 5.6301(3.9322) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 48.2411(48.1473) | Bit/dim 4.4199(4.5273) | Xent 1.7986(1.8668) | Loss 5.3192(5.4606) | Error 0.6324(0.6553) Steps 502(513.41) | Grad Norm 3.9212(3.9319) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 48.3312(48.1528) | Bit/dim 4.4268(4.5242) | Xent 1.8067(1.8650) | Loss 5.3302(5.4567) | Error 0.6382(0.6548) Steps 538(514.15) | Grad Norm 5.1400(3.9681) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 47.1111(48.1215) | Bit/dim 4.4476(4.5219) | Xent 1.8185(1.8636) | Loss 5.3569(5.4537) | Error 0.6404(0.6544) Steps 496(513.61) | Grad Norm 5.9015(4.0261) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 24.8452, Epoch Time 326.7005(338.3555), Bit/dim 4.4524(best: 4.4220), Xent 1.7522, Loss 5.3285, Error 0.6098(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 48.1793(48.1233) | Bit/dim 4.4465(4.5197) | Xent 1.7876(1.8613) | Loss 5.3403(5.4503) | Error 0.6294(0.6536) Steps 538(514.34) | Grad Norm 4.5373(4.0415) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 47.4644(48.1035) | Bit/dim 4.4142(4.5165) | Xent 1.7729(1.8586) | Loss 5.3006(5.4458) | Error 0.6261(0.6528) Steps 508(514.15) | Grad Norm 1.3789(3.9616) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 48.5458(48.1168) | Bit/dim 4.4311(4.5140) | Xent 1.7955(1.8567) | Loss 5.3288(5.4423) | Error 0.6329(0.6522) Steps 502(513.78) | Grad Norm 4.3145(3.9722) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 48.5082(48.1285) | Bit/dim 4.4308(4.5115) | Xent 1.8183(1.8556) | Loss 5.3399(5.4393) | Error 0.6438(0.6519) Steps 520(513.97) | Grad Norm 4.7980(3.9970) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 48.1377(48.1288) | Bit/dim 4.3942(4.5079) | Xent 1.7713(1.8531) | Loss 5.2799(5.4345) | Error 0.6208(0.6510) Steps 508(513.79) | Grad Norm 2.3941(3.9489) | Total Time 14.00(14.00)\n",
      "Iter 0738 | Time 46.7827(48.0884) | Bit/dim 4.4157(4.5052) | Xent 1.7970(1.8514) | Loss 5.3142(5.4309) | Error 0.6310(0.6504) Steps 502(513.44) | Grad Norm 5.3833(3.9919) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 25.3762, Epoch Time 328.7770(338.0682), Bit/dim 4.4505(best: 4.4220), Xent 1.7628, Loss 5.3319, Error 0.6096(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 49.1654(48.1207) | Bit/dim 4.4432(4.5033) | Xent 1.7783(1.8492) | Loss 5.3324(5.4279) | Error 0.6191(0.6495) Steps 544(514.35) | Grad Norm 6.9293(4.0800) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 47.5957(48.1050) | Bit/dim 4.4084(4.5005) | Xent 1.8198(1.8483) | Loss 5.3183(5.4246) | Error 0.6431(0.6493) Steps 508(514.16) | Grad Norm 5.8537(4.1332) | Total Time 14.00(14.00)\n",
      "Iter 0741 | Time 46.6070(48.0600) | Bit/dim 4.4327(4.4984) | Xent 1.8755(1.8491) | Loss 5.3705(5.4230) | Error 0.6532(0.6494) Steps 508(513.98) | Grad Norm 9.5206(4.2949) | Total Time 14.00(14.00)\n",
      "Iter 0742 | Time 47.5892(48.0459) | Bit/dim 4.4484(4.4969) | Xent 2.0483(1.8551) | Loss 5.4725(5.4245) | Error 0.7120(0.6513) Steps 514(513.98) | Grad Norm 10.9840(4.4955) | Total Time 14.00(14.00)\n",
      "Iter 0743 | Time 47.7674(48.0376) | Bit/dim 4.4163(4.4945) | Xent 1.7928(1.8532) | Loss 5.3127(5.4211) | Error 0.6301(0.6506) Steps 520(514.16) | Grad Norm 5.7106(4.5320) | Total Time 14.00(14.00)\n",
      "Iter 0744 | Time 47.4859(48.0210) | Bit/dim 4.4397(4.4929) | Xent 1.8367(1.8527) | Loss 5.3581(5.4192) | Error 0.6430(0.6504) Steps 496(513.61) | Grad Norm 6.1996(4.5820) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 22.4839, Epoch Time 324.5195(337.6617), Bit/dim 4.4263(best: 4.4220), Xent 1.8322, Loss 5.3424, Error 0.6546(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 47.6781(48.0107) | Bit/dim 4.4286(4.4909) | Xent 1.8776(1.8535) | Loss 5.3674(5.4177) | Error 0.6680(0.6509) Steps 502(513.27) | Grad Norm 7.5401(4.6708) | Total Time 14.00(14.00)\n",
      "Iter 0746 | Time 49.3502(48.0509) | Bit/dim 4.4682(4.4903) | Xent 1.7938(1.8517) | Loss 5.3651(5.4161) | Error 0.6306(0.6503) Steps 544(514.19) | Grad Norm 6.4320(4.7236) | Total Time 14.00(14.00)\n",
      "Iter 0747 | Time 47.9909(48.0491) | Bit/dim 4.4137(4.4880) | Xent 1.8078(1.8504) | Loss 5.3176(5.4131) | Error 0.6418(0.6501) Steps 514(514.18) | Grad Norm 2.2327(4.6489) | Total Time 14.00(14.00)\n",
      "Iter 0748 | Time 46.4450(48.0010) | Bit/dim 4.4430(4.4866) | Xent 1.8241(1.8496) | Loss 5.3550(5.4114) | Error 0.6447(0.6499) Steps 496(513.64) | Grad Norm 5.5920(4.6772) | Total Time 14.00(14.00)\n",
      "Iter 0749 | Time 48.3140(48.0104) | Bit/dim 4.4565(4.4857) | Xent 1.8003(1.8481) | Loss 5.3566(5.4098) | Error 0.6378(0.6495) Steps 544(514.55) | Grad Norm 4.3194(4.6664) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 49.7658(48.0630) | Bit/dim 4.4047(4.4833) | Xent 1.8048(1.8468) | Loss 5.3071(5.4067) | Error 0.6334(0.6491) Steps 538(515.25) | Grad Norm 3.1898(4.6221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 22.6307, Epoch Time 328.3346(337.3819), Bit/dim 4.4378(best: 4.4220), Xent 1.7646, Loss 5.3201, Error 0.6286(best: 0.6012)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 47.4358(48.0442) | Bit/dim 4.4296(4.4817) | Xent 1.8038(1.8455) | Loss 5.3315(5.4044) | Error 0.6359(0.6487) Steps 496(514.67) | Grad Norm 6.0670(4.6655) | Total Time 14.00(14.00)\n",
      "Iter 0752 | Time 47.4151(48.0253) | Bit/dim 4.4209(4.4798) | Xent 1.7915(1.8439) | Loss 5.3166(5.4018) | Error 0.6300(0.6481) Steps 526(515.01) | Grad Norm 3.0348(4.6166) | Total Time 14.00(14.00)\n",
      "Iter 0753 | Time 48.4692(48.0387) | Bit/dim 4.3997(4.4774) | Xent 1.7943(1.8424) | Loss 5.2969(5.3986) | Error 0.6327(0.6476) Steps 526(515.34) | Grad Norm 3.5929(4.5858) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_sn100_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 100 \n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
