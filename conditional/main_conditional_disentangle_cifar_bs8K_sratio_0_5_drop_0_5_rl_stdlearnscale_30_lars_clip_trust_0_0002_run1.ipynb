{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_lars.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import apex\n",
      "from apex.parallel.LARC import LARC\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "# for lars\n",
      "parser.add_argument(\"--trust_coefficient\", type=float, default=0.02)\n",
      "parser.add_argument('--clip', type=eval, default=False, choices=[True, False])\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    optimizer = LARC(optimizer, trust_coefficient=args.trust_coefficient, clip=args.clip, eps=1e-8)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.optim.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, clip=True, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.01, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_0002_run1', scale=1.0, scale_fac=1.0, scale_std=30.0, seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', trust_coefficient=0.0002, val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0001 | Time 99.2801(99.2801) | Bit/dim 8.8974(8.8974) | Xent 2.3026(2.3026) | Loss 20.9455(20.9455) | Error 0.8982(0.8982) Steps 430(430.00) | Grad Norm 27.4564(27.4564) | Total Time 0.00(0.00)\n",
      "Iter 0002 | Time 39.8442(97.4970) | Bit/dim 8.8295(8.8954) | Xent 2.2924(2.3023) | Loss 20.7812(20.9405) | Error 0.7836(0.8948) Steps 478(431.44) | Grad Norm 24.9296(27.3806) | Total Time 0.00(0.00)\n",
      "Iter 0003 | Time 36.6001(95.6701) | Bit/dim 8.7462(8.8909) | Xent 2.2830(2.3017) | Loss 20.6537(20.9319) | Error 0.7722(0.8911) Steps 490(433.20) | Grad Norm 20.9582(27.1879) | Total Time 0.00(0.00)\n",
      "Iter 0004 | Time 37.4714(93.9241) | Bit/dim 8.6350(8.8832) | Xent 2.2736(2.3009) | Loss 19.9343(20.9020) | Error 0.7684(0.8875) Steps 466(434.18) | Grad Norm 16.3106(26.8616) | Total Time 0.00(0.00)\n",
      "Iter 0005 | Time 36.1024(92.1895) | Bit/dim 8.5964(8.8746) | Xent 2.2652(2.2998) | Loss 20.0940(20.8778) | Error 0.7692(0.8839) Steps 442(434.42) | Grad Norm 11.2352(26.3928) | Total Time 0.00(0.00)\n",
      "Iter 0006 | Time 36.3199(90.5134) | Bit/dim 8.4965(8.8633) | Xent 2.2540(2.2984) | Loss 19.8162(20.8459) | Error 0.7536(0.8800) Steps 454(435.00) | Grad Norm 8.3528(25.8516) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 34.2390, Epoch Time 336.3846(336.3846), Bit/dim 8.4561(best: inf), Xent 2.2440, Loss 9.5781, Error 0.7477(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0007 | Time 47.7256(89.2297) | Bit/dim 8.4636(8.8513) | Xent 2.2450(2.2968) | Loss 22.4185(20.8931) | Error 0.7580(0.8763) Steps 460(435.75) | Grad Norm 8.1088(25.3193) | Total Time 0.00(0.00)\n",
      "Iter 0008 | Time 36.8466(87.6583) | Bit/dim 8.4286(8.8386) | Xent 2.2387(2.2951) | Loss 19.9095(20.8636) | Error 0.7618(0.8729) Steps 460(436.48) | Grad Norm 10.8120(24.8841) | Total Time 0.00(0.00)\n",
      "Iter 0009 | Time 38.3919(86.1803) | Bit/dim 8.4143(8.8259) | Xent 2.2292(2.2931) | Loss 19.3675(20.8187) | Error 0.7619(0.8696) Steps 448(436.83) | Grad Norm 13.3395(24.5378) | Total Time 0.00(0.00)\n",
      "Iter 0010 | Time 35.5258(84.6606) | Bit/dim 8.3786(8.8124) | Xent 2.2233(2.2910) | Loss 19.6148(20.7826) | Error 0.7757(0.8668) Steps 454(437.34) | Grad Norm 15.0010(24.2517) | Total Time 0.00(0.00)\n",
      "Iter 0011 | Time 33.9599(83.1396) | Bit/dim 8.3520(8.7986) | Xent 2.2189(2.2888) | Loss 19.5851(20.7467) | Error 0.7796(0.8641) Steps 430(437.12) | Grad Norm 15.7517(23.9967) | Total Time 0.00(0.00)\n",
      "Iter 0012 | Time 37.0946(81.7583) | Bit/dim 8.2679(8.7827) | Xent 2.2137(2.2866) | Loss 19.5458(20.7106) | Error 0.7821(0.8617) Steps 466(437.99) | Grad Norm 15.0721(23.7289) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 18.6974, Epoch Time 264.2918(334.2219), Bit/dim 8.2094(best: 8.4561), Xent 2.2022, Loss 9.3105, Error 0.7676(best: 0.7477)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0013 | Time 38.8470(80.4709) | Bit/dim 8.2483(8.7667) | Xent 2.2013(2.2840) | Loss 21.7561(20.7420) | Error 0.7702(0.8589) Steps 490(439.55) | Grad Norm 13.5047(23.4222) | Total Time 0.00(0.00)\n",
      "Iter 0014 | Time 34.8344(79.1018) | Bit/dim 8.1358(8.7478) | Xent 2.1997(2.2815) | Loss 18.9450(20.6881) | Error 0.7681(0.8562) Steps 430(439.26) | Grad Norm 10.7637(23.0425) | Total Time 0.00(0.00)\n",
      "Iter 0015 | Time 37.4418(77.8520) | Bit/dim 8.0908(8.7280) | Xent 2.1920(2.2788) | Loss 18.5845(20.6250) | Error 0.7534(0.8531) Steps 454(439.70) | Grad Norm 7.9795(22.5906) | Total Time 0.00(0.00)\n",
      "Iter 0016 | Time 36.5404(76.6127) | Bit/dim 7.9737(8.7054) | Xent 2.1842(2.2760) | Loss 18.9501(20.5747) | Error 0.7475(0.8500) Steps 418(439.05) | Grad Norm 5.9676(22.0919) | Total Time 0.00(0.00)\n",
      "Iter 0017 | Time 38.9598(75.4831) | Bit/dim 7.9203(8.6819) | Xent 2.1784(2.2730) | Loss 18.5223(20.5132) | Error 0.7424(0.8467) Steps 460(439.68) | Grad Norm 6.7432(21.6314) | Total Time 0.00(0.00)\n",
      "Iter 0018 | Time 36.2700(74.3067) | Bit/dim 7.8689(8.6575) | Xent 2.1744(2.2701) | Loss 18.8878(20.4644) | Error 0.7335(0.8433) Steps 466(440.47) | Grad Norm 8.4555(21.2361) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 18.4110, Epoch Time 257.3852(331.9168), Bit/dim 7.7927(best: 8.2094), Xent 2.1667, Loss 8.8760, Error 0.7389(best: 0.7477)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0019 | Time 41.4295(73.3204) | Bit/dim 7.7988(8.6317) | Xent 2.1718(2.2671) | Loss 21.2907(20.4892) | Error 0.7449(0.8404) Steps 490(441.96) | Grad Norm 9.6396(20.8882) | Total Time 0.00(0.00)\n",
      "Iter 0020 | Time 38.5831(72.2783) | Bit/dim 7.7202(8.6044) | Xent 2.1704(2.2642) | Loss 18.3378(20.4247) | Error 0.7448(0.8375) Steps 442(441.96) | Grad Norm 9.5055(20.5468) | Total Time 0.00(0.00)\n",
      "Iter 0021 | Time 39.3330(71.2899) | Bit/dim 7.6460(8.5756) | Xent 2.1657(2.2613) | Loss 17.6686(20.3420) | Error 0.7332(0.8344) Steps 436(441.78) | Grad Norm 7.8544(20.1660) | Total Time 0.00(0.00)\n",
      "Iter 0022 | Time 37.0141(70.2616) | Bit/dim 7.5544(8.5450) | Xent 2.1578(2.2582) | Loss 18.0658(20.2737) | Error 0.7323(0.8313) Steps 454(442.15) | Grad Norm 6.1458(19.7454) | Total Time 0.00(0.00)\n",
      "Iter 0023 | Time 36.9711(69.2629) | Bit/dim 7.4694(8.5127) | Xent 2.1590(2.2552) | Loss 17.3057(20.1846) | Error 0.7289(0.8282) Steps 424(441.60) | Grad Norm 4.7728(19.2962) | Total Time 0.00(0.00)\n",
      "Iter 0024 | Time 38.2822(68.3335) | Bit/dim 7.4069(8.4795) | Xent 2.1630(2.2524) | Loss 17.8228(20.1138) | Error 0.7363(0.8255) Steps 442(441.61) | Grad Norm 5.6503(18.8868) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 19.4958, Epoch Time 267.2914(329.9780), Bit/dim 7.3517(best: 7.7927), Xent 2.1578, Loss 8.4306, Error 0.7284(best: 0.7389)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0025 | Time 44.4151(67.6159) | Bit/dim 7.3534(8.4458) | Xent 2.1569(2.2496) | Loss 20.5323(20.1263) | Error 0.7359(0.8228) Steps 424(441.08) | Grad Norm 7.0121(18.5306) | Total Time 0.00(0.00)\n",
      "Iter 0026 | Time 40.7559(66.8101) | Bit/dim 7.2987(8.4113) | Xent 2.1657(2.2470) | Loss 17.5851(20.0501) | Error 0.7516(0.8207) Steps 484(442.37) | Grad Norm 7.9067(18.2119) | Total Time 0.00(0.00)\n",
      "Iter 0027 | Time 38.1104(65.9491) | Bit/dim 7.2396(8.3762) | Xent 2.1672(2.2447) | Loss 17.6049(19.9768) | Error 0.7521(0.8186) Steps 466(443.08) | Grad Norm 7.5963(17.8934) | Total Time 0.00(0.00)\n",
      "Iter 0028 | Time 38.6060(65.1289) | Bit/dim 7.1939(8.3407) | Xent 2.1691(2.2424) | Loss 17.3550(19.8981) | Error 0.7345(0.8161) Steps 478(444.13) | Grad Norm 6.4096(17.5489) | Total Time 0.00(0.00)\n",
      "Iter 0029 | Time 42.4651(64.4489) | Bit/dim 7.1575(8.3052) | Xent 2.1668(2.2401) | Loss 17.5704(19.8283) | Error 0.7185(0.8132) Steps 478(445.14) | Grad Norm 4.3684(17.1535) | Total Time 0.00(0.00)\n",
      "Iter 0030 | Time 40.3474(63.7259) | Bit/dim 7.1123(8.2694) | Xent 2.1710(2.2380) | Loss 17.0850(19.7460) | Error 0.7232(0.8105) Steps 454(445.41) | Grad Norm 3.6006(16.7469) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 19.4926, Epoch Time 279.8910(328.4754), Bit/dim 7.0958(best: 7.3517), Xent 2.1700, Loss 8.1808, Error 0.7216(best: 0.7284)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0031 | Time 38.9950(62.9840) | Bit/dim 7.0962(8.2342) | Xent 2.1764(2.2362) | Loss 19.6562(19.7433) | Error 0.7351(0.8082) Steps 442(445.31) | Grad Norm 5.1400(16.3987) | Total Time 0.00(0.00)\n",
      "Iter 0032 | Time 38.4737(62.2487) | Bit/dim 7.0758(8.1995) | Xent 2.1796(2.2345) | Loss 17.0647(19.6629) | Error 0.7396(0.8061) Steps 466(445.93) | Grad Norm 5.7487(16.0792) | Total Time 0.00(0.00)\n",
      "Iter 0033 | Time 40.2291(61.5881) | Bit/dim 7.0517(8.1651) | Xent 2.1770(2.2328) | Loss 16.8473(19.5785) | Error 0.7264(0.8037) Steps 442(445.81) | Grad Norm 4.8437(15.7421) | Total Time 0.00(0.00)\n",
      "Iter 0034 | Time 37.8830(60.8769) | Bit/dim 7.0374(8.1312) | Xent 2.1788(2.2312) | Loss 16.8963(19.4980) | Error 0.7338(0.8016) Steps 466(446.42) | Grad Norm 2.7005(15.3509) | Total Time 0.00(0.00)\n",
      "Iter 0035 | Time 39.1876(60.2262) | Bit/dim 7.0227(8.0980) | Xent 2.1789(2.2296) | Loss 17.1374(19.4272) | Error 0.7655(0.8006) Steps 496(447.90) | Grad Norm 4.5428(15.0266) | Total Time 0.00(0.00)\n",
      "Iter 0036 | Time 36.9374(59.5276) | Bit/dim 7.0158(8.0655) | Xent 2.1847(2.2282) | Loss 16.9305(19.3523) | Error 0.7826(0.8000) Steps 454(448.09) | Grad Norm 6.0126(14.7562) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 20.7357, Epoch Time 268.6074(326.6793), Bit/dim 7.0059(best: 7.0958), Xent 2.1780, Loss 8.0949, Error 0.7621(best: 0.7216)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0037 | Time 39.4628(58.9256) | Bit/dim 7.0030(8.0336) | Xent 2.1768(2.2267) | Loss 18.6010(19.3297) | Error 0.7732(0.7992) Steps 484(449.16) | Grad Norm 4.2557(14.4412) | Total Time 0.00(0.00)\n",
      "Iter 0038 | Time 38.2153(58.3043) | Bit/dim 6.9922(8.0024) | Xent 2.1767(2.2252) | Loss 16.7452(19.2522) | Error 0.7636(0.7982) Steps 478(450.03) | Grad Norm 1.9939(14.0678) | Total Time 0.00(0.00)\n",
      "Iter 0039 | Time 39.6116(57.7435) | Bit/dim 6.9942(7.9721) | Xent 2.1791(2.2238) | Loss 16.4593(19.1684) | Error 0.7645(0.7971) Steps 496(451.41) | Grad Norm 5.2310(13.8027) | Total Time 0.00(0.00)\n",
      "Iter 0040 | Time 43.1335(57.3052) | Bit/dim 6.9922(7.9427) | Xent 2.1758(2.2224) | Loss 16.8256(19.0981) | Error 0.7576(0.7960) Steps 484(452.39) | Grad Norm 4.9129(13.5360) | Total Time 0.00(0.00)\n",
      "Iter 0041 | Time 43.2268(56.8829) | Bit/dim 6.9800(7.9139) | Xent 2.1696(2.2208) | Loss 17.0849(19.0377) | Error 0.7410(0.7943) Steps 472(452.97) | Grad Norm 2.7962(13.2138) | Total Time 0.00(0.00)\n",
      "Iter 0042 | Time 43.7692(56.4895) | Bit/dim 6.9731(7.8856) | Xent 2.1642(2.2191) | Loss 16.6641(18.9665) | Error 0.7755(0.7937) Steps 520(454.99) | Grad Norm 4.0697(12.9395) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 20.7608, Epoch Time 284.1257(325.4027), Bit/dim 6.9717(best: 7.0059), Xent 2.1544, Loss 8.0489, Error 0.7649(best: 0.7216)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0043 | Time 43.8696(56.1109) | Bit/dim 6.9614(7.8579) | Xent 2.1575(2.2172) | Loss 19.3806(18.9789) | Error 0.7740(0.7932) Steps 472(455.50) | Grad Norm 3.1850(12.6468) | Total Time 0.00(0.00)\n",
      "Iter 0044 | Time 43.0996(55.7205) | Bit/dim 6.9612(7.8310) | Xent 2.1475(2.2152) | Loss 16.6887(18.9102) | Error 0.7575(0.7921) Steps 514(457.25) | Grad Norm 3.0713(12.3596) | Total Time 0.00(0.00)\n",
      "Iter 0045 | Time 45.0178(55.3995) | Bit/dim 6.9560(7.8048) | Xent 2.1466(2.2131) | Loss 17.0965(18.8558) | Error 0.7492(0.7908) Steps 496(458.41) | Grad Norm 4.4838(12.1233) | Total Time 0.00(0.00)\n",
      "Iter 0046 | Time 46.4601(55.1313) | Bit/dim 6.9475(7.7790) | Xent 2.1347(2.2107) | Loss 16.5959(18.7880) | Error 0.7404(0.7893) Steps 538(460.80) | Grad Norm 2.6646(11.8395) | Total Time 0.00(0.00)\n",
      "Iter 0047 | Time 42.4653(54.7513) | Bit/dim 6.9466(7.7541) | Xent 2.1261(2.2082) | Loss 16.7377(18.7265) | Error 0.7489(0.7881) Steps 496(461.86) | Grad Norm 4.0685(11.6064) | Total Time 0.00(0.00)\n",
      "Iter 0048 | Time 43.8182(54.4233) | Bit/dim 6.9248(7.7292) | Xent 2.1238(2.2057) | Loss 16.9197(18.6723) | Error 0.7495(0.7869) Steps 496(462.88) | Grad Norm 4.9314(11.4061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 20.6706, Epoch Time 301.1475(324.6751), Bit/dim 6.9206(best: 6.9717), Xent 2.1072, Loss 7.9742, Error 0.7427(best: 0.7216)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0049 | Time 44.4920(54.1254) | Bit/dim 6.9061(7.7045) | Xent 2.1122(2.2029) | Loss 19.0047(18.6823) | Error 0.7430(0.7856) Steps 484(463.51) | Grad Norm 3.4029(11.1660) | Total Time 0.00(0.00)\n",
      "Iter 0050 | Time 46.6895(53.9023) | Bit/dim 6.9097(7.6807) | Xent 2.1071(2.2000) | Loss 16.7643(18.6247) | Error 0.7424(0.7843) Steps 496(464.49) | Grad Norm 2.7177(10.9126) | Total Time 0.00(0.00)\n",
      "Iter 0051 | Time 43.1839(53.5807) | Bit/dim 6.8844(7.6568) | Xent 2.1023(2.1971) | Loss 16.4896(18.5607) | Error 0.7400(0.7830) Steps 478(464.89) | Grad Norm 3.1009(10.6783) | Total Time 0.00(0.00)\n",
      "Iter 0052 | Time 42.4000(53.2453) | Bit/dim 6.8791(7.6334) | Xent 2.0921(2.1939) | Loss 15.8069(18.4781) | Error 0.7312(0.7814) Steps 502(466.01) | Grad Norm 7.2076(10.5741) | Total Time 0.00(0.00)\n",
      "Iter 0053 | Time 45.5986(53.0159) | Bit/dim 6.8681(7.6105) | Xent 2.1022(2.1912) | Loss 16.6339(18.4228) | Error 0.7479(0.7804) Steps 544(468.35) | Grad Norm 13.6342(10.6659) | Total Time 0.00(0.00)\n",
      "Iter 0054 | Time 44.6465(52.7648) | Bit/dim 6.8350(7.5872) | Xent 2.0884(2.1881) | Loss 16.4029(18.3622) | Error 0.7296(0.7789) Steps 502(469.36) | Grad Norm 8.1530(10.5905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 20.7726, Epoch Time 303.7448(324.0472), Bit/dim 6.8262(best: 6.9206), Xent 2.0755, Loss 7.8640, Error 0.7372(best: 0.7216)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0055 | Time 47.2142(52.5983) | Bit/dim 6.8147(7.5640) | Xent 2.0864(2.1850) | Loss 19.1205(18.3849) | Error 0.7472(0.7779) Steps 502(470.34) | Grad Norm 10.8324(10.5978) | Total Time 0.00(0.00)\n",
      "Iter 0056 | Time 46.4744(52.4146) | Bit/dim 6.8159(7.5416) | Xent 2.0887(2.1821) | Loss 16.5325(18.3293) | Error 0.7414(0.7768) Steps 508(471.47) | Grad Norm 21.4522(10.9234) | Total Time 0.00(0.00)\n",
      "Iter 0057 | Time 44.7201(52.1838) | Bit/dim 6.7774(7.5187) | Xent 2.0617(2.1785) | Loss 16.0109(18.2598) | Error 0.7094(0.7748) Steps 508(472.56) | Grad Norm 11.0483(10.9272) | Total Time 0.00(0.00)\n",
      "Iter 0058 | Time 42.9850(51.9078) | Bit/dim 6.7548(7.4957) | Xent 2.0553(2.1748) | Loss 16.2155(18.1985) | Error 0.7116(0.7729) Steps 484(472.91) | Grad Norm 7.8634(10.8353) | Total Time 0.00(0.00)\n",
      "Iter 0059 | Time 43.9179(51.6681) | Bit/dim 6.7320(7.4728) | Xent 2.0764(2.1719) | Loss 16.3195(18.1421) | Error 0.7354(0.7718) Steps 490(473.42) | Grad Norm 22.1521(11.1748) | Total Time 0.00(0.00)\n",
      "Iter 0060 | Time 43.8072(51.4323) | Bit/dim 6.7100(7.4499) | Xent 2.0767(2.1690) | Loss 16.2230(18.0845) | Error 0.7370(0.7708) Steps 490(473.92) | Grad Norm 22.5760(11.5168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 21.1873, Epoch Time 306.3110(323.5151), Bit/dim 6.6754(best: 6.8262), Xent 2.0303, Loss 7.6906, Error 0.6876(best: 0.7216)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0061 | Time 42.0133(51.1497) | Bit/dim 6.6659(7.4264) | Xent 2.0381(2.1651) | Loss 18.6486(18.1014) | Error 0.6940(0.7685) Steps 484(474.22) | Grad Norm 8.3389(11.4215) | Total Time 0.00(0.00)\n",
      "Iter 0062 | Time 43.1243(50.9089) | Bit/dim 6.6391(7.4028) | Xent 2.0564(2.1618) | Loss 15.8445(18.0337) | Error 0.7189(0.7670) Steps 478(474.33) | Grad Norm 17.1641(11.5938) | Total Time 0.00(0.00)\n",
      "Iter 0063 | Time 40.6220(50.6003) | Bit/dim 6.6182(7.3793) | Xent 2.0883(2.1596) | Loss 15.5775(17.9600) | Error 0.7515(0.7665) Steps 484(474.62) | Grad Norm 43.7401(12.5581) | Total Time 0.00(0.00)\n",
      "Iter 0064 | Time 43.6719(50.3925) | Bit/dim 6.5748(7.3551) | Xent 2.0286(2.1557) | Loss 16.0261(17.9020) | Error 0.6956(0.7644) Steps 460(474.18) | Grad Norm 9.7462(12.4738) | Total Time 0.00(0.00)\n",
      "Iter 0065 | Time 41.5176(50.1262) | Bit/dim 6.6365(7.3336) | Xent 2.3272(2.1608) | Loss 16.3327(17.8549) | Error 0.8049(0.7656) Steps 484(474.48) | Grad Norm 96.5966(14.9975) | Total Time 0.00(0.00)\n",
      "Iter 0066 | Time 49.8616(50.1183) | Bit/dim 6.6319(7.3125) | Xent 2.3072(2.1652) | Loss 16.4033(17.8114) | Error 0.8035(0.7667) Steps 490(474.94) | Grad Norm 99.2843(17.5261) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 20.7085, Epoch Time 297.6749(322.7399), Bit/dim 6.4659(best: 6.6754), Xent 2.0086, Loss 7.4702, Error 0.6922(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0067 | Time 41.4840(49.8593) | Bit/dim 6.4645(7.2871) | Xent 2.0309(2.1612) | Loss 18.0685(17.8191) | Error 0.7031(0.7648) Steps 472(474.86) | Grad Norm 12.7851(17.3838) | Total Time 0.00(0.00)\n",
      "Iter 0068 | Time 47.7030(49.7946) | Bit/dim 6.6954(7.2693) | Xent 2.6123(2.1747) | Loss 17.0451(17.7959) | Error 0.8488(0.7673) Steps 514(476.03) | Grad Norm 149.2438(21.3396) | Total Time 0.00(0.00)\n",
      "Iter 0069 | Time 43.6922(49.6115) | Bit/dim 6.9590(7.2600) | Xent 2.9285(2.1974) | Loss 17.6869(17.7926) | Error 0.8684(0.7704) Steps 466(475.73) | Grad Norm 180.2853(26.1080) | Total Time 0.00(0.00)\n",
      "Iter 0070 | Time 42.1270(49.3870) | Bit/dim 6.7552(7.2449) | Xent 2.4489(2.2049) | Loss 16.8997(17.7658) | Error 0.8327(0.7722) Steps 490(476.16) | Grad Norm 126.1169(29.1083) | Total Time 0.00(0.00)\n",
      "Iter 0071 | Time 45.0940(49.2582) | Bit/dim 6.3818(7.2190) | Xent 2.0506(2.2003) | Loss 15.5210(17.6985) | Error 0.7279(0.7709) Steps 496(476.75) | Grad Norm 22.8741(28.9213) | Total Time 0.00(0.00)\n",
      "Iter 0072 | Time 43.5157(49.0859) | Bit/dim 6.6135(7.2008) | Xent 2.2349(2.2013) | Loss 16.2511(17.6551) | Error 0.7702(0.7709) Steps 484(476.97) | Grad Norm 74.3678(30.2846) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 20.1115, Epoch Time 299.6033(322.0458), Bit/dim 6.9050(best: 6.4659), Xent 2.2139, Loss 8.0120, Error 0.7782(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0073 | Time 41.6857(48.8639) | Bit/dim 6.9024(7.1919) | Xent 2.2567(2.2030) | Loss 18.7034(17.6865) | Error 0.7924(0.7715) Steps 502(477.72) | Grad Norm 89.4268(32.0589) | Total Time 0.00(0.00)\n",
      "Iter 0074 | Time 44.7997(48.7420) | Bit/dim 6.6516(7.1757) | Xent 2.0740(2.1991) | Loss 16.2550(17.6436) | Error 0.7639(0.7713) Steps 448(476.83) | Grad Norm 76.3609(33.3880) | Total Time 0.00(0.00)\n",
      "Iter 0075 | Time 40.9347(48.5078) | Bit/dim 6.3296(7.1503) | Xent 2.0715(2.1953) | Loss 14.9535(17.5629) | Error 0.7450(0.7705) Steps 472(476.68) | Grad Norm 38.7369(33.5484) | Total Time 0.00(0.00)\n",
      "Iter 0076 | Time 42.1087(48.3158) | Bit/dim 6.3075(7.1250) | Xent 2.1808(2.1948) | Loss 15.5868(17.5036) | Error 0.7780(0.7707) Steps 484(476.90) | Grad Norm 75.3173(34.8015) | Total Time 0.00(0.00)\n",
      "Iter 0077 | Time 46.5565(48.2630) | Bit/dim 6.4341(7.1043) | Xent 2.2073(2.1952) | Loss 15.6727(17.4487) | Error 0.7734(0.7708) Steps 508(477.84) | Grad Norm 87.7711(36.3906) | Total Time 0.00(0.00)\n",
      "Iter 0078 | Time 43.7480(48.1276) | Bit/dim 6.4262(7.0839) | Xent 2.2037(2.1955) | Loss 15.8261(17.4000) | Error 0.7644(0.7706) Steps 448(476.94) | Grad Norm 44.8641(36.6448) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 20.1383, Epoch Time 295.9916(321.2642), Bit/dim 6.2410(best: 6.4659), Xent 2.1023, Loss 7.2921, Error 0.7656(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0079 | Time 44.2222(48.0104) | Bit/dim 6.2402(7.0586) | Xent 2.1056(2.1928) | Loss 18.0978(17.4209) | Error 0.7598(0.7703) Steps 502(477.69) | Grad Norm 25.7366(36.3176) | Total Time 0.00(0.00)\n",
      "Iter 0080 | Time 43.1896(47.8658) | Bit/dim 6.1602(7.0317) | Xent 2.3531(2.1976) | Loss 15.6171(17.3668) | Error 0.8293(0.7721) Steps 460(477.16) | Grad Norm 71.8207(37.3826) | Total Time 0.00(0.00)\n",
      "Iter 0081 | Time 42.7966(47.7137) | Bit/dim 6.1945(7.0066) | Xent 2.3267(2.2015) | Loss 15.5155(17.3113) | Error 0.8396(0.7741) Steps 472(477.01) | Grad Norm 61.6968(38.1121) | Total Time 0.00(0.00)\n",
      "Iter 0082 | Time 42.5650(47.5592) | Bit/dim 6.2491(6.9838) | Xent 2.1810(2.2008) | Loss 15.2540(17.2495) | Error 0.7934(0.7747) Steps 454(476.32) | Grad Norm 87.2827(39.5872) | Total Time 0.00(0.00)\n",
      "Iter 0083 | Time 42.2121(47.3988) | Bit/dim 6.2269(6.9611) | Xent 2.3001(2.2038) | Loss 15.3497(17.1926) | Error 0.8309(0.7764) Steps 442(475.29) | Grad Norm 104.7600(41.5424) | Total Time 0.00(0.00)\n",
      "Iter 0084 | Time 41.0271(47.2077) | Bit/dim 6.1297(6.9362) | Xent 2.0536(2.1993) | Loss 14.9949(17.1266) | Error 0.7020(0.7741) Steps 496(475.91) | Grad Norm 23.4853(41.0007) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 19.6712, Epoch Time 291.4615(320.3701), Bit/dim 6.0032(best: 6.2410), Xent 2.1135, Loss 7.0599, Error 0.7462(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0085 | Time 42.8761(47.0777) | Bit/dim 6.0040(6.9082) | Xent 2.1236(2.1970) | Loss 17.1282(17.1267) | Error 0.7538(0.7735) Steps 484(476.15) | Grad Norm 46.9670(41.1796) | Total Time 0.00(0.00)\n",
      "Iter 0086 | Time 42.0610(46.9272) | Bit/dim 6.0226(6.8816) | Xent 2.0607(2.1930) | Loss 14.7430(17.0552) | Error 0.7248(0.7721) Steps 484(476.39) | Grad Norm 36.6761(41.0445) | Total Time 0.00(0.00)\n",
      "Iter 0087 | Time 42.0646(46.7813) | Bit/dim 5.9454(6.8536) | Xent 2.0734(2.1894) | Loss 14.8898(16.9902) | Error 0.7360(0.7710) Steps 472(476.26) | Grad Norm 37.0559(40.9249) | Total Time 0.00(0.00)\n",
      "Iter 0088 | Time 44.4322(46.7109) | Bit/dim 5.9649(6.8269) | Xent 2.1119(2.1870) | Loss 14.7109(16.9218) | Error 0.7492(0.7703) Steps 460(475.77) | Grad Norm 62.2153(41.5636) | Total Time 0.00(0.00)\n",
      "Iter 0089 | Time 42.9330(46.5975) | Bit/dim 5.9810(6.8015) | Xent 2.0774(2.1838) | Loss 14.5797(16.8516) | Error 0.7276(0.7690) Steps 466(475.47) | Grad Norm 53.0170(41.9072) | Total Time 0.00(0.00)\n",
      "Iter 0090 | Time 40.6091(46.4179) | Bit/dim 5.8997(6.7745) | Xent 2.2258(2.1850) | Loss 14.7022(16.7871) | Error 0.8154(0.7704) Steps 472(475.37) | Grad Norm 81.3510(43.0905) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 19.4511, Epoch Time 290.1184(319.4625), Bit/dim 5.8639(best: 6.0032), Xent 2.1012, Loss 6.9145, Error 0.7692(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0091 | Time 40.9381(46.2535) | Bit/dim 5.8733(6.7474) | Xent 2.1072(2.1827) | Loss 16.7320(16.7854) | Error 0.7666(0.7703) Steps 478(475.45) | Grad Norm 50.3676(43.3088) | Total Time 0.00(0.00)\n",
      "Iter 0092 | Time 45.8481(46.2413) | Bit/dim 5.8662(6.7210) | Xent 2.2274(2.1840) | Loss 14.4141(16.7143) | Error 0.8121(0.7716) Steps 454(474.81) | Grad Norm 95.4963(44.8745) | Total Time 0.00(0.00)\n",
      "Iter 0093 | Time 39.4165(46.0366) | Bit/dim 5.8622(6.6952) | Xent 2.2855(2.1871) | Loss 14.7184(16.6544) | Error 0.8333(0.7734) Steps 442(473.82) | Grad Norm 102.7269(46.6100) | Total Time 0.00(0.00)\n",
      "Iter 0094 | Time 42.5556(45.9322) | Bit/dim 5.8078(6.6686) | Xent 2.0490(2.1829) | Loss 14.3798(16.5862) | Error 0.7189(0.7718) Steps 478(473.95) | Grad Norm 13.6146(45.6202) | Total Time 0.00(0.00)\n",
      "Iter 0095 | Time 41.9337(45.8122) | Bit/dim 5.7800(6.6420) | Xent 2.1875(2.1831) | Loss 14.5317(16.5245) | Error 0.7744(0.7719) Steps 478(474.07) | Grad Norm 75.1841(46.5071) | Total Time 0.00(0.00)\n",
      "Iter 0096 | Time 41.8585(45.6936) | Bit/dim 5.8042(6.6168) | Xent 2.1337(2.1816) | Loss 14.4367(16.4619) | Error 0.7522(0.7713) Steps 478(474.19) | Grad Norm 47.4037(46.5340) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 20.1582, Epoch Time 288.3650(318.5296), Bit/dim 5.8418(best: 5.8639), Xent 2.0658, Loss 6.8747, Error 0.7137(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0097 | Time 39.5817(45.5102) | Bit/dim 5.8347(6.5934) | Xent 2.0792(2.1785) | Loss 17.2413(16.4853) | Error 0.7283(0.7700) Steps 454(473.58) | Grad Norm 57.4214(46.8606) | Total Time 0.00(0.00)\n",
      "Iter 0098 | Time 41.0944(45.3778) | Bit/dim 5.7750(6.5688) | Xent 2.1009(2.1762) | Loss 14.1908(16.4164) | Error 0.7525(0.7695) Steps 466(473.35) | Grad Norm 56.3841(47.1463) | Total Time 0.00(0.00)\n",
      "Iter 0099 | Time 41.5945(45.2643) | Bit/dim 5.7220(6.5434) | Xent 2.0616(2.1727) | Loss 14.2378(16.3511) | Error 0.7171(0.7679) Steps 460(472.95) | Grad Norm 6.0195(45.9125) | Total Time 0.00(0.00)\n",
      "Iter 0100 | Time 41.6157(45.1548) | Bit/dim 5.6808(6.5175) | Xent 2.0701(2.1697) | Loss 14.1525(16.2851) | Error 0.7369(0.7670) Steps 466(472.74) | Grad Norm 15.9497(45.0136) | Total Time 0.00(0.00)\n",
      "Iter 0101 | Time 42.0245(45.0609) | Bit/dim 5.6368(6.4911) | Xent 2.0770(2.1669) | Loss 14.2521(16.2241) | Error 0.7271(0.7658) Steps 436(471.64) | Grad Norm 11.5485(44.0097) | Total Time 0.00(0.00)\n",
      "Iter 0102 | Time 39.8413(44.9043) | Bit/dim 5.6583(6.4661) | Xent 2.0540(2.1635) | Loss 13.7460(16.1498) | Error 0.7050(0.7639) Steps 454(471.11) | Grad Norm 4.7591(42.8321) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 19.2155, Epoch Time 280.8787(317.4001), Bit/dim 5.6378(best: 5.8418), Xent 2.0487, Loss 6.6622, Error 0.7021(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0103 | Time 42.0568(44.8189) | Bit/dim 5.6445(6.4415) | Xent 2.0676(2.1606) | Loss 16.5759(16.1626) | Error 0.7156(0.7625) Steps 454(470.60) | Grad Norm 7.7536(41.7798) | Total Time 0.00(0.00)\n",
      "Iter 0104 | Time 39.1359(44.6484) | Bit/dim 5.6075(6.4164) | Xent 2.0596(2.1576) | Loss 13.7629(16.0906) | Error 0.7081(0.7609) Steps 460(470.28) | Grad Norm 5.9626(40.7053) | Total Time 0.00(0.00)\n",
      "Iter 0105 | Time 42.0938(44.5718) | Bit/dim 5.5955(6.3918) | Xent 2.0664(2.1549) | Loss 13.9305(16.0258) | Error 0.7157(0.7595) Steps 484(470.69) | Grad Norm 15.2261(39.9409) | Total Time 0.00(0.00)\n",
      "Iter 0106 | Time 43.3576(44.5353) | Bit/dim 5.5882(6.3677) | Xent 2.0450(2.1516) | Loss 13.8712(15.9611) | Error 0.7184(0.7583) Steps 478(470.91) | Grad Norm 16.3614(39.2335) | Total Time 0.00(0.00)\n",
      "Iter 0107 | Time 39.4827(44.3838) | Bit/dim 5.5426(6.3430) | Xent 2.0276(2.1478) | Loss 13.9311(15.9002) | Error 0.6996(0.7565) Steps 454(470.40) | Grad Norm 5.6039(38.2246) | Total Time 0.00(0.00)\n",
      "Iter 0108 | Time 40.8950(44.2791) | Bit/dim 5.5621(6.3195) | Xent 2.0478(2.1448) | Loss 14.0111(15.8436) | Error 0.7101(0.7551) Steps 454(469.91) | Grad Norm 24.3123(37.8073) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 19.1487, Epoch Time 282.1300(316.3420), Bit/dim 5.5442(best: 5.6378), Xent 2.0122, Loss 6.5503, Error 0.6882(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0109 | Time 40.2680(44.1588) | Bit/dim 5.5470(6.2964) | Xent 2.0192(2.1411) | Loss 16.6346(15.8673) | Error 0.7031(0.7536) Steps 472(469.98) | Grad Norm 7.5691(36.9001) | Total Time 0.00(0.00)\n",
      "Iter 0110 | Time 38.9847(44.0035) | Bit/dim 5.5336(6.2735) | Xent 2.0391(2.1380) | Loss 13.3721(15.7924) | Error 0.7163(0.7524) Steps 436(468.96) | Grad Norm 23.5744(36.5003) | Total Time 0.00(0.00)\n",
      "Iter 0111 | Time 40.0348(43.8845) | Bit/dim 5.5177(6.2508) | Xent 2.0128(2.1343) | Loss 13.8514(15.7342) | Error 0.6964(0.7508) Steps 442(468.15) | Grad Norm 20.9287(36.0332) | Total Time 0.00(0.00)\n",
      "Iter 0112 | Time 41.2952(43.8068) | Bit/dim 5.5128(6.2287) | Xent 2.0291(2.1311) | Loss 13.9620(15.6810) | Error 0.7228(0.7499) Steps 466(468.08) | Grad Norm 22.6696(35.6323) | Total Time 0.00(0.00)\n",
      "Iter 0113 | Time 41.2536(43.7302) | Bit/dim 5.4709(6.2059) | Xent 2.0039(2.1273) | Loss 13.5648(15.6176) | Error 0.7030(0.7485) Steps 436(467.12) | Grad Norm 8.8116(34.8277) | Total Time 0.00(0.00)\n",
      "Iter 0114 | Time 40.0231(43.6190) | Bit/dim 5.5095(6.1850) | Xent 2.0374(2.1246) | Loss 13.8320(15.5640) | Error 0.7218(0.7477) Steps 448(466.55) | Grad Norm 35.6136(34.8512) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 19.4114, Epoch Time 277.1816(315.1672), Bit/dim 5.4864(best: 5.5442), Xent 2.0211, Loss 6.4970, Error 0.7181(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0115 | Time 42.7496(43.5929) | Bit/dim 5.4831(6.1640) | Xent 2.0377(2.1220) | Loss 16.5016(15.5921) | Error 0.7215(0.7469) Steps 448(465.99) | Grad Norm 29.5867(34.6933) | Total Time 0.00(0.00)\n",
      "Iter 0116 | Time 43.3042(43.5842) | Bit/dim 5.4380(6.1422) | Xent 1.9864(2.1179) | Loss 13.3776(15.5257) | Error 0.6889(0.7452) Steps 472(466.17) | Grad Norm 9.9231(33.9502) | Total Time 0.00(0.00)\n",
      "Iter 0117 | Time 40.3360(43.4868) | Bit/dim 5.4244(6.1207) | Xent 1.9922(2.1141) | Loss 13.4153(15.4624) | Error 0.6877(0.7435) Steps 466(466.17) | Grad Norm 18.4612(33.4855) | Total Time 0.00(0.00)\n",
      "Iter 0118 | Time 43.5977(43.4901) | Bit/dim 5.4129(6.0994) | Xent 2.0448(2.1121) | Loss 13.4113(15.4008) | Error 0.7266(0.7430) Steps 478(466.52) | Grad Norm 55.9454(34.1593) | Total Time 0.00(0.00)\n",
      "Iter 0119 | Time 43.9731(43.5046) | Bit/dim 5.4041(6.0786) | Xent 2.0106(2.1090) | Loss 13.5868(15.3464) | Error 0.7049(0.7418) Steps 442(465.79) | Grad Norm 25.6121(33.9029) | Total Time 0.00(0.00)\n",
      "Iter 0120 | Time 38.3468(43.3499) | Bit/dim 5.3866(6.0578) | Xent 2.0018(2.1058) | Loss 13.5973(15.2939) | Error 0.6905(0.7403) Steps 460(465.61) | Grad Norm 6.8382(33.0910) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 19.6839, Epoch Time 288.2462(314.3595), Bit/dim 5.3472(best: 5.4864), Xent 1.9691, Loss 6.3317, Error 0.6701(best: 0.6876)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0121 | Time 39.4688(43.2334) | Bit/dim 5.3447(6.0364) | Xent 1.9861(2.1022) | Loss 15.9297(15.3130) | Error 0.6755(0.7383) Steps 460(465.44) | Grad Norm 11.1136(32.4316) | Total Time 0.00(0.00)\n",
      "Iter 0122 | Time 44.1607(43.2613) | Bit/dim 5.3947(6.0172) | Xent 2.2143(2.1056) | Loss 13.9338(15.2716) | Error 0.7615(0.7390) Steps 478(465.82) | Grad Norm 106.8479(34.6641) | Total Time 0.00(0.00)\n",
      "Iter 0123 | Time 43.2105(43.2597) | Bit/dim 5.3965(5.9985) | Xent 2.0574(2.1041) | Loss 13.7168(15.2250) | Error 0.7248(0.7386) Steps 472(466.01) | Grad Norm 50.8880(35.1509) | Total Time 0.00(0.00)\n",
      "Iter 0124 | Time 42.8042(43.2461) | Bit/dim 5.3634(5.9795) | Xent 2.2149(2.1075) | Loss 13.6085(15.1765) | Error 0.7810(0.7399) Steps 424(464.75) | Grad Norm 93.1439(36.8906) | Total Time 0.00(0.00)\n",
      "Iter 0125 | Time 41.1600(43.1835) | Bit/dim 5.4460(5.9635) | Xent 2.1203(2.1078) | Loss 13.2035(15.1173) | Error 0.7552(0.7403) Steps 460(464.60) | Grad Norm 56.6859(37.4845) | Total Time 0.00(0.00)\n",
      "Iter 0126 | Time 40.8919(43.1147) | Bit/dim 5.4836(5.9491) | Xent 1.9896(2.1043) | Loss 13.7132(15.0752) | Error 0.6903(0.7388) Steps 454(464.28) | Grad Norm 45.7279(37.7318) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 19.4636, Epoch Time 287.1127(313.5421), Bit/dim 5.3410(best: 5.3472), Xent 2.0319, Loss 6.3569, Error 0.7149(best: 0.6701)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0127 | Time 44.4718(43.1555) | Bit/dim 5.3432(5.9309) | Xent 2.0442(2.1025) | Loss 16.1042(15.1061) | Error 0.7172(0.7382) Steps 454(463.98) | Grad Norm 45.9740(37.9791) | Total Time 0.00(0.00)\n",
      "Iter 0128 | Time 41.4711(43.1049) | Bit/dim 5.3426(5.9133) | Xent 2.0140(2.0998) | Loss 13.4565(15.0566) | Error 0.7087(0.7373) Steps 472(464.22) | Grad Norm 44.9977(38.1896) | Total Time 0.00(0.00)\n",
      "Iter 0129 | Time 42.8672(43.0978) | Bit/dim 5.3918(5.8976) | Xent 2.0310(2.0978) | Loss 13.5271(15.0107) | Error 0.7070(0.7364) Steps 502(465.35) | Grad Norm 51.3134(38.5833) | Total Time 0.00(0.00)\n",
      "Iter 0130 | Time 42.8049(43.0890) | Bit/dim 5.2511(5.8782) | Xent 1.9987(2.0948) | Loss 12.9976(14.9503) | Error 0.6892(0.7350) Steps 454(465.01) | Grad Norm 13.7014(37.8369) | Total Time 0.00(0.00)\n",
      "Iter 0131 | Time 42.0923(43.0591) | Bit/dim 5.2950(5.8607) | Xent 1.9993(2.0919) | Loss 13.4746(14.9060) | Error 0.7003(0.7339) Steps 490(465.76) | Grad Norm 18.7785(37.2651) | Total Time 0.00(0.00)\n",
      "Iter 0132 | Time 45.4895(43.1320) | Bit/dim 5.2524(5.8425) | Xent 2.0083(2.0894) | Loss 13.3660(14.8598) | Error 0.7061(0.7331) Steps 490(466.49) | Grad Norm 17.0629(36.6591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 19.0933, Epoch Time 294.5615(312.9727), Bit/dim 5.2580(best: 5.3410), Xent 1.9765, Loss 6.2463, Error 0.6689(best: 0.6701)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0133 | Time 41.5250(43.0838) | Bit/dim 5.2469(5.8246) | Xent 1.9962(2.0866) | Loss 15.7917(14.8878) | Error 0.6833(0.7316) Steps 472(466.65) | Grad Norm 22.1347(36.2233) | Total Time 0.00(0.00)\n",
      "Iter 0134 | Time 43.3311(43.0912) | Bit/dim 5.2412(5.8071) | Xent 2.0251(2.0848) | Loss 13.2441(14.8385) | Error 0.7195(0.7312) Steps 490(467.35) | Grad Norm 25.6286(35.9055) | Total Time 0.00(0.00)\n",
      "Iter 0135 | Time 42.9548(43.0871) | Bit/dim 5.2166(5.7894) | Xent 1.9580(2.0810) | Loss 13.1028(14.7864) | Error 0.6776(0.7296) Steps 460(467.13) | Grad Norm 15.4511(35.2919) | Total Time 0.00(0.00)\n",
      "Iter 0136 | Time 44.4931(43.1293) | Bit/dim 5.2085(5.7720) | Xent 1.9804(2.0780) | Loss 13.1468(14.7372) | Error 0.6939(0.7286) Steps 478(467.46) | Grad Norm 16.2502(34.7206) | Total Time 0.00(0.00)\n",
      "Iter 0137 | Time 42.9412(43.1237) | Bit/dim 5.2168(5.7553) | Xent 1.9883(2.0753) | Loss 13.0599(14.6869) | Error 0.6749(0.7269) Steps 478(467.77) | Grad Norm 21.9625(34.3379) | Total Time 0.00(0.00)\n",
      "Iter 0138 | Time 46.0808(43.2124) | Bit/dim 5.1475(5.7371) | Xent 1.9770(2.0723) | Loss 13.0076(14.6365) | Error 0.6840(0.7257) Steps 460(467.54) | Grad Norm 16.7515(33.8103) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 19.6691, Epoch Time 296.9745(312.4928), Bit/dim 5.1474(best: 5.2580), Xent 1.9422, Loss 6.1185, Error 0.6550(best: 0.6689)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0139 | Time 43.8863(43.2326) | Bit/dim 5.1454(5.7193) | Xent 1.9533(2.0687) | Loss 15.7233(14.6691) | Error 0.6744(0.7241) Steps 472(467.67) | Grad Norm 10.5152(33.1114) | Total Time 0.00(0.00)\n",
      "Iter 0140 | Time 45.0807(43.2880) | Bit/dim 5.1694(5.7028) | Xent 1.9981(2.0666) | Loss 13.2644(14.6270) | Error 0.7023(0.7235) Steps 466(467.62) | Grad Norm 35.4650(33.1820) | Total Time 0.00(0.00)\n",
      "Iter 0141 | Time 43.4885(43.2941) | Bit/dim 5.1307(5.6857) | Xent 1.9761(2.0639) | Loss 12.7401(14.5704) | Error 0.6945(0.7226) Steps 490(468.30) | Grad Norm 22.7436(32.8689) | Total Time 0.00(0.00)\n",
      "Iter 0142 | Time 42.6087(43.2735) | Bit/dim 5.1632(5.6700) | Xent 1.9566(2.0607) | Loss 12.5806(14.5107) | Error 0.6743(0.7211) Steps 496(469.13) | Grad Norm 27.2763(32.7011) | Total Time 0.00(0.00)\n",
      "Iter 0143 | Time 43.1105(43.2686) | Bit/dim 5.1131(5.6533) | Xent 1.9669(2.0579) | Loss 13.1320(14.4693) | Error 0.6919(0.7203) Steps 472(469.21) | Grad Norm 22.2110(32.3864) | Total Time 0.00(0.00)\n",
      "Iter 0144 | Time 48.7268(43.4324) | Bit/dim 5.1320(5.6376) | Xent 1.9832(2.0556) | Loss 12.9258(14.4230) | Error 0.6976(0.7196) Steps 466(469.12) | Grad Norm 34.4362(32.4479) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 19.9055, Epoch Time 302.7198(312.1996), Bit/dim 5.0871(best: 5.1474), Xent 1.9381, Loss 6.0562, Error 0.6596(best: 0.6550)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0145 | Time 45.6547(43.4990) | Bit/dim 5.1003(5.6215) | Xent 1.9632(2.0529) | Loss 15.2934(14.4491) | Error 0.6864(0.7186) Steps 460(468.84) | Grad Norm 13.1743(31.8697) | Total Time 0.00(0.00)\n",
      "Iter 0146 | Time 42.3040(43.4632) | Bit/dim 5.1106(5.6062) | Xent 1.9450(2.0496) | Loss 12.5779(14.3930) | Error 0.6725(0.7172) Steps 472(468.94) | Grad Norm 23.1963(31.6095) | Total Time 0.00(0.00)\n",
      "Iter 0147 | Time 45.8292(43.5342) | Bit/dim 5.0866(5.5906) | Xent 1.9614(2.0470) | Loss 12.7167(14.3427) | Error 0.6838(0.7162) Steps 472(469.03) | Grad Norm 9.0389(30.9324) | Total Time 0.00(0.00)\n",
      "Iter 0148 | Time 48.6248(43.6869) | Bit/dim 5.0515(5.5744) | Xent 1.9376(2.0437) | Loss 12.9574(14.3011) | Error 0.6623(0.7146) Steps 490(469.66) | Grad Norm 7.9783(30.2437) | Total Time 0.00(0.00)\n",
      "Iter 0149 | Time 41.9344(43.6343) | Bit/dim 5.0364(5.5583) | Xent 1.9623(2.0413) | Loss 12.8113(14.2564) | Error 0.6896(0.7138) Steps 460(469.37) | Grad Norm 14.9790(29.7858) | Total Time 0.00(0.00)\n",
      "Iter 0150 | Time 42.4794(43.5996) | Bit/dim 5.0641(5.5435) | Xent 1.9702(2.0391) | Loss 12.7468(14.2112) | Error 0.6924(0.7132) Steps 460(469.09) | Grad Norm 27.3997(29.7142) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 20.5368, Epoch Time 303.1226(311.9273), Bit/dim 5.0222(best: 5.0871), Xent 1.9184, Loss 5.9814, Error 0.6486(best: 0.6550)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0151 | Time 44.3766(43.6230) | Bit/dim 5.0317(5.5281) | Xent 1.9309(2.0359) | Loss 15.7893(14.2585) | Error 0.6625(0.7117) Steps 502(470.08) | Grad Norm 9.9502(29.1213) | Total Time 0.00(0.00)\n",
      "Iter 0152 | Time 42.0862(43.5769) | Bit/dim 5.0330(5.5133) | Xent 1.9538(2.0334) | Loss 12.4923(14.2055) | Error 0.6800(0.7107) Steps 484(470.49) | Grad Norm 20.6920(28.8684) | Total Time 0.00(0.00)\n",
      "Iter 0153 | Time 43.0839(43.5621) | Bit/dim 5.0208(5.4985) | Xent 1.9711(2.0316) | Loss 12.6517(14.1589) | Error 0.6927(0.7102) Steps 484(470.90) | Grad Norm 20.8739(28.6286) | Total Time 0.00(0.00)\n",
      "Iter 0154 | Time 42.7807(43.5386) | Bit/dim 4.9901(5.4832) | Xent 1.9049(2.0278) | Loss 12.3449(14.1045) | Error 0.6631(0.7088) Steps 478(471.11) | Grad Norm 4.8403(27.9149) | Total Time 0.00(0.00)\n",
      "Iter 0155 | Time 44.4073(43.5647) | Bit/dim 5.0746(5.4710) | Xent 1.9525(2.0255) | Loss 12.4883(14.0560) | Error 0.6805(0.7079) Steps 490(471.68) | Grad Norm 25.3308(27.8374) | Total Time 0.00(0.00)\n",
      "Iter 0156 | Time 45.1338(43.6118) | Bit/dim 5.0445(5.4582) | Xent 1.9387(2.0229) | Loss 12.7485(14.0168) | Error 0.6775(0.7070) Steps 490(472.23) | Grad Norm 22.8087(27.6866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 19.9797, Epoch Time 297.9953(311.5093), Bit/dim 5.0104(best: 5.0222), Xent 1.9207, Loss 5.9708, Error 0.6734(best: 0.6486)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0157 | Time 44.1954(43.6293) | Bit/dim 5.0199(5.4450) | Xent 1.9315(2.0201) | Loss 15.1579(14.0510) | Error 0.6846(0.7063) Steps 472(472.22) | Grad Norm 33.5404(27.8622) | Total Time 0.00(0.00)\n",
      "Iter 0158 | Time 43.2053(43.6165) | Bit/dim 5.0210(5.4323) | Xent 1.9265(2.0173) | Loss 12.7266(14.0113) | Error 0.6765(0.7054) Steps 478(472.39) | Grad Norm 17.6079(27.5545) | Total Time 0.00(0.00)\n",
      "Iter 0159 | Time 43.0139(43.5985) | Bit/dim 5.0252(5.4201) | Xent 1.9820(2.0163) | Loss 12.8497(13.9764) | Error 0.7039(0.7054) Steps 454(471.84) | Grad Norm 37.6465(27.8573) | Total Time 0.00(0.00)\n",
      "Iter 0160 | Time 44.5958(43.6284) | Bit/dim 5.0154(5.4080) | Xent 1.9100(2.0131) | Loss 12.4664(13.9311) | Error 0.6640(0.7042) Steps 466(471.67) | Grad Norm 21.2053(27.6577) | Total Time 0.00(0.00)\n",
      "Iter 0161 | Time 48.6175(43.7781) | Bit/dim 4.9771(5.3950) | Xent 1.9758(2.0120) | Loss 12.9514(13.9017) | Error 0.6976(0.7040) Steps 466(471.50) | Grad Norm 21.8119(27.4824) | Total Time 0.00(0.00)\n",
      "Iter 0162 | Time 49.7644(43.9577) | Bit/dim 4.9412(5.3814) | Xent 1.9479(2.0101) | Loss 12.2135(13.8511) | Error 0.6789(0.7032) Steps 538(473.49) | Grad Norm 19.8257(27.2527) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 20.7848, Epoch Time 309.9833(311.4635), Bit/dim 4.9537(best: 5.0104), Xent 1.9315, Loss 5.9195, Error 0.6727(best: 0.6486)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0163 | Time 43.2239(43.9356) | Bit/dim 4.9423(5.3682) | Xent 1.9463(2.0081) | Loss 15.4609(13.8994) | Error 0.6895(0.7028) Steps 502(474.35) | Grad Norm 14.7366(26.8772) | Total Time 0.00(0.00)\n",
      "Iter 0164 | Time 46.2522(44.0051) | Bit/dim 4.9431(5.3555) | Xent 1.9422(2.0062) | Loss 12.6427(13.8617) | Error 0.6774(0.7020) Steps 514(475.54) | Grad Norm 9.0644(26.3428) | Total Time 0.00(0.00)\n",
      "Iter 0165 | Time 45.3397(44.0452) | Bit/dim 4.9312(5.3428) | Xent 1.9416(2.0042) | Loss 12.4680(13.8199) | Error 0.6763(0.7013) Steps 496(476.15) | Grad Norm 15.4259(26.0153) | Total Time 0.00(0.00)\n",
      "Iter 0166 | Time 37.7940(43.8576) | Bit/dim 4.9371(5.3306) | Xent 1.9487(2.0026) | Loss 12.4171(13.7778) | Error 0.6853(0.7008) Steps 448(475.31) | Grad Norm 8.1169(25.4783) | Total Time 0.00(0.00)\n",
      "Iter 0167 | Time 42.1998(43.8079) | Bit/dim 4.9367(5.3188) | Xent 1.9086(1.9997) | Loss 12.6433(13.7437) | Error 0.6650(0.6997) Steps 478(475.39) | Grad Norm 10.6492(25.0335) | Total Time 0.00(0.00)\n",
      "Iter 0168 | Time 43.7000(43.8047) | Bit/dim 4.8965(5.3061) | Xent 1.9252(1.9975) | Loss 12.3214(13.7011) | Error 0.6670(0.6987) Steps 472(475.29) | Grad Norm 9.9948(24.5823) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 20.9211, Epoch Time 295.6557(310.9893), Bit/dim 4.8947(best: 4.9537), Xent 1.8772, Loss 5.8333, Error 0.6445(best: 0.6486)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0169 | Time 42.5082(43.7658) | Bit/dim 4.9004(5.2939) | Xent 1.9107(1.9949) | Loss 15.1203(13.7436) | Error 0.6640(0.6977) Steps 478(475.37) | Grad Norm 5.0188(23.9954) | Total Time 0.00(0.00)\n",
      "Iter 0170 | Time 44.4280(43.7856) | Bit/dim 4.9130(5.2825) | Xent 1.8716(1.9912) | Loss 12.4055(13.7035) | Error 0.6510(0.6963) Steps 490(475.81) | Grad Norm 18.0894(23.8182) | Total Time 0.00(0.00)\n",
      "Iter 0171 | Time 45.5751(43.8393) | Bit/dim 4.8728(5.2702) | Xent 1.8992(1.9884) | Loss 12.3689(13.6635) | Error 0.6700(0.6955) Steps 478(475.87) | Grad Norm 7.8740(23.3399) | Total Time 0.00(0.00)\n",
      "Iter 0172 | Time 46.8834(43.9306) | Bit/dim 4.9878(5.2617) | Xent 1.9285(1.9866) | Loss 12.8854(13.6401) | Error 0.6785(0.6950) Steps 496(476.48) | Grad Norm 39.5266(23.8255) | Total Time 0.00(0.00)\n",
      "Iter 0173 | Time 44.6098(43.9510) | Bit/dim 4.9649(5.2528) | Xent 1.9116(1.9844) | Loss 12.3650(13.6019) | Error 0.6641(0.6941) Steps 484(476.70) | Grad Norm 37.8186(24.2453) | Total Time 0.00(0.00)\n",
      "Iter 0174 | Time 46.3839(44.0240) | Bit/dim 4.8684(5.2413) | Xent 1.8540(1.9805) | Loss 12.2275(13.5606) | Error 0.6449(0.6926) Steps 490(477.10) | Grad Norm 5.1656(23.6729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 20.3823, Epoch Time 306.6777(310.8599), Bit/dim 4.8965(best: 4.8947), Xent 1.8741, Loss 5.8336, Error 0.6572(best: 0.6445)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0175 | Time 43.4172(44.0058) | Bit/dim 4.9050(5.2312) | Xent 1.8815(1.9775) | Loss 15.0919(13.6066) | Error 0.6566(0.6915) Steps 490(477.49) | Grad Norm 26.0899(23.7454) | Total Time 0.00(0.00)\n",
      "Iter 0176 | Time 44.1899(44.0113) | Bit/dim 4.8577(5.2200) | Xent 1.8664(1.9742) | Loss 12.3805(13.5698) | Error 0.6550(0.6904) Steps 490(477.86) | Grad Norm 18.7283(23.5949) | Total Time 0.00(0.00)\n",
      "Iter 0177 | Time 45.8712(44.0671) | Bit/dim 4.8679(5.2094) | Xent 1.9301(1.9729) | Loss 12.5212(13.5383) | Error 0.6766(0.6900) Steps 478(477.87) | Grad Norm 31.1129(23.8204) | Total Time 0.00(0.00)\n",
      "Iter 0178 | Time 45.8679(44.1211) | Bit/dim 4.8699(5.1993) | Xent 1.9372(1.9718) | Loss 12.2348(13.4992) | Error 0.6838(0.6898) Steps 472(477.69) | Grad Norm 34.9105(24.1531) | Total Time 0.00(0.00)\n",
      "Iter 0179 | Time 44.8175(44.1420) | Bit/dim 4.8288(5.1881) | Xent 1.8409(1.9679) | Loss 12.0578(13.4560) | Error 0.6326(0.6881) Steps 490(478.06) | Grad Norm 10.2828(23.7370) | Total Time 0.00(0.00)\n",
      "Iter 0180 | Time 47.8734(44.2540) | Bit/dim 4.8349(5.1775) | Xent 1.9349(1.9669) | Loss 12.1792(13.4177) | Error 0.6776(0.6878) Steps 532(479.68) | Grad Norm 24.9963(23.7748) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 21.2811, Epoch Time 309.2562(310.8118), Bit/dim 4.8159(best: 4.8947), Xent 1.8415, Loss 5.7367, Error 0.6398(best: 0.6445)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0181 | Time 42.9650(44.2153) | Bit/dim 4.8162(5.1667) | Xent 1.8435(1.9632) | Loss 15.1910(13.4709) | Error 0.6460(0.6865) Steps 496(480.17) | Grad Norm 17.6107(23.5899) | Total Time 0.00(0.00)\n",
      "Iter 0182 | Time 52.5424(44.4651) | Bit/dim 4.8041(5.1558) | Xent 1.9112(1.9616) | Loss 12.5137(13.4422) | Error 0.6753(0.6862) Steps 508(481.00) | Grad Norm 17.4015(23.4042) | Total Time 0.00(0.00)\n",
      "Iter 0183 | Time 46.1082(44.5144) | Bit/dim 4.8196(5.1457) | Xent 1.9070(1.9600) | Loss 12.2158(13.4054) | Error 0.6695(0.6857) Steps 502(481.63) | Grad Norm 20.9559(23.3308) | Total Time 0.00(0.00)\n",
      "Iter 0184 | Time 46.9290(44.5868) | Bit/dim 4.7924(5.1351) | Xent 1.8526(1.9568) | Loss 12.3250(13.3730) | Error 0.6542(0.6847) Steps 496(482.06) | Grad Norm 9.0424(22.9021) | Total Time 0.00(0.00)\n",
      "Iter 0185 | Time 49.4799(44.7336) | Bit/dim 4.8069(5.1253) | Xent 1.9019(1.9551) | Loss 12.4240(13.3445) | Error 0.6669(0.6842) Steps 520(483.20) | Grad Norm 22.6741(22.8953) | Total Time 0.00(0.00)\n",
      "Iter 0186 | Time 50.2467(44.8990) | Bit/dim 4.8061(5.1157) | Xent 1.8644(1.9524) | Loss 12.3305(13.3141) | Error 0.6456(0.6830) Steps 514(484.13) | Grad Norm 15.7466(22.6808) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 21.2293, Epoch Time 325.5507(311.2540), Bit/dim 4.7730(best: 4.8159), Xent 1.8219, Loss 5.6839, Error 0.6442(best: 0.6398)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0187 | Time 45.1779(44.9074) | Bit/dim 4.7852(5.1058) | Xent 1.8640(1.9497) | Loss 14.7310(13.3566) | Error 0.6587(0.6823) Steps 520(485.20) | Grad Norm 8.7591(22.2632) | Total Time 0.00(0.00)\n",
      "Iter 0188 | Time 49.3454(45.0405) | Bit/dim 4.8052(5.0968) | Xent 1.9501(1.9497) | Loss 12.3908(13.3276) | Error 0.6858(0.6824) Steps 544(486.97) | Grad Norm 26.0698(22.3774) | Total Time 0.00(0.00)\n",
      "Iter 0189 | Time 51.6572(45.2390) | Bit/dim 4.7896(5.0876) | Xent 1.9050(1.9484) | Loss 12.4856(13.3024) | Error 0.6679(0.6820) Steps 490(487.06) | Grad Norm 18.9658(22.2750) | Total Time 0.00(0.00)\n",
      "Iter 0190 | Time 45.8691(45.2579) | Bit/dim 4.8440(5.0803) | Xent 1.9918(1.9497) | Loss 12.1870(13.2689) | Error 0.7084(0.6828) Steps 514(487.87) | Grad Norm 40.3577(22.8175) | Total Time 0.00(0.00)\n",
      "Iter 0191 | Time 46.8414(45.3054) | Bit/dim 4.8191(5.0724) | Xent 1.9509(1.9497) | Loss 12.1535(13.2354) | Error 0.6891(0.6830) Steps 484(487.75) | Grad Norm 32.3520(23.1035) | Total Time 0.00(0.00)\n",
      "Iter 0192 | Time 45.0889(45.2989) | Bit/dim 4.7631(5.0631) | Xent 1.8717(1.9474) | Loss 12.3364(13.2085) | Error 0.6554(0.6821) Steps 508(488.36) | Grad Norm 13.4075(22.8127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 21.3956, Epoch Time 321.3946(311.5582), Bit/dim 4.7595(best: 4.7730), Xent 1.8259, Loss 5.6725, Error 0.6262(best: 0.6398)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0193 | Time 47.0541(45.3516) | Bit/dim 4.7559(5.0539) | Xent 1.8644(1.9449) | Loss 15.1274(13.2660) | Error 0.6419(0.6809) Steps 526(489.49) | Grad Norm 12.4381(22.5014) | Total Time 0.00(0.00)\n",
      "Iter 0194 | Time 48.2651(45.4390) | Bit/dim 4.7832(5.0458) | Xent 1.8812(1.9430) | Loss 12.4186(13.2406) | Error 0.6687(0.6806) Steps 526(490.58) | Grad Norm 21.8602(22.4822) | Total Time 0.00(0.00)\n",
      "Iter 0195 | Time 50.1236(45.5795) | Bit/dim 4.7492(5.0369) | Xent 1.8126(1.9391) | Loss 12.2471(13.2108) | Error 0.6329(0.6791) Steps 520(491.46) | Grad Norm 11.8055(22.1619) | Total Time 0.00(0.00)\n",
      "Iter 0196 | Time 48.8111(45.6765) | Bit/dim 4.7488(5.0283) | Xent 1.8660(1.9369) | Loss 12.2195(13.1811) | Error 0.6511(0.6783) Steps 490(491.42) | Grad Norm 16.6983(21.9980) | Total Time 0.00(0.00)\n",
      "Iter 0197 | Time 48.6345(45.7652) | Bit/dim 4.7710(5.0205) | Xent 1.8419(1.9340) | Loss 12.0538(13.1472) | Error 0.6421(0.6772) Steps 532(492.64) | Grad Norm 13.0679(21.7301) | Total Time 0.00(0.00)\n",
      "Iter 0198 | Time 52.7621(45.9751) | Bit/dim 4.7154(5.0114) | Xent 1.8317(1.9310) | Loss 12.1367(13.1169) | Error 0.6486(0.6764) Steps 514(493.28) | Grad Norm 12.8180(21.4627) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 22.6379, Epoch Time 334.3983(312.2434), Bit/dim 4.7250(best: 4.7595), Xent 1.7673, Loss 5.6086, Error 0.6138(best: 0.6262)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0199 | Time 47.3357(46.0160) | Bit/dim 4.7207(5.0027) | Xent 1.7924(1.9268) | Loss 15.0336(13.1744) | Error 0.6391(0.6752) Steps 538(494.62) | Grad Norm 11.5707(21.1660) | Total Time 0.00(0.00)\n",
      "Iter 0200 | Time 45.9986(46.0154) | Bit/dim 4.7063(4.9938) | Xent 1.7996(1.9230) | Loss 11.8978(13.1361) | Error 0.6315(0.6739) Steps 520(495.38) | Grad Norm 12.2171(20.8975) | Total Time 0.00(0.00)\n",
      "Iter 0201 | Time 47.4505(46.0585) | Bit/dim 4.7115(4.9853) | Xent 1.8184(1.9199) | Loss 12.0976(13.1050) | Error 0.6376(0.6728) Steps 508(495.76) | Grad Norm 10.8640(20.5965) | Total Time 0.00(0.00)\n",
      "Iter 0202 | Time 52.5070(46.2519) | Bit/dim 4.6800(4.9762) | Xent 1.8074(1.9165) | Loss 12.1998(13.0778) | Error 0.6402(0.6719) Steps 508(496.13) | Grad Norm 17.3970(20.5005) | Total Time 0.00(0.00)\n",
      "Iter 0203 | Time 49.5639(46.3513) | Bit/dim 4.7190(4.9684) | Xent 1.8409(1.9142) | Loss 12.0048(13.0456) | Error 0.6385(0.6709) Steps 574(498.46) | Grad Norm 33.5241(20.8912) | Total Time 0.00(0.00)\n",
      "Iter 0204 | Time 47.5818(46.3882) | Bit/dim 4.6928(4.9602) | Xent 1.7745(1.9100) | Loss 12.0348(13.0153) | Error 0.6234(0.6694) Steps 526(499.29) | Grad Norm 6.2494(20.4520) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 22.6307, Epoch Time 329.0369(312.7472), Bit/dim 4.7080(best: 4.7250), Xent 1.9242, Loss 5.6701, Error 0.6762(best: 0.6138)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0205 | Time 46.4457(46.3899) | Bit/dim 4.6995(4.9524) | Xent 1.9328(1.9107) | Loss 14.9909(13.0746) | Error 0.6770(0.6697) Steps 550(500.81) | Grad Norm 52.0481(21.3998) | Total Time 0.00(0.00)\n",
      "Iter 0206 | Time 54.5914(46.6360) | Bit/dim 4.7033(4.9449) | Xent 1.8288(1.9083) | Loss 12.2162(13.0488) | Error 0.6489(0.6690) Steps 550(502.29) | Grad Norm 27.6598(21.5876) | Total Time 0.00(0.00)\n",
      "Iter 0207 | Time 49.7869(46.7305) | Bit/dim 4.6882(4.9372) | Xent 2.0180(1.9115) | Loss 12.2861(13.0259) | Error 0.6990(0.6699) Steps 532(503.18) | Grad Norm 61.7707(22.7931) | Total Time 0.00(0.00)\n",
      "Iter 0208 | Time 48.1630(46.7735) | Bit/dim 4.7331(4.9311) | Xent 1.9571(1.9129) | Loss 12.2852(13.0037) | Error 0.6775(0.6702) Steps 538(504.22) | Grad Norm 48.0696(23.5514) | Total Time 0.00(0.00)\n",
      "Iter 0209 | Time 50.0036(46.8704) | Bit/dim 4.7199(4.9247) | Xent 1.7998(1.9095) | Loss 11.8614(12.9694) | Error 0.6381(0.6692) Steps 532(505.06) | Grad Norm 11.3896(23.1866) | Total Time 0.00(0.00)\n",
      "Iter 0210 | Time 48.8723(46.9304) | Bit/dim 4.7046(4.9181) | Xent 1.8272(1.9070) | Loss 11.9886(12.9400) | Error 0.6541(0.6687) Steps 526(505.68) | Grad Norm 25.9104(23.2683) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 22.4180, Epoch Time 336.3961(313.4567), Bit/dim 4.6799(best: 4.7080), Xent 1.7718, Loss 5.5658, Error 0.6304(best: 0.6138)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0211 | Time 53.4523(47.1261) | Bit/dim 4.6741(4.9108) | Xent 1.8248(1.9046) | Loss 15.3589(13.0126) | Error 0.6442(0.6680) Steps 544(506.83) | Grad Norm 13.3166(22.9697) | Total Time 0.00(0.00)\n",
      "Iter 0212 | Time 45.9771(47.0916) | Bit/dim 4.7016(4.9045) | Xent 1.8303(1.9024) | Loss 11.9786(12.9816) | Error 0.6424(0.6672) Steps 550(508.13) | Grad Norm 23.0800(22.9730) | Total Time 0.00(0.00)\n",
      "Iter 0213 | Time 49.1524(47.1535) | Bit/dim 4.6688(4.8975) | Xent 1.7920(1.8990) | Loss 11.8274(12.9469) | Error 0.6416(0.6665) Steps 514(508.30) | Grad Norm 15.6453(22.7532) | Total Time 0.00(0.00)\n",
      "Iter 0214 | Time 47.1788(47.1542) | Bit/dim 4.7170(4.8920) | Xent 1.7876(1.8957) | Loss 12.0751(12.9208) | Error 0.6278(0.6653) Steps 520(508.66) | Grad Norm 16.5409(22.5668) | Total Time 0.00(0.00)\n",
      "Iter 0215 | Time 45.4740(47.1038) | Bit/dim 4.7076(4.8865) | Xent 1.7622(1.8917) | Loss 12.0185(12.8937) | Error 0.6198(0.6639) Steps 520(509.00) | Grad Norm 11.9868(22.2494) | Total Time 0.00(0.00)\n",
      "Iter 0216 | Time 47.5176(47.1162) | Bit/dim 4.6693(4.8800) | Xent 1.8173(1.8895) | Loss 12.0066(12.8671) | Error 0.6327(0.6630) Steps 538(509.87) | Grad Norm 23.2308(22.2789) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 22.5320, Epoch Time 326.6413(313.8522), Bit/dim 4.6586(best: 4.6799), Xent 1.7429, Loss 5.5300, Error 0.6114(best: 0.6138)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0217 | Time 44.8821(47.0492) | Bit/dim 4.6621(4.8735) | Xent 1.7919(1.8865) | Loss 14.6990(12.9221) | Error 0.6381(0.6623) Steps 520(510.17) | Grad Norm 14.3196(22.0401) | Total Time 0.00(0.00)\n",
      "Iter 0218 | Time 49.8165(47.1322) | Bit/dim 4.6398(4.8664) | Xent 1.8320(1.8849) | Loss 11.7712(12.8875) | Error 0.6459(0.6618) Steps 520(510.47) | Grad Norm 15.0012(21.8289) | Total Time 0.00(0.00)\n",
      "Iter 0219 | Time 48.8730(47.1844) | Bit/dim 4.6289(4.8593) | Xent 1.7862(1.8819) | Loss 12.0295(12.8618) | Error 0.6339(0.6609) Steps 550(511.65) | Grad Norm 12.4717(21.5482) | Total Time 0.00(0.00)\n",
      "Iter 0220 | Time 51.4689(47.3130) | Bit/dim 4.6488(4.8530) | Xent 1.8147(1.8799) | Loss 12.0796(12.8383) | Error 0.6438(0.6604) Steps 532(512.26) | Grad Norm 23.8874(21.6184) | Total Time 0.00(0.00)\n",
      "Iter 0221 | Time 50.9263(47.4214) | Bit/dim 4.6537(4.8470) | Xent 1.7523(1.8761) | Loss 12.0707(12.8153) | Error 0.6215(0.6593) Steps 556(513.57) | Grad Norm 21.5991(21.6178) | Total Time 0.00(0.00)\n",
      "Iter 0222 | Time 49.6143(47.4872) | Bit/dim 4.6294(4.8405) | Xent 1.7963(1.8737) | Loss 11.6701(12.7809) | Error 0.6380(0.6586) Steps 538(514.31) | Grad Norm 18.2672(21.5173) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 22.4445, Epoch Time 334.0765(314.4590), Bit/dim 4.6084(best: 4.6586), Xent 1.7469, Loss 5.4818, Error 0.6153(best: 0.6114)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0223 | Time 52.1981(47.6285) | Bit/dim 4.6178(4.8338) | Xent 1.7994(1.8715) | Loss 14.4793(12.8319) | Error 0.6390(0.6580) Steps 568(515.92) | Grad Norm 13.8512(21.2873) | Total Time 0.00(0.00)\n",
      "Iter 0224 | Time 52.0959(47.7625) | Bit/dim 4.6270(4.8276) | Xent 1.7680(1.8684) | Loss 11.7943(12.8008) | Error 0.6329(0.6573) Steps 604(518.56) | Grad Norm 23.7705(21.3618) | Total Time 0.00(0.00)\n",
      "Iter 0225 | Time 52.6150(47.9081) | Bit/dim 4.6112(4.8211) | Xent 1.7618(1.8652) | Loss 11.5962(12.7646) | Error 0.6236(0.6563) Steps 562(519.86) | Grad Norm 11.6330(21.0699) | Total Time 0.00(0.00)\n",
      "Iter 0226 | Time 47.4041(47.8930) | Bit/dim 4.6135(4.8149) | Xent 1.7635(1.8621) | Loss 11.8890(12.7384) | Error 0.6295(0.6555) Steps 538(520.41) | Grad Norm 22.6771(21.1182) | Total Time 0.00(0.00)\n",
      "Iter 0227 | Time 53.5524(48.0628) | Bit/dim 4.5954(4.8083) | Xent 1.7478(1.8587) | Loss 11.8173(12.7107) | Error 0.6188(0.6544) Steps 508(520.03) | Grad Norm 12.9486(20.8731) | Total Time 0.00(0.00)\n",
      "Iter 0228 | Time 51.9795(48.1803) | Bit/dim 4.6142(4.8025) | Xent 1.7715(1.8561) | Loss 11.8384(12.6846) | Error 0.6310(0.6537) Steps 586(522.01) | Grad Norm 21.8106(20.9012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 22.4675, Epoch Time 348.6245(315.4839), Bit/dim 4.6054(best: 4.6084), Xent 1.7057, Loss 5.4582, Error 0.6045(best: 0.6114)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0229 | Time 50.4183(48.2474) | Bit/dim 4.6069(4.7966) | Xent 1.7526(1.8530) | Loss 14.5966(12.7419) | Error 0.6238(0.6528) Steps 544(522.67) | Grad Norm 17.2608(20.7920) | Total Time 0.00(0.00)\n",
      "Iter 0230 | Time 47.4299(48.2229) | Bit/dim 4.5887(4.7904) | Xent 1.7372(1.8495) | Loss 11.7105(12.7110) | Error 0.6138(0.6516) Steps 562(523.85) | Grad Norm 10.0816(20.4707) | Total Time 0.00(0.00)\n",
      "Iter 0231 | Time 52.7474(48.3586) | Bit/dim 4.5938(4.7845) | Xent 1.7573(1.8467) | Loss 11.8891(12.6863) | Error 0.6234(0.6507) Steps 580(525.54) | Grad Norm 19.7648(20.4495) | Total Time 0.00(0.00)\n",
      "Iter 0232 | Time 52.2816(48.4763) | Bit/dim 4.5576(4.7777) | Xent 1.7398(1.8435) | Loss 11.7481(12.6582) | Error 0.6201(0.6498) Steps 526(525.55) | Grad Norm 14.1843(20.2615) | Total Time 0.00(0.00)\n",
      "Iter 0233 | Time 51.5210(48.5676) | Bit/dim 4.5676(4.7714) | Xent 1.7535(1.8408) | Loss 11.7592(12.6312) | Error 0.6274(0.6492) Steps 550(526.28) | Grad Norm 14.0673(20.0757) | Total Time 0.00(0.00)\n",
      "Iter 0234 | Time 49.9615(48.6095) | Bit/dim 4.5746(4.7655) | Xent 1.7311(1.8375) | Loss 11.6915(12.6030) | Error 0.6125(0.6481) Steps 550(527.00) | Grad Norm 13.5012(19.8785) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 22.3360, Epoch Time 342.8955(316.3063), Bit/dim 4.5594(best: 4.6054), Xent 1.6799, Loss 5.3993, Error 0.5918(best: 0.6045)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0235 | Time 53.8434(48.7665) | Bit/dim 4.5553(4.7592) | Xent 1.7352(1.8345) | Loss 14.7877(12.6686) | Error 0.6206(0.6472) Steps 586(528.77) | Grad Norm 19.1421(19.8564) | Total Time 0.00(0.00)\n",
      "Iter 0236 | Time 50.6237(48.8222) | Bit/dim 4.5538(4.7530) | Xent 1.7439(1.8317) | Loss 11.7886(12.6422) | Error 0.6222(0.6465) Steps 580(530.30) | Grad Norm 16.8053(19.7649) | Total Time 0.00(0.00)\n",
      "Iter 0237 | Time 53.7468(48.9699) | Bit/dim 4.5495(4.7469) | Xent 1.7418(1.8290) | Loss 11.8325(12.6179) | Error 0.6162(0.6456) Steps 544(530.71) | Grad Norm 20.9590(19.8007) | Total Time 0.00(0.00)\n",
      "Iter 0238 | Time 51.3713(49.0420) | Bit/dim 4.5503(4.7410) | Xent 1.7107(1.8255) | Loss 11.8063(12.5935) | Error 0.6046(0.6443) Steps 580(532.19) | Grad Norm 14.4819(19.6411) | Total Time 0.00(0.00)\n",
      "Iter 0239 | Time 47.3149(48.9902) | Bit/dim 4.5311(4.7347) | Xent 1.7089(1.8220) | Loss 11.1891(12.5514) | Error 0.6045(0.6431) Steps 580(533.63) | Grad Norm 11.3061(19.3911) | Total Time 0.00(0.00)\n",
      "Iter 0240 | Time 48.8422(48.9857) | Bit/dim 4.5346(4.7287) | Xent 1.7152(1.8188) | Loss 11.2605(12.5127) | Error 0.6059(0.6420) Steps 538(533.76) | Grad Norm 11.4608(19.1532) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 22.8278, Epoch Time 344.9475(317.1655), Bit/dim 4.5472(best: 4.5594), Xent 1.6902, Loss 5.3923, Error 0.6035(best: 0.5918)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0241 | Time 48.7798(48.9795) | Bit/dim 4.5526(4.7234) | Xent 1.7232(1.8159) | Loss 14.4257(12.5701) | Error 0.6110(0.6411) Steps 580(535.15) | Grad Norm 14.0935(19.0014) | Total Time 0.00(0.00)\n",
      "Iter 0242 | Time 47.8916(48.9469) | Bit/dim 4.5311(4.7177) | Xent 1.6849(1.8120) | Loss 11.4246(12.5357) | Error 0.5920(0.6396) Steps 544(535.41) | Grad Norm 4.8684(18.5774) | Total Time 0.00(0.00)\n",
      "Iter 0243 | Time 49.7329(48.9705) | Bit/dim 4.5580(4.7129) | Xent 1.7525(1.8102) | Loss 11.6867(12.5102) | Error 0.6122(0.6388) Steps 580(536.75) | Grad Norm 21.7358(18.6721) | Total Time 0.00(0.00)\n",
      "Iter 0244 | Time 49.9602(49.0002) | Bit/dim 4.5561(4.7082) | Xent 1.7714(1.8090) | Loss 11.4838(12.4794) | Error 0.6218(0.6383) Steps 556(537.33) | Grad Norm 25.5609(18.8788) | Total Time 0.00(0.00)\n",
      "Iter 0245 | Time 46.4907(48.9249) | Bit/dim 4.5221(4.7026) | Xent 1.6793(1.8052) | Loss 11.6297(12.4539) | Error 0.5960(0.6370) Steps 526(536.99) | Grad Norm 7.7839(18.5460) | Total Time 0.00(0.00)\n",
      "Iter 0246 | Time 50.4701(48.9712) | Bit/dim 4.5142(4.6969) | Xent 1.7219(1.8027) | Loss 11.5263(12.4261) | Error 0.6089(0.6362) Steps 568(537.92) | Grad Norm 20.5130(18.6050) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 21.9944, Epoch Time 331.3571(317.5913), Bit/dim 4.5105(best: 4.5472), Xent 1.6678, Loss 5.3445, Error 0.5916(best: 0.5918)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0247 | Time 52.6986(49.0831) | Bit/dim 4.5194(4.6916) | Xent 1.7229(1.8003) | Loss 14.8380(12.4985) | Error 0.6102(0.6354) Steps 544(538.10) | Grad Norm 29.6934(18.9376) | Total Time 0.00(0.00)\n",
      "Iter 0248 | Time 47.0008(49.0206) | Bit/dim 4.4949(4.6857) | Xent 1.7172(1.7978) | Loss 11.5910(12.4712) | Error 0.6138(0.6348) Steps 532(537.92) | Grad Norm 5.9240(18.5472) | Total Time 0.00(0.00)\n",
      "Iter 0249 | Time 48.0706(48.9921) | Bit/dim 4.5458(4.6815) | Xent 1.6712(1.7940) | Loss 11.4873(12.4417) | Error 0.5958(0.6336) Steps 526(537.56) | Grad Norm 9.1681(18.2658) | Total Time 0.00(0.00)\n",
      "Iter 0250 | Time 49.8619(49.0182) | Bit/dim 4.5522(4.6776) | Xent 1.6680(1.7902) | Loss 11.4362(12.4116) | Error 0.5890(0.6322) Steps 550(537.93) | Grad Norm 19.1267(18.2917) | Total Time 0.00(0.00)\n",
      "Iter 0251 | Time 50.1753(49.0529) | Bit/dim 4.5060(4.6725) | Xent 1.7176(1.7880) | Loss 11.4515(12.3828) | Error 0.6124(0.6316) Steps 562(538.65) | Grad Norm 16.9896(18.2526) | Total Time 0.00(0.00)\n",
      "Iter 0252 | Time 48.3703(49.0324) | Bit/dim 4.4909(4.6670) | Xent 1.7175(1.7859) | Loss 11.3064(12.3505) | Error 0.6099(0.6310) Steps 550(538.99) | Grad Norm 23.1632(18.3999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 22.4462, Epoch Time 334.6324(318.1025), Bit/dim 4.5099(best: 4.5105), Xent 1.6406, Loss 5.3302, Error 0.5790(best: 0.5916)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0253 | Time 52.4708(49.1356) | Bit/dim 4.5011(4.6620) | Xent 1.6964(1.7832) | Loss 14.1174(12.4035) | Error 0.6030(0.6302) Steps 580(540.22) | Grad Norm 21.0480(18.4794) | Total Time 0.00(0.00)\n",
      "Iter 0254 | Time 47.0447(49.0729) | Bit/dim 4.5037(4.6573) | Xent 1.6504(1.7792) | Loss 11.3320(12.3713) | Error 0.5881(0.6289) Steps 556(540.70) | Grad Norm 8.6787(18.1853) | Total Time 0.00(0.00)\n",
      "Iter 0255 | Time 45.4860(48.9652) | Bit/dim 4.4903(4.6523) | Xent 1.6956(1.7767) | Loss 11.3118(12.3395) | Error 0.6074(0.6283) Steps 544(540.80) | Grad Norm 12.4964(18.0147) | Total Time 0.00(0.00)\n",
      "Iter 0256 | Time 50.6516(49.0158) | Bit/dim 4.4746(4.6470) | Xent 1.7171(1.7749) | Loss 11.2572(12.3071) | Error 0.6008(0.6274) Steps 568(541.61) | Grad Norm 24.4329(18.2072) | Total Time 0.00(0.00)\n",
      "Iter 0257 | Time 50.0594(49.0471) | Bit/dim 4.5024(4.6426) | Xent 1.7010(1.7727) | Loss 11.1968(12.2738) | Error 0.6154(0.6271) Steps 556(542.04) | Grad Norm 31.5578(18.6077) | Total Time 0.00(0.00)\n",
      "Iter 0258 | Time 47.3136(48.9951) | Bit/dim 4.4908(4.6381) | Xent 1.6608(1.7694) | Loss 11.6039(12.2537) | Error 0.5933(0.6260) Steps 544(542.10) | Grad Norm 6.6497(18.2490) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 22.9880, Epoch Time 331.8183(318.5140), Bit/dim 4.4985(best: 4.5099), Xent 1.7288, Loss 5.3629, Error 0.6105(best: 0.5790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0259 | Time 50.6090(49.0436) | Bit/dim 4.4899(4.6336) | Xent 1.7848(1.7698) | Loss 14.4563(12.3197) | Error 0.6260(0.6260) Steps 526(541.62) | Grad Norm 46.2172(19.0880) | Total Time 0.00(0.00)\n",
      "Iter 0260 | Time 49.5435(49.0586) | Bit/dim 4.5089(4.6299) | Xent 1.6998(1.7677) | Loss 11.5145(12.2956) | Error 0.6038(0.6254) Steps 544(541.69) | Grad Norm 15.7380(18.9875) | Total Time 0.00(0.00)\n",
      "Iter 0261 | Time 47.1695(49.0019) | Bit/dim 4.4864(4.6256) | Xent 1.7799(1.7681) | Loss 11.5914(12.2745) | Error 0.6316(0.6256) Steps 550(541.94) | Grad Norm 47.0350(19.8290) | Total Time 0.00(0.00)\n",
      "Iter 0262 | Time 52.8672(49.1178) | Bit/dim 4.5243(4.6225) | Xent 1.7071(1.7663) | Loss 11.7431(12.2585) | Error 0.6066(0.6250) Steps 580(543.08) | Grad Norm 22.7829(19.9176) | Total Time 0.00(0.00)\n",
      "Iter 0263 | Time 51.1754(49.1796) | Bit/dim 4.4791(4.6182) | Xent 1.7936(1.7671) | Loss 11.6758(12.2410) | Error 0.6352(0.6253) Steps 568(543.83) | Grad Norm 42.1098(20.5833) | Total Time 0.00(0.00)\n",
      "Iter 0264 | Time 49.2490(49.1816) | Bit/dim 4.4758(4.6140) | Xent 1.7711(1.7672) | Loss 11.4289(12.2167) | Error 0.6181(0.6251) Steps 574(544.74) | Grad Norm 36.6969(21.0668) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 23.2338, Epoch Time 339.4626(319.1424), Bit/dim 4.4574(best: 4.4985), Xent 1.7308, Loss 5.3228, Error 0.6265(best: 0.5790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0265 | Time 49.9631(49.2051) | Bit/dim 4.4668(4.6095) | Xent 1.7951(1.7680) | Loss 14.5083(12.2854) | Error 0.6414(0.6256) Steps 562(545.25) | Grad Norm 31.0990(21.3677) | Total Time 0.00(0.00)\n",
      "Iter 0266 | Time 55.0440(49.3803) | Bit/dim 4.4733(4.6055) | Xent 1.6855(1.7656) | Loss 11.7637(12.2698) | Error 0.5921(0.6246) Steps 580(546.30) | Grad Norm 22.6043(21.4048) | Total Time 0.00(0.00)\n",
      "Iter 0267 | Time 53.7016(49.5099) | Bit/dim 4.4670(4.6013) | Xent 1.8054(1.7668) | Loss 11.6247(12.2504) | Error 0.6435(0.6251) Steps 592(547.67) | Grad Norm 25.1126(21.5161) | Total Time 0.00(0.00)\n",
      "Iter 0268 | Time 51.2525(49.5622) | Bit/dim 4.4698(4.5974) | Xent 1.7772(1.7671) | Loss 11.5763(12.2302) | Error 0.6292(0.6253) Steps 586(548.82) | Grad Norm 23.7007(21.5816) | Total Time 0.00(0.00)\n",
      "Iter 0269 | Time 53.8887(49.6920) | Bit/dim 4.4679(4.5935) | Xent 1.7089(1.7653) | Loss 11.6023(12.2114) | Error 0.6045(0.6246) Steps 598(550.29) | Grad Norm 10.8474(21.2596) | Total Time 0.00(0.00)\n",
      "Iter 0270 | Time 50.3204(49.7108) | Bit/dim 4.4427(4.5889) | Xent 1.7169(1.7639) | Loss 11.4523(12.1886) | Error 0.6054(0.6241) Steps 574(551.00) | Grad Norm 19.0284(21.1926) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 23.3063, Epoch Time 353.2482(320.1656), Bit/dim 4.4569(best: 4.4574), Xent 1.6388, Loss 5.2763, Error 0.5797(best: 0.5790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0271 | Time 55.0338(49.8705) | Bit/dim 4.4636(4.5852) | Xent 1.6777(1.7613) | Loss 14.6813(12.2634) | Error 0.6014(0.6234) Steps 544(550.79) | Grad Norm 16.2132(21.0432) | Total Time 0.00(0.00)\n",
      "Iter 0272 | Time 53.1562(49.9691) | Bit/dim 4.4731(4.5818) | Xent 1.6953(1.7593) | Loss 11.6211(12.2441) | Error 0.6051(0.6228) Steps 586(551.85) | Grad Norm 16.3797(20.9033) | Total Time 0.00(0.00)\n",
      "Iter 0273 | Time 47.3720(49.8912) | Bit/dim 4.4432(4.5777) | Xent 1.6719(1.7567) | Loss 11.1595(12.2116) | Error 0.5945(0.6220) Steps 538(551.43) | Grad Norm 16.0707(20.7584) | Total Time 0.00(0.00)\n",
      "Iter 0274 | Time 46.4461(49.7878) | Bit/dim 4.4323(4.5733) | Xent 1.6937(1.7548) | Loss 11.3158(12.1847) | Error 0.6021(0.6214) Steps 544(551.21) | Grad Norm 11.2064(20.4718) | Total Time 0.00(0.00)\n",
      "Iter 0275 | Time 48.2487(49.7416) | Bit/dim 4.4493(4.5696) | Xent 1.6690(1.7522) | Loss 11.5014(12.1642) | Error 0.5979(0.6207) Steps 574(551.89) | Grad Norm 15.6420(20.3269) | Total Time 0.00(0.00)\n",
      "Iter 0276 | Time 49.1056(49.7226) | Bit/dim 4.4224(4.5652) | Xent 1.6573(1.7494) | Loss 11.4140(12.1417) | Error 0.5827(0.6195) Steps 544(551.66) | Grad Norm 10.5349(20.0332) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 22.5953, Epoch Time 338.0847(320.7032), Bit/dim 4.4551(best: 4.4569), Xent 1.6228, Loss 5.2665, Error 0.5719(best: 0.5790)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0277 | Time 44.8084(49.5751) | Bit/dim 4.4405(4.5614) | Xent 1.6957(1.7478) | Loss 14.4370(12.2105) | Error 0.6006(0.6190) Steps 556(551.79) | Grad Norm 22.3255(20.1019) | Total Time 0.00(0.00)\n",
      "Iter 0278 | Time 47.1934(49.5037) | Bit/dim 4.4483(4.5580) | Xent 1.6760(1.7456) | Loss 11.1893(12.1799) | Error 0.6006(0.6184) Steps 544(551.55) | Grad Norm 17.3487(20.0193) | Total Time 0.00(0.00)\n",
      "Iter 0279 | Time 48.1520(49.4631) | Bit/dim 4.4619(4.5552) | Xent 1.6831(1.7437) | Loss 11.4707(12.1586) | Error 0.5954(0.6177) Steps 550(551.51) | Grad Norm 24.5939(20.1566) | Total Time 0.00(0.00)\n",
      "Iter 0280 | Time 47.7239(49.4110) | Bit/dim 4.4389(4.5517) | Xent 1.6551(1.7411) | Loss 11.2553(12.1315) | Error 0.5867(0.6168) Steps 520(550.56) | Grad Norm 17.2044(20.0680) | Total Time 0.00(0.00)\n",
      "Iter 0281 | Time 48.2045(49.3748) | Bit/dim 4.4189(4.5477) | Xent 1.6389(1.7380) | Loss 11.2375(12.1047) | Error 0.5837(0.6158) Steps 526(549.83) | Grad Norm 20.1373(20.0701) | Total Time 0.00(0.00)\n",
      "Iter 0282 | Time 50.8325(49.4185) | Bit/dim 4.4335(4.5443) | Xent 1.6785(1.7362) | Loss 11.4355(12.0846) | Error 0.5996(0.6153) Steps 556(550.01) | Grad Norm 24.4353(20.2010) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 22.2705, Epoch Time 325.1342(320.8361), Bit/dim 4.4043(best: 4.4551), Xent 1.5893, Loss 5.1990, Error 0.5627(best: 0.5719)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0283 | Time 50.7113(49.4573) | Bit/dim 4.4036(4.5400) | Xent 1.6505(1.7337) | Loss 13.9692(12.1412) | Error 0.5927(0.6147) Steps 556(550.19) | Grad Norm 8.2751(19.8433) | Total Time 0.00(0.00)\n",
      "Iter 0284 | Time 47.8335(49.4086) | Bit/dim 4.4232(4.5365) | Xent 1.7929(1.7354) | Loss 11.3438(12.1173) | Error 0.6291(0.6151) Steps 514(549.11) | Grad Norm 49.3372(20.7281) | Total Time 0.00(0.00)\n",
      "Iter 0285 | Time 48.0095(49.3666) | Bit/dim 4.4312(4.5334) | Xent 1.6478(1.7328) | Loss 11.3659(12.0947) | Error 0.5826(0.6141) Steps 556(549.31) | Grad Norm 23.5829(20.8137) | Total Time 0.00(0.00)\n",
      "Iter 0286 | Time 48.8755(49.3519) | Bit/dim 4.4396(4.5306) | Xent 1.7950(1.7347) | Loss 11.3293(12.0717) | Error 0.6399(0.6149) Steps 550(549.33) | Grad Norm 45.8871(21.5659) | Total Time 0.00(0.00)\n",
      "Iter 0287 | Time 51.5492(49.4178) | Bit/dim 4.4515(4.5282) | Xent 1.8101(1.7369) | Loss 11.5230(12.0553) | Error 0.6482(0.6159) Steps 562(549.71) | Grad Norm 29.9916(21.8187) | Total Time 0.00(0.00)\n",
      "Iter 0288 | Time 49.6828(49.4257) | Bit/dim 4.3944(4.5242) | Xent 1.6829(1.7353) | Loss 10.9788(12.0230) | Error 0.5953(0.6153) Steps 538(549.36) | Grad Norm 11.4503(21.5076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 22.4764, Epoch Time 335.2335(321.2680), Bit/dim 4.4776(best: 4.4043), Xent 1.7805, Loss 5.3678, Error 0.6384(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0289 | Time 52.1721(49.5081) | Bit/dim 4.4746(4.5227) | Xent 1.8492(1.7387) | Loss 14.6282(12.1011) | Error 0.6502(0.6163) Steps 574(550.10) | Grad Norm 40.7974(22.0863) | Total Time 0.00(0.00)\n",
      "Iter 0290 | Time 47.9561(49.4616) | Bit/dim 4.5281(4.5229) | Xent 1.7215(1.7382) | Loss 11.7445(12.0904) | Error 0.6018(0.6159) Steps 562(550.46) | Grad Norm 31.1476(22.3582) | Total Time 0.00(0.00)\n",
      "Iter 0291 | Time 49.4345(49.4608) | Bit/dim 4.4854(4.5217) | Xent 1.7904(1.7398) | Loss 11.5690(12.0748) | Error 0.6271(0.6162) Steps 568(550.98) | Grad Norm 28.5198(22.5430) | Total Time 0.00(0.00)\n",
      "Iter 0292 | Time 52.8023(49.5610) | Bit/dim 4.4429(4.5194) | Xent 1.7675(1.7406) | Loss 11.4055(12.0547) | Error 0.6275(0.6166) Steps 586(552.03) | Grad Norm 29.9172(22.7642) | Total Time 0.00(0.00)\n",
      "Iter 0293 | Time 47.5620(49.5010) | Bit/dim 4.4289(4.5166) | Xent 1.6920(1.7392) | Loss 11.2643(12.0310) | Error 0.6041(0.6162) Steps 562(552.33) | Grad Norm 17.8220(22.6160) | Total Time 0.00(0.00)\n",
      "Iter 0294 | Time 48.9317(49.4840) | Bit/dim 4.4431(4.5144) | Xent 1.7698(1.7401) | Loss 11.2847(12.0086) | Error 0.6301(0.6166) Steps 550(552.26) | Grad Norm 29.8039(22.8316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 23.1855, Epoch Time 338.3660(321.7810), Bit/dim 4.4215(best: 4.4043), Xent 1.6579, Loss 5.2505, Error 0.5910(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0295 | Time 47.3157(49.4189) | Bit/dim 4.4081(4.5113) | Xent 1.7130(1.7393) | Loss 14.0474(12.0698) | Error 0.6165(0.6166) Steps 562(552.56) | Grad Norm 16.7785(22.6500) | Total Time 0.00(0.00)\n",
      "Iter 0296 | Time 52.1981(49.5023) | Bit/dim 4.4383(4.5091) | Xent 1.6561(1.7368) | Loss 11.4407(12.0509) | Error 0.5879(0.6157) Steps 574(553.20) | Grad Norm 19.3740(22.5517) | Total Time 0.00(0.00)\n",
      "Iter 0297 | Time 53.9727(49.6364) | Bit/dim 4.4253(4.5066) | Xent 1.7025(1.7357) | Loss 11.4814(12.0338) | Error 0.6093(0.6155) Steps 574(553.82) | Grad Norm 14.1157(22.2987) | Total Time 0.00(0.00)\n",
      "Iter 0298 | Time 49.7296(49.6392) | Bit/dim 4.4075(4.5036) | Xent 1.7008(1.7347) | Loss 11.1242(12.0065) | Error 0.6078(0.6153) Steps 562(554.07) | Grad Norm 21.5303(22.2756) | Total Time 0.00(0.00)\n",
      "Iter 0299 | Time 47.5509(49.5765) | Bit/dim 4.3842(4.5000) | Xent 1.6957(1.7335) | Loss 11.0934(11.9791) | Error 0.5990(0.6148) Steps 562(554.31) | Grad Norm 13.5243(22.0131) | Total Time 0.00(0.00)\n",
      "Iter 0300 | Time 53.1902(49.6849) | Bit/dim 4.4368(4.4981) | Xent 1.6779(1.7319) | Loss 11.4353(11.9628) | Error 0.6018(0.6144) Steps 592(555.44) | Grad Norm 27.3394(22.1729) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 22.8877, Epoch Time 342.6955(322.4084), Bit/dim 4.4112(best: 4.4043), Xent 1.6199, Loss 5.2211, Error 0.5706(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0301 | Time 55.9570(49.8731) | Bit/dim 4.4175(4.4957) | Xent 1.6793(1.7303) | Loss 13.6385(12.0131) | Error 0.6030(0.6141) Steps 586(556.35) | Grad Norm 247.7276(28.9395) | Total Time 0.00(0.00)\n",
      "Iter 0302 | Time 54.6151(50.0154) | Bit/dim 4.4033(4.4929) | Xent 1.6700(1.7285) | Loss 11.2761(11.9910) | Error 0.5954(0.6135) Steps 610(557.96) | Grad Norm 538111.6087(16171.4196) | Total Time 0.00(0.00)\n",
      "Iter 0303 | Time 53.5116(50.1203) | Bit/dim 4.5058(4.4933) | Xent 1.7086(1.7279) | Loss 11.5170(11.9768) | Error 0.5916(0.6129) Steps 598(559.16) | Grad Norm 13994.5171(16106.1125) | Total Time 0.00(0.00)\n",
      "Iter 0304 | Time 58.3292(50.3665) | Bit/dim 4.6571(4.4982) | Xent 1.8846(1.7326) | Loss 12.0933(11.9803) | Error 0.6355(0.6135) Steps 592(560.15) | Grad Norm 611.9699(15641.2882) | Total Time 0.00(0.00)\n",
      "Iter 0305 | Time 58.1151(50.5990) | Bit/dim 4.5331(4.4993) | Xent 1.8265(1.7354) | Loss 11.9885(11.9805) | Error 0.6300(0.6140) Steps 628(562.18) | Grad Norm 1069.1828(15204.1251) | Total Time 0.00(0.00)\n",
      "Iter 0306 | Time 57.8076(50.8152) | Bit/dim 4.4667(4.4983) | Xent 1.7375(1.7355) | Loss 11.4700(11.9652) | Error 0.6168(0.6141) Steps 610(563.62) | Grad Norm 1014.6241(14778.4400) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 23.5937, Epoch Time 378.1711(324.0813), Bit/dim 4.4915(best: 4.4043), Xent 1.6279, Loss 5.3054, Error 0.5807(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0307 | Time 54.3908(50.9225) | Bit/dim 4.5005(4.4984) | Xent 1.6846(1.7339) | Loss 14.6441(12.0456) | Error 0.6010(0.6137) Steps 634(565.73) | Grad Norm 474521.8072(28570.7410) | Total Time 0.00(0.00)\n",
      "Iter 0308 | Time 58.8450(51.1602) | Bit/dim 4.5822(4.5009) | Xent 1.6809(1.7323) | Loss 11.7827(12.0377) | Error 0.5986(0.6133) Steps 658(568.50) | Grad Norm 589337139.2679(17707827.7969) | Total Time 0.00(0.00)\n",
      "Iter 0309 | Time 56.8441(51.3307) | Bit/dim 4.7327(4.5078) | Xent 1.7226(1.7320) | Loss 11.9975(12.0365) | Error 0.6228(0.6136) Steps 682(571.90) | Grad Norm 330601715888129.6250(9918068653236.8613) | Total Time 0.00(0.00)\n",
      "Iter 0310 | Time 64.2605(51.7186) | Bit/dim 4.8789(4.5190) | Xent 1.7370(1.7322) | Loss 12.4914(12.0501) | Error 0.6252(0.6139) Steps 658(574.49) | Grad Norm 57473696653.6107(9622250804539.3633) | Total Time 0.00(0.00)\n",
      "Iter 0311 | Time 69.0365(52.2381) | Bit/dim 5.0222(4.5341) | Xent 1.8035(1.7343) | Loss 12.7990(12.0726) | Error 0.6549(0.6151) Steps 670(577.35) | Grad Norm 32034954.9742(9333584241451.8301) | Total Time 0.00(0.00)\n",
      "Iter 0312 | Time 65.2975(52.6299) | Bit/dim 5.1446(4.5524) | Xent 1.8214(1.7369) | Loss 13.0094(12.1007) | Error 0.6675(0.6167) Steps 712(581.39) | Grad Norm 21122294.4528(9053577347877.1094) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 24.8711, Epoch Time 409.7586(326.6516), Bit/dim 5.2547(best: 4.4043), Xent 1.8114, Loss 6.1604, Error 0.6673(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0313 | Time 57.6579(52.7808) | Bit/dim 5.2515(4.5733) | Xent 1.8536(1.7404) | Loss 16.6003(12.2357) | Error 0.6806(0.6186) Steps 658(583.69) | Grad Norm 56669117.6440(8781971727514.3252) | Total Time 0.00(0.00)\n",
      "Iter 0314 | Time 58.3200(52.9469) | Bit/dim 5.3566(4.5968) | Xent 1.8645(1.7442) | Loss 13.1820(12.2641) | Error 0.6809(0.6205) Steps 604(584.30) | Grad Norm 2403.1166(8518512575760.9893) | Total Time 0.00(0.00)\n",
      "Iter 0315 | Time 55.6217(53.0272) | Bit/dim 5.3808(4.6204) | Xent 1.8845(1.7484) | Loss 13.6862(12.3067) | Error 0.6826(0.6224) Steps 634(585.79) | Grad Norm 2836.3278(8262957198573.2490) | Total Time 0.00(0.00)\n",
      "Iter 0316 | Time 54.2012(53.0624) | Bit/dim 5.3263(4.6415) | Xent 1.8613(1.7518) | Loss 13.6247(12.3463) | Error 0.6700(0.6238) Steps 664(588.14) | Grad Norm 318.1725(8015068482625.5967) | Total Time 0.00(0.00)\n",
      "Iter 0317 | Time 50.0350(52.9716) | Bit/dim 4.9866(4.6519) | Xent 1.9115(1.7566) | Loss 13.0329(12.3669) | Error 0.6522(0.6246) Steps 586(588.07) | Grad Norm 34.1864(7774616428147.8535) | Total Time 0.00(0.00)\n",
      "Iter 0318 | Time 53.3382(52.9826) | Bit/dim 4.6358(4.6514) | Xent 2.0293(1.7647) | Loss 11.9024(12.3529) | Error 0.6670(0.6259) Steps 568(587.47) | Grad Norm 50.0128(7541377935304.9180) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 23.8499, Epoch Time 369.1645(327.9270), Bit/dim 4.5470(best: 4.4043), Xent 1.6632, Loss 5.3787, Error 0.5886(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0319 | Time 54.6223(53.0318) | Bit/dim 4.5656(4.6488) | Xent 1.7569(1.7645) | Loss 15.1709(12.4375) | Error 0.6049(0.6253) Steps 628(588.69) | Grad Norm 22.2919(7315136597246.4395) | Total Time 0.00(0.00)\n",
      "Iter 0320 | Time 57.4458(53.1642) | Bit/dim 4.6033(4.6475) | Xent 2.0436(1.7729) | Loss 12.2034(12.4305) | Error 0.7151(0.6280) Steps 652(590.59) | Grad Norm 54.3755(7095682499330.6768) | Total Time 0.00(0.00)\n",
      "Iter 0321 | Time 57.9082(53.3065) | Bit/dim 4.6372(4.6472) | Xent 1.9466(1.7781) | Loss 12.3208(12.4272) | Error 0.6778(0.6295) Steps 604(590.99) | Grad Norm 36.2519(6882812024351.8438) | Total Time 0.00(0.00)\n",
      "Iter 0322 | Time 57.7217(53.4390) | Bit/dim 4.6165(4.6462) | Xent 1.8812(1.7812) | Loss 12.1902(12.4201) | Error 0.6525(0.6302) Steps 598(591.20) | Grad Norm 34.5913(6676327663622.3262) | Total Time 0.00(0.00)\n",
      "Iter 0323 | Time 50.6729(53.3560) | Bit/dim 4.5597(4.6436) | Xent 1.8500(1.7832) | Loss 11.7105(12.3988) | Error 0.6542(0.6309) Steps 556(590.14) | Grad Norm 29.1715(6476037833714.5312) | Total Time 0.00(0.00)\n",
      "Iter 0324 | Time 47.5526(53.1819) | Bit/dim 4.5371(4.6404) | Xent 1.7894(1.7834) | Loss 11.6382(12.3760) | Error 0.6338(0.6310) Steps 574(589.66) | Grad Norm 12.7365(6281756698703.4766) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 23.0694, Epoch Time 364.5242(329.0249), Bit/dim 4.5472(best: 4.4043), Xent 1.7390, Loss 5.4167, Error 0.6199(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0325 | Time 46.7915(52.9902) | Bit/dim 4.5461(4.6376) | Xent 1.7732(1.7831) | Loss 14.8463(12.4501) | Error 0.6282(0.6309) Steps 532(587.93) | Grad Norm 20.0800(6093303997742.9746) | Total Time 0.00(0.00)\n",
      "Iter 0326 | Time 48.2047(52.8466) | Bit/dim 4.5114(4.6338) | Xent 1.7772(1.7829) | Loss 11.1203(12.4102) | Error 0.6378(0.6311) Steps 502(585.35) | Grad Norm 7.7554(5910504877810.9180) | Total Time 0.00(0.00)\n",
      "Iter 0327 | Time 46.3763(52.6525) | Bit/dim 4.4668(4.6288) | Xent 1.8320(1.7844) | Loss 11.4299(12.3808) | Error 0.6545(0.6318) Steps 532(583.75) | Grad Norm 20.1780(5733189731477.1953) | Total Time 0.00(0.00)\n",
      "Iter 0328 | Time 48.5296(52.5288) | Bit/dim 4.4556(4.6236) | Xent 1.7965(1.7848) | Loss 11.4795(12.3537) | Error 0.6432(0.6321) Steps 544(582.56) | Grad Norm 7.8067(5561194039533.1133) | Total Time 0.00(0.00)\n",
      "Iter 0329 | Time 51.2416(52.4902) | Bit/dim 4.4725(4.6191) | Xent 1.7733(1.7844) | Loss 11.5140(12.3285) | Error 0.6334(0.6322) Steps 550(581.58) | Grad Norm 7.2796(5394358218347.3389) | Total Time 0.00(0.00)\n",
      "Iter 0330 | Time 48.6059(52.3737) | Bit/dim 4.4584(4.6143) | Xent 1.7653(1.7839) | Loss 11.2483(12.2961) | Error 0.6236(0.6319) Steps 562(580.99) | Grad Norm 10.5890(5232527471797.2363) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 22.7896, Epoch Time 328.4273(329.0070), Bit/dim 4.4328(best: 4.4043), Xent 1.6602, Loss 5.2629, Error 0.5982(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0331 | Time 50.5170(52.3179) | Bit/dim 4.4300(4.6087) | Xent 1.7362(1.7824) | Loss 14.5380(12.3634) | Error 0.6238(0.6317) Steps 574(580.78) | Grad Norm 9.9578(5075551647643.6182) | Total Time 0.00(0.00)\n",
      "Iter 0332 | Time 52.9516(52.3370) | Bit/dim 4.4282(4.6033) | Xent 1.7460(1.7813) | Loss 11.4965(12.3374) | Error 0.6190(0.6313) Steps 538(579.50) | Grad Norm 17.8466(4923285098214.8447) | Total Time 0.00(0.00)\n",
      "Iter 0333 | Time 49.0723(52.2390) | Bit/dim 4.4207(4.5978) | Xent 1.7150(1.7793) | Loss 11.2241(12.3040) | Error 0.6181(0.6309) Steps 544(578.43) | Grad Norm 15.4290(4775586545268.8623) | Total Time 0.00(0.00)\n",
      "Iter 0334 | Time 48.0976(52.1148) | Bit/dim 4.4486(4.5934) | Xent 1.7012(1.7770) | Loss 11.2161(12.2713) | Error 0.6024(0.6300) Steps 550(577.58) | Grad Norm 20.4945(4632318948911.4111) | Total Time 0.00(0.00)\n",
      "Iter 0335 | Time 48.5389(52.0075) | Bit/dim 4.4409(4.5888) | Xent 1.6920(1.7745) | Loss 11.3802(12.2446) | Error 0.6136(0.6296) Steps 562(577.11) | Grad Norm 24.2256(4493349380444.7949) | Total Time 0.00(0.00)\n",
      "Iter 0336 | Time 53.6069(52.0555) | Bit/dim 4.4335(4.5841) | Xent 1.6840(1.7717) | Loss 11.2800(12.2157) | Error 0.5953(0.6285) Steps 586(577.38) | Grad Norm 16.6423(4358548899031.9507) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 23.7750, Epoch Time 342.3990(329.4087), Bit/dim 4.4377(best: 4.4043), Xent 1.8191, Loss 5.3473, Error 0.6490(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0337 | Time 50.7722(52.0170) | Bit/dim 4.4171(4.5791) | Xent 1.8690(1.7747) | Loss 13.4446(12.2525) | Error 0.6549(0.6293) Steps 556(576.74) | Grad Norm 34.2227(4227792432062.0190) | Total Time 0.00(0.00)\n",
      "Iter 0338 | Time 50.1091(51.9598) | Bit/dim 4.4234(4.5745) | Xent 1.7948(1.7753) | Loss 11.4064(12.2272) | Error 0.6347(0.6295) Steps 556(576.12) | Grad Norm 20.1361(4100958659100.7622) | Total Time 0.00(0.00)\n",
      "Iter 0339 | Time 53.6374(52.0101) | Bit/dim 4.4792(4.5716) | Xent 1.8679(1.7780) | Loss 11.4475(12.2038) | Error 0.6430(0.6299) Steps 586(576.41) | Grad Norm 45.2875(3977929899329.0977) | Total Time 0.00(0.00)\n",
      "Iter 0340 | Time 55.0950(52.1026) | Bit/dim 4.4547(4.5681) | Xent 1.7652(1.7777) | Loss 11.7197(12.1892) | Error 0.6169(0.6295) Steps 544(575.44) | Grad Norm 25.7248(3858592002349.9966) | Total Time 0.00(0.00)\n",
      "Iter 0341 | Time 48.5917(51.9973) | Bit/dim 4.5004(4.5661) | Xent 1.6822(1.7748) | Loss 11.6947(12.1744) | Error 0.5956(0.6285) Steps 538(574.32) | Grad Norm 21.1257(3742834242280.1304) | Total Time 0.00(0.00)\n",
      "Iter 0342 | Time 47.7785(51.8707) | Bit/dim 4.5093(4.5644) | Xent 1.6771(1.7719) | Loss 10.8669(12.1352) | Error 0.5975(0.6275) Steps 544(573.41) | Grad Norm 10.5116(3630549215012.0420) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 22.3986, Epoch Time 344.7100(329.8678), Bit/dim 4.4218(best: 4.4043), Xent 1.6278, Loss 5.2357, Error 0.5773(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0343 | Time 49.9097(51.8119) | Bit/dim 4.4302(4.5603) | Xent 1.6904(1.7694) | Loss 14.3063(12.2003) | Error 0.6025(0.6268) Steps 568(573.25) | Grad Norm 12.2171(3521632738562.0474) | Total Time 0.00(0.00)\n",
      "Iter 0344 | Time 51.6306(51.8065) | Bit/dim 4.4549(4.5572) | Xent 1.7156(1.7678) | Loss 11.3766(12.1756) | Error 0.6135(0.6264) Steps 550(572.55) | Grad Norm 22.9995(3415983756405.8760) | Total Time 0.00(0.00)\n",
      "Iter 0345 | Time 53.2040(51.8484) | Bit/dim 4.4048(4.5526) | Xent 1.6835(1.7653) | Loss 11.2903(12.1490) | Error 0.6048(0.6257) Steps 562(572.23) | Grad Norm 10.4636(3313504243714.0137) | Total Time 0.00(0.00)\n",
      "Iter 0346 | Time 49.8265(51.7877) | Bit/dim 4.3904(4.5477) | Xent 1.6849(1.7629) | Loss 11.3442(12.1249) | Error 0.6111(0.6253) Steps 562(571.93) | Grad Norm 17.3254(3214099116403.1128) | Total Time 0.00(0.00)\n",
      "Iter 0347 | Time 52.9849(51.8237) | Bit/dim 4.4049(4.5434) | Xent 1.6545(1.7596) | Loss 11.2161(12.0976) | Error 0.5914(0.6243) Steps 544(571.09) | Grad Norm 5.9108(3117676142911.1968) | Total Time 0.00(0.00)\n",
      "Iter 0348 | Time 51.2108(51.8053) | Bit/dim 4.3790(4.5385) | Xent 1.7232(1.7585) | Loss 11.5618(12.0816) | Error 0.6142(0.6240) Steps 562(570.82) | Grad Norm 25.8738(3024145858624.6372) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 23.4832, Epoch Time 348.5378(330.4279), Bit/dim 4.3728(best: 4.4043), Xent 1.6232, Loss 5.1844, Error 0.5831(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0349 | Time 50.0473(51.7525) | Bit/dim 4.3762(4.5336) | Xent 1.7119(1.7571) | Loss 14.6621(12.1590) | Error 0.6040(0.6234) Steps 568(570.73) | Grad Norm 16.5833(2933421482866.3955) | Total Time 0.00(0.00)\n",
      "Iter 0350 | Time 46.5220(51.5956) | Bit/dim 4.3731(4.5288) | Xent 1.6928(1.7552) | Loss 11.2448(12.1316) | Error 0.6014(0.6227) Steps 526(569.39) | Grad Norm 19.1726(2845418838380.9785) | Total Time 0.00(0.00)\n",
      "Iter 0351 | Time 51.1360(51.5818) | Bit/dim 4.3854(4.5245) | Xent 1.6583(1.7523) | Loss 11.1955(12.1035) | Error 0.5919(0.6218) Steps 550(568.81) | Grad Norm 12.2244(2760056273229.9160) | Total Time 0.00(0.00)\n",
      "Iter 0352 | Time 50.2219(51.5410) | Bit/dim 4.3565(4.5195) | Xent 1.7006(1.7507) | Loss 11.1919(12.0761) | Error 0.6029(0.6212) Steps 574(568.96) | Grad Norm 22.5071(2677254585033.6938) | Total Time 0.00(0.00)\n",
      "Iter 0353 | Time 49.3955(51.4767) | Bit/dim 4.3672(4.5149) | Xent 1.7394(1.7504) | Loss 11.1160(12.0473) | Error 0.6186(0.6212) Steps 544(568.21) | Grad Norm 33.3432(2596936947483.6836) | Total Time 0.00(0.00)\n",
      "Iter 0354 | Time 48.0501(51.3739) | Bit/dim 4.3428(4.5098) | Xent 1.6467(1.7473) | Loss 10.9493(12.0144) | Error 0.5865(0.6201) Steps 538(567.31) | Grad Norm 13.0417(2519028839059.5640) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 22.9721, Epoch Time 333.9925(330.5348), Bit/dim 4.3375(best: 4.3728), Xent 1.6949, Loss 5.1849, Error 0.6091(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0355 | Time 52.1257(51.3964) | Bit/dim 4.3571(4.5052) | Xent 1.7511(1.7474) | Loss 14.5698(12.0910) | Error 0.6231(0.6202) Steps 550(566.79) | Grad Norm 33.4551(2443457973888.7803) | Total Time 0.00(0.00)\n",
      "Iter 0356 | Time 51.4552(51.3982) | Bit/dim 4.3706(4.5011) | Xent 1.7196(1.7466) | Loss 11.3690(12.0694) | Error 0.6230(0.6203) Steps 538(565.92) | Grad Norm 19.5093(2370154234672.7021) | Total Time 0.00(0.00)\n",
      "Iter 0357 | Time 48.2214(51.3029) | Bit/dim 4.3219(4.4958) | Xent 1.6978(1.7451) | Loss 11.0066(12.0375) | Error 0.6051(0.6198) Steps 556(565.63) | Grad Norm 19.4754(2299049607633.1055) | Total Time 0.00(0.00)\n",
      "Iter 0358 | Time 50.3380(51.2739) | Bit/dim 4.3464(4.4913) | Xent 1.6513(1.7423) | Loss 10.9640(12.0053) | Error 0.5921(0.6190) Steps 562(565.52) | Grad Norm 14.5746(2230078119404.5493) | Total Time 0.00(0.00)\n",
      "Iter 0359 | Time 49.7987(51.2297) | Bit/dim 4.3571(4.4873) | Xent 1.6184(1.7386) | Loss 10.9171(11.9726) | Error 0.5716(0.6176) Steps 538(564.69) | Grad Norm 9.8884(2163175775822.7095) | Total Time 0.00(0.00)\n",
      "Iter 0360 | Time 50.9815(51.2222) | Bit/dim 4.3486(4.4831) | Xent 1.6269(1.7352) | Loss 11.0625(11.9453) | Error 0.5886(0.6167) Steps 574(564.97) | Grad Norm 14.3395(2098280502548.4583) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 22.5766, Epoch Time 341.5062(330.8640), Bit/dim 4.3271(best: 4.3375), Xent 1.5884, Loss 5.1213, Error 0.5707(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0361 | Time 50.0048(51.1857) | Bit/dim 4.3173(4.4781) | Xent 1.6667(1.7332) | Loss 14.1916(12.0127) | Error 0.5968(0.6161) Steps 520(563.62) | Grad Norm 17.1900(2035332087472.5200) | Total Time 0.00(0.00)\n",
      "Iter 0362 | Time 51.2273(51.1870) | Bit/dim 4.3263(4.4736) | Xent 1.6243(1.7299) | Loss 10.9918(11.9821) | Error 0.5842(0.6152) Steps 562(563.57) | Grad Norm 8.3343(1974272124848.5945) | Total Time 0.00(0.00)\n",
      "Iter 0363 | Time 52.0230(51.2120) | Bit/dim 4.3208(4.4690) | Xent 1.6768(1.7283) | Loss 11.0036(11.9527) | Error 0.5884(0.6144) Steps 538(562.81) | Grad Norm 28.7464(1915043961103.9988) | Total Time 0.00(0.00)\n",
      "Iter 0364 | Time 51.6137(51.2241) | Bit/dim 4.3410(4.4651) | Xent 1.6137(1.7249) | Loss 10.5208(11.9098) | Error 0.5730(0.6131) Steps 568(562.96) | Grad Norm 11.4192(1857592642271.2212) | Total Time 0.00(0.00)\n",
      "Iter 0365 | Time 50.3349(51.1974) | Bit/dim 4.3168(4.4607) | Xent 1.6243(1.7219) | Loss 11.0964(11.8854) | Error 0.5887(0.6124) Steps 556(562.75) | Grad Norm 22.8975(1801864863003.7715) | Total Time 0.00(0.00)\n",
      "Iter 0366 | Time 50.2066(51.1677) | Bit/dim 4.3181(4.4564) | Xent 1.6013(1.7182) | Loss 11.1471(11.8632) | Error 0.5705(0.6111) Steps 544(562.19) | Grad Norm 11.1414(1747808917113.9924) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 22.3716, Epoch Time 343.7096(331.2493), Bit/dim 4.3240(best: 4.3271), Xent 1.5544, Loss 5.1012, Error 0.5567(best: 0.5627)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0367 | Time 49.9015(51.1297) | Bit/dim 4.3107(4.4520) | Xent 1.6149(1.7151) | Loss 13.7498(11.9198) | Error 0.5747(0.6100) Steps 562(562.19) | Grad Norm 14.4000(1695374649601.0044) | Total Time 0.00(0.00)\n",
      "Iter 0368 | Time 50.1424(51.1001) | Bit/dim 4.3033(4.4476) | Xent 1.5728(1.7109) | Loss 10.9294(11.8901) | Error 0.5620(0.6086) Steps 556(562.00) | Grad Norm 6.2847(1644513410113.1626) | Total Time 0.00(0.00)\n",
      "Iter 0369 | Time 53.0271(51.1579) | Bit/dim 4.3195(4.4437) | Xent 1.5664(1.7065) | Loss 11.0405(11.8646) | Error 0.5586(0.6071) Steps 592(562.90) | Grad Norm 13.4536(1595178007810.1711) | Total Time 0.00(0.00)\n",
      "Iter 0370 | Time 53.2189(51.2197) | Bit/dim 4.3039(4.4395) | Xent 1.5704(1.7024) | Loss 11.0795(11.8411) | Error 0.5551(0.6055) Steps 562(562.87) | Grad Norm 9.5783(1547322667576.1533) | Total Time 0.00(0.00)\n",
      "Iter 0371 | Time 51.6535(51.2327) | Bit/dim 4.2899(4.4351) | Xent 1.5841(1.6989) | Loss 10.9127(11.8132) | Error 0.5690(0.6044) Steps 568(563.03) | Grad Norm 4.8706(1500902987549.0146) | Total Time 0.00(0.00)\n",
      "Iter 0372 | Time 54.0316(51.3167) | Bit/dim 4.2953(4.4309) | Xent 1.6011(1.6960) | Loss 11.0244(11.7896) | Error 0.5773(0.6036) Steps 538(562.28) | Grad Norm 10.5948(1455875897922.8621) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 23.5585, Epoch Time 351.3781(331.8532), Bit/dim 4.2940(best: 4.3240), Xent 1.5420, Loss 5.0650, Error 0.5475(best: 0.5567)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0373 | Time 48.7487(51.2397) | Bit/dim 4.3011(4.4270) | Xent 1.5977(1.6930) | Loss 13.5947(11.8437) | Error 0.5765(0.6028) Steps 538(561.55) | Grad Norm 20.9925(1412199620985.8062) | Total Time 0.00(0.00)\n",
      "Iter 0374 | Time 50.6818(51.2229) | Bit/dim 4.2964(4.4231) | Xent 1.5788(1.6896) | Loss 11.0152(11.8189) | Error 0.5646(0.6017) Steps 550(561.20) | Grad Norm 15.5990(1369833632356.7000) | Total Time 0.00(0.00)\n",
      "Iter 0375 | Time 52.9582(51.2750) | Bit/dim 4.2875(4.4190) | Xent 1.5773(1.6862) | Loss 11.0191(11.7949) | Error 0.5620(0.6005) Steps 580(561.77) | Grad Norm 10.2352(1328738623386.3062) | Total Time 0.00(0.00)\n",
      "Iter 0376 | Time 50.3556(51.2474) | Bit/dim 4.2825(4.4149) | Xent 1.6240(1.6844) | Loss 10.8514(11.7666) | Error 0.5875(0.6001) Steps 550(561.41) | Grad Norm 28.2974(1288876464685.5659) | Total Time 0.00(0.00)\n",
      "Iter 0377 | Time 52.0217(51.2706) | Bit/dim 4.2694(4.4105) | Xent 1.5599(1.6806) | Loss 10.8544(11.7392) | Error 0.5615(0.5989) Steps 574(561.79) | Grad Norm 9.6787(1250210170745.2893) | Total Time 0.00(0.00)\n",
      "Iter 0378 | Time 49.0092(51.2028) | Bit/dim 4.3008(4.4072) | Xent 1.7906(1.6839) | Loss 11.2282(11.7239) | Error 0.6299(0.5999) Steps 556(561.62) | Grad Norm 40.6039(1212703865624.1487) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 22.8354, Epoch Time 342.3144(332.1670), Bit/dim 4.3050(best: 4.2940), Xent 1.5848, Loss 5.0974, Error 0.5653(best: 0.5475)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0379 | Time 48.9170(51.1342) | Bit/dim 4.3043(4.4041) | Xent 1.6703(1.6835) | Loss 14.2580(11.7999) | Error 0.5927(0.5996) Steps 538(560.91) | Grad Norm 23.2346(1176322749656.1211) | Total Time 0.00(0.00)\n",
      "Iter 0380 | Time 51.9307(51.1581) | Bit/dim 4.3133(4.4014) | Xent 1.5991(1.6810) | Loss 11.0806(11.7783) | Error 0.5701(0.5988) Steps 562(560.94) | Grad Norm 21.2173(1141033067167.0740) | Total Time 0.00(0.00)\n",
      "Iter 0381 | Time 48.6116(51.0817) | Bit/dim 4.2999(4.3984) | Xent 1.5965(1.6784) | Loss 10.6653(11.7449) | Error 0.5697(0.5979) Steps 538(560.25) | Grad Norm 16.6764(1106802075152.5620) | Total Time 0.00(0.00)\n",
      "Iter 0382 | Time 49.6321(51.0382) | Bit/dim 4.2649(4.3944) | Xent 1.6241(1.6768) | Loss 10.7810(11.7160) | Error 0.5769(0.5973) Steps 538(559.58) | Grad Norm 10.2002(1073598012898.2911) | Total Time 0.00(0.00)\n",
      "Iter 0383 | Time 54.6543(51.1467) | Bit/dim 4.2577(4.3903) | Xent 1.6726(1.6767) | Loss 10.9538(11.6931) | Error 0.6026(0.5974) Steps 586(560.38) | Grad Norm 23.3600(1041390072512.0432) | Total Time 0.00(0.00)\n",
      "Iter 0384 | Time 49.0964(51.0852) | Bit/dim 4.2660(4.3865) | Xent 1.5585(1.6731) | Loss 10.8242(11.6671) | Error 0.5625(0.5964) Steps 544(559.89) | Grad Norm 7.1894(1010148370336.8976) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 22.4470, Epoch Time 341.3397(332.4422), Bit/dim 4.2671(best: 4.2940), Xent 1.5284, Loss 5.0313, Error 0.5458(best: 0.5475)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0385 | Time 46.9635(50.9615) | Bit/dim 4.2721(4.3831) | Xent 1.5946(1.6708) | Loss 13.7920(11.7308) | Error 0.5634(0.5954) Steps 574(560.31) | Grad Norm 16.1197(979843919227.2743) | Total Time 0.00(0.00)\n",
      "Iter 0386 | Time 49.3363(50.9128) | Bit/dim 4.2851(4.3802) | Xent 1.6094(1.6689) | Loss 11.0049(11.7090) | Error 0.5773(0.5948) Steps 538(559.64) | Grad Norm 14.2998(950448601650.8850) | Total Time 0.00(0.00)\n",
      "Iter 0387 | Time 51.7396(50.9376) | Bit/dim 4.2679(4.3768) | Xent 1.5554(1.6655) | Loss 10.8738(11.6840) | Error 0.5599(0.5938) Steps 568(559.89) | Grad Norm 7.5309(921935143601.5844) | Total Time 0.00(0.00)\n",
      "Iter 0388 | Time 50.0465(50.9109) | Bit/dim 4.3424(4.3758) | Xent 1.7073(1.6668) | Loss 10.8724(11.6596) | Error 0.5942(0.5938) Steps 550(559.59) | Grad Norm 26.0198(894277089294.3174) | Total Time 0.00(0.00)\n",
      "Iter 0389 | Time 54.3055(51.0127) | Bit/dim 4.3461(4.3749) | Xent 1.6257(1.6656) | Loss 11.2771(11.6482) | Error 0.5733(0.5932) Steps 592(560.57) | Grad Norm 19.3818(867448776616.0692) | Total Time 0.00(0.00)\n",
      "Iter 0390 | Time 51.1673(51.0173) | Bit/dim 4.2786(4.3720) | Xent 1.6073(1.6638) | Loss 11.0468(11.6301) | Error 0.5744(0.5926) Steps 562(560.61) | Grad Norm 20.3378(841425313318.1973) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 23.1876, Epoch Time 342.4560(332.7426), Bit/dim 4.2769(best: 4.2671), Xent 1.4727, Loss 5.0133, Error 0.5246(best: 0.5458)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0391 | Time 48.0371(50.9279) | Bit/dim 4.2724(4.3690) | Xent 1.5152(1.6594) | Loss 13.8598(11.6970) | Error 0.5346(0.5909) Steps 544(560.11) | Grad Norm 9.0600(816182553918.9232) | Total Time 0.00(0.00)\n",
      "Iter 0392 | Time 49.4821(50.8846) | Bit/dim 4.2394(4.3651) | Xent 1.5895(1.6573) | Loss 10.8151(11.6706) | Error 0.5682(0.5902) Steps 538(559.45) | Grad Norm 15.6239(791697077301.8242) | Total Time 0.00(0.00)\n",
      "Iter 0393 | Time 49.1053(50.8312) | Bit/dim 4.2381(4.3613) | Xent 1.5769(1.6548) | Loss 10.7632(11.6433) | Error 0.5565(0.5892) Steps 550(559.16) | Grad Norm 15.1775(767946164983.2249) | Total Time 0.00(0.00)\n",
      "Iter 0394 | Time 48.3764(50.7575) | Bit/dim 4.2386(4.3576) | Xent 1.5998(1.6532) | Loss 10.8169(11.6185) | Error 0.5702(0.5886) Steps 550(558.89) | Grad Norm 17.7584(744907780034.2607) | Total Time 0.00(0.00)\n",
      "Iter 0395 | Time 48.6756(50.6951) | Bit/dim 4.2520(4.3545) | Xent 1.5206(1.6492) | Loss 10.7436(11.5923) | Error 0.5451(0.5873) Steps 550(558.62) | Grad Norm 9.8845(722560546633.5294) | Total Time 0.00(0.00)\n",
      "Iter 0396 | Time 51.5483(50.7207) | Bit/dim 4.2306(4.3507) | Xent 1.5625(1.6466) | Loss 10.8121(11.5689) | Error 0.5653(0.5867) Steps 556(558.54) | Grad Norm 18.9045(700883730235.0907) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 22.8894, Epoch Time 334.1038(332.7834), Bit/dim 4.2280(best: 4.2671), Xent 1.4600, Loss 4.9580, Error 0.5219(best: 0.5246)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0397 | Time 53.1775(50.7944) | Bit/dim 4.2229(4.3469) | Xent 1.5232(1.6429) | Loss 13.6879(11.6325) | Error 0.5434(0.5854) Steps 586(559.37) | Grad Norm 10.4573(679857218328.3517) | Total Time 0.00(0.00)\n",
      "Iter 0398 | Time 49.9458(50.7689) | Bit/dim 4.2206(4.3431) | Xent 1.4796(1.6380) | Loss 10.5927(11.6013) | Error 0.5345(0.5838) Steps 562(559.45) | Grad Norm 10.0277(659461501778.8019) | Total Time 0.00(0.00)\n",
      "Iter 0399 | Time 55.3594(50.9066) | Bit/dim 4.2206(4.3394) | Xent 1.5001(1.6339) | Loss 10.8556(11.5789) | Error 0.5457(0.5827) Steps 580(560.06) | Grad Norm 12.0592(639677656725.7997) | Total Time 0.00(0.00)\n",
      "Iter 0400 | Time 49.1804(50.8548) | Bit/dim 4.2223(4.3359) | Xent 1.5147(1.6303) | Loss 10.6218(11.5502) | Error 0.5494(0.5817) Steps 562(560.12) | Grad Norm 16.0215(620487327024.5062) | Total Time 0.00(0.00)\n",
      "Iter 0401 | Time 48.7639(50.7921) | Bit/dim 4.2102(4.3322) | Xent 1.4946(1.6262) | Loss 10.6308(11.5226) | Error 0.5369(0.5803) Steps 556(560.00) | Grad Norm 5.0678(601872707213.9230) | Total Time 0.00(0.00)\n",
      "Iter 0402 | Time 49.6174(50.7569) | Bit/dim 4.2203(4.3288) | Xent 1.5305(1.6234) | Loss 10.6583(11.4967) | Error 0.5468(0.5793) Steps 544(559.52) | Grad Norm 19.9056(583816525998.1024) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 23.0283, Epoch Time 345.3999(333.1619), Bit/dim 4.2350(best: 4.2280), Xent 1.4355, Loss 4.9528, Error 0.5058(best: 0.5219)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0403 | Time 48.5780(50.6915) | Bit/dim 4.2414(4.3262) | Xent 1.5107(1.6200) | Loss 13.6901(11.5625) | Error 0.5386(0.5781) Steps 526(558.51) | Grad Norm 13.2941(566302030218.5581) | Total Time 0.00(0.00)\n",
      "Iter 0404 | Time 53.7406(50.7830) | Bit/dim 4.2169(4.3229) | Xent 1.4730(1.6156) | Loss 10.7554(11.5383) | Error 0.5370(0.5769) Steps 562(558.62) | Grad Norm 7.4877(549312969312.2260) | Total Time 0.00(0.00)\n",
      "Iter 0405 | Time 51.5674(50.8065) | Bit/dim 4.2222(4.3199) | Xent 1.4690(1.6112) | Loss 10.8244(11.5168) | Error 0.5231(0.5753) Steps 544(558.18) | Grad Norm 9.4288(532833580233.1420) | Total Time 0.00(0.00)\n",
      "Iter 0406 | Time 53.9202(50.8999) | Bit/dim 4.2364(4.3174) | Xent 1.4696(1.6069) | Loss 10.8520(11.4969) | Error 0.5224(0.5737) Steps 568(558.47) | Grad Norm 14.8534(516848572826.5933) | Total Time 0.00(0.00)\n",
      "Iter 0407 | Time 51.2701(50.9110) | Bit/dim 4.2185(4.3144) | Xent 1.4879(1.6034) | Loss 10.7857(11.4756) | Error 0.5389(0.5726) Steps 580(559.12) | Grad Norm 6.7077(501343115641.9968) | Total Time 0.00(0.00)\n",
      "Iter 0408 | Time 52.6269(50.9625) | Bit/dim 4.2165(4.3115) | Xent 1.5032(1.6004) | Loss 10.8556(11.4570) | Error 0.5306(0.5714) Steps 604(560.47) | Grad Norm 19.2562(486302822173.3146) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 23.8555, Epoch Time 351.4142(333.7095), Bit/dim 4.1994(best: 4.2280), Xent 1.4044, Loss 4.9015, Error 0.5034(best: 0.5058)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0409 | Time 52.0951(50.9965) | Bit/dim 4.1888(4.3078) | Xent 1.4475(1.5958) | Loss 13.5475(11.5197) | Error 0.5188(0.5698) Steps 586(561.23) | Grad Norm 11.0294(471713737508.4460) | Total Time 0.00(0.00)\n",
      "Iter 0410 | Time 51.3824(51.0081) | Bit/dim 4.1732(4.3037) | Xent 1.4721(1.5921) | Loss 10.3816(11.4855) | Error 0.5249(0.5685) Steps 586(561.97) | Grad Norm 7.6105(457562325383.4209) | Total Time 0.00(0.00)\n",
      "Iter 0411 | Time 53.6960(51.0887) | Bit/dim 4.2098(4.3009) | Xent 1.4376(1.5874) | Loss 10.6133(11.4594) | Error 0.5210(0.5670) Steps 598(563.06) | Grad Norm 13.7407(443835455622.3305) | Total Time 0.00(0.00)\n",
      "Iter 0412 | Time 52.2723(51.1242) | Bit/dim 4.1976(4.2978) | Xent 1.4435(1.5831) | Loss 10.6703(11.4357) | Error 0.5120(0.5654) Steps 550(562.66) | Grad Norm 15.4116(430520391954.1229) | Total Time 0.00(0.00)\n",
      "Iter 0413 | Time 52.3827(51.1620) | Bit/dim 4.1872(4.2945) | Xent 1.4682(1.5797) | Loss 10.6793(11.4130) | Error 0.5260(0.5642) Steps 550(562.28) | Grad Norm 14.7277(417604780195.9410) | Total Time 0.00(0.00)\n",
      "Iter 0414 | Time 53.7426(51.2394) | Bit/dim 4.2070(4.2919) | Xent 1.4524(1.5758) | Loss 10.6372(11.3897) | Error 0.5225(0.5629) Steps 610(563.71) | Grad Norm 17.5937(405076636790.5906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 23.0266, Epoch Time 354.6524(334.3378), Bit/dim 4.1879(best: 4.1994), Xent 1.4273, Loss 4.9016, Error 0.5209(best: 0.5034)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0415 | Time 51.0130(51.2326) | Bit/dim 4.2029(4.2892) | Xent 1.4651(1.5725) | Loss 13.8210(11.4627) | Error 0.5245(0.5618) Steps 592(564.56) | Grad Norm 12.5410(392924337687.2491) | Total Time 0.00(0.00)\n",
      "Iter 0416 | Time 48.5557(51.1523) | Bit/dim 4.1773(4.2859) | Xent 1.4710(1.5695) | Loss 10.6059(11.4370) | Error 0.5188(0.5605) Steps 532(563.59) | Grad Norm 24.2901(381136607557.3604) | Total Time 0.00(0.00)\n",
      "Iter 0417 | Time 50.3471(51.1281) | Bit/dim 4.1850(4.2828) | Xent 1.4361(1.5655) | Loss 10.8079(11.4181) | Error 0.5129(0.5591) Steps 568(563.72) | Grad Norm 13.6245(369702509331.0483) | Total Time 0.00(0.00)\n",
      "Iter 0418 | Time 48.9018(51.0613) | Bit/dim 4.1734(4.2795) | Xent 1.4157(1.5610) | Loss 10.2151(11.3820) | Error 0.5175(0.5578) Steps 574(564.03) | Grad Norm 11.0065(358611434051.4470) | Total Time 0.00(0.00)\n",
      "Iter 0419 | Time 52.7721(51.1127) | Bit/dim 4.1825(4.2766) | Xent 1.4589(1.5579) | Loss 10.5341(11.3566) | Error 0.5245(0.5568) Steps 550(563.61) | Grad Norm 17.9410(347853091030.4418) | Total Time 0.00(0.00)\n",
      "Iter 0420 | Time 49.7441(51.0716) | Bit/dim 4.1770(4.2736) | Xent 1.4396(1.5544) | Loss 10.5860(11.3335) | Error 0.5195(0.5557) Steps 562(563.56) | Grad Norm 10.8166(337417498299.8531) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 23.7397, Epoch Time 340.9313(334.5356), Bit/dim 4.1655(best: 4.1879), Xent 1.4092, Loss 4.8701, Error 0.5087(best: 0.5034)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0421 | Time 49.8248(51.0342) | Bit/dim 4.1701(4.2705) | Xent 1.4575(1.5515) | Loss 13.4693(11.3975) | Error 0.5285(0.5549) Steps 544(562.97) | Grad Norm 16.9502(327294973351.3660) | Total Time 0.00(0.00)\n",
      "Iter 0422 | Time 50.4175(51.0157) | Bit/dim 4.1738(4.2676) | Xent 1.4277(1.5478) | Loss 10.6014(11.3736) | Error 0.5112(0.5536) Steps 544(562.40) | Grad Norm 11.0025(317476124151.1550) | Total Time 0.00(0.00)\n",
      "Iter 0423 | Time 50.9391(51.0134) | Bit/dim 4.1772(4.2649) | Xent 1.4519(1.5449) | Loss 10.7305(11.3543) | Error 0.5237(0.5527) Steps 526(561.31) | Grad Norm 12.8940(307951840427.0072) | Total Time 0.00(0.00)\n",
      "Iter 0424 | Time 49.6124(50.9714) | Bit/dim 4.1781(4.2623) | Xent 1.4137(1.5409) | Loss 10.5186(11.3293) | Error 0.5095(0.5514) Steps 568(561.51) | Grad Norm 8.0951(298713285214.4398) | Total Time 0.00(0.00)\n",
      "Iter 0425 | Time 52.0450(51.0036) | Bit/dim 4.1899(4.2601) | Xent 1.6712(1.5448) | Loss 10.9280(11.3172) | Error 0.5925(0.5526) Steps 550(561.17) | Grad Norm 17.4300(289751886658.5295) | Total Time 0.00(0.00)\n",
      "Iter 0426 | Time 52.1277(51.0373) | Bit/dim 4.1667(4.2573) | Xent 1.5451(1.5449) | Loss 10.6966(11.2986) | Error 0.5484(0.5525) Steps 568(561.37) | Grad Norm 10.5103(281059330059.0889) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 22.9213, Epoch Time 344.0226(334.8202), Bit/dim 4.1691(best: 4.1655), Xent 1.3666, Loss 4.8525, Error 0.4915(best: 0.5034)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0427 | Time 50.8687(51.0322) | Bit/dim 4.1638(4.2545) | Xent 1.4237(1.5412) | Loss 13.1745(11.3549) | Error 0.5142(0.5513) Steps 562(561.39) | Grad Norm 13.9239(272627550157.7339) | Total Time 0.00(0.00)\n",
      "Iter 0428 | Time 49.7843(50.9948) | Bit/dim 4.2454(4.2543) | Xent 1.5083(1.5402) | Loss 10.7108(11.3356) | Error 0.5573(0.5515) Steps 580(561.95) | Grad Norm 20.6566(264448723653.6216) | Total Time 0.00(0.00)\n",
      "Iter 0429 | Time 52.9733(51.0542) | Bit/dim 4.2089(4.2529) | Xent 1.4365(1.5371) | Loss 10.7052(11.3167) | Error 0.5169(0.5505) Steps 574(562.31) | Grad Norm 9.5170(256515261944.2985) | Total Time 0.00(0.00)\n",
      "Iter 0430 | Time 52.4748(51.0968) | Bit/dim 4.2389(4.2525) | Xent 1.5205(1.5366) | Loss 10.7445(11.2995) | Error 0.5457(0.5503) Steps 532(561.40) | Grad Norm 24.3894(248819804086.7012) | Total Time 0.00(0.00)\n",
      "Iter 0431 | Time 49.5907(51.0516) | Bit/dim 4.2137(4.2513) | Xent 1.4698(1.5346) | Loss 10.5891(11.2782) | Error 0.5348(0.5499) Steps 550(561.06) | Grad Norm 14.6117(241355209964.5385) | Total Time 0.00(0.00)\n",
      "Iter 0432 | Time 50.7082(51.0413) | Bit/dim 4.2402(4.2510) | Xent 1.5544(1.5352) | Loss 10.9637(11.2688) | Error 0.5665(0.5504) Steps 574(561.45) | Grad Norm 22.3601(234114553666.2732) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 22.8384, Epoch Time 345.0379(335.1267), Bit/dim 4.2106(best: 4.1655), Xent 1.4820, Loss 4.9516, Error 0.5200(best: 0.4915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0433 | Time 51.1327(51.0440) | Bit/dim 4.2106(4.2498) | Xent 1.5363(1.5352) | Loss 13.7780(11.3440) | Error 0.5468(0.5503) Steps 580(562.00) | Grad Norm 22.8168(227091117056.9695) | Total Time 0.00(0.00)\n",
      "Iter 0434 | Time 51.5719(51.0599) | Bit/dim 4.1949(4.2481) | Xent 1.4298(1.5321) | Loss 10.7567(11.3264) | Error 0.5125(0.5491) Steps 568(562.18) | Grad Norm 12.6797(220278383545.6408) | Total Time 0.00(0.00)\n",
      "Iter 0435 | Time 51.3597(51.0689) | Bit/dim 4.1830(4.2462) | Xent 1.4120(1.5285) | Loss 10.6023(11.3047) | Error 0.5015(0.5477) Steps 556(562.00) | Grad Norm 8.6627(213670032039.5315) | Total Time 0.00(0.00)\n",
      "Iter 0436 | Time 55.4220(51.1995) | Bit/dim 4.2093(4.2451) | Xent 1.4476(1.5261) | Loss 10.9462(11.2939) | Error 0.5197(0.5469) Steps 604(563.26) | Grad Norm 13.0295(207259931078.7364) | Total Time 0.00(0.00)\n",
      "Iter 0437 | Time 50.4166(51.1760) | Bit/dim 4.1749(4.2430) | Xent 1.4566(1.5240) | Loss 10.5107(11.2704) | Error 0.5179(0.5460) Steps 592(564.12) | Grad Norm 18.8930(201042133146.9411) | Total Time 0.00(0.00)\n",
      "Iter 0438 | Time 53.0239(51.2314) | Bit/dim 4.2019(4.2417) | Xent 1.4465(1.5216) | Loss 10.5518(11.2489) | Error 0.5169(0.5451) Steps 580(564.60) | Grad Norm 11.1260(195010869152.8667) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 23.1245, Epoch Time 351.9581(335.6317), Bit/dim 4.1702(best: 4.1655), Xent 1.3419, Loss 4.8411, Error 0.4855(best: 0.4915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0439 | Time 50.3758(51.2057) | Bit/dim 4.1523(4.2390) | Xent 1.4224(1.5187) | Loss 13.1171(11.3049) | Error 0.5176(0.5443) Steps 556(564.34) | Grad Norm 12.6883(189160543078.6613) | Total Time 0.00(0.00)\n",
      "Iter 0440 | Time 50.9265(51.1974) | Bit/dim 4.1916(4.2376) | Xent 1.3969(1.5150) | Loss 10.5204(11.2814) | Error 0.5022(0.5430) Steps 544(563.73) | Grad Norm 10.1279(183485726786.6053) | Total Time 0.00(0.00)\n",
      "Iter 0441 | Time 48.2920(51.1102) | Bit/dim 4.1675(4.2355) | Xent 1.4071(1.5118) | Loss 10.4756(11.2572) | Error 0.5005(0.5418) Steps 550(563.32) | Grad Norm 12.6973(177981154983.3881) | Total Time 0.00(0.00)\n",
      "Iter 0442 | Time 48.2997(51.0259) | Bit/dim 4.1810(4.2339) | Xent 1.4649(1.5104) | Loss 10.5194(11.2351) | Error 0.5177(0.5410) Steps 556(563.10) | Grad Norm 24.7578(172641720334.6292) | Total Time 0.00(0.00)\n",
      "Iter 0443 | Time 48.3884(50.9468) | Bit/dim 4.1801(4.2323) | Xent 1.4506(1.5086) | Loss 10.5004(11.2130) | Error 0.5140(0.5402) Steps 562(563.06) | Grad Norm 19.7228(167462468725.1819) | Total Time 0.00(0.00)\n",
      "Iter 0444 | Time 50.8018(50.9424) | Bit/dim 4.1627(4.2302) | Xent 1.4047(1.5055) | Loss 10.4213(11.1893) | Error 0.5054(0.5392) Steps 556(562.85) | Grad Norm 11.4638(162438594663.7704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 23.9049, Epoch Time 336.8670(335.6687), Bit/dim 4.1649(best: 4.1655), Xent 1.6152, Loss 4.9725, Error 0.5797(best: 0.4855)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0445 | Time 53.6738(51.0244) | Bit/dim 4.1768(4.2286) | Xent 1.6842(1.5108) | Loss 14.0419(11.2749) | Error 0.5837(0.5405) Steps 568(563.01) | Grad Norm 37.7298(157565436824.9892) | Total Time 0.00(0.00)\n",
      "Iter 0446 | Time 50.7543(51.0163) | Bit/dim 4.1938(4.2275) | Xent 1.4751(1.5098) | Loss 10.5144(11.2521) | Error 0.5296(0.5402) Steps 592(563.88) | Grad Norm 19.9106(152838473720.8368) | Total Time 0.00(0.00)\n",
      "Iter 0447 | Time 47.1075(50.8990) | Bit/dim 4.1614(4.2256) | Xent 1.6622(1.5143) | Loss 10.7125(11.2359) | Error 0.5761(0.5413) Steps 562(563.82) | Grad Norm 30.7958(148253319510.1356) | Total Time 0.00(0.00)\n",
      "Iter 0448 | Time 52.1247(50.9358) | Bit/dim 4.2265(4.2256) | Xent 1.4332(1.5119) | Loss 10.7317(11.2207) | Error 0.5124(0.5404) Steps 604(565.03) | Grad Norm 14.6268(143805719925.2703) | Total Time 0.00(0.00)\n",
      "Iter 0449 | Time 53.1541(51.0023) | Bit/dim 4.2002(4.2248) | Xent 1.5428(1.5128) | Loss 10.7956(11.2080) | Error 0.5474(0.5406) Steps 562(564.93) | Grad Norm 16.0437(139491548327.9935) | Total Time 0.00(0.00)\n",
      "Iter 0450 | Time 53.7447(51.0846) | Bit/dim 4.1955(4.2239) | Xent 1.4339(1.5105) | Loss 10.7429(11.1940) | Error 0.5136(0.5398) Steps 550(564.49) | Grad Norm 10.1913(135306801878.4594) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 23.2716, Epoch Time 349.7737(336.0919), Bit/dim 4.1786(best: 4.1649), Xent 1.3539, Loss 4.8555, Error 0.4859(best: 0.4855)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0451 | Time 50.2488(51.0595) | Bit/dim 4.1866(4.2228) | Xent 1.4086(1.5074) | Loss 13.6293(11.2671) | Error 0.5028(0.5387) Steps 568(564.59) | Grad Norm 11.6620(131247597822.4555) | Total Time 0.00(0.00)\n",
      "Iter 0452 | Time 56.2198(51.2143) | Bit/dim 4.1891(4.2218) | Xent 1.4058(1.5044) | Loss 10.4977(11.2440) | Error 0.4988(0.5375) Steps 580(565.05) | Grad Norm 13.6481(127310169888.1913) | Total Time 0.00(0.00)\n",
      "Iter 0453 | Time 48.7230(51.1396) | Bit/dim 4.1948(4.2210) | Xent 1.4443(1.5026) | Loss 10.4414(11.2199) | Error 0.5077(0.5366) Steps 556(564.78) | Grad Norm 13.4313(123490864791.9485) | Total Time 0.00(0.00)\n",
      "Iter 0454 | Time 53.8439(51.2207) | Bit/dim 4.1699(4.2195) | Xent 1.4343(1.5005) | Loss 10.5168(11.1988) | Error 0.5054(0.5357) Steps 586(565.42) | Grad Norm 19.2360(119786138848.7671) | Total Time 0.00(0.00)\n",
      "Iter 0455 | Time 50.8897(51.2108) | Bit/dim 4.1761(4.2182) | Xent 1.3820(1.4969) | Loss 10.4679(11.1769) | Error 0.4962(0.5345) Steps 556(565.14) | Grad Norm 10.3575(116192554683.6148) | Total Time 0.00(0.00)\n",
      "Iter 0456 | Time 55.4636(51.3384) | Bit/dim 4.1475(4.2160) | Xent 1.5093(1.4973) | Loss 10.4617(11.1555) | Error 0.5380(0.5346) Steps 574(565.40) | Grad Norm 14.1759(112706778043.5316) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 23.1046, Epoch Time 354.3949(336.6410), Bit/dim 4.1613(best: 4.1649), Xent 1.3315, Loss 4.8270, Error 0.4832(best: 0.4855)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0457 | Time 50.3295(51.3081) | Bit/dim 4.1724(4.2147) | Xent 1.3995(1.4944) | Loss 12.3730(11.1920) | Error 0.5135(0.5340) Steps 580(565.84) | Grad Norm 7.2079(109325574702.4419) | Total Time 0.00(0.00)\n",
      "Iter 0458 | Time 53.4021(51.3709) | Bit/dim 4.1605(4.2131) | Xent 1.3605(1.4904) | Loss 10.3562(11.1669) | Error 0.4852(0.5325) Steps 580(566.27) | Grad Norm 7.6425(106045807461.5979) | Total Time 0.00(0.00)\n",
      "Iter 0459 | Time 51.7633(51.3827) | Bit/dim 4.1473(4.2111) | Xent 1.4638(1.4896) | Loss 10.6374(11.1510) | Error 0.5208(0.5321) Steps 562(566.14) | Grad Norm 13.4922(102864433238.1547) | Total Time 0.00(0.00)\n",
      "Iter 0460 | Time 58.0687(51.5833) | Bit/dim 4.1593(4.2096) | Xent 1.3791(1.4863) | Loss 10.7029(11.1376) | Error 0.4928(0.5310) Steps 568(566.19) | Grad Norm 9.9890(99778500241.3098) | Total Time 0.00(0.00)\n",
      "Iter 0461 | Time 48.6785(51.4961) | Bit/dim 4.1457(4.2077) | Xent 1.4758(1.4859) | Loss 10.4792(11.1178) | Error 0.5181(0.5306) Steps 568(566.25) | Grad Norm 25.7594(96785145234.8433) | Total Time 0.00(0.00)\n",
      "Iter 0462 | Time 51.2583(51.4890) | Bit/dim 4.1702(4.2065) | Xent 1.3444(1.4817) | Loss 10.4911(11.0990) | Error 0.4846(0.5292) Steps 586(566.84) | Grad Norm 8.5835(93881590878.0555) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 23.4230, Epoch Time 352.9895(337.1314), Bit/dim 4.1373(best: 4.1613), Xent 1.3403, Loss 4.8075, Error 0.4890(best: 0.4832)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0463 | Time 50.3225(51.4540) | Bit/dim 4.1396(4.2045) | Xent 1.3928(1.4790) | Loss 13.2534(11.1637) | Error 0.4990(0.5283) Steps 544(566.15) | Grad Norm 12.0094(91065143152.0741) | Total Time 0.00(0.00)\n",
      "Iter 0464 | Time 50.4107(51.4227) | Bit/dim 4.1489(4.2029) | Xent 1.3891(1.4763) | Loss 10.5449(11.1451) | Error 0.4974(0.5274) Steps 574(566.39) | Grad Norm 17.3003(88333188858.0309) | Total Time 0.00(0.00)\n",
      "Iter 0465 | Time 53.0389(51.4712) | Bit/dim 4.1365(4.2009) | Xent 1.3573(1.4728) | Loss 10.5234(11.1264) | Error 0.4944(0.5264) Steps 568(566.44) | Grad Norm 7.4091(85683193192.5122) | Total Time 0.00(0.00)\n",
      "Iter 0466 | Time 50.0138(51.4275) | Bit/dim 4.1549(4.1995) | Xent 1.4147(1.4710) | Loss 10.6167(11.1111) | Error 0.5021(0.5256) Steps 562(566.31) | Grad Norm 15.1859(83112697397.1924) | Total Time 0.00(0.00)\n",
      "Iter 0467 | Time 54.4643(51.5186) | Bit/dim 4.1634(4.1984) | Xent 1.3244(1.4666) | Loss 10.2674(11.0858) | Error 0.4731(0.5241) Steps 592(567.08) | Grad Norm 12.7611(80619316475.6595) | Total Time 0.00(0.00)\n",
      "Iter 0468 | Time 53.0933(51.5658) | Bit/dim 4.1224(4.1961) | Xent 1.3540(1.4632) | Loss 10.3456(11.0636) | Error 0.4795(0.5227) Steps 568(567.10) | Grad Norm 10.1937(78200736981.6955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 22.7749, Epoch Time 350.0920(337.5203), Bit/dim 4.1320(best: 4.1373), Xent 1.3909, Loss 4.8275, Error 0.4970(best: 0.4832)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0469 | Time 53.3950(51.6207) | Bit/dim 4.1499(4.1947) | Xent 1.4537(1.4630) | Loss 13.4657(11.1357) | Error 0.5091(0.5223) Steps 586(567.67) | Grad Norm 25.3665(75854714873.0056) | Total Time 0.00(0.00)\n",
      "Iter 0470 | Time 52.9556(51.6607) | Bit/dim 4.1206(4.1925) | Xent 1.3361(1.4591) | Loss 10.3586(11.1124) | Error 0.4866(0.5213) Steps 574(567.86) | Grad Norm 8.5357(73579073427.0715) | Total Time 0.00(0.00)\n",
      "Iter 0471 | Time 54.6986(51.7519) | Bit/dim 4.1229(4.1904) | Xent 1.4821(1.4598) | Loss 10.3837(11.0905) | Error 0.5156(0.5211) Steps 550(567.32) | Grad Norm 37.6868(71371701225.3900) | Total Time 0.00(0.00)\n",
      "Iter 0472 | Time 48.7268(51.6611) | Bit/dim 4.1665(4.1897) | Xent 1.3643(1.4570) | Loss 10.4628(11.0717) | Error 0.4989(0.5204) Steps 562(567.16) | Grad Norm 11.2456(69230550188.9657) | Total Time 0.00(0.00)\n",
      "Iter 0473 | Time 52.1983(51.6772) | Bit/dim 4.1368(4.1881) | Xent 1.4721(1.4574) | Loss 10.5346(11.0556) | Error 0.5302(0.5207) Steps 538(566.29) | Grad Norm 20.9890(67153633683.9264) | Total Time 0.00(0.00)\n",
      "Iter 0474 | Time 52.5433(51.7032) | Bit/dim 4.1491(4.1869) | Xent 1.3933(1.4555) | Loss 10.4821(11.0384) | Error 0.5005(0.5201) Steps 592(567.06) | Grad Norm 12.6096(65139024673.7869) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 23.0021, Epoch Time 353.3404(337.9949), Bit/dim 4.1288(best: 4.1320), Xent 1.3373, Loss 4.7975, Error 0.4844(best: 0.4832)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0475 | Time 51.6634(51.7020) | Bit/dim 4.1310(4.1853) | Xent 1.4309(1.4548) | Loss 13.2075(11.1034) | Error 0.5132(0.5199) Steps 598(567.99) | Grad Norm 13.8460(63184853933.9886) | Total Time 0.00(0.00)\n",
      "Iter 0476 | Time 53.3990(51.7529) | Bit/dim 4.1364(4.1838) | Xent 1.4120(1.4535) | Loss 10.1877(11.0760) | Error 0.5019(0.5194) Steps 574(568.17) | Grad Norm 13.1588(61289308316.3637) | Total Time 0.00(0.00)\n",
      "Iter 0477 | Time 51.1465(51.7347) | Bit/dim 4.1706(4.1834) | Xent 1.3173(1.4494) | Loss 10.4617(11.0575) | Error 0.4706(0.5179) Steps 598(569.06) | Grad Norm 9.8340(59450629067.1678) | Total Time 0.00(0.00)\n",
      "Iter 0478 | Time 49.3347(51.6627) | Bit/dim 4.1411(4.1821) | Xent 1.3734(1.4471) | Loss 10.4404(11.0390) | Error 0.4874(0.5170) Steps 568(569.03) | Grad Norm 15.9685(57667110195.6319) | Total Time 0.00(0.00)\n",
      "Iter 0479 | Time 53.9979(51.7328) | Bit/dim 4.1118(4.1800) | Xent 1.3778(1.4450) | Loss 10.4515(11.0214) | Error 0.4956(0.5163) Steps 592(569.72) | Grad Norm 11.1622(55937096890.0978) | Total Time 0.00(0.00)\n",
      "Iter 0480 | Time 52.1124(51.7442) | Bit/dim 4.1021(4.1777) | Xent 1.3535(1.4423) | Loss 9.9053(10.9879) | Error 0.4869(0.5155) Steps 538(568.77) | Grad Norm 10.1336(54258983983.6989) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 23.2005, Epoch Time 351.3075(338.3942), Bit/dim 4.1373(best: 4.1288), Xent 1.2872, Loss 4.7809, Error 0.4680(best: 0.4832)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0481 | Time 51.3278(51.7317) | Bit/dim 4.1204(4.1760) | Xent 1.3816(1.4405) | Loss 13.3366(11.0584) | Error 0.4904(0.5147) Steps 568(568.75) | Grad Norm 10.5951(52631214464.5057) | Total Time 0.00(0.00)\n",
      "Iter 0482 | Time 53.4338(51.7828) | Bit/dim 4.1154(4.1742) | Xent 1.3038(1.4364) | Loss 10.1853(11.0322) | Error 0.4700(0.5134) Steps 580(569.08) | Grad Norm 6.0359(51052278030.7516) | Total Time 0.00(0.00)\n",
      "Iter 0483 | Time 51.6996(51.7803) | Bit/dim 4.0980(4.1719) | Xent 1.3328(1.4333) | Loss 10.4055(11.0134) | Error 0.4785(0.5123) Steps 574(569.23) | Grad Norm 12.1776(49520709690.1944) | Total Time 0.00(0.00)\n",
      "Iter 0484 | Time 51.9555(51.7855) | Bit/dim 4.1062(4.1699) | Xent 1.3418(1.4305) | Loss 10.2724(10.9912) | Error 0.4774(0.5113) Steps 592(569.92) | Grad Norm 12.4119(48035088399.8609) | Total Time 0.00(0.00)\n",
      "Iter 0485 | Time 51.2592(51.7697) | Bit/dim 4.1136(4.1682) | Xent 1.3260(1.4274) | Loss 10.2057(10.9676) | Error 0.4759(0.5102) Steps 574(570.04) | Grad Norm 12.8808(46594035748.2515) | Total Time 0.00(0.00)\n",
      "Iter 0486 | Time 50.7137(51.7380) | Bit/dim 4.1106(4.1665) | Xent 1.2945(1.4234) | Loss 10.1517(10.9431) | Error 0.4637(0.5088) Steps 580(570.34) | Grad Norm 4.6891(45196214675.9447) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 23.2571, Epoch Time 349.8386(338.7376), Bit/dim 4.0928(best: 4.1288), Xent 1.2513, Loss 4.7185, Error 0.4521(best: 0.4680)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0487 | Time 52.4901(51.7606) | Bit/dim 4.0855(4.1641) | Xent 1.3074(1.4199) | Loss 13.3489(11.0153) | Error 0.4729(0.5077) Steps 556(569.91) | Grad Norm 7.6446(43840328235.8957) | Total Time 0.00(0.00)\n",
      "Iter 0488 | Time 50.4503(51.7213) | Bit/dim 4.1094(4.1624) | Xent 1.3773(1.4186) | Loss 10.3148(10.9943) | Error 0.4832(0.5070) Steps 574(570.03) | Grad Norm 11.9279(42525118389.1766) | Total Time 0.00(0.00)\n",
      "Iter 0489 | Time 53.1729(51.7649) | Bit/dim 4.0799(4.1599) | Xent 1.2910(1.4148) | Loss 10.1288(10.9683) | Error 0.4619(0.5056) Steps 568(569.97) | Grad Norm 7.5224(41249364837.7270) | Total Time 0.00(0.00)\n",
      "Iter 0490 | Time 55.3853(51.8735) | Bit/dim 4.0920(4.1579) | Xent 1.5304(1.4183) | Loss 10.6276(10.9581) | Error 0.5296(0.5064) Steps 592(570.63) | Grad Norm 30.5964(40011883893.5131) | Total Time 0.00(0.00)\n",
      "Iter 0491 | Time 53.1619(51.9121) | Bit/dim 4.1544(4.1578) | Xent 1.3841(1.4173) | Loss 10.3073(10.9386) | Error 0.4925(0.5060) Steps 610(571.81) | Grad Norm 23.6532(38811527377.4173) | Total Time 0.00(0.00)\n",
      "Iter 0492 | Time 57.0511(52.0663) | Bit/dim 4.0927(4.1558) | Xent 1.3544(1.4154) | Loss 10.4324(10.9234) | Error 0.4970(0.5057) Steps 586(572.24) | Grad Norm 13.5799(37647181556.5022) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 23.9271, Epoch Time 361.6234(339.4241), Bit/dim 4.1659(best: 4.0928), Xent 1.2281, Loss 4.7799, Error 0.4461(best: 0.4521)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0493 | Time 49.7539(51.9969) | Bit/dim 4.1674(4.1562) | Xent 1.2971(1.4118) | Loss 13.5727(11.0029) | Error 0.4621(0.5044) Steps 592(572.83) | Grad Norm 15.8946(36517766110.2839) | Total Time 0.00(0.00)\n",
      "Iter 0494 | Time 50.5202(51.9526) | Bit/dim 4.1298(4.1554) | Xent 1.3068(1.4087) | Loss 10.2924(10.9815) | Error 0.4674(0.5033) Steps 580(573.04) | Grad Norm 7.5768(35422233127.2027) | Total Time 0.00(0.00)\n",
      "Iter 0495 | Time 53.8003(52.0080) | Bit/dim 4.1433(4.1550) | Xent 1.3372(1.4065) | Loss 10.4403(10.9653) | Error 0.4796(0.5026) Steps 598(573.79) | Grad Norm 15.9438(34359566133.8649) | Total Time 0.00(0.00)\n",
      "Iter 0496 | Time 51.4458(51.9912) | Bit/dim 4.1529(4.1550) | Xent 1.3456(1.4047) | Loss 10.2608(10.9442) | Error 0.4831(0.5020) Steps 556(573.26) | Grad Norm 17.5377(33328779150.3751) | Total Time 0.00(0.00)\n",
      "Iter 0497 | Time 52.0146(51.9919) | Bit/dim 4.1292(4.1542) | Xent 1.3524(1.4031) | Loss 10.2494(10.9233) | Error 0.4856(0.5015) Steps 598(574.00) | Grad Norm 13.7386(32328915776.2760) | Total Time 0.00(0.00)\n",
      "Iter 0498 | Time 49.4604(51.9159) | Bit/dim 4.1000(4.1526) | Xent 1.3407(1.4013) | Loss 10.2993(10.9046) | Error 0.4722(0.5006) Steps 574(574.00) | Grad Norm 12.2274(31359048303.3546) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 23.4300, Epoch Time 347.2745(339.6597), Bit/dim 4.1044(best: 4.0928), Xent 1.2742, Loss 4.7415, Error 0.4586(best: 0.4461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0499 | Time 52.5003(51.9335) | Bit/dim 4.0993(4.1510) | Xent 1.3084(1.3985) | Loss 12.8583(10.9632) | Error 0.4810(0.5000) Steps 598(574.72) | Grad Norm 10.8357(30418276854.5790) | Total Time 0.00(0.00)\n",
      "Iter 0500 | Time 49.2851(51.8540) | Bit/dim 4.1200(4.1500) | Xent 1.3225(1.3962) | Loss 10.2344(10.9414) | Error 0.4759(0.4993) Steps 568(574.52) | Grad Norm 9.5137(29505728549.2270) | Total Time 0.00(0.00)\n",
      "Iter 0501 | Time 52.0660(51.8604) | Bit/dim 4.0908(4.1483) | Xent 1.3844(1.3958) | Loss 10.4967(10.9280) | Error 0.4862(0.4989) Steps 580(574.68) | Grad Norm 21.6468(28620556693.3996) | Total Time 0.00(0.00)\n",
      "Iter 0502 | Time 49.3234(51.7843) | Bit/dim 4.0829(4.1463) | Xent 1.2834(1.3925) | Loss 10.0309(10.9011) | Error 0.4551(0.4976) Steps 550(573.94) | Grad Norm 8.6385(27761939992.8568) | Total Time 0.00(0.00)\n",
      "Iter 0503 | Time 52.6747(51.8110) | Bit/dim 4.0678(4.1439) | Xent 1.3010(1.3897) | Loss 9.9169(10.8716) | Error 0.4741(0.4969) Steps 580(574.13) | Grad Norm 4.5724(26929081793.2083) | Total Time 0.00(0.00)\n",
      "Iter 0504 | Time 58.2855(52.0052) | Bit/dim 4.0879(4.1423) | Xent 1.3535(1.3886) | Loss 10.4931(10.8602) | Error 0.4856(0.4965) Steps 598(574.84) | Grad Norm 15.8901(26121209339.8887) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 23.7442, Epoch Time 353.9177(340.0874), Bit/dim 4.0940(best: 4.0928), Xent 1.2579, Loss 4.7230, Error 0.4513(best: 0.4461)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0505 | Time 52.4587(52.0188) | Bit/dim 4.0938(4.1408) | Xent 1.3189(1.3865) | Loss 13.7243(10.9461) | Error 0.4711(0.4958) Steps 598(575.54) | Grad Norm 11.8096(25337573060.0463) | Total Time 0.00(0.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_lars.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_30_lars_clip_trust_0_0002_run1 --seed 1 --conditional True --controlled_tol True --train_mode semisup --lr 0.01 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 30.0 --max_grad_norm 20.0 --trust_coefficient 0.0002 --clip True\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
