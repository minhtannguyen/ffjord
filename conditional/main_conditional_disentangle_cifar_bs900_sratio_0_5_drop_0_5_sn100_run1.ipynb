{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn100_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=100, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 15.3065(33.1793) | Bit/dim 9.9839(10.8674) | Xent 2.2822(2.3001) | Loss 11.1249(12.0174) | Error 0.7478(0.8648) Steps 574(574.00) | Grad Norm 43.0956(65.6788) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 15.2341(28.4651) | Bit/dim 8.9211(10.4487) | Xent 2.2382(2.2874) | Loss 10.0402(11.5924) | Error 0.7667(0.8368) Steps 574(574.00) | Grad Norm 13.4938(54.5696) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 15.3416(24.9844) | Bit/dim 8.4759(9.9735) | Xent 2.1722(2.2652) | Loss 9.5620(11.1061) | Error 0.7433(0.8175) Steps 574(574.00) | Grad Norm 11.0515(43.6697) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 14.8409(22.3998) | Bit/dim 8.1485(9.5354) | Xent 2.1393(2.2364) | Loss 9.2181(10.6537) | Error 0.7289(0.7956) Steps 574(574.00) | Grad Norm 7.4082(34.4377) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 15.0460(20.5086) | Bit/dim 7.8442(9.1270) | Xent 2.1337(2.2084) | Loss 8.9111(10.2313) | Error 0.7100(0.7772) Steps 574(574.00) | Grad Norm 5.0196(27.0110) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 93.7104, Epoch Time 971.1029(971.1029), Bit/dim 7.6727(best: inf), Xent 2.0963, Loss 8.7208, Error 0.6900(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 14.9448(19.1359) | Bit/dim 7.5849(8.7491) | Xent 2.0991(2.1808) | Loss 8.6345(9.8395) | Error 0.7100(0.7582) Steps 574(574.00) | Grad Norm 4.3203(21.1303) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 15.3242(18.1529) | Bit/dim 7.3224(8.4013) | Xent 2.1130(2.1597) | Loss 8.3789(9.4812) | Error 0.6900(0.7395) Steps 574(574.00) | Grad Norm 2.8848(16.4844) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 15.2654(17.3468) | Bit/dim 7.1853(8.0954) | Xent 2.0971(2.1430) | Loss 8.2338(9.1669) | Error 0.6867(0.7278) Steps 574(574.00) | Grad Norm 2.1608(12.8656) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 16.0169(16.8391) | Bit/dim 7.0922(7.8407) | Xent 2.1210(2.1321) | Loss 8.1527(8.9068) | Error 0.7322(0.7224) Steps 592(576.02) | Grad Norm 1.8938(10.0660) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.8060(16.5763) | Bit/dim 6.9982(7.6309) | Xent 2.0788(2.1189) | Loss 8.0376(8.6904) | Error 0.7033(0.7157) Steps 592(580.22) | Grad Norm 3.6142(7.9852) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 17.0346(16.5502) | Bit/dim 6.9620(7.4595) | Xent 2.0743(2.1079) | Loss 7.9991(8.5135) | Error 0.7056(0.7125) Steps 610(585.51) | Grad Norm 1.3631(6.3965) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 94.1393, Epoch Time 970.0455(971.0711), Bit/dim 6.9531(best: 7.6727), Xent 2.0644, Loss 7.9853, Error 0.6918(best: 0.6900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 17.1291(16.6969) | Bit/dim 6.8779(7.3183) | Xent 2.0539(2.0965) | Loss 7.9049(8.3666) | Error 0.7156(0.7096) Steps 610(591.94) | Grad Norm 3.6459(5.4308) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 17.5445(16.9336) | Bit/dim 6.8258(7.1973) | Xent 2.0558(2.0889) | Loss 7.8537(8.2418) | Error 0.7000(0.7080) Steps 622(598.83) | Grad Norm 1.2363(5.1874) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 17.6033(17.1314) | Bit/dim 6.7453(7.0885) | Xent 2.0481(2.0828) | Loss 7.7693(8.1299) | Error 0.7200(0.7105) Steps 622(604.91) | Grad Norm 5.0641(6.2529) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 18.0193(17.2646) | Bit/dim 6.6562(6.9874) | Xent 2.1047(2.0794) | Loss 7.7086(8.0271) | Error 0.7733(0.7147) Steps 628(610.52) | Grad Norm 30.1287(9.0053) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 18.0184(17.4035) | Bit/dim 6.5341(6.8822) | Xent 2.0658(2.0782) | Loss 7.5669(7.9212) | Error 0.7344(0.7205) Steps 634(615.47) | Grad Norm 20.4037(13.3122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 98.3354, Epoch Time 1084.0945(974.4618), Bit/dim 6.4465(best: 6.9531), Xent 2.0420, Loss 7.4675, Error 0.7060(best: 0.6900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 17.8668(17.5827) | Bit/dim 6.3899(6.7681) | Xent 2.0817(2.0764) | Loss 7.4307(7.8063) | Error 0.7144(0.7204) Steps 628(620.15) | Grad Norm 28.6894(16.1846) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 17.9367(17.6728) | Bit/dim 6.2005(6.6361) | Xent 2.1490(2.0721) | Loss 7.2750(7.6721) | Error 0.8022(0.7212) Steps 628(622.35) | Grad Norm 49.6246(18.1894) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.9590(17.7590) | Bit/dim 6.0305(6.4919) | Xent 2.0610(2.0770) | Loss 7.0610(7.5304) | Error 0.7567(0.7273) Steps 634(624.35) | Grad Norm 32.4823(24.0204) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 18.1552(17.8792) | Bit/dim 5.8698(6.3550) | Xent 2.0422(2.0926) | Loss 6.8909(7.4012) | Error 0.7067(0.7381) Steps 634(626.88) | Grad Norm 11.1602(29.5410) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 18.1928(17.9453) | Bit/dim 5.7983(6.2238) | Xent 2.0546(2.0853) | Loss 6.8256(7.2665) | Error 0.6944(0.7288) Steps 634(628.75) | Grad Norm 16.5606(26.7380) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 18.1354(17.9977) | Bit/dim 5.7268(6.1068) | Xent 2.0180(2.0754) | Loss 6.7358(7.1445) | Error 0.6978(0.7194) Steps 628(629.33) | Grad Norm 14.4086(22.5820) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 98.1755, Epoch Time 1110.2159(978.5345), Bit/dim 5.7336(best: 6.4465), Xent 2.0158, Loss 6.7416, Error 0.6519(best: 0.6900)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 18.2202(17.9612) | Bit/dim 5.7441(6.0036) | Xent 2.0375(2.0609) | Loss 6.7628(7.0341) | Error 0.7056(0.7083) Steps 622(628.77) | Grad Norm 24.2263(20.6512) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.9356(17.9124) | Bit/dim 5.7334(5.9300) | Xent 2.1186(2.0737) | Loss 6.7927(6.9668) | Error 0.7578(0.7193) Steps 628(628.57) | Grad Norm 45.5326(27.5439) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.7728(17.8743) | Bit/dim 5.6100(5.8556) | Xent 2.0005(2.0633) | Loss 6.6102(6.8873) | Error 0.6667(0.7132) Steps 628(628.42) | Grad Norm 8.2339(24.0768) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 17.8505(17.7349) | Bit/dim 5.7704(5.8067) | Xent 2.2792(2.0640) | Loss 6.9100(6.8387) | Error 0.8289(0.7145) Steps 622(626.87) | Grad Norm 98.7662(25.6942) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 17.4329(17.6693) | Bit/dim 5.5842(5.7779) | Xent 2.0890(2.0690) | Loss 6.6287(6.8124) | Error 0.7389(0.7175) Steps 622(625.59) | Grad Norm 15.3587(25.7097) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 98.4359, Epoch Time 1083.5334(981.6844), Bit/dim 5.6111(best: 5.7336), Xent 2.0337, Loss 6.6279, Error 0.6977(best: 0.6519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 17.0914(17.5647) | Bit/dim 5.6106(5.7329) | Xent 2.0420(2.0640) | Loss 6.6316(6.7649) | Error 0.7344(0.7159) Steps 616(624.47) | Grad Norm 5.4458(21.3596) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.4251(17.5243) | Bit/dim 5.6042(5.6963) | Xent 1.9833(2.0503) | Loss 6.5959(6.7215) | Error 0.6756(0.7104) Steps 622(623.66) | Grad Norm 10.7928(18.0093) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 17.1121(17.4411) | Bit/dim 5.5642(5.6589) | Xent 1.9983(2.0360) | Loss 6.5633(6.6769) | Error 0.7100(0.7045) Steps 622(622.25) | Grad Norm 5.2654(14.8483) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.4990(17.4431) | Bit/dim 5.5622(5.6276) | Xent 2.0236(2.0276) | Loss 6.5740(6.6414) | Error 0.7244(0.7055) Steps 628(622.24) | Grad Norm 28.9057(16.2874) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 17.4960(17.4879) | Bit/dim 5.5124(5.5984) | Xent 1.9856(2.0179) | Loss 6.5052(6.6073) | Error 0.6822(0.7007) Steps 640(624.78) | Grad Norm 13.9058(15.2645) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.7503(17.5085) | Bit/dim 5.4445(5.5620) | Xent 1.9595(2.0082) | Loss 6.4243(6.5661) | Error 0.6589(0.6968) Steps 640(629.06) | Grad Norm 9.9592(13.7144) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 100.2802, Epoch Time 1077.2820(984.5524), Bit/dim 5.4502(best: 5.6111), Xent 1.9371, Loss 6.4188, Error 0.6435(best: 0.6519)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 18.0818(17.5232) | Bit/dim 5.4078(5.5306) | Xent 1.9214(1.9914) | Loss 6.3686(6.5262) | Error 0.6556(0.6881) Steps 646(632.11) | Grad Norm 3.9842(11.7750) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 18.7080(17.7220) | Bit/dim 5.3806(5.4968) | Xent 1.9595(1.9786) | Loss 6.3604(6.4861) | Error 0.6833(0.6830) Steps 658(638.07) | Grad Norm 25.6388(12.3422) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 18.8792(17.9934) | Bit/dim 5.3375(5.4637) | Xent 2.0107(1.9735) | Loss 6.3429(6.4504) | Error 0.7111(0.6833) Steps 670(645.34) | Grad Norm 16.7660(13.8662) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 18.9717(18.2172) | Bit/dim 5.3146(5.4276) | Xent 1.9264(1.9614) | Loss 6.2778(6.4083) | Error 0.6667(0.6779) Steps 688(654.51) | Grad Norm 3.8291(12.4464) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 19.1749(18.4339) | Bit/dim 5.2617(5.4071) | Xent 2.0172(1.9832) | Loss 6.2703(6.3987) | Error 0.7200(0.6925) Steps 688(662.33) | Grad Norm 10.5080(17.0098) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 109.6156, Epoch Time 1147.1101(989.4291), Bit/dim 5.2727(best: 5.4502), Xent 1.9449, Loss 6.2451, Error 0.6581(best: 0.6435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 19.2876(18.6170) | Bit/dim 5.2443(5.3808) | Xent 1.9613(1.9841) | Loss 6.2249(6.3728) | Error 0.6811(0.6925) Steps 688(669.53) | Grad Norm 11.7411(16.4623) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 19.4461(18.6739) | Bit/dim 5.2664(5.3468) | Xent 1.9145(1.9719) | Loss 6.2236(6.3327) | Error 0.6633(0.6875) Steps 700(673.65) | Grad Norm 10.0278(14.4287) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 19.1579(18.7710) | Bit/dim 5.2417(5.3172) | Xent 1.9477(1.9610) | Loss 6.2156(6.2977) | Error 0.6811(0.6819) Steps 694(677.72) | Grad Norm 9.0377(13.5702) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 18.9791(18.8664) | Bit/dim 5.1404(5.2824) | Xent 1.8366(1.9484) | Loss 6.0587(6.2566) | Error 0.6467(0.6779) Steps 694(682.06) | Grad Norm 7.8754(12.0757) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 18.9971(18.9289) | Bit/dim 5.1494(5.2564) | Xent 1.9064(1.9439) | Loss 6.1026(6.2284) | Error 0.6889(0.6811) Steps 700(686.14) | Grad Norm 12.7789(13.0264) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 18.8645(18.9561) | Bit/dim 5.2004(5.2509) | Xent 1.9846(1.9663) | Loss 6.1926(6.2340) | Error 0.7011(0.6905) Steps 688(688.17) | Grad Norm 7.7748(15.0210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 107.2961, Epoch Time 1171.7247(994.8980), Bit/dim 5.1536(best: 5.2727), Xent 1.9561, Loss 6.1316, Error 0.6768(best: 0.6435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 19.0481(18.8613) | Bit/dim 5.1318(5.2282) | Xent 1.9067(1.9592) | Loss 6.0852(6.2077) | Error 0.6789(0.6882) Steps 682(685.68) | Grad Norm 5.9071(12.8022) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 18.8285(18.8212) | Bit/dim 5.2107(5.1953) | Xent 1.9793(1.9483) | Loss 6.2004(6.1694) | Error 0.6856(0.6829) Steps 682(683.24) | Grad Norm 13.5153(11.7249) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 18.9997(18.8567) | Bit/dim 5.0444(5.1686) | Xent 1.8746(1.9375) | Loss 5.9817(6.1374) | Error 0.6333(0.6785) Steps 682(682.91) | Grad Norm 6.9223(12.9108) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 18.9624(18.8314) | Bit/dim 5.0283(5.1448) | Xent 1.8776(1.9312) | Loss 5.9671(6.1104) | Error 0.6633(0.6752) Steps 682(682.33) | Grad Norm 7.8472(12.8314) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.7947(18.8286) | Bit/dim 5.1684(5.1240) | Xent 1.9411(1.9232) | Loss 6.1390(6.0856) | Error 0.6689(0.6710) Steps 676(681.76) | Grad Norm 34.8973(13.0097) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 107.2686, Epoch Time 1158.4834(999.8055), Bit/dim 5.0864(best: 5.1536), Xent 1.9519, Loss 6.0623, Error 0.6915(best: 0.6435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.9577(18.8898) | Bit/dim 5.1064(5.1223) | Xent 1.9843(1.9320) | Loss 6.0986(6.0883) | Error 0.6911(0.6759) Steps 694(683.82) | Grad Norm 14.8050(15.8602) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 19.5699(18.9247) | Bit/dim 5.0043(5.0981) | Xent 1.9268(1.9292) | Loss 5.9678(6.0626) | Error 0.6878(0.6753) Steps 688(682.86) | Grad Norm 4.9259(13.6498) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 19.2079(18.9736) | Bit/dim 4.9695(5.0676) | Xent 1.8684(1.9189) | Loss 5.9037(6.0271) | Error 0.6589(0.6714) Steps 682(682.33) | Grad Norm 4.4778(12.0344) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.5312(19.0197) | Bit/dim 5.8160(5.0982) | Xent 2.0691(1.9169) | Loss 6.8505(6.0566) | Error 0.7678(0.6733) Steps 694(684.35) | Grad Norm 25.6752(15.4706) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.5886(18.9193) | Bit/dim 5.1514(5.1418) | Xent 1.9127(1.9354) | Loss 6.1078(6.1095) | Error 0.6800(0.6787) Steps 676(682.61) | Grad Norm 6.5784(16.5527) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.5609(18.8374) | Bit/dim 5.0423(5.1212) | Xent 1.9895(1.9399) | Loss 6.0370(6.0911) | Error 0.6989(0.6812) Steps 670(678.93) | Grad Norm 6.1562(14.0170) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 103.2658, Epoch Time 1160.9042(1004.6385), Bit/dim 5.0132(best: 5.0864), Xent 1.9156, Loss 5.9710, Error 0.6634(best: 0.6435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.8274(18.7428) | Bit/dim 4.9645(5.0850) | Xent 1.8978(1.9321) | Loss 5.9134(6.0510) | Error 0.6733(0.6788) Steps 670(675.13) | Grad Norm 3.7761(11.6028) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 18.6898(18.6908) | Bit/dim 4.8914(5.0427) | Xent 1.8686(1.9179) | Loss 5.8258(6.0017) | Error 0.6622(0.6734) Steps 676(674.44) | Grad Norm 3.8619(9.5540) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 19.2891(18.8383) | Bit/dim 4.8832(5.0084) | Xent 1.8293(1.8994) | Loss 5.7978(5.9580) | Error 0.6356(0.6661) Steps 694(678.24) | Grad Norm 10.1108(8.4964) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 20.1971(19.0055) | Bit/dim 4.8318(4.9759) | Xent 1.8229(1.8855) | Loss 5.7432(5.9187) | Error 0.6489(0.6610) Steps 700(682.78) | Grad Norm 12.9603(8.9362) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 20.0700(19.2463) | Bit/dim 4.8932(4.9651) | Xent 1.9184(1.8883) | Loss 5.8524(5.9092) | Error 0.6633(0.6633) Steps 712(690.02) | Grad Norm 14.4380(13.1844) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 111.1702, Epoch Time 1184.3904(1010.0310), Bit/dim 4.8542(best: 5.0132), Xent 1.8069, Loss 5.7576, Error 0.6294(best: 0.6435)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 19.6200(19.4254) | Bit/dim 4.9219(4.9403) | Xent 1.9364(1.8833) | Loss 5.8901(5.8819) | Error 0.6889(0.6626) Steps 712(697.34) | Grad Norm 24.0620(13.6687) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 19.7310(19.5172) | Bit/dim 4.8760(4.9407) | Xent 1.9056(1.8982) | Loss 5.8288(5.8898) | Error 0.6867(0.6682) Steps 706(700.76) | Grad Norm 10.5248(14.8900) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 19.7392(19.5804) | Bit/dim 4.7821(4.9165) | Xent 1.8388(1.8915) | Loss 5.7014(5.8623) | Error 0.6544(0.6665) Steps 712(701.69) | Grad Norm 5.4605(13.1844) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 19.2742(19.6547) | Bit/dim 4.8688(4.8888) | Xent 1.8104(1.8660) | Loss 5.7740(5.8218) | Error 0.6211(0.6576) Steps 712(705.69) | Grad Norm 25.3404(13.4074) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 19.2390(19.7256) | Bit/dim 4.9538(4.8760) | Xent 1.9411(1.8533) | Loss 5.9244(5.8027) | Error 0.6656(0.6513) Steps 718(709.75) | Grad Norm 31.3420(15.4721) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 19.9854(19.8410) | Bit/dim 4.8092(4.8663) | Xent 1.8855(1.8637) | Loss 5.7519(5.7981) | Error 0.6578(0.6556) Steps 724(714.87) | Grad Norm 11.1143(15.8792) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 112.6072, Epoch Time 1224.8992(1016.4771), Bit/dim 4.7693(best: 4.8542), Xent 1.7966, Loss 5.6676, Error 0.6245(best: 0.6294)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 20.8408(19.9733) | Bit/dim 4.7707(4.8401) | Xent 1.8277(1.8509) | Loss 5.6845(5.7656) | Error 0.6622(0.6536) Steps 730(719.52) | Grad Norm 8.4911(13.3179) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 20.9182(20.0819) | Bit/dim 4.7273(4.8113) | Xent 1.7001(1.8302) | Loss 5.5773(5.7264) | Error 0.6067(0.6452) Steps 748(724.79) | Grad Norm 13.8292(12.2000) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 20.5644(20.1930) | Bit/dim 4.7111(4.7917) | Xent 1.7127(1.8120) | Loss 5.5675(5.6976) | Error 0.6089(0.6405) Steps 748(731.52) | Grad Norm 9.0862(13.4311) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 21.4055(20.3294) | Bit/dim 4.7661(4.7786) | Xent 1.7846(1.7885) | Loss 5.6584(5.6729) | Error 0.6289(0.6312) Steps 754(736.97) | Grad Norm 30.7462(14.0429) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 20.0704(20.4050) | Bit/dim 4.7782(4.7956) | Xent 1.8365(1.8007) | Loss 5.6965(5.6959) | Error 0.6400(0.6373) Steps 736(738.89) | Grad Norm 10.0012(15.8578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 113.9629, Epoch Time 1258.8418(1023.7480), Bit/dim 4.7116(best: 4.7693), Xent 1.6873, Loss 5.5553, Error 0.5927(best: 0.6245)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 19.8798(20.3773) | Bit/dim 4.7020(4.7763) | Xent 1.7325(1.7979) | Loss 5.5683(5.6753) | Error 0.6200(0.6387) Steps 724(737.13) | Grad Norm 5.2865(14.5742) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 19.8343(20.3555) | Bit/dim 4.7051(4.7691) | Xent 1.6938(1.7896) | Loss 5.5520(5.6639) | Error 0.6056(0.6354) Steps 730(738.52) | Grad Norm 11.5543(16.2305) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 20.5523(20.4294) | Bit/dim 4.6817(4.7532) | Xent 1.7569(1.7932) | Loss 5.5601(5.6498) | Error 0.6367(0.6368) Steps 736(740.03) | Grad Norm 7.0316(15.9687) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 20.8313(20.4252) | Bit/dim 4.6214(4.7247) | Xent 1.7184(1.7841) | Loss 5.4807(5.6168) | Error 0.6000(0.6341) Steps 760(741.74) | Grad Norm 4.9230(13.4370) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 20.2398(20.5024) | Bit/dim 4.6535(4.7031) | Xent 1.7392(1.7631) | Loss 5.5231(5.5846) | Error 0.6211(0.6274) Steps 760(747.12) | Grad Norm 16.3315(13.0962) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 21.1060(20.6295) | Bit/dim 4.6269(4.6822) | Xent 1.6547(1.7467) | Loss 5.4543(5.5555) | Error 0.5889(0.6230) Steps 784(753.59) | Grad Norm 16.7708(13.1429) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 121.9714, Epoch Time 1270.4782(1031.1499), Bit/dim 4.6122(best: 4.7116), Xent 1.6215, Loss 5.4229, Error 0.5670(best: 0.5927)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 22.1184(20.8170) | Bit/dim 4.6842(4.6665) | Xent 1.8408(1.7447) | Loss 5.6046(5.5388) | Error 0.6489(0.6207) Steps 796(762.03) | Grad Norm 25.3847(14.6787) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 21.9200(21.1007) | Bit/dim 4.6279(4.6615) | Xent 1.7647(1.7619) | Loss 5.5103(5.5424) | Error 0.6322(0.6284) Steps 796(773.73) | Grad Norm 9.7247(14.5930) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 21.3808(21.2263) | Bit/dim 4.5964(4.6414) | Xent 1.6906(1.7467) | Loss 5.4416(5.5147) | Error 0.5911(0.6223) Steps 778(779.58) | Grad Norm 7.3564(12.4104) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 21.1551(21.2891) | Bit/dim 4.5689(4.6213) | Xent 1.6709(1.7271) | Loss 5.4044(5.4849) | Error 0.6089(0.6157) Steps 796(782.45) | Grad Norm 14.0730(12.3061) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 21.2593(21.3267) | Bit/dim 4.5371(4.6010) | Xent 1.7176(1.7095) | Loss 5.3959(5.4558) | Error 0.5933(0.6092) Steps 802(787.21) | Grad Norm 13.6885(11.8495) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 126.2711, Epoch Time 1328.1517(1040.0600), Bit/dim 4.5195(best: 4.6122), Xent 1.6880, Loss 5.3634, Error 0.5954(best: 0.5670)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 21.7641(21.4123) | Bit/dim 4.5186(4.5784) | Xent 1.6650(1.7062) | Loss 5.3511(5.4315) | Error 0.5978(0.6099) Steps 796(790.28) | Grad Norm 10.3427(12.4173) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 21.6342(21.5014) | Bit/dim 4.5379(4.5677) | Xent 1.6469(1.6946) | Loss 5.3613(5.4150) | Error 0.5633(0.6033) Steps 796(794.76) | Grad Norm 11.2499(13.1285) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 21.9676(21.5861) | Bit/dim 4.8684(4.5818) | Xent 1.7010(1.7048) | Loss 5.7189(5.4342) | Error 0.6022(0.6061) Steps 826(799.71) | Grad Norm 18.7269(15.4771) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 21.1600(21.6480) | Bit/dim 4.6665(4.6080) | Xent 1.7347(1.7100) | Loss 5.5338(5.4630) | Error 0.6233(0.6088) Steps 790(804.82) | Grad Norm 9.4262(13.9590) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 20.8966(21.4960) | Bit/dim 4.6122(4.6151) | Xent 1.6991(1.7059) | Loss 5.4618(5.4681) | Error 0.6278(0.6091) Steps 760(800.18) | Grad Norm 13.2813(14.1356) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 20.9307(21.3275) | Bit/dim 4.5864(4.6085) | Xent 1.7280(1.7033) | Loss 5.4504(5.4601) | Error 0.6156(0.6091) Steps 772(789.46) | Grad Norm 17.4115(14.7847) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 117.1992, Epoch Time 1316.2397(1048.3454), Bit/dim 4.5347(best: 4.5195), Xent 1.5763, Loss 5.3228, Error 0.5560(best: 0.5670)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 21.3076(21.2014) | Bit/dim 4.5080(4.5838) | Xent 1.6238(1.6846) | Loss 5.3199(5.4261) | Error 0.5789(0.6022) Steps 772(782.64) | Grad Norm 16.3323(13.3336) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 20.7930(21.1136) | Bit/dim 4.5017(4.5649) | Xent 1.6780(1.6720) | Loss 5.3407(5.4008) | Error 0.6022(0.5978) Steps 754(777.54) | Grad Norm 14.8216(13.3353) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 21.1416(21.0689) | Bit/dim 4.4970(4.5400) | Xent 1.5774(1.6585) | Loss 5.2857(5.3692) | Error 0.5633(0.5943) Steps 766(773.94) | Grad Norm 7.9566(12.4330) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 20.2439(21.0800) | Bit/dim 4.4521(4.5191) | Xent 1.5481(1.6436) | Loss 5.2262(5.3409) | Error 0.5711(0.5907) Steps 766(775.20) | Grad Norm 10.1566(12.2919) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 20.9806(21.0804) | Bit/dim 4.4671(4.5054) | Xent 1.6508(1.6398) | Loss 5.2925(5.3252) | Error 0.5867(0.5879) Steps 784(774.58) | Grad Norm 15.5431(13.4120) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 120.9345, Epoch Time 1293.4054(1055.6972), Bit/dim 4.4364(best: 4.5195), Xent 1.5215, Loss 5.1971, Error 0.5472(best: 0.5560)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 21.5612(21.1462) | Bit/dim 4.5851(4.4954) | Xent 1.9314(1.6574) | Loss 5.5507(5.3241) | Error 0.6489(0.5916) Steps 772(774.94) | Grad Norm 28.0529(14.7282) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 21.2185(21.2408) | Bit/dim 4.4938(4.5004) | Xent 1.6153(1.6723) | Loss 5.3015(5.3366) | Error 0.5800(0.5978) Steps 784(780.45) | Grad Norm 14.5286(14.2739) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 21.9311(21.3644) | Bit/dim 4.4160(4.4859) | Xent 1.5889(1.6566) | Loss 5.2104(5.3142) | Error 0.5822(0.5947) Steps 796(784.39) | Grad Norm 10.8432(13.0181) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 21.3055(21.3778) | Bit/dim 4.4217(4.4659) | Xent 1.5795(1.6401) | Loss 5.2115(5.2859) | Error 0.5722(0.5884) Steps 778(783.78) | Grad Norm 5.5729(11.5505) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 21.3168(21.3920) | Bit/dim 4.3530(4.4428) | Xent 1.6149(1.6275) | Loss 5.1604(5.2566) | Error 0.5944(0.5862) Steps 772(785.31) | Grad Norm 11.7334(10.7423) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 21.4590(21.4476) | Bit/dim 4.3945(4.4272) | Xent 1.6284(1.6279) | Loss 5.2087(5.2412) | Error 0.5789(0.5854) Steps 796(788.04) | Grad Norm 10.8652(11.7522) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 122.5565, Epoch Time 1323.4955(1063.7311), Bit/dim 4.3744(best: 4.4364), Xent 1.5525, Loss 5.1506, Error 0.5491(best: 0.5472)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 21.7383(21.5072) | Bit/dim 4.3662(4.4138) | Xent 1.5257(1.6171) | Loss 5.1291(5.2224) | Error 0.5600(0.5812) Steps 796(790.05) | Grad Norm 5.7911(11.6200) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 22.1824(21.6520) | Bit/dim 4.3422(4.4022) | Xent 1.5617(1.5949) | Loss 5.1231(5.1996) | Error 0.5756(0.5744) Steps 820(796.70) | Grad Norm 7.0417(10.6500) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 21.9237(21.6956) | Bit/dim 4.3646(4.3895) | Xent 1.5610(1.5887) | Loss 5.1451(5.1839) | Error 0.5600(0.5721) Steps 814(799.54) | Grad Norm 15.2748(11.3129) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 21.7427(21.8212) | Bit/dim 4.3531(4.3789) | Xent 1.6041(1.5967) | Loss 5.1551(5.1773) | Error 0.5833(0.5743) Steps 820(805.87) | Grad Norm 8.8675(11.6623) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 22.6327(22.0022) | Bit/dim 4.4083(4.3833) | Xent 1.7025(1.6215) | Loss 5.2595(5.1941) | Error 0.6067(0.5810) Steps 832(811.28) | Grad Norm 18.5453(12.8715) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 123.1938, Epoch Time 1353.0218(1072.4098), Bit/dim 4.3846(best: 4.3744), Xent 1.5565, Loss 5.1629, Error 0.5602(best: 0.5472)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 22.5178(22.0467) | Bit/dim 4.3607(4.3806) | Xent 1.6015(1.6214) | Loss 5.1614(5.1913) | Error 0.5900(0.5817) Steps 832(813.15) | Grad Norm 11.1805(12.8718) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 22.3400(22.1002) | Bit/dim 4.3514(4.3737) | Xent 1.5061(1.6046) | Loss 5.1044(5.1760) | Error 0.5411(0.5756) Steps 826(813.68) | Grad Norm 14.0157(12.2426) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 22.0432(22.1141) | Bit/dim 4.2942(4.3619) | Xent 1.4882(1.5790) | Loss 5.0383(5.1514) | Error 0.5411(0.5671) Steps 832(815.96) | Grad Norm 6.7478(10.9368) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 22.6130(22.1857) | Bit/dim 4.3218(4.3537) | Xent 1.4683(1.5670) | Loss 5.0560(5.1372) | Error 0.5456(0.5633) Steps 826(817.82) | Grad Norm 5.2589(11.5252) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 22.9118(22.2857) | Bit/dim 4.2867(4.3400) | Xent 1.5699(1.5595) | Loss 5.0717(5.1197) | Error 0.5500(0.5603) Steps 832(820.67) | Grad Norm 13.2078(11.0199) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 22.5990(22.3823) | Bit/dim 4.2810(4.3271) | Xent 1.4557(1.5487) | Loss 5.0088(5.1014) | Error 0.5178(0.5570) Steps 832(823.59) | Grad Norm 5.6994(10.9067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 126.6996, Epoch Time 1375.4994(1081.5025), Bit/dim 4.2898(best: 4.3744), Xent 1.4103, Loss 4.9950, Error 0.5031(best: 0.5472)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 21.8116(22.4204) | Bit/dim 4.3030(4.3198) | Xent 1.5860(1.5362) | Loss 5.0960(5.0879) | Error 0.5744(0.5516) Steps 802(825.23) | Grad Norm 20.5603(11.5140) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 22.4021(22.4984) | Bit/dim 4.3357(4.3238) | Xent 1.5408(1.5487) | Loss 5.1061(5.0982) | Error 0.5378(0.5556) Steps 838(827.73) | Grad Norm 9.9919(11.6848) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 22.7649(22.5164) | Bit/dim 4.2960(4.3176) | Xent 1.4504(1.5452) | Loss 5.0212(5.0901) | Error 0.5256(0.5550) Steps 838(828.71) | Grad Norm 8.0941(11.0847) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 22.4242(22.4717) | Bit/dim 4.2860(4.3064) | Xent 1.4691(1.5333) | Loss 5.0205(5.0731) | Error 0.5344(0.5510) Steps 826(827.82) | Grad Norm 5.7417(10.6081) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 22.1737(22.4792) | Bit/dim 4.2464(4.3005) | Xent 1.4985(1.5260) | Loss 4.9957(5.0635) | Error 0.5433(0.5494) Steps 832(829.20) | Grad Norm 8.4974(11.0273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 126.6404, Epoch Time 1383.7641(1090.5704), Bit/dim 4.2819(best: 4.2898), Xent 1.4218, Loss 4.9928, Error 0.5136(best: 0.5031)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 21.8739(22.5135) | Bit/dim 4.4655(4.3031) | Xent 2.3493(1.5558) | Loss 5.6402(5.0810) | Error 0.7500(0.5538) Steps 808(830.44) | Grad Norm 31.4938(13.0573) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 21.9247(22.5543) | Bit/dim 4.3639(4.3321) | Xent 1.6433(1.5936) | Loss 5.1855(5.1289) | Error 0.5867(0.5667) Steps 820(834.80) | Grad Norm 7.1037(12.4152) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 22.6986(22.5472) | Bit/dim 4.2933(4.3277) | Xent 1.4809(1.5946) | Loss 5.0337(5.1250) | Error 0.5611(0.5703) Steps 844(835.11) | Grad Norm 4.7544(11.1885) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 21.7462(22.4103) | Bit/dim 4.6032(4.3766) | Xent 1.8090(1.6922) | Loss 5.5077(5.2227) | Error 0.6356(0.5942) Steps 814(829.41) | Grad Norm 11.3538(13.9943) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 22.4067(22.2773) | Bit/dim 4.3547(4.3914) | Xent 1.6952(1.7152) | Loss 5.2023(5.2490) | Error 0.6122(0.6047) Steps 832(826.10) | Grad Norm 4.3021(12.0378) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 21.8973(22.3507) | Bit/dim 4.3028(4.3738) | Xent 1.5808(1.7001) | Loss 5.0932(5.2239) | Error 0.5767(0.6030) Steps 820(827.91) | Grad Norm 2.8701(10.1112) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 124.3079, Epoch Time 1371.5180(1098.9988), Bit/dim 4.2927(best: 4.2819), Xent 1.4995, Loss 5.0424, Error 0.5368(best: 0.5031)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 22.3088(22.2948) | Bit/dim 4.2728(4.3469) | Xent 1.5364(1.6601) | Loss 5.0410(5.1769) | Error 0.5444(0.5920) Steps 820(825.06) | Grad Norm 5.2106(8.7427) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 22.2095(22.2650) | Bit/dim 4.2421(4.3207) | Xent 1.5597(1.6243) | Loss 5.0219(5.1328) | Error 0.5478(0.5812) Steps 820(823.56) | Grad Norm 6.0205(8.3994) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 22.4252(22.3022) | Bit/dim 4.2299(4.2954) | Xent 1.5274(1.5935) | Loss 4.9936(5.0921) | Error 0.5656(0.5703) Steps 820(823.45) | Grad Norm 10.8714(8.3762) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 22.6865(22.3977) | Bit/dim 4.2048(4.2721) | Xent 1.4994(1.5696) | Loss 4.9545(5.0569) | Error 0.5433(0.5627) Steps 832(825.74) | Grad Norm 7.2621(8.2666) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 22.5870(22.4714) | Bit/dim 4.1860(4.2539) | Xent 1.4329(1.5453) | Loss 4.9025(5.0265) | Error 0.5167(0.5542) Steps 838(828.83) | Grad Norm 4.6125(8.0314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 128.6372, Epoch Time 1381.3931(1107.4706), Bit/dim 4.1884(best: 4.2819), Xent 1.3692, Loss 4.8730, Error 0.4897(best: 0.5031)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 22.0811(22.5147) | Bit/dim 4.1747(4.2390) | Xent 1.4935(1.5254) | Loss 4.9215(5.0017) | Error 0.5467(0.5494) Steps 832(830.64) | Grad Norm 14.8125(7.9608) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 22.1067(22.5458) | Bit/dim 4.1930(4.2293) | Xent 1.3977(1.5106) | Loss 4.8918(4.9846) | Error 0.5144(0.5423) Steps 820(830.39) | Grad Norm 5.6691(8.9919) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 22.6050(22.5314) | Bit/dim 4.1770(4.2204) | Xent 1.5180(1.5003) | Loss 4.9360(4.9705) | Error 0.5356(0.5373) Steps 820(831.59) | Grad Norm 11.5995(9.3579) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 22.7364(22.5587) | Bit/dim 4.1808(4.2076) | Xent 1.4022(1.4810) | Loss 4.8819(4.9481) | Error 0.5056(0.5327) Steps 844(834.21) | Grad Norm 4.8197(8.7980) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 22.6348(22.5889) | Bit/dim 4.1797(4.2030) | Xent 1.4440(1.4792) | Loss 4.9017(4.9425) | Error 0.5233(0.5329) Steps 838(834.94) | Grad Norm 8.4664(9.6271) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 22.2347(22.6402) | Bit/dim 4.1736(4.1936) | Xent 1.4726(1.4626) | Loss 4.9099(4.9250) | Error 0.5189(0.5273) Steps 838(838.17) | Grad Norm 8.7853(8.6112) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 128.7071, Epoch Time 1390.6278(1115.9654), Bit/dim 4.1668(best: 4.1884), Xent 1.3625, Loss 4.8481, Error 0.4873(best: 0.4897)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 22.6549(22.7166) | Bit/dim 4.1645(4.1839) | Xent 1.4198(1.4468) | Loss 4.8744(4.9073) | Error 0.4856(0.5204) Steps 826(840.36) | Grad Norm 9.1058(8.2205) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 22.6601(22.7998) | Bit/dim 4.1434(4.1772) | Xent 1.4786(1.4395) | Loss 4.8826(4.8970) | Error 0.5522(0.5190) Steps 832(843.36) | Grad Norm 11.6103(8.7097) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 22.8352(22.7586) | Bit/dim 4.2511(4.1934) | Xent 1.7354(1.4942) | Loss 5.1188(4.9405) | Error 0.6200(0.5350) Steps 856(842.66) | Grad Norm 12.0148(11.4721) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 23.1601(22.7818) | Bit/dim 4.1904(4.1963) | Xent 1.4800(1.4997) | Loss 4.9304(4.9461) | Error 0.5589(0.5398) Steps 862(844.97) | Grad Norm 4.6832(10.2929) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 23.0678(22.8025) | Bit/dim 4.1841(4.1903) | Xent 1.3294(1.4783) | Loss 4.8488(4.9294) | Error 0.4844(0.5329) Steps 844(845.42) | Grad Norm 3.4246(8.8014) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 128.7565, Epoch Time 1404.7180(1124.6279), Bit/dim 4.1459(best: 4.1668), Xent 1.3192, Loss 4.8055, Error 0.4750(best: 0.4873)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 22.7426(22.8674) | Bit/dim 4.1435(4.1793) | Xent 1.3432(1.4500) | Loss 4.8151(4.9043) | Error 0.4844(0.5201) Steps 844(847.69) | Grad Norm 5.9454(8.0928) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 23.1136(22.8962) | Bit/dim 4.1569(4.1708) | Xent 1.5070(1.4376) | Loss 4.9104(4.8896) | Error 0.5333(0.5147) Steps 874(848.05) | Grad Norm 15.3270(9.0096) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 23.1271(22.9822) | Bit/dim 4.1126(4.1617) | Xent 1.3098(1.4237) | Loss 4.7675(4.8736) | Error 0.4711(0.5107) Steps 868(852.70) | Grad Norm 5.3109(8.3786) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 22.8589(23.0454) | Bit/dim 4.1189(4.1535) | Xent 1.4025(1.4126) | Loss 4.8201(4.8598) | Error 0.5022(0.5071) Steps 868(853.65) | Grad Norm 6.8003(8.3732) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 23.1110(23.1007) | Bit/dim 4.2288(4.1555) | Xent 1.4477(1.4169) | Loss 4.9527(4.8640) | Error 0.5344(0.5079) Steps 856(855.65) | Grad Norm 14.6936(9.7635) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 23.1045(23.0967) | Bit/dim 4.1110(4.1532) | Xent 1.3912(1.4160) | Loss 4.8066(4.8612) | Error 0.5022(0.5070) Steps 868(857.06) | Grad Norm 6.9608(9.5424) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 128.8741, Epoch Time 1419.5589(1133.4759), Bit/dim 4.1287(best: 4.1459), Xent 1.2932, Loss 4.7753, Error 0.4618(best: 0.4750)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 22.8811(23.1016) | Bit/dim 4.1053(4.1485) | Xent 1.3953(1.4042) | Loss 4.8029(4.8506) | Error 0.5000(0.5019) Steps 874(858.66) | Grad Norm 5.5926(9.1375) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 23.6480(23.2082) | Bit/dim 4.1307(4.1383) | Xent 1.3096(1.3887) | Loss 4.7855(4.8327) | Error 0.4656(0.4975) Steps 886(864.01) | Grad Norm 9.6677(9.0794) | Total Time 14.00(14.00)\n",
      "Iter 1550 | Time 23.4437(23.2326) | Bit/dim 4.0621(4.0938) | Xent 1.3453(1.3340) | Loss 4.7348(4.7608) | Error 0.4889(0.4792) Steps 874(865.59) | Grad Norm 9.4063(8.6832) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn100_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 100\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
