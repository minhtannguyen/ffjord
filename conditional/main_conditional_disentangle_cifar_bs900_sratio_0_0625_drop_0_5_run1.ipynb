{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.0625, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_0625_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=384, bias=True)\n",
      "  (project_class): LinearZeros(in_features=192, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1365814\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 13.7975(27.8305) | Bit/dim 8.9908(9.2897) | Xent 2.3026(2.3026) | Loss 8.9908(9.2897) | Error 0.8989(0.9041) Steps 574(574.00) | Grad Norm 13.2831(17.5046) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.5670(24.0356) | Bit/dim 8.6445(9.1431) | Xent 2.3026(2.3026) | Loss 8.6445(9.1431) | Error 0.9011(0.9030) Steps 574(574.00) | Grad Norm 4.8704(14.9890) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 13.3716(21.2043) | Bit/dim 8.4526(8.9701) | Xent 2.3026(2.3026) | Loss 8.4526(8.9701) | Error 0.8867(0.9026) Steps 574(574.00) | Grad Norm 3.6220(12.0480) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.8309(19.1671) | Bit/dim 8.2184(8.7923) | Xent 2.3026(2.3026) | Loss 8.2184(8.7923) | Error 0.9078(0.9017) Steps 574(574.00) | Grad Norm 2.8547(9.6991) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.5343(17.6704) | Bit/dim 7.9896(8.6011) | Xent 2.3026(2.3026) | Loss 7.9896(8.6011) | Error 0.9033(0.9018) Steps 574(574.00) | Grad Norm 2.7441(7.8852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 83.3265, Epoch Time 866.0116(866.0116), Bit/dim 7.7999(best: inf), Xent 2.3026, Loss 7.7999, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.7842(16.5780) | Bit/dim 7.6420(8.3920) | Xent 2.3026(2.3026) | Loss 7.6420(8.3920) | Error 0.8967(0.9010) Steps 574(574.00) | Grad Norm 2.3810(6.4974) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.4458(15.7883) | Bit/dim 7.3779(8.1588) | Xent 2.3026(2.3026) | Loss 7.3779(8.1588) | Error 0.9056(0.9008) Steps 574(574.00) | Grad Norm 2.2163(5.4003) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 13.2982(15.1398) | Bit/dim 7.1643(7.9202) | Xent 2.3026(2.3026) | Loss 7.1643(7.9202) | Error 0.9022(0.9007) Steps 574(574.00) | Grad Norm 1.5065(4.4271) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.4014(14.7984) | Bit/dim 7.0678(7.7082) | Xent 2.3026(2.3026) | Loss 7.0678(7.7082) | Error 0.9122(0.9015) Steps 598(577.76) | Grad Norm 0.8799(3.5493) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.5656(14.9233) | Bit/dim 7.0131(7.5309) | Xent 2.3026(2.3026) | Loss 7.0131(7.5309) | Error 0.8989(0.9008) Steps 610(585.38) | Grad Norm 0.6027(2.7969) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.5593(15.0512) | Bit/dim 6.9638(7.3885) | Xent 2.3026(2.3026) | Loss 6.9638(7.3885) | Error 0.8967(0.8997) Steps 622(593.32) | Grad Norm 0.7683(2.1996) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 82.8286, Epoch Time 881.7266(866.4830), Bit/dim 6.9768(best: 7.7999), Xent 2.3026, Loss 6.9768, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 16.4512(15.3065) | Bit/dim 6.9411(7.2770) | Xent 2.3026(2.3026) | Loss 6.9411(7.2770) | Error 0.8856(0.8989) Steps 628(602.43) | Grad Norm 0.6059(1.7915) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.0244(15.4629) | Bit/dim 6.9073(7.1809) | Xent 2.3026(2.3026) | Loss 6.9073(7.1809) | Error 0.9000(0.8989) Steps 628(609.14) | Grad Norm 0.6719(1.4901) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 15.9775(15.6878) | Bit/dim 6.8255(7.0992) | Xent 2.3026(2.3026) | Loss 6.8255(7.0992) | Error 0.8956(0.8986) Steps 634(615.10) | Grad Norm 0.4376(1.2227) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 17.1755(15.9085) | Bit/dim 6.7153(7.0159) | Xent 2.3026(2.3026) | Loss 6.7153(7.0159) | Error 0.8978(0.9002) Steps 640(620.58) | Grad Norm 0.5989(1.0658) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 16.4811(16.0922) | Bit/dim 6.6086(6.9249) | Xent 2.3026(2.3026) | Loss 6.6086(6.9249) | Error 0.9033(0.9004) Steps 640(625.68) | Grad Norm 0.8845(1.0706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 84.3533, Epoch Time 997.7760(870.4218), Bit/dim 6.4862(best: 6.9768), Xent 2.3026, Loss 6.4862, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 16.3661(16.1611) | Bit/dim 6.3848(6.8127) | Xent 2.3026(2.3026) | Loss 6.3848(6.8127) | Error 0.8956(0.9005) Steps 634(629.26) | Grad Norm 7.5542(4.0403) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 17.0774(16.2365) | Bit/dim 6.2435(6.6740) | Xent 2.3026(2.3026) | Loss 6.2435(6.6740) | Error 0.8833(0.8994) Steps 646(632.26) | Grad Norm 40.8854(8.1736) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.5501(16.4883) | Bit/dim 5.9841(6.5178) | Xent 2.3026(2.3026) | Loss 5.9841(6.5178) | Error 0.8967(0.8991) Steps 658(638.16) | Grad Norm 20.7582(11.4618) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.4148(16.7537) | Bit/dim 5.8341(6.3566) | Xent 2.3026(2.3026) | Loss 5.8341(6.3566) | Error 0.8778(0.8991) Steps 664(645.25) | Grad Norm 8.8624(12.1642) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.1378(16.8683) | Bit/dim 5.7352(6.2039) | Xent 2.3026(2.3026) | Loss 5.7352(6.2039) | Error 0.8911(0.8999) Steps 670(650.23) | Grad Norm 12.2313(11.6985) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 17.2704(16.9398) | Bit/dim 5.6482(6.0679) | Xent 2.3026(2.3026) | Loss 5.6482(6.0679) | Error 0.9122(0.9003) Steps 670(655.28) | Grad Norm 5.1669(11.0340) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 87.8979, Epoch Time 1041.8277(875.5640), Bit/dim 5.6558(best: 6.4862), Xent 2.3026, Loss 5.6558, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.6816(17.0816) | Bit/dim 5.6413(5.9557) | Xent 2.3026(2.3026) | Loss 5.6413(5.9557) | Error 0.9078(0.8997) Steps 676(660.30) | Grad Norm 1.6032(10.1274) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.5324(17.1691) | Bit/dim 5.5583(5.8615) | Xent 2.3026(2.3026) | Loss 5.5583(5.8615) | Error 0.8911(0.8997) Steps 688(665.41) | Grad Norm 4.1696(8.9473) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.6287(17.3323) | Bit/dim 5.5807(5.7857) | Xent 2.3026(2.3026) | Loss 5.5807(5.7857) | Error 0.8944(0.9004) Steps 688(670.31) | Grad Norm 31.0804(9.2399) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.6983(17.2541) | Bit/dim 5.5640(5.7378) | Xent 2.3026(2.3026) | Loss 5.5640(5.7378) | Error 0.8922(0.8996) Steps 658(668.65) | Grad Norm 6.1007(11.0912) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 17.0709(17.1046) | Bit/dim 5.5209(5.6908) | Xent 2.3026(2.3026) | Loss 5.5209(5.6908) | Error 0.9111(0.9006) Steps 652(665.02) | Grad Norm 3.5542(9.3971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 86.7170, Epoch Time 1052.1156(880.8606), Bit/dim 5.5194(best: 5.6558), Xent 2.3026, Loss 5.5194, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 17.1939(17.0095) | Bit/dim 5.5396(5.6451) | Xent 2.3026(2.3026) | Loss 5.5396(5.6451) | Error 0.8933(0.9009) Steps 670(663.91) | Grad Norm 3.9052(7.8033) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.3440(17.0846) | Bit/dim 5.4942(5.6096) | Xent 2.3026(2.3026) | Loss 5.4942(5.6096) | Error 0.9156(0.9021) Steps 676(665.64) | Grad Norm 4.8810(6.7140) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 17.7249(17.1787) | Bit/dim 5.4677(5.5729) | Xent 2.3026(2.3026) | Loss 5.4677(5.5729) | Error 0.9044(0.9022) Steps 676(667.73) | Grad Norm 1.3882(6.4634) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 16.8412(17.1917) | Bit/dim 5.3852(5.5358) | Xent 2.3026(2.3026) | Loss 5.3852(5.5358) | Error 0.8911(0.9000) Steps 670(668.61) | Grad Norm 2.1414(6.6183) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 16.6919(17.1070) | Bit/dim 5.4085(5.5003) | Xent 2.3026(2.3026) | Loss 5.4085(5.5003) | Error 0.8944(0.8998) Steps 670(669.93) | Grad Norm 6.0011(6.4269) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.5317(17.1131) | Bit/dim 5.3498(5.4605) | Xent 2.3026(2.3026) | Loss 5.3498(5.4605) | Error 0.8878(0.8991) Steps 682(672.19) | Grad Norm 16.0842(7.7944) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 88.6771, Epoch Time 1050.4970(885.9496), Bit/dim 5.3730(best: 5.5194), Xent 2.3026, Loss 5.3730, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 17.1026(17.0975) | Bit/dim 5.2735(5.4230) | Xent 2.3026(2.3026) | Loss 5.2735(5.4230) | Error 0.8900(0.8995) Steps 676(672.86) | Grad Norm 7.2083(8.5807) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 17.6214(17.0975) | Bit/dim 5.2304(5.3775) | Xent 2.3026(2.3026) | Loss 5.2304(5.3775) | Error 0.9078(0.9010) Steps 688(674.55) | Grad Norm 9.4607(7.9689) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 17.2962(17.1883) | Bit/dim 5.1692(5.3295) | Xent 2.3026(2.3026) | Loss 5.1692(5.3295) | Error 0.8878(0.8990) Steps 688(677.59) | Grad Norm 4.6420(8.3666) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 17.1486(17.2219) | Bit/dim 5.1106(5.2817) | Xent 2.3026(2.3026) | Loss 5.1106(5.2817) | Error 0.8933(0.8983) Steps 682(680.10) | Grad Norm 4.6581(7.8380) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.0487(17.1635) | Bit/dim 5.1407(5.2429) | Xent 2.3026(2.3026) | Loss 5.1407(5.2429) | Error 0.9078(0.8989) Steps 676(679.89) | Grad Norm 5.4015(8.7901) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 90.2239, Epoch Time 1050.6073(890.8894), Bit/dim 5.0672(best: 5.3730), Xent 2.3026, Loss 5.0672, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 16.1647(16.9915) | Bit/dim 5.0780(5.2084) | Xent 2.3026(2.3026) | Loss 5.0780(5.2084) | Error 0.9167(0.9000) Steps 664(677.20) | Grad Norm 4.5056(9.4992) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.5648(16.8161) | Bit/dim 5.0505(5.1650) | Xent 2.3026(2.3026) | Loss 5.0505(5.1650) | Error 0.8989(0.8995) Steps 664(674.04) | Grad Norm 6.8777(8.9566) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 16.7506(16.7842) | Bit/dim 4.9960(5.1218) | Xent 2.3026(2.3026) | Loss 4.9960(5.1218) | Error 0.8978(0.8987) Steps 670(672.65) | Grad Norm 3.9349(7.7200) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 16.8626(16.8036) | Bit/dim 4.9444(5.0774) | Xent 2.3026(2.3026) | Loss 4.9444(5.0774) | Error 0.8989(0.8986) Steps 670(671.34) | Grad Norm 6.0725(6.7180) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 16.5278(16.7805) | Bit/dim 4.9166(5.0388) | Xent 2.3026(2.3026) | Loss 4.9166(5.0388) | Error 0.9122(0.8991) Steps 664(670.49) | Grad Norm 4.4780(6.7508) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 16.6388(16.8123) | Bit/dim 4.8601(5.0018) | Xent 2.3026(2.3026) | Loss 4.8601(5.0018) | Error 0.9022(0.9005) Steps 664(669.27) | Grad Norm 5.5070(6.8462) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 91.0961, Epoch Time 1025.9138(894.9401), Bit/dim 4.9198(best: 5.0672), Xent 2.3026, Loss 4.9198, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 16.7118(16.8196) | Bit/dim 4.9175(4.9834) | Xent 2.3026(2.3026) | Loss 4.9175(4.9834) | Error 0.8944(0.9002) Steps 676(668.84) | Grad Norm 15.2183(9.8077) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 17.4739(16.8985) | Bit/dim 4.8609(4.9513) | Xent 2.3026(2.3026) | Loss 4.8609(4.9513) | Error 0.9089(0.9010) Steps 676(669.82) | Grad Norm 7.1557(9.5003) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 17.6517(17.0830) | Bit/dim 4.8065(4.9163) | Xent 2.3026(2.3026) | Loss 4.8065(4.9163) | Error 0.9056(0.9006) Steps 682(672.16) | Grad Norm 5.8457(8.6168) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 17.7872(17.3107) | Bit/dim 4.7556(4.8839) | Xent 2.3026(2.3026) | Loss 4.7556(4.8839) | Error 0.9067(0.8997) Steps 682(676.26) | Grad Norm 4.2879(8.3858) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.0504(17.4615) | Bit/dim 4.7917(4.8533) | Xent 2.3026(2.3026) | Loss 4.7917(4.8533) | Error 0.8944(0.9007) Steps 712(681.22) | Grad Norm 16.4437(9.1131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 94.0610, Epoch Time 1077.3080(900.4111), Bit/dim 4.7226(best: 4.9198), Xent 2.3026, Loss 4.7226, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.7948(17.6802) | Bit/dim 4.7502(4.8241) | Xent 2.3026(2.3026) | Loss 4.7502(4.8241) | Error 0.8844(0.9002) Steps 724(687.21) | Grad Norm 13.3709(9.4181) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 19.2507(17.9465) | Bit/dim 4.6978(4.7959) | Xent 2.3026(2.3026) | Loss 4.6978(4.7959) | Error 0.9000(0.8987) Steps 724(693.15) | Grad Norm 9.3443(9.5089) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 18.8314(18.0961) | Bit/dim 4.6804(4.7716) | Xent 2.3026(2.3026) | Loss 4.6804(4.7716) | Error 0.9044(0.8995) Steps 706(697.93) | Grad Norm 5.4237(9.8693) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 17.5318(18.1808) | Bit/dim 5.4587(4.7903) | Xent 2.3026(2.3026) | Loss 5.4587(4.7903) | Error 0.9011(0.9005) Steps 682(700.40) | Grad Norm 26.1441(12.4708) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 17.2345(17.9742) | Bit/dim 5.0619(4.9123) | Xent 2.3026(2.3026) | Loss 5.0619(4.9123) | Error 0.9089(0.9005) Steps 676(697.30) | Grad Norm 8.1876(13.1632) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 17.5860(17.7246) | Bit/dim 4.8314(4.9182) | Xent 2.3026(2.3026) | Loss 4.8314(4.9182) | Error 0.8944(0.9005) Steps 670(688.76) | Grad Norm 3.8661(10.9907) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 89.4141, Epoch Time 1098.9078(906.3660), Bit/dim 4.8404(best: 4.7226), Xent 2.3026, Loss 4.8404, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 16.6943(17.6222) | Bit/dim 4.7203(4.8786) | Xent 2.3026(2.3026) | Loss 4.7203(4.8786) | Error 0.9111(0.9008) Steps 664(685.61) | Grad Norm 2.2335(8.7451) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 17.2302(17.4509) | Bit/dim 4.6561(4.8315) | Xent 2.3026(2.3026) | Loss 4.6561(4.8315) | Error 0.9000(0.9002) Steps 670(681.42) | Grad Norm 1.7384(6.9122) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.4743(17.5779) | Bit/dim 4.6325(4.7831) | Xent 2.3026(2.3026) | Loss 4.6325(4.7831) | Error 0.9022(0.8998) Steps 712(686.06) | Grad Norm 1.1231(5.5107) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.6345(17.8193) | Bit/dim 4.6009(4.7380) | Xent 2.3026(2.3026) | Loss 4.6009(4.7380) | Error 0.9022(0.9012) Steps 718(693.08) | Grad Norm 3.0374(4.6868) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 18.5768(17.9742) | Bit/dim 4.5686(4.7033) | Xent 2.3026(2.3026) | Loss 4.5686(4.7033) | Error 0.8944(0.8998) Steps 724(698.09) | Grad Norm 2.7390(4.0238) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 95.8737, Epoch Time 1097.4691(912.0991), Bit/dim 4.5784(best: 4.7226), Xent 2.3026, Loss 4.5784, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 18.4018(18.1121) | Bit/dim 4.5490(4.6702) | Xent 2.3026(2.3026) | Loss 4.5490(4.6702) | Error 0.9056(0.9005) Steps 730(703.23) | Grad Norm 0.5515(3.3445) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 18.3280(18.2334) | Bit/dim 4.5790(4.6437) | Xent 2.3026(2.3026) | Loss 4.5790(4.6437) | Error 0.8844(0.9014) Steps 706(704.83) | Grad Norm 0.9812(2.7175) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 18.3414(18.2631) | Bit/dim 4.5551(4.6184) | Xent 2.3026(2.3026) | Loss 4.5551(4.6184) | Error 0.9011(0.9004) Steps 712(707.60) | Grad Norm 8.2800(3.0039) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 17.8599(18.2799) | Bit/dim 4.5232(4.5963) | Xent 2.3026(2.3026) | Loss 4.5232(4.5963) | Error 0.8922(0.9002) Steps 712(710.22) | Grad Norm 4.1183(3.9438) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 18.2726(18.2461) | Bit/dim 4.4988(4.5726) | Xent 2.3026(2.3026) | Loss 4.4988(4.5726) | Error 0.9044(0.9001) Steps 718(711.80) | Grad Norm 1.2954(3.4966) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.0430(18.2260) | Bit/dim 4.5398(4.5607) | Xent 2.3026(2.3026) | Loss 4.5398(4.5607) | Error 0.9078(0.9000) Steps 700(712.56) | Grad Norm 10.1699(4.8037) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 94.8146, Epoch Time 1121.6413(918.3854), Bit/dim 4.5337(best: 4.5784), Xent 2.3026, Loss 4.5337, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 17.5908(18.0739) | Bit/dim 4.5106(4.5477) | Xent 2.3026(2.3026) | Loss 4.5106(4.5477) | Error 0.9133(0.9002) Steps 694(708.82) | Grad Norm 8.4756(5.7056) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 18.0322(18.0820) | Bit/dim 4.5194(4.5397) | Xent 2.3026(2.3026) | Loss 4.5194(4.5397) | Error 0.9111(0.8991) Steps 694(705.99) | Grad Norm 7.9455(6.6341) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 17.6250(18.0205) | Bit/dim 4.5087(4.5280) | Xent 2.3026(2.3026) | Loss 4.5087(4.5280) | Error 0.9156(0.9007) Steps 694(703.26) | Grad Norm 6.7266(6.7073) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 18.1293(18.0433) | Bit/dim 4.4677(4.5084) | Xent 2.3026(2.3026) | Loss 4.4677(4.5084) | Error 0.9000(0.9001) Steps 694(699.92) | Grad Norm 4.8130(5.9552) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 18.1734(18.0173) | Bit/dim 4.4080(4.4865) | Xent 2.3026(2.3026) | Loss 4.4080(4.4865) | Error 0.9078(0.9006) Steps 706(699.18) | Grad Norm 2.2093(4.9056) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 87.2738, Epoch Time 1093.4796(923.6382), Bit/dim 4.5946(best: 4.5337), Xent 2.3026, Loss 4.5946, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 18.4378(18.0873) | Bit/dim 4.4632(4.4819) | Xent 2.3026(2.3026) | Loss 4.4632(4.4819) | Error 0.9100(0.8991) Steps 706(699.69) | Grad Norm 9.4003(6.5833) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 18.6927(18.1171) | Bit/dim 4.4332(4.4811) | Xent 2.3026(2.3026) | Loss 4.4332(4.4811) | Error 0.9111(0.8983) Steps 706(702.33) | Grad Norm 7.7068(7.2386) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 18.1313(18.1670) | Bit/dim 4.3913(4.4628) | Xent 2.3026(2.3026) | Loss 4.3913(4.4628) | Error 0.8989(0.8987) Steps 670(699.26) | Grad Norm 1.9565(6.2364) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 18.2707(18.1126) | Bit/dim 4.3562(4.4413) | Xent 2.3026(2.3026) | Loss 4.3562(4.4413) | Error 0.8989(0.8996) Steps 694(694.03) | Grad Norm 2.3432(5.2439) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 18.4393(18.2088) | Bit/dim 4.3525(4.4173) | Xent 2.3026(2.3026) | Loss 4.3525(4.4173) | Error 0.8956(0.9007) Steps 700(694.42) | Grad Norm 2.1657(4.6893) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 18.7310(18.3636) | Bit/dim 4.3702(4.4071) | Xent 2.3026(2.3026) | Loss 4.3702(4.4071) | Error 0.8933(0.9008) Steps 712(698.35) | Grad Norm 4.0076(4.6269) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 91.9429, Epoch Time 1118.0821(929.4715), Bit/dim 4.3546(best: 4.5337), Xent 2.3026, Loss 4.3546, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.6294(18.4726) | Bit/dim 4.3389(4.3954) | Xent 2.3026(2.3026) | Loss 4.3389(4.3954) | Error 0.8967(0.8998) Steps 718(703.25) | Grad Norm 2.1721(5.1643) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 17.7791(18.4869) | Bit/dim 4.3651(4.3912) | Xent 2.3026(2.3026) | Loss 4.3651(4.3912) | Error 0.9000(0.9007) Steps 694(704.03) | Grad Norm 9.8148(6.3307) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 18.7076(18.5013) | Bit/dim 4.3216(4.3793) | Xent 2.3026(2.3026) | Loss 4.3216(4.3793) | Error 0.9000(0.9015) Steps 712(705.42) | Grad Norm 4.9879(6.3621) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 18.3271(18.5312) | Bit/dim 4.2971(4.3613) | Xent 2.3026(2.3026) | Loss 4.2971(4.3613) | Error 0.8989(0.9009) Steps 694(707.79) | Grad Norm 2.3407(5.6361) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 19.0427(18.6506) | Bit/dim 4.5118(4.3501) | Xent 2.3026(2.3026) | Loss 4.5118(4.3501) | Error 0.8967(0.8996) Steps 760(714.77) | Grad Norm 18.1639(5.8011) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 93.3453, Epoch Time 1137.2563(935.7051), Bit/dim 4.3288(best: 4.3546), Xent 2.3026, Loss 4.3288, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 18.0758(18.4583) | Bit/dim 4.2912(4.3517) | Xent 2.3026(2.3026) | Loss 4.2912(4.3517) | Error 0.8889(0.8986) Steps 712(712.43) | Grad Norm 5.7126(6.3103) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 18.1154(18.4355) | Bit/dim 4.2599(4.3337) | Xent 2.3026(2.3026) | Loss 4.2599(4.3337) | Error 0.8978(0.8985) Steps 706(711.83) | Grad Norm 3.6960(5.6625) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 18.0412(18.4383) | Bit/dim 4.2189(4.3107) | Xent 2.3026(2.3026) | Loss 4.2189(4.3107) | Error 0.9122(0.8999) Steps 724(711.73) | Grad Norm 2.2662(4.8844) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 19.2570(18.4906) | Bit/dim 4.2806(4.2961) | Xent 2.3026(2.3026) | Loss 4.2806(4.2961) | Error 0.8956(0.8997) Steps 760(717.03) | Grad Norm 9.3096(5.1740) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 18.8663(18.5173) | Bit/dim 4.2924(4.2899) | Xent 2.3026(2.3026) | Loss 4.2924(4.2899) | Error 0.9044(0.9005) Steps 742(720.12) | Grad Norm 8.3949(5.7770) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 18.3149(18.5647) | Bit/dim 4.2129(4.2752) | Xent 2.3026(2.3026) | Loss 4.2129(4.2752) | Error 0.8811(0.9003) Steps 736(724.24) | Grad Norm 2.8211(5.6417) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 97.4485, Epoch Time 1130.8979(941.5609), Bit/dim 4.2172(best: 4.3288), Xent 2.3026, Loss 4.2172, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 18.8230(18.6656) | Bit/dim 4.1830(4.2562) | Xent 2.3026(2.3026) | Loss 4.1830(4.2562) | Error 0.8878(0.8990) Steps 766(729.90) | Grad Norm 2.1995(5.0496) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 19.7132(18.9263) | Bit/dim 4.3294(4.2475) | Xent 2.3026(2.3026) | Loss 4.3294(4.2475) | Error 0.9044(0.8997) Steps 742(738.38) | Grad Norm 14.7725(5.2731) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 18.8389(18.8992) | Bit/dim 4.2004(4.2419) | Xent 2.3026(2.3026) | Loss 4.2004(4.2419) | Error 0.8911(0.9006) Steps 748(738.67) | Grad Norm 3.8052(5.2301) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 17.9419(18.8658) | Bit/dim 4.1627(4.2244) | Xent 2.3026(2.3026) | Loss 4.1627(4.2244) | Error 0.9089(0.9016) Steps 724(735.68) | Grad Norm 1.9453(4.7902) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 18.8939(18.8906) | Bit/dim 4.1310(4.2026) | Xent 2.3026(2.3026) | Loss 4.1310(4.2026) | Error 0.9022(0.8999) Steps 742(734.61) | Grad Norm 2.1905(4.0920) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 93.6930, Epoch Time 1158.5451(948.0704), Bit/dim 4.2754(best: 4.2172), Xent 2.3026, Loss 4.2754, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 18.7636(18.9296) | Bit/dim 4.2628(4.2003) | Xent 2.3026(2.3026) | Loss 4.2628(4.2003) | Error 0.9211(0.9001) Steps 742(734.94) | Grad Norm 10.9900(4.9529) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 19.0106(19.0751) | Bit/dim 4.1549(4.1920) | Xent 2.3026(2.3026) | Loss 4.1549(4.1920) | Error 0.8956(0.8999) Steps 760(739.71) | Grad Norm 3.4738(5.4256) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 20.5013(19.2024) | Bit/dim 4.1409(4.1787) | Xent 2.3026(2.3026) | Loss 4.1409(4.1787) | Error 0.8878(0.9010) Steps 772(745.01) | Grad Norm 3.4510(5.0517) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 19.4293(19.4668) | Bit/dim 4.0889(4.1583) | Xent 2.3026(2.3026) | Loss 4.0889(4.1583) | Error 0.8922(0.9008) Steps 772(754.77) | Grad Norm 3.4526(4.5107) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 19.3853(19.5236) | Bit/dim 4.0656(4.1482) | Xent 2.3026(2.3026) | Loss 4.0656(4.1482) | Error 0.8889(0.8988) Steps 760(757.64) | Grad Norm 3.3467(5.0357) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 18.5997(19.5573) | Bit/dim 4.0818(4.1416) | Xent 2.3026(2.3026) | Loss 4.0818(4.1416) | Error 0.8956(0.8999) Steps 748(760.89) | Grad Norm 4.7749(5.7396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 94.7798, Epoch Time 1193.9469(955.4467), Bit/dim 4.1114(best: 4.2172), Xent 2.3026, Loss 4.1114, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 19.4175(19.5882) | Bit/dim 4.0743(4.1289) | Xent 2.3026(2.3026) | Loss 4.0743(4.1289) | Error 0.9011(0.9011) Steps 760(762.34) | Grad Norm 5.4211(5.4650) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 20.6937(19.7429) | Bit/dim 4.0270(4.1138) | Xent 2.3026(2.3026) | Loss 4.0270(4.1138) | Error 0.9111(0.8998) Steps 778(768.86) | Grad Norm 3.5784(5.0512) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 20.3816(19.8557) | Bit/dim 4.0071(4.0986) | Xent 2.3026(2.3026) | Loss 4.0071(4.0986) | Error 0.8911(0.9005) Steps 808(774.12) | Grad Norm 3.5153(5.0333) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 20.1503(19.8989) | Bit/dim 4.0316(4.0864) | Xent 2.3026(2.3026) | Loss 4.0316(4.0864) | Error 0.9122(0.8990) Steps 778(778.37) | Grad Norm 5.8243(5.1422) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 19.6072(19.9728) | Bit/dim 4.0486(4.0735) | Xent 2.3026(2.3026) | Loss 4.0486(4.0735) | Error 0.9222(0.8995) Steps 808(782.49) | Grad Norm 4.2243(4.8011) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 98.5738, Epoch Time 1219.1249(963.3570), Bit/dim 4.0213(best: 4.1114), Xent 2.3026, Loss 4.0213, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 20.3343(20.0335) | Bit/dim 4.0379(4.0630) | Xent 2.3026(2.3026) | Loss 4.0379(4.0630) | Error 0.9111(0.9005) Steps 796(786.07) | Grad Norm 5.2868(4.3379) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 20.7806(20.1599) | Bit/dim 3.9800(4.0499) | Xent 2.3026(2.3026) | Loss 3.9800(4.0499) | Error 0.8856(0.9008) Steps 808(791.28) | Grad Norm 2.7951(4.2755) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 20.4176(20.2389) | Bit/dim 4.0408(4.0433) | Xent 2.3026(2.3026) | Loss 4.0408(4.0433) | Error 0.8911(0.9012) Steps 820(796.40) | Grad Norm 6.9320(4.6828) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 19.9983(20.2626) | Bit/dim 3.9907(4.0351) | Xent 2.3026(2.3026) | Loss 3.9907(4.0351) | Error 0.8878(0.8991) Steps 790(796.35) | Grad Norm 3.9584(4.9111) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 19.9086(20.2287) | Bit/dim 4.0067(4.0250) | Xent 2.3026(2.3026) | Loss 4.0067(4.0250) | Error 0.9044(0.8999) Steps 796(794.94) | Grad Norm 2.6135(4.5949) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 19.8380(20.2268) | Bit/dim 4.0164(4.0152) | Xent 2.3026(2.3026) | Loss 4.0164(4.0152) | Error 0.8911(0.8995) Steps 796(796.66) | Grad Norm 5.0042(4.5092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 100.6440, Epoch Time 1236.9982(971.5663), Bit/dim 3.9938(best: 4.0213), Xent 2.3026, Loss 3.9938, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 20.3526(20.2825) | Bit/dim 3.9486(4.0060) | Xent 2.3026(2.3026) | Loss 3.9486(4.0060) | Error 0.9144(0.9003) Steps 802(798.47) | Grad Norm 2.7657(4.6908) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 20.8041(20.3457) | Bit/dim 3.9442(3.9960) | Xent 2.3026(2.3026) | Loss 3.9442(3.9960) | Error 0.9100(0.8994) Steps 826(801.22) | Grad Norm 1.9630(4.4095) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 20.6660(20.4299) | Bit/dim 3.9532(3.9896) | Xent 2.3026(2.3026) | Loss 3.9532(3.9896) | Error 0.8878(0.9003) Steps 814(802.56) | Grad Norm 1.0775(4.2160) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 20.8164(20.4675) | Bit/dim 3.9673(3.9816) | Xent 2.3026(2.3026) | Loss 3.9673(3.9816) | Error 0.8967(0.8988) Steps 814(803.92) | Grad Norm 4.4303(4.6003) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 20.5265(20.4517) | Bit/dim 3.9350(3.9741) | Xent 2.3026(2.3026) | Loss 3.9350(3.9741) | Error 0.8856(0.8991) Steps 784(803.38) | Grad Norm 3.0225(4.5118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 100.4964, Epoch Time 1245.5472(979.7857), Bit/dim 3.9374(best: 3.9938), Xent 2.3026, Loss 3.9374, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 19.9300(20.4189) | Bit/dim 3.9224(3.9637) | Xent 2.3026(2.3026) | Loss 3.9224(3.9637) | Error 0.9011(0.9002) Steps 814(802.25) | Grad Norm 1.4371(4.0864) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 20.8455(20.4037) | Bit/dim 3.9595(3.9610) | Xent 2.3026(2.3026) | Loss 3.9595(3.9610) | Error 0.8978(0.9008) Steps 814(803.38) | Grad Norm 5.8406(3.9964) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 20.7558(20.3540) | Bit/dim 3.8994(3.9522) | Xent 2.3026(2.3026) | Loss 3.8994(3.9522) | Error 0.9022(0.9015) Steps 802(801.96) | Grad Norm 1.8188(4.0273) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.1253(20.3019) | Bit/dim 3.9071(3.9447) | Xent 2.3026(2.3026) | Loss 3.9071(3.9447) | Error 0.8711(0.9000) Steps 796(801.31) | Grad Norm 5.9370(4.1541) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 19.2590(20.1907) | Bit/dim 3.9325(3.9377) | Xent 2.3026(2.3026) | Loss 3.9325(3.9377) | Error 0.8889(0.9015) Steps 796(798.56) | Grad Norm 6.8773(4.2409) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 20.0580(20.1388) | Bit/dim 3.8897(3.9288) | Xent 2.3026(2.3026) | Loss 3.8897(3.9288) | Error 0.8922(0.8991) Steps 796(797.05) | Grad Norm 3.2785(4.4333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 99.8194, Epoch Time 1226.1813(987.1776), Bit/dim 3.9020(best: 3.9374), Xent 2.3026, Loss 3.9020, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 20.8275(20.0756) | Bit/dim 3.9312(3.9197) | Xent 2.3026(2.3026) | Loss 3.9312(3.9197) | Error 0.9200(0.8997) Steps 790(796.77) | Grad Norm 3.6355(3.9143) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 19.9599(20.0448) | Bit/dim 3.9173(3.9167) | Xent 2.3026(2.3026) | Loss 3.9173(3.9167) | Error 0.9011(0.9004) Steps 796(795.51) | Grad Norm 4.4380(4.0619) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 19.5086(20.0321) | Bit/dim 3.9067(3.9093) | Xent 2.3026(2.3026) | Loss 3.9067(3.9093) | Error 0.9033(0.9000) Steps 802(795.82) | Grad Norm 4.4645(4.4480) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 20.1703(20.0538) | Bit/dim 3.9220(3.9064) | Xent 2.3026(2.3026) | Loss 3.9220(3.9064) | Error 0.9111(0.8987) Steps 772(794.23) | Grad Norm 2.6371(4.4995) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 19.4105(20.0178) | Bit/dim 3.8788(3.9008) | Xent 2.3026(2.3026) | Loss 3.8788(3.9008) | Error 0.9122(0.8996) Steps 784(792.85) | Grad Norm 3.0484(4.2021) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 100.4899, Epoch Time 1218.8861(994.1288), Bit/dim 3.8722(best: 3.9020), Xent 2.3026, Loss 3.8722, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 20.6514(20.1176) | Bit/dim 3.8637(3.8940) | Xent 2.3026(2.3026) | Loss 3.8637(3.8940) | Error 0.8944(0.9009) Steps 772(790.71) | Grad Norm 6.4451(4.0754) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 19.8763(20.0777) | Bit/dim 3.9031(3.8907) | Xent 2.3026(2.3026) | Loss 3.9031(3.8907) | Error 0.8967(0.9011) Steps 778(791.60) | Grad Norm 7.6078(4.7602) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 19.6008(20.0406) | Bit/dim 3.8749(3.8884) | Xent 2.3026(2.3026) | Loss 3.8749(3.8884) | Error 0.9022(0.9003) Steps 784(791.24) | Grad Norm 3.6541(4.7280) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 20.2642(20.0698) | Bit/dim 3.8697(3.8822) | Xent 2.3026(2.3026) | Loss 3.8697(3.8822) | Error 0.8944(0.8995) Steps 802(791.28) | Grad Norm 3.2045(4.1858) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 20.3752(20.0532) | Bit/dim 3.8878(3.8764) | Xent 2.3026(2.3026) | Loss 3.8878(3.8764) | Error 0.9078(0.9000) Steps 802(790.38) | Grad Norm 4.3928(4.3816) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 20.0674(20.0691) | Bit/dim 3.8454(3.8703) | Xent 2.3026(2.3026) | Loss 3.8454(3.8703) | Error 0.9078(0.8997) Steps 790(791.63) | Grad Norm 2.5016(4.1485) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 99.3521, Epoch Time 1221.3566(1000.9457), Bit/dim 3.8464(best: 3.8722), Xent 2.3026, Loss 3.8464, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 20.7990(20.1272) | Bit/dim 3.8828(3.8647) | Xent 2.3026(2.3026) | Loss 3.8828(3.8647) | Error 0.9022(0.8992) Steps 784(793.16) | Grad Norm 5.5363(4.3781) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 20.7858(20.1815) | Bit/dim 3.8616(3.8587) | Xent 2.3026(2.3026) | Loss 3.8616(3.8587) | Error 0.8933(0.8993) Steps 796(793.98) | Grad Norm 1.7455(4.0011) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 20.7565(20.2220) | Bit/dim 3.8432(3.8518) | Xent 2.3026(2.3026) | Loss 3.8432(3.8518) | Error 0.9044(0.8993) Steps 802(794.49) | Grad Norm 4.7207(3.5285) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 20.9118(20.2488) | Bit/dim 3.8257(3.8510) | Xent 2.3026(2.3026) | Loss 3.8257(3.8510) | Error 0.8844(0.8993) Steps 790(795.98) | Grad Norm 4.2563(4.1872) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 20.3372(20.2035) | Bit/dim 3.8186(3.8501) | Xent 2.3026(2.3026) | Loss 3.8186(3.8501) | Error 0.8778(0.8993) Steps 796(793.10) | Grad Norm 5.6386(4.6357) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 97.2963, Epoch Time 1226.8063(1007.7215), Bit/dim 3.8347(best: 3.8464), Xent 2.3026, Loss 3.8347, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 20.1110(20.0502) | Bit/dim 3.8299(3.8486) | Xent 2.3026(2.3026) | Loss 3.8299(3.8486) | Error 0.9067(0.9005) Steps 814(789.35) | Grad Norm 4.0961(4.7239) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 20.0273(20.1353) | Bit/dim 3.8021(3.8426) | Xent 2.3026(2.3026) | Loss 3.8021(3.8426) | Error 0.9133(0.9013) Steps 778(787.64) | Grad Norm 3.2936(4.3814) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 20.7594(20.2165) | Bit/dim 3.8117(3.8380) | Xent 2.3026(2.3026) | Loss 3.8117(3.8380) | Error 0.9078(0.9021) Steps 784(786.60) | Grad Norm 4.4165(3.8969) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 20.6815(20.3259) | Bit/dim 3.7998(3.8324) | Xent 2.3026(2.3026) | Loss 3.7998(3.8324) | Error 0.8989(0.8994) Steps 820(790.69) | Grad Norm 6.3159(4.3741) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 20.3562(20.3514) | Bit/dim 3.8154(3.8284) | Xent 2.3026(2.3026) | Loss 3.8154(3.8284) | Error 0.8889(0.8995) Steps 784(791.28) | Grad Norm 3.2892(4.1870) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 20.6313(20.3453) | Bit/dim 3.8010(3.8242) | Xent 2.3026(2.3026) | Loss 3.8010(3.8242) | Error 0.9122(0.9000) Steps 802(792.69) | Grad Norm 1.7362(4.1415) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 100.9635, Epoch Time 1238.3147(1014.6393), Bit/dim 3.8058(best: 3.8347), Xent 2.3026, Loss 3.8058, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 20.0515(20.3027) | Bit/dim 3.7993(3.8187) | Xent 2.3026(2.3026) | Loss 3.7993(3.8187) | Error 0.8844(0.8994) Steps 790(791.65) | Grad Norm 6.4245(4.1464) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 19.8246(20.3568) | Bit/dim 3.8284(3.8213) | Xent 2.3026(2.3026) | Loss 3.8284(3.8213) | Error 0.8944(0.8995) Steps 754(791.78) | Grad Norm 6.3451(4.7609) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 19.8367(20.3112) | Bit/dim 3.7758(3.8205) | Xent 2.3026(2.3026) | Loss 3.7758(3.8205) | Error 0.8900(0.8991) Steps 760(788.43) | Grad Norm 4.3033(4.9064) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 21.2775(20.2475) | Bit/dim 3.7690(3.8141) | Xent 2.3026(2.3026) | Loss 3.7690(3.8141) | Error 0.8911(0.8992) Steps 808(785.63) | Grad Norm 3.9421(4.6026) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 20.4758(20.2670) | Bit/dim 3.7793(3.8107) | Xent 2.3026(2.3026) | Loss 3.7793(3.8107) | Error 0.8989(0.8997) Steps 802(784.84) | Grad Norm 5.6292(4.2930) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 97.4486, Epoch Time 1231.1652(1021.1350), Bit/dim 3.7952(best: 3.8058), Xent 2.3026, Loss 3.7952, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 20.4683(20.3442) | Bit/dim 3.8165(3.8079) | Xent 2.3026(2.3026) | Loss 3.8165(3.8079) | Error 0.9000(0.8999) Steps 796(786.34) | Grad Norm 1.5323(4.3115) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 21.3040(20.4175) | Bit/dim 3.8003(3.8030) | Xent 2.3026(2.3026) | Loss 3.8003(3.8030) | Error 0.9022(0.9008) Steps 784(785.56) | Grad Norm 6.1592(3.9957) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 20.1806(20.4758) | Bit/dim 3.8279(3.8013) | Xent 2.3026(2.3026) | Loss 3.8279(3.8013) | Error 0.8967(0.8993) Steps 790(786.39) | Grad Norm 6.2996(4.2795) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 19.3992(20.4263) | Bit/dim 3.7703(3.7979) | Xent 2.3026(2.3026) | Loss 3.7703(3.7979) | Error 0.8878(0.8996) Steps 784(786.32) | Grad Norm 3.9005(4.6420) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 21.1347(20.5634) | Bit/dim 3.7770(3.7944) | Xent 2.3026(2.3026) | Loss 3.7770(3.7944) | Error 0.8989(0.9002) Steps 802(788.58) | Grad Norm 2.0385(4.5566) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 20.5654(20.5346) | Bit/dim 3.8041(3.7894) | Xent 2.3026(2.3026) | Loss 3.8041(3.7894) | Error 0.9100(0.9004) Steps 808(789.35) | Grad Norm 4.8817(4.3396) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 98.4806, Epoch Time 1248.8008(1027.9650), Bit/dim 3.7767(best: 3.7952), Xent 2.3026, Loss 3.7767, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 20.6909(20.5072) | Bit/dim 3.7652(3.7862) | Xent 2.3026(2.3026) | Loss 3.7652(3.7862) | Error 0.9011(0.9005) Steps 796(789.50) | Grad Norm 5.5478(4.2727) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 20.4599(20.4960) | Bit/dim 3.7898(3.7859) | Xent 2.3026(2.3026) | Loss 3.7898(3.7859) | Error 0.9000(0.8996) Steps 772(787.62) | Grad Norm 3.1053(4.3165) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 19.5206(20.5314) | Bit/dim 3.7537(3.7825) | Xent 2.3026(2.3026) | Loss 3.7537(3.7825) | Error 0.8956(0.8999) Steps 778(789.29) | Grad Norm 1.9152(4.1898) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 20.7519(20.5553) | Bit/dim 3.7487(3.7774) | Xent 2.3026(2.3026) | Loss 3.7487(3.7774) | Error 0.8944(0.8992) Steps 802(791.32) | Grad Norm 6.4597(4.4027) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 20.2985(20.5950) | Bit/dim 3.7727(3.7740) | Xent 2.3026(2.3026) | Loss 3.7727(3.7740) | Error 0.9078(0.8995) Steps 772(790.67) | Grad Norm 4.3855(4.2443) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 96.1159, Epoch Time 1244.3220(1034.4557), Bit/dim 3.7748(best: 3.7767), Xent 2.3026, Loss 3.7748, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 20.4667(20.5395) | Bit/dim 3.7563(3.7744) | Xent 2.3026(2.3026) | Loss 3.7563(3.7744) | Error 0.9133(0.9014) Steps 760(790.38) | Grad Norm 3.2102(4.5999) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 19.6466(20.5328) | Bit/dim 3.7720(3.7755) | Xent 2.3026(2.3026) | Loss 3.7720(3.7755) | Error 0.9011(0.9013) Steps 796(791.41) | Grad Norm 6.4718(4.9719) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 20.7087(20.4505) | Bit/dim 3.7545(3.7724) | Xent 2.3026(2.3026) | Loss 3.7545(3.7724) | Error 0.9200(0.9012) Steps 772(787.19) | Grad Norm 3.7556(4.6233) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 20.5193(20.4029) | Bit/dim 3.7727(3.7694) | Xent 2.3026(2.3026) | Loss 3.7727(3.7694) | Error 0.8989(0.9003) Steps 808(786.74) | Grad Norm 4.4812(4.2707) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 20.7001(20.5430) | Bit/dim 3.7811(3.7677) | Xent 2.3026(2.3026) | Loss 3.7811(3.7677) | Error 0.8944(0.9006) Steps 784(788.25) | Grad Norm 3.3696(4.0153) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 21.3069(20.5933) | Bit/dim 3.7277(3.7626) | Xent 2.3026(2.3026) | Loss 3.7277(3.7626) | Error 0.8900(0.8999) Steps 820(788.93) | Grad Norm 3.4731(4.0252) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 97.8585, Epoch Time 1243.8907(1040.7388), Bit/dim 3.7554(best: 3.7748), Xent 2.3026, Loss 3.7554, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 20.9661(20.6677) | Bit/dim 3.7823(3.7616) | Xent 2.3026(2.3026) | Loss 3.7823(3.7616) | Error 0.9089(0.9000) Steps 814(790.58) | Grad Norm 3.1447(4.0953) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 20.1818(20.7117) | Bit/dim 3.7641(3.7568) | Xent 2.3026(2.3026) | Loss 3.7641(3.7568) | Error 0.8956(0.8994) Steps 796(793.54) | Grad Norm 4.5569(4.3618) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 21.8707(20.7554) | Bit/dim 3.7714(3.7552) | Xent 2.3026(2.3026) | Loss 3.7714(3.7552) | Error 0.8989(0.8991) Steps 838(794.17) | Grad Norm 4.3702(4.3464) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 19.6792(20.6303) | Bit/dim 3.7560(3.7543) | Xent 2.3026(2.3026) | Loss 3.7560(3.7543) | Error 0.9044(0.8994) Steps 760(791.73) | Grad Norm 1.1400(4.2341) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 20.2614(20.5080) | Bit/dim 3.7496(3.7530) | Xent 2.3026(2.3026) | Loss 3.7496(3.7530) | Error 0.9033(0.8993) Steps 802(790.16) | Grad Norm 5.9755(4.3242) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 98.8171, Epoch Time 1249.8741(1047.0128), Bit/dim 3.7423(best: 3.7554), Xent 2.3026, Loss 3.7423, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 20.1080(20.4736) | Bit/dim 3.7292(3.7483) | Xent 2.3026(2.3026) | Loss 3.7292(3.7483) | Error 0.9011(0.9004) Steps 790(791.06) | Grad Norm 1.1840(4.1343) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 20.4022(20.4796) | Bit/dim 3.7346(3.7470) | Xent 2.3026(2.3026) | Loss 3.7346(3.7470) | Error 0.8878(0.9002) Steps 784(792.58) | Grad Norm 5.4354(4.0629) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 21.4250(20.5193) | Bit/dim 3.7189(3.7438) | Xent 2.3026(2.3026) | Loss 3.7189(3.7438) | Error 0.8900(0.8995) Steps 814(793.74) | Grad Norm 4.2224(4.3528) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 20.2113(20.5054) | Bit/dim 3.7210(3.7416) | Xent 2.3026(2.3026) | Loss 3.7210(3.7416) | Error 0.8922(0.8997) Steps 808(794.10) | Grad Norm 2.7284(4.4136) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 19.9915(20.4205) | Bit/dim 3.7105(3.7380) | Xent 2.3026(2.3026) | Loss 3.7105(3.7380) | Error 0.9033(0.8998) Steps 796(794.44) | Grad Norm 2.0982(4.0310) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 20.4070(20.4774) | Bit/dim 3.7249(3.7392) | Xent 2.3026(2.3026) | Loss 3.7249(3.7392) | Error 0.8922(0.9006) Steps 790(792.04) | Grad Norm 4.7600(3.9947) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 97.8995, Epoch Time 1241.6859(1052.8530), Bit/dim 3.7394(best: 3.7423), Xent 2.3026, Loss 3.7394, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 20.4493(20.5004) | Bit/dim 3.7495(3.7382) | Xent 2.3026(2.3026) | Loss 3.7495(3.7382) | Error 0.9078(0.8997) Steps 796(792.02) | Grad Norm 4.5118(4.2927) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 19.9874(20.4505) | Bit/dim 3.7519(3.7363) | Xent 2.3026(2.3026) | Loss 3.7519(3.7363) | Error 0.8956(0.8996) Steps 778(792.09) | Grad Norm 5.8971(4.2557) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 20.4705(20.4068) | Bit/dim 3.7218(3.7330) | Xent 2.3026(2.3026) | Loss 3.7218(3.7330) | Error 0.8867(0.9000) Steps 784(790.92) | Grad Norm 2.3612(4.1909) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 21.0694(20.4133) | Bit/dim 3.7165(3.7307) | Xent 2.3026(2.3026) | Loss 3.7165(3.7307) | Error 0.9056(0.9002) Steps 766(791.21) | Grad Norm 4.2822(4.0306) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 20.7920(20.4502) | Bit/dim 3.7187(3.7279) | Xent 2.3026(2.3026) | Loss 3.7187(3.7279) | Error 0.8933(0.9012) Steps 778(789.58) | Grad Norm 5.4411(4.1253) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 99.4922, Epoch Time 1241.6893(1058.5181), Bit/dim 3.7170(best: 3.7394), Xent 2.3026, Loss 3.7170, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 20.2595(20.4525) | Bit/dim 3.7451(3.7258) | Xent 2.3026(2.3026) | Loss 3.7451(3.7258) | Error 0.9244(0.9018) Steps 796(790.46) | Grad Norm 3.7274(4.1357) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 19.6743(20.4629) | Bit/dim 3.7103(3.7227) | Xent 2.3026(2.3026) | Loss 3.7103(3.7227) | Error 0.9067(0.9010) Steps 766(789.76) | Grad Norm 5.8231(4.0953) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 19.6721(20.4764) | Bit/dim 3.7108(3.7224) | Xent 2.3026(2.3026) | Loss 3.7108(3.7224) | Error 0.8844(0.8998) Steps 802(791.22) | Grad Norm 7.4681(4.3690) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 20.9539(20.4879) | Bit/dim 3.6835(3.7206) | Xent 2.3026(2.3026) | Loss 3.6835(3.7206) | Error 0.8978(0.8995) Steps 766(790.88) | Grad Norm 4.5882(4.5305) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 20.7656(20.4686) | Bit/dim 3.6979(3.7205) | Xent 2.3026(2.3026) | Loss 3.6979(3.7205) | Error 0.8922(0.8997) Steps 790(791.23) | Grad Norm 3.2472(4.2240) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 20.5888(20.4946) | Bit/dim 3.7195(3.7185) | Xent 2.3026(2.3026) | Loss 3.7195(3.7185) | Error 0.9133(0.9000) Steps 778(790.42) | Grad Norm 6.0744(4.0416) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 96.7190, Epoch Time 1241.5841(1064.0101), Bit/dim 3.7299(best: 3.7170), Xent 2.3026, Loss 3.7299, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 20.2198(20.4240) | Bit/dim 3.7033(3.7166) | Xent 2.3026(2.3026) | Loss 3.7033(3.7166) | Error 0.9011(0.8985) Steps 778(789.60) | Grad Norm 3.4665(4.1013) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 20.2455(20.3764) | Bit/dim 3.6885(3.7141) | Xent 2.3026(2.3026) | Loss 3.6885(3.7141) | Error 0.9011(0.8991) Steps 790(787.02) | Grad Norm 2.6989(3.8545) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 20.5463(20.3193) | Bit/dim 3.7182(3.7147) | Xent 2.3026(2.3026) | Loss 3.7182(3.7147) | Error 0.9100(0.9006) Steps 784(787.31) | Grad Norm 3.3160(3.7302) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 20.1658(20.3647) | Bit/dim 3.7102(3.7124) | Xent 2.3026(2.3026) | Loss 3.7102(3.7124) | Error 0.8989(0.9008) Steps 790(788.41) | Grad Norm 6.2462(3.7955) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 20.3144(20.4043) | Bit/dim 3.6815(3.7082) | Xent 2.3026(2.3026) | Loss 3.6815(3.7082) | Error 0.8944(0.9006) Steps 802(790.84) | Grad Norm 3.7809(3.7633) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 97.4438, Epoch Time 1231.9398(1069.0480), Bit/dim 3.7048(best: 3.7170), Xent 2.3026, Loss 3.7048, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 19.8035(20.3069) | Bit/dim 3.7365(3.7075) | Xent 2.3026(2.3026) | Loss 3.7365(3.7075) | Error 0.8956(0.8998) Steps 796(790.80) | Grad Norm 4.3074(3.9931) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 19.7853(20.3115) | Bit/dim 3.6662(3.7025) | Xent 2.3026(2.3026) | Loss 3.6662(3.7025) | Error 0.8911(0.8985) Steps 796(791.61) | Grad Norm 4.1313(3.8300) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 20.1904(20.3319) | Bit/dim 3.6832(3.7019) | Xent 2.3026(2.3026) | Loss 3.6832(3.7019) | Error 0.9078(0.9000) Steps 808(792.40) | Grad Norm 2.7134(3.6777) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 20.4968(20.2944) | Bit/dim 3.7137(3.7023) | Xent 2.3026(2.3026) | Loss 3.7137(3.7023) | Error 0.9100(0.9016) Steps 796(791.88) | Grad Norm 3.2106(3.5718) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 20.3075(20.3084) | Bit/dim 3.6707(3.6994) | Xent 2.3026(2.3026) | Loss 3.6707(3.6994) | Error 0.9022(0.9011) Steps 778(791.58) | Grad Norm 4.0690(3.8615) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 20.3670(20.3089) | Bit/dim 3.7187(3.7007) | Xent 2.3026(2.3026) | Loss 3.7187(3.7007) | Error 0.9156(0.9001) Steps 784(791.75) | Grad Norm 6.5239(4.1114) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 96.4886, Epoch Time 1229.8040(1073.8707), Bit/dim 3.7055(best: 3.7048), Xent 2.3026, Loss 3.7055, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 20.9467(20.3464) | Bit/dim 3.7202(3.7001) | Xent 2.3026(2.3026) | Loss 3.7202(3.7001) | Error 0.9056(0.9014) Steps 802(791.65) | Grad Norm 4.4465(4.1923) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 19.6155(20.3377) | Bit/dim 3.6664(3.6955) | Xent 2.3026(2.3026) | Loss 3.6664(3.6955) | Error 0.8911(0.8998) Steps 772(789.47) | Grad Norm 3.5259(3.8428) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 20.4627(20.3716) | Bit/dim 3.6871(3.6941) | Xent 2.3026(2.3026) | Loss 3.6871(3.6941) | Error 0.9000(0.8998) Steps 760(787.18) | Grad Norm 1.7217(3.7457) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 20.6056(20.3443) | Bit/dim 3.7248(3.6973) | Xent 2.3026(2.3026) | Loss 3.7248(3.6973) | Error 0.9089(0.9002) Steps 784(786.48) | Grad Norm 6.2166(4.1385) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 19.8372(20.2567) | Bit/dim 3.6926(3.6957) | Xent 2.3026(2.3026) | Loss 3.6926(3.6957) | Error 0.8922(0.8997) Steps 802(787.33) | Grad Norm 6.8454(4.3881) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 96.0911, Epoch Time 1231.0177(1078.5851), Bit/dim 3.6878(best: 3.7048), Xent 2.3026, Loss 3.6878, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 20.4102(20.2090) | Bit/dim 3.7122(3.6955) | Xent 2.3026(2.3026) | Loss 3.7122(3.6955) | Error 0.9089(0.9005) Steps 784(787.20) | Grad Norm 2.1224(4.0095) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 19.6400(20.2021) | Bit/dim 3.7094(3.6920) | Xent 2.3026(2.3026) | Loss 3.7094(3.6920) | Error 0.8844(0.8995) Steps 808(788.36) | Grad Norm 7.1342(4.0701) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 20.0765(20.1475) | Bit/dim 3.6738(3.6910) | Xent 2.3026(2.3026) | Loss 3.6738(3.6910) | Error 0.9033(0.8998) Steps 784(787.25) | Grad Norm 4.3715(4.1307) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 20.3472(20.1249) | Bit/dim 3.6680(3.6874) | Xent 2.3026(2.3026) | Loss 3.6680(3.6874) | Error 0.8856(0.8992) Steps 784(787.65) | Grad Norm 2.5867(3.6240) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 20.0694(20.0604) | Bit/dim 3.6735(3.6848) | Xent 2.3026(2.3026) | Loss 3.6735(3.6848) | Error 0.8978(0.9003) Steps 790(787.24) | Grad Norm 4.3451(3.6523) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 20.1082(20.0646) | Bit/dim 3.6644(3.6833) | Xent 2.3026(2.3026) | Loss 3.6644(3.6833) | Error 0.8967(0.8997) Steps 784(788.75) | Grad Norm 2.7211(3.8163) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 96.2313, Epoch Time 1214.6535(1082.6671), Bit/dim 3.6781(best: 3.6878), Xent 2.3026, Loss 3.6781, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 20.0551(20.0830) | Bit/dim 3.6882(3.6811) | Xent 2.3026(2.3026) | Loss 3.6882(3.6811) | Error 0.8956(0.8997) Steps 796(789.60) | Grad Norm 5.1694(3.7863) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 20.6498(20.1594) | Bit/dim 3.7007(3.6805) | Xent 2.3026(2.3026) | Loss 3.7007(3.6805) | Error 0.8944(0.8992) Steps 778(788.46) | Grad Norm 4.2371(4.0849) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 19.8633(20.0835) | Bit/dim 3.6746(3.6803) | Xent 2.3026(2.3026) | Loss 3.6746(3.6803) | Error 0.8933(0.9002) Steps 796(788.48) | Grad Norm 3.5938(3.7698) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 19.5717(19.9493) | Bit/dim 3.7167(3.6811) | Xent 2.3026(2.3026) | Loss 3.7167(3.6811) | Error 0.9078(0.9004) Steps 790(785.83) | Grad Norm 6.7810(3.7222) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 20.6299(19.9233) | Bit/dim 3.6365(3.6819) | Xent 2.3026(2.3026) | Loss 3.6365(3.6819) | Error 0.8956(0.9000) Steps 790(785.05) | Grad Norm 2.5095(3.9061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 95.7640, Epoch Time 1213.9279(1086.6050), Bit/dim 3.6790(best: 3.6781), Xent 2.3026, Loss 3.6790, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 20.3627(19.9773) | Bit/dim 3.6737(3.6786) | Xent 2.3026(2.3026) | Loss 3.6737(3.6786) | Error 0.9056(0.8997) Steps 808(784.90) | Grad Norm 1.7561(3.8822) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 20.1433(19.9417) | Bit/dim 3.6572(3.6769) | Xent 2.3026(2.3026) | Loss 3.6572(3.6769) | Error 0.8989(0.9006) Steps 784(783.69) | Grad Norm 3.3736(3.8449) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 19.3340(19.9439) | Bit/dim 3.6748(3.6759) | Xent 2.3026(2.3026) | Loss 3.6748(3.6759) | Error 0.9011(0.9000) Steps 766(783.37) | Grad Norm 2.5134(3.5528) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 20.4728(19.9690) | Bit/dim 3.6426(3.6714) | Xent 2.3026(2.3026) | Loss 3.6426(3.6714) | Error 0.9078(0.9014) Steps 784(783.50) | Grad Norm 2.6264(3.4482) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 19.8610(19.9758) | Bit/dim 3.6758(3.6718) | Xent 2.3026(2.3026) | Loss 3.6758(3.6718) | Error 0.8900(0.9001) Steps 802(783.74) | Grad Norm 5.3608(3.7020) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 19.5504(20.0111) | Bit/dim 3.6546(3.6703) | Xent 2.3026(2.3026) | Loss 3.6546(3.6703) | Error 0.9022(0.8998) Steps 772(784.49) | Grad Norm 2.9090(3.6557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 95.7072, Epoch Time 1212.5814(1090.3842), Bit/dim 3.6661(best: 3.6781), Xent 2.3026, Loss 3.6661, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 19.9608(19.9790) | Bit/dim 3.6389(3.6661) | Xent 2.3026(2.3026) | Loss 3.6389(3.6661) | Error 0.8978(0.8998) Steps 784(784.44) | Grad Norm 5.4618(3.4727) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 20.2213(19.9898) | Bit/dim 3.6591(3.6655) | Xent 2.3026(2.3026) | Loss 3.6591(3.6655) | Error 0.8989(0.8998) Steps 796(784.61) | Grad Norm 3.3130(3.7253) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 18.9796(19.9742) | Bit/dim 3.6750(3.6676) | Xent 2.3026(2.3026) | Loss 3.6750(3.6676) | Error 0.9033(0.9007) Steps 766(783.98) | Grad Norm 4.9294(3.8239) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 20.5695(20.0797) | Bit/dim 3.6618(3.6664) | Xent 2.3026(2.3026) | Loss 3.6618(3.6664) | Error 0.9000(0.9019) Steps 772(783.09) | Grad Norm 3.0324(3.6215) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 19.3598(20.0629) | Bit/dim 3.6916(3.6667) | Xent 2.3026(2.3026) | Loss 3.6916(3.6667) | Error 0.9000(0.8999) Steps 766(784.73) | Grad Norm 2.3447(3.5716) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 96.5716, Epoch Time 1220.8195(1094.2973), Bit/dim 3.6603(best: 3.6661), Xent 2.3026, Loss 3.6603, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 20.2623(20.1280) | Bit/dim 3.6649(3.6640) | Xent 2.3026(2.3026) | Loss 3.6649(3.6640) | Error 0.8878(0.9000) Steps 796(785.31) | Grad Norm 5.0952(3.7354) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 19.5334(20.1525) | Bit/dim 3.6463(3.6627) | Xent 2.3026(2.3026) | Loss 3.6463(3.6627) | Error 0.8967(0.9010) Steps 784(787.29) | Grad Norm 5.4865(3.7862) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 20.4374(20.1048) | Bit/dim 3.6381(3.6618) | Xent 2.3026(2.3026) | Loss 3.6381(3.6618) | Error 0.9100(0.9002) Steps 802(788.02) | Grad Norm 2.7242(3.6863) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 20.5532(20.1936) | Bit/dim 3.6527(3.6602) | Xent 2.3026(2.3026) | Loss 3.6527(3.6602) | Error 0.8933(0.9001) Steps 778(789.49) | Grad Norm 3.9463(3.7725) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 20.9866(20.1935) | Bit/dim 3.6568(3.6582) | Xent 2.3026(2.3026) | Loss 3.6568(3.6582) | Error 0.9011(0.8993) Steps 778(789.31) | Grad Norm 4.5463(3.7164) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 19.7579(20.1788) | Bit/dim 3.6720(3.6580) | Xent 2.3026(2.3026) | Loss 3.6720(3.6580) | Error 0.8978(0.8994) Steps 772(790.40) | Grad Norm 2.9374(3.8141) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 97.0901, Epoch Time 1224.8203(1098.2130), Bit/dim 3.6546(best: 3.6603), Xent 2.3026, Loss 3.6546, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 19.6178(20.2167) | Bit/dim 3.6403(3.6579) | Xent 2.3026(2.3026) | Loss 3.6403(3.6579) | Error 0.9044(0.9000) Steps 790(791.23) | Grad Norm 2.4171(3.7985) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 19.9411(20.1618) | Bit/dim 3.6536(3.6569) | Xent 2.3026(2.3026) | Loss 3.6536(3.6569) | Error 0.8978(0.9000) Steps 814(791.28) | Grad Norm 3.2603(3.4410) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 20.2172(20.2233) | Bit/dim 3.6632(3.6534) | Xent 2.3026(2.3026) | Loss 3.6632(3.6534) | Error 0.9211(0.9011) Steps 802(794.20) | Grad Norm 1.7362(3.4898) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 20.2651(20.2239) | Bit/dim 3.6443(3.6529) | Xent 2.3026(2.3026) | Loss 3.6443(3.6529) | Error 0.8789(0.9015) Steps 790(793.65) | Grad Norm 4.9772(3.7413) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 20.0105(20.1906) | Bit/dim 3.6122(3.6511) | Xent 2.3026(2.3026) | Loss 3.6122(3.6511) | Error 0.8856(0.8997) Steps 784(792.36) | Grad Norm 3.0158(3.7860) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 97.8328, Epoch Time 1226.5417(1102.0629), Bit/dim 3.6536(best: 3.6546), Xent 2.3026, Loss 3.6536, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 19.4365(20.2053) | Bit/dim 3.6463(3.6513) | Xent 2.3026(2.3026) | Loss 3.6463(3.6513) | Error 0.9089(0.8991) Steps 796(792.77) | Grad Norm 2.4495(3.6220) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 18.8391(20.1389) | Bit/dim 3.6563(3.6503) | Xent 2.3026(2.3026) | Loss 3.6563(3.6503) | Error 0.9100(0.8992) Steps 784(792.52) | Grad Norm 6.7951(3.6314) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 19.8809(20.0954) | Bit/dim 3.6126(3.6460) | Xent 2.3026(2.3026) | Loss 3.6126(3.6460) | Error 0.9056(0.8982) Steps 784(791.81) | Grad Norm 2.7220(3.4304) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 20.7437(20.1859) | Bit/dim 3.6389(3.6488) | Xent 2.3026(2.3026) | Loss 3.6389(3.6488) | Error 0.9011(0.8989) Steps 790(791.46) | Grad Norm 2.5576(3.5917) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 19.9018(20.1345) | Bit/dim 3.6229(3.6453) | Xent 2.3026(2.3026) | Loss 3.6229(3.6453) | Error 0.9133(0.8993) Steps 796(790.40) | Grad Norm 2.6550(3.2655) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 19.1360(20.0853) | Bit/dim 3.6291(3.6455) | Xent 2.3026(2.3026) | Loss 3.6291(3.6455) | Error 0.9044(0.9010) Steps 772(791.64) | Grad Norm 1.6275(3.4347) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 96.7919, Epoch Time 1219.9496(1105.5995), Bit/dim 3.6384(best: 3.6536), Xent 2.3026, Loss 3.6384, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 20.4439(20.1141) | Bit/dim 3.6367(3.6441) | Xent 2.3026(2.3026) | Loss 3.6367(3.6441) | Error 0.8956(0.9009) Steps 802(790.20) | Grad Norm 2.2204(3.3565) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 20.3144(20.1377) | Bit/dim 3.6421(3.6436) | Xent 2.3026(2.3026) | Loss 3.6421(3.6436) | Error 0.8911(0.8998) Steps 802(790.35) | Grad Norm 7.2464(3.6812) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 20.3191(20.2014) | Bit/dim 3.6402(3.6455) | Xent 2.3026(2.3026) | Loss 3.6402(3.6455) | Error 0.8900(0.8995) Steps 778(788.66) | Grad Norm 4.9204(3.6493) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 19.7033(20.1187) | Bit/dim 3.6402(3.6444) | Xent 2.3026(2.3026) | Loss 3.6402(3.6444) | Error 0.8944(0.8996) Steps 772(786.75) | Grad Norm 4.8663(3.8298) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 20.0610(20.1247) | Bit/dim 3.6235(3.6413) | Xent 2.3026(2.3026) | Loss 3.6235(3.6413) | Error 0.9011(0.8996) Steps 772(786.01) | Grad Norm 4.0332(3.6794) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 95.4968, Epoch Time 1223.5418(1109.1377), Bit/dim 3.6347(best: 3.6384), Xent 2.3026, Loss 3.6347, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 19.4106(20.1050) | Bit/dim 3.6620(3.6376) | Xent 2.3026(2.3026) | Loss 3.6620(3.6376) | Error 0.9044(0.9001) Steps 778(786.27) | Grad Norm 1.6817(3.4232) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 20.3308(20.1318) | Bit/dim 3.6213(3.6370) | Xent 2.3026(2.3026) | Loss 3.6213(3.6370) | Error 0.8978(0.9002) Steps 802(788.58) | Grad Norm 3.7223(3.5529) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 20.3272(20.1414) | Bit/dim 3.6337(3.6362) | Xent 2.3026(2.3026) | Loss 3.6337(3.6362) | Error 0.9067(0.9000) Steps 802(790.12) | Grad Norm 1.9140(3.2956) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 20.2519(20.1438) | Bit/dim 3.6295(3.6329) | Xent 2.3026(2.3026) | Loss 3.6295(3.6329) | Error 0.9067(0.8987) Steps 784(788.56) | Grad Norm 4.2461(3.1204) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 19.4457(20.0105) | Bit/dim 3.6493(3.6349) | Xent 2.3026(2.3026) | Loss 3.6493(3.6349) | Error 0.8844(0.8986) Steps 784(786.69) | Grad Norm 2.4918(3.4271) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 20.3843(20.0184) | Bit/dim 3.6501(3.6358) | Xent 2.3026(2.3026) | Loss 3.6501(3.6358) | Error 0.9133(0.9010) Steps 796(787.95) | Grad Norm 5.3041(3.5402) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 96.1817, Epoch Time 1215.0643(1112.3155), Bit/dim 3.6335(best: 3.6347), Xent 2.3026, Loss 3.6335, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 20.0424(20.0113) | Bit/dim 3.6502(3.6378) | Xent 2.3026(2.3026) | Loss 3.6502(3.6378) | Error 0.9011(0.9009) Steps 808(787.99) | Grad Norm 4.2316(3.6766) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 19.5554(20.0987) | Bit/dim 3.5914(3.6331) | Xent 2.3026(2.3026) | Loss 3.5914(3.6331) | Error 0.8922(0.9010) Steps 784(788.51) | Grad Norm 3.2630(3.6331) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 19.5519(19.9981) | Bit/dim 3.6337(3.6337) | Xent 2.3026(2.3026) | Loss 3.6337(3.6337) | Error 0.9033(0.9016) Steps 778(786.23) | Grad Norm 3.0080(3.6615) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 19.5413(19.9511) | Bit/dim 3.6003(3.6310) | Xent 2.3026(2.3026) | Loss 3.6003(3.6310) | Error 0.8800(0.9003) Steps 772(785.73) | Grad Norm 2.1566(3.3586) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 20.2474(19.9985) | Bit/dim 3.5952(3.6301) | Xent 2.3026(2.3026) | Loss 3.5952(3.6301) | Error 0.8789(0.8996) Steps 790(785.80) | Grad Norm 4.5313(3.4731) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 96.8804, Epoch Time 1213.7915(1115.3598), Bit/dim 3.6288(best: 3.6335), Xent 2.3026, Loss 3.6288, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 20.3817(19.9686) | Bit/dim 3.5903(3.6269) | Xent 2.3026(2.3026) | Loss 3.5903(3.6269) | Error 0.8956(0.8993) Steps 784(786.27) | Grad Norm 2.9621(3.3873) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 20.8608(20.0091) | Bit/dim 3.6433(3.6262) | Xent 2.3026(2.3026) | Loss 3.6433(3.6262) | Error 0.9211(0.9006) Steps 784(784.58) | Grad Norm 2.1643(3.2253) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 18.9511(20.0209) | Bit/dim 3.6121(3.6259) | Xent 2.3026(2.3026) | Loss 3.6121(3.6259) | Error 0.9056(0.9008) Steps 778(785.19) | Grad Norm 4.3263(3.2826) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 20.7110(20.0863) | Bit/dim 3.6403(3.6270) | Xent 2.3026(2.3026) | Loss 3.6403(3.6270) | Error 0.9089(0.9002) Steps 778(786.23) | Grad Norm 3.7957(3.5445) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 19.9403(19.9921) | Bit/dim 3.6159(3.6280) | Xent 2.3026(2.3026) | Loss 3.6159(3.6280) | Error 0.9056(0.8998) Steps 814(786.31) | Grad Norm 4.4346(3.6051) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 20.3085(19.9856) | Bit/dim 3.6388(3.6270) | Xent 2.3026(2.3026) | Loss 3.6388(3.6270) | Error 0.9000(0.8996) Steps 784(785.67) | Grad Norm 2.7237(3.4057) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 96.3625, Epoch Time 1215.6208(1118.3676), Bit/dim 3.6231(best: 3.6288), Xent 2.3026, Loss 3.6231, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 20.1241(19.8512) | Bit/dim 3.6092(3.6282) | Xent 2.3026(2.3026) | Loss 3.6092(3.6282) | Error 0.9033(0.9002) Steps 778(785.27) | Grad Norm 3.1198(3.3260) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 20.6683(19.8941) | Bit/dim 3.6335(3.6271) | Xent 2.3026(2.3026) | Loss 3.6335(3.6271) | Error 0.8911(0.8999) Steps 778(785.13) | Grad Norm 5.2617(3.3481) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 20.4469(19.8949) | Bit/dim 3.6206(3.6262) | Xent 2.3026(2.3026) | Loss 3.6206(3.6262) | Error 0.8856(0.8991) Steps 808(785.49) | Grad Norm 4.4702(3.4492) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 19.6586(19.9117) | Bit/dim 3.5792(3.6221) | Xent 2.3026(2.3026) | Loss 3.5792(3.6221) | Error 0.8822(0.8992) Steps 784(785.25) | Grad Norm 2.8101(3.4971) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 20.2218(19.9660) | Bit/dim 3.5973(3.6182) | Xent 2.3026(2.3026) | Loss 3.5973(3.6182) | Error 0.9067(0.9001) Steps 784(784.72) | Grad Norm 5.7047(3.4382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 96.1523, Epoch Time 1208.4895(1121.0713), Bit/dim 3.6220(best: 3.6231), Xent 2.3026, Loss 3.6220, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 20.1570(20.0035) | Bit/dim 3.6565(3.6219) | Xent 2.3026(2.3026) | Loss 3.6565(3.6219) | Error 0.9100(0.8998) Steps 778(783.93) | Grad Norm 1.9478(3.3592) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 20.5118(19.9630) | Bit/dim 3.6205(3.6172) | Xent 2.3026(2.3026) | Loss 3.6205(3.6172) | Error 0.8989(0.9002) Steps 796(783.85) | Grad Norm 2.7910(3.3526) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 19.7911(19.8917) | Bit/dim 3.6681(3.6201) | Xent 2.3026(2.3026) | Loss 3.6681(3.6201) | Error 0.9167(0.9016) Steps 784(784.08) | Grad Norm 4.0105(3.2341) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 19.9410(19.8676) | Bit/dim 3.6319(3.6175) | Xent 2.3026(2.3026) | Loss 3.6319(3.6175) | Error 0.9056(0.9006) Steps 778(783.88) | Grad Norm 3.6439(3.4226) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 20.0120(19.8795) | Bit/dim 3.6302(3.6164) | Xent 2.3026(2.3026) | Loss 3.6302(3.6164) | Error 0.8989(0.8994) Steps 790(785.18) | Grad Norm 4.5671(3.5864) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 19.3002(19.8037) | Bit/dim 3.6503(3.6153) | Xent 2.3026(2.3026) | Loss 3.6503(3.6153) | Error 0.9122(0.8998) Steps 784(784.14) | Grad Norm 5.1181(3.5999) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 97.3919, Epoch Time 1204.6372(1123.5783), Bit/dim 3.6174(best: 3.6220), Xent 2.3026, Loss 3.6174, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 19.2675(19.7863) | Bit/dim 3.6335(3.6161) | Xent 2.3026(2.3026) | Loss 3.6335(3.6161) | Error 0.9089(0.9008) Steps 778(784.00) | Grad Norm 3.0019(3.4998) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 18.9567(19.7506) | Bit/dim 3.6281(3.6168) | Xent 2.3026(2.3026) | Loss 3.6281(3.6168) | Error 0.9033(0.9008) Steps 772(781.46) | Grad Norm 3.6478(3.3149) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 20.0904(19.7621) | Bit/dim 3.5983(3.6142) | Xent 2.3026(2.3026) | Loss 3.5983(3.6142) | Error 0.8956(0.9006) Steps 772(781.67) | Grad Norm 2.1006(3.3908) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 18.5598(19.7788) | Bit/dim 3.6009(3.6099) | Xent 2.3026(2.3026) | Loss 3.6009(3.6099) | Error 0.8978(0.8991) Steps 766(781.71) | Grad Norm 3.7918(3.5307) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 20.9814(19.8185) | Bit/dim 3.6279(3.6108) | Xent 2.3026(2.3026) | Loss 3.6279(3.6108) | Error 0.8989(0.9001) Steps 784(781.74) | Grad Norm 2.9630(3.4813) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 97.1405, Epoch Time 1200.0854(1125.8735), Bit/dim 3.6082(best: 3.6174), Xent 2.3026, Loss 3.6082, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 19.8763(19.7606) | Bit/dim 3.6279(3.6108) | Xent 2.3026(2.3026) | Loss 3.6279(3.6108) | Error 0.9089(0.9002) Steps 778(781.94) | Grad Norm 2.4937(3.3216) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 19.7027(19.7969) | Bit/dim 3.5744(3.6083) | Xent 2.3026(2.3026) | Loss 3.5744(3.6083) | Error 0.8878(0.9003) Steps 784(782.03) | Grad Norm 1.7316(3.0586) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 18.7436(19.8159) | Bit/dim 3.6107(3.6104) | Xent 2.3026(2.3026) | Loss 3.6107(3.6104) | Error 0.9033(0.8993) Steps 778(782.74) | Grad Norm 2.7697(3.2334) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 19.5467(19.7639) | Bit/dim 3.6047(3.6095) | Xent 2.3026(2.3026) | Loss 3.6047(3.6095) | Error 0.8878(0.8997) Steps 772(782.43) | Grad Norm 3.8882(3.1921) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 20.0749(19.8215) | Bit/dim 3.5998(3.6078) | Xent 2.3026(2.3026) | Loss 3.5998(3.6078) | Error 0.9011(0.9006) Steps 772(782.20) | Grad Norm 2.5461(3.2535) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 20.4611(19.8112) | Bit/dim 3.6191(3.6076) | Xent 2.3026(2.3026) | Loss 3.6191(3.6076) | Error 0.8967(0.8993) Steps 796(781.63) | Grad Norm 4.1106(3.5194) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 97.1955, Epoch Time 1206.5090(1128.2925), Bit/dim 3.6125(best: 3.6082), Xent 2.3026, Loss 3.6125, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 18.6341(19.6839) | Bit/dim 3.6212(3.6093) | Xent 2.3026(2.3026) | Loss 3.6212(3.6093) | Error 0.9078(0.9011) Steps 778(779.69) | Grad Norm 1.8587(3.2554) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 19.3128(19.7030) | Bit/dim 3.6012(3.6092) | Xent 2.3026(2.3026) | Loss 3.6012(3.6092) | Error 0.8722(0.8999) Steps 772(779.01) | Grad Norm 4.4597(3.3225) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 20.4780(19.7098) | Bit/dim 3.5794(3.6059) | Xent 2.3026(2.3026) | Loss 3.5794(3.6059) | Error 0.8778(0.9002) Steps 784(779.23) | Grad Norm 2.9478(3.2831) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 20.0204(19.6741) | Bit/dim 3.5988(3.6044) | Xent 2.3026(2.3026) | Loss 3.5988(3.6044) | Error 0.8833(0.8987) Steps 778(780.08) | Grad Norm 2.2694(3.1062) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 19.9368(19.7093) | Bit/dim 3.5988(3.6033) | Xent 2.3026(2.3026) | Loss 3.5988(3.6033) | Error 0.9156(0.8995) Steps 772(780.44) | Grad Norm 2.2709(3.2652) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 97.0049, Epoch Time 1192.9652(1130.2327), Bit/dim 3.6047(best: 3.6082), Xent 2.3026, Loss 3.6047, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 19.6799(19.6289) | Bit/dim 3.6026(3.6030) | Xent 2.3026(2.3026) | Loss 3.6026(3.6030) | Error 0.8989(0.8990) Steps 784(779.50) | Grad Norm 2.1298(3.3289) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 19.4505(19.6333) | Bit/dim 3.6190(3.6019) | Xent 2.3026(2.3026) | Loss 3.6190(3.6019) | Error 0.9067(0.8996) Steps 772(778.71) | Grad Norm 5.1997(3.3863) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 20.2419(19.6597) | Bit/dim 3.5597(3.6011) | Xent 2.3026(2.3026) | Loss 3.5597(3.6011) | Error 0.8878(0.8983) Steps 778(779.16) | Grad Norm 1.8957(3.3228) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 19.8175(19.6437) | Bit/dim 3.5971(3.5999) | Xent 2.3026(2.3026) | Loss 3.5971(3.5999) | Error 0.8911(0.8987) Steps 778(778.99) | Grad Norm 2.8032(3.3335) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 19.7009(19.6023) | Bit/dim 3.5894(3.5982) | Xent 2.3026(2.3026) | Loss 3.5894(3.5982) | Error 0.9100(0.8983) Steps 784(780.50) | Grad Norm 1.2786(3.0846) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 19.8495(19.5846) | Bit/dim 3.6183(3.5990) | Xent 2.3026(2.3026) | Loss 3.6183(3.5990) | Error 0.9122(0.9007) Steps 796(780.74) | Grad Norm 5.3135(3.2312) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 97.4890, Epoch Time 1193.2947(1132.1246), Bit/dim 3.5990(best: 3.6047), Xent 2.3026, Loss 3.5990, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 20.4599(19.5623) | Bit/dim 3.6124(3.6018) | Xent 2.3026(2.3026) | Loss 3.6124(3.6018) | Error 0.8967(0.9010) Steps 784(781.83) | Grad Norm 3.7802(3.2594) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 19.9354(19.5543) | Bit/dim 3.5478(3.5983) | Xent 2.3026(2.3026) | Loss 3.5478(3.5983) | Error 0.9056(0.9017) Steps 778(781.34) | Grad Norm 2.4428(3.1722) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 19.4822(19.5494) | Bit/dim 3.5976(3.5992) | Xent 2.3026(2.3026) | Loss 3.5976(3.5992) | Error 0.8922(0.9008) Steps 784(779.89) | Grad Norm 4.8725(3.3648) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 19.5736(19.5219) | Bit/dim 3.5609(3.5961) | Xent 2.3026(2.3026) | Loss 3.5609(3.5961) | Error 0.9122(0.9008) Steps 784(780.40) | Grad Norm 4.3532(3.3472) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 19.2755(19.6091) | Bit/dim 3.6185(3.5973) | Xent 2.3026(2.3026) | Loss 3.6185(3.5973) | Error 0.8978(0.9001) Steps 790(780.57) | Grad Norm 3.3600(3.1293) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 96.9454, Epoch Time 1192.2642(1133.9288), Bit/dim 3.5962(best: 3.5990), Xent 2.3026, Loss 3.5962, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.9736(19.6477) | Bit/dim 3.5959(3.5950) | Xent 2.3026(2.3026) | Loss 3.5959(3.5950) | Error 0.9167(0.9002) Steps 784(781.79) | Grad Norm 3.3032(3.2344) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 20.0376(19.5992) | Bit/dim 3.6163(3.5956) | Xent 2.3026(2.3026) | Loss 3.6163(3.5956) | Error 0.9200(0.9013) Steps 766(780.94) | Grad Norm 5.7224(3.2561) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 19.8810(19.6079) | Bit/dim 3.6046(3.5925) | Xent 2.3026(2.3026) | Loss 3.6046(3.5925) | Error 0.8900(0.8991) Steps 766(780.39) | Grad Norm 2.7134(3.1382) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 19.8329(19.6406) | Bit/dim 3.6060(3.5903) | Xent 2.3026(2.3026) | Loss 3.6060(3.5903) | Error 0.9133(0.8991) Steps 802(780.46) | Grad Norm 2.5782(3.1894) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 20.3955(19.7270) | Bit/dim 3.6318(3.5894) | Xent 2.3026(2.3026) | Loss 3.6318(3.5894) | Error 0.9178(0.8990) Steps 784(781.00) | Grad Norm 2.2142(3.1443) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 19.5325(19.6662) | Bit/dim 3.5863(3.5901) | Xent 2.3026(2.3026) | Loss 3.5863(3.5901) | Error 0.8978(0.8999) Steps 772(779.72) | Grad Norm 4.1935(3.1591) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 97.1419, Epoch Time 1196.6360(1135.8100), Bit/dim 3.5902(best: 3.5962), Xent 2.3026, Loss 3.5902, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 19.1336(19.5966) | Bit/dim 3.5944(3.5918) | Xent 2.3026(2.3026) | Loss 3.5944(3.5918) | Error 0.9100(0.8998) Steps 796(780.47) | Grad Norm 3.9261(3.2171) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 19.7874(19.6007) | Bit/dim 3.5828(3.5911) | Xent 2.3026(2.3026) | Loss 3.5828(3.5911) | Error 0.9000(0.9002) Steps 778(780.40) | Grad Norm 4.7322(3.3994) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 19.1167(19.5722) | Bit/dim 3.5979(3.5932) | Xent 2.3026(2.3026) | Loss 3.5979(3.5932) | Error 0.9089(0.9003) Steps 778(780.72) | Grad Norm 2.4078(3.2523) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 19.0628(19.5397) | Bit/dim 3.5951(3.5913) | Xent 2.3026(2.3026) | Loss 3.5951(3.5913) | Error 0.9056(0.9010) Steps 790(780.24) | Grad Norm 2.2535(3.4092) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 19.1629(19.5764) | Bit/dim 3.5866(3.5884) | Xent 2.3026(2.3026) | Loss 3.5866(3.5884) | Error 0.9044(0.8994) Steps 790(781.05) | Grad Norm 3.9899(3.3613) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 96.5137, Epoch Time 1189.2606(1137.4135), Bit/dim 3.5851(best: 3.5902), Xent 2.3026, Loss 3.5851, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 19.9613(19.5973) | Bit/dim 3.6337(3.5895) | Xent 2.3026(2.3026) | Loss 3.6337(3.5895) | Error 0.9089(0.8992) Steps 784(780.91) | Grad Norm 3.2267(3.1801) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 19.9230(19.6872) | Bit/dim 3.5412(3.5879) | Xent 2.3026(2.3026) | Loss 3.5412(3.5879) | Error 0.8878(0.8996) Steps 772(780.09) | Grad Norm 4.2145(3.1854) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 20.6326(19.8400) | Bit/dim 3.5613(3.5867) | Xent 2.3026(2.3026) | Loss 3.5613(3.5867) | Error 0.8922(0.8987) Steps 760(779.67) | Grad Norm 3.0061(3.2282) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 20.1134(19.8429) | Bit/dim 3.5924(3.5876) | Xent 2.3026(2.3026) | Loss 3.5924(3.5876) | Error 0.9022(0.9008) Steps 778(779.06) | Grad Norm 2.3642(3.1786) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 20.2981(19.8877) | Bit/dim 3.5914(3.5830) | Xent 2.3026(2.3026) | Loss 3.5914(3.5830) | Error 0.8978(0.9003) Steps 772(778.39) | Grad Norm 2.1123(2.9078) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 20.2102(19.7970) | Bit/dim 3.5765(3.5833) | Xent 2.3026(2.3026) | Loss 3.5765(3.5833) | Error 0.8778(0.8995) Steps 790(779.47) | Grad Norm 6.9147(3.0386) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 99.7490, Epoch Time 1212.3960(1139.6630), Bit/dim 3.5922(best: 3.5851), Xent 2.3026, Loss 3.5922, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 20.6362(19.8252) | Bit/dim 3.5588(3.5822) | Xent 2.3026(2.3026) | Loss 3.5588(3.5822) | Error 0.8978(0.9002) Steps 778(778.79) | Grad Norm 2.0673(2.9572) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 19.4162(19.8304) | Bit/dim 3.5477(3.5804) | Xent 2.3026(2.3026) | Loss 3.5477(3.5804) | Error 0.8944(0.9003) Steps 784(778.79) | Grad Norm 2.7004(2.8729) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 19.6853(19.8049) | Bit/dim 3.5976(3.5815) | Xent 2.3026(2.3026) | Loss 3.5976(3.5815) | Error 0.9067(0.8995) Steps 790(778.07) | Grad Norm 2.7353(3.0098) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 20.0155(19.8215) | Bit/dim 3.5751(3.5810) | Xent 2.3026(2.3026) | Loss 3.5751(3.5810) | Error 0.9133(0.9003) Steps 778(780.36) | Grad Norm 3.2036(3.0126) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 20.3479(19.8800) | Bit/dim 3.5860(3.5820) | Xent 2.3026(2.3026) | Loss 3.5860(3.5820) | Error 0.9144(0.9005) Steps 766(780.10) | Grad Norm 2.5925(3.0867) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 97.7945, Epoch Time 1209.0178(1141.7436), Bit/dim 3.5803(best: 3.5851), Xent 2.3026, Loss 3.5803, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 20.4669(19.9395) | Bit/dim 3.5656(3.5786) | Xent 2.3026(2.3026) | Loss 3.5656(3.5786) | Error 0.8889(0.8986) Steps 802(783.56) | Grad Norm 4.0662(3.1909) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 19.3724(19.8886) | Bit/dim 3.5802(3.5790) | Xent 2.3026(2.3026) | Loss 3.5802(3.5790) | Error 0.8956(0.8990) Steps 760(782.48) | Grad Norm 3.6785(3.1523) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 19.4978(19.9103) | Bit/dim 3.5665(3.5788) | Xent 2.3026(2.3026) | Loss 3.5665(3.5788) | Error 0.9133(0.8988) Steps 784(784.07) | Grad Norm 3.7206(3.0659) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 20.4801(19.9869) | Bit/dim 3.5754(3.5796) | Xent 2.3026(2.3026) | Loss 3.5754(3.5796) | Error 0.9056(0.9003) Steps 796(786.65) | Grad Norm 4.4062(3.1856) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 19.5782(19.9969) | Bit/dim 3.5527(3.5785) | Xent 2.3026(2.3026) | Loss 3.5527(3.5785) | Error 0.8800(0.9002) Steps 766(783.85) | Grad Norm 2.0508(3.0616) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 20.2578(20.0571) | Bit/dim 3.5531(3.5768) | Xent 2.3026(2.3026) | Loss 3.5531(3.5768) | Error 0.8844(0.9002) Steps 796(784.56) | Grad Norm 3.2023(3.0357) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 99.4220, Epoch Time 1221.1063(1144.1245), Bit/dim 3.5733(best: 3.5803), Xent 2.3026, Loss 3.5733, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 19.9166(20.0158) | Bit/dim 3.5563(3.5761) | Xent 2.3026(2.3026) | Loss 3.5563(3.5761) | Error 0.9056(0.8993) Steps 778(785.98) | Grad Norm 2.1615(2.9672) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 20.0663(20.0195) | Bit/dim 3.5826(3.5741) | Xent 2.3026(2.3026) | Loss 3.5826(3.5741) | Error 0.9167(0.8990) Steps 784(785.99) | Grad Norm 5.0847(3.0736) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 20.3299(20.0563) | Bit/dim 3.5595(3.5754) | Xent 2.3026(2.3026) | Loss 3.5595(3.5754) | Error 0.8889(0.8997) Steps 802(787.14) | Grad Norm 2.2612(3.0597) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 21.1628(20.1378) | Bit/dim 3.5674(3.5745) | Xent 2.3026(2.3026) | Loss 3.5674(3.5745) | Error 0.8967(0.9002) Steps 808(788.60) | Grad Norm 3.2614(3.2506) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 20.1121(20.1432) | Bit/dim 3.5826(3.5744) | Xent 2.3026(2.3026) | Loss 3.5826(3.5744) | Error 0.9067(0.9004) Steps 766(787.99) | Grad Norm 4.7406(3.3420) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 97.6716, Epoch Time 1224.2135(1146.5272), Bit/dim 3.5746(best: 3.5733), Xent 2.3026, Loss 3.5746, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 20.3098(20.1877) | Bit/dim 3.5932(3.5740) | Xent 2.3026(2.3026) | Loss 3.5932(3.5740) | Error 0.8878(0.9001) Steps 790(787.87) | Grad Norm 3.0904(3.1082) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 19.9649(20.1490) | Bit/dim 3.5250(3.5740) | Xent 2.3026(2.3026) | Loss 3.5250(3.5740) | Error 0.8889(0.8994) Steps 796(788.62) | Grad Norm 3.2394(3.3529) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 20.4639(20.1349) | Bit/dim 3.5775(3.5728) | Xent 2.3026(2.3026) | Loss 3.5775(3.5728) | Error 0.8933(0.8996) Steps 778(788.54) | Grad Norm 2.0698(3.1732) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 20.7136(20.1494) | Bit/dim 3.5677(3.5715) | Xent 2.3026(2.3026) | Loss 3.5677(3.5715) | Error 0.9011(0.8990) Steps 790(787.86) | Grad Norm 1.9822(2.8124) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 20.0602(20.1876) | Bit/dim 3.5880(3.5726) | Xent 2.3026(2.3026) | Loss 3.5880(3.5726) | Error 0.8767(0.8998) Steps 796(787.94) | Grad Norm 2.7079(2.9932) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 20.6115(20.1933) | Bit/dim 3.5686(3.5730) | Xent 2.3026(2.3026) | Loss 3.5686(3.5730) | Error 0.8956(0.9012) Steps 784(789.07) | Grad Norm 2.0954(2.8953) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 99.6914, Epoch Time 1226.5681(1148.9284), Bit/dim 3.5680(best: 3.5733), Xent 2.3026, Loss 3.5680, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 20.3505(20.2310) | Bit/dim 3.5611(3.5699) | Xent 2.3026(2.3026) | Loss 3.5611(3.5699) | Error 0.9067(0.9005) Steps 778(790.05) | Grad Norm 3.8204(2.6733) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 19.8200(20.2479) | Bit/dim 3.5806(3.5710) | Xent 2.3026(2.3026) | Loss 3.5806(3.5710) | Error 0.9044(0.9003) Steps 790(791.98) | Grad Norm 3.6871(2.9890) | Total Time 14.00(14.00)\n",
      "Iter 3440 | Time 20.4635(20.2443) | Bit/dim 3.5567(3.5687) | Xent 2.3026(2.3026) | Loss 3.5567(3.5687) | Error 0.8956(0.9001) Steps 802(791.65) | Grad Norm 2.3754(3.0382) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 20.3270(20.2217) | Bit/dim 3.5828(3.5691) | Xent 2.3026(2.3026) | Loss 3.5828(3.5691) | Error 0.9111(0.9001) Steps 784(791.99) | Grad Norm 3.1113(3.0736) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 19.5842(20.2413) | Bit/dim 3.5810(3.5693) | Xent 2.3026(2.3026) | Loss 3.5810(3.5693) | Error 0.8900(0.9009) Steps 772(791.39) | Grad Norm 3.3330(3.2775) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 99.3596, Epoch Time 1232.1249(1151.4243), Bit/dim 3.5682(best: 3.5680), Xent 2.3026, Loss 3.5682, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 20.2368(20.2129) | Bit/dim 3.5557(3.5687) | Xent 2.3026(2.3026) | Loss 3.5557(3.5687) | Error 0.8978(0.9003) Steps 790(791.26) | Grad Norm 2.7411(3.0346) | Total Time 14.00(14.00)\n",
      "Iter 3480 | Time 20.3270(20.2206) | Bit/dim 3.5533(3.5672) | Xent 2.3026(2.3026) | Loss 3.5533(3.5672) | Error 0.8822(0.9005) Steps 808(793.62) | Grad Norm 2.5367(2.8280) | Total Time 14.00(14.00)\n",
      "Iter 3490 | Time 20.8552(20.2857) | Bit/dim 3.5452(3.5668) | Xent 2.3026(2.3026) | Loss 3.5452(3.5668) | Error 0.8967(0.9003) Steps 796(793.52) | Grad Norm 1.9366(2.8851) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 20.7020(20.3355) | Bit/dim 3.5674(3.5679) | Xent 2.3026(2.3026) | Loss 3.5674(3.5679) | Error 0.9056(0.9002) Steps 826(794.12) | Grad Norm 1.7941(2.8032) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 20.7963(20.3064) | Bit/dim 3.5544(3.5666) | Xent 2.3026(2.3026) | Loss 3.5544(3.5666) | Error 0.8978(0.8997) Steps 784(794.82) | Grad Norm 5.3885(2.9658) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 20.7336(20.3167) | Bit/dim 3.5667(3.5663) | Xent 2.3026(2.3026) | Loss 3.5667(3.5663) | Error 0.9067(0.9007) Steps 766(792.81) | Grad Norm 3.1270(3.0403) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 99.6689, Epoch Time 1235.4616(1153.9454), Bit/dim 3.5624(best: 3.5680), Xent 2.3026, Loss 3.5624, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 20.0885(20.3708) | Bit/dim 3.5728(3.5649) | Xent 2.3026(2.3026) | Loss 3.5728(3.5649) | Error 0.9167(0.9004) Steps 778(793.96) | Grad Norm 1.6547(2.7885) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 20.4591(20.3813) | Bit/dim 3.5551(3.5648) | Xent 2.3026(2.3026) | Loss 3.5551(3.5648) | Error 0.8856(0.8999) Steps 778(795.94) | Grad Norm 4.0855(2.9234) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 19.7126(20.3236) | Bit/dim 3.5782(3.5640) | Xent 2.3026(2.3026) | Loss 3.5782(3.5640) | Error 0.9122(0.9003) Steps 796(795.31) | Grad Norm 2.5447(2.8274) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 19.9571(20.2889) | Bit/dim 3.5865(3.5662) | Xent 2.3026(2.3026) | Loss 3.5865(3.5662) | Error 0.9244(0.9008) Steps 784(794.73) | Grad Norm 1.6482(2.8453) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 19.7489(20.2540) | Bit/dim 3.5398(3.5619) | Xent 2.3026(2.3026) | Loss 3.5398(3.5619) | Error 0.9044(0.9003) Steps 802(793.82) | Grad Norm 3.3678(3.0457) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 99.7841, Epoch Time 1234.8859(1156.3736), Bit/dim 3.5616(best: 3.5624), Xent 2.3026, Loss 3.5616, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 19.7615(20.2655) | Bit/dim 3.5669(3.5620) | Xent 2.3026(2.3026) | Loss 3.5669(3.5620) | Error 0.9111(0.9013) Steps 772(794.26) | Grad Norm 3.4069(2.9691) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 20.2842(20.2925) | Bit/dim 3.5509(3.5613) | Xent 2.3026(2.3026) | Loss 3.5509(3.5613) | Error 0.9022(0.9013) Steps 796(791.55) | Grad Norm 2.8554(2.8941) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 19.8668(20.2774) | Bit/dim 3.5557(3.5607) | Xent 2.3026(2.3026) | Loss 3.5557(3.5607) | Error 0.8911(0.9005) Steps 790(792.95) | Grad Norm 1.9365(2.9518) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 21.1168(20.2401) | Bit/dim 3.5253(3.5565) | Xent 2.3026(2.3026) | Loss 3.5253(3.5565) | Error 0.8989(0.9000) Steps 808(793.74) | Grad Norm 2.4204(2.9015) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 20.7465(20.2222) | Bit/dim 3.5761(3.5596) | Xent 2.3026(2.3026) | Loss 3.5761(3.5596) | Error 0.8789(0.9001) Steps 796(793.42) | Grad Norm 3.6145(3.0848) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 20.7324(20.1997) | Bit/dim 3.5498(3.5603) | Xent 2.3026(2.3026) | Loss 3.5498(3.5603) | Error 0.8989(0.8995) Steps 802(794.13) | Grad Norm 3.6456(3.0798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 100.8330, Epoch Time 1229.9781(1158.5818), Bit/dim 3.5589(best: 3.5616), Xent 2.3026, Loss 3.5589, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 20.3686(20.2074) | Bit/dim 3.5498(3.5613) | Xent 2.3026(2.3026) | Loss 3.5498(3.5613) | Error 0.8967(0.8996) Steps 784(794.07) | Grad Norm 2.3645(2.9105) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 20.2502(20.2107) | Bit/dim 3.5348(3.5572) | Xent 2.3026(2.3026) | Loss 3.5348(3.5572) | Error 0.8911(0.9003) Steps 784(793.98) | Grad Norm 3.4410(2.9700) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 20.8917(20.2461) | Bit/dim 3.5270(3.5545) | Xent 2.3026(2.3026) | Loss 3.5270(3.5545) | Error 0.9022(0.8994) Steps 796(794.30) | Grad Norm 4.2670(2.9935) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 19.8750(20.2084) | Bit/dim 3.5368(3.5541) | Xent 2.3026(2.3026) | Loss 3.5368(3.5541) | Error 0.8911(0.8992) Steps 784(793.08) | Grad Norm 3.2627(2.9530) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 20.6153(20.2611) | Bit/dim 3.5752(3.5584) | Xent 2.3026(2.3026) | Loss 3.5752(3.5584) | Error 0.8944(0.9001) Steps 778(793.40) | Grad Norm 4.5493(3.1222) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 102.0760, Epoch Time 1233.8221(1160.8390), Bit/dim 3.5574(best: 3.5589), Xent 2.3026, Loss 3.5574, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 19.9782(20.2099) | Bit/dim 3.5655(3.5593) | Xent 2.3026(2.3026) | Loss 3.5655(3.5593) | Error 0.9211(0.9005) Steps 796(792.17) | Grad Norm 2.0900(3.0457) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 20.8405(20.1382) | Bit/dim 3.5210(3.5551) | Xent 2.3026(2.3026) | Loss 3.5210(3.5551) | Error 0.8811(0.8997) Steps 802(793.52) | Grad Norm 3.8896(2.8225) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 19.5046(20.0956) | Bit/dim 3.5634(3.5564) | Xent 2.3026(2.3026) | Loss 3.5634(3.5564) | Error 0.8844(0.8992) Steps 778(793.59) | Grad Norm 2.5560(2.9267) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 20.6034(20.1212) | Bit/dim 3.6017(3.5567) | Xent 2.3026(2.3026) | Loss 3.6017(3.5567) | Error 0.9067(0.8987) Steps 808(792.75) | Grad Norm 3.3516(3.0018) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 19.6690(20.0750) | Bit/dim 3.5401(3.5549) | Xent 2.3026(2.3026) | Loss 3.5401(3.5549) | Error 0.9089(0.8987) Steps 772(792.31) | Grad Norm 1.9167(2.9236) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 20.0447(20.0739) | Bit/dim 3.5412(3.5520) | Xent 2.3026(2.3026) | Loss 3.5412(3.5520) | Error 0.9011(0.9006) Steps 796(794.15) | Grad Norm 3.6415(3.0611) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 101.0137, Epoch Time 1220.1717(1162.6190), Bit/dim 3.5536(best: 3.5574), Xent 2.3026, Loss 3.5536, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 19.5956(20.0380) | Bit/dim 3.5485(3.5532) | Xent 2.3026(2.3026) | Loss 3.5485(3.5532) | Error 0.9056(0.9002) Steps 796(793.23) | Grad Norm 2.7697(3.0378) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 19.8320(20.0773) | Bit/dim 3.5601(3.5517) | Xent 2.3026(2.3026) | Loss 3.5601(3.5517) | Error 0.8822(0.8993) Steps 772(794.50) | Grad Norm 2.7781(3.1828) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 20.2520(20.0710) | Bit/dim 3.5519(3.5508) | Xent 2.3026(2.3026) | Loss 3.5519(3.5508) | Error 0.8933(0.8987) Steps 790(794.31) | Grad Norm 2.1526(2.9114) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 20.3064(20.0644) | Bit/dim 3.5399(3.5535) | Xent 2.3026(2.3026) | Loss 3.5399(3.5535) | Error 0.8944(0.8991) Steps 826(796.58) | Grad Norm 1.9999(2.9732) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 19.6856(20.1424) | Bit/dim 3.5626(3.5525) | Xent 2.3026(2.3026) | Loss 3.5626(3.5525) | Error 0.9000(0.9005) Steps 796(796.03) | Grad Norm 2.7402(2.9475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 101.9893, Epoch Time 1228.0859(1164.5830), Bit/dim 3.5482(best: 3.5536), Xent 2.3026, Loss 3.5482, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 20.3419(20.1943) | Bit/dim 3.5264(3.5497) | Xent 2.3026(2.3026) | Loss 3.5264(3.5497) | Error 0.8911(0.9010) Steps 790(794.37) | Grad Norm 1.4123(2.7683) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 19.9671(20.1563) | Bit/dim 3.5728(3.5474) | Xent 2.3026(2.3026) | Loss 3.5728(3.5474) | Error 0.9067(0.9003) Steps 808(796.26) | Grad Norm 1.6416(2.4800) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 20.1056(20.1860) | Bit/dim 3.5395(3.5463) | Xent 2.3026(2.3026) | Loss 3.5395(3.5463) | Error 0.8967(0.9001) Steps 796(796.52) | Grad Norm 1.9317(2.8075) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 20.1816(20.2134) | Bit/dim 3.5338(3.5473) | Xent 2.3026(2.3026) | Loss 3.5338(3.5473) | Error 0.9078(0.9003) Steps 772(793.71) | Grad Norm 1.5663(2.7806) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 20.0861(20.2368) | Bit/dim 3.5356(3.5469) | Xent 2.3026(2.3026) | Loss 3.5356(3.5469) | Error 0.8833(0.9003) Steps 802(793.69) | Grad Norm 2.7684(2.9005) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 20.8868(20.2426) | Bit/dim 3.5446(3.5480) | Xent 2.3026(2.3026) | Loss 3.5446(3.5480) | Error 0.9011(0.9001) Steps 826(793.83) | Grad Norm 3.3943(2.8093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 101.4934, Epoch Time 1232.1953(1166.6113), Bit/dim 3.5486(best: 3.5482), Xent 2.3026, Loss 3.5486, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 20.0995(20.2673) | Bit/dim 3.5443(3.5470) | Xent 2.3026(2.3026) | Loss 3.5443(3.5470) | Error 0.8811(0.9001) Steps 784(793.15) | Grad Norm 2.0495(2.8679) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 19.6718(20.1577) | Bit/dim 3.5265(3.5464) | Xent 2.3026(2.3026) | Loss 3.5265(3.5464) | Error 0.9011(0.8991) Steps 802(792.37) | Grad Norm 4.2505(2.8089) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 20.3000(20.1604) | Bit/dim 3.5556(3.5455) | Xent 2.3026(2.3026) | Loss 3.5556(3.5455) | Error 0.8900(0.8992) Steps 790(791.86) | Grad Norm 2.6310(2.8274) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 20.5433(20.1552) | Bit/dim 3.5344(3.5449) | Xent 2.3026(2.3026) | Loss 3.5344(3.5449) | Error 0.9067(0.8997) Steps 796(792.36) | Grad Norm 2.9856(2.7678) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 20.6452(20.1757) | Bit/dim 3.5066(3.5451) | Xent 2.3026(2.3026) | Loss 3.5066(3.5451) | Error 0.9067(0.8996) Steps 796(794.24) | Grad Norm 4.9321(2.8806) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 103.3582, Epoch Time 1230.1209(1168.5166), Bit/dim 3.5493(best: 3.5482), Xent 2.3026, Loss 3.5493, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 20.3142(20.2110) | Bit/dim 3.5251(3.5452) | Xent 2.3026(2.3026) | Loss 3.5251(3.5452) | Error 0.8956(0.9006) Steps 796(796.40) | Grad Norm 2.7527(2.9029) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 20.2945(20.1880) | Bit/dim 3.5308(3.5438) | Xent 2.3026(2.3026) | Loss 3.5308(3.5438) | Error 0.9011(0.9005) Steps 790(795.68) | Grad Norm 1.5960(2.8176) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 20.0897(20.1804) | Bit/dim 3.5331(3.5445) | Xent 2.3026(2.3026) | Loss 3.5331(3.5445) | Error 0.8900(0.9014) Steps 784(796.43) | Grad Norm 3.6660(2.8643) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 20.0198(20.1629) | Bit/dim 3.5711(3.5447) | Xent 2.3026(2.3026) | Loss 3.5711(3.5447) | Error 0.9111(0.9009) Steps 784(795.59) | Grad Norm 3.3894(2.8764) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 20.1543(20.1523) | Bit/dim 3.5090(3.5452) | Xent 2.3026(2.3026) | Loss 3.5090(3.5452) | Error 0.8989(0.8999) Steps 796(796.47) | Grad Norm 2.5374(2.8903) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 19.9599(20.1808) | Bit/dim 3.5507(3.5418) | Xent 2.3026(2.3026) | Loss 3.5507(3.5418) | Error 0.8967(0.8994) Steps 790(796.13) | Grad Norm 3.0951(2.9371) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 103.5094, Epoch Time 1230.6040(1170.3792), Bit/dim 3.5396(best: 3.5482), Xent 2.3026, Loss 3.5396, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 20.4514(20.2189) | Bit/dim 3.5352(3.5411) | Xent 2.3026(2.3026) | Loss 3.5352(3.5411) | Error 0.9033(0.8990) Steps 814(797.93) | Grad Norm 3.4786(3.0105) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 19.8038(20.1447) | Bit/dim 3.5743(3.5408) | Xent 2.3026(2.3026) | Loss 3.5743(3.5408) | Error 0.8989(0.8989) Steps 778(795.70) | Grad Norm 4.1041(2.9808) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 20.6348(20.1311) | Bit/dim 3.5095(3.5376) | Xent 2.3026(2.3026) | Loss 3.5095(3.5376) | Error 0.9000(0.8997) Steps 802(794.28) | Grad Norm 2.6617(2.8660) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 20.2427(20.1233) | Bit/dim 3.5288(3.5392) | Xent 2.3026(2.3026) | Loss 3.5288(3.5392) | Error 0.8967(0.8992) Steps 814(796.28) | Grad Norm 2.5691(2.7721) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 20.4779(20.1350) | Bit/dim 3.5476(3.5406) | Xent 2.3026(2.3026) | Loss 3.5476(3.5406) | Error 0.9133(0.9007) Steps 808(797.78) | Grad Norm 3.1400(2.8469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 100.0593, Epoch Time 1224.5287(1172.0037), Bit/dim 3.5377(best: 3.5396), Xent 2.3026, Loss 3.5377, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 20.8389(20.1674) | Bit/dim 3.5237(3.5396) | Xent 2.3026(2.3026) | Loss 3.5237(3.5396) | Error 0.8989(0.9003) Steps 826(797.98) | Grad Norm 2.8973(2.7707) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 20.2767(20.1721) | Bit/dim 3.5442(3.5419) | Xent 2.3026(2.3026) | Loss 3.5442(3.5419) | Error 0.8944(0.9001) Steps 796(798.87) | Grad Norm 3.8361(2.8174) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 20.1443(20.1732) | Bit/dim 3.5231(3.5393) | Xent 2.3026(2.3026) | Loss 3.5231(3.5393) | Error 0.9144(0.9010) Steps 808(800.38) | Grad Norm 2.4704(2.8140) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 20.4015(20.1845) | Bit/dim 3.5248(3.5374) | Xent 2.3026(2.3026) | Loss 3.5248(3.5374) | Error 0.8956(0.9008) Steps 814(801.77) | Grad Norm 3.0773(2.7637) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 20.1419(20.1567) | Bit/dim 3.5467(3.5400) | Xent 2.3026(2.3026) | Loss 3.5467(3.5400) | Error 0.9033(0.9006) Steps 814(803.20) | Grad Norm 3.7945(2.8752) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 20.0361(20.1629) | Bit/dim 3.5211(3.5369) | Xent 2.3026(2.3026) | Loss 3.5211(3.5369) | Error 0.8822(0.8995) Steps 808(803.47) | Grad Norm 1.9611(2.8237) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 102.4090, Epoch Time 1230.1346(1173.7477), Bit/dim 3.5339(best: 3.5377), Xent 2.3026, Loss 3.5339, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 20.2028(20.1892) | Bit/dim 3.5171(3.5341) | Xent 2.3026(2.3026) | Loss 3.5171(3.5341) | Error 0.8889(0.8989) Steps 784(805.02) | Grad Norm 3.3804(2.8831) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 20.0731(20.2285) | Bit/dim 3.5492(3.5355) | Xent 2.3026(2.3026) | Loss 3.5492(3.5355) | Error 0.9022(0.8994) Steps 808(803.39) | Grad Norm 3.0014(2.6958) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 20.2333(20.2053) | Bit/dim 3.5321(3.5356) | Xent 2.3026(2.3026) | Loss 3.5321(3.5356) | Error 0.8833(0.8996) Steps 796(803.59) | Grad Norm 3.1665(2.6227) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 20.2282(20.1561) | Bit/dim 3.5270(3.5367) | Xent 2.3026(2.3026) | Loss 3.5270(3.5367) | Error 0.9044(0.9000) Steps 778(801.91) | Grad Norm 1.4159(2.7494) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 19.9989(20.1546) | Bit/dim 3.5668(3.5359) | Xent 2.3026(2.3026) | Loss 3.5668(3.5359) | Error 0.9100(0.8996) Steps 814(802.81) | Grad Norm 3.3192(2.8622) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 102.4723, Epoch Time 1230.3948(1175.4471), Bit/dim 3.5338(best: 3.5339), Xent 2.3026, Loss 3.5338, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 20.2567(20.1876) | Bit/dim 3.5115(3.5362) | Xent 2.3026(2.3026) | Loss 3.5115(3.5362) | Error 0.8922(0.9003) Steps 796(802.93) | Grad Norm 2.4082(2.8237) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 19.9938(20.2405) | Bit/dim 3.5171(3.5331) | Xent 2.3026(2.3026) | Loss 3.5171(3.5331) | Error 0.8856(0.8989) Steps 814(804.26) | Grad Norm 2.9897(2.8085) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 20.8362(20.2489) | Bit/dim 3.5742(3.5353) | Xent 2.3026(2.3026) | Loss 3.5742(3.5353) | Error 0.9244(0.9007) Steps 796(805.38) | Grad Norm 2.6289(2.8328) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 20.5555(20.2764) | Bit/dim 3.5388(3.5343) | Xent 2.3026(2.3026) | Loss 3.5388(3.5343) | Error 0.8956(0.9006) Steps 820(808.02) | Grad Norm 3.5093(2.9453) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 20.7726(20.3071) | Bit/dim 3.5084(3.5331) | Xent 2.3026(2.3026) | Loss 3.5084(3.5331) | Error 0.9111(0.9007) Steps 820(808.89) | Grad Norm 1.8336(2.8395) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 20.1045(20.3215) | Bit/dim 3.5088(3.5320) | Xent 2.3026(2.3026) | Loss 3.5088(3.5320) | Error 0.8733(0.9001) Steps 796(808.15) | Grad Norm 2.9354(3.0330) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 103.3741, Epoch Time 1242.5679(1177.4607), Bit/dim 3.5350(best: 3.5338), Xent 2.3026, Loss 3.5350, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 20.7348(20.3147) | Bit/dim 3.5464(3.5327) | Xent 2.3026(2.3026) | Loss 3.5464(3.5327) | Error 0.9044(0.8997) Steps 808(808.13) | Grad Norm 3.6960(2.8487) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 20.7658(20.3411) | Bit/dim 3.5506(3.5336) | Xent 2.3026(2.3026) | Loss 3.5506(3.5336) | Error 0.8922(0.8991) Steps 778(805.90) | Grad Norm 2.8819(2.9572) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 20.4841(20.2929) | Bit/dim 3.5186(3.5320) | Xent 2.3026(2.3026) | Loss 3.5186(3.5320) | Error 0.8978(0.8999) Steps 838(808.59) | Grad Norm 2.3581(2.8306) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 19.7995(20.2649) | Bit/dim 3.5299(3.5288) | Xent 2.3026(2.3026) | Loss 3.5299(3.5288) | Error 0.8989(0.8993) Steps 802(811.09) | Grad Norm 2.5089(2.8954) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 20.5448(20.2984) | Bit/dim 3.5533(3.5289) | Xent 2.3026(2.3026) | Loss 3.5533(3.5289) | Error 0.9144(0.9009) Steps 808(811.33) | Grad Norm 1.7401(2.9099) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 102.7501, Epoch Time 1237.3911(1179.2586), Bit/dim 3.5307(best: 3.5338), Xent 2.3026, Loss 3.5307, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 20.4718(20.2909) | Bit/dim 3.5186(3.5284) | Xent 2.3026(2.3026) | Loss 3.5186(3.5284) | Error 0.9033(0.8997) Steps 820(810.66) | Grad Norm 3.0676(2.8983) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 20.6341(20.3269) | Bit/dim 3.5504(3.5282) | Xent 2.3026(2.3026) | Loss 3.5504(3.5282) | Error 0.8867(0.8989) Steps 808(812.29) | Grad Norm 2.9790(2.7511) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 19.8173(20.3460) | Bit/dim 3.5231(3.5298) | Xent 2.3026(2.3026) | Loss 3.5231(3.5298) | Error 0.8967(0.9002) Steps 808(812.84) | Grad Norm 2.6302(2.7535) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 19.6785(20.3233) | Bit/dim 3.4884(3.5285) | Xent 2.3026(2.3026) | Loss 3.4884(3.5285) | Error 0.9000(0.8996) Steps 808(811.79) | Grad Norm 3.7894(2.7315) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 20.7257(20.3641) | Bit/dim 3.5002(3.5271) | Xent 2.3026(2.3026) | Loss 3.5002(3.5271) | Error 0.8978(0.9002) Steps 820(812.74) | Grad Norm 3.2210(2.8315) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 20.7640(20.3781) | Bit/dim 3.5162(3.5256) | Xent 2.3026(2.3026) | Loss 3.5162(3.5256) | Error 0.9000(0.9004) Steps 820(813.19) | Grad Norm 2.7142(2.6979) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 102.9279, Epoch Time 1242.4182(1181.1534), Bit/dim 3.5268(best: 3.5307), Xent 2.3026, Loss 3.5268, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 20.1426(20.3555) | Bit/dim 3.5438(3.5270) | Xent 2.3026(2.3026) | Loss 3.5438(3.5270) | Error 0.9044(0.9014) Steps 796(811.60) | Grad Norm 3.6474(2.9019) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 20.2770(20.3607) | Bit/dim 3.5083(3.5253) | Xent 2.3026(2.3026) | Loss 3.5083(3.5253) | Error 0.8967(0.9001) Steps 826(811.02) | Grad Norm 2.4739(2.7381) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 20.2009(20.3370) | Bit/dim 3.4902(3.5234) | Xent 2.3026(2.3026) | Loss 3.4902(3.5234) | Error 0.9033(0.9001) Steps 808(809.51) | Grad Norm 2.6989(2.6030) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 20.8490(20.3140) | Bit/dim 3.5010(3.5239) | Xent 2.3026(2.3026) | Loss 3.5010(3.5239) | Error 0.9022(0.8999) Steps 808(809.61) | Grad Norm 1.4873(2.5714) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 20.7320(20.3535) | Bit/dim 3.5363(3.5245) | Xent 2.3026(2.3026) | Loss 3.5363(3.5245) | Error 0.9089(0.9009) Steps 814(808.56) | Grad Norm 3.2510(2.6296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 102.6571, Epoch Time 1238.7263(1182.8806), Bit/dim 3.5274(best: 3.5268), Xent 2.3026, Loss 3.5274, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 20.0315(20.3402) | Bit/dim 3.5601(3.5243) | Xent 2.3026(2.3026) | Loss 3.5601(3.5243) | Error 0.9122(0.9004) Steps 796(810.30) | Grad Norm 1.8049(2.6015) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 20.3707(20.3018) | Bit/dim 3.5149(3.5211) | Xent 2.3026(2.3026) | Loss 3.5149(3.5211) | Error 0.8989(0.9000) Steps 820(811.58) | Grad Norm 2.9710(2.6476) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 20.7648(20.3083) | Bit/dim 3.4714(3.5227) | Xent 2.3026(2.3026) | Loss 3.4714(3.5227) | Error 0.9011(0.9009) Steps 808(811.34) | Grad Norm 2.8734(2.5588) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 20.7046(20.2789) | Bit/dim 3.4982(3.5223) | Xent 2.3026(2.3026) | Loss 3.4982(3.5223) | Error 0.8811(0.9005) Steps 808(810.72) | Grad Norm 3.0417(2.7090) | Total Time 14.00(14.00)\n",
      "Iter 4390 | Time 21.2037(20.3441) | Bit/dim 3.5173(3.5227) | Xent 2.3026(2.3026) | Loss 3.5173(3.5227) | Error 0.8967(0.8999) Steps 820(813.18) | Grad Norm 2.6434(2.6655) | Total Time 14.00(14.00)\n",
      "Iter 4400 | Time 20.1296(20.3853) | Bit/dim 3.5500(3.5236) | Xent 2.3026(2.3026) | Loss 3.5500(3.5236) | Error 0.9056(0.8995) Steps 814(813.95) | Grad Norm 1.9508(2.5785) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 101.9377, Epoch Time 1238.3675(1184.5452), Bit/dim 3.5224(best: 3.5268), Xent 2.3026, Loss 3.5224, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 19.5796(20.3717) | Bit/dim 3.5153(3.5195) | Xent 2.3026(2.3026) | Loss 3.5153(3.5195) | Error 0.9122(0.9000) Steps 820(814.08) | Grad Norm 5.2034(2.6955) | Total Time 14.00(14.00)\n",
      "Iter 4420 | Time 19.9806(20.3896) | Bit/dim 3.5184(3.5220) | Xent 2.3026(2.3026) | Loss 3.5184(3.5220) | Error 0.8989(0.9002) Steps 802(813.82) | Grad Norm 1.6404(2.6807) | Total Time 14.00(14.00)\n",
      "Iter 4430 | Time 20.7692(20.4581) | Bit/dim 3.5220(3.5215) | Xent 2.3026(2.3026) | Loss 3.5220(3.5215) | Error 0.8967(0.9004) Steps 826(814.93) | Grad Norm 2.6754(2.7675) | Total Time 14.00(14.00)\n",
      "Iter 4440 | Time 20.6779(20.5122) | Bit/dim 3.5402(3.5223) | Xent 2.3026(2.3026) | Loss 3.5402(3.5223) | Error 0.9000(0.8998) Steps 820(814.02) | Grad Norm 1.5245(2.7773) | Total Time 14.00(14.00)\n",
      "Iter 4450 | Time 20.1430(20.5339) | Bit/dim 3.5401(3.5242) | Xent 2.3026(2.3026) | Loss 3.5401(3.5242) | Error 0.9044(0.9004) Steps 814(813.94) | Grad Norm 3.0745(2.8339) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 102.7667, Epoch Time 1251.2132(1186.5452), Bit/dim 3.5195(best: 3.5224), Xent 2.3026, Loss 3.5195, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 20.5512(20.5302) | Bit/dim 3.5269(3.5208) | Xent 2.3026(2.3026) | Loss 3.5269(3.5208) | Error 0.8822(0.8986) Steps 826(815.46) | Grad Norm 3.6734(2.9110) | Total Time 14.00(14.00)\n",
      "Iter 4470 | Time 19.9961(20.4935) | Bit/dim 3.5481(3.5232) | Xent 2.3026(2.3026) | Loss 3.5481(3.5232) | Error 0.8956(0.9004) Steps 808(814.59) | Grad Norm 1.6417(2.7217) | Total Time 14.00(14.00)\n",
      "Iter 4480 | Time 20.5636(20.5374) | Bit/dim 3.4774(3.5195) | Xent 2.3026(2.3026) | Loss 3.4774(3.5195) | Error 0.8889(0.8984) Steps 826(815.05) | Grad Norm 2.2464(2.6847) | Total Time 14.00(14.00)\n",
      "Iter 4490 | Time 20.1656(20.5127) | Bit/dim 3.5355(3.5202) | Xent 2.3026(2.3026) | Loss 3.5355(3.5202) | Error 0.9067(0.9002) Steps 820(813.07) | Grad Norm 4.0117(2.8275) | Total Time 14.00(14.00)\n",
      "Iter 4500 | Time 20.5035(20.4827) | Bit/dim 3.5564(3.5215) | Xent 2.3026(2.3026) | Loss 3.5564(3.5215) | Error 0.8922(0.9011) Steps 820(813.51) | Grad Norm 2.4576(2.8113) | Total Time 14.00(14.00)\n",
      "Iter 4510 | Time 20.7823(20.5181) | Bit/dim 3.5288(3.5185) | Xent 2.3026(2.3026) | Loss 3.5288(3.5185) | Error 0.8856(0.8993) Steps 826(814.49) | Grad Norm 3.7413(2.9488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 101.6205, Epoch Time 1246.9339(1188.3569), Bit/dim 3.5205(best: 3.5195), Xent 2.3026, Loss 3.5205, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 20.7644(20.4391) | Bit/dim 3.5285(3.5194) | Xent 2.3026(2.3026) | Loss 3.5285(3.5194) | Error 0.9078(0.8996) Steps 826(813.70) | Grad Norm 2.1504(2.7524) | Total Time 14.00(14.00)\n",
      "Iter 4530 | Time 20.2678(20.4296) | Bit/dim 3.5371(3.5176) | Xent 2.3026(2.3026) | Loss 3.5371(3.5176) | Error 0.8922(0.8981) Steps 820(815.35) | Grad Norm 3.2850(2.7136) | Total Time 14.00(14.00)\n",
      "Iter 4540 | Time 20.4294(20.5112) | Bit/dim 3.5133(3.5172) | Xent 2.3026(2.3026) | Loss 3.5133(3.5172) | Error 0.9256(0.9009) Steps 820(816.01) | Grad Norm 2.5980(2.6043) | Total Time 14.00(14.00)\n",
      "Iter 4550 | Time 20.2915(20.4901) | Bit/dim 3.5234(3.5157) | Xent 2.3026(2.3026) | Loss 3.5234(3.5157) | Error 0.8922(0.8996) Steps 802(815.42) | Grad Norm 3.4975(2.6863) | Total Time 14.00(14.00)\n",
      "Iter 4560 | Time 20.2512(20.4839) | Bit/dim 3.4989(3.5156) | Xent 2.3026(2.3026) | Loss 3.4989(3.5156) | Error 0.9011(0.8998) Steps 814(814.87) | Grad Norm 2.4397(2.8109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 101.0169, Epoch Time 1243.0799(1189.9986), Bit/dim 3.5176(best: 3.5195), Xent 2.3026, Loss 3.5176, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 20.7712(20.4278) | Bit/dim 3.4901(3.5151) | Xent 2.3026(2.3026) | Loss 3.4901(3.5151) | Error 0.9033(0.9004) Steps 826(814.95) | Grad Norm 2.6495(2.8768) | Total Time 14.00(14.00)\n",
      "Iter 4580 | Time 20.1421(20.4187) | Bit/dim 3.5438(3.5143) | Xent 2.3026(2.3026) | Loss 3.5438(3.5143) | Error 0.8922(0.9006) Steps 802(814.09) | Grad Norm 2.3299(2.7675) | Total Time 14.00(14.00)\n",
      "Iter 4590 | Time 20.0931(20.4295) | Bit/dim 3.4959(3.5147) | Xent 2.3026(2.3026) | Loss 3.4959(3.5147) | Error 0.8989(0.9003) Steps 826(814.01) | Grad Norm 1.9672(2.8020) | Total Time 14.00(14.00)\n",
      "Iter 4600 | Time 19.9379(20.3813) | Bit/dim 3.5223(3.5155) | Xent 2.3026(2.3026) | Loss 3.5223(3.5155) | Error 0.9189(0.9002) Steps 826(814.73) | Grad Norm 2.4060(2.6490) | Total Time 14.00(14.00)\n",
      "Iter 4610 | Time 20.9093(20.4137) | Bit/dim 3.5086(3.5142) | Xent 2.3026(2.3026) | Loss 3.5086(3.5142) | Error 0.9000(0.8996) Steps 808(814.06) | Grad Norm 3.4120(2.7790) | Total Time 14.00(14.00)\n",
      "Iter 4620 | Time 19.9850(20.3181) | Bit/dim 3.5413(3.5133) | Xent 2.3026(2.3026) | Loss 3.5413(3.5133) | Error 0.9056(0.8999) Steps 808(815.76) | Grad Norm 3.1576(2.7870) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 102.0271, Epoch Time 1237.7889(1191.4323), Bit/dim 3.5155(best: 3.5176), Xent 2.3026, Loss 3.5155, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 19.9553(20.2980) | Bit/dim 3.5314(3.5140) | Xent 2.3026(2.3026) | Loss 3.5314(3.5140) | Error 0.9056(0.9007) Steps 814(814.51) | Grad Norm 1.5430(2.7031) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 20.4386(20.2976) | Bit/dim 3.5032(3.5130) | Xent 2.3026(2.3026) | Loss 3.5032(3.5130) | Error 0.9100(0.8998) Steps 802(813.98) | Grad Norm 3.0429(2.6416) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 20.8858(20.2732) | Bit/dim 3.4998(3.5112) | Xent 2.3026(2.3026) | Loss 3.4998(3.5112) | Error 0.8878(0.8987) Steps 832(813.71) | Grad Norm 2.9480(2.6488) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 20.0871(20.2805) | Bit/dim 3.5079(3.5123) | Xent 2.3026(2.3026) | Loss 3.5079(3.5123) | Error 0.8922(0.8997) Steps 808(814.37) | Grad Norm 3.1116(2.6849) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 20.1951(20.2888) | Bit/dim 3.5045(3.5111) | Xent 2.3026(2.3026) | Loss 3.5045(3.5111) | Error 0.9033(0.8997) Steps 820(815.78) | Grad Norm 2.0299(2.7489) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 101.6470, Epoch Time 1234.1772(1192.7146), Bit/dim 3.5135(best: 3.5155), Xent 2.3026, Loss 3.5135, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 20.3412(20.3250) | Bit/dim 3.4831(3.5121) | Xent 2.3026(2.3026) | Loss 3.4831(3.5121) | Error 0.8900(0.9005) Steps 814(815.78) | Grad Norm 1.9857(2.5525) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 20.6366(20.3904) | Bit/dim 3.4717(3.5101) | Xent 2.3026(2.3026) | Loss 3.4717(3.5101) | Error 0.9022(0.8996) Steps 808(816.88) | Grad Norm 2.1077(2.5016) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 20.2570(20.4037) | Bit/dim 3.4961(3.5097) | Xent 2.3026(2.3026) | Loss 3.4961(3.5097) | Error 0.8967(0.8996) Steps 826(817.83) | Grad Norm 1.5116(2.5523) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 20.9766(20.4518) | Bit/dim 3.4672(3.5096) | Xent 2.3026(2.3026) | Loss 3.4672(3.5096) | Error 0.9067(0.8999) Steps 802(819.26) | Grad Norm 3.3200(2.5823) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 19.9461(20.4143) | Bit/dim 3.5223(3.5116) | Xent 2.3026(2.3026) | Loss 3.5223(3.5116) | Error 0.9011(0.9006) Steps 814(817.89) | Grad Norm 2.7356(2.7151) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 20.3401(20.3752) | Bit/dim 3.5288(3.5114) | Xent 2.3026(2.3026) | Loss 3.5288(3.5114) | Error 0.8856(0.9010) Steps 820(818.26) | Grad Norm 2.3163(2.5447) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 102.5120, Epoch Time 1245.4822(1194.2977), Bit/dim 3.5095(best: 3.5135), Xent 2.3026, Loss 3.5095, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.9242(20.3619) | Bit/dim 3.4804(3.5107) | Xent 2.3026(2.3026) | Loss 3.4804(3.5107) | Error 0.8967(0.9013) Steps 820(818.87) | Grad Norm 2.2216(2.5910) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 20.6807(20.4248) | Bit/dim 3.4918(3.5100) | Xent 2.3026(2.3026) | Loss 3.4918(3.5100) | Error 0.9033(0.9005) Steps 808(817.90) | Grad Norm 2.2665(2.4910) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 19.5229(20.4061) | Bit/dim 3.5046(3.5087) | Xent 2.3026(2.3026) | Loss 3.5046(3.5087) | Error 0.9056(0.9026) Steps 820(817.49) | Grad Norm 2.2620(2.7052) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 20.5942(20.4135) | Bit/dim 3.5206(3.5099) | Xent 2.3026(2.3026) | Loss 3.5206(3.5099) | Error 0.9122(0.9016) Steps 820(817.36) | Grad Norm 2.5822(2.6980) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 20.3308(20.3826) | Bit/dim 3.5126(3.5091) | Xent 2.3026(2.3026) | Loss 3.5126(3.5091) | Error 0.8989(0.9008) Steps 814(818.80) | Grad Norm 1.8310(2.7310) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 102.9863, Epoch Time 1242.5180(1195.7443), Bit/dim 3.5074(best: 3.5095), Xent 2.3026, Loss 3.5074, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 21.2738(20.3644) | Bit/dim 3.5054(3.5060) | Xent 2.3026(2.3026) | Loss 3.5054(3.5060) | Error 0.9022(0.8988) Steps 826(818.15) | Grad Norm 2.8313(2.6832) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 20.0525(20.3497) | Bit/dim 3.5075(3.5061) | Xent 2.3026(2.3026) | Loss 3.5075(3.5061) | Error 0.9167(0.9000) Steps 808(816.97) | Grad Norm 2.1952(2.8126) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 20.4716(20.3603) | Bit/dim 3.5325(3.5110) | Xent 2.3026(2.3026) | Loss 3.5325(3.5110) | Error 0.9167(0.9004) Steps 820(816.09) | Grad Norm 2.8713(2.7046) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 20.2767(20.3327) | Bit/dim 3.4883(3.5069) | Xent 2.3026(2.3026) | Loss 3.4883(3.5069) | Error 0.8956(0.8989) Steps 820(817.65) | Grad Norm 1.9922(2.7890) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 20.0141(20.3360) | Bit/dim 3.4908(3.5070) | Xent 2.3026(2.3026) | Loss 3.4908(3.5070) | Error 0.8789(0.8989) Steps 820(817.22) | Grad Norm 3.1542(2.7423) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 20.5921(20.2996) | Bit/dim 3.5145(3.5068) | Xent 2.3026(2.3026) | Loss 3.5145(3.5068) | Error 0.9144(0.9003) Steps 820(815.53) | Grad Norm 2.4872(2.7948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 101.2696, Epoch Time 1236.2481(1196.9594), Bit/dim 3.5075(best: 3.5074), Xent 2.3026, Loss 3.5075, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 21.5207(20.3520) | Bit/dim 3.4638(3.5053) | Xent 2.3026(2.3026) | Loss 3.4638(3.5053) | Error 0.8978(0.9004) Steps 826(814.30) | Grad Norm 2.5517(2.6796) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 20.9537(20.3921) | Bit/dim 3.5003(3.5031) | Xent 2.3026(2.3026) | Loss 3.5003(3.5031) | Error 0.9022(0.8989) Steps 820(814.85) | Grad Norm 2.5191(2.7856) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 20.7660(20.4199) | Bit/dim 3.5194(3.5036) | Xent 2.3026(2.3026) | Loss 3.5194(3.5036) | Error 0.9211(0.8995) Steps 820(815.87) | Grad Norm 1.9241(2.6883) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 20.2881(20.3750) | Bit/dim 3.4938(3.5055) | Xent 2.3026(2.3026) | Loss 3.4938(3.5055) | Error 0.9000(0.9000) Steps 802(816.63) | Grad Norm 2.5042(2.5176) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 20.3172(20.3471) | Bit/dim 3.5344(3.5063) | Xent 2.3026(2.3026) | Loss 3.5344(3.5063) | Error 0.9044(0.9007) Steps 808(815.31) | Grad Norm 3.5343(2.5910) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 102.2039, Epoch Time 1242.2515(1198.3182), Bit/dim 3.5020(best: 3.5074), Xent 2.3026, Loss 3.5020, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 20.2419(20.3597) | Bit/dim 3.4826(3.5031) | Xent 2.3026(2.3026) | Loss 3.4826(3.5031) | Error 0.9156(0.9005) Steps 820(816.17) | Grad Norm 3.7872(2.6324) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 20.0660(20.3661) | Bit/dim 3.5204(3.5025) | Xent 2.3026(2.3026) | Loss 3.5204(3.5025) | Error 0.9178(0.9003) Steps 820(817.48) | Grad Norm 3.3569(2.8432) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 20.4651(20.3851) | Bit/dim 3.4909(3.5022) | Xent 2.3026(2.3026) | Loss 3.4909(3.5022) | Error 0.9022(0.9004) Steps 838(817.30) | Grad Norm 2.7772(2.7853) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 20.1150(20.3821) | Bit/dim 3.4767(3.5025) | Xent 2.3026(2.3026) | Loss 3.4767(3.5025) | Error 0.8789(0.8990) Steps 814(816.11) | Grad Norm 1.7738(2.6378) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 20.3008(20.3303) | Bit/dim 3.4719(3.5047) | Xent 2.3026(2.3026) | Loss 3.4719(3.5047) | Error 0.8844(0.9003) Steps 820(815.51) | Grad Norm 2.0586(2.3800) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 20.4513(20.2892) | Bit/dim 3.4533(3.5019) | Xent 2.3026(2.3026) | Loss 3.4533(3.5019) | Error 0.9000(0.9008) Steps 814(816.42) | Grad Norm 1.9339(2.4493) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 101.7363, Epoch Time 1236.8923(1199.4754), Bit/dim 3.5043(best: 3.5020), Xent 2.3026, Loss 3.5043, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 20.4049(20.3421) | Bit/dim 3.4851(3.5008) | Xent 2.3026(2.3026) | Loss 3.4851(3.5008) | Error 0.8967(0.8991) Steps 832(818.64) | Grad Norm 2.9422(2.5009) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 19.9535(20.3294) | Bit/dim 3.5313(3.5033) | Xent 2.3026(2.3026) | Loss 3.5313(3.5033) | Error 0.9089(0.8996) Steps 820(817.27) | Grad Norm 3.7091(2.6008) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 19.8874(20.3147) | Bit/dim 3.5138(3.5023) | Xent 2.3026(2.3026) | Loss 3.5138(3.5023) | Error 0.9033(0.8987) Steps 820(815.97) | Grad Norm 1.8618(2.5866) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 20.3357(20.2743) | Bit/dim 3.4888(3.5021) | Xent 2.3026(2.3026) | Loss 3.4888(3.5021) | Error 0.8989(0.8990) Steps 826(816.47) | Grad Norm 2.1412(2.7142) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 20.3870(20.2870) | Bit/dim 3.4853(3.5018) | Xent 2.3026(2.3026) | Loss 3.4853(3.5018) | Error 0.9100(0.9011) Steps 808(817.36) | Grad Norm 2.3723(2.8104) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 104.0171, Epoch Time 1239.1634(1200.6660), Bit/dim 3.4964(best: 3.5020), Xent 2.3026, Loss 3.4964, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 20.7403(20.2862) | Bit/dim 3.4953(3.4984) | Xent 2.3026(2.3026) | Loss 3.4953(3.4984) | Error 0.8956(0.9008) Steps 826(819.56) | Grad Norm 2.5319(2.5335) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 20.1955(20.2697) | Bit/dim 3.5353(3.4994) | Xent 2.3026(2.3026) | Loss 3.5353(3.4994) | Error 0.8867(0.9003) Steps 814(817.95) | Grad Norm 2.2886(2.4501) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 20.7027(20.2859) | Bit/dim 3.5143(3.4994) | Xent 2.3026(2.3026) | Loss 3.5143(3.4994) | Error 0.9122(0.9007) Steps 808(818.39) | Grad Norm 3.6046(2.6233) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 20.7616(20.3382) | Bit/dim 3.4713(3.4967) | Xent 2.3026(2.3026) | Loss 3.4713(3.4967) | Error 0.9189(0.9006) Steps 796(819.69) | Grad Norm 2.6553(2.5762) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 20.1405(20.2962) | Bit/dim 3.4720(3.4981) | Xent 2.3026(2.3026) | Loss 3.4720(3.4981) | Error 0.8733(0.9009) Steps 820(820.78) | Grad Norm 2.9461(2.6811) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 21.4225(20.3718) | Bit/dim 3.5053(3.4992) | Xent 2.3026(2.3026) | Loss 3.5053(3.4992) | Error 0.8967(0.9003) Steps 796(821.10) | Grad Norm 2.8085(2.7024) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 104.0018, Epoch Time 1241.1099(1201.8793), Bit/dim 3.4989(best: 3.4964), Xent 2.3026, Loss 3.4989, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.6314(20.3934) | Bit/dim 3.5025(3.4988) | Xent 2.3026(2.3026) | Loss 3.5025(3.4988) | Error 0.9044(0.9000) Steps 820(820.62) | Grad Norm 2.1040(2.6594) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 20.7880(20.4066) | Bit/dim 3.5095(3.5010) | Xent 2.3026(2.3026) | Loss 3.5095(3.5010) | Error 0.9167(0.9005) Steps 814(820.34) | Grad Norm 2.5336(2.6104) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 20.0164(20.4405) | Bit/dim 3.4940(3.4983) | Xent 2.3026(2.3026) | Loss 3.4940(3.4983) | Error 0.8878(0.8992) Steps 826(819.67) | Grad Norm 2.1856(2.6627) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 20.1862(20.4078) | Bit/dim 3.4894(3.4957) | Xent 2.3026(2.3026) | Loss 3.4894(3.4957) | Error 0.8911(0.8992) Steps 820(819.74) | Grad Norm 1.7745(2.7132) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 19.3112(20.4022) | Bit/dim 3.5107(3.4983) | Xent 2.3026(2.3026) | Loss 3.5107(3.4983) | Error 0.9056(0.9005) Steps 826(818.42) | Grad Norm 2.2359(2.6568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 101.4114, Epoch Time 1242.7523(1203.1055), Bit/dim 3.4979(best: 3.4964), Xent 2.3026, Loss 3.4979, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 19.5746(20.3712) | Bit/dim 3.5350(3.4977) | Xent 2.3026(2.3026) | Loss 3.5350(3.4977) | Error 0.9011(0.8994) Steps 820(818.55) | Grad Norm 2.1668(2.4706) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 19.8480(20.3675) | Bit/dim 3.4871(3.4985) | Xent 2.3026(2.3026) | Loss 3.4871(3.4985) | Error 0.9089(0.8997) Steps 808(816.52) | Grad Norm 3.3330(2.7343) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 20.2916(20.3686) | Bit/dim 3.4936(3.4990) | Xent 2.3026(2.3026) | Loss 3.4936(3.4990) | Error 0.8944(0.8996) Steps 820(817.08) | Grad Norm 2.3346(2.6806) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 20.8990(20.4106) | Bit/dim 3.4792(3.5022) | Xent 2.3026(2.3026) | Loss 3.4792(3.5022) | Error 0.8967(0.8998) Steps 814(816.45) | Grad Norm 2.5054(2.5868) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 19.8121(20.3874) | Bit/dim 3.4661(3.4967) | Xent 2.3026(2.3026) | Loss 3.4661(3.4967) | Error 0.9022(0.8999) Steps 790(817.14) | Grad Norm 2.7779(2.6920) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 20.3834(20.3802) | Bit/dim 3.4818(3.4937) | Xent 2.3026(2.3026) | Loss 3.4818(3.4937) | Error 0.8944(0.9006) Steps 820(818.73) | Grad Norm 2.2446(2.4828) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 103.0751, Epoch Time 1242.0467(1204.2738), Bit/dim 3.4946(best: 3.4964), Xent 2.3026, Loss 3.4946, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 19.7230(20.3267) | Bit/dim 3.4980(3.4927) | Xent 2.3026(2.3026) | Loss 3.4980(3.4927) | Error 0.9011(0.9008) Steps 814(817.70) | Grad Norm 3.5088(2.6431) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 20.4134(20.3079) | Bit/dim 3.4875(3.4942) | Xent 2.3026(2.3026) | Loss 3.4875(3.4942) | Error 0.8867(0.9005) Steps 820(816.74) | Grad Norm 1.6918(2.5383) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 19.8042(20.2451) | Bit/dim 3.5009(3.4954) | Xent 2.3026(2.3026) | Loss 3.5009(3.4954) | Error 0.9089(0.9006) Steps 814(816.03) | Grad Norm 2.7554(2.5487) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 19.7755(20.3010) | Bit/dim 3.5247(3.4940) | Xent 2.3026(2.3026) | Loss 3.5247(3.4940) | Error 0.8944(0.8995) Steps 826(816.60) | Grad Norm 2.0203(2.4863) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 20.2735(20.2404) | Bit/dim 3.5149(3.4939) | Xent 2.3026(2.3026) | Loss 3.5149(3.4939) | Error 0.9089(0.8999) Steps 820(817.30) | Grad Norm 2.1566(2.5677) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 102.7873, Epoch Time 1232.2675(1205.1136), Bit/dim 3.4957(best: 3.4946), Xent 2.3026, Loss 3.4957, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.7907(20.2731) | Bit/dim 3.5426(3.4935) | Xent 2.3026(2.3026) | Loss 3.5426(3.4935) | Error 0.9011(0.8996) Steps 802(817.39) | Grad Norm 3.2012(2.7249) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 21.2114(20.3435) | Bit/dim 3.4963(3.4943) | Xent 2.3026(2.3026) | Loss 3.4963(3.4943) | Error 0.9144(0.8999) Steps 838(819.68) | Grad Norm 1.7868(2.5135) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 20.5171(20.3320) | Bit/dim 3.4726(3.4942) | Xent 2.3026(2.3026) | Loss 3.4726(3.4942) | Error 0.8967(0.9001) Steps 832(819.14) | Grad Norm 2.0665(2.6014) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 20.0038(20.3623) | Bit/dim 3.4976(3.4927) | Xent 2.3026(2.3026) | Loss 3.4976(3.4927) | Error 0.8989(0.9001) Steps 820(821.12) | Grad Norm 3.0238(2.5282) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 20.5866(20.3634) | Bit/dim 3.5086(3.4933) | Xent 2.3026(2.3026) | Loss 3.5086(3.4933) | Error 0.8911(0.9005) Steps 820(821.07) | Grad Norm 3.0469(2.4901) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 20.3717(20.3752) | Bit/dim 3.4912(3.4908) | Xent 2.3026(2.3026) | Loss 3.4912(3.4908) | Error 0.9011(0.9000) Steps 814(821.51) | Grad Norm 2.8764(2.6313) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 103.7443, Epoch Time 1244.5271(1206.2960), Bit/dim 3.4942(best: 3.4946), Xent 2.3026, Loss 3.4942, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 19.8562(20.3351) | Bit/dim 3.4991(3.4904) | Xent 2.3026(2.3026) | Loss 3.4991(3.4904) | Error 0.9100(0.9000) Steps 826(822.17) | Grad Norm 2.3788(2.4752) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 20.9542(20.3303) | Bit/dim 3.4927(3.4894) | Xent 2.3026(2.3026) | Loss 3.4927(3.4894) | Error 0.8978(0.9013) Steps 820(820.98) | Grad Norm 2.9394(2.6834) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 21.0051(20.3304) | Bit/dim 3.4782(3.4919) | Xent 2.3026(2.3026) | Loss 3.4782(3.4919) | Error 0.8989(0.9026) Steps 820(821.39) | Grad Norm 1.2353(2.5538) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 20.6296(20.3963) | Bit/dim 3.4716(3.4914) | Xent 2.3026(2.3026) | Loss 3.4716(3.4914) | Error 0.8956(0.9007) Steps 832(822.51) | Grad Norm 2.7130(2.4234) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 20.5383(20.4141) | Bit/dim 3.4758(3.4898) | Xent 2.3026(2.3026) | Loss 3.4758(3.4898) | Error 0.8844(0.8990) Steps 826(824.06) | Grad Norm 3.1176(2.5822) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 101.0628, Epoch Time 1240.1905(1207.3128), Bit/dim 3.4926(best: 3.4942), Xent 2.3026, Loss 3.4926, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 20.3904(20.4290) | Bit/dim 3.4861(3.4896) | Xent 2.3026(2.3026) | Loss 3.4861(3.4896) | Error 0.8967(0.8993) Steps 826(824.09) | Grad Norm 2.9850(2.5404) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 20.2820(20.4207) | Bit/dim 3.4952(3.4900) | Xent 2.3026(2.3026) | Loss 3.4952(3.4900) | Error 0.9122(0.8999) Steps 832(823.16) | Grad Norm 2.8257(2.5630) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 20.3770(20.4821) | Bit/dim 3.5236(3.4913) | Xent 2.3026(2.3026) | Loss 3.5236(3.4913) | Error 0.9000(0.9011) Steps 820(823.82) | Grad Norm 1.3535(2.6137) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 20.7045(20.4382) | Bit/dim 3.4560(3.4896) | Xent 2.3026(2.3026) | Loss 3.4560(3.4896) | Error 0.8822(0.8999) Steps 838(824.29) | Grad Norm 3.2152(2.5709) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 20.3852(20.5232) | Bit/dim 3.4934(3.4902) | Xent 2.3026(2.3026) | Loss 3.4934(3.4902) | Error 0.9011(0.9000) Steps 820(826.42) | Grad Norm 2.0668(2.6522) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 20.1652(20.5897) | Bit/dim 3.4801(3.4884) | Xent 2.3026(2.3026) | Loss 3.4801(3.4884) | Error 0.9022(0.8992) Steps 832(827.84) | Grad Norm 1.9880(2.4400) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 102.4223, Epoch Time 1252.1330(1208.6574), Bit/dim 3.4886(best: 3.4926), Xent 2.3026, Loss 3.4886, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 20.7131(20.5947) | Bit/dim 3.4686(3.4880) | Xent 2.3026(2.3026) | Loss 3.4686(3.4880) | Error 0.8778(0.8988) Steps 844(829.43) | Grad Norm 4.1952(2.5054) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 21.0396(20.6490) | Bit/dim 3.4781(3.4872) | Xent 2.3026(2.3026) | Loss 3.4781(3.4872) | Error 0.8800(0.8975) Steps 820(828.92) | Grad Norm 3.0564(2.5870) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 20.3568(20.6575) | Bit/dim 3.4833(3.4879) | Xent 2.3026(2.3026) | Loss 3.4833(3.4879) | Error 0.9078(0.9000) Steps 832(828.07) | Grad Norm 2.0026(2.6505) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 20.6066(20.6937) | Bit/dim 3.4819(3.4886) | Xent 2.3026(2.3026) | Loss 3.4819(3.4886) | Error 0.8800(0.9009) Steps 844(829.69) | Grad Norm 2.8672(2.6676) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 20.0834(20.6255) | Bit/dim 3.5275(3.4865) | Xent 2.3026(2.3026) | Loss 3.5275(3.4865) | Error 0.9044(0.9008) Steps 826(830.86) | Grad Norm 1.4841(2.4578) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 103.3396, Epoch Time 1258.1337(1210.1417), Bit/dim 3.4862(best: 3.4886), Xent 2.3026, Loss 3.4862, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 20.6692(20.6374) | Bit/dim 3.4915(3.4890) | Xent 2.3026(2.3026) | Loss 3.4915(3.4890) | Error 0.9033(0.8998) Steps 832(832.10) | Grad Norm 4.9759(2.5565) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 20.3048(20.6665) | Bit/dim 3.4598(3.4863) | Xent 2.3026(2.3026) | Loss 3.4598(3.4863) | Error 0.8833(0.8977) Steps 820(831.44) | Grad Norm 1.7251(2.5079) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 19.8827(20.6424) | Bit/dim 3.4877(3.4874) | Xent 2.3026(2.3026) | Loss 3.4877(3.4874) | Error 0.8978(0.8984) Steps 826(830.65) | Grad Norm 2.6823(2.4823) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 20.7478(20.6451) | Bit/dim 3.5302(3.4854) | Xent 2.3026(2.3026) | Loss 3.5302(3.4854) | Error 0.9156(0.9004) Steps 838(831.02) | Grad Norm 2.5425(2.6968) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 20.7963(20.6687) | Bit/dim 3.5352(3.4855) | Xent 2.3026(2.3026) | Loss 3.5352(3.4855) | Error 0.9000(0.8990) Steps 844(832.86) | Grad Norm 2.3282(2.5121) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 20.4708(20.6404) | Bit/dim 3.4876(3.4863) | Xent 2.3026(2.3026) | Loss 3.4876(3.4863) | Error 0.9022(0.9010) Steps 820(831.82) | Grad Norm 2.4511(2.6427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 101.9339, Epoch Time 1256.0333(1211.5185), Bit/dim 3.4841(best: 3.4862), Xent 2.3026, Loss 3.4841, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 20.1112(20.6004) | Bit/dim 3.5085(3.4848) | Xent 2.3026(2.3026) | Loss 3.5085(3.4848) | Error 0.8944(0.9006) Steps 826(830.04) | Grad Norm 2.5277(2.4672) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 20.4064(20.6059) | Bit/dim 3.4911(3.4858) | Xent 2.3026(2.3026) | Loss 3.4911(3.4858) | Error 0.8911(0.9003) Steps 832(830.07) | Grad Norm 1.5482(2.4168) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 20.6882(20.6196) | Bit/dim 3.5001(3.4866) | Xent 2.3026(2.3026) | Loss 3.5001(3.4866) | Error 0.9100(0.9012) Steps 844(831.75) | Grad Norm 1.7626(2.3758) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 21.2591(20.6388) | Bit/dim 3.4825(3.4860) | Xent 2.3026(2.3026) | Loss 3.4825(3.4860) | Error 0.8911(0.9008) Steps 838(831.36) | Grad Norm 2.3994(2.3105) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 20.7063(20.6895) | Bit/dim 3.4934(3.4831) | Xent 2.3026(2.3026) | Loss 3.4934(3.4831) | Error 0.8967(0.8998) Steps 832(831.32) | Grad Norm 4.0606(2.4786) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 104.3530, Epoch Time 1258.5935(1212.9307), Bit/dim 3.4877(best: 3.4841), Xent 2.3026, Loss 3.4877, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 20.7995(20.7068) | Bit/dim 3.4766(3.4828) | Xent 2.3026(2.3026) | Loss 3.4766(3.4828) | Error 0.9067(0.9003) Steps 826(830.38) | Grad Norm 2.1458(2.4055) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 20.8388(20.6639) | Bit/dim 3.4760(3.4839) | Xent 2.3026(2.3026) | Loss 3.4760(3.4839) | Error 0.8889(0.9002) Steps 850(831.48) | Grad Norm 3.5473(2.4363) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 20.5063(20.7070) | Bit/dim 3.4831(3.4856) | Xent 2.3026(2.3026) | Loss 3.4831(3.4856) | Error 0.8967(0.9008) Steps 826(831.49) | Grad Norm 1.6591(2.4777) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 20.9537(20.7535) | Bit/dim 3.5004(3.4839) | Xent 2.3026(2.3026) | Loss 3.5004(3.4839) | Error 0.8989(0.9004) Steps 844(832.20) | Grad Norm 2.5528(2.6462) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 21.0386(20.7100) | Bit/dim 3.4728(3.4830) | Xent 2.3026(2.3026) | Loss 3.4728(3.4830) | Error 0.8967(0.9001) Steps 850(833.92) | Grad Norm 1.9649(2.5032) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 20.5633(20.7219) | Bit/dim 3.5029(3.4814) | Xent 2.3026(2.3026) | Loss 3.5029(3.4814) | Error 0.8989(0.8995) Steps 838(833.73) | Grad Norm 1.6596(2.3664) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 104.7626, Epoch Time 1263.9780(1214.4621), Bit/dim 3.4814(best: 3.4841), Xent 2.3026, Loss 3.4814, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 20.8302(20.7693) | Bit/dim 3.4655(3.4813) | Xent 2.3026(2.3026) | Loss 3.4655(3.4813) | Error 0.8900(0.8995) Steps 814(832.87) | Grad Norm 2.0137(2.5333) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 20.1156(20.7131) | Bit/dim 3.4559(3.4821) | Xent 2.3026(2.3026) | Loss 3.4559(3.4821) | Error 0.8933(0.8993) Steps 832(832.60) | Grad Norm 3.4108(2.6483) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 20.8785(20.6828) | Bit/dim 3.4694(3.4811) | Xent 2.3026(2.3026) | Loss 3.4694(3.4811) | Error 0.9122(0.8989) Steps 856(833.33) | Grad Norm 1.9539(2.5470) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 20.4641(20.7154) | Bit/dim 3.4977(3.4826) | Xent 2.3026(2.3026) | Loss 3.4977(3.4826) | Error 0.8889(0.8992) Steps 814(831.74) | Grad Norm 2.1536(2.5583) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 20.4079(20.6754) | Bit/dim 3.4910(3.4815) | Xent 2.3026(2.3026) | Loss 3.4910(3.4815) | Error 0.8933(0.8998) Steps 826(832.28) | Grad Norm 3.1801(2.4416) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 103.3096, Epoch Time 1258.6723(1215.7884), Bit/dim 3.4803(best: 3.4814), Xent 2.3026, Loss 3.4803, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 21.3270(20.6865) | Bit/dim 3.4894(3.4803) | Xent 2.3026(2.3026) | Loss 3.4894(3.4803) | Error 0.9178(0.9007) Steps 814(832.91) | Grad Norm 4.4089(2.4418) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 21.3883(20.7001) | Bit/dim 3.4819(3.4806) | Xent 2.3026(2.3026) | Loss 3.4819(3.4806) | Error 0.9111(0.9008) Steps 832(832.85) | Grad Norm 2.7043(2.5382) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 20.9666(20.7439) | Bit/dim 3.5041(3.4812) | Xent 2.3026(2.3026) | Loss 3.5041(3.4812) | Error 0.8967(0.9004) Steps 844(833.82) | Grad Norm 2.6935(2.5553) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 21.2224(20.7557) | Bit/dim 3.5063(3.4833) | Xent 2.3026(2.3026) | Loss 3.5063(3.4833) | Error 0.9211(0.9006) Steps 832(831.82) | Grad Norm 2.5206(2.5745) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 21.6626(20.8035) | Bit/dim 3.4737(3.4830) | Xent 2.3026(2.3026) | Loss 3.4737(3.4830) | Error 0.8989(0.9002) Steps 844(832.36) | Grad Norm 3.4438(2.5982) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 21.3272(20.8364) | Bit/dim 3.4701(3.4794) | Xent 2.3026(2.3026) | Loss 3.4701(3.4794) | Error 0.8944(0.8997) Steps 838(832.23) | Grad Norm 2.8215(2.6211) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 104.3783, Epoch Time 1268.1799(1217.3602), Bit/dim 3.4807(best: 3.4803), Xent 2.3026, Loss 3.4807, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 20.8676(20.7693) | Bit/dim 3.4804(3.4794) | Xent 2.3026(2.3026) | Loss 3.4804(3.4794) | Error 0.9044(0.9001) Steps 814(832.38) | Grad Norm 3.3226(2.6332) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 20.7178(20.7288) | Bit/dim 3.4956(3.4804) | Xent 2.3026(2.3026) | Loss 3.4956(3.4804) | Error 0.9111(0.9010) Steps 838(832.04) | Grad Norm 1.2528(2.5674) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 21.0855(20.7716) | Bit/dim 3.4639(3.4799) | Xent 2.3026(2.3026) | Loss 3.4639(3.4799) | Error 0.9000(0.9005) Steps 826(832.72) | Grad Norm 2.3561(2.5039) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 20.7396(20.7537) | Bit/dim 3.4330(3.4775) | Xent 2.3026(2.3026) | Loss 3.4330(3.4775) | Error 0.8822(0.8987) Steps 826(834.15) | Grad Norm 2.9810(2.6003) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 21.4054(20.8452) | Bit/dim 3.4609(3.4760) | Xent 2.3026(2.3026) | Loss 3.4609(3.4760) | Error 0.8911(0.8990) Steps 850(833.81) | Grad Norm 2.2683(2.6152) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 104.1949, Epoch Time 1265.5349(1218.8054), Bit/dim 3.4768(best: 3.4803), Xent 2.3026, Loss 3.4768, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 20.8533(20.8291) | Bit/dim 3.4969(3.4776) | Xent 2.3026(2.3026) | Loss 3.4969(3.4776) | Error 0.8989(0.8989) Steps 826(833.85) | Grad Norm 2.3646(2.4349) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 21.1344(20.8402) | Bit/dim 3.4845(3.4787) | Xent 2.3026(2.3026) | Loss 3.4845(3.4787) | Error 0.9067(0.8989) Steps 820(833.20) | Grad Norm 4.1926(2.5834) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 20.9419(20.8064) | Bit/dim 3.4820(3.4776) | Xent 2.3026(2.3026) | Loss 3.4820(3.4776) | Error 0.8989(0.8987) Steps 838(834.29) | Grad Norm 1.6063(2.5442) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 20.9165(20.7984) | Bit/dim 3.4651(3.4743) | Xent 2.3026(2.3026) | Loss 3.4651(3.4743) | Error 0.8844(0.8989) Steps 820(832.79) | Grad Norm 2.8941(2.3707) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 20.1796(20.7849) | Bit/dim 3.4827(3.4769) | Xent 2.3026(2.3026) | Loss 3.4827(3.4769) | Error 0.8944(0.8997) Steps 820(832.01) | Grad Norm 1.7578(2.4189) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 20.4487(20.7233) | Bit/dim 3.5035(3.4768) | Xent 2.3026(2.3026) | Loss 3.5035(3.4768) | Error 0.9078(0.9001) Steps 820(831.63) | Grad Norm 1.7314(2.4158) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 103.2980, Epoch Time 1261.2810(1220.0797), Bit/dim 3.4806(best: 3.4768), Xent 2.3026, Loss 3.4806, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 20.9492(20.6923) | Bit/dim 3.4586(3.4751) | Xent 2.3026(2.3026) | Loss 3.4586(3.4751) | Error 0.8889(0.8996) Steps 820(833.15) | Grad Norm 2.8789(2.4134) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 20.1624(20.6579) | Bit/dim 3.4588(3.4762) | Xent 2.3026(2.3026) | Loss 3.4588(3.4762) | Error 0.8900(0.8980) Steps 850(835.56) | Grad Norm 1.8080(2.4912) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 20.5997(20.6673) | Bit/dim 3.4617(3.4752) | Xent 2.3026(2.3026) | Loss 3.4617(3.4752) | Error 0.9144(0.8987) Steps 826(835.99) | Grad Norm 3.0964(2.4757) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 20.6422(20.6680) | Bit/dim 3.4848(3.4784) | Xent 2.3026(2.3026) | Loss 3.4848(3.4784) | Error 0.8789(0.8998) Steps 850(835.60) | Grad Norm 2.4323(2.6162) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 21.1310(20.6967) | Bit/dim 3.4940(3.4773) | Xent 2.3026(2.3026) | Loss 3.4940(3.4773) | Error 0.8978(0.8998) Steps 850(836.31) | Grad Norm 2.7553(2.4213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 102.3784, Epoch Time 1256.1223(1221.1610), Bit/dim 3.4789(best: 3.4768), Xent 2.3026, Loss 3.4789, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 21.4522(20.7506) | Bit/dim 3.4535(3.4767) | Xent 2.3026(2.3026) | Loss 3.4535(3.4767) | Error 0.8900(0.9011) Steps 862(836.71) | Grad Norm 2.0477(2.4128) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 20.7658(20.7632) | Bit/dim 3.4752(3.4757) | Xent 2.3026(2.3026) | Loss 3.4752(3.4757) | Error 0.8922(0.9019) Steps 838(836.66) | Grad Norm 2.0197(2.3436) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 20.0859(20.7572) | Bit/dim 3.4319(3.4724) | Xent 2.3026(2.3026) | Loss 3.4319(3.4724) | Error 0.8978(0.9010) Steps 838(836.85) | Grad Norm 2.6259(2.4789) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 20.7436(20.7983) | Bit/dim 3.4868(3.4754) | Xent 2.3026(2.3026) | Loss 3.4868(3.4754) | Error 0.8867(0.9004) Steps 820(838.16) | Grad Norm 2.3910(2.5617) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 20.2192(20.7394) | Bit/dim 3.4555(3.4737) | Xent 2.3026(2.3026) | Loss 3.4555(3.4737) | Error 0.8889(0.8995) Steps 838(838.10) | Grad Norm 3.4041(2.5077) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 21.1417(20.7224) | Bit/dim 3.4831(3.4747) | Xent 2.3026(2.3026) | Loss 3.4831(3.4747) | Error 0.8933(0.8994) Steps 838(838.06) | Grad Norm 2.2954(2.5359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 104.4256, Epoch Time 1266.4777(1222.5205), Bit/dim 3.4734(best: 3.4768), Xent 2.3026, Loss 3.4734, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 20.7176(20.6948) | Bit/dim 3.4669(3.4744) | Xent 2.3026(2.3026) | Loss 3.4669(3.4744) | Error 0.8967(0.8995) Steps 838(836.67) | Grad Norm 3.4071(2.4559) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 20.4487(20.7317) | Bit/dim 3.4738(3.4757) | Xent 2.3026(2.3026) | Loss 3.4738(3.4757) | Error 0.9089(0.8995) Steps 844(836.24) | Grad Norm 2.4248(2.5145) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 20.0616(20.7620) | Bit/dim 3.4265(3.4736) | Xent 2.3026(2.3026) | Loss 3.4265(3.4736) | Error 0.8856(0.8991) Steps 832(837.69) | Grad Norm 3.4117(2.5316) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 20.3009(20.7364) | Bit/dim 3.4768(3.4731) | Xent 2.3026(2.3026) | Loss 3.4768(3.4731) | Error 0.9056(0.8996) Steps 832(837.03) | Grad Norm 2.0184(2.4798) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 21.1409(20.7403) | Bit/dim 3.4556(3.4712) | Xent 2.3026(2.3026) | Loss 3.4556(3.4712) | Error 0.9000(0.8996) Steps 832(835.89) | Grad Norm 4.6222(2.4685) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 103.4816, Epoch Time 1262.1429(1223.7091), Bit/dim 3.4778(best: 3.4734), Xent 2.3026, Loss 3.4778, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 21.1198(20.7361) | Bit/dim 3.4727(3.4733) | Xent 2.3026(2.3026) | Loss 3.4727(3.4733) | Error 0.9100(0.8993) Steps 838(837.34) | Grad Norm 2.5710(2.5495) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 19.7913(20.6506) | Bit/dim 3.4721(3.4740) | Xent 2.3026(2.3026) | Loss 3.4721(3.4740) | Error 0.9100(0.8981) Steps 832(836.46) | Grad Norm 2.0644(2.4256) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 20.0987(20.6334) | Bit/dim 3.4829(3.4752) | Xent 2.3026(2.3026) | Loss 3.4829(3.4752) | Error 0.8933(0.8998) Steps 826(836.61) | Grad Norm 1.6185(2.4474) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 21.0877(20.6305) | Bit/dim 3.4901(3.4735) | Xent 2.3026(2.3026) | Loss 3.4901(3.4735) | Error 0.9022(0.9002) Steps 832(837.91) | Grad Norm 2.3524(2.3270) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 20.9276(20.6721) | Bit/dim 3.4847(3.4725) | Xent 2.3026(2.3026) | Loss 3.4847(3.4725) | Error 0.9011(0.9010) Steps 844(837.65) | Grad Norm 3.3024(2.4314) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 20.2299(20.6911) | Bit/dim 3.4667(3.4696) | Xent 2.3026(2.3026) | Loss 3.4667(3.4696) | Error 0.8867(0.9007) Steps 820(837.75) | Grad Norm 2.7286(2.3417) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 103.5590, Epoch Time 1257.0831(1224.7104), Bit/dim 3.4743(best: 3.4734), Xent 2.3026, Loss 3.4743, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 20.5125(20.6528) | Bit/dim 3.4745(3.4695) | Xent 2.3026(2.3026) | Loss 3.4745(3.4695) | Error 0.8922(0.9007) Steps 832(836.84) | Grad Norm 1.5626(2.2650) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 21.0082(20.7114) | Bit/dim 3.5022(3.4711) | Xent 2.3026(2.3026) | Loss 3.5022(3.4711) | Error 0.9067(0.8995) Steps 826(836.44) | Grad Norm 2.4140(2.4697) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 20.2419(20.7530) | Bit/dim 3.4457(3.4724) | Xent 2.3026(2.3026) | Loss 3.4457(3.4724) | Error 0.9078(0.9005) Steps 832(836.67) | Grad Norm 2.2045(2.3254) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 20.5363(20.8398) | Bit/dim 3.4611(3.4698) | Xent 2.3026(2.3026) | Loss 3.4611(3.4698) | Error 0.9133(0.9006) Steps 850(837.61) | Grad Norm 2.7269(2.4175) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 20.6087(20.8034) | Bit/dim 3.4928(3.4694) | Xent 2.3026(2.3026) | Loss 3.4928(3.4694) | Error 0.9200(0.9000) Steps 838(837.90) | Grad Norm 3.5122(2.4849) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 105.3232, Epoch Time 1268.6364(1226.0281), Bit/dim 3.4759(best: 3.4734), Xent 2.3026, Loss 3.4759, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 20.7364(20.8329) | Bit/dim 3.4649(3.4683) | Xent 2.3026(2.3026) | Loss 3.4649(3.4683) | Error 0.8978(0.8998) Steps 820(838.27) | Grad Norm 2.6394(2.5908) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 21.4582(20.9247) | Bit/dim 3.4483(3.4668) | Xent 2.3026(2.3026) | Loss 3.4483(3.4668) | Error 0.8978(0.8995) Steps 826(838.54) | Grad Norm 3.1524(2.4646) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 20.6323(20.9440) | Bit/dim 3.4913(3.4690) | Xent 2.3026(2.3026) | Loss 3.4913(3.4690) | Error 0.9011(0.8996) Steps 838(839.51) | Grad Norm 2.7323(2.5465) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 21.5905(20.9378) | Bit/dim 3.4773(3.4695) | Xent 2.3026(2.3026) | Loss 3.4773(3.4695) | Error 0.8933(0.8995) Steps 862(838.60) | Grad Norm 3.2858(2.6432) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 20.8811(20.9705) | Bit/dim 3.4701(3.4691) | Xent 2.3026(2.3026) | Loss 3.4701(3.4691) | Error 0.9056(0.8998) Steps 850(839.59) | Grad Norm 1.2839(2.4044) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 21.1474(20.9535) | Bit/dim 3.4452(3.4687) | Xent 2.3026(2.3026) | Loss 3.4452(3.4687) | Error 0.9267(0.9007) Steps 838(840.24) | Grad Norm 2.0055(2.2542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 103.9124, Epoch Time 1277.9602(1227.5861), Bit/dim 3.4691(best: 3.4734), Xent 2.3026, Loss 3.4691, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 20.7297(20.9516) | Bit/dim 3.4435(3.4674) | Xent 2.3026(2.3026) | Loss 3.4435(3.4674) | Error 0.8922(0.9003) Steps 850(841.17) | Grad Norm 2.3398(2.2361) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 21.1392(20.8865) | Bit/dim 3.4890(3.4682) | Xent 2.3026(2.3026) | Loss 3.4890(3.4682) | Error 0.9144(0.8995) Steps 862(842.55) | Grad Norm 2.3202(2.4683) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 20.8866(20.8796) | Bit/dim 3.4401(3.4662) | Xent 2.3026(2.3026) | Loss 3.4401(3.4662) | Error 0.9078(0.9004) Steps 850(842.23) | Grad Norm 2.2081(2.3254) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 21.0700(20.8445) | Bit/dim 3.4879(3.4689) | Xent 2.3026(2.3026) | Loss 3.4879(3.4689) | Error 0.9011(0.9012) Steps 844(842.24) | Grad Norm 3.0792(2.3970) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 20.1040(20.8528) | Bit/dim 3.4726(3.4708) | Xent 2.3026(2.3026) | Loss 3.4726(3.4708) | Error 0.8989(0.9006) Steps 832(842.65) | Grad Norm 2.7697(2.4670) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 105.2563, Epoch Time 1269.1063(1228.8317), Bit/dim 3.4733(best: 3.4691), Xent 2.3026, Loss 3.4733, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 20.2100(20.8214) | Bit/dim 3.4687(3.4703) | Xent 2.3026(2.3026) | Loss 3.4687(3.4703) | Error 0.9056(0.9007) Steps 838(841.73) | Grad Norm 2.8717(2.5141) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 20.6524(20.8192) | Bit/dim 3.4350(3.4688) | Xent 2.3026(2.3026) | Loss 3.4350(3.4688) | Error 0.8889(0.8999) Steps 844(841.57) | Grad Norm 1.4193(2.4991) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 20.9204(20.8502) | Bit/dim 3.4624(3.4688) | Xent 2.3026(2.3026) | Loss 3.4624(3.4688) | Error 0.9033(0.9003) Steps 826(841.77) | Grad Norm 3.1695(2.5689) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 20.2174(20.8769) | Bit/dim 3.4470(3.4668) | Xent 2.3026(2.3026) | Loss 3.4470(3.4668) | Error 0.8822(0.8990) Steps 850(844.44) | Grad Norm 1.7376(2.4324) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 21.2070(20.8775) | Bit/dim 3.4612(3.4676) | Xent 2.3026(2.3026) | Loss 3.4612(3.4676) | Error 0.9067(0.9000) Steps 850(843.99) | Grad Norm 3.1707(2.5026) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 21.2137(20.9053) | Bit/dim 3.4755(3.4671) | Xent 2.3026(2.3026) | Loss 3.4755(3.4671) | Error 0.8922(0.9001) Steps 838(842.29) | Grad Norm 2.4888(2.5679) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 105.0190, Epoch Time 1271.9677(1230.1258), Bit/dim 3.4656(best: 3.4691), Xent 2.3026, Loss 3.4656, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 20.4407(20.9022) | Bit/dim 3.4181(3.4648) | Xent 2.3026(2.3026) | Loss 3.4181(3.4648) | Error 0.8800(0.8989) Steps 862(843.11) | Grad Norm 3.0290(2.5064) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 21.3786(20.9905) | Bit/dim 3.4829(3.4666) | Xent 2.3026(2.3026) | Loss 3.4829(3.4666) | Error 0.8922(0.8994) Steps 844(842.71) | Grad Norm 1.6412(2.5325) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 20.0720(20.9285) | Bit/dim 3.4719(3.4664) | Xent 2.3026(2.3026) | Loss 3.4719(3.4664) | Error 0.9033(0.8995) Steps 838(842.64) | Grad Norm 2.3640(2.4656) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 21.9322(20.9094) | Bit/dim 3.4492(3.4657) | Xent 2.3026(2.3026) | Loss 3.4492(3.4657) | Error 0.9156(0.9002) Steps 850(844.05) | Grad Norm 1.4791(2.4458) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 20.4741(20.8036) | Bit/dim 3.4675(3.4651) | Xent 2.3026(2.3026) | Loss 3.4675(3.4651) | Error 0.8978(0.9009) Steps 850(844.31) | Grad Norm 2.5322(2.3618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 104.1021, Epoch Time 1269.2262(1231.2988), Bit/dim 3.4687(best: 3.4656), Xent 2.3026, Loss 3.4687, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 20.8679(20.8567) | Bit/dim 3.4753(3.4662) | Xent 2.3026(2.3026) | Loss 3.4753(3.4662) | Error 0.8889(0.9004) Steps 850(844.38) | Grad Norm 2.6523(2.3099) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 21.4481(20.8707) | Bit/dim 3.4462(3.4672) | Xent 2.3026(2.3026) | Loss 3.4462(3.4672) | Error 0.9189(0.9019) Steps 850(844.79) | Grad Norm 1.9893(2.4517) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 20.1403(20.8133) | Bit/dim 3.4485(3.4663) | Xent 2.3026(2.3026) | Loss 3.4485(3.4663) | Error 0.8989(0.9011) Steps 838(844.83) | Grad Norm 1.7503(2.4593) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 20.5048(20.8276) | Bit/dim 3.4451(3.4645) | Xent 2.3026(2.3026) | Loss 3.4451(3.4645) | Error 0.9022(0.9014) Steps 844(845.63) | Grad Norm 3.9337(2.5127) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 21.1657(20.8323) | Bit/dim 3.4625(3.4643) | Xent 2.3026(2.3026) | Loss 3.4625(3.4643) | Error 0.8967(0.8997) Steps 856(845.46) | Grad Norm 2.3875(2.4734) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 20.9899(20.9097) | Bit/dim 3.4441(3.4621) | Xent 2.3026(2.3026) | Loss 3.4441(3.4621) | Error 0.9011(0.8993) Steps 844(845.96) | Grad Norm 4.2128(2.4545) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 104.8324, Epoch Time 1272.1100(1232.5231), Bit/dim 3.4747(best: 3.4656), Xent 2.3026, Loss 3.4747, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 20.8328(20.8975) | Bit/dim 3.4629(3.4630) | Xent 2.3026(2.3026) | Loss 3.4629(3.4630) | Error 0.9044(0.9006) Steps 862(846.78) | Grad Norm 1.3029(2.4121) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 20.9669(20.8950) | Bit/dim 3.4963(3.4629) | Xent 2.3026(2.3026) | Loss 3.4963(3.4629) | Error 0.8922(0.9006) Steps 832(844.34) | Grad Norm 1.8769(2.4033) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 21.4321(20.9294) | Bit/dim 3.4562(3.4603) | Xent 2.3026(2.3026) | Loss 3.4562(3.4603) | Error 0.8989(0.8997) Steps 862(844.00) | Grad Norm 2.8399(2.3511) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 21.2205(20.9183) | Bit/dim 3.4828(3.4626) | Xent 2.3026(2.3026) | Loss 3.4828(3.4626) | Error 0.9011(0.8997) Steps 844(845.00) | Grad Norm 4.5720(2.4687) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 20.1314(20.8823) | Bit/dim 3.4734(3.4640) | Xent 2.3026(2.3026) | Loss 3.4734(3.4640) | Error 0.8956(0.8993) Steps 832(844.29) | Grad Norm 2.7472(2.5556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 104.6731, Epoch Time 1271.0500(1233.6789), Bit/dim 3.4674(best: 3.4656), Xent 2.3026, Loss 3.4674, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 21.1254(20.8697) | Bit/dim 3.4541(3.4639) | Xent 2.3026(2.3026) | Loss 3.4541(3.4639) | Error 0.8956(0.8991) Steps 850(842.94) | Grad Norm 1.6970(2.3967) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 21.0303(20.8586) | Bit/dim 3.4434(3.4628) | Xent 2.3026(2.3026) | Loss 3.4434(3.4628) | Error 0.8956(0.8990) Steps 844(842.65) | Grad Norm 3.3232(2.5125) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 21.2403(20.8618) | Bit/dim 3.4628(3.4631) | Xent 2.3026(2.3026) | Loss 3.4628(3.4631) | Error 0.9033(0.8989) Steps 838(842.06) | Grad Norm 1.3365(2.3806) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 20.8583(20.8595) | Bit/dim 3.4644(3.4611) | Xent 2.3026(2.3026) | Loss 3.4644(3.4611) | Error 0.9033(0.8987) Steps 838(842.80) | Grad Norm 1.8203(2.1798) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 20.8461(20.9217) | Bit/dim 3.4498(3.4612) | Xent 2.3026(2.3026) | Loss 3.4498(3.4612) | Error 0.8933(0.9010) Steps 856(842.68) | Grad Norm 2.0335(2.2476) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 20.9334(20.9113) | Bit/dim 3.4597(3.4608) | Xent 2.3026(2.3026) | Loss 3.4597(3.4608) | Error 0.9033(0.9007) Steps 838(842.24) | Grad Norm 2.1771(2.2438) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 104.0686, Epoch Time 1272.3859(1234.8402), Bit/dim 3.4633(best: 3.4656), Xent 2.3026, Loss 3.4633, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 20.7988(20.9940) | Bit/dim 3.4562(3.4625) | Xent 2.3026(2.3026) | Loss 3.4562(3.4625) | Error 0.9111(0.9010) Steps 850(844.03) | Grad Norm 2.6015(2.2653) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 21.2421(20.9260) | Bit/dim 3.4817(3.4633) | Xent 2.3026(2.3026) | Loss 3.4817(3.4633) | Error 0.9222(0.9004) Steps 850(845.07) | Grad Norm 1.8636(2.2207) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 20.9886(21.0249) | Bit/dim 3.4584(3.4633) | Xent 2.3026(2.3026) | Loss 3.4584(3.4633) | Error 0.8889(0.8998) Steps 850(847.06) | Grad Norm 2.9473(2.3078) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 20.9428(20.9144) | Bit/dim 3.4430(3.4593) | Xent 2.3026(2.3026) | Loss 3.4430(3.4593) | Error 0.8978(0.8995) Steps 838(846.84) | Grad Norm 2.6389(2.3937) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 20.6626(20.9298) | Bit/dim 3.4470(3.4573) | Xent 2.3026(2.3026) | Loss 3.4470(3.4573) | Error 0.8844(0.8997) Steps 844(845.08) | Grad Norm 1.4815(2.3448) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 105.2315, Epoch Time 1277.9134(1236.1323), Bit/dim 3.4648(best: 3.4633), Xent 2.3026, Loss 3.4648, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 21.0716(20.9105) | Bit/dim 3.4580(3.4585) | Xent 2.3026(2.3026) | Loss 3.4580(3.4585) | Error 0.9044(0.9003) Steps 862(846.53) | Grad Norm 1.3725(2.3079) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 20.3161(20.9481) | Bit/dim 3.4625(3.4582) | Xent 2.3026(2.3026) | Loss 3.4625(3.4582) | Error 0.8989(0.8999) Steps 850(846.43) | Grad Norm 2.7966(2.3720) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 20.7894(20.9194) | Bit/dim 3.4681(3.4591) | Xent 2.3026(2.3026) | Loss 3.4681(3.4591) | Error 0.9033(0.9012) Steps 844(846.50) | Grad Norm 3.0156(2.4108) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 21.2089(20.8747) | Bit/dim 3.4631(3.4597) | Xent 2.3026(2.3026) | Loss 3.4631(3.4597) | Error 0.8878(0.8997) Steps 856(846.40) | Grad Norm 2.6528(2.4755) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 20.4075(20.8513) | Bit/dim 3.4534(3.4591) | Xent 2.3026(2.3026) | Loss 3.4534(3.4591) | Error 0.8956(0.8991) Steps 844(845.31) | Grad Norm 2.0898(2.4188) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 20.2613(20.8291) | Bit/dim 3.4381(3.4590) | Xent 2.3026(2.3026) | Loss 3.4381(3.4590) | Error 0.9078(0.9002) Steps 844(845.53) | Grad Norm 2.9757(2.4067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 105.0739, Epoch Time 1268.8287(1237.1132), Bit/dim 3.4653(best: 3.4633), Xent 2.3026, Loss 3.4653, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 20.9480(20.8914) | Bit/dim 3.4706(3.4577) | Xent 2.3026(2.3026) | Loss 3.4706(3.4577) | Error 0.9100(0.8991) Steps 838(844.54) | Grad Norm 1.5747(2.4135) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 20.9258(20.8878) | Bit/dim 3.4621(3.4576) | Xent 2.3026(2.3026) | Loss 3.4621(3.4576) | Error 0.9056(0.8997) Steps 850(844.99) | Grad Norm 1.8622(2.4048) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 20.8252(20.9186) | Bit/dim 3.4657(3.4582) | Xent 2.3026(2.3026) | Loss 3.4657(3.4582) | Error 0.9089(0.9009) Steps 838(846.33) | Grad Norm 1.5950(2.4366) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 20.9298(20.9482) | Bit/dim 3.4617(3.4604) | Xent 2.3026(2.3026) | Loss 3.4617(3.4604) | Error 0.9067(0.9009) Steps 868(848.49) | Grad Norm 2.9958(2.5216) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 21.5538(21.0023) | Bit/dim 3.4743(3.4591) | Xent 2.3026(2.3026) | Loss 3.4743(3.4591) | Error 0.9022(0.9003) Steps 856(847.46) | Grad Norm 2.2193(2.4809) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 104.7798, Epoch Time 1281.3261(1238.4396), Bit/dim 3.4632(best: 3.4633), Xent 2.3026, Loss 3.4632, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 20.6627(21.0180) | Bit/dim 3.4316(3.4580) | Xent 2.3026(2.3026) | Loss 3.4316(3.4580) | Error 0.8922(0.8988) Steps 850(848.02) | Grad Norm 1.9712(2.4757) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 21.6237(21.0280) | Bit/dim 3.5193(3.4599) | Xent 2.3026(2.3026) | Loss 3.5193(3.4599) | Error 0.9133(0.9014) Steps 826(849.11) | Grad Norm 2.7574(2.4003) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 21.5524(21.0349) | Bit/dim 3.4462(3.4561) | Xent 2.3026(2.3026) | Loss 3.4462(3.4561) | Error 0.9011(0.9014) Steps 850(848.81) | Grad Norm 1.7990(2.4931) | Total Time 14.00(14.00)\n",
      "Iter 6690 | Time 21.1444(21.0458) | Bit/dim 3.5078(3.4586) | Xent 2.3026(2.3026) | Loss 3.5078(3.4586) | Error 0.8933(0.9016) Steps 862(850.64) | Grad Norm 2.5134(2.4801) | Total Time 14.00(14.00)\n",
      "Iter 6700 | Time 20.9671(20.9890) | Bit/dim 3.4656(3.4600) | Xent 2.3026(2.3026) | Loss 3.4656(3.4600) | Error 0.9033(0.9001) Steps 850(849.72) | Grad Norm 2.5316(2.4905) | Total Time 14.00(14.00)\n",
      "Iter 6710 | Time 20.7436(20.9594) | Bit/dim 3.4342(3.4552) | Xent 2.3026(2.3026) | Loss 3.4342(3.4552) | Error 0.9022(0.8991) Steps 844(848.47) | Grad Norm 1.5916(2.3892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 104.1802, Epoch Time 1274.3903(1239.5181), Bit/dim 3.4584(best: 3.4632), Xent 2.3026, Loss 3.4584, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 21.3939(20.9661) | Bit/dim 3.4461(3.4540) | Xent 2.3026(2.3026) | Loss 3.4461(3.4540) | Error 0.9078(0.8994) Steps 850(849.78) | Grad Norm 1.6659(2.3670) | Total Time 14.00(14.00)\n",
      "Iter 6730 | Time 20.5267(20.9581) | Bit/dim 3.4430(3.4533) | Xent 2.3026(2.3026) | Loss 3.4430(3.4533) | Error 0.9033(0.9000) Steps 844(849.91) | Grad Norm 3.1243(2.4568) | Total Time 14.00(14.00)\n",
      "Iter 6740 | Time 21.0909(20.9484) | Bit/dim 3.4545(3.4540) | Xent 2.3026(2.3026) | Loss 3.4545(3.4540) | Error 0.8889(0.8990) Steps 850(850.15) | Grad Norm 1.5191(2.2990) | Total Time 14.00(14.00)\n",
      "Iter 6750 | Time 21.1052(20.9383) | Bit/dim 3.4554(3.4550) | Xent 2.3026(2.3026) | Loss 3.4554(3.4550) | Error 0.9144(0.8998) Steps 868(850.48) | Grad Norm 1.7677(2.3108) | Total Time 14.00(14.00)\n",
      "Iter 6760 | Time 20.8707(20.9911) | Bit/dim 3.4467(3.4577) | Xent 2.3026(2.3026) | Loss 3.4467(3.4577) | Error 0.9200(0.9009) Steps 838(848.84) | Grad Norm 1.5945(2.2869) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 105.5367, Epoch Time 1277.6709(1240.6627), Bit/dim 3.4575(best: 3.4584), Xent 2.3026, Loss 3.4575, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 21.1125(20.9831) | Bit/dim 3.4365(3.4553) | Xent 2.3026(2.3026) | Loss 3.4365(3.4553) | Error 0.9022(0.8998) Steps 868(850.49) | Grad Norm 2.9635(2.2473) | Total Time 14.00(14.00)\n",
      "Iter 6780 | Time 20.3968(20.9157) | Bit/dim 3.4454(3.4566) | Xent 2.3026(2.3026) | Loss 3.4454(3.4566) | Error 0.8933(0.9003) Steps 850(848.41) | Grad Norm 1.5805(2.4078) | Total Time 14.00(14.00)\n",
      "Iter 6790 | Time 20.5659(20.9236) | Bit/dim 3.4526(3.4587) | Xent 2.3026(2.3026) | Loss 3.4526(3.4587) | Error 0.9100(0.9003) Steps 844(848.48) | Grad Norm 1.5747(2.3432) | Total Time 14.00(14.00)\n",
      "Iter 6800 | Time 21.0501(20.9299) | Bit/dim 3.4964(3.4580) | Xent 2.3026(2.3026) | Loss 3.4964(3.4580) | Error 0.9089(0.8998) Steps 856(850.56) | Grad Norm 3.1771(2.2305) | Total Time 14.00(14.00)\n",
      "Iter 6810 | Time 21.2842(20.9488) | Bit/dim 3.4577(3.4579) | Xent 2.3026(2.3026) | Loss 3.4577(3.4579) | Error 0.9078(0.9005) Steps 868(852.26) | Grad Norm 2.0542(2.1713) | Total Time 14.00(14.00)\n",
      "Iter 6820 | Time 21.5218(20.9827) | Bit/dim 3.4535(3.4557) | Xent 2.3026(2.3026) | Loss 3.4535(3.4557) | Error 0.8933(0.8998) Steps 868(853.36) | Grad Norm 1.7713(2.1946) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 103.9614, Epoch Time 1273.5508(1241.6494), Bit/dim 3.4551(best: 3.4575), Xent 2.3026, Loss 3.4551, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 21.5538(21.0101) | Bit/dim 3.4377(3.4550) | Xent 2.3026(2.3026) | Loss 3.4377(3.4550) | Error 0.8978(0.8988) Steps 850(853.09) | Grad Norm 1.9907(2.2877) | Total Time 14.00(14.00)\n",
      "Iter 6840 | Time 21.0614(21.0613) | Bit/dim 3.4827(3.4592) | Xent 2.3026(2.3026) | Loss 3.4827(3.4592) | Error 0.9022(0.8990) Steps 874(854.10) | Grad Norm 2.4068(2.2521) | Total Time 14.00(14.00)\n",
      "Iter 6850 | Time 21.2751(21.0853) | Bit/dim 3.4668(3.4556) | Xent 2.3026(2.3026) | Loss 3.4668(3.4556) | Error 0.9067(0.9000) Steps 844(853.26) | Grad Norm 3.5903(2.3167) | Total Time 14.00(14.00)\n",
      "Iter 6860 | Time 20.7660(21.1039) | Bit/dim 3.4366(3.4541) | Xent 2.3026(2.3026) | Loss 3.4366(3.4541) | Error 0.8789(0.9003) Steps 850(853.66) | Grad Norm 1.5374(2.3077) | Total Time 14.00(14.00)\n",
      "Iter 6870 | Time 21.2493(21.1288) | Bit/dim 3.4133(3.4530) | Xent 2.3026(2.3026) | Loss 3.4133(3.4530) | Error 0.9000(0.9004) Steps 850(852.63) | Grad Norm 3.0377(2.3496) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 104.8396, Epoch Time 1286.8591(1243.0057), Bit/dim 3.4532(best: 3.4551), Xent 2.3026, Loss 3.4532, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 21.4378(21.1515) | Bit/dim 3.4496(3.4500) | Xent 2.3026(2.3026) | Loss 3.4496(3.4500) | Error 0.9133(0.8997) Steps 850(852.66) | Grad Norm 2.1033(2.3471) | Total Time 14.00(14.00)\n",
      "Iter 6890 | Time 20.6156(21.1405) | Bit/dim 3.4893(3.4541) | Xent 2.3026(2.3026) | Loss 3.4893(3.4541) | Error 0.9133(0.9006) Steps 856(853.50) | Grad Norm 3.2780(2.3924) | Total Time 14.00(14.00)\n",
      "Iter 6900 | Time 20.9623(21.1535) | Bit/dim 3.4472(3.4519) | Xent 2.3026(2.3026) | Loss 3.4472(3.4519) | Error 0.9044(0.9014) Steps 856(853.54) | Grad Norm 1.5270(2.4596) | Total Time 14.00(14.00)\n",
      "Iter 6910 | Time 21.1116(21.2251) | Bit/dim 3.4628(3.4515) | Xent 2.3026(2.3026) | Loss 3.4628(3.4515) | Error 0.8944(0.9000) Steps 862(854.48) | Grad Norm 1.7109(2.3133) | Total Time 14.00(14.00)\n",
      "Iter 6920 | Time 20.7514(21.1577) | Bit/dim 3.4329(3.4531) | Xent 2.3026(2.3026) | Loss 3.4329(3.4531) | Error 0.9022(0.9001) Steps 856(854.91) | Grad Norm 2.7327(2.4100) | Total Time 14.00(14.00)\n",
      "Iter 6930 | Time 21.0293(21.1293) | Bit/dim 3.4419(3.4526) | Xent 2.3026(2.3026) | Loss 3.4419(3.4526) | Error 0.8978(0.8996) Steps 856(856.42) | Grad Norm 2.9914(2.2761) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 104.8407, Epoch Time 1286.5147(1244.3109), Bit/dim 3.4562(best: 3.4532), Xent 2.3026, Loss 3.4562, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 21.5967(21.2516) | Bit/dim 3.5053(3.4524) | Xent 2.3026(2.3026) | Loss 3.5053(3.4524) | Error 0.9044(0.9004) Steps 856(857.23) | Grad Norm 2.7191(2.2407) | Total Time 14.00(14.00)\n",
      "Iter 6950 | Time 21.1906(21.2431) | Bit/dim 3.4675(3.4543) | Xent 2.3026(2.3026) | Loss 3.4675(3.4543) | Error 0.9100(0.9007) Steps 856(856.27) | Grad Norm 3.6449(2.2926) | Total Time 14.00(14.00)\n",
      "Iter 6960 | Time 21.1684(21.2939) | Bit/dim 3.4555(3.4544) | Xent 2.3026(2.3026) | Loss 3.4555(3.4544) | Error 0.8900(0.9007) Steps 862(857.48) | Grad Norm 3.8814(2.4432) | Total Time 14.00(14.00)\n",
      "Iter 6970 | Time 21.4543(21.2944) | Bit/dim 3.4344(3.4514) | Xent 2.3026(2.3026) | Loss 3.4344(3.4514) | Error 0.9033(0.9008) Steps 856(856.30) | Grad Norm 2.6190(2.4664) | Total Time 14.00(14.00)\n",
      "Iter 6980 | Time 20.4472(21.1969) | Bit/dim 3.4017(3.4513) | Xent 2.3026(2.3026) | Loss 3.4017(3.4513) | Error 0.8833(0.8989) Steps 850(855.03) | Grad Norm 2.0909(2.4423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 104.7427, Epoch Time 1293.8073(1245.7958), Bit/dim 3.4535(best: 3.4532), Xent 2.3026, Loss 3.4535, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 20.5446(21.1537) | Bit/dim 3.4733(3.4529) | Xent 2.3026(2.3026) | Loss 3.4733(3.4529) | Error 0.9167(0.8997) Steps 868(856.44) | Grad Norm 2.4541(2.3016) | Total Time 14.00(14.00)\n",
      "Iter 7000 | Time 20.4614(21.1385) | Bit/dim 3.4281(3.4494) | Xent 2.3026(2.3026) | Loss 3.4281(3.4494) | Error 0.9022(0.8992) Steps 844(855.99) | Grad Norm 1.5188(2.4284) | Total Time 14.00(14.00)\n",
      "Iter 7010 | Time 21.2416(21.1277) | Bit/dim 3.4828(3.4507) | Xent 2.3026(2.3026) | Loss 3.4828(3.4507) | Error 0.9189(0.9003) Steps 850(854.51) | Grad Norm 2.7520(2.4412) | Total Time 14.00(14.00)\n",
      "Iter 7020 | Time 21.4313(21.1426) | Bit/dim 3.4447(3.4507) | Xent 2.3026(2.3026) | Loss 3.4447(3.4507) | Error 0.8967(0.9001) Steps 856(852.83) | Grad Norm 1.9042(2.3130) | Total Time 14.00(14.00)\n",
      "Iter 7030 | Time 21.3748(21.1299) | Bit/dim 3.4493(3.4496) | Xent 2.3026(2.3026) | Loss 3.4493(3.4496) | Error 0.9011(0.9000) Steps 850(852.89) | Grad Norm 2.5902(2.4478) | Total Time 14.00(14.00)\n",
      "Iter 7040 | Time 21.0221(21.1270) | Bit/dim 3.4659(3.4520) | Xent 2.3026(2.3026) | Loss 3.4659(3.4520) | Error 0.8967(0.8996) Steps 874(854.61) | Grad Norm 2.1078(2.3222) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 106.3373, Epoch Time 1284.7436(1246.9643), Bit/dim 3.4511(best: 3.4532), Xent 2.3026, Loss 3.4511, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 21.2188(21.1378) | Bit/dim 3.4281(3.4490) | Xent 2.3026(2.3026) | Loss 3.4281(3.4490) | Error 0.8889(0.8986) Steps 856(856.37) | Grad Norm 3.1769(2.3717) | Total Time 14.00(14.00)\n",
      "Iter 7060 | Time 20.7463(21.1527) | Bit/dim 3.4117(3.4475) | Xent 2.3026(2.3026) | Loss 3.4117(3.4475) | Error 0.9067(0.8997) Steps 868(857.95) | Grad Norm 2.8308(2.3960) | Total Time 14.00(14.00)\n",
      "Iter 7070 | Time 21.2824(21.2067) | Bit/dim 3.4490(3.4489) | Xent 2.3026(2.3026) | Loss 3.4490(3.4489) | Error 0.9122(0.8991) Steps 874(857.86) | Grad Norm 2.1756(2.4652) | Total Time 14.00(14.00)\n",
      "Iter 7080 | Time 21.3645(21.1856) | Bit/dim 3.4399(3.4496) | Xent 2.3026(2.3026) | Loss 3.4399(3.4496) | Error 0.9056(0.8986) Steps 862(858.06) | Grad Norm 2.5564(2.4576) | Total Time 14.00(14.00)\n",
      "Iter 7090 | Time 20.9607(21.1384) | Bit/dim 3.4444(3.4501) | Xent 2.3026(2.3026) | Loss 3.4444(3.4501) | Error 0.9078(0.9002) Steps 844(857.76) | Grad Norm 3.2342(2.4365) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 105.1253, Epoch Time 1287.6306(1248.1842), Bit/dim 3.4524(best: 3.4511), Xent 2.3026, Loss 3.4524, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 21.2259(21.1598) | Bit/dim 3.4049(3.4501) | Xent 2.3026(2.3026) | Loss 3.4049(3.4501) | Error 0.8922(0.9001) Steps 850(857.18) | Grad Norm 2.4175(2.4118) | Total Time 14.00(14.00)\n",
      "Iter 7110 | Time 21.6631(21.1477) | Bit/dim 3.4314(3.4505) | Xent 2.3026(2.3026) | Loss 3.4314(3.4505) | Error 0.8889(0.9007) Steps 856(857.87) | Grad Norm 2.1386(2.2663) | Total Time 14.00(14.00)\n",
      "Iter 7120 | Time 21.5214(21.1232) | Bit/dim 3.4393(3.4493) | Xent 2.3026(2.3026) | Loss 3.4393(3.4493) | Error 0.9089(0.9008) Steps 856(858.04) | Grad Norm 2.6559(2.4381) | Total Time 14.00(14.00)\n",
      "Iter 7130 | Time 20.7442(21.1067) | Bit/dim 3.4242(3.4489) | Xent 2.3026(2.3026) | Loss 3.4242(3.4489) | Error 0.9000(0.9006) Steps 868(858.07) | Grad Norm 1.3606(2.3637) | Total Time 14.00(14.00)\n",
      "Iter 7140 | Time 21.6150(21.1251) | Bit/dim 3.4904(3.4483) | Xent 2.3026(2.3026) | Loss 3.4904(3.4483) | Error 0.9067(0.9011) Steps 862(858.16) | Grad Norm 2.0762(2.4123) | Total Time 14.00(14.00)\n",
      "Iter 7150 | Time 21.7149(21.0826) | Bit/dim 3.4665(3.4494) | Xent 2.3026(2.3026) | Loss 3.4665(3.4494) | Error 0.8644(0.8998) Steps 838(857.15) | Grad Norm 2.7841(2.3446) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 105.3531, Epoch Time 1283.4177(1249.2412), Bit/dim 3.4528(best: 3.4511), Xent 2.3026, Loss 3.4528, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 21.4034(21.0971) | Bit/dim 3.4779(3.4502) | Xent 2.3026(2.3026) | Loss 3.4779(3.4502) | Error 0.8944(0.8989) Steps 856(856.87) | Grad Norm 1.3119(2.2053) | Total Time 14.00(14.00)\n",
      "Iter 7170 | Time 20.8412(21.1566) | Bit/dim 3.4593(3.4499) | Xent 2.3026(2.3026) | Loss 3.4593(3.4499) | Error 0.9100(0.8994) Steps 856(856.98) | Grad Norm 3.0376(2.2370) | Total Time 14.00(14.00)\n",
      "Iter 7180 | Time 21.3430(21.1707) | Bit/dim 3.4405(3.4479) | Xent 2.3026(2.3026) | Loss 3.4405(3.4479) | Error 0.9122(0.9002) Steps 862(857.39) | Grad Norm 3.1285(2.3991) | Total Time 14.00(14.00)\n",
      "Iter 7190 | Time 21.8969(21.2036) | Bit/dim 3.4357(3.4463) | Xent 2.3026(2.3026) | Loss 3.4357(3.4463) | Error 0.8978(0.9003) Steps 874(859.87) | Grad Norm 1.4128(2.2536) | Total Time 14.00(14.00)\n",
      "Iter 7200 | Time 21.0077(21.2326) | Bit/dim 3.4470(3.4462) | Xent 2.3026(2.3026) | Loss 3.4470(3.4462) | Error 0.9000(0.9004) Steps 856(859.70) | Grad Norm 2.0263(2.3839) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 106.2567, Epoch Time 1294.3430(1250.5943), Bit/dim 3.4503(best: 3.4511), Xent 2.3026, Loss 3.4503, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 20.3920(21.2202) | Bit/dim 3.4208(3.4472) | Xent 2.3026(2.3026) | Loss 3.4208(3.4472) | Error 0.8867(0.8999) Steps 856(858.92) | Grad Norm 2.7056(2.3325) | Total Time 14.00(14.00)\n",
      "Iter 7220 | Time 21.0272(21.1679) | Bit/dim 3.4139(3.4462) | Xent 2.3026(2.3026) | Loss 3.4139(3.4462) | Error 0.8911(0.9005) Steps 850(857.38) | Grad Norm 2.7119(2.4202) | Total Time 14.00(14.00)\n",
      "Iter 7230 | Time 21.3383(21.1651) | Bit/dim 3.4313(3.4463) | Xent 2.3026(2.3026) | Loss 3.4313(3.4463) | Error 0.8833(0.8995) Steps 850(857.28) | Grad Norm 1.7931(2.3989) | Total Time 14.00(14.00)\n",
      "Iter 7240 | Time 21.7738(21.1737) | Bit/dim 3.4502(3.4447) | Xent 2.3026(2.3026) | Loss 3.4502(3.4447) | Error 0.8978(0.8990) Steps 832(857.68) | Grad Norm 2.2632(2.2942) | Total Time 14.00(14.00)\n",
      "Iter 7250 | Time 21.5858(21.2087) | Bit/dim 3.4620(3.4467) | Xent 2.3026(2.3026) | Loss 3.4620(3.4467) | Error 0.9100(0.8991) Steps 862(858.49) | Grad Norm 2.6529(2.3036) | Total Time 14.00(14.00)\n",
      "Iter 7260 | Time 20.6892(21.2143) | Bit/dim 3.4583(3.4476) | Xent 2.3026(2.3026) | Loss 3.4583(3.4476) | Error 0.9056(0.9002) Steps 862(859.86) | Grad Norm 1.1565(2.3298) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 105.8914, Epoch Time 1288.2070(1251.7227), Bit/dim 3.4480(best: 3.4503), Xent 2.3026, Loss 3.4480, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 21.2440(21.2265) | Bit/dim 3.4415(3.4469) | Xent 2.3026(2.3026) | Loss 3.4415(3.4469) | Error 0.9156(0.9002) Steps 862(859.68) | Grad Norm 1.9524(2.4219) | Total Time 14.00(14.00)\n",
      "Iter 7280 | Time 21.6857(21.2908) | Bit/dim 3.4519(3.4449) | Xent 2.3026(2.3026) | Loss 3.4519(3.4449) | Error 0.8989(0.9005) Steps 874(859.53) | Grad Norm 1.8766(2.2868) | Total Time 14.00(14.00)\n",
      "Iter 7290 | Time 21.3514(21.3388) | Bit/dim 3.4618(3.4468) | Xent 2.3026(2.3026) | Loss 3.4618(3.4468) | Error 0.9000(0.9011) Steps 868(860.66) | Grad Norm 1.8760(2.3856) | Total Time 14.00(14.00)\n",
      "Iter 7300 | Time 21.2597(21.3448) | Bit/dim 3.4157(3.4452) | Xent 2.3026(2.3026) | Loss 3.4157(3.4452) | Error 0.8978(0.9004) Steps 862(861.59) | Grad Norm 2.5962(2.3714) | Total Time 14.00(14.00)\n",
      "Iter 7310 | Time 21.0753(21.3204) | Bit/dim 3.4444(3.4464) | Xent 2.3026(2.3026) | Loss 3.4444(3.4464) | Error 0.8967(0.8996) Steps 856(860.43) | Grad Norm 3.3574(2.3136) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 105.9212, Epoch Time 1298.4153(1253.1235), Bit/dim 3.4446(best: 3.4480), Xent 2.3026, Loss 3.4446, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 20.8792(21.2923) | Bit/dim 3.4323(3.4465) | Xent 2.3026(2.3026) | Loss 3.4323(3.4465) | Error 0.9000(0.9003) Steps 850(858.49) | Grad Norm 1.9721(2.2552) | Total Time 14.00(14.00)\n",
      "Iter 7330 | Time 20.9119(21.2468) | Bit/dim 3.4532(3.4458) | Xent 2.3026(2.3026) | Loss 3.4532(3.4458) | Error 0.9044(0.8993) Steps 868(858.36) | Grad Norm 2.7760(2.2602) | Total Time 14.00(14.00)\n",
      "Iter 7340 | Time 21.5693(21.2432) | Bit/dim 3.4651(3.4459) | Xent 2.3026(2.3026) | Loss 3.4651(3.4459) | Error 0.9100(0.8982) Steps 862(859.12) | Grad Norm 2.4438(2.3059) | Total Time 14.00(14.00)\n",
      "Iter 7350 | Time 21.1164(21.2092) | Bit/dim 3.4746(3.4471) | Xent 2.3026(2.3026) | Loss 3.4746(3.4471) | Error 0.9056(0.8998) Steps 868(859.95) | Grad Norm 1.9364(2.2913) | Total Time 14.00(14.00)\n",
      "Iter 7360 | Time 21.3979(21.1749) | Bit/dim 3.4259(3.4456) | Xent 2.3026(2.3026) | Loss 3.4259(3.4456) | Error 0.9044(0.8986) Steps 856(861.09) | Grad Norm 2.3083(2.2023) | Total Time 14.00(14.00)\n",
      "Iter 7370 | Time 21.2031(21.1993) | Bit/dim 3.4924(3.4457) | Xent 2.3026(2.3026) | Loss 3.4924(3.4457) | Error 0.9089(0.9005) Steps 862(861.38) | Grad Norm 1.9580(2.3381) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 106.6948, Epoch Time 1289.3707(1254.2109), Bit/dim 3.4456(best: 3.4446), Xent 2.3026, Loss 3.4456, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 21.4975(21.2365) | Bit/dim 3.4219(3.4451) | Xent 2.3026(2.3026) | Loss 3.4219(3.4451) | Error 0.9033(0.8996) Steps 862(860.48) | Grad Norm 2.6058(2.3762) | Total Time 14.00(14.00)\n",
      "Iter 7390 | Time 20.6900(21.2782) | Bit/dim 3.4305(3.4467) | Xent 2.3026(2.3026) | Loss 3.4305(3.4467) | Error 0.8956(0.8989) Steps 856(861.25) | Grad Norm 2.6264(2.3826) | Total Time 14.00(14.00)\n",
      "Iter 7400 | Time 20.9795(21.2986) | Bit/dim 3.4538(3.4447) | Xent 2.3026(2.3026) | Loss 3.4538(3.4447) | Error 0.8900(0.8996) Steps 850(859.42) | Grad Norm 1.0727(2.3171) | Total Time 14.00(14.00)\n",
      "Iter 7410 | Time 20.8475(21.2909) | Bit/dim 3.4379(3.4435) | Xent 2.3026(2.3026) | Loss 3.4379(3.4435) | Error 0.9189(0.9001) Steps 850(857.70) | Grad Norm 1.9315(2.2665) | Total Time 14.00(14.00)\n",
      "Iter 7420 | Time 21.3327(21.2661) | Bit/dim 3.4426(3.4459) | Xent 2.3026(2.3026) | Loss 3.4426(3.4459) | Error 0.9067(0.9009) Steps 862(859.65) | Grad Norm 1.7919(2.1254) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 106.4344, Epoch Time 1297.0129(1255.4949), Bit/dim 3.4470(best: 3.4446), Xent 2.3026, Loss 3.4470, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.8912(21.2263) | Bit/dim 3.4431(3.4457) | Xent 2.3026(2.3026) | Loss 3.4431(3.4457) | Error 0.9000(0.9010) Steps 862(860.67) | Grad Norm 1.6965(2.1076) | Total Time 14.00(14.00)\n",
      "Iter 7440 | Time 20.9037(21.2170) | Bit/dim 3.4197(3.4450) | Xent 2.3026(2.3026) | Loss 3.4197(3.4450) | Error 0.9022(0.8999) Steps 850(858.88) | Grad Norm 1.5484(2.2775) | Total Time 14.00(14.00)\n",
      "Iter 7450 | Time 20.7209(21.1969) | Bit/dim 3.4397(3.4459) | Xent 2.3026(2.3026) | Loss 3.4397(3.4459) | Error 0.9189(0.9004) Steps 874(858.93) | Grad Norm 1.9314(2.3352) | Total Time 14.00(14.00)\n",
      "Iter 7460 | Time 20.9009(21.2551) | Bit/dim 3.4280(3.4444) | Xent 2.3026(2.3026) | Loss 3.4280(3.4444) | Error 0.9022(0.9010) Steps 856(861.16) | Grad Norm 1.7091(2.2128) | Total Time 14.00(14.00)\n",
      "Iter 7470 | Time 20.8357(21.2751) | Bit/dim 3.4525(3.4448) | Xent 2.3026(2.3026) | Loss 3.4525(3.4448) | Error 0.8989(0.9007) Steps 844(860.38) | Grad Norm 2.2353(2.2920) | Total Time 14.00(14.00)\n",
      "Iter 7480 | Time 22.0351(21.3376) | Bit/dim 3.4404(3.4422) | Xent 2.3026(2.3026) | Loss 3.4404(3.4422) | Error 0.9089(0.9001) Steps 862(860.46) | Grad Norm 2.0027(2.3800) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 106.1532, Epoch Time 1295.4071(1256.6923), Bit/dim 3.4426(best: 3.4446), Xent 2.3026, Loss 3.4426, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 21.3747(21.3619) | Bit/dim 3.4337(3.4405) | Xent 2.3026(2.3026) | Loss 3.4337(3.4405) | Error 0.9156(0.8991) Steps 862(860.67) | Grad Norm 1.5766(2.3358) | Total Time 14.00(14.00)\n",
      "Iter 7500 | Time 21.1821(21.3319) | Bit/dim 3.4104(3.4403) | Xent 2.3026(2.3026) | Loss 3.4104(3.4403) | Error 0.8933(0.8983) Steps 844(859.60) | Grad Norm 2.8112(2.2747) | Total Time 14.00(14.00)\n",
      "Iter 7510 | Time 21.2418(21.3662) | Bit/dim 3.4107(3.4425) | Xent 2.3026(2.3026) | Loss 3.4107(3.4425) | Error 0.8800(0.8978) Steps 856(861.30) | Grad Norm 1.6189(2.3268) | Total Time 14.00(14.00)\n",
      "Iter 7520 | Time 22.0694(21.3015) | Bit/dim 3.4216(3.4417) | Xent 2.3026(2.3026) | Loss 3.4216(3.4417) | Error 0.9089(0.8996) Steps 880(862.22) | Grad Norm 2.2631(2.2394) | Total Time 14.00(14.00)\n",
      "Iter 7530 | Time 21.9459(21.3220) | Bit/dim 3.4428(3.4413) | Xent 2.3026(2.3026) | Loss 3.4428(3.4413) | Error 0.9200(0.9009) Steps 862(863.72) | Grad Norm 2.1999(2.3409) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 108.1750, Epoch Time 1301.1373(1258.0257), Bit/dim 3.4459(best: 3.4426), Xent 2.3026, Loss 3.4459, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 21.1569(21.3565) | Bit/dim 3.4568(3.4443) | Xent 2.3026(2.3026) | Loss 3.4568(3.4443) | Error 0.9033(0.9023) Steps 874(864.05) | Grad Norm 1.7151(2.3290) | Total Time 14.00(14.00)\n",
      "Iter 7550 | Time 21.3115(21.3353) | Bit/dim 3.4394(3.4450) | Xent 2.3026(2.3026) | Loss 3.4394(3.4450) | Error 0.9000(0.9008) Steps 850(864.55) | Grad Norm 2.0149(2.2935) | Total Time 14.00(14.00)\n",
      "Iter 7560 | Time 21.2096(21.2756) | Bit/dim 3.4454(3.4436) | Xent 2.3026(2.3026) | Loss 3.4454(3.4436) | Error 0.9111(0.9010) Steps 862(862.31) | Grad Norm 2.7530(2.3111) | Total Time 14.00(14.00)\n",
      "Iter 7570 | Time 22.2288(21.3683) | Bit/dim 3.4219(3.4419) | Xent 2.3026(2.3026) | Loss 3.4219(3.4419) | Error 0.8933(0.8992) Steps 868(863.13) | Grad Norm 1.5443(2.2612) | Total Time 14.00(14.00)\n",
      "Iter 7580 | Time 21.0166(21.3719) | Bit/dim 3.4375(3.4404) | Xent 2.3026(2.3026) | Loss 3.4375(3.4404) | Error 0.9111(0.8998) Steps 850(862.82) | Grad Norm 2.7305(2.3431) | Total Time 14.00(14.00)\n",
      "Iter 7590 | Time 21.2560(21.2898) | Bit/dim 3.4255(3.4407) | Xent 2.3026(2.3026) | Loss 3.4255(3.4407) | Error 0.8978(0.9004) Steps 862(860.94) | Grad Norm 2.5092(2.2827) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 106.6486, Epoch Time 1295.6055(1259.1530), Bit/dim 3.4476(best: 3.4426), Xent 2.3026, Loss 3.4476, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 21.4351(21.3481) | Bit/dim 3.4053(3.4411) | Xent 2.3026(2.3026) | Loss 3.4053(3.4411) | Error 0.8833(0.8996) Steps 856(862.36) | Grad Norm 1.9622(2.4054) | Total Time 14.00(14.00)\n",
      "Iter 7610 | Time 21.1523(21.3614) | Bit/dim 3.4408(3.4420) | Xent 2.3026(2.3026) | Loss 3.4408(3.4420) | Error 0.9033(0.9007) Steps 856(863.26) | Grad Norm 2.2816(2.3006) | Total Time 14.00(14.00)\n",
      "Iter 7620 | Time 21.5299(21.3882) | Bit/dim 3.4501(3.4401) | Xent 2.3026(2.3026) | Loss 3.4501(3.4401) | Error 0.9178(0.9012) Steps 856(863.28) | Grad Norm 1.6773(2.1857) | Total Time 14.00(14.00)\n",
      "Iter 7630 | Time 20.7363(21.4242) | Bit/dim 3.4683(3.4419) | Xent 2.3026(2.3026) | Loss 3.4683(3.4419) | Error 0.9067(0.9016) Steps 862(864.19) | Grad Norm 2.8028(2.2361) | Total Time 14.00(14.00)\n",
      "Iter 7640 | Time 21.3318(21.3933) | Bit/dim 3.4294(3.4418) | Xent 2.3026(2.3026) | Loss 3.4294(3.4418) | Error 0.9133(0.9002) Steps 856(862.36) | Grad Norm 2.0151(2.2961) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 106.5607, Epoch Time 1303.9842(1260.4980), Bit/dim 3.4428(best: 3.4426), Xent 2.3026, Loss 3.4428, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 21.1706(21.4180) | Bit/dim 3.4623(3.4397) | Xent 2.3026(2.3026) | Loss 3.4623(3.4397) | Error 0.9122(0.9002) Steps 862(862.57) | Grad Norm 2.6988(2.3581) | Total Time 14.00(14.00)\n",
      "Iter 7660 | Time 22.0881(21.4724) | Bit/dim 3.4226(3.4412) | Xent 2.3026(2.3026) | Loss 3.4226(3.4412) | Error 0.8944(0.9004) Steps 862(864.18) | Grad Norm 1.2874(2.3221) | Total Time 14.00(14.00)\n",
      "Iter 7670 | Time 21.3299(21.4563) | Bit/dim 3.4252(3.4404) | Xent 2.3026(2.3026) | Loss 3.4252(3.4404) | Error 0.9100(0.8992) Steps 856(862.63) | Grad Norm 2.0234(2.3079) | Total Time 14.00(14.00)\n",
      "Iter 7680 | Time 21.6093(21.4568) | Bit/dim 3.4587(3.4419) | Xent 2.3026(2.3026) | Loss 3.4587(3.4419) | Error 0.9022(0.8998) Steps 868(861.51) | Grad Norm 2.0276(2.3289) | Total Time 14.00(14.00)\n",
      "Iter 7690 | Time 21.1757(21.4543) | Bit/dim 3.4624(3.4406) | Xent 2.3026(2.3026) | Loss 3.4624(3.4406) | Error 0.9044(0.9002) Steps 856(861.18) | Grad Norm 1.6115(2.4300) | Total Time 14.00(14.00)\n",
      "Iter 7700 | Time 21.2975(21.3845) | Bit/dim 3.4322(3.4388) | Xent 2.3026(2.3026) | Loss 3.4322(3.4388) | Error 0.9133(0.8998) Steps 880(861.90) | Grad Norm 1.4136(2.3404) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 107.4926, Epoch Time 1305.0392(1261.8342), Bit/dim 3.4386(best: 3.4426), Xent 2.3026, Loss 3.4386, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 21.1059(21.3277) | Bit/dim 3.4530(3.4390) | Xent 2.3026(2.3026) | Loss 3.4530(3.4390) | Error 0.9100(0.9002) Steps 862(862.95) | Grad Norm 2.6773(2.3932) | Total Time 14.00(14.00)\n",
      "Iter 7720 | Time 21.7072(21.2992) | Bit/dim 3.4363(3.4410) | Xent 2.3026(2.3026) | Loss 3.4363(3.4410) | Error 0.8989(0.9008) Steps 874(864.08) | Grad Norm 3.0839(2.2421) | Total Time 14.00(14.00)\n",
      "Iter 7730 | Time 21.3891(21.2795) | Bit/dim 3.4582(3.4417) | Xent 2.3026(2.3026) | Loss 3.4582(3.4417) | Error 0.9067(0.9007) Steps 862(865.09) | Grad Norm 2.3628(2.3644) | Total Time 14.00(14.00)\n",
      "Iter 7740 | Time 21.6266(21.3432) | Bit/dim 3.4305(3.4393) | Xent 2.3026(2.3026) | Loss 3.4305(3.4393) | Error 0.9044(0.9006) Steps 874(865.86) | Grad Norm 1.6957(2.3830) | Total Time 14.00(14.00)\n",
      "Iter 7750 | Time 20.9673(21.3165) | Bit/dim 3.4210(3.4402) | Xent 2.3026(2.3026) | Loss 3.4210(3.4402) | Error 0.8978(0.9001) Steps 862(864.69) | Grad Norm 1.7658(2.2015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 105.5735, Epoch Time 1295.4631(1262.8431), Bit/dim 3.4423(best: 3.4386), Xent 2.3026, Loss 3.4423, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 21.8634(21.4017) | Bit/dim 3.4709(3.4362) | Xent 2.3026(2.3026) | Loss 3.4709(3.4362) | Error 0.9078(0.8994) Steps 862(866.19) | Grad Norm 2.0200(2.2367) | Total Time 14.00(14.00)\n",
      "Iter 7770 | Time 21.6115(21.3808) | Bit/dim 3.4634(3.4360) | Xent 2.3026(2.3026) | Loss 3.4634(3.4360) | Error 0.9178(0.9003) Steps 880(866.58) | Grad Norm 2.4421(2.2613) | Total Time 14.00(14.00)\n",
      "Iter 7780 | Time 21.6548(21.4184) | Bit/dim 3.4314(3.4352) | Xent 2.3026(2.3026) | Loss 3.4314(3.4352) | Error 0.8800(0.8990) Steps 868(867.37) | Grad Norm 3.0985(2.2947) | Total Time 14.00(14.00)\n",
      "Iter 7790 | Time 21.4320(21.4534) | Bit/dim 3.4397(3.4366) | Xent 2.3026(2.3026) | Loss 3.4397(3.4366) | Error 0.8989(0.8989) Steps 862(866.83) | Grad Norm 2.0335(2.2821) | Total Time 14.00(14.00)\n",
      "Iter 7800 | Time 22.0162(21.4754) | Bit/dim 3.4326(3.4361) | Xent 2.3026(2.3026) | Loss 3.4326(3.4361) | Error 0.9133(0.8994) Steps 880(867.83) | Grad Norm 3.2906(2.3363) | Total Time 14.00(14.00)\n",
      "Iter 7810 | Time 21.7185(21.4481) | Bit/dim 3.4188(3.4382) | Xent 2.3026(2.3026) | Loss 3.4188(3.4382) | Error 0.9100(0.9005) Steps 862(865.80) | Grad Norm 1.9992(2.3030) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 106.8717, Epoch Time 1306.0309(1264.1387), Bit/dim 3.4414(best: 3.4386), Xent 2.3026, Loss 3.4414, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 20.7818(21.4365) | Bit/dim 3.4118(3.4365) | Xent 2.3026(2.3026) | Loss 3.4118(3.4365) | Error 0.9022(0.9007) Steps 850(867.10) | Grad Norm 2.5561(2.3526) | Total Time 14.00(14.00)\n",
      "Iter 7830 | Time 21.1319(21.4116) | Bit/dim 3.4217(3.4351) | Xent 2.3026(2.3026) | Loss 3.4217(3.4351) | Error 0.8922(0.9000) Steps 868(866.26) | Grad Norm 2.0582(2.2481) | Total Time 14.00(14.00)\n",
      "Iter 7840 | Time 21.5074(21.4988) | Bit/dim 3.4051(3.4348) | Xent 2.3026(2.3026) | Loss 3.4051(3.4348) | Error 0.9133(0.8999) Steps 868(865.05) | Grad Norm 3.3977(2.2985) | Total Time 14.00(14.00)\n",
      "Iter 7850 | Time 21.6048(21.5389) | Bit/dim 3.4899(3.4369) | Xent 2.3026(2.3026) | Loss 3.4899(3.4369) | Error 0.9144(0.9004) Steps 868(865.53) | Grad Norm 2.3504(2.3057) | Total Time 14.00(14.00)\n",
      "Iter 7860 | Time 21.6003(21.5909) | Bit/dim 3.4427(3.4361) | Xent 2.3026(2.3026) | Loss 3.4427(3.4361) | Error 0.8978(0.9000) Steps 868(867.23) | Grad Norm 3.0369(2.2226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 106.2614, Epoch Time 1312.4268(1265.5874), Bit/dim 3.4396(best: 3.4386), Xent 2.3026, Loss 3.4396, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 21.4741(21.5958) | Bit/dim 3.4504(3.4376) | Xent 2.3026(2.3026) | Loss 3.4504(3.4376) | Error 0.8989(0.8993) Steps 844(867.08) | Grad Norm 2.1793(2.2127) | Total Time 14.00(14.00)\n",
      "Iter 7880 | Time 21.4252(21.5349) | Bit/dim 3.4241(3.4361) | Xent 2.3026(2.3026) | Loss 3.4241(3.4361) | Error 0.9044(0.8993) Steps 868(868.07) | Grad Norm 2.4707(2.2770) | Total Time 14.00(14.00)\n",
      "Iter 7890 | Time 22.1592(21.5575) | Bit/dim 3.4416(3.4360) | Xent 2.3026(2.3026) | Loss 3.4416(3.4360) | Error 0.8956(0.8983) Steps 874(868.38) | Grad Norm 1.4750(2.1750) | Total Time 14.00(14.00)\n",
      "Iter 7900 | Time 21.3158(21.5608) | Bit/dim 3.4101(3.4332) | Xent 2.3026(2.3026) | Loss 3.4101(3.4332) | Error 0.8978(0.8975) Steps 874(870.32) | Grad Norm 2.6975(2.2876) | Total Time 14.00(14.00)\n",
      "Iter 7910 | Time 22.3980(21.6582) | Bit/dim 3.4271(3.4369) | Xent 2.3026(2.3026) | Loss 3.4271(3.4369) | Error 0.8956(0.9001) Steps 880(871.37) | Grad Norm 2.0069(2.1535) | Total Time 14.00(14.00)\n",
      "Iter 7920 | Time 21.2262(21.6161) | Bit/dim 3.4426(3.4385) | Xent 2.3026(2.3026) | Loss 3.4426(3.4385) | Error 0.9100(0.9013) Steps 874(869.92) | Grad Norm 1.2356(2.2518) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 108.2164, Epoch Time 1313.1210(1267.0134), Bit/dim 3.4376(best: 3.4386), Xent 2.3026, Loss 3.4376, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 22.0668(21.6227) | Bit/dim 3.4310(3.4380) | Xent 2.3026(2.3026) | Loss 3.4310(3.4380) | Error 0.8967(0.9015) Steps 880(871.77) | Grad Norm 2.4766(2.3147) | Total Time 14.00(14.00)\n",
      "Iter 7940 | Time 21.5495(21.5760) | Bit/dim 3.4178(3.4365) | Xent 2.3026(2.3026) | Loss 3.4178(3.4365) | Error 0.9089(0.9009) Steps 880(873.15) | Grad Norm 2.7324(2.2884) | Total Time 14.00(14.00)\n",
      "Iter 7950 | Time 21.6935(21.6082) | Bit/dim 3.4263(3.4379) | Xent 2.3026(2.3026) | Loss 3.4263(3.4379) | Error 0.9133(0.9013) Steps 868(872.03) | Grad Norm 2.1974(2.2114) | Total Time 14.00(14.00)\n",
      "Iter 7960 | Time 21.3156(21.5507) | Bit/dim 3.4306(3.4341) | Xent 2.3026(2.3026) | Loss 3.4306(3.4341) | Error 0.9067(0.8994) Steps 886(872.71) | Grad Norm 1.9949(2.2446) | Total Time 14.00(14.00)\n",
      "Iter 7970 | Time 21.4871(21.5630) | Bit/dim 3.4365(3.4336) | Xent 2.3026(2.3026) | Loss 3.4365(3.4336) | Error 0.8967(0.8997) Steps 886(874.04) | Grad Norm 2.7491(2.1941) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 107.4839, Epoch Time 1312.4579(1268.3767), Bit/dim 3.4408(best: 3.4376), Xent 2.3026, Loss 3.4408, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 21.5633(21.6256) | Bit/dim 3.4362(3.4350) | Xent 2.3026(2.3026) | Loss 3.4362(3.4350) | Error 0.9011(0.9007) Steps 874(875.08) | Grad Norm 2.3429(2.2488) | Total Time 14.00(14.00)\n",
      "Iter 7990 | Time 21.4822(21.6300) | Bit/dim 3.4299(3.4338) | Xent 2.3026(2.3026) | Loss 3.4299(3.4338) | Error 0.8922(0.8992) Steps 856(873.76) | Grad Norm 1.6504(2.2560) | Total Time 14.00(14.00)\n",
      "Iter 8000 | Time 21.9477(21.7023) | Bit/dim 3.4239(3.4347) | Xent 2.3026(2.3026) | Loss 3.4239(3.4347) | Error 0.9078(0.8997) Steps 862(873.79) | Grad Norm 3.1087(2.3470) | Total Time 14.00(14.00)\n",
      "Iter 8010 | Time 21.9084(21.6676) | Bit/dim 3.4126(3.4333) | Xent 2.3026(2.3026) | Loss 3.4126(3.4333) | Error 0.9067(0.9004) Steps 868(874.15) | Grad Norm 2.8751(2.3807) | Total Time 14.00(14.00)\n",
      "Iter 8020 | Time 21.1673(21.5880) | Bit/dim 3.4276(3.4351) | Xent 2.3026(2.3026) | Loss 3.4276(3.4351) | Error 0.9011(0.8997) Steps 880(872.34) | Grad Norm 2.4855(2.3004) | Total Time 14.00(14.00)\n",
      "Iter 8030 | Time 22.0786(21.6233) | Bit/dim 3.4138(3.4351) | Xent 2.3026(2.3026) | Loss 3.4138(3.4351) | Error 0.9078(0.9005) Steps 892(873.09) | Grad Norm 1.4634(2.2605) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 106.7334, Epoch Time 1315.2486(1269.7829), Bit/dim 3.4339(best: 3.4376), Xent 2.3026, Loss 3.4339, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 20.8596(21.6153) | Bit/dim 3.4388(3.4324) | Xent 2.3026(2.3026) | Loss 3.4388(3.4324) | Error 0.8989(0.9005) Steps 868(875.61) | Grad Norm 1.8022(2.2418) | Total Time 14.00(14.00)\n",
      "Iter 8050 | Time 21.9687(21.6198) | Bit/dim 3.4482(3.4339) | Xent 2.3026(2.3026) | Loss 3.4482(3.4339) | Error 0.9011(0.9001) Steps 898(876.69) | Grad Norm 2.6360(2.2642) | Total Time 14.00(14.00)\n",
      "Iter 8060 | Time 21.6680(21.6282) | Bit/dim 3.4263(3.4345) | Xent 2.3026(2.3026) | Loss 3.4263(3.4345) | Error 0.8933(0.8994) Steps 892(876.89) | Grad Norm 2.0959(2.2318) | Total Time 14.00(14.00)\n",
      "Iter 8070 | Time 21.2124(21.6659) | Bit/dim 3.3942(3.4337) | Xent 2.3026(2.3026) | Loss 3.3942(3.4337) | Error 0.8844(0.8982) Steps 874(876.15) | Grad Norm 1.8569(2.3013) | Total Time 14.00(14.00)\n",
      "Iter 8080 | Time 21.4374(21.6920) | Bit/dim 3.4605(3.4331) | Xent 2.3026(2.3026) | Loss 3.4605(3.4331) | Error 0.9156(0.9000) Steps 898(876.43) | Grad Norm 1.8235(2.2444) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 107.4572, Epoch Time 1318.9952(1271.2592), Bit/dim 3.4344(best: 3.4339), Xent 2.3026, Loss 3.4344, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 21.7141(21.6661) | Bit/dim 3.4416(3.4355) | Xent 2.3026(2.3026) | Loss 3.4416(3.4355) | Error 0.9067(0.9011) Steps 874(874.52) | Grad Norm 1.5660(2.2347) | Total Time 14.00(14.00)\n",
      "Iter 8100 | Time 22.4445(21.6872) | Bit/dim 3.4092(3.4345) | Xent 2.3026(2.3026) | Loss 3.4092(3.4345) | Error 0.8867(0.9010) Steps 856(874.58) | Grad Norm 1.8369(2.1810) | Total Time 14.00(14.00)\n",
      "Iter 8110 | Time 21.4699(21.6432) | Bit/dim 3.4413(3.4328) | Xent 2.3026(2.3026) | Loss 3.4413(3.4328) | Error 0.8833(0.9010) Steps 892(873.76) | Grad Norm 2.1704(2.1162) | Total Time 14.00(14.00)\n",
      "Iter 8120 | Time 21.5419(21.6955) | Bit/dim 3.4508(3.4321) | Xent 2.3026(2.3026) | Loss 3.4508(3.4321) | Error 0.8989(0.9007) Steps 886(878.34) | Grad Norm 2.2057(2.1790) | Total Time 14.00(14.00)\n",
      "Iter 8130 | Time 21.7584(21.7431) | Bit/dim 3.4110(3.4315) | Xent 2.3026(2.3026) | Loss 3.4110(3.4315) | Error 0.8922(0.8998) Steps 886(880.00) | Grad Norm 2.7367(2.2883) | Total Time 14.00(14.00)\n",
      "Iter 8140 | Time 20.8969(21.6741) | Bit/dim 3.4186(3.4331) | Xent 2.3026(2.3026) | Loss 3.4186(3.4331) | Error 0.8911(0.8995) Steps 868(879.06) | Grad Norm 1.1480(2.2802) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 108.1819, Epoch Time 1318.2599(1272.6693), Bit/dim 3.4354(best: 3.4339), Xent 2.3026, Loss 3.4354, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 22.5500(21.7202) | Bit/dim 3.4171(3.4324) | Xent 2.3026(2.3026) | Loss 3.4171(3.4324) | Error 0.8911(0.8986) Steps 868(880.19) | Grad Norm 2.1507(2.3058) | Total Time 14.00(14.00)\n",
      "Iter 8160 | Time 22.1991(21.6808) | Bit/dim 3.4174(3.4311) | Xent 2.3026(2.3026) | Loss 3.4174(3.4311) | Error 0.8844(0.8972) Steps 880(881.32) | Grad Norm 2.8491(2.2076) | Total Time 14.00(14.00)\n",
      "Iter 8170 | Time 22.2247(21.7587) | Bit/dim 3.4284(3.4322) | Xent 2.3026(2.3026) | Loss 3.4284(3.4322) | Error 0.9089(0.8991) Steps 886(882.11) | Grad Norm 2.3835(2.2705) | Total Time 14.00(14.00)\n",
      "Iter 8180 | Time 21.8228(21.7732) | Bit/dim 3.4112(3.4324) | Xent 2.3026(2.3026) | Loss 3.4112(3.4324) | Error 0.8722(0.9007) Steps 910(883.43) | Grad Norm 3.1990(2.3452) | Total Time 14.00(14.00)\n",
      "Iter 8190 | Time 22.1601(21.8227) | Bit/dim 3.4134(3.4304) | Xent 2.3026(2.3026) | Loss 3.4134(3.4304) | Error 0.8989(0.9006) Steps 910(886.70) | Grad Norm 2.5399(2.3426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 108.4440, Epoch Time 1326.9344(1274.2972), Bit/dim 3.4324(best: 3.4339), Xent 2.3026, Loss 3.4324, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 21.5438(21.8051) | Bit/dim 3.4511(3.4328) | Xent 2.3026(2.3026) | Loss 3.4511(3.4328) | Error 0.8978(0.9016) Steps 892(887.27) | Grad Norm 2.9259(2.3988) | Total Time 14.00(14.00)\n",
      "Iter 8210 | Time 21.9859(21.8166) | Bit/dim 3.4231(3.4302) | Xent 2.3026(2.3026) | Loss 3.4231(3.4302) | Error 0.9100(0.9017) Steps 916(889.00) | Grad Norm 2.6235(2.3159) | Total Time 14.00(14.00)\n",
      "Iter 8220 | Time 22.3455(21.8578) | Bit/dim 3.4448(3.4293) | Xent 2.3026(2.3026) | Loss 3.4448(3.4293) | Error 0.8922(0.9006) Steps 880(889.84) | Grad Norm 1.6470(2.1635) | Total Time 14.00(14.00)\n",
      "Iter 8230 | Time 22.4286(21.8505) | Bit/dim 3.4308(3.4295) | Xent 2.3026(2.3026) | Loss 3.4308(3.4295) | Error 0.9067(0.9012) Steps 886(890.13) | Grad Norm 1.6218(2.2324) | Total Time 14.00(14.00)\n",
      "Iter 8240 | Time 22.2108(21.9187) | Bit/dim 3.4786(3.4312) | Xent 2.3026(2.3026) | Loss 3.4786(3.4312) | Error 0.9000(0.9000) Steps 886(888.93) | Grad Norm 2.5206(2.1653) | Total Time 14.00(14.00)\n",
      "Iter 8250 | Time 22.4068(21.9584) | Bit/dim 3.4573(3.4321) | Xent 2.3026(2.3026) | Loss 3.4573(3.4321) | Error 0.9044(0.8993) Steps 892(888.55) | Grad Norm 2.3063(2.2678) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 107.0009, Epoch Time 1332.3149(1276.0377), Bit/dim 3.4330(best: 3.4324), Xent 2.3026, Loss 3.4330, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 22.1781(21.9072) | Bit/dim 3.4143(3.4310) | Xent 2.3026(2.3026) | Loss 3.4143(3.4310) | Error 0.8922(0.8993) Steps 862(885.12) | Grad Norm 1.6029(2.3165) | Total Time 14.00(14.00)\n",
      "Iter 8270 | Time 21.8796(21.9363) | Bit/dim 3.4158(3.4303) | Xent 2.3026(2.3026) | Loss 3.4158(3.4303) | Error 0.8867(0.8995) Steps 892(884.92) | Grad Norm 2.2961(2.2914) | Total Time 14.00(14.00)\n",
      "Iter 8280 | Time 22.2966(21.9775) | Bit/dim 3.4248(3.4306) | Xent 2.3026(2.3026) | Loss 3.4248(3.4306) | Error 0.8989(0.9006) Steps 904(883.06) | Grad Norm 2.1476(2.1928) | Total Time 14.00(14.00)\n",
      "Iter 8290 | Time 21.9832(21.9560) | Bit/dim 3.4194(3.4325) | Xent 2.3026(2.3026) | Loss 3.4194(3.4325) | Error 0.8900(0.9008) Steps 880(884.14) | Grad Norm 1.9165(2.2311) | Total Time 14.00(14.00)\n",
      "Iter 8300 | Time 21.9824(21.9180) | Bit/dim 3.4417(3.4317) | Xent 2.3026(2.3026) | Loss 3.4417(3.4317) | Error 0.9000(0.9002) Steps 874(886.47) | Grad Norm 1.8302(2.1662) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 108.5731, Epoch Time 1332.0675(1277.7186), Bit/dim 3.4363(best: 3.4324), Xent 2.3026, Loss 3.4363, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 22.1905(21.9254) | Bit/dim 3.4698(3.4315) | Xent 2.3026(2.3026) | Loss 3.4698(3.4315) | Error 0.9033(0.8985) Steps 886(886.14) | Grad Norm 2.4448(2.2277) | Total Time 14.00(14.00)\n",
      "Iter 8320 | Time 21.9630(21.9672) | Bit/dim 3.4623(3.4296) | Xent 2.3026(2.3026) | Loss 3.4623(3.4296) | Error 0.9011(0.8978) Steps 868(886.59) | Grad Norm 2.1664(2.1676) | Total Time 14.00(14.00)\n",
      "Iter 8330 | Time 21.8985(22.0511) | Bit/dim 3.4482(3.4330) | Xent 2.3026(2.3026) | Loss 3.4482(3.4330) | Error 0.9056(0.8997) Steps 916(889.36) | Grad Norm 2.8893(2.2722) | Total Time 14.00(14.00)\n",
      "Iter 8340 | Time 21.3066(21.9742) | Bit/dim 3.4097(3.4323) | Xent 2.3026(2.3026) | Loss 3.4097(3.4323) | Error 0.8989(0.9009) Steps 868(888.24) | Grad Norm 2.5197(2.2435) | Total Time 14.00(14.00)\n",
      "Iter 8350 | Time 21.2257(21.9472) | Bit/dim 3.4160(3.4302) | Xent 2.3026(2.3026) | Loss 3.4160(3.4302) | Error 0.8967(0.8995) Steps 856(886.83) | Grad Norm 2.1074(2.2147) | Total Time 14.00(14.00)\n",
      "Iter 8360 | Time 21.8975(21.9549) | Bit/dim 3.4046(3.4285) | Xent 2.3026(2.3026) | Loss 3.4046(3.4285) | Error 0.9100(0.9002) Steps 880(887.52) | Grad Norm 2.0391(2.2513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 110.9675, Epoch Time 1339.1491(1279.5615), Bit/dim 3.4354(best: 3.4324), Xent 2.3026, Loss 3.4354, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 22.5246(21.9369) | Bit/dim 3.4412(3.4281) | Xent 2.3026(2.3026) | Loss 3.4412(3.4281) | Error 0.9033(0.9008) Steps 892(887.66) | Grad Norm 2.4133(2.1874) | Total Time 14.00(14.00)\n",
      "Iter 8380 | Time 21.3376(21.9015) | Bit/dim 3.4191(3.4274) | Xent 2.3026(2.3026) | Loss 3.4191(3.4274) | Error 0.8944(0.9011) Steps 904(888.35) | Grad Norm 1.6733(2.1424) | Total Time 14.00(14.00)\n",
      "Iter 8390 | Time 22.1011(21.9564) | Bit/dim 3.4328(3.4294) | Xent 2.3026(2.3026) | Loss 3.4328(3.4294) | Error 0.9156(0.9001) Steps 886(890.07) | Grad Norm 1.2832(2.1580) | Total Time 14.00(14.00)\n",
      "Iter 8400 | Time 21.6142(21.9556) | Bit/dim 3.4028(3.4281) | Xent 2.3026(2.3026) | Loss 3.4028(3.4281) | Error 0.8756(0.8991) Steps 898(891.59) | Grad Norm 2.3272(2.2392) | Total Time 14.00(14.00)\n",
      "Iter 8410 | Time 22.1361(22.0225) | Bit/dim 3.4022(3.4302) | Xent 2.3026(2.3026) | Loss 3.4022(3.4302) | Error 0.8789(0.8997) Steps 910(893.05) | Grad Norm 1.4613(2.2825) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 109.0533, Epoch Time 1337.6580(1281.3044), Bit/dim 3.4313(best: 3.4324), Xent 2.3026, Loss 3.4313, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 21.4918(22.0098) | Bit/dim 3.4115(3.4281) | Xent 2.3026(2.3026) | Loss 3.4115(3.4281) | Error 0.8978(0.8988) Steps 910(895.12) | Grad Norm 2.5273(2.2249) | Total Time 14.00(14.00)\n",
      "Iter 8430 | Time 22.2904(22.0226) | Bit/dim 3.4125(3.4287) | Xent 2.3026(2.3026) | Loss 3.4125(3.4287) | Error 0.9133(0.9007) Steps 886(896.90) | Grad Norm 1.5199(2.1567) | Total Time 14.00(14.00)\n",
      "Iter 8440 | Time 22.0426(22.0725) | Bit/dim 3.4320(3.4254) | Xent 2.3026(2.3026) | Loss 3.4320(3.4254) | Error 0.9056(0.9002) Steps 922(899.18) | Grad Norm 3.3090(2.1989) | Total Time 14.00(14.00)\n",
      "Iter 8450 | Time 22.5157(22.1219) | Bit/dim 3.4258(3.4292) | Xent 2.3026(2.3026) | Loss 3.4258(3.4292) | Error 0.8933(0.8998) Steps 880(899.09) | Grad Norm 1.9278(2.1503) | Total Time 14.00(14.00)\n",
      "Iter 8460 | Time 21.9954(22.1344) | Bit/dim 3.4401(3.4309) | Xent 2.3026(2.3026) | Loss 3.4401(3.4309) | Error 0.8956(0.9000) Steps 904(900.72) | Grad Norm 1.6917(2.1538) | Total Time 14.00(14.00)\n",
      "Iter 8470 | Time 22.3664(22.1577) | Bit/dim 3.3814(3.4268) | Xent 2.3026(2.3026) | Loss 3.3814(3.4268) | Error 0.8800(0.8996) Steps 892(901.60) | Grad Norm 2.6347(2.2092) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 108.4296, Epoch Time 1345.0546(1283.2169), Bit/dim 3.4352(best: 3.4313), Xent 2.3026, Loss 3.4352, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 22.3285(22.1217) | Bit/dim 3.4136(3.4269) | Xent 2.3026(2.3026) | Loss 3.4136(3.4269) | Error 0.9022(0.9000) Steps 904(902.29) | Grad Norm 1.6421(2.2868) | Total Time 14.00(14.00)\n",
      "Iter 8490 | Time 22.1233(22.1066) | Bit/dim 3.3908(3.4264) | Xent 2.3026(2.3026) | Loss 3.3908(3.4264) | Error 0.8889(0.8995) Steps 922(905.66) | Grad Norm 2.5151(2.2100) | Total Time 14.00(14.00)\n",
      "Iter 8500 | Time 22.3033(22.1113) | Bit/dim 3.3987(3.4265) | Xent 2.3026(2.3026) | Loss 3.3987(3.4265) | Error 0.9089(0.9009) Steps 898(904.40) | Grad Norm 1.5984(2.1611) | Total Time 14.00(14.00)\n",
      "Iter 8510 | Time 22.3895(22.1344) | Bit/dim 3.4358(3.4248) | Xent 2.3026(2.3026) | Loss 3.4358(3.4248) | Error 0.9000(0.9003) Steps 898(902.74) | Grad Norm 2.1202(2.2123) | Total Time 14.00(14.00)\n",
      "Iter 8520 | Time 22.5759(22.1228) | Bit/dim 3.4270(3.4267) | Xent 2.3026(2.3026) | Loss 3.4270(3.4267) | Error 0.9033(0.8994) Steps 856(901.41) | Grad Norm 2.4358(2.1803) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 111.6568, Epoch Time 1344.7613(1285.0633), Bit/dim 3.4345(best: 3.4313), Xent 2.3026, Loss 3.4345, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 21.8204(22.0971) | Bit/dim 3.4450(3.4290) | Xent 2.3026(2.3026) | Loss 3.4450(3.4290) | Error 0.9044(0.8998) Steps 892(899.98) | Grad Norm 2.3833(2.2490) | Total Time 14.00(14.00)\n",
      "Iter 8540 | Time 22.1347(22.1009) | Bit/dim 3.4082(3.4282) | Xent 2.3026(2.3026) | Loss 3.4082(3.4282) | Error 0.8833(0.8974) Steps 892(898.77) | Grad Norm 1.9156(2.2088) | Total Time 14.00(14.00)\n",
      "Iter 8550 | Time 22.1086(22.0751) | Bit/dim 3.4094(3.4270) | Xent 2.3026(2.3026) | Loss 3.4094(3.4270) | Error 0.9011(0.8985) Steps 886(900.36) | Grad Norm 2.2787(2.2066) | Total Time 14.00(14.00)\n",
      "Iter 8560 | Time 22.2531(22.1007) | Bit/dim 3.4564(3.4288) | Xent 2.3026(2.3026) | Loss 3.4564(3.4288) | Error 0.8989(0.8994) Steps 916(901.79) | Grad Norm 2.1783(2.2273) | Total Time 14.00(14.00)\n",
      "Iter 8570 | Time 22.2046(22.1321) | Bit/dim 3.4386(3.4272) | Xent 2.3026(2.3026) | Loss 3.4386(3.4272) | Error 0.9111(0.9000) Steps 904(902.23) | Grad Norm 3.1012(2.2384) | Total Time 14.00(14.00)\n",
      "Iter 8580 | Time 21.9213(22.1838) | Bit/dim 3.4214(3.4261) | Xent 2.3026(2.3026) | Loss 3.4214(3.4261) | Error 0.9111(0.9005) Steps 904(904.77) | Grad Norm 1.4651(2.2087) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 112.0331, Epoch Time 1348.3797(1286.9628), Bit/dim 3.4286(best: 3.4313), Xent 2.3026, Loss 3.4286, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 21.9754(22.1603) | Bit/dim 3.4417(3.4275) | Xent 2.3026(2.3026) | Loss 3.4417(3.4275) | Error 0.8833(0.8996) Steps 922(904.65) | Grad Norm 1.2891(2.2644) | Total Time 14.00(14.00)\n",
      "Iter 8600 | Time 21.8165(22.1019) | Bit/dim 3.4275(3.4268) | Xent 2.3026(2.3026) | Loss 3.4275(3.4268) | Error 0.9100(0.9005) Steps 922(905.90) | Grad Norm 1.9901(2.1338) | Total Time 14.00(14.00)\n",
      "Iter 8610 | Time 22.5641(22.1432) | Bit/dim 3.4488(3.4268) | Xent 2.3026(2.3026) | Loss 3.4488(3.4268) | Error 0.9089(0.8997) Steps 910(906.42) | Grad Norm 2.0161(2.1909) | Total Time 14.00(14.00)\n",
      "Iter 8620 | Time 21.8092(22.1698) | Bit/dim 3.4368(3.4257) | Xent 2.3026(2.3026) | Loss 3.4368(3.4257) | Error 0.9100(0.9004) Steps 856(903.69) | Grad Norm 1.6210(2.1972) | Total Time 14.00(14.00)\n",
      "Iter 8630 | Time 22.6184(22.2194) | Bit/dim 3.4149(3.4257) | Xent 2.3026(2.3026) | Loss 3.4149(3.4257) | Error 0.8967(0.9005) Steps 898(903.14) | Grad Norm 2.2553(2.1566) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 111.0170, Epoch Time 1349.4414(1288.8371), Bit/dim 3.4284(best: 3.4286), Xent 2.3026, Loss 3.4284, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 22.1773(22.2020) | Bit/dim 3.3840(3.4230) | Xent 2.3026(2.3026) | Loss 3.3840(3.4230) | Error 0.8933(0.8990) Steps 916(902.92) | Grad Norm 1.9986(2.1933) | Total Time 14.00(14.00)\n",
      "Iter 8650 | Time 22.2442(22.1943) | Bit/dim 3.4364(3.4268) | Xent 2.3026(2.3026) | Loss 3.4364(3.4268) | Error 0.9111(0.9001) Steps 916(906.93) | Grad Norm 1.6617(2.1278) | Total Time 14.00(14.00)\n",
      "Iter 8660 | Time 22.1185(22.1970) | Bit/dim 3.4217(3.4279) | Xent 2.3026(2.3026) | Loss 3.4217(3.4279) | Error 0.8989(0.9002) Steps 922(909.55) | Grad Norm 2.7028(2.0706) | Total Time 14.00(14.00)\n",
      "Iter 8670 | Time 21.8626(22.1564) | Bit/dim 3.4503(3.4257) | Xent 2.3026(2.3026) | Loss 3.4503(3.4257) | Error 0.9222(0.9002) Steps 886(909.22) | Grad Norm 2.9094(2.1214) | Total Time 14.00(14.00)\n",
      "Iter 8680 | Time 22.5200(22.1320) | Bit/dim 3.4196(3.4234) | Xent 2.3026(2.3026) | Loss 3.4196(3.4234) | Error 0.9122(0.9012) Steps 922(909.26) | Grad Norm 2.5205(2.1585) | Total Time 14.00(14.00)\n",
      "Iter 8690 | Time 21.6519(22.1597) | Bit/dim 3.4101(3.4238) | Xent 2.3026(2.3026) | Loss 3.4101(3.4238) | Error 0.9122(0.9005) Steps 904(908.86) | Grad Norm 2.6520(2.1628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 111.4586, Epoch Time 1347.3717(1290.5932), Bit/dim 3.4311(best: 3.4284), Xent 2.3026, Loss 3.4311, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 22.1627(22.1595) | Bit/dim 3.3987(3.4236) | Xent 2.3026(2.3026) | Loss 3.3987(3.4236) | Error 0.8878(0.9003) Steps 928(909.02) | Grad Norm 1.9987(2.2554) | Total Time 14.00(14.00)\n",
      "Iter 8710 | Time 21.8893(22.1107) | Bit/dim 3.4172(3.4226) | Xent 2.3026(2.3026) | Loss 3.4172(3.4226) | Error 0.8956(0.9005) Steps 898(908.68) | Grad Norm 2.1314(2.1081) | Total Time 14.00(14.00)\n",
      "Iter 8720 | Time 21.8218(22.2002) | Bit/dim 3.4153(3.4243) | Xent 2.3026(2.3026) | Loss 3.4153(3.4243) | Error 0.9156(0.9018) Steps 898(909.94) | Grad Norm 1.8485(2.1374) | Total Time 14.00(14.00)\n",
      "Iter 8730 | Time 21.6593(22.1951) | Bit/dim 3.4267(3.4219) | Xent 2.3026(2.3026) | Loss 3.4267(3.4219) | Error 0.9033(0.9002) Steps 916(905.35) | Grad Norm 3.0857(2.1174) | Total Time 14.00(14.00)\n",
      "Iter 8740 | Time 22.8630(22.2034) | Bit/dim 3.4506(3.4228) | Xent 2.3026(2.3026) | Loss 3.4506(3.4228) | Error 0.9067(0.9006) Steps 904(905.38) | Grad Norm 2.4910(2.1137) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 111.5438, Epoch Time 1350.6434(1292.3947), Bit/dim 3.4281(best: 3.4284), Xent 2.3026, Loss 3.4281, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 23.0412(22.2730) | Bit/dim 3.4092(3.4263) | Xent 2.3026(2.3026) | Loss 3.4092(3.4263) | Error 0.9022(0.8988) Steps 928(906.81) | Grad Norm 2.3286(2.0935) | Total Time 14.00(14.00)\n",
      "Iter 8760 | Time 22.3150(22.2973) | Bit/dim 3.4059(3.4241) | Xent 2.3026(2.3026) | Loss 3.4059(3.4241) | Error 0.8944(0.8982) Steps 934(907.41) | Grad Norm 3.3967(2.2464) | Total Time 14.00(14.00)\n",
      "Iter 8770 | Time 22.4498(22.3033) | Bit/dim 3.4411(3.4239) | Xent 2.3026(2.3026) | Loss 3.4411(3.4239) | Error 0.9144(0.8999) Steps 916(909.23) | Grad Norm 2.2437(2.2331) | Total Time 14.00(14.00)\n",
      "Iter 8780 | Time 22.1988(22.3376) | Bit/dim 3.4183(3.4251) | Xent 2.3026(2.3026) | Loss 3.4183(3.4251) | Error 0.9089(0.8996) Steps 928(909.94) | Grad Norm 1.4245(2.1656) | Total Time 14.00(14.00)\n",
      "Iter 8790 | Time 22.4795(22.2970) | Bit/dim 3.4423(3.4253) | Xent 2.3026(2.3026) | Loss 3.4423(3.4253) | Error 0.9122(0.9017) Steps 910(911.29) | Grad Norm 1.3799(2.1886) | Total Time 14.00(14.00)\n",
      "Iter 8800 | Time 22.2775(22.3372) | Bit/dim 3.4003(3.4232) | Xent 2.3026(2.3026) | Loss 3.4003(3.4232) | Error 0.9078(0.9000) Steps 904(912.89) | Grad Norm 2.8369(2.3014) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 112.8562, Epoch Time 1361.8244(1294.4776), Bit/dim 3.4241(best: 3.4281), Xent 2.3026, Loss 3.4241, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 22.3311(22.3956) | Bit/dim 3.4177(3.4241) | Xent 2.3026(2.3026) | Loss 3.4177(3.4241) | Error 0.8867(0.8993) Steps 922(916.29) | Grad Norm 2.1557(2.2596) | Total Time 14.00(14.00)\n",
      "Iter 8820 | Time 22.8468(22.4196) | Bit/dim 3.4010(3.4229) | Xent 2.3026(2.3026) | Loss 3.4010(3.4229) | Error 0.9067(0.9009) Steps 904(914.67) | Grad Norm 1.7035(2.1517) | Total Time 14.00(14.00)\n",
      "Iter 8830 | Time 22.9792(22.4924) | Bit/dim 3.4320(3.4225) | Xent 2.3026(2.3026) | Loss 3.4320(3.4225) | Error 0.8967(0.9013) Steps 910(915.43) | Grad Norm 1.8712(2.1740) | Total Time 14.00(14.00)\n",
      "Iter 8840 | Time 22.3542(22.4978) | Bit/dim 3.4274(3.4213) | Xent 2.3026(2.3026) | Loss 3.4274(3.4213) | Error 0.8933(0.8995) Steps 898(916.39) | Grad Norm 2.8176(2.2016) | Total Time 14.00(14.00)\n",
      "Iter 8850 | Time 22.5843(22.5143) | Bit/dim 3.4613(3.4231) | Xent 2.3026(2.3026) | Loss 3.4613(3.4231) | Error 0.9000(0.8998) Steps 922(914.01) | Grad Norm 2.4589(2.1488) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 110.9181, Epoch Time 1369.0042(1296.7134), Bit/dim 3.4321(best: 3.4241), Xent 2.3026, Loss 3.4321, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 21.4257(22.4556) | Bit/dim 3.4764(3.4250) | Xent 2.3026(2.3026) | Loss 3.4764(3.4250) | Error 0.9022(0.9001) Steps 910(912.82) | Grad Norm 2.2305(2.1755) | Total Time 14.00(14.00)\n",
      "Iter 8870 | Time 21.6931(22.3833) | Bit/dim 3.3870(3.4231) | Xent 2.3026(2.3026) | Loss 3.3870(3.4231) | Error 0.9133(0.8998) Steps 928(913.72) | Grad Norm 2.8904(2.2759) | Total Time 14.00(14.00)\n",
      "Iter 8880 | Time 21.7273(22.3452) | Bit/dim 3.4444(3.4244) | Xent 2.3026(2.3026) | Loss 3.4444(3.4244) | Error 0.9044(0.9002) Steps 904(914.35) | Grad Norm 2.1353(2.2254) | Total Time 14.00(14.00)\n",
      "Iter 8890 | Time 22.6509(22.3356) | Bit/dim 3.4154(3.4246) | Xent 2.3026(2.3026) | Loss 3.4154(3.4246) | Error 0.8922(0.9008) Steps 946(916.85) | Grad Norm 2.3992(2.1796) | Total Time 14.00(14.00)\n",
      "Iter 8900 | Time 21.9952(22.3376) | Bit/dim 3.3914(3.4221) | Xent 2.3026(2.3026) | Loss 3.3914(3.4221) | Error 0.8933(0.8994) Steps 928(914.73) | Grad Norm 1.3019(2.0676) | Total Time 14.00(14.00)\n",
      "Iter 8910 | Time 21.6854(22.2995) | Bit/dim 3.4183(3.4214) | Xent 2.3026(2.3026) | Loss 3.4183(3.4214) | Error 0.9078(0.8995) Steps 904(911.81) | Grad Norm 1.7903(2.0283) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 112.9332, Epoch Time 1355.2824(1298.4704), Bit/dim 3.4262(best: 3.4241), Xent 2.3026, Loss 3.4262, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 22.6674(22.3176) | Bit/dim 3.4014(3.4204) | Xent 2.3026(2.3026) | Loss 3.4014(3.4204) | Error 0.9033(0.8993) Steps 922(911.65) | Grad Norm 2.5793(2.1528) | Total Time 14.00(14.00)\n",
      "Iter 8930 | Time 22.7345(22.3242) | Bit/dim 3.4044(3.4241) | Xent 2.3026(2.3026) | Loss 3.4044(3.4241) | Error 0.8933(0.9005) Steps 922(912.39) | Grad Norm 1.4680(1.9548) | Total Time 14.00(14.00)\n",
      "Iter 8940 | Time 22.7560(22.3354) | Bit/dim 3.3855(3.4228) | Xent 2.3026(2.3026) | Loss 3.3855(3.4228) | Error 0.8867(0.8999) Steps 922(914.18) | Grad Norm 2.1206(2.0581) | Total Time 14.00(14.00)\n",
      "Iter 8950 | Time 22.9212(22.3394) | Bit/dim 3.4209(3.4224) | Xent 2.3026(2.3026) | Loss 3.4209(3.4224) | Error 0.8978(0.9007) Steps 940(917.09) | Grad Norm 3.2629(2.1757) | Total Time 14.00(14.00)\n",
      "Iter 8960 | Time 21.4170(22.2865) | Bit/dim 3.3908(3.4203) | Xent 2.3026(2.3026) | Loss 3.3908(3.4203) | Error 0.8967(0.8999) Steps 922(918.05) | Grad Norm 2.9579(2.1486) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 114.2128, Epoch Time 1357.1447(1300.2307), Bit/dim 3.4216(best: 3.4241), Xent 2.3026, Loss 3.4216, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 22.6295(22.2775) | Bit/dim 3.4438(3.4204) | Xent 2.3026(2.3026) | Loss 3.4438(3.4204) | Error 0.9089(0.8997) Steps 910(917.08) | Grad Norm 2.3553(2.2068) | Total Time 14.00(14.00)\n",
      "Iter 8980 | Time 22.4495(22.3579) | Bit/dim 3.4129(3.4203) | Xent 2.3026(2.3026) | Loss 3.4129(3.4203) | Error 0.8989(0.8992) Steps 940(916.66) | Grad Norm 2.1543(2.2405) | Total Time 14.00(14.00)\n",
      "Iter 8990 | Time 22.4673(22.3210) | Bit/dim 3.4099(3.4204) | Xent 2.3026(2.3026) | Loss 3.4099(3.4204) | Error 0.9156(0.8982) Steps 910(917.03) | Grad Norm 2.0569(2.0952) | Total Time 14.00(14.00)\n",
      "Iter 9000 | Time 22.4776(22.3490) | Bit/dim 3.4322(3.4235) | Xent 2.3026(2.3026) | Loss 3.4322(3.4235) | Error 0.8989(0.8989) Steps 922(918.04) | Grad Norm 2.1467(2.1797) | Total Time 14.00(14.00)\n",
      "Iter 9010 | Time 23.2822(22.3454) | Bit/dim 3.4079(3.4214) | Xent 2.3026(2.3026) | Loss 3.4079(3.4214) | Error 0.8933(0.8994) Steps 940(918.71) | Grad Norm 2.2315(2.1587) | Total Time 14.00(14.00)\n",
      "Iter 9020 | Time 22.8417(22.3316) | Bit/dim 3.4100(3.4215) | Xent 2.3026(2.3026) | Loss 3.4100(3.4215) | Error 0.9067(0.9012) Steps 922(919.84) | Grad Norm 1.9865(2.1504) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 112.0082, Epoch Time 1361.7387(1302.0759), Bit/dim 3.4243(best: 3.4216), Xent 2.3026, Loss 3.4243, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 22.6174(22.3516) | Bit/dim 3.4229(3.4208) | Xent 2.3026(2.3026) | Loss 3.4229(3.4208) | Error 0.9078(0.9011) Steps 922(920.89) | Grad Norm 3.0580(2.1205) | Total Time 14.00(14.00)\n",
      "Iter 9040 | Time 22.0351(22.3855) | Bit/dim 3.4162(3.4225) | Xent 2.3026(2.3026) | Loss 3.4162(3.4225) | Error 0.8767(0.9011) Steps 922(922.42) | Grad Norm 1.5863(2.1509) | Total Time 14.00(14.00)\n",
      "Iter 9050 | Time 21.8633(22.3404) | Bit/dim 3.4054(3.4202) | Xent 2.3026(2.3026) | Loss 3.4054(3.4202) | Error 0.9000(0.9005) Steps 880(918.66) | Grad Norm 2.4686(2.1403) | Total Time 14.00(14.00)\n",
      "Iter 9060 | Time 22.7434(22.3874) | Bit/dim 3.4137(3.4213) | Xent 2.3026(2.3026) | Loss 3.4137(3.4213) | Error 0.8956(0.9004) Steps 922(918.76) | Grad Norm 2.8699(2.2010) | Total Time 14.00(14.00)\n",
      "Iter 9070 | Time 22.2960(22.3381) | Bit/dim 3.4275(3.4218) | Xent 2.3026(2.3026) | Loss 3.4275(3.4218) | Error 0.9044(0.8999) Steps 898(917.02) | Grad Norm 1.3564(2.1223) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 113.3286, Epoch Time 1359.8831(1303.8101), Bit/dim 3.4279(best: 3.4216), Xent 2.3026, Loss 3.4279, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 22.2191(22.3625) | Bit/dim 3.3818(3.4193) | Xent 2.3026(2.3026) | Loss 3.3818(3.4193) | Error 0.9000(0.9001) Steps 934(916.59) | Grad Norm 2.5731(2.2265) | Total Time 14.00(14.00)\n",
      "Iter 9090 | Time 23.3108(22.3476) | Bit/dim 3.4173(3.4176) | Xent 2.3026(2.3026) | Loss 3.4173(3.4176) | Error 0.8867(0.8999) Steps 916(916.25) | Grad Norm 3.1718(2.3067) | Total Time 14.00(14.00)\n",
      "Iter 9100 | Time 23.2055(22.4121) | Bit/dim 3.4132(3.4190) | Xent 2.3026(2.3026) | Loss 3.4132(3.4190) | Error 0.9089(0.9010) Steps 892(916.08) | Grad Norm 1.6927(2.2117) | Total Time 14.00(14.00)\n",
      "Iter 9110 | Time 22.8763(22.4557) | Bit/dim 3.4447(3.4191) | Xent 2.3026(2.3026) | Loss 3.4447(3.4191) | Error 0.8967(0.9005) Steps 940(919.07) | Grad Norm 2.2516(2.1612) | Total Time 14.00(14.00)\n",
      "Iter 9120 | Time 22.1805(22.4926) | Bit/dim 3.4253(3.4201) | Xent 2.3026(2.3026) | Loss 3.4253(3.4201) | Error 0.8944(0.8997) Steps 898(918.56) | Grad Norm 2.5493(2.0864) | Total Time 14.00(14.00)\n",
      "Iter 9130 | Time 22.7647(22.5431) | Bit/dim 3.4183(3.4189) | Xent 2.3026(2.3026) | Loss 3.4183(3.4189) | Error 0.9011(0.8996) Steps 934(918.82) | Grad Norm 1.3558(2.1325) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 112.8713, Epoch Time 1371.5623(1305.8427), Bit/dim 3.4205(best: 3.4216), Xent 2.3026, Loss 3.4205, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 22.8434(22.4900) | Bit/dim 3.4238(3.4184) | Xent 2.3026(2.3026) | Loss 3.4238(3.4184) | Error 0.9056(0.8988) Steps 910(916.76) | Grad Norm 2.4548(2.1188) | Total Time 14.00(14.00)\n",
      "Iter 9150 | Time 22.9242(22.4642) | Bit/dim 3.3994(3.4191) | Xent 2.3026(2.3026) | Loss 3.3994(3.4191) | Error 0.8989(0.8987) Steps 916(916.03) | Grad Norm 2.8272(2.0485) | Total Time 14.00(14.00)\n",
      "Iter 9160 | Time 22.1223(22.4944) | Bit/dim 3.3998(3.4174) | Xent 2.3026(2.3026) | Loss 3.3998(3.4174) | Error 0.9144(0.8996) Steps 904(916.01) | Grad Norm 2.3946(2.0898) | Total Time 14.00(14.00)\n",
      "Iter 9170 | Time 22.4631(22.4906) | Bit/dim 3.4279(3.4200) | Xent 2.3026(2.3026) | Loss 3.4279(3.4200) | Error 0.9156(0.9005) Steps 922(918.73) | Grad Norm 2.9303(2.2659) | Total Time 14.00(14.00)\n",
      "Iter 9180 | Time 22.3862(22.4695) | Bit/dim 3.4536(3.4188) | Xent 2.3026(2.3026) | Loss 3.4536(3.4188) | Error 0.9144(0.9007) Steps 940(922.96) | Grad Norm 2.3547(2.2715) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 112.5843, Epoch Time 1364.5452(1307.6038), Bit/dim 3.4220(best: 3.4205), Xent 2.3026, Loss 3.4220, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 21.8031(22.4440) | Bit/dim 3.4278(3.4200) | Xent 2.3026(2.3026) | Loss 3.4278(3.4200) | Error 0.8989(0.9004) Steps 928(924.69) | Grad Norm 2.6516(2.2139) | Total Time 14.00(14.00)\n",
      "Iter 9200 | Time 22.7038(22.3847) | Bit/dim 3.4371(3.4186) | Xent 2.3026(2.3026) | Loss 3.4371(3.4186) | Error 0.8867(0.8993) Steps 922(922.02) | Grad Norm 1.5274(2.1315) | Total Time 14.00(14.00)\n",
      "Iter 9210 | Time 22.3379(22.4244) | Bit/dim 3.4189(3.4201) | Xent 2.3026(2.3026) | Loss 3.4189(3.4201) | Error 0.9189(0.9014) Steps 940(921.94) | Grad Norm 2.6238(2.1259) | Total Time 14.00(14.00)\n",
      "Iter 9220 | Time 22.7301(22.4330) | Bit/dim 3.4304(3.4183) | Xent 2.3026(2.3026) | Loss 3.4304(3.4183) | Error 0.9011(0.8998) Steps 946(923.06) | Grad Norm 1.9215(2.0146) | Total Time 14.00(14.00)\n",
      "Iter 9230 | Time 22.2334(22.4804) | Bit/dim 3.4215(3.4187) | Xent 2.3026(2.3026) | Loss 3.4215(3.4187) | Error 0.9222(0.9007) Steps 910(921.25) | Grad Norm 1.7207(2.0818) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_0625_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.0625 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
