{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_rl_multiscale.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "plt.rcParams['figure.dpi'] = 300\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time, count_nfe_gate\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "GATES = [\"cnn1\", \"cnn2\", \"rnn\"]\n",
      "\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"colormnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--gate', type=str, default='cnn1', choices=GATES)\n",
      "parser.add_argument('--scale', type=float, default=1.0)\n",
      "parser.add_argument('--scale_fac', type=float, default=1.0)\n",
      "parser.add_argument('--scale_std', type=float, default=1.0)\n",
      "parser.add_argument('--eta', default=0.1, type=float,\n",
      "                        help='tuning parameter that allows us to trade-off the competing goals of' \n",
      "                                'minimizing the prediction loss and maximizing the gate rewards ')\n",
      "parser.add_argument('--rl-weight', default=0.01, type=float,\n",
      "                        help='rl weight')\n",
      "\n",
      "parser.add_argument('--gamma', default=0.99, type=float,\n",
      "                        help='discount factor, default: (0.99)')\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=500)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--annealing_std\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "import lib.odenvp_conditional_rl_multiscale as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "        \n",
      "def update_scale_std(model, epoch):\n",
      "    epoch_frac = 1.0 - float(epoch - 1) / max(args.num_epochs + 1, 1)\n",
      "    scale_std = args.scale_std * epoch_frac\n",
      "    model.set_scale_std(scale_std)\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z_sup, z_unsup, delta_logp, atol, rtol, logp_actions, nfe = model(x, zero)  # run model forward\n",
      "    \n",
      "    z_sup = torch.cat(z_sup, 1)\n",
      "    z_unsup = torch.cat(z_unsup, 1)\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z_sup).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z_unsup).view(z_unsup.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z_sup = model.module.dropout(z_sup)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z_sup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, atol, rtol, logp_actions, nfe\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol, \"scale\": args.scale, \"scale_fac\": args.scale_fac, \"scale_std\": args.scale_std, \"gate\": args.gate},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            \n",
      "            a_sup = fixed_z_sup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            a_unsup = fixed_z_unsup.shape[1] // (2**(model.module.n_scale - 1))\n",
      "            \n",
      "            fixed_z = []\n",
      "            start_sup = 0; start_unsup = 0\n",
      "            for ns in range(model.module.n_scale, 1, -1):\n",
      "                end_sup = start_sup + (2**(ns-2))*a_sup\n",
      "                end_unsup = start_unsup + (2**(ns-2))*a_unsup\n",
      "                \n",
      "                fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "                fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "                \n",
      "                start_sup = end_sup; start_unsup = end_unsup\n",
      "            \n",
      "            end_sup = start_sup + a_sup\n",
      "            end_unsup = start_unsup + a_unsup\n",
      "            \n",
      "            fixed_z.append(fixed_z_sup[:,start_sup:end_sup])\n",
      "            fixed_z.append(fixed_z_unsup[:,start_unsup:end_unsup])\n",
      "            \n",
      "            # for i_z in range(len(fixed_z)): print(fixed_z[i_z].shape)\n",
      "            \n",
      "            fixed_z = torch.cat(fixed_z,1)\n",
      "            \n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        if args.annealing_std:\n",
      "            update_scale_std(model.module, epoch)\n",
      "            \n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            if args.data == \"colormnist\":\n",
      "                y = y[0]\n",
      "            \n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            # re-weight the gate rewards\n",
      "            normalized_eta = args.eta / len(logp_actions)\n",
      "            \n",
      "            # collect cumulative future rewards\n",
      "            R = - loss\n",
      "            cum_rewards = []\n",
      "            for r in nfe[::-1]:\n",
      "                R = -normalized_eta * r.view(-1,1) + args.gamma * R\n",
      "                cum_rewards.insert(0,R)\n",
      "            \n",
      "            # apply REINFORCE\n",
      "            rl_loss = 0\n",
      "            for lpa, r in zip(logp_actions, cum_rewards):\n",
      "                rl_loss = rl_loss - lpa.view(-1,1) * args.rl_weight * r\n",
      "                \n",
      "            loss = loss + rl_loss.mean()\n",
      "            \n",
      "            loss.backward()\n",
      "            \n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe_gate(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            for idx in range(len(model.module.transforms)):\n",
      "                for layer in model.module.transforms[idx].chain:\n",
      "                    if hasattr(layer, 'atol'):\n",
      "                        layer.odefunc.after_odeint()\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'train': atol[tol_indx].mean()}, itr)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'train': rtol[tol_indx].mean()}, itr)\n",
      "                    \n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "            \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, atol, rtol, logp_actions, nfe = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                    \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                for tol_indx in range(len(atol)):\n",
      "                    writer.add_scalars('atol_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                    writer.add_scalars('rtol_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples, atol, rtol, logp_actions, nfe = model(fixed_z, reverse=True)\n",
      "            generated_samples = generated_samples.view(-1, *data_shape)\n",
      "            for tol_indx in range(len(atol)):\n",
      "                writer.add_scalars('atol_gen_%i'%tol_indx, {'validation': atol[tol_indx].mean()}, epoch)\n",
      "                writer.add_scalars('rtol_gen_%i'%tol_indx, {'validation': rtol[tol_indx].mean()}, epoch)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, annealing_std=False, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, eta=0.1, gamma=0.99, gate='cnn2', imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=20.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=500, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_multiscale_run2/epoch_117_checkpt.pth', rl_weight=0.01, rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_multiscale_run2', scale=1.0, scale_fac=1.0, scale_std=15.0, seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(6, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(12, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(48, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF_Gate(\n",
      "          (gate_net): FeedforwardGateII(\n",
      "            (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "            (conv1): Conv2d(24, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "            (bn1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu1): ReLU(inplace)\n",
      "            (conv2): Conv2d(10, 10, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "            (bn2): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            (relu2): ReLU(inplace)\n",
      "            (avg_layer): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "            (linear_layer): Conv2d(10, 2, kernel_size=(1, 1), stride=(1, 1))\n",
      "            (sigmoid): Sigmoid()\n",
      "          )\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc_rl(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1450886\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc_rl.py:291: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Iter 0703 | Time 116.8682(59.9272) | Bit/dim 3.7775(3.8421) | Xent 1.1508(1.3151) | Loss 12.5254(11.1882) | Error 0.4136(0.4687) Steps 580(615.62) | Grad Norm 3.5979(6.8253) | Total Time 0.00(0.00)\n",
      "Iter 0704 | Time 60.2510(59.9369) | Bit/dim 3.7778(3.8402) | Xent 1.1636(1.3106) | Loss 10.1056(11.1557) | Error 0.4146(0.4671) Steps 592(614.91) | Grad Norm 2.8647(6.7065) | Total Time 0.00(0.00)\n",
      "Iter 0705 | Time 57.4169(59.8613) | Bit/dim 3.7774(3.8383) | Xent 1.1473(1.3057) | Loss 10.0770(11.1233) | Error 0.4099(0.4654) Steps 592(614.22) | Grad Norm 1.7408(6.5575) | Total Time 0.00(0.00)\n",
      "Iter 0706 | Time 58.8919(59.8322) | Bit/dim 3.7749(3.8364) | Xent 1.1334(1.3005) | Loss 10.1267(11.0934) | Error 0.4020(0.4635) Steps 616(614.28) | Grad Norm 1.7071(6.4120) | Total Time 0.00(0.00)\n",
      "Iter 0707 | Time 58.3192(59.7868) | Bit/dim 3.7705(3.8344) | Xent 1.1579(1.2962) | Loss 10.3776(11.0720) | Error 0.4116(0.4619) Steps 598(613.79) | Grad Norm 2.6149(6.2981) | Total Time 0.00(0.00)\n",
      "Iter 0708 | Time 58.4696(59.7473) | Bit/dim 3.7631(3.8323) | Xent 1.1427(1.2916) | Loss 10.1058(11.0430) | Error 0.4021(0.4601) Steps 598(613.31) | Grad Norm 3.1775(6.2045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 40.4946, Epoch Time 467.2371(388.3145), Bit/dim 3.7675(best: inf), Xent 1.1486, Loss 4.3418, Error 0.4127(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 65.0407(59.9061) | Bit/dim 3.7764(3.8306) | Xent 1.1379(1.2870) | Loss 13.7511(11.1242) | Error 0.4075(0.4586) Steps 586(612.49) | Grad Norm 1.6918(6.0691) | Total Time 0.00(0.00)\n",
      "Iter 0710 | Time 62.0612(59.9708) | Bit/dim 3.7713(3.8288) | Xent 1.1406(1.2826) | Loss 10.3347(11.1005) | Error 0.4011(0.4568) Steps 568(611.16) | Grad Norm 1.4333(5.9300) | Total Time 0.00(0.00)\n",
      "Iter 0711 | Time 55.4308(59.8346) | Bit/dim 3.7677(3.8270) | Xent 1.1349(1.2782) | Loss 10.0313(11.0685) | Error 0.3994(0.4551) Steps 580(610.23) | Grad Norm 1.8148(5.8066) | Total Time 0.00(0.00)\n",
      "Iter 0712 | Time 61.3511(59.8801) | Bit/dim 3.7490(3.8246) | Xent 1.1361(1.2739) | Loss 10.1811(11.0418) | Error 0.4022(0.4535) Steps 568(608.96) | Grad Norm 2.7457(5.7148) | Total Time 0.00(0.00)\n",
      "Iter 0713 | Time 59.7108(59.8750) | Bit/dim 3.7605(3.8227) | Xent 1.1301(1.2696) | Loss 10.1423(11.0148) | Error 0.4000(0.4519) Steps 592(608.45) | Grad Norm 1.7028(5.5944) | Total Time 0.00(0.00)\n",
      "Iter 0714 | Time 61.4551(59.9224) | Bit/dim 3.7659(3.8210) | Xent 1.1152(1.2650) | Loss 10.2800(10.9928) | Error 0.3996(0.4504) Steps 610(608.50) | Grad Norm 1.3497(5.4671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 23.2281, Epoch Time 404.6939(388.8059), Bit/dim 3.7591(best: 3.7675), Xent 1.1497, Loss 4.3340, Error 0.4148(best: 0.4127)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 60.8557(59.9504) | Bit/dim 3.7586(3.8191) | Xent 1.1225(1.2607) | Loss 13.5219(11.0687) | Error 0.4029(0.4489) Steps 598(608.18) | Grad Norm 1.3263(5.3428) | Total Time 0.00(0.00)\n",
      "Iter 0716 | Time 52.1735(59.7171) | Bit/dim 3.7608(3.8174) | Xent 1.1134(1.2563) | Loss 9.8869(11.0332) | Error 0.3974(0.4474) Steps 592(607.70) | Grad Norm 1.5886(5.2302) | Total Time 0.00(0.00)\n",
      "Iter 0717 | Time 60.9754(59.7548) | Bit/dim 3.7609(3.8157) | Xent 1.1392(1.2528) | Loss 10.1773(11.0075) | Error 0.4117(0.4463) Steps 586(607.04) | Grad Norm 1.7674(5.1263) | Total Time 0.00(0.00)\n",
      "Iter 0718 | Time 55.3451(59.6226) | Bit/dim 3.7541(3.8139) | Xent 1.1517(1.2497) | Loss 10.0750(10.9796) | Error 0.4060(0.4451) Steps 580(606.23) | Grad Norm 1.7985(5.0265) | Total Time 0.00(0.00)\n",
      "Iter 0719 | Time 55.9135(59.5113) | Bit/dim 3.7585(3.8122) | Xent 1.1307(1.2462) | Loss 9.9732(10.9494) | Error 0.4038(0.4439) Steps 592(605.81) | Grad Norm 1.1408(4.9099) | Total Time 0.00(0.00)\n",
      "Iter 0720 | Time 58.0068(59.4661) | Bit/dim 3.7572(3.8105) | Xent 1.1258(1.2425) | Loss 10.1076(10.9241) | Error 0.3995(0.4425) Steps 592(605.39) | Grad Norm 1.4505(4.8061) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 23.1396, Epoch Time 383.0042(388.6318), Bit/dim 3.7576(best: 3.7591), Xent 1.1434, Loss 4.3294, Error 0.4070(best: 0.4127)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 63.7033(59.5933) | Bit/dim 3.7551(3.8089) | Xent 1.1296(1.2392) | Loss 13.9075(11.0136) | Error 0.4021(0.4413) Steps 568(604.27) | Grad Norm 1.1429(4.6962) | Total Time 0.00(0.00)\n",
      "Iter 0722 | Time 64.5873(59.7431) | Bit/dim 3.7634(3.8075) | Xent 1.1472(1.2364) | Loss 10.2954(10.9921) | Error 0.4022(0.4402) Steps 574(603.36) | Grad Norm 1.0218(4.5860) | Total Time 0.00(0.00)\n",
      "Iter 0723 | Time 54.0688(59.5729) | Bit/dim 3.7673(3.8063) | Xent 1.1351(1.2334) | Loss 10.1564(10.9670) | Error 0.4016(0.4390) Steps 598(603.20) | Grad Norm 0.9918(4.4782) | Total Time 0.00(0.00)\n",
      "Iter 0724 | Time 63.1337(59.6797) | Bit/dim 3.7473(3.8045) | Xent 1.1346(1.2304) | Loss 10.1261(10.9418) | Error 0.3995(0.4378) Steps 574(602.33) | Grad Norm 1.2729(4.3820) | Total Time 0.00(0.00)\n",
      "Iter 0725 | Time 63.8219(59.8039) | Bit/dim 3.7577(3.8031) | Xent 1.1064(1.2267) | Loss 9.9515(10.9121) | Error 0.3930(0.4365) Steps 550(600.76) | Grad Norm 1.1364(4.2847) | Total Time 0.00(0.00)\n",
      "Iter 0726 | Time 56.8292(59.7147) | Bit/dim 3.7564(3.8017) | Xent 1.1124(1.2232) | Loss 10.2505(10.8922) | Error 0.3986(0.4353) Steps 598(600.67) | Grad Norm 1.2210(4.1927) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 24.2758, Epoch Time 406.6330(389.1719), Bit/dim 3.7585(best: 3.7576), Xent 1.1358, Loss 4.3264, Error 0.4083(best: 0.4070)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 60.5410(59.7395) | Bit/dim 3.7639(3.8006) | Xent 1.1292(1.2204) | Loss 13.6811(10.9759) | Error 0.4030(0.4344) Steps 574(599.87) | Grad Norm 0.7703(4.0901) | Total Time 0.00(0.00)\n",
      "Iter 0728 | Time 56.3987(59.6393) | Bit/dim 3.7478(3.7990) | Xent 1.1038(1.2169) | Loss 10.0711(10.9487) | Error 0.3980(0.4333) Steps 586(599.46) | Grad Norm 0.9277(3.9952) | Total Time 0.00(0.00)\n",
      "Iter 0729 | Time 57.5597(59.5769) | Bit/dim 3.7527(3.7976) | Xent 1.0889(1.2131) | Loss 10.0014(10.9203) | Error 0.3882(0.4319) Steps 580(598.87) | Grad Norm 0.9059(3.9025) | Total Time 0.00(0.00)\n",
      "Iter 0730 | Time 58.6913(59.5503) | Bit/dim 3.7553(3.7964) | Xent 1.1089(1.2100) | Loss 10.0929(10.8955) | Error 0.3910(0.4307) Steps 580(598.31) | Grad Norm 1.5656(3.8324) | Total Time 0.00(0.00)\n",
      "Iter 0731 | Time 58.2701(59.5119) | Bit/dim 3.7590(3.7952) | Xent 1.1218(1.2073) | Loss 10.1238(10.8724) | Error 0.3994(0.4298) Steps 550(596.86) | Grad Norm 1.0248(3.7482) | Total Time 0.00(0.00)\n",
      "Iter 0732 | Time 58.4608(59.4804) | Bit/dim 3.7459(3.7938) | Xent 1.0988(1.2041) | Loss 10.0136(10.8466) | Error 0.3909(0.4286) Steps 598(596.89) | Grad Norm 0.8658(3.6617) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 23.5023, Epoch Time 390.0526(389.1983), Bit/dim 3.7526(best: 3.7576), Xent 1.1356, Loss 4.3204, Error 0.4110(best: 0.4070)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 59.4794(59.4803) | Bit/dim 3.7602(3.7927) | Xent 1.1385(1.2021) | Loss 13.7289(10.9331) | Error 0.4040(0.4278) Steps 592(596.75) | Grad Norm 0.8584(3.5776) | Total Time 0.00(0.00)\n",
      "Iter 0734 | Time 58.0494(59.4374) | Bit/dim 3.7563(3.7917) | Xent 1.1032(1.1991) | Loss 10.0397(10.9063) | Error 0.3951(0.4269) Steps 598(596.78) | Grad Norm 1.2281(3.5071) | Total Time 0.00(0.00)\n",
      "Iter 0735 | Time 56.4237(59.3470) | Bit/dim 3.7543(3.7905) | Xent 1.1151(1.1966) | Loss 10.1362(10.8832) | Error 0.3941(0.4259) Steps 574(596.10) | Grad Norm 1.4110(3.4442) | Total Time 0.00(0.00)\n",
      "Iter 0736 | Time 56.8425(59.2719) | Bit/dim 3.7536(3.7894) | Xent 1.1219(1.1944) | Loss 10.1623(10.8615) | Error 0.3972(0.4250) Steps 562(595.08) | Grad Norm 0.8429(3.3662) | Total Time 0.00(0.00)\n",
      "Iter 0737 | Time 60.1672(59.2987) | Bit/dim 3.7472(3.7882) | Xent 1.1109(1.1919) | Loss 10.0921(10.8384) | Error 0.3946(0.4241) Steps 580(594.62) | Grad Norm 1.4057(3.3074) | Total Time 0.00(0.00)\n",
      "Iter 0738 | Time 56.6715(59.2199) | Bit/dim 3.7546(3.7872) | Xent 1.1069(1.1893) | Loss 9.9589(10.8121) | Error 0.3975(0.4233) Steps 574(594.01) | Grad Norm 1.6595(3.2580) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 23.0715, Epoch Time 386.9795(389.1317), Bit/dim 3.7544(best: 3.7526), Xent 1.1302, Loss 4.3195, Error 0.4059(best: 0.4070)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 54.5139(59.0787) | Bit/dim 3.7488(3.7860) | Xent 1.1036(1.1867) | Loss 13.7555(10.9004) | Error 0.3959(0.4225) Steps 580(593.59) | Grad Norm 1.0940(3.1930) | Total Time 0.00(0.00)\n",
      "Iter 0740 | Time 57.3507(59.0269) | Bit/dim 3.7461(3.7848) | Xent 1.1092(1.1844) | Loss 9.8733(10.8696) | Error 0.3960(0.4217) Steps 586(593.36) | Grad Norm 0.9898(3.1269) | Total Time 0.00(0.00)\n",
      "Iter 0741 | Time 55.7412(58.9283) | Bit/dim 3.7507(3.7838) | Xent 1.1042(1.1820) | Loss 10.0958(10.8463) | Error 0.3959(0.4209) Steps 574(592.78) | Grad Norm 1.3545(3.0738) | Total Time 0.00(0.00)\n",
      "Iter 0742 | Time 61.6667(59.0105) | Bit/dim 3.7632(3.7832) | Xent 1.1220(1.1802) | Loss 10.1655(10.8259) | Error 0.3955(0.4202) Steps 580(592.39) | Grad Norm 1.1847(3.0171) | Total Time 0.00(0.00)\n",
      "Iter 0743 | Time 57.3959(58.9620) | Bit/dim 3.7531(3.7823) | Xent 1.1254(1.1786) | Loss 10.0012(10.8012) | Error 0.3999(0.4196) Steps 592(592.38) | Grad Norm 0.9701(2.9557) | Total Time 0.00(0.00)\n",
      "Iter 0744 | Time 54.2549(58.8208) | Bit/dim 3.7626(3.7817) | Xent 1.0967(1.1761) | Loss 10.1543(10.7818) | Error 0.3974(0.4189) Steps 586(592.19) | Grad Norm 0.6446(2.8863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 23.4550, Epoch Time 380.3506(388.8683), Bit/dim 3.7536(best: 3.7526), Xent 1.1313, Loss 4.3192, Error 0.4042(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 56.1943(58.7420) | Bit/dim 3.7605(3.7810) | Xent 1.1017(1.1739) | Loss 13.6671(10.8683) | Error 0.3912(0.4181) Steps 598(592.36) | Grad Norm 1.1757(2.8350) | Total Time 0.00(0.00)\n",
      "Iter 0746 | Time 56.7161(58.6812) | Bit/dim 3.7398(3.7798) | Xent 1.1304(1.1726) | Loss 10.0556(10.8439) | Error 0.4036(0.4176) Steps 586(592.17) | Grad Norm 1.7209(2.8016) | Total Time 0.00(0.00)\n",
      "Iter 0747 | Time 56.4585(58.6146) | Bit/dim 3.7638(3.7793) | Xent 1.0943(1.1702) | Loss 10.1207(10.8222) | Error 0.3926(0.4169) Steps 604(592.53) | Grad Norm 0.8398(2.7427) | Total Time 0.00(0.00)\n",
      "Iter 0748 | Time 59.9499(58.6546) | Bit/dim 3.7476(3.7784) | Xent 1.1005(1.1681) | Loss 10.2373(10.8047) | Error 0.3905(0.4161) Steps 592(592.51) | Grad Norm 1.2448(2.6978) | Total Time 0.00(0.00)\n",
      "Iter 0749 | Time 55.4529(58.5586) | Bit/dim 3.7419(3.7773) | Xent 1.1213(1.1667) | Loss 10.0580(10.7823) | Error 0.4025(0.4157) Steps 550(591.24) | Grad Norm 1.4869(2.6615) | Total Time 0.00(0.00)\n",
      "Iter 0750 | Time 58.3541(58.5524) | Bit/dim 3.7532(3.7765) | Xent 1.1168(1.1652) | Loss 10.0876(10.7615) | Error 0.4031(0.4153) Steps 568(590.54) | Grad Norm 0.8114(2.6060) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 23.3853, Epoch Time 382.5738(388.6794), Bit/dim 3.7503(best: 3.7526), Xent 1.1282, Loss 4.3144, Error 0.4090(best: 0.4042)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 54.9808(58.4453) | Bit/dim 3.7611(3.7761) | Xent 1.1080(1.1635) | Loss 13.0340(10.8296) | Error 0.3914(0.4146) Steps 598(590.76) | Grad Norm 0.8543(2.5534) | Total Time 0.00(0.00)\n",
      "Iter 0752 | Time 54.7750(58.3352) | Bit/dim 3.7567(3.7755) | Xent 1.0973(1.1615) | Loss 9.8987(10.8017) | Error 0.3985(0.4141) Steps 562(589.90) | Grad Norm 1.1917(2.5126) | Total Time 0.00(0.00)\n",
      "Iter 0753 | Time 59.4877(58.3698) | Bit/dim 3.7450(3.7746) | Xent 1.1084(1.1599) | Loss 10.1805(10.7831) | Error 0.3928(0.4135) Steps 568(589.24) | Grad Norm 0.9266(2.4650) | Total Time 0.00(0.00)\n",
      "Iter 0754 | Time 59.0704(58.3908) | Bit/dim 3.7425(3.7736) | Xent 1.0838(1.1577) | Loss 9.8945(10.7564) | Error 0.3840(0.4126) Steps 586(589.15) | Grad Norm 0.6950(2.4119) | Total Time 0.00(0.00)\n",
      "Iter 0755 | Time 55.9716(58.3182) | Bit/dim 3.7609(3.7732) | Xent 1.1288(1.1568) | Loss 9.9635(10.7326) | Error 0.4087(0.4125) Steps 580(588.87) | Grad Norm 1.4643(2.3835) | Total Time 0.00(0.00)\n",
      "Iter 0756 | Time 52.2486(58.1361) | Bit/dim 3.7507(3.7726) | Xent 1.1147(1.1555) | Loss 9.9109(10.7080) | Error 0.3996(0.4121) Steps 568(588.25) | Grad Norm 1.2309(2.3489) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 23.1460, Epoch Time 376.3997(388.3111), Bit/dim 3.7556(best: 3.7503), Xent 1.1231, Loss 4.3172, Error 0.4035(best: 0.4042)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 54.2309(58.0190) | Bit/dim 3.7476(3.7718) | Xent 1.0748(1.1531) | Loss 13.4853(10.7913) | Error 0.3822(0.4112) Steps 586(588.18) | Grad Norm 0.6698(2.2985) | Total Time 0.00(0.00)\n",
      "Iter 0758 | Time 53.8722(57.8945) | Bit/dim 3.7439(3.7710) | Xent 1.1088(1.1518) | Loss 9.7896(10.7612) | Error 0.3940(0.4107) Steps 586(588.11) | Grad Norm 1.0099(2.2599) | Total Time 0.00(0.00)\n",
      "Iter 0759 | Time 57.9920(57.8975) | Bit/dim 3.7525(3.7704) | Xent 1.0983(1.1502) | Loss 10.0778(10.7407) | Error 0.3894(0.4100) Steps 580(587.87) | Grad Norm 1.2548(2.2297) | Total Time 0.00(0.00)\n",
      "Iter 0760 | Time 57.9478(57.8990) | Bit/dim 3.7491(3.7698) | Xent 1.1019(1.1487) | Loss 10.0692(10.7206) | Error 0.3955(0.4096) Steps 598(588.17) | Grad Norm 0.7978(2.1867) | Total Time 0.00(0.00)\n",
      "Iter 0761 | Time 55.5351(57.8281) | Bit/dim 3.7520(3.7693) | Xent 1.1142(1.1477) | Loss 10.0343(10.7000) | Error 0.4006(0.4093) Steps 574(587.75) | Grad Norm 0.7162(2.1426) | Total Time 0.00(0.00)\n",
      "Iter 0762 | Time 55.7712(57.7664) | Bit/dim 3.7502(3.7687) | Xent 1.1159(1.1467) | Loss 10.1359(10.6831) | Error 0.3981(0.4090) Steps 580(587.52) | Grad Norm 0.8342(2.1034) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 23.4641, Epoch Time 375.3215(387.9214), Bit/dim 3.7497(best: 3.7503), Xent 1.1216, Loss 4.3105, Error 0.4044(best: 0.4035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 54.4589(57.6671) | Bit/dim 3.7381(3.7678) | Xent 1.1103(1.1456) | Loss 13.3874(10.7642) | Error 0.3946(0.4086) Steps 580(587.29) | Grad Norm 1.0083(2.0705) | Total Time 0.00(0.00)\n",
      "Iter 0764 | Time 55.0856(57.5897) | Bit/dim 3.7510(3.7673) | Xent 1.0793(1.1437) | Loss 10.1603(10.7461) | Error 0.3882(0.4079) Steps 574(586.89) | Grad Norm 1.0781(2.0408) | Total Time 0.00(0.00)\n",
      "Iter 0765 | Time 54.1898(57.4877) | Bit/dim 3.7549(3.7669) | Xent 1.1032(1.1424) | Loss 10.0147(10.7241) | Error 0.3962(0.4076) Steps 586(586.87) | Grad Norm 0.7534(2.0021) | Total Time 0.00(0.00)\n",
      "Iter 0766 | Time 55.5941(57.4309) | Bit/dim 3.7583(3.7666) | Xent 1.0966(1.1411) | Loss 10.0953(10.7053) | Error 0.3858(0.4069) Steps 574(586.48) | Grad Norm 1.0624(1.9739) | Total Time 0.00(0.00)\n",
      "Iter 0767 | Time 57.2879(57.4266) | Bit/dim 3.7424(3.7659) | Xent 1.1066(1.1400) | Loss 10.0578(10.6859) | Error 0.3965(0.4066) Steps 598(586.82) | Grad Norm 0.7765(1.9380) | Total Time 0.00(0.00)\n",
      "Iter 0768 | Time 58.8222(57.4685) | Bit/dim 3.7566(3.7656) | Xent 1.0823(1.1383) | Loss 9.9640(10.6642) | Error 0.3852(0.4060) Steps 616(587.70) | Grad Norm 0.9117(1.9072) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 23.8831, Epoch Time 375.4225(387.5464), Bit/dim 3.7499(best: 3.7497), Xent 1.1164, Loss 4.3081, Error 0.4002(best: 0.4035)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 56.8624(57.4503) | Bit/dim 3.7479(3.7651) | Xent 1.1094(1.1374) | Loss 13.8976(10.7612) | Error 0.4021(0.4059) Steps 592(587.83) | Grad Norm 0.7449(1.8724) | Total Time 0.00(0.00)\n",
      "Iter 0770 | Time 55.8407(57.4020) | Bit/dim 3.7481(3.7646) | Xent 1.0949(1.1362) | Loss 10.2160(10.7448) | Error 0.3870(0.4053) Steps 568(587.23) | Grad Norm 0.8621(1.8420) | Total Time 0.00(0.00)\n",
      "Iter 0771 | Time 58.5692(57.4370) | Bit/dim 3.7579(3.7644) | Xent 1.0930(1.1349) | Loss 10.1866(10.7281) | Error 0.3938(0.4050) Steps 550(586.12) | Grad Norm 1.2101(1.8231) | Total Time 0.00(0.00)\n",
      "Iter 0772 | Time 53.1902(57.3096) | Bit/dim 3.7341(3.7635) | Xent 1.0874(1.1334) | Loss 10.0484(10.7077) | Error 0.3879(0.4044) Steps 580(585.93) | Grad Norm 0.7124(1.7898) | Total Time 0.00(0.00)\n",
      "Iter 0773 | Time 58.0364(57.3314) | Bit/dim 3.7472(3.7630) | Xent 1.0747(1.1317) | Loss 10.1626(10.6914) | Error 0.3789(0.4037) Steps 586(585.94) | Grad Norm 0.8103(1.7604) | Total Time 0.00(0.00)\n",
      "Iter 0774 | Time 55.6287(57.2803) | Bit/dim 3.7620(3.7630) | Xent 1.0997(1.1307) | Loss 10.2193(10.6772) | Error 0.3902(0.4033) Steps 562(585.22) | Grad Norm 0.9418(1.7358) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 23.4785, Epoch Time 377.8466(387.2554), Bit/dim 3.7538(best: 3.7497), Xent 1.1173, Loss 4.3124, Error 0.4020(best: 0.4002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 54.5668(57.1989) | Bit/dim 3.7572(3.7628) | Xent 1.1070(1.1300) | Loss 13.7075(10.7681) | Error 0.3875(0.4028) Steps 580(585.06) | Grad Norm 0.7868(1.7074) | Total Time 0.00(0.00)\n",
      "Iter 0776 | Time 53.7454(57.0953) | Bit/dim 3.7534(3.7625) | Xent 1.1083(1.1294) | Loss 10.0120(10.7454) | Error 0.3976(0.4026) Steps 580(584.91) | Grad Norm 1.2114(1.6925) | Total Time 0.00(0.00)\n",
      "Iter 0777 | Time 52.3620(56.9533) | Bit/dim 3.7409(3.7619) | Xent 1.1033(1.1286) | Loss 9.8776(10.7194) | Error 0.3966(0.4025) Steps 568(584.40) | Grad Norm 0.6871(1.6623) | Total Time 0.00(0.00)\n",
      "Iter 0778 | Time 60.1804(57.0501) | Bit/dim 3.7386(3.7612) | Xent 1.0835(1.1272) | Loss 10.1012(10.7008) | Error 0.3829(0.4019) Steps 592(584.63) | Grad Norm 1.6406(1.6617) | Total Time 0.00(0.00)\n",
      "Iter 0779 | Time 55.6681(57.0087) | Bit/dim 3.7441(3.7606) | Xent 1.0766(1.1257) | Loss 10.1739(10.6850) | Error 0.3884(0.4015) Steps 580(584.49) | Grad Norm 1.7266(1.6636) | Total Time 0.00(0.00)\n",
      "Iter 0780 | Time 58.4511(57.0519) | Bit/dim 3.7444(3.7602) | Xent 1.0722(1.1241) | Loss 10.0139(10.6649) | Error 0.3844(0.4010) Steps 574(584.18) | Grad Norm 0.8163(1.6382) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 23.5041, Epoch Time 374.3103(386.8671), Bit/dim 3.7511(best: 3.7497), Xent 1.1164, Loss 4.3093, Error 0.4037(best: 0.4002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 56.7248(57.0421) | Bit/dim 3.7395(3.7595) | Xent 1.0958(1.1233) | Loss 13.7426(10.7572) | Error 0.3918(0.4007) Steps 592(584.41) | Grad Norm 1.4155(1.6315) | Total Time 0.00(0.00)\n",
      "Iter 0782 | Time 57.1908(57.0466) | Bit/dim 3.7571(3.7595) | Xent 1.0783(1.1219) | Loss 10.0191(10.7351) | Error 0.3840(0.4002) Steps 580(584.28) | Grad Norm 1.8650(1.6385) | Total Time 0.00(0.00)\n",
      "Iter 0783 | Time 60.3188(57.1447) | Bit/dim 3.7531(3.7593) | Xent 1.0709(1.1204) | Loss 10.0386(10.7142) | Error 0.3832(0.3997) Steps 616(585.23) | Grad Norm 0.7464(1.6118) | Total Time 0.00(0.00)\n",
      "Iter 0784 | Time 54.1188(57.0540) | Bit/dim 3.7438(3.7588) | Xent 1.1006(1.1198) | Loss 10.1681(10.6978) | Error 0.3884(0.3993) Steps 586(585.25) | Grad Norm 2.4270(1.6362) | Total Time 0.00(0.00)\n",
      "Iter 0785 | Time 57.9356(57.0804) | Bit/dim 3.7435(3.7583) | Xent 1.0861(1.1188) | Loss 9.9798(10.6763) | Error 0.3921(0.3991) Steps 592(585.46) | Grad Norm 1.4544(1.6308) | Total Time 0.00(0.00)\n",
      "Iter 0786 | Time 56.1242(57.0517) | Bit/dim 3.7499(3.7581) | Xent 1.0814(1.1177) | Loss 10.0253(10.6567) | Error 0.3890(0.3988) Steps 562(584.75) | Grad Norm 1.7785(1.6352) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 23.0694, Epoch Time 381.8978(386.7180), Bit/dim 3.7471(best: 3.7497), Xent 1.1127, Loss 4.3034, Error 0.3981(best: 0.4002)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 56.6665(57.0402) | Bit/dim 3.7445(3.7577) | Xent 1.0908(1.1168) | Loss 13.9354(10.7551) | Error 0.3864(0.3984) Steps 616(585.69) | Grad Norm 1.8398(1.6413) | Total Time 0.00(0.00)\n",
      "Iter 0788 | Time 53.9251(56.9467) | Bit/dim 3.7483(3.7574) | Xent 1.0936(1.1161) | Loss 10.0646(10.7344) | Error 0.3862(0.3981) Steps 586(585.70) | Grad Norm 1.0697(1.6242) | Total Time 0.00(0.00)\n",
      "Iter 0789 | Time 56.7581(56.9411) | Bit/dim 3.7521(3.7572) | Xent 1.0929(1.1155) | Loss 10.1552(10.7170) | Error 0.3895(0.3978) Steps 604(586.25) | Grad Norm 2.2886(1.6441) | Total Time 0.00(0.00)\n",
      "Iter 0790 | Time 55.8974(56.9098) | Bit/dim 3.7420(3.7568) | Xent 1.0776(1.1143) | Loss 10.1762(10.7008) | Error 0.3779(0.3972) Steps 598(586.60) | Grad Norm 0.8033(1.6189) | Total Time 0.00(0.00)\n",
      "Iter 0791 | Time 55.2530(56.8600) | Bit/dim 3.7451(3.7564) | Xent 1.0832(1.1134) | Loss 9.9877(10.6794) | Error 0.3891(0.3970) Steps 586(586.58) | Grad Norm 1.3636(1.6112) | Total Time 0.00(0.00)\n",
      "Iter 0792 | Time 55.8853(56.8308) | Bit/dim 3.7513(3.7563) | Xent 1.0624(1.1119) | Loss 9.7928(10.6528) | Error 0.3794(0.3964) Steps 586(586.56) | Grad Norm 1.2345(1.5999) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 22.8436, Epoch Time 373.6204(386.3250), Bit/dim 3.7462(best: 3.7471), Xent 1.1116, Loss 4.3021, Error 0.4005(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 52.1855(56.6914) | Bit/dim 3.7377(3.7557) | Xent 1.0793(1.1109) | Loss 13.7009(10.7442) | Error 0.3860(0.3961) Steps 574(586.19) | Grad Norm 0.9529(1.5805) | Total Time 0.00(0.00)\n",
      "Iter 0794 | Time 57.4941(56.7155) | Bit/dim 3.7522(3.7556) | Xent 1.0673(1.1096) | Loss 10.1242(10.7256) | Error 0.3815(0.3957) Steps 574(585.82) | Grad Norm 1.1546(1.5677) | Total Time 0.00(0.00)\n",
      "Iter 0795 | Time 55.2155(56.6705) | Bit/dim 3.7450(3.7553) | Xent 1.0943(1.1091) | Loss 10.1127(10.7072) | Error 0.3861(0.3954) Steps 604(586.37) | Grad Norm 0.9781(1.5500) | Total Time 0.00(0.00)\n",
      "Iter 0796 | Time 58.5324(56.7264) | Bit/dim 3.7420(3.7549) | Xent 1.0711(1.1080) | Loss 10.1663(10.6910) | Error 0.3846(0.3951) Steps 598(586.72) | Grad Norm 0.9930(1.5333) | Total Time 0.00(0.00)\n",
      "Iter 0797 | Time 58.8417(56.7898) | Bit/dim 3.7485(3.7547) | Xent 1.0757(1.1070) | Loss 9.7240(10.6620) | Error 0.3892(0.3949) Steps 604(587.24) | Grad Norm 0.9333(1.5153) | Total Time 0.00(0.00)\n",
      "Iter 0798 | Time 57.8091(56.8204) | Bit/dim 3.7427(3.7544) | Xent 1.0571(1.1055) | Loss 10.1965(10.6480) | Error 0.3749(0.3943) Steps 568(586.66) | Grad Norm 0.7694(1.4930) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 23.5570, Epoch Time 379.9659(386.1343), Bit/dim 3.7475(best: 3.7462), Xent 1.1076, Loss 4.3013, Error 0.4019(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 56.5900(56.8135) | Bit/dim 3.7584(3.7545) | Xent 1.0679(1.1044) | Loss 13.4326(10.7316) | Error 0.3816(0.3939) Steps 598(587.00) | Grad Norm 1.2613(1.4860) | Total Time 0.00(0.00)\n",
      "Iter 0800 | Time 55.5072(56.7743) | Bit/dim 3.7408(3.7541) | Xent 1.0748(1.1035) | Loss 10.1230(10.7133) | Error 0.3870(0.3937) Steps 586(586.97) | Grad Norm 0.6446(1.4608) | Total Time 0.00(0.00)\n",
      "Iter 0801 | Time 54.3106(56.7004) | Bit/dim 3.7452(3.7538) | Xent 1.0812(1.1028) | Loss 10.0211(10.6926) | Error 0.3881(0.3936) Steps 598(587.30) | Grad Norm 1.1455(1.4513) | Total Time 0.00(0.00)\n",
      "Iter 0802 | Time 60.6079(56.8176) | Bit/dim 3.7472(3.7536) | Xent 1.0898(1.1024) | Loss 10.0461(10.6732) | Error 0.3885(0.3934) Steps 628(588.52) | Grad Norm 0.7841(1.4313) | Total Time 0.00(0.00)\n",
      "Iter 0803 | Time 55.2415(56.7703) | Bit/dim 3.7475(3.7534) | Xent 1.0680(1.1014) | Loss 10.0390(10.6541) | Error 0.3820(0.3931) Steps 574(588.08) | Grad Norm 0.8513(1.4139) | Total Time 0.00(0.00)\n",
      "Iter 0804 | Time 56.0404(56.7484) | Bit/dim 3.7453(3.7532) | Xent 1.0427(1.0996) | Loss 10.0964(10.6374) | Error 0.3680(0.3923) Steps 568(587.48) | Grad Norm 1.1091(1.4047) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 23.7497, Epoch Time 378.3196(385.8998), Bit/dim 3.7482(best: 3.7462), Xent 1.1044, Loss 4.3004, Error 0.3986(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 56.0244(56.7267) | Bit/dim 3.7406(3.7528) | Xent 1.0762(1.0989) | Loss 13.3777(10.7196) | Error 0.3779(0.3919) Steps 586(587.44) | Grad Norm 0.9380(1.3907) | Total Time 0.00(0.00)\n",
      "Iter 0806 | Time 59.1724(56.8001) | Bit/dim 3.7444(3.7525) | Xent 1.0809(1.0984) | Loss 10.0507(10.6995) | Error 0.3876(0.3917) Steps 604(587.93) | Grad Norm 1.3495(1.3895) | Total Time 0.00(0.00)\n",
      "Iter 0807 | Time 58.5332(56.8521) | Bit/dim 3.7538(3.7526) | Xent 1.0654(1.0974) | Loss 10.0445(10.6799) | Error 0.3801(0.3914) Steps 580(587.70) | Grad Norm 1.1686(1.3829) | Total Time 0.00(0.00)\n",
      "Iter 0808 | Time 56.4095(56.8388) | Bit/dim 3.7489(3.7525) | Xent 1.0623(1.0964) | Loss 9.7924(10.6533) | Error 0.3822(0.3911) Steps 586(587.65) | Grad Norm 1.0052(1.3716) | Total Time 0.00(0.00)\n",
      "Iter 0809 | Time 62.9868(57.0233) | Bit/dim 3.7444(3.7522) | Xent 1.0584(1.0952) | Loss 10.1823(10.6391) | Error 0.3790(0.3908) Steps 592(587.78) | Grad Norm 1.1493(1.3649) | Total Time 0.00(0.00)\n",
      "Iter 0810 | Time 61.2431(57.1498) | Bit/dim 3.7436(3.7520) | Xent 1.0583(1.0941) | Loss 9.9822(10.6194) | Error 0.3778(0.3904) Steps 634(589.16) | Grad Norm 0.7815(1.3474) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 23.9577, Epoch Time 394.7945(386.1667), Bit/dim 3.7474(best: 3.7462), Xent 1.1036, Loss 4.2992, Error 0.3950(best: 0.3981)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 55.6609(57.1052) | Bit/dim 3.7521(3.7520) | Xent 1.0656(1.0933) | Loss 13.8513(10.7164) | Error 0.3771(0.3900) Steps 580(588.89) | Grad Norm 1.2588(1.3447) | Total Time 0.00(0.00)\n",
      "Iter 0812 | Time 57.4210(57.1147) | Bit/dim 3.7350(3.7515) | Xent 1.0785(1.0928) | Loss 9.9550(10.6935) | Error 0.3868(0.3899) Steps 568(588.26) | Grad Norm 0.8992(1.3314) | Total Time 0.00(0.00)\n",
      "Iter 0813 | Time 54.4329(57.0342) | Bit/dim 3.7448(3.7513) | Xent 1.0682(1.0921) | Loss 9.9772(10.6721) | Error 0.3790(0.3896) Steps 574(587.83) | Grad Norm 1.8575(1.3471) | Total Time 0.00(0.00)\n",
      "Iter 0814 | Time 60.9798(57.1526) | Bit/dim 3.7383(3.7509) | Xent 1.0602(1.0911) | Loss 10.1975(10.6578) | Error 0.3769(0.3892) Steps 586(587.78) | Grad Norm 1.3115(1.3461) | Total Time 0.00(0.00)\n",
      "Iter 0815 | Time 56.1952(57.1238) | Bit/dim 3.7416(3.7506) | Xent 1.0526(1.0900) | Loss 10.1466(10.6425) | Error 0.3702(0.3886) Steps 610(588.45) | Grad Norm 1.2043(1.3418) | Total Time 0.00(0.00)\n",
      "Iter 0816 | Time 51.3342(56.9502) | Bit/dim 3.7463(3.7505) | Xent 1.0486(1.0887) | Loss 9.8507(10.6187) | Error 0.3655(0.3879) Steps 574(588.01) | Grad Norm 1.0345(1.3326) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 23.2958, Epoch Time 375.4871(385.8463), Bit/dim 3.7437(best: 3.7462), Xent 1.0973, Loss 4.2923, Error 0.3919(best: 0.3950)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 55.1746(56.8969) | Bit/dim 3.7394(3.7501) | Xent 1.0800(1.0885) | Loss 13.7799(10.7136) | Error 0.3860(0.3879) Steps 598(588.31) | Grad Norm 1.4346(1.3357) | Total Time 0.00(0.00)\n",
      "Iter 0818 | Time 54.2647(56.8179) | Bit/dim 3.7506(3.7501) | Xent 1.0512(1.0873) | Loss 9.8874(10.6888) | Error 0.3756(0.3875) Steps 574(587.88) | Grad Norm 2.3114(1.3649) | Total Time 0.00(0.00)\n",
      "Iter 0819 | Time 61.0016(56.9434) | Bit/dim 3.7364(3.7497) | Xent 1.0862(1.0873) | Loss 10.1967(10.6740) | Error 0.3891(0.3875) Steps 586(587.83) | Grad Norm 1.4292(1.3669) | Total Time 0.00(0.00)\n",
      "Iter 0820 | Time 57.3812(56.9566) | Bit/dim 3.7334(3.7492) | Xent 1.0522(1.0862) | Loss 9.9946(10.6536) | Error 0.3825(0.3874) Steps 562(587.05) | Grad Norm 1.4471(1.3693) | Total Time 0.00(0.00)\n",
      "Iter 0821 | Time 56.2575(56.9356) | Bit/dim 3.7484(3.7492) | Xent 1.0883(1.0863) | Loss 10.1783(10.6394) | Error 0.3884(0.3874) Steps 580(586.84) | Grad Norm 1.8829(1.3847) | Total Time 0.00(0.00)\n",
      "Iter 0822 | Time 54.9279(56.8754) | Bit/dim 3.7509(3.7493) | Xent 1.0414(1.0850) | Loss 9.9418(10.6184) | Error 0.3740(0.3870) Steps 556(585.91) | Grad Norm 1.0689(1.3752) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 23.4353, Epoch Time 378.3923(385.6227), Bit/dim 3.7427(best: 3.7437), Xent 1.1029, Loss 4.2941, Error 0.3952(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 54.9808(56.8185) | Bit/dim 3.7561(3.7495) | Xent 1.0582(1.0842) | Loss 13.6860(10.7105) | Error 0.3814(0.3868) Steps 592(586.10) | Grad Norm 1.6713(1.3841) | Total Time 0.00(0.00)\n",
      "Iter 0824 | Time 55.2131(56.7704) | Bit/dim 3.7494(3.7495) | Xent 1.0652(1.0836) | Loss 9.8726(10.6853) | Error 0.3780(0.3866) Steps 592(586.27) | Grad Norm 1.1220(1.3762) | Total Time 0.00(0.00)\n",
      "Iter 0825 | Time 52.9550(56.6559) | Bit/dim 3.7434(3.7493) | Xent 1.0397(1.0823) | Loss 9.8868(10.6614) | Error 0.3725(0.3862) Steps 586(586.27) | Grad Norm 1.7575(1.3877) | Total Time 0.00(0.00)\n",
      "Iter 0826 | Time 50.7904(56.4799) | Bit/dim 3.7395(3.7490) | Xent 1.0644(1.0817) | Loss 9.8730(10.6377) | Error 0.3749(0.3858) Steps 574(585.90) | Grad Norm 1.8005(1.4000) | Total Time 0.00(0.00)\n",
      "Iter 0827 | Time 57.0604(56.4974) | Bit/dim 3.7524(3.7491) | Xent 1.0640(1.0812) | Loss 10.1397(10.6228) | Error 0.3809(0.3857) Steps 610(586.62) | Grad Norm 1.3277(1.3979) | Total Time 0.00(0.00)\n",
      "Iter 0828 | Time 58.8344(56.5675) | Bit/dim 3.7344(3.7487) | Xent 1.0776(1.0811) | Loss 9.8710(10.6002) | Error 0.3816(0.3855) Steps 610(587.32) | Grad Norm 1.7500(1.4084) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 22.7162, Epoch Time 368.9890(385.1237), Bit/dim 3.7423(best: 3.7427), Xent 1.1006, Loss 4.2927, Error 0.3955(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 56.5048(56.5656) | Bit/dim 3.7363(3.7483) | Xent 1.0636(1.0806) | Loss 13.7031(10.6933) | Error 0.3835(0.3855) Steps 604(587.82) | Grad Norm 2.0999(1.4292) | Total Time 0.00(0.00)\n",
      "Iter 0830 | Time 58.1086(56.6119) | Bit/dim 3.7486(3.7483) | Xent 1.0781(1.0805) | Loss 10.2580(10.6803) | Error 0.3862(0.3855) Steps 592(587.95) | Grad Norm 1.8145(1.4407) | Total Time 0.00(0.00)\n",
      "Iter 0831 | Time 57.4549(56.6372) | Bit/dim 3.7281(3.7477) | Xent 1.0613(1.0799) | Loss 9.9984(10.6598) | Error 0.3771(0.3853) Steps 604(588.43) | Grad Norm 2.7950(1.4814) | Total Time 0.00(0.00)\n",
      "Iter 0832 | Time 54.1588(56.5628) | Bit/dim 3.7460(3.7476) | Xent 1.0430(1.0788) | Loss 10.0418(10.6413) | Error 0.3776(0.3850) Steps 568(587.82) | Grad Norm 1.8950(1.4938) | Total Time 0.00(0.00)\n",
      "Iter 0833 | Time 56.3252(56.5557) | Bit/dim 3.7465(3.7476) | Xent 1.0586(1.0782) | Loss 10.2492(10.6295) | Error 0.3739(0.3847) Steps 568(587.22) | Grad Norm 1.8664(1.5050) | Total Time 0.00(0.00)\n",
      "Iter 0834 | Time 59.9212(56.6567) | Bit/dim 3.7410(3.7474) | Xent 1.0426(1.0771) | Loss 10.0903(10.6133) | Error 0.3745(0.3844) Steps 592(587.37) | Grad Norm 1.3507(1.5003) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 23.1130, Epoch Time 381.6009(385.0180), Bit/dim 3.7431(best: 3.7423), Xent 1.0947, Loss 4.2904, Error 0.3946(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 55.8723(56.6331) | Bit/dim 3.7439(3.7473) | Xent 1.0292(1.0757) | Loss 13.5119(10.7003) | Error 0.3670(0.3839) Steps 622(588.40) | Grad Norm 0.8665(1.4813) | Total Time 0.00(0.00)\n",
      "Iter 0836 | Time 55.4002(56.5961) | Bit/dim 3.7457(3.7473) | Xent 1.0559(1.0751) | Loss 9.9931(10.6791) | Error 0.3721(0.3835) Steps 592(588.51) | Grad Norm 1.5860(1.4844) | Total Time 0.00(0.00)\n",
      "Iter 0837 | Time 63.7289(56.8101) | Bit/dim 3.7364(3.7469) | Xent 1.0298(1.0737) | Loss 10.1082(10.6619) | Error 0.3658(0.3830) Steps 604(588.98) | Grad Norm 2.1503(1.5044) | Total Time 0.00(0.00)\n",
      "Iter 0838 | Time 58.1633(56.8507) | Bit/dim 3.7539(3.7471) | Xent 1.0867(1.0741) | Loss 9.9461(10.6405) | Error 0.3826(0.3830) Steps 586(588.89) | Grad Norm 1.4143(1.5017) | Total Time 0.00(0.00)\n",
      "Iter 0839 | Time 59.1312(56.9191) | Bit/dim 3.7256(3.7465) | Xent 1.0512(1.0734) | Loss 10.1405(10.6255) | Error 0.3788(0.3828) Steps 622(589.88) | Grad Norm 4.0757(1.5789) | Total Time 0.00(0.00)\n",
      "Iter 0840 | Time 55.8597(56.8873) | Bit/dim 3.7445(3.7464) | Xent 1.0498(1.0727) | Loss 10.1876(10.6123) | Error 0.3691(0.3824) Steps 586(589.76) | Grad Norm 3.2108(1.6279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 23.6521, Epoch Time 388.0340(385.1085), Bit/dim 3.7416(best: 3.7423), Xent 1.0972, Loss 4.2902, Error 0.3958(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 55.0960(56.8336) | Bit/dim 3.7355(3.7461) | Xent 1.0407(1.0718) | Loss 13.6249(10.7027) | Error 0.3711(0.3821) Steps 574(589.29) | Grad Norm 1.1961(1.6149) | Total Time 0.00(0.00)\n",
      "Iter 0842 | Time 57.6688(56.8587) | Bit/dim 3.7420(3.7460) | Xent 1.0363(1.0707) | Loss 10.0257(10.6824) | Error 0.3699(0.3817) Steps 592(589.37) | Grad Norm 2.2429(1.6338) | Total Time 0.00(0.00)\n",
      "Iter 0843 | Time 61.9775(57.0122) | Bit/dim 3.7332(3.7456) | Xent 1.0409(1.0698) | Loss 10.1887(10.6676) | Error 0.3716(0.3814) Steps 652(591.25) | Grad Norm 1.9019(1.6418) | Total Time 0.00(0.00)\n",
      "Iter 0844 | Time 55.6328(56.9708) | Bit/dim 3.7447(3.7456) | Xent 1.0484(1.0692) | Loss 10.0691(10.6496) | Error 0.3692(0.3811) Steps 580(590.91) | Grad Norm 3.3085(1.6918) | Total Time 0.00(0.00)\n",
      "Iter 0845 | Time 55.1176(56.9152) | Bit/dim 3.7377(3.7453) | Xent 1.0656(1.0691) | Loss 10.0492(10.6316) | Error 0.3872(0.3812) Steps 574(590.41) | Grad Norm 1.3999(1.6831) | Total Time 0.00(0.00)\n",
      "Iter 0846 | Time 55.4697(56.8719) | Bit/dim 3.7390(3.7451) | Xent 1.0558(1.0687) | Loss 9.9638(10.6116) | Error 0.3776(0.3811) Steps 568(589.73) | Grad Norm 5.1911(1.7883) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 23.7996, Epoch Time 380.9576(384.9839), Bit/dim 3.7378(best: 3.7416), Xent 1.0949, Loss 4.2852, Error 0.3942(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 56.2039(56.8518) | Bit/dim 3.7522(3.7454) | Xent 1.0600(1.0684) | Loss 13.6248(10.7020) | Error 0.3770(0.3810) Steps 586(589.62) | Grad Norm 3.0852(1.8272) | Total Time 0.00(0.00)\n",
      "Iter 0848 | Time 56.9172(56.8538) | Bit/dim 3.7364(3.7451) | Xent 1.0404(1.0676) | Loss 10.0643(10.6829) | Error 0.3721(0.3807) Steps 586(589.51) | Grad Norm 2.9565(1.8611) | Total Time 0.00(0.00)\n",
      "Iter 0849 | Time 54.0232(56.7689) | Bit/dim 3.7365(3.7448) | Xent 1.0316(1.0665) | Loss 10.0326(10.6633) | Error 0.3695(0.3804) Steps 592(589.59) | Grad Norm 3.3389(1.9054) | Total Time 0.00(0.00)\n",
      "Iter 0850 | Time 53.4095(56.6681) | Bit/dim 3.7315(3.7444) | Xent 1.0264(1.0653) | Loss 9.8287(10.6383) | Error 0.3638(0.3799) Steps 586(589.48) | Grad Norm 1.1156(1.8817) | Total Time 0.00(0.00)\n",
      "Iter 0851 | Time 52.0228(56.5287) | Bit/dim 3.7462(3.7445) | Xent 1.0441(1.0647) | Loss 10.0410(10.6204) | Error 0.3745(0.3797) Steps 580(589.20) | Grad Norm 2.9791(1.9147) | Total Time 0.00(0.00)\n",
      "Iter 0852 | Time 54.0709(56.4550) | Bit/dim 3.7336(3.7442) | Xent 1.0443(1.0640) | Loss 9.9031(10.5989) | Error 0.3695(0.3794) Steps 580(588.92) | Grad Norm 1.5769(1.9045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 23.4608, Epoch Time 366.6033(384.4325), Bit/dim 3.7414(best: 3.7378), Xent 1.0917, Loss 4.2872, Error 0.3960(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 55.2626(56.4192) | Bit/dim 3.7431(3.7441) | Xent 1.0322(1.0631) | Loss 13.6905(10.6916) | Error 0.3672(0.3791) Steps 580(588.65) | Grad Norm 2.8283(1.9322) | Total Time 0.00(0.00)\n",
      "Iter 0854 | Time 55.8161(56.4011) | Bit/dim 3.7351(3.7438) | Xent 1.0513(1.0627) | Loss 10.0500(10.6724) | Error 0.3778(0.3790) Steps 598(588.93) | Grad Norm 3.1147(1.9677) | Total Time 0.00(0.00)\n",
      "Iter 0855 | Time 57.4313(56.4320) | Bit/dim 3.7394(3.7437) | Xent 1.0468(1.0623) | Loss 9.9082(10.6494) | Error 0.3772(0.3790) Steps 574(588.49) | Grad Norm 0.9172(1.9362) | Total Time 0.00(0.00)\n",
      "Iter 0856 | Time 56.6597(56.4389) | Bit/dim 3.7418(3.7437) | Xent 1.0215(1.0610) | Loss 10.0697(10.6321) | Error 0.3686(0.3787) Steps 580(588.23) | Grad Norm 1.6480(1.9276) | Total Time 0.00(0.00)\n",
      "Iter 0857 | Time 59.8656(56.5417) | Bit/dim 3.7378(3.7435) | Xent 1.0493(1.0607) | Loss 9.9769(10.6124) | Error 0.3719(0.3785) Steps 604(588.70) | Grad Norm 1.5561(1.9164) | Total Time 0.00(0.00)\n",
      "Iter 0858 | Time 56.1616(56.5303) | Bit/dim 3.7376(3.7433) | Xent 1.0442(1.0602) | Loss 10.0250(10.5948) | Error 0.3705(0.3782) Steps 580(588.44) | Grad Norm 1.4233(1.9016) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 23.2881, Epoch Time 380.8277(384.3244), Bit/dim 3.7406(best: 3.7378), Xent 1.0886, Loss 4.2850, Error 0.3905(best: 0.3919)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 55.7978(56.5083) | Bit/dim 3.7403(3.7432) | Xent 1.0443(1.0597) | Loss 13.5418(10.6832) | Error 0.3708(0.3780) Steps 622(589.45) | Grad Norm 2.0965(1.9075) | Total Time 0.00(0.00)\n",
      "Iter 0860 | Time 55.5787(56.4804) | Bit/dim 3.7306(3.7428) | Xent 1.0284(1.0588) | Loss 10.1044(10.6658) | Error 0.3640(0.3776) Steps 598(589.71) | Grad Norm 1.4875(1.8949) | Total Time 0.00(0.00)\n",
      "Iter 0861 | Time 57.7293(56.5179) | Bit/dim 3.7429(3.7428) | Xent 1.0576(1.0587) | Loss 10.0208(10.6465) | Error 0.3722(0.3774) Steps 622(590.67) | Grad Norm 1.8143(1.8924) | Total Time 0.00(0.00)\n",
      "Iter 0862 | Time 56.9249(56.5301) | Bit/dim 3.7401(3.7428) | Xent 1.0226(1.0577) | Loss 10.0160(10.6276) | Error 0.3638(0.3770) Steps 592(590.71) | Grad Norm 1.7252(1.8874) | Total Time 0.00(0.00)\n",
      "Iter 0863 | Time 59.1879(56.6098) | Bit/dim 3.7411(3.7427) | Xent 1.0329(1.0569) | Loss 10.0895(10.6114) | Error 0.3670(0.3767) Steps 556(589.67) | Grad Norm 1.5020(1.8759) | Total Time 0.00(0.00)\n",
      "Iter 0864 | Time 55.0390(56.5627) | Bit/dim 3.7365(3.7425) | Xent 1.0228(1.0559) | Loss 10.0870(10.5957) | Error 0.3714(0.3766) Steps 574(589.20) | Grad Norm 2.7435(1.9019) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 22.8064, Epoch Time 379.3911(384.1764), Bit/dim 3.7444(best: 3.7378), Xent 1.0927, Loss 4.2907, Error 0.3909(best: 0.3905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 52.7382(56.4480) | Bit/dim 3.7431(3.7425) | Xent 1.0600(1.0560) | Loss 13.7792(10.6912) | Error 0.3789(0.3766) Steps 580(588.93) | Grad Norm 2.4374(1.9180) | Total Time 0.00(0.00)\n",
      "Iter 0866 | Time 59.3056(56.5337) | Bit/dim 3.7308(3.7422) | Xent 1.0312(1.0553) | Loss 9.9691(10.6695) | Error 0.3681(0.3764) Steps 580(588.66) | Grad Norm 2.2330(1.9274) | Total Time 0.00(0.00)\n",
      "Iter 0867 | Time 57.4836(56.5622) | Bit/dim 3.7445(3.7423) | Xent 1.0273(1.0544) | Loss 9.8125(10.6438) | Error 0.3672(0.3761) Steps 640(590.20) | Grad Norm 3.1122(1.9630) | Total Time 0.00(0.00)\n",
      "Iter 0868 | Time 57.3950(56.5872) | Bit/dim 3.7390(3.7422) | Xent 1.0286(1.0537) | Loss 9.8841(10.6210) | Error 0.3652(0.3758) Steps 604(590.61) | Grad Norm 4.5939(2.0419) | Total Time 0.00(0.00)\n",
      "Iter 0869 | Time 55.9423(56.5678) | Bit/dim 3.7347(3.7419) | Xent 1.0249(1.0528) | Loss 9.9589(10.6012) | Error 0.3655(0.3755) Steps 562(589.75) | Grad Norm 2.3109(2.0500) | Total Time 0.00(0.00)\n",
      "Iter 0870 | Time 54.7703(56.5139) | Bit/dim 3.7357(3.7417) | Xent 1.0324(1.0522) | Loss 9.8921(10.5799) | Error 0.3605(0.3750) Steps 580(589.46) | Grad Norm 2.7983(2.0724) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 23.3797, Epoch Time 377.1247(383.9648), Bit/dim 3.7424(best: 3.7378), Xent 1.0904, Loss 4.2875, Error 0.3911(best: 0.3905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 52.7443(56.4008) | Bit/dim 3.7530(3.7421) | Xent 1.0256(1.0514) | Loss 13.6695(10.6726) | Error 0.3684(0.3748) Steps 568(588.82) | Grad Norm 3.9313(2.1282) | Total Time 0.00(0.00)\n",
      "Iter 0872 | Time 56.5682(56.4058) | Bit/dim 3.7335(3.7418) | Xent 1.0268(1.0506) | Loss 9.9541(10.6510) | Error 0.3699(0.3747) Steps 586(588.73) | Grad Norm 3.0747(2.1566) | Total Time 0.00(0.00)\n",
      "Iter 0873 | Time 58.0977(56.4566) | Bit/dim 3.7448(3.7419) | Xent 1.0149(1.0496) | Loss 10.1224(10.6352) | Error 0.3691(0.3745) Steps 568(588.11) | Grad Norm 3.3773(2.1932) | Total Time 0.00(0.00)\n",
      "Iter 0874 | Time 55.1426(56.4172) | Bit/dim 3.7379(3.7418) | Xent 1.0435(1.0494) | Loss 10.1086(10.6194) | Error 0.3704(0.3744) Steps 586(588.05) | Grad Norm 5.3946(2.2892) | Total Time 0.00(0.00)\n",
      "Iter 0875 | Time 57.5454(56.4510) | Bit/dim 3.7255(3.7413) | Xent 1.0223(1.0486) | Loss 10.0532(10.6024) | Error 0.3698(0.3742) Steps 568(587.45) | Grad Norm 1.6766(2.2709) | Total Time 0.00(0.00)\n",
      "Iter 0876 | Time 57.1121(56.4709) | Bit/dim 3.7313(3.7410) | Xent 1.0421(1.0484) | Loss 9.9632(10.5832) | Error 0.3688(0.3741) Steps 574(587.04) | Grad Norm 1.7222(2.2544) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 23.8150, Epoch Time 377.4457(383.7692), Bit/dim 3.7341(best: 3.7378), Xent 1.0883, Loss 4.2783, Error 0.3931(best: 0.3905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 55.6149(56.4452) | Bit/dim 3.7457(3.7411) | Xent 1.0080(1.0472) | Loss 13.3558(10.6664) | Error 0.3520(0.3734) Steps 568(586.47) | Grad Norm 1.1988(2.2227) | Total Time 0.00(0.00)\n",
      "Iter 0878 | Time 60.1530(56.5564) | Bit/dim 3.7313(3.7408) | Xent 1.0222(1.0464) | Loss 9.9728(10.6456) | Error 0.3670(0.3732) Steps 592(586.64) | Grad Norm 1.8096(2.2103) | Total Time 0.00(0.00)\n",
      "Iter 0879 | Time 56.6035(56.5578) | Bit/dim 3.7436(3.7409) | Xent 1.0228(1.0457) | Loss 9.9054(10.6234) | Error 0.3686(0.3731) Steps 586(586.62) | Grad Norm 0.8598(2.1698) | Total Time 0.00(0.00)\n",
      "Iter 0880 | Time 54.6072(56.4993) | Bit/dim 3.7393(3.7409) | Xent 1.0369(1.0454) | Loss 9.7162(10.5962) | Error 0.3735(0.3731) Steps 586(586.60) | Grad Norm 2.4382(2.1779) | Total Time 0.00(0.00)\n",
      "Iter 0881 | Time 54.9743(56.4536) | Bit/dim 3.7320(3.7406) | Xent 1.0159(1.0446) | Loss 10.0268(10.5791) | Error 0.3631(0.3728) Steps 586(586.58) | Grad Norm 1.9810(2.1720) | Total Time 0.00(0.00)\n",
      "Iter 0882 | Time 57.6149(56.4884) | Bit/dim 3.7329(3.7404) | Xent 1.0326(1.0442) | Loss 9.9919(10.5615) | Error 0.3640(0.3725) Steps 592(586.74) | Grad Norm 2.0089(2.1671) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 23.1702, Epoch Time 379.4533(383.6398), Bit/dim 3.7435(best: 3.7341), Xent 1.0868, Loss 4.2868, Error 0.3934(best: 0.3905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 57.6982(56.5247) | Bit/dim 3.7329(3.7402) | Xent 1.0317(1.0438) | Loss 13.5712(10.6518) | Error 0.3632(0.3722) Steps 586(586.72) | Grad Norm 1.7860(2.1556) | Total Time 0.00(0.00)\n",
      "Iter 0884 | Time 55.9395(56.5071) | Bit/dim 3.7332(3.7400) | Xent 1.0224(1.0432) | Loss 10.0111(10.6325) | Error 0.3648(0.3720) Steps 568(586.16) | Grad Norm 1.8408(2.1462) | Total Time 0.00(0.00)\n",
      "Iter 0885 | Time 57.0171(56.5224) | Bit/dim 3.7355(3.7398) | Xent 1.0002(1.0419) | Loss 9.9685(10.6126) | Error 0.3524(0.3714) Steps 598(586.52) | Grad Norm 4.5475(2.2182) | Total Time 0.00(0.00)\n",
      "Iter 0886 | Time 58.7065(56.5880) | Bit/dim 3.7318(3.7396) | Xent 1.0287(1.0415) | Loss 9.8364(10.5893) | Error 0.3715(0.3714) Steps 592(586.68) | Grad Norm 4.3996(2.2837) | Total Time 0.00(0.00)\n",
      "Iter 0887 | Time 57.8195(56.6249) | Bit/dim 3.7445(3.7397) | Xent 1.0055(1.0404) | Loss 10.0139(10.5721) | Error 0.3560(0.3710) Steps 610(587.38) | Grad Norm 2.8293(2.3000) | Total Time 0.00(0.00)\n",
      "Iter 0888 | Time 62.3566(56.7969) | Bit/dim 3.7424(3.7398) | Xent 1.0202(1.0398) | Loss 10.1118(10.5583) | Error 0.3598(0.3706) Steps 592(587.52) | Grad Norm 1.8426(2.2863) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 22.9321, Epoch Time 388.6576(383.7903), Bit/dim 3.7398(best: 3.7341), Xent 1.0867, Loss 4.2832, Error 0.3895(best: 0.3905)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 60.8546(56.9186) | Bit/dim 3.7377(3.7397) | Xent 1.0498(1.0401) | Loss 13.7515(10.6540) | Error 0.3764(0.3708) Steps 610(588.19) | Grad Norm 2.5853(2.2953) | Total Time 0.00(0.00)\n",
      "Iter 0890 | Time 56.5867(56.9086) | Bit/dim 3.7303(3.7395) | Xent 1.0054(1.0391) | Loss 9.9655(10.6334) | Error 0.3552(0.3703) Steps 604(588.67) | Grad Norm 3.4285(2.3293) | Total Time 0.00(0.00)\n",
      "Iter 0891 | Time 54.9876(56.8510) | Bit/dim 3.7367(3.7394) | Xent 1.0181(1.0384) | Loss 9.8251(10.6091) | Error 0.3609(0.3701) Steps 586(588.59) | Grad Norm 2.5417(2.3357) | Total Time 0.00(0.00)\n",
      "Iter 0892 | Time 58.8946(56.9123) | Bit/dim 3.7364(3.7393) | Xent 1.0041(1.0374) | Loss 9.7071(10.5821) | Error 0.3582(0.3697) Steps 580(588.33) | Grad Norm 1.9393(2.3238) | Total Time 0.00(0.00)\n",
      "Iter 0893 | Time 57.5597(56.9317) | Bit/dim 3.7291(3.7390) | Xent 0.9876(1.0359) | Loss 9.9592(10.5634) | Error 0.3569(0.3693) Steps 562(587.54) | Grad Norm 1.3248(2.2938) | Total Time 0.00(0.00)\n",
      "Iter 0894 | Time 60.8495(57.0493) | Bit/dim 3.7367(3.7389) | Xent 1.0255(1.0356) | Loss 10.1307(10.5504) | Error 0.3679(0.3693) Steps 574(587.13) | Grad Norm 2.8938(2.3118) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 23.4226, Epoch Time 389.5651(383.9635), Bit/dim 3.7332(best: 3.7341), Xent 1.0891, Loss 4.2777, Error 0.3923(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 54.3308(56.9677) | Bit/dim 3.7298(3.7386) | Xent 1.0253(1.0353) | Loss 13.7354(10.6460) | Error 0.3669(0.3692) Steps 574(586.74) | Grad Norm 4.7848(2.3860) | Total Time 0.00(0.00)\n",
      "Iter 0896 | Time 55.2082(56.9149) | Bit/dim 3.7398(3.7387) | Xent 1.0169(1.0347) | Loss 10.0125(10.6270) | Error 0.3635(0.3690) Steps 592(586.90) | Grad Norm 5.9324(2.4924) | Total Time 0.00(0.00)\n",
      "Iter 0897 | Time 54.2207(56.8341) | Bit/dim 3.7484(3.7390) | Xent 1.0369(1.0348) | Loss 9.9982(10.6081) | Error 0.3696(0.3691) Steps 550(585.79) | Grad Norm 6.0544(2.5992) | Total Time 0.00(0.00)\n",
      "Iter 0898 | Time 55.6523(56.7986) | Bit/dim 3.7362(3.7389) | Xent 1.0198(1.0344) | Loss 9.7905(10.5836) | Error 0.3661(0.3690) Steps 592(585.98) | Grad Norm 3.6745(2.6315) | Total Time 0.00(0.00)\n",
      "Iter 0899 | Time 53.2635(56.6926) | Bit/dim 3.7394(3.7389) | Xent 0.9909(1.0331) | Loss 9.9157(10.5635) | Error 0.3516(0.3684) Steps 574(585.62) | Grad Norm 4.1604(2.6774) | Total Time 0.00(0.00)\n",
      "Iter 0900 | Time 53.7104(56.6031) | Bit/dim 3.7317(3.7387) | Xent 1.0051(1.0322) | Loss 9.8518(10.5422) | Error 0.3646(0.3683) Steps 580(585.45) | Grad Norm 7.1204(2.8107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 23.2978, Epoch Time 366.3448(383.4350), Bit/dim 3.7427(best: 3.7332), Xent 1.0983, Loss 4.2919, Error 0.3944(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 59.7854(56.6986) | Bit/dim 3.7398(3.7387) | Xent 1.0193(1.0318) | Loss 13.3234(10.6256) | Error 0.3642(0.3682) Steps 592(585.65) | Grad Norm 7.2450(2.9437) | Total Time 0.00(0.00)\n",
      "Iter 0902 | Time 53.9244(56.6154) | Bit/dim 3.7345(3.7386) | Xent 1.0186(1.0314) | Loss 9.9381(10.6050) | Error 0.3636(0.3681) Steps 574(585.30) | Grad Norm 3.2564(2.9531) | Total Time 0.00(0.00)\n",
      "Iter 0903 | Time 58.1544(56.6615) | Bit/dim 3.7324(3.7384) | Xent 1.0133(1.0309) | Loss 9.8008(10.5809) | Error 0.3622(0.3679) Steps 634(586.76) | Grad Norm 3.6566(2.9742) | Total Time 0.00(0.00)\n",
      "Iter 0904 | Time 56.3621(56.6525) | Bit/dim 3.7312(3.7382) | Xent 1.0084(1.0302) | Loss 9.9606(10.5623) | Error 0.3645(0.3678) Steps 586(586.73) | Grad Norm 7.3698(3.1060) | Total Time 0.00(0.00)\n",
      "Iter 0905 | Time 55.4329(56.6160) | Bit/dim 3.7436(3.7383) | Xent 0.9874(1.0289) | Loss 9.7347(10.5374) | Error 0.3486(0.3672) Steps 580(586.53) | Grad Norm 3.0226(3.1035) | Total Time 0.00(0.00)\n",
      "Iter 0906 | Time 62.4835(56.7920) | Bit/dim 3.7262(3.7380) | Xent 1.0154(1.0285) | Loss 10.0073(10.5215) | Error 0.3644(0.3671) Steps 622(587.60) | Grad Norm 4.6264(3.1492) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 23.3451, Epoch Time 385.9451(383.5103), Bit/dim 3.7424(best: 3.7332), Xent 1.0935, Loss 4.2892, Error 0.3954(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 57.5264(56.8140) | Bit/dim 3.7376(3.7380) | Xent 0.9956(1.0275) | Loss 13.1431(10.6002) | Error 0.3591(0.3669) Steps 592(587.73) | Grad Norm 5.0579(3.2065) | Total Time 0.00(0.00)\n",
      "Iter 0908 | Time 55.4032(56.7717) | Bit/dim 3.7289(3.7377) | Xent 0.9907(1.0264) | Loss 9.9747(10.5814) | Error 0.3569(0.3666) Steps 592(587.86) | Grad Norm 1.8075(3.1645) | Total Time 0.00(0.00)\n",
      "Iter 0909 | Time 53.2952(56.6674) | Bit/dim 3.7389(3.7377) | Xent 1.0062(1.0258) | Loss 9.8322(10.5589) | Error 0.3525(0.3662) Steps 592(587.98) | Grad Norm 3.5289(3.1754) | Total Time 0.00(0.00)\n",
      "Iter 0910 | Time 59.6360(56.7565) | Bit/dim 3.7416(3.7379) | Xent 1.0169(1.0256) | Loss 9.7837(10.5357) | Error 0.3610(0.3660) Steps 616(588.82) | Grad Norm 7.0398(3.2914) | Total Time 0.00(0.00)\n",
      "Iter 0911 | Time 55.0863(56.7064) | Bit/dim 3.7302(3.7376) | Xent 1.0069(1.0250) | Loss 9.8541(10.5152) | Error 0.3604(0.3658) Steps 592(588.92) | Grad Norm 2.9409(3.2809) | Total Time 0.00(0.00)\n",
      "Iter 0912 | Time 61.6565(56.8549) | Bit/dim 3.7345(3.7375) | Xent 1.0122(1.0246) | Loss 9.8806(10.4962) | Error 0.3616(0.3657) Steps 568(588.29) | Grad Norm 6.0125(3.3628) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 23.4323, Epoch Time 382.2199(383.4716), Bit/dim 3.7394(best: 3.7332), Xent 1.1107, Loss 4.2948, Error 0.3995(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 55.9287(56.8271) | Bit/dim 3.7459(3.7378) | Xent 1.0353(1.0249) | Loss 13.8781(10.5977) | Error 0.3684(0.3658) Steps 610(588.94) | Grad Norm 9.6221(3.5506) | Total Time 0.00(0.00)\n",
      "Iter 0914 | Time 63.9808(57.0417) | Bit/dim 3.7282(3.7375) | Xent 1.0337(1.0252) | Loss 10.0781(10.5821) | Error 0.3629(0.3657) Steps 604(589.39) | Grad Norm 7.3344(3.6641) | Total Time 0.00(0.00)\n",
      "Iter 0915 | Time 55.6183(56.9990) | Bit/dim 3.7289(3.7372) | Xent 0.9849(1.0240) | Loss 9.8723(10.5608) | Error 0.3534(0.3653) Steps 586(589.29) | Grad Norm 3.5273(3.6600) | Total Time 0.00(0.00)\n",
      "Iter 0916 | Time 58.1885(57.0347) | Bit/dim 3.7360(3.7372) | Xent 1.0165(1.0238) | Loss 10.1192(10.5475) | Error 0.3629(0.3653) Steps 586(589.19) | Grad Norm 6.7352(3.7523) | Total Time 0.00(0.00)\n",
      "Iter 0917 | Time 59.3690(57.1047) | Bit/dim 3.7347(3.7371) | Xent 1.0016(1.0231) | Loss 10.0275(10.5319) | Error 0.3618(0.3652) Steps 586(589.10) | Grad Norm 5.7472(3.8121) | Total Time 0.00(0.00)\n",
      "Iter 0918 | Time 60.7976(57.2155) | Bit/dim 3.7371(3.7371) | Xent 0.9840(1.0219) | Loss 10.0628(10.5178) | Error 0.3546(0.3648) Steps 586(589.00) | Grad Norm 2.1509(3.7623) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 23.3747, Epoch Time 393.7911(383.7812), Bit/dim 3.7376(best: 3.7332), Xent 1.0840, Loss 4.2796, Error 0.3905(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 56.5997(57.1970) | Bit/dim 3.7391(3.7372) | Xent 0.9882(1.0209) | Loss 13.6924(10.6131) | Error 0.3510(0.3644) Steps 586(588.91) | Grad Norm 3.6372(3.7585) | Total Time 0.00(0.00)\n",
      "Iter 0920 | Time 57.8623(57.2170) | Bit/dim 3.7375(3.7372) | Xent 0.9861(1.0199) | Loss 9.9995(10.5947) | Error 0.3500(0.3640) Steps 598(589.19) | Grad Norm 1.9854(3.7053) | Total Time 0.00(0.00)\n",
      "Iter 0921 | Time 63.7358(57.4125) | Bit/dim 3.7290(3.7369) | Xent 1.0108(1.0196) | Loss 9.9594(10.5756) | Error 0.3611(0.3639) Steps 586(589.09) | Grad Norm 3.8298(3.7091) | Total Time 0.00(0.00)\n",
      "Iter 0922 | Time 58.9482(57.4586) | Bit/dim 3.7322(3.7368) | Xent 0.9865(1.0186) | Loss 10.0781(10.5607) | Error 0.3556(0.3637) Steps 598(589.36) | Grad Norm 2.6415(3.6770) | Total Time 0.00(0.00)\n",
      "Iter 0923 | Time 58.2400(57.4820) | Bit/dim 3.7322(3.7367) | Xent 0.9862(1.0176) | Loss 10.0967(10.5468) | Error 0.3491(0.3632) Steps 628(590.52) | Grad Norm 1.5382(3.6129) | Total Time 0.00(0.00)\n",
      "Iter 0924 | Time 58.5776(57.5149) | Bit/dim 3.7359(3.7366) | Xent 0.9848(1.0166) | Loss 9.7340(10.5224) | Error 0.3481(0.3628) Steps 586(590.38) | Grad Norm 2.8692(3.5906) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 23.1824, Epoch Time 393.4759(384.0720), Bit/dim 3.7361(best: 3.7332), Xent 1.0853, Loss 4.2788, Error 0.3891(best: 0.3895)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 55.8626(57.4653) | Bit/dim 3.7431(3.7368) | Xent 0.9939(1.0160) | Loss 13.4613(10.6106) | Error 0.3560(0.3626) Steps 592(590.43) | Grad Norm 2.6190(3.5614) | Total Time 0.00(0.00)\n",
      "Iter 0926 | Time 60.2788(57.5497) | Bit/dim 3.7260(3.7365) | Xent 0.9843(1.0150) | Loss 9.8508(10.5878) | Error 0.3528(0.3623) Steps 574(589.94) | Grad Norm 2.6594(3.5343) | Total Time 0.00(0.00)\n",
      "Iter 0927 | Time 56.8377(57.5284) | Bit/dim 3.7338(3.7364) | Xent 0.9879(1.0142) | Loss 9.9472(10.5686) | Error 0.3560(0.3621) Steps 610(590.54) | Grad Norm 4.0083(3.5486) | Total Time 0.00(0.00)\n",
      "Iter 0928 | Time 59.7213(57.5942) | Bit/dim 3.7346(3.7364) | Xent 1.0040(1.0139) | Loss 10.0144(10.5519) | Error 0.3546(0.3619) Steps 604(590.94) | Grad Norm 4.1955(3.5680) | Total Time 0.00(0.00)\n",
      "Iter 0929 | Time 53.1993(57.4623) | Bit/dim 3.7338(3.7363) | Xent 0.9941(1.0133) | Loss 9.8868(10.5320) | Error 0.3508(0.3615) Steps 574(590.43) | Grad Norm 4.0888(3.5836) | Total Time 0.00(0.00)\n",
      "Iter 0930 | Time 56.4308(57.4314) | Bit/dim 3.7273(3.7360) | Xent 0.9894(1.0126) | Loss 9.9027(10.5131) | Error 0.3516(0.3612) Steps 622(591.38) | Grad Norm 2.7194(3.5577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 22.9558, Epoch Time 381.6084(383.9981), Bit/dim 3.7357(best: 3.7332), Xent 1.0770, Loss 4.2742, Error 0.3886(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0931 | Time 58.8217(57.4731) | Bit/dim 3.7160(3.7354) | Xent 0.9723(1.0114) | Loss 13.4480(10.6011) | Error 0.3436(0.3607) Steps 598(591.58) | Grad Norm 1.8924(3.5077) | Total Time 0.00(0.00)\n",
      "Iter 0932 | Time 57.5610(57.4757) | Bit/dim 3.7385(3.7355) | Xent 0.9899(1.0107) | Loss 9.7216(10.5748) | Error 0.3504(0.3604) Steps 604(591.95) | Grad Norm 2.1177(3.4660) | Total Time 0.00(0.00)\n",
      "Iter 0933 | Time 58.7943(57.5153) | Bit/dim 3.7369(3.7356) | Xent 0.9738(1.0096) | Loss 9.8947(10.5544) | Error 0.3444(0.3599) Steps 610(592.49) | Grad Norm 1.5292(3.4079) | Total Time 0.00(0.00)\n",
      "Iter 0934 | Time 55.7530(57.4624) | Bit/dim 3.7383(3.7356) | Xent 0.9780(1.0087) | Loss 10.0463(10.5391) | Error 0.3449(0.3595) Steps 598(592.66) | Grad Norm 3.3014(3.4047) | Total Time 0.00(0.00)\n",
      "Iter 0935 | Time 51.5764(57.2858) | Bit/dim 3.7346(3.7356) | Xent 0.9802(1.0078) | Loss 9.8188(10.5175) | Error 0.3536(0.3593) Steps 592(592.64) | Grad Norm 4.7487(3.4450) | Total Time 0.00(0.00)\n",
      "Iter 0936 | Time 58.1662(57.3122) | Bit/dim 3.7409(3.7358) | Xent 1.0041(1.0077) | Loss 9.9816(10.5014) | Error 0.3588(0.3593) Steps 616(593.34) | Grad Norm 3.1758(3.4370) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 23.1855, Epoch Time 380.2718(383.8863), Bit/dim 3.7323(best: 3.7332), Xent 1.0801, Loss 4.2723, Error 0.3871(best: 0.3886)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0937 | Time 62.3535(57.4635) | Bit/dim 3.7309(3.7356) | Xent 0.9730(1.0067) | Loss 13.8617(10.6022) | Error 0.3479(0.3589) Steps 592(593.30) | Grad Norm 2.9277(3.4217) | Total Time 0.00(0.00)\n",
      "Iter 0938 | Time 58.3146(57.4890) | Bit/dim 3.7348(3.7356) | Xent 0.9740(1.0057) | Loss 9.9149(10.5816) | Error 0.3395(0.3583) Steps 598(593.44) | Grad Norm 2.6157(3.3975) | Total Time 0.00(0.00)\n",
      "Iter 0939 | Time 55.8975(57.4413) | Bit/dim 3.7366(3.7356) | Xent 0.9834(1.0050) | Loss 9.9316(10.5621) | Error 0.3532(0.3582) Steps 562(592.50) | Grad Norm 1.9618(3.3544) | Total Time 0.00(0.00)\n",
      "Iter 0940 | Time 53.6376(57.3272) | Bit/dim 3.7403(3.7358) | Xent 0.9762(1.0042) | Loss 9.9818(10.5447) | Error 0.3400(0.3576) Steps 574(591.94) | Grad Norm 1.9015(3.3108) | Total Time 0.00(0.00)\n",
      "Iter 0941 | Time 57.1123(57.3207) | Bit/dim 3.7199(3.7353) | Xent 0.9870(1.0036) | Loss 10.1122(10.5317) | Error 0.3546(0.3576) Steps 616(592.66) | Grad Norm 4.4749(3.3458) | Total Time 0.00(0.00)\n",
      "Iter 0942 | Time 57.5334(57.3271) | Bit/dim 3.7416(3.7355) | Xent 0.9816(1.0030) | Loss 9.7050(10.5069) | Error 0.3529(0.3574) Steps 592(592.64) | Grad Norm 5.6300(3.4143) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 24.1000, Epoch Time 385.0724(383.9219), Bit/dim 3.7356(best: 3.7323), Xent 1.0981, Loss 4.2847, Error 0.3985(best: 0.3871)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0943 | Time 61.5797(57.4547) | Bit/dim 3.7354(3.7355) | Xent 0.9893(1.0026) | Loss 13.6783(10.6021) | Error 0.3604(0.3575) Steps 574(592.09) | Grad Norm 8.2618(3.5597) | Total Time 0.00(0.00)\n",
      "Iter 0944 | Time 60.3605(57.5419) | Bit/dim 3.7401(3.7356) | Xent 1.0358(1.0036) | Loss 9.9836(10.5835) | Error 0.3696(0.3579) Steps 646(593.70) | Grad Norm 15.5709(3.9200) | Total Time 0.00(0.00)\n",
      "Iter 0945 | Time 57.8625(57.5515) | Bit/dim 3.7288(3.7354) | Xent 1.0846(1.0060) | Loss 10.0620(10.5679) | Error 0.3918(0.3589) Steps 622(594.55) | Grad Norm 17.5745(4.3297) | Total Time 0.00(0.00)\n",
      "Iter 0946 | Time 54.8799(57.4713) | Bit/dim 3.7180(3.7349) | Xent 0.9977(1.0058) | Loss 9.8428(10.5461) | Error 0.3559(0.3588) Steps 598(594.66) | Grad Norm 8.3439(4.4501) | Total Time 0.00(0.00)\n",
      "Iter 0947 | Time 55.0693(57.3993) | Bit/dim 3.7398(3.7350) | Xent 1.0004(1.0056) | Loss 9.8914(10.5265) | Error 0.3539(0.3586) Steps 634(595.84) | Grad Norm 4.9016(4.4637) | Total Time 0.00(0.00)\n",
      "Iter 0948 | Time 63.4265(57.5801) | Bit/dim 3.7356(3.7351) | Xent 1.0093(1.0057) | Loss 9.7874(10.5043) | Error 0.3564(0.3586) Steps 628(596.80) | Grad Norm 5.8098(4.5040) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 23.3297, Epoch Time 392.8746(384.1905), Bit/dim 3.7347(best: 3.7323), Xent 1.1024, Loss 4.2859, Error 0.3982(best: 0.3871)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0949 | Time 53.2887(57.4513) | Bit/dim 3.7292(3.7349) | Xent 1.0047(1.0057) | Loss 13.3676(10.5902) | Error 0.3592(0.3586) Steps 592(596.66) | Grad Norm 5.1887(4.5246) | Total Time 0.00(0.00)\n",
      "Iter 0950 | Time 58.9632(57.4967) | Bit/dim 3.7413(3.7351) | Xent 1.0046(1.0056) | Loss 9.9827(10.5720) | Error 0.3532(0.3584) Steps 592(596.52) | Grad Norm 4.7887(4.5325) | Total Time 0.00(0.00)\n",
      "Iter 0951 | Time 60.5435(57.5881) | Bit/dim 3.7294(3.7349) | Xent 1.0011(1.0055) | Loss 9.8961(10.5517) | Error 0.3598(0.3585) Steps 568(595.66) | Grad Norm 5.2732(4.5547) | Total Time 0.00(0.00)\n",
      "Iter 0952 | Time 55.8804(57.5369) | Bit/dim 3.7204(3.7345) | Xent 1.0023(1.0054) | Loss 9.9951(10.5350) | Error 0.3592(0.3585) Steps 562(594.65) | Grad Norm 2.9808(4.5075) | Total Time 0.00(0.00)\n",
      "Iter 0953 | Time 55.7253(57.4825) | Bit/dim 3.7336(3.7344) | Xent 1.0338(1.0063) | Loss 10.0199(10.5196) | Error 0.3681(0.3588) Steps 610(595.11) | Grad Norm 5.5007(4.5373) | Total Time 0.00(0.00)\n",
      "Iter 0954 | Time 62.8599(57.6438) | Bit/dim 3.7316(3.7344) | Xent 0.9530(1.0047) | Loss 9.8580(10.4997) | Error 0.3354(0.3581) Steps 610(595.56) | Grad Norm 1.8825(4.4577) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 23.3876, Epoch Time 386.9895(384.2744), Bit/dim 3.7368(best: 3.7323), Xent 1.1076, Loss 4.2906, Error 0.3966(best: 0.3871)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0955 | Time 52.4919(57.4893) | Bit/dim 3.7344(3.7344) | Xent 1.0066(1.0047) | Loss 13.3609(10.5855) | Error 0.3588(0.3581) Steps 568(594.73) | Grad Norm 6.5439(4.5202) | Total Time 0.00(0.00)\n",
      "Iter 0956 | Time 58.3392(57.5148) | Bit/dim 3.7420(3.7346) | Xent 0.9930(1.0044) | Loss 10.0971(10.5709) | Error 0.3552(0.3580) Steps 586(594.47) | Grad Norm 2.6991(4.4656) | Total Time 0.00(0.00)\n",
      "Iter 0957 | Time 56.7194(57.4909) | Bit/dim 3.7257(3.7343) | Xent 0.9868(1.0038) | Loss 9.8698(10.5499) | Error 0.3569(0.3580) Steps 616(595.12) | Grad Norm 6.6914(4.5324) | Total Time 0.00(0.00)\n",
      "Iter 0958 | Time 55.4182(57.4287) | Bit/dim 3.7370(3.7344) | Xent 0.9699(1.0028) | Loss 9.7234(10.5251) | Error 0.3441(0.3576) Steps 574(594.48) | Grad Norm 3.0531(4.4880) | Total Time 0.00(0.00)\n",
      "Iter 0959 | Time 59.5052(57.4910) | Bit/dim 3.7309(3.7343) | Xent 0.9947(1.0026) | Loss 9.7537(10.5019) | Error 0.3580(0.3576) Steps 604(594.77) | Grad Norm 6.6004(4.5514) | Total Time 0.00(0.00)\n",
      "Iter 0960 | Time 55.5584(57.4331) | Bit/dim 3.7186(3.7338) | Xent 0.9738(1.0017) | Loss 9.8227(10.4815) | Error 0.3425(0.3571) Steps 592(594.68) | Grad Norm 1.7922(4.4686) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 22.7849, Epoch Time 376.9238(384.0539), Bit/dim 3.7317(best: 3.7323), Xent 1.0908, Loss 4.2771, Error 0.3907(best: 0.3871)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0961 | Time 60.6092(57.5283) | Bit/dim 3.7358(3.7339) | Xent 0.9966(1.0016) | Loss 13.6575(10.5768) | Error 0.3498(0.3569) Steps 592(594.60) | Grad Norm 5.7909(4.5083) | Total Time 0.00(0.00)\n",
      "Iter 0962 | Time 62.6695(57.6826) | Bit/dim 3.7239(3.7336) | Xent 0.9615(1.0004) | Loss 9.9712(10.5587) | Error 0.3420(0.3565) Steps 616(595.25) | Grad Norm 2.1966(4.4389) | Total Time 0.00(0.00)\n",
      "Iter 0963 | Time 55.5222(57.6178) | Bit/dim 3.7341(3.7336) | Xent 0.9792(0.9997) | Loss 10.0001(10.5419) | Error 0.3534(0.3564) Steps 580(594.79) | Grad Norm 4.7529(4.4483) | Total Time 0.00(0.00)\n",
      "Iter 0964 | Time 58.1606(57.6341) | Bit/dim 3.7226(3.7333) | Xent 0.9793(0.9991) | Loss 9.8664(10.5216) | Error 0.3465(0.3561) Steps 556(593.62) | Grad Norm 5.0402(4.4661) | Total Time 0.00(0.00)\n",
      "Iter 0965 | Time 52.1056(57.4682) | Bit/dim 3.7354(3.7333) | Xent 0.9581(0.9979) | Loss 9.7999(10.5000) | Error 0.3365(0.3555) Steps 580(593.22) | Grad Norm 1.6111(4.3804) | Total Time 0.00(0.00)\n",
      "Iter 0966 | Time 55.4276(57.4070) | Bit/dim 3.7263(3.7331) | Xent 0.9575(0.9967) | Loss 9.6592(10.4748) | Error 0.3396(0.3550) Steps 574(592.64) | Grad Norm 2.8656(4.3350) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 23.4153, Epoch Time 384.1495(384.0568), Bit/dim 3.7295(best: 3.7317), Xent 1.0815, Loss 4.2702, Error 0.3851(best: 0.3871)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0967 | Time 53.1055(57.2779) | Bit/dim 3.7442(3.7335) | Xent 0.9834(0.9963) | Loss 13.3242(10.5602) | Error 0.3472(0.3548) Steps 550(591.36) | Grad Norm 2.3682(4.2760) | Total Time 0.00(0.00)\n",
      "Iter 0968 | Time 60.0953(57.3625) | Bit/dim 3.7255(3.7332) | Xent 0.9498(0.9949) | Loss 9.8117(10.5378) | Error 0.3403(0.3543) Steps 634(592.64) | Grad Norm 1.7557(4.2004) | Total Time 0.00(0.00)\n",
      "Iter 0969 | Time 58.4774(57.3959) | Bit/dim 3.7229(3.7329) | Xent 0.9753(0.9943) | Loss 9.9872(10.5213) | Error 0.3528(0.3543) Steps 580(592.26) | Grad Norm 4.0510(4.1959) | Total Time 0.00(0.00)\n",
      "Iter 0970 | Time 56.8470(57.3794) | Bit/dim 3.7336(3.7329) | Xent 0.9485(0.9929) | Loss 9.8266(10.5004) | Error 0.3329(0.3537) Steps 622(593.15) | Grad Norm 3.7790(4.1834) | Total Time 0.00(0.00)\n",
      "Iter 0971 | Time 58.5876(57.4157) | Bit/dim 3.7211(3.7326) | Xent 0.9536(0.9917) | Loss 9.9012(10.4825) | Error 0.3495(0.3535) Steps 580(592.76) | Grad Norm 2.2835(4.1264) | Total Time 0.00(0.00)\n",
      "Iter 0972 | Time 55.6622(57.3631) | Bit/dim 3.7314(3.7325) | Xent 0.9693(0.9911) | Loss 9.8440(10.4633) | Error 0.3479(0.3534) Steps 586(592.56) | Grad Norm 2.9955(4.0925) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 23.4582, Epoch Time 381.9852(383.9946), Bit/dim 3.7277(best: 3.7295), Xent 1.0921, Loss 4.2737, Error 0.3880(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0973 | Time 53.7089(57.2535) | Bit/dim 3.7336(3.7326) | Xent 0.9877(0.9910) | Loss 13.7218(10.5611) | Error 0.3542(0.3534) Steps 604(592.90) | Grad Norm 5.7949(4.1435) | Total Time 0.00(0.00)\n",
      "Iter 0974 | Time 53.3048(57.1350) | Bit/dim 3.7308(3.7325) | Xent 0.9697(0.9903) | Loss 9.8672(10.5402) | Error 0.3475(0.3532) Steps 592(592.87) | Grad Norm 6.4518(4.2128) | Total Time 0.00(0.00)\n",
      "Iter 0975 | Time 55.7470(57.0934) | Bit/dim 3.7241(3.7323) | Xent 0.9612(0.9895) | Loss 9.5063(10.5092) | Error 0.3426(0.3529) Steps 574(592.31) | Grad Norm 3.5439(4.1927) | Total Time 0.00(0.00)\n",
      "Iter 0976 | Time 58.5316(57.1365) | Bit/dim 3.7285(3.7322) | Xent 0.9472(0.9882) | Loss 9.9543(10.4926) | Error 0.3411(0.3525) Steps 598(592.48) | Grad Norm 5.4921(4.2317) | Total Time 0.00(0.00)\n",
      "Iter 0977 | Time 60.8753(57.2487) | Bit/dim 3.7338(3.7322) | Xent 0.9687(0.9876) | Loss 9.9946(10.4776) | Error 0.3416(0.3522) Steps 562(591.56) | Grad Norm 7.5661(4.3317) | Total Time 0.00(0.00)\n",
      "Iter 0978 | Time 57.0838(57.2437) | Bit/dim 3.7203(3.7318) | Xent 0.9501(0.9865) | Loss 9.8489(10.4588) | Error 0.3367(0.3517) Steps 610(592.12) | Grad Norm 5.9315(4.3797) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 23.3461, Epoch Time 379.1482(383.8492), Bit/dim 3.7304(best: 3.7277), Xent 1.0811, Loss 4.2709, Error 0.3888(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0979 | Time 57.7570(57.2591) | Bit/dim 3.7263(3.7317) | Xent 0.9418(0.9851) | Loss 13.2064(10.5412) | Error 0.3359(0.3513) Steps 586(591.93) | Grad Norm 4.1193(4.3719) | Total Time 0.00(0.00)\n",
      "Iter 0980 | Time 56.7129(57.2427) | Bit/dim 3.7315(3.7317) | Xent 0.9625(0.9845) | Loss 9.9472(10.5234) | Error 0.3439(0.3510) Steps 604(592.29) | Grad Norm 3.8285(4.3556) | Total Time 0.00(0.00)\n",
      "Iter 0981 | Time 51.1483(57.0599) | Bit/dim 3.7340(3.7317) | Xent 0.9340(0.9829) | Loss 9.7710(10.5008) | Error 0.3339(0.3505) Steps 574(591.75) | Grad Norm 1.8046(4.2791) | Total Time 0.00(0.00)\n",
      "Iter 0982 | Time 59.3414(57.1283) | Bit/dim 3.7113(3.7311) | Xent 0.9590(0.9822) | Loss 9.9862(10.4854) | Error 0.3364(0.3501) Steps 616(592.47) | Grad Norm 3.8049(4.2649) | Total Time 0.00(0.00)\n",
      "Iter 0983 | Time 56.8305(57.1194) | Bit/dim 3.7415(3.7314) | Xent 0.9585(0.9815) | Loss 9.8587(10.4666) | Error 0.3431(0.3499) Steps 616(593.18) | Grad Norm 4.2432(4.2642) | Total Time 0.00(0.00)\n",
      "Iter 0984 | Time 56.4711(57.1000) | Bit/dim 3.7243(3.7312) | Xent 0.9487(0.9805) | Loss 10.0200(10.4532) | Error 0.3405(0.3496) Steps 598(593.32) | Grad Norm 5.0970(4.2892) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 23.3731, Epoch Time 378.0867(383.6764), Bit/dim 3.7287(best: 3.7277), Xent 1.0916, Loss 4.2745, Error 0.3887(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0985 | Time 57.8254(57.1217) | Bit/dim 3.7338(3.7313) | Xent 0.9393(0.9793) | Loss 13.6624(10.5494) | Error 0.3355(0.3492) Steps 586(593.10) | Grad Norm 7.9316(4.3985) | Total Time 0.00(0.00)\n",
      "Iter 0986 | Time 57.2351(57.1251) | Bit/dim 3.7210(3.7310) | Xent 0.9571(0.9786) | Loss 9.8453(10.5283) | Error 0.3459(0.3491) Steps 598(593.25) | Grad Norm 7.4402(4.4897) | Total Time 0.00(0.00)\n",
      "Iter 0987 | Time 54.0311(57.0323) | Bit/dim 3.7310(3.7310) | Xent 0.9559(0.9779) | Loss 9.8893(10.5092) | Error 0.3420(0.3489) Steps 556(592.13) | Grad Norm 5.1813(4.5105) | Total Time 0.00(0.00)\n",
      "Iter 0988 | Time 58.5541(57.0780) | Bit/dim 3.7284(3.7309) | Xent 0.9416(0.9769) | Loss 9.9207(10.4915) | Error 0.3420(0.3487) Steps 610(592.67) | Grad Norm 4.2103(4.5015) | Total Time 0.00(0.00)\n",
      "Iter 0989 | Time 59.0292(57.1365) | Bit/dim 3.7306(3.7309) | Xent 0.9344(0.9756) | Loss 9.8696(10.4728) | Error 0.3331(0.3482) Steps 598(592.83) | Grad Norm 1.8380(4.4216) | Total Time 0.00(0.00)\n",
      "Iter 0990 | Time 58.3801(57.1738) | Bit/dim 3.7302(3.7309) | Xent 0.9523(0.9749) | Loss 9.8119(10.4530) | Error 0.3393(0.3479) Steps 592(592.80) | Grad Norm 3.9574(4.4076) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 23.1297, Epoch Time 384.0206(383.6867), Bit/dim 3.7359(best: 3.7277), Xent 1.0987, Loss 4.2852, Error 0.3916(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0991 | Time 55.7703(57.1317) | Bit/dim 3.7189(3.7305) | Xent 0.9620(0.9745) | Loss 13.5089(10.5447) | Error 0.3409(0.3477) Steps 574(592.24) | Grad Norm 7.6653(4.5054) | Total Time 0.00(0.00)\n",
      "Iter 0992 | Time 61.6489(57.2672) | Bit/dim 3.7274(3.7304) | Xent 0.9582(0.9740) | Loss 9.9542(10.5270) | Error 0.3384(0.3474) Steps 634(593.49) | Grad Norm 8.4106(4.6225) | Total Time 0.00(0.00)\n",
      "Iter 0993 | Time 58.8655(57.3152) | Bit/dim 3.7382(3.7307) | Xent 0.9305(0.9727) | Loss 9.8748(10.5074) | Error 0.3271(0.3468) Steps 610(593.99) | Grad Norm 6.7113(4.6852) | Total Time 0.00(0.00)\n",
      "Iter 0994 | Time 59.3810(57.3771) | Bit/dim 3.7228(3.7304) | Xent 0.9533(0.9721) | Loss 9.8106(10.4865) | Error 0.3370(0.3465) Steps 574(593.39) | Grad Norm 3.8507(4.6601) | Total Time 0.00(0.00)\n",
      "Iter 0995 | Time 56.8253(57.3606) | Bit/dim 3.7171(3.7300) | Xent 0.9195(0.9705) | Loss 9.8122(10.4663) | Error 0.3280(0.3460) Steps 604(593.71) | Grad Norm 1.3375(4.5605) | Total Time 0.00(0.00)\n",
      "Iter 0996 | Time 62.0250(57.5005) | Bit/dim 3.7315(3.7301) | Xent 0.9430(0.9697) | Loss 9.9298(10.4502) | Error 0.3324(0.3456) Steps 622(594.56) | Grad Norm 5.4111(4.5860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 23.2074, Epoch Time 393.6121(383.9845), Bit/dim 3.7366(best: 3.7277), Xent 1.0927, Loss 4.2830, Error 0.3886(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0997 | Time 55.8127(57.4499) | Bit/dim 3.7344(3.7302) | Xent 0.9384(0.9688) | Loss 13.6142(10.5451) | Error 0.3373(0.3453) Steps 610(595.02) | Grad Norm 6.9662(4.6574) | Total Time 0.00(0.00)\n",
      "Iter 0998 | Time 59.1661(57.5014) | Bit/dim 3.7328(3.7303) | Xent 0.9300(0.9676) | Loss 9.9610(10.5276) | Error 0.3366(0.3451) Steps 598(595.11) | Grad Norm 4.8180(4.6622) | Total Time 0.00(0.00)\n",
      "Iter 0999 | Time 55.7703(57.4494) | Bit/dim 3.7194(3.7300) | Xent 0.9423(0.9669) | Loss 9.7009(10.5028) | Error 0.3399(0.3449) Steps 592(595.01) | Grad Norm 2.6541(4.6020) | Total Time 0.00(0.00)\n",
      "Iter 1000 | Time 59.8509(57.5215) | Bit/dim 3.7205(3.7297) | Xent 0.9355(0.9659) | Loss 9.9164(10.4852) | Error 0.3344(0.3446) Steps 616(595.64) | Grad Norm 3.7788(4.5773) | Total Time 0.00(0.00)\n",
      "Iter 1001 | Time 57.5331(57.5218) | Bit/dim 3.7322(3.7297) | Xent 0.9759(0.9662) | Loss 9.8351(10.4657) | Error 0.3492(0.3447) Steps 610(596.08) | Grad Norm 7.9739(4.6792) | Total Time 0.00(0.00)\n",
      "Iter 1002 | Time 54.4045(57.4283) | Bit/dim 3.7324(3.7298) | Xent 0.9563(0.9659) | Loss 9.9271(10.4495) | Error 0.3434(0.3447) Steps 586(595.77) | Grad Norm 10.6757(4.8591) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 23.1572, Epoch Time 382.1405(383.9291), Bit/dim 3.7328(best: 3.7277), Xent 1.1460, Loss 4.3058, Error 0.4041(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1003 | Time 56.7518(57.4080) | Bit/dim 3.7259(3.7297) | Xent 0.9806(0.9664) | Loss 13.4040(10.5382) | Error 0.3498(0.3448) Steps 580(595.30) | Grad Norm 14.8204(5.1579) | Total Time 0.00(0.00)\n",
      "Iter 1004 | Time 59.1529(57.4604) | Bit/dim 3.7256(3.7296) | Xent 1.0045(0.9675) | Loss 10.0012(10.5221) | Error 0.3635(0.3454) Steps 598(595.38) | Grad Norm 13.2775(5.4015) | Total Time 0.00(0.00)\n",
      "Iter 1005 | Time 57.6679(57.4666) | Bit/dim 3.7259(3.7295) | Xent 0.9323(0.9664) | Loss 9.8913(10.5031) | Error 0.3346(0.3451) Steps 598(595.46) | Grad Norm 2.8091(5.3237) | Total Time 0.00(0.00)\n",
      "Iter 1006 | Time 51.0648(57.2745) | Bit/dim 3.7428(3.7299) | Xent 0.9641(0.9664) | Loss 9.7718(10.4812) | Error 0.3471(0.3451) Steps 592(595.36) | Grad Norm 6.5735(5.3612) | Total Time 0.00(0.00)\n",
      "Iter 1007 | Time 59.2138(57.3327) | Bit/dim 3.7312(3.7299) | Xent 0.9443(0.9657) | Loss 9.8547(10.4624) | Error 0.3410(0.3450) Steps 586(595.07) | Grad Norm 5.8068(5.3746) | Total Time 0.00(0.00)\n",
      "Iter 1008 | Time 57.9208(57.3504) | Bit/dim 3.7315(3.7300) | Xent 0.9527(0.9653) | Loss 9.8128(10.4429) | Error 0.3356(0.3447) Steps 616(595.70) | Grad Norm 3.1768(5.3087) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 22.7766, Epoch Time 381.1369(383.8454), Bit/dim 3.7326(best: 3.7277), Xent 1.1017, Loss 4.2834, Error 0.3949(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1009 | Time 57.3782(57.3512) | Bit/dim 3.7165(3.7296) | Xent 0.9716(0.9655) | Loss 13.2519(10.5272) | Error 0.3430(0.3447) Steps 610(596.13) | Grad Norm 6.7692(5.3525) | Total Time 0.00(0.00)\n",
      "Iter 1010 | Time 57.0945(57.3435) | Bit/dim 3.7339(3.7297) | Xent 0.9423(0.9648) | Loss 9.9044(10.5085) | Error 0.3400(0.3445) Steps 604(596.37) | Grad Norm 3.0185(5.2825) | Total Time 0.00(0.00)\n",
      "Iter 1011 | Time 61.1191(57.4568) | Bit/dim 3.7280(3.7296) | Xent 0.9249(0.9636) | Loss 9.7630(10.4861) | Error 0.3293(0.3441) Steps 616(596.96) | Grad Norm 4.8323(5.2689) | Total Time 0.00(0.00)\n",
      "Iter 1012 | Time 57.5473(57.4595) | Bit/dim 3.7265(3.7295) | Xent 0.9232(0.9624) | Loss 10.0767(10.4738) | Error 0.3273(0.3436) Steps 568(596.09) | Grad Norm 3.9545(5.2295) | Total Time 0.00(0.00)\n",
      "Iter 1013 | Time 53.5106(57.3410) | Bit/dim 3.7221(3.7293) | Xent 0.9460(0.9619) | Loss 9.8792(10.4560) | Error 0.3347(0.3433) Steps 574(595.43) | Grad Norm 2.9986(5.1626) | Total Time 0.00(0.00)\n",
      "Iter 1014 | Time 60.7243(57.4425) | Bit/dim 3.7319(3.7294) | Xent 0.9517(0.9616) | Loss 9.9446(10.4407) | Error 0.3417(0.3433) Steps 604(595.68) | Grad Norm 6.9686(5.2168) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 23.3868, Epoch Time 387.0024(383.9401), Bit/dim 3.7268(best: 3.7277), Xent 1.0903, Loss 4.2720, Error 0.3882(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1015 | Time 56.3191(57.4088) | Bit/dim 3.7318(3.7295) | Xent 0.9398(0.9610) | Loss 13.3431(10.5277) | Error 0.3351(0.3430) Steps 598(595.75) | Grad Norm 4.0486(5.1817) | Total Time 0.00(0.00)\n",
      "Iter 1016 | Time 63.8051(57.6007) | Bit/dim 3.7268(3.7294) | Xent 0.9316(0.9601) | Loss 10.0049(10.5121) | Error 0.3355(0.3428) Steps 580(595.28) | Grad Norm 4.3975(5.1582) | Total Time 0.00(0.00)\n",
      "Iter 1017 | Time 58.1549(57.6173) | Bit/dim 3.7211(3.7291) | Xent 0.9145(0.9587) | Loss 9.8758(10.4930) | Error 0.3265(0.3423) Steps 592(595.18) | Grad Norm 5.3942(5.1653) | Total Time 0.00(0.00)\n",
      "Iter 1018 | Time 55.5662(57.5558) | Bit/dim 3.7225(3.7289) | Xent 0.9446(0.9583) | Loss 9.8712(10.4743) | Error 0.3376(0.3422) Steps 580(594.73) | Grad Norm 6.1511(5.1948) | Total Time 0.00(0.00)\n",
      "Iter 1019 | Time 55.9383(57.5073) | Bit/dim 3.7302(3.7290) | Xent 0.9283(0.9574) | Loss 9.7133(10.4515) | Error 0.3293(0.3418) Steps 580(594.28) | Grad Norm 5.5146(5.2044) | Total Time 0.00(0.00)\n",
      "Iter 1020 | Time 55.1501(57.4366) | Bit/dim 3.7226(3.7288) | Xent 0.9249(0.9564) | Loss 9.9224(10.4356) | Error 0.3260(0.3413) Steps 592(594.22) | Grad Norm 2.8760(5.1346) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 23.3933, Epoch Time 384.4118(383.9542), Bit/dim 3.7282(best: 3.7268), Xent 1.0854, Loss 4.2709, Error 0.3845(best: 0.3851)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1021 | Time 52.8563(57.2991) | Bit/dim 3.7211(3.7286) | Xent 0.9266(0.9555) | Loss 13.5115(10.5279) | Error 0.3300(0.3410) Steps 562(593.25) | Grad Norm 3.8681(5.0966) | Total Time 0.00(0.00)\n",
      "Iter 1022 | Time 56.0876(57.2628) | Bit/dim 3.7119(3.7281) | Xent 0.9133(0.9542) | Loss 9.6630(10.5019) | Error 0.3299(0.3406) Steps 604(593.57) | Grad Norm 1.9863(5.0033) | Total Time 0.00(0.00)\n",
      "Iter 1023 | Time 55.9466(57.2233) | Bit/dim 3.7284(3.7281) | Xent 0.9284(0.9535) | Loss 9.7716(10.4800) | Error 0.3313(0.3404) Steps 628(594.60) | Grad Norm 4.6933(4.9940) | Total Time 0.00(0.00)\n",
      "Iter 1024 | Time 59.2251(57.2834) | Bit/dim 3.7399(3.7284) | Xent 0.9335(0.9529) | Loss 9.7043(10.4568) | Error 0.3366(0.3402) Steps 562(593.63) | Grad Norm 3.0333(4.9352) | Total Time 0.00(0.00)\n",
      "Iter 1025 | Time 56.4225(57.2575) | Bit/dim 3.7276(3.7284) | Xent 0.9147(0.9517) | Loss 9.9437(10.4414) | Error 0.3294(0.3399) Steps 580(593.22) | Grad Norm 2.8579(4.8728) | Total Time 0.00(0.00)\n",
      "Iter 1026 | Time 59.1745(57.3150) | Bit/dim 3.7268(3.7283) | Xent 0.9157(0.9506) | Loss 9.8137(10.4225) | Error 0.3264(0.3395) Steps 568(592.46) | Grad Norm 3.6309(4.8356) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 23.3149, Epoch Time 379.3894(383.8173), Bit/dim 3.7318(best: 3.7268), Xent 1.0868, Loss 4.2752, Error 0.3844(best: 0.3845)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1027 | Time 56.6383(57.2947) | Bit/dim 3.7232(3.7282) | Xent 0.9016(0.9492) | Loss 13.1263(10.5037) | Error 0.3229(0.3390) Steps 556(591.37) | Grad Norm 4.4573(4.8242) | Total Time 0.00(0.00)\n",
      "Iter 1028 | Time 60.9619(57.4048) | Bit/dim 3.7304(3.7283) | Xent 0.9121(0.9481) | Loss 9.7777(10.4819) | Error 0.3256(0.3386) Steps 616(592.11) | Grad Norm 2.9201(4.7671) | Total Time 0.00(0.00)\n",
      "Iter 1029 | Time 58.2758(57.4309) | Bit/dim 3.7238(3.7281) | Xent 0.9310(0.9475) | Loss 9.8642(10.4633) | Error 0.3345(0.3385) Steps 598(592.28) | Grad Norm 2.3992(4.6961) | Total Time 0.00(0.00)\n",
      "Iter 1030 | Time 56.4814(57.4024) | Bit/dim 3.7126(3.7277) | Xent 0.9031(0.9462) | Loss 9.7531(10.4420) | Error 0.3226(0.3380) Steps 604(592.63) | Grad Norm 3.6471(4.6646) | Total Time 0.00(0.00)\n",
      "Iter 1031 | Time 53.7626(57.2932) | Bit/dim 3.7364(3.7279) | Xent 0.9211(0.9455) | Loss 9.9402(10.4270) | Error 0.3337(0.3379) Steps 580(592.26) | Grad Norm 3.4779(4.6290) | Total Time 0.00(0.00)\n",
      "Iter 1032 | Time 54.9826(57.2239) | Bit/dim 3.7212(3.7277) | Xent 0.9044(0.9442) | Loss 9.7139(10.4056) | Error 0.3206(0.3374) Steps 580(591.89) | Grad Norm 2.5674(4.5672) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 23.6120, Epoch Time 381.4263(383.7456), Bit/dim 3.7270(best: 3.7268), Xent 1.0877, Loss 4.2708, Error 0.3813(best: 0.3844)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1033 | Time 59.1557(57.2819) | Bit/dim 3.7277(3.7277) | Xent 0.8999(0.9429) | Loss 13.3225(10.4931) | Error 0.3230(0.3369) Steps 586(591.71) | Grad Norm 5.1257(4.5839) | Total Time 0.00(0.00)\n",
      "Iter 1034 | Time 52.9344(57.1514) | Bit/dim 3.7195(3.7275) | Xent 0.9293(0.9425) | Loss 9.6520(10.4679) | Error 0.3306(0.3367) Steps 580(591.36) | Grad Norm 5.3596(4.6072) | Total Time 0.00(0.00)\n",
      "Iter 1035 | Time 56.3724(57.1281) | Bit/dim 3.7238(3.7274) | Xent 0.9473(0.9426) | Loss 9.8584(10.4496) | Error 0.3404(0.3369) Steps 586(591.20) | Grad Norm 11.1821(4.8044) | Total Time 0.00(0.00)\n",
      "Iter 1036 | Time 58.9611(57.1831) | Bit/dim 3.7331(3.7275) | Xent 1.0195(0.9449) | Loss 9.9402(10.4343) | Error 0.3634(0.3376) Steps 574(590.68) | Grad Norm 24.8935(5.4071) | Total Time 0.00(0.00)\n",
      "Iter 1037 | Time 60.2302(57.2745) | Bit/dim 3.7359(3.7278) | Xent 1.0246(0.9473) | Loss 9.8755(10.4175) | Error 0.3670(0.3385) Steps 646(592.34) | Grad Norm 18.5060(5.8001) | Total Time 0.00(0.00)\n",
      "Iter 1038 | Time 61.7199(57.4078) | Bit/dim 3.7308(3.7279) | Xent 0.9397(0.9471) | Loss 9.9539(10.4036) | Error 0.3365(0.3385) Steps 610(592.87) | Grad Norm 5.1925(5.7818) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 23.5694, Epoch Time 389.2235(383.9099), Bit/dim 3.7293(best: 3.7268), Xent 1.1308, Loss 4.2948, Error 0.4018(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1039 | Time 59.3092(57.4649) | Bit/dim 3.7194(3.7276) | Xent 0.9803(0.9481) | Loss 13.5621(10.4984) | Error 0.3480(0.3388) Steps 598(593.03) | Grad Norm 11.2609(5.9462) | Total Time 0.00(0.00)\n",
      "Iter 1040 | Time 57.4730(57.4651) | Bit/dim 3.7234(3.7275) | Xent 0.9796(0.9490) | Loss 9.7466(10.4758) | Error 0.3511(0.3391) Steps 604(593.36) | Grad Norm 6.6113(5.9662) | Total Time 0.00(0.00)\n",
      "Iter 1041 | Time 58.1645(57.4861) | Bit/dim 3.7353(3.7277) | Xent 0.9781(0.9499) | Loss 9.8916(10.4583) | Error 0.3420(0.3392) Steps 616(594.03) | Grad Norm 10.1857(6.0928) | Total Time 0.00(0.00)\n",
      "Iter 1042 | Time 56.8236(57.4662) | Bit/dim 3.7238(3.7276) | Xent 0.9573(0.9501) | Loss 9.8275(10.4394) | Error 0.3389(0.3392) Steps 592(593.97) | Grad Norm 7.0211(6.1206) | Total Time 0.00(0.00)\n",
      "Iter 1043 | Time 60.1392(57.5464) | Bit/dim 3.7193(3.7274) | Xent 0.9829(0.9511) | Loss 9.9073(10.4234) | Error 0.3425(0.3393) Steps 616(594.63) | Grad Norm 9.6409(6.2262) | Total Time 0.00(0.00)\n",
      "Iter 1044 | Time 60.1418(57.6243) | Bit/dim 3.7372(3.7277) | Xent 0.9465(0.9510) | Loss 9.7306(10.4026) | Error 0.3330(0.3391) Steps 616(595.28) | Grad Norm 5.4689(6.2035) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 23.7570, Epoch Time 392.3102(384.1619), Bit/dim 3.7255(best: 3.7268), Xent 1.1249, Loss 4.2880, Error 0.4013(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1045 | Time 57.8002(57.6296) | Bit/dim 3.7290(3.7277) | Xent 0.9604(0.9513) | Loss 13.6866(10.5011) | Error 0.3476(0.3394) Steps 598(595.36) | Grad Norm 9.4873(6.3020) | Total Time 0.00(0.00)\n",
      "Iter 1046 | Time 60.9117(57.7280) | Bit/dim 3.7258(3.7276) | Xent 0.9205(0.9503) | Loss 9.7296(10.4780) | Error 0.3297(0.3391) Steps 586(595.08) | Grad Norm 5.1289(6.2668) | Total Time 0.00(0.00)\n",
      "Iter 1047 | Time 59.3320(57.7761) | Bit/dim 3.7258(3.7276) | Xent 0.9552(0.9505) | Loss 10.0238(10.4644) | Error 0.3391(0.3391) Steps 598(595.16) | Grad Norm 8.2062(6.3250) | Total Time 0.00(0.00)\n",
      "Iter 1048 | Time 56.0114(57.7232) | Bit/dim 3.7186(3.7273) | Xent 0.9228(0.9497) | Loss 9.7293(10.4423) | Error 0.3301(0.3388) Steps 598(595.25) | Grad Norm 4.0667(6.2573) | Total Time 0.00(0.00)\n",
      "Iter 1049 | Time 57.1179(57.7050) | Bit/dim 3.7246(3.7272) | Xent 0.9376(0.9493) | Loss 9.9662(10.4280) | Error 0.3311(0.3386) Steps 586(594.97) | Grad Norm 6.8543(6.2752) | Total Time 0.00(0.00)\n",
      "Iter 1050 | Time 59.9799(57.7733) | Bit/dim 3.7329(3.7274) | Xent 0.9011(0.9479) | Loss 9.8330(10.4102) | Error 0.3233(0.3381) Steps 580(594.52) | Grad Norm 4.1252(6.2107) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 23.2332, Epoch Time 391.1456(384.3714), Bit/dim 3.7254(best: 3.7255), Xent 1.1092, Loss 4.2800, Error 0.3893(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1051 | Time 54.7223(57.6818) | Bit/dim 3.7264(3.7274) | Xent 0.9218(0.9471) | Loss 13.0286(10.4887) | Error 0.3295(0.3379) Steps 598(594.63) | Grad Norm 6.1114(6.2077) | Total Time 0.00(0.00)\n",
      "Iter 1052 | Time 58.2854(57.6999) | Bit/dim 3.7260(3.7273) | Xent 0.9223(0.9463) | Loss 9.6972(10.4650) | Error 0.3243(0.3375) Steps 598(594.73) | Grad Norm 4.1379(6.1456) | Total Time 0.00(0.00)\n",
      "Iter 1053 | Time 57.9348(57.7069) | Bit/dim 3.7236(3.7272) | Xent 0.9199(0.9455) | Loss 9.7459(10.4434) | Error 0.3166(0.3368) Steps 622(595.55) | Grad Norm 4.1771(6.0865) | Total Time 0.00(0.00)\n",
      "Iter 1054 | Time 62.7989(57.8597) | Bit/dim 3.7190(3.7270) | Xent 0.9124(0.9445) | Loss 9.8874(10.4267) | Error 0.3285(0.3366) Steps 634(596.70) | Grad Norm 4.8723(6.0501) | Total Time 0.00(0.00)\n",
      "Iter 1055 | Time 59.6945(57.9147) | Bit/dim 3.7191(3.7267) | Xent 0.9057(0.9434) | Loss 9.8492(10.4094) | Error 0.3209(0.3361) Steps 598(596.74) | Grad Norm 3.4678(5.9726) | Total Time 0.00(0.00)\n",
      "Iter 1056 | Time 59.3952(57.9591) | Bit/dim 3.7299(3.7268) | Xent 0.9242(0.9428) | Loss 10.0555(10.3988) | Error 0.3255(0.3358) Steps 598(596.78) | Grad Norm 3.7825(5.9069) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 23.3503, Epoch Time 392.4293(384.6132), Bit/dim 3.7267(best: 3.7254), Xent 1.0900, Loss 4.2717, Error 0.3896(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1057 | Time 54.8639(57.8663) | Bit/dim 3.7248(3.7268) | Xent 0.9051(0.9417) | Loss 13.4255(10.4896) | Error 0.3237(0.3354) Steps 592(596.63) | Grad Norm 3.5323(5.8357) | Total Time 0.00(0.00)\n",
      "Iter 1058 | Time 53.2331(57.7273) | Bit/dim 3.7278(3.7268) | Xent 0.9073(0.9406) | Loss 9.8092(10.4692) | Error 0.3271(0.3352) Steps 610(597.03) | Grad Norm 3.4090(5.7629) | Total Time 0.00(0.00)\n",
      "Iter 1059 | Time 58.2146(57.7419) | Bit/dim 3.7265(3.7268) | Xent 0.8931(0.9392) | Loss 9.6133(10.4435) | Error 0.3151(0.3346) Steps 586(596.70) | Grad Norm 2.9368(5.6781) | Total Time 0.00(0.00)\n",
      "Iter 1060 | Time 57.0813(57.7221) | Bit/dim 3.7308(3.7269) | Xent 0.8860(0.9376) | Loss 9.5877(10.4178) | Error 0.3163(0.3340) Steps 586(596.38) | Grad Norm 3.1593(5.6026) | Total Time 0.00(0.00)\n",
      "Iter 1061 | Time 56.7470(57.6928) | Bit/dim 3.7172(3.7266) | Xent 0.9124(0.9369) | Loss 9.6097(10.3936) | Error 0.3266(0.3338) Steps 598(596.43) | Grad Norm 3.3455(5.5348) | Total Time 0.00(0.00)\n",
      "Iter 1062 | Time 53.2514(57.5596) | Bit/dim 3.7101(3.7261) | Xent 0.8888(0.9354) | Loss 9.7591(10.3746) | Error 0.3216(0.3334) Steps 592(596.30) | Grad Norm 3.0025(5.4589) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 23.2764, Epoch Time 372.4835(384.2493), Bit/dim 3.7293(best: 3.7254), Xent 1.0940, Loss 4.2763, Error 0.3864(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1063 | Time 61.9104(57.6901) | Bit/dim 3.7241(3.7261) | Xent 0.8985(0.9343) | Loss 13.5607(10.4701) | Error 0.3179(0.3330) Steps 592(596.17) | Grad Norm 3.1290(5.3890) | Total Time 0.00(0.00)\n",
      "Iter 1064 | Time 56.9551(57.6681) | Bit/dim 3.7226(3.7260) | Xent 0.9097(0.9336) | Loss 9.7253(10.4478) | Error 0.3216(0.3326) Steps 616(596.76) | Grad Norm 3.6099(5.3356) | Total Time 0.00(0.00)\n",
      "Iter 1065 | Time 56.4773(57.6323) | Bit/dim 3.7275(3.7260) | Xent 0.8863(0.9322) | Loss 9.7207(10.4260) | Error 0.3171(0.3322) Steps 616(597.34) | Grad Norm 8.2449(5.4229) | Total Time 0.00(0.00)\n",
      "Iter 1066 | Time 60.2600(57.7112) | Bit/dim 3.7274(3.7260) | Xent 0.9143(0.9316) | Loss 9.6365(10.4023) | Error 0.3273(0.3320) Steps 592(597.18) | Grad Norm 8.9348(5.5282) | Total Time 0.00(0.00)\n",
      "Iter 1067 | Time 52.1274(57.5437) | Bit/dim 3.7203(3.7259) | Xent 0.9045(0.9308) | Loss 9.8833(10.3867) | Error 0.3261(0.3318) Steps 598(597.21) | Grad Norm 7.6511(5.5919) | Total Time 0.00(0.00)\n",
      "Iter 1068 | Time 60.1869(57.6229) | Bit/dim 3.7133(3.7255) | Xent 0.8706(0.9290) | Loss 9.6665(10.3651) | Error 0.3129(0.3313) Steps 634(598.31) | Grad Norm 3.4477(5.5276) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 23.7681, Epoch Time 387.5745(384.3490), Bit/dim 3.7217(best: 3.7254), Xent 1.0960, Loss 4.2697, Error 0.3854(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1069 | Time 59.4118(57.6766) | Bit/dim 3.7229(3.7254) | Xent 0.8856(0.9277) | Loss 13.7858(10.4677) | Error 0.3195(0.3309) Steps 580(597.76) | Grad Norm 4.8429(5.5071) | Total Time 0.00(0.00)\n",
      "Iter 1070 | Time 58.0135(57.6867) | Bit/dim 3.7244(3.7254) | Xent 0.8873(0.9265) | Loss 9.8031(10.4478) | Error 0.3203(0.3306) Steps 604(597.95) | Grad Norm 4.6677(5.4819) | Total Time 0.00(0.00)\n",
      "Iter 1071 | Time 62.0183(57.8167) | Bit/dim 3.7161(3.7251) | Xent 0.8902(0.9254) | Loss 9.8623(10.4302) | Error 0.3193(0.3303) Steps 610(598.31) | Grad Norm 4.1503(5.4419) | Total Time 0.00(0.00)\n",
      "Iter 1072 | Time 54.6807(57.7226) | Bit/dim 3.7132(3.7248) | Xent 0.8841(0.9242) | Loss 9.8625(10.4132) | Error 0.3140(0.3298) Steps 586(597.94) | Grad Norm 4.1901(5.4044) | Total Time 0.00(0.00)\n",
      "Iter 1073 | Time 61.5327(57.8369) | Bit/dim 3.7288(3.7249) | Xent 0.8787(0.9228) | Loss 9.8919(10.3976) | Error 0.3146(0.3293) Steps 598(597.94) | Grad Norm 2.9427(5.3305) | Total Time 0.00(0.00)\n",
      "Iter 1074 | Time 59.3480(57.8822) | Bit/dim 3.7234(3.7248) | Xent 0.8813(0.9216) | Loss 9.8772(10.3820) | Error 0.3160(0.3289) Steps 622(598.66) | Grad Norm 2.7022(5.2517) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 23.3073, Epoch Time 394.5823(384.6560), Bit/dim 3.7258(best: 3.7217), Xent 1.1026, Loss 4.2771, Error 0.3877(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1075 | Time 57.0893(57.8584) | Bit/dim 3.7066(3.7243) | Xent 0.8794(0.9203) | Loss 13.4272(10.4733) | Error 0.3141(0.3285) Steps 586(598.28) | Grad Norm 2.7346(5.1762) | Total Time 0.00(0.00)\n",
      "Iter 1076 | Time 53.8466(57.7381) | Bit/dim 3.7199(3.7242) | Xent 0.9221(0.9203) | Loss 9.5079(10.4444) | Error 0.3274(0.3284) Steps 556(597.01) | Grad Norm 11.0890(5.3536) | Total Time 0.00(0.00)\n",
      "Iter 1077 | Time 59.1292(57.7798) | Bit/dim 3.7327(3.7244) | Xent 0.9544(0.9214) | Loss 9.8360(10.4261) | Error 0.3415(0.3288) Steps 610(597.40) | Grad Norm 18.8729(5.7591) | Total Time 0.00(0.00)\n",
      "Iter 1078 | Time 63.7344(57.9585) | Bit/dim 3.7252(3.7244) | Xent 0.9379(0.9219) | Loss 9.9743(10.4125) | Error 0.3345(0.3290) Steps 556(596.16) | Grad Norm 16.9360(6.0944) | Total Time 0.00(0.00)\n",
      "Iter 1079 | Time 60.5901(58.0374) | Bit/dim 3.7250(3.7244) | Xent 0.8982(0.9212) | Loss 9.7927(10.3940) | Error 0.3219(0.3288) Steps 580(595.68) | Grad Norm 8.3257(6.1614) | Total Time 0.00(0.00)\n",
      "Iter 1080 | Time 58.4265(58.0491) | Bit/dim 3.7357(3.7248) | Xent 0.8927(0.9203) | Loss 9.8986(10.3791) | Error 0.3210(0.3286) Steps 616(596.29) | Grad Norm 5.0451(6.1279) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 23.3531, Epoch Time 391.9726(384.8755), Bit/dim 3.7314(best: 3.7217), Xent 1.1296, Loss 4.2962, Error 0.4027(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1081 | Time 56.8945(58.0144) | Bit/dim 3.7232(3.7247) | Xent 0.9122(0.9201) | Loss 13.5017(10.4728) | Error 0.3276(0.3285) Steps 604(596.52) | Grad Norm 9.3559(6.2247) | Total Time 0.00(0.00)\n",
      "Iter 1082 | Time 56.8139(57.9784) | Bit/dim 3.7256(3.7248) | Xent 0.8791(0.9188) | Loss 9.9546(10.4572) | Error 0.3080(0.3279) Steps 604(596.74) | Grad Norm 3.5323(6.1440) | Total Time 0.00(0.00)\n",
      "Iter 1083 | Time 56.9773(57.9484) | Bit/dim 3.7278(3.7249) | Xent 0.9048(0.9184) | Loss 9.9772(10.4428) | Error 0.3279(0.3279) Steps 622(597.50) | Grad Norm 5.5731(6.1268) | Total Time 0.00(0.00)\n",
      "Iter 1084 | Time 60.3709(58.0211) | Bit/dim 3.7319(3.7251) | Xent 0.8785(0.9172) | Loss 9.9002(10.4265) | Error 0.3139(0.3275) Steps 598(597.52) | Grad Norm 5.5290(6.1089) | Total Time 0.00(0.00)\n",
      "Iter 1085 | Time 59.9369(58.0785) | Bit/dim 3.7190(3.7249) | Xent 0.8927(0.9165) | Loss 9.8342(10.4088) | Error 0.3136(0.3271) Steps 586(597.17) | Grad Norm 3.9756(6.0449) | Total Time 0.00(0.00)\n",
      "Iter 1086 | Time 58.0251(58.0769) | Bit/dim 3.7193(3.7247) | Xent 0.9142(0.9164) | Loss 9.7095(10.3878) | Error 0.3257(0.3270) Steps 556(595.93) | Grad Norm 8.0323(6.1045) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 22.8928, Epoch Time 388.3337(384.9793), Bit/dim 3.7289(best: 3.7217), Xent 1.1181, Loss 4.2879, Error 0.3895(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1087 | Time 64.0878(58.2573) | Bit/dim 3.7155(3.7244) | Xent 0.8866(0.9155) | Loss 13.6307(10.4851) | Error 0.3135(0.3266) Steps 586(595.64) | Grad Norm 3.5663(6.0284) | Total Time 0.00(0.00)\n",
      "Iter 1088 | Time 60.3501(58.3201) | Bit/dim 3.7139(3.7241) | Xent 0.8802(0.9145) | Loss 10.0538(10.4721) | Error 0.3145(0.3263) Steps 586(595.35) | Grad Norm 7.0771(6.0598) | Total Time 0.00(0.00)\n",
      "Iter 1089 | Time 57.0775(58.2828) | Bit/dim 3.7296(3.7243) | Xent 0.8896(0.9137) | Loss 9.7845(10.4515) | Error 0.3136(0.3259) Steps 622(596.15) | Grad Norm 7.4290(6.1009) | Total Time 0.00(0.00)\n",
      "Iter 1090 | Time 60.7582(58.3570) | Bit/dim 3.7172(3.7241) | Xent 0.8998(0.9133) | Loss 9.8038(10.4321) | Error 0.3194(0.3257) Steps 616(596.74) | Grad Norm 4.9683(6.0669) | Total Time 0.00(0.00)\n",
      "Iter 1091 | Time 55.9427(58.2846) | Bit/dim 3.7362(3.7244) | Xent 0.8726(0.9121) | Loss 9.8032(10.4132) | Error 0.3131(0.3253) Steps 598(596.78) | Grad Norm 8.1022(6.1280) | Total Time 0.00(0.00)\n",
      "Iter 1092 | Time 57.1788(58.2514) | Bit/dim 3.7221(3.7244) | Xent 0.8901(0.9114) | Loss 9.9072(10.3980) | Error 0.3139(0.3250) Steps 646(598.26) | Grad Norm 5.8006(6.1182) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 23.1616, Epoch Time 395.5174(385.2954), Bit/dim 3.7264(best: 3.7217), Xent 1.1060, Loss 4.2794, Error 0.3883(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1093 | Time 52.4970(58.0788) | Bit/dim 3.7122(3.7240) | Xent 0.8879(0.9107) | Loss 13.2387(10.4833) | Error 0.3113(0.3246) Steps 568(597.35) | Grad Norm 5.7227(6.1063) | Total Time 0.00(0.00)\n",
      "Iter 1094 | Time 61.7064(58.1876) | Bit/dim 3.7220(3.7239) | Xent 0.8638(0.9093) | Loss 9.8435(10.4641) | Error 0.3071(0.3240) Steps 634(598.45) | Grad Norm 4.3761(6.0544) | Total Time 0.00(0.00)\n",
      "Iter 1095 | Time 62.9491(58.3305) | Bit/dim 3.7227(3.7239) | Xent 0.8732(0.9082) | Loss 9.8088(10.4444) | Error 0.3146(0.3238) Steps 574(597.72) | Grad Norm 6.4913(6.0675) | Total Time 0.00(0.00)\n",
      "Iter 1096 | Time 57.5859(58.3081) | Bit/dim 3.7245(3.7239) | Xent 0.8861(0.9075) | Loss 9.6489(10.4205) | Error 0.3135(0.3234) Steps 610(598.08) | Grad Norm 7.6978(6.1164) | Total Time 0.00(0.00)\n",
      "Iter 1097 | Time 57.7748(58.2921) | Bit/dim 3.7287(3.7241) | Xent 0.8625(0.9062) | Loss 9.8155(10.4024) | Error 0.3130(0.3231) Steps 586(597.72) | Grad Norm 5.1087(6.0862) | Total Time 0.00(0.00)\n",
      "Iter 1098 | Time 58.4267(58.2962) | Bit/dim 3.7240(3.7241) | Xent 0.8764(0.9053) | Loss 9.7520(10.3829) | Error 0.3145(0.3229) Steps 622(598.45) | Grad Norm 4.3577(6.0343) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 23.5496, Epoch Time 390.5489(385.4530), Bit/dim 3.7279(best: 3.7217), Xent 1.1094, Loss 4.2826, Error 0.3913(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1099 | Time 52.8423(58.1326) | Bit/dim 3.7292(3.7242) | Xent 0.8739(0.9044) | Loss 12.8458(10.4568) | Error 0.3099(0.3225) Steps 586(598.08) | Grad Norm 8.9229(6.1210) | Total Time 0.00(0.00)\n",
      "Iter 1100 | Time 59.0847(58.1611) | Bit/dim 3.7299(3.7244) | Xent 0.8824(0.9037) | Loss 9.8316(10.4380) | Error 0.3153(0.3223) Steps 616(598.61) | Grad Norm 7.8215(6.1720) | Total Time 0.00(0.00)\n",
      "Iter 1101 | Time 58.7847(58.1798) | Bit/dim 3.7087(3.7239) | Xent 0.8648(0.9025) | Loss 9.8804(10.4213) | Error 0.3155(0.3221) Steps 640(599.86) | Grad Norm 5.0922(6.1396) | Total Time 0.00(0.00)\n",
      "Iter 1102 | Time 55.3763(58.0957) | Bit/dim 3.7264(3.7240) | Xent 0.8682(0.9015) | Loss 9.6388(10.3978) | Error 0.3103(0.3217) Steps 592(599.62) | Grad Norm 3.9655(6.0744) | Total Time 0.00(0.00)\n",
      "Iter 1103 | Time 58.4671(58.1069) | Bit/dim 3.7197(3.7239) | Xent 0.8630(0.9003) | Loss 9.9272(10.3837) | Error 0.3134(0.3215) Steps 586(599.21) | Grad Norm 6.0882(6.0748) | Total Time 0.00(0.00)\n",
      "Iter 1104 | Time 58.3378(58.1138) | Bit/dim 3.7211(3.7238) | Xent 0.8871(0.8999) | Loss 9.7499(10.3647) | Error 0.3136(0.3212) Steps 598(599.18) | Grad Norm 5.3108(6.0519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 23.4318, Epoch Time 383.0547(385.3811), Bit/dim 3.7256(best: 3.7217), Xent 1.1008, Loss 4.2760, Error 0.3837(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1105 | Time 60.2331(58.1774) | Bit/dim 3.7185(3.7236) | Xent 0.8794(0.8993) | Loss 13.1131(10.4471) | Error 0.3145(0.3210) Steps 622(599.86) | Grad Norm 4.5698(6.0074) | Total Time 0.00(0.00)\n",
      "Iter 1106 | Time 60.9183(58.2596) | Bit/dim 3.7236(3.7236) | Xent 0.8591(0.8981) | Loss 9.8677(10.4297) | Error 0.3084(0.3206) Steps 592(599.62) | Grad Norm 2.1262(5.8910) | Total Time 0.00(0.00)\n",
      "Iter 1107 | Time 57.7888(58.2455) | Bit/dim 3.7154(3.7234) | Xent 0.8766(0.8975) | Loss 9.7976(10.4108) | Error 0.3157(0.3205) Steps 610(599.94) | Grad Norm 4.9657(5.8632) | Total Time 0.00(0.00)\n",
      "Iter 1108 | Time 63.8205(58.4127) | Bit/dim 3.7184(3.7232) | Xent 0.8483(0.8960) | Loss 9.6879(10.3891) | Error 0.2965(0.3198) Steps 616(600.42) | Grad Norm 7.1474(5.9017) | Total Time 0.00(0.00)\n",
      "Iter 1109 | Time 57.1655(58.3753) | Bit/dim 3.7212(3.7232) | Xent 0.8671(0.8951) | Loss 9.4457(10.3608) | Error 0.3041(0.3193) Steps 604(600.52) | Grad Norm 5.8466(5.9001) | Total Time 0.00(0.00)\n",
      "Iter 1110 | Time 62.5857(58.5016) | Bit/dim 3.7286(3.7233) | Xent 0.8761(0.8946) | Loss 9.9686(10.3490) | Error 0.3107(0.3190) Steps 634(601.53) | Grad Norm 6.6745(5.9233) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 22.8664, Epoch Time 401.4819(385.8641), Bit/dim 3.7218(best: 3.7217), Xent 1.1239, Loss 4.2837, Error 0.3895(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1111 | Time 56.7982(58.4505) | Bit/dim 3.7311(3.7236) | Xent 0.8894(0.8944) | Loss 13.3353(10.4386) | Error 0.3193(0.3191) Steps 592(601.24) | Grad Norm 10.1443(6.0500) | Total Time 0.00(0.00)\n",
      "Iter 1112 | Time 61.9633(58.5559) | Bit/dim 3.7159(3.7233) | Xent 0.8872(0.8942) | Loss 9.9078(10.4227) | Error 0.3197(0.3191) Steps 616(601.69) | Grad Norm 10.8371(6.1936) | Total Time 0.00(0.00)\n",
      "Iter 1113 | Time 56.2976(58.4882) | Bit/dim 3.7298(3.7235) | Xent 0.8872(0.8940) | Loss 9.8330(10.4050) | Error 0.3206(0.3191) Steps 562(600.50) | Grad Norm 7.2880(6.2264) | Total Time 0.00(0.00)\n",
      "Iter 1114 | Time 60.6841(58.5540) | Bit/dim 3.7096(3.7231) | Xent 0.8649(0.8931) | Loss 9.9568(10.3916) | Error 0.3107(0.3189) Steps 574(599.70) | Grad Norm 9.2399(6.3168) | Total Time 0.00(0.00)\n",
      "Iter 1115 | Time 57.8405(58.5326) | Bit/dim 3.7176(3.7229) | Xent 0.9027(0.8934) | Loss 9.5563(10.3665) | Error 0.3184(0.3189) Steps 616(600.19) | Grad Norm 13.0599(6.5191) | Total Time 0.00(0.00)\n",
      "Iter 1116 | Time 59.8837(58.5732) | Bit/dim 3.7319(3.7232) | Xent 0.8808(0.8930) | Loss 9.8322(10.3505) | Error 0.3156(0.3188) Steps 592(599.94) | Grad Norm 10.9444(6.6519) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 23.2989, Epoch Time 392.7625(386.0710), Bit/dim 3.7283(best: 3.7217), Xent 1.1112, Loss 4.2840, Error 0.3871(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1117 | Time 58.0791(58.5583) | Bit/dim 3.7258(3.7233) | Xent 0.8874(0.8929) | Loss 13.2749(10.4382) | Error 0.3151(0.3187) Steps 604(600.07) | Grad Norm 6.9021(6.6594) | Total Time 0.00(0.00)\n",
      "Iter 1118 | Time 58.4447(58.5549) | Bit/dim 3.7101(3.7229) | Xent 0.8636(0.8920) | Loss 9.9450(10.4234) | Error 0.3123(0.3185) Steps 592(599.82) | Grad Norm 3.2465(6.5570) | Total Time 0.00(0.00)\n",
      "Iter 1119 | Time 55.0436(58.4496) | Bit/dim 3.7181(3.7228) | Xent 0.8845(0.8917) | Loss 9.6996(10.4017) | Error 0.3173(0.3184) Steps 610(600.13) | Grad Norm 9.5794(6.6477) | Total Time 0.00(0.00)\n",
      "Iter 1120 | Time 56.8481(58.4015) | Bit/dim 3.7165(3.7226) | Xent 0.8710(0.8911) | Loss 9.8584(10.3854) | Error 0.3067(0.3181) Steps 592(599.88) | Grad Norm 11.9141(6.8056) | Total Time 0.00(0.00)\n",
      "Iter 1121 | Time 60.6612(58.4693) | Bit/dim 3.7330(3.7229) | Xent 0.8749(0.8906) | Loss 9.7671(10.3668) | Error 0.3097(0.3178) Steps 616(600.37) | Grad Norm 12.1258(6.9652) | Total Time 0.00(0.00)\n",
      "Iter 1122 | Time 60.5615(58.5321) | Bit/dim 3.7195(3.7228) | Xent 0.8664(0.8899) | Loss 9.9018(10.3529) | Error 0.3055(0.3175) Steps 610(600.66) | Grad Norm 7.1217(6.9699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 23.4264, Epoch Time 389.3142(386.1683), Bit/dim 3.7263(best: 3.7217), Xent 1.1247, Loss 4.2887, Error 0.3963(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1123 | Time 55.1541(58.4308) | Bit/dim 3.7176(3.7226) | Xent 0.8923(0.8900) | Loss 13.7507(10.4548) | Error 0.3175(0.3175) Steps 586(600.22) | Grad Norm 11.0826(7.0933) | Total Time 0.00(0.00)\n",
      "Iter 1124 | Time 59.0227(58.4485) | Bit/dim 3.7231(3.7226) | Xent 0.8732(0.8895) | Loss 9.9736(10.4404) | Error 0.3129(0.3173) Steps 586(599.79) | Grad Norm 8.8774(7.1468) | Total Time 0.00(0.00)\n",
      "Iter 1125 | Time 61.2378(58.5322) | Bit/dim 3.7206(3.7226) | Xent 0.8736(0.8890) | Loss 9.7203(10.4188) | Error 0.3119(0.3172) Steps 550(598.30) | Grad Norm 3.7501(7.0449) | Total Time 0.00(0.00)\n",
      "Iter 1126 | Time 60.8608(58.6021) | Bit/dim 3.7155(3.7224) | Xent 0.8537(0.8880) | Loss 9.8724(10.4024) | Error 0.3000(0.3166) Steps 598(598.29) | Grad Norm 6.0683(7.0156) | Total Time 0.00(0.00)\n",
      "Iter 1127 | Time 55.0799(58.4964) | Bit/dim 3.7215(3.7223) | Xent 0.8615(0.8872) | Loss 9.7425(10.3826) | Error 0.3065(0.3163) Steps 610(598.64) | Grad Norm 6.8780(7.0115) | Total Time 0.00(0.00)\n",
      "Iter 1128 | Time 56.2345(58.4285) | Bit/dim 3.7158(3.7221) | Xent 0.8356(0.8856) | Loss 9.5325(10.3571) | Error 0.2967(0.3157) Steps 586(598.26) | Grad Norm 3.8247(6.9159) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 23.5267, Epoch Time 386.7230(386.1850), Bit/dim 3.7209(best: 3.7217), Xent 1.1020, Loss 4.2719, Error 0.3829(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1129 | Time 60.1888(58.4813) | Bit/dim 3.7120(3.7218) | Xent 0.8709(0.8852) | Loss 13.2328(10.4434) | Error 0.3093(0.3156) Steps 604(598.43) | Grad Norm 6.1452(6.8928) | Total Time 0.00(0.00)\n",
      "Iter 1130 | Time 60.3142(58.5363) | Bit/dim 3.7117(3.7215) | Xent 0.8575(0.8843) | Loss 10.0044(10.4302) | Error 0.3044(0.3152) Steps 634(599.50) | Grad Norm 2.8542(6.7716) | Total Time 0.00(0.00)\n",
      "Iter 1131 | Time 53.5179(58.3858) | Bit/dim 3.7184(3.7214) | Xent 0.8354(0.8829) | Loss 9.6063(10.4055) | Error 0.2997(0.3148) Steps 574(598.73) | Grad Norm 5.3304(6.7284) | Total Time 0.00(0.00)\n",
      "Iter 1132 | Time 59.1917(58.4100) | Bit/dim 3.7270(3.7216) | Xent 0.8484(0.8818) | Loss 9.6415(10.3826) | Error 0.3085(0.3146) Steps 598(598.71) | Grad Norm 8.0054(6.7667) | Total Time 0.00(0.00)\n",
      "Iter 1133 | Time 62.2241(58.5244) | Bit/dim 3.7262(3.7217) | Xent 0.8613(0.8812) | Loss 9.8534(10.3667) | Error 0.3116(0.3145) Steps 592(598.51) | Grad Norm 8.5583(6.8205) | Total Time 0.00(0.00)\n",
      "Iter 1134 | Time 58.1690(58.5137) | Bit/dim 3.7233(3.7218) | Xent 0.8735(0.8810) | Loss 9.7676(10.3487) | Error 0.3083(0.3143) Steps 616(599.04) | Grad Norm 10.7278(6.9377) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 23.2478, Epoch Time 393.5135(386.4048), Bit/dim 3.7261(best: 3.7209), Xent 1.1252, Loss 4.2888, Error 0.3934(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1135 | Time 57.3193(58.4779) | Bit/dim 3.7190(3.7217) | Xent 0.8717(0.8807) | Loss 13.3432(10.4386) | Error 0.3140(0.3143) Steps 598(599.00) | Grad Norm 11.7669(7.0825) | Total Time 0.00(0.00)\n",
      "Iter 1136 | Time 57.2379(58.4407) | Bit/dim 3.7223(3.7217) | Xent 0.8539(0.8799) | Loss 9.9427(10.4237) | Error 0.3077(0.3141) Steps 592(598.79) | Grad Norm 8.1862(7.1157) | Total Time 0.00(0.00)\n",
      "Iter 1137 | Time 58.5983(58.4454) | Bit/dim 3.7166(3.7216) | Xent 0.8462(0.8789) | Loss 9.6407(10.4002) | Error 0.3005(0.3137) Steps 598(598.77) | Grad Norm 3.4397(7.0054) | Total Time 0.00(0.00)\n",
      "Iter 1138 | Time 57.8409(58.4273) | Bit/dim 3.7238(3.7216) | Xent 0.8557(0.8782) | Loss 9.6743(10.3784) | Error 0.3084(0.3135) Steps 610(599.11) | Grad Norm 7.0064(7.0054) | Total Time 0.00(0.00)\n",
      "Iter 1139 | Time 58.0190(58.4150) | Bit/dim 3.7161(3.7215) | Xent 0.8564(0.8775) | Loss 9.9924(10.3668) | Error 0.3114(0.3135) Steps 622(599.79) | Grad Norm 6.0747(6.9775) | Total Time 0.00(0.00)\n",
      "Iter 1140 | Time 63.0892(58.5552) | Bit/dim 3.7203(3.7214) | Xent 0.8528(0.8768) | Loss 9.7160(10.3473) | Error 0.3049(0.3132) Steps 574(599.02) | Grad Norm 6.9961(6.9780) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 23.2238, Epoch Time 391.0716(386.5448), Bit/dim 3.7215(best: 3.7209), Xent 1.1078, Loss 4.2754, Error 0.3883(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1141 | Time 61.9556(58.6573) | Bit/dim 3.7152(3.7212) | Xent 0.8457(0.8759) | Loss 13.6342(10.4459) | Error 0.3014(0.3128) Steps 628(599.89) | Grad Norm 5.6006(6.9367) | Total Time 0.00(0.00)\n",
      "Iter 1142 | Time 53.9105(58.5149) | Bit/dim 3.7077(3.7208) | Xent 0.8233(0.8743) | Loss 9.7420(10.4248) | Error 0.2967(0.3124) Steps 586(599.47) | Grad Norm 2.6961(6.8095) | Total Time 0.00(0.00)\n",
      "Iter 1143 | Time 55.1006(58.4124) | Bit/dim 3.7236(3.7209) | Xent 0.8351(0.8731) | Loss 9.7266(10.4038) | Error 0.2989(0.3120) Steps 586(599.07) | Grad Norm 5.4796(6.7696) | Total Time 0.00(0.00)\n",
      "Iter 1144 | Time 58.1301(58.4040) | Bit/dim 3.7209(3.7209) | Xent 0.8437(0.8722) | Loss 9.4880(10.3764) | Error 0.3069(0.3118) Steps 616(599.58) | Grad Norm 3.9390(6.6847) | Total Time 0.00(0.00)\n",
      "Iter 1145 | Time 56.6939(58.3527) | Bit/dim 3.7197(3.7209) | Xent 0.8299(0.8710) | Loss 9.7037(10.3562) | Error 0.3027(0.3115) Steps 610(599.89) | Grad Norm 3.6644(6.5941) | Total Time 0.00(0.00)\n",
      "Iter 1146 | Time 57.8041(58.3362) | Bit/dim 3.7307(3.7212) | Xent 0.8364(0.8699) | Loss 9.7207(10.3371) | Error 0.2926(0.3110) Steps 604(600.01) | Grad Norm 2.4723(6.4704) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 23.1460, Epoch Time 382.9804(386.4379), Bit/dim 3.7198(best: 3.7209), Xent 1.1057, Loss 4.2727, Error 0.3826(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1147 | Time 55.9137(58.2635) | Bit/dim 3.7152(3.7210) | Xent 0.8411(0.8691) | Loss 13.4800(10.4314) | Error 0.3041(0.3108) Steps 598(599.95) | Grad Norm 3.9396(6.3945) | Total Time 0.00(0.00)\n",
      "Iter 1148 | Time 58.7234(58.2773) | Bit/dim 3.7211(3.7210) | Xent 0.8375(0.8681) | Loss 9.7283(10.4103) | Error 0.2963(0.3103) Steps 598(599.89) | Grad Norm 2.4061(6.2748) | Total Time 0.00(0.00)\n",
      "Iter 1149 | Time 56.7656(58.2320) | Bit/dim 3.7029(3.7205) | Xent 0.8183(0.8666) | Loss 9.2117(10.3744) | Error 0.2940(0.3098) Steps 604(600.02) | Grad Norm 4.5518(6.2232) | Total Time 0.00(0.00)\n",
      "Iter 1150 | Time 60.1492(58.2895) | Bit/dim 3.7173(3.7204) | Xent 0.8530(0.8662) | Loss 9.8293(10.3580) | Error 0.2997(0.3095) Steps 616(600.50) | Grad Norm 7.8975(6.2734) | Total Time 0.00(0.00)\n",
      "Iter 1151 | Time 58.5078(58.2960) | Bit/dim 3.7258(3.7205) | Xent 0.8585(0.8660) | Loss 9.8022(10.3413) | Error 0.3049(0.3094) Steps 604(600.60) | Grad Norm 11.7329(6.4372) | Total Time 0.00(0.00)\n",
      "Iter 1152 | Time 56.9014(58.2542) | Bit/dim 3.7282(3.7208) | Xent 0.9215(0.8676) | Loss 9.8512(10.3266) | Error 0.3316(0.3101) Steps 586(600.16) | Grad Norm 20.8619(6.8699) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 23.9833, Epoch Time 386.7972(386.4487), Bit/dim 3.7313(best: 3.7198), Xent 1.2665, Loss 4.3646, Error 0.4371(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1153 | Time 62.5932(58.3844) | Bit/dim 3.7312(3.7211) | Xent 1.0289(0.8725) | Loss 13.8539(10.4324) | Error 0.3644(0.3117) Steps 610(600.46) | Grad Norm 25.0515(7.4154) | Total Time 0.00(0.00)\n",
      "Iter 1154 | Time 61.9265(58.4906) | Bit/dim 3.7161(3.7209) | Xent 0.8500(0.8718) | Loss 9.8031(10.4136) | Error 0.3015(0.3114) Steps 586(600.03) | Grad Norm 4.6723(7.3331) | Total Time 0.00(0.00)\n",
      "Iter 1155 | Time 52.5287(58.3118) | Bit/dim 3.7344(3.7213) | Xent 1.0038(0.8758) | Loss 9.8945(10.3980) | Error 0.3570(0.3128) Steps 574(599.24) | Grad Norm 20.7208(7.7347) | Total Time 0.00(0.00)\n",
      "Iter 1156 | Time 61.3587(58.4032) | Bit/dim 3.7198(3.7213) | Xent 0.8686(0.8756) | Loss 9.6709(10.3762) | Error 0.3086(0.3126) Steps 604(599.39) | Grad Norm 5.0574(7.6544) | Total Time 0.00(0.00)\n",
      "Iter 1157 | Time 59.8152(58.4455) | Bit/dim 3.7152(3.7211) | Xent 0.9440(0.8776) | Loss 10.0424(10.3662) | Error 0.3430(0.3135) Steps 628(600.25) | Grad Norm 11.8689(7.7808) | Total Time 0.00(0.00)\n",
      "Iter 1158 | Time 61.3300(58.5321) | Bit/dim 3.7245(3.7212) | Xent 0.8897(0.8780) | Loss 9.8985(10.3521) | Error 0.3189(0.3137) Steps 616(600.72) | Grad Norm 6.6886(7.7480) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 23.0435, Epoch Time 398.7306(386.8171), Bit/dim 3.7238(best: 3.7198), Xent 1.1369, Loss 4.2923, Error 0.3958(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1159 | Time 54.7507(58.4186) | Bit/dim 3.7221(3.7212) | Xent 0.8744(0.8779) | Loss 13.4406(10.4448) | Error 0.3079(0.3135) Steps 586(600.28) | Grad Norm 8.1898(7.7613) | Total Time 0.00(0.00)\n",
      "Iter 1160 | Time 58.5970(58.4240) | Bit/dim 3.7271(3.7214) | Xent 0.8838(0.8780) | Loss 9.8690(10.4275) | Error 0.3169(0.3136) Steps 574(599.49) | Grad Norm 9.1109(7.8018) | Total Time 0.00(0.00)\n",
      "Iter 1161 | Time 58.1117(58.4146) | Bit/dim 3.7164(3.7213) | Xent 0.8835(0.8782) | Loss 9.7299(10.4066) | Error 0.3189(0.3138) Steps 610(599.80) | Grad Norm 4.8204(7.7123) | Total Time 0.00(0.00)\n",
      "Iter 1162 | Time 56.3005(58.3512) | Bit/dim 3.7229(3.7213) | Xent 0.8979(0.8788) | Loss 9.7415(10.3866) | Error 0.3229(0.3141) Steps 640(601.01) | Grad Norm 7.0779(7.6933) | Total Time 0.00(0.00)\n",
      "Iter 1163 | Time 53.4766(58.2050) | Bit/dim 3.7243(3.7214) | Xent 0.8823(0.8789) | Loss 9.8767(10.3713) | Error 0.3153(0.3141) Steps 556(599.66) | Grad Norm 5.1500(7.6170) | Total Time 0.00(0.00)\n",
      "Iter 1164 | Time 59.0308(58.2297) | Bit/dim 3.7197(3.7213) | Xent 0.8759(0.8788) | Loss 9.9005(10.3572) | Error 0.3135(0.3141) Steps 598(599.61) | Grad Norm 5.8794(7.5649) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 23.0678, Epoch Time 379.3069(386.5918), Bit/dim 3.7264(best: 3.7198), Xent 1.1469, Loss 4.2998, Error 0.3942(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1165 | Time 55.6496(58.1523) | Bit/dim 3.7310(3.7216) | Xent 0.8769(0.8788) | Loss 13.4537(10.4501) | Error 0.3113(0.3140) Steps 568(598.66) | Grad Norm 7.7288(7.5698) | Total Time 0.00(0.00)\n",
      "Iter 1166 | Time 63.1415(58.3020) | Bit/dim 3.7217(3.7216) | Xent 0.8487(0.8778) | Loss 10.0049(10.4367) | Error 0.3045(0.3137) Steps 586(598.28) | Grad Norm 3.8834(7.4592) | Total Time 0.00(0.00)\n",
      "Iter 1167 | Time 58.3354(58.3030) | Bit/dim 3.7142(3.7214) | Xent 0.8433(0.8768) | Loss 9.5835(10.4112) | Error 0.3014(0.3133) Steps 592(598.09) | Grad Norm 6.5607(7.4323) | Total Time 0.00(0.00)\n",
      "Iter 1168 | Time 60.3095(58.3632) | Bit/dim 3.7138(3.7212) | Xent 0.8459(0.8759) | Loss 9.8682(10.3949) | Error 0.3046(0.3131) Steps 580(597.55) | Grad Norm 5.2822(7.3678) | Total Time 0.00(0.00)\n",
      "Iter 1169 | Time 56.5304(58.3082) | Bit/dim 3.7225(3.7212) | Xent 0.8391(0.8748) | Loss 9.6673(10.3730) | Error 0.2984(0.3126) Steps 598(597.56) | Grad Norm 5.9580(7.3255) | Total Time 0.00(0.00)\n",
      "Iter 1170 | Time 59.9113(58.3563) | Bit/dim 3.7196(3.7212) | Xent 0.8508(0.8741) | Loss 9.9201(10.3594) | Error 0.3041(0.3124) Steps 586(597.22) | Grad Norm 6.5159(7.3012) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 23.8280, Epoch Time 393.7232(386.8058), Bit/dim 3.7243(best: 3.7198), Xent 1.1161, Loss 4.2823, Error 0.3849(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1171 | Time 58.7026(58.3667) | Bit/dim 3.7096(3.7208) | Xent 0.8337(0.8729) | Loss 13.3566(10.4494) | Error 0.2999(0.3120) Steps 610(597.60) | Grad Norm 5.0949(7.2350) | Total Time 0.00(0.00)\n",
      "Iter 1172 | Time 59.6125(58.4041) | Bit/dim 3.7208(3.7208) | Xent 0.8411(0.8719) | Loss 9.8642(10.4318) | Error 0.3000(0.3116) Steps 586(597.25) | Grad Norm 5.0174(7.1685) | Total Time 0.00(0.00)\n",
      "Iter 1173 | Time 59.4974(58.4369) | Bit/dim 3.7223(3.7209) | Xent 0.8520(0.8713) | Loss 9.5948(10.4067) | Error 0.3057(0.3115) Steps 616(597.81) | Grad Norm 7.2909(7.1721) | Total Time 0.00(0.00)\n",
      "Iter 1174 | Time 54.4176(58.3163) | Bit/dim 3.7182(3.7208) | Xent 0.8224(0.8698) | Loss 9.6906(10.3852) | Error 0.2909(0.3108) Steps 574(597.10) | Grad Norm 3.5955(7.0648) | Total Time 0.00(0.00)\n",
      "Iter 1175 | Time 59.5426(58.3531) | Bit/dim 3.7189(3.7207) | Xent 0.8262(0.8685) | Loss 9.8463(10.3690) | Error 0.2973(0.3104) Steps 586(596.77) | Grad Norm 5.2552(7.0105) | Total Time 0.00(0.00)\n",
      "Iter 1176 | Time 61.9670(58.4615) | Bit/dim 3.7223(3.7208) | Xent 0.8546(0.8681) | Loss 9.7823(10.3514) | Error 0.3079(0.3104) Steps 640(598.06) | Grad Norm 4.0701(6.9223) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 22.9294, Epoch Time 392.8592(386.9874), Bit/dim 3.7172(best: 3.7198), Xent 1.1169, Loss 4.2756, Error 0.3839(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1177 | Time 59.5059(58.4928) | Bit/dim 3.7173(3.7207) | Xent 0.8300(0.8670) | Loss 13.1155(10.4344) | Error 0.2945(0.3099) Steps 598(598.06) | Grad Norm 2.5172(6.7902) | Total Time 0.00(0.00)\n",
      "Iter 1178 | Time 62.1931(58.6038) | Bit/dim 3.7262(3.7208) | Xent 0.8176(0.8655) | Loss 9.8924(10.4181) | Error 0.2943(0.3094) Steps 598(598.06) | Grad Norm 3.3610(6.6873) | Total Time 0.00(0.00)\n",
      "Iter 1179 | Time 55.3322(58.5057) | Bit/dim 3.7256(3.7210) | Xent 0.8256(0.8643) | Loss 9.7607(10.3984) | Error 0.2956(0.3090) Steps 592(597.88) | Grad Norm 3.7191(6.5983) | Total Time 0.00(0.00)\n",
      "Iter 1180 | Time 57.2299(58.4674) | Bit/dim 3.7197(3.7209) | Xent 0.8251(0.8631) | Loss 9.7922(10.3802) | Error 0.2941(0.3086) Steps 592(597.70) | Grad Norm 2.7712(6.4834) | Total Time 0.00(0.00)\n",
      "Iter 1181 | Time 59.9553(58.5121) | Bit/dim 3.7117(3.7207) | Xent 0.8062(0.8614) | Loss 9.3352(10.3488) | Error 0.2877(0.3079) Steps 598(597.71) | Grad Norm 3.6377(6.3981) | Total Time 0.00(0.00)\n",
      "Iter 1182 | Time 56.6207(58.4553) | Bit/dim 3.7122(3.7204) | Xent 0.8195(0.8601) | Loss 9.7069(10.3296) | Error 0.2917(0.3074) Steps 598(597.72) | Grad Norm 3.4452(6.3095) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 22.6910, Epoch Time 389.6717(387.0679), Bit/dim 3.7271(best: 3.7172), Xent 1.1193, Loss 4.2867, Error 0.3863(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1183 | Time 57.8959(58.4385) | Bit/dim 3.7113(3.7201) | Xent 0.7885(0.8580) | Loss 13.3515(10.4202) | Error 0.2815(0.3067) Steps 592(597.55) | Grad Norm 4.3444(6.2505) | Total Time 0.00(0.00)\n",
      "Iter 1184 | Time 58.5926(58.4432) | Bit/dim 3.7223(3.7202) | Xent 0.7965(0.8562) | Loss 9.8555(10.4033) | Error 0.2847(0.3060) Steps 640(598.82) | Grad Norm 5.5946(6.2309) | Total Time 0.00(0.00)\n",
      "Iter 1185 | Time 56.8615(58.3957) | Bit/dim 3.7197(3.7202) | Xent 0.8172(0.8550) | Loss 9.6487(10.3807) | Error 0.2920(0.3056) Steps 610(599.16) | Grad Norm 7.1633(6.2588) | Total Time 0.00(0.00)\n",
      "Iter 1186 | Time 59.7767(58.4371) | Bit/dim 3.7163(3.7201) | Xent 0.8439(0.8547) | Loss 9.9105(10.3666) | Error 0.2993(0.3054) Steps 622(599.84) | Grad Norm 5.0714(6.2232) | Total Time 0.00(0.00)\n",
      "Iter 1187 | Time 62.2736(58.5522) | Bit/dim 3.7219(3.7201) | Xent 0.8201(0.8536) | Loss 9.8212(10.3502) | Error 0.2891(0.3049) Steps 592(599.61) | Grad Norm 3.1331(6.1305) | Total Time 0.00(0.00)\n",
      "Iter 1188 | Time 57.4585(58.5194) | Bit/dim 3.7191(3.7201) | Xent 0.8162(0.8525) | Loss 9.9028(10.3368) | Error 0.2920(0.3045) Steps 610(599.92) | Grad Norm 5.4813(6.1110) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 23.0761, Epoch Time 391.9545(387.2145), Bit/dim 3.7179(best: 3.7172), Xent 1.1147, Loss 4.2752, Error 0.3870(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1189 | Time 59.7187(58.5554) | Bit/dim 3.7248(3.7202) | Xent 0.8326(0.8519) | Loss 13.8549(10.4423) | Error 0.3034(0.3045) Steps 598(599.86) | Grad Norm 9.0163(6.1982) | Total Time 0.00(0.00)\n",
      "Iter 1190 | Time 60.9453(58.6271) | Bit/dim 3.7113(3.7200) | Xent 0.8403(0.8516) | Loss 9.6605(10.4189) | Error 0.3000(0.3044) Steps 622(600.53) | Grad Norm 12.3106(6.3816) | Total Time 0.00(0.00)\n",
      "Iter 1191 | Time 60.6142(58.6867) | Bit/dim 3.7157(3.7198) | Xent 0.8276(0.8508) | Loss 9.8118(10.4007) | Error 0.2904(0.3039) Steps 604(600.63) | Grad Norm 8.7996(6.4541) | Total Time 0.00(0.00)\n",
      "Iter 1192 | Time 60.4389(58.7393) | Bit/dim 3.7242(3.7200) | Xent 0.8336(0.8503) | Loss 9.9578(10.3874) | Error 0.2929(0.3036) Steps 592(600.37) | Grad Norm 3.9396(6.3787) | Total Time 0.00(0.00)\n",
      "Iter 1193 | Time 57.2695(58.6952) | Bit/dim 3.7094(3.7197) | Xent 0.8001(0.8488) | Loss 9.7600(10.3686) | Error 0.2861(0.3031) Steps 604(600.48) | Grad Norm 3.7178(6.2988) | Total Time 0.00(0.00)\n",
      "Iter 1194 | Time 59.5579(58.7211) | Bit/dim 3.7281(3.7199) | Xent 0.8260(0.8481) | Loss 9.8720(10.3537) | Error 0.2964(0.3029) Steps 604(600.59) | Grad Norm 6.7164(6.3114) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 22.4320, Epoch Time 396.9306(387.5060), Bit/dim 3.7199(best: 3.7172), Xent 1.1191, Loss 4.2795, Error 0.3842(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1195 | Time 59.8146(58.7539) | Bit/dim 3.7153(3.7198) | Xent 0.8046(0.8468) | Loss 13.4354(10.4461) | Error 0.2895(0.3025) Steps 598(600.51) | Grad Norm 7.9277(6.3599) | Total Time 0.00(0.00)\n",
      "Iter 1196 | Time 53.5785(58.5986) | Bit/dim 3.7216(3.7198) | Xent 0.8043(0.8455) | Loss 9.5517(10.4193) | Error 0.2844(0.3019) Steps 592(600.25) | Grad Norm 4.3439(6.2994) | Total Time 0.00(0.00)\n",
      "Iter 1197 | Time 59.2402(58.6179) | Bit/dim 3.7244(3.7200) | Xent 0.8053(0.8443) | Loss 9.8265(10.4015) | Error 0.2864(0.3015) Steps 604(600.36) | Grad Norm 5.8821(6.2869) | Total Time 0.00(0.00)\n",
      "Iter 1198 | Time 58.8587(58.6251) | Bit/dim 3.7145(3.7198) | Xent 0.8315(0.8440) | Loss 9.6549(10.3791) | Error 0.2965(0.3013) Steps 628(601.19) | Grad Norm 10.5652(6.4152) | Total Time 0.00(0.00)\n",
      "Iter 1199 | Time 62.6343(58.7454) | Bit/dim 3.7144(3.7196) | Xent 0.8286(0.8435) | Loss 9.7416(10.3600) | Error 0.2979(0.3012) Steps 574(600.38) | Grad Norm 11.5623(6.5696) | Total Time 0.00(0.00)\n",
      "Iter 1200 | Time 59.1201(58.7566) | Bit/dim 3.7177(3.7196) | Xent 0.8398(0.8434) | Loss 9.8493(10.3447) | Error 0.2983(0.3011) Steps 556(599.05) | Grad Norm 10.4480(6.6860) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 22.2479, Epoch Time 391.2027(387.6169), Bit/dim 3.7191(best: 3.7172), Xent 1.1242, Loss 4.2811, Error 0.3850(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1201 | Time 57.7114(58.7252) | Bit/dim 3.7206(3.7196) | Xent 0.8157(0.8425) | Loss 13.6100(10.4426) | Error 0.2894(0.3008) Steps 598(599.02) | Grad Norm 8.6125(6.7438) | Total Time 0.00(0.00)\n",
      "Iter 1202 | Time 56.2317(58.6504) | Bit/dim 3.7140(3.7194) | Xent 0.8163(0.8418) | Loss 9.8375(10.4245) | Error 0.2979(0.3007) Steps 610(599.34) | Grad Norm 4.4630(6.6753) | Total Time 0.00(0.00)\n",
      "Iter 1203 | Time 59.8924(58.6877) | Bit/dim 3.7163(3.7193) | Xent 0.8097(0.8408) | Loss 9.7610(10.4046) | Error 0.2850(0.3002) Steps 598(599.30) | Grad Norm 4.2690(6.6032) | Total Time 0.00(0.00)\n",
      "Iter 1204 | Time 61.4066(58.7693) | Bit/dim 3.7146(3.7192) | Xent 0.7982(0.8395) | Loss 9.8059(10.3866) | Error 0.2891(0.2999) Steps 592(599.09) | Grad Norm 4.9522(6.5536) | Total Time 0.00(0.00)\n",
      "Iter 1205 | Time 63.4087(58.9084) | Bit/dim 3.7117(3.7190) | Xent 0.8258(0.8391) | Loss 9.6994(10.3660) | Error 0.2997(0.2999) Steps 574(598.33) | Grad Norm 6.0910(6.5397) | Total Time 0.00(0.00)\n",
      "Iter 1206 | Time 56.3631(58.8321) | Bit/dim 3.7058(3.7186) | Xent 0.8012(0.8380) | Loss 9.8763(10.3513) | Error 0.2870(0.2995) Steps 598(598.32) | Grad Norm 3.7946(6.4574) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 23.0781, Epoch Time 394.2233(387.8151), Bit/dim 3.7221(best: 3.7172), Xent 1.1369, Loss 4.2906, Error 0.3857(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1207 | Time 62.0253(58.9279) | Bit/dim 3.7109(3.7183) | Xent 0.7945(0.8367) | Loss 13.6937(10.4516) | Error 0.2820(0.2990) Steps 616(598.85) | Grad Norm 5.4090(6.4259) | Total Time 0.00(0.00)\n",
      "Iter 1208 | Time 60.9616(58.9889) | Bit/dim 3.7144(3.7182) | Xent 0.8123(0.8359) | Loss 9.7593(10.4308) | Error 0.2881(0.2986) Steps 592(598.65) | Grad Norm 9.2752(6.5114) | Total Time 0.00(0.00)\n",
      "Iter 1209 | Time 57.2722(58.9374) | Bit/dim 3.7151(3.7181) | Xent 0.8027(0.8349) | Loss 9.5613(10.4047) | Error 0.2890(0.2984) Steps 604(598.81) | Grad Norm 10.6430(6.6354) | Total Time 0.00(0.00)\n",
      "Iter 1210 | Time 60.2106(58.9756) | Bit/dim 3.7124(3.7180) | Xent 0.8150(0.8343) | Loss 9.8755(10.3888) | Error 0.2927(0.2982) Steps 568(597.88) | Grad Norm 5.0507(6.5878) | Total Time 0.00(0.00)\n",
      "Iter 1211 | Time 58.4135(58.9587) | Bit/dim 3.7258(3.7182) | Xent 0.8106(0.8336) | Loss 9.7420(10.3694) | Error 0.2911(0.2980) Steps 592(597.71) | Grad Norm 5.7659(6.5632) | Total Time 0.00(0.00)\n",
      "Iter 1212 | Time 59.3267(58.9698) | Bit/dim 3.7114(3.7180) | Xent 0.8390(0.8338) | Loss 9.5902(10.3461) | Error 0.2997(0.2980) Steps 622(598.44) | Grad Norm 9.4532(6.6499) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 22.6967, Epoch Time 396.9255(388.0884), Bit/dim 3.7188(best: 3.7172), Xent 1.1640, Loss 4.3007, Error 0.3969(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1213 | Time 56.3788(58.8920) | Bit/dim 3.7233(3.7182) | Xent 0.8295(0.8337) | Loss 13.5627(10.4425) | Error 0.2920(0.2978) Steps 550(596.98) | Grad Norm 7.6423(6.6796) | Total Time 0.00(0.00)\n",
      "Iter 1214 | Time 57.7708(58.8584) | Bit/dim 3.7089(3.7179) | Xent 0.7888(0.8323) | Loss 9.7118(10.4206) | Error 0.2760(0.2972) Steps 592(596.83) | Grad Norm 3.6918(6.5900) | Total Time 0.00(0.00)\n",
      "Iter 1215 | Time 60.4898(58.9073) | Bit/dim 3.7056(3.7175) | Xent 0.8012(0.8314) | Loss 9.6972(10.3989) | Error 0.2866(0.2969) Steps 580(596.33) | Grad Norm 5.7026(6.5634) | Total Time 0.00(0.00)\n",
      "Iter 1216 | Time 57.1000(58.8531) | Bit/dim 3.7189(3.7176) | Xent 0.8098(0.8307) | Loss 9.7305(10.3789) | Error 0.2931(0.2968) Steps 574(595.66) | Grad Norm 7.7312(6.5984) | Total Time 0.00(0.00)\n",
      "Iter 1217 | Time 60.6558(58.9072) | Bit/dim 3.7198(3.7176) | Xent 0.8017(0.8299) | Loss 9.6465(10.3569) | Error 0.2885(0.2965) Steps 592(595.55) | Grad Norm 9.0194(6.6710) | Total Time 0.00(0.00)\n",
      "Iter 1218 | Time 55.8300(58.8149) | Bit/dim 3.7073(3.7173) | Xent 0.8372(0.8301) | Loss 9.6079(10.3344) | Error 0.2955(0.2965) Steps 598(595.62) | Grad Norm 11.2892(6.8096) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 22.5706, Epoch Time 387.1261(388.0595), Bit/dim 3.7219(best: 3.7172), Xent 1.1813, Loss 4.3126, Error 0.3965(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1219 | Time 60.3607(58.8613) | Bit/dim 3.7209(3.7174) | Xent 0.8279(0.8300) | Loss 13.5915(10.4321) | Error 0.2983(0.2965) Steps 616(596.23) | Grad Norm 14.8629(7.0512) | Total Time 0.00(0.00)\n",
      "Iter 1220 | Time 60.6317(58.9144) | Bit/dim 3.7085(3.7172) | Xent 0.8637(0.8310) | Loss 9.7196(10.4108) | Error 0.3074(0.2969) Steps 610(596.65) | Grad Norm 14.8673(7.2857) | Total Time 0.00(0.00)\n",
      "Iter 1221 | Time 56.4333(58.8399) | Bit/dim 3.7287(3.7175) | Xent 0.8429(0.8314) | Loss 9.7095(10.3897) | Error 0.3029(0.2970) Steps 544(595.07) | Grad Norm 12.5550(7.4438) | Total Time 0.00(0.00)\n",
      "Iter 1222 | Time 59.7163(58.8662) | Bit/dim 3.7012(3.7170) | Xent 0.8033(0.8305) | Loss 9.5482(10.3645) | Error 0.2833(0.2966) Steps 598(595.16) | Grad Norm 9.1365(7.4945) | Total Time 0.00(0.00)\n",
      "Iter 1223 | Time 59.1156(58.8737) | Bit/dim 3.7184(3.7171) | Xent 0.8266(0.8304) | Loss 9.5967(10.3414) | Error 0.2936(0.2965) Steps 616(595.78) | Grad Norm 4.9823(7.4192) | Total Time 0.00(0.00)\n",
      "Iter 1224 | Time 62.8153(58.9920) | Bit/dim 3.7170(3.7171) | Xent 0.8200(0.8301) | Loss 9.6654(10.3212) | Error 0.2914(0.2964) Steps 622(596.57) | Grad Norm 11.5361(7.5427) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 23.6067, Epoch Time 398.2499(388.3652), Bit/dim 3.7263(best: 3.7172), Xent 1.1448, Loss 4.2987, Error 0.3896(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1225 | Time 57.9599(58.9610) | Bit/dim 3.7139(3.7170) | Xent 0.8056(0.8294) | Loss 13.5232(10.4172) | Error 0.2900(0.2962) Steps 592(596.43) | Grad Norm 7.8654(7.5524) | Total Time 0.00(0.00)\n",
      "Iter 1226 | Time 54.3769(58.8235) | Bit/dim 3.7178(3.7170) | Xent 0.8033(0.8286) | Loss 9.3555(10.3854) | Error 0.2890(0.2960) Steps 574(595.76) | Grad Norm 9.4617(7.6096) | Total Time 0.00(0.00)\n",
      "Iter 1227 | Time 56.2943(58.7476) | Bit/dim 3.7066(3.7167) | Xent 0.8399(0.8289) | Loss 9.8387(10.3690) | Error 0.3039(0.2962) Steps 574(595.10) | Grad Norm 14.3180(7.8109) | Total Time 0.00(0.00)\n",
      "Iter 1228 | Time 61.5219(58.8308) | Bit/dim 3.7272(3.7170) | Xent 0.7986(0.8280) | Loss 9.8047(10.3520) | Error 0.2824(0.2958) Steps 604(595.37) | Grad Norm 6.1087(7.7598) | Total Time 0.00(0.00)\n",
      "Iter 1229 | Time 54.9268(58.7137) | Bit/dim 3.7063(3.7167) | Xent 0.7991(0.8272) | Loss 9.5487(10.3279) | Error 0.2865(0.2955) Steps 568(594.55) | Grad Norm 8.0344(7.7681) | Total Time 0.00(0.00)\n",
      "Iter 1230 | Time 61.6749(58.8025) | Bit/dim 3.7163(3.7167) | Xent 0.8473(0.8278) | Loss 9.8965(10.3150) | Error 0.3073(0.2959) Steps 574(593.93) | Grad Norm 14.9771(7.9843) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 22.4397, Epoch Time 385.1425(388.2686), Bit/dim 3.7225(best: 3.7172), Xent 1.1700, Loss 4.3075, Error 0.3968(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1231 | Time 56.2350(58.7255) | Bit/dim 3.7066(3.7164) | Xent 0.8131(0.8273) | Loss 13.2930(10.4043) | Error 0.2869(0.2956) Steps 562(592.98) | Grad Norm 12.3175(8.1143) | Total Time 0.00(0.00)\n",
      "Iter 1232 | Time 59.9132(58.7611) | Bit/dim 3.7218(3.7165) | Xent 0.8329(0.8275) | Loss 9.7588(10.3850) | Error 0.2939(0.2955) Steps 586(592.77) | Grad Norm 7.5389(8.0971) | Total Time 0.00(0.00)\n",
      "Iter 1233 | Time 55.3792(58.6597) | Bit/dim 3.7148(3.7165) | Xent 0.8041(0.8268) | Loss 9.6676(10.3635) | Error 0.2881(0.2953) Steps 610(593.28) | Grad Norm 5.7183(8.0257) | Total Time 0.00(0.00)\n",
      "Iter 1234 | Time 57.3875(58.6215) | Bit/dim 3.7095(3.7163) | Xent 0.8110(0.8263) | Loss 9.7727(10.3457) | Error 0.2880(0.2951) Steps 580(592.89) | Grad Norm 5.5485(7.9514) | Total Time 0.00(0.00)\n",
      "Iter 1235 | Time 59.8826(58.6594) | Bit/dim 3.7212(3.7164) | Xent 0.8058(0.8257) | Loss 9.7655(10.3283) | Error 0.2860(0.2948) Steps 598(593.04) | Grad Norm 5.6475(7.8823) | Total Time 0.00(0.00)\n",
      "Iter 1236 | Time 51.8317(58.4545) | Bit/dim 3.7170(3.7164) | Xent 0.7980(0.8249) | Loss 9.6917(10.3092) | Error 0.2849(0.2945) Steps 574(592.47) | Grad Norm 8.0274(7.8866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 23.0800, Epoch Time 379.8795(388.0169), Bit/dim 3.7169(best: 3.7172), Xent 1.1329, Loss 4.2834, Error 0.3903(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1237 | Time 58.5244(58.4566) | Bit/dim 3.7115(3.7163) | Xent 0.7897(0.8238) | Loss 13.3679(10.4010) | Error 0.2815(0.2941) Steps 580(592.09) | Grad Norm 6.6516(7.8496) | Total Time 0.00(0.00)\n",
      "Iter 1238 | Time 58.6790(58.4633) | Bit/dim 3.7035(3.7159) | Xent 0.7843(0.8226) | Loss 9.3810(10.3704) | Error 0.2820(0.2938) Steps 592(592.09) | Grad Norm 8.5787(7.8714) | Total Time 0.00(0.00)\n",
      "Iter 1239 | Time 64.1665(58.6344) | Bit/dim 3.7198(3.7160) | Xent 0.8091(0.8222) | Loss 9.8593(10.3551) | Error 0.2850(0.2935) Steps 580(591.73) | Grad Norm 10.2761(7.9436) | Total Time 0.00(0.00)\n",
      "Iter 1240 | Time 57.1056(58.5885) | Bit/dim 3.7184(3.7161) | Xent 0.8086(0.8218) | Loss 9.8029(10.3385) | Error 0.2854(0.2933) Steps 610(592.28) | Grad Norm 7.2606(7.9231) | Total Time 0.00(0.00)\n",
      "Iter 1241 | Time 56.6457(58.5302) | Bit/dim 3.7121(3.7160) | Xent 0.8027(0.8212) | Loss 9.8572(10.3240) | Error 0.2809(0.2929) Steps 622(593.17) | Grad Norm 3.8447(7.8007) | Total Time 0.00(0.00)\n",
      "Iter 1242 | Time 58.1954(58.5202) | Bit/dim 3.7204(3.7161) | Xent 0.8011(0.8206) | Loss 9.6704(10.3044) | Error 0.2865(0.2927) Steps 586(592.95) | Grad Norm 8.9129(7.8341) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 23.4070, Epoch Time 392.6963(388.1573), Bit/dim 3.7212(best: 3.7169), Xent 1.1466, Loss 4.2945, Error 0.3870(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1243 | Time 59.5848(58.5521) | Bit/dim 3.7161(3.7161) | Xent 0.7906(0.8197) | Loss 13.3879(10.3969) | Error 0.2834(0.2924) Steps 592(592.92) | Grad Norm 9.6246(7.8878) | Total Time 0.00(0.00)\n",
      "Iter 1244 | Time 59.8186(58.5901) | Bit/dim 3.7107(3.7159) | Xent 0.7675(0.8182) | Loss 9.6398(10.3742) | Error 0.2765(0.2919) Steps 592(592.90) | Grad Norm 3.3855(7.7528) | Total Time 0.00(0.00)\n",
      "Iter 1245 | Time 59.7454(58.6248) | Bit/dim 3.7164(3.7159) | Xent 0.7750(0.8169) | Loss 9.7477(10.3554) | Error 0.2742(0.2914) Steps 586(592.69) | Grad Norm 8.0766(7.7625) | Total Time 0.00(0.00)\n",
      "Iter 1246 | Time 61.4974(58.7110) | Bit/dim 3.7097(3.7158) | Xent 0.7958(0.8162) | Loss 9.6638(10.3347) | Error 0.2802(0.2911) Steps 634(593.93) | Grad Norm 8.2083(7.7758) | Total Time 0.00(0.00)\n",
      "Iter 1247 | Time 63.0732(58.8418) | Bit/dim 3.7145(3.7157) | Xent 0.8068(0.8160) | Loss 9.7877(10.3183) | Error 0.2865(0.2909) Steps 586(593.69) | Grad Norm 12.3674(7.9136) | Total Time 0.00(0.00)\n",
      "Iter 1248 | Time 56.5988(58.7745) | Bit/dim 3.7212(3.7159) | Xent 0.8230(0.8162) | Loss 9.5337(10.2947) | Error 0.2989(0.2912) Steps 580(593.28) | Grad Norm 13.1316(8.0701) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 22.9589, Epoch Time 399.3252(388.4923), Bit/dim 3.7184(best: 3.7169), Xent 1.1395, Loss 4.2881, Error 0.3812(best: 0.3813)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1249 | Time 61.0663(58.8433) | Bit/dim 3.7202(3.7160) | Xent 0.7906(0.8154) | Loss 13.5186(10.3915) | Error 0.2837(0.2910) Steps 574(592.70) | Grad Norm 6.0209(8.0087) | Total Time 0.00(0.00)\n",
      "Iter 1250 | Time 59.5067(58.8632) | Bit/dim 3.7256(3.7163) | Xent 0.7766(0.8142) | Loss 9.9066(10.3769) | Error 0.2805(0.2906) Steps 562(591.78) | Grad Norm 6.1861(7.9540) | Total Time 0.00(0.00)\n",
      "Iter 1251 | Time 54.4738(58.7315) | Bit/dim 3.7142(3.7162) | Xent 0.7695(0.8129) | Loss 9.5956(10.3535) | Error 0.2755(0.2902) Steps 598(591.97) | Grad Norm 7.0582(7.9271) | Total Time 0.00(0.00)\n",
      "Iter 1252 | Time 59.2660(58.7476) | Bit/dim 3.7124(3.7161) | Xent 0.7881(0.8122) | Loss 9.5058(10.3280) | Error 0.2788(0.2898) Steps 604(592.33) | Grad Norm 5.3652(7.8503) | Total Time 0.00(0.00)\n",
      "Iter 1253 | Time 58.6502(58.7446) | Bit/dim 3.7116(3.7160) | Xent 0.7924(0.8116) | Loss 9.7153(10.3097) | Error 0.2849(0.2897) Steps 610(592.86) | Grad Norm 7.4690(7.8388) | Total Time 0.00(0.00)\n",
      "Iter 1254 | Time 56.1050(58.6654) | Bit/dim 3.7120(3.7159) | Xent 0.7778(0.8105) | Loss 9.5637(10.2873) | Error 0.2792(0.2894) Steps 598(593.01) | Grad Norm 3.6347(7.7127) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 23.4358, Epoch Time 388.9587(388.5063), Bit/dim 3.7180(best: 3.7169), Xent 1.1535, Loss 4.2948, Error 0.3919(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1255 | Time 64.8890(58.8521) | Bit/dim 3.7177(3.7159) | Xent 0.7932(0.8100) | Loss 13.4937(10.3835) | Error 0.2782(0.2891) Steps 568(592.26) | Grad Norm 7.2110(7.6976) | Total Time 0.00(0.00)\n",
      "Iter 1256 | Time 58.5487(58.8430) | Bit/dim 3.7226(3.7161) | Xent 0.7981(0.8097) | Loss 9.9101(10.3693) | Error 0.2835(0.2889) Steps 610(592.79) | Grad Norm 10.5264(7.7825) | Total Time 0.00(0.00)\n",
      "Iter 1257 | Time 59.8100(58.8721) | Bit/dim 3.7077(3.7159) | Xent 0.7988(0.8093) | Loss 9.7594(10.3510) | Error 0.2860(0.2888) Steps 568(592.05) | Grad Norm 12.7906(7.9327) | Total Time 0.00(0.00)\n",
      "Iter 1258 | Time 60.9843(58.9354) | Bit/dim 3.7122(3.7158) | Xent 0.7694(0.8081) | Loss 9.7383(10.3326) | Error 0.2706(0.2883) Steps 598(592.23) | Grad Norm 9.1589(7.9695) | Total Time 0.00(0.00)\n",
      "Iter 1259 | Time 57.5775(58.8947) | Bit/dim 3.7096(3.7156) | Xent 0.7864(0.8075) | Loss 9.6447(10.3120) | Error 0.2756(0.2879) Steps 622(593.12) | Grad Norm 3.5159(7.8359) | Total Time 0.00(0.00)\n",
      "Iter 1260 | Time 58.6900(58.8885) | Bit/dim 3.7105(3.7154) | Xent 0.7280(0.8051) | Loss 9.6515(10.2921) | Error 0.2586(0.2870) Steps 622(593.99) | Grad Norm 2.4481(7.6743) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 23.4853, Epoch Time 400.4453(388.8645), Bit/dim 3.7157(best: 3.7169), Xent 1.1503, Loss 4.2909, Error 0.3835(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1261 | Time 62.2388(58.9890) | Bit/dim 3.7035(3.7151) | Xent 0.7724(0.8041) | Loss 13.3213(10.3830) | Error 0.2826(0.2869) Steps 556(592.85) | Grad Norm 2.8968(7.5310) | Total Time 0.00(0.00)\n",
      "Iter 1262 | Time 66.6828(59.2199) | Bit/dim 3.7112(3.7149) | Xent 0.7581(0.8027) | Loss 9.5744(10.3588) | Error 0.2658(0.2862) Steps 640(594.26) | Grad Norm 2.9195(7.3926) | Total Time 0.00(0.00)\n",
      "Iter 1263 | Time 55.8711(59.1194) | Bit/dim 3.7004(3.7145) | Xent 0.7573(0.8014) | Loss 9.7643(10.3409) | Error 0.2706(0.2858) Steps 592(594.20) | Grad Norm 3.4090(7.2731) | Total Time 0.00(0.00)\n",
      "Iter 1264 | Time 57.0131(59.0562) | Bit/dim 3.7185(3.7146) | Xent 0.7525(0.7999) | Loss 9.4035(10.3128) | Error 0.2696(0.2853) Steps 592(594.13) | Grad Norm 3.0834(7.1474) | Total Time 0.00(0.00)\n",
      "Iter 1265 | Time 56.9642(58.9935) | Bit/dim 3.7195(3.7148) | Xent 0.7618(0.7988) | Loss 9.7488(10.2959) | Error 0.2781(0.2851) Steps 604(594.43) | Grad Norm 5.0115(7.0833) | Total Time 0.00(0.00)\n",
      "Iter 1266 | Time 60.7668(59.0467) | Bit/dim 3.7151(3.7148) | Xent 0.7827(0.7983) | Loss 9.7859(10.2806) | Error 0.2760(0.2848) Steps 634(595.61) | Grad Norm 10.5271(7.1866) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 22.6385, Epoch Time 398.2966(389.1474), Bit/dim 3.7239(best: 3.7157), Xent 1.1903, Loss 4.3190, Error 0.3992(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1267 | Time 59.5156(59.0607) | Bit/dim 3.7284(3.7152) | Xent 0.7884(0.7980) | Loss 13.8530(10.3878) | Error 0.2740(0.2845) Steps 592(595.50) | Grad Norm 15.7612(7.4439) | Total Time 0.00(0.00)\n",
      "Iter 1268 | Time 60.4638(59.1028) | Bit/dim 3.7184(3.7153) | Xent 0.8596(0.7998) | Loss 9.8714(10.3723) | Error 0.3049(0.2851) Steps 604(595.76) | Grad Norm 23.5076(7.9258) | Total Time 0.00(0.00)\n",
      "Iter 1269 | Time 62.5324(59.2057) | Bit/dim 3.7204(3.7154) | Xent 0.7919(0.7996) | Loss 9.7509(10.3536) | Error 0.2837(0.2850) Steps 610(596.19) | Grad Norm 13.8057(8.1022) | Total Time 0.00(0.00)\n",
      "Iter 1270 | Time 56.1933(59.1153) | Bit/dim 3.7114(3.7153) | Xent 0.7732(0.7988) | Loss 9.7869(10.3366) | Error 0.2726(0.2847) Steps 574(595.52) | Grad Norm 5.5151(8.0246) | Total Time 0.00(0.00)\n",
      "Iter 1271 | Time 56.4601(59.0357) | Bit/dim 3.7084(3.7151) | Xent 0.8380(0.8000) | Loss 9.5932(10.3143) | Error 0.3003(0.2851) Steps 586(595.24) | Grad Norm 12.9528(8.1724) | Total Time 0.00(0.00)\n",
      "Iter 1272 | Time 54.4831(58.8991) | Bit/dim 3.7027(3.7147) | Xent 0.7763(0.7993) | Loss 9.4666(10.2889) | Error 0.2792(0.2850) Steps 574(594.60) | Grad Norm 6.3632(8.1181) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 23.2684, Epoch Time 388.9653(389.1420), Bit/dim 3.7237(best: 3.7157), Xent 1.1638, Loss 4.3056, Error 0.3902(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1273 | Time 57.4697(58.8562) | Bit/dim 3.7065(3.7145) | Xent 0.7927(0.7991) | Loss 13.2479(10.3777) | Error 0.2841(0.2849) Steps 598(594.70) | Grad Norm 7.4676(8.0986) | Total Time 0.00(0.00)\n",
      "Iter 1274 | Time 59.3380(58.8707) | Bit/dim 3.7122(3.7144) | Xent 0.7970(0.7990) | Loss 9.7780(10.3597) | Error 0.2880(0.2850) Steps 556(593.54) | Grad Norm 6.8195(8.0603) | Total Time 0.00(0.00)\n",
      "Iter 1275 | Time 58.5571(58.8613) | Bit/dim 3.7128(3.7144) | Xent 0.7864(0.7986) | Loss 9.6786(10.3392) | Error 0.2748(0.2847) Steps 592(593.49) | Grad Norm 5.9084(7.9957) | Total Time 0.00(0.00)\n",
      "Iter 1276 | Time 57.0526(58.8070) | Bit/dim 3.7101(3.7142) | Xent 0.7677(0.7977) | Loss 9.6475(10.3185) | Error 0.2804(0.2846) Steps 574(592.91) | Grad Norm 7.8357(7.9909) | Total Time 0.00(0.00)\n",
      "Iter 1277 | Time 57.8047(58.7769) | Bit/dim 3.7136(3.7142) | Xent 0.7641(0.7967) | Loss 9.8289(10.3038) | Error 0.2718(0.2842) Steps 580(592.52) | Grad Norm 7.1321(7.9651) | Total Time 0.00(0.00)\n",
      "Iter 1278 | Time 54.3407(58.6438) | Bit/dim 3.7151(3.7142) | Xent 0.7812(0.7962) | Loss 9.7823(10.2882) | Error 0.2810(0.2841) Steps 592(592.51) | Grad Norm 7.6550(7.9558) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 23.1298, Epoch Time 383.5988(388.9757), Bit/dim 3.7218(best: 3.7157), Xent 1.1439, Loss 4.2938, Error 0.3888(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1279 | Time 57.0493(58.5960) | Bit/dim 3.7259(3.7146) | Xent 0.7826(0.7958) | Loss 13.3424(10.3798) | Error 0.2754(0.2838) Steps 562(591.59) | Grad Norm 6.3475(7.9076) | Total Time 0.00(0.00)\n",
      "Iter 1280 | Time 61.0746(58.6704) | Bit/dim 3.7176(3.7147) | Xent 0.7558(0.7946) | Loss 9.9221(10.3661) | Error 0.2640(0.2833) Steps 592(591.60) | Grad Norm 4.3033(7.7995) | Total Time 0.00(0.00)\n",
      "Iter 1281 | Time 62.1861(58.7758) | Bit/dim 3.7152(3.7147) | Xent 0.7648(0.7937) | Loss 9.4398(10.3383) | Error 0.2701(0.2829) Steps 592(591.61) | Grad Norm 7.7682(7.7985) | Total Time 0.00(0.00)\n",
      "Iter 1282 | Time 52.9246(58.6003) | Bit/dim 3.6984(3.7142) | Xent 0.7801(0.7933) | Loss 9.4565(10.3118) | Error 0.2764(0.2827) Steps 592(591.63) | Grad Norm 5.3726(7.7257) | Total Time 0.00(0.00)\n",
      "Iter 1283 | Time 62.2157(58.7088) | Bit/dim 3.7099(3.7141) | Xent 0.7737(0.7927) | Loss 9.4346(10.2855) | Error 0.2785(0.2825) Steps 580(591.28) | Grad Norm 9.8092(7.7882) | Total Time 0.00(0.00)\n",
      "Iter 1284 | Time 57.9136(58.6849) | Bit/dim 3.7114(3.7140) | Xent 0.7786(0.7923) | Loss 9.6930(10.2677) | Error 0.2754(0.2823) Steps 550(590.04) | Grad Norm 11.6172(7.9031) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 23.6970, Epoch Time 393.0323(389.0974), Bit/dim 3.7157(best: 3.7157), Xent 1.1811, Loss 4.3062, Error 0.3881(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1285 | Time 57.2759(58.6426) | Bit/dim 3.7124(3.7140) | Xent 0.7999(0.7925) | Loss 13.3011(10.3587) | Error 0.2866(0.2825) Steps 628(591.18) | Grad Norm 12.3374(8.0361) | Total Time 0.00(0.00)\n",
      "Iter 1286 | Time 61.2335(58.7204) | Bit/dim 3.7122(3.7139) | Xent 0.7619(0.7916) | Loss 9.6320(10.3369) | Error 0.2749(0.2822) Steps 604(591.56) | Grad Norm 6.2407(7.9823) | Total Time 0.00(0.00)\n",
      "Iter 1287 | Time 64.5640(58.8957) | Bit/dim 3.7142(3.7139) | Xent 0.7687(0.7909) | Loss 9.7766(10.3201) | Error 0.2696(0.2818) Steps 640(593.02) | Grad Norm 9.4021(8.0249) | Total Time 0.00(0.00)\n",
      "Iter 1288 | Time 59.7941(58.9226) | Bit/dim 3.7093(3.7138) | Xent 0.7763(0.7905) | Loss 9.7761(10.3038) | Error 0.2794(0.2818) Steps 604(593.35) | Grad Norm 14.1602(8.2089) | Total Time 0.00(0.00)\n",
      "Iter 1289 | Time 56.5451(58.8513) | Bit/dim 3.7159(3.7138) | Xent 0.8228(0.7915) | Loss 9.6219(10.2833) | Error 0.2957(0.2822) Steps 592(593.30) | Grad Norm 11.1158(8.2961) | Total Time 0.00(0.00)\n",
      "Iter 1290 | Time 56.2262(58.7725) | Bit/dim 3.7147(3.7139) | Xent 0.8209(0.7923) | Loss 9.7112(10.2662) | Error 0.2940(0.2825) Steps 604(593.63) | Grad Norm 10.7259(8.3690) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 23.1618, Epoch Time 395.5113(389.2898), Bit/dim 3.7219(best: 3.7157), Xent 1.1569, Loss 4.3003, Error 0.3951(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1291 | Time 60.0081(58.8096) | Bit/dim 3.7144(3.7139) | Xent 0.8031(0.7927) | Loss 13.4617(10.3620) | Error 0.2859(0.2826) Steps 604(593.94) | Grad Norm 9.6803(8.4084) | Total Time 0.00(0.00)\n",
      "Iter 1292 | Time 57.5332(58.7713) | Bit/dim 3.7085(3.7137) | Xent 0.7681(0.7919) | Loss 9.6139(10.3396) | Error 0.2734(0.2824) Steps 610(594.42) | Grad Norm 4.7242(8.2978) | Total Time 0.00(0.00)\n",
      "Iter 1293 | Time 61.2401(58.8454) | Bit/dim 3.7234(3.7140) | Xent 0.7805(0.7916) | Loss 9.6801(10.3198) | Error 0.2764(0.2822) Steps 562(593.45) | Grad Norm 7.1555(8.2636) | Total Time 0.00(0.00)\n",
      "Iter 1294 | Time 60.9362(58.9081) | Bit/dim 3.7086(3.7138) | Xent 0.7453(0.7902) | Loss 9.7428(10.3025) | Error 0.2685(0.2818) Steps 604(593.76) | Grad Norm 3.6869(8.1263) | Total Time 0.00(0.00)\n",
      "Iter 1295 | Time 61.1896(58.9766) | Bit/dim 3.7065(3.7136) | Xent 0.7388(0.7887) | Loss 9.5465(10.2798) | Error 0.2682(0.2814) Steps 616(594.43) | Grad Norm 6.1826(8.0680) | Total Time 0.00(0.00)\n",
      "Iter 1296 | Time 61.6756(59.0575) | Bit/dim 3.7061(3.7134) | Xent 0.7547(0.7876) | Loss 9.6791(10.2618) | Error 0.2730(0.2811) Steps 628(595.44) | Grad Norm 6.8224(8.0306) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 22.9664, Epoch Time 402.5014(389.6861), Bit/dim 3.7175(best: 3.7157), Xent 1.1569, Loss 4.2960, Error 0.3834(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1297 | Time 53.8541(58.9014) | Bit/dim 3.7176(3.7135) | Xent 0.7445(0.7863) | Loss 13.0382(10.3451) | Error 0.2650(0.2806) Steps 586(595.15) | Grad Norm 3.6187(7.8982) | Total Time 0.00(0.00)\n",
      "Iter 1298 | Time 59.7082(58.9256) | Bit/dim 3.7163(3.7136) | Xent 0.7604(0.7856) | Loss 9.3652(10.3157) | Error 0.2725(0.2804) Steps 574(594.52) | Grad Norm 8.2740(7.9095) | Total Time 0.00(0.00)\n",
      "Iter 1299 | Time 55.8602(58.8337) | Bit/dim 3.7174(3.7137) | Xent 0.7739(0.7852) | Loss 9.5306(10.2921) | Error 0.2716(0.2801) Steps 574(593.90) | Grad Norm 13.9167(8.0897) | Total Time 0.00(0.00)\n",
      "Iter 1300 | Time 57.2951(58.7875) | Bit/dim 3.7054(3.7135) | Xent 0.7541(0.7843) | Loss 9.8053(10.2775) | Error 0.2702(0.2798) Steps 610(594.39) | Grad Norm 12.6466(8.2264) | Total Time 0.00(0.00)\n",
      "Iter 1301 | Time 59.3141(58.8033) | Bit/dim 3.7076(3.7133) | Xent 0.7803(0.7842) | Loss 9.7048(10.2604) | Error 0.2819(0.2799) Steps 628(595.40) | Grad Norm 7.7363(8.2117) | Total Time 0.00(0.00)\n",
      "Iter 1302 | Time 60.0731(58.8414) | Bit/dim 3.7027(3.7130) | Xent 0.7629(0.7835) | Loss 9.6872(10.2432) | Error 0.2732(0.2797) Steps 598(595.47) | Grad Norm 4.7451(8.1077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 23.0295, Epoch Time 385.4784(389.5599), Bit/dim 3.7147(best: 3.7157), Xent 1.1457, Loss 4.2876, Error 0.3841(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1303 | Time 59.1438(58.8505) | Bit/dim 3.7097(3.7129) | Xent 0.7794(0.7834) | Loss 13.2345(10.3329) | Error 0.2788(0.2797) Steps 592(595.37) | Grad Norm 8.1885(8.1102) | Total Time 0.00(0.00)\n",
      "Iter 1304 | Time 58.5844(58.8425) | Bit/dim 3.7052(3.7126) | Xent 0.7592(0.7827) | Loss 9.5701(10.3100) | Error 0.2692(0.2794) Steps 610(595.81) | Grad Norm 4.4413(8.0001) | Total Time 0.00(0.00)\n",
      "Iter 1305 | Time 50.8729(58.6034) | Bit/dim 3.7161(3.7128) | Xent 0.7421(0.7815) | Loss 9.6917(10.2915) | Error 0.2631(0.2789) Steps 580(595.33) | Grad Norm 5.9010(7.9371) | Total Time 0.00(0.00)\n",
      "Iter 1306 | Time 58.2174(58.5918) | Bit/dim 3.7130(3.7128) | Xent 0.7195(0.7796) | Loss 9.6088(10.2710) | Error 0.2568(0.2782) Steps 598(595.41) | Grad Norm 3.8972(7.8159) | Total Time 0.00(0.00)\n",
      "Iter 1307 | Time 60.6180(58.6526) | Bit/dim 3.7075(3.7126) | Xent 0.7425(0.7785) | Loss 9.5947(10.2507) | Error 0.2718(0.2780) Steps 586(595.13) | Grad Norm 4.0593(7.7032) | Total Time 0.00(0.00)\n",
      "Iter 1308 | Time 59.1670(58.6680) | Bit/dim 3.7101(3.7125) | Xent 0.7492(0.7776) | Loss 9.6708(10.2333) | Error 0.2659(0.2776) Steps 586(594.86) | Grad Norm 8.0273(7.7129) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 22.8912, Epoch Time 385.8435(389.4484), Bit/dim 3.7181(best: 3.7147), Xent 1.1612, Loss 4.2986, Error 0.3831(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1309 | Time 55.6512(58.5775) | Bit/dim 3.7079(3.7124) | Xent 0.7584(0.7770) | Loss 13.1317(10.3203) | Error 0.2691(0.2774) Steps 598(594.95) | Grad Norm 6.6049(7.6797) | Total Time 0.00(0.00)\n",
      "Iter 1310 | Time 61.0679(58.6522) | Bit/dim 3.7127(3.7124) | Xent 0.7475(0.7761) | Loss 9.7637(10.3036) | Error 0.2704(0.2772) Steps 604(595.22) | Grad Norm 5.5293(7.6152) | Total Time 0.00(0.00)\n",
      "Iter 1311 | Time 57.7941(58.6265) | Bit/dim 3.7137(3.7124) | Xent 0.7348(0.7749) | Loss 9.6949(10.2853) | Error 0.2621(0.2767) Steps 622(596.03) | Grad Norm 6.9632(7.5956) | Total Time 0.00(0.00)\n",
      "Iter 1312 | Time 56.2148(58.5541) | Bit/dim 3.7159(3.7125) | Xent 0.7381(0.7738) | Loss 9.6033(10.2648) | Error 0.2644(0.2764) Steps 604(596.27) | Grad Norm 9.2732(7.6460) | Total Time 0.00(0.00)\n",
      "Iter 1313 | Time 56.8406(58.5027) | Bit/dim 3.7138(3.7126) | Xent 0.7283(0.7724) | Loss 9.7439(10.2492) | Error 0.2692(0.2761) Steps 580(595.78) | Grad Norm 8.7486(7.6790) | Total Time 0.00(0.00)\n",
      "Iter 1314 | Time 56.0525(58.4292) | Bit/dim 3.7188(3.7128) | Xent 0.7550(0.7719) | Loss 9.4923(10.2265) | Error 0.2636(0.2758) Steps 604(596.02) | Grad Norm 10.9463(7.7771) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 23.4584, Epoch Time 383.2043(389.2611), Bit/dim 3.7180(best: 3.7147), Xent 1.1806, Loss 4.3083, Error 0.3891(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1315 | Time 56.7454(58.3787) | Bit/dim 3.7104(3.7127) | Xent 0.7569(0.7715) | Loss 13.5435(10.3260) | Error 0.2715(0.2756) Steps 586(595.72) | Grad Norm 11.3407(7.8840) | Total Time 0.00(0.00)\n",
      "Iter 1316 | Time 58.0851(58.3699) | Bit/dim 3.7123(3.7127) | Xent 0.7570(0.7710) | Loss 9.7435(10.3085) | Error 0.2712(0.2755) Steps 556(594.53) | Grad Norm 12.3724(8.0186) | Total Time 0.00(0.00)\n",
      "Iter 1317 | Time 60.8169(58.4433) | Bit/dim 3.7115(3.7126) | Xent 0.7750(0.7711) | Loss 9.6984(10.2902) | Error 0.2798(0.2756) Steps 592(594.46) | Grad Norm 13.4987(8.1830) | Total Time 0.00(0.00)\n",
      "Iter 1318 | Time 60.7892(58.5137) | Bit/dim 3.7142(3.7127) | Xent 0.7614(0.7709) | Loss 9.7455(10.2739) | Error 0.2692(0.2754) Steps 574(593.84) | Grad Norm 8.6056(8.1957) | Total Time 0.00(0.00)\n",
      "Iter 1319 | Time 60.7038(58.5794) | Bit/dim 3.7067(3.7125) | Xent 0.7395(0.7699) | Loss 9.6503(10.2552) | Error 0.2660(0.2752) Steps 580(593.43) | Grad Norm 5.1056(8.1030) | Total Time 0.00(0.00)\n",
      "Iter 1320 | Time 59.5991(58.6100) | Bit/dim 3.7066(3.7123) | Xent 0.7524(0.7694) | Loss 9.6309(10.2364) | Error 0.2726(0.2751) Steps 628(594.46) | Grad Norm 7.6026(8.0880) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 23.1544, Epoch Time 396.0229(389.4639), Bit/dim 3.7126(best: 3.7147), Xent 1.2010, Loss 4.3131, Error 0.3895(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1321 | Time 57.0729(58.5639) | Bit/dim 3.7059(3.7121) | Xent 0.7710(0.7694) | Loss 13.7302(10.3413) | Error 0.2799(0.2752) Steps 616(595.11) | Grad Norm 9.8176(8.1399) | Total Time 0.00(0.00)\n",
      "Iter 1322 | Time 56.0417(58.4882) | Bit/dim 3.7135(3.7122) | Xent 0.7475(0.7688) | Loss 9.6319(10.3200) | Error 0.2636(0.2749) Steps 598(595.20) | Grad Norm 7.7128(8.1271) | Total Time 0.00(0.00)\n",
      "Iter 1323 | Time 55.3362(58.3936) | Bit/dim 3.7126(3.7122) | Xent 0.7274(0.7675) | Loss 9.6055(10.2985) | Error 0.2600(0.2744) Steps 580(594.74) | Grad Norm 4.6732(8.0234) | Total Time 0.00(0.00)\n",
      "Iter 1324 | Time 62.8622(58.5277) | Bit/dim 3.7144(3.7123) | Xent 0.7478(0.7669) | Loss 9.6231(10.2783) | Error 0.2670(0.2742) Steps 568(593.94) | Grad Norm 9.5222(8.0684) | Total Time 0.00(0.00)\n",
      "Iter 1325 | Time 58.8126(58.5363) | Bit/dim 3.7025(3.7120) | Xent 0.7454(0.7663) | Loss 9.6940(10.2608) | Error 0.2661(0.2740) Steps 604(594.24) | Grad Norm 8.9812(8.0958) | Total Time 0.00(0.00)\n",
      "Iter 1326 | Time 58.9652(58.5491) | Bit/dim 3.7100(3.7119) | Xent 0.7229(0.7650) | Loss 9.4849(10.2375) | Error 0.2555(0.2734) Steps 604(594.53) | Grad Norm 2.8830(7.9394) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 22.6368, Epoch Time 387.8508(389.4156), Bit/dim 3.7201(best: 3.7126), Xent 1.1824, Loss 4.3113, Error 0.3890(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1327 | Time 56.5698(58.4897) | Bit/dim 3.7181(3.7121) | Xent 0.7353(0.7641) | Loss 13.2301(10.3273) | Error 0.2626(0.2731) Steps 592(594.46) | Grad Norm 10.1021(8.0043) | Total Time 0.00(0.00)\n",
      "Iter 1328 | Time 61.9631(58.5939) | Bit/dim 3.7077(3.7120) | Xent 0.7507(0.7637) | Loss 9.7180(10.3090) | Error 0.2635(0.2728) Steps 574(593.84) | Grad Norm 10.9432(8.0924) | Total Time 0.00(0.00)\n",
      "Iter 1329 | Time 59.0658(58.6081) | Bit/dim 3.7176(3.7121) | Xent 0.7357(0.7629) | Loss 9.5495(10.2862) | Error 0.2621(0.2725) Steps 592(593.79) | Grad Norm 13.4225(8.2523) | Total Time 0.00(0.00)\n",
      "Iter 1330 | Time 53.7993(58.4638) | Bit/dim 3.7064(3.7120) | Xent 0.7733(0.7632) | Loss 9.5386(10.2638) | Error 0.2769(0.2726) Steps 580(593.37) | Grad Norm 16.2867(8.4934) | Total Time 0.00(0.00)\n",
      "Iter 1331 | Time 60.3084(58.5192) | Bit/dim 3.7078(3.7118) | Xent 0.7539(0.7629) | Loss 9.6227(10.2445) | Error 0.2745(0.2727) Steps 616(594.05) | Grad Norm 10.6905(8.5593) | Total Time 0.00(0.00)\n",
      "Iter 1332 | Time 58.0109(58.5039) | Bit/dim 3.7079(3.7117) | Xent 0.7390(0.7622) | Loss 9.6744(10.2274) | Error 0.2600(0.2723) Steps 634(595.25) | Grad Norm 6.4341(8.4955) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 22.8449, Epoch Time 388.9625(389.4020), Bit/dim 3.7163(best: 3.7126), Xent 1.1888, Loss 4.3106, Error 0.3943(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1333 | Time 57.3622(58.4697) | Bit/dim 3.7088(3.7116) | Xent 0.7432(0.7616) | Loss 13.5287(10.3265) | Error 0.2651(0.2721) Steps 598(595.33) | Grad Norm 5.9818(8.4201) | Total Time 0.00(0.00)\n",
      "Iter 1334 | Time 56.3835(58.4071) | Bit/dim 3.7029(3.7114) | Xent 0.7523(0.7613) | Loss 9.7009(10.3077) | Error 0.2676(0.2719) Steps 610(595.77) | Grad Norm 8.1990(8.4135) | Total Time 0.00(0.00)\n",
      "Iter 1335 | Time 60.3711(58.4660) | Bit/dim 3.7082(3.7113) | Xent 0.7677(0.7615) | Loss 9.7535(10.2911) | Error 0.2754(0.2720) Steps 622(596.56) | Grad Norm 11.0646(8.4930) | Total Time 0.00(0.00)\n",
      "Iter 1336 | Time 64.9387(58.6602) | Bit/dim 3.7139(3.7113) | Xent 0.7722(0.7618) | Loss 9.7667(10.2753) | Error 0.2706(0.2720) Steps 574(595.88) | Grad Norm 12.5247(8.6140) | Total Time 0.00(0.00)\n",
      "Iter 1337 | Time 60.4642(58.7143) | Bit/dim 3.7095(3.7113) | Xent 0.7747(0.7622) | Loss 9.6461(10.2565) | Error 0.2778(0.2722) Steps 586(595.59) | Grad Norm 9.5333(8.6416) | Total Time 0.00(0.00)\n",
      "Iter 1338 | Time 60.7450(58.7752) | Bit/dim 3.7227(3.7116) | Xent 0.7489(0.7618) | Loss 9.8109(10.2431) | Error 0.2669(0.2720) Steps 598(595.66) | Grad Norm 8.9447(8.6506) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 23.1750, Epoch Time 399.1426(389.6942), Bit/dim 3.7210(best: 3.7126), Xent 1.1677, Loss 4.3048, Error 0.3910(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1339 | Time 63.4396(58.9152) | Bit/dim 3.7165(3.7118) | Xent 0.7642(0.7619) | Loss 13.8127(10.3502) | Error 0.2699(0.2720) Steps 616(596.27) | Grad Norm 11.1038(8.7242) | Total Time 0.00(0.00)\n",
      "Iter 1340 | Time 54.6347(58.7867) | Bit/dim 3.7094(3.7117) | Xent 0.7240(0.7608) | Loss 9.1279(10.3135) | Error 0.2611(0.2716) Steps 598(596.32) | Grad Norm 4.1433(8.5868) | Total Time 0.00(0.00)\n",
      "Iter 1341 | Time 52.8797(58.6095) | Bit/dim 3.7063(3.7115) | Xent 0.7494(0.7604) | Loss 9.7354(10.2962) | Error 0.2668(0.2715) Steps 604(596.55) | Grad Norm 6.0286(8.5101) | Total Time 0.00(0.00)\n",
      "Iter 1342 | Time 58.0524(58.5928) | Bit/dim 3.7087(3.7115) | Xent 0.7300(0.7595) | Loss 9.4740(10.2715) | Error 0.2604(0.2711) Steps 610(596.96) | Grad Norm 4.5174(8.3903) | Total Time 0.00(0.00)\n",
      "Iter 1343 | Time 58.0004(58.5750) | Bit/dim 3.7094(3.7114) | Xent 0.7562(0.7594) | Loss 9.7086(10.2546) | Error 0.2695(0.2711) Steps 544(595.37) | Grad Norm 7.9170(8.3761) | Total Time 0.00(0.00)\n",
      "Iter 1344 | Time 58.6285(58.5767) | Bit/dim 3.7149(3.7115) | Xent 0.7400(0.7588) | Loss 9.3943(10.2288) | Error 0.2618(0.2708) Steps 580(594.91) | Grad Norm 14.1199(8.5484) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 22.8559, Epoch Time 384.9937(389.5532), Bit/dim 3.7203(best: 3.7126), Xent 1.1921, Loss 4.3163, Error 0.3957(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1345 | Time 57.6083(58.5476) | Bit/dim 3.7070(3.7114) | Xent 0.7672(0.7591) | Loss 13.7053(10.3331) | Error 0.2741(0.2709) Steps 598(595.00) | Grad Norm 15.9476(8.7704) | Total Time 0.00(0.00)\n",
      "Iter 1346 | Time 56.2264(58.4780) | Bit/dim 3.7229(3.7117) | Xent 0.7845(0.7598) | Loss 9.7442(10.3154) | Error 0.2840(0.2713) Steps 616(595.63) | Grad Norm 14.9981(8.9572) | Total Time 0.00(0.00)\n",
      "Iter 1347 | Time 61.4411(58.5669) | Bit/dim 3.7121(3.7117) | Xent 0.7791(0.7604) | Loss 9.6977(10.2969) | Error 0.2782(0.2715) Steps 574(594.98) | Grad Norm 13.7612(9.1013) | Total Time 0.00(0.00)\n",
      "Iter 1348 | Time 60.3211(58.6195) | Bit/dim 3.7211(3.7120) | Xent 0.7604(0.7604) | Loss 9.6111(10.2763) | Error 0.2698(0.2715) Steps 628(595.97) | Grad Norm 12.7137(9.2097) | Total Time 0.00(0.00)\n",
      "Iter 1349 | Time 59.6719(58.6511) | Bit/dim 3.7170(3.7122) | Xent 0.7369(0.7597) | Loss 9.6410(10.2573) | Error 0.2655(0.2713) Steps 598(596.03) | Grad Norm 6.2721(9.1216) | Total Time 0.00(0.00)\n",
      "Iter 1350 | Time 54.0018(58.5116) | Bit/dim 3.7011(3.7118) | Xent 0.7678(0.7600) | Loss 9.6378(10.2387) | Error 0.2744(0.2714) Steps 604(596.27) | Grad Norm 13.2231(9.2446) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 22.8813, Epoch Time 388.5672(389.5236), Bit/dim 3.7130(best: 3.7126), Xent 1.1443, Loss 4.2852, Error 0.3840(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1351 | Time 57.5448(58.4826) | Bit/dim 3.7043(3.7116) | Xent 0.7346(0.7592) | Loss 13.2223(10.3282) | Error 0.2626(0.2711) Steps 586(595.96) | Grad Norm 3.5274(9.0731) | Total Time 0.00(0.00)\n",
      "Iter 1352 | Time 63.2781(58.6264) | Bit/dim 3.7076(3.7115) | Xent 0.7281(0.7583) | Loss 9.7990(10.3123) | Error 0.2590(0.2708) Steps 592(595.84) | Grad Norm 10.1724(9.1061) | Total Time 0.00(0.00)\n",
      "Iter 1353 | Time 59.3888(58.6493) | Bit/dim 3.7023(3.7112) | Xent 0.7131(0.7569) | Loss 9.6449(10.2923) | Error 0.2574(0.2704) Steps 616(596.45) | Grad Norm 5.3400(8.9931) | Total Time 0.00(0.00)\n",
      "Iter 1354 | Time 59.6754(58.6801) | Bit/dim 3.7091(3.7111) | Xent 0.7509(0.7567) | Loss 9.8406(10.2787) | Error 0.2696(0.2703) Steps 568(595.59) | Grad Norm 4.8089(8.8676) | Total Time 0.00(0.00)\n",
      "Iter 1355 | Time 50.8245(58.4444) | Bit/dim 3.7221(3.7115) | Xent 0.7222(0.7557) | Loss 9.4834(10.2549) | Error 0.2575(0.2699) Steps 568(594.77) | Grad Norm 4.2582(8.7293) | Total Time 0.00(0.00)\n",
      "Iter 1356 | Time 66.4349(58.6841) | Bit/dim 3.7148(3.7116) | Xent 0.7101(0.7543) | Loss 9.8025(10.2413) | Error 0.2555(0.2695) Steps 598(594.86) | Grad Norm 4.7053(8.6086) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 23.3836, Epoch Time 397.0214(389.7485), Bit/dim 3.7163(best: 3.7126), Xent 1.1910, Loss 4.3118, Error 0.3846(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1357 | Time 56.9874(58.6332) | Bit/dim 3.7295(3.7121) | Xent 0.7228(0.7534) | Loss 13.5139(10.3395) | Error 0.2610(0.2693) Steps 628(595.86) | Grad Norm 6.5572(8.5470) | Total Time 0.00(0.00)\n",
      "Iter 1358 | Time 64.7773(58.8176) | Bit/dim 3.7005(3.7118) | Xent 0.7336(0.7528) | Loss 9.5584(10.3161) | Error 0.2635(0.2691) Steps 592(595.74) | Grad Norm 8.4629(8.5445) | Total Time 0.00(0.00)\n",
      "Iter 1359 | Time 55.0088(58.7033) | Bit/dim 3.7085(3.7117) | Xent 0.7155(0.7517) | Loss 9.7214(10.2982) | Error 0.2558(0.2687) Steps 622(596.53) | Grad Norm 8.0075(8.5284) | Total Time 0.00(0.00)\n",
      "Iter 1360 | Time 54.7665(58.5852) | Bit/dim 3.7101(3.7116) | Xent 0.7137(0.7505) | Loss 9.6006(10.2773) | Error 0.2546(0.2683) Steps 592(596.39) | Grad Norm 4.0574(8.3943) | Total Time 0.00(0.00)\n",
      "Iter 1361 | Time 57.1077(58.5409) | Bit/dim 3.7129(3.7117) | Xent 0.6984(0.7490) | Loss 9.5122(10.2543) | Error 0.2501(0.2677) Steps 574(595.72) | Grad Norm 8.6230(8.4011) | Total Time 0.00(0.00)\n",
      "Iter 1362 | Time 58.4085(58.5369) | Bit/dim 3.6985(3.7113) | Xent 0.7396(0.7487) | Loss 9.7295(10.2386) | Error 0.2675(0.2677) Steps 598(595.79) | Grad Norm 11.2438(8.4864) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 23.0857, Epoch Time 386.3902(389.6478), Bit/dim 3.7128(best: 3.7126), Xent 1.1918, Loss 4.3087, Error 0.3929(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1363 | Time 61.1223(58.6145) | Bit/dim 3.6992(3.7109) | Xent 0.7321(0.7482) | Loss 13.3974(10.3334) | Error 0.2606(0.2675) Steps 598(595.86) | Grad Norm 11.1804(8.5672) | Total Time 0.00(0.00)\n",
      "Iter 1364 | Time 57.8142(58.5905) | Bit/dim 3.6969(3.7105) | Xent 0.7156(0.7472) | Loss 9.6062(10.3115) | Error 0.2541(0.2671) Steps 622(596.64) | Grad Norm 9.8084(8.6045) | Total Time 0.00(0.00)\n",
      "Iter 1365 | Time 59.8463(58.6281) | Bit/dim 3.7124(3.7105) | Xent 0.7177(0.7463) | Loss 9.6214(10.2908) | Error 0.2551(0.2667) Steps 568(595.78) | Grad Norm 6.8985(8.5533) | Total Time 0.00(0.00)\n",
      "Iter 1366 | Time 63.3760(58.7706) | Bit/dim 3.7099(3.7105) | Xent 0.7113(0.7453) | Loss 9.7067(10.2733) | Error 0.2531(0.2663) Steps 580(595.31) | Grad Norm 7.0720(8.5089) | Total Time 0.00(0.00)\n",
      "Iter 1367 | Time 57.5434(58.7338) | Bit/dim 3.7173(3.7107) | Xent 0.6918(0.7437) | Loss 9.5975(10.2530) | Error 0.2490(0.2658) Steps 610(595.75) | Grad Norm 6.5725(8.4508) | Total Time 0.00(0.00)\n",
      "Iter 1368 | Time 61.9495(58.8302) | Bit/dim 3.7165(3.7109) | Xent 0.7070(0.7426) | Loss 9.6774(10.2358) | Error 0.2522(0.2654) Steps 616(596.36) | Grad Norm 6.1538(8.3819) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 22.8449, Epoch Time 400.7991(389.9823), Bit/dim 3.7145(best: 3.7126), Xent 1.1966, Loss 4.3128, Error 0.3901(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1369 | Time 58.3087(58.8146) | Bit/dim 3.7128(3.7109) | Xent 0.7039(0.7414) | Loss 13.4396(10.3319) | Error 0.2451(0.2648) Steps 622(597.13) | Grad Norm 7.6827(8.3609) | Total Time 0.00(0.00)\n",
      "Iter 1370 | Time 59.0562(58.8218) | Bit/dim 3.7001(3.7106) | Xent 0.7097(0.7405) | Loss 9.5754(10.3092) | Error 0.2562(0.2645) Steps 610(597.51) | Grad Norm 9.3715(8.3912) | Total Time 0.00(0.00)\n",
      "Iter 1371 | Time 53.1971(58.6531) | Bit/dim 3.7129(3.7107) | Xent 0.6827(0.7387) | Loss 9.6034(10.2880) | Error 0.2459(0.2640) Steps 556(596.27) | Grad Norm 6.3083(8.3287) | Total Time 0.00(0.00)\n",
      "Iter 1372 | Time 61.1135(58.7269) | Bit/dim 3.7088(3.7106) | Xent 0.6990(0.7375) | Loss 9.6285(10.2682) | Error 0.2449(0.2634) Steps 610(596.68) | Grad Norm 9.0061(8.3490) | Total Time 0.00(0.00)\n",
      "Iter 1373 | Time 56.2134(58.6515) | Bit/dim 3.7108(3.7106) | Xent 0.7059(0.7366) | Loss 9.2641(10.2381) | Error 0.2519(0.2631) Steps 616(597.26) | Grad Norm 6.5975(8.2965) | Total Time 0.00(0.00)\n",
      "Iter 1374 | Time 60.6054(58.7101) | Bit/dim 3.7085(3.7106) | Xent 0.7053(0.7356) | Loss 9.4505(10.2145) | Error 0.2541(0.2628) Steps 598(597.28) | Grad Norm 3.7607(8.1604) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 23.1512, Epoch Time 387.9912(389.9226), Bit/dim 3.7100(best: 3.7126), Xent 1.1862, Loss 4.3031, Error 0.3839(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1375 | Time 55.7958(58.6227) | Bit/dim 3.6988(3.7102) | Xent 0.7049(0.7347) | Loss 13.1826(10.3035) | Error 0.2491(0.2624) Steps 580(596.76) | Grad Norm 5.5976(8.0835) | Total Time 0.00(0.00)\n",
      "Iter 1376 | Time 59.7437(58.6563) | Bit/dim 3.7100(3.7102) | Xent 0.7020(0.7337) | Loss 9.5721(10.2816) | Error 0.2471(0.2619) Steps 604(596.98) | Grad Norm 8.4816(8.0955) | Total Time 0.00(0.00)\n",
      "Iter 1377 | Time 61.4652(58.7406) | Bit/dim 3.7094(3.7102) | Xent 0.6964(0.7326) | Loss 9.6401(10.2623) | Error 0.2470(0.2615) Steps 634(598.09) | Grad Norm 10.6209(8.1712) | Total Time 0.00(0.00)\n",
      "Iter 1378 | Time 60.4211(58.7910) | Bit/dim 3.7168(3.7104) | Xent 0.7192(0.7322) | Loss 9.7460(10.2468) | Error 0.2568(0.2613) Steps 610(598.45) | Grad Norm 12.5134(8.3015) | Total Time 0.00(0.00)\n",
      "Iter 1379 | Time 57.4843(58.7518) | Bit/dim 3.7123(3.7104) | Xent 0.7277(0.7321) | Loss 9.4335(10.2224) | Error 0.2619(0.2614) Steps 610(598.79) | Grad Norm 12.0991(8.4154) | Total Time 0.00(0.00)\n",
      "Iter 1380 | Time 59.6615(58.7791) | Bit/dim 3.7020(3.7102) | Xent 0.7296(0.7320) | Loss 9.7248(10.2075) | Error 0.2632(0.2614) Steps 598(598.77) | Grad Norm 10.2076(8.4692) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 23.2776, Epoch Time 393.9202(390.0425), Bit/dim 3.7148(best: 3.7100), Xent 1.1698, Loss 4.2997, Error 0.3832(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1381 | Time 56.1990(58.7017) | Bit/dim 3.7101(3.7102) | Xent 0.7160(0.7315) | Loss 13.3050(10.3004) | Error 0.2558(0.2612) Steps 592(598.57) | Grad Norm 7.2332(8.4321) | Total Time 0.00(0.00)\n",
      "Iter 1382 | Time 59.5388(58.7268) | Bit/dim 3.7115(3.7102) | Xent 0.6715(0.7297) | Loss 9.3887(10.2731) | Error 0.2434(0.2607) Steps 598(598.55) | Grad Norm 2.6233(8.2579) | Total Time 0.00(0.00)\n",
      "Iter 1383 | Time 58.3747(58.7162) | Bit/dim 3.7066(3.7101) | Xent 0.7022(0.7289) | Loss 9.6457(10.2543) | Error 0.2598(0.2607) Steps 628(599.43) | Grad Norm 6.9611(8.2189) | Total Time 0.00(0.00)\n",
      "Iter 1384 | Time 62.4168(58.8273) | Bit/dim 3.7030(3.7099) | Xent 0.6906(0.7278) | Loss 9.6859(10.2372) | Error 0.2442(0.2602) Steps 568(598.49) | Grad Norm 6.5485(8.1688) | Total Time 0.00(0.00)\n",
      "Iter 1385 | Time 64.0142(58.9829) | Bit/dim 3.7057(3.7098) | Xent 0.6683(0.7260) | Loss 9.6337(10.2191) | Error 0.2404(0.2596) Steps 586(598.12) | Grad Norm 3.0093(8.0140) | Total Time 0.00(0.00)\n",
      "Iter 1386 | Time 62.4825(59.0879) | Bit/dim 3.7049(3.7096) | Xent 0.6982(0.7251) | Loss 9.6819(10.2030) | Error 0.2469(0.2592) Steps 616(598.65) | Grad Norm 6.3997(7.9656) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 23.0810, Epoch Time 402.2607(390.4091), Bit/dim 3.7138(best: 3.7100), Xent 1.2048, Loss 4.3162, Error 0.3853(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1387 | Time 60.7790(59.1386) | Bit/dim 3.7069(3.7096) | Xent 0.6790(0.7238) | Loss 13.2935(10.2957) | Error 0.2409(0.2587) Steps 598(598.63) | Grad Norm 4.8488(7.8721) | Total Time 0.00(0.00)\n",
      "Iter 1388 | Time 57.4225(59.0871) | Bit/dim 3.7054(3.7094) | Xent 0.6886(0.7227) | Loss 9.6726(10.2770) | Error 0.2498(0.2584) Steps 628(599.51) | Grad Norm 6.2845(7.8245) | Total Time 0.00(0.00)\n",
      "Iter 1389 | Time 58.9240(59.0822) | Bit/dim 3.7087(3.7094) | Xent 0.7015(0.7221) | Loss 9.3532(10.2493) | Error 0.2515(0.2582) Steps 610(599.83) | Grad Norm 9.7135(7.8812) | Total Time 0.00(0.00)\n",
      "Iter 1390 | Time 65.3775(59.2711) | Bit/dim 3.7178(3.7097) | Xent 0.7284(0.7223) | Loss 9.7096(10.2331) | Error 0.2592(0.2582) Steps 592(599.59) | Grad Norm 11.8919(8.0015) | Total Time 0.00(0.00)\n",
      "Iter 1391 | Time 63.9968(59.4128) | Bit/dim 3.7132(3.7098) | Xent 0.7274(0.7224) | Loss 9.6099(10.2144) | Error 0.2581(0.2582) Steps 616(600.09) | Grad Norm 15.5953(8.2293) | Total Time 0.00(0.00)\n",
      "Iter 1392 | Time 59.7169(59.4220) | Bit/dim 3.7140(3.7099) | Xent 0.7590(0.7235) | Loss 9.3333(10.1880) | Error 0.2745(0.2587) Steps 616(600.56) | Grad Norm 17.0703(8.4945) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 22.6845, Epoch Time 404.9286(390.8446), Bit/dim 3.7153(best: 3.7100), Xent 1.2393, Loss 4.3349, Error 0.3934(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1393 | Time 54.9677(59.2883) | Bit/dim 3.6950(3.7094) | Xent 0.7385(0.7240) | Loss 13.2540(10.2800) | Error 0.2644(0.2589) Steps 586(600.13) | Grad Norm 12.9273(8.6275) | Total Time 0.00(0.00)\n",
      "Iter 1394 | Time 60.6413(59.3289) | Bit/dim 3.7053(3.7093) | Xent 0.7303(0.7241) | Loss 9.3789(10.2529) | Error 0.2614(0.2589) Steps 616(600.60) | Grad Norm 7.7155(8.6001) | Total Time 0.00(0.00)\n",
      "Iter 1395 | Time 61.8936(59.4059) | Bit/dim 3.7219(3.7097) | Xent 0.7084(0.7237) | Loss 9.6703(10.2354) | Error 0.2539(0.2588) Steps 580(599.98) | Grad Norm 9.9904(8.6419) | Total Time 0.00(0.00)\n",
      "Iter 1396 | Time 66.0878(59.6063) | Bit/dim 3.7047(3.7095) | Xent 0.7367(0.7241) | Loss 9.7128(10.2198) | Error 0.2625(0.2589) Steps 604(600.10) | Grad Norm 8.7531(8.6452) | Total Time 0.00(0.00)\n",
      "Iter 1397 | Time 59.6919(59.6089) | Bit/dim 3.7107(3.7096) | Xent 0.7225(0.7240) | Loss 9.5627(10.2001) | Error 0.2596(0.2589) Steps 598(600.04) | Grad Norm 5.9241(8.5636) | Total Time 0.00(0.00)\n",
      "Iter 1398 | Time 56.6717(59.5208) | Bit/dim 3.7109(3.7096) | Xent 0.7272(0.7241) | Loss 9.7474(10.1865) | Error 0.2571(0.2589) Steps 610(600.34) | Grad Norm 10.1950(8.6125) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 23.3829, Epoch Time 399.0841(391.0918), Bit/dim 3.7167(best: 3.7100), Xent 1.2103, Loss 4.3219, Error 0.3969(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1399 | Time 63.1185(59.6287) | Bit/dim 3.7112(3.7097) | Xent 0.7278(0.7242) | Loss 13.3912(10.2826) | Error 0.2642(0.2590) Steps 640(601.53) | Grad Norm 13.1894(8.7498) | Total Time 0.00(0.00)\n",
      "Iter 1400 | Time 60.2132(59.6462) | Bit/dim 3.7176(3.7099) | Xent 0.7517(0.7250) | Loss 9.8344(10.2692) | Error 0.2685(0.2593) Steps 598(601.42) | Grad Norm 15.9019(8.9644) | Total Time 0.00(0.00)\n",
      "Iter 1401 | Time 59.7575(59.6496) | Bit/dim 3.6988(3.7096) | Xent 0.7292(0.7252) | Loss 9.7477(10.2535) | Error 0.2588(0.2593) Steps 616(601.86) | Grad Norm 10.5868(9.0130) | Total Time 0.00(0.00)\n",
      "Iter 1402 | Time 57.7933(59.5939) | Bit/dim 3.7078(3.7095) | Xent 0.7105(0.7247) | Loss 9.5116(10.2313) | Error 0.2555(0.2592) Steps 574(601.03) | Grad Norm 12.7241(9.1244) | Total Time 0.00(0.00)\n",
      "Iter 1403 | Time 59.6709(59.5962) | Bit/dim 3.7123(3.7096) | Xent 0.7291(0.7249) | Loss 9.7086(10.2156) | Error 0.2668(0.2594) Steps 598(600.94) | Grad Norm 14.1385(9.2748) | Total Time 0.00(0.00)\n",
      "Iter 1404 | Time 62.4749(59.6826) | Bit/dim 3.7077(3.7095) | Xent 0.7384(0.7253) | Loss 9.6900(10.1998) | Error 0.2682(0.2597) Steps 628(601.75) | Grad Norm 10.5168(9.3121) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 22.9555, Epoch Time 402.5376(391.4352), Bit/dim 3.7185(best: 3.7100), Xent 1.2149, Loss 4.3260, Error 0.3927(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1405 | Time 60.1003(59.6951) | Bit/dim 3.7216(3.7099) | Xent 0.7611(0.7263) | Loss 13.2739(10.2920) | Error 0.2722(0.2601) Steps 610(601.99) | Grad Norm 12.1390(9.3969) | Total Time 0.00(0.00)\n",
      "Iter 1406 | Time 60.0553(59.7059) | Bit/dim 3.7129(3.7100) | Xent 0.7151(0.7260) | Loss 9.5791(10.2707) | Error 0.2560(0.2599) Steps 562(600.79) | Grad Norm 6.9469(9.3234) | Total Time 0.00(0.00)\n",
      "Iter 1407 | Time 59.5228(59.7004) | Bit/dim 3.7048(3.7098) | Xent 0.7228(0.7259) | Loss 9.2826(10.2410) | Error 0.2565(0.2598) Steps 628(601.61) | Grad Norm 7.2160(9.2601) | Total Time 0.00(0.00)\n",
      "Iter 1408 | Time 61.8094(59.7637) | Bit/dim 3.6978(3.7095) | Xent 0.7546(0.7268) | Loss 9.7131(10.2252) | Error 0.2668(0.2600) Steps 616(602.04) | Grad Norm 9.1101(9.2556) | Total Time 0.00(0.00)\n",
      "Iter 1409 | Time 61.3392(59.8109) | Bit/dim 3.7115(3.7095) | Xent 0.7206(0.7266) | Loss 9.4136(10.2008) | Error 0.2575(0.2600) Steps 598(601.92) | Grad Norm 7.4384(9.2011) | Total Time 0.00(0.00)\n",
      "Iter 1410 | Time 64.1911(59.9423) | Bit/dim 3.7148(3.7097) | Xent 0.7080(0.7260) | Loss 9.4934(10.1796) | Error 0.2525(0.2597) Steps 604(601.98) | Grad Norm 9.4213(9.2077) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 23.6466, Epoch Time 406.8658(391.8981), Bit/dim 3.7234(best: 3.7100), Xent 1.1612, Loss 4.3040, Error 0.3771(best: 0.3812)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1411 | Time 63.6260(60.0529) | Bit/dim 3.7090(3.7097) | Xent 0.6990(0.7252) | Loss 13.2989(10.2732) | Error 0.2514(0.2595) Steps 634(602.94) | Grad Norm 7.8929(9.1683) | Total Time 0.00(0.00)\n",
      "Iter 1412 | Time 60.9207(60.0789) | Bit/dim 3.7090(3.7097) | Xent 0.6661(0.7234) | Loss 9.6356(10.2541) | Error 0.2359(0.2588) Steps 604(602.98) | Grad Norm 3.3942(8.9951) | Total Time 0.00(0.00)\n",
      "Iter 1413 | Time 61.6298(60.1254) | Bit/dim 3.7163(3.7099) | Xent 0.6837(0.7223) | Loss 9.7560(10.2391) | Error 0.2438(0.2583) Steps 640(604.09) | Grad Norm 5.2645(8.8831) | Total Time 0.00(0.00)\n",
      "Iter 1414 | Time 57.8655(60.0576) | Bit/dim 3.7186(3.7101) | Xent 0.6737(0.7208) | Loss 9.5499(10.2184) | Error 0.2368(0.2577) Steps 604(604.08) | Grad Norm 4.9945(8.7665) | Total Time 0.00(0.00)\n",
      "Iter 1415 | Time 60.2436(60.0632) | Bit/dim 3.7142(3.7102) | Xent 0.6863(0.7198) | Loss 9.5252(10.1976) | Error 0.2439(0.2573) Steps 604(604.08) | Grad Norm 6.8076(8.7077) | Total Time 0.00(0.00)\n",
      "Iter 1416 | Time 62.1209(60.1249) | Bit/dim 3.6988(3.7099) | Xent 0.6937(0.7190) | Loss 9.7582(10.1845) | Error 0.2515(0.2571) Steps 592(603.72) | Grad Norm 9.9266(8.7443) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 23.1247, Epoch Time 405.6626(392.3110), Bit/dim 3.7184(best: 3.7100), Xent 1.2304, Loss 4.3335, Error 0.3875(best: 0.3771)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1417 | Time 58.0045(60.0613) | Bit/dim 3.7180(3.7101) | Xent 0.6895(0.7181) | Loss 12.8448(10.2643) | Error 0.2486(0.2568) Steps 580(603.01) | Grad Norm 11.8946(8.8388) | Total Time 0.00(0.00)\n",
      "Iter 1418 | Time 63.1600(60.1543) | Bit/dim 3.6995(3.7098) | Xent 0.7250(0.7183) | Loss 9.5616(10.2432) | Error 0.2600(0.2569) Steps 634(603.94) | Grad Norm 13.2520(8.9712) | Total Time 0.00(0.00)\n",
      "Iter 1419 | Time 60.1984(60.1556) | Bit/dim 3.7015(3.7096) | Xent 0.7032(0.7178) | Loss 9.2787(10.2143) | Error 0.2578(0.2570) Steps 610(604.12) | Grad Norm 12.9549(9.0907) | Total Time 0.00(0.00)\n",
      "Iter 1420 | Time 63.1158(60.2444) | Bit/dim 3.7083(3.7095) | Xent 0.6980(0.7173) | Loss 9.4892(10.1925) | Error 0.2530(0.2568) Steps 586(603.58) | Grad Norm 8.9120(9.0853) | Total Time 0.00(0.00)\n",
      "Iter 1421 | Time 61.7025(60.2881) | Bit/dim 3.7124(3.7096) | Xent 0.6918(0.7165) | Loss 9.5533(10.1733) | Error 0.2414(0.2564) Steps 622(604.13) | Grad Norm 7.3497(9.0333) | Total Time 0.00(0.00)\n",
      "Iter 1422 | Time 62.0970(60.3424) | Bit/dim 3.7054(3.7095) | Xent 0.6702(0.7151) | Loss 9.5057(10.1533) | Error 0.2415(0.2559) Steps 616(604.48) | Grad Norm 3.4995(8.8673) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 23.1161, Epoch Time 407.4242(392.7644), Bit/dim 3.7125(best: 3.7100), Xent 1.1673, Loss 4.2962, Error 0.3809(best: 0.3771)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1423 | Time 62.1178(60.3957) | Bit/dim 3.7151(3.7097) | Xent 0.6858(0.7142) | Loss 13.2525(10.2463) | Error 0.2424(0.2555) Steps 616(604.83) | Grad Norm 5.2955(8.7601) | Total Time 0.00(0.00)\n",
      "Iter 1424 | Time 62.5870(60.4614) | Bit/dim 3.7119(3.7097) | Xent 0.6688(0.7129) | Loss 9.6454(10.2282) | Error 0.2361(0.2549) Steps 628(605.52) | Grad Norm 6.9283(8.7052) | Total Time 0.00(0.00)\n",
      "Iter 1425 | Time 64.2657(60.5755) | Bit/dim 3.7112(3.7098) | Xent 0.6586(0.7112) | Loss 9.6367(10.2105) | Error 0.2381(0.2544) Steps 592(605.12) | Grad Norm 6.0152(8.6245) | Total Time 0.00(0.00)\n",
      "Iter 1426 | Time 61.8617(60.6141) | Bit/dim 3.6986(3.7094) | Xent 0.6946(0.7107) | Loss 9.3579(10.1849) | Error 0.2442(0.2541) Steps 598(604.91) | Grad Norm 8.3935(8.6175) | Total Time 0.00(0.00)\n",
      "Iter 1427 | Time 62.6247(60.6744) | Bit/dim 3.7046(3.7093) | Xent 0.6663(0.7094) | Loss 9.5944(10.1672) | Error 0.2390(0.2537) Steps 604(604.88) | Grad Norm 7.2892(8.5777) | Total Time 0.00(0.00)\n",
      "Iter 1428 | Time 66.0419(60.8355) | Bit/dim 3.7020(3.7091) | Xent 0.6679(0.7082) | Loss 9.4642(10.1461) | Error 0.2406(0.2533) Steps 622(605.39) | Grad Norm 4.1011(8.4434) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 23.3891, Epoch Time 419.0732(393.5537), Bit/dim 3.7117(best: 3.7100), Xent 1.1855, Loss 4.3045, Error 0.3797(best: 0.3771)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1429 | Time 63.0530(60.9020) | Bit/dim 3.6930(3.7086) | Xent 0.6668(0.7069) | Loss 13.2229(10.2384) | Error 0.2344(0.2527) Steps 634(606.25) | Grad Norm 6.9241(8.3978) | Total Time 0.00(0.00)\n",
      "Iter 1430 | Time 59.3571(60.8556) | Bit/dim 3.7088(3.7086) | Xent 0.6708(0.7058) | Loss 9.4811(10.2157) | Error 0.2416(0.2524) Steps 646(607.44) | Grad Norm 8.7915(8.4096) | Total Time 0.00(0.00)\n",
      "Iter 1431 | Time 58.6437(60.7893) | Bit/dim 3.7132(3.7087) | Xent 0.6849(0.7052) | Loss 9.6636(10.1991) | Error 0.2395(0.2520) Steps 616(607.70) | Grad Norm 9.2224(8.4340) | Total Time 0.00(0.00)\n",
      "Iter 1432 | Time 60.4595(60.7794) | Bit/dim 3.7064(3.7087) | Xent 0.6842(0.7046) | Loss 9.5060(10.1783) | Error 0.2499(0.2519) Steps 610(607.77) | Grad Norm 11.2571(8.5187) | Total Time 0.00(0.00)\n",
      "Iter 1433 | Time 61.4479(60.7994) | Bit/dim 3.7067(3.7086) | Xent 0.7252(0.7052) | Loss 9.5321(10.1590) | Error 0.2589(0.2521) Steps 616(608.02) | Grad Norm 16.6844(8.7637) | Total Time 0.00(0.00)\n",
      "Iter 1434 | Time 57.6138(60.7039) | Bit/dim 3.7134(3.7087) | Xent 0.7284(0.7059) | Loss 9.7265(10.1460) | Error 0.2654(0.2525) Steps 574(607.00) | Grad Norm 17.2273(9.0176) | Total Time 0.00(0.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 23.6827, Epoch Time 400.8085(393.7713), Bit/dim 3.7112(best: 3.7100), Xent 1.2002, Loss 4.3112, Error 0.3910(best: 0.3771)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_rl_multiscale.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_multiscale_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_rl_stdlearnscale_15_multiscale_run2/epoch_117_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --scale_fac 1.0 --gate cnn2 --scale_std 15.0 --max_grad_norm 20.0\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
