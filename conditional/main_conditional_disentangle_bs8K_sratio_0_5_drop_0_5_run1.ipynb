{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='mnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_5_drop_0_5_run1/epoch_200_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_5_drop_0_5_run1', seed=0, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=113.0, weight_decay=0.0, weight_y=0.5)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(2, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(9, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(5, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=784, bias=True)\n",
      "  (project_class): LinearZeros(in_features=392, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 814778\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 1401 | Time 73.0543(36.4374) | Bit/dim 1.1667(1.1969) | Xent 0.0561(0.0689) | Loss 1.1947(1.2314) | Error 0.0170(0.0213) Steps 452(463.64) | Grad Norm 3.1655(2.1494) | Total Time 10.00(10.00)\n",
      "Iter 1402 | Time 35.7917(36.4180) | Bit/dim 1.1572(1.1958) | Xent 0.0686(0.0688) | Loss 1.1915(1.2302) | Error 0.0202(0.0213) Steps 452(463.29) | Grad Norm 2.4026(2.1570) | Total Time 10.00(10.00)\n",
      "Iter 1403 | Time 33.9668(36.3445) | Bit/dim 1.1571(1.1946) | Xent 0.0654(0.0687) | Loss 1.1898(1.2290) | Error 0.0196(0.0212) Steps 452(462.95) | Grad Norm 0.7532(2.1149) | Total Time 10.00(10.00)\n",
      "Iter 1404 | Time 34.3747(36.2854) | Bit/dim 1.1541(1.1934) | Xent 0.0593(0.0685) | Loss 1.1837(1.2276) | Error 0.0194(0.0212) Steps 446(462.44) | Grad Norm 1.6144(2.0999) | Total Time 10.00(10.00)\n",
      "Iter 1405 | Time 34.7481(36.2393) | Bit/dim 1.1536(1.1922) | Xent 0.0561(0.0681) | Loss 1.1817(1.2262) | Error 0.0171(0.0210) Steps 458(462.31) | Grad Norm 2.7165(2.1184) | Total Time 10.00(10.00)\n",
      "Iter 1406 | Time 34.2979(36.1811) | Bit/dim 1.1542(1.1910) | Xent 0.0652(0.0680) | Loss 1.1868(1.2250) | Error 0.0191(0.0210) Steps 464(462.36) | Grad Norm 2.3950(2.1267) | Total Time 10.00(10.00)\n",
      "Iter 1407 | Time 36.0530(36.1772) | Bit/dim 1.1514(1.1899) | Xent 0.0570(0.0677) | Loss 1.1799(1.2237) | Error 0.0181(0.0209) Steps 464(462.41) | Grad Norm 0.8618(2.0887) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 26.1148, Epoch Time 320.8190(264.5156), Bit/dim 1.1457(best: inf), Xent 0.0274, Loss 1.1594, Error 0.0100(best: inf)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1408 | Time 36.4105(36.1842) | Bit/dim 1.1521(1.1887) | Xent 0.0602(0.0675) | Loss 1.1822(1.2224) | Error 0.0199(0.0209) Steps 470(462.64) | Grad Norm 1.0573(2.0578) | Total Time 10.00(10.00)\n",
      "Iter 1409 | Time 34.7933(36.1425) | Bit/dim 1.1553(1.1877) | Xent 0.0536(0.0670) | Loss 1.1820(1.2212) | Error 0.0182(0.0208) Steps 470(462.86) | Grad Norm 1.8824(2.0525) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 35.6045(36.1263) | Bit/dim 1.1537(1.1867) | Xent 0.0594(0.0668) | Loss 1.1834(1.2201) | Error 0.0184(0.0207) Steps 470(463.07) | Grad Norm 2.0361(2.0520) | Total Time 10.00(10.00)\n",
      "Iter 1411 | Time 35.1827(36.0980) | Bit/dim 1.1494(1.1856) | Xent 0.0558(0.0665) | Loss 1.1773(1.2188) | Error 0.0181(0.0206) Steps 470(463.28) | Grad Norm 1.2732(2.0287) | Total Time 10.00(10.00)\n",
      "Iter 1412 | Time 36.1039(36.0982) | Bit/dim 1.1458(1.1844) | Xent 0.0658(0.0665) | Loss 1.1787(1.2176) | Error 0.0199(0.0206) Steps 452(462.94) | Grad Norm 0.3133(1.9772) | Total Time 10.00(10.00)\n",
      "Iter 1413 | Time 33.8731(36.0315) | Bit/dim 1.1565(1.1835) | Xent 0.0582(0.0662) | Loss 1.1856(1.2167) | Error 0.0191(0.0206) Steps 464(462.97) | Grad Norm 1.3799(1.9593) | Total Time 10.00(10.00)\n",
      "Iter 1414 | Time 34.2235(35.9772) | Bit/dim 1.1528(1.1826) | Xent 0.0645(0.0662) | Loss 1.1851(1.2157) | Error 0.0185(0.0205) Steps 464(463.00) | Grad Norm 1.9742(1.9597) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 16.7491, Epoch Time 275.3991(264.8421), Bit/dim 1.1440(best: 1.1457), Xent 0.0307, Loss 1.1593, Error 0.0108(best: 0.0100)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1415 | Time 35.2627(35.9558) | Bit/dim 1.1531(1.1817) | Xent 0.0562(0.0659) | Loss 1.1811(1.2147) | Error 0.0185(0.0204) Steps 464(463.03) | Grad Norm 1.2930(1.9397) | Total Time 10.00(10.00)\n",
      "Iter 1416 | Time 35.4450(35.9405) | Bit/dim 1.1480(1.1807) | Xent 0.0578(0.0656) | Loss 1.1769(1.2135) | Error 0.0171(0.0203) Steps 470(463.24) | Grad Norm 0.2979(1.8905) | Total Time 10.00(10.00)\n",
      "Iter 1417 | Time 35.2676(35.9203) | Bit/dim 1.1505(1.1798) | Xent 0.0519(0.0652) | Loss 1.1764(1.2124) | Error 0.0161(0.0202) Steps 452(462.91) | Grad Norm 0.9344(1.8618) | Total Time 10.00(10.00)\n",
      "Iter 1418 | Time 35.0787(35.8950) | Bit/dim 1.1520(1.1790) | Xent 0.0606(0.0651) | Loss 1.1823(1.2115) | Error 0.0191(0.0202) Steps 470(463.12) | Grad Norm 1.3951(1.8478) | Total Time 10.00(10.00)\n",
      "Iter 1419 | Time 35.0083(35.8684) | Bit/dim 1.1500(1.1781) | Xent 0.0596(0.0649) | Loss 1.1798(1.2106) | Error 0.0178(0.0201) Steps 452(462.79) | Grad Norm 1.1857(1.8279) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 33.7844(35.8059) | Bit/dim 1.1461(1.1772) | Xent 0.0616(0.0648) | Loss 1.1769(1.2096) | Error 0.0201(0.0201) Steps 452(462.46) | Grad Norm 0.4991(1.7881) | Total Time 10.00(10.00)\n",
      "Iter 1421 | Time 34.8619(35.7776) | Bit/dim 1.1489(1.1763) | Xent 0.0639(0.0648) | Loss 1.1809(1.2087) | Error 0.0208(0.0201) Steps 464(462.51) | Grad Norm 0.4957(1.7493) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 16.3814, Epoch Time 273.5127(265.1022), Bit/dim 1.1432(best: 1.1440), Xent 0.0301, Loss 1.1583, Error 0.0103(best: 0.0100)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1422 | Time 35.3705(35.7654) | Bit/dim 1.1495(1.1755) | Xent 0.0605(0.0646) | Loss 1.1797(1.2078) | Error 0.0181(0.0201) Steps 446(462.01) | Grad Norm 1.1802(1.7322) | Total Time 10.00(10.00)\n",
      "Iter 1423 | Time 35.6977(35.7633) | Bit/dim 1.1512(1.1748) | Xent 0.0641(0.0646) | Loss 1.1832(1.2071) | Error 0.0195(0.0201) Steps 464(462.07) | Grad Norm 1.1262(1.7140) | Total Time 10.00(10.00)\n",
      "Iter 1424 | Time 35.8912(35.7672) | Bit/dim 1.1516(1.1741) | Xent 0.0637(0.0646) | Loss 1.1834(1.2064) | Error 0.0208(0.0201) Steps 470(462.31) | Grad Norm 0.6449(1.6820) | Total Time 10.00(10.00)\n",
      "Iter 1425 | Time 35.2329(35.7511) | Bit/dim 1.1495(1.1733) | Xent 0.0539(0.0643) | Loss 1.1764(1.2055) | Error 0.0172(0.0200) Steps 470(462.54) | Grad Norm 0.4422(1.6448) | Total Time 10.00(10.00)\n",
      "Iter 1426 | Time 35.6188(35.7472) | Bit/dim 1.1461(1.1725) | Xent 0.0562(0.0640) | Loss 1.1743(1.2045) | Error 0.0179(0.0199) Steps 470(462.76) | Grad Norm 1.0592(1.6272) | Total Time 10.00(10.00)\n",
      "Iter 1427 | Time 35.4015(35.7368) | Bit/dim 1.1462(1.1717) | Xent 0.0550(0.0638) | Loss 1.1737(1.2036) | Error 0.0166(0.0198) Steps 470(462.98) | Grad Norm 0.9770(1.6077) | Total Time 10.00(10.00)\n",
      "Iter 1428 | Time 34.4281(35.6975) | Bit/dim 1.1460(1.1710) | Xent 0.0595(0.0636) | Loss 1.1757(1.2028) | Error 0.0188(0.0198) Steps 470(463.19) | Grad Norm 0.6561(1.5791) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 16.6724, Epoch Time 276.8595(265.4549), Bit/dim 1.1403(best: 1.1432), Xent 0.0289, Loss 1.1548, Error 0.0106(best: 0.0100)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1429 | Time 34.7812(35.6701) | Bit/dim 1.1452(1.1702) | Xent 0.0658(0.0637) | Loss 1.1781(1.2020) | Error 0.0199(0.0198) Steps 452(462.86) | Grad Norm 0.3656(1.5427) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 35.2527(35.6575) | Bit/dim 1.1451(1.1694) | Xent 0.0540(0.0634) | Loss 1.1721(1.2011) | Error 0.0172(0.0197) Steps 464(462.89) | Grad Norm 0.6997(1.5174) | Total Time 10.00(10.00)\n",
      "Iter 1431 | Time 34.0301(35.6087) | Bit/dim 1.1440(1.1687) | Xent 0.0549(0.0632) | Loss 1.1715(1.2003) | Error 0.0174(0.0197) Steps 464(462.92) | Grad Norm 0.7732(1.4951) | Total Time 10.00(10.00)\n",
      "Iter 1432 | Time 34.9959(35.5903) | Bit/dim 1.1431(1.1679) | Xent 0.0642(0.0632) | Loss 1.1752(1.1995) | Error 0.0185(0.0196) Steps 446(462.42) | Grad Norm 0.6234(1.4690) | Total Time 10.00(10.00)\n",
      "Iter 1433 | Time 35.5979(35.5906) | Bit/dim 1.1518(1.1674) | Xent 0.0647(0.0632) | Loss 1.1841(1.1990) | Error 0.0175(0.0196) Steps 464(462.46) | Grad Norm 0.2110(1.4312) | Total Time 10.00(10.00)\n",
      "Iter 1434 | Time 35.7950(35.5967) | Bit/dim 1.1474(1.1668) | Xent 0.0604(0.0631) | Loss 1.1776(1.1984) | Error 0.0175(0.0195) Steps 452(462.15) | Grad Norm 0.5559(1.4050) | Total Time 10.00(10.00)\n",
      "Iter 1435 | Time 35.0520(35.5803) | Bit/dim 1.1476(1.1662) | Xent 0.0607(0.0631) | Loss 1.1779(1.1978) | Error 0.0182(0.0195) Steps 452(461.85) | Grad Norm 0.7484(1.3853) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 16.3957, Epoch Time 274.2632(265.7192), Bit/dim 1.1402(best: 1.1403), Xent 0.0279, Loss 1.1542, Error 0.0087(best: 0.0100)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1436 | Time 35.2149(35.5694) | Bit/dim 1.1469(1.1657) | Xent 0.0490(0.0626) | Loss 1.1715(1.1970) | Error 0.0144(0.0193) Steps 470(462.09) | Grad Norm 0.4664(1.3577) | Total Time 10.00(10.00)\n",
      "Iter 1437 | Time 34.5263(35.5381) | Bit/dim 1.1424(1.1650) | Xent 0.0605(0.0626) | Loss 1.1727(1.1963) | Error 0.0184(0.0193) Steps 446(461.61) | Grad Norm 0.1954(1.3228) | Total Time 10.00(10.00)\n",
      "Iter 1438 | Time 36.3275(35.5618) | Bit/dim 1.1484(1.1645) | Xent 0.0641(0.0626) | Loss 1.1805(1.1958) | Error 0.0211(0.0193) Steps 464(461.68) | Grad Norm 0.5028(1.2982) | Total Time 10.00(10.00)\n",
      "Iter 1439 | Time 34.5572(35.5316) | Bit/dim 1.1471(1.1640) | Xent 0.0554(0.0624) | Loss 1.1748(1.1952) | Error 0.0180(0.0193) Steps 464(461.75) | Grad Norm 0.5601(1.2761) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 35.3024(35.5248) | Bit/dim 1.1450(1.1634) | Xent 0.0576(0.0623) | Loss 1.1738(1.1945) | Error 0.0176(0.0192) Steps 464(461.82) | Grad Norm 0.4131(1.2502) | Total Time 10.00(10.00)\n",
      "Iter 1441 | Time 35.9623(35.5379) | Bit/dim 1.1506(1.1630) | Xent 0.0604(0.0622) | Loss 1.1808(1.1941) | Error 0.0196(0.0193) Steps 470(462.06) | Grad Norm 0.1887(1.2184) | Total Time 10.00(10.00)\n",
      "Iter 1442 | Time 35.4926(35.5365) | Bit/dim 1.1410(1.1623) | Xent 0.0643(0.0623) | Loss 1.1731(1.1935) | Error 0.0179(0.0192) Steps 470(462.30) | Grad Norm 0.4801(1.1962) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 16.0531, Epoch Time 275.6166(266.0161), Bit/dim 1.1397(best: 1.1402), Xent 0.0298, Loss 1.1546, Error 0.0108(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1443 | Time 35.8649(35.5464) | Bit/dim 1.1452(1.1618) | Xent 0.0633(0.0623) | Loss 1.1769(1.1930) | Error 0.0180(0.0192) Steps 470(462.53) | Grad Norm 0.4494(1.1738) | Total Time 10.00(10.00)\n",
      "Iter 1444 | Time 35.1445(35.5343) | Bit/dim 1.1419(1.1612) | Xent 0.0613(0.0623) | Loss 1.1726(1.1924) | Error 0.0181(0.0191) Steps 464(462.57) | Grad Norm 0.2945(1.1474) | Total Time 10.00(10.00)\n",
      "Iter 1445 | Time 35.4409(35.5315) | Bit/dim 1.1511(1.1609) | Xent 0.0561(0.0621) | Loss 1.1791(1.1920) | Error 0.0172(0.0191) Steps 470(462.80) | Grad Norm 0.1823(1.1185) | Total Time 10.00(10.00)\n",
      "Iter 1446 | Time 34.7755(35.5088) | Bit/dim 1.1399(1.1603) | Xent 0.0669(0.0622) | Loss 1.1733(1.1914) | Error 0.0210(0.0191) Steps 470(463.01) | Grad Norm 0.4806(1.0993) | Total Time 10.00(10.00)\n",
      "Iter 1447 | Time 35.6933(35.5144) | Bit/dim 1.1472(1.1599) | Xent 0.0554(0.0620) | Loss 1.1749(1.1909) | Error 0.0182(0.0191) Steps 470(463.22) | Grad Norm 0.3979(1.0783) | Total Time 10.00(10.00)\n",
      "Iter 1448 | Time 35.6641(35.5189) | Bit/dim 1.1445(1.1594) | Xent 0.0604(0.0620) | Loss 1.1747(1.1904) | Error 0.0186(0.0191) Steps 464(463.25) | Grad Norm 0.2344(1.0530) | Total Time 10.00(10.00)\n",
      "Iter 1449 | Time 35.3874(35.5149) | Bit/dim 1.1437(1.1590) | Xent 0.0537(0.0617) | Loss 1.1706(1.1898) | Error 0.0179(0.0191) Steps 464(463.27) | Grad Norm 0.1937(1.0272) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 16.6205, Epoch Time 276.8239(266.3403), Bit/dim 1.1393(best: 1.1397), Xent 0.0278, Loss 1.1532, Error 0.0094(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1450 | Time 35.5174(35.5150) | Bit/dim 1.1471(1.1586) | Xent 0.0518(0.0614) | Loss 1.1730(1.1893) | Error 0.0165(0.0190) Steps 470(463.47) | Grad Norm 0.2798(1.0048) | Total Time 10.00(10.00)\n",
      "Iter 1451 | Time 35.0002(35.4995) | Bit/dim 1.1405(1.1581) | Xent 0.0544(0.0612) | Loss 1.1677(1.1887) | Error 0.0150(0.0189) Steps 470(463.67) | Grad Norm 0.3126(0.9840) | Total Time 10.00(10.00)\n",
      "Iter 1452 | Time 35.2637(35.4925) | Bit/dim 1.1463(1.1577) | Xent 0.0578(0.0611) | Loss 1.1752(1.1883) | Error 0.0176(0.0188) Steps 452(463.32) | Grad Norm 0.2171(0.9610) | Total Time 10.00(10.00)\n",
      "Iter 1453 | Time 36.0355(35.5088) | Bit/dim 1.1461(1.1574) | Xent 0.0600(0.0611) | Loss 1.1761(1.1879) | Error 0.0200(0.0189) Steps 470(463.52) | Grad Norm 0.2341(0.9392) | Total Time 10.00(10.00)\n",
      "Iter 1454 | Time 35.4510(35.5070) | Bit/dim 1.1468(1.1570) | Xent 0.0645(0.0612) | Loss 1.1790(1.1876) | Error 0.0195(0.0189) Steps 464(463.53) | Grad Norm 0.2637(0.9189) | Total Time 10.00(10.00)\n",
      "Iter 1455 | Time 34.9068(35.4890) | Bit/dim 1.1441(1.1567) | Xent 0.0607(0.0612) | Loss 1.1744(1.1872) | Error 0.0166(0.0188) Steps 470(463.73) | Grad Norm 0.2139(0.8978) | Total Time 10.00(10.00)\n",
      "Iter 1456 | Time 36.2365(35.5114) | Bit/dim 1.1395(1.1561) | Xent 0.0557(0.0610) | Loss 1.1673(1.1866) | Error 0.0186(0.0188) Steps 464(463.73) | Grad Norm 0.1752(0.8761) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 16.3071, Epoch Time 277.0205(266.6607), Bit/dim 1.1376(best: 1.1393), Xent 0.0303, Loss 1.1527, Error 0.0099(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1457 | Time 35.4993(35.5111) | Bit/dim 1.1454(1.1558) | Xent 0.0690(0.0612) | Loss 1.1799(1.1864) | Error 0.0198(0.0188) Steps 464(463.74) | Grad Norm 0.1677(0.8549) | Total Time 10.00(10.00)\n",
      "Iter 1458 | Time 35.4202(35.5084) | Bit/dim 1.1462(1.1555) | Xent 0.0563(0.0611) | Loss 1.1744(1.1861) | Error 0.0185(0.0188) Steps 464(463.75) | Grad Norm 0.2858(0.8378) | Total Time 10.00(10.00)\n",
      "Iter 1459 | Time 35.1249(35.4969) | Bit/dim 1.1423(1.1551) | Xent 0.0593(0.0610) | Loss 1.1719(1.1857) | Error 0.0181(0.0188) Steps 470(463.94) | Grad Norm 0.2035(0.8188) | Total Time 10.00(10.00)\n",
      "Iter 1460 | Time 36.2883(35.5206) | Bit/dim 1.1428(1.1548) | Xent 0.0578(0.0610) | Loss 1.1717(1.1852) | Error 0.0170(0.0188) Steps 464(463.94) | Grad Norm 0.1423(0.7985) | Total Time 10.00(10.00)\n",
      "Iter 1461 | Time 35.9685(35.5340) | Bit/dim 1.1425(1.1544) | Xent 0.0560(0.0608) | Loss 1.1706(1.1848) | Error 0.0166(0.0187) Steps 464(463.94) | Grad Norm 0.1730(0.7797) | Total Time 10.00(10.00)\n",
      "Iter 1462 | Time 35.1212(35.5216) | Bit/dim 1.1454(1.1541) | Xent 0.0602(0.0608) | Loss 1.1755(1.1845) | Error 0.0182(0.0187) Steps 470(464.12) | Grad Norm 0.3244(0.7660) | Total Time 10.00(10.00)\n",
      "Iter 1463 | Time 35.5318(35.5220) | Bit/dim 1.1450(1.1539) | Xent 0.0651(0.0609) | Loss 1.1775(1.1843) | Error 0.0212(0.0188) Steps 464(464.12) | Grad Norm 0.2299(0.7500) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 16.5393, Epoch Time 277.7316(266.9929), Bit/dim 1.1377(best: 1.1376), Xent 0.0313, Loss 1.1533, Error 0.0106(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1464 | Time 35.4281(35.5191) | Bit/dim 1.1375(1.1534) | Xent 0.0586(0.0608) | Loss 1.1668(1.1838) | Error 0.0181(0.0187) Steps 470(464.30) | Grad Norm 0.2122(0.7338) | Total Time 10.00(10.00)\n",
      "Iter 1465 | Time 35.2535(35.5112) | Bit/dim 1.1490(1.1532) | Xent 0.0652(0.0610) | Loss 1.1816(1.1837) | Error 0.0184(0.0187) Steps 464(464.29) | Grad Norm 0.1981(0.7177) | Total Time 10.00(10.00)\n",
      "Iter 1466 | Time 34.9880(35.4955) | Bit/dim 1.1421(1.1529) | Xent 0.0602(0.0610) | Loss 1.1722(1.1834) | Error 0.0165(0.0187) Steps 470(464.46) | Grad Norm 0.3654(0.7072) | Total Time 10.00(10.00)\n",
      "Iter 1467 | Time 35.4103(35.4929) | Bit/dim 1.1428(1.1526) | Xent 0.0600(0.0609) | Loss 1.1728(1.1831) | Error 0.0199(0.0187) Steps 464(464.44) | Grad Norm 0.2702(0.6941) | Total Time 10.00(10.00)\n",
      "Iter 1468 | Time 35.0208(35.4788) | Bit/dim 1.1433(1.1523) | Xent 0.0591(0.0609) | Loss 1.1728(1.1828) | Error 0.0188(0.0187) Steps 464(464.43) | Grad Norm 0.2150(0.6797) | Total Time 10.00(10.00)\n",
      "Iter 1469 | Time 36.4203(35.5070) | Bit/dim 1.1470(1.1522) | Xent 0.0551(0.0607) | Loss 1.1745(1.1825) | Error 0.0176(0.0187) Steps 470(464.60) | Grad Norm 0.3083(0.6686) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 35.2749(35.5000) | Bit/dim 1.1416(1.1518) | Xent 0.0517(0.0604) | Loss 1.1674(1.1821) | Error 0.0155(0.0186) Steps 464(464.58) | Grad Norm 0.1755(0.6538) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 16.7126, Epoch Time 276.9731(267.2923), Bit/dim 1.1368(best: 1.1376), Xent 0.0297, Loss 1.1516, Error 0.0101(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1471 | Time 34.8653(35.4810) | Bit/dim 1.1422(1.1515) | Xent 0.0691(0.0607) | Loss 1.1767(1.1819) | Error 0.0191(0.0186) Steps 452(464.20) | Grad Norm 0.1855(0.6397) | Total Time 10.00(10.00)\n",
      "Iter 1472 | Time 34.9159(35.4640) | Bit/dim 1.1403(1.1512) | Xent 0.0605(0.0607) | Loss 1.1706(1.1816) | Error 0.0186(0.0186) Steps 464(464.20) | Grad Norm 0.1899(0.6262) | Total Time 10.00(10.00)\n",
      "Iter 1473 | Time 35.5656(35.4671) | Bit/dim 1.1431(1.1510) | Xent 0.0593(0.0606) | Loss 1.1728(1.1813) | Error 0.0192(0.0186) Steps 464(464.19) | Grad Norm 0.2290(0.6143) | Total Time 10.00(10.00)\n",
      "Iter 1474 | Time 35.8275(35.4779) | Bit/dim 1.1469(1.1508) | Xent 0.0641(0.0607) | Loss 1.1789(1.1812) | Error 0.0186(0.0186) Steps 470(464.37) | Grad Norm 0.2265(0.6027) | Total Time 10.00(10.00)\n",
      "Iter 1475 | Time 35.5176(35.4791) | Bit/dim 1.1386(1.1505) | Xent 0.0556(0.0606) | Loss 1.1664(1.1808) | Error 0.0178(0.0186) Steps 470(464.53) | Grad Norm 0.2331(0.5916) | Total Time 10.00(10.00)\n",
      "Iter 1476 | Time 35.7167(35.4862) | Bit/dim 1.1457(1.1503) | Xent 0.0578(0.0605) | Loss 1.1746(1.1806) | Error 0.0160(0.0185) Steps 464(464.52) | Grad Norm 0.2061(0.5800) | Total Time 10.00(10.00)\n",
      "Iter 1477 | Time 36.0195(35.5022) | Bit/dim 1.1379(1.1500) | Xent 0.0590(0.0605) | Loss 1.1674(1.1802) | Error 0.0172(0.0185) Steps 464(464.50) | Grad Norm 0.2202(0.5692) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 16.5798, Epoch Time 277.3248(267.5932), Bit/dim 1.1359(best: 1.1368), Xent 0.0294, Loss 1.1506, Error 0.0093(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1478 | Time 35.0339(35.4882) | Bit/dim 1.1483(1.1499) | Xent 0.0580(0.0604) | Loss 1.1773(1.1801) | Error 0.0188(0.0185) Steps 464(464.49) | Grad Norm 0.1642(0.5571) | Total Time 10.00(10.00)\n",
      "Iter 1479 | Time 34.6181(35.4621) | Bit/dim 1.1383(1.1496) | Xent 0.0569(0.0603) | Loss 1.1667(1.1797) | Error 0.0162(0.0184) Steps 464(464.47) | Grad Norm 0.2245(0.5471) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 36.6042(35.4963) | Bit/dim 1.1459(1.1495) | Xent 0.0567(0.0602) | Loss 1.1742(1.1795) | Error 0.0179(0.0184) Steps 470(464.64) | Grad Norm 0.2137(0.5371) | Total Time 10.00(10.00)\n",
      "Iter 1481 | Time 36.0336(35.5124) | Bit/dim 1.1467(1.1494) | Xent 0.0612(0.0602) | Loss 1.1773(1.1795) | Error 0.0195(0.0184) Steps 464(464.62) | Grad Norm 0.2290(0.5278) | Total Time 10.00(10.00)\n",
      "Iter 1482 | Time 34.1022(35.4701) | Bit/dim 1.1363(1.1490) | Xent 0.0603(0.0602) | Loss 1.1665(1.1791) | Error 0.0184(0.0184) Steps 464(464.60) | Grad Norm 0.1987(0.5180) | Total Time 10.00(10.00)\n",
      "Iter 1483 | Time 35.9846(35.4856) | Bit/dim 1.1409(1.1487) | Xent 0.0482(0.0598) | Loss 1.1650(1.1787) | Error 0.0142(0.0183) Steps 470(464.76) | Grad Norm 0.1885(0.5081) | Total Time 10.00(10.00)\n",
      "Iter 1484 | Time 35.6223(35.4897) | Bit/dim 1.1370(1.1484) | Xent 0.0640(0.0600) | Loss 1.1690(1.1784) | Error 0.0205(0.0184) Steps 464(464.74) | Grad Norm 0.1983(0.4988) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 17.1426, Epoch Time 277.3704(267.8866), Bit/dim 1.1346(best: 1.1359), Xent 0.0293, Loss 1.1493, Error 0.0104(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1485 | Time 35.9684(35.5040) | Bit/dim 1.1456(1.1483) | Xent 0.0649(0.0601) | Loss 1.1780(1.1784) | Error 0.0195(0.0184) Steps 464(464.72) | Grad Norm 0.2141(0.4903) | Total Time 10.00(10.00)\n",
      "Iter 1486 | Time 36.3070(35.5281) | Bit/dim 1.1362(1.1479) | Xent 0.0624(0.0602) | Loss 1.1674(1.1780) | Error 0.0211(0.0185) Steps 470(464.88) | Grad Norm 0.1675(0.4806) | Total Time 10.00(10.00)\n",
      "Iter 1487 | Time 35.5226(35.5280) | Bit/dim 1.1424(1.1478) | Xent 0.0528(0.0600) | Loss 1.1687(1.1778) | Error 0.0166(0.0184) Steps 464(464.85) | Grad Norm 0.2222(0.4728) | Total Time 10.00(10.00)\n",
      "Iter 1488 | Time 34.8842(35.5086) | Bit/dim 1.1406(1.1476) | Xent 0.0643(0.0601) | Loss 1.1728(1.1776) | Error 0.0195(0.0185) Steps 470(465.00) | Grad Norm 0.2283(0.4655) | Total Time 10.00(10.00)\n",
      "Iter 1489 | Time 37.2424(35.5607) | Bit/dim 1.1391(1.1473) | Xent 0.0585(0.0601) | Loss 1.1683(1.1773) | Error 0.0178(0.0184) Steps 464(464.97) | Grad Norm 0.2342(0.4585) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 35.1481(35.5483) | Bit/dim 1.1429(1.1472) | Xent 0.0582(0.0600) | Loss 1.1721(1.1772) | Error 0.0182(0.0184) Steps 464(464.95) | Grad Norm 0.1396(0.4490) | Total Time 10.00(10.00)\n",
      "Iter 1491 | Time 36.5791(35.5792) | Bit/dim 1.1427(1.1470) | Xent 0.0564(0.0599) | Loss 1.1709(1.1770) | Error 0.0185(0.0184) Steps 464(464.92) | Grad Norm 0.2807(0.4439) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 16.7515, Epoch Time 280.7426(268.2722), Bit/dim 1.1346(best: 1.1346), Xent 0.0294, Loss 1.1493, Error 0.0093(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1492 | Time 35.3539(35.5724) | Bit/dim 1.1427(1.1469) | Xent 0.0581(0.0598) | Loss 1.1717(1.1768) | Error 0.0169(0.0184) Steps 464(464.89) | Grad Norm 0.2065(0.4368) | Total Time 10.00(10.00)\n",
      "Iter 1493 | Time 35.3372(35.5654) | Bit/dim 1.1397(1.1467) | Xent 0.0537(0.0597) | Loss 1.1666(1.1765) | Error 0.0185(0.0184) Steps 464(464.86) | Grad Norm 0.2012(0.4297) | Total Time 10.00(10.00)\n",
      "Iter 1494 | Time 36.1710(35.5836) | Bit/dim 1.1404(1.1465) | Xent 0.0597(0.0597) | Loss 1.1702(1.1763) | Error 0.0196(0.0184) Steps 470(465.02) | Grad Norm 0.2583(0.4246) | Total Time 10.00(10.00)\n",
      "Iter 1495 | Time 35.9813(35.5955) | Bit/dim 1.1419(1.1464) | Xent 0.0511(0.0594) | Loss 1.1675(1.1761) | Error 0.0160(0.0184) Steps 464(464.99) | Grad Norm 0.2238(0.4186) | Total Time 10.00(10.00)\n",
      "Iter 1496 | Time 34.9403(35.5758) | Bit/dim 1.1444(1.1463) | Xent 0.0532(0.0592) | Loss 1.1710(1.1759) | Error 0.0166(0.0183) Steps 464(464.96) | Grad Norm 0.1809(0.4114) | Total Time 10.00(10.00)\n",
      "Iter 1497 | Time 35.1358(35.5626) | Bit/dim 1.1391(1.1461) | Xent 0.0529(0.0590) | Loss 1.1656(1.1756) | Error 0.0172(0.0183) Steps 464(464.93) | Grad Norm 0.1975(0.4050) | Total Time 10.00(10.00)\n",
      "Iter 1498 | Time 35.1666(35.5508) | Bit/dim 1.1381(1.1458) | Xent 0.0658(0.0592) | Loss 1.1710(1.1755) | Error 0.0194(0.0183) Steps 470(465.08) | Grad Norm 0.2472(0.4003) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 16.9285, Epoch Time 277.2008(268.5401), Bit/dim 1.1340(best: 1.1346), Xent 0.0312, Loss 1.1496, Error 0.0094(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1499 | Time 34.2474(35.5117) | Bit/dim 1.1447(1.1458) | Xent 0.0542(0.0591) | Loss 1.1718(1.1753) | Error 0.0184(0.0183) Steps 464(465.05) | Grad Norm 0.2011(0.3943) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 35.6454(35.5157) | Bit/dim 1.1357(1.1455) | Xent 0.0500(0.0588) | Loss 1.1607(1.1749) | Error 0.0146(0.0182) Steps 464(465.02) | Grad Norm 0.1485(0.3869) | Total Time 10.00(10.00)\n",
      "Iter 1501 | Time 35.1002(35.5032) | Bit/dim 1.1396(1.1453) | Xent 0.0593(0.0588) | Loss 1.1692(1.1747) | Error 0.0189(0.0182) Steps 464(464.99) | Grad Norm 0.2060(0.3815) | Total Time 10.00(10.00)\n",
      "Iter 1502 | Time 36.6557(35.5378) | Bit/dim 1.1430(1.1453) | Xent 0.0496(0.0585) | Loss 1.1677(1.1745) | Error 0.0162(0.0182) Steps 470(465.14) | Grad Norm 0.2850(0.3786) | Total Time 10.00(10.00)\n",
      "Iter 1503 | Time 36.4263(35.5644) | Bit/dim 1.1364(1.1450) | Xent 0.0575(0.0585) | Loss 1.1651(1.1742) | Error 0.0165(0.0181) Steps 464(465.10) | Grad Norm 0.2074(0.3735) | Total Time 10.00(10.00)\n",
      "Iter 1504 | Time 34.3373(35.5276) | Bit/dim 1.1381(1.1448) | Xent 0.0601(0.0586) | Loss 1.1681(1.1741) | Error 0.0180(0.0181) Steps 464(465.07) | Grad Norm 0.2598(0.3701) | Total Time 10.00(10.00)\n",
      "Iter 1505 | Time 35.3242(35.5215) | Bit/dim 1.1440(1.1448) | Xent 0.0650(0.0587) | Loss 1.1765(1.1741) | Error 0.0188(0.0181) Steps 464(465.04) | Grad Norm 0.2921(0.3677) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 16.6385, Epoch Time 276.7097(268.7852), Bit/dim 1.1339(best: 1.1340), Xent 0.0297, Loss 1.1487, Error 0.0112(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1506 | Time 35.3668(35.5169) | Bit/dim 1.1436(1.1447) | Xent 0.0558(0.0587) | Loss 1.1715(1.1741) | Error 0.0166(0.0181) Steps 464(465.01) | Grad Norm 0.1931(0.3625) | Total Time 10.00(10.00)\n",
      "Iter 1507 | Time 35.8359(35.5264) | Bit/dim 1.1378(1.1445) | Xent 0.0618(0.0588) | Loss 1.1687(1.1739) | Error 0.0176(0.0181) Steps 470(465.16) | Grad Norm 0.1681(0.3567) | Total Time 10.00(10.00)\n",
      "Iter 1508 | Time 36.3674(35.5517) | Bit/dim 1.1409(1.1444) | Xent 0.0610(0.0588) | Loss 1.1714(1.1738) | Error 0.0214(0.0182) Steps 464(465.12) | Grad Norm 0.1840(0.3515) | Total Time 10.00(10.00)\n",
      "Iter 1509 | Time 35.4394(35.5483) | Bit/dim 1.1417(1.1443) | Xent 0.0545(0.0587) | Loss 1.1690(1.1737) | Error 0.0162(0.0181) Steps 464(465.09) | Grad Norm 0.2142(0.3474) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 34.2972(35.5108) | Bit/dim 1.1365(1.1441) | Xent 0.0518(0.0585) | Loss 1.1624(1.1733) | Error 0.0160(0.0180) Steps 464(465.05) | Grad Norm 0.1583(0.3417) | Total Time 10.00(10.00)\n",
      "Iter 1511 | Time 35.8671(35.5215) | Bit/dim 1.1446(1.1441) | Xent 0.0585(0.0585) | Loss 1.1738(1.1734) | Error 0.0184(0.0181) Steps 464(465.02) | Grad Norm 0.1901(0.3371) | Total Time 10.00(10.00)\n",
      "Iter 1512 | Time 35.5392(35.5220) | Bit/dim 1.1327(1.1438) | Xent 0.0564(0.0584) | Loss 1.1609(1.1730) | Error 0.0155(0.0180) Steps 470(465.17) | Grad Norm 0.1802(0.3324) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 16.8475, Epoch Time 277.7753(269.0549), Bit/dim 1.1325(best: 1.1339), Xent 0.0318, Loss 1.1484, Error 0.0103(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1513 | Time 34.4842(35.4909) | Bit/dim 1.1337(1.1435) | Xent 0.0559(0.0583) | Loss 1.1616(1.1726) | Error 0.0166(0.0179) Steps 464(465.14) | Grad Norm 0.2812(0.3309) | Total Time 10.00(10.00)\n",
      "Iter 1514 | Time 36.7677(35.5292) | Bit/dim 1.1381(1.1433) | Xent 0.0577(0.0583) | Loss 1.1670(1.1725) | Error 0.0192(0.0180) Steps 464(465.10) | Grad Norm 0.3067(0.3302) | Total Time 10.00(10.00)\n",
      "Iter 1515 | Time 35.6946(35.5341) | Bit/dim 1.1405(1.1432) | Xent 0.0583(0.0583) | Loss 1.1697(1.1724) | Error 0.0186(0.0180) Steps 464(465.07) | Grad Norm 0.3011(0.3293) | Total Time 10.00(10.00)\n",
      "Iter 1516 | Time 35.7844(35.5416) | Bit/dim 1.1378(1.1431) | Xent 0.0580(0.0583) | Loss 1.1668(1.1722) | Error 0.0172(0.0180) Steps 464(465.04) | Grad Norm 0.1420(0.3237) | Total Time 10.00(10.00)\n",
      "Iter 1517 | Time 35.0771(35.5277) | Bit/dim 1.1426(1.1430) | Xent 0.0507(0.0581) | Loss 1.1680(1.1721) | Error 0.0174(0.0180) Steps 464(465.01) | Grad Norm 0.2244(0.3207) | Total Time 10.00(10.00)\n",
      "Iter 1518 | Time 33.8182(35.4764) | Bit/dim 1.1390(1.1429) | Xent 0.0637(0.0583) | Loss 1.1708(1.1721) | Error 0.0198(0.0180) Steps 464(464.98) | Grad Norm 0.1888(0.3167) | Total Time 10.00(10.00)\n",
      "Iter 1519 | Time 34.4272(35.4449) | Bit/dim 1.1400(1.1428) | Xent 0.0614(0.0584) | Loss 1.1707(1.1720) | Error 0.0194(0.0180) Steps 464(464.95) | Grad Norm 0.1830(0.3127) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 16.4336, Epoch Time 274.7389(269.2254), Bit/dim 1.1331(best: 1.1325), Xent 0.0288, Loss 1.1475, Error 0.0093(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1520 | Time 35.4239(35.4443) | Bit/dim 1.1337(1.1426) | Xent 0.0592(0.0584) | Loss 1.1633(1.1718) | Error 0.0178(0.0180) Steps 464(464.92) | Grad Norm 0.2140(0.3098) | Total Time 10.00(10.00)\n",
      "Iter 1521 | Time 35.4431(35.4443) | Bit/dim 1.1394(1.1425) | Xent 0.0539(0.0582) | Loss 1.1664(1.1716) | Error 0.0162(0.0180) Steps 464(464.89) | Grad Norm 0.2176(0.3070) | Total Time 10.00(10.00)\n",
      "Iter 1522 | Time 35.6912(35.4517) | Bit/dim 1.1359(1.1423) | Xent 0.0564(0.0582) | Loss 1.1641(1.1714) | Error 0.0168(0.0179) Steps 464(464.86) | Grad Norm 0.1893(0.3035) | Total Time 10.00(10.00)\n",
      "Iter 1523 | Time 34.1278(35.4120) | Bit/dim 1.1436(1.1423) | Xent 0.0554(0.0581) | Loss 1.1713(1.1714) | Error 0.0171(0.0179) Steps 464(464.84) | Grad Norm 0.1672(0.2994) | Total Time 10.00(10.00)\n",
      "Iter 1524 | Time 34.3475(35.3800) | Bit/dim 1.1366(1.1421) | Xent 0.0543(0.0580) | Loss 1.1638(1.1711) | Error 0.0170(0.0179) Steps 464(464.81) | Grad Norm 0.2613(0.2982) | Total Time 10.00(10.00)\n",
      "Iter 1525 | Time 34.2955(35.3475) | Bit/dim 1.1361(1.1420) | Xent 0.0581(0.0580) | Loss 1.1651(1.1710) | Error 0.0185(0.0179) Steps 464(464.79) | Grad Norm 0.2331(0.2963) | Total Time 10.00(10.00)\n",
      "Iter 1526 | Time 35.3981(35.3490) | Bit/dim 1.1420(1.1420) | Xent 0.0672(0.0583) | Loss 1.1756(1.1711) | Error 0.0199(0.0180) Steps 464(464.77) | Grad Norm 0.2237(0.2941) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 16.6301, Epoch Time 273.7250(269.3604), Bit/dim 1.1324(best: 1.1325), Xent 0.0268, Loss 1.1458, Error 0.0091(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1527 | Time 35.5449(35.3549) | Bit/dim 1.1454(1.1421) | Xent 0.0552(0.0582) | Loss 1.1729(1.1712) | Error 0.0182(0.0180) Steps 464(464.74) | Grad Norm 0.2395(0.2925) | Total Time 10.00(10.00)\n",
      "Iter 1528 | Time 35.2227(35.3509) | Bit/dim 1.1371(1.1419) | Xent 0.0506(0.0579) | Loss 1.1624(1.1709) | Error 0.0142(0.0179) Steps 464(464.72) | Grad Norm 0.1640(0.2886) | Total Time 10.00(10.00)\n",
      "Iter 1529 | Time 35.9952(35.3703) | Bit/dim 1.1388(1.1418) | Xent 0.0543(0.0578) | Loss 1.1659(1.1707) | Error 0.0174(0.0179) Steps 464(464.70) | Grad Norm 0.2386(0.2871) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 35.5825(35.3766) | Bit/dim 1.1385(1.1417) | Xent 0.0572(0.0578) | Loss 1.1671(1.1706) | Error 0.0182(0.0179) Steps 464(464.68) | Grad Norm 0.1584(0.2833) | Total Time 10.00(10.00)\n",
      "Iter 1531 | Time 34.5876(35.3529) | Bit/dim 1.1350(1.1415) | Xent 0.0623(0.0580) | Loss 1.1662(1.1705) | Error 0.0196(0.0179) Steps 464(464.66) | Grad Norm 0.1775(0.2801) | Total Time 10.00(10.00)\n",
      "Iter 1532 | Time 36.0696(35.3744) | Bit/dim 1.1349(1.1413) | Xent 0.0675(0.0582) | Loss 1.1686(1.1704) | Error 0.0190(0.0180) Steps 464(464.64) | Grad Norm 0.2102(0.2780) | Total Time 10.00(10.00)\n",
      "Iter 1533 | Time 35.3870(35.3748) | Bit/dim 1.1340(1.1411) | Xent 0.0571(0.0582) | Loss 1.1626(1.1702) | Error 0.0199(0.0180) Steps 470(464.80) | Grad Norm 0.2242(0.2764) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 16.8743, Epoch Time 277.6179(269.6081), Bit/dim 1.1307(best: 1.1324), Xent 0.0285, Loss 1.1450, Error 0.0100(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1534 | Time 34.3792(35.3450) | Bit/dim 1.1375(1.1410) | Xent 0.0662(0.0584) | Loss 1.1706(1.1702) | Error 0.0184(0.0180) Steps 464(464.77) | Grad Norm 0.2136(0.2745) | Total Time 10.00(10.00)\n",
      "Iter 1535 | Time 35.4831(35.3491) | Bit/dim 1.1362(1.1409) | Xent 0.0559(0.0584) | Loss 1.1642(1.1700) | Error 0.0189(0.0180) Steps 464(464.75) | Grad Norm 0.2883(0.2749) | Total Time 10.00(10.00)\n",
      "Iter 1536 | Time 34.8781(35.3350) | Bit/dim 1.1362(1.1407) | Xent 0.0534(0.0582) | Loss 1.1629(1.1698) | Error 0.0164(0.0180) Steps 464(464.73) | Grad Norm 0.2248(0.2734) | Total Time 10.00(10.00)\n",
      "Iter 1537 | Time 34.5761(35.3122) | Bit/dim 1.1352(1.1405) | Xent 0.0505(0.0580) | Loss 1.1604(1.1695) | Error 0.0169(0.0180) Steps 464(464.71) | Grad Norm 0.2744(0.2734) | Total Time 10.00(10.00)\n",
      "Iter 1538 | Time 35.2291(35.3097) | Bit/dim 1.1397(1.1405) | Xent 0.0641(0.0582) | Loss 1.1718(1.1696) | Error 0.0190(0.0180) Steps 464(464.69) | Grad Norm 0.1707(0.2704) | Total Time 10.00(10.00)\n",
      "Iter 1539 | Time 35.3251(35.3102) | Bit/dim 1.1374(1.1404) | Xent 0.0552(0.0581) | Loss 1.1650(1.1695) | Error 0.0170(0.0180) Steps 464(464.67) | Grad Norm 0.1797(0.2676) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 35.3289(35.3107) | Bit/dim 1.1362(1.1403) | Xent 0.0547(0.0580) | Loss 1.1635(1.1693) | Error 0.0178(0.0180) Steps 464(464.65) | Grad Norm 0.1633(0.2645) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 16.9601, Epoch Time 274.4602(269.7537), Bit/dim 1.1307(best: 1.1307), Xent 0.0293, Loss 1.1453, Error 0.0114(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1541 | Time 34.8645(35.2973) | Bit/dim 1.1354(1.1402) | Xent 0.0600(0.0580) | Loss 1.1654(1.1692) | Error 0.0186(0.0180) Steps 464(464.63) | Grad Norm 0.2182(0.2631) | Total Time 10.00(10.00)\n",
      "Iter 1542 | Time 34.9574(35.2872) | Bit/dim 1.1357(1.1400) | Xent 0.0608(0.0581) | Loss 1.1662(1.1691) | Error 0.0191(0.0180) Steps 464(464.61) | Grad Norm 0.2922(0.2640) | Total Time 10.00(10.00)\n",
      "Iter 1543 | Time 35.4631(35.2924) | Bit/dim 1.1375(1.1399) | Xent 0.0555(0.0580) | Loss 1.1653(1.1690) | Error 0.0164(0.0180) Steps 464(464.59) | Grad Norm 0.2081(0.2623) | Total Time 10.00(10.00)\n",
      "Iter 1544 | Time 34.7113(35.2750) | Bit/dim 1.1361(1.1398) | Xent 0.0540(0.0579) | Loss 1.1631(1.1688) | Error 0.0165(0.0179) Steps 464(464.57) | Grad Norm 0.3309(0.2644) | Total Time 10.00(10.00)\n",
      "Iter 1545 | Time 34.1513(35.2413) | Bit/dim 1.1403(1.1398) | Xent 0.0546(0.0578) | Loss 1.1677(1.1688) | Error 0.0174(0.0179) Steps 464(464.55) | Grad Norm 0.2904(0.2651) | Total Time 10.00(10.00)\n",
      "Iter 1546 | Time 35.2365(35.2411) | Bit/dim 1.1416(1.1399) | Xent 0.0582(0.0578) | Loss 1.1707(1.1688) | Error 0.0164(0.0179) Steps 464(464.54) | Grad Norm 0.2353(0.2643) | Total Time 10.00(10.00)\n",
      "Iter 1547 | Time 35.6624(35.2538) | Bit/dim 1.1325(1.1397) | Xent 0.0532(0.0577) | Loss 1.1590(1.1685) | Error 0.0172(0.0178) Steps 464(464.52) | Grad Norm 0.2302(0.2632) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 16.8656, Epoch Time 274.0350(269.8821), Bit/dim 1.1312(best: 1.1307), Xent 0.0283, Loss 1.1454, Error 0.0086(best: 0.0087)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1548 | Time 35.2943(35.2550) | Bit/dim 1.1367(1.1396) | Xent 0.0664(0.0580) | Loss 1.1700(1.1686) | Error 0.0188(0.0179) Steps 464(464.51) | Grad Norm 0.4910(0.2701) | Total Time 10.00(10.00)\n",
      "Iter 1549 | Time 33.5384(35.2035) | Bit/dim 1.1329(1.1394) | Xent 0.0537(0.0578) | Loss 1.1598(1.1683) | Error 0.0155(0.0178) Steps 464(464.49) | Grad Norm 0.2168(0.2685) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 35.3748(35.2086) | Bit/dim 1.1366(1.1393) | Xent 0.0578(0.0578) | Loss 1.1655(1.1682) | Error 0.0179(0.0178) Steps 464(464.48) | Grad Norm 0.2928(0.2692) | Total Time 10.00(10.00)\n",
      "Iter 1551 | Time 35.4971(35.2173) | Bit/dim 1.1370(1.1392) | Xent 0.0534(0.0577) | Loss 1.1637(1.1681) | Error 0.0176(0.0178) Steps 464(464.46) | Grad Norm 0.3304(0.2710) | Total Time 10.00(10.00)\n",
      "Iter 1552 | Time 33.6128(35.1692) | Bit/dim 1.1347(1.1391) | Xent 0.0562(0.0577) | Loss 1.1628(1.1679) | Error 0.0179(0.0178) Steps 464(464.45) | Grad Norm 0.2565(0.2706) | Total Time 10.00(10.00)\n",
      "Iter 1553 | Time 35.2070(35.1703) | Bit/dim 1.1396(1.1391) | Xent 0.0581(0.0577) | Loss 1.1687(1.1679) | Error 0.0200(0.0179) Steps 464(464.43) | Grad Norm 0.2286(0.2693) | Total Time 10.00(10.00)\n",
      "Iter 1554 | Time 35.5637(35.1821) | Bit/dim 1.1356(1.1390) | Xent 0.0595(0.0577) | Loss 1.1654(1.1679) | Error 0.0192(0.0179) Steps 464(464.42) | Grad Norm 0.2953(0.2701) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 17.0646, Epoch Time 273.4473(269.9891), Bit/dim 1.1297(best: 1.1307), Xent 0.0302, Loss 1.1448, Error 0.0103(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1555 | Time 34.6030(35.1647) | Bit/dim 1.1313(1.1388) | Xent 0.0561(0.0577) | Loss 1.1594(1.1676) | Error 0.0171(0.0179) Steps 464(464.41) | Grad Norm 0.3014(0.2711) | Total Time 10.00(10.00)\n",
      "Iter 1556 | Time 34.8167(35.1543) | Bit/dim 1.1330(1.1386) | Xent 0.0606(0.0578) | Loss 1.1633(1.1675) | Error 0.0180(0.0179) Steps 464(464.40) | Grad Norm 0.3401(0.2731) | Total Time 10.00(10.00)\n",
      "Iter 1557 | Time 36.0819(35.1821) | Bit/dim 1.1398(1.1386) | Xent 0.0569(0.0577) | Loss 1.1682(1.1675) | Error 0.0175(0.0179) Steps 464(464.38) | Grad Norm 0.3133(0.2743) | Total Time 10.00(10.00)\n",
      "Iter 1558 | Time 34.6669(35.1667) | Bit/dim 1.1401(1.1387) | Xent 0.0582(0.0578) | Loss 1.1692(1.1676) | Error 0.0166(0.0178) Steps 464(464.37) | Grad Norm 0.2284(0.2730) | Total Time 10.00(10.00)\n",
      "Iter 1559 | Time 34.5354(35.1477) | Bit/dim 1.1351(1.1386) | Xent 0.0649(0.0580) | Loss 1.1676(1.1676) | Error 0.0192(0.0179) Steps 470(464.54) | Grad Norm 0.3786(0.2761) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 34.6482(35.1327) | Bit/dim 1.1288(1.1383) | Xent 0.0590(0.0580) | Loss 1.1583(1.1673) | Error 0.0189(0.0179) Steps 464(464.53) | Grad Norm 0.3654(0.2788) | Total Time 10.00(10.00)\n",
      "Iter 1561 | Time 35.5337(35.1448) | Bit/dim 1.1407(1.1384) | Xent 0.0593(0.0580) | Loss 1.1703(1.1674) | Error 0.0181(0.0179) Steps 464(464.51) | Grad Norm 0.2855(0.2790) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 16.8478, Epoch Time 273.9572(270.1081), Bit/dim 1.1294(best: 1.1297), Xent 0.0304, Loss 1.1446, Error 0.0101(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1562 | Time 35.0172(35.1409) | Bit/dim 1.1363(1.1383) | Xent 0.0621(0.0582) | Loss 1.1674(1.1674) | Error 0.0190(0.0179) Steps 464(464.49) | Grad Norm 0.3419(0.2809) | Total Time 10.00(10.00)\n",
      "Iter 1563 | Time 35.4696(35.1508) | Bit/dim 1.1332(1.1381) | Xent 0.0515(0.0580) | Loss 1.1590(1.1671) | Error 0.0182(0.0180) Steps 464(464.48) | Grad Norm 0.2363(0.2796) | Total Time 10.00(10.00)\n",
      "Iter 1564 | Time 34.4962(35.1312) | Bit/dim 1.1338(1.1380) | Xent 0.0535(0.0578) | Loss 1.1605(1.1669) | Error 0.0166(0.0179) Steps 464(464.47) | Grad Norm 0.2143(0.2776) | Total Time 10.00(10.00)\n",
      "Iter 1565 | Time 35.1095(35.1305) | Bit/dim 1.1377(1.1380) | Xent 0.0528(0.0577) | Loss 1.1641(1.1668) | Error 0.0174(0.0179) Steps 464(464.45) | Grad Norm 0.3272(0.2791) | Total Time 10.00(10.00)\n",
      "Iter 1566 | Time 34.5741(35.1138) | Bit/dim 1.1358(1.1379) | Xent 0.0614(0.0578) | Loss 1.1665(1.1668) | Error 0.0178(0.0179) Steps 464(464.44) | Grad Norm 0.2616(0.2786) | Total Time 10.00(10.00)\n",
      "Iter 1567 | Time 34.1681(35.0854) | Bit/dim 1.1397(1.1380) | Xent 0.0629(0.0579) | Loss 1.1712(1.1670) | Error 0.0206(0.0180) Steps 470(464.60) | Grad Norm 0.3084(0.2795) | Total Time 10.00(10.00)\n",
      "Iter 1568 | Time 35.5774(35.1002) | Bit/dim 1.1306(1.1378) | Xent 0.0662(0.0582) | Loss 1.1637(1.1669) | Error 0.0178(0.0180) Steps 464(464.59) | Grad Norm 0.1904(0.2768) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 17.1383, Epoch Time 273.8130(270.2193), Bit/dim 1.1284(best: 1.1294), Xent 0.0307, Loss 1.1437, Error 0.0108(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1569 | Time 34.4950(35.0820) | Bit/dim 1.1328(1.1376) | Xent 0.0619(0.0583) | Loss 1.1637(1.1668) | Error 0.0174(0.0180) Steps 464(464.57) | Grad Norm 0.2632(0.2764) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 36.9828(35.1391) | Bit/dim 1.1361(1.1376) | Xent 0.0551(0.0582) | Loss 1.1637(1.1667) | Error 0.0178(0.0179) Steps 470(464.73) | Grad Norm 0.2434(0.2754) | Total Time 10.00(10.00)\n",
      "Iter 1571 | Time 35.9608(35.1637) | Bit/dim 1.1383(1.1376) | Xent 0.0583(0.0582) | Loss 1.1674(1.1667) | Error 0.0168(0.0179) Steps 464(464.71) | Grad Norm 0.3757(0.2784) | Total Time 10.00(10.00)\n",
      "Iter 1572 | Time 34.2605(35.1366) | Bit/dim 1.1326(1.1374) | Xent 0.0601(0.0583) | Loss 1.1627(1.1666) | Error 0.0191(0.0179) Steps 458(464.51) | Grad Norm 0.2295(0.2769) | Total Time 10.00(10.00)\n",
      "Iter 1573 | Time 36.3662(35.1735) | Bit/dim 1.1324(1.1373) | Xent 0.0568(0.0582) | Loss 1.1608(1.1664) | Error 0.0180(0.0179) Steps 470(464.67) | Grad Norm 0.2079(0.2749) | Total Time 10.00(10.00)\n",
      "Iter 1574 | Time 34.1545(35.1429) | Bit/dim 1.1375(1.1373) | Xent 0.0668(0.0585) | Loss 1.1710(1.1665) | Error 0.0190(0.0180) Steps 464(464.65) | Grad Norm 0.2568(0.2743) | Total Time 10.00(10.00)\n",
      "Iter 1575 | Time 34.2242(35.1154) | Bit/dim 1.1339(1.1372) | Xent 0.0583(0.0585) | Loss 1.1630(1.1664) | Error 0.0176(0.0180) Steps 464(464.63) | Grad Norm 0.2997(0.2751) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 16.7763, Epoch Time 275.4263(270.3755), Bit/dim 1.1287(best: 1.1284), Xent 0.0294, Loss 1.1434, Error 0.0097(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1576 | Time 35.6546(35.1316) | Bit/dim 1.1365(1.1372) | Xent 0.0480(0.0582) | Loss 1.1604(1.1663) | Error 0.0151(0.0179) Steps 464(464.61) | Grad Norm 0.2564(0.2745) | Total Time 10.00(10.00)\n",
      "Iter 1577 | Time 34.9004(35.1246) | Bit/dim 1.1360(1.1371) | Xent 0.0633(0.0583) | Loss 1.1676(1.1663) | Error 0.0182(0.0179) Steps 464(464.60) | Grad Norm 0.2055(0.2724) | Total Time 10.00(10.00)\n",
      "Iter 1578 | Time 34.1487(35.0953) | Bit/dim 1.1348(1.1371) | Xent 0.0674(0.0586) | Loss 1.1685(1.1664) | Error 0.0210(0.0180) Steps 464(464.58) | Grad Norm 0.1708(0.2694) | Total Time 10.00(10.00)\n",
      "Iter 1579 | Time 33.9588(35.0612) | Bit/dim 1.1352(1.1370) | Xent 0.0501(0.0583) | Loss 1.1602(1.1662) | Error 0.0159(0.0179) Steps 464(464.56) | Grad Norm 0.2059(0.2675) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 34.4935(35.0442) | Bit/dim 1.1354(1.1370) | Xent 0.0566(0.0583) | Loss 1.1637(1.1661) | Error 0.0155(0.0179) Steps 464(464.54) | Grad Norm 0.3357(0.2695) | Total Time 10.00(10.00)\n",
      "Iter 1581 | Time 34.5067(35.0281) | Bit/dim 1.1287(1.1367) | Xent 0.0621(0.0584) | Loss 1.1597(1.1659) | Error 0.0189(0.0179) Steps 464(464.53) | Grad Norm 0.3105(0.2708) | Total Time 10.00(10.00)\n",
      "Iter 1582 | Time 33.7166(34.9887) | Bit/dim 1.1349(1.1367) | Xent 0.0606(0.0585) | Loss 1.1652(1.1659) | Error 0.0180(0.0179) Steps 464(464.51) | Grad Norm 0.4490(0.2761) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 16.6507, Epoch Time 270.1666(270.3692), Bit/dim 1.1284(best: 1.1284), Xent 0.0267, Loss 1.1418, Error 0.0088(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1583 | Time 34.0230(34.9598) | Bit/dim 1.1346(1.1366) | Xent 0.0509(0.0582) | Loss 1.1601(1.1657) | Error 0.0155(0.0178) Steps 464(464.50) | Grad Norm 0.2204(0.2744) | Total Time 10.00(10.00)\n",
      "Iter 1584 | Time 34.7834(34.9545) | Bit/dim 1.1336(1.1365) | Xent 0.0614(0.0583) | Loss 1.1643(1.1657) | Error 0.0179(0.0178) Steps 470(464.66) | Grad Norm 0.2603(0.2740) | Total Time 10.00(10.00)\n",
      "Iter 1585 | Time 33.8072(34.9201) | Bit/dim 1.1372(1.1365) | Xent 0.0658(0.0586) | Loss 1.1701(1.1658) | Error 0.0211(0.0179) Steps 464(464.64) | Grad Norm 0.3105(0.2751) | Total Time 10.00(10.00)\n",
      "Iter 1586 | Time 35.1553(34.9271) | Bit/dim 1.1318(1.1364) | Xent 0.0563(0.0585) | Loss 1.1600(1.1656) | Error 0.0179(0.0179) Steps 470(464.80) | Grad Norm 0.3024(0.2759) | Total Time 10.00(10.00)\n",
      "Iter 1587 | Time 34.9419(34.9276) | Bit/dim 1.1301(1.1362) | Xent 0.0595(0.0585) | Loss 1.1599(1.1655) | Error 0.0174(0.0179) Steps 464(464.78) | Grad Norm 0.4576(0.2814) | Total Time 10.00(10.00)\n",
      "Iter 1588 | Time 34.2609(34.9076) | Bit/dim 1.1350(1.1362) | Xent 0.0557(0.0584) | Loss 1.1628(1.1654) | Error 0.0175(0.0179) Steps 464(464.76) | Grad Norm 0.2161(0.2794) | Total Time 10.00(10.00)\n",
      "Iter 1589 | Time 35.4976(34.9253) | Bit/dim 1.1345(1.1361) | Xent 0.0568(0.0584) | Loss 1.1629(1.1653) | Error 0.0166(0.0178) Steps 464(464.73) | Grad Norm 0.3445(0.2814) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 17.0115, Epoch Time 271.5701(270.4052), Bit/dim 1.1280(best: 1.1284), Xent 0.0278, Loss 1.1419, Error 0.0096(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1590 | Time 34.2355(34.9046) | Bit/dim 1.1334(1.1360) | Xent 0.0546(0.0583) | Loss 1.1607(1.1652) | Error 0.0160(0.0178) Steps 464(464.71) | Grad Norm 0.3433(0.2832) | Total Time 10.00(10.00)\n",
      "Iter 1591 | Time 34.1759(34.8827) | Bit/dim 1.1348(1.1360) | Xent 0.0550(0.0582) | Loss 1.1623(1.1651) | Error 0.0165(0.0178) Steps 464(464.69) | Grad Norm 0.4919(0.2895) | Total Time 10.00(10.00)\n",
      "Iter 1592 | Time 34.8530(34.8818) | Bit/dim 1.1314(1.1359) | Xent 0.0585(0.0582) | Loss 1.1607(1.1650) | Error 0.0188(0.0178) Steps 458(464.49) | Grad Norm 0.2774(0.2891) | Total Time 10.00(10.00)\n",
      "Iter 1593 | Time 34.3699(34.8665) | Bit/dim 1.1322(1.1358) | Xent 0.0554(0.0581) | Loss 1.1599(1.1648) | Error 0.0178(0.0178) Steps 458(464.29) | Grad Norm 0.3081(0.2897) | Total Time 10.00(10.00)\n",
      "Iter 1594 | Time 36.0004(34.9005) | Bit/dim 1.1351(1.1357) | Xent 0.0522(0.0579) | Loss 1.1612(1.1647) | Error 0.0165(0.0177) Steps 464(464.28) | Grad Norm 0.4305(0.2939) | Total Time 10.00(10.00)\n",
      "Iter 1595 | Time 35.6566(34.9232) | Bit/dim 1.1324(1.1356) | Xent 0.0561(0.0579) | Loss 1.1605(1.1646) | Error 0.0172(0.0177) Steps 458(464.10) | Grad Norm 0.2120(0.2915) | Total Time 10.00(10.00)\n",
      "Iter 1596 | Time 34.4158(34.9079) | Bit/dim 1.1324(1.1355) | Xent 0.0639(0.0580) | Loss 1.1644(1.1646) | Error 0.0181(0.0177) Steps 470(464.27) | Grad Norm 0.5808(0.3001) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 16.8628, Epoch Time 272.8878(270.4797), Bit/dim 1.1262(best: 1.1280), Xent 0.0308, Loss 1.1416, Error 0.0105(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1597 | Time 35.4814(34.9251) | Bit/dim 1.1303(1.1354) | Xent 0.0563(0.0580) | Loss 1.1585(1.1644) | Error 0.0156(0.0177) Steps 470(464.45) | Grad Norm 0.2140(0.2976) | Total Time 10.00(10.00)\n",
      "Iter 1598 | Time 35.9905(34.9571) | Bit/dim 1.1322(1.1353) | Xent 0.0552(0.0579) | Loss 1.1598(1.1642) | Error 0.0154(0.0176) Steps 470(464.61) | Grad Norm 0.5294(0.3045) | Total Time 10.00(10.00)\n",
      "Iter 1599 | Time 34.7135(34.9498) | Bit/dim 1.1364(1.1353) | Xent 0.0531(0.0578) | Loss 1.1629(1.1642) | Error 0.0159(0.0176) Steps 464(464.59) | Grad Norm 0.4899(0.3101) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 35.6819(34.9718) | Bit/dim 1.1333(1.1353) | Xent 0.0602(0.0578) | Loss 1.1634(1.1642) | Error 0.0171(0.0175) Steps 476(464.94) | Grad Norm 0.6387(0.3199) | Total Time 10.00(10.00)\n",
      "Iter 1601 | Time 33.8291(34.9375) | Bit/dim 1.1358(1.1353) | Xent 0.0623(0.0580) | Loss 1.1669(1.1643) | Error 0.0196(0.0176) Steps 464(464.91) | Grad Norm 0.4118(0.3227) | Total Time 10.00(10.00)\n",
      "Iter 1602 | Time 34.1905(34.9151) | Bit/dim 1.1283(1.1351) | Xent 0.0546(0.0579) | Loss 1.1556(1.1640) | Error 0.0168(0.0176) Steps 470(465.06) | Grad Norm 0.3139(0.3224) | Total Time 10.00(10.00)\n",
      "Iter 1605 | Time 33.7230(34.9011) | Bit/dim 1.1314(1.1348) | Xent 0.0598(0.0581) | Loss 1.1613(1.1638) | Error 0.0204(0.0177) Steps 464(464.97) | Grad Norm 0.4782(0.3308) | Total Time 10.00(10.00)\n",
      "Iter 1606 | Time 33.9043(34.8712) | Bit/dim 1.1340(1.1347) | Xent 0.0590(0.0581) | Loss 1.1635(1.1638) | Error 0.0182(0.0177) Steps 464(464.94) | Grad Norm 0.1641(0.3258) | Total Time 10.00(10.00)\n",
      "Iter 1607 | Time 35.3114(34.8844) | Bit/dim 1.1352(1.1348) | Xent 0.0574(0.0581) | Loss 1.1639(1.1638) | Error 0.0161(0.0177) Steps 470(465.09) | Grad Norm 0.2390(0.3232) | Total Time 10.00(10.00)\n",
      "Iter 1608 | Time 35.3192(34.8974) | Bit/dim 1.1304(1.1346) | Xent 0.0605(0.0582) | Loss 1.1607(1.1637) | Error 0.0204(0.0178) Steps 458(464.88) | Grad Norm 0.4311(0.3264) | Total Time 10.00(10.00)\n",
      "Iter 1609 | Time 35.3188(34.9101) | Bit/dim 1.1329(1.1346) | Xent 0.0546(0.0581) | Loss 1.1601(1.1636) | Error 0.0165(0.0177) Steps 458(464.67) | Grad Norm 0.3049(0.3258) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 33.8646(34.8787) | Bit/dim 1.1311(1.1345) | Xent 0.0492(0.0578) | Loss 1.1557(1.1634) | Error 0.0155(0.0177) Steps 458(464.47) | Grad Norm 0.2738(0.3242) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 17.0893, Epoch Time 272.7875(270.6516), Bit/dim 1.1256(best: 1.1262), Xent 0.0288, Loss 1.1400, Error 0.0089(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1611 | Time 34.2333(34.8593) | Bit/dim 1.1312(1.1344) | Xent 0.0568(0.0578) | Loss 1.1596(1.1632) | Error 0.0181(0.0177) Steps 464(464.46) | Grad Norm 0.1773(0.3198) | Total Time 10.00(10.00)\n",
      "Iter 1612 | Time 34.2783(34.8419) | Bit/dim 1.1315(1.1343) | Xent 0.0602(0.0578) | Loss 1.1616(1.1632) | Error 0.0195(0.0177) Steps 458(464.26) | Grad Norm 0.2311(0.3171) | Total Time 10.00(10.00)\n",
      "Iter 1613 | Time 34.1145(34.8201) | Bit/dim 1.1313(1.1342) | Xent 0.0600(0.0579) | Loss 1.1613(1.1631) | Error 0.0190(0.0178) Steps 458(464.08) | Grad Norm 0.2760(0.3159) | Total Time 10.00(10.00)\n",
      "Iter 1614 | Time 33.9411(34.7937) | Bit/dim 1.1327(1.1341) | Xent 0.0599(0.0580) | Loss 1.1626(1.1631) | Error 0.0196(0.0178) Steps 464(464.07) | Grad Norm 0.2660(0.3144) | Total Time 10.00(10.00)\n",
      "Iter 1615 | Time 33.6355(34.7590) | Bit/dim 1.1309(1.1340) | Xent 0.0544(0.0578) | Loss 1.1581(1.1630) | Error 0.0160(0.0178) Steps 458(463.89) | Grad Norm 0.3857(0.3165) | Total Time 10.00(10.00)\n",
      "Iter 1616 | Time 34.1100(34.7395) | Bit/dim 1.1302(1.1339) | Xent 0.0586(0.0579) | Loss 1.1595(1.1629) | Error 0.0184(0.0178) Steps 458(463.71) | Grad Norm 0.1522(0.3116) | Total Time 10.00(10.00)\n",
      "Iter 1617 | Time 33.9017(34.7144) | Bit/dim 1.1320(1.1339) | Xent 0.0609(0.0580) | Loss 1.1625(1.1629) | Error 0.0180(0.0178) Steps 458(463.54) | Grad Norm 0.2302(0.3092) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 16.5957, Epoch Time 267.1061(270.5453), Bit/dim 1.1262(best: 1.1256), Xent 0.0297, Loss 1.1411, Error 0.0106(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1618 | Time 33.5662(34.6799) | Bit/dim 1.1309(1.1338) | Xent 0.0510(0.0578) | Loss 1.1564(1.1627) | Error 0.0178(0.0178) Steps 458(463.38) | Grad Norm 0.3664(0.3109) | Total Time 10.00(10.00)\n",
      "Iter 1619 | Time 35.0607(34.6913) | Bit/dim 1.1335(1.1338) | Xent 0.0505(0.0575) | Loss 1.1588(1.1625) | Error 0.0155(0.0177) Steps 458(463.22) | Grad Norm 0.2706(0.3097) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 33.8810(34.6670) | Bit/dim 1.1331(1.1338) | Xent 0.0566(0.0575) | Loss 1.1614(1.1625) | Error 0.0170(0.0177) Steps 458(463.06) | Grad Norm 0.3240(0.3101) | Total Time 10.00(10.00)\n",
      "Iter 1621 | Time 35.0989(34.6800) | Bit/dim 1.1302(1.1336) | Xent 0.0544(0.0574) | Loss 1.1573(1.1624) | Error 0.0165(0.0177) Steps 464(463.09) | Grad Norm 0.2341(0.3078) | Total Time 10.00(10.00)\n",
      "Iter 1622 | Time 34.4518(34.6731) | Bit/dim 1.1261(1.1334) | Xent 0.0560(0.0574) | Loss 1.1541(1.1621) | Error 0.0165(0.0176) Steps 458(462.93) | Grad Norm 0.3245(0.3083) | Total Time 10.00(10.00)\n",
      "Iter 1623 | Time 35.2479(34.6904) | Bit/dim 1.1367(1.1335) | Xent 0.0631(0.0575) | Loss 1.1683(1.1623) | Error 0.0199(0.0177) Steps 476(463.33) | Grad Norm 0.3398(0.3093) | Total Time 10.00(10.00)\n",
      "Iter 1624 | Time 34.5648(34.6866) | Bit/dim 1.1303(1.1334) | Xent 0.0622(0.0577) | Loss 1.1614(1.1623) | Error 0.0190(0.0177) Steps 470(463.53) | Grad Norm 0.2298(0.3069) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 16.9094, Epoch Time 270.9660(270.5579), Bit/dim 1.1247(best: 1.1256), Xent 0.0290, Loss 1.1392, Error 0.0096(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1625 | Time 35.4667(34.7100) | Bit/dim 1.1328(1.1334) | Xent 0.0573(0.0577) | Loss 1.1615(1.1622) | Error 0.0198(0.0178) Steps 464(463.54) | Grad Norm 0.4133(0.3101) | Total Time 10.00(10.00)\n",
      "Iter 1626 | Time 34.3605(34.6995) | Bit/dim 1.1289(1.1333) | Xent 0.0618(0.0578) | Loss 1.1598(1.1622) | Error 0.0205(0.0179) Steps 470(463.73) | Grad Norm 0.2943(0.3096) | Total Time 10.00(10.00)\n",
      "Iter 1627 | Time 34.3879(34.6902) | Bit/dim 1.1294(1.1332) | Xent 0.0549(0.0577) | Loss 1.1568(1.1620) | Error 0.0178(0.0179) Steps 464(463.74) | Grad Norm 0.3811(0.3117) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 35.2730(34.6607) | Bit/dim 1.1285(1.1329) | Xent 0.0539(0.0574) | Loss 1.1554(1.1616) | Error 0.0160(0.0178) Steps 458(463.59) | Grad Norm 0.2528(0.3108) | Total Time 10.00(10.00)\n",
      "Iter 1631 | Time 34.7587(34.6636) | Bit/dim 1.1336(1.1329) | Xent 0.0617(0.0575) | Loss 1.1645(1.1616) | Error 0.0176(0.0177) Steps 458(463.42) | Grad Norm 0.4625(0.3153) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 17.1790, Epoch Time 271.9407(270.5994), Bit/dim 1.1246(best: 1.1247), Xent 0.0294, Loss 1.1393, Error 0.0096(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1632 | Time 34.4951(34.6586) | Bit/dim 1.1291(1.1328) | Xent 0.0564(0.0575) | Loss 1.1573(1.1615) | Error 0.0184(0.0178) Steps 458(463.26) | Grad Norm 0.1548(0.3105) | Total Time 10.00(10.00)\n",
      "Iter 1633 | Time 33.0081(34.6090) | Bit/dim 1.1344(1.1328) | Xent 0.0503(0.0573) | Loss 1.1595(1.1615) | Error 0.0148(0.0177) Steps 458(463.10) | Grad Norm 0.1595(0.3060) | Total Time 10.00(10.00)\n",
      "Iter 1634 | Time 34.6107(34.6091) | Bit/dim 1.1366(1.1329) | Xent 0.0635(0.0575) | Loss 1.1683(1.1617) | Error 0.0189(0.0177) Steps 458(462.94) | Grad Norm 0.2513(0.3044) | Total Time 10.00(10.00)\n",
      "Iter 1635 | Time 34.1241(34.5945) | Bit/dim 1.1268(1.1327) | Xent 0.0626(0.0576) | Loss 1.1581(1.1616) | Error 0.0200(0.0178) Steps 458(462.80) | Grad Norm 0.1871(0.3008) | Total Time 10.00(10.00)\n",
      "Iter 1636 | Time 33.9963(34.5766) | Bit/dim 1.1256(1.1325) | Xent 0.0538(0.0575) | Loss 1.1525(1.1613) | Error 0.0172(0.0178) Steps 464(462.83) | Grad Norm 0.1869(0.2974) | Total Time 10.00(10.00)\n",
      "Iter 1637 | Time 34.7711(34.5824) | Bit/dim 1.1271(1.1324) | Xent 0.0559(0.0575) | Loss 1.1551(1.1611) | Error 0.0170(0.0177) Steps 464(462.87) | Grad Norm 0.3518(0.2990) | Total Time 10.00(10.00)\n",
      "Iter 1638 | Time 33.7150(34.5564) | Bit/dim 1.1368(1.1325) | Xent 0.0592(0.0575) | Loss 1.1664(1.1613) | Error 0.0186(0.0178) Steps 458(462.72) | Grad Norm 0.2083(0.2963) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 17.0284, Epoch Time 267.8101(270.5157), Bit/dim 1.1244(best: 1.1246), Xent 0.0302, Loss 1.1395, Error 0.0102(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1639 | Time 32.9227(34.5074) | Bit/dim 1.1312(1.1325) | Xent 0.0566(0.0575) | Loss 1.1594(1.1612) | Error 0.0155(0.0177) Steps 458(462.58) | Grad Norm 0.4161(0.2999) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 33.4695(34.4763) | Bit/dim 1.1285(1.1323) | Xent 0.0593(0.0575) | Loss 1.1582(1.1611) | Error 0.0184(0.0177) Steps 458(462.44) | Grad Norm 0.2040(0.2970) | Total Time 10.00(10.00)\n",
      "Iter 1641 | Time 34.2562(34.4697) | Bit/dim 1.1309(1.1323) | Xent 0.0547(0.0575) | Loss 1.1582(1.1610) | Error 0.0160(0.0177) Steps 464(462.49) | Grad Norm 0.5882(0.3058) | Total Time 10.00(10.00)\n",
      "Iter 1642 | Time 34.0203(34.4562) | Bit/dim 1.1288(1.1322) | Xent 0.0614(0.0576) | Loss 1.1595(1.1610) | Error 0.0185(0.0177) Steps 464(462.53) | Grad Norm 0.2743(0.3048) | Total Time 10.00(10.00)\n",
      "Iter 1643 | Time 34.8258(34.4673) | Bit/dim 1.1306(1.1321) | Xent 0.0539(0.0575) | Loss 1.1576(1.1609) | Error 0.0185(0.0177) Steps 464(462.58) | Grad Norm 0.7551(0.3183) | Total Time 10.00(10.00)\n",
      "Iter 1644 | Time 35.4376(34.4964) | Bit/dim 1.1299(1.1321) | Xent 0.0623(0.0576) | Loss 1.1611(1.1609) | Error 0.0180(0.0177) Steps 464(462.62) | Grad Norm 0.2077(0.3150) | Total Time 10.00(10.00)\n",
      "Iter 1645 | Time 33.3658(34.4625) | Bit/dim 1.1301(1.1320) | Xent 0.0607(0.0577) | Loss 1.1605(1.1609) | Error 0.0195(0.0178) Steps 458(462.48) | Grad Norm 0.5718(0.3227) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 17.0100, Epoch Time 267.9052(270.4374), Bit/dim 1.1229(best: 1.1244), Xent 0.0298, Loss 1.1378, Error 0.0104(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1646 | Time 33.5741(34.4358) | Bit/dim 1.1278(1.1319) | Xent 0.0601(0.0578) | Loss 1.1579(1.1608) | Error 0.0188(0.0178) Steps 458(462.35) | Grad Norm 0.3366(0.3231) | Total Time 10.00(10.00)\n",
      "Iter 1647 | Time 34.1842(34.4283) | Bit/dim 1.1314(1.1319) | Xent 0.0615(0.0579) | Loss 1.1622(1.1608) | Error 0.0175(0.0178) Steps 464(462.40) | Grad Norm 0.5692(0.3305) | Total Time 10.00(10.00)\n",
      "Iter 1648 | Time 34.7256(34.4372) | Bit/dim 1.1296(1.1318) | Xent 0.0529(0.0577) | Loss 1.1560(1.1607) | Error 0.0174(0.0178) Steps 464(462.45) | Grad Norm 0.1488(0.3251) | Total Time 10.00(10.00)\n",
      "Iter 1649 | Time 33.7709(34.4172) | Bit/dim 1.1315(1.1318) | Xent 0.0562(0.0577) | Loss 1.1596(1.1606) | Error 0.0176(0.0178) Steps 464(462.49) | Grad Norm 0.3637(0.3262) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 33.1785(34.3800) | Bit/dim 1.1313(1.1318) | Xent 0.0617(0.0578) | Loss 1.1622(1.1607) | Error 0.0195(0.0178) Steps 464(462.54) | Grad Norm 0.2992(0.3254) | Total Time 10.00(10.00)\n",
      "Iter 1651 | Time 33.4334(34.3516) | Bit/dim 1.1291(1.1317) | Xent 0.0503(0.0576) | Loss 1.1543(1.1605) | Error 0.0155(0.0178) Steps 464(462.58) | Grad Norm 0.4547(0.3293) | Total Time 10.00(10.00)\n",
      "Iter 1652 | Time 33.6568(34.3308) | Bit/dim 1.1265(1.1316) | Xent 0.0548(0.0575) | Loss 1.1540(1.1603) | Error 0.0161(0.0177) Steps 458(462.44) | Grad Norm 0.3137(0.3288) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 16.7112, Epoch Time 265.5272(270.2901), Bit/dim 1.1229(best: 1.1229), Xent 0.0278, Loss 1.1368, Error 0.0092(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1653 | Time 34.5165(34.3364) | Bit/dim 1.1267(1.1314) | Xent 0.0553(0.0574) | Loss 1.1544(1.1601) | Error 0.0158(0.0177) Steps 458(462.31) | Grad Norm 0.4914(0.3337) | Total Time 10.00(10.00)\n",
      "Iter 1654 | Time 33.1576(34.3010) | Bit/dim 1.1251(1.1312) | Xent 0.0589(0.0575) | Loss 1.1546(1.1600) | Error 0.0161(0.0176) Steps 458(462.18) | Grad Norm 0.2521(0.3313) | Total Time 10.00(10.00)\n",
      "Iter 1655 | Time 33.8990(34.2889) | Bit/dim 1.1271(1.1311) | Xent 0.0606(0.0576) | Loss 1.1574(1.1599) | Error 0.0161(0.0176) Steps 464(462.24) | Grad Norm 0.6310(0.3403) | Total Time 10.00(10.00)\n",
      "Iter 1656 | Time 33.5070(34.2655) | Bit/dim 1.1307(1.1311) | Xent 0.0633(0.0577) | Loss 1.1624(1.1600) | Error 0.0191(0.0176) Steps 458(462.11) | Grad Norm 0.2326(0.3370) | Total Time 10.00(10.00)\n",
      "Iter 1657 | Time 34.4491(34.2710) | Bit/dim 1.1317(1.1311) | Xent 0.0586(0.0578) | Loss 1.1610(1.1600) | Error 0.0168(0.0176) Steps 458(461.99) | Grad Norm 0.7417(0.3492) | Total Time 10.00(10.00)\n",
      "Iter 1658 | Time 33.7631(34.2558) | Bit/dim 1.1309(1.1311) | Xent 0.0471(0.0575) | Loss 1.1544(1.1598) | Error 0.0141(0.0175) Steps 458(461.87) | Grad Norm 0.2122(0.3450) | Total Time 10.00(10.00)\n",
      "Iter 1659 | Time 33.4508(34.2316) | Bit/dim 1.1317(1.1311) | Xent 0.0562(0.0574) | Loss 1.1598(1.1598) | Error 0.0171(0.0175) Steps 464(461.93) | Grad Norm 1.0249(0.3654) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 17.1175, Epoch Time 266.2150(270.1678), Bit/dim 1.1232(best: 1.1229), Xent 0.0289, Loss 1.1377, Error 0.0102(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1660 | Time 33.0543(34.1963) | Bit/dim 1.1282(1.1310) | Xent 0.0641(0.0576) | Loss 1.1602(1.1598) | Error 0.0210(0.0176) Steps 458(461.81) | Grad Norm 0.4976(0.3694) | Total Time 10.00(10.00)\n",
      "Iter 1661 | Time 33.3860(34.1720) | Bit/dim 1.1317(1.1310) | Xent 0.0616(0.0577) | Loss 1.1625(1.1599) | Error 0.0202(0.0177) Steps 464(461.88) | Grad Norm 0.9396(0.3865) | Total Time 10.00(10.00)\n",
      "Iter 1662 | Time 33.4631(34.1507) | Bit/dim 1.1323(1.1311) | Xent 0.0584(0.0578) | Loss 1.1615(1.1600) | Error 0.0159(0.0176) Steps 464(461.94) | Grad Norm 0.6845(0.3955) | Total Time 10.00(10.00)\n",
      "Iter 1663 | Time 33.2375(34.1233) | Bit/dim 1.1249(1.1309) | Xent 0.0535(0.0576) | Loss 1.1517(1.1597) | Error 0.0185(0.0176) Steps 458(461.82) | Grad Norm 0.6689(0.4037) | Total Time 10.00(10.00)\n",
      "Iter 1664 | Time 33.7533(34.1122) | Bit/dim 1.1281(1.1308) | Xent 0.0507(0.0574) | Loss 1.1534(1.1595) | Error 0.0168(0.0176) Steps 464(461.89) | Grad Norm 0.5534(0.4082) | Total Time 10.00(10.00)\n",
      "Iter 1665 | Time 32.9970(34.0788) | Bit/dim 1.1268(1.1307) | Xent 0.0550(0.0573) | Loss 1.1543(1.1594) | Error 0.0164(0.0176) Steps 464(461.95) | Grad Norm 0.6568(0.4156) | Total Time 10.00(10.00)\n",
      "Iter 1666 | Time 34.0285(34.0772) | Bit/dim 1.1277(1.1306) | Xent 0.0537(0.0572) | Loss 1.1546(1.1592) | Error 0.0171(0.0176) Steps 452(461.65) | Grad Norm 0.5229(0.4188) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 16.7563, Epoch Time 263.0247(269.9535), Bit/dim 1.1225(best: 1.1229), Xent 0.0285, Loss 1.1367, Error 0.0089(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1667 | Time 33.4000(34.0569) | Bit/dim 1.1308(1.1306) | Xent 0.0600(0.0573) | Loss 1.1609(1.1593) | Error 0.0186(0.0176) Steps 458(461.54) | Grad Norm 0.4485(0.4197) | Total Time 10.00(10.00)\n",
      "Iter 1668 | Time 33.2586(34.0330) | Bit/dim 1.1255(1.1305) | Xent 0.0497(0.0571) | Loss 1.1504(1.1590) | Error 0.0156(0.0175) Steps 464(461.62) | Grad Norm 0.2881(0.4158) | Total Time 10.00(10.00)\n",
      "Iter 1669 | Time 33.3870(34.0136) | Bit/dim 1.1305(1.1305) | Xent 0.0526(0.0570) | Loss 1.1568(1.1589) | Error 0.0172(0.0175) Steps 458(461.51) | Grad Norm 0.7695(0.4264) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 34.6488(34.0327) | Bit/dim 1.1322(1.1305) | Xent 0.0653(0.0572) | Loss 1.1648(1.1591) | Error 0.0201(0.0176) Steps 464(461.58) | Grad Norm 0.7947(0.4374) | Total Time 10.00(10.00)\n",
      "Iter 1671 | Time 33.2989(34.0106) | Bit/dim 1.1288(1.1305) | Xent 0.0545(0.0571) | Loss 1.1560(1.1590) | Error 0.0162(0.0176) Steps 464(461.66) | Grad Norm 0.3886(0.4360) | Total Time 10.00(10.00)\n",
      "Iter 1672 | Time 33.0720(33.9825) | Bit/dim 1.1265(1.1303) | Xent 0.0523(0.0570) | Loss 1.1527(1.1588) | Error 0.0155(0.0175) Steps 464(461.73) | Grad Norm 0.6532(0.4425) | Total Time 10.00(10.00)\n",
      "Iter 1673 | Time 33.2155(33.9595) | Bit/dim 1.1246(1.1302) | Xent 0.0558(0.0569) | Loss 1.1525(1.1586) | Error 0.0175(0.0175) Steps 464(461.79) | Grad Norm 0.2331(0.4362) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 17.0383, Epoch Time 263.4302(269.7578), Bit/dim 1.1223(best: 1.1225), Xent 0.0285, Loss 1.1365, Error 0.0100(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1674 | Time 33.6066(33.9489) | Bit/dim 1.1245(1.1300) | Xent 0.0581(0.0570) | Loss 1.1535(1.1585) | Error 0.0172(0.0175) Steps 464(461.86) | Grad Norm 0.6139(0.4415) | Total Time 10.00(10.00)\n",
      "Iter 1675 | Time 33.8042(33.9446) | Bit/dim 1.1259(1.1299) | Xent 0.0629(0.0572) | Loss 1.1573(1.1585) | Error 0.0174(0.0175) Steps 464(461.92) | Grad Norm 0.2026(0.4344) | Total Time 10.00(10.00)\n",
      "Iter 1676 | Time 35.4826(33.9907) | Bit/dim 1.1282(1.1298) | Xent 0.0454(0.0568) | Loss 1.1509(1.1582) | Error 0.0159(0.0174) Steps 464(461.99) | Grad Norm 0.7111(0.4427) | Total Time 10.00(10.00)\n",
      "Iter 1677 | Time 33.8385(33.9861) | Bit/dim 1.1309(1.1299) | Xent 0.0519(0.0567) | Loss 1.1568(1.1582) | Error 0.0178(0.0174) Steps 458(461.87) | Grad Norm 0.2494(0.4369) | Total Time 10.00(10.00)\n",
      "Iter 1678 | Time 34.0483(33.9880) | Bit/dim 1.1303(1.1299) | Xent 0.0546(0.0566) | Loss 1.1576(1.1582) | Error 0.0178(0.0175) Steps 464(461.93) | Grad Norm 0.7878(0.4474) | Total Time 10.00(10.00)\n",
      "Iter 1679 | Time 33.0734(33.9606) | Bit/dim 1.1277(1.1298) | Xent 0.0546(0.0565) | Loss 1.1550(1.1581) | Error 0.0165(0.0174) Steps 458(461.81) | Grad Norm 0.8120(0.4583) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 34.4144(33.9742) | Bit/dim 1.1237(1.1296) | Xent 0.0561(0.0565) | Loss 1.1518(1.1579) | Error 0.0181(0.0174) Steps 458(461.70) | Grad Norm 0.2588(0.4524) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 17.1139, Epoch Time 267.6737(269.6953), Bit/dim 1.1210(best: 1.1223), Xent 0.0266, Loss 1.1343, Error 0.0090(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1681 | Time 33.1585(33.9497) | Bit/dim 1.1316(1.1297) | Xent 0.0541(0.0565) | Loss 1.1586(1.1579) | Error 0.0179(0.0175) Steps 458(461.59) | Grad Norm 0.6442(0.4581) | Total Time 10.00(10.00)\n",
      "Iter 1682 | Time 35.2038(33.9873) | Bit/dim 1.1281(1.1296) | Xent 0.0576(0.0565) | Loss 1.1569(1.1579) | Error 0.0182(0.0175) Steps 464(461.66) | Grad Norm 0.2460(0.4517) | Total Time 10.00(10.00)\n",
      "Iter 1683 | Time 34.3698(33.9988) | Bit/dim 1.1283(1.1296) | Xent 0.0576(0.0565) | Loss 1.1570(1.1579) | Error 0.0161(0.0174) Steps 464(461.73) | Grad Norm 0.6594(0.4580) | Total Time 10.00(10.00)\n",
      "Iter 1684 | Time 34.8898(34.0255) | Bit/dim 1.1268(1.1295) | Xent 0.0628(0.0567) | Loss 1.1582(1.1579) | Error 0.0186(0.0175) Steps 464(461.80) | Grad Norm 0.2704(0.4523) | Total Time 10.00(10.00)\n",
      "Iter 1685 | Time 34.6516(34.0443) | Bit/dim 1.1243(1.1294) | Xent 0.0502(0.0565) | Loss 1.1494(1.1576) | Error 0.0169(0.0175) Steps 464(461.86) | Grad Norm 0.4109(0.4511) | Total Time 10.00(10.00)\n",
      "Iter 1686 | Time 33.8268(34.0378) | Bit/dim 1.1292(1.1293) | Xent 0.0555(0.0565) | Loss 1.1569(1.1576) | Error 0.0160(0.0174) Steps 458(461.75) | Grad Norm 0.2483(0.4450) | Total Time 10.00(10.00)\n",
      "Iter 1687 | Time 33.2596(34.0144) | Bit/dim 1.1249(1.1292) | Xent 0.0560(0.0565) | Loss 1.1530(1.1574) | Error 0.0179(0.0174) Steps 464(461.82) | Grad Norm 0.2167(0.4382) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 16.8667, Epoch Time 268.6101(269.6627), Bit/dim 1.1207(best: 1.1210), Xent 0.0297, Loss 1.1355, Error 0.0101(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1688 | Time 33.4636(33.9979) | Bit/dim 1.1263(1.1291) | Xent 0.0532(0.0564) | Loss 1.1528(1.1573) | Error 0.0168(0.0174) Steps 458(461.70) | Grad Norm 0.3404(0.4352) | Total Time 10.00(10.00)\n",
      "Iter 1689 | Time 33.5155(33.9834) | Bit/dim 1.1271(1.1291) | Xent 0.0569(0.0564) | Loss 1.1555(1.1573) | Error 0.0166(0.0174) Steps 464(461.77) | Grad Norm 0.2167(0.4287) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 35.2420(34.0212) | Bit/dim 1.1272(1.1290) | Xent 0.0524(0.0563) | Loss 1.1534(1.1571) | Error 0.0150(0.0173) Steps 464(461.84) | Grad Norm 0.2466(0.4232) | Total Time 10.00(10.00)\n",
      "Iter 1691 | Time 33.9649(34.0195) | Bit/dim 1.1273(1.1290) | Xent 0.0544(0.0562) | Loss 1.1545(1.1571) | Error 0.0178(0.0173) Steps 458(461.72) | Grad Norm 0.2092(0.4168) | Total Time 10.00(10.00)\n",
      "Iter 1692 | Time 35.2228(34.0556) | Bit/dim 1.1288(1.1290) | Xent 0.0597(0.0563) | Loss 1.1586(1.1571) | Error 0.0185(0.0174) Steps 464(461.79) | Grad Norm 0.2224(0.4110) | Total Time 10.00(10.00)\n",
      "Iter 1693 | Time 34.3637(34.0649) | Bit/dim 1.1264(1.1289) | Xent 0.0582(0.0564) | Loss 1.1555(1.1571) | Error 0.0184(0.0174) Steps 458(461.68) | Grad Norm 0.2392(0.4058) | Total Time 10.00(10.00)\n",
      "Iter 1694 | Time 33.6146(34.0513) | Bit/dim 1.1262(1.1288) | Xent 0.0565(0.0564) | Loss 1.1545(1.1570) | Error 0.0179(0.0174) Steps 458(461.57) | Grad Norm 0.2349(0.4007) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 17.2725, Epoch Time 268.8946(269.6397), Bit/dim 1.1209(best: 1.1207), Xent 0.0305, Loss 1.1362, Error 0.0106(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1695 | Time 34.2669(34.0578) | Bit/dim 1.1276(1.1288) | Xent 0.0624(0.0566) | Loss 1.1588(1.1570) | Error 0.0190(0.0175) Steps 458(461.46) | Grad Norm 0.3643(0.3996) | Total Time 10.00(10.00)\n",
      "Iter 1696 | Time 33.8524(34.0517) | Bit/dim 1.1298(1.1288) | Xent 0.0574(0.0566) | Loss 1.1585(1.1571) | Error 0.0189(0.0175) Steps 458(461.36) | Grad Norm 0.2607(0.3954) | Total Time 10.00(10.00)\n",
      "Iter 1697 | Time 33.3839(34.0316) | Bit/dim 1.1236(1.1286) | Xent 0.0560(0.0566) | Loss 1.1516(1.1569) | Error 0.0162(0.0175) Steps 458(461.26) | Grad Norm 0.5307(0.3995) | Total Time 10.00(10.00)\n",
      "Iter 1698 | Time 34.4223(34.0433) | Bit/dim 1.1263(1.1286) | Xent 0.0590(0.0566) | Loss 1.1558(1.1569) | Error 0.0201(0.0175) Steps 464(461.34) | Grad Norm 0.2468(0.3949) | Total Time 10.00(10.00)\n",
      "Iter 1699 | Time 33.8974(34.0390) | Bit/dim 1.1227(1.1284) | Xent 0.0548(0.0566) | Loss 1.1501(1.1567) | Error 0.0174(0.0175) Steps 464(461.42) | Grad Norm 0.3806(0.3945) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 34.5441(34.0541) | Bit/dim 1.1274(1.1284) | Xent 0.0512(0.0564) | Loss 1.1530(1.1566) | Error 0.0161(0.0175) Steps 464(461.50) | Grad Norm 0.2143(0.3891) | Total Time 10.00(10.00)\n",
      "Iter 1701 | Time 33.2726(34.0307) | Bit/dim 1.1267(1.1283) | Xent 0.0564(0.0564) | Loss 1.1549(1.1565) | Error 0.0180(0.0175) Steps 464(461.57) | Grad Norm 0.4209(0.3900) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 16.8303, Epoch Time 266.6845(269.5510), Bit/dim 1.1208(best: 1.1207), Xent 0.0277, Loss 1.1346, Error 0.0087(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1702 | Time 35.4575(34.0735) | Bit/dim 1.1261(1.1282) | Xent 0.0496(0.0562) | Loss 1.1509(1.1563) | Error 0.0160(0.0175) Steps 464(461.64) | Grad Norm 0.4448(0.3917) | Total Time 10.00(10.00)\n",
      "Iter 1703 | Time 34.6129(34.0897) | Bit/dim 1.1280(1.1282) | Xent 0.0614(0.0564) | Loss 1.1587(1.1564) | Error 0.0201(0.0175) Steps 464(461.71) | Grad Norm 0.2591(0.3877) | Total Time 10.00(10.00)\n",
      "Iter 1704 | Time 33.8843(34.0835) | Bit/dim 1.1296(1.1283) | Xent 0.0537(0.0563) | Loss 1.1565(1.1564) | Error 0.0176(0.0175) Steps 464(461.78) | Grad Norm 0.2398(0.3833) | Total Time 10.00(10.00)\n",
      "Iter 1705 | Time 33.9927(34.0808) | Bit/dim 1.1266(1.1282) | Xent 0.0587(0.0564) | Loss 1.1559(1.1564) | Error 0.0180(0.0176) Steps 464(461.85) | Grad Norm 0.4247(0.3845) | Total Time 10.00(10.00)\n",
      "Iter 1706 | Time 33.3862(34.0599) | Bit/dim 1.1238(1.1281) | Xent 0.0570(0.0564) | Loss 1.1523(1.1563) | Error 0.0174(0.0176) Steps 464(461.91) | Grad Norm 0.6858(0.3935) | Total Time 10.00(10.00)\n",
      "Iter 1707 | Time 32.9651(34.0271) | Bit/dim 1.1268(1.1281) | Xent 0.0713(0.0568) | Loss 1.1625(1.1565) | Error 0.0222(0.0177) Steps 464(461.98) | Grad Norm 0.4873(0.3963) | Total Time 10.00(10.00)\n",
      "Iter 1708 | Time 33.1241(34.0000) | Bit/dim 1.1227(1.1279) | Xent 0.0588(0.0569) | Loss 1.1521(1.1563) | Error 0.0178(0.0177) Steps 464(462.04) | Grad Norm 0.5279(0.4003) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 16.9671, Epoch Time 266.6714(269.4647), Bit/dim 1.1204(best: 1.1207), Xent 0.0278, Loss 1.1343, Error 0.0094(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1709 | Time 33.6979(33.9909) | Bit/dim 1.1262(1.1278) | Xent 0.0539(0.0568) | Loss 1.1532(1.1562) | Error 0.0176(0.0177) Steps 458(461.92) | Grad Norm 0.9593(0.4171) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 34.0017(33.9913) | Bit/dim 1.1270(1.1278) | Xent 0.0591(0.0569) | Loss 1.1565(1.1563) | Error 0.0198(0.0178) Steps 458(461.80) | Grad Norm 0.3455(0.4149) | Total Time 10.00(10.00)\n",
      "Iter 1711 | Time 34.9938(34.0213) | Bit/dim 1.1249(1.1277) | Xent 0.0553(0.0568) | Loss 1.1526(1.1561) | Error 0.0195(0.0178) Steps 464(461.86) | Grad Norm 0.7259(0.4242) | Total Time 10.00(10.00)\n",
      "Iter 1712 | Time 33.9086(34.0180) | Bit/dim 1.1309(1.1278) | Xent 0.0514(0.0567) | Loss 1.1566(1.1562) | Error 0.0158(0.0177) Steps 464(461.93) | Grad Norm 1.1660(0.4465) | Total Time 10.00(10.00)\n",
      "Iter 1713 | Time 33.3678(33.9984) | Bit/dim 1.1213(1.1276) | Xent 0.0541(0.0566) | Loss 1.1483(1.1559) | Error 0.0162(0.0177) Steps 464(461.99) | Grad Norm 0.5730(0.4503) | Total Time 10.00(10.00)\n",
      "Iter 1714 | Time 33.8800(33.9949) | Bit/dim 1.1252(1.1276) | Xent 0.0595(0.0567) | Loss 1.1550(1.1559) | Error 0.0181(0.0177) Steps 464(462.05) | Grad Norm 0.5481(0.4532) | Total Time 10.00(10.00)\n",
      "Iter 1715 | Time 33.4193(33.9776) | Bit/dim 1.1255(1.1275) | Xent 0.0547(0.0566) | Loss 1.1528(1.1558) | Error 0.0160(0.0177) Steps 464(462.11) | Grad Norm 0.6645(0.4596) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 17.0580, Epoch Time 266.6152(269.3792), Bit/dim 1.1195(best: 1.1204), Xent 0.0280, Loss 1.1335, Error 0.0093(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1716 | Time 33.2800(33.9567) | Bit/dim 1.1283(1.1275) | Xent 0.0556(0.0566) | Loss 1.1561(1.1558) | Error 0.0179(0.0177) Steps 464(462.17) | Grad Norm 0.2454(0.4531) | Total Time 10.00(10.00)\n",
      "Iter 1717 | Time 34.7150(33.9794) | Bit/dim 1.1237(1.1274) | Xent 0.0569(0.0566) | Loss 1.1521(1.1557) | Error 0.0174(0.0177) Steps 464(462.22) | Grad Norm 0.3569(0.4503) | Total Time 10.00(10.00)\n",
      "Iter 1718 | Time 34.3161(33.9895) | Bit/dim 1.1295(1.1275) | Xent 0.0537(0.0565) | Loss 1.1564(1.1557) | Error 0.0169(0.0176) Steps 458(462.09) | Grad Norm 0.3012(0.4458) | Total Time 10.00(10.00)\n",
      "Iter 1719 | Time 34.7494(34.0123) | Bit/dim 1.1295(1.1275) | Xent 0.0535(0.0564) | Loss 1.1563(1.1557) | Error 0.0161(0.0176) Steps 464(462.15) | Grad Norm 0.3707(0.4435) | Total Time 10.00(10.00)\n",
      "Iter 1720 | Time 33.5311(33.9979) | Bit/dim 1.1220(1.1274) | Xent 0.0541(0.0563) | Loss 1.1490(1.1555) | Error 0.0166(0.0176) Steps 464(462.21) | Grad Norm 0.2949(0.4391) | Total Time 10.00(10.00)\n",
      "Iter 1721 | Time 32.7608(33.9608) | Bit/dim 1.1190(1.1271) | Xent 0.0605(0.0565) | Loss 1.1493(1.1553) | Error 0.0205(0.0177) Steps 464(462.26) | Grad Norm 0.7386(0.4481) | Total Time 10.00(10.00)\n",
      "Iter 1722 | Time 34.6124(33.9803) | Bit/dim 1.1243(1.1270) | Xent 0.0564(0.0565) | Loss 1.1525(1.1553) | Error 0.0160(0.0176) Steps 464(462.31) | Grad Norm 0.7898(0.4583) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 17.1389, Epoch Time 267.7537(269.3304), Bit/dim 1.1196(best: 1.1195), Xent 0.0279, Loss 1.1335, Error 0.0095(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1723 | Time 34.8423(34.0062) | Bit/dim 1.1314(1.1272) | Xent 0.0536(0.0564) | Loss 1.1582(1.1554) | Error 0.0168(0.0176) Steps 464(462.36) | Grad Norm 0.5141(0.4600) | Total Time 10.00(10.00)\n",
      "Iter 1724 | Time 32.9770(33.9753) | Bit/dim 1.1230(1.1270) | Xent 0.0614(0.0565) | Loss 1.1536(1.1553) | Error 0.0185(0.0176) Steps 464(462.41) | Grad Norm 0.2929(0.4550) | Total Time 10.00(10.00)\n",
      "Iter 1725 | Time 33.0153(33.9465) | Bit/dim 1.1199(1.1268) | Xent 0.0514(0.0564) | Loss 1.1456(1.1550) | Error 0.0174(0.0176) Steps 464(462.46) | Grad Norm 0.2598(0.4491) | Total Time 10.00(10.00)\n",
      "Iter 1726 | Time 33.3473(33.9285) | Bit/dim 1.1283(1.1269) | Xent 0.0564(0.0564) | Loss 1.1565(1.1551) | Error 0.0186(0.0176) Steps 464(462.51) | Grad Norm 0.2150(0.4421) | Total Time 10.00(10.00)\n",
      "Iter 1727 | Time 34.2018(33.9367) | Bit/dim 1.1220(1.1267) | Xent 0.0532(0.0563) | Loss 1.1486(1.1549) | Error 0.0182(0.0176) Steps 464(462.55) | Grad Norm 0.3500(0.4393) | Total Time 10.00(10.00)\n",
      "Iter 1728 | Time 33.3617(33.9195) | Bit/dim 1.1228(1.1266) | Xent 0.0538(0.0562) | Loss 1.1497(1.1547) | Error 0.0162(0.0176) Steps 470(462.77) | Grad Norm 0.6473(0.4456) | Total Time 10.00(10.00)\n",
      "Iter 1729 | Time 33.2261(33.8987) | Bit/dim 1.1266(1.1266) | Xent 0.0559(0.0562) | Loss 1.1546(1.1547) | Error 0.0156(0.0175) Steps 470(462.99) | Grad Norm 0.5038(0.4473) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 16.9653, Epoch Time 264.1368(269.1746), Bit/dim 1.1187(best: 1.1195), Xent 0.0285, Loss 1.1329, Error 0.0099(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1730 | Time 32.7490(33.8642) | Bit/dim 1.1224(1.1265) | Xent 0.0506(0.0560) | Loss 1.1477(1.1545) | Error 0.0159(0.0175) Steps 464(463.02) | Grad Norm 0.1932(0.4397) | Total Time 10.00(10.00)\n",
      "Iter 1731 | Time 34.3909(33.8800) | Bit/dim 1.1247(1.1264) | Xent 0.0601(0.0562) | Loss 1.1547(1.1545) | Error 0.0179(0.0175) Steps 464(463.05) | Grad Norm 0.3607(0.4373) | Total Time 10.00(10.00)\n",
      "Iter 1732 | Time 34.7347(33.9056) | Bit/dim 1.1226(1.1263) | Xent 0.0636(0.0564) | Loss 1.1544(1.1545) | Error 0.0221(0.0176) Steps 464(463.08) | Grad Norm 0.2532(0.4318) | Total Time 10.00(10.00)\n",
      "Iter 1733 | Time 33.7426(33.9007) | Bit/dim 1.1225(1.1262) | Xent 0.0543(0.0563) | Loss 1.1497(1.1543) | Error 0.0162(0.0176) Steps 464(463.11) | Grad Norm 0.5741(0.4361) | Total Time 10.00(10.00)\n",
      "Iter 1734 | Time 34.9773(33.9330) | Bit/dim 1.1246(1.1261) | Xent 0.0598(0.0564) | Loss 1.1545(1.1544) | Error 0.0182(0.0176) Steps 464(463.13) | Grad Norm 1.0087(0.4532) | Total Time 10.00(10.00)\n",
      "Iter 1735 | Time 33.9958(33.9349) | Bit/dim 1.1260(1.1261) | Xent 0.0574(0.0564) | Loss 1.1547(1.1544) | Error 0.0181(0.0176) Steps 464(463.16) | Grad Norm 0.8358(0.4647) | Total Time 10.00(10.00)\n",
      "Iter 1736 | Time 34.4222(33.9495) | Bit/dim 1.1284(1.1262) | Xent 0.0552(0.0564) | Loss 1.1560(1.1544) | Error 0.0161(0.0176) Steps 464(463.19) | Grad Norm 0.2303(0.4577) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 16.7597, Epoch Time 268.1232(269.1431), Bit/dim 1.1188(best: 1.1187), Xent 0.0279, Loss 1.1327, Error 0.0099(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1737 | Time 33.8094(33.9453) | Bit/dim 1.1247(1.1262) | Xent 0.0532(0.0563) | Loss 1.1513(1.1543) | Error 0.0169(0.0176) Steps 458(463.03) | Grad Norm 0.7765(0.4673) | Total Time 10.00(10.00)\n",
      "Iter 1738 | Time 34.0427(33.9483) | Bit/dim 1.1231(1.1261) | Xent 0.0557(0.0563) | Loss 1.1510(1.1542) | Error 0.0180(0.0176) Steps 458(462.88) | Grad Norm 1.0973(0.4862) | Total Time 10.00(10.00)\n",
      "Iter 1739 | Time 34.8106(33.9741) | Bit/dim 1.1219(1.1259) | Xent 0.0516(0.0561) | Loss 1.1477(1.1540) | Error 0.0151(0.0175) Steps 464(462.91) | Grad Norm 0.6728(0.4918) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 33.4223(33.9576) | Bit/dim 1.1260(1.1260) | Xent 0.0525(0.0560) | Loss 1.1522(1.1540) | Error 0.0179(0.0175) Steps 464(462.94) | Grad Norm 0.2033(0.4831) | Total Time 10.00(10.00)\n",
      "Iter 1741 | Time 34.5451(33.9752) | Bit/dim 1.1232(1.1259) | Xent 0.0607(0.0562) | Loss 1.1535(1.1540) | Error 0.0180(0.0175) Steps 464(462.98) | Grad Norm 0.6303(0.4875) | Total Time 10.00(10.00)\n",
      "Iter 1742 | Time 33.7776(33.9693) | Bit/dim 1.1241(1.1258) | Xent 0.0543(0.0561) | Loss 1.1513(1.1539) | Error 0.0166(0.0175) Steps 464(463.01) | Grad Norm 0.8160(0.4974) | Total Time 10.00(10.00)\n",
      "Iter 1743 | Time 33.7784(33.9635) | Bit/dim 1.1260(1.1258) | Xent 0.0543(0.0561) | Loss 1.1532(1.1539) | Error 0.0174(0.0175) Steps 464(463.04) | Grad Norm 0.8208(0.5071) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 16.8878, Epoch Time 267.6545(269.0984), Bit/dim 1.1170(best: 1.1187), Xent 0.0281, Loss 1.1311, Error 0.0100(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1744 | Time 34.4839(33.9792) | Bit/dim 1.1231(1.1257) | Xent 0.0590(0.0562) | Loss 1.1526(1.1538) | Error 0.0169(0.0175) Steps 464(463.07) | Grad Norm 0.5248(0.5076) | Total Time 10.00(10.00)\n",
      "Iter 1745 | Time 34.4476(33.9932) | Bit/dim 1.1215(1.1256) | Xent 0.0463(0.0559) | Loss 1.1447(1.1535) | Error 0.0155(0.0174) Steps 464(463.09) | Grad Norm 0.2077(0.4986) | Total Time 10.00(10.00)\n",
      "Iter 1746 | Time 33.1231(33.9671) | Bit/dim 1.1261(1.1256) | Xent 0.0530(0.0558) | Loss 1.1526(1.1535) | Error 0.0164(0.0174) Steps 464(463.12) | Grad Norm 0.5619(0.5005) | Total Time 10.00(10.00)\n",
      "Iter 1747 | Time 32.8371(33.9332) | Bit/dim 1.1224(1.1255) | Xent 0.0603(0.0559) | Loss 1.1526(1.1535) | Error 0.0185(0.0174) Steps 458(462.97) | Grad Norm 0.7174(0.5070) | Total Time 10.00(10.00)\n",
      "Iter 1748 | Time 34.2809(33.9436) | Bit/dim 1.1215(1.1254) | Xent 0.0565(0.0559) | Loss 1.1497(1.1534) | Error 0.0202(0.0175) Steps 458(462.82) | Grad Norm 0.2903(0.5005) | Total Time 10.00(10.00)\n",
      "Iter 1749 | Time 34.0262(33.9461) | Bit/dim 1.1277(1.1255) | Xent 0.0501(0.0558) | Loss 1.1527(1.1534) | Error 0.0154(0.0174) Steps 464(462.85) | Grad Norm 0.5268(0.5013) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 33.5385(33.9339) | Bit/dim 1.1237(1.1254) | Xent 0.0555(0.0557) | Loss 1.1515(1.1533) | Error 0.0160(0.0174) Steps 464(462.89) | Grad Norm 0.7902(0.5100) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 16.9306, Epoch Time 265.9059(269.0026), Bit/dim 1.1177(best: 1.1170), Xent 0.0280, Loss 1.1317, Error 0.0091(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1751 | Time 33.5297(33.9218) | Bit/dim 1.1243(1.1254) | Xent 0.0563(0.0558) | Loss 1.1524(1.1533) | Error 0.0181(0.0174) Steps 464(462.92) | Grad Norm 0.5295(0.5106) | Total Time 10.00(10.00)\n",
      "Iter 1752 | Time 33.1072(33.8973) | Bit/dim 1.1229(1.1253) | Xent 0.0537(0.0557) | Loss 1.1497(1.1532) | Error 0.0180(0.0174) Steps 464(462.95) | Grad Norm 0.2364(0.5023) | Total Time 10.00(10.00)\n",
      "Iter 1753 | Time 33.9386(33.8986) | Bit/dim 1.1206(1.1252) | Xent 0.0473(0.0554) | Loss 1.1443(1.1529) | Error 0.0151(0.0174) Steps 464(462.99) | Grad Norm 0.2508(0.4948) | Total Time 10.00(10.00)\n",
      "Iter 1754 | Time 34.5563(33.9183) | Bit/dim 1.1208(1.1250) | Xent 0.0469(0.0552) | Loss 1.1443(1.1526) | Error 0.0138(0.0173) Steps 452(462.66) | Grad Norm 0.5609(0.4968) | Total Time 10.00(10.00)\n",
      "Iter 1755 | Time 34.0000(33.9207) | Bit/dim 1.1263(1.1251) | Xent 0.0580(0.0553) | Loss 1.1553(1.1527) | Error 0.0180(0.0173) Steps 458(462.52) | Grad Norm 0.8544(0.5075) | Total Time 10.00(10.00)\n",
      "Iter 1756 | Time 34.8806(33.9495) | Bit/dim 1.1271(1.1251) | Xent 0.0595(0.0554) | Loss 1.1569(1.1528) | Error 0.0185(0.0173) Steps 464(462.56) | Grad Norm 0.7768(0.5156) | Total Time 10.00(10.00)\n",
      "Iter 1757 | Time 33.5488(33.9375) | Bit/dim 1.1231(1.1251) | Xent 0.0602(0.0555) | Loss 1.1532(1.1529) | Error 0.0181(0.0173) Steps 464(462.60) | Grad Norm 0.2511(0.5076) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 17.0110, Epoch Time 266.8172(268.9371), Bit/dim 1.1169(best: 1.1170), Xent 0.0270, Loss 1.1304, Error 0.0087(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1758 | Time 34.6219(33.9580) | Bit/dim 1.1188(1.1249) | Xent 0.0576(0.0556) | Loss 1.1476(1.1527) | Error 0.0184(0.0174) Steps 458(462.47) | Grad Norm 0.5439(0.5087) | Total Time 10.00(10.00)\n",
      "Iter 1759 | Time 35.1781(33.9947) | Bit/dim 1.1222(1.1248) | Xent 0.0530(0.0555) | Loss 1.1488(1.1526) | Error 0.0160(0.0173) Steps 464(462.51) | Grad Norm 0.8280(0.5183) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 34.6451(34.0142) | Bit/dim 1.1264(1.1249) | Xent 0.0559(0.0555) | Loss 1.1543(1.1526) | Error 0.0168(0.0173) Steps 464(462.56) | Grad Norm 0.6335(0.5218) | Total Time 10.00(10.00)\n",
      "Iter 1761 | Time 34.1786(34.0191) | Bit/dim 1.1201(1.1247) | Xent 0.0541(0.0555) | Loss 1.1472(1.1525) | Error 0.0171(0.0173) Steps 464(462.60) | Grad Norm 0.4666(0.5201) | Total Time 10.00(10.00)\n",
      "Iter 1762 | Time 34.2236(34.0252) | Bit/dim 1.1196(1.1246) | Xent 0.0555(0.0555) | Loss 1.1473(1.1523) | Error 0.0176(0.0173) Steps 458(462.46) | Grad Norm 0.4808(0.5189) | Total Time 10.00(10.00)\n",
      "Iter 1763 | Time 34.0439(34.0258) | Bit/dim 1.1263(1.1246) | Xent 0.0575(0.0556) | Loss 1.1550(1.1524) | Error 0.0161(0.0173) Steps 464(462.51) | Grad Norm 0.5911(0.5211) | Total Time 10.00(10.00)\n",
      "Iter 1764 | Time 35.0501(34.0565) | Bit/dim 1.1271(1.1247) | Xent 0.0560(0.0556) | Loss 1.1551(1.1525) | Error 0.0184(0.0173) Steps 464(462.55) | Grad Norm 0.5726(0.5226) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 16.9912, Epoch Time 271.2324(269.0059), Bit/dim 1.1171(best: 1.1169), Xent 0.0279, Loss 1.1311, Error 0.0097(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1765 | Time 33.2924(34.0336) | Bit/dim 1.1231(1.1246) | Xent 0.0598(0.0557) | Loss 1.1530(1.1525) | Error 0.0179(0.0173) Steps 452(462.24) | Grad Norm 0.4066(0.5192) | Total Time 10.00(10.00)\n",
      "Iter 1766 | Time 32.6970(33.9935) | Bit/dim 1.1253(1.1247) | Xent 0.0582(0.0558) | Loss 1.1545(1.1526) | Error 0.0169(0.0173) Steps 458(462.11) | Grad Norm 0.2646(0.5115) | Total Time 10.00(10.00)\n",
      "Iter 1767 | Time 34.8919(34.0205) | Bit/dim 1.1207(1.1245) | Xent 0.0516(0.0557) | Loss 1.1465(1.1524) | Error 0.0162(0.0173) Steps 458(461.99) | Grad Norm 0.4734(0.5104) | Total Time 10.00(10.00)\n",
      "Iter 1768 | Time 36.4651(34.0938) | Bit/dim 1.1240(1.1245) | Xent 0.0629(0.0559) | Loss 1.1554(1.1525) | Error 0.0204(0.0174) Steps 452(461.69) | Grad Norm 0.9984(0.5250) | Total Time 10.00(10.00)\n",
      "Iter 1769 | Time 37.1844(34.1865) | Bit/dim 1.1174(1.1243) | Xent 0.0514(0.0557) | Loss 1.1431(1.1522) | Error 0.0182(0.0174) Steps 476(462.12) | Grad Norm 1.0227(0.5399) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 36.3477(34.2513) | Bit/dim 1.1200(1.1242) | Xent 0.0553(0.0557) | Loss 1.1477(1.1520) | Error 0.0175(0.0174) Steps 452(461.81) | Grad Norm 0.4352(0.5368) | Total Time 10.00(10.00)\n",
      "Iter 1771 | Time 37.5864(34.3514) | Bit/dim 1.1233(1.1242) | Xent 0.0534(0.0557) | Loss 1.1500(1.1520) | Error 0.0175(0.0174) Steps 458(461.70) | Grad Norm 0.2632(0.5286) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 16.6479, Epoch Time 277.4556(269.2594), Bit/dim 1.1168(best: 1.1169), Xent 0.0280, Loss 1.1308, Error 0.0091(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1772 | Time 38.2964(34.4697) | Bit/dim 1.1227(1.1241) | Xent 0.0598(0.0558) | Loss 1.1526(1.1520) | Error 0.0170(0.0174) Steps 458(461.59) | Grad Norm 0.3601(0.5235) | Total Time 10.00(10.00)\n",
      "Iter 1773 | Time 33.1901(34.4314) | Bit/dim 1.1226(1.1241) | Xent 0.0524(0.0557) | Loss 1.1488(1.1519) | Error 0.0165(0.0174) Steps 464(461.66) | Grad Norm 0.3722(0.5190) | Total Time 10.00(10.00)\n",
      "Iter 1774 | Time 34.1670(34.4234) | Bit/dim 1.1186(1.1239) | Xent 0.0560(0.0557) | Loss 1.1466(1.1517) | Error 0.0162(0.0173) Steps 464(461.73) | Grad Norm 0.2339(0.5104) | Total Time 10.00(10.00)\n",
      "Iter 1775 | Time 37.3742(34.5120) | Bit/dim 1.1244(1.1239) | Xent 0.0547(0.0557) | Loss 1.1518(1.1518) | Error 0.0189(0.0174) Steps 464(461.80) | Grad Norm 0.5004(0.5101) | Total Time 10.00(10.00)\n",
      "Iter 1776 | Time 36.1092(34.5599) | Bit/dim 1.1231(1.1239) | Xent 0.0519(0.0555) | Loss 1.1491(1.1517) | Error 0.0166(0.0174) Steps 464(461.86) | Grad Norm 0.7245(0.5166) | Total Time 10.00(10.00)\n",
      "Iter 1777 | Time 37.0385(34.6342) | Bit/dim 1.1197(1.1238) | Xent 0.0547(0.0555) | Loss 1.1471(1.1515) | Error 0.0164(0.0173) Steps 470(462.11) | Grad Norm 0.5026(0.5162) | Total Time 10.00(10.00)\n",
      "Iter 1778 | Time 36.9233(34.7029) | Bit/dim 1.1224(1.1237) | Xent 0.0581(0.0556) | Loss 1.1515(1.1515) | Error 0.0180(0.0174) Steps 458(461.98) | Grad Norm 0.2463(0.5081) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 16.8674, Epoch Time 282.1988(269.6476), Bit/dim 1.1170(best: 1.1168), Xent 0.0270, Loss 1.1305, Error 0.0088(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1779 | Time 36.5090(34.7571) | Bit/dim 1.1186(1.1236) | Xent 0.0550(0.0556) | Loss 1.1461(1.1514) | Error 0.0162(0.0173) Steps 458(461.86) | Grad Norm 0.4390(0.5060) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 37.3835(34.8359) | Bit/dim 1.1210(1.1235) | Xent 0.0565(0.0556) | Loss 1.1492(1.1513) | Error 0.0175(0.0173) Steps 452(461.57) | Grad Norm 1.1471(0.5252) | Total Time 10.00(10.00)\n",
      "Iter 1781 | Time 36.4026(34.8829) | Bit/dim 1.1285(1.1236) | Xent 0.0559(0.0556) | Loss 1.1564(1.1515) | Error 0.0165(0.0173) Steps 470(461.82) | Grad Norm 1.9442(0.5678) | Total Time 10.00(10.00)\n",
      "Iter 1782 | Time 33.5738(34.8436) | Bit/dim 1.1221(1.1236) | Xent 0.0510(0.0555) | Loss 1.1476(1.1513) | Error 0.0179(0.0173) Steps 452(461.53) | Grad Norm 2.6300(0.6297) | Total Time 10.00(10.00)\n",
      "Iter 1783 | Time 35.5378(34.8644) | Bit/dim 1.1274(1.1237) | Xent 0.0582(0.0556) | Loss 1.1565(1.1515) | Error 0.0189(0.0174) Steps 476(461.96) | Grad Norm 2.8405(0.6960) | Total Time 10.00(10.00)\n",
      "Iter 1784 | Time 37.8198(34.9531) | Bit/dim 1.1207(1.1236) | Xent 0.0504(0.0554) | Loss 1.1459(1.1513) | Error 0.0146(0.0173) Steps 452(461.66) | Grad Norm 2.2936(0.7439) | Total Time 10.00(10.00)\n",
      "Iter 1785 | Time 36.1876(34.9901) | Bit/dim 1.1189(1.1235) | Xent 0.0540(0.0554) | Loss 1.1459(1.1512) | Error 0.0158(0.0172) Steps 458(461.55) | Grad Norm 1.2468(0.7590) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 16.8034, Epoch Time 282.8088(270.0424), Bit/dim 1.1155(best: 1.1168), Xent 0.0278, Loss 1.1294, Error 0.0098(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1786 | Time 36.8653(35.0464) | Bit/dim 1.1179(1.1233) | Xent 0.0562(0.0554) | Loss 1.1460(1.1510) | Error 0.0168(0.0172) Steps 458(461.45) | Grad Norm 0.1513(0.7408) | Total Time 10.00(10.00)\n",
      "Iter 1787 | Time 33.9923(35.0148) | Bit/dim 1.1208(1.1232) | Xent 0.0597(0.0555) | Loss 1.1507(1.1510) | Error 0.0175(0.0172) Steps 452(461.16) | Grad Norm 1.3437(0.7589) | Total Time 10.00(10.00)\n",
      "Iter 1788 | Time 37.3280(35.0842) | Bit/dim 1.1228(1.1232) | Xent 0.0566(0.0556) | Loss 1.1511(1.1510) | Error 0.0178(0.0172) Steps 482(461.79) | Grad Norm 2.5716(0.8132) | Total Time 10.00(10.00)\n",
      "Iter 1789 | Time 33.4633(35.0355) | Bit/dim 1.1241(1.1233) | Xent 0.0538(0.0555) | Loss 1.1510(1.1510) | Error 0.0170(0.0172) Steps 458(461.67) | Grad Norm 3.2770(0.8871) | Total Time 10.00(10.00)\n",
      "Iter 1790 | Time 36.3400(35.0747) | Bit/dim 1.1219(1.1232) | Xent 0.0497(0.0553) | Loss 1.1468(1.1509) | Error 0.0174(0.0172) Steps 464(461.74) | Grad Norm 3.3486(0.9610) | Total Time 10.00(10.00)\n",
      "Iter 1791 | Time 35.2079(35.0787) | Bit/dim 1.1241(1.1232) | Xent 0.0588(0.0554) | Loss 1.1535(1.1510) | Error 0.0175(0.0173) Steps 452(461.45) | Grad Norm 2.3705(1.0033) | Total Time 10.00(10.00)\n",
      "Iter 1792 | Time 35.8990(35.1033) | Bit/dim 1.1228(1.1232) | Xent 0.0566(0.0555) | Loss 1.1510(1.1510) | Error 0.0201(0.0173) Steps 458(461.35) | Grad Norm 0.7863(0.9968) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 16.9771, Epoch Time 278.3646(270.2921), Bit/dim 1.1157(best: 1.1155), Xent 0.0280, Loss 1.1297, Error 0.0099(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1793 | Time 36.1795(35.1356) | Bit/dim 1.1215(1.1232) | Xent 0.0558(0.0555) | Loss 1.1494(1.1509) | Error 0.0179(0.0174) Steps 476(461.79) | Grad Norm 0.6875(0.9875) | Total Time 10.00(10.00)\n",
      "Iter 1794 | Time 38.3010(35.2305) | Bit/dim 1.1190(1.1230) | Xent 0.0541(0.0554) | Loss 1.1460(1.1508) | Error 0.0181(0.0174) Steps 476(462.21) | Grad Norm 1.8285(1.0127) | Total Time 10.00(10.00)\n",
      "Iter 1795 | Time 36.5420(35.2699) | Bit/dim 1.1200(1.1230) | Xent 0.0557(0.0554) | Loss 1.1479(1.1507) | Error 0.0166(0.0174) Steps 470(462.45) | Grad Norm 2.5714(1.0595) | Total Time 10.00(10.00)\n",
      "Iter 1796 | Time 34.2842(35.2403) | Bit/dim 1.1236(1.1230) | Xent 0.0542(0.0554) | Loss 1.1507(1.1507) | Error 0.0179(0.0174) Steps 458(462.31) | Grad Norm 3.0452(1.1191) | Total Time 10.00(10.00)\n",
      "Iter 1797 | Time 36.5387(35.2792) | Bit/dim 1.1264(1.1231) | Xent 0.0587(0.0555) | Loss 1.1557(1.1508) | Error 0.0192(0.0174) Steps 452(462.00) | Grad Norm 3.1332(1.1795) | Total Time 10.00(10.00)\n",
      "Iter 1798 | Time 34.8313(35.2658) | Bit/dim 1.1232(1.1231) | Xent 0.0590(0.0556) | Loss 1.1527(1.1509) | Error 0.0189(0.0175) Steps 458(461.88) | Grad Norm 2.5979(1.2220) | Total Time 10.00(10.00)\n",
      "Iter 1799 | Time 36.4181(35.3004) | Bit/dim 1.1254(1.1231) | Xent 0.0582(0.0557) | Loss 1.1545(1.1510) | Error 0.0179(0.0175) Steps 476(462.31) | Grad Norm 2.1217(1.2490) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 16.9382, Epoch Time 282.3203(270.6529), Bit/dim 1.1159(best: 1.1155), Xent 0.0259, Loss 1.1289, Error 0.0086(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1800 | Time 34.0309(35.2623) | Bit/dim 1.1252(1.1232) | Xent 0.0579(0.0558) | Loss 1.1542(1.1511) | Error 0.0190(0.0175) Steps 464(462.36) | Grad Norm 1.7420(1.2638) | Total Time 10.00(10.00)\n",
      "Iter 1801 | Time 37.6240(35.3331) | Bit/dim 1.1209(1.1231) | Xent 0.0531(0.0557) | Loss 1.1475(1.1510) | Error 0.0164(0.0175) Steps 476(462.77) | Grad Norm 1.4275(1.2687) | Total Time 10.00(10.00)\n",
      "Iter 1802 | Time 36.3976(35.3651) | Bit/dim 1.1217(1.1231) | Xent 0.0579(0.0557) | Loss 1.1506(1.1510) | Error 0.0192(0.0175) Steps 464(462.81) | Grad Norm 1.5357(1.2767) | Total Time 10.00(10.00)\n",
      "Iter 1803 | Time 35.5667(35.3711) | Bit/dim 1.1249(1.1232) | Xent 0.0507(0.0556) | Loss 1.1502(1.1509) | Error 0.0184(0.0176) Steps 458(462.66) | Grad Norm 1.9964(1.2983) | Total Time 10.00(10.00)\n",
      "Iter 1804 | Time 37.4892(35.4347) | Bit/dim 1.1162(1.1229) | Xent 0.0594(0.0557) | Loss 1.1459(1.1508) | Error 0.0189(0.0176) Steps 458(462.52) | Grad Norm 2.5545(1.3360) | Total Time 10.00(10.00)\n",
      "Iter 1805 | Time 35.7740(35.4448) | Bit/dim 1.1251(1.1230) | Xent 0.0531(0.0556) | Loss 1.1517(1.1508) | Error 0.0172(0.0176) Steps 476(462.93) | Grad Norm 3.0465(1.3873) | Total Time 10.00(10.00)\n",
      "Iter 1806 | Time 38.1156(35.5250) | Bit/dim 1.1214(1.1230) | Xent 0.0621(0.0558) | Loss 1.1525(1.1509) | Error 0.0200(0.0177) Steps 464(462.96) | Grad Norm 3.1085(1.4390) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 16.9840, Epoch Time 284.9147(271.0808), Bit/dim 1.1177(best: 1.1155), Xent 0.0283, Loss 1.1319, Error 0.0095(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1807 | Time 36.2835(35.5477) | Bit/dim 1.1231(1.1230) | Xent 0.0524(0.0557) | Loss 1.1494(1.1508) | Error 0.0160(0.0176) Steps 458(462.81) | Grad Norm 2.8880(1.4824) | Total Time 10.00(10.00)\n",
      "Iter 1808 | Time 33.1707(35.4764) | Bit/dim 1.1226(1.1230) | Xent 0.0496(0.0555) | Loss 1.1474(1.1507) | Error 0.0152(0.0175) Steps 470(463.02) | Grad Norm 2.7132(1.5193) | Total Time 10.00(10.00)\n",
      "Iter 1809 | Time 36.1097(35.4954) | Bit/dim 1.1252(1.1230) | Xent 0.0617(0.0557) | Loss 1.1560(1.1509) | Error 0.0198(0.0176) Steps 482(463.59) | Grad Norm 2.5893(1.5514) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 35.3832(35.4920) | Bit/dim 1.1243(1.1231) | Xent 0.0631(0.0559) | Loss 1.1558(1.1510) | Error 0.0191(0.0177) Steps 464(463.61) | Grad Norm 2.9090(1.5922) | Total Time 10.00(10.00)\n",
      "Iter 1811 | Time 37.1748(35.5425) | Bit/dim 1.1219(1.1230) | Xent 0.0457(0.0556) | Loss 1.1447(1.1508) | Error 0.0145(0.0176) Steps 464(463.62) | Grad Norm 3.3367(1.6445) | Total Time 10.00(10.00)\n",
      "Iter 1812 | Time 34.1838(35.5018) | Bit/dim 1.1190(1.1229) | Xent 0.0579(0.0557) | Loss 1.1479(1.1508) | Error 0.0169(0.0175) Steps 470(463.81) | Grad Norm 3.6680(1.7052) | Total Time 10.00(10.00)\n",
      "Iter 1813 | Time 37.6426(35.5660) | Bit/dim 1.1258(1.1230) | Xent 0.0513(0.0556) | Loss 1.1515(1.1508) | Error 0.0165(0.0175) Steps 476(464.18) | Grad Norm 4.0000(1.7741) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 17.6643, Epoch Time 279.9598(271.3472), Bit/dim 1.1165(best: 1.1155), Xent 0.0279, Loss 1.1304, Error 0.0095(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1814 | Time 33.6684(35.5091) | Bit/dim 1.1213(1.1229) | Xent 0.0479(0.0553) | Loss 1.1453(1.1506) | Error 0.0142(0.0174) Steps 458(463.99) | Grad Norm 4.0619(1.8427) | Total Time 10.00(10.00)\n",
      "Iter 1815 | Time 36.4063(35.5360) | Bit/dim 1.1255(1.1230) | Xent 0.0595(0.0555) | Loss 1.1553(1.1507) | Error 0.0191(0.0175) Steps 464(463.99) | Grad Norm 3.4263(1.8902) | Total Time 10.00(10.00)\n",
      "Iter 1816 | Time 33.9868(35.4895) | Bit/dim 1.1200(1.1229) | Xent 0.0576(0.0555) | Loss 1.1488(1.1507) | Error 0.0185(0.0175) Steps 458(463.81) | Grad Norm 1.3818(1.8750) | Total Time 10.00(10.00)\n",
      "Iter 1817 | Time 35.4235(35.4875) | Bit/dim 1.1210(1.1229) | Xent 0.0521(0.0554) | Loss 1.1470(1.1506) | Error 0.0172(0.0175) Steps 464(463.82) | Grad Norm 1.2406(1.8559) | Total Time 10.00(10.00)\n",
      "Iter 1818 | Time 35.4039(35.4850) | Bit/dim 1.1219(1.1228) | Xent 0.0520(0.0553) | Loss 1.1479(1.1505) | Error 0.0156(0.0174) Steps 452(463.46) | Grad Norm 3.1163(1.8937) | Total Time 10.00(10.00)\n",
      "Iter 1819 | Time 35.2784(35.4788) | Bit/dim 1.1220(1.1228) | Xent 0.0600(0.0555) | Loss 1.1520(1.1505) | Error 0.0175(0.0174) Steps 470(463.66) | Grad Norm 4.8144(1.9814) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 35.8951(35.4913) | Bit/dim 1.1309(1.1231) | Xent 0.0519(0.0554) | Loss 1.1568(1.1507) | Error 0.0160(0.0174) Steps 470(463.85) | Grad Norm 5.5799(2.0893) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 16.8881, Epoch Time 275.0849(271.4593), Bit/dim 1.1152(best: 1.1155), Xent 0.0273, Loss 1.1289, Error 0.0093(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1821 | Time 33.7720(35.4397) | Bit/dim 1.1211(1.1230) | Xent 0.0592(0.0555) | Loss 1.1507(1.1507) | Error 0.0192(0.0174) Steps 470(464.03) | Grad Norm 4.4456(2.1600) | Total Time 10.00(10.00)\n",
      "Iter 1822 | Time 36.6557(35.4762) | Bit/dim 1.1248(1.1231) | Xent 0.0531(0.0554) | Loss 1.1514(1.1508) | Error 0.0161(0.0174) Steps 476(464.39) | Grad Norm 2.6269(2.1740) | Total Time 10.00(10.00)\n",
      "Iter 1823 | Time 36.6097(35.5102) | Bit/dim 1.1186(1.1229) | Xent 0.0587(0.0555) | Loss 1.1479(1.1507) | Error 0.0189(0.0175) Steps 458(464.20) | Grad Norm 0.9065(2.1360) | Total Time 10.00(10.00)\n",
      "Iter 1824 | Time 36.2233(35.5316) | Bit/dim 1.1191(1.1228) | Xent 0.0570(0.0555) | Loss 1.1476(1.1506) | Error 0.0169(0.0174) Steps 476(464.55) | Grad Norm 0.7963(2.0958) | Total Time 10.00(10.00)\n",
      "Iter 1825 | Time 36.3832(35.5572) | Bit/dim 1.1205(1.1227) | Xent 0.0568(0.0556) | Loss 1.1489(1.1505) | Error 0.0160(0.0174) Steps 452(464.18) | Grad Norm 2.0818(2.0954) | Total Time 10.00(10.00)\n",
      "Iter 1826 | Time 34.4828(35.5249) | Bit/dim 1.1229(1.1227) | Xent 0.0528(0.0555) | Loss 1.1493(1.1505) | Error 0.0158(0.0173) Steps 464(464.17) | Grad Norm 3.4084(2.1348) | Total Time 10.00(10.00)\n",
      "Iter 1827 | Time 36.2312(35.5461) | Bit/dim 1.1252(1.1228) | Xent 0.0488(0.0553) | Loss 1.1496(1.1505) | Error 0.0155(0.0173) Steps 458(463.99) | Grad Norm 4.5012(2.2058) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 17.2554, Epoch Time 279.9152(271.7130), Bit/dim 1.1154(best: 1.1152), Xent 0.0295, Loss 1.1302, Error 0.0098(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1828 | Time 35.2649(35.5377) | Bit/dim 1.1213(1.1228) | Xent 0.0590(0.0554) | Loss 1.1508(1.1505) | Error 0.0178(0.0173) Steps 464(463.99) | Grad Norm 4.7446(2.2819) | Total Time 10.00(10.00)\n",
      "Iter 1829 | Time 35.7375(35.5437) | Bit/dim 1.1231(1.1228) | Xent 0.0575(0.0555) | Loss 1.1518(1.1505) | Error 0.0192(0.0174) Steps 458(463.81) | Grad Norm 4.1972(2.3394) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 34.6932(35.5182) | Bit/dim 1.1228(1.1228) | Xent 0.0576(0.0555) | Loss 1.1516(1.1505) | Error 0.0181(0.0174) Steps 458(463.63) | Grad Norm 2.7701(2.3523) | Total Time 10.00(10.00)\n",
      "Iter 1831 | Time 35.8090(35.5269) | Bit/dim 1.1217(1.1227) | Xent 0.0553(0.0555) | Loss 1.1493(1.1505) | Error 0.0165(0.0174) Steps 458(463.46) | Grad Norm 1.1617(2.3166) | Total Time 10.00(10.00)\n",
      "Iter 1832 | Time 33.7670(35.4741) | Bit/dim 1.1202(1.1227) | Xent 0.0518(0.0554) | Loss 1.1460(1.1504) | Error 0.0154(0.0173) Steps 458(463.30) | Grad Norm 0.2056(2.2533) | Total Time 10.00(10.00)\n",
      "Iter 1833 | Time 36.8541(35.5155) | Bit/dim 1.1209(1.1226) | Xent 0.0503(0.0553) | Loss 1.1460(1.1502) | Error 0.0159(0.0173) Steps 452(462.96) | Grad Norm 1.1354(2.2197) | Total Time 10.00(10.00)\n",
      "Iter 1834 | Time 36.3055(35.5392) | Bit/dim 1.1194(1.1225) | Xent 0.0505(0.0551) | Loss 1.1446(1.1501) | Error 0.0161(0.0172) Steps 476(463.35) | Grad Norm 2.3561(2.2238) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 17.0114, Epoch Time 277.6641(271.8915), Bit/dim 1.1146(best: 1.1152), Xent 0.0304, Loss 1.1299, Error 0.0108(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1835 | Time 37.6216(35.6017) | Bit/dim 1.1263(1.1226) | Xent 0.0609(0.0553) | Loss 1.1568(1.1503) | Error 0.0196(0.0173) Steps 494(464.27) | Grad Norm 3.9044(2.2742) | Total Time 10.00(10.00)\n",
      "Iter 1836 | Time 35.9266(35.6114) | Bit/dim 1.1249(1.1227) | Xent 0.0496(0.0551) | Loss 1.1496(1.1503) | Error 0.0150(0.0172) Steps 464(464.26) | Grad Norm 5.3862(2.3676) | Total Time 10.00(10.00)\n",
      "Iter 1837 | Time 35.9294(35.6209) | Bit/dim 1.1220(1.1227) | Xent 0.0625(0.0553) | Loss 1.1533(1.1503) | Error 0.0195(0.0173) Steps 470(464.44) | Grad Norm 6.1870(2.4822) | Total Time 10.00(10.00)\n",
      "Iter 1838 | Time 35.9468(35.6307) | Bit/dim 1.1286(1.1229) | Xent 0.0535(0.0553) | Loss 1.1553(1.1505) | Error 0.0164(0.0173) Steps 452(464.06) | Grad Norm 6.0783(2.5901) | Total Time 10.00(10.00)\n",
      "Iter 1839 | Time 33.5936(35.5696) | Bit/dim 1.1238(1.1229) | Xent 0.0579(0.0554) | Loss 1.1527(1.1506) | Error 0.0178(0.0173) Steps 458(463.88) | Grad Norm 3.7932(2.6261) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 37.7981(35.6365) | Bit/dim 1.1157(1.1227) | Xent 0.0486(0.0552) | Loss 1.1400(1.1502) | Error 0.0171(0.0173) Steps 476(464.24) | Grad Norm 1.0739(2.5796) | Total Time 10.00(10.00)\n",
      "Iter 1841 | Time 36.2173(35.6539) | Bit/dim 1.1220(1.1226) | Xent 0.0515(0.0551) | Loss 1.1477(1.1502) | Error 0.0161(0.0172) Steps 476(464.60) | Grad Norm 0.7615(2.5250) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 17.0206, Epoch Time 282.5138(272.2102), Bit/dim 1.1139(best: 1.1146), Xent 0.0286, Loss 1.1283, Error 0.0095(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1842 | Time 35.8190(35.6588) | Bit/dim 1.1251(1.1227) | Xent 0.0607(0.0552) | Loss 1.1554(1.1503) | Error 0.0176(0.0173) Steps 458(464.40) | Grad Norm 2.4143(2.5217) | Total Time 10.00(10.00)\n",
      "Iter 1843 | Time 36.9804(35.6985) | Bit/dim 1.1220(1.1227) | Xent 0.0490(0.0550) | Loss 1.1465(1.1502) | Error 0.0161(0.0172) Steps 476(464.75) | Grad Norm 3.8110(2.5604) | Total Time 10.00(10.00)\n",
      "Iter 1844 | Time 35.6263(35.6963) | Bit/dim 1.1223(1.1227) | Xent 0.0496(0.0549) | Loss 1.1471(1.1501) | Error 0.0169(0.0172) Steps 470(464.90) | Grad Norm 4.8174(2.6281) | Total Time 10.00(10.00)\n",
      "Iter 1845 | Time 37.1054(35.7386) | Bit/dim 1.1251(1.1228) | Xent 0.0600(0.0550) | Loss 1.1551(1.1503) | Error 0.0175(0.0172) Steps 464(464.88) | Grad Norm 5.1537(2.7039) | Total Time 10.00(10.00)\n",
      "Iter 1846 | Time 34.6521(35.7060) | Bit/dim 1.1248(1.1228) | Xent 0.0583(0.0551) | Loss 1.1540(1.1504) | Error 0.0188(0.0173) Steps 470(465.03) | Grad Norm 4.4768(2.7571) | Total Time 10.00(10.00)\n",
      "Iter 1847 | Time 37.6748(35.7651) | Bit/dim 1.1194(1.1227) | Xent 0.0567(0.0552) | Loss 1.1478(1.1503) | Error 0.0168(0.0172) Steps 470(465.18) | Grad Norm 3.5239(2.7801) | Total Time 10.00(10.00)\n",
      "Iter 1848 | Time 37.9839(35.8316) | Bit/dim 1.1165(1.1225) | Xent 0.0579(0.0553) | Loss 1.1454(1.1502) | Error 0.0172(0.0172) Steps 452(464.79) | Grad Norm 2.4221(2.7693) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 16.9511, Epoch Time 285.3818(272.6053), Bit/dim 1.1150(best: 1.1139), Xent 0.0302, Loss 1.1301, Error 0.0097(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1849 | Time 35.4703(35.8208) | Bit/dim 1.1202(1.1225) | Xent 0.0573(0.0553) | Loss 1.1488(1.1501) | Error 0.0184(0.0173) Steps 452(464.40) | Grad Norm 1.4261(2.7290) | Total Time 10.00(10.00)\n",
      "Iter 1850 | Time 37.6062(35.8744) | Bit/dim 1.1258(1.1226) | Xent 0.0539(0.0553) | Loss 1.1527(1.1502) | Error 0.0169(0.0173) Steps 452(464.03) | Grad Norm 1.0498(2.6787) | Total Time 10.00(10.00)\n",
      "Iter 1851 | Time 37.1840(35.9136) | Bit/dim 1.1176(1.1224) | Xent 0.0605(0.0554) | Loss 1.1479(1.1501) | Error 0.0190(0.0173) Steps 482(464.57) | Grad Norm 1.1708(2.6334) | Total Time 10.00(10.00)\n",
      "Iter 1852 | Time 36.6259(35.9350) | Bit/dim 1.1179(1.1223) | Xent 0.0592(0.0555) | Loss 1.1475(1.1500) | Error 0.0181(0.0173) Steps 464(464.55) | Grad Norm 1.4507(2.5979) | Total Time 10.00(10.00)\n",
      "Iter 1853 | Time 35.6871(35.9276) | Bit/dim 1.1180(1.1221) | Xent 0.0541(0.0555) | Loss 1.1450(1.1499) | Error 0.0160(0.0173) Steps 458(464.35) | Grad Norm 1.8900(2.5767) | Total Time 10.00(10.00)\n",
      "Iter 1854 | Time 34.2415(35.8770) | Bit/dim 1.1143(1.1219) | Xent 0.0503(0.0553) | Loss 1.1395(1.1496) | Error 0.0165(0.0173) Steps 458(464.16) | Grad Norm 2.2485(2.5669) | Total Time 10.00(10.00)\n",
      "Iter 1855 | Time 36.7935(35.9045) | Bit/dim 1.1220(1.1219) | Xent 0.0528(0.0553) | Loss 1.1484(1.1495) | Error 0.0148(0.0172) Steps 476(464.52) | Grad Norm 2.7720(2.5730) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 17.2773, Epoch Time 283.1006(272.9202), Bit/dim 1.1134(best: 1.1139), Xent 0.0293, Loss 1.1281, Error 0.0101(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1856 | Time 33.5428(35.8336) | Bit/dim 1.1205(1.1219) | Xent 0.0540(0.0552) | Loss 1.1475(1.1495) | Error 0.0176(0.0172) Steps 458(464.32) | Grad Norm 4.0825(2.6183) | Total Time 10.00(10.00)\n",
      "Iter 1857 | Time 35.6289(35.8275) | Bit/dim 1.1262(1.1220) | Xent 0.0507(0.0551) | Loss 1.1515(1.1495) | Error 0.0146(0.0171) Steps 464(464.31) | Grad Norm 5.1623(2.6946) | Total Time 10.00(10.00)\n",
      "Iter 1858 | Time 34.9654(35.8016) | Bit/dim 1.1228(1.1220) | Xent 0.0542(0.0551) | Loss 1.1499(1.1496) | Error 0.0175(0.0172) Steps 452(463.94) | Grad Norm 5.2995(2.7728) | Total Time 10.00(10.00)\n",
      "Iter 1859 | Time 38.0312(35.8685) | Bit/dim 1.1301(1.1223) | Xent 0.0566(0.0551) | Loss 1.1584(1.1498) | Error 0.0186(0.0172) Steps 470(464.13) | Grad Norm 5.0711(2.8417) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 35.0496(35.8439) | Bit/dim 1.1198(1.1222) | Xent 0.0606(0.0553) | Loss 1.1501(1.1498) | Error 0.0195(0.0173) Steps 458(463.94) | Grad Norm 4.6056(2.8946) | Total Time 10.00(10.00)\n",
      "Iter 1861 | Time 37.2400(35.8858) | Bit/dim 1.1229(1.1222) | Xent 0.0567(0.0553) | Loss 1.1512(1.1499) | Error 0.0170(0.0173) Steps 476(464.30) | Grad Norm 4.0288(2.9287) | Total Time 10.00(10.00)\n",
      "Iter 1862 | Time 35.2918(35.8680) | Bit/dim 1.1147(1.1220) | Xent 0.0525(0.0552) | Loss 1.1409(1.1496) | Error 0.0144(0.0172) Steps 476(464.66) | Grad Norm 3.3138(2.9402) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 16.8940, Epoch Time 279.1457(273.1069), Bit/dim 1.1148(best: 1.1134), Xent 0.0281, Loss 1.1288, Error 0.0102(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1863 | Time 37.8484(35.9274) | Bit/dim 1.1127(1.1217) | Xent 0.0555(0.0552) | Loss 1.1404(1.1493) | Error 0.0170(0.0172) Steps 470(464.82) | Grad Norm 2.5436(2.9283) | Total Time 10.00(10.00)\n",
      "Iter 1864 | Time 38.6504(36.0091) | Bit/dim 1.1175(1.1216) | Xent 0.0570(0.0553) | Loss 1.1460(1.1492) | Error 0.0175(0.0172) Steps 458(464.61) | Grad Norm 1.7855(2.8940) | Total Time 10.00(10.00)\n",
      "Iter 1865 | Time 37.6687(36.0589) | Bit/dim 1.1202(1.1215) | Xent 0.0579(0.0554) | Loss 1.1492(1.1492) | Error 0.0194(0.0172) Steps 458(464.41) | Grad Norm 0.7654(2.8302) | Total Time 10.00(10.00)\n",
      "Iter 1866 | Time 37.2619(36.0950) | Bit/dim 1.1188(1.1215) | Xent 0.0496(0.0552) | Loss 1.1436(1.1491) | Error 0.0145(0.0172) Steps 452(464.04) | Grad Norm 0.3390(2.7554) | Total Time 10.00(10.00)\n",
      "Iter 1867 | Time 36.3303(36.1020) | Bit/dim 1.1209(1.1214) | Xent 0.0566(0.0552) | Loss 1.1492(1.1491) | Error 0.0180(0.0172) Steps 458(463.86) | Grad Norm 1.3086(2.7120) | Total Time 10.00(10.00)\n",
      "Iter 1868 | Time 36.9175(36.1265) | Bit/dim 1.1177(1.1213) | Xent 0.0487(0.0550) | Loss 1.1420(1.1489) | Error 0.0152(0.0171) Steps 452(463.50) | Grad Norm 2.4529(2.7043) | Total Time 10.00(10.00)\n",
      "Iter 1869 | Time 34.4272(36.0755) | Bit/dim 1.1217(1.1213) | Xent 0.0589(0.0552) | Loss 1.1511(1.1489) | Error 0.0179(0.0171) Steps 458(463.34) | Grad Norm 4.0821(2.7456) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 16.8580, Epoch Time 288.3944(273.5656), Bit/dim 1.1200(best: 1.1134), Xent 0.0285, Loss 1.1342, Error 0.0094(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1870 | Time 36.2838(36.0818) | Bit/dim 1.1259(1.1215) | Xent 0.0484(0.0550) | Loss 1.1502(1.1490) | Error 0.0159(0.0171) Steps 476(463.72) | Grad Norm 5.4371(2.8263) | Total Time 10.00(10.00)\n",
      "Iter 1871 | Time 35.4788(36.0637) | Bit/dim 1.1184(1.1214) | Xent 0.0560(0.0550) | Loss 1.1464(1.1489) | Error 0.0176(0.0171) Steps 470(463.91) | Grad Norm 5.8234(2.9162) | Total Time 10.00(10.00)\n",
      "Iter 1872 | Time 35.8682(36.0578) | Bit/dim 1.1266(1.1215) | Xent 0.0579(0.0551) | Loss 1.1555(1.1491) | Error 0.0172(0.0171) Steps 458(463.73) | Grad Norm 5.5885(2.9964) | Total Time 10.00(10.00)\n",
      "Iter 1873 | Time 33.3380(35.9762) | Bit/dim 1.1223(1.1216) | Xent 0.0548(0.0551) | Loss 1.1497(1.1491) | Error 0.0156(0.0171) Steps 458(463.56) | Grad Norm 4.0867(3.0291) | Total Time 10.00(10.00)\n",
      "Iter 1874 | Time 36.9500(36.0054) | Bit/dim 1.1182(1.1215) | Xent 0.0531(0.0550) | Loss 1.1448(1.1490) | Error 0.0150(0.0170) Steps 476(463.93) | Grad Norm 2.4038(3.0104) | Total Time 10.00(10.00)\n",
      "Iter 1875 | Time 38.0829(36.0678) | Bit/dim 1.1191(1.1214) | Xent 0.0611(0.0552) | Loss 1.1496(1.1490) | Error 0.0181(0.0171) Steps 488(464.65) | Grad Norm 1.3523(2.9606) | Total Time 10.00(10.00)\n",
      "Iter 1876 | Time 37.8033(36.1198) | Bit/dim 1.1182(1.1213) | Xent 0.0565(0.0552) | Loss 1.1464(1.1489) | Error 0.0176(0.0171) Steps 458(464.45) | Grad Norm 0.5212(2.8874) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 16.9394, Epoch Time 282.8551(273.8443), Bit/dim 1.1120(best: 1.1134), Xent 0.0260, Loss 1.1250, Error 0.0088(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1877 | Time 37.4230(36.1589) | Bit/dim 1.1197(1.1213) | Xent 0.0555(0.0552) | Loss 1.1474(1.1489) | Error 0.0154(0.0170) Steps 476(464.80) | Grad Norm 0.3537(2.8114) | Total Time 10.00(10.00)\n",
      "Iter 1878 | Time 37.5374(36.2003) | Bit/dim 1.1217(1.1213) | Xent 0.0614(0.0554) | Loss 1.1524(1.1490) | Error 0.0184(0.0171) Steps 482(465.32) | Grad Norm 0.9656(2.7561) | Total Time 10.00(10.00)\n",
      "Iter 1879 | Time 38.1879(36.2599) | Bit/dim 1.1125(1.1210) | Xent 0.0532(0.0554) | Loss 1.1392(1.1487) | Error 0.0149(0.0170) Steps 482(465.82) | Grad Norm 1.7130(2.7248) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 34.5267(36.2079) | Bit/dim 1.1160(1.1209) | Xent 0.0554(0.0554) | Loss 1.1437(1.1485) | Error 0.0171(0.0170) Steps 470(465.94) | Grad Norm 3.2822(2.7415) | Total Time 10.00(10.00)\n",
      "Iter 1881 | Time 37.5104(36.2470) | Bit/dim 1.1224(1.1209) | Xent 0.0503(0.0552) | Loss 1.1475(1.1485) | Error 0.0162(0.0170) Steps 482(466.42) | Grad Norm 4.9715(2.8084) | Total Time 10.00(10.00)\n",
      "Iter 1882 | Time 34.7387(36.2017) | Bit/dim 1.1242(1.1210) | Xent 0.0577(0.0553) | Loss 1.1531(1.1486) | Error 0.0162(0.0170) Steps 470(466.53) | Grad Norm 7.0492(2.9356) | Total Time 10.00(10.00)\n",
      "Iter 1883 | Time 36.9825(36.2252) | Bit/dim 1.1376(1.1215) | Xent 0.0575(0.0554) | Loss 1.1663(1.1492) | Error 0.0195(0.0170) Steps 458(466.27) | Grad Norm 7.6487(3.0770) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 17.3897, Epoch Time 286.7008(274.2300), Bit/dim 1.1130(best: 1.1120), Xent 0.0316, Loss 1.1287, Error 0.0105(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1884 | Time 34.8429(36.1837) | Bit/dim 1.1154(1.1213) | Xent 0.0643(0.0556) | Loss 1.1476(1.1491) | Error 0.0200(0.0171) Steps 458(466.03) | Grad Norm 3.1540(3.0793) | Total Time 10.00(10.00)\n",
      "Iter 1885 | Time 34.3242(36.1279) | Bit/dim 1.1180(1.1212) | Xent 0.0547(0.0556) | Loss 1.1454(1.1490) | Error 0.0176(0.0171) Steps 458(465.79) | Grad Norm 3.8521(3.1025) | Total Time 10.00(10.00)\n",
      "Iter 1886 | Time 36.8211(36.1487) | Bit/dim 1.1388(1.1217) | Xent 0.0594(0.0557) | Loss 1.1686(1.1496) | Error 0.0202(0.0172) Steps 476(466.09) | Grad Norm 6.8809(3.2158) | Total Time 10.00(10.00)\n",
      "Iter 1887 | Time 36.6323(36.1632) | Bit/dim 1.1132(1.1215) | Xent 0.0527(0.0556) | Loss 1.1396(1.1493) | Error 0.0160(0.0172) Steps 464(466.03) | Grad Norm 3.2484(3.2168) | Total Time 10.00(10.00)\n",
      "Iter 1888 | Time 33.5961(36.0862) | Bit/dim 1.1231(1.1215) | Xent 0.0589(0.0557) | Loss 1.1525(1.1494) | Error 0.0162(0.0172) Steps 458(465.79) | Grad Norm 3.0876(3.2129) | Total Time 10.00(10.00)\n",
      "Iter 1889 | Time 35.8775(36.0799) | Bit/dim 1.1265(1.1217) | Xent 0.0507(0.0556) | Loss 1.1518(1.1495) | Error 0.0162(0.0171) Steps 470(465.91) | Grad Norm 5.8257(3.2913) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 34.2552(36.0252) | Bit/dim 1.1254(1.1218) | Xent 0.0561(0.0556) | Loss 1.1534(1.1496) | Error 0.0182(0.0172) Steps 458(465.68) | Grad Norm 3.8023(3.3067) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 16.9294, Epoch Time 275.8948(274.2799), Bit/dim 1.1118(best: 1.1120), Xent 0.0280, Loss 1.1258, Error 0.0090(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1891 | Time 37.1316(36.0584) | Bit/dim 1.1203(1.1218) | Xent 0.0516(0.0555) | Loss 1.1461(1.1495) | Error 0.0144(0.0171) Steps 482(466.17) | Grad Norm 0.5970(3.2254) | Total Time 10.00(10.00)\n",
      "Iter 1892 | Time 36.5624(36.0735) | Bit/dim 1.1202(1.1217) | Xent 0.0518(0.0553) | Loss 1.1461(1.1494) | Error 0.0168(0.0171) Steps 458(465.92) | Grad Norm 1.7648(3.1816) | Total Time 10.00(10.00)\n",
      "Iter 1893 | Time 37.8929(36.1281) | Bit/dim 1.1185(1.1216) | Xent 0.0634(0.0556) | Loss 1.1502(1.1494) | Error 0.0204(0.0172) Steps 470(466.04) | Grad Norm 3.8326(3.2011) | Total Time 10.00(10.00)\n",
      "Iter 1894 | Time 36.0746(36.1265) | Bit/dim 1.1247(1.1217) | Xent 0.0503(0.0554) | Loss 1.1498(1.1494) | Error 0.0176(0.0172) Steps 470(466.16) | Grad Norm 5.0640(3.2570) | Total Time 10.00(10.00)\n",
      "Iter 1895 | Time 38.2594(36.1905) | Bit/dim 1.1201(1.1217) | Xent 0.0577(0.0555) | Loss 1.1489(1.1494) | Error 0.0185(0.0172) Steps 470(466.28) | Grad Norm 4.0976(3.2822) | Total Time 10.00(10.00)\n",
      "Iter 1896 | Time 37.8010(36.2388) | Bit/dim 1.1165(1.1215) | Xent 0.0586(0.0556) | Loss 1.1458(1.1493) | Error 0.0169(0.0172) Steps 482(466.75) | Grad Norm 2.7841(3.2672) | Total Time 10.00(10.00)\n",
      "Iter 1897 | Time 37.8896(36.2883) | Bit/dim 1.1188(1.1214) | Xent 0.0564(0.0556) | Loss 1.1470(1.1492) | Error 0.0184(0.0173) Steps 458(466.49) | Grad Norm 2.3085(3.2385) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 16.7336, Epoch Time 290.8340(274.7765), Bit/dim 1.1124(best: 1.1118), Xent 0.0289, Loss 1.1269, Error 0.0097(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1898 | Time 36.7584(36.3024) | Bit/dim 1.1186(1.1213) | Xent 0.0508(0.0555) | Loss 1.1440(1.1491) | Error 0.0168(0.0172) Steps 452(466.05) | Grad Norm 2.1223(3.2050) | Total Time 10.00(10.00)\n",
      "Iter 1899 | Time 34.2140(36.2398) | Bit/dim 1.1176(1.1212) | Xent 0.0520(0.0554) | Loss 1.1436(1.1489) | Error 0.0165(0.0172) Steps 458(465.81) | Grad Norm 2.0609(3.1707) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 36.4041(36.2447) | Bit/dim 1.1205(1.1212) | Xent 0.0544(0.0553) | Loss 1.1476(1.1489) | Error 0.0155(0.0172) Steps 476(466.12) | Grad Norm 2.2325(3.1425) | Total Time 10.00(10.00)\n",
      "Iter 1901 | Time 34.3843(36.1889) | Bit/dim 1.1167(1.1211) | Xent 0.0543(0.0553) | Loss 1.1438(1.1487) | Error 0.0175(0.0172) Steps 458(465.87) | Grad Norm 3.2826(3.1467) | Total Time 10.00(10.00)\n",
      "Iter 1902 | Time 36.2866(36.1918) | Bit/dim 1.1211(1.1211) | Xent 0.0537(0.0553) | Loss 1.1480(1.1487) | Error 0.0170(0.0172) Steps 476(466.18) | Grad Norm 4.7123(3.1937) | Total Time 10.00(10.00)\n",
      "Iter 1903 | Time 35.6471(36.1755) | Bit/dim 1.1198(1.1210) | Xent 0.0518(0.0552) | Loss 1.1457(1.1486) | Error 0.0184(0.0172) Steps 470(466.29) | Grad Norm 6.7263(3.2997) | Total Time 10.00(10.00)\n",
      "Iter 1904 | Time 36.5965(36.1881) | Bit/dim 1.1367(1.1215) | Xent 0.0547(0.0551) | Loss 1.1640(1.1491) | Error 0.0164(0.0172) Steps 470(466.40) | Grad Norm 7.2113(3.4170) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 17.1952, Epoch Time 280.0215(274.9339), Bit/dim 1.1124(best: 1.1118), Xent 0.0304, Loss 1.1276, Error 0.0104(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1905 | Time 36.4591(36.1962) | Bit/dim 1.1172(1.1214) | Xent 0.0580(0.0552) | Loss 1.1462(1.1490) | Error 0.0181(0.0172) Steps 458(466.15) | Grad Norm 2.6101(3.3928) | Total Time 10.00(10.00)\n",
      "Iter 1906 | Time 34.9366(36.1584) | Bit/dim 1.1170(1.1212) | Xent 0.0553(0.0552) | Loss 1.1446(1.1488) | Error 0.0166(0.0172) Steps 470(466.27) | Grad Norm 4.8454(3.4364) | Total Time 10.00(10.00)\n",
      "Iter 1907 | Time 34.3061(36.1029) | Bit/dim 1.1385(1.1218) | Xent 0.0518(0.0551) | Loss 1.1644(1.1493) | Error 0.0152(0.0171) Steps 446(465.66) | Grad Norm 7.4898(3.5580) | Total Time 10.00(10.00)\n",
      "Iter 1908 | Time 39.5170(36.2053) | Bit/dim 1.1187(1.1217) | Xent 0.0560(0.0552) | Loss 1.1467(1.1492) | Error 0.0161(0.0171) Steps 458(465.43) | Grad Norm 0.8165(3.4758) | Total Time 10.00(10.00)\n",
      "Iter 1909 | Time 38.1659(36.2641) | Bit/dim 1.1344(1.1220) | Xent 0.0538(0.0551) | Loss 1.1613(1.1496) | Error 0.0149(0.0170) Steps 464(465.39) | Grad Norm 12.8755(3.7577) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 34.8644(36.2221) | Bit/dim 1.1867(1.1240) | Xent 0.0571(0.0552) | Loss 1.2153(1.1516) | Error 0.0169(0.0170) Steps 452(464.98) | Grad Norm 9.5183(3.9306) | Total Time 10.00(10.00)\n",
      "Iter 1911 | Time 35.2178(36.1920) | Bit/dim 1.1815(1.1257) | Xent 0.0466(0.0549) | Loss 1.2048(1.1532) | Error 0.0139(0.0169) Steps 452(464.59) | Grad Norm 8.8303(4.0776) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 17.2284, Epoch Time 283.0949(275.1787), Bit/dim 1.1160(best: 1.1118), Xent 0.0284, Loss 1.1302, Error 0.0106(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1912 | Time 37.3499(36.2267) | Bit/dim 1.1240(1.1257) | Xent 0.0531(0.0549) | Loss 1.1505(1.1531) | Error 0.0160(0.0169) Steps 446(464.04) | Grad Norm 1.8336(4.0102) | Total Time 10.00(10.00)\n",
      "Iter 1913 | Time 36.6789(36.2403) | Bit/dim 1.2147(1.1283) | Xent 0.1024(0.0563) | Loss 1.2659(1.1565) | Error 0.0329(0.0174) Steps 476(464.40) | Grad Norm 28.3552(4.7406) | Total Time 10.00(10.00)\n",
      "Iter 1914 | Time 35.6979(36.2240) | Bit/dim 1.2310(1.1314) | Xent 0.0544(0.0562) | Loss 1.2582(1.1595) | Error 0.0165(0.0174) Steps 452(464.02) | Grad Norm 7.7229(4.8301) | Total Time 10.00(10.00)\n",
      "Iter 1915 | Time 35.4102(36.1996) | Bit/dim 1.3303(1.1374) | Xent 0.0469(0.0559) | Loss 1.3537(1.1654) | Error 0.0158(0.0173) Steps 452(463.66) | Grad Norm 7.5940(4.9130) | Total Time 10.00(10.00)\n",
      "Iter 1916 | Time 34.9128(36.1610) | Bit/dim 1.3389(1.1434) | Xent 0.0428(0.0556) | Loss 1.3603(1.1712) | Error 0.0128(0.0172) Steps 452(463.31) | Grad Norm 6.5389(4.9618) | Total Time 10.00(10.00)\n",
      "Iter 1917 | Time 34.5132(36.1116) | Bit/dim 1.3054(1.1483) | Xent 0.0456(0.0553) | Loss 1.3282(1.1759) | Error 0.0141(0.0171) Steps 440(462.61) | Grad Norm 5.3255(4.9727) | Total Time 10.00(10.00)\n",
      "Iter 1918 | Time 33.2298(36.0251) | Bit/dim 1.2540(1.1515) | Xent 0.0397(0.0548) | Loss 1.2738(1.1788) | Error 0.0121(0.0169) Steps 434(461.76) | Grad Norm 4.1361(4.9476) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 16.5100, Epoch Time 276.6674(275.2234), Bit/dim 1.2164(best: 1.1118), Xent 0.0268, Loss 1.2298, Error 0.0089(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1919 | Time 34.0194(35.9649) | Bit/dim 1.2259(1.1537) | Xent 0.0468(0.0545) | Loss 1.2493(1.1810) | Error 0.0141(0.0169) Steps 446(461.28) | Grad Norm 2.8173(4.8837) | Total Time 10.00(10.00)\n",
      "Iter 1920 | Time 36.2927(35.9748) | Bit/dim 1.2089(1.1553) | Xent 0.0702(0.0550) | Loss 1.2440(1.1829) | Error 0.0214(0.0170) Steps 458(461.18) | Grad Norm 4.4357(4.8702) | Total Time 10.00(10.00)\n",
      "Iter 1921 | Time 36.9755(36.0048) | Bit/dim 1.2396(1.1579) | Xent 0.1021(0.0564) | Loss 1.2907(1.1861) | Error 0.0306(0.0174) Steps 482(461.81) | Grad Norm 13.3409(5.1243) | Total Time 10.00(10.00)\n",
      "Iter 1922 | Time 36.7819(36.0281) | Bit/dim 1.1781(1.1585) | Xent 0.0728(0.0569) | Loss 1.2145(1.1869) | Error 0.0225(0.0175) Steps 458(461.69) | Grad Norm 2.7277(5.0524) | Total Time 10.00(10.00)\n",
      "Iter 1923 | Time 34.4256(35.9800) | Bit/dim 1.1854(1.1593) | Xent 0.0728(0.0574) | Loss 1.2217(1.1880) | Error 0.0219(0.0177) Steps 464(461.76) | Grad Norm 3.5735(5.0081) | Total Time 10.00(10.00)\n",
      "Iter 1924 | Time 34.0860(35.9232) | Bit/dim 1.2049(1.1606) | Xent 0.0572(0.0574) | Loss 1.2335(1.1893) | Error 0.0190(0.0177) Steps 464(461.83) | Grad Norm 4.4687(4.9919) | Total Time 10.00(10.00)\n",
      "Iter 1925 | Time 34.3673(35.8765) | Bit/dim 1.2038(1.1619) | Xent 0.0509(0.0572) | Loss 1.2293(1.1905) | Error 0.0160(0.0177) Steps 452(461.54) | Grad Norm 4.3640(4.9731) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 16.5997, Epoch Time 276.0943(275.2495), Bit/dim 1.1801(best: 1.1118), Xent 0.0305, Loss 1.1953, Error 0.0104(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1926 | Time 33.3051(35.7994) | Bit/dim 1.1886(1.1627) | Xent 0.0543(0.0571) | Loss 1.2158(1.1913) | Error 0.0150(0.0176) Steps 452(461.25) | Grad Norm 3.6511(4.9334) | Total Time 10.00(10.00)\n",
      "Iter 1927 | Time 31.5148(35.6709) | Bit/dim 1.1667(1.1629) | Xent 0.0558(0.0571) | Loss 1.1946(1.1914) | Error 0.0188(0.0176) Steps 440(460.61) | Grad Norm 2.7533(4.8680) | Total Time 10.00(10.00)\n",
      "Iter 1928 | Time 36.3046(35.6899) | Bit/dim 1.1831(1.1635) | Xent 0.0718(0.0575) | Loss 1.2191(1.1922) | Error 0.0234(0.0178) Steps 476(461.07) | Grad Norm 10.6377(5.0411) | Total Time 10.00(10.00)\n",
      "Iter 1929 | Time 35.1951(35.6750) | Bit/dim 1.1472(1.1630) | Xent 0.0634(0.0577) | Loss 1.1789(1.1918) | Error 0.0189(0.0178) Steps 446(460.62) | Grad Norm 1.0361(4.9209) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 35.0327(35.6558) | Bit/dim 1.1570(1.1628) | Xent 0.0486(0.0574) | Loss 1.1813(1.1915) | Error 0.0154(0.0178) Steps 458(460.54) | Grad Norm 4.1921(4.8991) | Total Time 10.00(10.00)\n",
      "Iter 1931 | Time 33.7921(35.5998) | Bit/dim 1.1645(1.1629) | Xent 0.0602(0.0575) | Loss 1.1946(1.1916) | Error 0.0175(0.0177) Steps 458(460.47) | Grad Norm 4.8275(4.8969) | Total Time 10.00(10.00)\n",
      "Iter 1932 | Time 35.2939(35.5907) | Bit/dim 1.1503(1.1625) | Xent 0.0566(0.0575) | Loss 1.1786(1.1912) | Error 0.0176(0.0177) Steps 452(460.21) | Grad Norm 3.1922(4.8458) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 16.3763, Epoch Time 269.0274(275.0628), Bit/dim 1.1381(best: 1.1118), Xent 0.0320, Loss 1.1541, Error 0.0093(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1933 | Time 32.0221(35.4836) | Bit/dim 1.1493(1.1621) | Xent 0.0678(0.0578) | Loss 1.1832(1.1910) | Error 0.0199(0.0178) Steps 446(459.79) | Grad Norm 4.2798(4.8288) | Total Time 10.00(10.00)\n",
      "Iter 1934 | Time 33.1235(35.4128) | Bit/dim 1.1491(1.1617) | Xent 0.0633(0.0580) | Loss 1.1807(1.1907) | Error 0.0200(0.0179) Steps 452(459.55) | Grad Norm 7.0694(4.8960) | Total Time 10.00(10.00)\n",
      "Iter 1935 | Time 33.5864(35.3580) | Bit/dim 1.1411(1.1611) | Xent 0.0537(0.0578) | Loss 1.1680(1.1900) | Error 0.0162(0.0178) Steps 452(459.33) | Grad Norm 1.7926(4.8029) | Total Time 10.00(10.00)\n",
      "Iter 1936 | Time 36.2324(35.3843) | Bit/dim 1.1499(1.1607) | Xent 0.0518(0.0576) | Loss 1.1757(1.1896) | Error 0.0168(0.0178) Steps 446(458.93) | Grad Norm 4.3983(4.7908) | Total Time 10.00(10.00)\n",
      "Iter 1937 | Time 34.8856(35.3693) | Bit/dim 1.1428(1.1602) | Xent 0.0561(0.0576) | Loss 1.1709(1.1890) | Error 0.0186(0.0178) Steps 458(458.90) | Grad Norm 3.4923(4.7518) | Total Time 10.00(10.00)\n",
      "Iter 1938 | Time 37.7052(35.4394) | Bit/dim 1.1388(1.1596) | Xent 0.0560(0.0575) | Loss 1.1667(1.1883) | Error 0.0164(0.0178) Steps 470(459.23) | Grad Norm 4.0053(4.7294) | Total Time 10.00(10.00)\n",
      "Iter 1939 | Time 38.3585(35.5269) | Bit/dim 1.1348(1.1588) | Xent 0.0515(0.0574) | Loss 1.1606(1.1875) | Error 0.0171(0.0178) Steps 470(459.55) | Grad Norm 5.8807(4.7640) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 16.7793, Epoch Time 275.0561(275.0626), Bit/dim 1.1313(best: 1.1118), Xent 0.0285, Loss 1.1456, Error 0.0090(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1940 | Time 34.3563(35.4918) | Bit/dim 1.1388(1.1582) | Xent 0.0597(0.0574) | Loss 1.1687(1.1869) | Error 0.0164(0.0177) Steps 458(459.51) | Grad Norm 3.1798(4.7164) | Total Time 10.00(10.00)\n",
      "Iter 1941 | Time 35.9780(35.5064) | Bit/dim 1.1428(1.1578) | Xent 0.0516(0.0573) | Loss 1.1686(1.1864) | Error 0.0174(0.0177) Steps 458(459.46) | Grad Norm 4.4036(4.7071) | Total Time 10.00(10.00)\n",
      "Iter 1942 | Time 35.6554(35.5109) | Bit/dim 1.1281(1.1569) | Xent 0.0534(0.0571) | Loss 1.1549(1.1854) | Error 0.0159(0.0176) Steps 476(459.96) | Grad Norm 1.2716(4.6040) | Total Time 10.00(10.00)\n",
      "Iter 1943 | Time 37.0922(35.5583) | Bit/dim 1.1387(1.1563) | Xent 0.0583(0.0572) | Loss 1.1679(1.1849) | Error 0.0170(0.0176) Steps 452(459.72) | Grad Norm 6.3746(4.6571) | Total Time 10.00(10.00)\n",
      "Iter 1944 | Time 37.9266(35.6294) | Bit/dim 1.1339(1.1556) | Xent 0.0550(0.0571) | Loss 1.1614(1.1842) | Error 0.0181(0.0176) Steps 452(459.49) | Grad Norm 0.9698(4.5465) | Total Time 10.00(10.00)\n",
      "Iter 1945 | Time 34.3617(35.5913) | Bit/dim 1.1372(1.1551) | Xent 0.0508(0.0569) | Loss 1.1627(1.1836) | Error 0.0158(0.0176) Steps 452(459.26) | Grad Norm 3.8792(4.5265) | Total Time 10.00(10.00)\n",
      "Iter 1946 | Time 34.1956(35.5495) | Bit/dim 1.1272(1.1543) | Xent 0.0500(0.0567) | Loss 1.1522(1.1826) | Error 0.0159(0.0175) Steps 458(459.23) | Grad Norm 2.9605(4.4795) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 17.0424, Epoch Time 279.3394(275.1909), Bit/dim 1.1223(best: 1.1118), Xent 0.0303, Loss 1.1374, Error 0.0109(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1947 | Time 33.1809(35.4784) | Bit/dim 1.1265(1.1534) | Xent 0.0583(0.0568) | Loss 1.1557(1.1818) | Error 0.0176(0.0175) Steps 452(459.01) | Grad Norm 3.5627(4.4520) | Total Time 10.00(10.00)\n",
      "Iter 1948 | Time 37.0598(35.5259) | Bit/dim 1.1306(1.1527) | Xent 0.0637(0.0570) | Loss 1.1624(1.1812) | Error 0.0195(0.0176) Steps 470(459.34) | Grad Norm 3.3624(4.4193) | Total Time 10.00(10.00)\n",
      "Iter 1949 | Time 34.4654(35.4940) | Bit/dim 1.1287(1.1520) | Xent 0.0528(0.0568) | Loss 1.1551(1.1804) | Error 0.0181(0.0176) Steps 452(459.12) | Grad Norm 2.9267(4.3745) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 33.9260(35.4470) | Bit/dim 1.1271(1.1513) | Xent 0.0522(0.0567) | Loss 1.1532(1.1796) | Error 0.0159(0.0176) Steps 452(458.91) | Grad Norm 3.2339(4.3403) | Total Time 10.00(10.00)\n",
      "Iter 1951 | Time 35.9961(35.4635) | Bit/dim 1.1341(1.1508) | Xent 0.0491(0.0565) | Loss 1.1587(1.1790) | Error 0.0149(0.0175) Steps 470(459.24) | Grad Norm 1.6022(4.2582) | Total Time 10.00(10.00)\n",
      "Iter 1952 | Time 38.0826(35.5420) | Bit/dim 1.1255(1.1500) | Xent 0.0587(0.0565) | Loss 1.1548(1.1783) | Error 0.0179(0.0175) Steps 476(459.74) | Grad Norm 3.9498(4.2489) | Total Time 10.00(10.00)\n",
      "Iter 1953 | Time 37.5015(35.6008) | Bit/dim 1.1219(1.1492) | Xent 0.0581(0.0566) | Loss 1.1509(1.1774) | Error 0.0188(0.0175) Steps 470(460.05) | Grad Norm 1.5257(4.1672) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 16.7694, Epoch Time 279.4630(275.3191), Bit/dim 1.1224(best: 1.1118), Xent 0.0285, Loss 1.1367, Error 0.0100(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1954 | Time 36.9150(35.6403) | Bit/dim 1.1296(1.1486) | Xent 0.0545(0.0565) | Loss 1.1569(1.1768) | Error 0.0175(0.0175) Steps 476(460.53) | Grad Norm 2.8528(4.1278) | Total Time 10.00(10.00)\n",
      "Iter 1955 | Time 36.5095(35.6663) | Bit/dim 1.1217(1.1478) | Xent 0.0493(0.0563) | Loss 1.1463(1.1759) | Error 0.0152(0.0175) Steps 476(460.99) | Grad Norm 0.5592(4.0207) | Total Time 10.00(10.00)\n",
      "Iter 1956 | Time 37.0653(35.7083) | Bit/dim 1.1250(1.1471) | Xent 0.0549(0.0563) | Loss 1.1524(1.1752) | Error 0.0170(0.0174) Steps 494(461.98) | Grad Norm 3.5267(4.0059) | Total Time 10.00(10.00)\n",
      "Iter 1957 | Time 33.9659(35.6560) | Bit/dim 1.1235(1.1464) | Xent 0.0545(0.0562) | Loss 1.1507(1.1745) | Error 0.0162(0.0174) Steps 464(462.04) | Grad Norm 0.4529(3.8993) | Total Time 10.00(10.00)\n",
      "Iter 1958 | Time 34.0968(35.6093) | Bit/dim 1.1250(1.1457) | Xent 0.0535(0.0561) | Loss 1.1517(1.1738) | Error 0.0176(0.0174) Steps 464(462.10) | Grad Norm 2.5707(3.8595) | Total Time 10.00(10.00)\n",
      "Iter 1959 | Time 34.6329(35.5800) | Bit/dim 1.1181(1.1449) | Xent 0.0566(0.0561) | Loss 1.1464(1.1730) | Error 0.0161(0.0174) Steps 464(462.16) | Grad Norm 0.9466(3.7721) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 34.2073(35.5388) | Bit/dim 1.1251(1.1443) | Xent 0.0585(0.0562) | Loss 1.1544(1.1724) | Error 0.0168(0.0174) Steps 458(462.03) | Grad Norm 3.3603(3.7597) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 16.9402, Epoch Time 276.7800(275.3629), Bit/dim 1.1160(best: 1.1118), Xent 0.0264, Loss 1.1292, Error 0.0091(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1961 | Time 33.5883(35.4803) | Bit/dim 1.1198(1.1436) | Xent 0.0569(0.0562) | Loss 1.1483(1.1717) | Error 0.0170(0.0173) Steps 464(462.09) | Grad Norm 0.5122(3.6623) | Total Time 10.00(10.00)\n",
      "Iter 1962 | Time 35.3536(35.4765) | Bit/dim 1.1215(1.1429) | Xent 0.0467(0.0560) | Loss 1.1448(1.1709) | Error 0.0154(0.0173) Steps 464(462.15) | Grad Norm 2.4461(3.6258) | Total Time 10.00(10.00)\n",
      "Iter 1963 | Time 36.8383(35.5173) | Bit/dim 1.1229(1.1423) | Xent 0.0523(0.0558) | Loss 1.1491(1.1702) | Error 0.0166(0.0173) Steps 464(462.20) | Grad Norm 0.3473(3.5275) | Total Time 10.00(10.00)\n",
      "Iter 1964 | Time 37.5795(35.5792) | Bit/dim 1.1248(1.1418) | Xent 0.0532(0.0558) | Loss 1.1514(1.1697) | Error 0.0164(0.0172) Steps 476(462.62) | Grad Norm 2.7965(3.5055) | Total Time 10.00(10.00)\n",
      "Iter 1965 | Time 36.5312(35.6077) | Bit/dim 1.1196(1.1411) | Xent 0.0547(0.0557) | Loss 1.1470(1.1690) | Error 0.0190(0.0173) Steps 476(463.02) | Grad Norm 0.9207(3.4280) | Total Time 10.00(10.00)\n",
      "Iter 1966 | Time 36.6869(35.6401) | Bit/dim 1.1196(1.1405) | Xent 0.0509(0.0556) | Loss 1.1450(1.1683) | Error 0.0146(0.0172) Steps 476(463.41) | Grad Norm 2.0415(3.3864) | Total Time 10.00(10.00)\n",
      "Iter 1967 | Time 37.1920(35.6867) | Bit/dim 1.1224(1.1399) | Xent 0.0491(0.0554) | Loss 1.1470(1.1676) | Error 0.0150(0.0171) Steps 482(463.97) | Grad Norm 0.9342(3.3128) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 17.0739, Epoch Time 283.0407(275.5932), Bit/dim 1.1144(best: 1.1118), Xent 0.0278, Loss 1.1283, Error 0.0092(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1968 | Time 37.7458(35.7485) | Bit/dim 1.1185(1.1393) | Xent 0.0493(0.0552) | Loss 1.1431(1.1669) | Error 0.0172(0.0172) Steps 464(463.97) | Grad Norm 1.7663(3.2664) | Total Time 10.00(10.00)\n",
      "Iter 1969 | Time 33.5145(35.6814) | Bit/dim 1.1218(1.1388) | Xent 0.0664(0.0555) | Loss 1.1550(1.1665) | Error 0.0200(0.0172) Steps 464(463.97) | Grad Norm 1.0461(3.1998) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 34.7817(35.6544) | Bit/dim 1.1231(1.1383) | Xent 0.0553(0.0555) | Loss 1.1508(1.1661) | Error 0.0162(0.0172) Steps 464(463.97) | Grad Norm 1.7371(3.1559) | Total Time 10.00(10.00)\n",
      "Iter 1971 | Time 36.2415(35.6721) | Bit/dim 1.1221(1.1378) | Xent 0.0487(0.0553) | Loss 1.1465(1.1655) | Error 0.0135(0.0171) Steps 482(464.51) | Grad Norm 1.6103(3.1096) | Total Time 10.00(10.00)\n",
      "Iter 1972 | Time 36.0180(35.6824) | Bit/dim 1.1112(1.1370) | Xent 0.0531(0.0553) | Loss 1.1377(1.1646) | Error 0.0181(0.0171) Steps 464(464.50) | Grad Norm 1.2121(3.0526) | Total Time 10.00(10.00)\n",
      "Iter 1973 | Time 36.4058(35.7041) | Bit/dim 1.1227(1.1366) | Xent 0.0524(0.0552) | Loss 1.1489(1.1642) | Error 0.0178(0.0171) Steps 452(464.12) | Grad Norm 1.9287(3.0189) | Total Time 10.00(10.00)\n",
      "Iter 1974 | Time 36.9790(35.7424) | Bit/dim 1.1214(1.1361) | Xent 0.0461(0.0549) | Loss 1.1445(1.1636) | Error 0.0136(0.0170) Steps 458(463.94) | Grad Norm 0.6040(2.9465) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 17.2806, Epoch Time 281.2494(275.7629), Bit/dim 1.1137(best: 1.1118), Xent 0.0282, Loss 1.1278, Error 0.0091(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1975 | Time 38.6780(35.8304) | Bit/dim 1.1183(1.1356) | Xent 0.0559(0.0549) | Loss 1.1462(1.1631) | Error 0.0166(0.0170) Steps 482(464.48) | Grad Norm 2.7406(2.9403) | Total Time 10.00(10.00)\n",
      "Iter 1976 | Time 35.9999(35.8355) | Bit/dim 1.1197(1.1351) | Xent 0.0471(0.0547) | Loss 1.1432(1.1625) | Error 0.0149(0.0170) Steps 476(464.82) | Grad Norm 1.7822(2.9056) | Total Time 10.00(10.00)\n",
      "Iter 1977 | Time 37.7467(35.8929) | Bit/dim 1.1247(1.1348) | Xent 0.0534(0.0547) | Loss 1.1513(1.1621) | Error 0.0165(0.0169) Steps 458(464.62) | Grad Norm 1.4490(2.8619) | Total Time 10.00(10.00)\n",
      "Iter 1978 | Time 37.9800(35.9555) | Bit/dim 1.1203(1.1344) | Xent 0.0513(0.0546) | Loss 1.1460(1.1616) | Error 0.0152(0.0169) Steps 494(465.50) | Grad Norm 2.5521(2.8526) | Total Time 10.00(10.00)\n",
      "Iter 1979 | Time 38.0647(36.0188) | Bit/dim 1.1206(1.1340) | Xent 0.0509(0.0545) | Loss 1.1460(1.1612) | Error 0.0156(0.0169) Steps 464(465.46) | Grad Norm 0.6195(2.7856) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 36.5807(36.0356) | Bit/dim 1.1185(1.1335) | Xent 0.0554(0.0545) | Loss 1.1462(1.1607) | Error 0.0164(0.0168) Steps 452(465.05) | Grad Norm 1.6380(2.7511) | Total Time 10.00(10.00)\n",
      "Iter 1981 | Time 37.1777(36.0699) | Bit/dim 1.1178(1.1330) | Xent 0.0583(0.0546) | Loss 1.1469(1.1603) | Error 0.0186(0.0169) Steps 488(465.74) | Grad Norm 1.4004(2.7106) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 16.9596, Epoch Time 291.5725(276.2372), Bit/dim 1.1114(best: 1.1118), Xent 0.0275, Loss 1.1252, Error 0.0101(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1982 | Time 36.7733(36.0910) | Bit/dim 1.1192(1.1326) | Xent 0.0486(0.0544) | Loss 1.1436(1.1598) | Error 0.0145(0.0168) Steps 494(466.59) | Grad Norm 0.8094(2.6536) | Total Time 10.00(10.00)\n",
      "Iter 1983 | Time 36.4042(36.1004) | Bit/dim 1.1207(1.1322) | Xent 0.0519(0.0543) | Loss 1.1466(1.1594) | Error 0.0165(0.0168) Steps 458(466.33) | Grad Norm 1.8960(2.6309) | Total Time 10.00(10.00)\n",
      "Iter 1984 | Time 36.5279(36.1132) | Bit/dim 1.1176(1.1318) | Xent 0.0489(0.0542) | Loss 1.1420(1.1589) | Error 0.0148(0.0168) Steps 482(466.80) | Grad Norm 0.6070(2.5701) | Total Time 10.00(10.00)\n",
      "Iter 1985 | Time 36.5621(36.1267) | Bit/dim 1.1198(1.1314) | Xent 0.0498(0.0540) | Loss 1.1447(1.1585) | Error 0.0166(0.0168) Steps 476(467.08) | Grad Norm 1.6180(2.5416) | Total Time 10.00(10.00)\n",
      "Iter 1986 | Time 36.3654(36.1338) | Bit/dim 1.1181(1.1310) | Xent 0.0516(0.0540) | Loss 1.1439(1.1580) | Error 0.0155(0.0167) Steps 464(466.98) | Grad Norm 1.7643(2.5183) | Total Time 10.00(10.00)\n",
      "Iter 1987 | Time 38.1866(36.1954) | Bit/dim 1.1153(1.1306) | Xent 0.0531(0.0539) | Loss 1.1418(1.1575) | Error 0.0156(0.0167) Steps 482(467.44) | Grad Norm 0.4302(2.4556) | Total Time 10.00(10.00)\n",
      "Iter 1988 | Time 37.5030(36.2346) | Bit/dim 1.1166(1.1302) | Xent 0.0537(0.0539) | Loss 1.1435(1.1571) | Error 0.0186(0.0167) Steps 476(467.69) | Grad Norm 2.1089(2.4452) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 16.9636, Epoch Time 288.0895(276.5928), Bit/dim 1.1122(best: 1.1114), Xent 0.0263, Loss 1.1254, Error 0.0087(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1989 | Time 35.8296(36.2225) | Bit/dim 1.1215(1.1299) | Xent 0.0525(0.0539) | Loss 1.1477(1.1568) | Error 0.0160(0.0167) Steps 458(467.40) | Grad Norm 1.5173(2.4174) | Total Time 10.00(10.00)\n",
      "Iter 1990 | Time 35.9329(36.2138) | Bit/dim 1.1204(1.1296) | Xent 0.0453(0.0536) | Loss 1.1431(1.1564) | Error 0.0136(0.0166) Steps 482(467.84) | Grad Norm 0.8737(2.3711) | Total Time 10.00(10.00)\n",
      "Iter 1991 | Time 38.8638(36.2933) | Bit/dim 1.1164(1.1292) | Xent 0.0503(0.0535) | Loss 1.1416(1.1560) | Error 0.0166(0.0166) Steps 494(468.62) | Grad Norm 2.3222(2.3696) | Total Time 10.00(10.00)\n",
      "Iter 1992 | Time 37.3812(36.3259) | Bit/dim 1.1179(1.1289) | Xent 0.0498(0.0534) | Loss 1.1428(1.1556) | Error 0.0150(0.0166) Steps 482(469.03) | Grad Norm 1.6066(2.3467) | Total Time 10.00(10.00)\n",
      "Iter 1993 | Time 35.8996(36.3131) | Bit/dim 1.1184(1.1286) | Xent 0.0598(0.0536) | Loss 1.1483(1.1554) | Error 0.0198(0.0167) Steps 476(469.23) | Grad Norm 0.5112(2.2916) | Total Time 10.00(10.00)\n",
      "Iter 1994 | Time 36.7356(36.3258) | Bit/dim 1.1114(1.1280) | Xent 0.0590(0.0538) | Loss 1.1409(1.1549) | Error 0.0171(0.0167) Steps 476(469.44) | Grad Norm 1.8008(2.2769) | Total Time 10.00(10.00)\n",
      "Iter 1995 | Time 37.2172(36.3526) | Bit/dim 1.1166(1.1277) | Xent 0.0491(0.0536) | Loss 1.1411(1.1545) | Error 0.0154(0.0166) Steps 482(469.81) | Grad Norm 1.2670(2.2466) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 17.0407, Epoch Time 287.1953(276.9109), Bit/dim 1.1107(best: 1.1114), Xent 0.0290, Loss 1.1252, Error 0.0098(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 1996 | Time 38.0346(36.4030) | Bit/dim 1.1183(1.1274) | Xent 0.0522(0.0536) | Loss 1.1444(1.1542) | Error 0.0160(0.0166) Steps 470(469.82) | Grad Norm 0.4728(2.1934) | Total Time 10.00(10.00)\n",
      "Iter 1997 | Time 37.4635(36.4348) | Bit/dim 1.1195(1.1272) | Xent 0.0506(0.0535) | Loss 1.1448(1.1539) | Error 0.0160(0.0166) Steps 476(470.01) | Grad Norm 1.5819(2.1751) | Total Time 10.00(10.00)\n",
      "Iter 1998 | Time 37.1035(36.4549) | Bit/dim 1.1156(1.1268) | Xent 0.0486(0.0534) | Loss 1.1399(1.1535) | Error 0.0158(0.0166) Steps 482(470.37) | Grad Norm 1.5021(2.1549) | Total Time 10.00(10.00)\n",
      "Iter 1999 | Time 36.8878(36.4679) | Bit/dim 1.1149(1.1265) | Xent 0.0541(0.0534) | Loss 1.1419(1.1532) | Error 0.0161(0.0166) Steps 482(470.71) | Grad Norm 0.2743(2.0985) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 38.4482(36.5273) | Bit/dim 1.1185(1.1262) | Xent 0.0502(0.0533) | Loss 1.1436(1.1529) | Error 0.0171(0.0166) Steps 482(471.05) | Grad Norm 1.3635(2.0764) | Total Time 10.00(10.00)\n",
      "Iter 2001 | Time 37.5711(36.5586) | Bit/dim 1.1122(1.1258) | Xent 0.0528(0.0533) | Loss 1.1386(1.1524) | Error 0.0165(0.0166) Steps 482(471.38) | Grad Norm 1.5076(2.0594) | Total Time 10.00(10.00)\n",
      "Iter 2002 | Time 38.4968(36.6168) | Bit/dim 1.1195(1.1256) | Xent 0.0508(0.0532) | Loss 1.1449(1.1522) | Error 0.0156(0.0166) Steps 476(471.52) | Grad Norm 0.3653(2.0085) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 16.8437, Epoch Time 293.5718(277.4107), Bit/dim 1.1105(best: 1.1107), Xent 0.0260, Loss 1.1235, Error 0.0093(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2003 | Time 38.5159(36.6737) | Bit/dim 1.1158(1.1253) | Xent 0.0593(0.0534) | Loss 1.1454(1.1520) | Error 0.0172(0.0166) Steps 482(471.83) | Grad Norm 1.3207(1.9879) | Total Time 10.00(10.00)\n",
      "Iter 2004 | Time 37.4842(36.6980) | Bit/dim 1.1150(1.1250) | Xent 0.0542(0.0534) | Loss 1.1421(1.1517) | Error 0.0155(0.0165) Steps 482(472.14) | Grad Norm 1.9760(1.9875) | Total Time 10.00(10.00)\n",
      "Iter 2005 | Time 39.9602(36.7959) | Bit/dim 1.1193(1.1248) | Xent 0.0568(0.0535) | Loss 1.1477(1.1516) | Error 0.0161(0.0165) Steps 500(472.98) | Grad Norm 1.0646(1.9599) | Total Time 10.00(10.00)\n",
      "Iter 2006 | Time 39.1175(36.8656) | Bit/dim 1.1143(1.1245) | Xent 0.0531(0.0535) | Loss 1.1409(1.1513) | Error 0.0170(0.0165) Steps 482(473.25) | Grad Norm 0.8882(1.9277) | Total Time 10.00(10.00)\n",
      "Iter 2007 | Time 37.7099(36.8909) | Bit/dim 1.1165(1.1243) | Xent 0.0567(0.0536) | Loss 1.1448(1.1511) | Error 0.0164(0.0165) Steps 482(473.51) | Grad Norm 2.1692(1.9349) | Total Time 10.00(10.00)\n",
      "Iter 2008 | Time 38.4639(36.9381) | Bit/dim 1.1157(1.1240) | Xent 0.0499(0.0535) | Loss 1.1407(1.1508) | Error 0.0156(0.0165) Steps 488(473.94) | Grad Norm 1.6401(1.9261) | Total Time 10.00(10.00)\n",
      "Iter 2009 | Time 37.6086(36.9582) | Bit/dim 1.1162(1.1238) | Xent 0.0478(0.0533) | Loss 1.1400(1.1504) | Error 0.0145(0.0165) Steps 494(474.54) | Grad Norm 0.2282(1.8752) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 17.0362, Epoch Time 298.1827(278.0339), Bit/dim 1.1106(best: 1.1105), Xent 0.0279, Loss 1.1245, Error 0.0093(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2010 | Time 38.9213(37.0171) | Bit/dim 1.1180(1.1236) | Xent 0.0547(0.0533) | Loss 1.1454(1.1503) | Error 0.0152(0.0164) Steps 476(474.59) | Grad Norm 1.4010(1.8609) | Total Time 10.00(10.00)\n",
      "Iter 2011 | Time 38.5074(37.0618) | Bit/dim 1.1102(1.1232) | Xent 0.0587(0.0535) | Loss 1.1395(1.1500) | Error 0.0189(0.0165) Steps 494(475.17) | Grad Norm 2.0337(1.8661) | Total Time 10.00(10.00)\n",
      "Iter 2012 | Time 37.6122(37.0783) | Bit/dim 1.1153(1.1230) | Xent 0.0483(0.0533) | Loss 1.1395(1.1497) | Error 0.0151(0.0164) Steps 470(475.02) | Grad Norm 1.7684(1.8632) | Total Time 10.00(10.00)\n",
      "Iter 2013 | Time 39.1401(37.1402) | Bit/dim 1.1196(1.1229) | Xent 0.0519(0.0533) | Loss 1.1456(1.1495) | Error 0.0166(0.0165) Steps 482(475.23) | Grad Norm 0.4436(1.8206) | Total Time 10.00(10.00)\n",
      "Iter 2014 | Time 38.3247(37.1757) | Bit/dim 1.1145(1.1226) | Xent 0.0522(0.0533) | Loss 1.1406(1.1493) | Error 0.0158(0.0164) Steps 464(474.89) | Grad Norm 1.6112(1.8143) | Total Time 10.00(10.00)\n",
      "Iter 2015 | Time 37.3027(37.1795) | Bit/dim 1.1176(1.1225) | Xent 0.0528(0.0533) | Loss 1.1440(1.1491) | Error 0.0154(0.0164) Steps 470(474.74) | Grad Norm 2.6643(1.8398) | Total Time 10.00(10.00)\n",
      "Iter 2016 | Time 38.8885(37.2308) | Bit/dim 1.1208(1.1224) | Xent 0.0556(0.0533) | Loss 1.1485(1.1491) | Error 0.0172(0.0164) Steps 488(475.14) | Grad Norm 2.3922(1.8564) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 17.1343, Epoch Time 298.5333(278.6488), Bit/dim 1.1100(best: 1.1105), Xent 0.0274, Loss 1.1237, Error 0.0095(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2017 | Time 38.2013(37.2599) | Bit/dim 1.1148(1.1222) | Xent 0.0561(0.0534) | Loss 1.1429(1.1489) | Error 0.0172(0.0165) Steps 476(475.17) | Grad Norm 1.1595(1.8355) | Total Time 10.00(10.00)\n",
      "Iter 2018 | Time 37.6948(37.2729) | Bit/dim 1.1155(1.1220) | Xent 0.0478(0.0532) | Loss 1.1394(1.1486) | Error 0.0136(0.0164) Steps 488(475.55) | Grad Norm 0.5697(1.7975) | Total Time 10.00(10.00)\n",
      "Iter 2019 | Time 36.9470(37.2632) | Bit/dim 1.1190(1.1219) | Xent 0.0569(0.0534) | Loss 1.1474(1.1486) | Error 0.0179(0.0164) Steps 482(475.74) | Grad Norm 2.1802(1.8090) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 36.5045(37.2404) | Bit/dim 1.1206(1.1219) | Xent 0.0554(0.0534) | Loss 1.1483(1.1486) | Error 0.0174(0.0164) Steps 470(475.57) | Grad Norm 3.0686(1.8468) | Total Time 10.00(10.00)\n",
      "Iter 2021 | Time 39.4434(37.3065) | Bit/dim 1.1153(1.1217) | Xent 0.0522(0.0534) | Loss 1.1414(1.1484) | Error 0.0170(0.0165) Steps 482(475.76) | Grad Norm 2.5359(1.8674) | Total Time 10.00(10.00)\n",
      "Iter 2022 | Time 35.9944(37.2671) | Bit/dim 1.1122(1.1214) | Xent 0.0511(0.0533) | Loss 1.1377(1.1480) | Error 0.0146(0.0164) Steps 476(475.77) | Grad Norm 1.4584(1.8552) | Total Time 10.00(10.00)\n",
      "Iter 2023 | Time 39.3135(37.3285) | Bit/dim 1.1130(1.1211) | Xent 0.0572(0.0534) | Loss 1.1416(1.1478) | Error 0.0166(0.0164) Steps 476(475.78) | Grad Norm 0.4025(1.8116) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 17.2888, Epoch Time 293.6781(279.0997), Bit/dim 1.1091(best: 1.1100), Xent 0.0277, Loss 1.1230, Error 0.0096(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2024 | Time 37.8896(37.3454) | Bit/dim 1.1159(1.1210) | Xent 0.0569(0.0535) | Loss 1.1443(1.1477) | Error 0.0180(0.0165) Steps 458(475.25) | Grad Norm 1.1716(1.7924) | Total Time 10.00(10.00)\n",
      "Iter 2025 | Time 37.4996(37.3500) | Bit/dim 1.1118(1.1207) | Xent 0.0518(0.0535) | Loss 1.1377(1.1474) | Error 0.0158(0.0164) Steps 476(475.27) | Grad Norm 2.5441(1.8149) | Total Time 10.00(10.00)\n",
      "Iter 2026 | Time 39.4017(37.4115) | Bit/dim 1.1151(1.1205) | Xent 0.0589(0.0536) | Loss 1.1446(1.1474) | Error 0.0174(0.0165) Steps 476(475.29) | Grad Norm 3.7332(1.8725) | Total Time 10.00(10.00)\n",
      "Iter 2027 | Time 39.1669(37.4642) | Bit/dim 1.1206(1.1205) | Xent 0.0475(0.0535) | Loss 1.1443(1.1473) | Error 0.0135(0.0164) Steps 482(475.49) | Grad Norm 4.2745(1.9446) | Total Time 10.00(10.00)\n",
      "Iter 2028 | Time 38.1783(37.4856) | Bit/dim 1.1168(1.1204) | Xent 0.0504(0.0534) | Loss 1.1420(1.1471) | Error 0.0158(0.0164) Steps 488(475.87) | Grad Norm 3.4199(1.9888) | Total Time 10.00(10.00)\n",
      "Iter 2029 | Time 37.9079(37.4983) | Bit/dim 1.1188(1.1204) | Xent 0.0549(0.0534) | Loss 1.1462(1.1471) | Error 0.0164(0.0164) Steps 470(475.69) | Grad Norm 2.0826(1.9916) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 38.6631(37.5332) | Bit/dim 1.1110(1.1201) | Xent 0.0475(0.0532) | Loss 1.1347(1.1467) | Error 0.0134(0.0163) Steps 488(476.06) | Grad Norm 0.7301(1.9538) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 17.2286, Epoch Time 298.0914(279.6695), Bit/dim 1.1094(best: 1.1091), Xent 0.0279, Loss 1.1233, Error 0.0094(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2031 | Time 39.2512(37.5848) | Bit/dim 1.1119(1.1198) | Xent 0.0536(0.0532) | Loss 1.1387(1.1465) | Error 0.0171(0.0163) Steps 488(476.42) | Grad Norm 0.6898(1.9159) | Total Time 10.00(10.00)\n",
      "Iter 2032 | Time 39.5912(37.6450) | Bit/dim 1.1178(1.1198) | Xent 0.0549(0.0533) | Loss 1.1453(1.1464) | Error 0.0164(0.0163) Steps 482(476.59) | Grad Norm 1.2701(1.8965) | Total Time 10.00(10.00)\n",
      "Iter 2033 | Time 37.2058(37.6318) | Bit/dim 1.1152(1.1196) | Xent 0.0461(0.0531) | Loss 1.1382(1.1462) | Error 0.0141(0.0162) Steps 488(476.93) | Grad Norm 1.6868(1.8902) | Total Time 10.00(10.00)\n",
      "Iter 2034 | Time 38.9146(37.6703) | Bit/dim 1.1184(1.1196) | Xent 0.0488(0.0530) | Loss 1.1429(1.1461) | Error 0.0144(0.0162) Steps 482(477.08) | Grad Norm 1.7330(1.8855) | Total Time 10.00(10.00)\n",
      "Iter 2035 | Time 38.1611(37.6850) | Bit/dim 1.1120(1.1194) | Xent 0.0522(0.0529) | Loss 1.1381(1.1459) | Error 0.0158(0.0162) Steps 488(477.41) | Grad Norm 1.2310(1.8658) | Total Time 10.00(10.00)\n",
      "Iter 2036 | Time 40.4322(37.7674) | Bit/dim 1.1163(1.1193) | Xent 0.0553(0.0530) | Loss 1.1439(1.1458) | Error 0.0181(0.0162) Steps 494(477.91) | Grad Norm 0.6371(1.8290) | Total Time 10.00(10.00)\n",
      "Iter 2037 | Time 40.0915(37.8371) | Bit/dim 1.1145(1.1191) | Xent 0.0526(0.0530) | Loss 1.1408(1.1456) | Error 0.0178(0.0163) Steps 482(478.03) | Grad Norm 0.3318(1.7841) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 17.1996, Epoch Time 303.1591(280.3742), Bit/dim 1.1088(best: 1.1091), Xent 0.0285, Loss 1.1230, Error 0.0103(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2038 | Time 38.8692(37.8681) | Bit/dim 1.1138(1.1190) | Xent 0.0453(0.0528) | Loss 1.1364(1.1454) | Error 0.0140(0.0162) Steps 482(478.15) | Grad Norm 0.9671(1.7596) | Total Time 10.00(10.00)\n",
      "Iter 2039 | Time 39.1292(37.9059) | Bit/dim 1.1180(1.1190) | Xent 0.0553(0.0528) | Loss 1.1456(1.1454) | Error 0.0176(0.0162) Steps 482(478.26) | Grad Norm 1.3122(1.7461) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 38.0950(37.9116) | Bit/dim 1.1171(1.1189) | Xent 0.0544(0.0529) | Loss 1.1443(1.1453) | Error 0.0174(0.0163) Steps 476(478.19) | Grad Norm 1.2894(1.7324) | Total Time 10.00(10.00)\n",
      "Iter 2041 | Time 39.1670(37.9493) | Bit/dim 1.1143(1.1188) | Xent 0.0554(0.0530) | Loss 1.1420(1.1452) | Error 0.0171(0.0163) Steps 452(477.41) | Grad Norm 1.4590(1.7242) | Total Time 10.00(10.00)\n",
      "Iter 2042 | Time 39.1687(37.9858) | Bit/dim 1.1106(1.1185) | Xent 0.0531(0.0530) | Loss 1.1371(1.1450) | Error 0.0169(0.0163) Steps 464(477.01) | Grad Norm 2.2442(1.7398) | Total Time 10.00(10.00)\n",
      "Iter 2043 | Time 38.5896(38.0040) | Bit/dim 1.1151(1.1184) | Xent 0.0601(0.0532) | Loss 1.1452(1.1450) | Error 0.0186(0.0164) Steps 482(477.16) | Grad Norm 3.2058(1.7838) | Total Time 10.00(10.00)\n",
      "Iter 2044 | Time 36.9768(37.9731) | Bit/dim 1.1162(1.1183) | Xent 0.0508(0.0531) | Loss 1.1416(1.1449) | Error 0.0155(0.0164) Steps 476(477.12) | Grad Norm 4.2075(1.8565) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 17.0904, Epoch Time 299.3935(280.9447), Bit/dim 1.1148(best: 1.1088), Xent 0.0261, Loss 1.1278, Error 0.0087(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2045 | Time 37.0233(37.9446) | Bit/dim 1.1225(1.1185) | Xent 0.0466(0.0529) | Loss 1.1458(1.1449) | Error 0.0140(0.0163) Steps 476(477.09) | Grad Norm 5.0303(1.9517) | Total Time 10.00(10.00)\n",
      "Iter 2046 | Time 38.4727(37.9605) | Bit/dim 1.1138(1.1183) | Xent 0.0548(0.0530) | Loss 1.1412(1.1448) | Error 0.0171(0.0163) Steps 470(476.88) | Grad Norm 5.6248(2.0619) | Total Time 10.00(10.00)\n",
      "Iter 2047 | Time 38.0825(37.9641) | Bit/dim 1.1197(1.1184) | Xent 0.0492(0.0529) | Loss 1.1443(1.1448) | Error 0.0146(0.0163) Steps 476(476.85) | Grad Norm 5.7357(2.1721) | Total Time 10.00(10.00)\n",
      "Iter 2048 | Time 39.8875(38.0218) | Bit/dim 1.1108(1.1181) | Xent 0.0569(0.0530) | Loss 1.1393(1.1446) | Error 0.0165(0.0163) Steps 470(476.64) | Grad Norm 4.5462(2.2434) | Total Time 10.00(10.00)\n",
      "Iter 2049 | Time 38.9666(38.0502) | Bit/dim 1.1184(1.1182) | Xent 0.0520(0.0529) | Loss 1.1444(1.1446) | Error 0.0150(0.0162) Steps 482(476.80) | Grad Norm 2.8918(2.2628) | Total Time 10.00(10.00)\n",
      "Iter 2050 | Time 40.3182(38.1182) | Bit/dim 1.1163(1.1181) | Xent 0.0553(0.0530) | Loss 1.1440(1.1446) | Error 0.0178(0.0163) Steps 488(477.14) | Grad Norm 1.7272(2.2467) | Total Time 10.00(10.00)\n",
      "Iter 2051 | Time 39.3493(38.1552) | Bit/dim 1.1145(1.1180) | Xent 0.0517(0.0530) | Loss 1.1404(1.1445) | Error 0.0160(0.0163) Steps 488(477.47) | Grad Norm 1.0144(2.2098) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 17.1559, Epoch Time 301.9539(281.5750), Bit/dim 1.1078(best: 1.1088), Xent 0.0292, Loss 1.1224, Error 0.0094(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2052 | Time 38.4243(38.1632) | Bit/dim 1.1171(1.1180) | Xent 0.0576(0.0531) | Loss 1.1459(1.1445) | Error 0.0202(0.0164) Steps 488(477.78) | Grad Norm 0.5974(2.1614) | Total Time 10.00(10.00)\n",
      "Iter 2053 | Time 38.7366(38.1804) | Bit/dim 1.1179(1.1180) | Xent 0.0596(0.0533) | Loss 1.1477(1.1446) | Error 0.0181(0.0164) Steps 482(477.91) | Grad Norm 0.2591(2.1043) | Total Time 10.00(10.00)\n",
      "Iter 2054 | Time 39.5835(38.2225) | Bit/dim 1.1131(1.1178) | Xent 0.0466(0.0531) | Loss 1.1364(1.1444) | Error 0.0146(0.0164) Steps 464(477.49) | Grad Norm 0.4981(2.0562) | Total Time 10.00(10.00)\n",
      "Iter 2055 | Time 37.7579(38.2086) | Bit/dim 1.1117(1.1176) | Xent 0.0524(0.0531) | Loss 1.1379(1.1442) | Error 0.0160(0.0164) Steps 470(477.27) | Grad Norm 1.9489(2.0529) | Total Time 10.00(10.00)\n",
      "Iter 2056 | Time 37.6505(38.1918) | Bit/dim 1.1221(1.1178) | Xent 0.0494(0.0530) | Loss 1.1468(1.1443) | Error 0.0156(0.0164) Steps 470(477.05) | Grad Norm 4.2175(2.1179) | Total Time 10.00(10.00)\n",
      "Iter 2057 | Time 34.1734(38.0713) | Bit/dim 1.1136(1.1176) | Xent 0.0585(0.0531) | Loss 1.1429(1.1442) | Error 0.0190(0.0164) Steps 470(476.84) | Grad Norm 8.8494(2.3198) | Total Time 10.00(10.00)\n",
      "Iter 2058 | Time 36.9392(38.0373) | Bit/dim 1.1436(1.1184) | Xent 0.0489(0.0530) | Loss 1.1680(1.1449) | Error 0.0132(0.0163) Steps 476(476.81) | Grad Norm 8.7106(2.5115) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 16.9342, Epoch Time 292.4983(281.9027), Bit/dim 1.1131(best: 1.1078), Xent 0.0289, Loss 1.1275, Error 0.0096(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2059 | Time 38.4077(38.0484) | Bit/dim 1.1227(1.1186) | Xent 0.0490(0.0529) | Loss 1.1472(1.1450) | Error 0.0155(0.0163) Steps 476(476.79) | Grad Norm 4.7069(2.5774) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 36.9746(38.0162) | Bit/dim 1.1985(1.1210) | Xent 0.0822(0.0538) | Loss 1.2396(1.1478) | Error 0.0251(0.0166) Steps 476(476.76) | Grad Norm 30.1671(3.4051) | Total Time 10.00(10.00)\n",
      "Iter 2061 | Time 33.5667(37.8827) | Bit/dim 1.2590(1.1251) | Xent 0.0525(0.0537) | Loss 1.2853(1.1520) | Error 0.0168(0.0166) Steps 434(475.48) | Grad Norm 9.2546(3.5806) | Total Time 10.00(10.00)\n",
      "Iter 2062 | Time 34.3930(37.7781) | Bit/dim 1.3541(1.1320) | Xent 0.0431(0.0534) | Loss 1.3756(1.1587) | Error 0.0149(0.0165) Steps 434(474.24) | Grad Norm 8.5724(3.7303) | Total Time 10.00(10.00)\n",
      "Iter 2063 | Time 33.8106(37.6590) | Bit/dim 1.3379(1.1381) | Xent 0.0465(0.0532) | Loss 1.3612(1.1647) | Error 0.0134(0.0164) Steps 434(473.03) | Grad Norm 7.1952(3.8343) | Total Time 10.00(10.00)\n",
      "Iter 2064 | Time 33.6448(37.5386) | Bit/dim 1.2922(1.1428) | Xent 0.0464(0.0530) | Loss 1.3154(1.1693) | Error 0.0130(0.0163) Steps 434(471.86) | Grad Norm 5.6560(3.8889) | Total Time 10.00(10.00)\n",
      "Iter 2065 | Time 33.4327(37.4154) | Bit/dim 1.2376(1.1456) | Xent 0.0456(0.0528) | Loss 1.2604(1.1720) | Error 0.0141(0.0163) Steps 440(470.90) | Grad Norm 4.1385(3.8964) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 16.8577, Epoch Time 273.4416(281.6489), Bit/dim 1.1995(best: 1.1078), Xent 0.0293, Loss 1.2141, Error 0.0098(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2066 | Time 37.1545(37.4076) | Bit/dim 1.2008(1.1473) | Xent 0.0585(0.0530) | Loss 1.2300(1.1737) | Error 0.0204(0.0164) Steps 464(470.70) | Grad Norm 2.9484(3.8680) | Total Time 10.00(10.00)\n",
      "Iter 2067 | Time 37.9142(37.4228) | Bit/dim 1.2390(1.1500) | Xent 0.0871(0.0540) | Loss 1.2826(1.1770) | Error 0.0271(0.0167) Steps 482(471.04) | Grad Norm 13.9801(4.1713) | Total Time 10.00(10.00)\n",
      "Iter 2068 | Time 32.0086(37.2604) | Bit/dim 1.1848(1.1511) | Xent 0.0903(0.0551) | Loss 1.2299(1.1786) | Error 0.0280(0.0171) Steps 452(470.46) | Grad Norm 5.9306(4.2241) | Total Time 10.00(10.00)\n",
      "Iter 2069 | Time 34.5018(37.1776) | Bit/dim 1.1796(1.1519) | Xent 0.0839(0.0559) | Loss 1.2215(1.1799) | Error 0.0249(0.0173) Steps 464(470.27) | Grad Norm 3.3801(4.1988) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 35.5140(37.1277) | Bit/dim 1.2049(1.1535) | Xent 0.0668(0.0563) | Loss 1.2383(1.1816) | Error 0.0202(0.0174) Steps 464(470.08) | Grad Norm 4.3416(4.2031) | Total Time 10.00(10.00)\n",
      "Iter 2071 | Time 36.0821(37.0963) | Bit/dim 1.2059(1.1551) | Xent 0.0506(0.0561) | Loss 1.2312(1.1831) | Error 0.0148(0.0173) Steps 458(469.72) | Grad Norm 4.1027(4.2001) | Total Time 10.00(10.00)\n",
      "Iter 2072 | Time 33.7226(36.9951) | Bit/dim 1.1994(1.1564) | Xent 0.0542(0.0560) | Loss 1.2265(1.1844) | Error 0.0180(0.0173) Steps 452(469.19) | Grad Norm 3.6581(4.1838) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 16.8913, Epoch Time 275.8465(281.4748), Bit/dim 1.1677(best: 1.1078), Xent 0.0295, Loss 1.1825, Error 0.0104(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2073 | Time 34.9573(36.9340) | Bit/dim 1.1754(1.1570) | Xent 0.0601(0.0562) | Loss 1.2054(1.1851) | Error 0.0184(0.0173) Steps 446(468.49) | Grad Norm 2.3762(4.1296) | Total Time 10.00(10.00)\n",
      "Iter 2074 | Time 37.1006(36.9390) | Bit/dim 1.1856(1.1578) | Xent 0.0590(0.0562) | Loss 1.2151(1.1860) | Error 0.0172(0.0173) Steps 482(468.90) | Grad Norm 10.0059(4.3059) | Total Time 10.00(10.00)\n",
      "Iter 2075 | Time 34.8146(36.8753) | Bit/dim 1.1520(1.1577) | Xent 0.0561(0.0562) | Loss 1.1800(1.1858) | Error 0.0181(0.0174) Steps 470(468.93) | Grad Norm 1.6926(4.2275) | Total Time 10.00(10.00)\n",
      "Iter 2076 | Time 38.6512(36.9285) | Bit/dim 1.1571(1.1576) | Xent 0.0602(0.0564) | Loss 1.1872(1.1858) | Error 0.0166(0.0173) Steps 452(468.42) | Grad Norm 3.7340(4.2127) | Total Time 10.00(10.00)\n",
      "Iter 2077 | Time 36.7376(36.9228) | Bit/dim 1.1627(1.1578) | Xent 0.0549(0.0563) | Loss 1.1902(1.1860) | Error 0.0175(0.0174) Steps 476(468.65) | Grad Norm 4.4992(4.2213) | Total Time 10.00(10.00)\n",
      "Iter 2078 | Time 36.1600(36.8999) | Bit/dim 1.1416(1.1573) | Xent 0.0716(0.0568) | Loss 1.1773(1.1857) | Error 0.0216(0.0175) Steps 470(468.69) | Grad Norm 3.5815(4.2021) | Total Time 10.00(10.00)\n",
      "Iter 2079 | Time 32.8830(36.7794) | Bit/dim 1.1396(1.1568) | Xent 0.0646(0.0570) | Loss 1.1719(1.1853) | Error 0.0195(0.0175) Steps 446(468.01) | Grad Norm 3.3787(4.1774) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 16.9474, Epoch Time 280.6417(281.4498), Bit/dim 1.1389(best: 1.1078), Xent 0.0324, Loss 1.1551, Error 0.0117(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2080 | Time 36.8897(36.7827) | Bit/dim 1.1443(1.1564) | Xent 0.0667(0.0573) | Loss 1.1777(1.1851) | Error 0.0210(0.0176) Steps 452(467.53) | Grad Norm 7.8699(4.2881) | Total Time 10.00(10.00)\n",
      "Iter 2081 | Time 34.5721(36.7164) | Bit/dim 1.1418(1.1560) | Xent 0.0530(0.0572) | Loss 1.1683(1.1845) | Error 0.0165(0.0176) Steps 458(467.24) | Grad Norm 2.9297(4.2474) | Total Time 10.00(10.00)\n",
      "Iter 2082 | Time 34.2222(36.6416) | Bit/dim 1.1448(1.1556) | Xent 0.0473(0.0569) | Loss 1.1684(1.1841) | Error 0.0152(0.0175) Steps 458(466.97) | Grad Norm 4.5748(4.2572) | Total Time 10.00(10.00)\n",
      "Iter 2083 | Time 34.3282(36.5722) | Bit/dim 1.1348(1.1550) | Xent 0.0429(0.0564) | Loss 1.1562(1.1832) | Error 0.0144(0.0174) Steps 452(466.52) | Grad Norm 2.8312(4.2144) | Total Time 10.00(10.00)\n",
      "Iter 2084 | Time 39.9406(36.6732) | Bit/dim 1.1448(1.1547) | Xent 0.0602(0.0566) | Loss 1.1749(1.1830) | Error 0.0188(0.0175) Steps 482(466.98) | Grad Norm 8.5308(4.3439) | Total Time 10.00(10.00)\n",
      "Iter 2085 | Time 37.1794(36.6884) | Bit/dim 1.1336(1.1541) | Xent 0.0539(0.0565) | Loss 1.1605(1.1823) | Error 0.0180(0.0175) Steps 482(467.43) | Grad Norm 1.0759(4.2459) | Total Time 10.00(10.00)\n",
      "Iter 2086 | Time 38.0036(36.7279) | Bit/dim 1.1368(1.1535) | Xent 0.0564(0.0565) | Loss 1.1650(1.1818) | Error 0.0160(0.0175) Steps 488(468.05) | Grad Norm 4.4401(4.2517) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 16.7162, Epoch Time 284.1575(281.5310), Bit/dim 1.1220(best: 1.1078), Xent 0.0299, Loss 1.1370, Error 0.0100(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2087 | Time 38.9196(36.7936) | Bit/dim 1.1296(1.1528) | Xent 0.0525(0.0564) | Loss 1.1558(1.1810) | Error 0.0166(0.0174) Steps 488(468.65) | Grad Norm 2.0599(4.1860) | Total Time 10.00(10.00)\n",
      "Iter 2088 | Time 39.0890(36.8625) | Bit/dim 1.1320(1.1522) | Xent 0.0517(0.0562) | Loss 1.1579(1.1803) | Error 0.0166(0.0174) Steps 500(469.59) | Grad Norm 6.7070(4.2616) | Total Time 10.00(10.00)\n",
      "Iter 2089 | Time 38.0130(36.8970) | Bit/dim 1.1236(1.1513) | Xent 0.0492(0.0560) | Loss 1.1482(1.1793) | Error 0.0162(0.0174) Steps 482(469.96) | Grad Norm 1.6839(4.1843) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 38.2589(36.9379) | Bit/dim 1.1320(1.1508) | Xent 0.0503(0.0558) | Loss 1.1571(1.1787) | Error 0.0152(0.0173) Steps 482(470.32) | Grad Norm 3.9286(4.1766) | Total Time 10.00(10.00)\n",
      "Iter 2091 | Time 35.2670(36.8877) | Bit/dim 1.1238(1.1500) | Xent 0.0565(0.0559) | Loss 1.1521(1.1779) | Error 0.0164(0.0173) Steps 458(469.95) | Grad Norm 0.8869(4.0779) | Total Time 10.00(10.00)\n",
      "Iter 2092 | Time 37.4682(36.9051) | Bit/dim 1.1288(1.1493) | Xent 0.0573(0.0559) | Loss 1.1575(1.1773) | Error 0.0174(0.0173) Steps 470(469.95) | Grad Norm 5.5343(4.1216) | Total Time 10.00(10.00)\n",
      "Iter 2093 | Time 36.4624(36.8919) | Bit/dim 1.1249(1.1486) | Xent 0.0492(0.0557) | Loss 1.1495(1.1764) | Error 0.0146(0.0172) Steps 446(469.24) | Grad Norm 2.8457(4.0833) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 17.0501, Epoch Time 292.7633(281.8680), Bit/dim 1.1206(best: 1.1078), Xent 0.0283, Loss 1.1347, Error 0.0094(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2094 | Time 37.3135(36.9045) | Bit/dim 1.1244(1.1479) | Xent 0.0493(0.0555) | Loss 1.1490(1.1756) | Error 0.0146(0.0171) Steps 470(469.26) | Grad Norm 3.5502(4.0673) | Total Time 10.00(10.00)\n",
      "Iter 2095 | Time 38.4333(36.9504) | Bit/dim 1.1257(1.1472) | Xent 0.0525(0.0554) | Loss 1.1520(1.1749) | Error 0.0158(0.0171) Steps 482(469.64) | Grad Norm 2.3252(4.0151) | Total Time 10.00(10.00)\n",
      "Iter 2096 | Time 39.4247(37.0246) | Bit/dim 1.1249(1.1465) | Xent 0.0478(0.0552) | Loss 1.1488(1.1741) | Error 0.0142(0.0170) Steps 476(469.83) | Grad Norm 3.3809(3.9960) | Total Time 10.00(10.00)\n",
      "Iter 2097 | Time 36.1887(36.9995) | Bit/dim 1.1201(1.1457) | Xent 0.0445(0.0549) | Loss 1.1423(1.1732) | Error 0.0146(0.0169) Steps 464(469.66) | Grad Norm 3.0291(3.9670) | Total Time 10.00(10.00)\n",
      "Iter 2098 | Time 37.0115(36.9999) | Bit/dim 1.1254(1.1451) | Xent 0.0535(0.0548) | Loss 1.1522(1.1725) | Error 0.0174(0.0169) Steps 482(470.03) | Grad Norm 2.3545(3.9186) | Total Time 10.00(10.00)\n",
      "Iter 2099 | Time 38.9802(37.0593) | Bit/dim 1.1207(1.1444) | Xent 0.0595(0.0550) | Loss 1.1504(1.1719) | Error 0.0186(0.0170) Steps 500(470.93) | Grad Norm 3.4152(3.9035) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 37.8417(37.0828) | Bit/dim 1.1181(1.1436) | Xent 0.0536(0.0549) | Loss 1.1449(1.1711) | Error 0.0160(0.0170) Steps 482(471.26) | Grad Norm 1.1617(3.8213) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 17.1695, Epoch Time 294.5653(282.2489), Bit/dim 1.1147(best: 1.1078), Xent 0.0275, Loss 1.1285, Error 0.0089(best: 0.0086)\n",
      "===> Using batch size 8000. Total 7 iterations/epoch.\n",
      "Iter 2101 | Time 37.8503(37.1058) | Bit/dim 1.1184(1.1429) | Xent 0.0539(0.0549) | Loss 1.1454(1.1703) | Error 0.0169(0.0170) Steps 482(471.58) | Grad Norm 2.7835(3.7902) | Total Time 10.00(10.00)\n",
      "Iter 2102 | Time 39.0728(37.1648) | Bit/dim 1.1185(1.1421) | Xent 0.0521(0.0548) | Loss 1.1446(1.1695) | Error 0.0159(0.0169) Steps 488(472.07) | Grad Norm 0.8069(3.7007) | Total Time 10.00(10.00)\n",
      "Iter 2103 | Time 37.3424(37.1701) | Bit/dim 1.1209(1.1415) | Xent 0.0536(0.0548) | Loss 1.1477(1.1689) | Error 0.0181(0.0170) Steps 482(472.37) | Grad Norm 2.8202(3.6742) | Total Time 10.00(10.00)\n",
      "Iter 2104 | Time 37.4000(37.1770) | Bit/dim 1.1155(1.1407) | Xent 0.0534(0.0547) | Loss 1.1423(1.1681) | Error 0.0165(0.0169) Steps 482(472.66) | Grad Norm 0.5663(3.5810) | Total Time 10.00(10.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle.py --data mnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_5_drop_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_bs8K_sratio_0_5_drop_0_5_run1/epoch_200_checkpt.pth --seed 0 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 113 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
