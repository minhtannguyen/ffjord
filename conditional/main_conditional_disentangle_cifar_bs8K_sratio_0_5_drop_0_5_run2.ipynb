{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=0.0001, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=True, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run2/epoch_90_checkpt.pth', rtol=0.0001, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run2', seed=2, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0541 | Time 125.7793(67.1951) | Bit/dim 4.0291(4.1405) | Xent 1.7682(1.8785) | Loss 4.9132(5.0798) | Error 0.6274(0.6612) Steps 748(693.66) | Grad Norm 2.4867(4.6095) | Total Time 14.00(14.00)\n",
      "Iter 0542 | Time 69.4766(67.2635) | Bit/dim 4.0353(4.1374) | Xent 1.7557(1.8748) | Loss 4.9132(5.0748) | Error 0.6292(0.6602) Steps 748(695.29) | Grad Norm 2.2879(4.5398) | Total Time 14.00(14.00)\n",
      "Iter 0543 | Time 68.9285(67.3135) | Bit/dim 4.0295(4.1342) | Xent 1.7554(1.8712) | Loss 4.9072(5.0697) | Error 0.6219(0.6591) Steps 754(697.06) | Grad Norm 1.7538(4.4563) | Total Time 14.00(14.00)\n",
      "Iter 0544 | Time 67.6646(67.3240) | Bit/dim 4.0158(4.1306) | Xent 1.7448(1.8674) | Loss 4.8882(5.0643) | Error 0.6224(0.6580) Steps 754(698.76) | Grad Norm 1.1133(4.3560) | Total Time 14.00(14.00)\n",
      "Iter 0545 | Time 69.0239(67.3750) | Bit/dim 4.0088(4.1269) | Xent 1.7495(1.8639) | Loss 4.8836(5.0589) | Error 0.6199(0.6568) Steps 748(700.24) | Grad Norm 1.2179(4.2618) | Total Time 14.00(14.00)\n",
      "Iter 0546 | Time 67.8176(67.3883) | Bit/dim 4.0073(4.1234) | Xent 1.7426(1.8602) | Loss 4.8785(5.0535) | Error 0.6205(0.6557) Steps 742(701.49) | Grad Norm 1.5605(4.1808) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 41.5624, Epoch Time 525.0917(385.3437), Bit/dim 4.0059(best: inf), Xent 1.7102, Loss 4.8610, Error 0.6028(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0547 | Time 81.5768(67.8139) | Bit/dim 3.9985(4.1196) | Xent 1.7653(1.8574) | Loss 4.8812(5.0483) | Error 0.6195(0.6546) Steps 730(702.35) | Grad Norm 1.9225(4.1130) | Total Time 14.00(14.00)\n",
      "Iter 0548 | Time 66.3136(67.7689) | Bit/dim 4.0181(4.1166) | Xent 1.7565(1.8543) | Loss 4.8964(5.0437) | Error 0.6304(0.6539) Steps 730(703.18) | Grad Norm 1.7880(4.0433) | Total Time 14.00(14.00)\n",
      "Iter 0549 | Time 69.7535(67.8285) | Bit/dim 4.0101(4.1134) | Xent 1.7625(1.8516) | Loss 4.8913(5.0392) | Error 0.6185(0.6529) Steps 724(703.80) | Grad Norm 1.4704(3.9661) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 65.8473(67.7690) | Bit/dim 3.9912(4.1097) | Xent 1.7348(1.8481) | Loss 4.8586(5.0338) | Error 0.6145(0.6517) Steps 736(704.77) | Grad Norm 1.1073(3.8803) | Total Time 14.00(14.00)\n",
      "Iter 0551 | Time 65.9783(67.7153) | Bit/dim 3.9884(4.1061) | Xent 1.7349(1.8447) | Loss 4.8559(5.0284) | Error 0.6156(0.6506) Steps 730(705.53) | Grad Norm 1.1220(3.7976) | Total Time 14.00(14.00)\n",
      "Iter 0552 | Time 67.8006(67.7179) | Bit/dim 3.9897(4.1026) | Xent 1.7308(1.8413) | Loss 4.8552(5.0232) | Error 0.6151(0.6496) Steps 736(706.44) | Grad Norm 1.4339(3.7267) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 26.9335, Epoch Time 460.9068(387.6106), Bit/dim 3.9937(best: 4.0059), Xent 1.6934, Loss 4.8403, Error 0.5915(best: 0.6028)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0553 | Time 67.9571(67.7250) | Bit/dim 3.9890(4.0992) | Xent 1.7439(1.8384) | Loss 4.8609(5.0183) | Error 0.6210(0.6487) Steps 724(706.97) | Grad Norm 1.5621(3.6617) | Total Time 14.00(14.00)\n",
      "Iter 0554 | Time 68.2257(67.7401) | Bit/dim 3.9886(4.0959) | Xent 1.7369(1.8353) | Loss 4.8571(5.0135) | Error 0.6104(0.6476) Steps 730(707.66) | Grad Norm 0.9943(3.5817) | Total Time 14.00(14.00)\n",
      "Iter 0555 | Time 69.1549(67.7825) | Bit/dim 3.9889(4.0926) | Xent 1.7301(1.8322) | Loss 4.8539(5.0087) | Error 0.6081(0.6464) Steps 718(707.97) | Grad Norm 0.8319(3.4992) | Total Time 14.00(14.00)\n",
      "Iter 0556 | Time 67.8216(67.7837) | Bit/dim 3.9895(4.0895) | Xent 1.7384(1.8293) | Loss 4.8586(5.0042) | Error 0.6221(0.6456) Steps 730(708.63) | Grad Norm 0.8712(3.4204) | Total Time 14.00(14.00)\n",
      "Iter 0557 | Time 69.9779(67.8495) | Bit/dim 3.9895(4.0865) | Xent 1.7229(1.8262) | Loss 4.8510(4.9996) | Error 0.6095(0.6446) Steps 718(708.91) | Grad Norm 1.0201(3.3484) | Total Time 14.00(14.00)\n",
      "Iter 0558 | Time 68.0601(67.8558) | Bit/dim 3.9998(4.0839) | Xent 1.7163(1.8229) | Loss 4.8580(4.9954) | Error 0.6084(0.6435) Steps 730(709.54) | Grad Norm 1.0661(3.2799) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 26.5947, Epoch Time 454.6301(389.6212), Bit/dim 3.9871(best: 3.9937), Xent 1.6811, Loss 4.8277, Error 0.5861(best: 0.5915)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0559 | Time 68.5965(67.8781) | Bit/dim 3.9915(4.0812) | Xent 1.7098(1.8195) | Loss 4.8464(4.9909) | Error 0.6002(0.6422) Steps 724(709.98) | Grad Norm 0.9901(3.2112) | Total Time 14.00(14.00)\n",
      "Iter 0560 | Time 68.4918(67.8965) | Bit/dim 3.9835(4.0782) | Xent 1.7230(1.8166) | Loss 4.8450(4.9865) | Error 0.6020(0.6410) Steps 736(710.76) | Grad Norm 0.7348(3.1369) | Total Time 14.00(14.00)\n",
      "Iter 0561 | Time 65.5953(67.8274) | Bit/dim 3.9950(4.0757) | Xent 1.7237(1.8138) | Loss 4.8569(4.9826) | Error 0.6136(0.6401) Steps 718(710.97) | Grad Norm 0.5311(3.0587) | Total Time 14.00(14.00)\n",
      "Iter 0562 | Time 66.2462(67.7800) | Bit/dim 3.9737(4.0727) | Xent 1.7199(1.8110) | Loss 4.8337(4.9782) | Error 0.5991(0.6389) Steps 712(711.01) | Grad Norm 0.6875(2.9876) | Total Time 14.00(14.00)\n",
      "Iter 0563 | Time 65.7380(67.7187) | Bit/dim 3.9838(4.0700) | Xent 1.7276(1.8085) | Loss 4.8476(4.9743) | Error 0.6060(0.6379) Steps 724(711.40) | Grad Norm 0.7829(2.9215) | Total Time 14.00(14.00)\n",
      "Iter 0564 | Time 66.6072(67.6854) | Bit/dim 3.9834(4.0674) | Xent 1.7032(1.8053) | Loss 4.8350(4.9701) | Error 0.6046(0.6369) Steps 712(711.41) | Grad Norm 0.7745(2.8571) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 26.3981, Epoch Time 444.9757(391.2819), Bit/dim 3.9833(best: 3.9871), Xent 1.6739, Loss 4.8203, Error 0.5880(best: 0.5861)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0565 | Time 67.3274(67.6746) | Bit/dim 3.9771(4.0647) | Xent 1.7155(1.8026) | Loss 4.8348(4.9660) | Error 0.5989(0.6358) Steps 718(711.61) | Grad Norm 0.6218(2.7900) | Total Time 14.00(14.00)\n",
      "Iter 0566 | Time 65.3496(67.6049) | Bit/dim 3.9908(4.0625) | Xent 1.7109(1.7999) | Loss 4.8463(4.9624) | Error 0.5981(0.6347) Steps 730(712.16) | Grad Norm 0.6948(2.7271) | Total Time 14.00(14.00)\n",
      "Iter 0567 | Time 66.8497(67.5822) | Bit/dim 3.9785(4.0600) | Xent 1.6864(1.7965) | Loss 4.8217(4.9582) | Error 0.5986(0.6336) Steps 724(712.52) | Grad Norm 0.7356(2.6674) | Total Time 14.00(14.00)\n",
      "Iter 0568 | Time 69.9514(67.6533) | Bit/dim 3.9695(4.0573) | Xent 1.6971(1.7935) | Loss 4.8181(4.9540) | Error 0.5958(0.6324) Steps 724(712.86) | Grad Norm 0.6730(2.6076) | Total Time 14.00(14.00)\n",
      "Iter 0569 | Time 65.8264(67.5985) | Bit/dim 3.9799(4.0549) | Xent 1.7122(1.7910) | Loss 4.8360(4.9505) | Error 0.6110(0.6318) Steps 724(713.20) | Grad Norm 0.4737(2.5435) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 66.7820(67.5740) | Bit/dim 3.9822(4.0528) | Xent 1.6978(1.7882) | Loss 4.8311(4.9469) | Error 0.6045(0.6310) Steps 712(713.16) | Grad Norm 0.6180(2.4858) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 26.5981, Epoch Time 444.9618(392.8922), Bit/dim 3.9788(best: 3.9833), Xent 1.6660, Loss 4.8118, Error 0.5798(best: 0.5861)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0571 | Time 66.9228(67.5545) | Bit/dim 3.9787(4.0505) | Xent 1.7028(1.7857) | Loss 4.8301(4.9434) | Error 0.5994(0.6300) Steps 718(713.31) | Grad Norm 0.5970(2.4291) | Total Time 14.00(14.00)\n",
      "Iter 0572 | Time 66.1765(67.5131) | Bit/dim 3.9746(4.0483) | Xent 1.6987(1.7831) | Loss 4.8239(4.9398) | Error 0.5994(0.6291) Steps 730(713.81) | Grad Norm 0.6085(2.3745) | Total Time 14.00(14.00)\n",
      "Iter 0573 | Time 69.3431(67.5680) | Bit/dim 3.9816(4.0463) | Xent 1.7119(1.7809) | Loss 4.8376(4.9367) | Error 0.6136(0.6286) Steps 724(714.11) | Grad Norm 0.7264(2.3251) | Total Time 14.00(14.00)\n",
      "Iter 0574 | Time 66.6762(67.5413) | Bit/dim 3.9784(4.0442) | Xent 1.7093(1.7788) | Loss 4.8330(4.9336) | Error 0.6040(0.6279) Steps 724(714.41) | Grad Norm 0.6304(2.2742) | Total Time 14.00(14.00)\n",
      "Iter 0575 | Time 66.5481(67.5115) | Bit/dim 3.9718(4.0420) | Xent 1.6944(1.7763) | Loss 4.8190(4.9302) | Error 0.6031(0.6272) Steps 718(714.52) | Grad Norm 0.6125(2.2244) | Total Time 14.00(14.00)\n",
      "Iter 0576 | Time 65.1953(67.4420) | Bit/dim 3.9743(4.0400) | Xent 1.6933(1.7738) | Loss 4.8209(4.9269) | Error 0.6059(0.6265) Steps 724(714.80) | Grad Norm 0.6461(2.1770) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 25.8706, Epoch Time 443.9236(394.4232), Bit/dim 3.9743(best: 3.9788), Xent 1.6560, Loss 4.8023, Error 0.5784(best: 0.5798)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0577 | Time 67.5000(67.4437) | Bit/dim 3.9763(4.0381) | Xent 1.6974(1.7715) | Loss 4.8250(4.9238) | Error 0.6104(0.6260) Steps 712(714.72) | Grad Norm 0.7556(2.1344) | Total Time 14.00(14.00)\n",
      "Iter 0578 | Time 67.5434(67.4467) | Bit/dim 3.9777(4.0363) | Xent 1.6864(1.7689) | Loss 4.8209(4.9208) | Error 0.5959(0.6251) Steps 718(714.82) | Grad Norm 0.4975(2.0853) | Total Time 14.00(14.00)\n",
      "Iter 0579 | Time 66.7999(67.4273) | Bit/dim 3.9838(4.0347) | Xent 1.6978(1.7668) | Loss 4.8327(4.9181) | Error 0.5979(0.6243) Steps 724(715.09) | Grad Norm 0.6822(2.0432) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 66.3107(67.3938) | Bit/dim 3.9836(4.0332) | Xent 1.7058(1.7650) | Loss 4.8364(4.9157) | Error 0.6078(0.6238) Steps 712(715.00) | Grad Norm 0.5455(1.9982) | Total Time 14.00(14.00)\n",
      "Iter 0581 | Time 65.7236(67.3437) | Bit/dim 3.9549(4.0308) | Xent 1.6867(1.7626) | Loss 4.7983(4.9121) | Error 0.5897(0.6228) Steps 724(715.27) | Grad Norm 0.7540(1.9609) | Total Time 14.00(14.00)\n",
      "Iter 0582 | Time 65.9380(67.3015) | Bit/dim 3.9655(4.0289) | Xent 1.6739(1.7599) | Loss 4.8025(4.9088) | Error 0.5867(0.6217) Steps 730(715.71) | Grad Norm 0.6414(1.9213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 26.2066, Epoch Time 443.3883(395.8921), Bit/dim 3.9714(best: 3.9743), Xent 1.6483, Loss 4.7956, Error 0.5784(best: 0.5784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0583 | Time 65.6586(67.2523) | Bit/dim 3.9653(4.0270) | Xent 1.6804(1.7576) | Loss 4.8055(4.9057) | Error 0.5970(0.6210) Steps 718(715.78) | Grad Norm 0.3547(1.8743) | Total Time 14.00(14.00)\n",
      "Iter 0584 | Time 67.2132(67.2511) | Bit/dim 3.9680(4.0252) | Xent 1.6719(1.7550) | Loss 4.8039(4.9027) | Error 0.5958(0.6202) Steps 718(715.85) | Grad Norm 0.5209(1.8337) | Total Time 14.00(14.00)\n",
      "Iter 0585 | Time 64.2838(67.1621) | Bit/dim 3.9759(4.0237) | Xent 1.6850(1.7529) | Loss 4.8184(4.9002) | Error 0.5981(0.6196) Steps 706(715.55) | Grad Norm 0.7359(1.8008) | Total Time 14.00(14.00)\n",
      "Iter 0586 | Time 66.0576(67.1289) | Bit/dim 3.9648(4.0220) | Xent 1.6885(1.7510) | Loss 4.8090(4.8974) | Error 0.6006(0.6190) Steps 718(715.62) | Grad Norm 0.7707(1.7699) | Total Time 14.00(14.00)\n",
      "Iter 0587 | Time 65.7513(67.0876) | Bit/dim 3.9696(4.0204) | Xent 1.6911(1.7492) | Loss 4.8152(4.8950) | Error 0.5981(0.6184) Steps 706(715.34) | Grad Norm 0.5481(1.7332) | Total Time 14.00(14.00)\n",
      "Iter 0588 | Time 67.5802(67.1024) | Bit/dim 3.9800(4.0192) | Xent 1.6922(1.7475) | Loss 4.8261(4.8929) | Error 0.6014(0.6179) Steps 718(715.42) | Grad Norm 0.4954(1.6961) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 26.1415, Epoch Time 439.2355(397.1924), Bit/dim 3.9700(best: 3.9714), Xent 1.6384, Loss 4.7892, Error 0.5740(best: 0.5784)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0589 | Time 65.8370(67.0644) | Bit/dim 3.9598(4.0174) | Xent 1.6941(1.7459) | Loss 4.8068(4.8903) | Error 0.6016(0.6174) Steps 718(715.49) | Grad Norm 0.4745(1.6595) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 68.1768(67.0978) | Bit/dim 3.9622(4.0157) | Xent 1.6767(1.7438) | Loss 4.8006(4.8876) | Error 0.5956(0.6167) Steps 718(715.57) | Grad Norm 0.5187(1.6252) | Total Time 14.00(14.00)\n",
      "Iter 0591 | Time 65.5673(67.0519) | Bit/dim 3.9767(4.0146) | Xent 1.6825(1.7419) | Loss 4.8180(4.8855) | Error 0.5966(0.6161) Steps 718(715.64) | Grad Norm 0.6671(1.5965) | Total Time 14.00(14.00)\n",
      "Iter 0592 | Time 64.8031(66.9844) | Bit/dim 3.9704(4.0132) | Xent 1.6725(1.7399) | Loss 4.8067(4.8832) | Error 0.5921(0.6154) Steps 718(715.71) | Grad Norm 0.6903(1.5693) | Total Time 14.00(14.00)\n",
      "Iter 0593 | Time 67.2242(66.9916) | Bit/dim 3.9682(4.0119) | Xent 1.6685(1.7377) | Loss 4.8025(4.8807) | Error 0.5951(0.6148) Steps 718(715.78) | Grad Norm 0.5078(1.5375) | Total Time 14.00(14.00)\n",
      "Iter 0594 | Time 64.9186(66.9294) | Bit/dim 3.9715(4.0107) | Xent 1.6857(1.7362) | Loss 4.8144(4.8788) | Error 0.5924(0.6141) Steps 718(715.85) | Grad Norm 0.3731(1.5025) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 25.9786, Epoch Time 439.7240(398.4684), Bit/dim 3.9673(best: 3.9700), Xent 1.6312, Loss 4.7829, Error 0.5699(best: 0.5740)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0595 | Time 63.5179(66.8271) | Bit/dim 3.9559(4.0090) | Xent 1.6723(1.7342) | Loss 4.7921(4.8762) | Error 0.5935(0.6135) Steps 712(715.73) | Grad Norm 0.4495(1.4709) | Total Time 14.00(14.00)\n",
      "Iter 0596 | Time 66.9471(66.8307) | Bit/dim 3.9635(4.0077) | Xent 1.6672(1.7322) | Loss 4.7972(4.8738) | Error 0.5989(0.6131) Steps 712(715.62) | Grad Norm 0.5679(1.4438) | Total Time 14.00(14.00)\n",
      "Iter 0597 | Time 64.0415(66.7470) | Bit/dim 3.9549(4.0061) | Xent 1.6769(1.7306) | Loss 4.7933(4.8714) | Error 0.5948(0.6125) Steps 718(715.69) | Grad Norm 0.4332(1.4135) | Total Time 14.00(14.00)\n",
      "Iter 0598 | Time 67.1841(66.7601) | Bit/dim 3.9655(4.0049) | Xent 1.6658(1.7286) | Loss 4.7984(4.8692) | Error 0.5886(0.6118) Steps 712(715.58) | Grad Norm 0.3736(1.3823) | Total Time 14.00(14.00)\n",
      "Iter 0599 | Time 66.3375(66.7474) | Bit/dim 3.9739(4.0039) | Xent 1.6598(1.7266) | Loss 4.8037(4.8672) | Error 0.5900(0.6111) Steps 718(715.65) | Grad Norm 0.4690(1.3549) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 67.5442(66.7713) | Bit/dim 3.9757(4.0031) | Xent 1.6585(1.7245) | Loss 4.8049(4.8654) | Error 0.5889(0.6105) Steps 718(715.72) | Grad Norm 0.3934(1.3261) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 25.5527, Epoch Time 437.4475(399.6378), Bit/dim 3.9650(best: 3.9673), Xent 1.6179, Loss 4.7740, Error 0.5624(best: 0.5699)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0601 | Time 67.2570(66.7859) | Bit/dim 3.9659(4.0020) | Xent 1.6473(1.7222) | Loss 4.7895(4.8631) | Error 0.5845(0.6097) Steps 718(715.79) | Grad Norm 0.3999(1.2983) | Total Time 14.00(14.00)\n",
      "Iter 0602 | Time 66.6734(66.7825) | Bit/dim 3.9561(4.0006) | Xent 1.6625(1.7204) | Loss 4.7874(4.8608) | Error 0.5904(0.6091) Steps 712(715.68) | Grad Norm 0.3729(1.2705) | Total Time 14.00(14.00)\n",
      "Iter 0603 | Time 65.4369(66.7422) | Bit/dim 3.9673(3.9996) | Xent 1.6511(1.7183) | Loss 4.7929(4.8588) | Error 0.5845(0.6084) Steps 706(715.39) | Grad Norm 0.4596(1.2462) | Total Time 14.00(14.00)\n",
      "Iter 0604 | Time 64.9895(66.6896) | Bit/dim 3.9664(3.9986) | Xent 1.6566(1.7165) | Loss 4.7947(4.8568) | Error 0.5925(0.6079) Steps 706(715.11) | Grad Norm 0.4492(1.2223) | Total Time 14.00(14.00)\n",
      "Iter 0605 | Time 64.5231(66.6246) | Bit/dim 3.9617(3.9975) | Xent 1.6578(1.7147) | Loss 4.7906(4.8549) | Error 0.5862(0.6072) Steps 706(714.83) | Grad Norm 0.3955(1.1975) | Total Time 14.00(14.00)\n",
      "Iter 0606 | Time 64.2989(66.5548) | Bit/dim 3.9645(3.9965) | Xent 1.6575(1.7130) | Loss 4.7933(4.8530) | Error 0.5853(0.6066) Steps 712(714.75) | Grad Norm 0.4030(1.1737) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 25.6973, Epoch Time 436.1401(400.7328), Bit/dim 3.9643(best: 3.9650), Xent 1.6072, Loss 4.7679, Error 0.5626(best: 0.5624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0607 | Time 65.8973(66.5351) | Bit/dim 3.9677(3.9956) | Xent 1.6426(1.7109) | Loss 4.7890(4.8511) | Error 0.5806(0.6058) Steps 700(714.31) | Grad Norm 0.4702(1.1526) | Total Time 14.00(14.00)\n",
      "Iter 0608 | Time 62.9624(66.4279) | Bit/dim 3.9608(3.9946) | Xent 1.6328(1.7085) | Loss 4.7772(4.8489) | Error 0.5757(0.6049) Steps 706(714.06) | Grad Norm 0.4000(1.1300) | Total Time 14.00(14.00)\n",
      "Iter 0609 | Time 64.2899(66.3638) | Bit/dim 3.9662(3.9937) | Xent 1.6486(1.7068) | Loss 4.7905(4.8471) | Error 0.5782(0.6041) Steps 712(713.99) | Grad Norm 0.5921(1.1138) | Total Time 14.00(14.00)\n",
      "Iter 0610 | Time 63.1526(66.2674) | Bit/dim 3.9586(3.9927) | Xent 1.6298(1.7044) | Loss 4.7735(4.8449) | Error 0.5842(0.6035) Steps 706(713.75) | Grad Norm 0.6256(1.0992) | Total Time 14.00(14.00)\n",
      "Iter 0611 | Time 65.5521(66.2460) | Bit/dim 3.9652(3.9919) | Xent 1.6461(1.7027) | Loss 4.7883(4.8432) | Error 0.5793(0.6028) Steps 700(713.34) | Grad Norm 0.3537(1.0768) | Total Time 14.00(14.00)\n",
      "Iter 0612 | Time 67.0363(66.2697) | Bit/dim 3.9565(3.9908) | Xent 1.6375(1.7007) | Loss 4.7753(4.8412) | Error 0.5886(0.6024) Steps 706(713.12) | Grad Norm 0.3776(1.0558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 26.0183, Epoch Time 431.6658(401.6608), Bit/dim 3.9611(best: 3.9643), Xent 1.5936, Loss 4.7579, Error 0.5631(best: 0.5624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0613 | Time 64.6235(66.2203) | Bit/dim 3.9635(3.9900) | Xent 1.6294(1.6986) | Loss 4.7782(4.8393) | Error 0.5815(0.6017) Steps 706(712.91) | Grad Norm 0.7547(1.0468) | Total Time 14.00(14.00)\n",
      "Iter 0614 | Time 66.0733(66.2159) | Bit/dim 3.9581(3.9890) | Xent 1.6210(1.6963) | Loss 4.7686(4.8372) | Error 0.5782(0.6010) Steps 700(712.52) | Grad Norm 0.4188(1.0280) | Total Time 14.00(14.00)\n",
      "Iter 0615 | Time 65.8859(66.2060) | Bit/dim 3.9623(3.9882) | Xent 1.6433(1.6947) | Loss 4.7840(4.8356) | Error 0.5815(0.6004) Steps 712(712.51) | Grad Norm 0.5827(1.0146) | Total Time 14.00(14.00)\n",
      "Iter 0616 | Time 65.4678(66.1838) | Bit/dim 3.9681(3.9876) | Xent 1.6296(1.6927) | Loss 4.7829(4.8340) | Error 0.5827(0.5999) Steps 700(712.13) | Grad Norm 0.3917(0.9959) | Total Time 14.00(14.00)\n",
      "Iter 0617 | Time 65.1931(66.1541) | Bit/dim 3.9590(3.9868) | Xent 1.6237(1.6907) | Loss 4.7708(4.8321) | Error 0.5755(0.5992) Steps 718(712.31) | Grad Norm 0.4874(0.9807) | Total Time 14.00(14.00)\n",
      "Iter 0618 | Time 64.5898(66.1072) | Bit/dim 3.9541(3.9858) | Xent 1.6406(1.6892) | Loss 4.7744(4.8304) | Error 0.5835(0.5987) Steps 706(712.12) | Grad Norm 0.4755(0.9655) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 25.9094, Epoch Time 434.4569(402.6447), Bit/dim 3.9616(best: 3.9611), Xent 1.5799, Loss 4.7516, Error 0.5548(best: 0.5624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0619 | Time 65.5188(66.0895) | Bit/dim 3.9543(3.9848) | Xent 1.6062(1.6867) | Loss 4.7574(4.8282) | Error 0.5697(0.5978) Steps 706(711.93) | Grad Norm 0.6102(0.9549) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 64.0784(66.0292) | Bit/dim 3.9643(3.9842) | Xent 1.6135(1.6845) | Loss 4.7711(4.8265) | Error 0.5710(0.5970) Steps 706(711.76) | Grad Norm 0.7545(0.9489) | Total Time 14.00(14.00)\n",
      "Iter 0621 | Time 66.5546(66.0450) | Bit/dim 3.9607(3.9835) | Xent 1.6167(1.6824) | Loss 4.7690(4.8247) | Error 0.5761(0.5964) Steps 700(711.40) | Grad Norm 0.8364(0.9455) | Total Time 14.00(14.00)\n",
      "Iter 0622 | Time 68.5225(66.1193) | Bit/dim 3.9624(3.9829) | Xent 1.6183(1.6805) | Loss 4.7716(4.8231) | Error 0.5747(0.5958) Steps 712(711.42) | Grad Norm 0.4307(0.9300) | Total Time 14.00(14.00)\n",
      "Iter 0623 | Time 66.4742(66.1299) | Bit/dim 3.9577(3.9821) | Xent 1.6061(1.6783) | Loss 4.7608(4.8213) | Error 0.5722(0.5951) Steps 694(710.90) | Grad Norm 0.8822(0.9286) | Total Time 14.00(14.00)\n",
      "Iter 0624 | Time 65.8512(66.1216) | Bit/dim 3.9576(3.9814) | Xent 1.6197(1.6765) | Loss 4.7674(4.8197) | Error 0.5671(0.5942) Steps 712(710.93) | Grad Norm 0.7129(0.9221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 26.0684, Epoch Time 439.9646(403.7643), Bit/dim 3.9603(best: 3.9611), Xent 1.5656, Loss 4.7431, Error 0.5523(best: 0.5548)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0625 | Time 66.5649(66.1349) | Bit/dim 3.9629(3.9808) | Xent 1.5982(1.6742) | Loss 4.7620(4.8179) | Error 0.5782(0.5937) Steps 712(710.96) | Grad Norm 1.3189(0.9340) | Total Time 14.00(14.00)\n",
      "Iter 0626 | Time 64.0594(66.0726) | Bit/dim 3.9562(3.9801) | Xent 1.6020(1.6720) | Loss 4.7572(4.8161) | Error 0.5680(0.5930) Steps 706(710.81) | Grad Norm 0.4572(0.9197) | Total Time 14.00(14.00)\n",
      "Iter 0627 | Time 66.3074(66.0797) | Bit/dim 3.9555(3.9794) | Xent 1.5972(1.6698) | Loss 4.7541(4.8142) | Error 0.5675(0.5922) Steps 712(710.85) | Grad Norm 0.6292(0.9110) | Total Time 14.00(14.00)\n",
      "Iter 0628 | Time 68.8218(66.1619) | Bit/dim 3.9571(3.9787) | Xent 1.6218(1.6683) | Loss 4.7680(4.8129) | Error 0.5741(0.5917) Steps 706(710.70) | Grad Norm 0.7551(0.9063) | Total Time 14.00(14.00)\n",
      "Iter 0629 | Time 65.6466(66.1465) | Bit/dim 3.9580(3.9781) | Xent 1.5821(1.6657) | Loss 4.7491(4.8109) | Error 0.5600(0.5907) Steps 706(710.56) | Grad Norm 0.4277(0.8920) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 65.0263(66.1129) | Bit/dim 3.9636(3.9776) | Xent 1.5840(1.6633) | Loss 4.7556(4.8093) | Error 0.5659(0.5900) Steps 712(710.61) | Grad Norm 0.9596(0.8940) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 25.7603, Epoch Time 439.1461(404.8258), Bit/dim 3.9581(best: 3.9603), Xent 1.5516, Loss 4.7339, Error 0.5507(best: 0.5523)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0631 | Time 65.7009(66.1005) | Bit/dim 3.9490(3.9768) | Xent 1.5845(1.6609) | Loss 4.7412(4.8072) | Error 0.5633(0.5892) Steps 706(710.47) | Grad Norm 0.4124(0.8796) | Total Time 14.00(14.00)\n",
      "Iter 0632 | Time 66.2428(66.1048) | Bit/dim 3.9524(3.9760) | Xent 1.5755(1.6584) | Loss 4.7401(4.8052) | Error 0.5640(0.5884) Steps 712(710.51) | Grad Norm 0.9387(0.8813) | Total Time 14.00(14.00)\n",
      "Iter 0633 | Time 64.7321(66.0636) | Bit/dim 3.9636(3.9757) | Xent 1.5976(1.6565) | Loss 4.7624(4.8039) | Error 0.5675(0.5878) Steps 712(710.56) | Grad Norm 0.5300(0.8708) | Total Time 14.00(14.00)\n",
      "Iter 0634 | Time 68.0762(66.1240) | Bit/dim 3.9519(3.9750) | Xent 1.5769(1.6542) | Loss 4.7403(4.8020) | Error 0.5640(0.5871) Steps 700(710.24) | Grad Norm 1.3794(0.8861) | Total Time 14.00(14.00)\n",
      "Iter 0635 | Time 64.9834(66.0898) | Bit/dim 3.9543(3.9743) | Xent 1.5872(1.6521) | Loss 4.7479(4.8004) | Error 0.5617(0.5863) Steps 700(709.93) | Grad Norm 0.5816(0.8769) | Total Time 14.00(14.00)\n",
      "Iter 0636 | Time 65.9170(66.0846) | Bit/dim 3.9718(3.9743) | Xent 1.5971(1.6505) | Loss 4.7703(4.7995) | Error 0.5702(0.5858) Steps 688(709.28) | Grad Norm 1.2358(0.8877) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 26.2754, Epoch Time 439.0771(405.8533), Bit/dim 3.9573(best: 3.9581), Xent 1.5376, Loss 4.7261, Error 0.5453(best: 0.5507)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0637 | Time 64.1292(66.0259) | Bit/dim 3.9512(3.9736) | Xent 1.5698(1.6481) | Loss 4.7361(4.7976) | Error 0.5513(0.5848) Steps 706(709.18) | Grad Norm 0.5458(0.8774) | Total Time 14.00(14.00)\n",
      "Iter 0638 | Time 67.6132(66.0735) | Bit/dim 3.9624(3.9732) | Xent 1.5797(1.6460) | Loss 4.7523(4.7962) | Error 0.5634(0.5841) Steps 700(708.90) | Grad Norm 1.0682(0.8831) | Total Time 14.00(14.00)\n",
      "Iter 0639 | Time 64.2463(66.0187) | Bit/dim 3.9564(3.9727) | Xent 1.5759(1.6439) | Loss 4.7443(4.7947) | Error 0.5639(0.5835) Steps 700(708.64) | Grad Norm 0.4443(0.8700) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 66.3517(66.0287) | Bit/dim 3.9440(3.9719) | Xent 1.5672(1.6416) | Loss 4.7276(4.7927) | Error 0.5586(0.5828) Steps 682(707.84) | Grad Norm 0.3885(0.8555) | Total Time 14.00(14.00)\n",
      "Iter 0641 | Time 67.4565(66.0715) | Bit/dim 3.9672(3.9717) | Xent 1.5914(1.6401) | Loss 4.7629(4.7918) | Error 0.5625(0.5822) Steps 700(707.60) | Grad Norm 0.3660(0.8409) | Total Time 14.00(14.00)\n",
      "Iter 0642 | Time 65.3747(66.0506) | Bit/dim 3.9508(3.9711) | Xent 1.5576(1.6376) | Loss 4.7296(4.7899) | Error 0.5516(0.5813) Steps 682(706.83) | Grad Norm 0.3952(0.8275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 25.7213, Epoch Time 438.1761(406.8230), Bit/dim 3.9554(best: 3.9573), Xent 1.5270, Loss 4.7189, Error 0.5411(best: 0.5453)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0643 | Time 62.0605(65.9309) | Bit/dim 3.9633(3.9709) | Xent 1.5761(1.6358) | Loss 4.7513(4.7888) | Error 0.5617(0.5807) Steps 676(705.91) | Grad Norm 0.5542(0.8193) | Total Time 14.00(14.00)\n",
      "Iter 0644 | Time 62.2124(65.8194) | Bit/dim 3.9554(3.9704) | Xent 1.5865(1.6343) | Loss 4.7487(4.7876) | Error 0.5656(0.5802) Steps 700(705.73) | Grad Norm 0.8791(0.8211) | Total Time 14.00(14.00)\n",
      "Iter 0645 | Time 66.0894(65.8275) | Bit/dim 3.9580(3.9700) | Xent 1.5470(1.6317) | Loss 4.7315(4.7859) | Error 0.5525(0.5794) Steps 676(704.84) | Grad Norm 0.3871(0.8081) | Total Time 14.00(14.00)\n",
      "Iter 0646 | Time 62.3143(65.7221) | Bit/dim 3.9525(3.9695) | Xent 1.5493(1.6292) | Loss 4.7272(4.7841) | Error 0.5588(0.5788) Steps 694(704.51) | Grad Norm 1.2578(0.8216) | Total Time 14.00(14.00)\n",
      "Iter 0647 | Time 64.5896(65.6881) | Bit/dim 3.9453(3.9688) | Xent 1.5474(1.6268) | Loss 4.7190(4.7822) | Error 0.5497(0.5779) Steps 682(703.84) | Grad Norm 0.9560(0.8256) | Total Time 14.00(14.00)\n",
      "Iter 0648 | Time 64.2185(65.6440) | Bit/dim 3.9436(3.9680) | Xent 1.5550(1.6246) | Loss 4.7211(4.7803) | Error 0.5593(0.5773) Steps 676(703.00) | Grad Norm 1.0564(0.8325) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 26.0413, Epoch Time 424.6192(407.3569), Bit/dim 3.9545(best: 3.9554), Xent 1.5130, Loss 4.7110, Error 0.5387(best: 0.5411)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0649 | Time 63.4091(65.5770) | Bit/dim 3.9473(3.9674) | Xent 1.5632(1.6228) | Loss 4.7289(4.7788) | Error 0.5639(0.5769) Steps 676(702.19) | Grad Norm 1.6524(0.8571) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 65.2071(65.5659) | Bit/dim 3.9468(3.9668) | Xent 1.5526(1.6207) | Loss 4.7231(4.7771) | Error 0.5536(0.5762) Steps 700(702.13) | Grad Norm 0.3915(0.8431) | Total Time 14.00(14.00)\n",
      "Iter 0651 | Time 65.1289(65.5528) | Bit/dim 3.9636(3.9667) | Xent 1.5524(1.6186) | Loss 4.7398(4.7760) | Error 0.5526(0.5755) Steps 682(701.52) | Grad Norm 1.3261(0.8576) | Total Time 14.00(14.00)\n",
      "Iter 0652 | Time 62.4300(65.4591) | Bit/dim 3.9584(3.9664) | Xent 1.5508(1.6166) | Loss 4.7338(4.7747) | Error 0.5548(0.5749) Steps 688(701.12) | Grad Norm 0.6456(0.8513) | Total Time 14.00(14.00)\n",
      "Iter 0653 | Time 62.6176(65.3738) | Bit/dim 3.9475(3.9659) | Xent 1.5568(1.6148) | Loss 4.7259(4.7733) | Error 0.5568(0.5744) Steps 682(700.54) | Grad Norm 2.1337(0.8897) | Total Time 14.00(14.00)\n",
      "Iter 0654 | Time 65.3380(65.3728) | Bit/dim 3.9488(3.9654) | Xent 1.5278(1.6122) | Loss 4.7126(4.7714) | Error 0.5370(0.5732) Steps 712(700.89) | Grad Norm 0.5623(0.8799) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 25.4794, Epoch Time 426.2344(407.9232), Bit/dim 3.9501(best: 3.9545), Xent 1.5161, Loss 4.7082, Error 0.5435(best: 0.5387)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0655 | Time 62.2406(65.2788) | Bit/dim 3.9548(3.9650) | Xent 1.5615(1.6107) | Loss 4.7356(4.7704) | Error 0.5574(0.5728) Steps 676(700.14) | Grad Norm 2.5343(0.9295) | Total Time 14.00(14.00)\n",
      "Iter 0656 | Time 66.0515(65.3020) | Bit/dim 3.9465(3.9645) | Xent 1.5276(1.6082) | Loss 4.7103(4.7686) | Error 0.5524(0.5722) Steps 682(699.60) | Grad Norm 1.1435(0.9360) | Total Time 14.00(14.00)\n",
      "Iter 0657 | Time 67.0173(65.3534) | Bit/dim 3.9489(3.9640) | Xent 1.5433(1.6062) | Loss 4.7206(4.7671) | Error 0.5491(0.5715) Steps 706(699.79) | Grad Norm 2.1014(0.9709) | Total Time 14.00(14.00)\n",
      "Iter 0658 | Time 66.0326(65.3738) | Bit/dim 3.9423(3.9634) | Xent 1.5439(1.6044) | Loss 4.7143(4.7655) | Error 0.5524(0.5709) Steps 670(698.90) | Grad Norm 1.5141(0.9872) | Total Time 14.00(14.00)\n",
      "Iter 0659 | Time 67.6600(65.4424) | Bit/dim 3.9392(3.9626) | Xent 1.5283(1.6021) | Loss 4.7033(4.7637) | Error 0.5550(0.5704) Steps 676(698.21) | Grad Norm 1.9785(1.0170) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 65.6388(65.4483) | Bit/dim 3.9539(3.9624) | Xent 1.5466(1.6004) | Loss 4.7272(4.7626) | Error 0.5486(0.5698) Steps 700(698.26) | Grad Norm 3.0028(1.0765) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 25.7694, Epoch Time 436.9773(408.7948), Bit/dim 3.9469(best: 3.9501), Xent 1.4968, Loss 4.6953, Error 0.5332(best: 0.5387)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0661 | Time 65.5316(65.4508) | Bit/dim 3.9428(3.9618) | Xent 1.5537(1.5990) | Loss 4.7197(4.7613) | Error 0.5529(0.5693) Steps 688(697.95) | Grad Norm 0.4810(1.0587) | Total Time 14.00(14.00)\n",
      "Iter 0662 | Time 67.4623(65.5111) | Bit/dim 3.9469(3.9613) | Xent 1.5339(1.5971) | Loss 4.7139(4.7599) | Error 0.5454(0.5685) Steps 694(697.84) | Grad Norm 2.2818(1.0954) | Total Time 14.00(14.00)\n",
      "Iter 0663 | Time 63.5524(65.4524) | Bit/dim 3.9488(3.9610) | Xent 1.5350(1.5952) | Loss 4.7163(4.7586) | Error 0.5486(0.5679) Steps 700(697.90) | Grad Norm 1.6964(1.1134) | Total Time 14.00(14.00)\n",
      "Iter 0664 | Time 67.0328(65.4998) | Bit/dim 3.9467(3.9605) | Xent 1.5208(1.5930) | Loss 4.7071(4.7570) | Error 0.5477(0.5673) Steps 706(698.14) | Grad Norm 2.0279(1.1408) | Total Time 14.00(14.00)\n",
      "Iter 0665 | Time 61.3509(65.3753) | Bit/dim 3.9497(3.9602) | Xent 1.5202(1.5908) | Loss 4.7098(4.7556) | Error 0.5431(0.5666) Steps 652(696.76) | Grad Norm 2.2184(1.1732) | Total Time 14.00(14.00)\n",
      "Iter 0666 | Time 62.4547(65.2877) | Bit/dim 3.9430(3.9597) | Xent 1.5254(1.5888) | Loss 4.7057(4.7541) | Error 0.5525(0.5662) Steps 670(695.96) | Grad Norm 2.0335(1.1990) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 25.8073, Epoch Time 429.2009(409.4070), Bit/dim 3.9463(best: 3.9469), Xent 1.4933, Loss 4.6929, Error 0.5315(best: 0.5332)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0667 | Time 66.2911(65.3178) | Bit/dim 3.9381(3.9591) | Xent 1.5254(1.5869) | Loss 4.7008(4.7525) | Error 0.5517(0.5658) Steps 712(696.44) | Grad Norm 2.2255(1.2298) | Total Time 14.00(14.00)\n",
      "Iter 0668 | Time 66.7998(65.3623) | Bit/dim 3.9430(3.9586) | Xent 1.5369(1.5854) | Loss 4.7114(4.7513) | Error 0.5516(0.5653) Steps 682(696.01) | Grad Norm 2.3408(1.2631) | Total Time 14.00(14.00)\n",
      "Iter 0669 | Time 61.5111(65.2467) | Bit/dim 3.9455(3.9582) | Xent 1.5070(1.5831) | Loss 4.6990(4.7497) | Error 0.5364(0.5645) Steps 682(695.58) | Grad Norm 1.5143(1.2706) | Total Time 14.00(14.00)\n",
      "Iter 0670 | Time 60.1214(65.0930) | Bit/dim 3.9395(3.9576) | Xent 1.5260(1.5813) | Loss 4.7025(4.7483) | Error 0.5460(0.5639) Steps 670(694.82) | Grad Norm 2.2567(1.3002) | Total Time 14.00(14.00)\n",
      "Iter 0671 | Time 62.3223(65.0098) | Bit/dim 3.9450(3.9572) | Xent 1.5326(1.5799) | Loss 4.7113(4.7472) | Error 0.5457(0.5634) Steps 676(694.25) | Grad Norm 0.8925(1.2880) | Total Time 14.00(14.00)\n",
      "Iter 0672 | Time 66.9483(65.0680) | Bit/dim 3.9516(3.9571) | Xent 1.5246(1.5782) | Loss 4.7139(4.7462) | Error 0.5444(0.5628) Steps 688(694.07) | Grad Norm 1.3073(1.2886) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 26.0502, Epoch Time 427.0813(409.9372), Bit/dim 3.9436(best: 3.9463), Xent 1.4863, Loss 4.6867, Error 0.5351(best: 0.5315)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0673 | Time 63.4366(65.0191) | Bit/dim 3.9447(3.9567) | Xent 1.5169(1.5764) | Loss 4.7031(4.7449) | Error 0.5423(0.5622) Steps 682(693.70) | Grad Norm 1.5812(1.2973) | Total Time 14.00(14.00)\n",
      "Iter 0674 | Time 64.1656(64.9935) | Bit/dim 3.9365(3.9561) | Xent 1.5183(1.5746) | Loss 4.6956(4.7434) | Error 0.5374(0.5614) Steps 706(694.07) | Grad Norm 0.6567(1.2781) | Total Time 14.00(14.00)\n",
      "Iter 0675 | Time 61.5962(64.8915) | Bit/dim 3.9501(3.9559) | Xent 1.4948(1.5722) | Loss 4.6975(4.7420) | Error 0.5344(0.5606) Steps 682(693.71) | Grad Norm 1.5920(1.2875) | Total Time 14.00(14.00)\n",
      "Iter 0676 | Time 59.6640(64.7347) | Bit/dim 3.9373(3.9554) | Xent 1.5151(1.5705) | Loss 4.6949(4.7406) | Error 0.5433(0.5601) Steps 676(693.18) | Grad Norm 1.0153(1.2794) | Total Time 14.00(14.00)\n",
      "Iter 0677 | Time 61.5496(64.6392) | Bit/dim 3.9429(3.9550) | Xent 1.5143(1.5688) | Loss 4.7001(4.7394) | Error 0.5404(0.5595) Steps 682(692.84) | Grad Norm 1.4184(1.2835) | Total Time 14.00(14.00)\n",
      "Iter 0678 | Time 61.8820(64.5564) | Bit/dim 3.9368(3.9544) | Xent 1.5189(1.5674) | Loss 4.6962(4.7381) | Error 0.5446(0.5591) Steps 664(691.98) | Grad Norm 2.0311(1.3060) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 25.3772, Epoch Time 415.2243(410.0958), Bit/dim 3.9409(best: 3.9436), Xent 1.4699, Loss 4.6758, Error 0.5263(best: 0.5315)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0679 | Time 63.5777(64.5271) | Bit/dim 3.9414(3.9540) | Xent 1.5213(1.5660) | Loss 4.7021(4.7370) | Error 0.5454(0.5587) Steps 670(691.32) | Grad Norm 0.8206(1.2914) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 59.9404(64.3895) | Bit/dim 3.9373(3.9535) | Xent 1.5144(1.5644) | Loss 4.6945(4.7358) | Error 0.5454(0.5583) Steps 670(690.68) | Grad Norm 2.1929(1.3184) | Total Time 14.00(14.00)\n",
      "Iter 0681 | Time 60.6929(64.2786) | Bit/dim 3.9338(3.9530) | Xent 1.5145(1.5629) | Loss 4.6911(4.7344) | Error 0.5425(0.5578) Steps 670(690.06) | Grad Norm 0.9858(1.3085) | Total Time 14.00(14.00)\n",
      "Iter 0682 | Time 59.1012(64.1233) | Bit/dim 3.9329(3.9524) | Xent 1.4865(1.5606) | Loss 4.6762(4.7327) | Error 0.5291(0.5569) Steps 670(689.46) | Grad Norm 1.7559(1.3219) | Total Time 14.00(14.00)\n",
      "Iter 0683 | Time 60.0466(64.0010) | Bit/dim 3.9363(3.9519) | Xent 1.5006(1.5588) | Loss 4.6866(4.7313) | Error 0.5348(0.5563) Steps 664(688.69) | Grad Norm 1.5216(1.3279) | Total Time 14.00(14.00)\n",
      "Iter 0684 | Time 63.4444(63.9843) | Bit/dim 3.9473(3.9517) | Xent 1.5018(1.5571) | Loss 4.6982(4.7303) | Error 0.5406(0.5558) Steps 646(687.41) | Grad Norm 0.7871(1.3117) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 24.7517, Epoch Time 408.7285(410.0548), Bit/dim 3.9396(best: 3.9409), Xent 1.4656, Loss 4.6724, Error 0.5248(best: 0.5263)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0685 | Time 60.9390(63.8929) | Bit/dim 3.9342(3.9512) | Xent 1.4941(1.5552) | Loss 4.6812(4.7288) | Error 0.5369(0.5552) Steps 676(687.07) | Grad Norm 1.6665(1.3223) | Total Time 14.00(14.00)\n",
      "Iter 0686 | Time 59.4586(63.7599) | Bit/dim 3.9410(3.9509) | Xent 1.4947(1.5534) | Loss 4.6884(4.7276) | Error 0.5388(0.5547) Steps 628(685.30) | Grad Norm 1.6849(1.3332) | Total Time 14.00(14.00)\n",
      "Iter 0687 | Time 60.9719(63.6762) | Bit/dim 3.9419(3.9506) | Xent 1.5188(1.5524) | Loss 4.7013(4.7268) | Error 0.5485(0.5545) Steps 676(685.02) | Grad Norm 3.0135(1.3836) | Total Time 14.00(14.00)\n",
      "Iter 0688 | Time 59.1078(63.5392) | Bit/dim 3.9412(3.9504) | Xent 1.5087(1.5511) | Loss 4.6956(4.7259) | Error 0.5415(0.5541) Steps 652(684.03) | Grad Norm 2.1794(1.4075) | Total Time 14.00(14.00)\n",
      "Iter 0689 | Time 59.1137(63.4064) | Bit/dim 3.9337(3.9499) | Xent 1.4971(1.5494) | Loss 4.6823(4.7246) | Error 0.5335(0.5535) Steps 658(683.25) | Grad Norm 1.9562(1.4239) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 61.3386(63.3444) | Bit/dim 3.9340(3.9494) | Xent 1.5027(1.5480) | Loss 4.6853(4.7234) | Error 0.5383(0.5531) Steps 640(681.95) | Grad Norm 5.0059(1.5314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 25.7729, Epoch Time 403.5945(409.8610), Bit/dim 3.9354(best: 3.9396), Xent 1.4572, Loss 4.6640, Error 0.5225(best: 0.5248)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0691 | Time 59.1670(63.2191) | Bit/dim 3.9392(3.9491) | Xent 1.5041(1.5467) | Loss 4.6912(4.7224) | Error 0.5431(0.5528) Steps 664(681.41) | Grad Norm 1.2589(1.5232) | Total Time 14.00(14.00)\n",
      "Iter 0692 | Time 59.4674(63.1065) | Bit/dim 3.9385(3.9488) | Xent 1.5017(1.5454) | Loss 4.6893(4.7214) | Error 0.5354(0.5522) Steps 664(680.89) | Grad Norm 3.8859(1.5941) | Total Time 14.00(14.00)\n",
      "Iter 0693 | Time 62.5038(63.0884) | Bit/dim 3.9274(3.9481) | Xent 1.4910(1.5437) | Loss 4.6729(4.7200) | Error 0.5279(0.5515) Steps 652(680.02) | Grad Norm 1.8557(1.6019) | Total Time 14.00(14.00)\n",
      "Iter 0694 | Time 61.0896(63.0285) | Bit/dim 3.9290(3.9475) | Xent 1.4952(1.5423) | Loss 4.6766(4.7187) | Error 0.5339(0.5510) Steps 676(679.90) | Grad Norm 2.3254(1.6236) | Total Time 14.00(14.00)\n",
      "Iter 0695 | Time 60.3899(62.9493) | Bit/dim 3.9428(3.9474) | Xent 1.4990(1.5410) | Loss 4.6923(4.7179) | Error 0.5356(0.5505) Steps 670(679.60) | Grad Norm 4.2460(1.7023) | Total Time 14.00(14.00)\n",
      "Iter 0696 | Time 60.0908(62.8636) | Bit/dim 3.9290(3.9468) | Xent 1.4997(1.5397) | Loss 4.6789(4.7167) | Error 0.5323(0.5500) Steps 658(678.96) | Grad Norm 2.9351(1.7393) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 25.4647, Epoch Time 405.0690(409.7173), Bit/dim 3.9318(best: 3.9354), Xent 1.4559, Loss 4.6598, Error 0.5252(best: 0.5225)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0697 | Time 61.4329(62.8206) | Bit/dim 3.9415(3.9467) | Xent 1.4820(1.5380) | Loss 4.6825(4.7157) | Error 0.5299(0.5494) Steps 658(678.33) | Grad Norm 1.8169(1.7416) | Total Time 14.00(14.00)\n",
      "Iter 0698 | Time 60.6730(62.7562) | Bit/dim 3.9342(3.9463) | Xent 1.4859(1.5365) | Loss 4.6772(4.7145) | Error 0.5281(0.5487) Steps 658(677.72) | Grad Norm 3.8607(1.8052) | Total Time 14.00(14.00)\n",
      "Iter 0699 | Time 58.2934(62.6223) | Bit/dim 3.9279(3.9458) | Xent 1.4885(1.5350) | Loss 4.6721(4.7133) | Error 0.5300(0.5482) Steps 652(676.95) | Grad Norm 2.0682(1.8131) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 62.3388(62.6138) | Bit/dim 3.9217(3.9450) | Xent 1.4689(1.5330) | Loss 4.6562(4.7116) | Error 0.5172(0.5472) Steps 664(676.56) | Grad Norm 2.2471(1.8261) | Total Time 14.00(14.00)\n",
      "Iter 0701 | Time 62.8289(62.6203) | Bit/dim 3.9271(3.9445) | Xent 1.4868(1.5316) | Loss 4.6705(4.7103) | Error 0.5366(0.5469) Steps 664(676.18) | Grad Norm 4.4568(1.9050) | Total Time 14.00(14.00)\n",
      "Iter 0702 | Time 62.6529(62.6212) | Bit/dim 3.9328(3.9441) | Xent 1.4937(1.5305) | Loss 4.6797(4.7094) | Error 0.5286(0.5464) Steps 682(676.36) | Grad Norm 1.6834(1.8984) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 24.8283, Epoch Time 410.7257(409.7475), Bit/dim 3.9304(best: 3.9318), Xent 1.4448, Loss 4.6528, Error 0.5197(best: 0.5225)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0703 | Time 62.9930(62.6324) | Bit/dim 3.9344(3.9439) | Xent 1.4731(1.5288) | Loss 4.6710(4.7083) | Error 0.5228(0.5457) Steps 694(676.89) | Grad Norm 2.3495(1.9119) | Total Time 14.00(14.00)\n",
      "Iter 0704 | Time 61.6267(62.6022) | Bit/dim 3.9263(3.9433) | Xent 1.5100(1.5282) | Loss 4.6813(4.7074) | Error 0.5345(0.5453) Steps 676(676.86) | Grad Norm 3.1836(1.9501) | Total Time 14.00(14.00)\n",
      "Iter 0705 | Time 61.8593(62.5799) | Bit/dim 3.9191(3.9426) | Xent 1.4900(1.5271) | Loss 4.6642(4.7061) | Error 0.5281(0.5448) Steps 664(676.47) | Grad Norm 1.8681(1.9476) | Total Time 14.00(14.00)\n",
      "Iter 0706 | Time 59.9876(62.5022) | Bit/dim 3.9351(3.9424) | Xent 1.4704(1.5254) | Loss 4.6703(4.7051) | Error 0.5279(0.5443) Steps 670(676.28) | Grad Norm 1.1838(1.9247) | Total Time 14.00(14.00)\n",
      "Iter 0707 | Time 60.0968(62.4300) | Bit/dim 3.9327(3.9421) | Xent 1.4635(1.5235) | Loss 4.6645(4.7039) | Error 0.5229(0.5437) Steps 664(675.91) | Grad Norm 1.3436(1.9073) | Total Time 14.00(14.00)\n",
      "Iter 0708 | Time 58.8240(62.3218) | Bit/dim 3.9220(3.9415) | Xent 1.4839(1.5223) | Loss 4.6640(4.7027) | Error 0.5328(0.5433) Steps 676(675.91) | Grad Norm 1.1085(1.8833) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 24.9653, Epoch Time 407.5261(409.6809), Bit/dim 3.9286(best: 3.9304), Xent 1.4330, Loss 4.6451, Error 0.5130(best: 0.5197)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0709 | Time 59.6561(62.2419) | Bit/dim 3.9368(3.9413) | Xent 1.4762(1.5209) | Loss 4.6750(4.7018) | Error 0.5240(0.5428) Steps 640(674.84) | Grad Norm 1.0234(1.8575) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 62.3817(62.2461) | Bit/dim 3.9172(3.9406) | Xent 1.4713(1.5195) | Loss 4.6528(4.7004) | Error 0.5250(0.5422) Steps 664(674.51) | Grad Norm 1.1482(1.8362) | Total Time 14.00(14.00)\n",
      "Iter 0711 | Time 60.6089(62.1969) | Bit/dim 3.9266(3.9402) | Xent 1.4695(1.5180) | Loss 4.6614(4.6992) | Error 0.5206(0.5416) Steps 664(674.20) | Grad Norm 1.1353(1.8152) | Total Time 14.00(14.00)\n",
      "Iter 0712 | Time 62.5060(62.2062) | Bit/dim 3.9208(3.9396) | Xent 1.4668(1.5164) | Loss 4.6542(4.6978) | Error 0.5284(0.5412) Steps 664(673.89) | Grad Norm 2.2969(1.8296) | Total Time 14.00(14.00)\n",
      "Iter 0713 | Time 62.3755(62.2113) | Bit/dim 3.9359(3.9395) | Xent 1.4745(1.5152) | Loss 4.6732(4.6971) | Error 0.5216(0.5406) Steps 664(673.59) | Grad Norm 3.5477(1.8812) | Total Time 14.00(14.00)\n",
      "Iter 0714 | Time 63.3707(62.2461) | Bit/dim 3.9327(3.9393) | Xent 1.4505(1.5132) | Loss 4.6579(4.6959) | Error 0.5216(0.5400) Steps 664(673.31) | Grad Norm 1.8267(1.8796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 24.7477, Epoch Time 414.3242(409.8202), Bit/dim 3.9267(best: 3.9286), Xent 1.4311, Loss 4.6423, Error 0.5120(best: 0.5130)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0715 | Time 60.8795(62.2051) | Bit/dim 3.9350(3.9392) | Xent 1.4597(1.5116) | Loss 4.6649(4.6950) | Error 0.5151(0.5393) Steps 670(673.21) | Grad Norm 2.2697(1.8913) | Total Time 14.00(14.00)\n",
      "Iter 0716 | Time 59.4479(62.1224) | Bit/dim 3.9205(3.9386) | Xent 1.4704(1.5104) | Loss 4.6557(4.6938) | Error 0.5220(0.5388) Steps 634(672.03) | Grad Norm 3.2214(1.9312) | Total Time 14.00(14.00)\n",
      "Iter 0717 | Time 60.3636(62.0696) | Bit/dim 3.9315(3.9384) | Xent 1.4580(1.5088) | Loss 4.6605(4.6928) | Error 0.5216(0.5382) Steps 652(671.43) | Grad Norm 1.5837(1.9207) | Total Time 14.00(14.00)\n",
      "Iter 0718 | Time 59.7499(62.0000) | Bit/dim 3.9191(3.9378) | Xent 1.4564(1.5072) | Loss 4.6473(4.6914) | Error 0.5199(0.5377) Steps 640(670.49) | Grad Norm 1.8657(1.9191) | Total Time 14.00(14.00)\n",
      "Iter 0719 | Time 60.9435(61.9683) | Bit/dim 3.9197(3.9373) | Xent 1.4686(1.5061) | Loss 4.6541(4.6903) | Error 0.5225(0.5372) Steps 634(669.39) | Grad Norm 3.8972(1.9784) | Total Time 14.00(14.00)\n",
      "Iter 0720 | Time 62.1927(61.9750) | Bit/dim 3.9215(3.9368) | Xent 1.4631(1.5048) | Loss 4.6530(4.6892) | Error 0.5268(0.5369) Steps 664(669.23) | Grad Norm 3.4991(2.0240) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 24.6530, Epoch Time 404.5708(409.6627), Bit/dim 3.9233(best: 3.9267), Xent 1.4242, Loss 4.6354, Error 0.5098(best: 0.5120)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0721 | Time 61.0650(61.9477) | Bit/dim 3.9279(3.9365) | Xent 1.4332(1.5027) | Loss 4.6445(4.6879) | Error 0.5056(0.5360) Steps 658(668.89) | Grad Norm 2.2724(2.0315) | Total Time 14.00(14.00)\n",
      "Iter 0722 | Time 57.9972(61.8292) | Bit/dim 3.9290(3.9363) | Xent 1.4629(1.5015) | Loss 4.6605(4.6870) | Error 0.5191(0.5355) Steps 652(668.39) | Grad Norm 1.4520(2.0141) | Total Time 14.00(14.00)\n",
      "Iter 0723 | Time 58.9083(61.7416) | Bit/dim 3.9222(3.9359) | Xent 1.4581(1.5002) | Loss 4.6513(4.6860) | Error 0.5160(0.5349) Steps 652(667.89) | Grad Norm 0.7569(1.9764) | Total Time 14.00(14.00)\n",
      "Iter 0724 | Time 57.9360(61.6274) | Bit/dim 3.9304(3.9357) | Xent 1.4590(1.4989) | Loss 4.6599(4.6852) | Error 0.5189(0.5344) Steps 640(667.06) | Grad Norm 1.1552(1.9518) | Total Time 14.00(14.00)\n",
      "Iter 0725 | Time 59.3402(61.5588) | Bit/dim 3.9195(3.9352) | Xent 1.4629(1.4978) | Loss 4.6510(4.6842) | Error 0.5214(0.5340) Steps 664(666.97) | Grad Norm 1.6777(1.9435) | Total Time 14.00(14.00)\n",
      "Iter 0726 | Time 60.4283(61.5249) | Bit/dim 3.9053(3.9343) | Xent 1.4540(1.4965) | Loss 4.6323(4.6826) | Error 0.5248(0.5337) Steps 634(665.98) | Grad Norm 2.0586(1.9470) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 23.7761, Epoch Time 396.4636(409.2667), Bit/dim 3.9212(best: 3.9233), Xent 1.4199, Loss 4.6311, Error 0.5102(best: 0.5098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0727 | Time 57.6933(61.4099) | Bit/dim 3.9271(3.9341) | Xent 1.4692(1.4957) | Loss 4.6617(4.6820) | Error 0.5154(0.5332) Steps 634(665.02) | Grad Norm 3.3632(1.9895) | Total Time 14.00(14.00)\n",
      "Iter 0728 | Time 62.7543(61.4503) | Bit/dim 3.9249(3.9338) | Xent 1.4463(1.4942) | Loss 4.6480(4.6810) | Error 0.5128(0.5326) Steps 664(664.99) | Grad Norm 4.3293(2.0597) | Total Time 14.00(14.00)\n",
      "Iter 0729 | Time 58.2916(61.3555) | Bit/dim 3.9123(3.9332) | Xent 1.4596(1.4932) | Loss 4.6421(4.6798) | Error 0.5254(0.5324) Steps 634(664.06) | Grad Norm 4.6099(2.1362) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 62.0214(61.3755) | Bit/dim 3.9168(3.9327) | Xent 1.4439(1.4917) | Loss 4.6388(4.6786) | Error 0.5206(0.5320) Steps 652(663.70) | Grad Norm 2.0162(2.1326) | Total Time 14.00(14.00)\n",
      "Iter 0731 | Time 61.9658(61.3932) | Bit/dim 3.9259(3.9325) | Xent 1.4571(1.4907) | Loss 4.6545(4.6778) | Error 0.5157(0.5315) Steps 676(664.06) | Grad Norm 2.1566(2.1333) | Total Time 14.00(14.00)\n",
      "Iter 0732 | Time 60.6958(61.3723) | Bit/dim 3.9101(3.9318) | Xent 1.4614(1.4898) | Loss 4.6408(4.6767) | Error 0.5221(0.5312) Steps 634(663.16) | Grad Norm 5.1621(2.2242) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 25.1214, Epoch Time 404.8840(409.1352), Bit/dim 3.9209(best: 3.9212), Xent 1.4239, Loss 4.6328, Error 0.5111(best: 0.5098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0733 | Time 61.3842(61.3726) | Bit/dim 3.9240(3.9316) | Xent 1.4669(1.4891) | Loss 4.6574(4.6762) | Error 0.5252(0.5311) Steps 658(663.01) | Grad Norm 4.7243(2.2992) | Total Time 14.00(14.00)\n",
      "Iter 0734 | Time 57.5823(61.2589) | Bit/dim 3.9182(3.9312) | Xent 1.4488(1.4879) | Loss 4.6426(4.6751) | Error 0.5106(0.5305) Steps 658(662.86) | Grad Norm 0.8261(2.2550) | Total Time 14.00(14.00)\n",
      "Iter 0735 | Time 59.0830(61.1937) | Bit/dim 3.9137(3.9307) | Xent 1.4587(1.4870) | Loss 4.6430(4.6742) | Error 0.5079(0.5298) Steps 634(661.99) | Grad Norm 4.2903(2.3160) | Total Time 14.00(14.00)\n",
      "Iter 0736 | Time 61.8371(61.2130) | Bit/dim 3.9140(3.9302) | Xent 1.4804(1.4868) | Loss 4.6543(4.6736) | Error 0.5315(0.5298) Steps 658(661.87) | Grad Norm 5.2425(2.4038) | Total Time 14.00(14.00)\n",
      "Iter 0737 | Time 60.0444(61.1779) | Bit/dim 3.9162(3.9298) | Xent 1.4332(1.4852) | Loss 4.6328(4.6724) | Error 0.5122(0.5293) Steps 676(662.30) | Grad Norm 2.9422(2.4200) | Total Time 14.00(14.00)\n",
      "Iter 0738 | Time 59.9058(61.1397) | Bit/dim 3.9144(3.9293) | Xent 1.4444(1.4840) | Loss 4.6366(4.6713) | Error 0.5156(0.5289) Steps 634(661.45) | Grad Norm 1.3120(2.3867) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 24.7168, Epoch Time 401.5650(408.9081), Bit/dim 3.9180(best: 3.9209), Xent 1.4186, Loss 4.6273, Error 0.5072(best: 0.5098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0739 | Time 61.6543(61.1552) | Bit/dim 3.9137(3.9288) | Xent 1.4554(1.4831) | Loss 4.6414(4.6704) | Error 0.5115(0.5284) Steps 658(661.34) | Grad Norm 5.0439(2.4665) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 59.9379(61.1187) | Bit/dim 3.9247(3.9287) | Xent 1.4720(1.4828) | Loss 4.6607(4.6701) | Error 0.5171(0.5280) Steps 634(660.52) | Grad Norm 5.1816(2.5479) | Total Time 14.00(14.00)\n",
      "Iter 0741 | Time 58.3995(61.0371) | Bit/dim 3.9143(3.9283) | Xent 1.4468(1.4817) | Loss 4.6377(4.6691) | Error 0.5264(0.5280) Steps 658(660.45) | Grad Norm 1.5539(2.5181) | Total Time 14.00(14.00)\n",
      "Iter 0742 | Time 62.4831(61.0805) | Bit/dim 3.9091(3.9277) | Xent 1.4293(1.4801) | Loss 4.6238(4.6678) | Error 0.5064(0.5273) Steps 658(660.37) | Grad Norm 3.3779(2.5439) | Total Time 14.00(14.00)\n",
      "Iter 0743 | Time 58.7211(61.0097) | Bit/dim 3.9112(3.9272) | Xent 1.4386(1.4789) | Loss 4.6304(4.6667) | Error 0.5111(0.5268) Steps 634(659.58) | Grad Norm 4.6535(2.6072) | Total Time 14.00(14.00)\n",
      "Iter 0744 | Time 61.0449(61.0107) | Bit/dim 3.9187(3.9269) | Xent 1.4392(1.4777) | Loss 4.6383(4.6658) | Error 0.5062(0.5262) Steps 664(659.72) | Grad Norm 2.5231(2.6046) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 24.8808, Epoch Time 404.5281(408.7767), Bit/dim 3.9153(best: 3.9180), Xent 1.3953, Loss 4.6129, Error 0.4968(best: 0.5072)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0745 | Time 59.4328(60.9634) | Bit/dim 3.9231(3.9268) | Xent 1.4341(1.4764) | Loss 4.6401(4.6650) | Error 0.5054(0.5256) Steps 652(659.48) | Grad Norm 1.5831(2.5740) | Total Time 14.00(14.00)\n",
      "Iter 0746 | Time 59.4131(60.9169) | Bit/dim 3.9057(3.9262) | Xent 1.4296(1.4750) | Loss 4.6205(4.6637) | Error 0.5065(0.5250) Steps 634(658.72) | Grad Norm 4.2987(2.6257) | Total Time 14.00(14.00)\n",
      "Iter 0747 | Time 59.6737(60.8796) | Bit/dim 3.9064(3.9256) | Xent 1.4441(1.4741) | Loss 4.6284(4.6626) | Error 0.5175(0.5248) Steps 670(659.06) | Grad Norm 4.7499(2.6895) | Total Time 14.00(14.00)\n",
      "Iter 0748 | Time 60.1316(60.8572) | Bit/dim 3.9246(3.9256) | Xent 1.4327(1.4728) | Loss 4.6409(4.6620) | Error 0.5077(0.5243) Steps 664(659.21) | Grad Norm 2.3039(2.6779) | Total Time 14.00(14.00)\n",
      "Iter 0749 | Time 60.1681(60.8365) | Bit/dim 3.9232(3.9255) | Xent 1.4509(1.4722) | Loss 4.6486(4.6616) | Error 0.5199(0.5242) Steps 634(658.45) | Grad Norm 3.5253(2.7033) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 63.1086(60.9046) | Bit/dim 3.9064(3.9249) | Xent 1.4398(1.4712) | Loss 4.6263(4.6605) | Error 0.5114(0.5238) Steps 670(658.80) | Grad Norm 4.3230(2.7519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 23.8305, Epoch Time 403.1161(408.6069), Bit/dim 3.9134(best: 3.9153), Xent 1.3895, Loss 4.6082, Error 0.4993(best: 0.4968)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0751 | Time 59.9713(60.8766) | Bit/dim 3.9068(3.9244) | Xent 1.4321(1.4700) | Loss 4.6229(4.6594) | Error 0.5074(0.5233) Steps 634(658.05) | Grad Norm 1.6760(2.7196) | Total Time 14.00(14.00)\n",
      "Iter 0752 | Time 62.8104(60.9347) | Bit/dim 3.9180(3.9242) | Xent 1.4354(1.4690) | Loss 4.6357(4.6587) | Error 0.5196(0.5232) Steps 640(657.51) | Grad Norm 1.8368(2.6931) | Total Time 14.00(14.00)\n",
      "Iter 0753 | Time 62.5170(60.9821) | Bit/dim 3.9058(3.9236) | Xent 1.4164(1.4674) | Loss 4.6140(4.6573) | Error 0.5015(0.5225) Steps 670(657.89) | Grad Norm 2.4325(2.6853) | Total Time 14.00(14.00)\n",
      "Iter 0754 | Time 57.7319(60.8846) | Bit/dim 3.9069(3.9231) | Xent 1.4269(1.4662) | Loss 4.6204(4.6562) | Error 0.5056(0.5220) Steps 634(657.17) | Grad Norm 1.1634(2.6397) | Total Time 14.00(14.00)\n",
      "Iter 0755 | Time 61.0229(60.8888) | Bit/dim 3.9204(3.9231) | Xent 1.4418(1.4655) | Loss 4.6413(4.6558) | Error 0.5105(0.5217) Steps 634(656.47) | Grad Norm 2.2447(2.6278) | Total Time 14.00(14.00)\n",
      "Iter 0756 | Time 61.7276(60.9139) | Bit/dim 3.9110(3.9227) | Xent 1.4196(1.4641) | Loss 4.6208(4.6547) | Error 0.5041(0.5211) Steps 652(656.34) | Grad Norm 2.3327(2.6190) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 24.3959, Epoch Time 407.6083(408.5769), Bit/dim 3.9118(best: 3.9134), Xent 1.3872, Loss 4.6054, Error 0.4935(best: 0.4968)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0757 | Time 62.0408(60.9477) | Bit/dim 3.9033(3.9221) | Xent 1.4294(1.4630) | Loss 4.6179(4.6536) | Error 0.5148(0.5210) Steps 640(655.85) | Grad Norm 3.4297(2.6433) | Total Time 14.00(14.00)\n",
      "Iter 0758 | Time 61.7915(60.9730) | Bit/dim 3.9100(3.9217) | Xent 1.4206(1.4618) | Loss 4.6203(4.6526) | Error 0.5091(0.5206) Steps 634(655.19) | Grad Norm 4.1365(2.6881) | Total Time 14.00(14.00)\n",
      "Iter 0759 | Time 58.6399(60.9031) | Bit/dim 3.9146(3.9215) | Xent 1.4386(1.4611) | Loss 4.6339(4.6521) | Error 0.5100(0.5203) Steps 652(655.10) | Grad Norm 2.7550(2.6901) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 58.1275(60.8198) | Bit/dim 3.9093(3.9212) | Xent 1.4110(1.4596) | Loss 4.6148(4.6510) | Error 0.5006(0.5197) Steps 664(655.37) | Grad Norm 4.1965(2.7353) | Total Time 14.00(14.00)\n",
      "Iter 0761 | Time 58.0878(60.7378) | Bit/dim 3.9133(3.9209) | Xent 1.4581(1.4595) | Loss 4.6423(4.6507) | Error 0.5179(0.5196) Steps 622(654.36) | Grad Norm 5.0064(2.8034) | Total Time 14.00(14.00)\n",
      "Iter 0762 | Time 61.4636(60.7596) | Bit/dim 3.9106(3.9206) | Xent 1.4103(1.4581) | Loss 4.6158(4.6497) | Error 0.4978(0.5190) Steps 646(654.11) | Grad Norm 1.1908(2.7550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 25.1505, Epoch Time 402.9502(408.4081), Bit/dim 3.9114(best: 3.9118), Xent 1.4018, Loss 4.6123, Error 0.5019(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0763 | Time 61.6065(60.7850) | Bit/dim 3.9144(3.9204) | Xent 1.4257(1.4571) | Loss 4.6272(4.6490) | Error 0.5075(0.5186) Steps 664(654.41) | Grad Norm 5.6583(2.8421) | Total Time 14.00(14.00)\n",
      "Iter 0764 | Time 60.0110(60.7618) | Bit/dim 3.9069(3.9200) | Xent 1.4673(1.4574) | Loss 4.6406(4.6487) | Error 0.5353(0.5191) Steps 622(653.44) | Grad Norm 5.2734(2.9151) | Total Time 14.00(14.00)\n",
      "Iter 0765 | Time 61.2111(60.7753) | Bit/dim 3.9199(3.9200) | Xent 1.4313(1.4566) | Loss 4.6356(4.6483) | Error 0.5060(0.5187) Steps 634(652.85) | Grad Norm 4.5126(2.9630) | Total Time 14.00(14.00)\n",
      "Iter 0766 | Time 64.3274(60.8818) | Bit/dim 3.9133(3.9198) | Xent 1.4587(1.4567) | Loss 4.6427(4.6482) | Error 0.5154(0.5186) Steps 664(653.19) | Grad Norm 8.5708(3.1312) | Total Time 14.00(14.00)\n",
      "Iter 0767 | Time 60.4975(60.8703) | Bit/dim 3.8998(3.9192) | Xent 1.4341(1.4560) | Loss 4.6169(4.6472) | Error 0.5114(0.5184) Steps 634(652.61) | Grad Norm 3.3651(3.1383) | Total Time 14.00(14.00)\n",
      "Iter 0768 | Time 59.2417(60.8214) | Bit/dim 3.8945(3.9185) | Xent 1.4391(1.4555) | Loss 4.6141(4.6462) | Error 0.5202(0.5185) Steps 640(652.23) | Grad Norm 4.0634(3.1660) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 24.4350, Epoch Time 409.1845(408.4314), Bit/dim 3.9063(best: 3.9114), Xent 1.3821, Loss 4.5974, Error 0.4941(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0769 | Time 58.4795(60.7512) | Bit/dim 3.9132(3.9183) | Xent 1.4191(1.4544) | Loss 4.6228(4.6455) | Error 0.5076(0.5181) Steps 658(652.41) | Grad Norm 1.6899(3.1217) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 57.6146(60.6571) | Bit/dim 3.9124(3.9182) | Xent 1.4308(1.4537) | Loss 4.6278(4.6450) | Error 0.5134(0.5180) Steps 670(652.94) | Grad Norm 5.3070(3.1873) | Total Time 14.00(14.00)\n",
      "Iter 0771 | Time 61.5787(60.6847) | Bit/dim 3.9051(3.9178) | Xent 1.4266(1.4529) | Loss 4.6184(4.6442) | Error 0.5108(0.5178) Steps 646(652.73) | Grad Norm 4.6237(3.2304) | Total Time 14.00(14.00)\n",
      "Iter 0772 | Time 61.7874(60.7178) | Bit/dim 3.9039(3.9173) | Xent 1.4480(1.4527) | Loss 4.6279(4.6437) | Error 0.5112(0.5176) Steps 634(652.17) | Grad Norm 6.9230(3.3412) | Total Time 14.00(14.00)\n",
      "Iter 0773 | Time 60.8200(60.7209) | Bit/dim 3.9051(3.9170) | Xent 1.4479(1.4526) | Loss 4.6290(4.6433) | Error 0.5196(0.5177) Steps 676(652.88) | Grad Norm 6.4532(3.4345) | Total Time 14.00(14.00)\n",
      "Iter 0774 | Time 59.2181(60.6758) | Bit/dim 3.8972(3.9164) | Xent 1.4301(1.4519) | Loss 4.6123(4.6423) | Error 0.5059(0.5173) Steps 664(653.21) | Grad Norm 4.5023(3.4666) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 23.6462, Epoch Time 400.5436(408.1948), Bit/dim 3.9055(best: 3.9063), Xent 1.3872, Loss 4.5991, Error 0.4982(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0775 | Time 59.2952(60.6344) | Bit/dim 3.9029(3.9160) | Xent 1.4350(1.4514) | Loss 4.6204(4.6417) | Error 0.5088(0.5170) Steps 670(653.72) | Grad Norm 3.9755(3.4818) | Total Time 14.00(14.00)\n",
      "Iter 0776 | Time 60.9936(60.6452) | Bit/dim 3.9050(3.9156) | Xent 1.4375(1.4510) | Loss 4.6237(4.6411) | Error 0.5096(0.5168) Steps 670(654.21) | Grad Norm 6.3732(3.5686) | Total Time 14.00(14.00)\n",
      "Iter 0777 | Time 60.2784(60.6342) | Bit/dim 3.9135(3.9156) | Xent 1.4691(1.4515) | Loss 4.6481(4.6413) | Error 0.5205(0.5169) Steps 658(654.32) | Grad Norm 7.7634(3.6944) | Total Time 14.00(14.00)\n",
      "Iter 0778 | Time 60.2141(60.6216) | Bit/dim 3.8918(3.9149) | Xent 1.4092(1.4503) | Loss 4.5964(4.6400) | Error 0.5020(0.5165) Steps 658(654.43) | Grad Norm 2.5345(3.6596) | Total Time 14.00(14.00)\n",
      "Iter 0779 | Time 61.2416(60.6402) | Bit/dim 3.9032(3.9145) | Xent 1.4635(1.4507) | Loss 4.6350(4.6398) | Error 0.5280(0.5168) Steps 670(654.90) | Grad Norm 8.8153(3.8143) | Total Time 14.00(14.00)\n",
      "Iter 0780 | Time 56.6208(60.5196) | Bit/dim 3.9042(3.9142) | Xent 1.4786(1.4515) | Loss 4.6435(4.6400) | Error 0.5294(0.5172) Steps 634(654.27) | Grad Norm 12.4843(4.0744) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 25.4225, Epoch Time 400.9095(407.9762), Bit/dim 3.9085(best: 3.9055), Xent 1.4238, Loss 4.6203, Error 0.5042(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0781 | Time 63.3629(60.6049) | Bit/dim 3.9011(3.9138) | Xent 1.4558(1.4516) | Loss 4.6290(4.6396) | Error 0.5144(0.5171) Steps 670(654.74) | Grad Norm 8.5561(4.2088) | Total Time 14.00(14.00)\n",
      "Iter 0782 | Time 59.6017(60.5748) | Bit/dim 3.9103(3.9137) | Xent 1.4390(1.4512) | Loss 4.6299(4.6393) | Error 0.5185(0.5172) Steps 670(655.20) | Grad Norm 7.3905(4.3043) | Total Time 14.00(14.00)\n",
      "Iter 0783 | Time 59.6548(60.5472) | Bit/dim 3.9027(3.9134) | Xent 1.5091(1.4530) | Loss 4.6573(4.6399) | Error 0.5379(0.5178) Steps 634(654.56) | Grad Norm 11.3860(4.5167) | Total Time 14.00(14.00)\n",
      "Iter 0784 | Time 59.4039(60.5129) | Bit/dim 3.9041(3.9131) | Xent 1.4314(1.4523) | Loss 4.6198(4.6393) | Error 0.5050(0.5174) Steps 676(655.21) | Grad Norm 2.9387(4.4694) | Total Time 14.00(14.00)\n",
      "Iter 0785 | Time 61.9835(60.5570) | Bit/dim 3.9132(3.9131) | Xent 1.4641(1.4527) | Loss 4.6452(4.6394) | Error 0.5172(0.5174) Steps 688(656.19) | Grad Norm 7.9958(4.5752) | Total Time 14.00(14.00)\n",
      "Iter 0786 | Time 58.8781(60.5066) | Bit/dim 3.8972(3.9126) | Xent 1.4344(1.4521) | Loss 4.6145(4.6387) | Error 0.5151(0.5173) Steps 634(655.53) | Grad Norm 4.4956(4.5728) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 23.8757, Epoch Time 403.3625(407.8378), Bit/dim 3.9009(best: 3.9055), Xent 1.3942, Loss 4.5980, Error 0.4978(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0787 | Time 59.1140(60.4648) | Bit/dim 3.8994(3.9122) | Xent 1.4217(1.4512) | Loss 4.6103(4.6378) | Error 0.5002(0.5168) Steps 640(655.06) | Grad Norm 5.1143(4.5890) | Total Time 14.00(14.00)\n",
      "Iter 0788 | Time 59.6763(60.4412) | Bit/dim 3.9024(3.9119) | Xent 1.4575(1.4514) | Loss 4.6311(4.6376) | Error 0.5194(0.5169) Steps 676(655.69) | Grad Norm 7.9023(4.6884) | Total Time 14.00(14.00)\n",
      "Iter 0789 | Time 60.8292(60.4528) | Bit/dim 3.8969(3.9115) | Xent 1.4117(1.4502) | Loss 4.6028(4.6366) | Error 0.5019(0.5164) Steps 664(655.94) | Grad Norm 1.4956(4.5927) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 60.4937(60.4541) | Bit/dim 3.8998(3.9111) | Xent 1.4387(1.4499) | Loss 4.6192(4.6361) | Error 0.5088(0.5162) Steps 658(656.00) | Grad Norm 5.9127(4.6323) | Total Time 14.00(14.00)\n",
      "Iter 0791 | Time 61.3810(60.4819) | Bit/dim 3.8962(3.9107) | Xent 1.4115(1.4487) | Loss 4.6020(4.6351) | Error 0.4992(0.5157) Steps 664(656.24) | Grad Norm 3.3492(4.5938) | Total Time 14.00(14.00)\n",
      "Iter 0792 | Time 60.6552(60.4871) | Bit/dim 3.9005(3.9104) | Xent 1.4222(1.4479) | Loss 4.6116(4.6343) | Error 0.5070(0.5154) Steps 652(656.11) | Grad Norm 3.7679(4.5690) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 25.3108, Epoch Time 404.5570(407.7394), Bit/dim 3.9011(best: 3.9009), Xent 1.3835, Loss 4.5928, Error 0.5000(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0793 | Time 59.2788(60.4508) | Bit/dim 3.9053(3.9102) | Xent 1.4139(1.4469) | Loss 4.6123(4.6337) | Error 0.5061(0.5152) Steps 652(655.99) | Grad Norm 3.9233(4.5496) | Total Time 14.00(14.00)\n",
      "Iter 0794 | Time 60.2283(60.4441) | Bit/dim 3.9094(3.9102) | Xent 1.4175(1.4460) | Loss 4.6182(4.6332) | Error 0.5048(0.5148) Steps 664(656.23) | Grad Norm 3.2620(4.5110) | Total Time 14.00(14.00)\n",
      "Iter 0795 | Time 61.0358(60.4619) | Bit/dim 3.8923(3.9097) | Xent 1.4050(1.4448) | Loss 4.5948(4.6321) | Error 0.5022(0.5145) Steps 658(656.28) | Grad Norm 4.7752(4.5189) | Total Time 14.00(14.00)\n",
      "Iter 0796 | Time 58.6113(60.4064) | Bit/dim 3.8949(3.9092) | Xent 1.3978(1.4434) | Loss 4.5938(4.6309) | Error 0.4976(0.5140) Steps 628(655.43) | Grad Norm 3.5964(4.4912) | Total Time 14.00(14.00)\n",
      "Iter 0797 | Time 62.8906(60.4809) | Bit/dim 3.9028(3.9090) | Xent 1.3847(1.4416) | Loss 4.5951(4.6298) | Error 0.4860(0.5131) Steps 622(654.43) | Grad Norm 2.7275(4.4383) | Total Time 14.00(14.00)\n",
      "Iter 0798 | Time 61.6546(60.5161) | Bit/dim 3.9003(3.9088) | Xent 1.4164(1.4409) | Loss 4.6085(4.6292) | Error 0.5065(0.5129) Steps 676(655.08) | Grad Norm 5.1640(4.4601) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 24.2927, Epoch Time 404.9680(407.6563), Bit/dim 3.8985(best: 3.9009), Xent 1.3608, Loss 4.5789, Error 0.4899(best: 0.4935)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0799 | Time 60.5431(60.5169) | Bit/dim 3.8992(3.9085) | Xent 1.4022(1.4397) | Loss 4.6003(4.6283) | Error 0.5037(0.5127) Steps 634(654.45) | Grad Norm 2.6166(4.4048) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 58.3042(60.4505) | Bit/dim 3.9004(3.9082) | Xent 1.4056(1.4387) | Loss 4.6032(4.6276) | Error 0.4978(0.5122) Steps 622(653.47) | Grad Norm 4.8068(4.4168) | Total Time 14.00(14.00)\n",
      "Iter 0801 | Time 61.0200(60.4676) | Bit/dim 3.8961(3.9079) | Xent 1.3938(1.4373) | Loss 4.5930(4.6265) | Error 0.4999(0.5118) Steps 676(654.15) | Grad Norm 4.4455(4.4177) | Total Time 14.00(14.00)\n",
      "Iter 0802 | Time 61.9143(60.5110) | Bit/dim 3.8887(3.9073) | Xent 1.3900(1.4359) | Loss 4.5837(4.6253) | Error 0.4896(0.5112) Steps 670(654.62) | Grad Norm 1.4099(4.3275) | Total Time 14.00(14.00)\n",
      "Iter 0803 | Time 60.4361(60.5088) | Bit/dim 3.8964(3.9070) | Xent 1.4043(1.4350) | Loss 4.5985(4.6245) | Error 0.4970(0.5107) Steps 640(654.18) | Grad Norm 4.3193(4.3272) | Total Time 14.00(14.00)\n",
      "Iter 0804 | Time 61.1068(60.5267) | Bit/dim 3.9031(3.9069) | Xent 1.4013(1.4340) | Loss 4.6038(4.6238) | Error 0.4989(0.5104) Steps 688(655.20) | Grad Norm 2.8368(4.2825) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 24.4312, Epoch Time 405.6511(407.5961), Bit/dim 3.8988(best: 3.8985), Xent 1.3482, Loss 4.5729, Error 0.4805(best: 0.4899)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0805 | Time 60.1191(60.5145) | Bit/dim 3.9048(3.9068) | Xent 1.3601(1.4317) | Loss 4.5849(4.6227) | Error 0.4849(0.5096) Steps 628(654.38) | Grad Norm 1.3440(4.1944) | Total Time 14.00(14.00)\n",
      "Iter 0806 | Time 59.4553(60.4827) | Bit/dim 3.8934(3.9064) | Xent 1.3874(1.4304) | Loss 4.5871(4.6216) | Error 0.4912(0.5091) Steps 628(653.59) | Grad Norm 2.7965(4.1524) | Total Time 14.00(14.00)\n",
      "Iter 0807 | Time 59.9354(60.4663) | Bit/dim 3.9078(3.9064) | Xent 1.3907(1.4292) | Loss 4.6031(4.6210) | Error 0.4940(0.5086) Steps 646(653.36) | Grad Norm 1.6846(4.0784) | Total Time 14.00(14.00)\n",
      "Iter 0808 | Time 61.4699(60.4964) | Bit/dim 3.8964(3.9061) | Xent 1.3941(1.4282) | Loss 4.5935(4.6202) | Error 0.4915(0.5081) Steps 640(652.96) | Grad Norm 1.2034(3.9921) | Total Time 14.00(14.00)\n",
      "Iter 0809 | Time 59.9940(60.4813) | Bit/dim 3.8825(3.9054) | Xent 1.3829(1.4268) | Loss 4.5740(4.6188) | Error 0.4910(0.5076) Steps 634(652.39) | Grad Norm 1.6520(3.9219) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 59.1226(60.4406) | Bit/dim 3.8905(3.9050) | Xent 1.3781(1.4253) | Loss 4.5796(4.6177) | Error 0.4820(0.5068) Steps 640(652.02) | Grad Norm 1.1652(3.8392) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 24.1990, Epoch Time 402.3209(407.4379), Bit/dim 3.8960(best: 3.8985), Xent 1.3463, Loss 4.5691, Error 0.4794(best: 0.4805)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0811 | Time 59.6517(60.4169) | Bit/dim 3.8928(3.9046) | Xent 1.3813(1.4240) | Loss 4.5834(4.6166) | Error 0.4925(0.5064) Steps 628(651.30) | Grad Norm 3.3185(3.8236) | Total Time 14.00(14.00)\n",
      "Iter 0812 | Time 61.6407(60.4536) | Bit/dim 3.9016(3.9045) | Xent 1.3874(1.4229) | Loss 4.5953(4.6160) | Error 0.4936(0.5060) Steps 640(650.96) | Grad Norm 3.2507(3.8064) | Total Time 14.00(14.00)\n",
      "Iter 0813 | Time 61.0070(60.4702) | Bit/dim 3.8933(3.9042) | Xent 1.3740(1.4215) | Loss 4.5803(4.6149) | Error 0.4870(0.5054) Steps 646(650.81) | Grad Norm 2.1587(3.7570) | Total Time 14.00(14.00)\n",
      "Iter 0814 | Time 59.8885(60.4528) | Bit/dim 3.8948(3.9039) | Xent 1.3967(1.4207) | Loss 4.5931(4.6143) | Error 0.4951(0.5051) Steps 658(651.03) | Grad Norm 5.0841(3.7968) | Total Time 14.00(14.00)\n",
      "Iter 0815 | Time 55.9133(60.3166) | Bit/dim 3.8955(3.9037) | Xent 1.3883(1.4197) | Loss 4.5896(4.6135) | Error 0.4908(0.5047) Steps 628(650.34) | Grad Norm 4.1921(3.8087) | Total Time 14.00(14.00)\n",
      "Iter 0816 | Time 58.9234(60.2748) | Bit/dim 3.8882(3.9032) | Xent 1.3827(1.4186) | Loss 4.5795(4.6125) | Error 0.4859(0.5041) Steps 622(649.49) | Grad Norm 1.4831(3.7389) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 24.9237, Epoch Time 399.5360(407.2008), Bit/dim 3.8954(best: 3.8960), Xent 1.3604, Loss 4.5756, Error 0.4869(best: 0.4794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0817 | Time 62.1597(60.3313) | Bit/dim 3.8922(3.9029) | Xent 1.3846(1.4176) | Loss 4.5845(4.6117) | Error 0.4902(0.5037) Steps 658(649.74) | Grad Norm 5.3178(3.7863) | Total Time 14.00(14.00)\n",
      "Iter 0818 | Time 60.2267(60.3282) | Bit/dim 3.8912(3.9025) | Xent 1.3860(1.4167) | Loss 4.5843(4.6108) | Error 0.4912(0.5033) Steps 628(649.09) | Grad Norm 3.8209(3.7873) | Total Time 14.00(14.00)\n",
      "Iter 0819 | Time 59.8635(60.3143) | Bit/dim 3.8842(3.9020) | Xent 1.3744(1.4154) | Loss 4.5714(4.6097) | Error 0.4842(0.5028) Steps 628(648.46) | Grad Norm 2.7338(3.7557) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 58.6628(60.2647) | Bit/dim 3.9017(3.9020) | Xent 1.3932(1.4147) | Loss 4.5983(4.6093) | Error 0.4962(0.5026) Steps 652(648.56) | Grad Norm 7.1139(3.8564) | Total Time 14.00(14.00)\n",
      "Iter 0821 | Time 60.1814(60.2622) | Bit/dim 3.8987(3.9019) | Xent 1.3975(1.4142) | Loss 4.5975(4.6090) | Error 0.4908(0.5022) Steps 622(647.77) | Grad Norm 3.9580(3.8595) | Total Time 14.00(14.00)\n",
      "Iter 0822 | Time 57.7281(60.1862) | Bit/dim 3.8894(3.9015) | Xent 1.3933(1.4136) | Loss 4.5860(4.6083) | Error 0.4955(0.5020) Steps 622(646.99) | Grad Norm 4.0617(3.8656) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 25.3203, Epoch Time 402.4377(407.0579), Bit/dim 3.8931(best: 3.8954), Xent 1.3603, Loss 4.5733, Error 0.4841(best: 0.4794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0823 | Time 61.3098(60.2199) | Bit/dim 3.8892(3.9011) | Xent 1.3906(1.4129) | Loss 4.5845(4.6076) | Error 0.4866(0.5016) Steps 664(647.51) | Grad Norm 5.9084(3.9268) | Total Time 14.00(14.00)\n",
      "Iter 0824 | Time 59.3295(60.1932) | Bit/dim 3.8890(3.9008) | Xent 1.3812(1.4119) | Loss 4.5796(4.6067) | Error 0.4871(0.5011) Steps 634(647.10) | Grad Norm 2.8540(3.8947) | Total Time 14.00(14.00)\n",
      "Iter 0825 | Time 61.6919(60.2381) | Bit/dim 3.8875(3.9004) | Xent 1.3816(1.4110) | Loss 4.5783(4.6059) | Error 0.4982(0.5010) Steps 634(646.71) | Grad Norm 4.7624(3.9207) | Total Time 14.00(14.00)\n",
      "Iter 0826 | Time 60.6904(60.2517) | Bit/dim 3.8933(3.9001) | Xent 1.3725(1.4099) | Loss 4.5795(4.6051) | Error 0.4849(0.5006) Steps 634(646.33) | Grad Norm 4.4885(3.9377) | Total Time 14.00(14.00)\n",
      "Iter 0827 | Time 58.8812(60.2106) | Bit/dim 3.8885(3.8998) | Xent 1.3950(1.4094) | Loss 4.5860(4.6045) | Error 0.4950(0.5004) Steps 682(647.40) | Grad Norm 6.6583(4.0193) | Total Time 14.00(14.00)\n",
      "Iter 0828 | Time 61.6168(60.2528) | Bit/dim 3.8866(3.8994) | Xent 1.3932(1.4089) | Loss 4.5832(4.6039) | Error 0.4914(0.5001) Steps 634(646.99) | Grad Norm 6.4823(4.0932) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 24.1975, Epoch Time 405.2296(407.0031), Bit/dim 3.8896(best: 3.8931), Xent 1.3487, Loss 4.5639, Error 0.4849(best: 0.4794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0829 | Time 61.6183(60.2937) | Bit/dim 3.8885(3.8991) | Xent 1.3789(1.4080) | Loss 4.5779(4.6031) | Error 0.4869(0.4997) Steps 640(646.78) | Grad Norm 6.1841(4.1560) | Total Time 14.00(14.00)\n",
      "Iter 0830 | Time 63.0497(60.3764) | Bit/dim 3.8945(3.8989) | Xent 1.3854(1.4074) | Loss 4.5872(4.6026) | Error 0.4891(0.4994) Steps 634(646.40) | Grad Norm 7.4633(4.2552) | Total Time 14.00(14.00)\n",
      "Iter 0831 | Time 59.6274(60.3540) | Bit/dim 3.8933(3.8988) | Xent 1.3896(1.4068) | Loss 4.5881(4.6022) | Error 0.4919(0.4992) Steps 634(646.03) | Grad Norm 6.7955(4.3314) | Total Time 14.00(14.00)\n",
      "Iter 0832 | Time 62.7677(60.4264) | Bit/dim 3.8730(3.8980) | Xent 1.3816(1.4061) | Loss 4.5639(4.6010) | Error 0.4914(0.4989) Steps 622(645.31) | Grad Norm 4.2431(4.3287) | Total Time 14.00(14.00)\n",
      "Iter 0833 | Time 60.6941(60.4344) | Bit/dim 3.8878(3.8977) | Xent 1.3762(1.4052) | Loss 4.5759(4.6003) | Error 0.4838(0.4985) Steps 664(645.87) | Grad Norm 4.4774(4.3332) | Total Time 14.00(14.00)\n",
      "Iter 0834 | Time 59.7018(60.4124) | Bit/dim 3.8907(3.8975) | Xent 1.3772(1.4043) | Loss 4.5793(4.5996) | Error 0.4859(0.4981) Steps 634(645.51) | Grad Norm 3.4603(4.3070) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 23.1042, Epoch Time 407.9891(407.0326), Bit/dim 3.8882(best: 3.8896), Xent 1.3432, Loss 4.5597, Error 0.4817(best: 0.4794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0835 | Time 62.6237(60.4788) | Bit/dim 3.8900(3.8973) | Xent 1.3844(1.4037) | Loss 4.5822(4.5991) | Error 0.4892(0.4978) Steps 634(645.17) | Grad Norm 6.7117(4.3792) | Total Time 14.00(14.00)\n",
      "Iter 0836 | Time 60.4658(60.4784) | Bit/dim 3.8916(3.8971) | Xent 1.4103(1.4039) | Loss 4.5967(4.5991) | Error 0.4939(0.4977) Steps 676(646.09) | Grad Norm 8.7269(4.5096) | Total Time 14.00(14.00)\n",
      "Iter 0837 | Time 60.3879(60.4757) | Bit/dim 3.8871(3.8968) | Xent 1.3643(1.4028) | Loss 4.5692(4.5982) | Error 0.4748(0.4970) Steps 634(645.73) | Grad Norm 5.5390(4.5405) | Total Time 14.00(14.00)\n",
      "Iter 0838 | Time 59.2257(60.4382) | Bit/dim 3.8830(3.8964) | Xent 1.3655(1.4016) | Loss 4.5658(4.5972) | Error 0.4870(0.4967) Steps 634(645.38) | Grad Norm 3.0875(4.4969) | Total Time 14.00(14.00)\n",
      "Iter 0839 | Time 62.0065(60.4852) | Bit/dim 3.8880(3.8961) | Xent 1.3641(1.4005) | Loss 4.5701(4.5964) | Error 0.4859(0.4964) Steps 670(646.12) | Grad Norm 5.6450(4.5313) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 59.9108(60.4680) | Bit/dim 3.8789(3.8956) | Xent 1.3823(1.4000) | Loss 4.5700(4.5956) | Error 0.4920(0.4963) Steps 634(645.75) | Grad Norm 4.7125(4.5368) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 23.6006, Epoch Time 404.7984(406.9656), Bit/dim 3.8862(best: 3.8882), Xent 1.3318, Loss 4.5521, Error 0.4756(best: 0.4794)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0841 | Time 58.8343(60.4190) | Bit/dim 3.8810(3.8952) | Xent 1.3773(1.3993) | Loss 4.5697(4.5948) | Error 0.4851(0.4959) Steps 634(645.40) | Grad Norm 4.1790(4.5260) | Total Time 14.00(14.00)\n",
      "Iter 0842 | Time 62.4913(60.4811) | Bit/dim 3.8835(3.8948) | Xent 1.3810(1.3987) | Loss 4.5740(4.5942) | Error 0.4824(0.4955) Steps 682(646.50) | Grad Norm 6.0520(4.5718) | Total Time 14.00(14.00)\n",
      "Iter 0843 | Time 59.6747(60.4569) | Bit/dim 3.8885(3.8946) | Xent 1.3873(1.3984) | Loss 4.5822(4.5938) | Error 0.4854(0.4952) Steps 622(645.76) | Grad Norm 2.9596(4.5234) | Total Time 14.00(14.00)\n",
      "Iter 0844 | Time 59.6688(60.4333) | Bit/dim 3.8805(3.8942) | Xent 1.3646(1.3974) | Loss 4.5628(4.5929) | Error 0.4798(0.4948) Steps 622(645.05) | Grad Norm 7.3973(4.6097) | Total Time 14.00(14.00)\n",
      "Iter 0845 | Time 61.2018(60.4564) | Bit/dim 3.8885(3.8940) | Xent 1.4008(1.3975) | Loss 4.5889(4.5928) | Error 0.4900(0.4946) Steps 676(645.98) | Grad Norm 10.2054(4.7775) | Total Time 14.00(14.00)\n",
      "Iter 0846 | Time 59.5800(60.4301) | Bit/dim 3.8943(3.8940) | Xent 1.3762(1.3968) | Loss 4.5824(4.5925) | Error 0.4978(0.4947) Steps 634(645.62) | Grad Norm 8.1781(4.8795) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 23.5102, Epoch Time 402.0556(406.8183), Bit/dim 3.8868(best: 3.8862), Xent 1.3549, Loss 4.5642, Error 0.4897(best: 0.4756)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0847 | Time 63.1133(60.5106) | Bit/dim 3.8850(3.8938) | Xent 1.3730(1.3961) | Loss 4.5715(4.5918) | Error 0.4850(0.4944) Steps 622(644.91) | Grad Norm 6.4080(4.9254) | Total Time 14.00(14.00)\n",
      "Iter 0848 | Time 60.3639(60.5062) | Bit/dim 3.8880(3.8936) | Xent 1.3756(1.3955) | Loss 4.5758(4.5914) | Error 0.4878(0.4942) Steps 670(645.66) | Grad Norm 2.3535(4.8482) | Total Time 14.00(14.00)\n",
      "Iter 0849 | Time 60.4979(60.5059) | Bit/dim 3.8828(3.8933) | Xent 1.4068(1.3958) | Loss 4.5862(4.5912) | Error 0.5005(0.4944) Steps 664(646.21) | Grad Norm 9.9507(5.0013) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 57.5316(60.4167) | Bit/dim 3.8828(3.8930) | Xent 1.4296(1.3969) | Loss 4.5976(4.5914) | Error 0.5219(0.4952) Steps 622(645.49) | Grad Norm 7.1438(5.0656) | Total Time 14.00(14.00)\n",
      "Iter 0851 | Time 59.4063(60.3864) | Bit/dim 3.8884(3.8928) | Xent 1.4248(1.3977) | Loss 4.6009(4.5917) | Error 0.5049(0.4955) Steps 622(644.78) | Grad Norm 6.5218(5.1093) | Total Time 14.00(14.00)\n",
      "Iter 0852 | Time 59.9433(60.3731) | Bit/dim 3.8805(3.8925) | Xent 1.3950(1.3976) | Loss 4.5780(4.5913) | Error 0.4904(0.4954) Steps 676(645.72) | Grad Norm 6.4624(5.1499) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 24.6432, Epoch Time 402.7425(406.6960), Bit/dim 3.8865(best: 3.8862), Xent 1.3751, Loss 4.5741, Error 0.4931(best: 0.4756)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0853 | Time 60.7763(60.3852) | Bit/dim 3.8836(3.8922) | Xent 1.4189(1.3983) | Loss 4.5930(4.5913) | Error 0.5071(0.4957) Steps 670(646.45) | Grad Norm 7.1177(5.2089) | Total Time 14.00(14.00)\n",
      "Iter 0854 | Time 63.1674(60.4686) | Bit/dim 3.8803(3.8918) | Xent 1.3902(1.3980) | Loss 4.5754(4.5908) | Error 0.4922(0.4956) Steps 640(646.25) | Grad Norm 4.8475(5.1981) | Total Time 14.00(14.00)\n",
      "Iter 0855 | Time 57.7590(60.3874) | Bit/dim 3.8862(3.8917) | Xent 1.4427(1.3994) | Loss 4.6076(4.5913) | Error 0.5161(0.4962) Steps 640(646.07) | Grad Norm 10.5354(5.3582) | Total Time 14.00(14.00)\n",
      "Iter 0856 | Time 57.6589(60.3055) | Bit/dim 3.8875(3.8915) | Xent 1.4286(1.4002) | Loss 4.6018(4.5917) | Error 0.5072(0.4966) Steps 622(645.34) | Grad Norm 10.9250(5.5252) | Total Time 14.00(14.00)\n",
      "Iter 0857 | Time 65.9683(60.4754) | Bit/dim 3.8824(3.8913) | Xent 1.4257(1.4010) | Loss 4.5952(4.5918) | Error 0.4998(0.4967) Steps 682(646.44) | Grad Norm 8.1623(5.6043) | Total Time 14.00(14.00)\n",
      "Iter 0858 | Time 63.1065(60.5543) | Bit/dim 3.8955(3.8914) | Xent 1.3946(1.4008) | Loss 4.5928(4.5918) | Error 0.4962(0.4966) Steps 664(646.97) | Grad Norm 5.0791(5.5885) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 23.3720, Epoch Time 409.2501(406.7727), Bit/dim 3.8824(best: 3.8862), Xent 1.3689, Loss 4.5668, Error 0.4865(best: 0.4756)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0859 | Time 60.4463(60.5511) | Bit/dim 3.8739(3.8909) | Xent 1.4326(1.4018) | Loss 4.5902(4.5917) | Error 0.5051(0.4969) Steps 628(646.40) | Grad Norm 9.5332(5.7069) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 60.6474(60.5540) | Bit/dim 3.8833(3.8906) | Xent 1.4279(1.4025) | Loss 4.5973(4.5919) | Error 0.5052(0.4972) Steps 646(646.39) | Grad Norm 8.7892(5.7993) | Total Time 14.00(14.00)\n",
      "Iter 0861 | Time 59.4542(60.5210) | Bit/dim 3.8792(3.8903) | Xent 1.3782(1.4018) | Loss 4.5683(4.5912) | Error 0.4874(0.4969) Steps 640(646.20) | Grad Norm 3.3008(5.7244) | Total Time 14.00(14.00)\n",
      "Iter 0862 | Time 62.7681(60.5884) | Bit/dim 3.8839(3.8901) | Xent 1.3911(1.4015) | Loss 4.5795(4.5908) | Error 0.4910(0.4967) Steps 670(646.91) | Grad Norm 5.2225(5.7093) | Total Time 14.00(14.00)\n",
      "Iter 0863 | Time 57.9244(60.5085) | Bit/dim 3.8827(3.8899) | Xent 1.3673(1.4005) | Loss 4.5663(4.5901) | Error 0.4866(0.4964) Steps 658(647.24) | Grad Norm 4.4016(5.6701) | Total Time 14.00(14.00)\n",
      "Iter 0864 | Time 58.7500(60.4557) | Bit/dim 3.8797(3.8896) | Xent 1.3685(1.3995) | Loss 4.5640(4.5893) | Error 0.4858(0.4961) Steps 646(647.21) | Grad Norm 3.4074(5.6022) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 23.7495, Epoch Time 400.6536(406.5891), Bit/dim 3.8804(best: 3.8824), Xent 1.3469, Loss 4.5539, Error 0.4799(best: 0.4756)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0865 | Time 58.8140(60.4065) | Bit/dim 3.8677(3.8889) | Xent 1.3787(1.3989) | Loss 4.5570(4.5884) | Error 0.4865(0.4958) Steps 622(646.45) | Grad Norm 6.1625(5.6190) | Total Time 14.00(14.00)\n",
      "Iter 0866 | Time 60.7183(60.4158) | Bit/dim 3.8757(3.8885) | Xent 1.3622(1.3978) | Loss 4.5568(4.5874) | Error 0.4851(0.4955) Steps 670(647.16) | Grad Norm 4.6205(5.5891) | Total Time 14.00(14.00)\n",
      "Iter 0867 | Time 61.4785(60.4477) | Bit/dim 3.8808(3.8883) | Xent 1.3669(1.3969) | Loss 4.5643(4.5867) | Error 0.4788(0.4950) Steps 658(647.48) | Grad Norm 4.3111(5.5507) | Total Time 14.00(14.00)\n",
      "Iter 0868 | Time 60.1855(60.4398) | Bit/dim 3.8790(3.8880) | Xent 1.3636(1.3959) | Loss 4.5608(4.5859) | Error 0.4798(0.4945) Steps 622(646.72) | Grad Norm 7.5093(5.6095) | Total Time 14.00(14.00)\n",
      "Iter 0869 | Time 58.4357(60.3797) | Bit/dim 3.8921(3.8881) | Xent 1.3768(1.3953) | Loss 4.5806(4.5858) | Error 0.4904(0.4944) Steps 652(646.88) | Grad Norm 6.6583(5.6410) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 58.5687(60.3254) | Bit/dim 3.8847(3.8880) | Xent 1.3819(1.3949) | Loss 4.5757(4.5855) | Error 0.4905(0.4943) Steps 658(647.21) | Grad Norm 3.7771(5.5850) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 25.0048, Epoch Time 400.4820(406.4059), Bit/dim 3.8791(best: 3.8804), Xent 1.3199, Loss 4.5390, Error 0.4730(best: 0.4756)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0871 | Time 60.6315(60.3346) | Bit/dim 3.8694(3.8875) | Xent 1.3524(1.3936) | Loss 4.5456(4.5843) | Error 0.4799(0.4938) Steps 646(647.17) | Grad Norm 2.9246(5.5052) | Total Time 14.00(14.00)\n",
      "Iter 0872 | Time 60.5676(60.3416) | Bit/dim 3.8919(3.8876) | Xent 1.3787(1.3932) | Loss 4.5812(4.5842) | Error 0.4856(0.4936) Steps 640(646.96) | Grad Norm 5.1168(5.4936) | Total Time 14.00(14.00)\n",
      "Iter 0873 | Time 56.1145(60.2147) | Bit/dim 3.8689(3.8870) | Xent 1.3644(1.3923) | Loss 4.5511(4.5832) | Error 0.4818(0.4932) Steps 622(646.21) | Grad Norm 5.3906(5.4905) | Total Time 14.00(14.00)\n",
      "Iter 0874 | Time 60.5331(60.2243) | Bit/dim 3.8804(3.8868) | Xent 1.3586(1.3913) | Loss 4.5598(4.5825) | Error 0.4829(0.4929) Steps 646(646.20) | Grad Norm 3.0657(5.4177) | Total Time 14.00(14.00)\n",
      "Iter 0875 | Time 57.2712(60.1357) | Bit/dim 3.8793(3.8866) | Xent 1.3578(1.3903) | Loss 4.5582(4.5818) | Error 0.4790(0.4925) Steps 640(646.02) | Grad Norm 4.3491(5.3857) | Total Time 14.00(14.00)\n",
      "Iter 0876 | Time 56.0273(60.0124) | Bit/dim 3.8819(3.8865) | Xent 1.3465(1.3890) | Loss 4.5551(4.5810) | Error 0.4785(0.4921) Steps 610(644.94) | Grad Norm 2.9361(5.3122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 23.9111, Epoch Time 391.6435(405.9630), Bit/dim 3.8797(best: 3.8791), Xent 1.3093, Loss 4.5344, Error 0.4624(best: 0.4730)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0877 | Time 59.9920(60.0118) | Bit/dim 3.8700(3.8860) | Xent 1.3394(1.3875) | Loss 4.5397(4.5797) | Error 0.4656(0.4913) Steps 628(644.43) | Grad Norm 3.1058(5.2460) | Total Time 14.00(14.00)\n",
      "Iter 0878 | Time 59.5486(59.9979) | Bit/dim 3.8867(3.8860) | Xent 1.3384(1.3860) | Loss 4.5559(4.5790) | Error 0.4754(0.4908) Steps 652(644.66) | Grad Norm 3.3712(5.1898) | Total Time 14.00(14.00)\n",
      "Iter 0879 | Time 61.5771(60.0453) | Bit/dim 3.8688(3.8855) | Xent 1.3489(1.3849) | Loss 4.5433(4.5779) | Error 0.4769(0.4904) Steps 616(643.80) | Grad Norm 3.9103(5.1514) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 60.0602(60.0458) | Bit/dim 3.8832(3.8854) | Xent 1.3456(1.3837) | Loss 4.5560(4.5773) | Error 0.4746(0.4899) Steps 628(643.32) | Grad Norm 3.3972(5.0987) | Total Time 14.00(14.00)\n",
      "Iter 0881 | Time 57.3557(59.9651) | Bit/dim 3.8776(3.8852) | Xent 1.3279(1.3820) | Loss 4.5416(4.5762) | Error 0.4692(0.4893) Steps 616(642.50) | Grad Norm 2.3206(5.0154) | Total Time 14.00(14.00)\n",
      "Iter 0882 | Time 57.7014(59.8971) | Bit/dim 3.8818(3.8851) | Xent 1.3357(1.3807) | Loss 4.5496(4.5754) | Error 0.4764(0.4889) Steps 610(641.53) | Grad Norm 3.0452(4.9563) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 23.4079, Epoch Time 397.3063(405.7033), Bit/dim 3.8763(best: 3.8791), Xent 1.2976, Loss 4.5251, Error 0.4633(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0883 | Time 55.2795(59.7586) | Bit/dim 3.8662(3.8845) | Xent 1.3195(1.3788) | Loss 4.5259(4.5739) | Error 0.4673(0.4883) Steps 610(640.58) | Grad Norm 1.1177(4.8411) | Total Time 14.00(14.00)\n",
      "Iter 0884 | Time 61.2988(59.8048) | Bit/dim 3.8766(3.8843) | Xent 1.3245(1.3772) | Loss 4.5389(4.5729) | Error 0.4665(0.4876) Steps 628(640.20) | Grad Norm 2.7851(4.7795) | Total Time 14.00(14.00)\n",
      "Iter 0885 | Time 60.7582(59.8334) | Bit/dim 3.8853(3.8843) | Xent 1.3406(1.3761) | Loss 4.5556(4.5724) | Error 0.4769(0.4873) Steps 622(639.66) | Grad Norm 2.2073(4.7023) | Total Time 14.00(14.00)\n",
      "Iter 0886 | Time 60.2494(59.8459) | Bit/dim 3.8691(3.8839) | Xent 1.3262(1.3746) | Loss 4.5322(4.5712) | Error 0.4587(0.4864) Steps 616(638.95) | Grad Norm 2.9421(4.6495) | Total Time 14.00(14.00)\n",
      "Iter 0887 | Time 59.6348(59.8396) | Bit/dim 3.8848(3.8839) | Xent 1.3395(1.3735) | Loss 4.5546(4.5707) | Error 0.4708(0.4860) Steps 622(638.44) | Grad Norm 4.6354(4.6491) | Total Time 14.00(14.00)\n",
      "Iter 0888 | Time 57.8779(59.7807) | Bit/dim 3.8744(3.8836) | Xent 1.3456(1.3727) | Loss 4.5472(4.5700) | Error 0.4844(0.4859) Steps 622(637.95) | Grad Norm 4.1769(4.6349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 23.8193, Epoch Time 396.2763(405.4205), Bit/dim 3.8767(best: 3.8763), Xent 1.3072, Loss 4.5302, Error 0.4690(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0889 | Time 60.6001(59.8053) | Bit/dim 3.8795(3.8835) | Xent 1.3408(1.3717) | Loss 4.5499(4.5694) | Error 0.4708(0.4855) Steps 634(637.83) | Grad Norm 3.6424(4.6051) | Total Time 14.00(14.00)\n",
      "Iter 0890 | Time 59.4907(59.7959) | Bit/dim 3.8667(3.8830) | Xent 1.3205(1.3702) | Loss 4.5270(4.5681) | Error 0.4683(0.4849) Steps 628(637.53) | Grad Norm 2.7453(4.5493) | Total Time 14.00(14.00)\n",
      "Iter 0891 | Time 58.4387(59.7552) | Bit/dim 3.8629(3.8824) | Xent 1.3397(1.3693) | Loss 4.5327(4.5670) | Error 0.4754(0.4847) Steps 640(637.61) | Grad Norm 2.3208(4.4825) | Total Time 14.00(14.00)\n",
      "Iter 0892 | Time 58.8139(59.7269) | Bit/dim 3.8726(3.8821) | Xent 1.3395(1.3684) | Loss 4.5423(4.5663) | Error 0.4722(0.4843) Steps 622(637.14) | Grad Norm 6.4888(4.5427) | Total Time 14.00(14.00)\n",
      "Iter 0893 | Time 59.6595(59.7249) | Bit/dim 3.8864(3.8822) | Xent 1.3702(1.3685) | Loss 4.5715(4.5664) | Error 0.4930(0.4845) Steps 616(636.51) | Grad Norm 8.8955(4.6733) | Total Time 14.00(14.00)\n",
      "Iter 0894 | Time 60.5988(59.7511) | Bit/dim 3.8701(3.8818) | Xent 1.3807(1.3688) | Loss 4.5605(4.5663) | Error 0.4815(0.4845) Steps 622(636.07) | Grad Norm 10.8968(4.8600) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 25.7203, Epoch Time 400.4207(405.2705), Bit/dim 3.8845(best: 3.8763), Xent 1.3857, Loss 4.5773, Error 0.4837(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0895 | Time 60.7678(59.7816) | Bit/dim 3.8844(3.8819) | Xent 1.4265(1.3705) | Loss 4.5977(4.5672) | Error 0.4975(0.4848) Steps 676(637.27) | Grad Norm 12.9836(5.1037) | Total Time 14.00(14.00)\n",
      "Iter 0896 | Time 59.1614(59.7630) | Bit/dim 3.8731(3.8817) | Xent 1.3663(1.3704) | Loss 4.5562(4.5669) | Error 0.4872(0.4849) Steps 616(636.63) | Grad Norm 5.0220(5.1012) | Total Time 14.00(14.00)\n",
      "Iter 0897 | Time 59.7648(59.7631) | Bit/dim 3.8714(3.8814) | Xent 1.3871(1.3709) | Loss 4.5649(4.5668) | Error 0.4898(0.4851) Steps 622(636.19) | Grad Norm 10.8192(5.2728) | Total Time 14.00(14.00)\n",
      "Iter 0898 | Time 63.0386(59.8613) | Bit/dim 3.8826(3.8814) | Xent 1.4316(1.3727) | Loss 4.5984(4.5678) | Error 0.5091(0.4858) Steps 664(637.03) | Grad Norm 12.4446(5.4879) | Total Time 14.00(14.00)\n",
      "Iter 0899 | Time 61.7141(59.9169) | Bit/dim 3.8769(3.8813) | Xent 1.3671(1.3726) | Loss 4.5605(4.5675) | Error 0.4841(0.4857) Steps 670(638.01) | Grad Norm 7.1712(5.5384) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 59.2694(59.8975) | Bit/dim 3.8762(3.8811) | Xent 1.4452(1.3748) | Loss 4.5988(4.5685) | Error 0.5059(0.4863) Steps 616(637.35) | Grad Norm 12.7831(5.7558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 23.3273, Epoch Time 404.8178(405.2569), Bit/dim 3.8759(best: 3.8763), Xent 1.4517, Loss 4.6017, Error 0.5312(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0901 | Time 58.4832(59.8551) | Bit/dim 3.8703(3.8808) | Xent 1.4872(1.3781) | Loss 4.6139(4.5698) | Error 0.5344(0.4878) Steps 634(637.25) | Grad Norm 14.7805(6.0265) | Total Time 14.00(14.00)\n",
      "Iter 0902 | Time 60.3767(59.8707) | Bit/dim 3.8767(3.8807) | Xent 1.4702(1.3809) | Loss 4.6118(4.5711) | Error 0.5217(0.4888) Steps 634(637.16) | Grad Norm 13.9558(6.2644) | Total Time 14.00(14.00)\n",
      "Iter 0903 | Time 62.4590(59.9484) | Bit/dim 3.8889(3.8809) | Xent 1.4287(1.3823) | Loss 4.6032(4.5721) | Error 0.5044(0.4893) Steps 688(638.68) | Grad Norm 4.8398(6.2216) | Total Time 14.00(14.00)\n",
      "Iter 0904 | Time 59.8069(59.9441) | Bit/dim 3.8623(3.8803) | Xent 1.4212(1.3835) | Loss 4.5729(4.5721) | Error 0.5080(0.4898) Steps 664(639.44) | Grad Norm 10.7296(6.3569) | Total Time 14.00(14.00)\n",
      "Iter 0905 | Time 58.2071(59.8920) | Bit/dim 3.8664(3.8799) | Xent 1.4053(1.3841) | Loss 4.5690(4.5720) | Error 0.4970(0.4900) Steps 622(638.92) | Grad Norm 4.6018(6.3042) | Total Time 14.00(14.00)\n",
      "Iter 0906 | Time 59.5599(59.8820) | Bit/dim 3.8817(3.8800) | Xent 1.4408(1.3858) | Loss 4.6021(4.5729) | Error 0.5105(0.4907) Steps 622(638.41) | Grad Norm 9.1494(6.3896) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 25.5839, Epoch Time 401.4060(405.1414), Bit/dim 3.8717(best: 3.8759), Xent 1.3697, Loss 4.5566, Error 0.4798(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0907 | Time 61.7262(59.9374) | Bit/dim 3.8613(3.8794) | Xent 1.4235(1.3870) | Loss 4.5731(4.5729) | Error 0.4986(0.4909) Steps 682(639.72) | Grad Norm 7.6927(6.4287) | Total Time 14.00(14.00)\n",
      "Iter 0908 | Time 62.4399(60.0124) | Bit/dim 3.8793(3.8794) | Xent 1.3839(1.3869) | Loss 4.5712(4.5729) | Error 0.4881(0.4908) Steps 682(640.99) | Grad Norm 4.0562(6.3575) | Total Time 14.00(14.00)\n",
      "Iter 0909 | Time 57.7933(59.9459) | Bit/dim 3.8738(3.8792) | Xent 1.4100(1.3876) | Loss 4.5788(4.5730) | Error 0.5025(0.4912) Steps 652(641.32) | Grad Norm 7.9126(6.4042) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 57.6754(59.8777) | Bit/dim 3.8740(3.8791) | Xent 1.3481(1.3864) | Loss 4.5481(4.5723) | Error 0.4729(0.4906) Steps 646(641.46) | Grad Norm 2.1177(6.2756) | Total Time 14.00(14.00)\n",
      "Iter 0911 | Time 57.1143(59.7948) | Bit/dim 3.8760(3.8790) | Xent 1.3962(1.3867) | Loss 4.5741(4.5723) | Error 0.4990(0.4909) Steps 646(641.59) | Grad Norm 7.4270(6.3101) | Total Time 14.00(14.00)\n",
      "Iter 0912 | Time 57.3243(59.7207) | Bit/dim 3.8761(3.8789) | Xent 1.3412(1.3853) | Loss 4.5467(4.5716) | Error 0.4685(0.4902) Steps 622(641.01) | Grad Norm 5.6646(6.2907) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 24.5665, Epoch Time 395.4582(404.8509), Bit/dim 3.8733(best: 3.8717), Xent 1.3138, Loss 4.5302, Error 0.4698(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0913 | Time 62.0126(59.7895) | Bit/dim 3.8704(3.8787) | Xent 1.3409(1.3840) | Loss 4.5408(4.5706) | Error 0.4838(0.4900) Steps 646(641.16) | Grad Norm 3.9373(6.2201) | Total Time 14.00(14.00)\n",
      "Iter 0914 | Time 60.7251(59.8176) | Bit/dim 3.8820(3.8788) | Xent 1.3759(1.3837) | Loss 4.5699(4.5706) | Error 0.4792(0.4897) Steps 658(641.66) | Grad Norm 7.1145(6.2470) | Total Time 14.00(14.00)\n",
      "Iter 0915 | Time 59.7031(59.8141) | Bit/dim 3.8706(3.8785) | Xent 1.3295(1.3821) | Loss 4.5354(4.5696) | Error 0.4764(0.4893) Steps 652(641.97) | Grad Norm 2.4415(6.1328) | Total Time 14.00(14.00)\n",
      "Iter 0916 | Time 62.1100(59.8830) | Bit/dim 3.8771(3.8785) | Xent 1.3878(1.3823) | Loss 4.5710(4.5696) | Error 0.4900(0.4893) Steps 610(641.01) | Grad Norm 7.6294(6.1777) | Total Time 14.00(14.00)\n",
      "Iter 0917 | Time 60.4276(59.8993) | Bit/dim 3.8682(3.8782) | Xent 1.3352(1.3809) | Loss 4.5358(4.5686) | Error 0.4708(0.4887) Steps 652(641.34) | Grad Norm 3.5324(6.0983) | Total Time 14.00(14.00)\n",
      "Iter 0918 | Time 58.5531(59.8589) | Bit/dim 3.8695(3.8779) | Xent 1.3376(1.3796) | Loss 4.5383(4.5677) | Error 0.4710(0.4882) Steps 658(641.84) | Grad Norm 5.1284(6.0692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 23.7283, Epoch Time 403.7881(404.8190), Bit/dim 3.8717(best: 3.8717), Xent 1.3032, Loss 4.5233, Error 0.4684(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0919 | Time 58.5221(59.8188) | Bit/dim 3.8700(3.8777) | Xent 1.3372(1.3783) | Loss 4.5386(4.5668) | Error 0.4624(0.4874) Steps 646(641.97) | Grad Norm 4.2614(6.0150) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 57.5063(59.7495) | Bit/dim 3.8674(3.8774) | Xent 1.3286(1.3768) | Loss 4.5317(4.5658) | Error 0.4650(0.4868) Steps 634(641.73) | Grad Norm 3.2740(5.9328) | Total Time 14.00(14.00)\n",
      "Iter 0921 | Time 60.9831(59.7865) | Bit/dim 3.8784(3.8774) | Xent 1.3173(1.3750) | Loss 4.5371(4.5649) | Error 0.4641(0.4861) Steps 640(641.68) | Grad Norm 4.4210(5.8874) | Total Time 14.00(14.00)\n",
      "Iter 0922 | Time 57.6801(59.7233) | Bit/dim 3.8728(3.8772) | Xent 1.3391(1.3740) | Loss 4.5423(4.5642) | Error 0.4748(0.4857) Steps 616(640.90) | Grad Norm 4.4959(5.8457) | Total Time 14.00(14.00)\n",
      "Iter 0923 | Time 57.5466(59.6580) | Bit/dim 3.8683(3.8770) | Xent 1.3350(1.3728) | Loss 4.5358(4.5634) | Error 0.4719(0.4853) Steps 610(639.98) | Grad Norm 4.4570(5.8040) | Total Time 14.00(14.00)\n",
      "Iter 0924 | Time 58.6938(59.6291) | Bit/dim 3.8695(3.8768) | Xent 1.3292(1.3715) | Loss 4.5341(4.5625) | Error 0.4730(0.4850) Steps 628(639.62) | Grad Norm 4.1499(5.7544) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 24.4615, Epoch Time 392.6054(404.4526), Bit/dim 3.8751(best: 3.8717), Xent 1.3037, Loss 4.5270, Error 0.4666(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0925 | Time 61.5301(59.6861) | Bit/dim 3.8771(3.8768) | Xent 1.3361(1.3704) | Loss 4.5451(4.5620) | Error 0.4674(0.4844) Steps 646(639.81) | Grad Norm 7.2082(5.7980) | Total Time 14.00(14.00)\n",
      "Iter 0926 | Time 56.5274(59.5913) | Bit/dim 3.8682(3.8765) | Xent 1.3491(1.3698) | Loss 4.5427(4.5614) | Error 0.4871(0.4845) Steps 610(638.92) | Grad Norm 5.9996(5.8041) | Total Time 14.00(14.00)\n",
      "Iter 0927 | Time 56.7915(59.5073) | Bit/dim 3.8763(3.8765) | Xent 1.3455(1.3691) | Loss 4.5491(4.5610) | Error 0.4812(0.4844) Steps 616(638.23) | Grad Norm 7.4241(5.8527) | Total Time 14.00(14.00)\n",
      "Iter 0928 | Time 61.8260(59.5769) | Bit/dim 3.8755(3.8765) | Xent 1.3262(1.3678) | Loss 4.5387(4.5604) | Error 0.4629(0.4838) Steps 664(639.00) | Grad Norm 7.2223(5.8937) | Total Time 14.00(14.00)\n",
      "Iter 0929 | Time 58.0308(59.5305) | Bit/dim 3.8565(3.8759) | Xent 1.3390(1.3669) | Loss 4.5260(4.5593) | Error 0.4734(0.4835) Steps 640(639.03) | Grad Norm 8.3983(5.9689) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 58.2487(59.4921) | Bit/dim 3.8663(3.8756) | Xent 1.3862(1.3675) | Loss 4.5594(4.5593) | Error 0.4832(0.4834) Steps 616(638.34) | Grad Norm 11.6715(6.1400) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 24.8395, Epoch Time 395.5962(404.1869), Bit/dim 3.8690(best: 3.8717), Xent 1.2962, Loss 4.5171, Error 0.4582(best: 0.4624)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0931 | Time 60.0361(59.5084) | Bit/dim 3.8735(3.8755) | Xent 1.3169(1.3660) | Loss 4.5320(4.5585) | Error 0.4699(0.4830) Steps 652(638.75) | Grad Norm 5.3303(6.1157) | Total Time 14.00(14.00)\n",
      "Iter 0932 | Time 60.5800(59.5405) | Bit/dim 3.8712(3.8754) | Xent 1.3473(1.3654) | Loss 4.5448(4.5581) | Error 0.4801(0.4830) Steps 652(639.15) | Grad Norm 6.4584(6.1260) | Total Time 14.00(14.00)\n",
      "Iter 0933 | Time 58.1050(59.4975) | Bit/dim 3.8625(3.8750) | Xent 1.3465(1.3648) | Loss 4.5357(4.5574) | Error 0.4735(0.4827) Steps 622(638.63) | Grad Norm 7.6291(6.1711) | Total Time 14.00(14.00)\n",
      "Iter 0934 | Time 60.6791(59.5329) | Bit/dim 3.8553(3.8744) | Xent 1.3184(1.3634) | Loss 4.5145(4.5561) | Error 0.4625(0.4821) Steps 622(638.13) | Grad Norm 2.8272(6.0707) | Total Time 14.00(14.00)\n",
      "Iter 0935 | Time 60.2028(59.5530) | Bit/dim 3.8702(3.8743) | Xent 1.3300(1.3624) | Loss 4.5352(4.5555) | Error 0.4685(0.4817) Steps 652(638.55) | Grad Norm 6.3128(6.0780) | Total Time 14.00(14.00)\n",
      "Iter 0936 | Time 60.7667(59.5894) | Bit/dim 3.8653(3.8740) | Xent 1.3332(1.3616) | Loss 4.5319(4.5548) | Error 0.4666(0.4812) Steps 646(638.77) | Grad Norm 5.5793(6.0630) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 23.0050, Epoch Time 401.0568(404.0930), Bit/dim 3.8671(best: 3.8690), Xent 1.3054, Loss 4.5197, Error 0.4675(best: 0.4582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0937 | Time 57.8227(59.5364) | Bit/dim 3.8611(3.8736) | Xent 1.3301(1.3606) | Loss 4.5262(4.5539) | Error 0.4681(0.4808) Steps 604(637.73) | Grad Norm 5.6043(6.0493) | Total Time 14.00(14.00)\n",
      "Iter 0938 | Time 62.3675(59.6213) | Bit/dim 3.8614(3.8733) | Xent 1.3233(1.3595) | Loss 4.5230(4.5530) | Error 0.4736(0.4806) Steps 622(637.26) | Grad Norm 3.7718(5.9809) | Total Time 14.00(14.00)\n",
      "Iter 0939 | Time 59.3532(59.6133) | Bit/dim 3.8721(3.8732) | Xent 1.3298(1.3586) | Loss 4.5370(4.5525) | Error 0.4708(0.4803) Steps 664(638.06) | Grad Norm 10.1177(6.1050) | Total Time 14.00(14.00)\n",
      "Iter 0940 | Time 59.6274(59.6137) | Bit/dim 3.8610(3.8729) | Xent 1.3518(1.3584) | Loss 4.5370(4.5521) | Error 0.4804(0.4803) Steps 616(637.40) | Grad Norm 6.5002(6.1169) | Total Time 14.00(14.00)\n",
      "Iter 0941 | Time 59.8386(59.6205) | Bit/dim 3.8714(3.8728) | Xent 1.3639(1.3586) | Loss 4.5533(4.5521) | Error 0.4806(0.4803) Steps 622(636.94) | Grad Norm 7.6822(6.1639) | Total Time 14.00(14.00)\n",
      "Iter 0942 | Time 59.8002(59.6259) | Bit/dim 3.8576(3.8724) | Xent 1.3283(1.3577) | Loss 4.5218(4.5512) | Error 0.4649(0.4799) Steps 658(637.57) | Grad Norm 3.8228(6.0936) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 24.8787, Epoch Time 401.0471(404.0016), Bit/dim 3.8680(best: 3.8671), Xent 1.3107, Loss 4.5233, Error 0.4663(best: 0.4582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0943 | Time 60.9997(59.6671) | Bit/dim 3.8733(3.8724) | Xent 1.3693(1.3580) | Loss 4.5579(4.5514) | Error 0.4901(0.4802) Steps 658(638.18) | Grad Norm 6.6178(6.1094) | Total Time 14.00(14.00)\n",
      "Iter 0944 | Time 58.3986(59.6290) | Bit/dim 3.8529(3.8718) | Xent 1.3212(1.3569) | Loss 4.5135(4.5503) | Error 0.4710(0.4799) Steps 640(638.24) | Grad Norm 4.5138(6.0615) | Total Time 14.00(14.00)\n",
      "Iter 0945 | Time 56.8177(59.5447) | Bit/dim 3.8714(3.8718) | Xent 1.3404(1.3564) | Loss 4.5416(4.5500) | Error 0.4690(0.4796) Steps 616(637.57) | Grad Norm 5.0481(6.0311) | Total Time 14.00(14.00)\n",
      "Iter 0946 | Time 59.4910(59.5431) | Bit/dim 3.8592(3.8714) | Xent 1.3242(1.3554) | Loss 4.5213(4.5491) | Error 0.4699(0.4793) Steps 652(638.00) | Grad Norm 4.4376(5.9833) | Total Time 14.00(14.00)\n",
      "Iter 0947 | Time 62.2744(59.6250) | Bit/dim 3.8672(3.8713) | Xent 1.3331(1.3548) | Loss 4.5338(4.5487) | Error 0.4679(0.4789) Steps 670(638.96) | Grad Norm 8.3849(6.0553) | Total Time 14.00(14.00)\n",
      "Iter 0948 | Time 57.7646(59.5692) | Bit/dim 3.8619(3.8710) | Xent 1.3133(1.3535) | Loss 4.5186(4.5478) | Error 0.4615(0.4784) Steps 604(637.91) | Grad Norm 6.0558(6.0554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 23.2395, Epoch Time 396.3402(403.7718), Bit/dim 3.8623(best: 3.8671), Xent 1.2920, Loss 4.5083, Error 0.4656(best: 0.4582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0949 | Time 57.3248(59.5019) | Bit/dim 3.8541(3.8705) | Xent 1.3295(1.3528) | Loss 4.5189(4.5469) | Error 0.4704(0.4782) Steps 604(636.90) | Grad Norm 5.0668(6.0257) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 60.6029(59.5349) | Bit/dim 3.8648(3.8703) | Xent 1.3229(1.3519) | Loss 4.5263(4.5463) | Error 0.4676(0.4778) Steps 634(636.81) | Grad Norm 5.6171(6.0134) | Total Time 14.00(14.00)\n",
      "Iter 0951 | Time 61.6566(59.5986) | Bit/dim 3.8687(3.8703) | Xent 1.3133(1.3508) | Loss 4.5253(4.5457) | Error 0.4610(0.4773) Steps 646(637.08) | Grad Norm 3.2976(5.9320) | Total Time 14.00(14.00)\n",
      "Iter 0952 | Time 57.9937(59.5504) | Bit/dim 3.8749(3.8704) | Xent 1.3000(1.3492) | Loss 4.5249(4.5450) | Error 0.4636(0.4769) Steps 604(636.09) | Grad Norm 4.7453(5.8964) | Total Time 14.00(14.00)\n",
      "Iter 0953 | Time 62.6910(59.6446) | Bit/dim 3.8554(3.8700) | Xent 1.3135(1.3482) | Loss 4.5121(4.5440) | Error 0.4681(0.4767) Steps 634(636.03) | Grad Norm 3.6610(5.8293) | Total Time 14.00(14.00)\n",
      "Iter 0954 | Time 58.9197(59.6229) | Bit/dim 3.8589(3.8696) | Xent 1.3048(1.3469) | Loss 4.5113(4.5431) | Error 0.4624(0.4762) Steps 652(636.51) | Grad Norm 5.7490(5.8269) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 23.2890, Epoch Time 399.5972(403.6465), Bit/dim 3.8613(best: 3.8623), Xent 1.2740, Loss 4.4983, Error 0.4539(best: 0.4582)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0955 | Time 56.9080(59.5414) | Bit/dim 3.8611(3.8694) | Xent 1.3061(1.3456) | Loss 4.5142(4.5422) | Error 0.4557(0.4756) Steps 622(636.07) | Grad Norm 4.7431(5.7944) | Total Time 14.00(14.00)\n",
      "Iter 0956 | Time 58.1752(59.5004) | Bit/dim 3.8767(3.8696) | Xent 1.3117(1.3446) | Loss 4.5325(4.5419) | Error 0.4673(0.4754) Steps 616(635.47) | Grad Norm 8.2147(5.8670) | Total Time 14.00(14.00)\n",
      "Iter 0957 | Time 57.8496(59.4509) | Bit/dim 3.8676(3.8695) | Xent 1.3167(1.3438) | Loss 4.5260(4.5414) | Error 0.4633(0.4750) Steps 646(635.79) | Grad Norm 7.0676(5.9030) | Total Time 14.00(14.00)\n",
      "Iter 0958 | Time 61.0897(59.5001) | Bit/dim 3.8573(3.8692) | Xent 1.3051(1.3426) | Loss 4.5098(4.5405) | Error 0.4565(0.4745) Steps 634(635.73) | Grad Norm 4.9845(5.8754) | Total Time 14.00(14.00)\n",
      "Iter 0959 | Time 60.2965(59.5240) | Bit/dim 3.8570(3.8688) | Xent 1.3203(1.3420) | Loss 4.5171(4.5398) | Error 0.4665(0.4742) Steps 616(635.14) | Grad Norm 5.3475(5.8596) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 58.7385(59.5004) | Bit/dim 3.8513(3.8683) | Xent 1.3189(1.3413) | Loss 4.5107(4.5389) | Error 0.4686(0.4740) Steps 616(634.57) | Grad Norm 4.0889(5.8065) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 24.1122, Epoch Time 394.6405(403.3764), Bit/dim 3.8643(best: 3.8613), Xent 1.3149, Loss 4.5217, Error 0.4705(best: 0.4539)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0961 | Time 58.1816(59.4608) | Bit/dim 3.8648(3.8682) | Xent 1.3306(1.3409) | Loss 4.5301(4.5386) | Error 0.4725(0.4740) Steps 628(634.37) | Grad Norm 10.3847(5.9438) | Total Time 14.00(14.00)\n",
      "Iter 0962 | Time 57.9002(59.4140) | Bit/dim 3.8589(3.8679) | Xent 1.3448(1.3411) | Loss 4.5314(4.5384) | Error 0.4805(0.4742) Steps 610(633.64) | Grad Norm 8.3132(6.0149) | Total Time 14.00(14.00)\n",
      "Iter 0963 | Time 56.9899(59.3413) | Bit/dim 3.8650(3.8678) | Xent 1.3225(1.3405) | Loss 4.5262(4.5381) | Error 0.4644(0.4739) Steps 610(632.93) | Grad Norm 7.0927(6.0472) | Total Time 14.00(14.00)\n",
      "Iter 0964 | Time 63.7513(59.4736) | Bit/dim 3.8539(3.8674) | Xent 1.3180(1.3398) | Loss 4.5130(4.5373) | Error 0.4623(0.4736) Steps 628(632.78) | Grad Norm 4.8535(6.0114) | Total Time 14.00(14.00)\n",
      "Iter 0965 | Time 60.8440(59.5147) | Bit/dim 3.8437(3.8667) | Xent 1.3172(1.3391) | Loss 4.5023(4.5363) | Error 0.4564(0.4730) Steps 658(633.54) | Grad Norm 5.5066(5.9963) | Total Time 14.00(14.00)\n",
      "Iter 0966 | Time 58.3999(59.4813) | Bit/dim 3.8644(3.8666) | Xent 1.3369(1.3391) | Loss 4.5329(4.5362) | Error 0.4700(0.4729) Steps 610(632.83) | Grad Norm 6.7062(6.0176) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 23.1034, Epoch Time 396.0428(403.1564), Bit/dim 3.8605(best: 3.8613), Xent 1.2812, Loss 4.5011, Error 0.4616(best: 0.4539)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0967 | Time 56.5354(59.3929) | Bit/dim 3.8652(3.8666) | Xent 1.3157(1.3384) | Loss 4.5231(4.5358) | Error 0.4681(0.4728) Steps 610(632.15) | Grad Norm 6.2534(6.0247) | Total Time 14.00(14.00)\n",
      "Iter 0968 | Time 58.8276(59.3759) | Bit/dim 3.8538(3.8662) | Xent 1.2968(1.3371) | Loss 4.5023(4.5348) | Error 0.4604(0.4724) Steps 622(631.84) | Grad Norm 4.6430(5.9832) | Total Time 14.00(14.00)\n",
      "Iter 0969 | Time 57.9643(59.3336) | Bit/dim 3.8577(3.8659) | Xent 1.2962(1.3359) | Loss 4.5058(4.5339) | Error 0.4544(0.4719) Steps 616(631.37) | Grad Norm 3.0295(5.8946) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 59.6366(59.3427) | Bit/dim 3.8545(3.8656) | Xent 1.3135(1.3352) | Loss 4.5113(4.5332) | Error 0.4660(0.4717) Steps 610(630.73) | Grad Norm 5.0201(5.8684) | Total Time 14.00(14.00)\n",
      "Iter 0971 | Time 58.1819(59.3079) | Bit/dim 3.8564(3.8653) | Xent 1.3342(1.3352) | Loss 4.5235(4.5329) | Error 0.4778(0.4719) Steps 616(630.28) | Grad Norm 7.2667(5.9103) | Total Time 14.00(14.00)\n",
      "Iter 0972 | Time 61.7562(59.3813) | Bit/dim 3.8649(3.8653) | Xent 1.3053(1.3343) | Loss 4.5176(4.5325) | Error 0.4615(0.4716) Steps 640(630.58) | Grad Norm 7.9058(5.9702) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 23.0134, Epoch Time 393.1306(402.8556), Bit/dim 3.8591(best: 3.8605), Xent 1.2574, Loss 4.4878, Error 0.4489(best: 0.4539)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0973 | Time 57.7971(59.3338) | Bit/dim 3.8563(3.8650) | Xent 1.2824(1.3327) | Loss 4.4975(4.5314) | Error 0.4480(0.4709) Steps 616(630.14) | Grad Norm 2.5146(5.8665) | Total Time 14.00(14.00)\n",
      "Iter 0974 | Time 59.6965(59.3447) | Bit/dim 3.8565(3.8648) | Xent 1.3180(1.3323) | Loss 4.5155(4.5309) | Error 0.4634(0.4706) Steps 610(629.53) | Grad Norm 7.9452(5.9289) | Total Time 14.00(14.00)\n",
      "Iter 0975 | Time 59.6051(59.3525) | Bit/dim 3.8560(3.8645) | Xent 1.3397(1.3325) | Loss 4.5259(4.5308) | Error 0.4759(0.4708) Steps 610(628.95) | Grad Norm 11.5157(6.0965) | Total Time 14.00(14.00)\n",
      "Iter 0976 | Time 59.9551(59.3706) | Bit/dim 3.8657(3.8646) | Xent 1.3972(1.3345) | Loss 4.5643(4.5318) | Error 0.4891(0.4714) Steps 652(629.64) | Grad Norm 15.4271(6.3764) | Total Time 14.00(14.00)\n",
      "Iter 0977 | Time 57.4110(59.3118) | Bit/dim 3.8647(3.8646) | Xent 1.4355(1.3375) | Loss 4.5824(4.5333) | Error 0.5160(0.4727) Steps 610(629.05) | Grad Norm 9.4359(6.4682) | Total Time 14.00(14.00)\n",
      "Iter 0978 | Time 57.2580(59.2502) | Bit/dim 3.8601(3.8644) | Xent 1.4090(1.3396) | Loss 4.5646(4.5342) | Error 0.5101(0.4738) Steps 610(628.48) | Grad Norm 6.2951(6.4630) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 23.3261, Epoch Time 392.8965(402.5568), Bit/dim 3.8618(best: 3.8591), Xent 1.2991, Loss 4.5114, Error 0.4591(best: 0.4489)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0979 | Time 58.7731(59.2358) | Bit/dim 3.8702(3.8646) | Xent 1.3487(1.3399) | Loss 4.5446(4.5346) | Error 0.4788(0.4740) Steps 616(628.11) | Grad Norm 3.4381(6.3722) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 57.4232(59.1815) | Bit/dim 3.8755(3.8649) | Xent 1.3851(1.3413) | Loss 4.5681(4.5356) | Error 0.4949(0.4746) Steps 652(628.82) | Grad Norm 7.7594(6.4139) | Total Time 14.00(14.00)\n",
      "Iter 0981 | Time 59.3295(59.1859) | Bit/dim 3.8523(3.8645) | Xent 1.3552(1.3417) | Loss 4.5299(4.5354) | Error 0.4849(0.4749) Steps 640(629.16) | Grad Norm 3.0127(6.3118) | Total Time 14.00(14.00)\n",
      "Iter 0982 | Time 57.2800(59.1287) | Bit/dim 3.8534(3.8642) | Xent 1.3499(1.3419) | Loss 4.5283(4.5352) | Error 0.4784(0.4750) Steps 610(628.58) | Grad Norm 4.0115(6.2428) | Total Time 14.00(14.00)\n",
      "Iter 0983 | Time 56.7427(59.0571) | Bit/dim 3.8683(3.8643) | Xent 1.3887(1.3433) | Loss 4.5627(4.5360) | Error 0.4940(0.4756) Steps 616(628.21) | Grad Norm 6.6115(6.2539) | Total Time 14.00(14.00)\n",
      "Iter 0984 | Time 58.6392(59.0446) | Bit/dim 3.8533(3.8640) | Xent 1.3469(1.3434) | Loss 4.5268(4.5357) | Error 0.4696(0.4754) Steps 622(628.02) | Grad Norm 6.5363(6.2624) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 24.2894, Epoch Time 389.7615(402.1730), Bit/dim 3.8622(best: 3.8591), Xent 1.2936, Loss 4.5090, Error 0.4618(best: 0.4489)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0985 | Time 58.9342(59.0413) | Bit/dim 3.8598(3.8639) | Xent 1.3316(1.3431) | Loss 4.5255(4.5354) | Error 0.4718(0.4753) Steps 658(628.92) | Grad Norm 3.7927(6.1883) | Total Time 14.00(14.00)\n",
      "Iter 0986 | Time 62.2919(59.1388) | Bit/dim 3.8620(3.8638) | Xent 1.3449(1.3431) | Loss 4.5344(4.5354) | Error 0.4880(0.4757) Steps 640(629.25) | Grad Norm 5.0669(6.1546) | Total Time 14.00(14.00)\n",
      "Iter 0987 | Time 59.6667(59.1546) | Bit/dim 3.8572(3.8636) | Xent 1.3337(1.3429) | Loss 4.5240(4.5351) | Error 0.4736(0.4756) Steps 622(629.03) | Grad Norm 4.5180(6.1055) | Total Time 14.00(14.00)\n",
      "Iter 0988 | Time 60.4847(59.1945) | Bit/dim 3.8736(3.8639) | Xent 1.2931(1.3414) | Loss 4.5202(4.5346) | Error 0.4573(0.4751) Steps 616(628.64) | Grad Norm 2.9661(6.0113) | Total Time 14.00(14.00)\n",
      "Iter 0989 | Time 59.4696(59.2028) | Bit/dim 3.8618(3.8639) | Xent 1.3303(1.3410) | Loss 4.5270(4.5344) | Error 0.4669(0.4748) Steps 616(628.26) | Grad Norm 6.1094(6.0143) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 56.9258(59.1345) | Bit/dim 3.8485(3.8634) | Xent 1.3169(1.3403) | Loss 4.5069(4.5336) | Error 0.4605(0.4744) Steps 610(627.72) | Grad Norm 8.9583(6.1026) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 24.0872, Epoch Time 399.1621(402.0826), Bit/dim 3.8658(best: 3.8591), Xent 1.3121, Loss 4.5218, Error 0.4659(best: 0.4489)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0991 | Time 59.7856(59.1540) | Bit/dim 3.8582(3.8632) | Xent 1.3429(1.3404) | Loss 4.5297(4.5334) | Error 0.4799(0.4745) Steps 628(627.72) | Grad Norm 8.0217(6.1602) | Total Time 14.00(14.00)\n",
      "Iter 0992 | Time 58.3366(59.1295) | Bit/dim 3.8420(3.8626) | Xent 1.3240(1.3399) | Loss 4.5040(4.5326) | Error 0.4665(0.4743) Steps 622(627.55) | Grad Norm 7.4974(6.2003) | Total Time 14.00(14.00)\n",
      "Iter 0993 | Time 61.5571(59.2023) | Bit/dim 3.8677(3.8628) | Xent 1.3756(1.3410) | Loss 4.5555(4.5332) | Error 0.4844(0.4746) Steps 610(627.03) | Grad Norm 13.7593(6.4271) | Total Time 14.00(14.00)\n",
      "Iter 0994 | Time 59.9955(59.2261) | Bit/dim 3.8577(3.8626) | Xent 1.3468(1.3411) | Loss 4.5311(4.5332) | Error 0.4785(0.4747) Steps 622(626.87) | Grad Norm 7.9014(6.4713) | Total Time 14.00(14.00)\n",
      "Iter 0995 | Time 62.5737(59.3266) | Bit/dim 3.8667(3.8627) | Xent 1.3373(1.3410) | Loss 4.5354(4.5332) | Error 0.4689(0.4745) Steps 646(627.45) | Grad Norm 5.8840(6.4537) | Total Time 14.00(14.00)\n",
      "Iter 0996 | Time 57.3477(59.2672) | Bit/dim 3.8478(3.8623) | Xent 1.3075(1.3400) | Loss 4.5016(4.5323) | Error 0.4664(0.4743) Steps 610(626.92) | Grad Norm 5.0729(6.4123) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 23.4670, Epoch Time 400.5255(402.0359), Bit/dim 3.8541(best: 3.8591), Xent 1.2775, Loss 4.4928, Error 0.4594(best: 0.4489)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 0997 | Time 57.7935(59.2230) | Bit/dim 3.8622(3.8623) | Xent 1.3204(1.3394) | Loss 4.5223(4.5320) | Error 0.4614(0.4739) Steps 616(626.60) | Grad Norm 4.9627(6.3688) | Total Time 14.00(14.00)\n",
      "Iter 0998 | Time 61.0380(59.2774) | Bit/dim 3.8531(3.8620) | Xent 1.3071(1.3385) | Loss 4.5067(4.5312) | Error 0.4599(0.4735) Steps 610(626.10) | Grad Norm 4.2211(6.3043) | Total Time 14.00(14.00)\n",
      "Iter 0999 | Time 55.5374(59.1652) | Bit/dim 3.8533(3.8617) | Xent 1.3390(1.3385) | Loss 4.5228(4.5310) | Error 0.4770(0.4736) Steps 610(625.62) | Grad Norm 9.5320(6.4012) | Total Time 14.00(14.00)\n",
      "Iter 1000 | Time 58.9102(59.1576) | Bit/dim 3.8562(3.8616) | Xent 1.3291(1.3382) | Loss 4.5208(4.5307) | Error 0.4716(0.4735) Steps 610(625.15) | Grad Norm 5.9865(6.3887) | Total Time 14.00(14.00)\n",
      "Iter 1001 | Time 56.2103(59.0692) | Bit/dim 3.8498(3.8612) | Xent 1.2967(1.3370) | Loss 4.4981(4.5297) | Error 0.4580(0.4731) Steps 610(624.69) | Grad Norm 4.9652(6.3460) | Total Time 14.00(14.00)\n",
      "Iter 1002 | Time 59.3853(59.0786) | Bit/dim 3.8542(3.8610) | Xent 1.2874(1.3355) | Loss 4.4979(4.5288) | Error 0.4524(0.4725) Steps 640(625.15) | Grad Norm 4.3872(6.2873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 22.9913, Epoch Time 389.4332(401.6578), Bit/dim 3.8576(best: 3.8541), Xent 1.2553, Loss 4.4853, Error 0.4456(best: 0.4489)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1003 | Time 58.8143(59.0707) | Bit/dim 3.8556(3.8609) | Xent 1.2820(1.3339) | Loss 4.4965(4.5278) | Error 0.4497(0.4718) Steps 610(624.70) | Grad Norm 3.3648(6.1996) | Total Time 14.00(14.00)\n",
      "Iter 1004 | Time 57.5247(59.0243) | Bit/dim 3.8669(3.8610) | Xent 1.2876(1.3325) | Loss 4.5107(4.5273) | Error 0.4574(0.4713) Steps 610(624.26) | Grad Norm 3.3890(6.1153) | Total Time 14.00(14.00)\n",
      "Iter 1005 | Time 59.8880(59.0502) | Bit/dim 3.8469(3.8606) | Xent 1.3103(1.3318) | Loss 4.5021(4.5265) | Error 0.4604(0.4710) Steps 616(624.01) | Grad Norm 5.3626(6.0927) | Total Time 14.00(14.00)\n",
      "Iter 1006 | Time 60.7122(59.1001) | Bit/dim 3.8540(3.8604) | Xent 1.2925(1.3306) | Loss 4.5002(4.5257) | Error 0.4626(0.4708) Steps 610(623.59) | Grad Norm 5.4910(6.0746) | Total Time 14.00(14.00)\n",
      "Iter 1007 | Time 58.2296(59.0740) | Bit/dim 3.8445(3.8599) | Xent 1.2999(1.3297) | Loss 4.4945(4.5248) | Error 0.4577(0.4704) Steps 610(623.18) | Grad Norm 4.8328(6.0374) | Total Time 14.00(14.00)\n",
      "Iter 1008 | Time 57.1031(59.0149) | Bit/dim 3.8545(3.8598) | Xent 1.3088(1.3291) | Loss 4.5088(4.5243) | Error 0.4599(0.4701) Steps 610(622.79) | Grad Norm 11.4016(6.1983) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 23.4465, Epoch Time 393.0958(401.4010), Bit/dim 3.8527(best: 3.8541), Xent 1.2828, Loss 4.4941, Error 0.4541(best: 0.4456)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1009 | Time 61.4925(59.0892) | Bit/dim 3.8541(3.8596) | Xent 1.3179(1.3287) | Loss 4.5131(4.5240) | Error 0.4613(0.4698) Steps 610(622.40) | Grad Norm 8.9698(6.2814) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 59.2470(59.0939) | Bit/dim 3.8499(3.8593) | Xent 1.3015(1.3279) | Loss 4.5006(4.5233) | Error 0.4597(0.4695) Steps 604(621.85) | Grad Norm 4.9214(6.2406) | Total Time 14.00(14.00)\n",
      "Iter 1011 | Time 56.3916(59.0129) | Bit/dim 3.8449(3.8589) | Xent 1.2951(1.3269) | Loss 4.4924(4.5223) | Error 0.4615(0.4692) Steps 610(621.49) | Grad Norm 6.1632(6.2383) | Total Time 14.00(14.00)\n",
      "Iter 1012 | Time 58.2909(58.9912) | Bit/dim 3.8429(3.8584) | Xent 1.3313(1.3271) | Loss 4.5085(4.5219) | Error 0.4702(0.4693) Steps 604(620.97) | Grad Norm 7.9538(6.2898) | Total Time 14.00(14.00)\n",
      "Iter 1013 | Time 57.0062(58.9316) | Bit/dim 3.8589(3.8584) | Xent 1.3224(1.3269) | Loss 4.5201(4.5219) | Error 0.4676(0.4692) Steps 604(620.46) | Grad Norm 9.2463(6.3785) | Total Time 14.00(14.00)\n",
      "Iter 1014 | Time 56.1914(58.8494) | Bit/dim 3.8458(3.8580) | Xent 1.3004(1.3261) | Loss 4.4960(4.5211) | Error 0.4555(0.4688) Steps 598(619.79) | Grad Norm 5.4366(6.3502) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 22.1585, Epoch Time 387.3920(400.9807), Bit/dim 3.8443(best: 3.8527), Xent 1.2631, Loss 4.4758, Error 0.4494(best: 0.4456)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1015 | Time 57.4187(58.8065) | Bit/dim 3.8499(3.8578) | Xent 1.2966(1.3253) | Loss 4.4982(4.5204) | Error 0.4601(0.4686) Steps 604(619.31) | Grad Norm 4.6858(6.3003) | Total Time 14.00(14.00)\n",
      "Iter 1016 | Time 56.9360(58.7504) | Bit/dim 3.8511(3.8576) | Xent 1.3160(1.3250) | Loss 4.5091(4.5201) | Error 0.4634(0.4684) Steps 604(618.85) | Grad Norm 8.1208(6.3549) | Total Time 14.00(14.00)\n",
      "Iter 1017 | Time 56.1586(58.6726) | Bit/dim 3.8468(3.8573) | Xent 1.2835(1.3237) | Loss 4.4886(4.5191) | Error 0.4465(0.4677) Steps 610(618.59) | Grad Norm 4.9219(6.3119) | Total Time 14.00(14.00)\n",
      "Iter 1018 | Time 56.3902(58.6042) | Bit/dim 3.8452(3.8569) | Xent 1.2815(1.3225) | Loss 4.4859(4.5181) | Error 0.4530(0.4673) Steps 604(618.15) | Grad Norm 1.9869(6.1822) | Total Time 14.00(14.00)\n",
      "Iter 1019 | Time 57.9305(58.5840) | Bit/dim 3.8475(3.8566) | Xent 1.2756(1.3211) | Loss 4.4852(4.5172) | Error 0.4489(0.4667) Steps 604(617.73) | Grad Norm 4.4169(6.1292) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 58.6475(58.5859) | Bit/dim 3.8464(3.8563) | Xent 1.2862(1.3200) | Loss 4.4895(4.5163) | Error 0.4530(0.4663) Steps 610(617.49) | Grad Norm 2.3954(6.0172) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 22.9378, Epoch Time 383.4259(400.4541), Bit/dim 3.8501(best: 3.8443), Xent 1.2378, Loss 4.4690, Error 0.4439(best: 0.4456)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1021 | Time 59.4783(58.6126) | Bit/dim 3.8525(3.8562) | Xent 1.2570(1.3181) | Loss 4.4810(4.5153) | Error 0.4465(0.4657) Steps 604(617.09) | Grad Norm 1.9752(5.8959) | Total Time 14.00(14.00)\n",
      "Iter 1022 | Time 57.8333(58.5893) | Bit/dim 3.8531(3.8561) | Xent 1.2623(1.3164) | Loss 4.4842(4.5143) | Error 0.4480(0.4652) Steps 610(616.88) | Grad Norm 2.8444(5.8044) | Total Time 14.00(14.00)\n",
      "Iter 1023 | Time 58.8630(58.5975) | Bit/dim 3.8475(3.8558) | Xent 1.2674(1.3150) | Loss 4.4812(4.5133) | Error 0.4409(0.4645) Steps 604(616.49) | Grad Norm 3.6165(5.7388) | Total Time 14.00(14.00)\n",
      "Iter 1024 | Time 58.3814(58.5910) | Bit/dim 3.8453(3.8555) | Xent 1.2797(1.3139) | Loss 4.4852(4.5125) | Error 0.4534(0.4641) Steps 598(615.94) | Grad Norm 2.6319(5.6455) | Total Time 14.00(14.00)\n",
      "Iter 1025 | Time 58.8471(58.5987) | Bit/dim 3.8450(3.8552) | Xent 1.2644(1.3124) | Loss 4.4772(4.5114) | Error 0.4420(0.4635) Steps 616(615.94) | Grad Norm 2.6973(5.5571) | Total Time 14.00(14.00)\n",
      "Iter 1026 | Time 57.0746(58.5529) | Bit/dim 3.8411(3.8548) | Xent 1.2747(1.3113) | Loss 4.4785(4.5104) | Error 0.4530(0.4632) Steps 616(615.94) | Grad Norm 2.0500(5.4519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0171 | Time 23.1138, Epoch Time 390.1220(400.1441), Bit/dim 3.8485(best: 3.8443), Xent 1.2378, Loss 4.4674, Error 0.4422(best: 0.4439)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1027 | Time 57.9297(58.5342) | Bit/dim 3.8448(3.8545) | Xent 1.2629(1.3098) | Loss 4.4762(4.5094) | Error 0.4471(0.4627) Steps 598(615.40) | Grad Norm 4.2559(5.4160) | Total Time 14.00(14.00)\n",
      "Iter 1028 | Time 60.0651(58.5802) | Bit/dim 3.8520(3.8544) | Xent 1.2928(1.3093) | Loss 4.4984(4.5091) | Error 0.4533(0.4624) Steps 610(615.24) | Grad Norm 4.2281(5.3804) | Total Time 14.00(14.00)\n",
      "Iter 1029 | Time 56.5017(58.5178) | Bit/dim 3.8444(3.8541) | Xent 1.2569(1.3078) | Loss 4.4728(4.5080) | Error 0.4467(0.4619) Steps 592(614.54) | Grad Norm 1.7329(5.2709) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 57.5784(58.4896) | Bit/dim 3.8434(3.8538) | Xent 1.2830(1.3070) | Loss 4.4849(4.5073) | Error 0.4557(0.4617) Steps 604(614.23) | Grad Norm 5.9926(5.2926) | Total Time 14.00(14.00)\n",
      "Iter 1031 | Time 57.6472(58.4644) | Bit/dim 3.8367(3.8533) | Xent 1.2849(1.3064) | Loss 4.4791(4.5065) | Error 0.4540(0.4615) Steps 610(614.10) | Grad Norm 6.0018(5.3139) | Total Time 14.00(14.00)\n",
      "Iter 1032 | Time 57.1947(58.4263) | Bit/dim 3.8414(3.8529) | Xent 1.2659(1.3051) | Loss 4.4743(4.5055) | Error 0.4441(0.4610) Steps 604(613.80) | Grad Norm 6.0534(5.3361) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0172 | Time 23.4776, Epoch Time 387.4252(399.7625), Bit/dim 3.8463(best: 3.8443), Xent 1.2543, Loss 4.4734, Error 0.4430(best: 0.4422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1033 | Time 58.0027(58.4136) | Bit/dim 3.8440(3.8527) | Xent 1.2887(1.3047) | Loss 4.4884(4.5050) | Error 0.4470(0.4606) Steps 628(614.22) | Grad Norm 7.6236(5.4047) | Total Time 14.00(14.00)\n",
      "Iter 1034 | Time 59.8935(58.4580) | Bit/dim 3.8384(3.8522) | Xent 1.2733(1.3037) | Loss 4.4751(4.5041) | Error 0.4506(0.4603) Steps 610(614.10) | Grad Norm 4.6010(5.3806) | Total Time 14.00(14.00)\n",
      "Iter 1035 | Time 58.7624(58.4671) | Bit/dim 3.8522(3.8522) | Xent 1.2712(1.3027) | Loss 4.4877(4.5036) | Error 0.4501(0.4600) Steps 592(613.43) | Grad Norm 2.4484(5.2926) | Total Time 14.00(14.00)\n",
      "Iter 1036 | Time 61.2325(58.5501) | Bit/dim 3.8346(3.8517) | Xent 1.2428(1.3009) | Loss 4.4560(4.5022) | Error 0.4400(0.4594) Steps 610(613.33) | Grad Norm 4.6126(5.2722) | Total Time 14.00(14.00)\n",
      "Iter 1037 | Time 58.8867(58.5602) | Bit/dim 3.8480(3.8516) | Xent 1.2463(1.2993) | Loss 4.4711(4.5012) | Error 0.4400(0.4588) Steps 604(613.05) | Grad Norm 2.6731(5.1942) | Total Time 14.00(14.00)\n",
      "Iter 1038 | Time 58.6291(58.5622) | Bit/dim 3.8402(3.8512) | Xent 1.2572(1.2980) | Loss 4.4689(4.5003) | Error 0.4395(0.4582) Steps 604(612.78) | Grad Norm 2.1597(5.1032) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0173 | Time 22.7915, Epoch Time 395.5839(399.6372), Bit/dim 3.8458(best: 3.8443), Xent 1.2313, Loss 4.4615, Error 0.4396(best: 0.4422)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1039 | Time 56.9991(58.5153) | Bit/dim 3.8475(3.8511) | Xent 1.2495(1.2966) | Loss 4.4723(4.4994) | Error 0.4376(0.4576) Steps 604(612.52) | Grad Norm 3.9283(5.0680) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 57.5599(58.4867) | Bit/dim 3.8449(3.8510) | Xent 1.2384(1.2948) | Loss 4.4641(4.4984) | Error 0.4343(0.4569) Steps 604(612.26) | Grad Norm 2.3716(4.9871) | Total Time 14.00(14.00)\n",
      "Iter 1041 | Time 55.1303(58.3860) | Bit/dim 3.8514(3.8510) | Xent 1.2616(1.2938) | Loss 4.4822(4.4979) | Error 0.4477(0.4566) Steps 598(611.83) | Grad Norm 2.8921(4.9242) | Total Time 14.00(14.00)\n",
      "Iter 1042 | Time 57.2873(58.3530) | Bit/dim 3.8273(3.8503) | Xent 1.2757(1.2933) | Loss 4.4651(4.4969) | Error 0.4506(0.4564) Steps 616(611.96) | Grad Norm 5.8875(4.9531) | Total Time 14.00(14.00)\n",
      "Iter 1043 | Time 60.0406(58.4036) | Bit/dim 3.8487(3.8502) | Xent 1.2817(1.2929) | Loss 4.4895(4.4967) | Error 0.4497(0.4562) Steps 604(611.72) | Grad Norm 8.6472(5.0639) | Total Time 14.00(14.00)\n",
      "Iter 1044 | Time 56.9559(58.3602) | Bit/dim 3.8352(3.8498) | Xent 1.3319(1.2941) | Loss 4.5011(4.4968) | Error 0.4749(0.4568) Steps 610(611.67) | Grad Norm 14.6510(5.3515) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0174 | Time 23.7248, Epoch Time 385.1932(399.2039), Bit/dim 3.8500(best: 3.8443), Xent 1.4009, Loss 4.5504, Error 0.4955(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1045 | Time 58.2601(58.3572) | Bit/dim 3.8449(3.8496) | Xent 1.4252(1.2980) | Loss 4.5575(4.4986) | Error 0.4935(0.4579) Steps 610(611.62) | Grad Norm 17.6837(5.7215) | Total Time 14.00(14.00)\n",
      "Iter 1046 | Time 57.0258(58.3173) | Bit/dim 3.8320(3.8491) | Xent 1.2846(1.2976) | Loss 4.4743(4.4979) | Error 0.4613(0.4580) Steps 592(611.03) | Grad Norm 7.3608(5.7707) | Total Time 14.00(14.00)\n",
      "Iter 1047 | Time 57.8471(58.3032) | Bit/dim 3.8365(3.8487) | Xent 1.3534(1.2993) | Loss 4.5131(4.4984) | Error 0.4806(0.4587) Steps 598(610.64) | Grad Norm 11.4040(5.9397) | Total Time 14.00(14.00)\n",
      "Iter 1048 | Time 59.0499(58.3256) | Bit/dim 3.8548(3.8489) | Xent 1.4087(1.3026) | Loss 4.5591(4.5002) | Error 0.4958(0.4598) Steps 592(610.08) | Grad Norm 13.3742(6.1627) | Total Time 14.00(14.00)\n",
      "Iter 1049 | Time 56.2396(58.2630) | Bit/dim 3.8396(3.8486) | Xent 1.3884(1.3052) | Loss 4.5338(4.5012) | Error 0.5014(0.4610) Steps 622(610.44) | Grad Norm 10.3203(6.2875) | Total Time 14.00(14.00)\n",
      "Iter 1050 | Time 57.9884(58.2548) | Bit/dim 3.8468(3.8485) | Xent 1.3215(1.3057) | Loss 4.5076(4.5014) | Error 0.4684(0.4613) Steps 592(609.88) | Grad Norm 5.2668(6.2568) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0175 | Time 22.4901, Epoch Time 387.0982(398.8407), Bit/dim 3.8492(best: 3.8443), Xent 1.3337, Loss 4.5161, Error 0.4859(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1051 | Time 56.8806(58.2135) | Bit/dim 3.8569(3.8488) | Xent 1.3690(1.3076) | Loss 4.5414(4.5026) | Error 0.4866(0.4620) Steps 592(609.35) | Grad Norm 7.1938(6.2849) | Total Time 14.00(14.00)\n",
      "Iter 1052 | Time 56.5846(58.1647) | Bit/dim 3.8438(3.8487) | Xent 1.3367(1.3084) | Loss 4.5122(4.5029) | Error 0.4742(0.4624) Steps 592(608.83) | Grad Norm 4.3767(6.2277) | Total Time 14.00(14.00)\n",
      "Iter 1053 | Time 59.7973(58.2136) | Bit/dim 3.8461(3.8486) | Xent 1.3251(1.3089) | Loss 4.5087(4.5030) | Error 0.4686(0.4626) Steps 598(608.50) | Grad Norm 4.5413(6.1771) | Total Time 14.00(14.00)\n",
      "Iter 1054 | Time 57.3077(58.1865) | Bit/dim 3.8334(3.8481) | Xent 1.3198(1.3093) | Loss 4.4933(4.5028) | Error 0.4721(0.4629) Steps 586(607.83) | Grad Norm 5.3476(6.1522) | Total Time 14.00(14.00)\n",
      "Iter 1055 | Time 57.2160(58.1573) | Bit/dim 3.8477(3.8481) | Xent 1.2764(1.3083) | Loss 4.4859(4.5022) | Error 0.4509(0.4625) Steps 598(607.53) | Grad Norm 3.3100(6.0670) | Total Time 14.00(14.00)\n",
      "Iter 1056 | Time 56.4301(58.1055) | Bit/dim 3.8354(3.8477) | Xent 1.3034(1.3081) | Loss 4.4872(4.5018) | Error 0.4634(0.4625) Steps 586(606.89) | Grad Norm 7.3513(6.1055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0176 | Time 22.4269, Epoch Time 383.3624(398.3763), Bit/dim 3.8437(best: 3.8443), Xent 1.2505, Loss 4.4689, Error 0.4450(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1057 | Time 58.8437(58.1277) | Bit/dim 3.8393(3.8475) | Xent 1.2735(1.3071) | Loss 4.4760(4.5010) | Error 0.4549(0.4623) Steps 586(606.26) | Grad Norm 3.3848(6.0239) | Total Time 14.00(14.00)\n",
      "Iter 1058 | Time 56.8819(58.0903) | Bit/dim 3.8436(3.8474) | Xent 1.2768(1.3062) | Loss 4.4820(4.5005) | Error 0.4530(0.4620) Steps 592(605.83) | Grad Norm 3.9018(5.9602) | Total Time 14.00(14.00)\n",
      "Iter 1059 | Time 55.3358(58.0077) | Bit/dim 3.8492(3.8474) | Xent 1.2941(1.3058) | Loss 4.4963(4.5003) | Error 0.4619(0.4620) Steps 592(605.42) | Grad Norm 4.7075(5.9226) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 58.7631(58.0303) | Bit/dim 3.8317(3.8469) | Xent 1.2599(1.3044) | Loss 4.4617(4.4992) | Error 0.4395(0.4613) Steps 592(605.01) | Grad Norm 4.0085(5.8652) | Total Time 14.00(14.00)\n",
      "Iter 1061 | Time 56.0273(57.9702) | Bit/dim 3.8482(3.8470) | Xent 1.2814(1.3038) | Loss 4.4889(4.4989) | Error 0.4544(0.4611) Steps 592(604.62) | Grad Norm 4.0091(5.8095) | Total Time 14.00(14.00)\n",
      "Iter 1062 | Time 58.1984(57.9771) | Bit/dim 3.8443(3.8469) | Xent 1.2898(1.3033) | Loss 4.4892(4.4986) | Error 0.4619(0.4612) Steps 598(604.42) | Grad Norm 4.5021(5.7703) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0177 | Time 23.1129, Epoch Time 384.7301(397.9669), Bit/dim 3.8439(best: 3.8437), Xent 1.2488, Loss 4.4683, Error 0.4462(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1063 | Time 60.9686(58.0668) | Bit/dim 3.8368(3.8466) | Xent 1.2921(1.3030) | Loss 4.4828(4.4981) | Error 0.4521(0.4609) Steps 604(604.41) | Grad Norm 4.3670(5.7282) | Total Time 14.00(14.00)\n",
      "Iter 1064 | Time 58.5893(58.0825) | Bit/dim 3.8487(3.8467) | Xent 1.2833(1.3024) | Loss 4.4903(4.4979) | Error 0.4555(0.4607) Steps 598(604.22) | Grad Norm 6.2710(5.7445) | Total Time 14.00(14.00)\n",
      "Iter 1065 | Time 57.7436(58.0723) | Bit/dim 3.8348(3.8463) | Xent 1.2714(1.3015) | Loss 4.4705(4.4970) | Error 0.4445(0.4602) Steps 604(604.21) | Grad Norm 3.8320(5.6871) | Total Time 14.00(14.00)\n",
      "Iter 1066 | Time 57.9892(58.0698) | Bit/dim 3.8478(3.8463) | Xent 1.2789(1.3008) | Loss 4.4873(4.4968) | Error 0.4509(0.4600) Steps 610(604.39) | Grad Norm 8.8845(5.7830) | Total Time 14.00(14.00)\n",
      "Iter 1067 | Time 58.5104(58.0831) | Bit/dim 3.8364(3.8461) | Xent 1.2669(1.2998) | Loss 4.4699(4.4959) | Error 0.4440(0.4595) Steps 610(604.55) | Grad Norm 1.8685(5.6656) | Total Time 14.00(14.00)\n",
      "Iter 1068 | Time 57.2470(58.0580) | Bit/dim 3.8412(3.8459) | Xent 1.2952(1.2996) | Loss 4.4888(4.4957) | Error 0.4614(0.4595) Steps 622(605.08) | Grad Norm 6.5743(5.6929) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0178 | Time 23.2383, Epoch Time 391.1787(397.7633), Bit/dim 3.8381(best: 3.8437), Xent 1.2211, Loss 4.4487, Error 0.4339(best: 0.4396)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1069 | Time 57.7270(58.0480) | Bit/dim 3.8300(3.8454) | Xent 1.2531(1.2983) | Loss 4.4565(4.4946) | Error 0.4426(0.4590) Steps 628(605.77) | Grad Norm 1.2771(5.5604) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 59.1848(58.0821) | Bit/dim 3.8427(3.8453) | Xent 1.2573(1.2970) | Loss 4.4714(4.4939) | Error 0.4447(0.4586) Steps 604(605.71) | Grad Norm 5.9986(5.5735) | Total Time 14.00(14.00)\n",
      "Iter 1071 | Time 60.0626(58.1416) | Bit/dim 3.8409(3.8452) | Xent 1.2486(1.2956) | Loss 4.4652(4.4930) | Error 0.4470(0.4582) Steps 622(606.20) | Grad Norm 3.7959(5.5202) | Total Time 14.00(14.00)\n",
      "Iter 1072 | Time 59.4641(58.1812) | Bit/dim 3.8346(3.8449) | Xent 1.2597(1.2945) | Loss 4.4644(4.4921) | Error 0.4465(0.4579) Steps 610(606.32) | Grad Norm 3.1262(5.4484) | Total Time 14.00(14.00)\n",
      "Iter 1073 | Time 58.6462(58.1952) | Bit/dim 3.8425(3.8448) | Xent 1.2677(1.2937) | Loss 4.4763(4.4917) | Error 0.4504(0.4577) Steps 604(606.25) | Grad Norm 4.6864(5.4255) | Total Time 14.00(14.00)\n",
      "Iter 1074 | Time 58.6389(58.2085) | Bit/dim 3.8405(3.8447) | Xent 1.2516(1.2924) | Loss 4.4663(4.4909) | Error 0.4396(0.4571) Steps 610(606.36) | Grad Norm 2.5991(5.3407) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0179 | Time 22.8941, Epoch Time 393.6836(397.6409), Bit/dim 3.8385(best: 3.8381), Xent 1.2280, Loss 4.4525, Error 0.4392(best: 0.4339)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1075 | Time 57.8024(58.1963) | Bit/dim 3.8253(3.8441) | Xent 1.2583(1.2914) | Loss 4.4545(4.4898) | Error 0.4459(0.4568) Steps 610(606.47) | Grad Norm 3.1046(5.2736) | Total Time 14.00(14.00)\n",
      "Iter 1076 | Time 59.8098(58.2447) | Bit/dim 3.8383(3.8439) | Xent 1.2494(1.2901) | Loss 4.4630(4.4890) | Error 0.4350(0.4561) Steps 604(606.39) | Grad Norm 5.7951(5.2893) | Total Time 14.00(14.00)\n",
      "Iter 1077 | Time 58.8272(58.2622) | Bit/dim 3.8369(3.8437) | Xent 1.2482(1.2889) | Loss 4.4610(4.4882) | Error 0.4413(0.4557) Steps 604(606.32) | Grad Norm 5.6291(5.2995) | Total Time 14.00(14.00)\n",
      "Iter 1078 | Time 59.5018(58.2994) | Bit/dim 3.8335(3.8434) | Xent 1.2433(1.2875) | Loss 4.4551(4.4872) | Error 0.4327(0.4550) Steps 598(606.07) | Grad Norm 1.6196(5.1891) | Total Time 14.00(14.00)\n",
      "Iter 1079 | Time 59.0833(58.3229) | Bit/dim 3.8388(3.8433) | Xent 1.2488(1.2864) | Loss 4.4632(4.4865) | Error 0.4466(0.4548) Steps 592(605.65) | Grad Norm 2.5458(5.1098) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 58.8440(58.3385) | Bit/dim 3.8374(3.8431) | Xent 1.2640(1.2857) | Loss 4.4693(4.4859) | Error 0.4416(0.4544) Steps 598(605.42) | Grad Norm 2.7223(5.0382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0180 | Time 22.2775, Epoch Time 393.6532(397.5213), Bit/dim 3.8350(best: 3.8381), Xent 1.2116, Loss 4.4408, Error 0.4332(best: 0.4339)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1081 | Time 57.5911(58.3161) | Bit/dim 3.8336(3.8428) | Xent 1.2331(1.2841) | Loss 4.4501(4.4849) | Error 0.4333(0.4537) Steps 580(604.66) | Grad Norm 1.5824(4.9345) | Total Time 14.00(14.00)\n",
      "Iter 1082 | Time 60.2553(58.3743) | Bit/dim 3.8365(3.8426) | Xent 1.2529(1.2832) | Loss 4.4629(4.4842) | Error 0.4451(0.4535) Steps 580(603.92) | Grad Norm 3.4710(4.8906) | Total Time 14.00(14.00)\n",
      "Iter 1083 | Time 58.5431(58.3793) | Bit/dim 3.8362(3.8424) | Xent 1.2192(1.2812) | Loss 4.4458(4.4831) | Error 0.4327(0.4528) Steps 616(604.28) | Grad Norm 1.5051(4.7890) | Total Time 14.00(14.00)\n",
      "Iter 1084 | Time 57.8553(58.3636) | Bit/dim 3.8350(3.8422) | Xent 1.2456(1.2802) | Loss 4.4578(4.4823) | Error 0.4313(0.4522) Steps 604(604.27) | Grad Norm 3.1000(4.7383) | Total Time 14.00(14.00)\n",
      "Iter 1085 | Time 61.3119(58.4521) | Bit/dim 3.8280(3.8418) | Xent 1.2569(1.2795) | Loss 4.4565(4.4815) | Error 0.4530(0.4522) Steps 598(604.08) | Grad Norm 3.0631(4.6881) | Total Time 14.00(14.00)\n",
      "Iter 1086 | Time 60.0449(58.4999) | Bit/dim 3.8324(3.8415) | Xent 1.2547(1.2787) | Loss 4.4597(4.4809) | Error 0.4367(0.4518) Steps 610(604.26) | Grad Norm 3.1613(4.6423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0181 | Time 22.8073, Epoch Time 395.3507(397.4562), Bit/dim 3.8332(best: 3.8350), Xent 1.2065, Loss 4.4365, Error 0.4317(best: 0.4332)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1087 | Time 56.3962(58.4367) | Bit/dim 3.8386(3.8414) | Xent 1.2422(1.2776) | Loss 4.4597(4.4802) | Error 0.4373(0.4513) Steps 592(603.89) | Grad Norm 1.8207(4.5576) | Total Time 14.00(14.00)\n",
      "Iter 1088 | Time 58.7502(58.4461) | Bit/dim 3.8228(3.8409) | Xent 1.2186(1.2759) | Loss 4.4321(4.4788) | Error 0.4300(0.4507) Steps 586(603.36) | Grad Norm 1.8381(4.4761) | Total Time 14.00(14.00)\n",
      "Iter 1089 | Time 57.1959(58.4086) | Bit/dim 3.8243(3.8404) | Xent 1.2224(1.2743) | Loss 4.4355(4.4775) | Error 0.4336(0.4502) Steps 592(603.02) | Grad Norm 2.3154(4.4112) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 59.8195(58.4510) | Bit/dim 3.8373(3.8403) | Xent 1.2389(1.2732) | Loss 4.4568(4.4769) | Error 0.4343(0.4497) Steps 580(602.33) | Grad Norm 1.4408(4.3221) | Total Time 14.00(14.00)\n",
      "Iter 1091 | Time 58.0453(58.4388) | Bit/dim 3.8339(3.8401) | Xent 1.2456(1.2724) | Loss 4.4567(4.4763) | Error 0.4416(0.4495) Steps 598(602.20) | Grad Norm 4.0517(4.3140) | Total Time 14.00(14.00)\n",
      "Iter 1092 | Time 59.7712(58.4788) | Bit/dim 3.8354(3.8399) | Xent 1.2447(1.2716) | Loss 4.4578(4.4757) | Error 0.4327(0.4489) Steps 598(602.07) | Grad Norm 5.3809(4.3460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0182 | Time 22.9786, Epoch Time 390.4290(397.2453), Bit/dim 3.8351(best: 3.8332), Xent 1.2333, Loss 4.4517, Error 0.4406(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1093 | Time 57.0551(58.4361) | Bit/dim 3.8378(3.8399) | Xent 1.2834(1.2719) | Loss 4.4795(4.4758) | Error 0.4586(0.4492) Steps 592(601.77) | Grad Norm 8.8162(4.4801) | Total Time 14.00(14.00)\n",
      "Iter 1094 | Time 61.0841(58.5155) | Bit/dim 3.8330(3.8397) | Xent 1.2948(1.2726) | Loss 4.4803(4.4760) | Error 0.4527(0.4493) Steps 610(602.01) | Grad Norm 13.5066(4.7509) | Total Time 14.00(14.00)\n",
      "Iter 1095 | Time 57.5430(58.4863) | Bit/dim 3.8282(3.8393) | Xent 1.3339(1.2744) | Loss 4.4952(4.4765) | Error 0.4646(0.4498) Steps 574(601.17) | Grad Norm 15.6429(5.0777) | Total Time 14.00(14.00)\n",
      "Iter 1096 | Time 58.3380(58.4819) | Bit/dim 3.8265(3.8389) | Xent 1.3020(1.2753) | Loss 4.4775(4.4766) | Error 0.4651(0.4503) Steps 598(601.08) | Grad Norm 9.3486(5.2058) | Total Time 14.00(14.00)\n",
      "Iter 1097 | Time 56.5959(58.4253) | Bit/dim 3.8290(3.8386) | Xent 1.2298(1.2739) | Loss 4.4439(4.4756) | Error 0.4316(0.4497) Steps 574(600.27) | Grad Norm 2.2899(5.1183) | Total Time 14.00(14.00)\n",
      "Iter 1098 | Time 58.2929(58.4213) | Bit/dim 3.8387(3.8386) | Xent 1.3177(1.2752) | Loss 4.4975(4.4762) | Error 0.4666(0.4502) Steps 580(599.66) | Grad Norm 10.7616(5.2876) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0183 | Time 23.1395, Epoch Time 389.5789(397.0153), Bit/dim 3.8318(best: 3.8332), Xent 1.2743, Loss 4.4690, Error 0.4554(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1099 | Time 57.7475(58.4011) | Bit/dim 3.8308(3.8384) | Xent 1.3002(1.2760) | Loss 4.4809(4.4764) | Error 0.4657(0.4507) Steps 592(599.43) | Grad Norm 8.6956(5.3899) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 58.0732(58.3913) | Bit/dim 3.8420(3.8385) | Xent 1.2836(1.2762) | Loss 4.4838(4.4766) | Error 0.4519(0.4507) Steps 574(598.67) | Grad Norm 6.4871(5.4228) | Total Time 14.00(14.00)\n",
      "Iter 1101 | Time 57.1629(58.3544) | Bit/dim 3.8314(3.8383) | Xent 1.2996(1.2769) | Loss 4.4812(4.4767) | Error 0.4640(0.4511) Steps 586(598.29) | Grad Norm 10.3770(5.5714) | Total Time 14.00(14.00)\n",
      "Iter 1102 | Time 58.0358(58.3449) | Bit/dim 3.8328(3.8381) | Xent 1.2464(1.2760) | Loss 4.4559(4.4761) | Error 0.4377(0.4507) Steps 592(598.10) | Grad Norm 3.5166(5.5098) | Total Time 14.00(14.00)\n",
      "Iter 1103 | Time 58.3292(58.3444) | Bit/dim 3.8302(3.8379) | Xent 1.2824(1.2762) | Loss 4.4714(4.4760) | Error 0.4563(0.4509) Steps 580(597.55) | Grad Norm 7.0724(5.5566) | Total Time 14.00(14.00)\n",
      "Iter 1104 | Time 59.3365(58.3742) | Bit/dim 3.8245(3.8375) | Xent 1.2796(1.2763) | Loss 4.4643(4.4756) | Error 0.4516(0.4509) Steps 568(596.67) | Grad Norm 6.4814(5.5844) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0184 | Time 22.6411, Epoch Time 388.1088(396.7482), Bit/dim 3.8305(best: 3.8318), Xent 1.2284, Loss 4.4447, Error 0.4427(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1105 | Time 56.6327(58.3219) | Bit/dim 3.8278(3.8372) | Xent 1.2590(1.2758) | Loss 4.4573(4.4751) | Error 0.4446(0.4507) Steps 580(596.17) | Grad Norm 5.2368(5.5740) | Total Time 14.00(14.00)\n",
      "Iter 1106 | Time 55.7233(58.2440) | Bit/dim 3.8265(3.8369) | Xent 1.2459(1.2749) | Loss 4.4494(4.4743) | Error 0.4374(0.4503) Steps 574(595.50) | Grad Norm 6.3644(5.5977) | Total Time 14.00(14.00)\n",
      "Iter 1107 | Time 56.9960(58.2065) | Bit/dim 3.8371(3.8369) | Xent 1.2311(1.2735) | Loss 4.4526(4.4737) | Error 0.4341(0.4498) Steps 586(595.22) | Grad Norm 2.8226(5.5144) | Total Time 14.00(14.00)\n",
      "Iter 1108 | Time 57.9952(58.2002) | Bit/dim 3.8342(3.8368) | Xent 1.2524(1.2729) | Loss 4.4604(4.4733) | Error 0.4441(0.4497) Steps 586(594.94) | Grad Norm 6.2493(5.5365) | Total Time 14.00(14.00)\n",
      "Iter 1109 | Time 55.4962(58.1191) | Bit/dim 3.8219(3.8364) | Xent 1.2291(1.2716) | Loss 4.4365(4.4722) | Error 0.4381(0.4493) Steps 592(594.85) | Grad Norm 2.7629(5.4533) | Total Time 14.00(14.00)\n",
      "Iter 1110 | Time 56.7468(58.0779) | Bit/dim 3.8379(3.8364) | Xent 1.2389(1.2706) | Loss 4.4574(4.4717) | Error 0.4419(0.4491) Steps 592(594.77) | Grad Norm 6.5845(5.4872) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0185 | Time 22.9629, Epoch Time 379.4943(396.2305), Bit/dim 3.8295(best: 3.8305), Xent 1.2173, Loss 4.4381, Error 0.4353(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1111 | Time 57.4624(58.0594) | Bit/dim 3.8361(3.8364) | Xent 1.2581(1.2702) | Loss 4.4652(4.4715) | Error 0.4429(0.4489) Steps 598(594.86) | Grad Norm 3.9909(5.4423) | Total Time 14.00(14.00)\n",
      "Iter 1112 | Time 58.8846(58.0842) | Bit/dim 3.8259(3.8361) | Xent 1.2532(1.2697) | Loss 4.4525(4.4709) | Error 0.4429(0.4487) Steps 580(594.42) | Grad Norm 1.8894(5.3357) | Total Time 14.00(14.00)\n",
      "Iter 1113 | Time 62.4756(58.2159) | Bit/dim 3.8233(3.8357) | Xent 1.2300(1.2685) | Loss 4.4383(4.4700) | Error 0.4383(0.4484) Steps 574(593.81) | Grad Norm 5.5178(5.3412) | Total Time 14.00(14.00)\n",
      "Iter 1114 | Time 57.8685(58.2055) | Bit/dim 3.8306(3.8355) | Xent 1.2297(1.2674) | Loss 4.4455(4.4692) | Error 0.4370(0.4481) Steps 586(593.57) | Grad Norm 6.0150(5.3614) | Total Time 14.00(14.00)\n",
      "Iter 1115 | Time 58.7249(58.2211) | Bit/dim 3.8251(3.8352) | Xent 1.2634(1.2673) | Loss 4.4568(4.4689) | Error 0.4457(0.4480) Steps 598(593.70) | Grad Norm 5.6506(5.3701) | Total Time 14.00(14.00)\n",
      "Iter 1116 | Time 58.8893(58.2411) | Bit/dim 3.8288(3.8350) | Xent 1.2364(1.2663) | Loss 4.4470(4.4682) | Error 0.4331(0.4475) Steps 592(593.65) | Grad Norm 5.3650(5.3699) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0186 | Time 22.9022, Epoch Time 394.2137(396.1700), Bit/dim 3.8302(best: 3.8295), Xent 1.2136, Loss 4.4370, Error 0.4327(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1117 | Time 54.9466(58.1423) | Bit/dim 3.8164(3.8345) | Xent 1.2209(1.2650) | Loss 4.4268(4.4670) | Error 0.4320(0.4471) Steps 580(593.24) | Grad Norm 5.2664(5.3668) | Total Time 14.00(14.00)\n",
      "Iter 1118 | Time 58.4471(58.1514) | Bit/dim 3.8301(3.8343) | Xent 1.2291(1.2639) | Loss 4.4446(4.4663) | Error 0.4403(0.4469) Steps 574(592.67) | Grad Norm 5.3969(5.3677) | Total Time 14.00(14.00)\n",
      "Iter 1119 | Time 57.6927(58.1377) | Bit/dim 3.8256(3.8341) | Xent 1.2452(1.2633) | Loss 4.4482(4.4658) | Error 0.4356(0.4465) Steps 604(593.01) | Grad Norm 6.9505(5.4152) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 57.5411(58.1198) | Bit/dim 3.8402(3.8343) | Xent 1.2474(1.2629) | Loss 4.4640(4.4657) | Error 0.4420(0.4464) Steps 586(592.80) | Grad Norm 7.7517(5.4853) | Total Time 14.00(14.00)\n",
      "Iter 1121 | Time 58.4244(58.1289) | Bit/dim 3.8363(3.8343) | Xent 1.2622(1.2628) | Loss 4.4674(4.4657) | Error 0.4445(0.4463) Steps 592(592.77) | Grad Norm 7.2437(5.5380) | Total Time 14.00(14.00)\n",
      "Iter 1122 | Time 57.5126(58.1104) | Bit/dim 3.8262(3.8341) | Xent 1.2156(1.2614) | Loss 4.4340(4.4648) | Error 0.4280(0.4458) Steps 592(592.75) | Grad Norm 2.9502(5.4604) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0187 | Time 23.0308, Epoch Time 384.0003(395.8049), Bit/dim 3.8265(best: 3.8295), Xent 1.2056, Loss 4.4293, Error 0.4294(best: 0.4317)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1123 | Time 55.7656(58.0401) | Bit/dim 3.8231(3.8338) | Xent 1.2410(1.2608) | Loss 4.4436(4.4642) | Error 0.4395(0.4456) Steps 580(592.37) | Grad Norm 4.7719(5.4398) | Total Time 14.00(14.00)\n",
      "Iter 1124 | Time 56.7138(58.0003) | Bit/dim 3.8342(3.8338) | Xent 1.2497(1.2605) | Loss 4.4591(4.4640) | Error 0.4394(0.4454) Steps 604(592.72) | Grad Norm 7.7599(5.5094) | Total Time 14.00(14.00)\n",
      "Iter 1125 | Time 55.8563(57.9360) | Bit/dim 3.8311(3.8337) | Xent 1.2149(1.2591) | Loss 4.4386(4.4632) | Error 0.4310(0.4450) Steps 586(592.51) | Grad Norm 5.1039(5.4972) | Total Time 14.00(14.00)\n",
      "Iter 1126 | Time 55.6552(57.8676) | Bit/dim 3.8270(3.8335) | Xent 1.2109(1.2577) | Loss 4.4325(4.4623) | Error 0.4305(0.4446) Steps 574(591.96) | Grad Norm 3.2280(5.4291) | Total Time 14.00(14.00)\n",
      "Iter 1127 | Time 56.9430(57.8398) | Bit/dim 3.8072(3.8327) | Xent 1.2376(1.2571) | Loss 4.4260(4.4612) | Error 0.4320(0.4442) Steps 592(591.96) | Grad Norm 4.8842(5.4128) | Total Time 14.00(14.00)\n",
      "Iter 1128 | Time 56.3375(57.7947) | Bit/dim 3.8332(3.8327) | Xent 1.2329(1.2563) | Loss 4.4496(4.4609) | Error 0.4346(0.4439) Steps 574(591.42) | Grad Norm 4.9782(5.3997) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0188 | Time 23.0652, Epoch Time 377.0169(395.2413), Bit/dim 3.8260(best: 3.8265), Xent 1.1900, Loss 4.4210, Error 0.4242(best: 0.4294)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1129 | Time 56.5295(57.7568) | Bit/dim 3.8273(3.8326) | Xent 1.2286(1.2555) | Loss 4.4416(4.4603) | Error 0.4341(0.4436) Steps 574(590.90) | Grad Norm 2.4476(5.3112) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 57.3712(57.7452) | Bit/dim 3.8210(3.8322) | Xent 1.2170(1.2543) | Loss 4.4295(4.4594) | Error 0.4310(0.4432) Steps 580(590.57) | Grad Norm 1.8762(5.2081) | Total Time 14.00(14.00)\n",
      "Iter 1131 | Time 55.9333(57.6909) | Bit/dim 3.8287(3.8321) | Xent 1.2167(1.2532) | Loss 4.4370(4.4587) | Error 0.4294(0.4428) Steps 580(590.25) | Grad Norm 3.3404(5.1521) | Total Time 14.00(14.00)\n",
      "Iter 1132 | Time 58.9546(57.7288) | Bit/dim 3.8233(3.8318) | Xent 1.2140(1.2520) | Loss 4.4303(4.4579) | Error 0.4295(0.4424) Steps 598(590.49) | Grad Norm 2.8254(5.0823) | Total Time 14.00(14.00)\n",
      "Iter 1133 | Time 56.3172(57.6864) | Bit/dim 3.8301(3.8318) | Xent 1.2134(1.2509) | Loss 4.4369(4.4572) | Error 0.4330(0.4421) Steps 574(589.99) | Grad Norm 3.0238(5.0205) | Total Time 14.00(14.00)\n",
      "Iter 1134 | Time 57.6147(57.6843) | Bit/dim 3.8223(3.8315) | Xent 1.2308(1.2503) | Loss 4.4376(4.4566) | Error 0.4414(0.4421) Steps 616(590.77) | Grad Norm 5.1271(5.0237) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0189 | Time 22.4979, Epoch Time 382.9407(394.8723), Bit/dim 3.8254(best: 3.8260), Xent 1.2146, Loss 4.4327, Error 0.4399(best: 0.4242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1135 | Time 58.7203(57.7154) | Bit/dim 3.8255(3.8313) | Xent 1.2304(1.2497) | Loss 4.4407(4.4562) | Error 0.4361(0.4419) Steps 592(590.81) | Grad Norm 7.7834(5.1065) | Total Time 14.00(14.00)\n",
      "Iter 1136 | Time 58.0052(57.7241) | Bit/dim 3.8326(3.8314) | Xent 1.2626(1.2501) | Loss 4.4639(4.4564) | Error 0.4505(0.4422) Steps 598(591.03) | Grad Norm 10.8896(5.2800) | Total Time 14.00(14.00)\n",
      "Iter 1137 | Time 57.7256(57.7241) | Bit/dim 3.8257(3.8312) | Xent 1.3175(1.2521) | Loss 4.4844(4.4572) | Error 0.4699(0.4430) Steps 598(591.23) | Grad Norm 10.4036(5.4337) | Total Time 14.00(14.00)\n",
      "Iter 1138 | Time 56.9532(57.7010) | Bit/dim 3.8221(3.8309) | Xent 1.2380(1.2517) | Loss 4.4411(4.4567) | Error 0.4380(0.4429) Steps 598(591.44) | Grad Norm 3.1378(5.3648) | Total Time 14.00(14.00)\n",
      "Iter 1139 | Time 58.9174(57.7375) | Bit/dim 3.8226(3.8307) | Xent 1.2970(1.2530) | Loss 4.4712(4.4572) | Error 0.4589(0.4433) Steps 610(591.99) | Grad Norm 10.3331(5.5139) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 58.0612(57.7472) | Bit/dim 3.8275(3.8306) | Xent 1.2773(1.2538) | Loss 4.4661(4.4574) | Error 0.4489(0.4435) Steps 580(591.63) | Grad Norm 7.8677(5.5845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0190 | Time 22.3791, Epoch Time 388.7640(394.6890), Bit/dim 3.8235(best: 3.8254), Xent 1.2399, Loss 4.4435, Error 0.4462(best: 0.4242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1141 | Time 54.4687(57.6488) | Bit/dim 3.8217(3.8303) | Xent 1.2802(1.2546) | Loss 4.4619(4.4576) | Error 0.4556(0.4439) Steps 574(591.11) | Grad Norm 5.9855(5.5965) | Total Time 14.00(14.00)\n",
      "Iter 1142 | Time 58.5619(57.6762) | Bit/dim 3.8162(3.8299) | Xent 1.2559(1.2546) | Loss 4.4441(4.4572) | Error 0.4395(0.4437) Steps 574(590.59) | Grad Norm 6.7690(5.6317) | Total Time 14.00(14.00)\n",
      "Iter 1143 | Time 57.8352(57.6810) | Bit/dim 3.8291(3.8299) | Xent 1.2554(1.2546) | Loss 4.4567(4.4572) | Error 0.4494(0.4439) Steps 592(590.63) | Grad Norm 5.5563(5.6294) | Total Time 14.00(14.00)\n",
      "Iter 1144 | Time 57.7043(57.6817) | Bit/dim 3.8303(3.8299) | Xent 1.2334(1.2540) | Loss 4.4470(4.4569) | Error 0.4377(0.4437) Steps 592(590.68) | Grad Norm 7.2227(5.6772) | Total Time 14.00(14.00)\n",
      "Iter 1145 | Time 55.6640(57.6212) | Bit/dim 3.8167(3.8295) | Xent 1.2596(1.2541) | Loss 4.4466(4.4565) | Error 0.4493(0.4439) Steps 586(590.54) | Grad Norm 7.2493(5.7244) | Total Time 14.00(14.00)\n",
      "Iter 1146 | Time 57.3577(57.6132) | Bit/dim 3.8441(3.8299) | Xent 1.2591(1.2543) | Loss 4.4736(4.4571) | Error 0.4414(0.4438) Steps 592(590.58) | Grad Norm 8.5818(5.8101) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0191 | Time 22.5902, Epoch Time 380.9115(394.2757), Bit/dim 3.8281(best: 3.8235), Xent 1.2133, Loss 4.4348, Error 0.4286(best: 0.4242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1147 | Time 57.4354(57.6079) | Bit/dim 3.8233(3.8297) | Xent 1.2160(1.2531) | Loss 4.4313(4.4563) | Error 0.4261(0.4433) Steps 586(590.44) | Grad Norm 7.9370(5.8739) | Total Time 14.00(14.00)\n",
      "Iter 1148 | Time 56.6405(57.5789) | Bit/dim 3.8343(3.8299) | Xent 1.2431(1.2528) | Loss 4.4558(4.4563) | Error 0.4406(0.4432) Steps 586(590.31) | Grad Norm 6.9715(5.9069) | Total Time 14.00(14.00)\n",
      "Iter 1149 | Time 55.3817(57.5130) | Bit/dim 3.8142(3.8294) | Xent 1.2339(1.2523) | Loss 4.4312(4.4555) | Error 0.4341(0.4429) Steps 580(590.00) | Grad Norm 5.2377(5.8868) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 59.6999(57.5786) | Bit/dim 3.8279(3.8293) | Xent 1.2735(1.2529) | Loss 4.4647(4.4558) | Error 0.4551(0.4433) Steps 580(589.70) | Grad Norm 9.2167(5.9867) | Total Time 14.00(14.00)\n",
      "Iter 1151 | Time 56.6371(57.5503) | Bit/dim 3.8167(3.8290) | Xent 1.2152(1.2518) | Loss 4.4242(4.4548) | Error 0.4249(0.4427) Steps 592(589.77) | Grad Norm 2.9421(5.8953) | Total Time 14.00(14.00)\n",
      "Iter 1152 | Time 57.2433(57.5411) | Bit/dim 3.8225(3.8288) | Xent 1.2693(1.2523) | Loss 4.4571(4.4549) | Error 0.4543(0.4431) Steps 586(589.66) | Grad Norm 5.7389(5.8907) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0192 | Time 22.1135, Epoch Time 382.5199(393.9230), Bit/dim 3.8212(best: 3.8235), Xent 1.2078, Loss 4.4251, Error 0.4272(best: 0.4242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1153 | Time 59.5742(57.6021) | Bit/dim 3.8108(3.8282) | Xent 1.2328(1.2517) | Loss 4.4272(4.4541) | Error 0.4426(0.4431) Steps 580(589.37) | Grad Norm 4.4760(5.8482) | Total Time 14.00(14.00)\n",
      "Iter 1154 | Time 60.1529(57.6786) | Bit/dim 3.8189(3.8279) | Xent 1.2401(1.2514) | Loss 4.4389(4.4536) | Error 0.4354(0.4428) Steps 598(589.62) | Grad Norm 5.9399(5.8510) | Total Time 14.00(14.00)\n",
      "Iter 1155 | Time 54.6541(57.5879) | Bit/dim 3.8219(3.8278) | Xent 1.2486(1.2513) | Loss 4.4462(4.4534) | Error 0.4397(0.4428) Steps 592(589.70) | Grad Norm 5.0063(5.8256) | Total Time 14.00(14.00)\n",
      "Iter 1156 | Time 58.6315(57.6192) | Bit/dim 3.8150(3.8274) | Xent 1.2401(1.2510) | Loss 4.4350(4.4529) | Error 0.4413(0.4427) Steps 580(589.40) | Grad Norm 6.6379(5.8500) | Total Time 14.00(14.00)\n",
      "Iter 1157 | Time 58.7244(57.6524) | Bit/dim 3.8412(3.8278) | Xent 1.2690(1.2515) | Loss 4.4758(4.4535) | Error 0.4491(0.4429) Steps 568(588.76) | Grad Norm 8.9538(5.9431) | Total Time 14.00(14.00)\n",
      "Iter 1158 | Time 59.0115(57.6931) | Bit/dim 3.8211(3.8276) | Xent 1.2140(1.2504) | Loss 4.4281(4.4528) | Error 0.4306(0.4425) Steps 586(588.68) | Grad Norm 3.1432(5.8591) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0193 | Time 22.9880, Epoch Time 391.4526(393.8489), Bit/dim 3.8234(best: 3.8212), Xent 1.1933, Loss 4.4201, Error 0.4241(best: 0.4242)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1159 | Time 55.8544(57.6380) | Bit/dim 3.8187(3.8273) | Xent 1.2234(1.2496) | Loss 4.4304(4.4521) | Error 0.4345(0.4423) Steps 580(588.42) | Grad Norm 4.3734(5.8145) | Total Time 14.00(14.00)\n",
      "Iter 1160 | Time 56.1556(57.5935) | Bit/dim 3.8278(3.8273) | Xent 1.2371(1.2492) | Loss 4.4463(4.4519) | Error 0.4356(0.4421) Steps 580(588.17) | Grad Norm 7.9531(5.8787) | Total Time 14.00(14.00)\n",
      "Iter 1161 | Time 58.2967(57.6146) | Bit/dim 3.8269(3.8273) | Xent 1.2644(1.2496) | Loss 4.4591(4.4521) | Error 0.4391(0.4420) Steps 604(588.64) | Grad Norm 11.2760(6.0406) | Total Time 14.00(14.00)\n",
      "Iter 1162 | Time 56.0092(57.5664) | Bit/dim 3.8178(3.8270) | Xent 1.2469(1.2496) | Loss 4.4413(4.4518) | Error 0.4427(0.4420) Steps 580(588.38) | Grad Norm 8.9278(6.1272) | Total Time 14.00(14.00)\n",
      "Iter 1163 | Time 55.8643(57.5154) | Bit/dim 3.8203(3.8268) | Xent 1.2295(1.2490) | Loss 4.4350(4.4513) | Error 0.4349(0.4418) Steps 586(588.31) | Grad Norm 2.9384(6.0316) | Total Time 14.00(14.00)\n",
      "Iter 1164 | Time 56.1726(57.4751) | Bit/dim 3.8146(3.8265) | Xent 1.2766(1.2498) | Loss 4.4529(4.4514) | Error 0.4496(0.4420) Steps 574(587.88) | Grad Norm 10.5972(6.1685) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0194 | Time 22.5921, Epoch Time 377.8715(393.3696), Bit/dim 3.8204(best: 3.8212), Xent 1.2222, Loss 4.4315, Error 0.4341(best: 0.4241)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1165 | Time 54.0755(57.3731) | Bit/dim 3.8346(3.8267) | Xent 1.2400(1.2495) | Loss 4.4546(4.4515) | Error 0.4365(0.4419) Steps 586(587.83) | Grad Norm 9.6582(6.2732) | Total Time 14.00(14.00)\n",
      "Iter 1166 | Time 57.4206(57.3745) | Bit/dim 3.8213(3.8266) | Xent 1.2633(1.2499) | Loss 4.4530(4.4515) | Error 0.4435(0.4419) Steps 586(587.77) | Grad Norm 9.1055(6.3582) | Total Time 14.00(14.00)\n",
      "Iter 1167 | Time 58.5138(57.4087) | Bit/dim 3.8260(3.8265) | Xent 1.2386(1.2496) | Loss 4.4453(4.4513) | Error 0.4419(0.4419) Steps 592(587.90) | Grad Norm 7.0426(6.3787) | Total Time 14.00(14.00)\n",
      "Iter 1168 | Time 55.6535(57.3561) | Bit/dim 3.8221(3.8264) | Xent 1.2636(1.2500) | Loss 4.4539(4.4514) | Error 0.4511(0.4422) Steps 574(587.48) | Grad Norm 5.9981(6.3673) | Total Time 14.00(14.00)\n",
      "Iter 1169 | Time 59.0212(57.4060) | Bit/dim 3.8115(3.8260) | Xent 1.2323(1.2495) | Loss 4.4277(4.4507) | Error 0.4320(0.4419) Steps 592(587.62) | Grad Norm 8.9469(6.4447) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 57.8279(57.4187) | Bit/dim 3.8149(3.8256) | Xent 1.2496(1.2495) | Loss 4.4397(4.4504) | Error 0.4339(0.4417) Steps 580(587.39) | Grad Norm 6.1876(6.4370) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0195 | Time 22.8987, Epoch Time 382.5674(393.0455), Bit/dim 3.8209(best: 3.8204), Xent 1.1943, Loss 4.4181, Error 0.4225(best: 0.4241)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1171 | Time 58.7097(57.4574) | Bit/dim 3.8175(3.8254) | Xent 1.2429(1.2493) | Loss 4.4390(4.4500) | Error 0.4325(0.4414) Steps 580(587.17) | Grad Norm 5.1409(6.3981) | Total Time 14.00(14.00)\n",
      "Iter 1172 | Time 57.2478(57.4511) | Bit/dim 3.8249(3.8254) | Xent 1.2674(1.2498) | Loss 4.4586(4.4503) | Error 0.4467(0.4415) Steps 592(587.31) | Grad Norm 9.6116(6.4945) | Total Time 14.00(14.00)\n",
      "Iter 1173 | Time 54.3641(57.3585) | Bit/dim 3.8247(3.8253) | Xent 1.2233(1.2490) | Loss 4.4363(4.4499) | Error 0.4315(0.4412) Steps 574(586.91) | Grad Norm 6.5559(6.4963) | Total Time 14.00(14.00)\n",
      "Iter 1174 | Time 61.6373(57.4869) | Bit/dim 3.8235(3.8253) | Xent 1.2200(1.2481) | Loss 4.4335(4.4494) | Error 0.4339(0.4410) Steps 604(587.42) | Grad Norm 5.4287(6.4643) | Total Time 14.00(14.00)\n",
      "Iter 1175 | Time 54.3139(57.3917) | Bit/dim 3.8136(3.8249) | Xent 1.2307(1.2476) | Loss 4.4289(4.4488) | Error 0.4367(0.4409) Steps 568(586.84) | Grad Norm 6.9412(6.4786) | Total Time 14.00(14.00)\n",
      "Iter 1176 | Time 55.2530(57.3275) | Bit/dim 3.8136(3.8246) | Xent 1.2028(1.2463) | Loss 4.4150(4.4477) | Error 0.4265(0.4405) Steps 574(586.46) | Grad Norm 3.3653(6.3852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0196 | Time 23.2949, Epoch Time 382.1663(392.7192), Bit/dim 3.8232(best: 3.8204), Xent 1.2018, Loss 4.4241, Error 0.4285(best: 0.4225)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1177 | Time 57.8677(57.3437) | Bit/dim 3.8132(3.8243) | Xent 1.2689(1.2470) | Loss 4.4476(4.4477) | Error 0.4467(0.4406) Steps 598(586.80) | Grad Norm 7.8393(6.4288) | Total Time 14.00(14.00)\n",
      "Iter 1178 | Time 56.7105(57.3247) | Bit/dim 3.8043(3.8237) | Xent 1.2171(1.2461) | Loss 4.4129(4.4467) | Error 0.4341(0.4405) Steps 586(586.78) | Grad Norm 5.1338(6.3900) | Total Time 14.00(14.00)\n",
      "Iter 1179 | Time 54.3014(57.2340) | Bit/dim 3.8230(3.8236) | Xent 1.2182(1.2452) | Loss 4.4321(4.4463) | Error 0.4341(0.4403) Steps 586(586.76) | Grad Norm 6.1457(6.3827) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 61.3957(57.3589) | Bit/dim 3.8293(3.8238) | Xent 1.2366(1.2450) | Loss 4.4475(4.4463) | Error 0.4410(0.4403) Steps 592(586.91) | Grad Norm 8.1852(6.4367) | Total Time 14.00(14.00)\n",
      "Iter 1181 | Time 56.8076(57.3423) | Bit/dim 3.8070(3.8233) | Xent 1.2085(1.2439) | Loss 4.4112(4.4452) | Error 0.4225(0.4397) Steps 580(586.71) | Grad Norm 2.4187(6.3162) | Total Time 14.00(14.00)\n",
      "Iter 1182 | Time 55.9419(57.3003) | Bit/dim 3.8211(3.8232) | Xent 1.2034(1.2427) | Loss 4.4228(4.4446) | Error 0.4214(0.4392) Steps 586(586.68) | Grad Norm 7.2244(6.3434) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0197 | Time 22.9432, Epoch Time 382.9697(392.4267), Bit/dim 3.8204(best: 3.8204), Xent 1.1861, Loss 4.4135, Error 0.4175(best: 0.4225)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1183 | Time 58.9356(57.3494) | Bit/dim 3.8186(3.8231) | Xent 1.2016(1.2414) | Loss 4.4194(4.4438) | Error 0.4227(0.4387) Steps 574(586.30) | Grad Norm 5.4441(6.3165) | Total Time 14.00(14.00)\n",
      "Iter 1184 | Time 55.3310(57.2888) | Bit/dim 3.8095(3.8227) | Xent 1.2291(1.2411) | Loss 4.4240(4.4432) | Error 0.4307(0.4385) Steps 592(586.47) | Grad Norm 3.7706(6.2401) | Total Time 14.00(14.00)\n",
      "Iter 1185 | Time 54.9331(57.2182) | Bit/dim 3.8243(3.8227) | Xent 1.2204(1.2404) | Loss 4.4345(4.4430) | Error 0.4341(0.4383) Steps 574(586.10) | Grad Norm 5.0625(6.2048) | Total Time 14.00(14.00)\n",
      "Iter 1186 | Time 56.0021(57.1817) | Bit/dim 3.8202(3.8227) | Xent 1.2050(1.2394) | Loss 4.4227(4.4424) | Error 0.4304(0.4381) Steps 574(585.74) | Grad Norm 2.8033(6.1027) | Total Time 14.00(14.00)\n",
      "Iter 1187 | Time 56.2578(57.1540) | Bit/dim 3.8088(3.8222) | Xent 1.2221(1.2389) | Loss 4.4198(4.4417) | Error 0.4381(0.4381) Steps 598(586.11) | Grad Norm 8.6997(6.1806) | Total Time 14.00(14.00)\n",
      "Iter 1188 | Time 56.3801(57.1307) | Bit/dim 3.8243(3.8223) | Xent 1.2283(1.2385) | Loss 4.4385(4.4416) | Error 0.4301(0.4379) Steps 574(585.74) | Grad Norm 8.7863(6.2588) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0198 | Time 22.5920, Epoch Time 378.0695(391.9960), Bit/dim 3.8158(best: 3.8204), Xent 1.2092, Loss 4.4204, Error 0.4287(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1189 | Time 53.3859(57.0184) | Bit/dim 3.8137(3.8221) | Xent 1.2314(1.2383) | Loss 4.4294(4.4412) | Error 0.4309(0.4376) Steps 586(585.75) | Grad Norm 10.0427(6.3723) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 56.7451(57.0102) | Bit/dim 3.8085(3.8216) | Xent 1.2600(1.2390) | Loss 4.4385(4.4411) | Error 0.4450(0.4379) Steps 592(585.94) | Grad Norm 9.3808(6.4626) | Total Time 14.00(14.00)\n",
      "Iter 1191 | Time 54.6619(56.9397) | Bit/dim 3.8141(3.8214) | Xent 1.1922(1.2376) | Loss 4.4102(4.4402) | Error 0.4201(0.4373) Steps 580(585.76) | Grad Norm 1.4842(6.3132) | Total Time 14.00(14.00)\n",
      "Iter 1192 | Time 57.7682(56.9646) | Bit/dim 3.8305(3.8217) | Xent 1.2535(1.2380) | Loss 4.4572(4.4407) | Error 0.4375(0.4373) Steps 568(585.23) | Grad Norm 8.1967(6.3697) | Total Time 14.00(14.00)\n",
      "Iter 1193 | Time 54.8587(56.9014) | Bit/dim 3.8108(3.8214) | Xent 1.2472(1.2383) | Loss 4.4344(4.4405) | Error 0.4350(0.4373) Steps 580(585.07) | Grad Norm 10.5956(6.4965) | Total Time 14.00(14.00)\n",
      "Iter 1194 | Time 55.7989(56.8683) | Bit/dim 3.8172(3.8212) | Xent 1.2425(1.2384) | Loss 4.4385(4.4405) | Error 0.4409(0.4374) Steps 580(584.92) | Grad Norm 10.5668(6.6186) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0199 | Time 22.5098, Epoch Time 372.7182(391.4176), Bit/dim 3.8201(best: 3.8158), Xent 1.2230, Loss 4.4316, Error 0.4359(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1195 | Time 58.8878(56.9289) | Bit/dim 3.8262(3.8214) | Xent 1.2395(1.2385) | Loss 4.4459(4.4406) | Error 0.4405(0.4375) Steps 592(585.13) | Grad Norm 8.5767(6.6773) | Total Time 14.00(14.00)\n",
      "Iter 1196 | Time 56.3492(56.9115) | Bit/dim 3.8195(3.8213) | Xent 1.2740(1.2395) | Loss 4.4565(4.4411) | Error 0.4533(0.4379) Steps 586(585.16) | Grad Norm 11.6813(6.8275) | Total Time 14.00(14.00)\n",
      "Iter 1197 | Time 59.3220(56.9839) | Bit/dim 3.8160(3.8212) | Xent 1.3194(1.2419) | Loss 4.4757(4.4421) | Error 0.4674(0.4388) Steps 598(585.54) | Grad Norm 13.3322(7.0226) | Total Time 14.00(14.00)\n",
      "Iter 1198 | Time 56.6623(56.9742) | Bit/dim 3.7990(3.8205) | Xent 1.2470(1.2421) | Loss 4.4225(4.4416) | Error 0.4495(0.4392) Steps 580(585.38) | Grad Norm 5.5136(6.9773) | Total Time 14.00(14.00)\n",
      "Iter 1199 | Time 56.5730(56.9622) | Bit/dim 3.8213(3.8205) | Xent 1.3092(1.2441) | Loss 4.4759(4.4426) | Error 0.4616(0.4398) Steps 592(585.57) | Grad Norm 10.8203(7.0926) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 56.0731(56.9355) | Bit/dim 3.8207(3.8205) | Xent 1.2523(1.2443) | Loss 4.4469(4.4427) | Error 0.4447(0.4400) Steps 586(585.59) | Grad Norm 6.3001(7.0689) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0200 | Time 22.9905, Epoch Time 384.0095(391.1954), Bit/dim 3.8150(best: 3.8158), Xent 1.2149, Loss 4.4225, Error 0.4282(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1201 | Time 56.6281(56.9263) | Bit/dim 3.8148(3.8204) | Xent 1.2587(1.2448) | Loss 4.4441(4.4428) | Error 0.4415(0.4400) Steps 592(585.78) | Grad Norm 4.8316(7.0017) | Total Time 14.00(14.00)\n",
      "Iter 1202 | Time 55.7918(56.8922) | Bit/dim 3.8215(3.8204) | Xent 1.2408(1.2447) | Loss 4.4419(4.4427) | Error 0.4434(0.4401) Steps 586(585.79) | Grad Norm 5.6705(6.9618) | Total Time 14.00(14.00)\n",
      "Iter 1203 | Time 55.6355(56.8545) | Bit/dim 3.8171(3.8203) | Xent 1.2407(1.2445) | Loss 4.4374(4.4426) | Error 0.4327(0.4399) Steps 586(585.79) | Grad Norm 5.9711(6.9321) | Total Time 14.00(14.00)\n",
      "Iter 1204 | Time 53.8341(56.7639) | Bit/dim 3.8086(3.8199) | Xent 1.2470(1.2446) | Loss 4.4321(4.4423) | Error 0.4436(0.4400) Steps 586(585.80) | Grad Norm 5.4578(6.8878) | Total Time 14.00(14.00)\n",
      "Iter 1205 | Time 55.1057(56.7142) | Bit/dim 3.8211(3.8200) | Xent 1.2121(1.2436) | Loss 4.4271(4.4418) | Error 0.4230(0.4395) Steps 586(585.80) | Grad Norm 3.0341(6.7722) | Total Time 14.00(14.00)\n",
      "Iter 1206 | Time 59.6832(56.8032) | Bit/dim 3.8228(3.8201) | Xent 1.2225(1.2430) | Loss 4.4341(4.4416) | Error 0.4284(0.4392) Steps 574(585.45) | Grad Norm 5.6562(6.7388) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0201 | Time 22.7235, Epoch Time 376.4090(390.7518), Bit/dim 3.8157(best: 3.8150), Xent 1.1761, Loss 4.4037, Error 0.4181(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1207 | Time 55.2409(56.7564) | Bit/dim 3.8088(3.8197) | Xent 1.1923(1.2415) | Loss 4.4049(4.4405) | Error 0.4199(0.4386) Steps 586(585.47) | Grad Norm 3.4769(6.6409) | Total Time 14.00(14.00)\n",
      "Iter 1208 | Time 55.7513(56.7262) | Bit/dim 3.8092(3.8194) | Xent 1.2467(1.2416) | Loss 4.4325(4.4402) | Error 0.4397(0.4386) Steps 586(585.48) | Grad Norm 7.7422(6.6739) | Total Time 14.00(14.00)\n",
      "Iter 1209 | Time 57.0816(56.7369) | Bit/dim 3.8099(3.8191) | Xent 1.1997(1.2404) | Loss 4.4098(4.4393) | Error 0.4230(0.4382) Steps 598(585.86) | Grad Norm 6.5446(6.6701) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 56.6312(56.7337) | Bit/dim 3.8151(3.8190) | Xent 1.2132(1.2396) | Loss 4.4217(4.4388) | Error 0.4260(0.4378) Steps 580(585.68) | Grad Norm 4.9077(6.6172) | Total Time 14.00(14.00)\n",
      "Iter 1211 | Time 55.9533(56.7103) | Bit/dim 3.8246(3.8192) | Xent 1.2554(1.2400) | Loss 4.4523(4.4392) | Error 0.4474(0.4381) Steps 598(586.05) | Grad Norm 9.6783(6.7090) | Total Time 14.00(14.00)\n",
      "Iter 1212 | Time 57.3326(56.7290) | Bit/dim 3.8167(3.8191) | Xent 1.2092(1.2391) | Loss 4.4213(4.4387) | Error 0.4314(0.4379) Steps 580(585.87) | Grad Norm 4.9306(6.6557) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0202 | Time 23.6083, Epoch Time 378.5530(390.3858), Bit/dim 3.8163(best: 3.8150), Xent 1.1908, Loss 4.4117, Error 0.4214(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1213 | Time 55.9958(56.7070) | Bit/dim 3.8194(3.8191) | Xent 1.2177(1.2385) | Loss 4.4282(4.4383) | Error 0.4294(0.4376) Steps 598(586.23) | Grad Norm 7.2650(6.6739) | Total Time 14.00(14.00)\n",
      "Iter 1214 | Time 56.2170(56.6923) | Bit/dim 3.8158(3.8190) | Xent 1.2270(1.2381) | Loss 4.4293(4.4381) | Error 0.4321(0.4375) Steps 592(586.41) | Grad Norm 7.5224(6.6994) | Total Time 14.00(14.00)\n",
      "Iter 1215 | Time 58.1384(56.7357) | Bit/dim 3.8091(3.8187) | Xent 1.2382(1.2381) | Loss 4.4282(4.4378) | Error 0.4319(0.4373) Steps 598(586.76) | Grad Norm 7.5217(6.7241) | Total Time 14.00(14.00)\n",
      "Iter 1216 | Time 55.3853(56.6952) | Bit/dim 3.8031(3.8182) | Xent 1.1953(1.2369) | Loss 4.4008(4.4367) | Error 0.4217(0.4368) Steps 598(587.09) | Grad Norm 3.5420(6.6286) | Total Time 14.00(14.00)\n",
      "Iter 1217 | Time 54.7275(56.6361) | Bit/dim 3.8116(3.8180) | Xent 1.2247(1.2365) | Loss 4.4240(4.4363) | Error 0.4306(0.4366) Steps 586(587.06) | Grad Norm 4.1618(6.5546) | Total Time 14.00(14.00)\n",
      "Iter 1218 | Time 55.0799(56.5894) | Bit/dim 3.8100(3.8178) | Xent 1.2061(1.2356) | Loss 4.4130(4.4356) | Error 0.4263(0.4363) Steps 586(587.03) | Grad Norm 4.9373(6.5061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0203 | Time 22.8660, Epoch Time 376.0348(389.9553), Bit/dim 3.8104(best: 3.8150), Xent 1.1606, Loss 4.3907, Error 0.4132(best: 0.4175)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1219 | Time 57.0280(56.6026) | Bit/dim 3.8108(3.8176) | Xent 1.1750(1.2338) | Loss 4.3983(4.4345) | Error 0.4107(0.4356) Steps 580(586.82) | Grad Norm 1.5604(6.3577) | Total Time 14.00(14.00)\n",
      "Iter 1220 | Time 57.4889(56.6292) | Bit/dim 3.8110(3.8174) | Xent 1.2013(1.2328) | Loss 4.4116(4.4338) | Error 0.4213(0.4351) Steps 586(586.79) | Grad Norm 5.8988(6.3440) | Total Time 14.00(14.00)\n",
      "Iter 1221 | Time 53.9734(56.5495) | Bit/dim 3.8211(3.8175) | Xent 1.2030(1.2319) | Loss 4.4226(4.4334) | Error 0.4223(0.4347) Steps 580(586.59) | Grad Norm 5.0054(6.3038) | Total Time 14.00(14.00)\n",
      "Iter 1222 | Time 55.2532(56.5106) | Bit/dim 3.7992(3.8170) | Xent 1.2100(1.2312) | Loss 4.4042(4.4326) | Error 0.4273(0.4345) Steps 586(586.57) | Grad Norm 3.6781(6.2250) | Total Time 14.00(14.00)\n",
      "Iter 1223 | Time 55.2391(56.4725) | Bit/dim 3.8170(3.8170) | Xent 1.2059(1.2305) | Loss 4.4200(4.4322) | Error 0.4245(0.4342) Steps 580(586.37) | Grad Norm 4.5447(6.1746) | Total Time 14.00(14.00)\n",
      "Iter 1224 | Time 58.0293(56.5192) | Bit/dim 3.8103(3.8168) | Xent 1.2020(1.2296) | Loss 4.4113(4.4316) | Error 0.4287(0.4341) Steps 580(586.18) | Grad Norm 1.7448(6.0417) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0204 | Time 22.9066, Epoch Time 377.1788(389.5720), Bit/dim 3.8104(best: 3.8104), Xent 1.1637, Loss 4.3922, Error 0.4168(best: 0.4132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1225 | Time 53.1130(56.4170) | Bit/dim 3.7985(3.8162) | Xent 1.1795(1.2281) | Loss 4.3883(4.4303) | Error 0.4199(0.4336) Steps 586(586.18) | Grad Norm 3.4456(5.9638) | Total Time 14.00(14.00)\n",
      "Iter 1226 | Time 55.7932(56.3983) | Bit/dim 3.8102(3.8160) | Xent 1.1826(1.2267) | Loss 4.4015(4.4294) | Error 0.4129(0.4330) Steps 586(586.17) | Grad Norm 2.6269(5.8637) | Total Time 14.00(14.00)\n",
      "Iter 1227 | Time 55.2067(56.3625) | Bit/dim 3.8124(3.8159) | Xent 1.1887(1.2256) | Loss 4.4067(4.4287) | Error 0.4146(0.4325) Steps 592(586.35) | Grad Norm 1.9327(5.7458) | Total Time 14.00(14.00)\n",
      "Iter 1228 | Time 55.3108(56.3310) | Bit/dim 3.8050(3.8156) | Xent 1.2017(1.2249) | Loss 4.4058(4.4280) | Error 0.4300(0.4324) Steps 580(586.16) | Grad Norm 2.9754(5.6627) | Total Time 14.00(14.00)\n",
      "Iter 1229 | Time 54.8574(56.2868) | Bit/dim 3.8078(3.8154) | Xent 1.1803(1.2236) | Loss 4.3979(4.4271) | Error 0.4229(0.4321) Steps 580(585.97) | Grad Norm 2.3619(5.5637) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 55.6879(56.2688) | Bit/dim 3.8114(3.8152) | Xent 1.2076(1.2231) | Loss 4.4152(4.4268) | Error 0.4235(0.4318) Steps 580(585.79) | Grad Norm 3.6266(5.5055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0205 | Time 23.0102, Epoch Time 370.1991(388.9908), Bit/dim 3.8090(best: 3.8104), Xent 1.1629, Loss 4.3905, Error 0.4157(best: 0.4132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1231 | Time 55.4939(56.2456) | Bit/dim 3.8059(3.8150) | Xent 1.1800(1.2218) | Loss 4.3959(4.4258) | Error 0.4111(0.4312) Steps 580(585.62) | Grad Norm 5.8748(5.5166) | Total Time 14.00(14.00)\n",
      "Iter 1232 | Time 54.2007(56.1842) | Bit/dim 3.8122(3.8149) | Xent 1.1919(1.2209) | Loss 4.4081(4.4253) | Error 0.4276(0.4311) Steps 586(585.63) | Grad Norm 6.9423(5.5594) | Total Time 14.00(14.00)\n",
      "Iter 1233 | Time 52.6938(56.0795) | Bit/dim 3.8098(3.8147) | Xent 1.1920(1.2200) | Loss 4.4058(4.4247) | Error 0.4159(0.4307) Steps 586(585.64) | Grad Norm 7.5495(5.6191) | Total Time 14.00(14.00)\n",
      "Iter 1234 | Time 55.3417(56.0574) | Bit/dim 3.8106(3.8146) | Xent 1.2017(1.2195) | Loss 4.4115(4.4243) | Error 0.4220(0.4304) Steps 580(585.47) | Grad Norm 6.1561(5.6352) | Total Time 14.00(14.00)\n",
      "Iter 1235 | Time 55.2765(56.0339) | Bit/dim 3.8058(3.8143) | Xent 1.2181(1.2194) | Loss 4.4148(4.4240) | Error 0.4257(0.4303) Steps 586(585.49) | Grad Norm 1.4013(5.5082) | Total Time 14.00(14.00)\n",
      "Iter 1236 | Time 54.6888(55.9936) | Bit/dim 3.7967(3.8138) | Xent 1.1823(1.2183) | Loss 4.3879(4.4230) | Error 0.4187(0.4299) Steps 586(585.50) | Grad Norm 5.4719(5.5071) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0206 | Time 23.0534, Epoch Time 367.5262(388.3469), Bit/dim 3.8083(best: 3.8090), Xent 1.1675, Loss 4.3921, Error 0.4222(best: 0.4132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1237 | Time 55.6008(55.9818) | Bit/dim 3.8141(3.8138) | Xent 1.1757(1.2170) | Loss 4.4019(4.4223) | Error 0.4231(0.4297) Steps 580(585.34) | Grad Norm 5.2832(5.5004) | Total Time 14.00(14.00)\n",
      "Iter 1238 | Time 56.8315(56.0073) | Bit/dim 3.7993(3.8134) | Xent 1.1864(1.2161) | Loss 4.3924(4.4214) | Error 0.4157(0.4293) Steps 580(585.18) | Grad Norm 2.9480(5.4238) | Total Time 14.00(14.00)\n",
      "Iter 1239 | Time 54.7821(55.9705) | Bit/dim 3.8083(3.8132) | Xent 1.1883(1.2153) | Loss 4.4025(4.4209) | Error 0.4174(0.4289) Steps 586(585.20) | Grad Norm 4.4255(5.3939) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 55.9120(55.9688) | Bit/dim 3.8026(3.8129) | Xent 1.1884(1.2145) | Loss 4.3968(4.4201) | Error 0.4159(0.4285) Steps 580(585.05) | Grad Norm 1.5968(5.2800) | Total Time 14.00(14.00)\n",
      "Iter 1241 | Time 55.9457(55.9681) | Bit/dim 3.8113(3.8129) | Xent 1.1828(1.2135) | Loss 4.4027(4.4196) | Error 0.4145(0.4281) Steps 580(584.90) | Grad Norm 3.4697(5.2256) | Total Time 14.00(14.00)\n",
      "Iter 1242 | Time 54.9589(55.9378) | Bit/dim 3.8072(3.8127) | Xent 1.1960(1.2130) | Loss 4.4052(4.4192) | Error 0.4261(0.4281) Steps 580(584.75) | Grad Norm 5.5397(5.2351) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0207 | Time 22.8897, Epoch Time 374.5050(387.9316), Bit/dim 3.8065(best: 3.8083), Xent 1.1523, Loss 4.3827, Error 0.4098(best: 0.4132)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1243 | Time 55.0894(55.9124) | Bit/dim 3.8003(3.8123) | Xent 1.1815(1.2121) | Loss 4.3911(4.4183) | Error 0.4203(0.4278) Steps 586(584.79) | Grad Norm 3.9965(5.1979) | Total Time 14.00(14.00)\n",
      "Iter 1244 | Time 55.5731(55.9022) | Bit/dim 3.8068(3.8122) | Xent 1.1838(1.2112) | Loss 4.3987(4.4178) | Error 0.4174(0.4275) Steps 580(584.64) | Grad Norm 4.9329(5.1900) | Total Time 14.00(14.00)\n",
      "Iter 1245 | Time 55.5434(55.8914) | Bit/dim 3.8141(3.8122) | Xent 1.2169(1.2114) | Loss 4.4225(4.4179) | Error 0.4257(0.4275) Steps 586(584.68) | Grad Norm 6.8510(5.2398) | Total Time 14.00(14.00)\n",
      "Iter 1246 | Time 55.1060(55.8679) | Bit/dim 3.7945(3.8117) | Xent 1.2042(1.2112) | Loss 4.3966(4.4173) | Error 0.4287(0.4275) Steps 586(584.72) | Grad Norm 6.1355(5.2667) | Total Time 14.00(14.00)\n",
      "Iter 1247 | Time 55.9406(55.8700) | Bit/dim 3.8004(3.8113) | Xent 1.1768(1.2101) | Loss 4.3888(4.4164) | Error 0.4160(0.4271) Steps 592(584.94) | Grad Norm 4.1796(5.2341) | Total Time 14.00(14.00)\n",
      "Iter 1248 | Time 54.1253(55.8177) | Bit/dim 3.8133(3.8114) | Xent 1.1598(1.2086) | Loss 4.3932(4.4157) | Error 0.4115(0.4267) Steps 586(584.97) | Grad Norm 1.8221(5.1317) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0208 | Time 23.1190, Epoch Time 371.4880(387.4383), Bit/dim 3.8037(best: 3.8065), Xent 1.1572, Loss 4.3822, Error 0.4131(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1249 | Time 56.5371(55.8393) | Bit/dim 3.8083(3.8113) | Xent 1.1678(1.2074) | Loss 4.3922(4.4150) | Error 0.4169(0.4264) Steps 586(585.00) | Grad Norm 4.9236(5.1255) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 54.5220(55.7998) | Bit/dim 3.8079(3.8112) | Xent 1.2114(1.2075) | Loss 4.4136(4.4150) | Error 0.4224(0.4263) Steps 580(584.85) | Grad Norm 9.3597(5.2525) | Total Time 14.00(14.00)\n",
      "Iter 1251 | Time 59.8942(55.9226) | Bit/dim 3.8040(3.8110) | Xent 1.2148(1.2077) | Loss 4.4114(4.4149) | Error 0.4194(0.4261) Steps 580(584.71) | Grad Norm 10.9198(5.4225) | Total Time 14.00(14.00)\n",
      "Iter 1252 | Time 54.2570(55.8726) | Bit/dim 3.8136(3.8111) | Xent 1.2045(1.2076) | Loss 4.4159(4.4149) | Error 0.4241(0.4260) Steps 574(584.39) | Grad Norm 8.0479(5.5013) | Total Time 14.00(14.00)\n",
      "Iter 1253 | Time 54.5692(55.8335) | Bit/dim 3.7983(3.8107) | Xent 1.1578(1.2061) | Loss 4.3772(4.4138) | Error 0.4114(0.4256) Steps 586(584.43) | Grad Norm 2.7403(5.4184) | Total Time 14.00(14.00)\n",
      "Iter 1254 | Time 52.7869(55.7421) | Bit/dim 3.8013(3.8104) | Xent 1.1920(1.2057) | Loss 4.3973(4.4133) | Error 0.4235(0.4255) Steps 586(584.48) | Grad Norm 4.7287(5.3977) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0209 | Time 23.2199, Epoch Time 372.0494(386.9766), Bit/dim 3.8055(best: 3.8037), Xent 1.1612, Loss 4.3861, Error 0.4118(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1255 | Time 53.9857(55.6894) | Bit/dim 3.7947(3.8099) | Xent 1.1716(1.2047) | Loss 4.3805(4.4123) | Error 0.4137(0.4251) Steps 586(584.53) | Grad Norm 6.5563(5.4325) | Total Time 14.00(14.00)\n",
      "Iter 1256 | Time 55.9190(55.6963) | Bit/dim 3.8024(3.8097) | Xent 1.1889(1.2042) | Loss 4.3969(4.4118) | Error 0.4201(0.4250) Steps 586(584.57) | Grad Norm 5.8075(5.4437) | Total Time 14.00(14.00)\n",
      "Iter 1257 | Time 56.0660(55.7074) | Bit/dim 3.8084(3.8097) | Xent 1.2057(1.2043) | Loss 4.4112(4.4118) | Error 0.4231(0.4249) Steps 592(584.79) | Grad Norm 7.0595(5.4922) | Total Time 14.00(14.00)\n",
      "Iter 1258 | Time 53.6127(55.6446) | Bit/dim 3.8062(3.8096) | Xent 1.2320(1.2051) | Loss 4.4222(4.4121) | Error 0.4341(0.4252) Steps 580(584.65) | Grad Norm 8.5322(5.5834) | Total Time 14.00(14.00)\n",
      "Iter 1259 | Time 55.1426(55.6295) | Bit/dim 3.7986(3.8092) | Xent 1.1772(1.2043) | Loss 4.3872(4.4114) | Error 0.4141(0.4249) Steps 586(584.69) | Grad Norm 4.9030(5.5630) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 55.0723(55.6128) | Bit/dim 3.8041(3.8091) | Xent 1.1903(1.2038) | Loss 4.3993(4.4110) | Error 0.4224(0.4248) Steps 580(584.55) | Grad Norm 2.9934(5.4859) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0210 | Time 23.0582, Epoch Time 370.1144(386.4708), Bit/dim 3.8040(best: 3.8037), Xent 1.1631, Loss 4.3856, Error 0.4161(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1261 | Time 56.3755(55.6357) | Bit/dim 3.8069(3.8090) | Xent 1.2025(1.2038) | Loss 4.4082(4.4109) | Error 0.4224(0.4247) Steps 580(584.41) | Grad Norm 6.2245(5.5081) | Total Time 14.00(14.00)\n",
      "Iter 1262 | Time 57.4733(55.6908) | Bit/dim 3.7939(3.8086) | Xent 1.1688(1.2028) | Loss 4.3783(4.4099) | Error 0.4081(0.4242) Steps 586(584.46) | Grad Norm 3.8229(5.4575) | Total Time 14.00(14.00)\n",
      "Iter 1263 | Time 53.0793(55.6125) | Bit/dim 3.8108(3.8086) | Xent 1.1726(1.2018) | Loss 4.3971(4.4096) | Error 0.4139(0.4239) Steps 580(584.33) | Grad Norm 4.3693(5.4249) | Total Time 14.00(14.00)\n",
      "Iter 1264 | Time 53.1015(55.5371) | Bit/dim 3.8006(3.8084) | Xent 1.1670(1.2008) | Loss 4.3841(4.4088) | Error 0.4126(0.4236) Steps 580(584.20) | Grad Norm 7.7512(5.4947) | Total Time 14.00(14.00)\n",
      "Iter 1265 | Time 53.3060(55.4702) | Bit/dim 3.8067(3.8083) | Xent 1.2052(1.2009) | Loss 4.4093(4.4088) | Error 0.4241(0.4236) Steps 580(584.07) | Grad Norm 7.9923(5.5696) | Total Time 14.00(14.00)\n",
      "Iter 1266 | Time 55.7493(55.4786) | Bit/dim 3.8017(3.8081) | Xent 1.2177(1.2014) | Loss 4.4105(4.4089) | Error 0.4355(0.4240) Steps 592(584.31) | Grad Norm 9.8298(5.6974) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0211 | Time 22.8881, Epoch Time 370.2756(385.9849), Bit/dim 3.8074(best: 3.8037), Xent 1.2022, Loss 4.4085, Error 0.4330(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1267 | Time 55.0448(55.4656) | Bit/dim 3.7993(3.8079) | Xent 1.2326(1.2024) | Loss 4.4156(4.4091) | Error 0.4381(0.4244) Steps 586(584.36) | Grad Norm 10.3919(5.8382) | Total Time 14.00(14.00)\n",
      "Iter 1268 | Time 55.2245(55.4583) | Bit/dim 3.7951(3.8075) | Xent 1.2081(1.2025) | Loss 4.3991(4.4088) | Error 0.4261(0.4244) Steps 586(584.41) | Grad Norm 5.5943(5.8309) | Total Time 14.00(14.00)\n",
      "Iter 1269 | Time 55.5427(55.4609) | Bit/dim 3.7980(3.8072) | Xent 1.1835(1.2020) | Loss 4.3897(4.4082) | Error 0.4219(0.4244) Steps 598(584.82) | Grad Norm 6.9394(5.8642) | Total Time 14.00(14.00)\n",
      "Iter 1270 | Time 53.4981(55.4020) | Bit/dim 3.8114(3.8073) | Xent 1.2113(1.2022) | Loss 4.4171(4.4085) | Error 0.4236(0.4243) Steps 580(584.67) | Grad Norm 6.4867(5.8828) | Total Time 14.00(14.00)\n",
      "Iter 1271 | Time 53.5817(55.3474) | Bit/dim 3.8098(3.8074) | Xent 1.1895(1.2019) | Loss 4.4046(4.4083) | Error 0.4217(0.4243) Steps 580(584.53) | Grad Norm 7.8745(5.9426) | Total Time 14.00(14.00)\n",
      "Iter 1272 | Time 55.7250(55.3587) | Bit/dim 3.8150(3.8076) | Xent 1.2648(1.2038) | Loss 4.4474(4.4095) | Error 0.4461(0.4249) Steps 580(584.40) | Grad Norm 12.2689(6.1324) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0212 | Time 22.5569, Epoch Time 368.2954(385.4542), Bit/dim 3.8041(best: 3.8037), Xent 1.1733, Loss 4.3908, Error 0.4204(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1273 | Time 53.1687(55.2930) | Bit/dim 3.8132(3.8078) | Xent 1.2102(1.2040) | Loss 4.4183(4.4098) | Error 0.4296(0.4251) Steps 580(584.26) | Grad Norm 6.5281(6.1443) | Total Time 14.00(14.00)\n",
      "Iter 1274 | Time 57.3854(55.3558) | Bit/dim 3.7987(3.8075) | Xent 1.2050(1.2040) | Loss 4.4012(4.4095) | Error 0.4245(0.4250) Steps 586(584.32) | Grad Norm 8.3376(6.2101) | Total Time 14.00(14.00)\n",
      "Iter 1275 | Time 56.4689(55.3892) | Bit/dim 3.8040(3.8074) | Xent 1.2452(1.2052) | Loss 4.4266(4.4100) | Error 0.4354(0.4254) Steps 580(584.19) | Grad Norm 9.8074(6.3180) | Total Time 14.00(14.00)\n",
      "Iter 1276 | Time 57.7926(55.4613) | Bit/dim 3.8043(3.8073) | Xent 1.2128(1.2054) | Loss 4.4107(4.4101) | Error 0.4249(0.4253) Steps 586(584.24) | Grad Norm 6.7672(6.3315) | Total Time 14.00(14.00)\n",
      "Iter 1277 | Time 55.7128(55.4688) | Bit/dim 3.8010(3.8071) | Xent 1.2280(1.2061) | Loss 4.4150(4.4102) | Error 0.4374(0.4257) Steps 580(584.11) | Grad Norm 7.9832(6.3810) | Total Time 14.00(14.00)\n",
      "Iter 1278 | Time 55.1117(55.4581) | Bit/dim 3.8023(3.8070) | Xent 1.2208(1.2066) | Loss 4.4127(4.4103) | Error 0.4355(0.4260) Steps 580(583.99) | Grad Norm 7.8651(6.4255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0213 | Time 23.4621, Epoch Time 376.2102(385.1769), Bit/dim 3.8081(best: 3.8037), Xent 1.2310, Loss 4.4236, Error 0.4369(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1279 | Time 58.4407(55.5476) | Bit/dim 3.8068(3.8070) | Xent 1.2650(1.2083) | Loss 4.4393(4.4111) | Error 0.4486(0.4267) Steps 604(584.59) | Grad Norm 12.7074(6.6140) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 55.7111(55.5525) | Bit/dim 3.8086(3.8070) | Xent 1.2154(1.2085) | Loss 4.4163(4.4113) | Error 0.4233(0.4266) Steps 586(584.63) | Grad Norm 9.8425(6.7108) | Total Time 14.00(14.00)\n",
      "Iter 1281 | Time 55.2199(55.5425) | Bit/dim 3.8030(3.8069) | Xent 1.2148(1.2087) | Loss 4.4104(4.4113) | Error 0.4275(0.4266) Steps 580(584.49) | Grad Norm 7.4592(6.7333) | Total Time 14.00(14.00)\n",
      "Iter 1282 | Time 55.0595(55.5280) | Bit/dim 3.8080(3.8069) | Xent 1.1786(1.2078) | Loss 4.3973(4.4109) | Error 0.4181(0.4263) Steps 586(584.54) | Grad Norm 3.4327(6.6343) | Total Time 14.00(14.00)\n",
      "Iter 1283 | Time 57.5230(55.5879) | Bit/dim 3.7897(3.8064) | Xent 1.1966(1.2075) | Loss 4.3880(4.4102) | Error 0.4176(0.4261) Steps 580(584.40) | Grad Norm 6.2014(6.6213) | Total Time 14.00(14.00)\n",
      "Iter 1284 | Time 54.8914(55.5670) | Bit/dim 3.8033(3.8063) | Xent 1.1870(1.2069) | Loss 4.3968(4.4098) | Error 0.4207(0.4259) Steps 592(584.63) | Grad Norm 2.8032(6.5067) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0214 | Time 22.7002, Epoch Time 376.5472(384.9180), Bit/dim 3.8015(best: 3.8037), Xent 1.1604, Loss 4.3817, Error 0.4164(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1285 | Time 53.8389(55.5151) | Bit/dim 3.7996(3.8061) | Xent 1.1854(1.2062) | Loss 4.3923(4.4092) | Error 0.4217(0.4258) Steps 586(584.67) | Grad Norm 4.5716(6.4487) | Total Time 14.00(14.00)\n",
      "Iter 1286 | Time 53.0297(55.4406) | Bit/dim 3.7944(3.8058) | Xent 1.1903(1.2057) | Loss 4.3895(4.4087) | Error 0.4136(0.4254) Steps 586(584.71) | Grad Norm 3.2266(6.3520) | Total Time 14.00(14.00)\n",
      "Iter 1287 | Time 56.1601(55.4622) | Bit/dim 3.8054(3.8058) | Xent 1.1871(1.2052) | Loss 4.3989(4.4084) | Error 0.4187(0.4252) Steps 598(585.11) | Grad Norm 6.5225(6.3571) | Total Time 14.00(14.00)\n",
      "Iter 1288 | Time 56.3717(55.4894) | Bit/dim 3.8026(3.8057) | Xent 1.1776(1.2044) | Loss 4.3914(4.4079) | Error 0.4115(0.4248) Steps 580(584.96) | Grad Norm 6.3889(6.3581) | Total Time 14.00(14.00)\n",
      "Iter 1289 | Time 54.3318(55.4547) | Bit/dim 3.8067(3.8057) | Xent 1.1895(1.2039) | Loss 4.4015(4.4077) | Error 0.4251(0.4248) Steps 580(584.81) | Grad Norm 7.3816(6.3888) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 54.9141(55.4385) | Bit/dim 3.7942(3.8054) | Xent 1.1807(1.2032) | Loss 4.3846(4.4070) | Error 0.4115(0.4244) Steps 580(584.66) | Grad Norm 5.6779(6.3675) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0215 | Time 23.5144, Epoch Time 369.0182(384.4410), Bit/dim 3.8006(best: 3.8015), Xent 1.1460, Loss 4.3736, Error 0.4059(best: 0.4098)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1291 | Time 56.6994(55.4763) | Bit/dim 3.7973(3.8051) | Xent 1.1647(1.2021) | Loss 4.3796(4.4061) | Error 0.4104(0.4240) Steps 586(584.70) | Grad Norm 3.3907(6.2782) | Total Time 14.00(14.00)\n",
      "Iter 1292 | Time 53.3821(55.4135) | Bit/dim 3.7967(3.8049) | Xent 1.1868(1.2016) | Loss 4.3901(4.4057) | Error 0.4230(0.4240) Steps 586(584.74) | Grad Norm 5.4290(6.2527) | Total Time 14.00(14.00)\n",
      "Iter 1293 | Time 55.5505(55.4176) | Bit/dim 3.7993(3.8047) | Xent 1.1800(1.2010) | Loss 4.3893(4.4052) | Error 0.4140(0.4237) Steps 586(584.78) | Grad Norm 5.3421(6.2254) | Total Time 14.00(14.00)\n",
      "Iter 1294 | Time 57.2575(55.4728) | Bit/dim 3.7949(3.8044) | Xent 1.2000(1.2009) | Loss 4.3949(4.4049) | Error 0.4279(0.4238) Steps 586(584.82) | Grad Norm 10.6575(6.3583) | Total Time 14.00(14.00)\n",
      "Iter 1295 | Time 53.9541(55.4272) | Bit/dim 3.8049(3.8044) | Xent 1.2295(1.2018) | Loss 4.4196(4.4053) | Error 0.4334(0.4241) Steps 586(584.85) | Grad Norm 10.5878(6.4852) | Total Time 14.00(14.00)\n",
      "Iter 1296 | Time 53.9380(55.3826) | Bit/dim 3.8045(3.8044) | Xent 1.2176(1.2023) | Loss 4.4133(4.4055) | Error 0.4313(0.4243) Steps 592(585.07) | Grad Norm 10.5196(6.6062) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0216 | Time 23.6053, Epoch Time 371.2610(384.0456), Bit/dim 3.8031(best: 3.8006), Xent 1.1914, Loss 4.3988, Error 0.4192(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1297 | Time 57.2882(55.4397) | Bit/dim 3.7926(3.8041) | Xent 1.2222(1.2029) | Loss 4.4037(4.4055) | Error 0.4330(0.4246) Steps 586(585.10) | Grad Norm 8.7921(6.6718) | Total Time 14.00(14.00)\n",
      "Iter 1298 | Time 56.7199(55.4781) | Bit/dim 3.7950(3.8038) | Xent 1.1896(1.2025) | Loss 4.3898(4.4050) | Error 0.4250(0.4246) Steps 592(585.30) | Grad Norm 3.5300(6.5776) | Total Time 14.00(14.00)\n",
      "Iter 1299 | Time 57.3424(55.5341) | Bit/dim 3.8009(3.8037) | Xent 1.2513(1.2039) | Loss 4.4265(4.4057) | Error 0.4325(0.4248) Steps 586(585.32) | Grad Norm 12.5698(6.7573) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 55.9709(55.5472) | Bit/dim 3.8132(3.8040) | Xent 1.2005(1.2038) | Loss 4.4134(4.4059) | Error 0.4241(0.4248) Steps 580(585.16) | Grad Norm 11.7782(6.9080) | Total Time 14.00(14.00)\n",
      "Iter 1301 | Time 57.9340(55.6188) | Bit/dim 3.8030(3.8040) | Xent 1.2253(1.2045) | Loss 4.4157(4.4062) | Error 0.4375(0.4252) Steps 586(585.19) | Grad Norm 11.5328(7.0467) | Total Time 14.00(14.00)\n",
      "Iter 1302 | Time 57.0737(55.6624) | Bit/dim 3.8042(3.8040) | Xent 1.2085(1.2046) | Loss 4.4084(4.4063) | Error 0.4197(0.4250) Steps 586(585.21) | Grad Norm 5.9528(7.0139) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0217 | Time 22.8366, Epoch Time 382.2407(383.9915), Bit/dim 3.8023(best: 3.8006), Xent 1.2126, Loss 4.4086, Error 0.4397(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1303 | Time 55.0725(55.6447) | Bit/dim 3.8063(3.8040) | Xent 1.2498(1.2059) | Loss 4.4312(4.4070) | Error 0.4469(0.4257) Steps 586(585.24) | Grad Norm 9.7767(7.0968) | Total Time 14.00(14.00)\n",
      "Iter 1304 | Time 57.4511(55.6989) | Bit/dim 3.7956(3.8038) | Xent 1.1988(1.2057) | Loss 4.3950(4.4066) | Error 0.4224(0.4256) Steps 598(585.62) | Grad Norm 4.8938(7.0307) | Total Time 14.00(14.00)\n",
      "Iter 1305 | Time 59.2154(55.8044) | Bit/dim 3.7915(3.8034) | Xent 1.2172(1.2061) | Loss 4.4001(4.4065) | Error 0.4357(0.4259) Steps 610(586.35) | Grad Norm 7.5823(7.0472) | Total Time 14.00(14.00)\n",
      "Iter 1306 | Time 56.2510(55.8178) | Bit/dim 3.8025(3.8034) | Xent 1.1920(1.2056) | Loss 4.3985(4.4062) | Error 0.4183(0.4256) Steps 586(586.34) | Grad Norm 5.4756(7.0001) | Total Time 14.00(14.00)\n",
      "Iter 1307 | Time 54.8834(55.7898) | Bit/dim 3.8055(3.8035) | Xent 1.2024(1.2056) | Loss 4.4067(4.4062) | Error 0.4230(0.4256) Steps 580(586.15) | Grad Norm 6.0978(6.9730) | Total Time 14.00(14.00)\n",
      "Iter 1308 | Time 56.8679(55.8221) | Bit/dim 3.7987(3.8033) | Xent 1.1741(1.2046) | Loss 4.3858(4.4056) | Error 0.4130(0.4252) Steps 580(585.97) | Grad Norm 1.7686(6.8169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0218 | Time 22.9304, Epoch Time 379.8415(383.8670), Bit/dim 3.8054(best: 3.8006), Xent 1.1629, Loss 4.3869, Error 0.4106(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1309 | Time 55.4149(55.8099) | Bit/dim 3.8132(3.8036) | Xent 1.2224(1.2051) | Loss 4.4244(4.4062) | Error 0.4320(0.4254) Steps 586(585.97) | Grad Norm 6.0648(6.7943) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 56.5494(55.8321) | Bit/dim 3.7948(3.8033) | Xent 1.1683(1.2040) | Loss 4.3789(4.4054) | Error 0.4166(0.4251) Steps 586(585.97) | Grad Norm 1.4949(6.6353) | Total Time 14.00(14.00)\n",
      "Iter 1311 | Time 55.3336(55.8171) | Bit/dim 3.8048(3.8034) | Xent 1.1812(1.2033) | Loss 4.3954(4.4051) | Error 0.4185(0.4249) Steps 586(585.97) | Grad Norm 4.5671(6.5733) | Total Time 14.00(14.00)\n",
      "Iter 1312 | Time 53.9929(55.7624) | Bit/dim 3.7967(3.8032) | Xent 1.1452(1.2016) | Loss 4.3693(4.4040) | Error 0.4039(0.4243) Steps 586(585.97) | Grad Norm 2.9227(6.4638) | Total Time 14.00(14.00)\n",
      "Iter 1313 | Time 54.9891(55.7392) | Bit/dim 3.8005(3.8031) | Xent 1.1953(1.2014) | Loss 4.3982(4.4038) | Error 0.4174(0.4241) Steps 592(586.15) | Grad Norm 7.7747(6.5031) | Total Time 14.00(14.00)\n",
      "Iter 1314 | Time 57.5270(55.7928) | Bit/dim 3.7907(3.8027) | Xent 1.1644(1.2003) | Loss 4.3729(4.4029) | Error 0.4186(0.4239) Steps 580(585.97) | Grad Norm 3.4101(6.4103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0219 | Time 23.4018, Epoch Time 373.5320(383.5569), Bit/dim 3.7965(best: 3.8006), Xent 1.1421, Loss 4.3676, Error 0.4089(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1315 | Time 54.9428(55.7673) | Bit/dim 3.8027(3.8027) | Xent 1.1938(1.2001) | Loss 4.3996(4.4028) | Error 0.4273(0.4240) Steps 586(585.97) | Grad Norm 3.7001(6.3290) | Total Time 14.00(14.00)\n",
      "Iter 1316 | Time 54.3375(55.7244) | Bit/dim 3.7962(3.8025) | Xent 1.1771(1.1994) | Loss 4.3848(4.4022) | Error 0.4213(0.4239) Steps 586(585.97) | Grad Norm 5.8050(6.3133) | Total Time 14.00(14.00)\n",
      "Iter 1317 | Time 55.9541(55.7313) | Bit/dim 3.8057(3.8026) | Xent 1.2206(1.2001) | Loss 4.4160(4.4027) | Error 0.4229(0.4239) Steps 598(586.33) | Grad Norm 11.0961(6.4568) | Total Time 14.00(14.00)\n",
      "Iter 1318 | Time 57.0583(55.7711) | Bit/dim 3.7914(3.8023) | Xent 1.1949(1.1999) | Loss 4.3889(4.4022) | Error 0.4211(0.4238) Steps 586(586.32) | Grad Norm 9.7944(6.5569) | Total Time 14.00(14.00)\n",
      "Iter 1319 | Time 54.9169(55.7455) | Bit/dim 3.7928(3.8020) | Xent 1.1601(1.1987) | Loss 4.3729(4.4014) | Error 0.4091(0.4234) Steps 592(586.49) | Grad Norm 3.2316(6.4571) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 56.2117(55.7595) | Bit/dim 3.7932(3.8017) | Xent 1.1843(1.1983) | Loss 4.3854(4.4009) | Error 0.4211(0.4233) Steps 580(586.30) | Grad Norm 7.3019(6.4825) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0220 | Time 23.7166, Epoch Time 373.7611(383.2631), Bit/dim 3.7993(best: 3.7965), Xent 1.2035, Loss 4.4010, Error 0.4224(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1321 | Time 58.2037(55.8328) | Bit/dim 3.7919(3.8015) | Xent 1.2450(1.1997) | Loss 4.4144(4.4013) | Error 0.4413(0.4239) Steps 598(586.65) | Grad Norm 11.8322(6.6430) | Total Time 14.00(14.00)\n",
      "Iter 1322 | Time 54.2694(55.7859) | Bit/dim 3.8009(3.8014) | Xent 1.1940(1.1995) | Loss 4.3979(4.4012) | Error 0.4196(0.4237) Steps 586(586.63) | Grad Norm 5.2273(6.6005) | Total Time 14.00(14.00)\n",
      "Iter 1323 | Time 54.9660(55.7613) | Bit/dim 3.7985(3.8013) | Xent 1.2089(1.1998) | Loss 4.4029(4.4012) | Error 0.4253(0.4238) Steps 586(586.61) | Grad Norm 6.9583(6.6112) | Total Time 14.00(14.00)\n",
      "Iter 1324 | Time 57.0961(55.8014) | Bit/dim 3.7917(3.8011) | Xent 1.2075(1.2000) | Loss 4.3955(4.4011) | Error 0.4249(0.4238) Steps 598(586.95) | Grad Norm 9.9170(6.7104) | Total Time 14.00(14.00)\n",
      "Iter 1325 | Time 54.0671(55.7493) | Bit/dim 3.7972(3.8009) | Xent 1.1946(1.1999) | Loss 4.3945(4.4009) | Error 0.4229(0.4238) Steps 592(587.10) | Grad Norm 5.4443(6.6724) | Total Time 14.00(14.00)\n",
      "Iter 1326 | Time 56.2408(55.7641) | Bit/dim 3.8006(3.8009) | Xent 1.1730(1.1990) | Loss 4.3871(4.4005) | Error 0.4171(0.4236) Steps 574(586.71) | Grad Norm 7.2410(6.6895) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0221 | Time 23.1500, Epoch Time 375.5373(383.0313), Bit/dim 3.7989(best: 3.7965), Xent 1.1662, Loss 4.3820, Error 0.4154(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1327 | Time 54.7814(55.7346) | Bit/dim 3.8044(3.8010) | Xent 1.1937(1.1989) | Loss 4.4012(4.4005) | Error 0.4254(0.4236) Steps 586(586.69) | Grad Norm 8.6959(6.7497) | Total Time 14.00(14.00)\n",
      "Iter 1328 | Time 57.0155(55.7730) | Bit/dim 3.7996(3.8010) | Xent 1.1501(1.1974) | Loss 4.3746(4.3997) | Error 0.4024(0.4230) Steps 586(586.67) | Grad Norm 2.1839(6.6127) | Total Time 14.00(14.00)\n",
      "Iter 1329 | Time 58.7493(55.8623) | Bit/dim 3.7963(3.8009) | Xent 1.1713(1.1966) | Loss 4.3819(4.3992) | Error 0.4113(0.4226) Steps 580(586.47) | Grad Norm 5.7796(6.5877) | Total Time 14.00(14.00)\n",
      "Iter 1330 | Time 55.8826(55.8629) | Bit/dim 3.7920(3.8006) | Xent 1.1781(1.1961) | Loss 4.3810(4.3986) | Error 0.4120(0.4223) Steps 592(586.63) | Grad Norm 4.2104(6.5164) | Total Time 14.00(14.00)\n",
      "Iter 1331 | Time 56.7091(55.8883) | Bit/dim 3.7907(3.8003) | Xent 1.1768(1.1955) | Loss 4.3791(4.3980) | Error 0.4180(0.4222) Steps 598(586.97) | Grad Norm 3.9156(6.4384) | Total Time 14.00(14.00)\n",
      "Iter 1332 | Time 55.5923(55.8794) | Bit/dim 3.7937(3.8001) | Xent 1.1446(1.1940) | Loss 4.3660(4.3971) | Error 0.4042(0.4217) Steps 586(586.94) | Grad Norm 2.4988(6.3202) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0222 | Time 23.6679, Epoch Time 379.7147(382.9318), Bit/dim 3.7946(best: 3.7965), Xent 1.1467, Loss 4.3680, Error 0.4108(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1333 | Time 56.0785(55.8854) | Bit/dim 3.8017(3.8001) | Xent 1.1836(1.1937) | Loss 4.3935(4.3970) | Error 0.4160(0.4215) Steps 586(586.92) | Grad Norm 4.3248(6.2603) | Total Time 14.00(14.00)\n",
      "Iter 1334 | Time 58.8228(55.9735) | Bit/dim 3.7845(3.7997) | Xent 1.1522(1.1924) | Loss 4.3606(4.3959) | Error 0.4099(0.4211) Steps 592(587.07) | Grad Norm 1.2868(6.1111) | Total Time 14.00(14.00)\n",
      "Iter 1335 | Time 54.6596(55.9341) | Bit/dim 3.7855(3.7992) | Xent 1.1693(1.1917) | Loss 4.3701(4.3951) | Error 0.4207(0.4211) Steps 586(587.04) | Grad Norm 3.3218(6.0274) | Total Time 14.00(14.00)\n",
      "Iter 1336 | Time 55.6787(55.9264) | Bit/dim 3.7990(3.7992) | Xent 1.1644(1.1909) | Loss 4.3812(4.3947) | Error 0.4074(0.4207) Steps 592(587.19) | Grad Norm 2.6392(5.9258) | Total Time 14.00(14.00)\n",
      "Iter 1337 | Time 54.3812(55.8801) | Bit/dim 3.7971(3.7992) | Xent 1.1448(1.1895) | Loss 4.3695(4.3939) | Error 0.4041(0.4202) Steps 580(586.97) | Grad Norm 3.4447(5.8514) | Total Time 14.00(14.00)\n",
      "Iter 1338 | Time 56.1635(55.8886) | Bit/dim 3.7849(3.7987) | Xent 1.1689(1.1889) | Loss 4.3693(4.3932) | Error 0.4173(0.4201) Steps 586(586.94) | Grad Norm 4.4779(5.8102) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0223 | Time 23.0387, Epoch Time 376.1565(382.7285), Bit/dim 3.7936(best: 3.7946), Xent 1.1388, Loss 4.3630, Error 0.4096(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1339 | Time 52.8565(55.7976) | Bit/dim 3.7909(3.7985) | Xent 1.1739(1.1885) | Loss 4.3779(4.3927) | Error 0.4155(0.4200) Steps 580(586.73) | Grad Norm 3.9599(5.7546) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 53.9637(55.7426) | Bit/dim 3.7954(3.7984) | Xent 1.1557(1.1875) | Loss 4.3733(4.3922) | Error 0.4130(0.4198) Steps 586(586.71) | Grad Norm 3.7387(5.6942) | Total Time 14.00(14.00)\n",
      "Iter 1341 | Time 55.9150(55.7478) | Bit/dim 3.7916(3.7982) | Xent 1.1787(1.1872) | Loss 4.3809(4.3918) | Error 0.4183(0.4197) Steps 586(586.69) | Grad Norm 6.3484(5.7138) | Total Time 14.00(14.00)\n",
      "Iter 1342 | Time 53.7866(55.6889) | Bit/dim 3.7817(3.7977) | Xent 1.1416(1.1858) | Loss 4.3525(4.3906) | Error 0.4089(0.4194) Steps 592(586.85) | Grad Norm 4.4726(5.6766) | Total Time 14.00(14.00)\n",
      "Iter 1343 | Time 56.7453(55.7206) | Bit/dim 3.7992(3.7978) | Xent 1.1668(1.1853) | Loss 4.3826(4.3904) | Error 0.4159(0.4193) Steps 580(586.64) | Grad Norm 5.2981(5.6652) | Total Time 14.00(14.00)\n",
      "Iter 1344 | Time 53.1944(55.6448) | Bit/dim 3.7955(3.7977) | Xent 1.1701(1.1848) | Loss 4.3805(4.3901) | Error 0.4187(0.4193) Steps 586(586.62) | Grad Norm 6.7412(5.6975) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0224 | Time 22.7449, Epoch Time 366.7209(382.2483), Bit/dim 3.7926(best: 3.7936), Xent 1.1392, Loss 4.3622, Error 0.4104(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1345 | Time 53.7654(55.5885) | Bit/dim 3.7970(3.7977) | Xent 1.1791(1.1846) | Loss 4.3866(4.3900) | Error 0.4199(0.4193) Steps 586(586.60) | Grad Norm 7.1500(5.7411) | Total Time 14.00(14.00)\n",
      "Iter 1346 | Time 55.3134(55.5802) | Bit/dim 3.7946(3.7976) | Xent 1.1882(1.1847) | Loss 4.3887(4.3900) | Error 0.4144(0.4192) Steps 586(586.59) | Grad Norm 6.6265(5.7676) | Total Time 14.00(14.00)\n",
      "Iter 1347 | Time 54.4390(55.5460) | Bit/dim 3.7912(3.7974) | Xent 1.1503(1.1837) | Loss 4.3663(4.3892) | Error 0.4080(0.4188) Steps 586(586.57) | Grad Norm 3.9399(5.7128) | Total Time 14.00(14.00)\n",
      "Iter 1348 | Time 55.2912(55.5383) | Bit/dim 3.7937(3.7973) | Xent 1.1511(1.1827) | Loss 4.3692(4.3886) | Error 0.4105(0.4186) Steps 586(586.55) | Grad Norm 3.2444(5.6387) | Total Time 14.00(14.00)\n",
      "Iter 1349 | Time 54.0889(55.4948) | Bit/dim 3.7796(3.7967) | Xent 1.1792(1.1826) | Loss 4.3693(4.3881) | Error 0.4206(0.4186) Steps 586(586.54) | Grad Norm 6.0534(5.6512) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 58.1355(55.5741) | Bit/dim 3.7928(3.7966) | Xent 1.1557(1.1818) | Loss 4.3707(4.3875) | Error 0.4146(0.4185) Steps 586(586.52) | Grad Norm 6.0342(5.6627) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0225 | Time 22.9860, Epoch Time 371.2680(381.9189), Bit/dim 3.7919(best: 3.7926), Xent 1.1276, Loss 4.3557, Error 0.4016(best: 0.4059)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1351 | Time 55.5899(55.5745) | Bit/dim 3.7817(3.7962) | Xent 1.1524(1.1809) | Loss 4.3580(4.3867) | Error 0.4084(0.4182) Steps 586(586.50) | Grad Norm 4.8203(5.6374) | Total Time 14.00(14.00)\n",
      "Iter 1352 | Time 55.5184(55.5729) | Bit/dim 3.7937(3.7961) | Xent 1.1624(1.1804) | Loss 4.3749(4.3863) | Error 0.4060(0.4178) Steps 580(586.31) | Grad Norm 5.7465(5.6407) | Total Time 14.00(14.00)\n",
      "Iter 1353 | Time 58.8212(55.6703) | Bit/dim 3.7872(3.7958) | Xent 1.1520(1.1795) | Loss 4.3633(4.3856) | Error 0.4050(0.4175) Steps 586(586.30) | Grad Norm 3.2614(5.5693) | Total Time 14.00(14.00)\n",
      "Iter 1354 | Time 56.3341(55.6902) | Bit/dim 3.7866(3.7956) | Xent 1.1679(1.1792) | Loss 4.3705(4.3852) | Error 0.4119(0.4173) Steps 586(586.29) | Grad Norm 2.7245(5.4839) | Total Time 14.00(14.00)\n",
      "Iter 1355 | Time 56.0751(55.7018) | Bit/dim 3.7942(3.7955) | Xent 1.1549(1.1785) | Loss 4.3716(4.3848) | Error 0.4119(0.4171) Steps 586(586.28) | Grad Norm 3.3601(5.4202) | Total Time 14.00(14.00)\n",
      "Iter 1356 | Time 55.4750(55.6950) | Bit/dim 3.7926(3.7954) | Xent 1.1645(1.1780) | Loss 4.3749(4.3845) | Error 0.4141(0.4170) Steps 586(586.27) | Grad Norm 2.7350(5.3397) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0226 | Time 23.0448, Epoch Time 377.4814(381.7858), Bit/dim 3.7914(best: 3.7919), Xent 1.1192, Loss 4.3510, Error 0.3992(best: 0.4016)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1357 | Time 55.1788(55.6795) | Bit/dim 3.7939(3.7954) | Xent 1.1444(1.1770) | Loss 4.3661(4.3839) | Error 0.4077(0.4168) Steps 586(586.27) | Grad Norm 1.5176(5.2250) | Total Time 14.00(14.00)\n",
      "Iter 1358 | Time 56.6056(55.7073) | Bit/dim 3.7839(3.7950) | Xent 1.1364(1.1758) | Loss 4.3521(4.3829) | Error 0.4005(0.4163) Steps 580(586.08) | Grad Norm 3.7422(5.1805) | Total Time 14.00(14.00)\n",
      "Iter 1359 | Time 54.5249(55.6718) | Bit/dim 3.7863(3.7948) | Xent 1.1575(1.1753) | Loss 4.3650(4.3824) | Error 0.4056(0.4160) Steps 586(586.07) | Grad Norm 3.7459(5.1375) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 56.5509(55.6982) | Bit/dim 3.7913(3.7947) | Xent 1.1715(1.1751) | Loss 4.3770(4.3822) | Error 0.4209(0.4161) Steps 586(586.07) | Grad Norm 5.2547(5.1410) | Total Time 14.00(14.00)\n",
      "Iter 1361 | Time 53.7143(55.6387) | Bit/dim 3.7989(3.7948) | Xent 1.1617(1.1747) | Loss 4.3797(4.3822) | Error 0.4065(0.4158) Steps 586(586.07) | Grad Norm 7.3755(5.2080) | Total Time 14.00(14.00)\n",
      "Iter 1362 | Time 55.9995(55.6495) | Bit/dim 3.7856(3.7945) | Xent 1.1725(1.1747) | Loss 4.3719(4.3819) | Error 0.4124(0.4157) Steps 592(586.25) | Grad Norm 9.5205(5.3374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0227 | Time 23.5048, Epoch Time 373.2747(381.5304), Bit/dim 3.7898(best: 3.7914), Xent 1.1588, Loss 4.3692, Error 0.4118(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1363 | Time 56.8613(55.6858) | Bit/dim 3.7887(3.7944) | Xent 1.1714(1.1746) | Loss 4.3744(4.3816) | Error 0.4139(0.4157) Steps 586(586.24) | Grad Norm 9.3548(5.4579) | Total Time 14.00(14.00)\n",
      "Iter 1364 | Time 55.0565(55.6670) | Bit/dim 3.7929(3.7943) | Xent 1.1643(1.1743) | Loss 4.3751(4.3814) | Error 0.4094(0.4155) Steps 586(586.23) | Grad Norm 4.8933(5.4410) | Total Time 14.00(14.00)\n",
      "Iter 1365 | Time 56.7295(55.6988) | Bit/dim 3.7896(3.7942) | Xent 1.1396(1.1732) | Loss 4.3594(4.3808) | Error 0.4076(0.4152) Steps 586(586.23) | Grad Norm 4.1276(5.4016) | Total Time 14.00(14.00)\n",
      "Iter 1366 | Time 55.2113(55.6842) | Bit/dim 3.7886(3.7940) | Xent 1.1440(1.1724) | Loss 4.3607(4.3802) | Error 0.4049(0.4149) Steps 592(586.40) | Grad Norm 6.3702(5.4306) | Total Time 14.00(14.00)\n",
      "Iter 1367 | Time 57.0580(55.7254) | Bit/dim 3.7910(3.7939) | Xent 1.1719(1.1723) | Loss 4.3769(4.3801) | Error 0.4136(0.4149) Steps 586(586.39) | Grad Norm 7.0738(5.4799) | Total Time 14.00(14.00)\n",
      "Iter 1368 | Time 56.7332(55.7557) | Bit/dim 3.7827(3.7936) | Xent 1.1747(1.1724) | Loss 4.3701(4.3798) | Error 0.4149(0.4149) Steps 580(586.20) | Grad Norm 4.9702(5.4647) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0228 | Time 23.5405, Epoch Time 378.3609(381.4353), Bit/dim 3.7890(best: 3.7898), Xent 1.1188, Loss 4.3484, Error 0.4018(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1369 | Time 54.9868(55.7326) | Bit/dim 3.7784(3.7931) | Xent 1.1242(1.1710) | Loss 4.3405(4.3786) | Error 0.3929(0.4142) Steps 586(586.19) | Grad Norm 2.3031(5.3698) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 55.5031(55.7257) | Bit/dim 3.7903(3.7930) | Xent 1.1625(1.1707) | Loss 4.3716(4.3784) | Error 0.4103(0.4141) Steps 586(586.18) | Grad Norm 4.0774(5.3310) | Total Time 14.00(14.00)\n",
      "Iter 1371 | Time 57.2339(55.7709) | Bit/dim 3.7957(3.7931) | Xent 1.1718(1.1707) | Loss 4.3816(4.3785) | Error 0.4186(0.4142) Steps 586(586.18) | Grad Norm 9.9433(5.4694) | Total Time 14.00(14.00)\n",
      "Iter 1372 | Time 55.4440(55.7611) | Bit/dim 3.7978(3.7933) | Xent 1.2178(1.1722) | Loss 4.4068(4.3793) | Error 0.4319(0.4148) Steps 604(586.71) | Grad Norm 14.0459(5.7267) | Total Time 14.00(14.00)\n",
      "Iter 1373 | Time 55.5951(55.7562) | Bit/dim 3.7913(3.7932) | Xent 1.2063(1.1732) | Loss 4.3944(4.3798) | Error 0.4295(0.4152) Steps 586(586.69) | Grad Norm 11.4699(5.8990) | Total Time 14.00(14.00)\n",
      "Iter 1374 | Time 55.7028(55.7546) | Bit/dim 3.7788(3.7928) | Xent 1.1874(1.1736) | Loss 4.3725(4.3796) | Error 0.4216(0.4154) Steps 586(586.67) | Grad Norm 4.9063(5.8692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0229 | Time 23.4888, Epoch Time 374.6370(381.2314), Bit/dim 3.7873(best: 3.7890), Xent 1.1488, Loss 4.3617, Error 0.4085(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1375 | Time 54.9087(55.7292) | Bit/dim 3.7859(3.7926) | Xent 1.1998(1.1744) | Loss 4.3858(4.3798) | Error 0.4176(0.4155) Steps 586(586.65) | Grad Norm 6.1550(5.8778) | Total Time 14.00(14.00)\n",
      "Iter 1376 | Time 56.0106(55.7376) | Bit/dim 3.7951(3.7926) | Xent 1.1864(1.1747) | Loss 4.3884(4.3800) | Error 0.4249(0.4157) Steps 592(586.81) | Grad Norm 7.3889(5.9231) | Total Time 14.00(14.00)\n",
      "Iter 1377 | Time 54.2422(55.6928) | Bit/dim 3.7949(3.7927) | Xent 1.1542(1.1741) | Loss 4.3721(4.3798) | Error 0.4062(0.4155) Steps 586(586.79) | Grad Norm 3.5652(5.8524) | Total Time 14.00(14.00)\n",
      "Iter 1378 | Time 56.7933(55.7258) | Bit/dim 3.7814(3.7924) | Xent 1.1930(1.1747) | Loss 4.3779(4.3797) | Error 0.4271(0.4158) Steps 586(586.76) | Grad Norm 7.8278(5.9116) | Total Time 14.00(14.00)\n",
      "Iter 1379 | Time 57.1367(55.7681) | Bit/dim 3.7847(3.7921) | Xent 1.1717(1.1746) | Loss 4.3706(4.3794) | Error 0.4201(0.4159) Steps 592(586.92) | Grad Norm 6.3714(5.9254) | Total Time 14.00(14.00)\n",
      "Iter 1380 | Time 55.6388(55.7642) | Bit/dim 3.7913(3.7921) | Xent 1.1435(1.1737) | Loss 4.3631(4.3790) | Error 0.4076(0.4157) Steps 580(586.71) | Grad Norm 5.7889(5.9213) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0230 | Time 23.3638, Epoch Time 375.2542(381.0521), Bit/dim 3.7919(best: 3.7873), Xent 1.1665, Loss 4.3751, Error 0.4122(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1381 | Time 54.3869(55.7229) | Bit/dim 3.7849(3.7919) | Xent 1.2029(1.1746) | Loss 4.3864(4.3792) | Error 0.4191(0.4158) Steps 586(586.69) | Grad Norm 8.4668(5.9977) | Total Time 14.00(14.00)\n",
      "Iter 1382 | Time 54.9017(55.6983) | Bit/dim 3.7811(3.7916) | Xent 1.1573(1.1740) | Loss 4.3597(4.3786) | Error 0.4097(0.4156) Steps 586(586.67) | Grad Norm 4.6182(5.9563) | Total Time 14.00(14.00)\n",
      "Iter 1383 | Time 55.3362(55.6874) | Bit/dim 3.7943(3.7917) | Xent 1.2356(1.1759) | Loss 4.4121(4.3796) | Error 0.4341(0.4162) Steps 586(586.65) | Grad Norm 12.5595(6.1544) | Total Time 14.00(14.00)\n",
      "Iter 1384 | Time 57.5851(55.7443) | Bit/dim 3.8055(3.7921) | Xent 1.2235(1.1773) | Loss 4.4172(4.3807) | Error 0.4311(0.4166) Steps 592(586.81) | Grad Norm 13.0681(6.3618) | Total Time 14.00(14.00)\n",
      "Iter 1385 | Time 55.0246(55.7227) | Bit/dim 3.7885(3.7920) | Xent 1.2156(1.1785) | Loss 4.3962(4.3812) | Error 0.4226(0.4168) Steps 586(586.79) | Grad Norm 7.9755(6.4102) | Total Time 14.00(14.00)\n",
      "Iter 1386 | Time 55.2179(55.7076) | Bit/dim 3.7800(3.7916) | Xent 1.1548(1.1778) | Loss 4.3575(4.3805) | Error 0.4124(0.4167) Steps 586(586.76) | Grad Norm 6.8241(6.4226) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0231 | Time 23.2203, Epoch Time 372.2413(380.7878), Bit/dim 3.7913(best: 3.7873), Xent 1.1727, Loss 4.3777, Error 0.4154(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1387 | Time 55.0677(55.6884) | Bit/dim 3.7902(3.7916) | Xent 1.2168(1.1789) | Loss 4.3986(4.3810) | Error 0.4280(0.4170) Steps 586(586.74) | Grad Norm 11.1734(6.5652) | Total Time 14.00(14.00)\n",
      "Iter 1388 | Time 55.9798(55.6971) | Bit/dim 3.7931(3.7916) | Xent 1.1958(1.1794) | Loss 4.3910(4.3813) | Error 0.4176(0.4170) Steps 586(586.72) | Grad Norm 8.2995(6.6172) | Total Time 14.00(14.00)\n",
      "Iter 1389 | Time 55.0657(55.6782) | Bit/dim 3.7917(3.7916) | Xent 1.1697(1.1791) | Loss 4.3766(4.3812) | Error 0.4076(0.4167) Steps 586(586.70) | Grad Norm 6.5516(6.6152) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 57.2678(55.7259) | Bit/dim 3.7828(3.7913) | Xent 1.1894(1.1794) | Loss 4.3775(4.3811) | Error 0.4213(0.4169) Steps 586(586.68) | Grad Norm 6.5297(6.6127) | Total Time 14.00(14.00)\n",
      "Iter 1391 | Time 54.9487(55.7026) | Bit/dim 3.7912(3.7913) | Xent 1.1793(1.1794) | Loss 4.3808(4.3811) | Error 0.4205(0.4170) Steps 586(586.66) | Grad Norm 6.3524(6.6049) | Total Time 14.00(14.00)\n",
      "Iter 1392 | Time 56.5542(55.7281) | Bit/dim 3.7792(3.7910) | Xent 1.1415(1.1783) | Loss 4.3499(4.3801) | Error 0.4052(0.4166) Steps 598(587.00) | Grad Norm 3.5040(6.5118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0232 | Time 23.3970, Epoch Time 375.3767(380.6254), Bit/dim 3.7906(best: 3.7873), Xent 1.1442, Loss 4.3627, Error 0.4104(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1393 | Time 57.3783(55.7776) | Bit/dim 3.7848(3.7908) | Xent 1.1721(1.1781) | Loss 4.3708(4.3799) | Error 0.4177(0.4167) Steps 586(586.97) | Grad Norm 5.9632(6.4954) | Total Time 14.00(14.00)\n",
      "Iter 1394 | Time 56.4634(55.7982) | Bit/dim 3.7826(3.7905) | Xent 1.1603(1.1776) | Loss 4.3627(4.3793) | Error 0.4090(0.4164) Steps 586(586.94) | Grad Norm 3.4941(6.4053) | Total Time 14.00(14.00)\n",
      "Iter 1395 | Time 54.9569(55.7730) | Bit/dim 3.7829(3.7903) | Xent 1.1771(1.1776) | Loss 4.3715(4.3791) | Error 0.4163(0.4164) Steps 586(586.91) | Grad Norm 6.6248(6.4119) | Total Time 14.00(14.00)\n",
      "Iter 1396 | Time 57.0013(55.8098) | Bit/dim 3.7995(3.7906) | Xent 1.1839(1.1778) | Loss 4.3915(4.3795) | Error 0.4129(0.4163) Steps 586(586.88) | Grad Norm 10.0536(6.5212) | Total Time 14.00(14.00)\n",
      "Iter 1397 | Time 56.0091(55.8158) | Bit/dim 3.7964(3.7908) | Xent 1.1937(1.1782) | Loss 4.3932(4.3799) | Error 0.4157(0.4163) Steps 592(587.04) | Grad Norm 8.7727(6.5887) | Total Time 14.00(14.00)\n",
      "Iter 1398 | Time 54.9270(55.7891) | Bit/dim 3.7726(3.7902) | Xent 1.1561(1.1776) | Loss 4.3507(4.3790) | Error 0.4137(0.4162) Steps 592(587.18) | Grad Norm 3.1263(6.4848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0233 | Time 23.5621, Epoch Time 378.1650(380.5516), Bit/dim 3.7895(best: 3.7873), Xent 1.1716, Loss 4.3753, Error 0.4157(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1399 | Time 52.9515(55.7040) | Bit/dim 3.7895(3.7902) | Xent 1.1941(1.1781) | Loss 4.3865(4.3792) | Error 0.4229(0.4164) Steps 592(587.33) | Grad Norm 12.1746(6.6555) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 54.8468(55.6783) | Bit/dim 3.7822(3.7900) | Xent 1.1879(1.1784) | Loss 4.3761(4.3791) | Error 0.4179(0.4165) Steps 586(587.29) | Grad Norm 10.4357(6.7689) | Total Time 14.00(14.00)\n",
      "Iter 1401 | Time 57.0928(55.7207) | Bit/dim 3.7912(3.7900) | Xent 1.1810(1.1784) | Loss 4.3817(4.3792) | Error 0.4191(0.4166) Steps 586(587.25) | Grad Norm 6.4767(6.7602) | Total Time 14.00(14.00)\n",
      "Iter 1402 | Time 55.6368(55.7182) | Bit/dim 3.7819(3.7898) | Xent 1.1658(1.1781) | Loss 4.3648(4.3788) | Error 0.4159(0.4165) Steps 586(587.21) | Grad Norm 7.2072(6.7736) | Total Time 14.00(14.00)\n",
      "Iter 1403 | Time 55.6866(55.7173) | Bit/dim 3.7813(3.7895) | Xent 1.1835(1.1782) | Loss 4.3731(4.3786) | Error 0.4181(0.4166) Steps 592(587.36) | Grad Norm 7.4904(6.7951) | Total Time 14.00(14.00)\n",
      "Iter 1404 | Time 56.9223(55.7534) | Bit/dim 3.7843(3.7893) | Xent 1.1833(1.1784) | Loss 4.3759(4.3785) | Error 0.4115(0.4164) Steps 586(587.32) | Grad Norm 5.2080(6.7475) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0234 | Time 23.2330, Epoch Time 373.7266(380.3469), Bit/dim 3.7895(best: 3.7873), Xent 1.1569, Loss 4.3680, Error 0.4144(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1405 | Time 57.6929(55.8116) | Bit/dim 3.8001(3.7897) | Xent 1.1879(1.1787) | Loss 4.3941(4.3790) | Error 0.4224(0.4166) Steps 586(587.28) | Grad Norm 7.3339(6.7651) | Total Time 14.00(14.00)\n",
      "Iter 1406 | Time 54.3581(55.7680) | Bit/dim 3.7752(3.7892) | Xent 1.1427(1.1776) | Loss 4.3466(4.3780) | Error 0.4030(0.4162) Steps 586(587.24) | Grad Norm 3.6614(6.6720) | Total Time 14.00(14.00)\n",
      "Iter 1407 | Time 59.6045(55.8831) | Bit/dim 3.7968(3.7895) | Xent 1.1684(1.1773) | Loss 4.3810(4.3781) | Error 0.4104(0.4160) Steps 586(587.20) | Grad Norm 4.3445(6.6021) | Total Time 14.00(14.00)\n",
      "Iter 1408 | Time 54.1380(55.8307) | Bit/dim 3.7839(3.7893) | Xent 1.1792(1.1774) | Loss 4.3735(4.3780) | Error 0.4170(0.4161) Steps 586(587.16) | Grad Norm 6.4669(6.5981) | Total Time 14.00(14.00)\n",
      "Iter 1409 | Time 56.7390(55.8580) | Bit/dim 3.7744(3.7888) | Xent 1.1570(1.1768) | Loss 4.3529(4.3772) | Error 0.4081(0.4158) Steps 586(587.13) | Grad Norm 6.9020(6.6072) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 55.7499(55.8547) | Bit/dim 3.7850(3.7887) | Xent 1.1705(1.1766) | Loss 4.3703(4.3770) | Error 0.4180(0.4159) Steps 592(587.28) | Grad Norm 7.7884(6.6426) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0235 | Time 23.1337, Epoch Time 378.6353(380.2955), Bit/dim 3.7859(best: 3.7873), Xent 1.1292, Loss 4.3506, Error 0.4080(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1411 | Time 53.6702(55.7892) | Bit/dim 3.7805(3.7885) | Xent 1.1732(1.1765) | Loss 4.3671(4.3767) | Error 0.4170(0.4159) Steps 586(587.24) | Grad Norm 4.9675(6.5924) | Total Time 14.00(14.00)\n",
      "Iter 1412 | Time 54.9832(55.7650) | Bit/dim 3.7890(3.7885) | Xent 1.1690(1.1762) | Loss 4.3735(4.3766) | Error 0.4103(0.4157) Steps 586(587.20) | Grad Norm 5.9779(6.5739) | Total Time 14.00(14.00)\n",
      "Iter 1413 | Time 58.1444(55.8364) | Bit/dim 3.7800(3.7882) | Xent 1.1969(1.1769) | Loss 4.3785(4.3767) | Error 0.4254(0.4160) Steps 586(587.16) | Grad Norm 10.3726(6.6879) | Total Time 14.00(14.00)\n",
      "Iter 1414 | Time 58.6263(55.9201) | Bit/dim 3.7872(3.7882) | Xent 1.1615(1.1764) | Loss 4.3680(4.3764) | Error 0.4120(0.4159) Steps 592(587.31) | Grad Norm 7.0650(6.6992) | Total Time 14.00(14.00)\n",
      "Iter 1415 | Time 54.5461(55.8789) | Bit/dim 3.7854(3.7881) | Xent 1.1618(1.1760) | Loss 4.3663(4.3761) | Error 0.4110(0.4158) Steps 586(587.27) | Grad Norm 3.8110(6.6126) | Total Time 14.00(14.00)\n",
      "Iter 1416 | Time 56.9725(55.9117) | Bit/dim 3.7828(3.7880) | Xent 1.1835(1.1762) | Loss 4.3745(4.3761) | Error 0.4186(0.4159) Steps 586(587.23) | Grad Norm 8.0339(6.6552) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0236 | Time 23.7743, Epoch Time 378.0214(380.2273), Bit/dim 3.7862(best: 3.7859), Xent 1.1230, Loss 4.3477, Error 0.3986(best: 0.3992)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1417 | Time 55.2122(55.8907) | Bit/dim 3.7882(3.7880) | Xent 1.1411(1.1751) | Loss 4.3587(4.3755) | Error 0.3986(0.4153) Steps 586(587.20) | Grad Norm 5.6904(6.6263) | Total Time 14.00(14.00)\n",
      "Iter 1418 | Time 56.7138(55.9154) | Bit/dim 3.7828(3.7878) | Xent 1.1540(1.1745) | Loss 4.3598(4.3751) | Error 0.4055(0.4150) Steps 592(587.34) | Grad Norm 4.5608(6.5643) | Total Time 14.00(14.00)\n",
      "Iter 1419 | Time 56.7219(55.9396) | Bit/dim 3.7774(3.7875) | Xent 1.1525(1.1738) | Loss 4.3537(4.3744) | Error 0.4110(0.4149) Steps 604(587.84) | Grad Norm 3.8416(6.4826) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 58.1079(56.0046) | Bit/dim 3.7929(3.7877) | Xent 1.1560(1.1733) | Loss 4.3710(4.3743) | Error 0.4120(0.4148) Steps 586(587.78) | Grad Norm 4.1371(6.4123) | Total Time 14.00(14.00)\n",
      "Iter 1421 | Time 53.5522(55.9311) | Bit/dim 3.7818(3.7875) | Xent 1.1419(1.1724) | Loss 4.3527(4.3737) | Error 0.4091(0.4147) Steps 586(587.73) | Grad Norm 2.8852(6.3064) | Total Time 14.00(14.00)\n",
      "Iter 1422 | Time 55.7341(55.9252) | Bit/dim 3.7900(3.7876) | Xent 1.1294(1.1711) | Loss 4.3547(4.3731) | Error 0.3952(0.4141) Steps 586(587.68) | Grad Norm 3.1698(6.2123) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0237 | Time 23.1798, Epoch Time 376.0116(380.1008), Bit/dim 3.7826(best: 3.7859), Xent 1.1086, Loss 4.3369, Error 0.3956(best: 0.3986)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1423 | Time 56.8327(55.9524) | Bit/dim 3.7749(3.7872) | Xent 1.1447(1.1703) | Loss 4.3473(4.3723) | Error 0.4097(0.4139) Steps 586(587.63) | Grad Norm 1.3737(6.0672) | Total Time 14.00(14.00)\n",
      "Iter 1424 | Time 53.5799(55.8812) | Bit/dim 3.7862(3.7872) | Xent 1.1527(1.1698) | Loss 4.3626(4.3720) | Error 0.4084(0.4138) Steps 586(587.58) | Grad Norm 4.0469(6.0066) | Total Time 14.00(14.00)\n",
      "Iter 1425 | Time 53.6154(55.8132) | Bit/dim 3.7815(3.7870) | Xent 1.1493(1.1691) | Loss 4.3561(4.3716) | Error 0.4006(0.4134) Steps 586(587.53) | Grad Norm 5.7718(5.9995) | Total Time 14.00(14.00)\n",
      "Iter 1426 | Time 57.1928(55.8546) | Bit/dim 3.7806(3.7868) | Xent 1.1519(1.1686) | Loss 4.3565(4.3711) | Error 0.3994(0.4130) Steps 592(587.67) | Grad Norm 7.5173(6.0451) | Total Time 14.00(14.00)\n",
      "Iter 1427 | Time 56.3853(55.8705) | Bit/dim 3.7940(3.7870) | Xent 1.1466(1.1680) | Loss 4.3673(4.3710) | Error 0.4076(0.4128) Steps 586(587.62) | Grad Norm 3.2039(5.9598) | Total Time 14.00(14.00)\n",
      "Iter 1428 | Time 56.6695(55.8945) | Bit/dim 3.7759(3.7867) | Xent 1.1498(1.1674) | Loss 4.3508(4.3704) | Error 0.4085(0.4127) Steps 586(587.57) | Grad Norm 2.9005(5.8680) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0238 | Time 23.3901, Epoch Time 374.3328(379.9278), Bit/dim 3.7810(best: 3.7826), Xent 1.1137, Loss 4.3378, Error 0.3989(best: 0.3956)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1429 | Time 54.0853(55.8402) | Bit/dim 3.7850(3.7866) | Xent 1.1342(1.1664) | Loss 4.3521(4.3698) | Error 0.4081(0.4125) Steps 580(587.34) | Grad Norm 4.3561(5.8227) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 54.0534(55.7866) | Bit/dim 3.7820(3.7865) | Xent 1.1459(1.1658) | Loss 4.3549(4.3694) | Error 0.4014(0.4122) Steps 592(587.48) | Grad Norm 6.5422(5.8443) | Total Time 14.00(14.00)\n",
      "Iter 1431 | Time 54.1591(55.7378) | Bit/dim 3.7856(3.7865) | Xent 1.1438(1.1651) | Loss 4.3575(4.3690) | Error 0.4083(0.4121) Steps 592(587.62) | Grad Norm 5.8946(5.8458) | Total Time 14.00(14.00)\n",
      "Iter 1432 | Time 54.8394(55.7108) | Bit/dim 3.7713(3.7860) | Xent 1.1372(1.1643) | Loss 4.3399(4.3682) | Error 0.4048(0.4119) Steps 586(587.57) | Grad Norm 1.9177(5.7279) | Total Time 14.00(14.00)\n",
      "Iter 1433 | Time 56.5363(55.7356) | Bit/dim 3.7910(3.7862) | Xent 1.1603(1.1642) | Loss 4.3711(4.3683) | Error 0.4109(0.4118) Steps 586(587.52) | Grad Norm 6.1990(5.7421) | Total Time 14.00(14.00)\n",
      "Iter 1434 | Time 59.1082(55.8368) | Bit/dim 3.7748(3.7858) | Xent 1.1481(1.1637) | Loss 4.3488(4.3677) | Error 0.4077(0.4117) Steps 598(587.83) | Grad Norm 7.1668(5.7848) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0239 | Time 23.3154, Epoch Time 373.3209(379.7296), Bit/dim 3.7812(best: 3.7810), Xent 1.1111, Loss 4.3367, Error 0.3944(best: 0.3956)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1435 | Time 55.7170(55.8332) | Bit/dim 3.7878(3.7859) | Xent 1.1300(1.1627) | Loss 4.3528(4.3672) | Error 0.3978(0.4113) Steps 592(587.96) | Grad Norm 3.9309(5.7292) | Total Time 14.00(14.00)\n",
      "Iter 1436 | Time 57.0099(55.8685) | Bit/dim 3.7828(3.7858) | Xent 1.1192(1.1614) | Loss 4.3424(4.3665) | Error 0.3928(0.4107) Steps 586(587.90) | Grad Norm 3.0115(5.6477) | Total Time 14.00(14.00)\n",
      "Iter 1437 | Time 56.2863(55.8810) | Bit/dim 3.7782(3.7856) | Xent 1.1732(1.1617) | Loss 4.3648(4.3664) | Error 0.4189(0.4110) Steps 586(587.84) | Grad Norm 8.0760(5.7205) | Total Time 14.00(14.00)\n",
      "Iter 1438 | Time 58.5972(55.9625) | Bit/dim 3.7735(3.7852) | Xent 1.1894(1.1626) | Loss 4.3682(4.3665) | Error 0.4123(0.4110) Steps 598(588.15) | Grad Norm 11.7090(5.9002) | Total Time 14.00(14.00)\n",
      "Iter 1439 | Time 54.3295(55.9135) | Bit/dim 3.7808(3.7851) | Xent 1.1727(1.1629) | Loss 4.3672(4.3665) | Error 0.4124(0.4111) Steps 598(588.44) | Grad Norm 8.6319(5.9821) | Total Time 14.00(14.00)\n",
      "Iter 1440 | Time 55.4775(55.9004) | Bit/dim 3.7797(3.7849) | Xent 1.1437(1.1623) | Loss 4.3515(4.3661) | Error 0.4021(0.4108) Steps 586(588.37) | Grad Norm 3.7960(5.9165) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0240 | Time 23.6361, Epoch Time 378.1028(379.6808), Bit/dim 3.7832(best: 3.7810), Xent 1.1493, Loss 4.3579, Error 0.4090(best: 0.3944)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1441 | Time 57.0487(55.9349) | Bit/dim 3.7766(3.7847) | Xent 1.1601(1.1622) | Loss 4.3567(4.3658) | Error 0.4129(0.4109) Steps 604(588.84) | Grad Norm 10.4867(6.0536) | Total Time 14.00(14.00)\n",
      "Iter 1442 | Time 58.0189(55.9974) | Bit/dim 3.7748(3.7844) | Xent 1.1826(1.1628) | Loss 4.3662(4.3658) | Error 0.4135(0.4109) Steps 586(588.75) | Grad Norm 7.9407(6.1103) | Total Time 14.00(14.00)\n",
      "Iter 1443 | Time 57.0691(56.0296) | Bit/dim 3.7776(3.7842) | Xent 1.1641(1.1629) | Loss 4.3596(4.3656) | Error 0.4097(0.4109) Steps 592(588.85) | Grad Norm 7.0861(6.1395) | Total Time 14.00(14.00)\n",
      "Iter 1444 | Time 57.8338(56.0837) | Bit/dim 3.7827(3.7841) | Xent 1.1563(1.1627) | Loss 4.3609(4.3655) | Error 0.4097(0.4109) Steps 610(589.49) | Grad Norm 8.5074(6.2106) | Total Time 14.00(14.00)\n",
      "Iter 1445 | Time 58.6986(56.1621) | Bit/dim 3.7829(3.7841) | Xent 1.1681(1.1629) | Loss 4.3670(4.3655) | Error 0.4076(0.4108) Steps 610(590.10) | Grad Norm 5.9935(6.2041) | Total Time 14.00(14.00)\n",
      "Iter 1446 | Time 56.2912(56.1660) | Bit/dim 3.7823(3.7840) | Xent 1.1524(1.1625) | Loss 4.3585(4.3653) | Error 0.4062(0.4106) Steps 592(590.16) | Grad Norm 4.1008(6.1410) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0241 | Time 23.6479, Epoch Time 386.0331(379.8713), Bit/dim 3.7799(best: 3.7810), Xent 1.1194, Loss 4.3395, Error 0.3983(best: 0.3944)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1447 | Time 60.0740(56.2833) | Bit/dim 3.7777(3.7838) | Xent 1.1530(1.1623) | Loss 4.3542(4.3650) | Error 0.4139(0.4107) Steps 604(590.57) | Grad Norm 4.7210(6.0984) | Total Time 14.00(14.00)\n",
      "Iter 1448 | Time 58.5636(56.3517) | Bit/dim 3.7800(3.7837) | Xent 1.1264(1.1612) | Loss 4.3432(4.3643) | Error 0.3946(0.4102) Steps 586(590.44) | Grad Norm 2.8867(6.0020) | Total Time 14.00(14.00)\n",
      "Iter 1449 | Time 58.8793(56.4275) | Bit/dim 3.7712(3.7833) | Xent 1.1356(1.1604) | Loss 4.3390(4.3635) | Error 0.4034(0.4100) Steps 586(590.30) | Grad Norm 2.4818(5.8964) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 56.2403(56.4219) | Bit/dim 3.7782(3.7832) | Xent 1.1381(1.1597) | Loss 4.3472(4.3631) | Error 0.4014(0.4098) Steps 586(590.17) | Grad Norm 4.7861(5.8631) | Total Time 14.00(14.00)\n",
      "Iter 1451 | Time 56.9722(56.4384) | Bit/dim 3.7733(3.7829) | Xent 1.1466(1.1593) | Loss 4.3466(4.3626) | Error 0.4038(0.4096) Steps 586(590.05) | Grad Norm 4.1128(5.8106) | Total Time 14.00(14.00)\n",
      "Iter 1452 | Time 56.2108(56.4316) | Bit/dim 3.7864(3.7830) | Xent 1.1049(1.1577) | Loss 4.3389(4.3619) | Error 0.3929(0.4091) Steps 604(590.47) | Grad Norm 2.2170(5.7028) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0242 | Time 23.3957, Epoch Time 387.1383(380.0893), Bit/dim 3.7802(best: 3.7799), Xent 1.1071, Loss 4.3338, Error 0.3914(best: 0.3944)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1453 | Time 59.4908(56.5233) | Bit/dim 3.7812(3.7829) | Xent 1.1301(1.1569) | Loss 4.3462(4.3614) | Error 0.4016(0.4089) Steps 586(590.33) | Grad Norm 4.6420(5.6710) | Total Time 14.00(14.00)\n",
      "Iter 1454 | Time 59.0239(56.5983) | Bit/dim 3.7880(3.7831) | Xent 1.1282(1.1560) | Loss 4.3521(4.3611) | Error 0.4056(0.4088) Steps 592(590.38) | Grad Norm 4.6287(5.6397) | Total Time 14.00(14.00)\n",
      "Iter 1455 | Time 54.2872(56.5290) | Bit/dim 3.7760(3.7829) | Xent 1.1352(1.1554) | Loss 4.3436(4.3606) | Error 0.4045(0.4086) Steps 598(590.61) | Grad Norm 3.9376(5.5886) | Total Time 14.00(14.00)\n",
      "Iter 1456 | Time 57.2540(56.5508) | Bit/dim 3.7747(3.7826) | Xent 1.1406(1.1550) | Loss 4.3450(4.3601) | Error 0.4024(0.4085) Steps 586(590.47) | Grad Norm 4.7174(5.5625) | Total Time 14.00(14.00)\n",
      "Iter 1457 | Time 56.2546(56.5419) | Bit/dim 3.7752(3.7824) | Xent 1.1508(1.1548) | Loss 4.3506(4.3598) | Error 0.4025(0.4083) Steps 592(590.52) | Grad Norm 5.3722(5.5568) | Total Time 14.00(14.00)\n",
      "Iter 1458 | Time 54.6596(56.4854) | Bit/dim 3.7711(3.7821) | Xent 1.1331(1.1542) | Loss 4.3377(4.3592) | Error 0.4045(0.4082) Steps 604(590.92) | Grad Norm 5.3596(5.5509) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0243 | Time 22.9995, Epoch Time 381.2138(380.1231), Bit/dim 3.7767(best: 3.7799), Xent 1.1192, Loss 4.3363, Error 0.4035(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1459 | Time 55.5159(56.4563) | Bit/dim 3.7699(3.7817) | Xent 1.1528(1.1541) | Loss 4.3463(4.3588) | Error 0.4146(0.4084) Steps 586(590.78) | Grad Norm 6.2701(5.5724) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 57.0154(56.4731) | Bit/dim 3.7842(3.7818) | Xent 1.1440(1.1538) | Loss 4.3562(4.3587) | Error 0.4052(0.4083) Steps 586(590.63) | Grad Norm 6.2407(5.5925) | Total Time 14.00(14.00)\n",
      "Iter 1461 | Time 54.7553(56.4216) | Bit/dim 3.7831(3.7818) | Xent 1.1323(1.1532) | Loss 4.3493(4.3584) | Error 0.4022(0.4081) Steps 598(590.85) | Grad Norm 4.3239(5.5544) | Total Time 14.00(14.00)\n",
      "Iter 1462 | Time 56.4131(56.4213) | Bit/dim 3.7831(3.7819) | Xent 1.1328(1.1526) | Loss 4.3495(4.3581) | Error 0.4025(0.4079) Steps 604(591.25) | Grad Norm 1.4951(5.4327) | Total Time 14.00(14.00)\n",
      "Iter 1463 | Time 60.2933(56.5375) | Bit/dim 3.7634(3.7813) | Xent 1.1413(1.1522) | Loss 4.3340(4.3574) | Error 0.4040(0.4078) Steps 586(591.09) | Grad Norm 6.5907(5.4674) | Total Time 14.00(14.00)\n",
      "Iter 1464 | Time 55.0978(56.4943) | Bit/dim 3.7752(3.7811) | Xent 1.1467(1.1521) | Loss 4.3485(4.3572) | Error 0.4034(0.4077) Steps 598(591.30) | Grad Norm 6.0652(5.4853) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0244 | Time 23.4913, Epoch Time 379.6911(380.1101), Bit/dim 3.7789(best: 3.7767), Xent 1.1022, Loss 4.3300, Error 0.3926(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1465 | Time 56.5295(56.4953) | Bit/dim 3.7622(3.7806) | Xent 1.1395(1.1517) | Loss 4.3320(4.3564) | Error 0.3980(0.4074) Steps 604(591.68) | Grad Norm 3.0611(5.4126) | Total Time 14.00(14.00)\n",
      "Iter 1466 | Time 57.5923(56.5283) | Bit/dim 3.7823(3.7806) | Xent 1.1419(1.1514) | Loss 4.3532(4.3563) | Error 0.4045(0.4073) Steps 586(591.51) | Grad Norm 7.1354(5.4643) | Total Time 14.00(14.00)\n",
      "Iter 1467 | Time 57.5682(56.5595) | Bit/dim 3.7769(3.7805) | Xent 1.2050(1.1530) | Loss 4.3794(4.3570) | Error 0.4300(0.4080) Steps 598(591.70) | Grad Norm 9.9258(5.5981) | Total Time 14.00(14.00)\n",
      "Iter 1468 | Time 58.9215(56.6303) | Bit/dim 3.7793(3.7805) | Xent 1.1335(1.1524) | Loss 4.3461(4.3567) | Error 0.3999(0.4077) Steps 586(591.53) | Grad Norm 5.2466(5.5876) | Total Time 14.00(14.00)\n",
      "Iter 1469 | Time 56.3876(56.6230) | Bit/dim 3.7820(3.7805) | Xent 1.1398(1.1520) | Loss 4.3519(4.3565) | Error 0.4032(0.4076) Steps 598(591.73) | Grad Norm 5.1502(5.5745) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 56.3469(56.6147) | Bit/dim 3.7831(3.7806) | Xent 1.1508(1.1520) | Loss 4.3585(4.3566) | Error 0.4054(0.4075) Steps 586(591.55) | Grad Norm 5.8066(5.5814) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0245 | Time 23.5045, Epoch Time 383.6992(380.2178), Bit/dim 3.7749(best: 3.7767), Xent 1.1079, Loss 4.3288, Error 0.3961(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1471 | Time 55.1055(56.5695) | Bit/dim 3.7790(3.7805) | Xent 1.1243(1.1512) | Loss 4.3412(4.3561) | Error 0.3900(0.4070) Steps 586(591.39) | Grad Norm 3.5656(5.5210) | Total Time 14.00(14.00)\n",
      "Iter 1472 | Time 57.6944(56.6032) | Bit/dim 3.7758(3.7804) | Xent 1.1662(1.1516) | Loss 4.3589(4.3562) | Error 0.4141(0.4072) Steps 598(591.59) | Grad Norm 8.4549(5.6090) | Total Time 14.00(14.00)\n",
      "Iter 1473 | Time 54.2045(56.5313) | Bit/dim 3.7796(3.7804) | Xent 1.1593(1.1519) | Loss 4.3593(4.3563) | Error 0.4160(0.4075) Steps 598(591.78) | Grad Norm 5.8630(5.6166) | Total Time 14.00(14.00)\n",
      "Iter 1474 | Time 56.2725(56.5235) | Bit/dim 3.7737(3.7802) | Xent 1.1413(1.1515) | Loss 4.3443(4.3559) | Error 0.4035(0.4074) Steps 592(591.79) | Grad Norm 6.1408(5.6323) | Total Time 14.00(14.00)\n",
      "Iter 1475 | Time 55.3167(56.4873) | Bit/dim 3.7813(3.7802) | Xent 1.1759(1.1523) | Loss 4.3692(4.3563) | Error 0.4099(0.4074) Steps 586(591.61) | Grad Norm 10.8326(5.7883) | Total Time 14.00(14.00)\n",
      "Iter 1476 | Time 54.9721(56.4418) | Bit/dim 3.7817(3.7803) | Xent 1.1403(1.1519) | Loss 4.3519(4.3562) | Error 0.4006(0.4072) Steps 598(591.80) | Grad Norm 5.9907(5.7944) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0246 | Time 23.7033, Epoch Time 373.9708(380.0304), Bit/dim 3.7764(best: 3.7749), Xent 1.1126, Loss 4.3327, Error 0.3954(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1477 | Time 54.7787(56.3919) | Bit/dim 3.7764(3.7801) | Xent 1.1274(1.1512) | Loss 4.3401(4.3557) | Error 0.3979(0.4070) Steps 592(591.81) | Grad Norm 5.9285(5.7984) | Total Time 14.00(14.00)\n",
      "Iter 1478 | Time 53.6690(56.3102) | Bit/dim 3.7881(3.7804) | Xent 1.1924(1.1524) | Loss 4.3843(4.3566) | Error 0.4175(0.4073) Steps 592(591.82) | Grad Norm 10.1491(5.9289) | Total Time 14.00(14.00)\n",
      "Iter 1479 | Time 58.8030(56.3850) | Bit/dim 3.7700(3.7801) | Xent 1.1648(1.1528) | Loss 4.3524(4.3565) | Error 0.4153(0.4075) Steps 598(592.00) | Grad Norm 3.4690(5.8551) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 56.6283(56.3923) | Bit/dim 3.7743(3.7799) | Xent 1.1799(1.1536) | Loss 4.3642(4.3567) | Error 0.4195(0.4079) Steps 598(592.18) | Grad Norm 8.3818(5.9309) | Total Time 14.00(14.00)\n",
      "Iter 1481 | Time 58.0772(56.4429) | Bit/dim 3.7746(3.7797) | Xent 1.1961(1.1549) | Loss 4.3727(4.3572) | Error 0.4210(0.4083) Steps 598(592.36) | Grad Norm 12.8699(6.1391) | Total Time 14.00(14.00)\n",
      "Iter 1482 | Time 55.4876(56.4142) | Bit/dim 3.7833(3.7798) | Xent 1.2841(1.1588) | Loss 4.4253(4.3592) | Error 0.4519(0.4096) Steps 604(592.70) | Grad Norm 17.0265(6.4657) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0247 | Time 23.2707, Epoch Time 377.5619(379.9563), Bit/dim 3.7858(best: 3.7749), Xent 1.2797, Loss 4.4256, Error 0.4559(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1483 | Time 54.4638(56.3557) | Bit/dim 3.7860(3.7800) | Xent 1.2900(1.1627) | Loss 4.4310(4.3614) | Error 0.4530(0.4109) Steps 592(592.68) | Grad Norm 14.8882(6.7184) | Total Time 14.00(14.00)\n",
      "Iter 1484 | Time 56.7043(56.3662) | Bit/dim 3.7838(3.7801) | Xent 1.2555(1.1655) | Loss 4.4116(4.3629) | Error 0.4434(0.4118) Steps 592(592.66) | Grad Norm 9.8164(6.8113) | Total Time 14.00(14.00)\n",
      "Iter 1485 | Time 55.6779(56.3455) | Bit/dim 3.7891(3.7804) | Xent 1.2811(1.1689) | Loss 4.4296(4.3649) | Error 0.4564(0.4132) Steps 592(592.64) | Grad Norm 11.5004(6.9520) | Total Time 14.00(14.00)\n",
      "Iter 1486 | Time 57.6520(56.3847) | Bit/dim 3.7807(3.7804) | Xent 1.1954(1.1697) | Loss 4.3784(4.3653) | Error 0.4213(0.4134) Steps 592(592.62) | Grad Norm 4.7334(6.8855) | Total Time 14.00(14.00)\n",
      "Iter 1487 | Time 54.5230(56.3289) | Bit/dim 3.7916(3.7807) | Xent 1.2318(1.1716) | Loss 4.4075(4.3665) | Error 0.4320(0.4140) Steps 592(592.61) | Grad Norm 10.0752(6.9812) | Total Time 14.00(14.00)\n",
      "Iter 1488 | Time 54.7495(56.2815) | Bit/dim 3.7780(3.7807) | Xent 1.2183(1.1730) | Loss 4.3872(4.3672) | Error 0.4301(0.4145) Steps 586(592.41) | Grad Norm 5.6043(6.9398) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0248 | Time 23.4672, Epoch Time 373.7156(379.7691), Bit/dim 3.7895(best: 3.7749), Xent 1.1385, Loss 4.3587, Error 0.4021(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1489 | Time 54.3833(56.2245) | Bit/dim 3.7886(3.7809) | Xent 1.1772(1.1731) | Loss 4.3772(4.3675) | Error 0.4166(0.4145) Steps 604(592.75) | Grad Norm 5.9913(6.9114) | Total Time 14.00(14.00)\n",
      "Iter 1490 | Time 55.9354(56.2159) | Bit/dim 3.7889(3.7811) | Xent 1.2229(1.1746) | Loss 4.4003(4.3685) | Error 0.4321(0.4151) Steps 598(592.91) | Grad Norm 12.4436(7.0774) | Total Time 14.00(14.00)\n",
      "Iter 1491 | Time 57.3460(56.2498) | Bit/dim 3.7912(3.7814) | Xent 1.1891(1.1751) | Loss 4.3857(4.3690) | Error 0.4197(0.4152) Steps 586(592.70) | Grad Norm 8.3213(7.1147) | Total Time 14.00(14.00)\n",
      "Iter 1492 | Time 56.0947(56.2451) | Bit/dim 3.7865(3.7816) | Xent 1.1946(1.1756) | Loss 4.3838(4.3694) | Error 0.4160(0.4152) Steps 592(592.68) | Grad Norm 7.7482(7.1337) | Total Time 14.00(14.00)\n",
      "Iter 1493 | Time 56.6947(56.2586) | Bit/dim 3.7912(3.7819) | Xent 1.1995(1.1764) | Loss 4.3909(4.3701) | Error 0.4246(0.4155) Steps 592(592.66) | Grad Norm 6.0252(7.1004) | Total Time 14.00(14.00)\n",
      "Iter 1494 | Time 56.1242(56.2546) | Bit/dim 3.7836(3.7819) | Xent 1.1894(1.1767) | Loss 4.3783(4.3703) | Error 0.4217(0.4157) Steps 592(592.64) | Grad Norm 6.9057(7.0946) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0249 | Time 23.7315, Epoch Time 377.5833(379.7035), Bit/dim 3.7850(best: 3.7749), Xent 1.1561, Loss 4.3630, Error 0.4133(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1495 | Time 56.3080(56.2562) | Bit/dim 3.7829(3.7820) | Xent 1.1937(1.1773) | Loss 4.3798(4.3706) | Error 0.4244(0.4160) Steps 592(592.62) | Grad Norm 4.5081(7.0170) | Total Time 14.00(14.00)\n",
      "Iter 1496 | Time 54.2507(56.1960) | Bit/dim 3.7855(3.7821) | Xent 1.1857(1.1775) | Loss 4.3784(4.3708) | Error 0.4185(0.4160) Steps 604(592.97) | Grad Norm 6.5280(7.0023) | Total Time 14.00(14.00)\n",
      "Iter 1497 | Time 58.5708(56.2673) | Bit/dim 3.7836(3.7821) | Xent 1.1647(1.1771) | Loss 4.3660(4.3707) | Error 0.4093(0.4158) Steps 598(593.12) | Grad Norm 6.8630(6.9981) | Total Time 14.00(14.00)\n",
      "Iter 1498 | Time 56.7767(56.2825) | Bit/dim 3.7872(3.7823) | Xent 1.1688(1.1769) | Loss 4.3717(4.3707) | Error 0.4170(0.4159) Steps 598(593.26) | Grad Norm 5.3945(6.9500) | Total Time 14.00(14.00)\n",
      "Iter 1499 | Time 58.3653(56.3450) | Bit/dim 3.7773(3.7821) | Xent 1.1545(1.1762) | Loss 4.3546(4.3702) | Error 0.4089(0.4157) Steps 598(593.40) | Grad Norm 6.2163(6.9280) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 54.6798(56.2951) | Bit/dim 3.7750(3.7819) | Xent 1.1591(1.1757) | Loss 4.3545(4.3698) | Error 0.4104(0.4155) Steps 610(593.90) | Grad Norm 7.7962(6.9541) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0250 | Time 23.8737, Epoch Time 380.3152(379.7219), Bit/dim 3.7826(best: 3.7749), Xent 1.1149, Loss 4.3400, Error 0.3970(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1501 | Time 56.9406(56.3144) | Bit/dim 3.7845(3.7820) | Xent 1.1546(1.1751) | Loss 4.3618(4.3695) | Error 0.4065(0.4152) Steps 604(594.21) | Grad Norm 4.1868(6.8710) | Total Time 14.00(14.00)\n",
      "Iter 1502 | Time 56.5022(56.3201) | Bit/dim 3.7809(3.7820) | Xent 1.1396(1.1740) | Loss 4.3507(4.3689) | Error 0.4038(0.4149) Steps 598(594.32) | Grad Norm 4.0637(6.7868) | Total Time 14.00(14.00)\n",
      "Iter 1503 | Time 54.7870(56.2741) | Bit/dim 3.7707(3.7816) | Xent 1.1662(1.1738) | Loss 4.3539(4.3685) | Error 0.4095(0.4147) Steps 592(594.25) | Grad Norm 6.8662(6.7892) | Total Time 14.00(14.00)\n",
      "Iter 1504 | Time 54.2505(56.2134) | Bit/dim 3.7737(3.7814) | Xent 1.1385(1.1727) | Loss 4.3430(4.3677) | Error 0.4014(0.4143) Steps 580(593.82) | Grad Norm 2.8408(6.6708) | Total Time 14.00(14.00)\n",
      "Iter 1505 | Time 57.1546(56.2416) | Bit/dim 3.7802(3.7813) | Xent 1.1597(1.1723) | Loss 4.3600(4.3675) | Error 0.4069(0.4141) Steps 592(593.77) | Grad Norm 4.0123(6.5910) | Total Time 14.00(14.00)\n",
      "Iter 1506 | Time 57.5992(56.2823) | Bit/dim 3.7795(3.7813) | Xent 1.1388(1.1713) | Loss 4.3489(4.3669) | Error 0.4058(0.4138) Steps 592(593.71) | Grad Norm 5.6175(6.5618) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 22.7211, Epoch Time 377.3694(379.6513), Bit/dim 3.7769(best: 3.7749), Xent 1.1042, Loss 4.3290, Error 0.3966(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 54.4660(56.2278) | Bit/dim 3.7792(3.7812) | Xent 1.1347(1.1702) | Loss 4.3465(4.3663) | Error 0.4064(0.4136) Steps 580(593.30) | Grad Norm 4.0417(6.4862) | Total Time 14.00(14.00)\n",
      "Iter 1508 | Time 56.2033(56.2271) | Bit/dim 3.7814(3.7812) | Xent 1.1387(1.1693) | Loss 4.3508(4.3659) | Error 0.4045(0.4133) Steps 586(593.08) | Grad Norm 4.5383(6.4278) | Total Time 14.00(14.00)\n",
      "Iter 1509 | Time 58.1789(56.2857) | Bit/dim 3.7736(3.7810) | Xent 1.1529(1.1688) | Loss 4.3500(4.3654) | Error 0.4096(0.4132) Steps 586(592.87) | Grad Norm 4.5519(6.3715) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 57.7045(56.3282) | Bit/dim 3.7635(3.7805) | Xent 1.1269(1.1675) | Loss 4.3269(4.3642) | Error 0.3940(0.4127) Steps 586(592.67) | Grad Norm 2.4864(6.2549) | Total Time 14.00(14.00)\n",
      "Iter 1511 | Time 56.8923(56.3451) | Bit/dim 3.7733(3.7803) | Xent 1.1502(1.1670) | Loss 4.3484(4.3638) | Error 0.4004(0.4123) Steps 604(593.01) | Grad Norm 5.1914(6.2230) | Total Time 14.00(14.00)\n",
      "Iter 1512 | Time 57.0350(56.3658) | Bit/dim 3.7761(3.7801) | Xent 1.1063(1.1652) | Loss 4.3293(4.3627) | Error 0.3932(0.4117) Steps 592(592.98) | Grad Norm 3.3998(6.1383) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 23.4963, Epoch Time 380.3746(379.6730), Bit/dim 3.7738(best: 3.7749), Xent 1.1046, Loss 4.3261, Error 0.3938(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 56.5463(56.3713) | Bit/dim 3.7652(3.7797) | Xent 1.1430(1.1645) | Loss 4.3367(4.3619) | Error 0.4065(0.4116) Steps 604(593.31) | Grad Norm 4.2032(6.0803) | Total Time 14.00(14.00)\n",
      "Iter 1514 | Time 55.5176(56.3456) | Bit/dim 3.7746(3.7795) | Xent 1.1277(1.1634) | Loss 4.3384(4.3612) | Error 0.3978(0.4111) Steps 604(593.63) | Grad Norm 4.2288(6.0247) | Total Time 14.00(14.00)\n",
      "Iter 1515 | Time 54.9152(56.3027) | Bit/dim 3.7806(3.7796) | Xent 1.1057(1.1617) | Loss 4.3334(4.3604) | Error 0.3895(0.4105) Steps 598(593.76) | Grad Norm 1.4281(5.8868) | Total Time 14.00(14.00)\n",
      "Iter 1516 | Time 59.2569(56.3914) | Bit/dim 3.7731(3.7794) | Xent 1.1341(1.1608) | Loss 4.3401(4.3598) | Error 0.4039(0.4103) Steps 598(593.89) | Grad Norm 2.3403(5.7804) | Total Time 14.00(14.00)\n",
      "Iter 1517 | Time 54.1752(56.3249) | Bit/dim 3.7672(3.7790) | Xent 1.1228(1.1597) | Loss 4.3286(4.3589) | Error 0.3954(0.4099) Steps 604(594.19) | Grad Norm 1.4378(5.6502) | Total Time 14.00(14.00)\n",
      "Iter 1518 | Time 58.2917(56.3839) | Bit/dim 3.7716(3.7788) | Xent 1.1102(1.1582) | Loss 4.3267(4.3579) | Error 0.3928(0.4093) Steps 598(594.30) | Grad Norm 2.4615(5.5545) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 23.1993, Epoch Time 378.3496(379.6333), Bit/dim 3.7714(best: 3.7738), Xent 1.0966, Loss 4.3197, Error 0.3906(best: 0.3914)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 57.6253(56.4211) | Bit/dim 3.7588(3.7782) | Xent 1.1239(1.1572) | Loss 4.3207(4.3568) | Error 0.3970(0.4090) Steps 592(594.23) | Grad Norm 2.9480(5.4763) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 57.2606(56.4463) | Bit/dim 3.7858(3.7784) | Xent 1.1195(1.1561) | Loss 4.3456(4.3564) | Error 0.3991(0.4087) Steps 598(594.35) | Grad Norm 2.8219(5.3967) | Total Time 14.00(14.00)\n",
      "Iter 1521 | Time 57.0047(56.4631) | Bit/dim 3.7672(3.7781) | Xent 1.1287(1.1552) | Loss 4.3316(4.3557) | Error 0.4026(0.4085) Steps 592(594.28) | Grad Norm 4.4290(5.3676) | Total Time 14.00(14.00)\n",
      "Iter 1522 | Time 58.7254(56.5309) | Bit/dim 3.7657(3.7777) | Xent 1.1304(1.1545) | Loss 4.3309(4.3550) | Error 0.3931(0.4080) Steps 598(594.39) | Grad Norm 4.2506(5.3341) | Total Time 14.00(14.00)\n",
      "Iter 1523 | Time 55.9311(56.5129) | Bit/dim 3.7804(3.7778) | Xent 1.1323(1.1538) | Loss 4.3466(4.3547) | Error 0.3964(0.4077) Steps 598(594.50) | Grad Norm 3.1368(5.2682) | Total Time 14.00(14.00)\n",
      "Iter 1524 | Time 57.1972(56.5335) | Bit/dim 3.7703(3.7776) | Xent 1.1120(1.1526) | Loss 4.3263(4.3538) | Error 0.3942(0.4073) Steps 604(594.78) | Grad Norm 3.2589(5.2079) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 23.6748, Epoch Time 384.7626(379.7872), Bit/dim 3.7704(best: 3.7714), Xent 1.0938, Loss 4.3173, Error 0.3930(best: 0.3906)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 57.4266(56.5603) | Bit/dim 3.7736(3.7774) | Xent 1.1131(1.1514) | Loss 4.3301(4.3531) | Error 0.3918(0.4068) Steps 592(594.70) | Grad Norm 2.0684(5.1137) | Total Time 14.00(14.00)\n",
      "Iter 1526 | Time 55.7090(56.5347) | Bit/dim 3.7559(3.7768) | Xent 1.1128(1.1502) | Loss 4.3123(4.3519) | Error 0.3976(0.4065) Steps 598(594.80) | Grad Norm 1.7413(5.0126) | Total Time 14.00(14.00)\n",
      "Iter 1527 | Time 58.2369(56.5858) | Bit/dim 3.7717(3.7766) | Xent 1.1546(1.1504) | Loss 4.3490(4.3518) | Error 0.4083(0.4066) Steps 604(595.07) | Grad Norm 2.8897(4.9489) | Total Time 14.00(14.00)\n",
      "Iter 1528 | Time 57.4965(56.6131) | Bit/dim 3.7747(3.7766) | Xent 1.1133(1.1493) | Loss 4.3314(4.3512) | Error 0.3982(0.4063) Steps 604(595.34) | Grad Norm 2.6631(4.8803) | Total Time 14.00(14.00)\n",
      "Iter 1529 | Time 58.0391(56.6559) | Bit/dim 3.7747(3.7765) | Xent 1.1116(1.1481) | Loss 4.3305(4.3506) | Error 0.3898(0.4058) Steps 592(595.24) | Grad Norm 1.8783(4.7903) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 56.8624(56.6621) | Bit/dim 3.7641(3.7762) | Xent 1.0960(1.1466) | Loss 4.3121(4.3494) | Error 0.3871(0.4053) Steps 598(595.32) | Grad Norm 4.2105(4.7729) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 22.9902, Epoch Time 383.2573(379.8913), Bit/dim 3.7717(best: 3.7704), Xent 1.1047, Loss 4.3240, Error 0.3981(best: 0.3906)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 56.6972(56.6631) | Bit/dim 3.7746(3.7761) | Xent 1.1049(1.1453) | Loss 4.3270(4.3488) | Error 0.3966(0.4050) Steps 598(595.40) | Grad Norm 7.5798(4.8571) | Total Time 14.00(14.00)\n",
      "Iter 1532 | Time 58.2021(56.7093) | Bit/dim 3.7760(3.7761) | Xent 1.1369(1.1451) | Loss 4.3444(4.3486) | Error 0.3994(0.4049) Steps 598(595.48) | Grad Norm 8.5857(4.9689) | Total Time 14.00(14.00)\n",
      "Iter 1533 | Time 56.6008(56.7060) | Bit/dim 3.7602(3.7756) | Xent 1.1460(1.1451) | Loss 4.3332(4.3482) | Error 0.4074(0.4049) Steps 598(595.56) | Grad Norm 8.1492(5.0643) | Total Time 14.00(14.00)\n",
      "Iter 1534 | Time 54.6443(56.6442) | Bit/dim 3.7656(3.7753) | Xent 1.1358(1.1448) | Loss 4.3335(4.3477) | Error 0.4009(0.4048) Steps 592(595.45) | Grad Norm 6.4189(5.1050) | Total Time 14.00(14.00)\n",
      "Iter 1535 | Time 59.2034(56.7210) | Bit/dim 3.7659(3.7750) | Xent 1.1254(1.1442) | Loss 4.3286(4.3472) | Error 0.4020(0.4047) Steps 598(595.53) | Grad Norm 4.4913(5.0866) | Total Time 14.00(14.00)\n",
      "Iter 1536 | Time 56.5334(56.7153) | Bit/dim 3.7705(3.7749) | Xent 1.1151(1.1433) | Loss 4.3280(4.3466) | Error 0.3990(0.4045) Steps 604(595.78) | Grad Norm 2.7812(5.0174) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 23.5536, Epoch Time 383.0645(379.9865), Bit/dim 3.7697(best: 3.7704), Xent 1.0970, Loss 4.3182, Error 0.3932(best: 0.3906)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 53.9702(56.6330) | Bit/dim 3.7787(3.7750) | Xent 1.1380(1.1432) | Loss 4.3477(4.3466) | Error 0.4016(0.4045) Steps 604(596.03) | Grad Norm 6.3389(5.0570) | Total Time 14.00(14.00)\n",
      "Iter 1538 | Time 58.1099(56.6773) | Bit/dim 3.7609(3.7746) | Xent 1.1308(1.1428) | Loss 4.3263(4.3460) | Error 0.4024(0.4044) Steps 604(596.27) | Grad Norm 7.9701(5.1444) | Total Time 14.00(14.00)\n",
      "Iter 1539 | Time 56.3694(56.6681) | Bit/dim 3.7774(3.7747) | Xent 1.1295(1.1424) | Loss 4.3421(4.3459) | Error 0.4000(0.4043) Steps 604(596.50) | Grad Norm 7.8055(5.2243) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 57.8921(56.7048) | Bit/dim 3.7694(3.7745) | Xent 1.1311(1.1421) | Loss 4.3350(4.3456) | Error 0.3981(0.4041) Steps 598(596.54) | Grad Norm 5.1701(5.2226) | Total Time 14.00(14.00)\n",
      "Iter 1541 | Time 57.1201(56.7172) | Bit/dim 3.7620(3.7741) | Xent 1.1078(1.1410) | Loss 4.3159(4.3447) | Error 0.3868(0.4036) Steps 598(596.59) | Grad Norm 5.1913(5.2217) | Total Time 14.00(14.00)\n",
      "Iter 1542 | Time 57.1821(56.7312) | Bit/dim 3.7745(3.7742) | Xent 1.1620(1.1417) | Loss 4.3555(4.3450) | Error 0.4116(0.4038) Steps 598(596.63) | Grad Norm 9.6797(5.3554) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 23.5577, Epoch Time 381.1562(380.0216), Bit/dim 3.7697(best: 3.7697), Xent 1.1058, Loss 4.3226, Error 0.3962(best: 0.3906)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 54.5140(56.6647) | Bit/dim 3.7628(3.7738) | Xent 1.1243(1.1412) | Loss 4.3250(4.3444) | Error 0.4011(0.4037) Steps 598(596.67) | Grad Norm 7.1070(5.4080) | Total Time 14.00(14.00)\n",
      "Iter 1544 | Time 56.5494(56.6612) | Bit/dim 3.7769(3.7739) | Xent 1.1113(1.1403) | Loss 4.3326(4.3440) | Error 0.3985(0.4036) Steps 598(596.71) | Grad Norm 1.6825(5.2962) | Total Time 14.00(14.00)\n",
      "Iter 1545 | Time 56.4033(56.6535) | Bit/dim 3.7609(3.7735) | Xent 1.1238(1.1398) | Loss 4.3227(4.3434) | Error 0.3936(0.4033) Steps 598(596.75) | Grad Norm 7.2624(5.3552) | Total Time 14.00(14.00)\n",
      "Iter 1546 | Time 56.5231(56.6496) | Bit/dim 3.7620(3.7732) | Xent 1.1105(1.1389) | Loss 4.3173(4.3426) | Error 0.3935(0.4030) Steps 598(596.79) | Grad Norm 5.2145(5.3510) | Total Time 14.00(14.00)\n",
      "Iter 1547 | Time 56.8488(56.6555) | Bit/dim 3.7735(3.7732) | Xent 1.1266(1.1385) | Loss 4.3368(4.3424) | Error 0.3975(0.4028) Steps 598(596.82) | Grad Norm 1.4686(5.2345) | Total Time 14.00(14.00)\n",
      "Iter 1548 | Time 57.2590(56.6736) | Bit/dim 3.7709(3.7731) | Xent 1.1222(1.1380) | Loss 4.3321(4.3421) | Error 0.3975(0.4027) Steps 598(596.86) | Grad Norm 4.4611(5.2113) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 23.0248, Epoch Time 378.1868(379.9665), Bit/dim 3.7676(best: 3.7697), Xent 1.0866, Loss 4.3109, Error 0.3891(best: 0.3906)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 55.1726(56.6286) | Bit/dim 3.7648(3.7729) | Xent 1.0966(1.1368) | Loss 4.3131(4.3413) | Error 0.3875(0.4022) Steps 598(596.89) | Grad Norm 3.4844(5.1595) | Total Time 14.00(14.00)\n",
      "Iter 1550 | Time 57.5205(56.6554) | Bit/dim 3.7685(3.7727) | Xent 1.1322(1.1367) | Loss 4.3346(4.3411) | Error 0.3985(0.4021) Steps 604(597.11) | Grad Norm 2.8662(5.0907) | Total Time 14.00(14.00)\n",
      "Iter 1551 | Time 54.8349(56.6008) | Bit/dim 3.7711(3.7727) | Xent 1.1375(1.1367) | Loss 4.3398(4.3410) | Error 0.4031(0.4021) Steps 598(597.13) | Grad Norm 6.0529(5.1196) | Total Time 14.00(14.00)\n",
      "Iter 1552 | Time 55.5208(56.5684) | Bit/dim 3.7665(3.7725) | Xent 1.1292(1.1365) | Loss 4.3311(4.3407) | Error 0.3998(0.4020) Steps 598(597.16) | Grad Norm 7.5636(5.1929) | Total Time 14.00(14.00)\n",
      "Iter 1553 | Time 56.7727(56.5745) | Bit/dim 3.7571(3.7720) | Xent 1.1197(1.1360) | Loss 4.3170(4.3400) | Error 0.3991(0.4020) Steps 598(597.18) | Grad Norm 8.2053(5.2833) | Total Time 14.00(14.00)\n",
      "Iter 1554 | Time 57.5515(56.6038) | Bit/dim 3.7715(3.7720) | Xent 1.1477(1.1363) | Loss 4.3453(4.3402) | Error 0.4019(0.4020) Steps 598(597.21) | Grad Norm 8.6547(5.3844) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 23.2199, Epoch Time 377.7271(379.8993), Bit/dim 3.7687(best: 3.7676), Xent 1.1405, Loss 4.3389, Error 0.4116(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 56.5231(56.6014) | Bit/dim 3.7706(3.7720) | Xent 1.1755(1.1375) | Loss 4.3584(4.3407) | Error 0.4220(0.4026) Steps 610(597.59) | Grad Norm 8.6745(5.4831) | Total Time 14.00(14.00)\n",
      "Iter 1556 | Time 57.7112(56.6347) | Bit/dim 3.7680(3.7719) | Xent 1.1118(1.1367) | Loss 4.3239(4.3402) | Error 0.3939(0.4023) Steps 604(597.78) | Grad Norm 4.3932(5.4504) | Total Time 14.00(14.00)\n",
      "Iter 1557 | Time 58.6825(56.6961) | Bit/dim 3.7542(3.7713) | Xent 1.1380(1.1367) | Loss 4.3232(4.3397) | Error 0.4022(0.4023) Steps 598(597.79) | Grad Norm 6.0295(5.4678) | Total Time 14.00(14.00)\n",
      "Iter 1558 | Time 56.5159(56.6907) | Bit/dim 3.7694(3.7713) | Xent 1.1603(1.1375) | Loss 4.3495(4.3400) | Error 0.4136(0.4026) Steps 592(597.62) | Grad Norm 7.1805(5.5192) | Total Time 14.00(14.00)\n",
      "Iter 1559 | Time 55.8890(56.6666) | Bit/dim 3.7731(3.7713) | Xent 1.1080(1.1366) | Loss 4.3272(4.3396) | Error 0.3900(0.4023) Steps 598(597.63) | Grad Norm 8.1532(5.5982) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 58.3763(56.7179) | Bit/dim 3.7732(3.7714) | Xent 1.1962(1.1384) | Loss 4.3714(4.3406) | Error 0.4155(0.4027) Steps 598(597.64) | Grad Norm 10.7211(5.7519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 23.1007, Epoch Time 383.7579(380.0151), Bit/dim 3.7700(best: 3.7676), Xent 1.0972, Loss 4.3186, Error 0.3917(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 54.8532(56.6620) | Bit/dim 3.7762(3.7715) | Xent 1.1181(1.1378) | Loss 4.3352(4.3404) | Error 0.3992(0.4026) Steps 598(597.65) | Grad Norm 5.1226(5.7330) | Total Time 14.00(14.00)\n",
      "Iter 1562 | Time 56.1457(56.6465) | Bit/dim 3.7637(3.7713) | Xent 1.1619(1.1385) | Loss 4.3446(4.3405) | Error 0.4041(0.4026) Steps 604(597.84) | Grad Norm 8.9932(5.8308) | Total Time 14.00(14.00)\n",
      "Iter 1563 | Time 57.8250(56.6819) | Bit/dim 3.7684(3.7712) | Xent 1.1519(1.1389) | Loss 4.3443(4.3406) | Error 0.4044(0.4027) Steps 598(597.85) | Grad Norm 10.8480(5.9813) | Total Time 14.00(14.00)\n",
      "Iter 1564 | Time 60.6609(56.8012) | Bit/dim 3.7757(3.7713) | Xent 1.1513(1.1393) | Loss 4.3513(4.3410) | Error 0.4080(0.4028) Steps 598(597.85) | Grad Norm 6.7377(6.0040) | Total Time 14.00(14.00)\n",
      "Iter 1565 | Time 56.7571(56.7999) | Bit/dim 3.7697(3.7713) | Xent 1.1444(1.1394) | Loss 4.3419(4.3410) | Error 0.4114(0.4031) Steps 592(597.68) | Grad Norm 7.9134(6.0613) | Total Time 14.00(14.00)\n",
      "Iter 1566 | Time 57.2492(56.8134) | Bit/dim 3.7647(3.7711) | Xent 1.1440(1.1395) | Loss 4.3367(4.3409) | Error 0.4031(0.4031) Steps 598(597.69) | Grad Norm 8.2617(6.1273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 23.6333, Epoch Time 384.0369(380.1358), Bit/dim 3.7734(best: 3.7676), Xent 1.1536, Loss 4.3502, Error 0.4093(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 58.5269(56.8648) | Bit/dim 3.7651(3.7709) | Xent 1.1871(1.1410) | Loss 4.3586(4.3414) | Error 0.4150(0.4034) Steps 604(597.87) | Grad Norm 12.4828(6.3180) | Total Time 14.00(14.00)\n",
      "Iter 1568 | Time 57.5104(56.8842) | Bit/dim 3.7759(3.7711) | Xent 1.2205(1.1434) | Loss 4.3861(4.3427) | Error 0.4355(0.4044) Steps 604(598.06) | Grad Norm 11.3107(6.4678) | Total Time 14.00(14.00)\n",
      "Iter 1569 | Time 57.7999(56.9116) | Bit/dim 3.7704(3.7710) | Xent 1.1518(1.1436) | Loss 4.3463(4.3428) | Error 0.4094(0.4045) Steps 592(597.88) | Grad Norm 4.3883(6.4054) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 57.3137(56.9237) | Bit/dim 3.7749(3.7712) | Xent 1.1716(1.1444) | Loss 4.3607(4.3434) | Error 0.4185(0.4050) Steps 610(598.24) | Grad Norm 8.1568(6.4579) | Total Time 14.00(14.00)\n",
      "Iter 1571 | Time 54.3779(56.8473) | Bit/dim 3.7708(3.7711) | Xent 1.1147(1.1436) | Loss 4.3281(4.3429) | Error 0.3914(0.4046) Steps 604(598.41) | Grad Norm 3.5876(6.3718) | Total Time 14.00(14.00)\n",
      "Iter 1572 | Time 54.5051(56.7771) | Bit/dim 3.7759(3.7713) | Xent 1.1450(1.1436) | Loss 4.3484(4.3431) | Error 0.4065(0.4046) Steps 592(598.22) | Grad Norm 5.7904(6.3544) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 23.5115, Epoch Time 380.5184(380.1472), Bit/dim 3.7679(best: 3.7676), Xent 1.1099, Loss 4.3229, Error 0.3932(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 58.0571(56.8155) | Bit/dim 3.7696(3.7712) | Xent 1.1444(1.1436) | Loss 4.3418(4.3431) | Error 0.4032(0.4046) Steps 586(597.85) | Grad Norm 3.6673(6.2738) | Total Time 14.00(14.00)\n",
      "Iter 1574 | Time 57.3230(56.8307) | Bit/dim 3.7771(3.7714) | Xent 1.1423(1.1436) | Loss 4.3483(4.3432) | Error 0.4081(0.4047) Steps 598(597.86) | Grad Norm 4.5117(6.2209) | Total Time 14.00(14.00)\n",
      "Iter 1575 | Time 55.5901(56.7935) | Bit/dim 3.7727(3.7715) | Xent 1.1313(1.1432) | Loss 4.3383(4.3431) | Error 0.4001(0.4045) Steps 610(598.22) | Grad Norm 3.5162(6.1397) | Total Time 14.00(14.00)\n",
      "Iter 1576 | Time 59.1303(56.8636) | Bit/dim 3.7757(3.7716) | Xent 1.0921(1.1417) | Loss 4.3217(4.3424) | Error 0.3898(0.4041) Steps 598(598.22) | Grad Norm 2.6570(6.0353) | Total Time 14.00(14.00)\n",
      "Iter 1577 | Time 56.1873(56.8433) | Bit/dim 3.7668(3.7714) | Xent 1.1171(1.1409) | Loss 4.3254(4.3419) | Error 0.3944(0.4038) Steps 598(598.21) | Grad Norm 3.6554(5.9639) | Total Time 14.00(14.00)\n",
      "Iter 1578 | Time 56.7226(56.8397) | Bit/dim 3.7610(3.7711) | Xent 1.1059(1.1399) | Loss 4.3139(4.3411) | Error 0.3950(0.4035) Steps 598(598.20) | Grad Norm 3.6895(5.8956) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 22.9697, Epoch Time 383.9876(380.2624), Bit/dim 3.7672(best: 3.7676), Xent 1.0924, Loss 4.3134, Error 0.3910(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 56.1151(56.8179) | Bit/dim 3.7660(3.7710) | Xent 1.1286(1.1396) | Loss 4.3303(4.3407) | Error 0.3976(0.4034) Steps 598(598.20) | Grad Norm 3.5948(5.8266) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 58.1549(56.8580) | Bit/dim 3.7683(3.7709) | Xent 1.0992(1.1383) | Loss 4.3179(4.3401) | Error 0.3926(0.4030) Steps 592(598.01) | Grad Norm 4.0030(5.7719) | Total Time 14.00(14.00)\n",
      "Iter 1581 | Time 57.5429(56.8786) | Bit/dim 3.7726(3.7709) | Xent 1.1238(1.1379) | Loss 4.3345(4.3399) | Error 0.4008(0.4030) Steps 604(598.19) | Grad Norm 6.3352(5.7888) | Total Time 14.00(14.00)\n",
      "Iter 1582 | Time 56.2790(56.8606) | Bit/dim 3.7640(3.7707) | Xent 1.1268(1.1376) | Loss 4.3274(4.3395) | Error 0.3982(0.4028) Steps 610(598.55) | Grad Norm 5.6324(5.7841) | Total Time 14.00(14.00)\n",
      "Iter 1583 | Time 60.9397(56.9830) | Bit/dim 3.7657(3.7706) | Xent 1.1200(1.1370) | Loss 4.3257(4.3391) | Error 0.3924(0.4025) Steps 598(598.53) | Grad Norm 6.0664(5.7926) | Total Time 14.00(14.00)\n",
      "Iter 1584 | Time 55.1967(56.9294) | Bit/dim 3.7574(3.7702) | Xent 1.1265(1.1367) | Loss 4.3207(4.3385) | Error 0.3980(0.4024) Steps 598(598.51) | Grad Norm 4.9948(5.7686) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 23.5977, Epoch Time 384.6862(380.3952), Bit/dim 3.7655(best: 3.7672), Xent 1.0797, Loss 4.3054, Error 0.3865(best: 0.3891)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 58.7689(56.9846) | Bit/dim 3.7786(3.7704) | Xent 1.1214(1.1363) | Loss 4.3393(4.3386) | Error 0.4000(0.4023) Steps 604(598.68) | Grad Norm 2.1107(5.6589) | Total Time 14.00(14.00)\n",
      "Iter 1586 | Time 59.4598(57.0588) | Bit/dim 3.7570(3.7700) | Xent 1.1243(1.1359) | Loss 4.3191(4.3380) | Error 0.4004(0.4023) Steps 598(598.66) | Grad Norm 2.3199(5.5587) | Total Time 14.00(14.00)\n",
      "Iter 1587 | Time 56.9928(57.0568) | Bit/dim 3.7660(3.7699) | Xent 1.1002(1.1348) | Loss 4.3161(4.3373) | Error 0.3874(0.4018) Steps 604(598.82) | Grad Norm 2.2635(5.4599) | Total Time 14.00(14.00)\n",
      "Iter 1588 | Time 55.4551(57.0088) | Bit/dim 3.7652(3.7698) | Xent 1.0896(1.1335) | Loss 4.3100(4.3365) | Error 0.3894(0.4014) Steps 598(598.79) | Grad Norm 1.5581(5.3428) | Total Time 14.00(14.00)\n",
      "Iter 1589 | Time 56.2996(56.9875) | Bit/dim 3.7671(3.7697) | Xent 1.0978(1.1324) | Loss 4.3161(4.3359) | Error 0.3888(0.4011) Steps 604(598.95) | Grad Norm 1.9271(5.2404) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 55.3875(56.9395) | Bit/dim 3.7535(3.7692) | Xent 1.0989(1.1314) | Loss 4.3029(4.3349) | Error 0.3859(0.4006) Steps 604(599.10) | Grad Norm 4.3347(5.2132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 23.2135, Epoch Time 382.8104(380.4676), Bit/dim 3.7661(best: 3.7655), Xent 1.0936, Loss 4.3129, Error 0.3922(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 57.1667(56.9463) | Bit/dim 3.7626(3.7690) | Xent 1.1043(1.1306) | Loss 4.3147(4.3343) | Error 0.3870(0.4002) Steps 592(598.89) | Grad Norm 6.9870(5.2664) | Total Time 14.00(14.00)\n",
      "Iter 1592 | Time 55.1759(56.8932) | Bit/dim 3.7641(3.7689) | Xent 1.1194(1.1303) | Loss 4.3238(4.3340) | Error 0.3986(0.4001) Steps 604(599.04) | Grad Norm 9.2648(5.3864) | Total Time 14.00(14.00)\n",
      "Iter 1593 | Time 55.4151(56.8489) | Bit/dim 3.7680(3.7688) | Xent 1.1310(1.1303) | Loss 4.3335(4.3340) | Error 0.3980(0.4001) Steps 598(599.01) | Grad Norm 7.8558(5.4604) | Total Time 14.00(14.00)\n",
      "Iter 1594 | Time 57.8644(56.8793) | Bit/dim 3.7660(3.7687) | Xent 1.1166(1.1299) | Loss 4.3243(4.3337) | Error 0.3941(0.3999) Steps 598(598.98) | Grad Norm 1.8776(5.3530) | Total Time 14.00(14.00)\n",
      "Iter 1595 | Time 56.8543(56.8786) | Bit/dim 3.7708(3.7688) | Xent 1.1236(1.1297) | Loss 4.3326(4.3336) | Error 0.3968(0.3998) Steps 598(598.95) | Grad Norm 7.3524(5.4129) | Total Time 14.00(14.00)\n",
      "Iter 1596 | Time 58.2148(56.9187) | Bit/dim 3.7527(3.7683) | Xent 1.1522(1.1304) | Loss 4.3288(4.3335) | Error 0.4050(0.4000) Steps 598(598.92) | Grad Norm 12.2182(5.6171) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 23.6723, Epoch Time 381.6740(380.5038), Bit/dim 3.7668(best: 3.7655), Xent 1.1345, Loss 4.3341, Error 0.4057(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 60.0763(57.0134) | Bit/dim 3.7675(3.7683) | Xent 1.1645(1.1314) | Loss 4.3498(4.3340) | Error 0.4115(0.4003) Steps 610(599.25) | Grad Norm 10.0541(5.7502) | Total Time 14.00(14.00)\n",
      "Iter 1598 | Time 57.9606(57.0418) | Bit/dim 3.7633(3.7682) | Xent 1.1374(1.1316) | Loss 4.3320(4.3339) | Error 0.4081(0.4005) Steps 592(599.04) | Grad Norm 7.5730(5.8049) | Total Time 14.00(14.00)\n",
      "Iter 1599 | Time 56.4198(57.0232) | Bit/dim 3.7701(3.7682) | Xent 1.1670(1.1326) | Loss 4.3537(4.3345) | Error 0.4161(0.4010) Steps 604(599.19) | Grad Norm 11.8673(5.9868) | Total Time 14.00(14.00)\n",
      "Iter 1600 | Time 57.6189(57.0410) | Bit/dim 3.7710(3.7683) | Xent 1.1923(1.1344) | Loss 4.3671(4.3355) | Error 0.4220(0.4016) Steps 598(599.15) | Grad Norm 12.1656(6.1721) | Total Time 14.00(14.00)\n",
      "Iter 1601 | Time 56.3833(57.0213) | Bit/dim 3.7668(3.7682) | Xent 1.2160(1.1369) | Loss 4.3748(4.3367) | Error 0.4317(0.4025) Steps 592(598.94) | Grad Norm 13.2348(6.3840) | Total Time 14.00(14.00)\n",
      "Iter 1602 | Time 54.0594(56.9324) | Bit/dim 3.7616(3.7680) | Xent 1.2398(1.1399) | Loss 4.3815(4.3380) | Error 0.4444(0.4038) Steps 586(598.55) | Grad Norm 17.0029(6.7026) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 23.7242, Epoch Time 383.0424(380.5800), Bit/dim 3.7718(best: 3.7655), Xent 1.1788, Loss 4.3612, Error 0.4190(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 60.3615(57.0353) | Bit/dim 3.7774(3.7683) | Xent 1.1956(1.1416) | Loss 4.3752(4.3391) | Error 0.4261(0.4045) Steps 598(598.53) | Grad Norm 9.9859(6.8011) | Total Time 14.00(14.00)\n",
      "Iter 1604 | Time 57.5133(57.0496) | Bit/dim 3.7668(3.7683) | Xent 1.2213(1.1440) | Loss 4.3775(4.3403) | Error 0.4316(0.4053) Steps 610(598.87) | Grad Norm 12.2067(6.9632) | Total Time 14.00(14.00)\n",
      "Iter 1605 | Time 56.5251(57.0339) | Bit/dim 3.7662(3.7682) | Xent 1.2067(1.1459) | Loss 4.3695(4.3412) | Error 0.4297(0.4060) Steps 592(598.67) | Grad Norm 8.9320(7.0223) | Total Time 14.00(14.00)\n",
      "Iter 1606 | Time 56.2397(57.0101) | Bit/dim 3.7680(3.7682) | Xent 1.2197(1.1481) | Loss 4.3778(4.3423) | Error 0.4301(0.4067) Steps 604(598.83) | Grad Norm 9.6022(7.0997) | Total Time 14.00(14.00)\n",
      "Iter 1607 | Time 57.2395(57.0170) | Bit/dim 3.7641(3.7681) | Xent 1.1368(1.1478) | Loss 4.3325(4.3420) | Error 0.3990(0.4065) Steps 598(598.80) | Grad Norm 5.5338(7.0527) | Total Time 14.00(14.00)\n",
      "Iter 1608 | Time 57.0758(57.0187) | Bit/dim 3.7660(3.7680) | Xent 1.2485(1.1508) | Loss 4.3902(4.3434) | Error 0.4403(0.4075) Steps 604(598.96) | Grad Norm 10.5129(7.1565) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 23.3064, Epoch Time 385.0601(380.7144), Bit/dim 3.7668(best: 3.7655), Xent 1.1108, Loss 4.3222, Error 0.3927(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 57.8441(57.0435) | Bit/dim 3.7689(3.7681) | Xent 1.1434(1.1506) | Loss 4.3406(4.3433) | Error 0.4012(0.4073) Steps 604(599.11) | Grad Norm 3.6935(7.0526) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 53.8812(56.9486) | Bit/dim 3.7658(3.7680) | Xent 1.1861(1.1516) | Loss 4.3589(4.3438) | Error 0.4221(0.4078) Steps 598(599.08) | Grad Norm 7.2522(7.0586) | Total Time 14.00(14.00)\n",
      "Iter 1611 | Time 57.0338(56.9512) | Bit/dim 3.7649(3.7679) | Xent 1.1537(1.1517) | Loss 4.3418(4.3437) | Error 0.4034(0.4076) Steps 592(598.87) | Grad Norm 4.9853(6.9964) | Total Time 14.00(14.00)\n",
      "Iter 1612 | Time 56.9512(56.9512) | Bit/dim 3.7604(3.7677) | Xent 1.1245(1.1509) | Loss 4.3226(4.3431) | Error 0.3944(0.4072) Steps 598(598.84) | Grad Norm 4.8137(6.9309) | Total Time 14.00(14.00)\n",
      "Iter 1613 | Time 56.8925(56.9494) | Bit/dim 3.7671(3.7677) | Xent 1.1814(1.1518) | Loss 4.3577(4.3436) | Error 0.4157(0.4075) Steps 610(599.17) | Grad Norm 5.4726(6.8872) | Total Time 14.00(14.00)\n",
      "Iter 1614 | Time 56.9996(56.9509) | Bit/dim 3.7642(3.7676) | Xent 1.1190(1.1508) | Loss 4.3237(4.3430) | Error 0.3945(0.4071) Steps 604(599.32) | Grad Norm 3.7291(6.7924) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 23.3793, Epoch Time 379.8036(380.6870), Bit/dim 3.7687(best: 3.7655), Xent 1.1198, Loss 4.3286, Error 0.4022(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 56.2507(56.9299) | Bit/dim 3.7651(3.7675) | Xent 1.1611(1.1511) | Loss 4.3456(4.3430) | Error 0.4103(0.4072) Steps 598(599.28) | Grad Norm 5.7097(6.7600) | Total Time 14.00(14.00)\n",
      "Iter 1616 | Time 56.1222(56.9057) | Bit/dim 3.7660(3.7674) | Xent 1.1279(1.1504) | Loss 4.3300(4.3426) | Error 0.3974(0.4069) Steps 598(599.24) | Grad Norm 3.2023(6.6532) | Total Time 14.00(14.00)\n",
      "Iter 1617 | Time 54.9721(56.8477) | Bit/dim 3.7593(3.7672) | Xent 1.1204(1.1495) | Loss 4.3195(4.3419) | Error 0.3916(0.4065) Steps 604(599.38) | Grad Norm 3.2055(6.5498) | Total Time 14.00(14.00)\n",
      "Iter 1618 | Time 57.5266(56.8680) | Bit/dim 3.7790(3.7675) | Xent 1.1333(1.1490) | Loss 4.3456(4.3421) | Error 0.4045(0.4064) Steps 604(599.52) | Grad Norm 5.1940(6.5091) | Total Time 14.00(14.00)\n",
      "Iter 1619 | Time 58.4343(56.9150) | Bit/dim 3.7615(3.7674) | Xent 1.1223(1.1482) | Loss 4.3227(4.3415) | Error 0.3948(0.4060) Steps 604(599.66) | Grad Norm 4.7123(6.4552) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 55.7310(56.8795) | Bit/dim 3.7528(3.7669) | Xent 1.1055(1.1470) | Loss 4.3056(4.3404) | Error 0.3950(0.4057) Steps 580(599.07) | Grad Norm 4.2488(6.3890) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 23.6751, Epoch Time 379.1685(380.6415), Bit/dim 3.7634(best: 3.7655), Xent 1.0942, Loss 4.3105, Error 0.3903(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 57.0579(56.8849) | Bit/dim 3.7624(3.7668) | Xent 1.1074(1.1458) | Loss 4.3161(4.3397) | Error 0.3890(0.4052) Steps 598(599.03) | Grad Norm 3.8827(6.3138) | Total Time 14.00(14.00)\n",
      "Iter 1622 | Time 56.9044(56.8855) | Bit/dim 3.7728(3.7670) | Xent 1.1293(1.1453) | Loss 4.3374(4.3396) | Error 0.4051(0.4052) Steps 598(599.00) | Grad Norm 3.5667(6.2314) | Total Time 14.00(14.00)\n",
      "Iter 1623 | Time 56.8745(56.8851) | Bit/dim 3.7707(3.7671) | Xent 1.1311(1.1448) | Loss 4.3363(4.3395) | Error 0.3920(0.4048) Steps 598(598.97) | Grad Norm 3.3399(6.1447) | Total Time 14.00(14.00)\n",
      "Iter 1624 | Time 59.6665(56.9686) | Bit/dim 3.7600(3.7669) | Xent 1.0991(1.1435) | Loss 4.3095(4.3386) | Error 0.3912(0.4044) Steps 598(598.94) | Grad Norm 5.8556(6.1360) | Total Time 14.00(14.00)\n",
      "Iter 1625 | Time 57.4866(56.9841) | Bit/dim 3.7550(3.7665) | Xent 1.1156(1.1426) | Loss 4.3128(4.3378) | Error 0.3986(0.4042) Steps 598(598.92) | Grad Norm 4.1506(6.0764) | Total Time 14.00(14.00)\n",
      "Iter 1626 | Time 57.0713(56.9867) | Bit/dim 3.7651(3.7665) | Xent 1.1027(1.1414) | Loss 4.3165(4.3372) | Error 0.3912(0.4038) Steps 604(599.07) | Grad Norm 4.3527(6.0247) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 23.6033, Epoch Time 385.2022(380.7783), Bit/dim 3.7639(best: 3.7634), Xent 1.0908, Loss 4.3093, Error 0.3882(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 55.2282(56.9340) | Bit/dim 3.7699(3.7666) | Xent 1.1171(1.1407) | Loss 4.3284(4.3369) | Error 0.3914(0.4035) Steps 610(599.40) | Grad Norm 5.7282(6.0158) | Total Time 14.00(14.00)\n",
      "Iter 1628 | Time 56.8778(56.9323) | Bit/dim 3.7636(3.7665) | Xent 1.1036(1.1396) | Loss 4.3154(4.3363) | Error 0.3911(0.4031) Steps 604(599.53) | Grad Norm 5.0873(5.9880) | Total Time 14.00(14.00)\n",
      "Iter 1629 | Time 58.8849(56.9909) | Bit/dim 3.7672(3.7665) | Xent 1.1257(1.1392) | Loss 4.3301(4.3361) | Error 0.3944(0.4028) Steps 610(599.85) | Grad Norm 5.5217(5.9740) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 57.2717(56.9993) | Bit/dim 3.7578(3.7662) | Xent 1.1123(1.1384) | Loss 4.3140(4.3354) | Error 0.3928(0.4025) Steps 604(599.97) | Grad Norm 8.0702(6.0369) | Total Time 14.00(14.00)\n",
      "Iter 1631 | Time 56.1315(56.9733) | Bit/dim 3.7581(3.7660) | Xent 1.1058(1.1374) | Loss 4.3110(4.3347) | Error 0.3961(0.4023) Steps 610(600.27) | Grad Norm 4.6370(5.9949) | Total Time 14.00(14.00)\n",
      "Iter 1632 | Time 58.5741(57.0213) | Bit/dim 3.7603(3.7658) | Xent 1.1223(1.1369) | Loss 4.3214(4.3343) | Error 0.3920(0.4020) Steps 604(600.39) | Grad Norm 5.2787(5.9734) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 23.4082, Epoch Time 383.5261(380.8607), Bit/dim 3.7616(best: 3.7634), Xent 1.1138, Loss 4.3185, Error 0.3969(best: 0.3865)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 55.8394(56.9858) | Bit/dim 3.7535(3.7655) | Xent 1.1361(1.1369) | Loss 4.3215(4.3339) | Error 0.4029(0.4021) Steps 604(600.49) | Grad Norm 9.3255(6.0740) | Total Time 14.00(14.00)\n",
      "Iter 1634 | Time 57.5468(57.0027) | Bit/dim 3.7610(3.7653) | Xent 1.1415(1.1371) | Loss 4.3318(4.3339) | Error 0.4060(0.4022) Steps 610(600.78) | Grad Norm 7.4413(6.1150) | Total Time 14.00(14.00)\n",
      "Iter 1635 | Time 56.0130(56.9730) | Bit/dim 3.7582(3.7651) | Xent 1.1318(1.1369) | Loss 4.3242(4.3336) | Error 0.4032(0.4022) Steps 604(600.88) | Grad Norm 6.5604(6.1283) | Total Time 14.00(14.00)\n",
      "Iter 1636 | Time 57.1988(56.9797) | Bit/dim 3.7627(3.7650) | Xent 1.1333(1.1368) | Loss 4.3293(4.3334) | Error 0.3995(0.4021) Steps 592(600.61) | Grad Norm 9.5083(6.2297) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run2 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs8K_sratio_0_5_drop_0_5_run2/epoch_90_checkpt.pth --seed 2 --conditional True --controlled_tol True --train_mode semisup --lr 0.001 --warmup_iters 1000 --atol 1e-4  --rtol 1e-4 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
