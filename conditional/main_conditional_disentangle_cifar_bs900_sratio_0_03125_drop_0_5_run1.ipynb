{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "import glob\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--load_dir\", type=str, default=None)\n",
      "parser.add_argument(\"--resume_load\", type=int, default=1)\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    \n",
      "    # compute the conditional nll\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # compute the marginal nll\n",
      "    logpz_sup_marginal = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup_marginal.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup_marginal = torch.cat(logpz_sup_marginal,dim=1)\n",
      "    logpz_sup_marginal = torch.logsumexp(logpz_sup_marginal, 1, keepdim=True)\n",
      "    \n",
      "    logpz_marginal = logpz_sup_marginal + logpz_unsup\n",
      "    \n",
      "    logpx_marginal = logpz_marginal - delta_logp\n",
      "\n",
      "    logpx_per_dim_marginal = torch.sum(logpx_marginal) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim_marginal = -(logpx_per_dim_marginal - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted, bits_per_dim_marginal\n",
      "\n",
      "def compute_marginal_bits_per_dim(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    logpz_sup = []\n",
      "    \n",
      "    for indx in range(model.module.y_class):\n",
      "        y_i = torch.full_like(y, indx).to(y)\n",
      "        y_onehot = thops.onehot(y_i, num_classes=model.module.y_class).to(x)\n",
      "        # prior\n",
      "        mean, logs = model.module._prior(y_onehot)\n",
      "        logpz_sup.append(modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1) - torch.log(torch.tensor(model.module.y_class).to(z)))  # logp(z)_sup\n",
      "        \n",
      "    logpz_sup = torch.cat(logpz_sup,dim=1)\n",
      "    logpz_sup = torch.logsumexp(logpz_sup, 1, keepdim=True)\n",
      "    \n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    \n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "    nll_marginal_meter = utils.RunningAverageMeter(0.97) # track negative marginal log-likelihood\n",
      "    \n",
      "    if args.load_dir is not None:\n",
      "        filelist = glob.glob(os.path.join(args.load_dir,\"*epoch_*_checkpt.pth\"))\n",
      "        all_indx = []\n",
      "        for infile in sorted(filelist):\n",
      "            all_indx.append(int(infile.split('_')[-2]))\n",
      "            \n",
      "        indxmax = max(all_indx)\n",
      "        indxstart = max(1, args.resume_load)\n",
      "        \n",
      "        for i in range(indxstart,indxmax+1):\n",
      "            model = create_model(args, data_shape, regularization_fns)\n",
      "            infile = os.path.join(args.load_dir,\"epoch_%i_checkpt.pth\"%i)\n",
      "            print(infile)\n",
      "            checkpt = torch.load(infile, map_location=lambda storage, loc: storage)\n",
      "            model.load_state_dict(checkpt[\"state_dict\"])\n",
      "            \n",
      "            if torch.cuda.is_available():\n",
      "                model = torch.nn.DataParallel(model).cuda()\n",
      "                \n",
      "            model.eval()\n",
      "            \n",
      "            with torch.no_grad():\n",
      "                logger.info(\"validating...\")\n",
      "                losses = []\n",
      "                for (x, y) in test_loader:\n",
      "                    if args.data == \"colormnist\":\n",
      "                        y = y[0]\n",
      "                        \n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    nll = compute_marginal_bits_per_dim(x, y, model)\n",
      "                    losses.append(nll.cpu().numpy())\n",
      "                    \n",
      "                loss = np.mean(losses)\n",
      "                writer.add_scalars('nll_marginal', {'validation': loss}, i)\n",
      "    \n",
      "        raise SystemExit(0)\n",
      "        \n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "        nll_marginal_meter.set(checkpt['bits_per_dim_marginal_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            nll_marginal_meter.update(loss_nll_marginal.item())\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim_marginal', {'train_iter': nll_marginal_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'train_epoch': nll_marginal_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted, loss_nll_marginal = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('bits_per_dim_marginal', {'validation': loss_nll_marginal}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"bits_per_dim_marginal\": loss_nll_marginal,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"bits_per_dim_marginal_train\": nll_marginal_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.03125, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', load_dir=None, log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, resume_load=1, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_03125_drop_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='unsup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=192, bias=True)\n",
      "  (project_class): LinearZeros(in_features=96, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1362358\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 12.9952(33.0883) | Bit/dim 8.9251(9.2403) | Xent 2.3026(2.3026) | Loss 8.9251(9.2403) | Error 0.9011(0.8975) Steps 574(574.00) | Grad Norm 12.5269(16.9655) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 13.1396(27.8499) | Bit/dim 8.5753(9.1060) | Xent 2.3026(2.3026) | Loss 8.5753(9.1060) | Error 0.9000(0.8979) Steps 574(574.00) | Grad Norm 4.7246(14.5970) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 12.9829(23.9951) | Bit/dim 8.4077(8.9371) | Xent 2.3026(2.3026) | Loss 8.4077(8.9371) | Error 0.8889(0.8970) Steps 574(574.00) | Grad Norm 2.9977(11.7042) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 13.4242(21.1758) | Bit/dim 8.2297(8.7728) | Xent 2.3026(2.3026) | Loss 8.2297(8.7728) | Error 0.9089(0.8983) Steps 574(574.00) | Grad Norm 2.8642(9.4620) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 13.4177(19.1118) | Bit/dim 7.9185(8.5922) | Xent 2.3026(2.3026) | Loss 7.9185(8.5922) | Error 0.9000(0.8984) Steps 574(574.00) | Grad Norm 2.4440(7.7244) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 79.9256, Epoch Time 855.6748(855.6748), Bit/dim 7.7886(best: inf), Xent 2.3026, Loss 7.7886, Error 0.9000(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 12.9970(17.5681) | Bit/dim 7.6537(8.3859) | Xent 2.3026(2.3026) | Loss 7.6537(8.3859) | Error 0.9044(0.8992) Steps 574(574.00) | Grad Norm 2.6010(6.3986) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 13.1692(16.4006) | Bit/dim 7.3628(8.1501) | Xent 2.3026(2.3026) | Loss 7.3628(8.1501) | Error 0.9144(0.9007) Steps 574(574.00) | Grad Norm 1.7895(5.3098) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 12.9475(15.5739) | Bit/dim 7.1574(7.9119) | Xent 2.3026(2.3026) | Loss 7.1574(7.9119) | Error 0.8856(0.8996) Steps 574(574.00) | Grad Norm 1.5565(4.3567) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 13.9099(15.1105) | Bit/dim 7.0664(7.7012) | Xent 2.3026(2.3026) | Loss 7.0664(7.7012) | Error 0.8911(0.8994) Steps 598(577.60) | Grad Norm 0.8819(3.4966) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 14.9668(15.1810) | Bit/dim 7.0211(7.5248) | Xent 2.3026(2.3026) | Loss 7.0211(7.5248) | Error 0.8978(0.8999) Steps 610(586.11) | Grad Norm 0.5159(2.7582) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 15.7320(15.2746) | Bit/dim 7.0003(7.3856) | Xent 2.3026(2.3026) | Loss 7.0003(7.3856) | Error 0.9044(0.8998) Steps 622(593.72) | Grad Norm 0.4557(2.1560) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 81.4249, Epoch Time 877.1409(856.3188), Bit/dim 6.9764(best: 7.7886), Xent 2.3026, Loss 6.9764, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 15.9871(15.3928) | Bit/dim 6.9470(7.2754) | Xent 2.3026(2.3026) | Loss 6.9470(7.2754) | Error 0.8933(0.8992) Steps 628(602.57) | Grad Norm 0.3685(1.6969) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 15.8036(15.5122) | Bit/dim 6.8957(7.1801) | Xent 2.3026(2.3026) | Loss 6.8957(7.1801) | Error 0.8933(0.8989) Steps 628(609.25) | Grad Norm 1.1385(1.4435) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 16.2464(15.6945) | Bit/dim 6.8337(7.0962) | Xent 2.3026(2.3026) | Loss 6.8337(7.0962) | Error 0.8822(0.8987) Steps 634(615.32) | Grad Norm 1.4715(1.3253) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.7325(15.9139) | Bit/dim 6.7525(7.0145) | Xent 2.3026(2.3026) | Loss 6.7525(7.0145) | Error 0.8933(0.8993) Steps 640(620.58) | Grad Norm 2.1144(1.3053) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 16.9720(16.0917) | Bit/dim 6.6023(6.9250) | Xent 2.3026(2.3026) | Loss 6.6023(6.9250) | Error 0.9000(0.8995) Steps 640(625.68) | Grad Norm 1.2538(1.5130) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 82.4890, Epoch Time 991.6597(860.3790), Bit/dim 6.4920(best: 6.9764), Xent 2.3026, Loss 6.4920, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 16.4982(16.1685) | Bit/dim 6.3809(6.8131) | Xent 2.3026(2.3026) | Loss 6.3809(6.8131) | Error 0.9078(0.9011) Steps 640(629.27) | Grad Norm 5.7031(4.0632) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 16.7513(16.2761) | Bit/dim 6.1553(6.6763) | Xent 2.3026(2.3026) | Loss 6.1553(6.6763) | Error 0.9000(0.9003) Steps 640(632.08) | Grad Norm 2.9505(8.2266) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.6546(16.5056) | Bit/dim 6.0323(6.5209) | Xent 2.3026(2.3026) | Loss 6.0323(6.5209) | Error 0.8989(0.9011) Steps 670(639.10) | Grad Norm 33.7197(12.0065) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.5095(16.8434) | Bit/dim 5.8172(6.3598) | Xent 2.3026(2.3026) | Loss 5.8172(6.3598) | Error 0.8989(0.9014) Steps 670(647.22) | Grad Norm 14.6284(13.0509) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.0555(16.9809) | Bit/dim 5.6982(6.2072) | Xent 2.3026(2.3026) | Loss 5.6982(6.2072) | Error 0.9100(0.9004) Steps 670(650.85) | Grad Norm 11.2328(12.2270) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 17.3987(17.0417) | Bit/dim 5.6632(6.0692) | Xent 2.3026(2.3026) | Loss 5.6632(6.0692) | Error 0.8800(0.8996) Steps 670(655.46) | Grad Norm 9.3803(11.1968) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 87.5378, Epoch Time 1047.6826(865.9981), Bit/dim 5.6553(best: 6.4920), Xent 2.3026, Loss 5.6553, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.3449(17.1194) | Bit/dim 5.5977(5.9554) | Xent 2.3026(2.3026) | Loss 5.5977(5.9554) | Error 0.9067(0.9008) Steps 676(660.06) | Grad Norm 8.5411(9.4729) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.3835(17.2276) | Bit/dim 5.6168(5.8654) | Xent 2.3026(2.3026) | Loss 5.6168(5.8654) | Error 0.9056(0.9011) Steps 664(663.70) | Grad Norm 8.4424(8.9949) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.7929(17.3710) | Bit/dim 5.5360(5.7879) | Xent 2.3026(2.3026) | Loss 5.5360(5.7879) | Error 0.9200(0.9010) Steps 688(669.01) | Grad Norm 4.0805(8.6380) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 17.1037(17.3864) | Bit/dim 5.5322(5.7298) | Xent 2.3026(2.3026) | Loss 5.5322(5.7298) | Error 0.8967(0.8996) Steps 658(670.65) | Grad Norm 8.2353(10.2070) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 17.0302(17.2044) | Bit/dim 5.5322(5.6780) | Xent 2.3026(2.3026) | Loss 5.5322(5.6780) | Error 0.8900(0.8998) Steps 664(668.28) | Grad Norm 6.6869(9.1154) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 86.2096, Epoch Time 1056.8906(871.7249), Bit/dim 5.5164(best: 5.6553), Xent 2.3026, Loss 5.5164, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.9588(17.1629) | Bit/dim 5.4892(5.6364) | Xent 2.3026(2.3026) | Loss 5.4892(5.6364) | Error 0.8889(0.8985) Steps 670(668.33) | Grad Norm 2.6199(7.8902) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 17.3800(17.1692) | Bit/dim 5.5009(5.5998) | Xent 2.3026(2.3026) | Loss 5.5009(5.5998) | Error 0.8978(0.8984) Steps 670(669.74) | Grad Norm 5.0376(7.4313) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 17.7101(17.2517) | Bit/dim 5.4222(5.5624) | Xent 2.3026(2.3026) | Loss 5.4222(5.5624) | Error 0.9133(0.8983) Steps 682(671.40) | Grad Norm 6.0339(6.4411) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 17.1167(17.2726) | Bit/dim 5.3689(5.5305) | Xent 2.3026(2.3026) | Loss 5.3689(5.5305) | Error 0.8967(0.8990) Steps 682(672.62) | Grad Norm 6.0830(7.1388) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 17.0249(17.1940) | Bit/dim 5.4089(5.4957) | Xent 2.3026(2.3026) | Loss 5.4089(5.4957) | Error 0.9189(0.9002) Steps 676(673.68) | Grad Norm 15.6725(7.0115) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 17.0711(17.1603) | Bit/dim 5.3366(5.4606) | Xent 2.3026(2.3026) | Loss 5.3366(5.4606) | Error 0.9289(0.9007) Steps 676(673.20) | Grad Norm 11.3613(8.3830) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 86.6136, Epoch Time 1049.9762(877.0724), Bit/dim 5.3328(best: 5.5164), Xent 2.3026, Loss 5.3328, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 16.8945(17.1254) | Bit/dim 5.2859(5.4163) | Xent 2.3026(2.3026) | Loss 5.2859(5.4163) | Error 0.9200(0.9011) Steps 676(673.19) | Grad Norm 10.9467(8.5622) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 17.4587(17.1550) | Bit/dim 5.2382(5.3682) | Xent 2.3026(2.3026) | Loss 5.2382(5.3682) | Error 0.9033(0.9016) Steps 682(676.12) | Grad Norm 17.5590(8.9723) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 17.5529(17.1889) | Bit/dim 5.2770(5.3330) | Xent 2.3026(2.3026) | Loss 5.2770(5.3330) | Error 0.9200(0.9009) Steps 694(678.34) | Grad Norm 16.8857(10.7900) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 16.5594(17.1200) | Bit/dim 5.1815(5.2979) | Xent 2.3026(2.3026) | Loss 5.1815(5.2979) | Error 0.8967(0.9004) Steps 682(680.22) | Grad Norm 12.5565(11.2228) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 16.5792(17.0202) | Bit/dim 5.1411(5.2525) | Xent 2.3026(2.3026) | Loss 5.1411(5.2525) | Error 0.9078(0.8989) Steps 670(680.45) | Grad Norm 5.1935(9.7899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 88.1926, Epoch Time 1040.6759(881.9806), Bit/dim 5.0680(best: 5.3328), Xent 2.3026, Loss 5.0680, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 16.4236(16.8760) | Bit/dim 5.0337(5.2087) | Xent 2.3026(2.3026) | Loss 5.0337(5.2087) | Error 0.9067(0.8995) Steps 670(677.89) | Grad Norm 2.1378(8.0712) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 16.8401(16.8186) | Bit/dim 4.9941(5.1615) | Xent 2.3026(2.3026) | Loss 4.9941(5.1615) | Error 0.8922(0.9013) Steps 670(675.63) | Grad Norm 1.9577(7.4008) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 16.6002(16.8055) | Bit/dim 5.0548(5.1211) | Xent 2.3026(2.3026) | Loss 5.0548(5.1211) | Error 0.8689(0.9003) Steps 664(673.78) | Grad Norm 17.3771(7.4033) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 17.1988(16.7789) | Bit/dim 4.9709(5.0860) | Xent 2.3026(2.3026) | Loss 4.9709(5.0860) | Error 0.9356(0.9014) Steps 670(672.79) | Grad Norm 1.5753(7.9774) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 17.4375(16.8054) | Bit/dim 4.9369(5.0453) | Xent 2.3026(2.3026) | Loss 4.9369(5.0453) | Error 0.8956(0.9006) Steps 676(671.94) | Grad Norm 8.0473(7.3248) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 16.8793(16.8432) | Bit/dim 4.8707(5.0062) | Xent 2.3026(2.3026) | Loss 4.8707(5.0062) | Error 0.8933(0.8990) Steps 670(671.55) | Grad Norm 7.9966(7.0915) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 88.6542, Epoch Time 1028.3223(886.3708), Bit/dim 4.8815(best: 5.0680), Xent 2.3026, Loss 4.8815, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 16.0832(16.7716) | Bit/dim 4.8320(4.9684) | Xent 2.3026(2.3026) | Loss 4.8320(4.9684) | Error 0.8889(0.8985) Steps 658(670.01) | Grad Norm 9.3403(7.0341) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 17.7116(16.8661) | Bit/dim 4.8084(4.9330) | Xent 2.3026(2.3026) | Loss 4.8084(4.9330) | Error 0.9111(0.8987) Steps 676(670.32) | Grad Norm 6.8340(7.0554) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 17.6545(17.0403) | Bit/dim 4.7753(4.8961) | Xent 2.3026(2.3026) | Loss 4.7753(4.8961) | Error 0.8956(0.8983) Steps 688(671.85) | Grad Norm 8.9192(7.8271) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 18.3592(17.2531) | Bit/dim 4.7990(4.8685) | Xent 2.3026(2.3026) | Loss 4.7990(4.8685) | Error 0.8900(0.8983) Steps 688(675.09) | Grad Norm 15.4945(8.7911) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.1057(17.4815) | Bit/dim 4.9319(4.8509) | Xent 2.3026(2.3026) | Loss 4.9319(4.8509) | Error 0.9100(0.9006) Steps 712(680.81) | Grad Norm 36.7280(11.0757) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 91.6120, Epoch Time 1069.9992(891.8797), Bit/dim 4.8602(best: 4.8815), Xent 2.3026, Loss 4.8602, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 18.3400(17.5991) | Bit/dim 4.8641(4.8645) | Xent 2.3026(2.3026) | Loss 4.8641(4.8645) | Error 0.9044(0.9007) Steps 712(687.11) | Grad Norm 14.2702(13.6677) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 17.7560(17.7093) | Bit/dim 4.7861(4.8575) | Xent 2.3026(2.3026) | Loss 4.7861(4.8575) | Error 0.8878(0.9013) Steps 706(691.26) | Grad Norm 6.4008(13.2557) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 18.5675(17.9122) | Bit/dim 4.7373(4.8315) | Xent 2.3026(2.3026) | Loss 4.7373(4.8315) | Error 0.8911(0.9012) Steps 724(697.54) | Grad Norm 3.7604(11.4776) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.4464(17.9978) | Bit/dim 4.7297(4.7975) | Xent 2.3026(2.3026) | Loss 4.7297(4.7975) | Error 0.9000(0.8998) Steps 700(699.57) | Grad Norm 6.7692(9.9181) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.0378(18.1168) | Bit/dim 4.6974(4.7653) | Xent 2.3026(2.3026) | Loss 4.6974(4.7653) | Error 0.8878(0.8993) Steps 694(701.78) | Grad Norm 3.7330(8.5933) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 17.9870(18.2169) | Bit/dim 4.6263(4.7326) | Xent 2.3026(2.3026) | Loss 4.6263(4.7326) | Error 0.9089(0.9001) Steps 706(704.12) | Grad Norm 3.6390(7.4271) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 95.3517, Epoch Time 1120.7554(898.7459), Bit/dim 4.6375(best: 4.8602), Xent 2.3026, Loss 4.6375, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.2720(18.2625) | Bit/dim 4.6396(4.7090) | Xent 2.3026(2.3026) | Loss 4.6396(4.7090) | Error 0.9033(0.8992) Steps 724(706.53) | Grad Norm 13.1124(7.7205) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 18.5274(18.2396) | Bit/dim 4.6821(4.6920) | Xent 2.3026(2.3026) | Loss 4.6821(4.6920) | Error 0.8889(0.9008) Steps 700(707.92) | Grad Norm 17.2954(8.6870) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.5975(18.2489) | Bit/dim 4.6368(4.6904) | Xent 2.3026(2.3026) | Loss 4.6368(4.6904) | Error 0.8811(0.9005) Steps 730(708.89) | Grad Norm 10.8577(9.8209) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.2351(18.3056) | Bit/dim 4.5861(4.6682) | Xent 2.3026(2.3026) | Loss 4.5861(4.6682) | Error 0.9022(0.8995) Steps 730(712.73) | Grad Norm 10.9005(9.4743) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 18.0845(18.3987) | Bit/dim 4.5702(4.6458) | Xent 2.3026(2.3026) | Loss 4.5702(4.6458) | Error 0.8900(0.8995) Steps 724(714.22) | Grad Norm 3.2695(8.0103) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 95.5852, Epoch Time 1127.8299(905.6184), Bit/dim 4.5515(best: 4.6375), Xent 2.3026, Loss 4.5515, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 18.2023(18.5013) | Bit/dim 4.6139(4.6229) | Xent 2.3026(2.3026) | Loss 4.6139(4.6229) | Error 0.9167(0.9006) Steps 730(717.03) | Grad Norm 15.2498(7.3579) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 19.3848(18.5593) | Bit/dim 4.5382(4.6072) | Xent 2.3026(2.3026) | Loss 4.5382(4.6072) | Error 0.8989(0.9001) Steps 724(719.72) | Grad Norm 7.7635(7.5975) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 19.0626(18.5877) | Bit/dim 4.5305(4.5886) | Xent 2.3026(2.3026) | Loss 4.5305(4.5886) | Error 0.9022(0.8998) Steps 724(719.76) | Grad Norm 4.8955(7.9939) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 18.7260(18.5481) | Bit/dim 4.4554(4.5686) | Xent 2.3026(2.3026) | Loss 4.4554(4.5686) | Error 0.9100(0.9001) Steps 706(719.09) | Grad Norm 5.8941(7.0686) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 17.5652(18.4520) | Bit/dim 4.5365(4.5609) | Xent 2.3026(2.3026) | Loss 4.5365(4.5609) | Error 0.9211(0.9003) Steps 706(715.95) | Grad Norm 7.0902(7.8899) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.2929(18.3526) | Bit/dim 4.4985(4.5468) | Xent 2.3026(2.3026) | Loss 4.4985(4.5468) | Error 0.9033(0.9006) Steps 688(710.09) | Grad Norm 6.8783(7.9840) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 88.9076, Epoch Time 1120.5628(912.0668), Bit/dim 4.5221(best: 4.5515), Xent 2.3026, Loss 4.5221, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 17.4408(18.1249) | Bit/dim 4.4941(4.5322) | Xent 2.3026(2.3026) | Loss 4.4941(4.5322) | Error 0.9089(0.9011) Steps 694(704.46) | Grad Norm 10.7542(7.6424) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 17.9111(18.0342) | Bit/dim 4.4646(4.5203) | Xent 2.3026(2.3026) | Loss 4.4646(4.5203) | Error 0.9111(0.8996) Steps 712(703.07) | Grad Norm 3.5391(7.8179) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 17.5725(18.0060) | Bit/dim 4.3970(4.4988) | Xent 2.3026(2.3026) | Loss 4.3970(4.4988) | Error 0.8989(0.8992) Steps 706(703.28) | Grad Norm 1.5636(7.6237) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 17.9979(18.0059) | Bit/dim 4.4959(4.4979) | Xent 2.3026(2.3026) | Loss 4.4959(4.4979) | Error 0.8967(0.8999) Steps 712(703.89) | Grad Norm 11.6784(8.4848) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 17.8075(18.0010) | Bit/dim 4.7846(4.5975) | Xent 2.3026(2.3026) | Loss 4.7846(4.5975) | Error 0.9022(0.8997) Steps 688(705.44) | Grad Norm 11.0893(10.8229) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 90.6393, Epoch Time 1085.9008(917.2818), Bit/dim 4.6362(best: 4.5221), Xent 2.3026, Loss 4.6362, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 18.3608(17.9007) | Bit/dim 4.5556(4.6119) | Xent 2.3026(2.3026) | Loss 4.5556(4.6119) | Error 0.8944(0.9007) Steps 700(700.08) | Grad Norm 3.3993(9.4171) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 16.8563(17.8413) | Bit/dim 4.5205(4.5940) | Xent 2.3026(2.3026) | Loss 4.5205(4.5940) | Error 0.8967(0.9005) Steps 670(695.38) | Grad Norm 1.6489(7.5782) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 17.7283(17.7597) | Bit/dim 4.4787(4.5623) | Xent 2.3026(2.3026) | Loss 4.4787(4.5623) | Error 0.9011(0.8998) Steps 676(689.52) | Grad Norm 1.2805(6.1043) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 18.4763(17.9078) | Bit/dim 4.3825(4.5257) | Xent 2.3026(2.3026) | Loss 4.3825(4.5257) | Error 0.9022(0.9006) Steps 694(692.76) | Grad Norm 1.0122(4.8275) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 17.6198(17.9522) | Bit/dim 4.4124(4.4943) | Xent 2.3026(2.3026) | Loss 4.4124(4.4943) | Error 0.8889(0.9017) Steps 676(691.26) | Grad Norm 0.9745(3.8625) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 18.2106(18.0011) | Bit/dim 4.3518(4.4656) | Xent 2.3026(2.3026) | Loss 4.3518(4.4656) | Error 0.9000(0.8999) Steps 688(690.33) | Grad Norm 1.1400(3.1111) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 90.0608, Epoch Time 1095.2192(922.6199), Bit/dim 4.3744(best: 4.5221), Xent 2.3026, Loss 4.3744, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 18.2329(18.0225) | Bit/dim 4.4086(4.4424) | Xent 2.3026(2.3026) | Loss 4.4086(4.4424) | Error 0.9056(0.8996) Steps 700(691.76) | Grad Norm 1.7543(2.6453) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 18.4276(18.0994) | Bit/dim 4.3568(4.4182) | Xent 2.3026(2.3026) | Loss 4.3568(4.4182) | Error 0.8844(0.9003) Steps 700(692.53) | Grad Norm 1.0922(2.3080) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 18.4613(18.1360) | Bit/dim 4.3420(4.3949) | Xent 2.3026(2.3026) | Loss 4.3420(4.3949) | Error 0.8967(0.8996) Steps 694(694.15) | Grad Norm 1.3440(2.0212) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 18.1650(18.1268) | Bit/dim 4.2994(4.3748) | Xent 2.3026(2.3026) | Loss 4.2994(4.3748) | Error 0.8911(0.9003) Steps 700(694.76) | Grad Norm 3.1068(1.9947) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 18.9449(18.2295) | Bit/dim 4.3038(4.3589) | Xent 2.3026(2.3026) | Loss 4.3038(4.3589) | Error 0.9033(0.9003) Steps 712(697.36) | Grad Norm 3.6645(2.1591) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 92.1945, Epoch Time 1114.4492(928.3748), Bit/dim 4.3034(best: 4.3744), Xent 2.3026, Loss 4.3034, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 18.0261(18.1771) | Bit/dim 4.2955(4.3429) | Xent 2.3026(2.3026) | Loss 4.2955(4.3429) | Error 0.9144(0.9004) Steps 706(696.64) | Grad Norm 6.3524(2.9956) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 17.7860(18.0077) | Bit/dim 4.2457(4.3243) | Xent 2.3026(2.3026) | Loss 4.2457(4.3243) | Error 0.8911(0.9008) Steps 694(695.00) | Grad Norm 3.3055(3.1673) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 18.2543(17.9291) | Bit/dim 4.2570(4.3101) | Xent 2.3026(2.3026) | Loss 4.2570(4.3101) | Error 0.9167(0.9013) Steps 688(693.76) | Grad Norm 5.3806(3.8903) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 18.4800(17.9117) | Bit/dim 4.3157(4.2992) | Xent 2.3026(2.3026) | Loss 4.3157(4.2992) | Error 0.8989(0.9009) Steps 694(694.72) | Grad Norm 10.8615(4.4494) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 18.4141(17.9501) | Bit/dim 4.2306(4.2875) | Xent 2.3026(2.3026) | Loss 4.2306(4.2875) | Error 0.8911(0.9003) Steps 694(695.45) | Grad Norm 3.3307(4.6719) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 18.0388(17.9282) | Bit/dim 4.2413(4.2786) | Xent 2.3026(2.3026) | Loss 4.2413(4.2786) | Error 0.9022(0.8990) Steps 700(697.07) | Grad Norm 5.2116(4.9699) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 94.0960, Epoch Time 1090.3129(933.2329), Bit/dim 4.2267(best: 4.3034), Xent 2.3026, Loss 4.2267, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 19.2935(18.1533) | Bit/dim 4.2345(4.2653) | Xent 2.3026(2.3026) | Loss 4.2345(4.2653) | Error 0.9100(0.8993) Steps 724(704.01) | Grad Norm 8.0833(5.0444) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 19.1007(18.3467) | Bit/dim 4.2288(4.2561) | Xent 2.3026(2.3026) | Loss 4.2288(4.2561) | Error 0.9089(0.9000) Steps 718(707.69) | Grad Norm 3.9585(5.6971) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 17.9813(18.3639) | Bit/dim 4.2014(4.2508) | Xent 2.3026(2.3026) | Loss 4.2014(4.2508) | Error 0.9011(0.9007) Steps 712(710.65) | Grad Norm 4.1849(6.3858) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 19.2608(18.4688) | Bit/dim 4.2222(4.2465) | Xent 2.3026(2.3026) | Loss 4.2222(4.2465) | Error 0.9033(0.8998) Steps 754(716.15) | Grad Norm 7.0604(6.7678) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 18.4046(18.6469) | Bit/dim 4.1809(4.2282) | Xent 2.3026(2.3026) | Loss 4.1809(4.2282) | Error 0.8933(0.8993) Steps 736(724.30) | Grad Norm 2.0627(5.8971) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 94.8607, Epoch Time 1148.7393(939.6981), Bit/dim 4.1568(best: 4.2267), Xent 2.3026, Loss 4.1568, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 19.7111(18.7727) | Bit/dim 4.1672(4.2130) | Xent 2.3026(2.3026) | Loss 4.1672(4.2130) | Error 0.8967(0.8996) Steps 754(729.27) | Grad Norm 8.4139(5.6254) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 19.4528(18.8883) | Bit/dim 4.1543(4.1957) | Xent 2.3026(2.3026) | Loss 4.1543(4.1957) | Error 0.8967(0.8988) Steps 766(734.27) | Grad Norm 3.8496(5.4067) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 19.5468(18.9512) | Bit/dim 4.1599(4.1861) | Xent 2.3026(2.3026) | Loss 4.1599(4.1861) | Error 0.9056(0.8997) Steps 766(738.06) | Grad Norm 6.5155(5.7050) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 19.3907(19.0577) | Bit/dim 4.1211(4.1690) | Xent 2.3026(2.3026) | Loss 4.1211(4.1690) | Error 0.9122(0.8990) Steps 754(742.65) | Grad Norm 6.1247(5.8110) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 19.0232(19.1693) | Bit/dim 4.1311(4.1572) | Xent 2.3026(2.3026) | Loss 4.1311(4.1572) | Error 0.9111(0.9019) Steps 766(748.29) | Grad Norm 6.6365(5.8582) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 19.0182(19.2884) | Bit/dim 4.1031(4.1437) | Xent 2.3026(2.3026) | Loss 4.1031(4.1437) | Error 0.9211(0.9005) Steps 754(753.22) | Grad Norm 7.3053(5.9289) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 98.4497, Epoch Time 1179.6886(946.8978), Bit/dim 4.0938(best: 4.1568), Xent 2.3026, Loss 4.0938, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 19.6701(19.2912) | Bit/dim 4.1539(4.1358) | Xent 2.3026(2.3026) | Loss 4.1539(4.1358) | Error 0.8922(0.9001) Steps 760(757.03) | Grad Norm 10.3429(6.1192) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 19.7656(19.3194) | Bit/dim 4.1070(4.1362) | Xent 2.3026(2.3026) | Loss 4.1070(4.1362) | Error 0.8900(0.9002) Steps 760(757.57) | Grad Norm 4.5256(6.7691) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 19.3943(19.4050) | Bit/dim 4.0817(4.1226) | Xent 2.3026(2.3026) | Loss 4.0817(4.1226) | Error 0.8978(0.8991) Steps 778(762.67) | Grad Norm 4.3318(6.0167) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 19.4658(19.3939) | Bit/dim 4.0340(4.1137) | Xent 2.3026(2.3026) | Loss 4.0340(4.1137) | Error 0.8967(0.8991) Steps 784(766.20) | Grad Norm 2.9557(5.8292) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 20.9136(19.4655) | Bit/dim 4.0357(4.0970) | Xent 2.3026(2.3026) | Loss 4.0357(4.0970) | Error 0.9033(0.9002) Steps 790(770.35) | Grad Norm 4.9031(5.2883) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 101.2640, Epoch Time 1191.1921(954.2267), Bit/dim 4.0426(best: 4.0938), Xent 2.3026, Loss 4.0426, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 20.1453(19.5547) | Bit/dim 4.0350(4.0822) | Xent 2.3026(2.3026) | Loss 4.0350(4.0822) | Error 0.9078(0.9003) Steps 784(774.29) | Grad Norm 2.8891(4.8692) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 19.5971(19.6392) | Bit/dim 4.0194(4.0690) | Xent 2.3026(2.3026) | Loss 4.0194(4.0690) | Error 0.9289(0.8997) Steps 790(776.87) | Grad Norm 2.8841(4.9300) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 19.2332(19.6543) | Bit/dim 4.0146(4.0578) | Xent 2.3026(2.3026) | Loss 4.0146(4.0578) | Error 0.8822(0.8992) Steps 790(779.53) | Grad Norm 2.6366(4.4809) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 19.8570(19.6538) | Bit/dim 3.9959(4.0464) | Xent 2.3026(2.3026) | Loss 3.9959(4.0464) | Error 0.9111(0.9002) Steps 790(781.01) | Grad Norm 5.4202(4.3247) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 18.9814(19.6270) | Bit/dim 4.0299(4.0334) | Xent 2.3026(2.3026) | Loss 4.0299(4.0334) | Error 0.9022(0.9001) Steps 784(782.54) | Grad Norm 2.2750(3.8980) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 19.5085(19.6873) | Bit/dim 4.0062(4.0280) | Xent 2.3026(2.3026) | Loss 4.0062(4.0280) | Error 0.8889(0.9003) Steps 790(784.05) | Grad Norm 2.3159(4.2234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 100.9267, Epoch Time 1204.5616(961.7367), Bit/dim 4.0663(best: 4.0426), Xent 2.3026, Loss 4.0663, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 19.3847(19.6748) | Bit/dim 4.0291(4.0322) | Xent 2.3026(2.3026) | Loss 4.0291(4.0322) | Error 0.8778(0.8983) Steps 766(784.70) | Grad Norm 7.6423(5.3302) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 19.8497(19.7248) | Bit/dim 3.9912(4.0225) | Xent 2.3026(2.3026) | Loss 3.9912(4.0225) | Error 0.9089(0.8994) Steps 802(784.52) | Grad Norm 3.8689(5.0460) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 20.2015(19.7376) | Bit/dim 3.9701(4.0101) | Xent 2.3026(2.3026) | Loss 3.9701(4.0101) | Error 0.9289(0.9006) Steps 784(784.57) | Grad Norm 1.8381(4.4414) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 19.2400(19.7521) | Bit/dim 3.9656(3.9994) | Xent 2.3026(2.3026) | Loss 3.9656(3.9994) | Error 0.9033(0.8998) Steps 784(784.57) | Grad Norm 0.9317(3.7010) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 19.3586(19.7496) | Bit/dim 3.9864(3.9904) | Xent 2.3026(2.3026) | Loss 3.9864(3.9904) | Error 0.8956(0.9013) Steps 778(784.76) | Grad Norm 0.8135(2.9869) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 101.2476, Epoch Time 1207.4678(969.1087), Bit/dim 3.9457(best: 4.0426), Xent 2.3026, Loss 3.9457, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 19.5044(19.7697) | Bit/dim 3.9424(3.9775) | Xent 2.3026(2.3026) | Loss 3.9424(3.9775) | Error 0.8889(0.9008) Steps 784(784.60) | Grad Norm 3.1225(2.6953) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 19.5116(19.7611) | Bit/dim 3.9431(3.9697) | Xent 2.3026(2.3026) | Loss 3.9431(3.9697) | Error 0.9022(0.9008) Steps 784(785.53) | Grad Norm 5.2328(3.1345) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 20.2400(19.7568) | Bit/dim 3.9493(3.9623) | Xent 2.3026(2.3026) | Loss 3.9493(3.9623) | Error 0.8967(0.9008) Steps 796(787.67) | Grad Norm 2.8903(3.2864) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 20.4946(19.7684) | Bit/dim 3.9390(3.9549) | Xent 2.3026(2.3026) | Loss 3.9390(3.9549) | Error 0.8889(0.8996) Steps 802(788.92) | Grad Norm 5.0329(3.5095) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 19.4186(19.7534) | Bit/dim 3.9306(3.9463) | Xent 2.3026(2.3026) | Loss 3.9306(3.9463) | Error 0.8956(0.8985) Steps 790(790.47) | Grad Norm 4.8565(3.5546) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 19.8703(19.7126) | Bit/dim 3.9354(3.9389) | Xent 2.3026(2.3026) | Loss 3.9354(3.9389) | Error 0.9078(0.9003) Steps 784(788.49) | Grad Norm 3.6565(3.9309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 101.0074, Epoch Time 1202.7777(976.1187), Bit/dim 3.9120(best: 3.9457), Xent 2.3026, Loss 3.9120, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 19.4789(19.6623) | Bit/dim 3.8758(3.9301) | Xent 2.3026(2.3026) | Loss 3.8758(3.9301) | Error 0.8833(0.8988) Steps 790(787.53) | Grad Norm 2.7175(3.6073) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 18.5808(19.5557) | Bit/dim 3.9189(3.9246) | Xent 2.3026(2.3026) | Loss 3.9189(3.9246) | Error 0.9044(0.9005) Steps 772(787.29) | Grad Norm 3.2861(3.4840) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 19.9348(19.5504) | Bit/dim 3.9082(3.9211) | Xent 2.3026(2.3026) | Loss 3.9082(3.9211) | Error 0.9144(0.9004) Steps 772(786.62) | Grad Norm 6.9061(4.3164) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 18.8120(19.4589) | Bit/dim 3.9089(3.9254) | Xent 2.3026(2.3026) | Loss 3.9089(3.9254) | Error 0.8922(0.9002) Steps 760(782.06) | Grad Norm 4.6126(4.9221) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 19.3374(19.3602) | Bit/dim 3.8924(3.9191) | Xent 2.3026(2.3026) | Loss 3.8924(3.9191) | Error 0.8922(0.9004) Steps 766(776.32) | Grad Norm 5.1768(4.7745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 95.7169, Epoch Time 1173.9212(982.0528), Bit/dim 3.8856(best: 3.9120), Xent 2.3026, Loss 3.8856, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 18.8927(19.2304) | Bit/dim 3.8981(3.9102) | Xent 2.3026(2.3026) | Loss 3.8981(3.9102) | Error 0.8933(0.8996) Steps 754(771.28) | Grad Norm 1.4099(4.3333) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 19.0255(19.1419) | Bit/dim 3.8448(3.8991) | Xent 2.3026(2.3026) | Loss 3.8448(3.8991) | Error 0.8956(0.8995) Steps 766(768.99) | Grad Norm 2.0642(3.8520) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 19.3808(19.1922) | Bit/dim 3.8577(3.8916) | Xent 2.3026(2.3026) | Loss 3.8577(3.8916) | Error 0.9033(0.9002) Steps 772(768.57) | Grad Norm 2.1489(3.4595) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 19.6758(19.2050) | Bit/dim 3.8351(3.8844) | Xent 2.3026(2.3026) | Loss 3.8351(3.8844) | Error 0.8911(0.8990) Steps 784(769.01) | Grad Norm 4.4968(3.4824) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 18.8052(19.1861) | Bit/dim 3.8943(3.8817) | Xent 2.3026(2.3026) | Loss 3.8943(3.8817) | Error 0.8978(0.8990) Steps 766(769.39) | Grad Norm 2.3842(4.0869) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 18.7650(19.2041) | Bit/dim 3.8803(3.8810) | Xent 2.3026(2.3026) | Loss 3.8803(3.8810) | Error 0.9122(0.9007) Steps 760(768.85) | Grad Norm 4.8570(4.1271) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 97.9203, Epoch Time 1168.5190(987.6468), Bit/dim 3.8609(best: 3.8856), Xent 2.3026, Loss 3.8609, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 20.0064(19.2462) | Bit/dim 3.8490(3.8743) | Xent 2.3026(2.3026) | Loss 3.8490(3.8743) | Error 0.8956(0.9000) Steps 760(768.27) | Grad Norm 2.3083(3.7784) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 19.4576(19.1938) | Bit/dim 3.8517(3.8690) | Xent 2.3026(2.3026) | Loss 3.8517(3.8690) | Error 0.9078(0.9004) Steps 772(766.94) | Grad Norm 2.5633(4.1335) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 19.0789(19.2069) | Bit/dim 3.8439(3.8626) | Xent 2.3026(2.3026) | Loss 3.8439(3.8626) | Error 0.9089(0.8998) Steps 766(767.57) | Grad Norm 1.1555(3.8644) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 19.1562(19.1515) | Bit/dim 3.8275(3.8591) | Xent 2.3026(2.3026) | Loss 3.8275(3.8591) | Error 0.8922(0.8994) Steps 772(767.75) | Grad Norm 7.7165(4.1390) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 18.9443(19.0567) | Bit/dim 3.8474(3.8578) | Xent 2.3026(2.3026) | Loss 3.8474(3.8578) | Error 0.8978(0.8992) Steps 748(766.58) | Grad Norm 3.8291(4.5854) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 95.1927, Epoch Time 1162.2963(992.8863), Bit/dim 3.8380(best: 3.8609), Xent 2.3026, Loss 3.8380, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 19.1246(19.0153) | Bit/dim 3.8291(3.8537) | Xent 2.3026(2.3026) | Loss 3.8291(3.8537) | Error 0.8922(0.9002) Steps 748(761.81) | Grad Norm 5.6643(4.3261) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 18.9620(18.9161) | Bit/dim 3.8041(3.8478) | Xent 2.3026(2.3026) | Loss 3.8041(3.8478) | Error 0.9100(0.9000) Steps 736(757.23) | Grad Norm 3.0109(4.0247) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 18.6175(18.8735) | Bit/dim 3.8250(3.8438) | Xent 2.3026(2.3026) | Loss 3.8250(3.8438) | Error 0.9044(0.9008) Steps 754(754.80) | Grad Norm 3.4489(3.8317) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 18.8379(18.8551) | Bit/dim 3.8058(3.8393) | Xent 2.3026(2.3026) | Loss 3.8058(3.8393) | Error 0.8967(0.9008) Steps 748(753.18) | Grad Norm 1.8305(4.3844) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 19.5902(18.9235) | Bit/dim 3.8125(3.8348) | Xent 2.3026(2.3026) | Loss 3.8125(3.8348) | Error 0.8856(0.9002) Steps 742(752.04) | Grad Norm 2.7988(4.0456) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 18.4285(18.8766) | Bit/dim 3.8458(3.8327) | Xent 2.3026(2.3026) | Loss 3.8458(3.8327) | Error 0.9089(0.9002) Steps 742(749.54) | Grad Norm 3.9023(4.3653) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 96.2951, Epoch Time 1148.7775(997.5630), Bit/dim 3.8176(best: 3.8380), Xent 2.3026, Loss 3.8176, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 19.5761(18.9305) | Bit/dim 3.7669(3.8248) | Xent 2.3026(2.3026) | Loss 3.7669(3.8248) | Error 0.9000(0.8998) Steps 766(749.30) | Grad Norm 5.1375(4.1466) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 18.4525(18.9068) | Bit/dim 3.8344(3.8231) | Xent 2.3026(2.3026) | Loss 3.8344(3.8231) | Error 0.9178(0.9014) Steps 754(750.02) | Grad Norm 6.1392(4.2921) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 19.2020(18.9074) | Bit/dim 3.8270(3.8215) | Xent 2.3026(2.3026) | Loss 3.8270(3.8215) | Error 0.8911(0.9010) Steps 748(749.46) | Grad Norm 8.1438(4.5916) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 18.5344(18.7843) | Bit/dim 3.8149(3.8223) | Xent 2.3026(2.3026) | Loss 3.8149(3.8223) | Error 0.9033(0.9017) Steps 742(748.43) | Grad Norm 2.8021(4.6945) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 18.3305(18.7700) | Bit/dim 3.7975(3.8176) | Xent 2.3026(2.3026) | Loss 3.7975(3.8176) | Error 0.8811(0.8999) Steps 748(748.81) | Grad Norm 2.5561(4.3273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 93.5255, Epoch Time 1144.8205(1001.9807), Bit/dim 3.7993(best: 3.8176), Xent 2.3026, Loss 3.7993, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 18.2281(18.7339) | Bit/dim 3.7909(3.8125) | Xent 2.3026(2.3026) | Loss 3.7909(3.8125) | Error 0.9011(0.8992) Steps 736(747.49) | Grad Norm 9.2771(4.4871) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 18.9377(18.8142) | Bit/dim 3.7888(3.8115) | Xent 2.3026(2.3026) | Loss 3.7888(3.8115) | Error 0.8878(0.8992) Steps 754(747.74) | Grad Norm 2.8231(4.6512) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 18.9702(18.8487) | Bit/dim 3.7641(3.8061) | Xent 2.3026(2.3026) | Loss 3.7641(3.8061) | Error 0.8956(0.8999) Steps 742(747.05) | Grad Norm 3.8107(4.3465) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 18.7252(18.8439) | Bit/dim 3.7901(3.8024) | Xent 2.3026(2.3026) | Loss 3.7901(3.8024) | Error 0.8956(0.8993) Steps 760(748.21) | Grad Norm 6.4506(4.2990) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 19.2311(18.9161) | Bit/dim 3.7821(3.7990) | Xent 2.3026(2.3026) | Loss 3.7821(3.7990) | Error 0.8922(0.8989) Steps 760(747.73) | Grad Norm 2.7656(4.3107) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 19.0389(18.9328) | Bit/dim 3.7721(3.7983) | Xent 2.3026(2.3026) | Loss 3.7721(3.7983) | Error 0.9022(0.9004) Steps 760(748.79) | Grad Norm 0.9605(4.1995) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 94.5508, Epoch Time 1154.3578(1006.5520), Bit/dim 3.7813(best: 3.7993), Xent 2.3026, Loss 3.7813, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 18.5437(18.9230) | Bit/dim 3.7953(3.7957) | Xent 2.3026(2.3026) | Loss 3.7953(3.7957) | Error 0.9033(0.8995) Steps 736(748.62) | Grad Norm 5.7092(4.2906) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 18.8801(18.9271) | Bit/dim 3.7825(3.7966) | Xent 2.3026(2.3026) | Loss 3.7825(3.7966) | Error 0.8911(0.9016) Steps 754(750.56) | Grad Norm 6.1417(4.7708) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 18.2547(18.9352) | Bit/dim 3.7710(3.7971) | Xent 2.3026(2.3026) | Loss 3.7710(3.7971) | Error 0.9011(0.9011) Steps 760(752.01) | Grad Norm 2.6828(4.5740) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 18.6391(18.9962) | Bit/dim 3.7866(3.7908) | Xent 2.3026(2.3026) | Loss 3.7866(3.7908) | Error 0.9011(0.9013) Steps 754(752.32) | Grad Norm 5.9769(4.4428) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 18.7690(19.0166) | Bit/dim 3.7606(3.7820) | Xent 2.3026(2.3026) | Loss 3.7606(3.7820) | Error 0.8911(0.8993) Steps 754(752.92) | Grad Norm 3.3899(4.0118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 94.4607, Epoch Time 1158.7638(1011.1184), Bit/dim 3.7774(best: 3.7813), Xent 2.3026, Loss 3.7774, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 20.1469(19.0732) | Bit/dim 3.7225(3.7766) | Xent 2.3026(2.3026) | Loss 3.7225(3.7766) | Error 0.8844(0.8986) Steps 748(753.15) | Grad Norm 6.2457(4.3636) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 18.5560(19.1147) | Bit/dim 3.7685(3.7732) | Xent 2.3026(2.3026) | Loss 3.7685(3.7732) | Error 0.9056(0.8984) Steps 748(753.45) | Grad Norm 2.2144(4.2620) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 19.1974(19.1167) | Bit/dim 3.7670(3.7745) | Xent 2.3026(2.3026) | Loss 3.7670(3.7745) | Error 0.8967(0.8986) Steps 754(753.13) | Grad Norm 3.7074(4.4980) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 19.5285(19.1238) | Bit/dim 3.7559(3.7711) | Xent 2.3026(2.3026) | Loss 3.7559(3.7711) | Error 0.8967(0.8996) Steps 766(753.50) | Grad Norm 2.2744(4.4303) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 18.8489(19.0926) | Bit/dim 3.7766(3.7689) | Xent 2.3026(2.3026) | Loss 3.7766(3.7689) | Error 0.8956(0.9006) Steps 748(752.51) | Grad Norm 3.6923(4.3313) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 19.2483(19.0423) | Bit/dim 3.7760(3.7689) | Xent 2.3026(2.3026) | Loss 3.7760(3.7689) | Error 0.8933(0.9004) Steps 754(752.46) | Grad Norm 6.5800(4.4012) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 94.9131, Epoch Time 1162.8300(1015.6697), Bit/dim 3.7642(best: 3.7774), Xent 2.3026, Loss 3.7642, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1660 | Time 18.5872(19.0276) | Bit/dim 3.7397(3.7665) | Xent 2.3026(2.3026) | Loss 3.7397(3.7665) | Error 0.8911(0.9009) Steps 742(753.00) | Grad Norm 4.9467(4.4192) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 19.0769(19.0239) | Bit/dim 3.7798(3.7629) | Xent 2.3026(2.3026) | Loss 3.7798(3.7629) | Error 0.9056(0.9006) Steps 742(752.90) | Grad Norm 4.3828(4.1313) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 18.6718(19.1281) | Bit/dim 3.7785(3.7624) | Xent 2.3026(2.3026) | Loss 3.7785(3.7624) | Error 0.8778(0.8996) Steps 748(753.78) | Grad Norm 7.6682(4.2048) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 18.8087(19.2289) | Bit/dim 3.7702(3.7610) | Xent 2.3026(2.3026) | Loss 3.7702(3.7610) | Error 0.9056(0.8998) Steps 754(753.24) | Grad Norm 5.9955(4.6986) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 19.0042(19.1836) | Bit/dim 3.7197(3.7575) | Xent 2.3026(2.3026) | Loss 3.7197(3.7575) | Error 0.8878(0.9001) Steps 742(753.22) | Grad Norm 1.8579(4.2969) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 94.4724, Epoch Time 1168.6804(1020.2601), Bit/dim 3.7446(best: 3.7642), Xent 2.3026, Loss 3.7446, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1710 | Time 19.3288(19.2070) | Bit/dim 3.7406(3.7551) | Xent 2.3026(2.3026) | Loss 3.7406(3.7551) | Error 0.8900(0.8999) Steps 772(753.19) | Grad Norm 5.1523(3.9540) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 19.3961(19.2767) | Bit/dim 3.7282(3.7529) | Xent 2.3026(2.3026) | Loss 3.7282(3.7529) | Error 0.8889(0.8988) Steps 760(755.36) | Grad Norm 4.4287(4.0265) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 19.0707(19.3096) | Bit/dim 3.7366(3.7525) | Xent 2.3026(2.3026) | Loss 3.7366(3.7525) | Error 0.8900(0.8986) Steps 766(755.51) | Grad Norm 5.5943(4.3326) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 19.1127(19.2786) | Bit/dim 3.7381(3.7500) | Xent 2.3026(2.3026) | Loss 3.7381(3.7500) | Error 0.8867(0.8982) Steps 760(756.26) | Grad Norm 3.5686(4.4171) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 19.2208(19.2369) | Bit/dim 3.7304(3.7455) | Xent 2.3026(2.3026) | Loss 3.7304(3.7455) | Error 0.9011(0.8991) Steps 754(756.31) | Grad Norm 4.7213(4.1430) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 19.2102(19.3089) | Bit/dim 3.7326(3.7429) | Xent 2.3026(2.3026) | Loss 3.7326(3.7429) | Error 0.9033(0.9010) Steps 778(760.49) | Grad Norm 3.8722(4.1382) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 94.1924, Epoch Time 1174.7506(1024.8948), Bit/dim 3.7371(best: 3.7446), Xent 2.3026, Loss 3.7371, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1770 | Time 19.5878(19.3421) | Bit/dim 3.7532(3.7419) | Xent 2.3026(2.3026) | Loss 3.7532(3.7419) | Error 0.9033(0.9007) Steps 784(763.24) | Grad Norm 5.3551(4.3190) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 19.7299(19.4262) | Bit/dim 3.7304(3.7401) | Xent 2.3026(2.3026) | Loss 3.7304(3.7401) | Error 0.9122(0.9007) Steps 772(763.25) | Grad Norm 4.4894(4.3647) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 19.4398(19.4470) | Bit/dim 3.7492(3.7394) | Xent 2.3026(2.3026) | Loss 3.7492(3.7394) | Error 0.9156(0.9007) Steps 766(764.44) | Grad Norm 4.7002(4.5486) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 20.0359(19.5142) | Bit/dim 3.6681(3.7338) | Xent 2.3026(2.3026) | Loss 3.6681(3.7338) | Error 0.9078(0.9002) Steps 778(766.02) | Grad Norm 5.1867(4.4171) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 19.6836(19.5109) | Bit/dim 3.7254(3.7342) | Xent 2.3026(2.3026) | Loss 3.7254(3.7342) | Error 0.9022(0.9011) Steps 778(766.44) | Grad Norm 1.9808(4.2967) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 95.0616, Epoch Time 1189.1430(1029.8222), Bit/dim 3.7215(best: 3.7371), Xent 2.3026, Loss 3.7215, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1820 | Time 20.2075(19.5669) | Bit/dim 3.7418(3.7300) | Xent 2.3026(2.3026) | Loss 3.7418(3.7300) | Error 0.9144(0.9005) Steps 784(768.87) | Grad Norm 4.8224(4.1640) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 19.6949(19.6056) | Bit/dim 3.7365(3.7279) | Xent 2.3026(2.3026) | Loss 3.7365(3.7279) | Error 0.9044(0.9003) Steps 778(770.29) | Grad Norm 2.9449(3.9530) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 19.5094(19.6221) | Bit/dim 3.7288(3.7273) | Xent 2.3026(2.3026) | Loss 3.7288(3.7273) | Error 0.9067(0.8999) Steps 760(769.93) | Grad Norm 4.2192(3.9689) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 20.3565(19.7483) | Bit/dim 3.7754(3.7270) | Xent 2.3026(2.3026) | Loss 3.7754(3.7270) | Error 0.9133(0.9003) Steps 772(771.38) | Grad Norm 5.9255(3.8454) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 19.6331(19.6735) | Bit/dim 3.7156(3.7231) | Xent 2.3026(2.3026) | Loss 3.7156(3.7231) | Error 0.8978(0.9000) Steps 772(772.91) | Grad Norm 3.7781(3.9009) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 19.4046(19.6512) | Bit/dim 3.7003(3.7195) | Xent 2.3026(2.3026) | Loss 3.7003(3.7195) | Error 0.8978(0.9000) Steps 778(773.24) | Grad Norm 4.3429(3.7500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 95.5241, Epoch Time 1197.5242(1034.8533), Bit/dim 3.7201(best: 3.7215), Xent 2.3026, Loss 3.7201, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1880 | Time 19.4821(19.5785) | Bit/dim 3.6975(3.7187) | Xent 2.3026(2.3026) | Loss 3.6975(3.7187) | Error 0.8989(0.9005) Steps 778(774.04) | Grad Norm 2.4172(4.1781) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 19.6313(19.6246) | Bit/dim 3.7308(3.7178) | Xent 2.3026(2.3026) | Loss 3.7308(3.7178) | Error 0.9133(0.9005) Steps 784(775.28) | Grad Norm 2.3816(3.9464) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 19.1852(19.5928) | Bit/dim 3.7422(3.7179) | Xent 2.3026(2.3026) | Loss 3.7422(3.7179) | Error 0.8989(0.9008) Steps 784(777.37) | Grad Norm 2.1140(3.5521) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 19.3679(19.5726) | Bit/dim 3.6957(3.7131) | Xent 2.3026(2.3026) | Loss 3.6957(3.7131) | Error 0.8889(0.9001) Steps 778(778.13) | Grad Norm 5.9580(3.9355) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 18.6734(19.5026) | Bit/dim 3.7201(3.7125) | Xent 2.3026(2.3026) | Loss 3.7201(3.7125) | Error 0.9033(0.9004) Steps 766(776.00) | Grad Norm 4.2266(3.8206) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 95.3639, Epoch Time 1183.7926(1039.3215), Bit/dim 3.7052(best: 3.7201), Xent 2.3026, Loss 3.7052, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1930 | Time 19.6804(19.4877) | Bit/dim 3.7000(3.7102) | Xent 2.3026(2.3026) | Loss 3.7000(3.7102) | Error 0.9167(0.9001) Steps 778(774.65) | Grad Norm 6.1493(3.9731) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 19.9881(19.4609) | Bit/dim 3.6909(3.7121) | Xent 2.3026(2.3026) | Loss 3.6909(3.7121) | Error 0.8811(0.8996) Steps 790(776.22) | Grad Norm 4.3418(4.2057) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 19.4030(19.3980) | Bit/dim 3.7275(3.7116) | Xent 2.3026(2.3026) | Loss 3.7275(3.7116) | Error 0.8944(0.9002) Steps 778(774.80) | Grad Norm 2.6808(3.9362) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 18.9230(19.3350) | Bit/dim 3.6866(3.7080) | Xent 2.3026(2.3026) | Loss 3.6866(3.7080) | Error 0.8911(0.8999) Steps 778(774.12) | Grad Norm 3.3492(3.8709) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 19.3376(19.3459) | Bit/dim 3.6744(3.7031) | Xent 2.3026(2.3026) | Loss 3.6744(3.7031) | Error 0.8789(0.8991) Steps 784(775.64) | Grad Norm 2.3843(3.6941) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 19.1984(19.4135) | Bit/dim 3.7104(3.7038) | Xent 2.3026(2.3026) | Loss 3.7104(3.7038) | Error 0.9033(0.8994) Steps 772(776.44) | Grad Norm 2.6586(3.9847) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 96.9101, Epoch Time 1180.7292(1043.5637), Bit/dim 3.7135(best: 3.7052), Xent 2.3026, Loss 3.7135, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1990 | Time 19.0314(19.4024) | Bit/dim 3.6867(3.7057) | Xent 2.3026(2.3026) | Loss 3.6867(3.7057) | Error 0.9067(0.9007) Steps 778(778.59) | Grad Norm 4.9283(4.1333) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 19.2626(19.4662) | Bit/dim 3.7347(3.7084) | Xent 2.3026(2.3026) | Loss 3.7347(3.7084) | Error 0.8922(0.8997) Steps 754(777.66) | Grad Norm 3.4954(4.1828) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 18.6151(19.4055) | Bit/dim 3.7016(3.7064) | Xent 2.3026(2.3026) | Loss 3.7016(3.7064) | Error 0.8967(0.9000) Steps 778(776.26) | Grad Norm 2.3175(3.6997) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 19.7773(19.3993) | Bit/dim 3.6838(3.7014) | Xent 2.3026(2.3026) | Loss 3.6838(3.7014) | Error 0.9044(0.9008) Steps 778(775.65) | Grad Norm 3.4821(3.4622) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 18.2225(19.3577) | Bit/dim 3.6728(3.6968) | Xent 2.3026(2.3026) | Loss 3.6728(3.6968) | Error 0.8800(0.8988) Steps 772(775.78) | Grad Norm 2.8836(3.4296) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 96.8674, Epoch Time 1181.3212(1047.6964), Bit/dim 3.6954(best: 3.7052), Xent 2.3026, Loss 3.6954, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2040 | Time 19.3741(19.3942) | Bit/dim 3.6789(3.6962) | Xent 2.3026(2.3026) | Loss 3.6789(3.6962) | Error 0.9022(0.8997) Steps 772(775.02) | Grad Norm 1.7973(3.5738) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 19.3241(19.3705) | Bit/dim 3.6438(3.6934) | Xent 2.3026(2.3026) | Loss 3.6438(3.6934) | Error 0.8922(0.8994) Steps 778(775.15) | Grad Norm 3.4713(3.7505) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 19.3342(19.2974) | Bit/dim 3.6719(3.6899) | Xent 2.3026(2.3026) | Loss 3.6719(3.6899) | Error 0.9122(0.9001) Steps 784(776.42) | Grad Norm 3.6074(3.6965) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 19.2707(19.2573) | Bit/dim 3.6773(3.6894) | Xent 2.3026(2.3026) | Loss 3.6773(3.6894) | Error 0.8978(0.8999) Steps 796(777.39) | Grad Norm 4.3014(3.4301) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 19.3667(19.2940) | Bit/dim 3.6913(3.6876) | Xent 2.3026(2.3026) | Loss 3.6913(3.6876) | Error 0.9144(0.8992) Steps 784(778.55) | Grad Norm 4.6058(3.9438) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 19.2934(19.2620) | Bit/dim 3.6617(3.6868) | Xent 2.3026(2.3026) | Loss 3.6617(3.6868) | Error 0.8944(0.9000) Steps 778(778.50) | Grad Norm 2.9816(3.7468) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0038 | Time 95.0380, Epoch Time 1171.5436(1051.4118), Bit/dim 3.6804(best: 3.6954), Xent 2.3026, Loss 3.6804, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2100 | Time 19.4229(19.2582) | Bit/dim 3.6756(3.6851) | Xent 2.3026(2.3026) | Loss 3.6756(3.6851) | Error 0.8989(0.9007) Steps 766(776.94) | Grad Norm 4.1932(3.8994) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 19.3688(19.2212) | Bit/dim 3.6552(3.6842) | Xent 2.3026(2.3026) | Loss 3.6552(3.6842) | Error 0.9111(0.9021) Steps 796(777.47) | Grad Norm 6.1859(3.8246) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 19.8544(19.2075) | Bit/dim 3.6700(3.6830) | Xent 2.3026(2.3026) | Loss 3.6700(3.6830) | Error 0.8933(0.9016) Steps 790(776.63) | Grad Norm 3.4058(3.7997) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 18.9639(19.2820) | Bit/dim 3.6709(3.6798) | Xent 2.3026(2.3026) | Loss 3.6709(3.6798) | Error 0.8978(0.9000) Steps 778(777.46) | Grad Norm 5.7796(3.8057) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 18.6423(19.2627) | Bit/dim 3.6763(3.6784) | Xent 2.3026(2.3026) | Loss 3.6763(3.6784) | Error 0.8811(0.8994) Steps 784(778.21) | Grad Norm 2.9390(3.6901) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0039 | Time 95.5469, Epoch Time 1171.8078(1055.0237), Bit/dim 3.6759(best: 3.6804), Xent 2.3026, Loss 3.6759, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2150 | Time 19.6371(19.2422) | Bit/dim 3.6615(3.6771) | Xent 2.3026(2.3026) | Loss 3.6615(3.6771) | Error 0.8878(0.8971) Steps 784(778.17) | Grad Norm 4.9771(3.7366) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 19.3744(19.2607) | Bit/dim 3.7181(3.6736) | Xent 2.3026(2.3026) | Loss 3.7181(3.6736) | Error 0.8833(0.8982) Steps 796(779.31) | Grad Norm 4.5871(3.6288) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 18.7005(19.2783) | Bit/dim 3.6550(3.6725) | Xent 2.3026(2.3026) | Loss 3.6550(3.6725) | Error 0.9122(0.8988) Steps 772(779.58) | Grad Norm 4.8946(3.7215) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 18.9592(19.3053) | Bit/dim 3.6746(3.6734) | Xent 2.3026(2.3026) | Loss 3.6746(3.6734) | Error 0.9089(0.9006) Steps 796(780.02) | Grad Norm 4.5440(3.9388) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 18.9328(19.2688) | Bit/dim 3.6599(3.6741) | Xent 2.3026(2.3026) | Loss 3.6599(3.6741) | Error 0.8900(0.8994) Steps 778(779.54) | Grad Norm 2.3155(3.8274) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 19.0354(19.1539) | Bit/dim 3.6904(3.6753) | Xent 2.3026(2.3026) | Loss 3.6904(3.6753) | Error 0.8867(0.8999) Steps 778(779.93) | Grad Norm 5.7497(3.7080) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0040 | Time 96.2059, Epoch Time 1169.8637(1058.4689), Bit/dim 3.6708(best: 3.6759), Xent 2.3026, Loss 3.6708, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2210 | Time 18.8787(19.1302) | Bit/dim 3.6374(3.6728) | Xent 2.3026(2.3026) | Loss 3.6374(3.6728) | Error 0.8989(0.8990) Steps 790(780.87) | Grad Norm 3.1625(3.6412) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 18.8086(19.1904) | Bit/dim 3.7010(3.6738) | Xent 2.3026(2.3026) | Loss 3.7010(3.6738) | Error 0.9267(0.8994) Steps 766(780.13) | Grad Norm 4.0006(3.9459) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 19.8855(19.2559) | Bit/dim 3.6577(3.6728) | Xent 2.3026(2.3026) | Loss 3.6577(3.6728) | Error 0.9033(0.8988) Steps 772(781.28) | Grad Norm 5.0584(4.0694) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 19.5628(19.2164) | Bit/dim 3.6773(3.6703) | Xent 2.3026(2.3026) | Loss 3.6773(3.6703) | Error 0.9000(0.8995) Steps 778(781.28) | Grad Norm 2.2294(3.5805) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 19.0450(19.2078) | Bit/dim 3.6801(3.6691) | Xent 2.3026(2.3026) | Loss 3.6801(3.6691) | Error 0.8978(0.9003) Steps 778(782.02) | Grad Norm 3.6796(3.6176) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0041 | Time 97.3410, Epoch Time 1171.2269(1061.8517), Bit/dim 3.6647(best: 3.6708), Xent 2.3026, Loss 3.6647, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2260 | Time 18.7860(19.1518) | Bit/dim 3.6664(3.6659) | Xent 2.3026(2.3026) | Loss 3.6664(3.6659) | Error 0.9022(0.9000) Steps 778(781.95) | Grad Norm 3.3704(3.5558) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 18.3530(19.2068) | Bit/dim 3.6618(3.6662) | Xent 2.3026(2.3026) | Loss 3.6618(3.6662) | Error 0.8933(0.9000) Steps 784(781.53) | Grad Norm 1.8987(3.6165) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 19.6417(19.2255) | Bit/dim 3.6685(3.6655) | Xent 2.3026(2.3026) | Loss 3.6685(3.6655) | Error 0.9144(0.9014) Steps 778(782.85) | Grad Norm 2.1605(3.6622) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 19.1353(19.2462) | Bit/dim 3.6330(3.6627) | Xent 2.3026(2.3026) | Loss 3.6330(3.6627) | Error 0.9022(0.9002) Steps 790(784.74) | Grad Norm 3.3425(3.4139) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 18.6235(19.2548) | Bit/dim 3.6339(3.6591) | Xent 2.3026(2.3026) | Loss 3.6339(3.6591) | Error 0.8978(0.9002) Steps 790(785.29) | Grad Norm 2.6738(3.6755) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 19.0202(19.1869) | Bit/dim 3.6558(3.6610) | Xent 2.3026(2.3026) | Loss 3.6558(3.6610) | Error 0.8867(0.8998) Steps 778(783.56) | Grad Norm 6.2455(3.9333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0042 | Time 96.3582, Epoch Time 1171.8478(1065.1515), Bit/dim 3.6584(best: 3.6647), Xent 2.3026, Loss 3.6584, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2320 | Time 18.2805(19.1452) | Bit/dim 3.6457(3.6619) | Xent 2.3026(2.3026) | Loss 3.6457(3.6619) | Error 0.9011(0.8987) Steps 778(783.67) | Grad Norm 3.0913(3.9276) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 20.3584(19.1359) | Bit/dim 3.6591(3.6566) | Xent 2.3026(2.3026) | Loss 3.6591(3.6566) | Error 0.9044(0.8984) Steps 784(783.14) | Grad Norm 2.2197(3.5695) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 18.4993(19.0834) | Bit/dim 3.6558(3.6552) | Xent 2.3026(2.3026) | Loss 3.6558(3.6552) | Error 0.9133(0.8999) Steps 778(781.73) | Grad Norm 2.9440(3.3399) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 19.8134(19.1465) | Bit/dim 3.6403(3.6537) | Xent 2.3026(2.3026) | Loss 3.6403(3.6537) | Error 0.9011(0.9006) Steps 796(782.58) | Grad Norm 3.3304(3.4183) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 19.2869(19.1564) | Bit/dim 3.6465(3.6526) | Xent 2.3026(2.3026) | Loss 3.6465(3.6526) | Error 0.8989(0.9000) Steps 790(783.32) | Grad Norm 1.9659(3.3873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0043 | Time 96.4175, Epoch Time 1165.4780(1068.1613), Bit/dim 3.6521(best: 3.6584), Xent 2.3026, Loss 3.6521, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2370 | Time 18.7603(19.1216) | Bit/dim 3.6637(3.6532) | Xent 2.3026(2.3026) | Loss 3.6637(3.6532) | Error 0.9144(0.9009) Steps 772(782.61) | Grad Norm 2.4698(3.3508) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 18.6984(19.0973) | Bit/dim 3.6511(3.6528) | Xent 2.3026(2.3026) | Loss 3.6511(3.6528) | Error 0.9056(0.9010) Steps 778(783.41) | Grad Norm 2.2650(3.3817) | Total Time 14.00(14.00)\n",
      "Iter 2390 | Time 19.2396(19.1528) | Bit/dim 3.6536(3.6507) | Xent 2.3026(2.3026) | Loss 3.6536(3.6507) | Error 0.8989(0.9005) Steps 778(783.04) | Grad Norm 3.4161(3.0118) | Total Time 14.00(14.00)\n",
      "Iter 2400 | Time 19.0489(19.1466) | Bit/dim 3.6214(3.6501) | Xent 2.3026(2.3026) | Loss 3.6214(3.6501) | Error 0.8978(0.8990) Steps 772(782.50) | Grad Norm 3.9563(3.2387) | Total Time 14.00(14.00)\n",
      "Iter 2410 | Time 19.5691(19.1513) | Bit/dim 3.6416(3.6500) | Xent 2.3026(2.3026) | Loss 3.6416(3.6500) | Error 0.8967(0.8989) Steps 790(781.85) | Grad Norm 3.5070(3.4985) | Total Time 14.00(14.00)\n",
      "Iter 2420 | Time 18.9681(19.1917) | Bit/dim 3.6343(3.6499) | Xent 2.3026(2.3026) | Loss 3.6343(3.6499) | Error 0.9078(0.9007) Steps 778(780.90) | Grad Norm 4.6131(3.5650) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0044 | Time 97.3879, Epoch Time 1169.0341(1071.1875), Bit/dim 3.6491(best: 3.6521), Xent 2.3026, Loss 3.6491, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2430 | Time 19.0650(19.1640) | Bit/dim 3.6309(3.6493) | Xent 2.3026(2.3026) | Loss 3.6309(3.6493) | Error 0.9056(0.9004) Steps 778(780.88) | Grad Norm 4.2194(3.5814) | Total Time 14.00(14.00)\n",
      "Iter 2440 | Time 20.0585(19.1548) | Bit/dim 3.6166(3.6468) | Xent 2.3026(2.3026) | Loss 3.6166(3.6468) | Error 0.8844(0.9004) Steps 808(782.02) | Grad Norm 4.2262(3.5637) | Total Time 14.00(14.00)\n",
      "Iter 2450 | Time 19.5438(19.1442) | Bit/dim 3.6388(3.6439) | Xent 2.3026(2.3026) | Loss 3.6388(3.6439) | Error 0.8900(0.8995) Steps 784(782.33) | Grad Norm 1.9822(3.1774) | Total Time 14.00(14.00)\n",
      "Iter 2460 | Time 19.4674(19.1465) | Bit/dim 3.6240(3.6411) | Xent 2.3026(2.3026) | Loss 3.6240(3.6411) | Error 0.8911(0.8995) Steps 790(783.13) | Grad Norm 1.9661(3.3211) | Total Time 14.00(14.00)\n",
      "Iter 2470 | Time 18.8640(19.1473) | Bit/dim 3.6671(3.6418) | Xent 2.3026(2.3026) | Loss 3.6671(3.6418) | Error 0.9067(0.9007) Steps 784(782.39) | Grad Norm 5.6337(3.4094) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0045 | Time 96.6937, Epoch Time 1166.8554(1074.0576), Bit/dim 3.6494(best: 3.6491), Xent 2.3026, Loss 3.6494, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2480 | Time 19.4499(19.1713) | Bit/dim 3.6339(3.6434) | Xent 2.3026(2.3026) | Loss 3.6339(3.6434) | Error 0.8900(0.8999) Steps 778(782.15) | Grad Norm 3.7384(3.5810) | Total Time 14.00(14.00)\n",
      "Iter 2490 | Time 18.9234(19.2285) | Bit/dim 3.6047(3.6410) | Xent 2.3026(2.3026) | Loss 3.6047(3.6410) | Error 0.8633(0.8982) Steps 772(782.12) | Grad Norm 2.6144(3.3497) | Total Time 14.00(14.00)\n",
      "Iter 2500 | Time 19.1916(19.3029) | Bit/dim 3.6611(3.6412) | Xent 2.3026(2.3026) | Loss 3.6611(3.6412) | Error 0.9144(0.8993) Steps 778(782.45) | Grad Norm 1.4931(3.2334) | Total Time 14.00(14.00)\n",
      "Iter 2510 | Time 18.4719(19.3536) | Bit/dim 3.6524(3.6421) | Xent 2.3026(2.3026) | Loss 3.6524(3.6421) | Error 0.9022(0.8996) Steps 772(781.42) | Grad Norm 2.0376(3.4148) | Total Time 14.00(14.00)\n",
      "Iter 2520 | Time 19.6872(19.3155) | Bit/dim 3.6339(3.6405) | Xent 2.3026(2.3026) | Loss 3.6339(3.6405) | Error 0.8933(0.8995) Steps 778(780.03) | Grad Norm 2.0141(3.6136) | Total Time 14.00(14.00)\n",
      "Iter 2530 | Time 18.8138(19.2428) | Bit/dim 3.6400(3.6366) | Xent 2.3026(2.3026) | Loss 3.6400(3.6366) | Error 0.9156(0.9010) Steps 772(780.49) | Grad Norm 3.7657(3.4272) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0046 | Time 96.0855, Epoch Time 1176.3579(1077.1266), Bit/dim 3.6376(best: 3.6491), Xent 2.3026, Loss 3.6376, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2540 | Time 18.9571(19.1438) | Bit/dim 3.6346(3.6372) | Xent 2.3026(2.3026) | Loss 3.6346(3.6372) | Error 0.8967(0.9019) Steps 784(779.10) | Grad Norm 2.7190(3.5999) | Total Time 14.00(14.00)\n",
      "Iter 2550 | Time 19.4090(19.1280) | Bit/dim 3.6637(3.6388) | Xent 2.3026(2.3026) | Loss 3.6637(3.6388) | Error 0.9100(0.9016) Steps 790(779.16) | Grad Norm 3.2810(3.2369) | Total Time 14.00(14.00)\n",
      "Iter 2560 | Time 18.8630(19.1059) | Bit/dim 3.6090(3.6352) | Xent 2.3026(2.3026) | Loss 3.6090(3.6352) | Error 0.8978(0.9002) Steps 784(779.69) | Grad Norm 2.1821(3.2562) | Total Time 14.00(14.00)\n",
      "Iter 2570 | Time 19.5576(19.1700) | Bit/dim 3.6422(3.6310) | Xent 2.3026(2.3026) | Loss 3.6422(3.6310) | Error 0.9033(0.8999) Steps 772(780.37) | Grad Norm 3.4485(3.3206) | Total Time 14.00(14.00)\n",
      "Iter 2580 | Time 19.3442(19.2475) | Bit/dim 3.6472(3.6315) | Xent 2.3026(2.3026) | Loss 3.6472(3.6315) | Error 0.9122(0.9001) Steps 790(781.53) | Grad Norm 6.5553(3.5087) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0047 | Time 97.8333, Epoch Time 1169.6988(1079.9037), Bit/dim 3.6397(best: 3.6376), Xent 2.3026, Loss 3.6397, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2590 | Time 18.9694(19.1716) | Bit/dim 3.6076(3.6324) | Xent 2.3026(2.3026) | Loss 3.6076(3.6324) | Error 0.8944(0.8994) Steps 778(781.15) | Grad Norm 3.3451(3.6318) | Total Time 14.00(14.00)\n",
      "Iter 2600 | Time 19.1912(19.1976) | Bit/dim 3.6620(3.6328) | Xent 2.3026(2.3026) | Loss 3.6620(3.6328) | Error 0.9089(0.9001) Steps 772(780.44) | Grad Norm 2.9399(3.3377) | Total Time 14.00(14.00)\n",
      "Iter 2610 | Time 18.4086(19.1276) | Bit/dim 3.6011(3.6317) | Xent 2.3026(2.3026) | Loss 3.6011(3.6317) | Error 0.8956(0.9004) Steps 784(781.27) | Grad Norm 4.1172(3.2180) | Total Time 14.00(14.00)\n",
      "Iter 2620 | Time 18.6671(19.1208) | Bit/dim 3.6082(3.6304) | Xent 2.3026(2.3026) | Loss 3.6082(3.6304) | Error 0.9000(0.9001) Steps 790(781.18) | Grad Norm 2.4012(3.3041) | Total Time 14.00(14.00)\n",
      "Iter 2630 | Time 19.4962(19.1750) | Bit/dim 3.6351(3.6290) | Xent 2.3026(2.3026) | Loss 3.6351(3.6290) | Error 0.8978(0.9008) Steps 784(782.23) | Grad Norm 4.7614(3.4881) | Total Time 14.00(14.00)\n",
      "Iter 2640 | Time 19.0077(19.1828) | Bit/dim 3.6362(3.6271) | Xent 2.3026(2.3026) | Loss 3.6362(3.6271) | Error 0.8978(0.8995) Steps 766(781.50) | Grad Norm 5.1528(3.6064) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0048 | Time 96.1527, Epoch Time 1166.5361(1082.5027), Bit/dim 3.6283(best: 3.6376), Xent 2.3026, Loss 3.6283, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2650 | Time 19.6583(19.2329) | Bit/dim 3.6105(3.6265) | Xent 2.3026(2.3026) | Loss 3.6105(3.6265) | Error 0.8900(0.8988) Steps 778(780.27) | Grad Norm 4.9428(3.5850) | Total Time 14.00(14.00)\n",
      "Iter 2660 | Time 19.2748(19.2509) | Bit/dim 3.6008(3.6240) | Xent 2.3026(2.3026) | Loss 3.6008(3.6240) | Error 0.9011(0.8990) Steps 784(779.77) | Grad Norm 3.1252(3.3175) | Total Time 14.00(14.00)\n",
      "Iter 2670 | Time 18.4770(19.2303) | Bit/dim 3.6030(3.6224) | Xent 2.3026(2.3026) | Loss 3.6030(3.6224) | Error 0.8967(0.8995) Steps 772(779.91) | Grad Norm 4.2606(3.3998) | Total Time 14.00(14.00)\n",
      "Iter 2680 | Time 18.7259(19.1826) | Bit/dim 3.6546(3.6235) | Xent 2.3026(2.3026) | Loss 3.6546(3.6235) | Error 0.8989(0.8997) Steps 778(779.40) | Grad Norm 3.6407(3.4306) | Total Time 14.00(14.00)\n",
      "Iter 2690 | Time 18.8902(19.1554) | Bit/dim 3.6596(3.6225) | Xent 2.3026(2.3026) | Loss 3.6596(3.6225) | Error 0.9022(0.8995) Steps 778(779.67) | Grad Norm 4.4630(3.5678) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0049 | Time 97.2455, Epoch Time 1171.2185(1085.1642), Bit/dim 3.6206(best: 3.6283), Xent 2.3026, Loss 3.6206, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2700 | Time 19.0176(19.1970) | Bit/dim 3.6322(3.6235) | Xent 2.3026(2.3026) | Loss 3.6322(3.6235) | Error 0.8867(0.9006) Steps 778(779.27) | Grad Norm 3.7552(3.5618) | Total Time 14.00(14.00)\n",
      "Iter 2710 | Time 18.4015(19.1676) | Bit/dim 3.6213(3.6195) | Xent 2.3026(2.3026) | Loss 3.6213(3.6195) | Error 0.9122(0.8999) Steps 790(780.07) | Grad Norm 2.9122(3.3473) | Total Time 14.00(14.00)\n",
      "Iter 2720 | Time 18.8578(19.1598) | Bit/dim 3.6468(3.6208) | Xent 2.3026(2.3026) | Loss 3.6468(3.6208) | Error 0.9122(0.9010) Steps 772(780.26) | Grad Norm 2.6412(3.3691) | Total Time 14.00(14.00)\n",
      "Iter 2730 | Time 19.4920(19.1044) | Bit/dim 3.6266(3.6205) | Xent 2.3026(2.3026) | Loss 3.6266(3.6205) | Error 0.9000(0.9009) Steps 790(779.62) | Grad Norm 5.4261(3.3077) | Total Time 14.00(14.00)\n",
      "Iter 2740 | Time 18.6983(19.1251) | Bit/dim 3.6129(3.6167) | Xent 2.3026(2.3026) | Loss 3.6129(3.6167) | Error 0.8967(0.9003) Steps 772(778.96) | Grad Norm 2.2917(3.3130) | Total Time 14.00(14.00)\n",
      "Iter 2750 | Time 19.5096(19.1031) | Bit/dim 3.6477(3.6186) | Xent 2.3026(2.3026) | Loss 3.6477(3.6186) | Error 0.8767(0.8994) Steps 778(779.80) | Grad Norm 3.4234(3.3791) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0050 | Time 96.1713, Epoch Time 1165.0952(1087.5621), Bit/dim 3.6146(best: 3.6206), Xent 2.3026, Loss 3.6146, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2760 | Time 19.5455(19.0600) | Bit/dim 3.5782(3.6175) | Xent 2.3026(2.3026) | Loss 3.5782(3.6175) | Error 0.9022(0.9008) Steps 790(779.43) | Grad Norm 5.9774(3.4449) | Total Time 14.00(14.00)\n",
      "Iter 2770 | Time 19.1789(19.1046) | Bit/dim 3.6283(3.6167) | Xent 2.3026(2.3026) | Loss 3.6283(3.6167) | Error 0.8978(0.9003) Steps 778(778.77) | Grad Norm 3.1112(3.3845) | Total Time 14.00(14.00)\n",
      "Iter 2780 | Time 19.6643(19.0585) | Bit/dim 3.5954(3.6152) | Xent 2.3026(2.3026) | Loss 3.5954(3.6152) | Error 0.8856(0.9002) Steps 766(778.63) | Grad Norm 3.3640(3.1100) | Total Time 14.00(14.00)\n",
      "Iter 2790 | Time 19.5642(19.1558) | Bit/dim 3.6270(3.6128) | Xent 2.3026(2.3026) | Loss 3.6270(3.6128) | Error 0.9111(0.8998) Steps 778(779.68) | Grad Norm 3.0886(3.2248) | Total Time 14.00(14.00)\n",
      "Iter 2800 | Time 19.8572(19.1769) | Bit/dim 3.6055(3.6117) | Xent 2.3026(2.3026) | Loss 3.6055(3.6117) | Error 0.9011(0.8988) Steps 772(779.18) | Grad Norm 3.1819(3.0219) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0051 | Time 96.6588, Epoch Time 1168.0488(1089.9767), Bit/dim 3.6131(best: 3.6146), Xent 2.3026, Loss 3.6131, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2810 | Time 19.3525(19.1937) | Bit/dim 3.6189(3.6149) | Xent 2.3026(2.3026) | Loss 3.6189(3.6149) | Error 0.8989(0.8997) Steps 772(778.56) | Grad Norm 3.7619(3.2978) | Total Time 14.00(14.00)\n",
      "Iter 2820 | Time 19.0285(19.1735) | Bit/dim 3.5605(3.6099) | Xent 2.3026(2.3026) | Loss 3.5605(3.6099) | Error 0.8700(0.8991) Steps 784(778.90) | Grad Norm 3.6198(3.2228) | Total Time 14.00(14.00)\n",
      "Iter 2830 | Time 18.8455(19.0885) | Bit/dim 3.6269(3.6126) | Xent 2.3026(2.3026) | Loss 3.6269(3.6126) | Error 0.9078(0.8997) Steps 778(778.52) | Grad Norm 2.7553(3.1522) | Total Time 14.00(14.00)\n",
      "Iter 2840 | Time 19.1317(19.0055) | Bit/dim 3.5804(3.6101) | Xent 2.3026(2.3026) | Loss 3.5804(3.6101) | Error 0.9189(0.9001) Steps 790(778.29) | Grad Norm 4.0473(3.2072) | Total Time 14.00(14.00)\n",
      "Iter 2850 | Time 19.2065(19.0127) | Bit/dim 3.6130(3.6126) | Xent 2.3026(2.3026) | Loss 3.6130(3.6126) | Error 0.9011(0.9004) Steps 778(778.35) | Grad Norm 4.4735(3.4097) | Total Time 14.00(14.00)\n",
      "Iter 2860 | Time 18.8246(19.0288) | Bit/dim 3.5921(3.6097) | Xent 2.3026(2.3026) | Loss 3.5921(3.6097) | Error 0.8944(0.8997) Steps 766(778.73) | Grad Norm 4.3234(3.3022) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0052 | Time 97.0809, Epoch Time 1160.0988(1092.0804), Bit/dim 3.6116(best: 3.6131), Xent 2.3026, Loss 3.6116, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2870 | Time 19.3549(19.0134) | Bit/dim 3.6061(3.6074) | Xent 2.3026(2.3026) | Loss 3.6061(3.6074) | Error 0.9144(0.9000) Steps 778(779.08) | Grad Norm 4.9725(3.4014) | Total Time 14.00(14.00)\n",
      "Iter 2880 | Time 19.1699(19.0541) | Bit/dim 3.5840(3.6052) | Xent 2.3026(2.3026) | Loss 3.5840(3.6052) | Error 0.8967(0.9002) Steps 772(778.82) | Grad Norm 1.5351(3.0880) | Total Time 14.00(14.00)\n",
      "Iter 2890 | Time 19.0586(19.1072) | Bit/dim 3.6315(3.6089) | Xent 2.3026(2.3026) | Loss 3.6315(3.6089) | Error 0.9056(0.9006) Steps 784(778.55) | Grad Norm 6.2350(3.1562) | Total Time 14.00(14.00)\n",
      "Iter 2900 | Time 18.4249(19.0827) | Bit/dim 3.6097(3.6068) | Xent 2.3026(2.3026) | Loss 3.6097(3.6068) | Error 0.9122(0.9008) Steps 760(778.28) | Grad Norm 4.2366(3.3709) | Total Time 14.00(14.00)\n",
      "Iter 2910 | Time 19.0651(19.0540) | Bit/dim 3.6153(3.6072) | Xent 2.3026(2.3026) | Loss 3.6153(3.6072) | Error 0.9056(0.9005) Steps 772(778.11) | Grad Norm 1.9386(3.0679) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0053 | Time 96.5816, Epoch Time 1163.0604(1094.2098), Bit/dim 3.6042(best: 3.6116), Xent 2.3026, Loss 3.6042, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2920 | Time 18.5515(19.0461) | Bit/dim 3.5951(3.6053) | Xent 2.3026(2.3026) | Loss 3.5951(3.6053) | Error 0.8978(0.8987) Steps 790(777.98) | Grad Norm 2.2996(2.9241) | Total Time 14.00(14.00)\n",
      "Iter 2930 | Time 19.0402(19.0156) | Bit/dim 3.5952(3.6044) | Xent 2.3026(2.3026) | Loss 3.5952(3.6044) | Error 0.8978(0.8984) Steps 778(777.22) | Grad Norm 2.5487(3.1634) | Total Time 14.00(14.00)\n",
      "Iter 2940 | Time 18.6382(19.0436) | Bit/dim 3.5961(3.6035) | Xent 2.3026(2.3026) | Loss 3.5961(3.6035) | Error 0.8933(0.8996) Steps 784(776.64) | Grad Norm 2.1116(3.2493) | Total Time 14.00(14.00)\n",
      "Iter 2950 | Time 18.9882(19.0851) | Bit/dim 3.5927(3.6003) | Xent 2.3026(2.3026) | Loss 3.5927(3.6003) | Error 0.9056(0.8991) Steps 760(776.85) | Grad Norm 1.6726(3.3252) | Total Time 14.00(14.00)\n",
      "Iter 2960 | Time 19.9126(19.0992) | Bit/dim 3.6231(3.6008) | Xent 2.3026(2.3026) | Loss 3.6231(3.6008) | Error 0.9022(0.9011) Steps 790(776.73) | Grad Norm 2.7479(3.2516) | Total Time 14.00(14.00)\n",
      "Iter 2970 | Time 19.3013(19.1389) | Bit/dim 3.5774(3.6016) | Xent 2.3026(2.3026) | Loss 3.5774(3.6016) | Error 0.8967(0.9007) Steps 766(776.47) | Grad Norm 2.7463(3.1737) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0054 | Time 96.1236, Epoch Time 1166.2766(1096.3718), Bit/dim 3.5998(best: 3.6042), Xent 2.3026, Loss 3.5998, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 2980 | Time 18.1994(19.0952) | Bit/dim 3.5795(3.6002) | Xent 2.3026(2.3026) | Loss 3.5795(3.6002) | Error 0.8800(0.8995) Steps 766(776.53) | Grad Norm 3.1467(3.1040) | Total Time 14.00(14.00)\n",
      "Iter 2990 | Time 18.4802(19.0722) | Bit/dim 3.5977(3.6015) | Xent 2.3026(2.3026) | Loss 3.5977(3.6015) | Error 0.9100(0.9009) Steps 778(776.49) | Grad Norm 3.5239(3.1316) | Total Time 14.00(14.00)\n",
      "Iter 3000 | Time 19.2523(19.0910) | Bit/dim 3.6027(3.6001) | Xent 2.3026(2.3026) | Loss 3.6027(3.6001) | Error 0.9000(0.8997) Steps 772(776.84) | Grad Norm 2.2773(2.8431) | Total Time 14.00(14.00)\n",
      "Iter 3010 | Time 18.8112(19.0940) | Bit/dim 3.5513(3.5989) | Xent 2.3026(2.3026) | Loss 3.5513(3.5989) | Error 0.9078(0.9014) Steps 766(775.84) | Grad Norm 3.4310(3.0528) | Total Time 14.00(14.00)\n",
      "Iter 3020 | Time 19.6201(19.1595) | Bit/dim 3.6231(3.5977) | Xent 2.3026(2.3026) | Loss 3.6231(3.5977) | Error 0.9011(0.9008) Steps 778(777.33) | Grad Norm 3.8035(3.1801) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0055 | Time 98.4243, Epoch Time 1169.7555(1098.5733), Bit/dim 3.5986(best: 3.5998), Xent 2.3026, Loss 3.5986, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3030 | Time 18.8764(19.1889) | Bit/dim 3.5758(3.5965) | Xent 2.3026(2.3026) | Loss 3.5758(3.5965) | Error 0.8967(0.9002) Steps 772(777.18) | Grad Norm 2.4518(3.2076) | Total Time 14.00(14.00)\n",
      "Iter 3040 | Time 18.5188(19.1437) | Bit/dim 3.6026(3.5947) | Xent 2.3026(2.3026) | Loss 3.6026(3.5947) | Error 0.9100(0.9009) Steps 790(777.87) | Grad Norm 2.3855(3.1859) | Total Time 14.00(14.00)\n",
      "Iter 3050 | Time 19.2669(19.1197) | Bit/dim 3.5821(3.5934) | Xent 2.3026(2.3026) | Loss 3.5821(3.5934) | Error 0.8878(0.8998) Steps 790(777.20) | Grad Norm 4.5158(3.3664) | Total Time 14.00(14.00)\n",
      "Iter 3060 | Time 19.6963(19.0868) | Bit/dim 3.6059(3.5937) | Xent 2.3026(2.3026) | Loss 3.6059(3.5937) | Error 0.8867(0.9007) Steps 796(776.40) | Grad Norm 3.0406(3.1349) | Total Time 14.00(14.00)\n",
      "Iter 3070 | Time 19.3964(19.0009) | Bit/dim 3.5819(3.5920) | Xent 2.3026(2.3026) | Loss 3.5819(3.5920) | Error 0.8967(0.8993) Steps 790(776.02) | Grad Norm 2.4242(3.1026) | Total Time 14.00(14.00)\n",
      "Iter 3080 | Time 19.3077(19.0080) | Bit/dim 3.6235(3.5931) | Xent 2.3026(2.3026) | Loss 3.6235(3.5931) | Error 0.9056(0.8996) Steps 778(776.63) | Grad Norm 2.6810(3.1785) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0056 | Time 97.0965, Epoch Time 1158.8007(1100.3801), Bit/dim 3.5911(best: 3.5986), Xent 2.3026, Loss 3.5911, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3090 | Time 19.1635(19.0252) | Bit/dim 3.5944(3.5914) | Xent 2.3026(2.3026) | Loss 3.5944(3.5914) | Error 0.9111(0.8994) Steps 784(777.03) | Grad Norm 2.9239(3.0246) | Total Time 14.00(14.00)\n",
      "Iter 3100 | Time 18.6973(18.9894) | Bit/dim 3.6064(3.5952) | Xent 2.3026(2.3026) | Loss 3.6064(3.5952) | Error 0.9033(0.9014) Steps 790(776.87) | Grad Norm 3.8155(3.2310) | Total Time 14.00(14.00)\n",
      "Iter 3110 | Time 19.3205(19.0114) | Bit/dim 3.5744(3.5961) | Xent 2.3026(2.3026) | Loss 3.5744(3.5961) | Error 0.9133(0.9019) Steps 772(775.82) | Grad Norm 2.5055(3.0808) | Total Time 14.00(14.00)\n",
      "Iter 3120 | Time 19.9260(19.0715) | Bit/dim 3.5858(3.5915) | Xent 2.3026(2.3026) | Loss 3.5858(3.5915) | Error 0.9000(0.9002) Steps 796(777.05) | Grad Norm 4.8857(3.2089) | Total Time 14.00(14.00)\n",
      "Iter 3130 | Time 19.2377(19.0805) | Bit/dim 3.5733(3.5900) | Xent 2.3026(2.3026) | Loss 3.5733(3.5900) | Error 0.8822(0.8996) Steps 766(776.85) | Grad Norm 2.2232(3.1042) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0057 | Time 97.7251, Epoch Time 1166.6523(1102.3683), Bit/dim 3.5883(best: 3.5911), Xent 2.3026, Loss 3.5883, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3140 | Time 19.6369(19.1922) | Bit/dim 3.5815(3.5884) | Xent 2.3026(2.3026) | Loss 3.5815(3.5884) | Error 0.9067(0.9006) Steps 766(776.86) | Grad Norm 1.6184(3.0169) | Total Time 14.00(14.00)\n",
      "Iter 3150 | Time 18.8594(19.1910) | Bit/dim 3.5939(3.5858) | Xent 2.3026(2.3026) | Loss 3.5939(3.5858) | Error 0.9122(0.9000) Steps 784(776.72) | Grad Norm 1.7800(2.7132) | Total Time 14.00(14.00)\n",
      "Iter 3160 | Time 19.6990(19.2841) | Bit/dim 3.5995(3.5843) | Xent 2.3026(2.3026) | Loss 3.5995(3.5843) | Error 0.9000(0.8993) Steps 772(778.65) | Grad Norm 3.2198(3.0429) | Total Time 14.00(14.00)\n",
      "Iter 3170 | Time 19.9122(19.3201) | Bit/dim 3.6052(3.5856) | Xent 2.3026(2.3026) | Loss 3.6052(3.5856) | Error 0.8956(0.8988) Steps 772(777.73) | Grad Norm 3.0700(3.0924) | Total Time 14.00(14.00)\n",
      "Iter 3180 | Time 19.5622(19.2991) | Bit/dim 3.5964(3.5857) | Xent 2.3026(2.3026) | Loss 3.5964(3.5857) | Error 0.9000(0.8989) Steps 784(778.84) | Grad Norm 2.8762(3.1982) | Total Time 14.00(14.00)\n",
      "Iter 3190 | Time 19.5379(19.2553) | Bit/dim 3.6171(3.5889) | Xent 2.3026(2.3026) | Loss 3.6171(3.5889) | Error 0.9011(0.9001) Steps 766(777.13) | Grad Norm 2.2159(2.8891) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0058 | Time 97.6879, Epoch Time 1178.9855(1104.6668), Bit/dim 3.5847(best: 3.5883), Xent 2.3026, Loss 3.5847, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3200 | Time 19.1408(19.2414) | Bit/dim 3.5382(3.5861) | Xent 2.3026(2.3026) | Loss 3.5382(3.5861) | Error 0.9000(0.8996) Steps 796(777.28) | Grad Norm 3.1786(2.9244) | Total Time 14.00(14.00)\n",
      "Iter 3210 | Time 19.3430(19.2344) | Bit/dim 3.5753(3.5844) | Xent 2.3026(2.3026) | Loss 3.5753(3.5844) | Error 0.8978(0.8996) Steps 766(777.65) | Grad Norm 3.1051(2.7517) | Total Time 14.00(14.00)\n",
      "Iter 3220 | Time 19.1767(19.3132) | Bit/dim 3.5645(3.5848) | Xent 2.3026(2.3026) | Loss 3.5645(3.5848) | Error 0.9044(0.8984) Steps 778(777.81) | Grad Norm 3.3425(2.9534) | Total Time 14.00(14.00)\n",
      "Iter 3230 | Time 19.3932(19.3777) | Bit/dim 3.5673(3.5841) | Xent 2.3026(2.3026) | Loss 3.5673(3.5841) | Error 0.8844(0.8993) Steps 802(779.21) | Grad Norm 2.7388(3.0367) | Total Time 14.00(14.00)\n",
      "Iter 3240 | Time 19.5630(19.4606) | Bit/dim 3.5703(3.5826) | Xent 2.3026(2.3026) | Loss 3.5703(3.5826) | Error 0.8978(0.9001) Steps 784(780.88) | Grad Norm 3.6299(3.0502) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0059 | Time 98.5907, Epoch Time 1187.0301(1107.1377), Bit/dim 3.5795(best: 3.5847), Xent 2.3026, Loss 3.5795, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3250 | Time 19.2565(19.4880) | Bit/dim 3.6026(3.5820) | Xent 2.3026(2.3026) | Loss 3.6026(3.5820) | Error 0.9022(0.9013) Steps 772(780.62) | Grad Norm 2.6045(2.7800) | Total Time 14.00(14.00)\n",
      "Iter 3260 | Time 19.9706(19.5667) | Bit/dim 3.5559(3.5804) | Xent 2.3026(2.3026) | Loss 3.5559(3.5804) | Error 0.8778(0.9003) Steps 808(781.39) | Grad Norm 3.2217(2.9648) | Total Time 14.00(14.00)\n",
      "Iter 3270 | Time 19.5788(19.5419) | Bit/dim 3.5823(3.5811) | Xent 2.3026(2.3026) | Loss 3.5823(3.5811) | Error 0.9100(0.9003) Steps 790(780.09) | Grad Norm 2.9826(2.9503) | Total Time 14.00(14.00)\n",
      "Iter 3280 | Time 19.5194(19.5621) | Bit/dim 3.5724(3.5813) | Xent 2.3026(2.3026) | Loss 3.5724(3.5813) | Error 0.9000(0.9003) Steps 778(781.04) | Grad Norm 3.1176(2.9395) | Total Time 14.00(14.00)\n",
      "Iter 3290 | Time 19.2115(19.4838) | Bit/dim 3.5915(3.5797) | Xent 2.3026(2.3026) | Loss 3.5915(3.5797) | Error 0.8844(0.9004) Steps 778(781.47) | Grad Norm 3.7035(3.1954) | Total Time 14.00(14.00)\n",
      "Iter 3300 | Time 19.3303(19.4843) | Bit/dim 3.5708(3.5813) | Xent 2.3026(2.3026) | Loss 3.5708(3.5813) | Error 0.9022(0.8994) Steps 808(783.24) | Grad Norm 2.9854(3.1973) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0060 | Time 98.2796, Epoch Time 1189.3768(1109.6049), Bit/dim 3.5843(best: 3.5795), Xent 2.3026, Loss 3.5843, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3310 | Time 20.3779(19.5907) | Bit/dim 3.5872(3.5812) | Xent 2.3026(2.3026) | Loss 3.5872(3.5812) | Error 0.9089(0.8996) Steps 802(784.50) | Grad Norm 3.0804(2.9838) | Total Time 14.00(14.00)\n",
      "Iter 3320 | Time 20.1421(19.7309) | Bit/dim 3.5670(3.5784) | Xent 2.3026(2.3026) | Loss 3.5670(3.5784) | Error 0.8944(0.8997) Steps 814(784.98) | Grad Norm 3.2319(3.0038) | Total Time 14.00(14.00)\n",
      "Iter 3330 | Time 19.6018(19.7797) | Bit/dim 3.5686(3.5754) | Xent 2.3026(2.3026) | Loss 3.5686(3.5754) | Error 0.8922(0.8994) Steps 784(785.02) | Grad Norm 2.4344(3.0583) | Total Time 14.00(14.00)\n",
      "Iter 3340 | Time 20.7095(19.7882) | Bit/dim 3.5983(3.5770) | Xent 2.3026(2.3026) | Loss 3.5983(3.5770) | Error 0.9000(0.9001) Steps 778(784.63) | Grad Norm 2.4536(3.0654) | Total Time 14.00(14.00)\n",
      "Iter 3350 | Time 19.0806(19.7312) | Bit/dim 3.5848(3.5762) | Xent 2.3026(2.3026) | Loss 3.5848(3.5762) | Error 0.8778(0.8992) Steps 796(785.07) | Grad Norm 2.7867(3.0724) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0061 | Time 99.0460, Epoch Time 1209.0852(1112.5893), Bit/dim 3.5731(best: 3.5795), Xent 2.3026, Loss 3.5731, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3360 | Time 19.2700(19.7367) | Bit/dim 3.5492(3.5761) | Xent 2.3026(2.3026) | Loss 3.5492(3.5761) | Error 0.9089(0.8995) Steps 790(786.60) | Grad Norm 2.8551(2.8218) | Total Time 14.00(14.00)\n",
      "Iter 3370 | Time 19.8272(19.7054) | Bit/dim 3.5692(3.5733) | Xent 2.3026(2.3026) | Loss 3.5692(3.5733) | Error 0.8878(0.8992) Steps 820(788.33) | Grad Norm 1.8979(2.8383) | Total Time 14.00(14.00)\n",
      "Iter 3380 | Time 19.9478(19.7424) | Bit/dim 3.5611(3.5719) | Xent 2.3026(2.3026) | Loss 3.5611(3.5719) | Error 0.8889(0.8990) Steps 790(788.67) | Grad Norm 3.1621(2.7635) | Total Time 14.00(14.00)\n",
      "Iter 3390 | Time 19.4216(19.7553) | Bit/dim 3.5770(3.5712) | Xent 2.3026(2.3026) | Loss 3.5770(3.5712) | Error 0.9167(0.8996) Steps 790(789.99) | Grad Norm 2.9593(2.9589) | Total Time 14.00(14.00)\n",
      "Iter 3400 | Time 19.9340(19.8373) | Bit/dim 3.6013(3.5747) | Xent 2.3026(2.3026) | Loss 3.6013(3.5747) | Error 0.9189(0.9011) Steps 808(790.47) | Grad Norm 2.1501(2.8719) | Total Time 14.00(14.00)\n",
      "Iter 3410 | Time 20.2135(19.9126) | Bit/dim 3.5876(3.5748) | Xent 2.3026(2.3026) | Loss 3.5876(3.5748) | Error 0.8967(0.9005) Steps 808(791.59) | Grad Norm 6.4377(3.1358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0062 | Time 99.6510, Epoch Time 1210.7704(1115.5347), Bit/dim 3.5754(best: 3.5731), Xent 2.3026, Loss 3.5754, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3420 | Time 19.4164(19.8880) | Bit/dim 3.5451(3.5747) | Xent 2.3026(2.3026) | Loss 3.5451(3.5747) | Error 0.8978(0.9009) Steps 778(791.32) | Grad Norm 1.9261(3.0911) | Total Time 14.00(14.00)\n",
      "Iter 3430 | Time 19.4082(19.9173) | Bit/dim 3.5610(3.5734) | Xent 2.3026(2.3026) | Loss 3.5610(3.5734) | Error 0.8900(0.9007) Steps 784(791.37) | Grad Norm 2.1629(3.0064) | Total Time 14.00(14.00)\n",
      "Iter 3450 | Time 20.1014(19.9085) | Bit/dim 3.5604(3.5680) | Xent 2.3026(2.3026) | Loss 3.5604(3.5680) | Error 0.9033(0.8998) Steps 796(791.23) | Grad Norm 2.3310(2.7324) | Total Time 14.00(14.00)\n",
      "Iter 3460 | Time 19.9951(19.9486) | Bit/dim 3.5659(3.5686) | Xent 2.3026(2.3026) | Loss 3.5659(3.5686) | Error 0.9089(0.9001) Steps 796(790.96) | Grad Norm 2.7769(2.7698) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0063 | Time 99.8670, Epoch Time 1214.7468(1118.5111), Bit/dim 3.5661(best: 3.5731), Xent 2.3026, Loss 3.5661, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3470 | Time 20.3953(19.9788) | Bit/dim 3.5419(3.5684) | Xent 2.3026(2.3026) | Loss 3.5419(3.5684) | Error 0.8900(0.8990) Steps 796(792.37) | Grad Norm 5.6048(2.7757) | Total Time 14.00(14.00)\n",
      "Iter 3500 | Time 20.0507(20.0486) | Bit/dim 3.5418(3.5672) | Xent 2.3026(2.3026) | Loss 3.5418(3.5672) | Error 0.8978(0.8998) Steps 778(792.86) | Grad Norm 2.7544(2.8240) | Total Time 14.00(14.00)\n",
      "Iter 3510 | Time 19.8692(20.0734) | Bit/dim 3.5550(3.5688) | Xent 2.3026(2.3026) | Loss 3.5550(3.5688) | Error 0.8956(0.8998) Steps 808(793.63) | Grad Norm 2.8664(2.9788) | Total Time 14.00(14.00)\n",
      "Iter 3520 | Time 19.9193(19.9695) | Bit/dim 3.5887(3.5682) | Xent 2.3026(2.3026) | Loss 3.5887(3.5682) | Error 0.9044(0.9008) Steps 784(794.31) | Grad Norm 4.6054(3.0387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0064 | Time 99.8667, Epoch Time 1219.0319(1121.5267), Bit/dim 3.5687(best: 3.5661), Xent 2.3026, Loss 3.5687, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3530 | Time 20.1786(20.0414) | Bit/dim 3.5632(3.5649) | Xent 2.3026(2.3026) | Loss 3.5632(3.5649) | Error 0.8956(0.9004) Steps 778(793.53) | Grad Norm 3.0040(2.8911) | Total Time 14.00(14.00)\n",
      "Iter 3540 | Time 19.4434(20.0008) | Bit/dim 3.5566(3.5644) | Xent 2.3026(2.3026) | Loss 3.5566(3.5644) | Error 0.9033(0.8997) Steps 778(793.61) | Grad Norm 3.8460(3.0021) | Total Time 14.00(14.00)\n",
      "Iter 3550 | Time 20.2333(20.0201) | Bit/dim 3.5501(3.5661) | Xent 2.3026(2.3026) | Loss 3.5501(3.5661) | Error 0.8967(0.8999) Steps 802(794.77) | Grad Norm 2.4921(2.7210) | Total Time 14.00(14.00)\n",
      "Iter 3560 | Time 20.1294(20.0335) | Bit/dim 3.5687(3.5659) | Xent 2.3026(2.3026) | Loss 3.5687(3.5659) | Error 0.8944(0.9000) Steps 802(794.85) | Grad Norm 1.9100(2.7433) | Total Time 14.00(14.00)\n",
      "Iter 3570 | Time 18.9075(20.0299) | Bit/dim 3.5491(3.5662) | Xent 2.3026(2.3026) | Loss 3.5491(3.5662) | Error 0.9033(0.9008) Steps 790(795.42) | Grad Norm 3.1561(2.7270) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0065 | Time 99.4324, Epoch Time 1221.3615(1124.5217), Bit/dim 3.5664(best: 3.5661), Xent 2.3026, Loss 3.5664, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3580 | Time 20.2032(20.0186) | Bit/dim 3.5500(3.5638) | Xent 2.3026(2.3026) | Loss 3.5500(3.5638) | Error 0.9067(0.9001) Steps 796(794.57) | Grad Norm 1.8583(2.8333) | Total Time 14.00(14.00)\n",
      "Iter 3590 | Time 19.7317(20.0456) | Bit/dim 3.5465(3.5629) | Xent 2.3026(2.3026) | Loss 3.5465(3.5629) | Error 0.9044(0.9001) Steps 802(794.99) | Grad Norm 2.5156(2.6416) | Total Time 14.00(14.00)\n",
      "Iter 3600 | Time 20.1351(20.0477) | Bit/dim 3.5596(3.5645) | Xent 2.3026(2.3026) | Loss 3.5596(3.5645) | Error 0.8911(0.9005) Steps 802(794.82) | Grad Norm 3.1137(2.6524) | Total Time 14.00(14.00)\n",
      "Iter 3610 | Time 20.2499(20.0979) | Bit/dim 3.5786(3.5635) | Xent 2.3026(2.3026) | Loss 3.5786(3.5635) | Error 0.8989(0.9004) Steps 790(795.17) | Grad Norm 3.1504(2.8831) | Total Time 14.00(14.00)\n",
      "Iter 3620 | Time 19.6069(20.0273) | Bit/dim 3.5645(3.5617) | Xent 2.3026(2.3026) | Loss 3.5645(3.5617) | Error 0.9056(0.9005) Steps 814(795.44) | Grad Norm 1.6714(2.8651) | Total Time 14.00(14.00)\n",
      "Iter 3630 | Time 20.1545(20.0667) | Bit/dim 3.5822(3.5595) | Xent 2.3026(2.3026) | Loss 3.5822(3.5595) | Error 0.9389(0.9003) Steps 784(796.12) | Grad Norm 1.7066(2.7369) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0066 | Time 99.3669, Epoch Time 1220.9855(1127.4157), Bit/dim 3.5568(best: 3.5661), Xent 2.3026, Loss 3.5568, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3640 | Time 19.9030(20.0758) | Bit/dim 3.5387(3.5620) | Xent 2.3026(2.3026) | Loss 3.5387(3.5620) | Error 0.8922(0.9007) Steps 796(795.81) | Grad Norm 6.0436(2.8148) | Total Time 14.00(14.00)\n",
      "Iter 3650 | Time 20.0024(20.0810) | Bit/dim 3.5084(3.5606) | Xent 2.3026(2.3026) | Loss 3.5084(3.5606) | Error 0.8722(0.8998) Steps 772(795.15) | Grad Norm 3.6222(2.9569) | Total Time 14.00(14.00)\n",
      "Iter 3660 | Time 20.3780(20.1712) | Bit/dim 3.5626(3.5606) | Xent 2.3026(2.3026) | Loss 3.5626(3.5606) | Error 0.8889(0.8997) Steps 796(794.58) | Grad Norm 1.5692(2.8535) | Total Time 14.00(14.00)\n",
      "Iter 3670 | Time 19.4266(20.1293) | Bit/dim 3.5396(3.5574) | Xent 2.3026(2.3026) | Loss 3.5396(3.5574) | Error 0.9056(0.9002) Steps 802(796.08) | Grad Norm 2.9351(2.7684) | Total Time 14.00(14.00)\n",
      "Iter 3680 | Time 19.3731(20.0932) | Bit/dim 3.5704(3.5576) | Xent 2.3026(2.3026) | Loss 3.5704(3.5576) | Error 0.9011(0.8993) Steps 778(794.61) | Grad Norm 2.7545(3.0508) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0067 | Time 97.3726, Epoch Time 1221.2874(1130.2318), Bit/dim 3.5572(best: 3.5568), Xent 2.3026, Loss 3.5572, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3690 | Time 20.0937(20.0298) | Bit/dim 3.5581(3.5570) | Xent 2.3026(2.3026) | Loss 3.5581(3.5570) | Error 0.8933(0.9005) Steps 784(793.85) | Grad Norm 1.7892(2.8258) | Total Time 14.00(14.00)\n",
      "Iter 3700 | Time 20.3962(20.0760) | Bit/dim 3.5842(3.5561) | Xent 2.3026(2.3026) | Loss 3.5842(3.5561) | Error 0.9144(0.9002) Steps 796(793.68) | Grad Norm 1.8866(2.7804) | Total Time 14.00(14.00)\n",
      "Iter 3710 | Time 19.9398(20.0220) | Bit/dim 3.5351(3.5538) | Xent 2.3026(2.3026) | Loss 3.5351(3.5538) | Error 0.9033(0.9006) Steps 802(794.01) | Grad Norm 3.0417(2.8804) | Total Time 14.00(14.00)\n",
      "Iter 3720 | Time 20.5536(20.0504) | Bit/dim 3.5437(3.5535) | Xent 2.3026(2.3026) | Loss 3.5437(3.5535) | Error 0.9111(0.9002) Steps 790(794.15) | Grad Norm 3.9724(2.8513) | Total Time 14.00(14.00)\n",
      "Iter 3730 | Time 20.0853(20.0829) | Bit/dim 3.5191(3.5513) | Xent 2.3026(2.3026) | Loss 3.5191(3.5513) | Error 0.8922(0.8997) Steps 790(793.80) | Grad Norm 2.2341(2.7920) | Total Time 14.00(14.00)\n",
      "Iter 3740 | Time 19.4788(20.0172) | Bit/dim 3.5475(3.5556) | Xent 2.3026(2.3026) | Loss 3.5475(3.5556) | Error 0.9022(0.9002) Steps 796(794.85) | Grad Norm 2.6212(2.6187) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0068 | Time 98.3153, Epoch Time 1217.1358(1132.8389), Bit/dim 3.5553(best: 3.5568), Xent 2.3026, Loss 3.5553, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3750 | Time 19.3743(19.9640) | Bit/dim 3.5615(3.5540) | Xent 2.3026(2.3026) | Loss 3.5615(3.5540) | Error 0.8856(0.8998) Steps 790(793.78) | Grad Norm 2.4997(2.7204) | Total Time 14.00(14.00)\n",
      "Iter 3760 | Time 20.2448(19.9672) | Bit/dim 3.5480(3.5552) | Xent 2.3026(2.3026) | Loss 3.5480(3.5552) | Error 0.8978(0.8999) Steps 814(794.60) | Grad Norm 2.0066(2.7628) | Total Time 14.00(14.00)\n",
      "Iter 3770 | Time 20.0037(19.9876) | Bit/dim 3.5430(3.5564) | Xent 2.3026(2.3026) | Loss 3.5430(3.5564) | Error 0.8911(0.9001) Steps 784(793.65) | Grad Norm 2.7915(2.8632) | Total Time 14.00(14.00)\n",
      "Iter 3780 | Time 20.0983(20.0219) | Bit/dim 3.5521(3.5545) | Xent 2.3026(2.3026) | Loss 3.5521(3.5545) | Error 0.9100(0.9002) Steps 796(795.38) | Grad Norm 1.8729(2.7834) | Total Time 14.00(14.00)\n",
      "Iter 3790 | Time 19.6259(19.9747) | Bit/dim 3.5755(3.5522) | Xent 2.3026(2.3026) | Loss 3.5755(3.5522) | Error 0.9011(0.9001) Steps 778(794.12) | Grad Norm 4.6297(2.7812) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0069 | Time 97.5989, Epoch Time 1213.0618(1135.2456), Bit/dim 3.5506(best: 3.5553), Xent 2.3026, Loss 3.5506, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3800 | Time 19.9980(19.9909) | Bit/dim 3.5412(3.5523) | Xent 2.3026(2.3026) | Loss 3.5412(3.5523) | Error 0.8911(0.8990) Steps 790(796.29) | Grad Norm 1.6084(2.7499) | Total Time 14.00(14.00)\n",
      "Iter 3810 | Time 20.3467(20.0207) | Bit/dim 3.5224(3.5518) | Xent 2.3026(2.3026) | Loss 3.5224(3.5518) | Error 0.9022(0.9004) Steps 790(795.62) | Grad Norm 2.4472(2.8629) | Total Time 14.00(14.00)\n",
      "Iter 3820 | Time 19.8632(19.9925) | Bit/dim 3.5088(3.5492) | Xent 2.3026(2.3026) | Loss 3.5088(3.5492) | Error 0.8778(0.8998) Steps 826(795.11) | Grad Norm 1.9341(2.7820) | Total Time 14.00(14.00)\n",
      "Iter 3830 | Time 20.1270(19.9876) | Bit/dim 3.5434(3.5506) | Xent 2.3026(2.3026) | Loss 3.5434(3.5506) | Error 0.9067(0.8996) Steps 802(795.16) | Grad Norm 1.7874(2.7777) | Total Time 14.00(14.00)\n",
      "Iter 3840 | Time 19.9889(20.0023) | Bit/dim 3.5524(3.5494) | Xent 2.3026(2.3026) | Loss 3.5524(3.5494) | Error 0.9144(0.9007) Steps 772(795.70) | Grad Norm 3.8138(2.8571) | Total Time 14.00(14.00)\n",
      "Iter 3850 | Time 20.1219(20.0483) | Bit/dim 3.5709(3.5501) | Xent 2.3026(2.3026) | Loss 3.5709(3.5501) | Error 0.8822(0.9000) Steps 796(796.62) | Grad Norm 2.5250(2.8270) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0070 | Time 97.5498, Epoch Time 1218.7029(1137.7493), Bit/dim 3.5501(best: 3.5506), Xent 2.3026, Loss 3.5501, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3860 | Time 20.5752(20.0086) | Bit/dim 3.5512(3.5507) | Xent 2.3026(2.3026) | Loss 3.5512(3.5507) | Error 0.9044(0.9003) Steps 790(797.23) | Grad Norm 3.6324(2.7868) | Total Time 14.00(14.00)\n",
      "Iter 3870 | Time 19.4587(19.9499) | Bit/dim 3.5227(3.5483) | Xent 2.3026(2.3026) | Loss 3.5227(3.5483) | Error 0.8833(0.9000) Steps 796(796.63) | Grad Norm 2.0972(2.8315) | Total Time 14.00(14.00)\n",
      "Iter 3880 | Time 19.5739(19.8949) | Bit/dim 3.5685(3.5487) | Xent 2.3026(2.3026) | Loss 3.5685(3.5487) | Error 0.9189(0.8997) Steps 790(795.56) | Grad Norm 2.2295(2.7138) | Total Time 14.00(14.00)\n",
      "Iter 3890 | Time 20.0896(19.9075) | Bit/dim 3.5233(3.5481) | Xent 2.3026(2.3026) | Loss 3.5233(3.5481) | Error 0.8944(0.8995) Steps 802(794.42) | Grad Norm 1.9453(2.8202) | Total Time 14.00(14.00)\n",
      "Iter 3900 | Time 19.6581(19.9014) | Bit/dim 3.5464(3.5467) | Xent 2.3026(2.3026) | Loss 3.5464(3.5467) | Error 0.9011(0.8999) Steps 796(792.99) | Grad Norm 2.7482(2.6507) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0071 | Time 98.7238, Epoch Time 1208.2732(1139.8650), Bit/dim 3.5472(best: 3.5501), Xent 2.3026, Loss 3.5472, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3910 | Time 19.6492(19.9068) | Bit/dim 3.5633(3.5448) | Xent 2.3026(2.3026) | Loss 3.5633(3.5448) | Error 0.8844(0.9000) Steps 778(793.07) | Grad Norm 3.1485(2.7992) | Total Time 14.00(14.00)\n",
      "Iter 3920 | Time 20.2704(19.9561) | Bit/dim 3.5238(3.5465) | Xent 2.3026(2.3026) | Loss 3.5238(3.5465) | Error 0.8900(0.9001) Steps 808(794.74) | Grad Norm 3.6250(2.7588) | Total Time 14.00(14.00)\n",
      "Iter 3930 | Time 20.1076(19.9649) | Bit/dim 3.5805(3.5474) | Xent 2.3026(2.3026) | Loss 3.5805(3.5474) | Error 0.9222(0.8995) Steps 796(796.50) | Grad Norm 2.3565(2.6415) | Total Time 14.00(14.00)\n",
      "Iter 3940 | Time 20.2330(20.0055) | Bit/dim 3.5308(3.5456) | Xent 2.3026(2.3026) | Loss 3.5308(3.5456) | Error 0.9022(0.8995) Steps 790(796.26) | Grad Norm 1.9792(2.8146) | Total Time 14.00(14.00)\n",
      "Iter 3950 | Time 19.8943(19.9608) | Bit/dim 3.5036(3.5450) | Xent 2.3026(2.3026) | Loss 3.5036(3.5450) | Error 0.8800(0.9004) Steps 796(795.96) | Grad Norm 2.0958(2.6253) | Total Time 14.00(14.00)\n",
      "Iter 3960 | Time 20.1891(19.9142) | Bit/dim 3.5537(3.5453) | Xent 2.3026(2.3026) | Loss 3.5537(3.5453) | Error 0.8967(0.9006) Steps 772(795.12) | Grad Norm 2.9849(2.6712) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0072 | Time 98.0057, Epoch Time 1214.1479(1142.0935), Bit/dim 3.5456(best: 3.5472), Xent 2.3026, Loss 3.5456, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 3970 | Time 20.0759(19.9493) | Bit/dim 3.5631(3.5464) | Xent 2.3026(2.3026) | Loss 3.5631(3.5464) | Error 0.9033(0.8995) Steps 790(792.67) | Grad Norm 3.6220(2.9021) | Total Time 14.00(14.00)\n",
      "Iter 3980 | Time 20.0015(19.9551) | Bit/dim 3.5293(3.5422) | Xent 2.3026(2.3026) | Loss 3.5293(3.5422) | Error 0.8900(0.8977) Steps 796(792.75) | Grad Norm 3.5896(2.9037) | Total Time 14.00(14.00)\n",
      "Iter 3990 | Time 19.4270(19.8790) | Bit/dim 3.5442(3.5404) | Xent 2.3026(2.3026) | Loss 3.5442(3.5404) | Error 0.9067(0.8986) Steps 796(791.86) | Grad Norm 2.1118(2.7202) | Total Time 14.00(14.00)\n",
      "Iter 4000 | Time 19.2607(19.8403) | Bit/dim 3.5481(3.5433) | Xent 2.3026(2.3026) | Loss 3.5481(3.5433) | Error 0.9178(0.9014) Steps 778(791.47) | Grad Norm 1.7728(2.4599) | Total Time 14.00(14.00)\n",
      "Iter 4010 | Time 19.2010(19.8538) | Bit/dim 3.5344(3.5410) | Xent 2.3026(2.3026) | Loss 3.5344(3.5410) | Error 0.8922(0.9011) Steps 796(791.97) | Grad Norm 3.1739(2.6637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0073 | Time 97.9226, Epoch Time 1208.6260(1144.0895), Bit/dim 3.5429(best: 3.5456), Xent 2.3026, Loss 3.5429, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4020 | Time 19.9589(19.8836) | Bit/dim 3.5098(3.5400) | Xent 2.3026(2.3026) | Loss 3.5098(3.5400) | Error 0.9111(0.9013) Steps 778(790.64) | Grad Norm 3.2461(2.7006) | Total Time 14.00(14.00)\n",
      "Iter 4030 | Time 20.4640(19.9756) | Bit/dim 3.5097(3.5407) | Xent 2.3026(2.3026) | Loss 3.5097(3.5407) | Error 0.8733(0.8995) Steps 760(789.88) | Grad Norm 2.7664(2.7627) | Total Time 14.00(14.00)\n",
      "Iter 4040 | Time 19.5697(19.9875) | Bit/dim 3.5339(3.5420) | Xent 2.3026(2.3026) | Loss 3.5339(3.5420) | Error 0.9178(0.9019) Steps 802(790.74) | Grad Norm 2.2723(2.6312) | Total Time 14.00(14.00)\n",
      "Iter 4050 | Time 20.0428(19.9923) | Bit/dim 3.5348(3.5407) | Xent 2.3026(2.3026) | Loss 3.5348(3.5407) | Error 0.9044(0.9017) Steps 802(790.74) | Grad Norm 4.8482(2.7799) | Total Time 14.00(14.00)\n",
      "Iter 4060 | Time 19.8623(19.9883) | Bit/dim 3.5459(3.5407) | Xent 2.3026(2.3026) | Loss 3.5459(3.5407) | Error 0.9022(0.9010) Steps 784(790.82) | Grad Norm 3.4793(2.8646) | Total Time 14.00(14.00)\n",
      "Iter 4070 | Time 20.4396(19.9907) | Bit/dim 3.5423(3.5393) | Xent 2.3026(2.3026) | Loss 3.5423(3.5393) | Error 0.8878(0.8998) Steps 784(790.60) | Grad Norm 2.3514(2.7985) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0074 | Time 97.5057, Epoch Time 1217.5761(1146.2941), Bit/dim 3.5395(best: 3.5429), Xent 2.3026, Loss 3.5395, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4080 | Time 19.9630(19.9829) | Bit/dim 3.5149(3.5397) | Xent 2.3026(2.3026) | Loss 3.5149(3.5397) | Error 0.8944(0.9000) Steps 802(791.26) | Grad Norm 2.5202(2.8130) | Total Time 14.00(14.00)\n",
      "Iter 4090 | Time 20.1506(19.9739) | Bit/dim 3.5442(3.5382) | Xent 2.3026(2.3026) | Loss 3.5442(3.5382) | Error 0.8900(0.8996) Steps 790(791.86) | Grad Norm 2.8419(2.8081) | Total Time 14.00(14.00)\n",
      "Iter 4100 | Time 19.9031(19.9718) | Bit/dim 3.5208(3.5379) | Xent 2.3026(2.3026) | Loss 3.5208(3.5379) | Error 0.9100(0.8999) Steps 784(791.42) | Grad Norm 3.1549(2.7941) | Total Time 14.00(14.00)\n",
      "Iter 4110 | Time 19.4963(19.9169) | Bit/dim 3.5293(3.5371) | Xent 2.3026(2.3026) | Loss 3.5293(3.5371) | Error 0.8956(0.8997) Steps 778(791.25) | Grad Norm 3.0822(2.6541) | Total Time 14.00(14.00)\n",
      "Iter 4120 | Time 20.0802(19.9116) | Bit/dim 3.5156(3.5387) | Xent 2.3026(2.3026) | Loss 3.5156(3.5387) | Error 0.9000(0.8990) Steps 802(792.20) | Grad Norm 2.1961(2.7585) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0075 | Time 97.3365, Epoch Time 1211.1066(1148.2385), Bit/dim 3.5402(best: 3.5395), Xent 2.3026, Loss 3.5402, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4130 | Time 19.9666(19.9040) | Bit/dim 3.5596(3.5396) | Xent 2.3026(2.3026) | Loss 3.5596(3.5396) | Error 0.8822(0.8992) Steps 784(791.43) | Grad Norm 2.0239(2.7537) | Total Time 14.00(14.00)\n",
      "Iter 4140 | Time 20.3381(19.9310) | Bit/dim 3.5252(3.5385) | Xent 2.3026(2.3026) | Loss 3.5252(3.5385) | Error 0.8900(0.8999) Steps 796(790.85) | Grad Norm 3.2892(2.7139) | Total Time 14.00(14.00)\n",
      "Iter 4150 | Time 20.2006(19.9479) | Bit/dim 3.4947(3.5365) | Xent 2.3026(2.3026) | Loss 3.4947(3.5365) | Error 0.9033(0.8993) Steps 796(790.94) | Grad Norm 1.2065(2.7491) | Total Time 14.00(14.00)\n",
      "Iter 4160 | Time 19.5788(19.9620) | Bit/dim 3.5502(3.5374) | Xent 2.3026(2.3026) | Loss 3.5502(3.5374) | Error 0.8878(0.8989) Steps 796(790.01) | Grad Norm 3.8461(2.8279) | Total Time 14.00(14.00)\n",
      "Iter 4170 | Time 19.8559(19.9348) | Bit/dim 3.5425(3.5336) | Xent 2.3026(2.3026) | Loss 3.5425(3.5336) | Error 0.9156(0.8998) Steps 778(788.77) | Grad Norm 1.4554(2.6511) | Total Time 14.00(14.00)\n",
      "Iter 4180 | Time 19.9348(19.9058) | Bit/dim 3.5461(3.5326) | Xent 2.3026(2.3026) | Loss 3.5461(3.5326) | Error 0.9011(0.9005) Steps 796(789.56) | Grad Norm 1.7738(2.5030) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0076 | Time 97.9325, Epoch Time 1211.5537(1150.1379), Bit/dim 3.5333(best: 3.5395), Xent 2.3026, Loss 3.5333, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4190 | Time 19.8721(19.8755) | Bit/dim 3.5324(3.5336) | Xent 2.3026(2.3026) | Loss 3.5324(3.5336) | Error 0.9133(0.9012) Steps 790(790.84) | Grad Norm 1.5930(2.6670) | Total Time 14.00(14.00)\n",
      "Iter 4200 | Time 19.7468(19.8667) | Bit/dim 3.5366(3.5325) | Xent 2.3026(2.3026) | Loss 3.5366(3.5325) | Error 0.8789(0.9003) Steps 790(789.81) | Grad Norm 2.4473(2.6406) | Total Time 14.00(14.00)\n",
      "Iter 4210 | Time 19.7781(19.8511) | Bit/dim 3.5298(3.5295) | Xent 2.3026(2.3026) | Loss 3.5298(3.5295) | Error 0.8844(0.8995) Steps 796(790.27) | Grad Norm 2.0372(2.7889) | Total Time 14.00(14.00)\n",
      "Iter 4220 | Time 19.9187(19.7932) | Bit/dim 3.4913(3.5296) | Xent 2.3026(2.3026) | Loss 3.4913(3.5296) | Error 0.8933(0.8989) Steps 784(789.58) | Grad Norm 2.6297(2.7023) | Total Time 14.00(14.00)\n",
      "Iter 4230 | Time 20.0084(19.8361) | Bit/dim 3.5402(3.5342) | Xent 2.3026(2.3026) | Loss 3.5402(3.5342) | Error 0.9111(0.8999) Steps 790(790.02) | Grad Norm 1.9236(2.5591) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0077 | Time 98.5978, Epoch Time 1209.3361(1151.9139), Bit/dim 3.5305(best: 3.5333), Xent 2.3026, Loss 3.5305, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4240 | Time 20.1391(19.8841) | Bit/dim 3.5552(3.5336) | Xent 2.3026(2.3026) | Loss 3.5552(3.5336) | Error 0.9000(0.9007) Steps 778(790.84) | Grad Norm 4.3557(2.6035) | Total Time 14.00(14.00)\n",
      "Iter 4250 | Time 19.5737(19.8557) | Bit/dim 3.5091(3.5320) | Xent 2.3026(2.3026) | Loss 3.5091(3.5320) | Error 0.8867(0.8997) Steps 778(789.96) | Grad Norm 2.2254(2.4741) | Total Time 14.00(14.00)\n",
      "Iter 4260 | Time 20.0623(19.8096) | Bit/dim 3.5249(3.5315) | Xent 2.3026(2.3026) | Loss 3.5249(3.5315) | Error 0.9156(0.8997) Steps 790(789.36) | Grad Norm 1.9685(2.6297) | Total Time 14.00(14.00)\n",
      "Iter 4270 | Time 20.4683(19.8550) | Bit/dim 3.5540(3.5293) | Xent 2.3026(2.3026) | Loss 3.5540(3.5293) | Error 0.9011(0.8993) Steps 790(790.16) | Grad Norm 2.6043(2.6813) | Total Time 14.00(14.00)\n",
      "Iter 4280 | Time 20.3149(19.9143) | Bit/dim 3.5395(3.5308) | Xent 2.3026(2.3026) | Loss 3.5395(3.5308) | Error 0.9033(0.8991) Steps 790(790.63) | Grad Norm 1.9521(2.6692) | Total Time 14.00(14.00)\n",
      "Iter 4290 | Time 19.8738(19.8837) | Bit/dim 3.4903(3.5302) | Xent 2.3026(2.3026) | Loss 3.4903(3.5302) | Error 0.9000(0.9010) Steps 802(791.75) | Grad Norm 1.6554(2.5329) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0078 | Time 98.3077, Epoch Time 1208.0856(1153.5990), Bit/dim 3.5264(best: 3.5305), Xent 2.3026, Loss 3.5264, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4300 | Time 20.0494(19.8592) | Bit/dim 3.5151(3.5303) | Xent 2.3026(2.3026) | Loss 3.5151(3.5303) | Error 0.8911(0.9007) Steps 784(791.82) | Grad Norm 3.7120(2.6490) | Total Time 14.00(14.00)\n",
      "Iter 4310 | Time 19.6929(19.8320) | Bit/dim 3.5263(3.5285) | Xent 2.3026(2.3026) | Loss 3.5263(3.5285) | Error 0.9100(0.9009) Steps 784(791.94) | Grad Norm 1.6823(2.6254) | Total Time 14.00(14.00)\n",
      "Iter 4320 | Time 19.4426(19.7769) | Bit/dim 3.5133(3.5276) | Xent 2.3026(2.3026) | Loss 3.5133(3.5276) | Error 0.9022(0.9009) Steps 772(791.71) | Grad Norm 2.6807(2.6700) | Total Time 14.00(14.00)\n",
      "Iter 4330 | Time 19.5586(19.7981) | Bit/dim 3.5307(3.5269) | Xent 2.3026(2.3026) | Loss 3.5307(3.5269) | Error 0.8911(0.9001) Steps 808(793.71) | Grad Norm 2.8839(2.6485) | Total Time 14.00(14.00)\n",
      "Iter 4340 | Time 20.0672(19.8684) | Bit/dim 3.5182(3.5275) | Xent 2.3026(2.3026) | Loss 3.5182(3.5275) | Error 0.8989(0.9000) Steps 790(794.34) | Grad Norm 3.2086(2.6912) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0079 | Time 97.4147, Epoch Time 1207.4641(1155.2150), Bit/dim 3.5323(best: 3.5264), Xent 2.3026, Loss 3.5323, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4350 | Time 20.0468(19.9194) | Bit/dim 3.5173(3.5301) | Xent 2.3026(2.3026) | Loss 3.5173(3.5301) | Error 0.9011(0.9002) Steps 790(795.01) | Grad Norm 3.1083(2.8502) | Total Time 14.00(14.00)\n",
      "Iter 4360 | Time 20.0199(19.9150) | Bit/dim 3.5067(3.5275) | Xent 2.3026(2.3026) | Loss 3.5067(3.5275) | Error 0.8867(0.9001) Steps 814(796.28) | Grad Norm 3.0372(2.6677) | Total Time 14.00(14.00)\n",
      "Iter 4370 | Time 19.6075(19.9526) | Bit/dim 3.5246(3.5274) | Xent 2.3026(2.3026) | Loss 3.5246(3.5274) | Error 0.9011(0.9011) Steps 802(798.03) | Grad Norm 4.0838(2.7849) | Total Time 14.00(14.00)\n",
      "Iter 4380 | Time 19.3999(19.8791) | Bit/dim 3.5612(3.5288) | Xent 2.3026(2.3026) | Loss 3.5612(3.5288) | Error 0.8867(0.9009) Steps 784(796.40) | Grad Norm 1.7503(2.7610) | Total Time 14.00(14.00)\n",
      "Iter 4390 | Time 20.0909(19.8826) | Bit/dim 3.5119(3.5264) | Xent 2.3026(2.3026) | Loss 3.5119(3.5264) | Error 0.8989(0.9004) Steps 796(796.13) | Grad Norm 3.1482(2.5503) | Total Time 14.00(14.00)\n",
      "Iter 4400 | Time 19.5327(19.9200) | Bit/dim 3.5303(3.5248) | Xent 2.3026(2.3026) | Loss 3.5303(3.5248) | Error 0.8844(0.8989) Steps 790(796.21) | Grad Norm 2.5475(2.6191) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0080 | Time 99.6045, Epoch Time 1212.9486(1156.9470), Bit/dim 3.5240(best: 3.5264), Xent 2.3026, Loss 3.5240, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4410 | Time 20.3657(19.9254) | Bit/dim 3.5557(3.5264) | Xent 2.3026(2.3026) | Loss 3.5557(3.5264) | Error 0.9144(0.9009) Steps 802(797.02) | Grad Norm 3.8314(2.6169) | Total Time 14.00(14.00)\n",
      "Iter 4420 | Time 19.8910(19.9438) | Bit/dim 3.5103(3.5244) | Xent 2.3026(2.3026) | Loss 3.5103(3.5244) | Error 0.9078(0.9005) Steps 796(797.79) | Grad Norm 1.9365(2.7366) | Total Time 14.00(14.00)\n",
      "Iter 4430 | Time 20.1279(19.9835) | Bit/dim 3.5105(3.5233) | Xent 2.3026(2.3026) | Loss 3.5105(3.5233) | Error 0.8989(0.9001) Steps 802(796.24) | Grad Norm 2.6248(2.5215) | Total Time 14.00(14.00)\n",
      "Iter 4440 | Time 19.9507(20.0599) | Bit/dim 3.5225(3.5213) | Xent 2.3026(2.3026) | Loss 3.5225(3.5213) | Error 0.9044(0.9001) Steps 814(797.37) | Grad Norm 2.3773(2.7022) | Total Time 14.00(14.00)\n",
      "Iter 4450 | Time 20.3978(20.0365) | Bit/dim 3.5230(3.5229) | Xent 2.3026(2.3026) | Loss 3.5230(3.5229) | Error 0.9133(0.8990) Steps 784(797.28) | Grad Norm 2.8494(2.6309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0081 | Time 97.9273, Epoch Time 1218.6087(1158.7968), Bit/dim 3.5258(best: 3.5240), Xent 2.3026, Loss 3.5258, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4460 | Time 19.7791(20.0065) | Bit/dim 3.4857(3.5219) | Xent 2.3026(2.3026) | Loss 3.4857(3.5219) | Error 0.8944(0.8985) Steps 814(796.77) | Grad Norm 2.8375(2.7533) | Total Time 14.00(14.00)\n",
      "Iter 4470 | Time 19.4804(19.9946) | Bit/dim 3.5366(3.5220) | Xent 2.3026(2.3026) | Loss 3.5366(3.5220) | Error 0.9144(0.8994) Steps 796(795.16) | Grad Norm 1.8416(2.5633) | Total Time 14.00(14.00)\n",
      "Iter 4480 | Time 19.4240(19.9425) | Bit/dim 3.5243(3.5215) | Xent 2.3026(2.3026) | Loss 3.5243(3.5215) | Error 0.9056(0.8990) Steps 790(794.86) | Grad Norm 3.1602(2.6540) | Total Time 14.00(14.00)\n",
      "Iter 4490 | Time 20.3054(19.9109) | Bit/dim 3.5520(3.5208) | Xent 2.3026(2.3026) | Loss 3.5520(3.5208) | Error 0.8856(0.8992) Steps 802(795.29) | Grad Norm 1.8788(2.5115) | Total Time 14.00(14.00)\n",
      "Iter 4500 | Time 19.8364(19.9694) | Bit/dim 3.5116(3.5207) | Xent 2.3026(2.3026) | Loss 3.5116(3.5207) | Error 0.9089(0.8990) Steps 808(796.53) | Grad Norm 2.4562(2.4865) | Total Time 14.00(14.00)\n",
      "Iter 4510 | Time 20.0287(19.9928) | Bit/dim 3.5318(3.5211) | Xent 2.3026(2.3026) | Loss 3.5318(3.5211) | Error 0.9056(0.9007) Steps 784(797.05) | Grad Norm 2.0598(2.6333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0082 | Time 98.9874, Epoch Time 1214.3037(1160.4621), Bit/dim 3.5186(best: 3.5240), Xent 2.3026, Loss 3.5186, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4520 | Time 20.3973(20.0393) | Bit/dim 3.5046(3.5205) | Xent 2.3026(2.3026) | Loss 3.5046(3.5205) | Error 0.8878(0.8990) Steps 808(798.06) | Grad Norm 3.3696(2.7536) | Total Time 14.00(14.00)\n",
      "Iter 4530 | Time 19.5925(20.0353) | Bit/dim 3.5224(3.5207) | Xent 2.3026(2.3026) | Loss 3.5224(3.5207) | Error 0.8944(0.8988) Steps 796(799.85) | Grad Norm 2.3037(2.7911) | Total Time 14.00(14.00)\n",
      "Iter 4540 | Time 19.9729(20.0272) | Bit/dim 3.5216(3.5185) | Xent 2.3026(2.3026) | Loss 3.5216(3.5185) | Error 0.9122(0.8995) Steps 808(800.44) | Grad Norm 1.9721(2.5478) | Total Time 14.00(14.00)\n",
      "Iter 4550 | Time 20.1030(20.0817) | Bit/dim 3.4992(3.5190) | Xent 2.3026(2.3026) | Loss 3.4992(3.5190) | Error 0.8767(0.9000) Steps 808(800.83) | Grad Norm 2.2926(2.6921) | Total Time 14.00(14.00)\n",
      "Iter 4560 | Time 20.3420(20.0499) | Bit/dim 3.5274(3.5176) | Xent 2.3026(2.3026) | Loss 3.5274(3.5176) | Error 0.8978(0.9005) Steps 796(799.16) | Grad Norm 1.9334(2.5506) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0083 | Time 101.5028, Epoch Time 1223.2667(1162.3462), Bit/dim 3.5231(best: 3.5186), Xent 2.3026, Loss 3.5231, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4570 | Time 19.9548(20.0048) | Bit/dim 3.5302(3.5201) | Xent 2.3026(2.3026) | Loss 3.5302(3.5201) | Error 0.9078(0.9010) Steps 802(800.11) | Grad Norm 2.6925(2.7100) | Total Time 14.00(14.00)\n",
      "Iter 4580 | Time 19.5758(19.9568) | Bit/dim 3.4896(3.5183) | Xent 2.3026(2.3026) | Loss 3.4896(3.5183) | Error 0.8878(0.9028) Steps 778(798.64) | Grad Norm 2.1426(2.6206) | Total Time 14.00(14.00)\n",
      "Iter 4590 | Time 20.1589(19.9713) | Bit/dim 3.5026(3.5176) | Xent 2.3026(2.3026) | Loss 3.5026(3.5176) | Error 0.9122(0.9025) Steps 802(798.86) | Grad Norm 2.5602(2.7557) | Total Time 14.00(14.00)\n",
      "Iter 4600 | Time 20.5471(20.0594) | Bit/dim 3.4967(3.5156) | Xent 2.3026(2.3026) | Loss 3.4967(3.5156) | Error 0.8922(0.8997) Steps 814(800.41) | Grad Norm 3.2846(2.6057) | Total Time 14.00(14.00)\n",
      "Iter 4610 | Time 19.8395(20.0943) | Bit/dim 3.5374(3.5166) | Xent 2.3026(2.3026) | Loss 3.5374(3.5166) | Error 0.8900(0.8988) Steps 808(801.23) | Grad Norm 3.3675(2.6571) | Total Time 14.00(14.00)\n",
      "Iter 4620 | Time 19.5343(20.0734) | Bit/dim 3.4964(3.5169) | Xent 2.3026(2.3026) | Loss 3.4964(3.5169) | Error 0.8833(0.8995) Steps 796(801.62) | Grad Norm 1.9050(2.6005) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0084 | Time 100.9429, Epoch Time 1221.1252(1164.1096), Bit/dim 3.5170(best: 3.5186), Xent 2.3026, Loss 3.5170, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4630 | Time 20.6247(20.0699) | Bit/dim 3.4941(3.5171) | Xent 2.3026(2.3026) | Loss 3.4941(3.5171) | Error 0.8878(0.9010) Steps 808(801.23) | Grad Norm 1.7667(2.5129) | Total Time 14.00(14.00)\n",
      "Iter 4640 | Time 19.4490(20.0153) | Bit/dim 3.5126(3.5145) | Xent 2.3026(2.3026) | Loss 3.5126(3.5145) | Error 0.9078(0.9013) Steps 796(801.20) | Grad Norm 2.1691(2.5510) | Total Time 14.00(14.00)\n",
      "Iter 4650 | Time 20.2671(19.9756) | Bit/dim 3.4885(3.5136) | Xent 2.3026(2.3026) | Loss 3.4885(3.5136) | Error 0.8933(0.9017) Steps 820(800.69) | Grad Norm 2.2395(2.3385) | Total Time 14.00(14.00)\n",
      "Iter 4660 | Time 19.8045(19.9739) | Bit/dim 3.5136(3.5124) | Xent 2.3026(2.3026) | Loss 3.5136(3.5124) | Error 0.8956(0.8994) Steps 814(803.16) | Grad Norm 4.3999(2.6176) | Total Time 14.00(14.00)\n",
      "Iter 4670 | Time 20.0480(19.9787) | Bit/dim 3.4937(3.5131) | Xent 2.3026(2.3026) | Loss 3.4937(3.5131) | Error 0.9044(0.8991) Steps 820(802.33) | Grad Norm 1.9146(2.6062) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0085 | Time 101.4493, Epoch Time 1218.6383(1165.7454), Bit/dim 3.5171(best: 3.5170), Xent 2.3026, Loss 3.5171, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4680 | Time 19.6319(19.9669) | Bit/dim 3.5461(3.5165) | Xent 2.3026(2.3026) | Loss 3.5461(3.5165) | Error 0.8933(0.8990) Steps 790(802.71) | Grad Norm 3.4060(2.6722) | Total Time 14.00(14.00)\n",
      "Iter 4690 | Time 19.0256(19.9921) | Bit/dim 3.5128(3.5160) | Xent 2.3026(2.3026) | Loss 3.5128(3.5160) | Error 0.9100(0.8993) Steps 802(803.50) | Grad Norm 1.9483(2.5267) | Total Time 14.00(14.00)\n",
      "Iter 4700 | Time 20.3501(20.0161) | Bit/dim 3.5504(3.5155) | Xent 2.3026(2.3026) | Loss 3.5504(3.5155) | Error 0.8978(0.8999) Steps 796(802.78) | Grad Norm 3.2072(2.6666) | Total Time 14.00(14.00)\n",
      "Iter 4710 | Time 20.6972(20.0218) | Bit/dim 3.5122(3.5144) | Xent 2.3026(2.3026) | Loss 3.5122(3.5144) | Error 0.8844(0.8990) Steps 820(803.44) | Grad Norm 3.1096(2.7173) | Total Time 14.00(14.00)\n",
      "Iter 4720 | Time 19.9117(19.9932) | Bit/dim 3.5142(3.5129) | Xent 2.3026(2.3026) | Loss 3.5142(3.5129) | Error 0.9122(0.8989) Steps 808(804.09) | Grad Norm 2.5567(2.5721) | Total Time 14.00(14.00)\n",
      "Iter 4730 | Time 19.2253(19.9391) | Bit/dim 3.5154(3.5121) | Xent 2.3026(2.3026) | Loss 3.5154(3.5121) | Error 0.9144(0.8999) Steps 796(803.53) | Grad Norm 2.2097(2.6574) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0086 | Time 101.3829, Epoch Time 1216.0054(1167.2532), Bit/dim 3.5101(best: 3.5170), Xent 2.3026, Loss 3.5101, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4740 | Time 20.1368(19.8676) | Bit/dim 3.5067(3.5123) | Xent 2.3026(2.3026) | Loss 3.5067(3.5123) | Error 0.8900(0.8999) Steps 808(804.10) | Grad Norm 3.5313(2.6087) | Total Time 14.00(14.00)\n",
      "Iter 4750 | Time 20.0578(19.9278) | Bit/dim 3.5114(3.5090) | Xent 2.3026(2.3026) | Loss 3.5114(3.5090) | Error 0.9022(0.8998) Steps 808(805.74) | Grad Norm 1.8426(2.7152) | Total Time 14.00(14.00)\n",
      "Iter 4760 | Time 19.3506(19.9402) | Bit/dim 3.5183(3.5098) | Xent 2.3026(2.3026) | Loss 3.5183(3.5098) | Error 0.8989(0.9002) Steps 796(804.55) | Grad Norm 2.7997(2.6365) | Total Time 14.00(14.00)\n",
      "Iter 4770 | Time 20.2946(19.9019) | Bit/dim 3.5103(3.5116) | Xent 2.3026(2.3026) | Loss 3.5103(3.5116) | Error 0.9067(0.9004) Steps 790(803.63) | Grad Norm 1.4875(2.4947) | Total Time 14.00(14.00)\n",
      "Iter 4780 | Time 19.7958(19.8983) | Bit/dim 3.5341(3.5117) | Xent 2.3026(2.3026) | Loss 3.5341(3.5117) | Error 0.9122(0.9004) Steps 814(805.05) | Grad Norm 3.9206(2.6001) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0087 | Time 101.3944, Epoch Time 1214.9153(1168.6831), Bit/dim 3.5120(best: 3.5101), Xent 2.3026, Loss 3.5120, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4790 | Time 19.6190(19.9236) | Bit/dim 3.4938(3.5110) | Xent 2.3026(2.3026) | Loss 3.4938(3.5110) | Error 0.9000(0.9003) Steps 808(805.71) | Grad Norm 1.3786(2.5146) | Total Time 14.00(14.00)\n",
      "Iter 4800 | Time 19.6961(19.9393) | Bit/dim 3.5169(3.5107) | Xent 2.3026(2.3026) | Loss 3.5169(3.5107) | Error 0.9022(0.9002) Steps 802(807.89) | Grad Norm 1.7806(2.6100) | Total Time 14.00(14.00)\n",
      "Iter 4810 | Time 20.6063(19.9754) | Bit/dim 3.4941(3.5109) | Xent 2.3026(2.3026) | Loss 3.4941(3.5109) | Error 0.9000(0.9002) Steps 832(808.13) | Grad Norm 2.4337(2.5578) | Total Time 14.00(14.00)\n",
      "Iter 4820 | Time 20.3177(19.9985) | Bit/dim 3.4916(3.5087) | Xent 2.3026(2.3026) | Loss 3.4916(3.5087) | Error 0.9122(0.9005) Steps 796(806.82) | Grad Norm 3.8248(2.5997) | Total Time 14.00(14.00)\n",
      "Iter 4830 | Time 19.4416(19.9544) | Bit/dim 3.4732(3.5066) | Xent 2.3026(2.3026) | Loss 3.4732(3.5066) | Error 0.9133(0.9000) Steps 796(807.15) | Grad Norm 1.8493(2.5272) | Total Time 14.00(14.00)\n",
      "Iter 4840 | Time 20.2570(20.0205) | Bit/dim 3.5079(3.5069) | Xent 2.3026(2.3026) | Loss 3.5079(3.5069) | Error 0.9033(0.9002) Steps 820(806.72) | Grad Norm 1.9515(2.4866) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0088 | Time 101.4171, Epoch Time 1220.1856(1170.2282), Bit/dim 3.5091(best: 3.5101), Xent 2.3026, Loss 3.5091, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4850 | Time 19.8833(20.0058) | Bit/dim 3.5162(3.5056) | Xent 2.3026(2.3026) | Loss 3.5162(3.5056) | Error 0.9067(0.9008) Steps 820(808.49) | Grad Norm 2.3659(2.4475) | Total Time 14.00(14.00)\n",
      "Iter 4860 | Time 20.6004(20.0954) | Bit/dim 3.4876(3.5075) | Xent 2.3026(2.3026) | Loss 3.4876(3.5075) | Error 0.8933(0.9003) Steps 802(807.95) | Grad Norm 2.3040(2.5528) | Total Time 14.00(14.00)\n",
      "Iter 4870 | Time 19.8377(20.0223) | Bit/dim 3.5231(3.5083) | Xent 2.3026(2.3026) | Loss 3.5231(3.5083) | Error 0.9111(0.9002) Steps 802(807.92) | Grad Norm 2.8054(2.4851) | Total Time 14.00(14.00)\n",
      "Iter 4880 | Time 19.8981(19.9728) | Bit/dim 3.4982(3.5039) | Xent 2.3026(2.3026) | Loss 3.4982(3.5039) | Error 0.9044(0.8996) Steps 826(810.15) | Grad Norm 2.0504(2.6795) | Total Time 14.00(14.00)\n",
      "Iter 4890 | Time 19.6874(19.9864) | Bit/dim 3.5492(3.5060) | Xent 2.3026(2.3026) | Loss 3.5492(3.5060) | Error 0.9067(0.8999) Steps 796(811.33) | Grad Norm 2.9633(2.4673) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0089 | Time 102.3186, Epoch Time 1221.5472(1171.7677), Bit/dim 3.5073(best: 3.5091), Xent 2.3026, Loss 3.5073, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4900 | Time 19.8235(19.9961) | Bit/dim 3.5338(3.5048) | Xent 2.3026(2.3026) | Loss 3.5338(3.5048) | Error 0.9111(0.8995) Steps 820(811.27) | Grad Norm 2.1322(2.4908) | Total Time 14.00(14.00)\n",
      "Iter 4910 | Time 20.0920(19.9953) | Bit/dim 3.5230(3.5060) | Xent 2.3026(2.3026) | Loss 3.5230(3.5060) | Error 0.9067(0.8993) Steps 802(812.01) | Grad Norm 1.7399(2.4939) | Total Time 14.00(14.00)\n",
      "Iter 4920 | Time 19.9757(19.9996) | Bit/dim 3.4894(3.5056) | Xent 2.3026(2.3026) | Loss 3.4894(3.5056) | Error 0.8967(0.8992) Steps 808(812.64) | Grad Norm 3.1754(2.6215) | Total Time 14.00(14.00)\n",
      "Iter 4930 | Time 19.7389(19.9684) | Bit/dim 3.4867(3.5033) | Xent 2.3026(2.3026) | Loss 3.4867(3.5033) | Error 0.9033(0.8994) Steps 796(812.08) | Grad Norm 1.7491(2.6549) | Total Time 14.00(14.00)\n",
      "Iter 4940 | Time 19.0389(19.9218) | Bit/dim 3.5147(3.5051) | Xent 2.3026(2.3026) | Loss 3.5147(3.5051) | Error 0.8989(0.9000) Steps 814(811.04) | Grad Norm 2.3412(2.6178) | Total Time 14.00(14.00)\n",
      "Iter 4950 | Time 20.2017(19.9883) | Bit/dim 3.5037(3.5040) | Xent 2.3026(2.3026) | Loss 3.5037(3.5040) | Error 0.9167(0.9007) Steps 796(811.27) | Grad Norm 1.8912(2.6247) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0090 | Time 102.0885, Epoch Time 1217.9680(1173.1537), Bit/dim 3.5055(best: 3.5073), Xent 2.3026, Loss 3.5055, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 4960 | Time 19.5054(19.9791) | Bit/dim 3.4999(3.5026) | Xent 2.3026(2.3026) | Loss 3.4999(3.5026) | Error 0.9022(0.9000) Steps 802(811.77) | Grad Norm 2.5447(2.7016) | Total Time 14.00(14.00)\n",
      "Iter 4970 | Time 20.1372(19.9722) | Bit/dim 3.5187(3.5025) | Xent 2.3026(2.3026) | Loss 3.5187(3.5025) | Error 0.9033(0.8996) Steps 826(812.72) | Grad Norm 1.6504(2.6236) | Total Time 14.00(14.00)\n",
      "Iter 4980 | Time 20.2343(19.9882) | Bit/dim 3.4984(3.5019) | Xent 2.3026(2.3026) | Loss 3.4984(3.5019) | Error 0.8989(0.8997) Steps 814(812.93) | Grad Norm 3.3810(2.5492) | Total Time 14.00(14.00)\n",
      "Iter 4990 | Time 20.0774(20.0433) | Bit/dim 3.4706(3.5013) | Xent 2.3026(2.3026) | Loss 3.4706(3.5013) | Error 0.9033(0.8999) Steps 820(814.27) | Grad Norm 2.4353(2.7129) | Total Time 14.00(14.00)\n",
      "Iter 5000 | Time 19.3203(20.0178) | Bit/dim 3.4969(3.5035) | Xent 2.3026(2.3026) | Loss 3.4969(3.5035) | Error 0.9056(0.9011) Steps 802(813.76) | Grad Norm 2.9209(2.5480) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0091 | Time 101.0587, Epoch Time 1219.9498(1174.5576), Bit/dim 3.4998(best: 3.5055), Xent 2.3026, Loss 3.4998, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5010 | Time 19.9527(19.9981) | Bit/dim 3.5120(3.5042) | Xent 2.3026(2.3026) | Loss 3.5120(3.5042) | Error 0.9111(0.9004) Steps 802(812.07) | Grad Norm 2.1543(2.4035) | Total Time 14.00(14.00)\n",
      "Iter 5020 | Time 20.3805(20.0805) | Bit/dim 3.4900(3.5027) | Xent 2.3026(2.3026) | Loss 3.4900(3.5027) | Error 0.9044(0.9008) Steps 820(812.42) | Grad Norm 1.8216(2.6228) | Total Time 14.00(14.00)\n",
      "Iter 5030 | Time 20.2632(20.1067) | Bit/dim 3.5021(3.5036) | Xent 2.3026(2.3026) | Loss 3.5021(3.5036) | Error 0.8989(0.9013) Steps 826(813.14) | Grad Norm 1.4701(2.4462) | Total Time 14.00(14.00)\n",
      "Iter 5040 | Time 19.9490(20.1689) | Bit/dim 3.5073(3.5014) | Xent 2.3026(2.3026) | Loss 3.5073(3.5014) | Error 0.9033(0.9007) Steps 820(815.57) | Grad Norm 4.1783(2.4630) | Total Time 14.00(14.00)\n",
      "Iter 5050 | Time 20.5033(20.1529) | Bit/dim 3.4898(3.5002) | Xent 2.3026(2.3026) | Loss 3.4898(3.5002) | Error 0.8844(0.9000) Steps 814(815.72) | Grad Norm 1.9238(2.6380) | Total Time 14.00(14.00)\n",
      "Iter 5060 | Time 19.6558(20.1141) | Bit/dim 3.4964(3.4989) | Xent 2.3026(2.3026) | Loss 3.4964(3.4989) | Error 0.9033(0.8994) Steps 820(814.27) | Grad Norm 2.0224(2.4516) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0092 | Time 102.0016, Epoch Time 1229.3287(1176.2008), Bit/dim 3.5007(best: 3.4998), Xent 2.3026, Loss 3.5007, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5070 | Time 20.0904(20.1315) | Bit/dim 3.5051(3.4981) | Xent 2.3026(2.3026) | Loss 3.5051(3.4981) | Error 0.8878(0.8986) Steps 808(812.78) | Grad Norm 2.1465(2.5882) | Total Time 14.00(14.00)\n",
      "Iter 5080 | Time 20.3675(20.1350) | Bit/dim 3.4891(3.5003) | Xent 2.3026(2.3026) | Loss 3.4891(3.5003) | Error 0.8900(0.9003) Steps 802(812.80) | Grad Norm 2.5740(2.4661) | Total Time 14.00(14.00)\n",
      "Iter 5090 | Time 20.0214(20.0885) | Bit/dim 3.4913(3.5010) | Xent 2.3026(2.3026) | Loss 3.4913(3.5010) | Error 0.9089(0.8997) Steps 808(813.98) | Grad Norm 3.4837(2.4207) | Total Time 14.00(14.00)\n",
      "Iter 5100 | Time 19.5598(20.1684) | Bit/dim 3.5009(3.5010) | Xent 2.3026(2.3026) | Loss 3.5009(3.5010) | Error 0.9000(0.9000) Steps 820(815.48) | Grad Norm 3.2075(2.5097) | Total Time 14.00(14.00)\n",
      "Iter 5110 | Time 19.5745(20.1999) | Bit/dim 3.4879(3.4979) | Xent 2.3026(2.3026) | Loss 3.4879(3.4979) | Error 0.9167(0.9008) Steps 814(815.05) | Grad Norm 2.5215(2.4888) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0093 | Time 103.0240, Epoch Time 1230.9530(1177.8433), Bit/dim 3.5026(best: 3.4998), Xent 2.3026, Loss 3.5026, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5120 | Time 19.9450(20.1887) | Bit/dim 3.5208(3.4991) | Xent 2.3026(2.3026) | Loss 3.5208(3.4991) | Error 0.9133(0.9003) Steps 808(814.84) | Grad Norm 1.5985(2.4976) | Total Time 14.00(14.00)\n",
      "Iter 5130 | Time 19.8184(20.0718) | Bit/dim 3.4996(3.4982) | Xent 2.3026(2.3026) | Loss 3.4996(3.4982) | Error 0.9011(0.9018) Steps 802(814.02) | Grad Norm 2.7368(2.5249) | Total Time 14.00(14.00)\n",
      "Iter 5140 | Time 19.9839(20.0566) | Bit/dim 3.4982(3.4966) | Xent 2.3026(2.3026) | Loss 3.4982(3.4966) | Error 0.9078(0.8995) Steps 808(813.33) | Grad Norm 2.0907(2.5376) | Total Time 14.00(14.00)\n",
      "Iter 5150 | Time 20.4637(20.0870) | Bit/dim 3.5273(3.4969) | Xent 2.3026(2.3026) | Loss 3.5273(3.4969) | Error 0.9033(0.8993) Steps 820(814.52) | Grad Norm 2.1770(2.6007) | Total Time 14.00(14.00)\n",
      "Iter 5160 | Time 20.0022(20.1164) | Bit/dim 3.5081(3.4998) | Xent 2.3026(2.3026) | Loss 3.5081(3.4998) | Error 0.9033(0.9000) Steps 820(815.39) | Grad Norm 2.6190(2.5136) | Total Time 14.00(14.00)\n",
      "Iter 5170 | Time 19.7790(20.0528) | Bit/dim 3.5066(3.4994) | Xent 2.3026(2.3026) | Loss 3.5066(3.4994) | Error 0.9100(0.8999) Steps 820(813.90) | Grad Norm 1.9141(2.3519) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0094 | Time 102.3860, Epoch Time 1221.9031(1179.1651), Bit/dim 3.4968(best: 3.4998), Xent 2.3026, Loss 3.4968, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5180 | Time 20.2065(20.0486) | Bit/dim 3.5215(3.5014) | Xent 2.3026(2.3026) | Loss 3.5215(3.5014) | Error 0.9078(0.8990) Steps 820(815.54) | Grad Norm 2.9857(2.5744) | Total Time 14.00(14.00)\n",
      "Iter 5190 | Time 20.2982(20.0395) | Bit/dim 3.4568(3.4993) | Xent 2.3026(2.3026) | Loss 3.4568(3.4993) | Error 0.8833(0.8983) Steps 826(816.45) | Grad Norm 1.9364(2.5797) | Total Time 14.00(14.00)\n",
      "Iter 5200 | Time 20.0691(20.0224) | Bit/dim 3.4648(3.4961) | Xent 2.3026(2.3026) | Loss 3.4648(3.4961) | Error 0.9000(0.8992) Steps 820(816.99) | Grad Norm 4.6016(2.6301) | Total Time 14.00(14.00)\n",
      "Iter 5210 | Time 19.8791(19.9973) | Bit/dim 3.4859(3.4951) | Xent 2.3026(2.3026) | Loss 3.4859(3.4951) | Error 0.9122(0.8988) Steps 814(814.80) | Grad Norm 1.6128(2.6071) | Total Time 14.00(14.00)\n",
      "Iter 5220 | Time 20.1948(20.0190) | Bit/dim 3.4870(3.4974) | Xent 2.3026(2.3026) | Loss 3.4870(3.4974) | Error 0.9078(0.9017) Steps 832(816.64) | Grad Norm 1.8444(2.5943) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0095 | Time 104.0078, Epoch Time 1222.7827(1180.4736), Bit/dim 3.4980(best: 3.4968), Xent 2.3026, Loss 3.4980, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5230 | Time 20.0836(20.0460) | Bit/dim 3.5036(3.4962) | Xent 2.3026(2.3026) | Loss 3.5036(3.4962) | Error 0.9056(0.9007) Steps 814(819.48) | Grad Norm 1.2577(2.5814) | Total Time 14.00(14.00)\n",
      "Iter 5240 | Time 20.3174(20.0404) | Bit/dim 3.4970(3.4942) | Xent 2.3026(2.3026) | Loss 3.4970(3.4942) | Error 0.8989(0.9007) Steps 838(818.47) | Grad Norm 2.3341(2.4645) | Total Time 14.00(14.00)\n",
      "Iter 5250 | Time 21.1552(20.1155) | Bit/dim 3.4826(3.4933) | Xent 2.3026(2.3026) | Loss 3.4826(3.4933) | Error 0.9078(0.8999) Steps 832(819.18) | Grad Norm 2.0898(2.6106) | Total Time 14.00(14.00)\n",
      "Iter 5260 | Time 19.5482(20.1405) | Bit/dim 3.5148(3.4941) | Xent 2.3026(2.3026) | Loss 3.5148(3.4941) | Error 0.8989(0.9004) Steps 820(817.56) | Grad Norm 2.5095(2.4658) | Total Time 14.00(14.00)\n",
      "Iter 5270 | Time 19.9772(20.1798) | Bit/dim 3.5017(3.4920) | Xent 2.3026(2.3026) | Loss 3.5017(3.4920) | Error 0.8911(0.8998) Steps 820(819.79) | Grad Norm 2.3922(2.5673) | Total Time 14.00(14.00)\n",
      "Iter 5280 | Time 20.0276(20.1475) | Bit/dim 3.5375(3.4951) | Xent 2.3026(2.3026) | Loss 3.5375(3.4951) | Error 0.9156(0.9008) Steps 820(819.39) | Grad Norm 1.7705(2.3846) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0096 | Time 104.0876, Epoch Time 1231.9416(1182.0177), Bit/dim 3.4940(best: 3.4968), Xent 2.3026, Loss 3.4940, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5290 | Time 20.2765(20.1651) | Bit/dim 3.5071(3.4954) | Xent 2.3026(2.3026) | Loss 3.5071(3.4954) | Error 0.8756(0.8991) Steps 820(819.40) | Grad Norm 2.1884(2.4200) | Total Time 14.00(14.00)\n",
      "Iter 5300 | Time 20.0534(20.2255) | Bit/dim 3.5093(3.4925) | Xent 2.3026(2.3026) | Loss 3.5093(3.4925) | Error 0.9167(0.8986) Steps 832(821.17) | Grad Norm 1.7982(2.4342) | Total Time 14.00(14.00)\n",
      "Iter 5310 | Time 19.8717(20.2044) | Bit/dim 3.5279(3.4933) | Xent 2.3026(2.3026) | Loss 3.5279(3.4933) | Error 0.9178(0.8997) Steps 832(820.61) | Grad Norm 2.8854(2.3819) | Total Time 14.00(14.00)\n",
      "Iter 5320 | Time 20.4472(20.2070) | Bit/dim 3.4686(3.4915) | Xent 2.3026(2.3026) | Loss 3.4686(3.4915) | Error 0.9056(0.9008) Steps 832(820.39) | Grad Norm 1.7775(2.4242) | Total Time 14.00(14.00)\n",
      "Iter 5330 | Time 20.2947(20.1990) | Bit/dim 3.5018(3.4938) | Xent 2.3026(2.3026) | Loss 3.5018(3.4938) | Error 0.9000(0.9013) Steps 802(818.70) | Grad Norm 3.7976(2.5130) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0097 | Time 101.8684, Epoch Time 1231.3198(1183.4967), Bit/dim 3.4931(best: 3.4940), Xent 2.3026, Loss 3.4931, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5340 | Time 20.4646(20.2018) | Bit/dim 3.4791(3.4910) | Xent 2.3026(2.3026) | Loss 3.4791(3.4910) | Error 0.8978(0.9018) Steps 826(820.60) | Grad Norm 3.1862(2.5331) | Total Time 14.00(14.00)\n",
      "Iter 5350 | Time 20.1476(20.2018) | Bit/dim 3.4740(3.4887) | Xent 2.3026(2.3026) | Loss 3.4740(3.4887) | Error 0.8789(0.9005) Steps 820(819.95) | Grad Norm 4.0623(2.6684) | Total Time 14.00(14.00)\n",
      "Iter 5360 | Time 20.3637(20.2119) | Bit/dim 3.4993(3.4899) | Xent 2.3026(2.3026) | Loss 3.4993(3.4899) | Error 0.9000(0.9012) Steps 808(819.19) | Grad Norm 2.2996(2.6033) | Total Time 14.00(14.00)\n",
      "Iter 5370 | Time 19.6437(20.1632) | Bit/dim 3.4967(3.4918) | Xent 2.3026(2.3026) | Loss 3.4967(3.4918) | Error 0.9089(0.9006) Steps 820(817.35) | Grad Norm 2.3037(2.5122) | Total Time 14.00(14.00)\n",
      "Iter 5380 | Time 19.9873(20.1756) | Bit/dim 3.4507(3.4915) | Xent 2.3026(2.3026) | Loss 3.4507(3.4915) | Error 0.8967(0.8999) Steps 826(818.96) | Grad Norm 2.0329(2.4904) | Total Time 14.00(14.00)\n",
      "Iter 5390 | Time 20.4398(20.2219) | Bit/dim 3.4701(3.4908) | Xent 2.3026(2.3026) | Loss 3.4701(3.4908) | Error 0.8967(0.9002) Steps 814(819.20) | Grad Norm 2.7097(2.4372) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0098 | Time 103.5698, Epoch Time 1233.3171(1184.9913), Bit/dim 3.4920(best: 3.4931), Xent 2.3026, Loss 3.4920, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5400 | Time 20.2922(20.2078) | Bit/dim 3.5155(3.4907) | Xent 2.3026(2.3026) | Loss 3.5155(3.4907) | Error 0.8911(0.9001) Steps 808(818.00) | Grad Norm 2.3807(2.5480) | Total Time 14.00(14.00)\n",
      "Iter 5410 | Time 19.9046(20.1760) | Bit/dim 3.4737(3.4898) | Xent 2.3026(2.3026) | Loss 3.4737(3.4898) | Error 0.9078(0.8988) Steps 808(817.74) | Grad Norm 1.2916(2.4404) | Total Time 14.00(14.00)\n",
      "Iter 5420 | Time 19.7979(20.1567) | Bit/dim 3.5007(3.4889) | Xent 2.3026(2.3026) | Loss 3.5007(3.4889) | Error 0.8911(0.8981) Steps 832(818.17) | Grad Norm 3.8868(2.4440) | Total Time 14.00(14.00)\n",
      "Iter 5430 | Time 20.2382(20.1451) | Bit/dim 3.4940(3.4885) | Xent 2.3026(2.3026) | Loss 3.4940(3.4885) | Error 0.9078(0.9004) Steps 826(818.69) | Grad Norm 2.4620(2.4462) | Total Time 14.00(14.00)\n",
      "Iter 5440 | Time 19.7498(20.1066) | Bit/dim 3.5086(3.4896) | Xent 2.3026(2.3026) | Loss 3.5086(3.4896) | Error 0.9033(0.9005) Steps 820(818.47) | Grad Norm 2.2241(2.5215) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0099 | Time 102.2918, Epoch Time 1226.5967(1186.2395), Bit/dim 3.4904(best: 3.4920), Xent 2.3026, Loss 3.4904, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5450 | Time 20.4795(20.1585) | Bit/dim 3.4728(3.4890) | Xent 2.3026(2.3026) | Loss 3.4728(3.4890) | Error 0.9022(0.9013) Steps 832(820.09) | Grad Norm 2.3120(2.4800) | Total Time 14.00(14.00)\n",
      "Iter 5460 | Time 20.2164(20.1644) | Bit/dim 3.4665(3.4887) | Xent 2.3026(2.3026) | Loss 3.4665(3.4887) | Error 0.8944(0.9010) Steps 838(820.46) | Grad Norm 2.0785(2.3598) | Total Time 14.00(14.00)\n",
      "Iter 5470 | Time 20.5958(20.1707) | Bit/dim 3.4848(3.4878) | Xent 2.3026(2.3026) | Loss 3.4848(3.4878) | Error 0.8978(0.9008) Steps 826(820.16) | Grad Norm 2.8182(2.3443) | Total Time 14.00(14.00)\n",
      "Iter 5480 | Time 20.5063(20.2105) | Bit/dim 3.4821(3.4904) | Xent 2.3026(2.3026) | Loss 3.4821(3.4904) | Error 0.9078(0.9007) Steps 838(821.62) | Grad Norm 3.4919(2.3956) | Total Time 14.00(14.00)\n",
      "Iter 5490 | Time 20.0097(20.2608) | Bit/dim 3.4490(3.4875) | Xent 2.3026(2.3026) | Loss 3.4490(3.4875) | Error 0.8944(0.9005) Steps 820(822.00) | Grad Norm 3.5466(2.6270) | Total Time 14.00(14.00)\n",
      "Iter 5500 | Time 20.8345(20.2987) | Bit/dim 3.4813(3.4879) | Xent 2.3026(2.3026) | Loss 3.4813(3.4879) | Error 0.8856(0.8999) Steps 832(823.66) | Grad Norm 2.2606(2.5931) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0100 | Time 103.0284, Epoch Time 1236.8226(1187.7570), Bit/dim 3.4873(best: 3.4904), Xent 2.3026, Loss 3.4873, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5510 | Time 21.4437(20.3844) | Bit/dim 3.4979(3.4857) | Xent 2.3026(2.3026) | Loss 3.4979(3.4857) | Error 0.8922(0.8989) Steps 814(824.18) | Grad Norm 3.1322(2.5379) | Total Time 14.00(14.00)\n",
      "Iter 5520 | Time 19.8048(20.3296) | Bit/dim 3.4954(3.4869) | Xent 2.3026(2.3026) | Loss 3.4954(3.4869) | Error 0.9078(0.8991) Steps 802(823.31) | Grad Norm 3.1660(2.5360) | Total Time 14.00(14.00)\n",
      "Iter 5530 | Time 20.7283(20.2910) | Bit/dim 3.5076(3.4874) | Xent 2.3026(2.3026) | Loss 3.5076(3.4874) | Error 0.9067(0.8999) Steps 820(822.24) | Grad Norm 1.7120(2.5828) | Total Time 14.00(14.00)\n",
      "Iter 5540 | Time 20.3666(20.2586) | Bit/dim 3.4746(3.4875) | Xent 2.3026(2.3026) | Loss 3.4746(3.4875) | Error 0.9078(0.9002) Steps 820(821.54) | Grad Norm 3.2086(2.6149) | Total Time 14.00(14.00)\n",
      "Iter 5550 | Time 20.1300(20.2426) | Bit/dim 3.4887(3.4861) | Xent 2.3026(2.3026) | Loss 3.4887(3.4861) | Error 0.8989(0.9000) Steps 826(819.76) | Grad Norm 2.4083(2.5282) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0101 | Time 101.8615, Epoch Time 1234.3247(1189.1540), Bit/dim 3.4860(best: 3.4873), Xent 2.3026, Loss 3.4860, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5560 | Time 19.8676(20.2540) | Bit/dim 3.4852(3.4892) | Xent 2.3026(2.3026) | Loss 3.4852(3.4892) | Error 0.9011(0.9012) Steps 820(819.53) | Grad Norm 2.7304(2.4546) | Total Time 14.00(14.00)\n",
      "Iter 5570 | Time 20.1469(20.2736) | Bit/dim 3.4723(3.4895) | Xent 2.3026(2.3026) | Loss 3.4723(3.4895) | Error 0.8900(0.9007) Steps 838(821.83) | Grad Norm 2.3637(2.4012) | Total Time 14.00(14.00)\n",
      "Iter 5580 | Time 19.9970(20.2699) | Bit/dim 3.4778(3.4844) | Xent 2.3026(2.3026) | Loss 3.4778(3.4844) | Error 0.8956(0.8998) Steps 820(820.20) | Grad Norm 3.3699(2.5303) | Total Time 14.00(14.00)\n",
      "Iter 5590 | Time 20.1766(20.2197) | Bit/dim 3.4891(3.4837) | Xent 2.3026(2.3026) | Loss 3.4891(3.4837) | Error 0.9056(0.8989) Steps 832(820.68) | Grad Norm 1.7401(2.5298) | Total Time 14.00(14.00)\n",
      "Iter 5600 | Time 20.2897(20.1558) | Bit/dim 3.4669(3.4841) | Xent 2.3026(2.3026) | Loss 3.4669(3.4841) | Error 0.9078(0.9005) Steps 820(821.04) | Grad Norm 1.9245(2.3369) | Total Time 14.00(14.00)\n",
      "Iter 5610 | Time 19.7416(20.1119) | Bit/dim 3.4932(3.4853) | Xent 2.3026(2.3026) | Loss 3.4932(3.4853) | Error 0.8900(0.9001) Steps 826(820.84) | Grad Norm 2.6240(2.4408) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0102 | Time 104.2824, Epoch Time 1230.1242(1190.3831), Bit/dim 3.4864(best: 3.4860), Xent 2.3026, Loss 3.4864, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5620 | Time 19.6195(20.1013) | Bit/dim 3.5046(3.4867) | Xent 2.3026(2.3026) | Loss 3.5046(3.4867) | Error 0.9000(0.9004) Steps 814(820.48) | Grad Norm 2.7145(2.4018) | Total Time 14.00(14.00)\n",
      "Iter 5630 | Time 20.2526(20.1431) | Bit/dim 3.4988(3.4880) | Xent 2.3026(2.3026) | Loss 3.4988(3.4880) | Error 0.9033(0.8992) Steps 814(820.04) | Grad Norm 2.2057(2.5710) | Total Time 14.00(14.00)\n",
      "Iter 5640 | Time 20.5929(20.1723) | Bit/dim 3.4629(3.4842) | Xent 2.3026(2.3026) | Loss 3.4629(3.4842) | Error 0.9122(0.9005) Steps 820(819.53) | Grad Norm 1.6602(2.4630) | Total Time 14.00(14.00)\n",
      "Iter 5650 | Time 20.0470(20.2458) | Bit/dim 3.4588(3.4844) | Xent 2.3026(2.3026) | Loss 3.4588(3.4844) | Error 0.8789(0.9007) Steps 808(821.02) | Grad Norm 2.1399(2.3534) | Total Time 14.00(14.00)\n",
      "Iter 5660 | Time 20.4859(20.2506) | Bit/dim 3.4912(3.4820) | Xent 2.3026(2.3026) | Loss 3.4912(3.4820) | Error 0.8933(0.8995) Steps 826(821.32) | Grad Norm 3.2065(2.3790) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0103 | Time 103.4200, Epoch Time 1236.6306(1191.7706), Bit/dim 3.4836(best: 3.4860), Xent 2.3026, Loss 3.4836, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5670 | Time 20.5195(20.2951) | Bit/dim 3.4908(3.4797) | Xent 2.3026(2.3026) | Loss 3.4908(3.4797) | Error 0.9044(0.8993) Steps 826(820.40) | Grad Norm 1.8486(2.4398) | Total Time 14.00(14.00)\n",
      "Iter 5680 | Time 19.9738(20.3082) | Bit/dim 3.5157(3.4809) | Xent 2.3026(2.3026) | Loss 3.5157(3.4809) | Error 0.9144(0.8998) Steps 820(822.01) | Grad Norm 4.7232(2.4415) | Total Time 14.00(14.00)\n",
      "Iter 5690 | Time 20.4391(20.3111) | Bit/dim 3.4779(3.4825) | Xent 2.3026(2.3026) | Loss 3.4779(3.4825) | Error 0.8978(0.8988) Steps 808(821.06) | Grad Norm 2.5377(2.5603) | Total Time 14.00(14.00)\n",
      "Iter 5700 | Time 20.5888(20.3339) | Bit/dim 3.4857(3.4825) | Xent 2.3026(2.3026) | Loss 3.4857(3.4825) | Error 0.8967(0.9001) Steps 808(821.07) | Grad Norm 1.0282(2.5002) | Total Time 14.00(14.00)\n",
      "Iter 5710 | Time 20.1326(20.3099) | Bit/dim 3.5164(3.4824) | Xent 2.3026(2.3026) | Loss 3.5164(3.4824) | Error 0.9011(0.8997) Steps 832(821.67) | Grad Norm 2.8206(2.5028) | Total Time 14.00(14.00)\n",
      "Iter 5720 | Time 19.6033(20.2859) | Bit/dim 3.4607(3.4817) | Xent 2.3026(2.3026) | Loss 3.4607(3.4817) | Error 0.9078(0.9007) Steps 832(822.63) | Grad Norm 2.0010(2.4055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0104 | Time 104.5560, Epoch Time 1239.4285(1193.2003), Bit/dim 3.4811(best: 3.4836), Xent 2.3026, Loss 3.4811, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5730 | Time 20.1777(20.3185) | Bit/dim 3.5342(3.4823) | Xent 2.3026(2.3026) | Loss 3.5342(3.4823) | Error 0.9022(0.9006) Steps 802(823.11) | Grad Norm 3.0255(2.6169) | Total Time 14.00(14.00)\n",
      "Iter 5740 | Time 20.9237(20.3739) | Bit/dim 3.4955(3.4819) | Xent 2.3026(2.3026) | Loss 3.4955(3.4819) | Error 0.8922(0.9007) Steps 814(821.39) | Grad Norm 2.7594(2.5195) | Total Time 14.00(14.00)\n",
      "Iter 5750 | Time 20.1647(20.3392) | Bit/dim 3.4657(3.4810) | Xent 2.3026(2.3026) | Loss 3.4657(3.4810) | Error 0.8911(0.8992) Steps 832(821.33) | Grad Norm 2.1257(2.4102) | Total Time 14.00(14.00)\n",
      "Iter 5760 | Time 20.3616(20.4087) | Bit/dim 3.4551(3.4832) | Xent 2.3026(2.3026) | Loss 3.4551(3.4832) | Error 0.8856(0.9001) Steps 838(823.43) | Grad Norm 1.6259(2.5238) | Total Time 14.00(14.00)\n",
      "Iter 5770 | Time 20.7221(20.4472) | Bit/dim 3.4827(3.4815) | Xent 2.3026(2.3026) | Loss 3.4827(3.4815) | Error 0.9089(0.9002) Steps 808(823.82) | Grad Norm 2.1099(2.5142) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0105 | Time 103.5148, Epoch Time 1247.6455(1194.8337), Bit/dim 3.4845(best: 3.4811), Xent 2.3026, Loss 3.4845, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5780 | Time 20.4950(20.4589) | Bit/dim 3.4642(3.4775) | Xent 2.3026(2.3026) | Loss 3.4642(3.4775) | Error 0.9033(0.8994) Steps 826(825.61) | Grad Norm 4.0197(2.4942) | Total Time 14.00(14.00)\n",
      "Iter 5790 | Time 19.8122(20.4606) | Bit/dim 3.4905(3.4775) | Xent 2.3026(2.3026) | Loss 3.4905(3.4775) | Error 0.8989(0.8996) Steps 844(827.43) | Grad Norm 2.1410(2.4364) | Total Time 14.00(14.00)\n",
      "Iter 5800 | Time 20.5132(20.4393) | Bit/dim 3.4949(3.4804) | Xent 2.3026(2.3026) | Loss 3.4949(3.4804) | Error 0.9067(0.9001) Steps 808(825.30) | Grad Norm 1.1641(2.4909) | Total Time 14.00(14.00)\n",
      "Iter 5810 | Time 20.3940(20.4106) | Bit/dim 3.4503(3.4782) | Xent 2.3026(2.3026) | Loss 3.4503(3.4782) | Error 0.9000(0.8986) Steps 850(825.73) | Grad Norm 1.5523(2.3814) | Total Time 14.00(14.00)\n",
      "Iter 5820 | Time 20.0189(20.4033) | Bit/dim 3.4840(3.4788) | Xent 2.3026(2.3026) | Loss 3.4840(3.4788) | Error 0.9044(0.8996) Steps 838(826.37) | Grad Norm 2.7667(2.3445) | Total Time 14.00(14.00)\n",
      "Iter 5830 | Time 20.0495(20.4285) | Bit/dim 3.4663(3.4786) | Xent 2.3026(2.3026) | Loss 3.4663(3.4786) | Error 0.9011(0.9004) Steps 832(826.39) | Grad Norm 1.8169(2.2357) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0106 | Time 103.1330, Epoch Time 1243.5030(1196.2937), Bit/dim 3.4803(best: 3.4811), Xent 2.3026, Loss 3.4803, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5840 | Time 20.9746(20.3688) | Bit/dim 3.4936(3.4778) | Xent 2.3026(2.3026) | Loss 3.4936(3.4778) | Error 0.8956(0.8991) Steps 832(826.34) | Grad Norm 3.5499(2.1862) | Total Time 14.00(14.00)\n",
      "Iter 5850 | Time 20.4005(20.3394) | Bit/dim 3.4863(3.4785) | Xent 2.3026(2.3026) | Loss 3.4863(3.4785) | Error 0.8722(0.8981) Steps 832(825.74) | Grad Norm 2.7547(2.2501) | Total Time 14.00(14.00)\n",
      "Iter 5860 | Time 20.6854(20.3685) | Bit/dim 3.4901(3.4810) | Xent 2.3026(2.3026) | Loss 3.4901(3.4810) | Error 0.9111(0.8992) Steps 814(824.44) | Grad Norm 2.1650(2.3442) | Total Time 14.00(14.00)\n",
      "Iter 5870 | Time 20.3374(20.4031) | Bit/dim 3.4906(3.4793) | Xent 2.3026(2.3026) | Loss 3.4906(3.4793) | Error 0.9033(0.9010) Steps 802(823.61) | Grad Norm 2.4771(2.3292) | Total Time 14.00(14.00)\n",
      "Iter 5880 | Time 20.2223(20.4562) | Bit/dim 3.4667(3.4774) | Xent 2.3026(2.3026) | Loss 3.4667(3.4774) | Error 0.8911(0.9001) Steps 850(825.63) | Grad Norm 2.6439(2.4234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0107 | Time 103.3536, Epoch Time 1243.4625(1197.7088), Bit/dim 3.4754(best: 3.4803), Xent 2.3026, Loss 3.4754, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5890 | Time 19.7412(20.4096) | Bit/dim 3.4757(3.4748) | Xent 2.3026(2.3026) | Loss 3.4757(3.4748) | Error 0.8922(0.9010) Steps 820(825.94) | Grad Norm 2.8434(2.4739) | Total Time 14.00(14.00)\n",
      "Iter 5900 | Time 19.8803(20.3473) | Bit/dim 3.5097(3.4747) | Xent 2.3026(2.3026) | Loss 3.5097(3.4747) | Error 0.8967(0.9001) Steps 832(824.76) | Grad Norm 2.6891(2.4659) | Total Time 14.00(14.00)\n",
      "Iter 5910 | Time 20.0795(20.3243) | Bit/dim 3.4638(3.4727) | Xent 2.3026(2.3026) | Loss 3.4638(3.4727) | Error 0.8867(0.9012) Steps 820(825.31) | Grad Norm 1.8260(2.3937) | Total Time 14.00(14.00)\n",
      "Iter 5920 | Time 20.3174(20.3371) | Bit/dim 3.4547(3.4734) | Xent 2.3026(2.3026) | Loss 3.4547(3.4734) | Error 0.8978(0.9009) Steps 832(824.64) | Grad Norm 1.6693(2.3179) | Total Time 14.00(14.00)\n",
      "Iter 5930 | Time 20.9642(20.3638) | Bit/dim 3.4862(3.4778) | Xent 2.3026(2.3026) | Loss 3.4862(3.4778) | Error 0.9056(0.9010) Steps 808(824.34) | Grad Norm 4.0553(2.4687) | Total Time 14.00(14.00)\n",
      "Iter 5940 | Time 20.4813(20.4922) | Bit/dim 3.4774(3.4768) | Xent 2.3026(2.3026) | Loss 3.4774(3.4768) | Error 0.9111(0.8996) Steps 832(824.50) | Grad Norm 2.6090(2.4550) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0108 | Time 104.4122, Epoch Time 1245.0709(1199.1297), Bit/dim 3.4779(best: 3.4754), Xent 2.3026, Loss 3.4779, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 5950 | Time 20.2277(20.4086) | Bit/dim 3.4963(3.4753) | Xent 2.3026(2.3026) | Loss 3.4963(3.4753) | Error 0.9022(0.8991) Steps 808(826.94) | Grad Norm 2.1596(2.2942) | Total Time 14.00(14.00)\n",
      "Iter 5960 | Time 20.0037(20.4380) | Bit/dim 3.4724(3.4749) | Xent 2.3026(2.3026) | Loss 3.4724(3.4749) | Error 0.8989(0.8987) Steps 832(828.20) | Grad Norm 1.9222(2.3463) | Total Time 14.00(14.00)\n",
      "Iter 5970 | Time 20.5211(20.4808) | Bit/dim 3.4838(3.4779) | Xent 2.3026(2.3026) | Loss 3.4838(3.4779) | Error 0.8844(0.8997) Steps 832(828.01) | Grad Norm 2.6145(2.3893) | Total Time 14.00(14.00)\n",
      "Iter 5980 | Time 21.0825(20.5028) | Bit/dim 3.4871(3.4762) | Xent 2.3026(2.3026) | Loss 3.4871(3.4762) | Error 0.9056(0.9006) Steps 844(830.68) | Grad Norm 2.3677(2.3735) | Total Time 14.00(14.00)\n",
      "Iter 5990 | Time 20.9498(20.4672) | Bit/dim 3.4636(3.4754) | Xent 2.3026(2.3026) | Loss 3.4636(3.4754) | Error 0.8822(0.9001) Steps 850(831.39) | Grad Norm 2.6091(2.4730) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0109 | Time 104.7577, Epoch Time 1247.1033(1200.5689), Bit/dim 3.4763(best: 3.4754), Xent 2.3026, Loss 3.4763, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6000 | Time 20.2641(20.4132) | Bit/dim 3.4484(3.4734) | Xent 2.3026(2.3026) | Loss 3.4484(3.4734) | Error 0.9044(0.9001) Steps 796(829.04) | Grad Norm 1.4524(2.3379) | Total Time 14.00(14.00)\n",
      "Iter 6010 | Time 20.5787(20.4474) | Bit/dim 3.4589(3.4742) | Xent 2.3026(2.3026) | Loss 3.4589(3.4742) | Error 0.9000(0.9007) Steps 814(829.20) | Grad Norm 3.2016(2.3579) | Total Time 14.00(14.00)\n",
      "Iter 6020 | Time 20.7023(20.4072) | Bit/dim 3.5055(3.4733) | Xent 2.3026(2.3026) | Loss 3.5055(3.4733) | Error 0.8811(0.8996) Steps 838(829.20) | Grad Norm 2.1115(2.3891) | Total Time 14.00(14.00)\n",
      "Iter 6030 | Time 20.7585(20.4770) | Bit/dim 3.4747(3.4743) | Xent 2.3026(2.3026) | Loss 3.4747(3.4743) | Error 0.8933(0.8993) Steps 850(828.90) | Grad Norm 2.5279(2.3788) | Total Time 14.00(14.00)\n",
      "Iter 6040 | Time 20.6114(20.5340) | Bit/dim 3.4493(3.4747) | Xent 2.3026(2.3026) | Loss 3.4493(3.4747) | Error 0.9067(0.9009) Steps 814(828.14) | Grad Norm 2.0871(2.3831) | Total Time 14.00(14.00)\n",
      "Iter 6050 | Time 20.8572(20.5200) | Bit/dim 3.4590(3.4745) | Xent 2.3026(2.3026) | Loss 3.4590(3.4745) | Error 0.8900(0.9004) Steps 856(828.50) | Grad Norm 2.9256(2.4813) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0110 | Time 104.8000, Epoch Time 1249.8912(1202.0485), Bit/dim 3.4746(best: 3.4754), Xent 2.3026, Loss 3.4746, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6060 | Time 20.4266(20.5680) | Bit/dim 3.5094(3.4739) | Xent 2.3026(2.3026) | Loss 3.5094(3.4739) | Error 0.9000(0.9000) Steps 814(828.38) | Grad Norm 1.5443(2.3642) | Total Time 14.00(14.00)\n",
      "Iter 6070 | Time 21.5372(20.5443) | Bit/dim 3.4655(3.4754) | Xent 2.3026(2.3026) | Loss 3.4655(3.4754) | Error 0.9156(0.9004) Steps 838(827.68) | Grad Norm 3.2572(2.2712) | Total Time 14.00(14.00)\n",
      "Iter 6080 | Time 20.0380(20.5642) | Bit/dim 3.4492(3.4756) | Xent 2.3026(2.3026) | Loss 3.4492(3.4756) | Error 0.9056(0.9004) Steps 826(831.09) | Grad Norm 2.2124(2.3273) | Total Time 14.00(14.00)\n",
      "Iter 6090 | Time 19.7606(20.5513) | Bit/dim 3.4367(3.4726) | Xent 2.3026(2.3026) | Loss 3.4367(3.4726) | Error 0.9111(0.9006) Steps 826(831.21) | Grad Norm 4.2996(2.4507) | Total Time 14.00(14.00)\n",
      "Iter 6100 | Time 20.6615(20.5794) | Bit/dim 3.4901(3.4729) | Xent 2.3026(2.3026) | Loss 3.4901(3.4729) | Error 0.9189(0.8999) Steps 820(832.75) | Grad Norm 1.9764(2.5064) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0111 | Time 105.1134, Epoch Time 1255.5441(1203.6534), Bit/dim 3.4743(best: 3.4746), Xent 2.3026, Loss 3.4743, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6110 | Time 20.4149(20.5771) | Bit/dim 3.5115(3.4728) | Xent 2.3026(2.3026) | Loss 3.5115(3.4728) | Error 0.9144(0.9008) Steps 844(833.32) | Grad Norm 1.7930(2.4590) | Total Time 14.00(14.00)\n",
      "Iter 6120 | Time 20.1705(20.6478) | Bit/dim 3.4657(3.4709) | Xent 2.3026(2.3026) | Loss 3.4657(3.4709) | Error 0.8889(0.8998) Steps 862(836.71) | Grad Norm 2.7709(2.3827) | Total Time 14.00(14.00)\n",
      "Iter 6130 | Time 20.3728(20.6672) | Bit/dim 3.4782(3.4695) | Xent 2.3026(2.3026) | Loss 3.4782(3.4695) | Error 0.8900(0.8992) Steps 838(835.85) | Grad Norm 3.4374(2.3880) | Total Time 14.00(14.00)\n",
      "Iter 6140 | Time 20.3700(20.6557) | Bit/dim 3.4715(3.4702) | Xent 2.3026(2.3026) | Loss 3.4715(3.4702) | Error 0.9233(0.8996) Steps 814(834.47) | Grad Norm 1.5473(2.4466) | Total Time 14.00(14.00)\n",
      "Iter 6150 | Time 20.3661(20.6458) | Bit/dim 3.4846(3.4709) | Xent 2.3026(2.3026) | Loss 3.4846(3.4709) | Error 0.9022(0.9000) Steps 850(833.08) | Grad Norm 2.8286(2.4708) | Total Time 14.00(14.00)\n",
      "Iter 6160 | Time 20.9580(20.6283) | Bit/dim 3.4956(3.4709) | Xent 2.3026(2.3026) | Loss 3.4956(3.4709) | Error 0.9144(0.9003) Steps 850(832.93) | Grad Norm 2.3740(2.3076) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0112 | Time 105.3015, Epoch Time 1260.3179(1205.3533), Bit/dim 3.4729(best: 3.4743), Xent 2.3026, Loss 3.4729, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6170 | Time 20.4782(20.6231) | Bit/dim 3.4961(3.4711) | Xent 2.3026(2.3026) | Loss 3.4961(3.4711) | Error 0.9056(0.9002) Steps 826(833.43) | Grad Norm 2.2261(2.3289) | Total Time 14.00(14.00)\n",
      "Iter 6180 | Time 20.7334(20.5951) | Bit/dim 3.4393(3.4699) | Xent 2.3026(2.3026) | Loss 3.4393(3.4699) | Error 0.9178(0.9005) Steps 838(833.06) | Grad Norm 1.7375(2.3253) | Total Time 14.00(14.00)\n",
      "Iter 6190 | Time 20.5456(20.6455) | Bit/dim 3.4763(3.4704) | Xent 2.3026(2.3026) | Loss 3.4763(3.4704) | Error 0.9100(0.9011) Steps 838(834.68) | Grad Norm 1.1424(2.3443) | Total Time 14.00(14.00)\n",
      "Iter 6200 | Time 20.6865(20.6228) | Bit/dim 3.4606(3.4720) | Xent 2.3026(2.3026) | Loss 3.4606(3.4720) | Error 0.9011(0.9000) Steps 868(836.71) | Grad Norm 2.1477(2.3929) | Total Time 14.00(14.00)\n",
      "Iter 6210 | Time 20.4781(20.5862) | Bit/dim 3.4376(3.4693) | Xent 2.3026(2.3026) | Loss 3.4376(3.4693) | Error 0.8933(0.8997) Steps 820(837.48) | Grad Norm 2.2228(2.4374) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0113 | Time 106.0316, Epoch Time 1253.5957(1206.8006), Bit/dim 3.4706(best: 3.4729), Xent 2.3026, Loss 3.4706, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6220 | Time 21.0651(20.4895) | Bit/dim 3.4987(3.4718) | Xent 2.3026(2.3026) | Loss 3.4987(3.4718) | Error 0.8989(0.9004) Steps 862(835.76) | Grad Norm 1.6419(2.2725) | Total Time 14.00(14.00)\n",
      "Iter 6230 | Time 20.8814(20.5434) | Bit/dim 3.4140(3.4670) | Xent 2.3026(2.3026) | Loss 3.4140(3.4670) | Error 0.8933(0.9003) Steps 862(838.60) | Grad Norm 4.0740(2.4611) | Total Time 14.00(14.00)\n",
      "Iter 6240 | Time 21.2082(20.5937) | Bit/dim 3.4564(3.4677) | Xent 2.3026(2.3026) | Loss 3.4564(3.4677) | Error 0.9011(0.9006) Steps 844(837.08) | Grad Norm 1.4514(2.5648) | Total Time 14.00(14.00)\n",
      "Iter 6250 | Time 20.8002(20.6392) | Bit/dim 3.4774(3.4678) | Xent 2.3026(2.3026) | Loss 3.4774(3.4678) | Error 0.8933(0.8990) Steps 850(836.21) | Grad Norm 2.3438(2.4811) | Total Time 14.00(14.00)\n",
      "Iter 6260 | Time 20.6656(20.6962) | Bit/dim 3.4621(3.4681) | Xent 2.3026(2.3026) | Loss 3.4621(3.4681) | Error 0.9056(0.8998) Steps 826(835.71) | Grad Norm 2.4097(2.4200) | Total Time 14.00(14.00)\n",
      "Iter 6270 | Time 19.9998(20.7170) | Bit/dim 3.4782(3.4703) | Xent 2.3026(2.3026) | Loss 3.4782(3.4703) | Error 0.9111(0.9004) Steps 832(837.39) | Grad Norm 1.5755(2.3210) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0114 | Time 104.3342, Epoch Time 1262.5300(1208.4725), Bit/dim 3.4683(best: 3.4706), Xent 2.3026, Loss 3.4683, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6280 | Time 21.0722(20.7056) | Bit/dim 3.4756(3.4698) | Xent 2.3026(2.3026) | Loss 3.4756(3.4698) | Error 0.9044(0.9005) Steps 832(836.37) | Grad Norm 2.7285(2.2787) | Total Time 14.00(14.00)\n",
      "Iter 6290 | Time 20.8661(20.7405) | Bit/dim 3.4975(3.4690) | Xent 2.3026(2.3026) | Loss 3.4975(3.4690) | Error 0.8989(0.8997) Steps 832(836.28) | Grad Norm 3.8446(2.3131) | Total Time 14.00(14.00)\n",
      "Iter 6300 | Time 20.9681(20.7725) | Bit/dim 3.4542(3.4675) | Xent 2.3026(2.3026) | Loss 3.4542(3.4675) | Error 0.8989(0.8995) Steps 838(836.63) | Grad Norm 1.6837(2.2404) | Total Time 14.00(14.00)\n",
      "Iter 6310 | Time 21.1691(20.7462) | Bit/dim 3.4568(3.4680) | Xent 2.3026(2.3026) | Loss 3.4568(3.4680) | Error 0.9022(0.9009) Steps 856(838.69) | Grad Norm 2.1265(2.3344) | Total Time 14.00(14.00)\n",
      "Iter 6320 | Time 21.0914(20.7777) | Bit/dim 3.4934(3.4696) | Xent 2.3026(2.3026) | Loss 3.4934(3.4696) | Error 0.8978(0.9010) Steps 850(842.11) | Grad Norm 2.6363(2.3487) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0115 | Time 103.7709, Epoch Time 1264.0501(1210.1398), Bit/dim 3.4700(best: 3.4683), Xent 2.3026, Loss 3.4700, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6330 | Time 20.3190(20.7290) | Bit/dim 3.4953(3.4701) | Xent 2.3026(2.3026) | Loss 3.4953(3.4701) | Error 0.9078(0.9008) Steps 826(842.22) | Grad Norm 2.0450(2.2895) | Total Time 14.00(14.00)\n",
      "Iter 6340 | Time 20.3568(20.6826) | Bit/dim 3.4766(3.4701) | Xent 2.3026(2.3026) | Loss 3.4766(3.4701) | Error 0.8944(0.9010) Steps 832(839.89) | Grad Norm 3.1224(2.1951) | Total Time 14.00(14.00)\n",
      "Iter 6350 | Time 20.2912(20.6933) | Bit/dim 3.4787(3.4702) | Xent 2.3026(2.3026) | Loss 3.4787(3.4702) | Error 0.8911(0.9009) Steps 832(839.93) | Grad Norm 2.9531(2.3133) | Total Time 14.00(14.00)\n",
      "Iter 6360 | Time 20.1440(20.6672) | Bit/dim 3.4663(3.4672) | Xent 2.3026(2.3026) | Loss 3.4663(3.4672) | Error 0.9078(0.9005) Steps 844(842.44) | Grad Norm 3.6216(2.3443) | Total Time 14.00(14.00)\n",
      "Iter 6370 | Time 20.5604(20.7113) | Bit/dim 3.4438(3.4635) | Xent 2.3026(2.3026) | Loss 3.4438(3.4635) | Error 0.8922(0.8995) Steps 856(843.30) | Grad Norm 1.5113(2.2758) | Total Time 14.00(14.00)\n",
      "Iter 6380 | Time 19.4275(20.6901) | Bit/dim 3.4595(3.4628) | Xent 2.3026(2.3026) | Loss 3.4595(3.4628) | Error 0.8944(0.8993) Steps 838(843.65) | Grad Norm 2.6726(2.3065) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0116 | Time 105.3337, Epoch Time 1260.1476(1211.6401), Bit/dim 3.4702(best: 3.4683), Xent 2.3026, Loss 3.4702, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6390 | Time 20.4526(20.6730) | Bit/dim 3.4425(3.4616) | Xent 2.3026(2.3026) | Loss 3.4425(3.4616) | Error 0.8989(0.8992) Steps 844(844.04) | Grad Norm 2.1427(2.2745) | Total Time 14.00(14.00)\n",
      "Iter 6400 | Time 20.6193(20.6738) | Bit/dim 3.4710(3.4637) | Xent 2.3026(2.3026) | Loss 3.4710(3.4637) | Error 0.9156(0.8993) Steps 856(843.46) | Grad Norm 2.2211(2.3061) | Total Time 14.00(14.00)\n",
      "Iter 6410 | Time 20.4397(20.6647) | Bit/dim 3.4767(3.4629) | Xent 2.3026(2.3026) | Loss 3.4767(3.4629) | Error 0.9011(0.8980) Steps 862(846.33) | Grad Norm 2.3880(2.3506) | Total Time 14.00(14.00)\n",
      "Iter 6420 | Time 20.7417(20.6488) | Bit/dim 3.4656(3.4639) | Xent 2.3026(2.3026) | Loss 3.4656(3.4639) | Error 0.8944(0.8991) Steps 862(846.34) | Grad Norm 1.8875(2.2192) | Total Time 14.00(14.00)\n",
      "Iter 6430 | Time 21.2752(20.6634) | Bit/dim 3.4274(3.4626) | Xent 2.3026(2.3026) | Loss 3.4274(3.4626) | Error 0.8844(0.9004) Steps 856(845.02) | Grad Norm 1.3972(2.1423) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0117 | Time 108.7889, Epoch Time 1263.0877(1213.1835), Bit/dim 3.4670(best: 3.4683), Xent 2.3026, Loss 3.4670, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6440 | Time 21.4138(20.6801) | Bit/dim 3.4245(3.4626) | Xent 2.3026(2.3026) | Loss 3.4245(3.4626) | Error 0.8956(0.9010) Steps 862(845.39) | Grad Norm 3.2061(2.2502) | Total Time 14.00(14.00)\n",
      "Iter 6450 | Time 20.9255(20.6909) | Bit/dim 3.4280(3.4614) | Xent 2.3026(2.3026) | Loss 3.4280(3.4614) | Error 0.8944(0.9014) Steps 844(844.59) | Grad Norm 1.7549(2.2601) | Total Time 14.00(14.00)\n",
      "Iter 6460 | Time 21.1677(20.7135) | Bit/dim 3.4654(3.4649) | Xent 2.3026(2.3026) | Loss 3.4654(3.4649) | Error 0.8922(0.9011) Steps 850(843.29) | Grad Norm 2.7624(2.3390) | Total Time 14.00(14.00)\n",
      "Iter 6470 | Time 21.3293(20.8071) | Bit/dim 3.4875(3.4646) | Xent 2.3026(2.3026) | Loss 3.4875(3.4646) | Error 0.8911(0.9001) Steps 856(845.72) | Grad Norm 2.6241(2.2694) | Total Time 14.00(14.00)\n",
      "Iter 6480 | Time 20.7826(20.8015) | Bit/dim 3.4527(3.4659) | Xent 2.3026(2.3026) | Loss 3.4527(3.4659) | Error 0.8944(0.9000) Steps 820(843.11) | Grad Norm 3.3908(2.3397) | Total Time 14.00(14.00)\n",
      "Iter 6490 | Time 21.0021(20.7612) | Bit/dim 3.4573(3.4633) | Xent 2.3026(2.3026) | Loss 3.4573(3.4633) | Error 0.9133(0.8993) Steps 850(841.58) | Grad Norm 2.2372(2.3094) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0118 | Time 105.9767, Epoch Time 1267.7924(1214.8218), Bit/dim 3.4669(best: 3.4670), Xent 2.3026, Loss 3.4669, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6500 | Time 21.3420(20.7548) | Bit/dim 3.4905(3.4655) | Xent 2.3026(2.3026) | Loss 3.4905(3.4655) | Error 0.9100(0.8999) Steps 844(841.23) | Grad Norm 2.0573(2.3207) | Total Time 14.00(14.00)\n",
      "Iter 6510 | Time 20.7168(20.7990) | Bit/dim 3.4494(3.4640) | Xent 2.3026(2.3026) | Loss 3.4494(3.4640) | Error 0.8867(0.8993) Steps 832(841.34) | Grad Norm 2.0834(2.4062) | Total Time 14.00(14.00)\n",
      "Iter 6520 | Time 20.6706(20.7863) | Bit/dim 3.4467(3.4624) | Xent 2.3026(2.3026) | Loss 3.4467(3.4624) | Error 0.9078(0.8999) Steps 862(839.69) | Grad Norm 4.0585(2.3660) | Total Time 14.00(14.00)\n",
      "Iter 6530 | Time 20.9006(20.8285) | Bit/dim 3.4481(3.4631) | Xent 2.3026(2.3026) | Loss 3.4481(3.4631) | Error 0.8967(0.9002) Steps 832(841.06) | Grad Norm 1.3390(2.3191) | Total Time 14.00(14.00)\n",
      "Iter 6540 | Time 20.5999(20.7480) | Bit/dim 3.4652(3.4616) | Xent 2.3026(2.3026) | Loss 3.4652(3.4616) | Error 0.8967(0.9012) Steps 838(840.19) | Grad Norm 1.6802(2.4085) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0119 | Time 106.4069, Epoch Time 1266.4576(1216.3708), Bit/dim 3.4650(best: 3.4669), Xent 2.3026, Loss 3.4650, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6550 | Time 20.9502(20.7965) | Bit/dim 3.4854(3.4603) | Xent 2.3026(2.3026) | Loss 3.4854(3.4603) | Error 0.9144(0.8998) Steps 832(840.37) | Grad Norm 3.0711(2.4072) | Total Time 14.00(14.00)\n",
      "Iter 6560 | Time 21.3417(20.7631) | Bit/dim 3.4886(3.4580) | Xent 2.3026(2.3026) | Loss 3.4886(3.4580) | Error 0.8889(0.8992) Steps 850(842.94) | Grad Norm 3.1166(2.3333) | Total Time 14.00(14.00)\n",
      "Iter 6570 | Time 21.2758(20.8711) | Bit/dim 3.4800(3.4640) | Xent 2.3026(2.3026) | Loss 3.4800(3.4640) | Error 0.9100(0.8996) Steps 868(848.84) | Grad Norm 2.1595(2.4083) | Total Time 14.00(14.00)\n",
      "Iter 6580 | Time 20.6170(20.8950) | Bit/dim 3.4660(3.4633) | Xent 2.3026(2.3026) | Loss 3.4660(3.4633) | Error 0.9144(0.8998) Steps 856(851.20) | Grad Norm 3.4987(2.4280) | Total Time 14.00(14.00)\n",
      "Iter 6590 | Time 20.8816(20.8982) | Bit/dim 3.4642(3.4612) | Xent 2.3026(2.3026) | Loss 3.4642(3.4612) | Error 0.8967(0.8994) Steps 880(851.17) | Grad Norm 1.9421(2.3481) | Total Time 14.00(14.00)\n",
      "Iter 6600 | Time 20.7732(20.9505) | Bit/dim 3.4455(3.4598) | Xent 2.3026(2.3026) | Loss 3.4455(3.4598) | Error 0.8900(0.8998) Steps 868(851.64) | Grad Norm 1.8415(2.1940) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0120 | Time 107.1853, Epoch Time 1279.3763(1218.2610), Bit/dim 3.4627(best: 3.4650), Xent 2.3026, Loss 3.4627, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6610 | Time 21.6643(20.9984) | Bit/dim 3.4563(3.4612) | Xent 2.3026(2.3026) | Loss 3.4563(3.4612) | Error 0.8978(0.8997) Steps 862(852.41) | Grad Norm 2.9803(2.2953) | Total Time 14.00(14.00)\n",
      "Iter 6620 | Time 20.7616(20.9803) | Bit/dim 3.4309(3.4609) | Xent 2.3026(2.3026) | Loss 3.4309(3.4609) | Error 0.9000(0.8992) Steps 826(848.92) | Grad Norm 2.3262(2.2445) | Total Time 14.00(14.00)\n",
      "Iter 6630 | Time 21.2101(21.0554) | Bit/dim 3.4406(3.4596) | Xent 2.3026(2.3026) | Loss 3.4406(3.4596) | Error 0.8922(0.8990) Steps 838(850.01) | Grad Norm 2.2472(2.3017) | Total Time 14.00(14.00)\n",
      "Iter 6640 | Time 21.1650(21.0827) | Bit/dim 3.4373(3.4577) | Xent 2.3026(2.3026) | Loss 3.4373(3.4577) | Error 0.8844(0.8991) Steps 862(850.24) | Grad Norm 2.6639(2.1931) | Total Time 14.00(14.00)\n",
      "Iter 6650 | Time 20.8802(21.0890) | Bit/dim 3.4319(3.4580) | Xent 2.3026(2.3026) | Loss 3.4319(3.4580) | Error 0.8989(0.8989) Steps 862(851.25) | Grad Norm 1.8440(2.2646) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0121 | Time 109.2670, Epoch Time 1290.1525(1220.4177), Bit/dim 3.4610(best: 3.4627), Xent 2.3026, Loss 3.4610, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6660 | Time 20.7415(21.0962) | Bit/dim 3.4338(3.4572) | Xent 2.3026(2.3026) | Loss 3.4338(3.4572) | Error 0.9011(0.9004) Steps 868(852.57) | Grad Norm 4.1571(2.3346) | Total Time 14.00(14.00)\n",
      "Iter 6670 | Time 21.2164(21.1136) | Bit/dim 3.4577(3.4580) | Xent 2.3026(2.3026) | Loss 3.4577(3.4580) | Error 0.8933(0.9005) Steps 868(852.17) | Grad Norm 1.4450(2.3374) | Total Time 14.00(14.00)\n",
      "Iter 6680 | Time 20.7409(21.1370) | Bit/dim 3.4868(3.4584) | Xent 2.3026(2.3026) | Loss 3.4868(3.4584) | Error 0.9100(0.8996) Steps 862(853.09) | Grad Norm 2.8583(2.3171) | Total Time 14.00(14.00)\n",
      "Iter 6690 | Time 20.3916(21.0529) | Bit/dim 3.4492(3.4579) | Xent 2.3026(2.3026) | Loss 3.4492(3.4579) | Error 0.8889(0.9001) Steps 844(853.54) | Grad Norm 2.0273(2.2930) | Total Time 14.00(14.00)\n",
      "Iter 6700 | Time 20.9164(20.9845) | Bit/dim 3.4679(3.4582) | Xent 2.3026(2.3026) | Loss 3.4679(3.4582) | Error 0.8856(0.9004) Steps 868(854.59) | Grad Norm 1.8013(2.2457) | Total Time 14.00(14.00)\n",
      "Iter 6710 | Time 21.3094(20.9951) | Bit/dim 3.4893(3.4597) | Xent 2.3026(2.3026) | Loss 3.4893(3.4597) | Error 0.8944(0.9001) Steps 850(855.61) | Grad Norm 3.4192(2.3284) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0122 | Time 108.2660, Epoch Time 1281.0212(1222.2358), Bit/dim 3.4594(best: 3.4610), Xent 2.3026, Loss 3.4594, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6720 | Time 22.0509(21.0267) | Bit/dim 3.4429(3.4593) | Xent 2.3026(2.3026) | Loss 3.4429(3.4593) | Error 0.8944(0.9005) Steps 856(853.86) | Grad Norm 1.2348(2.2626) | Total Time 14.00(14.00)\n",
      "Iter 6730 | Time 21.1230(21.0592) | Bit/dim 3.4731(3.4583) | Xent 2.3026(2.3026) | Loss 3.4731(3.4583) | Error 0.9089(0.8999) Steps 886(856.50) | Grad Norm 2.1303(2.2553) | Total Time 14.00(14.00)\n",
      "Iter 6740 | Time 21.1728(21.1279) | Bit/dim 3.4561(3.4585) | Xent 2.3026(2.3026) | Loss 3.4561(3.4585) | Error 0.8978(0.8993) Steps 850(857.97) | Grad Norm 1.7527(2.2293) | Total Time 14.00(14.00)\n",
      "Iter 6750 | Time 21.8299(21.1722) | Bit/dim 3.4496(3.4588) | Xent 2.3026(2.3026) | Loss 3.4496(3.4588) | Error 0.9044(0.9009) Steps 856(858.44) | Grad Norm 1.8033(2.0728) | Total Time 14.00(14.00)\n",
      "Iter 6760 | Time 20.9080(21.1577) | Bit/dim 3.4619(3.4585) | Xent 2.3026(2.3026) | Loss 3.4619(3.4585) | Error 0.9078(0.9012) Steps 850(859.24) | Grad Norm 2.5948(2.1442) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0123 | Time 107.6391, Epoch Time 1291.2517(1224.3063), Bit/dim 3.4584(best: 3.4594), Xent 2.3026, Loss 3.4584, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6770 | Time 20.8760(21.0921) | Bit/dim 3.4959(3.4579) | Xent 2.3026(2.3026) | Loss 3.4959(3.4579) | Error 0.8900(0.8999) Steps 856(861.07) | Grad Norm 1.9708(2.1876) | Total Time 14.00(14.00)\n",
      "Iter 6780 | Time 20.4739(21.1056) | Bit/dim 3.4720(3.4574) | Xent 2.3026(2.3026) | Loss 3.4720(3.4574) | Error 0.9044(0.9000) Steps 868(861.27) | Grad Norm 1.8879(2.3428) | Total Time 14.00(14.00)\n",
      "Iter 6790 | Time 21.7624(21.1864) | Bit/dim 3.4300(3.4563) | Xent 2.3026(2.3026) | Loss 3.4300(3.4563) | Error 0.8778(0.8996) Steps 874(860.88) | Grad Norm 2.1457(2.2613) | Total Time 14.00(14.00)\n",
      "Iter 6800 | Time 21.0228(21.1538) | Bit/dim 3.4314(3.4537) | Xent 2.3026(2.3026) | Loss 3.4314(3.4537) | Error 0.9000(0.8992) Steps 862(861.66) | Grad Norm 3.1385(2.2677) | Total Time 14.00(14.00)\n",
      "Iter 6810 | Time 21.9048(21.2228) | Bit/dim 3.4401(3.4557) | Xent 2.3026(2.3026) | Loss 3.4401(3.4557) | Error 0.8933(0.8999) Steps 850(863.55) | Grad Norm 1.6237(2.2029) | Total Time 14.00(14.00)\n",
      "Iter 6820 | Time 21.5988(21.3048) | Bit/dim 3.4483(3.4564) | Xent 2.3026(2.3026) | Loss 3.4483(3.4564) | Error 0.9044(0.9000) Steps 874(862.34) | Grad Norm 3.2272(2.1970) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0124 | Time 107.9955, Epoch Time 1295.2261(1226.4339), Bit/dim 3.4630(best: 3.4584), Xent 2.3026, Loss 3.4630, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6830 | Time 21.2545(21.3488) | Bit/dim 3.4748(3.4552) | Xent 2.3026(2.3026) | Loss 3.4748(3.4552) | Error 0.8967(0.8997) Steps 874(865.19) | Grad Norm 2.4840(2.3523) | Total Time 14.00(14.00)\n",
      "Iter 6840 | Time 21.3735(21.2919) | Bit/dim 3.4480(3.4572) | Xent 2.3026(2.3026) | Loss 3.4480(3.4572) | Error 0.8956(0.9001) Steps 874(865.49) | Grad Norm 2.0490(2.2705) | Total Time 14.00(14.00)\n",
      "Iter 6850 | Time 20.5368(21.1930) | Bit/dim 3.4703(3.4565) | Xent 2.3026(2.3026) | Loss 3.4703(3.4565) | Error 0.9111(0.9003) Steps 880(864.16) | Grad Norm 2.8515(2.3588) | Total Time 14.00(14.00)\n",
      "Iter 6860 | Time 21.0543(21.1717) | Bit/dim 3.4596(3.4567) | Xent 2.3026(2.3026) | Loss 3.4596(3.4567) | Error 0.9033(0.9002) Steps 898(865.13) | Grad Norm 1.5985(2.2170) | Total Time 14.00(14.00)\n",
      "Iter 6870 | Time 21.0249(21.1663) | Bit/dim 3.4537(3.4562) | Xent 2.3026(2.3026) | Loss 3.4537(3.4562) | Error 0.9022(0.9010) Steps 880(865.85) | Grad Norm 3.5775(2.3585) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0125 | Time 109.0305, Epoch Time 1290.8939(1228.3677), Bit/dim 3.4568(best: 3.4584), Xent 2.3026, Loss 3.4568, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6880 | Time 21.6011(21.1855) | Bit/dim 3.4585(3.4544) | Xent 2.3026(2.3026) | Loss 3.4585(3.4544) | Error 0.8956(0.9003) Steps 874(867.31) | Grad Norm 1.5144(2.3578) | Total Time 14.00(14.00)\n",
      "Iter 6890 | Time 21.4224(21.2133) | Bit/dim 3.4588(3.4559) | Xent 2.3026(2.3026) | Loss 3.4588(3.4559) | Error 0.8944(0.9006) Steps 838(866.67) | Grad Norm 1.8476(2.2275) | Total Time 14.00(14.00)\n",
      "Iter 6900 | Time 21.4489(21.2750) | Bit/dim 3.4636(3.4558) | Xent 2.3026(2.3026) | Loss 3.4636(3.4558) | Error 0.8956(0.8999) Steps 880(870.41) | Grad Norm 2.4876(2.1757) | Total Time 14.00(14.00)\n",
      "Iter 6910 | Time 21.0246(21.1779) | Bit/dim 3.4509(3.4543) | Xent 2.3026(2.3026) | Loss 3.4509(3.4543) | Error 0.8911(0.9000) Steps 850(867.88) | Grad Norm 1.5611(2.1648) | Total Time 14.00(14.00)\n",
      "Iter 6920 | Time 20.6580(21.1018) | Bit/dim 3.4403(3.4555) | Xent 2.3026(2.3026) | Loss 3.4403(3.4555) | Error 0.8967(0.9003) Steps 880(866.08) | Grad Norm 1.6286(2.2933) | Total Time 14.00(14.00)\n",
      "Iter 6930 | Time 21.0493(21.1467) | Bit/dim 3.4322(3.4555) | Xent 2.3026(2.3026) | Loss 3.4322(3.4555) | Error 0.8989(0.9001) Steps 892(868.68) | Grad Norm 2.7205(2.4047) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0126 | Time 108.0962, Epoch Time 1290.7142(1230.2381), Bit/dim 3.4549(best: 3.4568), Xent 2.3026, Loss 3.4549, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6940 | Time 20.5927(21.0935) | Bit/dim 3.4403(3.4540) | Xent 2.3026(2.3026) | Loss 3.4403(3.4540) | Error 0.8989(0.9007) Steps 874(866.54) | Grad Norm 2.2243(2.3727) | Total Time 14.00(14.00)\n",
      "Iter 6950 | Time 21.8428(21.1103) | Bit/dim 3.4324(3.4539) | Xent 2.3026(2.3026) | Loss 3.4324(3.4539) | Error 0.9022(0.8989) Steps 910(867.05) | Grad Norm 2.0896(2.2237) | Total Time 14.00(14.00)\n",
      "Iter 6960 | Time 21.4768(21.1533) | Bit/dim 3.4276(3.4528) | Xent 2.3026(2.3026) | Loss 3.4276(3.4528) | Error 0.8967(0.8991) Steps 868(868.63) | Grad Norm 1.2924(2.2564) | Total Time 14.00(14.00)\n",
      "Iter 6970 | Time 21.5752(21.1805) | Bit/dim 3.4529(3.4533) | Xent 2.3026(2.3026) | Loss 3.4529(3.4533) | Error 0.9300(0.9010) Steps 844(866.81) | Grad Norm 2.0571(2.3567) | Total Time 14.00(14.00)\n",
      "Iter 6980 | Time 21.2255(21.1795) | Bit/dim 3.4347(3.4535) | Xent 2.3026(2.3026) | Loss 3.4347(3.4535) | Error 0.8944(0.9001) Steps 862(865.79) | Grad Norm 2.6701(2.3273) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0127 | Time 111.4356, Epoch Time 1293.9946(1232.1508), Bit/dim 3.4562(best: 3.4549), Xent 2.3026, Loss 3.4562, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 6990 | Time 21.3123(21.2056) | Bit/dim 3.4465(3.4557) | Xent 2.3026(2.3026) | Loss 3.4465(3.4557) | Error 0.8956(0.9002) Steps 862(866.74) | Grad Norm 2.1664(2.3214) | Total Time 14.00(14.00)\n",
      "Iter 7000 | Time 20.9125(21.2346) | Bit/dim 3.4222(3.4530) | Xent 2.3026(2.3026) | Loss 3.4222(3.4530) | Error 0.8833(0.9011) Steps 874(866.70) | Grad Norm 1.7610(2.3017) | Total Time 14.00(14.00)\n",
      "Iter 7010 | Time 21.0908(21.2951) | Bit/dim 3.4675(3.4542) | Xent 2.3026(2.3026) | Loss 3.4675(3.4542) | Error 0.9111(0.9003) Steps 856(867.07) | Grad Norm 1.9453(2.2271) | Total Time 14.00(14.00)\n",
      "Iter 7020 | Time 21.0864(21.2418) | Bit/dim 3.4638(3.4546) | Xent 2.3026(2.3026) | Loss 3.4638(3.4546) | Error 0.9000(0.8997) Steps 868(866.86) | Grad Norm 3.8975(2.2302) | Total Time 14.00(14.00)\n",
      "Iter 7030 | Time 21.3749(21.2308) | Bit/dim 3.4636(3.4532) | Xent 2.3026(2.3026) | Loss 3.4636(3.4532) | Error 0.9167(0.9001) Steps 892(868.52) | Grad Norm 1.6314(2.3432) | Total Time 14.00(14.00)\n",
      "Iter 7040 | Time 20.5843(21.2776) | Bit/dim 3.4655(3.4534) | Xent 2.3026(2.3026) | Loss 3.4655(3.4534) | Error 0.8989(0.9001) Steps 862(869.89) | Grad Norm 1.3736(2.2437) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0128 | Time 111.0283, Epoch Time 1300.7366(1234.2084), Bit/dim 3.4535(best: 3.4549), Xent 2.3026, Loss 3.4535, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7050 | Time 21.6772(21.2755) | Bit/dim 3.4579(3.4524) | Xent 2.3026(2.3026) | Loss 3.4579(3.4524) | Error 0.9089(0.9000) Steps 868(870.48) | Grad Norm 3.4475(2.2530) | Total Time 14.00(14.00)\n",
      "Iter 7060 | Time 21.6180(21.2960) | Bit/dim 3.4618(3.4517) | Xent 2.3026(2.3026) | Loss 3.4618(3.4517) | Error 0.8967(0.9001) Steps 862(869.63) | Grad Norm 1.7274(2.2031) | Total Time 14.00(14.00)\n",
      "Iter 7070 | Time 21.4730(21.2758) | Bit/dim 3.4469(3.4519) | Xent 2.3026(2.3026) | Loss 3.4469(3.4519) | Error 0.8811(0.8997) Steps 850(869.19) | Grad Norm 2.3967(2.2641) | Total Time 14.00(14.00)\n",
      "Iter 7080 | Time 22.1008(21.2875) | Bit/dim 3.4602(3.4508) | Xent 2.3026(2.3026) | Loss 3.4602(3.4508) | Error 0.8978(0.8986) Steps 904(870.72) | Grad Norm 2.4408(2.3561) | Total Time 14.00(14.00)\n",
      "Iter 7090 | Time 20.8333(21.1876) | Bit/dim 3.4562(3.4521) | Xent 2.3026(2.3026) | Loss 3.4562(3.4521) | Error 0.9178(0.9000) Steps 874(871.31) | Grad Norm 2.9605(2.3292) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0129 | Time 108.4272, Epoch Time 1294.2521(1236.0097), Bit/dim 3.4518(best: 3.4535), Xent 2.3026, Loss 3.4518, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7100 | Time 21.3099(21.2347) | Bit/dim 3.4382(3.4515) | Xent 2.3026(2.3026) | Loss 3.4382(3.4515) | Error 0.9056(0.9001) Steps 868(872.99) | Grad Norm 1.9579(2.2358) | Total Time 14.00(14.00)\n",
      "Iter 7110 | Time 20.5188(21.1938) | Bit/dim 3.4616(3.4501) | Xent 2.3026(2.3026) | Loss 3.4616(3.4501) | Error 0.9033(0.8995) Steps 862(870.88) | Grad Norm 1.5823(2.3290) | Total Time 14.00(14.00)\n",
      "Iter 7120 | Time 21.7570(21.2161) | Bit/dim 3.4454(3.4491) | Xent 2.3026(2.3026) | Loss 3.4454(3.4491) | Error 0.8844(0.8990) Steps 892(871.21) | Grad Norm 2.2973(2.1366) | Total Time 14.00(14.00)\n",
      "Iter 7130 | Time 20.5377(21.2473) | Bit/dim 3.4504(3.4489) | Xent 2.3026(2.3026) | Loss 3.4504(3.4489) | Error 0.9100(0.9009) Steps 874(872.45) | Grad Norm 1.7601(2.2506) | Total Time 14.00(14.00)\n",
      "Iter 7140 | Time 21.4444(21.2660) | Bit/dim 3.4489(3.4514) | Xent 2.3026(2.3026) | Loss 3.4489(3.4514) | Error 0.8856(0.8996) Steps 874(873.01) | Grad Norm 2.0494(2.1905) | Total Time 14.00(14.00)\n",
      "Iter 7150 | Time 21.0779(21.3452) | Bit/dim 3.4093(3.4509) | Xent 2.3026(2.3026) | Loss 3.4093(3.4509) | Error 0.8956(0.9001) Steps 874(875.33) | Grad Norm 2.4610(2.2463) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0130 | Time 109.0581, Epoch Time 1299.3602(1237.9102), Bit/dim 3.4549(best: 3.4518), Xent 2.3026, Loss 3.4549, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7160 | Time 21.3886(21.3110) | Bit/dim 3.4691(3.4525) | Xent 2.3026(2.3026) | Loss 3.4691(3.4525) | Error 0.8878(0.9002) Steps 868(875.70) | Grad Norm 2.1635(2.2698) | Total Time 14.00(14.00)\n",
      "Iter 7170 | Time 21.4077(21.2772) | Bit/dim 3.4658(3.4504) | Xent 2.3026(2.3026) | Loss 3.4658(3.4504) | Error 0.9033(0.8996) Steps 844(873.59) | Grad Norm 2.8381(2.3684) | Total Time 14.00(14.00)\n",
      "Iter 7180 | Time 21.2378(21.2644) | Bit/dim 3.4653(3.4500) | Xent 2.3026(2.3026) | Loss 3.4653(3.4500) | Error 0.9011(0.8996) Steps 874(871.99) | Grad Norm 2.2700(2.3399) | Total Time 14.00(14.00)\n",
      "Iter 7190 | Time 21.3688(21.2086) | Bit/dim 3.4527(3.4522) | Xent 2.3026(2.3026) | Loss 3.4527(3.4522) | Error 0.9233(0.9006) Steps 874(872.11) | Grad Norm 1.8894(2.2566) | Total Time 14.00(14.00)\n",
      "Iter 7200 | Time 21.7814(21.2523) | Bit/dim 3.4309(3.4507) | Xent 2.3026(2.3026) | Loss 3.4309(3.4507) | Error 0.8922(0.8997) Steps 880(873.24) | Grad Norm 2.0420(2.1938) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0131 | Time 110.9161, Epoch Time 1296.1304(1239.6568), Bit/dim 3.4517(best: 3.4518), Xent 2.3026, Loss 3.4517, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7210 | Time 21.5163(21.2412) | Bit/dim 3.4706(3.4482) | Xent 2.3026(2.3026) | Loss 3.4706(3.4482) | Error 0.8856(0.8991) Steps 862(872.38) | Grad Norm 2.7610(2.2685) | Total Time 14.00(14.00)\n",
      "Iter 7220 | Time 21.2679(21.1971) | Bit/dim 3.4577(3.4502) | Xent 2.3026(2.3026) | Loss 3.4577(3.4502) | Error 0.8867(0.8994) Steps 868(871.07) | Grad Norm 1.9576(2.2832) | Total Time 14.00(14.00)\n",
      "Iter 7230 | Time 21.3837(21.1646) | Bit/dim 3.4489(3.4500) | Xent 2.3026(2.3026) | Loss 3.4489(3.4500) | Error 0.9089(0.9009) Steps 868(870.51) | Grad Norm 2.5459(2.1997) | Total Time 14.00(14.00)\n",
      "Iter 7240 | Time 21.8495(21.2483) | Bit/dim 3.3980(3.4487) | Xent 2.3026(2.3026) | Loss 3.3980(3.4487) | Error 0.8933(0.9003) Steps 904(872.19) | Grad Norm 2.0573(2.2554) | Total Time 14.00(14.00)\n",
      "Iter 7250 | Time 21.6515(21.3998) | Bit/dim 3.4369(3.4486) | Xent 2.3026(2.3026) | Loss 3.4369(3.4486) | Error 0.8933(0.8998) Steps 850(873.90) | Grad Norm 2.3416(2.1600) | Total Time 14.00(14.00)\n",
      "Iter 7260 | Time 21.4324(21.3899) | Bit/dim 3.4559(3.4477) | Xent 2.3026(2.3026) | Loss 3.4559(3.4477) | Error 0.8944(0.8997) Steps 898(876.13) | Grad Norm 2.4125(2.2405) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0132 | Time 110.6844, Epoch Time 1302.1330(1241.5311), Bit/dim 3.4532(best: 3.4517), Xent 2.3026, Loss 3.4532, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7270 | Time 21.6401(21.4365) | Bit/dim 3.4486(3.4461) | Xent 2.3026(2.3026) | Loss 3.4486(3.4461) | Error 0.8856(0.8985) Steps 874(875.98) | Grad Norm 2.0470(2.1528) | Total Time 14.00(14.00)\n",
      "Iter 7280 | Time 20.4461(21.4419) | Bit/dim 3.4534(3.4460) | Xent 2.3026(2.3026) | Loss 3.4534(3.4460) | Error 0.8911(0.8980) Steps 874(878.27) | Grad Norm 1.8300(2.2120) | Total Time 14.00(14.00)\n",
      "Iter 7290 | Time 21.7034(21.4150) | Bit/dim 3.4360(3.4467) | Xent 2.3026(2.3026) | Loss 3.4360(3.4467) | Error 0.8967(0.8985) Steps 880(876.39) | Grad Norm 3.3481(2.2373) | Total Time 14.00(14.00)\n",
      "Iter 7300 | Time 20.8386(21.3685) | Bit/dim 3.4437(3.4456) | Xent 2.3026(2.3026) | Loss 3.4437(3.4456) | Error 0.9011(0.8999) Steps 868(877.12) | Grad Norm 1.8599(2.2834) | Total Time 14.00(14.00)\n",
      "Iter 7310 | Time 21.7457(21.3299) | Bit/dim 3.4452(3.4505) | Xent 2.3026(2.3026) | Loss 3.4452(3.4505) | Error 0.9078(0.9013) Steps 892(879.83) | Grad Norm 2.3481(2.1863) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0133 | Time 109.5654, Epoch Time 1302.8484(1243.3706), Bit/dim 3.4492(best: 3.4517), Xent 2.3026, Loss 3.4492, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7320 | Time 21.6391(21.3843) | Bit/dim 3.4340(3.4482) | Xent 2.3026(2.3026) | Loss 3.4340(3.4482) | Error 0.9033(0.9007) Steps 886(880.99) | Grad Norm 2.1737(2.2328) | Total Time 14.00(14.00)\n",
      "Iter 7330 | Time 21.5291(21.3902) | Bit/dim 3.4648(3.4472) | Xent 2.3026(2.3026) | Loss 3.4648(3.4472) | Error 0.9089(0.8995) Steps 874(881.47) | Grad Norm 3.1626(2.2400) | Total Time 14.00(14.00)\n",
      "Iter 7340 | Time 21.6002(21.3614) | Bit/dim 3.4518(3.4471) | Xent 2.3026(2.3026) | Loss 3.4518(3.4471) | Error 0.9011(0.9013) Steps 874(880.62) | Grad Norm 1.7460(2.2996) | Total Time 14.00(14.00)\n",
      "Iter 7350 | Time 21.3472(21.3131) | Bit/dim 3.4370(3.4476) | Xent 2.3026(2.3026) | Loss 3.4370(3.4476) | Error 0.8978(0.9000) Steps 880(878.75) | Grad Norm 1.6134(2.2204) | Total Time 14.00(14.00)\n",
      "Iter 7360 | Time 21.2926(21.3405) | Bit/dim 3.4603(3.4486) | Xent 2.3026(2.3026) | Loss 3.4603(3.4486) | Error 0.9022(0.9005) Steps 880(879.85) | Grad Norm 2.8679(2.3608) | Total Time 14.00(14.00)\n",
      "Iter 7370 | Time 21.0576(21.3067) | Bit/dim 3.4190(3.4477) | Xent 2.3026(2.3026) | Loss 3.4190(3.4477) | Error 0.9000(0.9001) Steps 880(878.87) | Grad Norm 1.6852(2.2516) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0134 | Time 107.4937, Epoch Time 1298.7708(1245.0326), Bit/dim 3.4506(best: 3.4492), Xent 2.3026, Loss 3.4506, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7380 | Time 21.1652(21.3538) | Bit/dim 3.4539(3.4470) | Xent 2.3026(2.3026) | Loss 3.4539(3.4470) | Error 0.9078(0.8998) Steps 898(879.25) | Grad Norm 1.8310(2.2706) | Total Time 14.00(14.00)\n",
      "Iter 7390 | Time 20.6096(21.3569) | Bit/dim 3.4508(3.4470) | Xent 2.3026(2.3026) | Loss 3.4508(3.4470) | Error 0.9133(0.8998) Steps 856(879.38) | Grad Norm 2.8021(2.2443) | Total Time 14.00(14.00)\n",
      "Iter 7400 | Time 21.4917(21.3237) | Bit/dim 3.4071(3.4457) | Xent 2.3026(2.3026) | Loss 3.4071(3.4457) | Error 0.8867(0.9000) Steps 886(877.35) | Grad Norm 2.6031(2.3087) | Total Time 14.00(14.00)\n",
      "Iter 7410 | Time 21.0322(21.2952) | Bit/dim 3.4694(3.4451) | Xent 2.3026(2.3026) | Loss 3.4694(3.4451) | Error 0.9167(0.9014) Steps 874(876.16) | Grad Norm 1.2631(2.1595) | Total Time 14.00(14.00)\n",
      "Iter 7420 | Time 22.0651(21.3283) | Bit/dim 3.4829(3.4469) | Xent 2.3026(2.3026) | Loss 3.4829(3.4469) | Error 0.9044(0.8998) Steps 886(876.67) | Grad Norm 2.0346(2.2105) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0135 | Time 109.7657, Epoch Time 1300.7666(1246.7046), Bit/dim 3.4495(best: 3.4492), Xent 2.3026, Loss 3.4495, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7430 | Time 20.5804(21.3340) | Bit/dim 3.4269(3.4457) | Xent 2.3026(2.3026) | Loss 3.4269(3.4457) | Error 0.8989(0.8994) Steps 862(876.16) | Grad Norm 3.0377(2.3514) | Total Time 14.00(14.00)\n",
      "Iter 7440 | Time 21.9243(21.4497) | Bit/dim 3.4575(3.4464) | Xent 2.3026(2.3026) | Loss 3.4575(3.4464) | Error 0.9122(0.8993) Steps 892(878.14) | Grad Norm 1.7864(2.1848) | Total Time 14.00(14.00)\n",
      "Iter 7450 | Time 21.7258(21.4609) | Bit/dim 3.4695(3.4454) | Xent 2.3026(2.3026) | Loss 3.4695(3.4454) | Error 0.9022(0.9002) Steps 898(880.92) | Grad Norm 2.1278(2.1724) | Total Time 14.00(14.00)\n",
      "Iter 7460 | Time 21.7855(21.4252) | Bit/dim 3.4243(3.4469) | Xent 2.3026(2.3026) | Loss 3.4243(3.4469) | Error 0.9078(0.9006) Steps 880(879.15) | Grad Norm 2.5442(2.1296) | Total Time 14.00(14.00)\n",
      "Iter 7470 | Time 21.3158(21.3795) | Bit/dim 3.4523(3.4455) | Xent 2.3026(2.3026) | Loss 3.4523(3.4455) | Error 0.9000(0.9008) Steps 892(880.18) | Grad Norm 1.5464(2.1575) | Total Time 14.00(14.00)\n",
      "Iter 7480 | Time 21.7566(21.3883) | Bit/dim 3.4441(3.4462) | Xent 2.3026(2.3026) | Loss 3.4441(3.4462) | Error 0.8844(0.8999) Steps 874(880.86) | Grad Norm 2.7811(2.1968) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0136 | Time 110.0715, Epoch Time 1307.9515(1248.5420), Bit/dim 3.4495(best: 3.4492), Xent 2.3026, Loss 3.4495, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7490 | Time 21.2352(21.3609) | Bit/dim 3.4448(3.4453) | Xent 2.3026(2.3026) | Loss 3.4448(3.4453) | Error 0.8911(0.8999) Steps 868(879.48) | Grad Norm 1.6321(2.1041) | Total Time 14.00(14.00)\n",
      "Iter 7500 | Time 20.7878(21.3320) | Bit/dim 3.4421(3.4478) | Xent 2.3026(2.3026) | Loss 3.4421(3.4478) | Error 0.8900(0.9011) Steps 868(880.17) | Grad Norm 1.6081(2.2581) | Total Time 14.00(14.00)\n",
      "Iter 7510 | Time 20.7033(21.3315) | Bit/dim 3.4527(3.4460) | Xent 2.3026(2.3026) | Loss 3.4527(3.4460) | Error 0.8956(0.9005) Steps 874(880.02) | Grad Norm 2.0017(2.2439) | Total Time 14.00(14.00)\n",
      "Iter 7520 | Time 21.8481(21.4585) | Bit/dim 3.4543(3.4446) | Xent 2.3026(2.3026) | Loss 3.4543(3.4446) | Error 0.9056(0.8997) Steps 892(882.30) | Grad Norm 1.6560(2.0989) | Total Time 14.00(14.00)\n",
      "Iter 7530 | Time 21.3904(21.4784) | Bit/dim 3.4445(3.4453) | Xent 2.3026(2.3026) | Loss 3.4445(3.4453) | Error 0.8878(0.8996) Steps 886(882.00) | Grad Norm 1.4518(2.1674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0137 | Time 112.1947, Epoch Time 1309.6874(1250.3764), Bit/dim 3.4483(best: 3.4492), Xent 2.3026, Loss 3.4483, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7540 | Time 21.3609(21.4655) | Bit/dim 3.4928(3.4457) | Xent 2.3026(2.3026) | Loss 3.4928(3.4457) | Error 0.8989(0.8998) Steps 898(882.50) | Grad Norm 2.3928(2.1655) | Total Time 14.00(14.00)\n",
      "Iter 7550 | Time 21.2464(21.4074) | Bit/dim 3.4366(3.4448) | Xent 2.3026(2.3026) | Loss 3.4366(3.4448) | Error 0.9022(0.9003) Steps 856(882.60) | Grad Norm 2.2595(2.0765) | Total Time 14.00(14.00)\n",
      "Iter 7560 | Time 21.5101(21.4765) | Bit/dim 3.4334(3.4426) | Xent 2.3026(2.3026) | Loss 3.4334(3.4426) | Error 0.8844(0.8980) Steps 892(884.35) | Grad Norm 2.2313(2.1975) | Total Time 14.00(14.00)\n",
      "Iter 7570 | Time 21.1480(21.4800) | Bit/dim 3.4640(3.4454) | Xent 2.3026(2.3026) | Loss 3.4640(3.4454) | Error 0.9033(0.8995) Steps 898(885.28) | Grad Norm 2.2020(2.1743) | Total Time 14.00(14.00)\n",
      "Iter 7580 | Time 20.8727(21.4514) | Bit/dim 3.4326(3.4450) | Xent 2.3026(2.3026) | Loss 3.4326(3.4450) | Error 0.9056(0.9005) Steps 880(885.25) | Grad Norm 2.2677(2.2353) | Total Time 14.00(14.00)\n",
      "Iter 7590 | Time 20.8891(21.3520) | Bit/dim 3.4513(3.4446) | Xent 2.3026(2.3026) | Loss 3.4513(3.4446) | Error 0.8989(0.9008) Steps 886(883.02) | Grad Norm 2.3449(2.1683) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0138 | Time 110.4766, Epoch Time 1304.6002(1252.0031), Bit/dim 3.4461(best: 3.4483), Xent 2.3026, Loss 3.4461, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7600 | Time 20.8960(21.3003) | Bit/dim 3.4352(3.4437) | Xent 2.3026(2.3026) | Loss 3.4352(3.4437) | Error 0.8744(0.8995) Steps 874(882.63) | Grad Norm 2.9793(2.2036) | Total Time 14.00(14.00)\n",
      "Iter 7610 | Time 21.4049(21.2671) | Bit/dim 3.4703(3.4451) | Xent 2.3026(2.3026) | Loss 3.4703(3.4451) | Error 0.9100(0.8995) Steps 880(881.11) | Grad Norm 2.5145(2.2489) | Total Time 14.00(14.00)\n",
      "Iter 7620 | Time 20.4469(21.2811) | Bit/dim 3.4651(3.4455) | Xent 2.3026(2.3026) | Loss 3.4651(3.4455) | Error 0.8800(0.8988) Steps 880(881.38) | Grad Norm 2.6120(2.2011) | Total Time 14.00(14.00)\n",
      "Iter 7630 | Time 20.2978(21.2429) | Bit/dim 3.4473(3.4432) | Xent 2.3026(2.3026) | Loss 3.4473(3.4432) | Error 0.9167(0.8998) Steps 874(881.37) | Grad Norm 1.2537(2.0801) | Total Time 14.00(14.00)\n",
      "Iter 7640 | Time 20.8717(21.2167) | Bit/dim 3.4429(3.4431) | Xent 2.3026(2.3026) | Loss 3.4429(3.4431) | Error 0.8989(0.9007) Steps 880(882.35) | Grad Norm 1.2602(2.0843) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0139 | Time 111.1101, Epoch Time 1295.6450(1253.3124), Bit/dim 3.4483(best: 3.4461), Xent 2.3026, Loss 3.4483, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7650 | Time 21.5077(21.2813) | Bit/dim 3.4343(3.4420) | Xent 2.3026(2.3026) | Loss 3.4343(3.4420) | Error 0.9022(0.9018) Steps 886(882.07) | Grad Norm 3.1397(2.1405) | Total Time 14.00(14.00)\n",
      "Iter 7660 | Time 20.9425(21.2579) | Bit/dim 3.4396(3.4422) | Xent 2.3026(2.3026) | Loss 3.4396(3.4422) | Error 0.9033(0.9021) Steps 880(880.85) | Grad Norm 2.7129(2.1491) | Total Time 14.00(14.00)\n",
      "Iter 7670 | Time 20.7156(21.1936) | Bit/dim 3.4741(3.4435) | Xent 2.3026(2.3026) | Loss 3.4741(3.4435) | Error 0.9000(0.9019) Steps 874(881.89) | Grad Norm 2.0027(2.1129) | Total Time 14.00(14.00)\n",
      "Iter 7680 | Time 20.9164(21.1743) | Bit/dim 3.4595(3.4423) | Xent 2.3026(2.3026) | Loss 3.4595(3.4423) | Error 0.8989(0.8999) Steps 874(882.29) | Grad Norm 1.9831(2.2389) | Total Time 14.00(14.00)\n",
      "Iter 7690 | Time 20.9546(21.2049) | Bit/dim 3.4345(3.4417) | Xent 2.3026(2.3026) | Loss 3.4345(3.4417) | Error 0.8956(0.9001) Steps 898(883.19) | Grad Norm 2.0253(2.2014) | Total Time 14.00(14.00)\n",
      "Iter 7700 | Time 21.2034(21.2167) | Bit/dim 3.4056(3.4402) | Xent 2.3026(2.3026) | Loss 3.4056(3.4402) | Error 0.8878(0.8997) Steps 886(882.06) | Grad Norm 1.9984(2.2717) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0140 | Time 107.8068, Epoch Time 1292.9080(1254.5002), Bit/dim 3.4446(best: 3.4461), Xent 2.3026, Loss 3.4446, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7710 | Time 21.0046(21.2386) | Bit/dim 3.4649(3.4414) | Xent 2.3026(2.3026) | Loss 3.4649(3.4414) | Error 0.9022(0.9006) Steps 886(883.09) | Grad Norm 2.0269(2.1882) | Total Time 14.00(14.00)\n",
      "Iter 7720 | Time 20.8511(21.2319) | Bit/dim 3.4364(3.4409) | Xent 2.3026(2.3026) | Loss 3.4364(3.4409) | Error 0.8822(0.8996) Steps 880(883.14) | Grad Norm 2.9075(2.0404) | Total Time 14.00(14.00)\n",
      "Iter 7730 | Time 21.3668(21.2044) | Bit/dim 3.4348(3.4423) | Xent 2.3026(2.3026) | Loss 3.4348(3.4423) | Error 0.9222(0.9009) Steps 880(884.21) | Grad Norm 1.7733(2.0855) | Total Time 14.00(14.00)\n",
      "Iter 7740 | Time 21.3250(21.2367) | Bit/dim 3.4379(3.4406) | Xent 2.3026(2.3026) | Loss 3.4379(3.4406) | Error 0.8944(0.9000) Steps 886(885.83) | Grad Norm 3.6556(2.1975) | Total Time 14.00(14.00)\n",
      "Iter 7750 | Time 21.5077(21.3389) | Bit/dim 3.4221(3.4401) | Xent 2.3026(2.3026) | Loss 3.4221(3.4401) | Error 0.8989(0.9000) Steps 898(889.08) | Grad Norm 1.7979(2.2402) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0141 | Time 110.9912, Epoch Time 1301.6075(1255.9135), Bit/dim 3.4436(best: 3.4446), Xent 2.3026, Loss 3.4436, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7760 | Time 21.2714(21.3271) | Bit/dim 3.4676(3.4421) | Xent 2.3026(2.3026) | Loss 3.4676(3.4421) | Error 0.9078(0.9001) Steps 904(887.57) | Grad Norm 1.8588(2.2349) | Total Time 14.00(14.00)\n",
      "Iter 7770 | Time 21.0120(21.3073) | Bit/dim 3.4513(3.4415) | Xent 2.3026(2.3026) | Loss 3.4513(3.4415) | Error 0.8989(0.8995) Steps 886(888.80) | Grad Norm 2.6667(2.2191) | Total Time 14.00(14.00)\n",
      "Iter 7780 | Time 21.3566(21.2941) | Bit/dim 3.4243(3.4415) | Xent 2.3026(2.3026) | Loss 3.4243(3.4415) | Error 0.8989(0.9002) Steps 880(888.63) | Grad Norm 2.2063(2.1867) | Total Time 14.00(14.00)\n",
      "Iter 7790 | Time 21.8204(21.3406) | Bit/dim 3.4372(3.4419) | Xent 2.3026(2.3026) | Loss 3.4372(3.4419) | Error 0.9022(0.8995) Steps 904(888.81) | Grad Norm 1.0929(2.2670) | Total Time 14.00(14.00)\n",
      "Iter 7800 | Time 21.1738(21.3370) | Bit/dim 3.4175(3.4401) | Xent 2.3026(2.3026) | Loss 3.4175(3.4401) | Error 0.8989(0.8980) Steps 898(889.18) | Grad Norm 1.7066(2.1425) | Total Time 14.00(14.00)\n",
      "Iter 7810 | Time 21.4797(21.3771) | Bit/dim 3.4528(3.4373) | Xent 2.3026(2.3026) | Loss 3.4528(3.4373) | Error 0.9067(0.8997) Steps 892(888.63) | Grad Norm 1.5960(1.9674) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0142 | Time 110.8148, Epoch Time 1303.6484(1257.3455), Bit/dim 3.4384(best: 3.4436), Xent 2.3026, Loss 3.4384, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7820 | Time 21.2867(21.3825) | Bit/dim 3.4022(3.4381) | Xent 2.3026(2.3026) | Loss 3.4022(3.4381) | Error 0.8789(0.8991) Steps 886(887.56) | Grad Norm 1.6145(2.0735) | Total Time 14.00(14.00)\n",
      "Iter 7830 | Time 21.4319(21.3816) | Bit/dim 3.4335(3.4364) | Xent 2.3026(2.3026) | Loss 3.4335(3.4364) | Error 0.9033(0.8981) Steps 898(886.18) | Grad Norm 2.6679(2.1806) | Total Time 14.00(14.00)\n",
      "Iter 7840 | Time 22.0189(21.3890) | Bit/dim 3.4348(3.4373) | Xent 2.3026(2.3026) | Loss 3.4348(3.4373) | Error 0.9189(0.8986) Steps 904(888.70) | Grad Norm 2.6837(2.2301) | Total Time 14.00(14.00)\n",
      "Iter 7850 | Time 21.2571(21.4386) | Bit/dim 3.4549(3.4373) | Xent 2.3026(2.3026) | Loss 3.4549(3.4373) | Error 0.9033(0.8998) Steps 868(887.87) | Grad Norm 1.4957(2.1341) | Total Time 14.00(14.00)\n",
      "Iter 7860 | Time 21.1975(21.4357) | Bit/dim 3.4458(3.4401) | Xent 2.3026(2.3026) | Loss 3.4458(3.4401) | Error 0.8911(0.9003) Steps 880(886.92) | Grad Norm 1.7023(2.1364) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0143 | Time 113.3441, Epoch Time 1312.8346(1259.0102), Bit/dim 3.4402(best: 3.4384), Xent 2.3026, Loss 3.4402, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7870 | Time 21.1844(21.5217) | Bit/dim 3.4748(3.4398) | Xent 2.3026(2.3026) | Loss 3.4748(3.4398) | Error 0.9022(0.9007) Steps 904(888.06) | Grad Norm 2.4259(2.2028) | Total Time 14.00(14.00)\n",
      "Iter 7880 | Time 20.7342(21.5124) | Bit/dim 3.4485(3.4382) | Xent 2.3026(2.3026) | Loss 3.4485(3.4382) | Error 0.9011(0.9000) Steps 880(889.88) | Grad Norm 2.2785(2.1919) | Total Time 14.00(14.00)\n",
      "Iter 7890 | Time 20.9812(21.4645) | Bit/dim 3.4316(3.4365) | Xent 2.3026(2.3026) | Loss 3.4316(3.4365) | Error 0.9022(0.8999) Steps 898(892.94) | Grad Norm 2.8932(2.2372) | Total Time 14.00(14.00)\n",
      "Iter 7900 | Time 21.0202(21.3900) | Bit/dim 3.4709(3.4371) | Xent 2.3026(2.3026) | Loss 3.4709(3.4371) | Error 0.9189(0.9006) Steps 874(893.12) | Grad Norm 1.9287(2.1542) | Total Time 14.00(14.00)\n",
      "Iter 7910 | Time 21.0410(21.3611) | Bit/dim 3.4269(3.4384) | Xent 2.3026(2.3026) | Loss 3.4269(3.4384) | Error 0.8889(0.8989) Steps 892(892.01) | Grad Norm 2.1000(2.1075) | Total Time 14.00(14.00)\n",
      "Iter 7920 | Time 21.5269(21.3935) | Bit/dim 3.4605(3.4396) | Xent 2.3026(2.3026) | Loss 3.4605(3.4396) | Error 0.9078(0.9008) Steps 910(891.51) | Grad Norm 2.6834(2.1349) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0144 | Time 112.4356, Epoch Time 1306.7299(1260.4418), Bit/dim 3.4399(best: 3.4384), Xent 2.3026, Loss 3.4399, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7930 | Time 21.7027(21.3694) | Bit/dim 3.4438(3.4382) | Xent 2.3026(2.3026) | Loss 3.4438(3.4382) | Error 0.8878(0.9006) Steps 904(892.31) | Grad Norm 2.2605(2.1518) | Total Time 14.00(14.00)\n",
      "Iter 7940 | Time 21.5351(21.4216) | Bit/dim 3.4221(3.4355) | Xent 2.3026(2.3026) | Loss 3.4221(3.4355) | Error 0.8989(0.9011) Steps 886(891.16) | Grad Norm 2.1373(2.0774) | Total Time 14.00(14.00)\n",
      "Iter 7950 | Time 21.8763(21.4561) | Bit/dim 3.4591(3.4367) | Xent 2.3026(2.3026) | Loss 3.4591(3.4367) | Error 0.8889(0.8996) Steps 892(891.26) | Grad Norm 2.3542(2.1806) | Total Time 14.00(14.00)\n",
      "Iter 7960 | Time 21.7869(21.4469) | Bit/dim 3.4058(3.4379) | Xent 2.3026(2.3026) | Loss 3.4058(3.4379) | Error 0.9033(0.9006) Steps 892(891.65) | Grad Norm 1.9875(2.0235) | Total Time 14.00(14.00)\n",
      "Iter 7970 | Time 21.4554(21.3947) | Bit/dim 3.4515(3.4389) | Xent 2.3026(2.3026) | Loss 3.4515(3.4389) | Error 0.8900(0.9003) Steps 892(890.56) | Grad Norm 1.8644(2.0309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0145 | Time 113.0171, Epoch Time 1308.8273(1261.8933), Bit/dim 3.4374(best: 3.4384), Xent 2.3026, Loss 3.4374, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 7980 | Time 21.3518(21.4624) | Bit/dim 3.4408(3.4387) | Xent 2.3026(2.3026) | Loss 3.4408(3.4387) | Error 0.9111(0.8997) Steps 886(891.07) | Grad Norm 3.0768(2.0427) | Total Time 14.00(14.00)\n",
      "Iter 7990 | Time 21.0969(21.4462) | Bit/dim 3.4525(3.4389) | Xent 2.3026(2.3026) | Loss 3.4525(3.4389) | Error 0.9122(0.9000) Steps 886(891.38) | Grad Norm 1.9703(2.1243) | Total Time 14.00(14.00)\n",
      "Iter 8000 | Time 21.3337(21.3955) | Bit/dim 3.4287(3.4382) | Xent 2.3026(2.3026) | Loss 3.4287(3.4382) | Error 0.9033(0.9001) Steps 892(892.19) | Grad Norm 2.2170(2.0973) | Total Time 14.00(14.00)\n",
      "Iter 8010 | Time 21.5087(21.4019) | Bit/dim 3.4474(3.4409) | Xent 2.3026(2.3026) | Loss 3.4474(3.4409) | Error 0.9000(0.9004) Steps 904(892.56) | Grad Norm 1.7638(2.0554) | Total Time 14.00(14.00)\n",
      "Iter 8020 | Time 20.9671(21.4065) | Bit/dim 3.4400(3.4380) | Xent 2.3026(2.3026) | Loss 3.4400(3.4380) | Error 0.9111(0.9011) Steps 892(891.88) | Grad Norm 2.2120(2.2022) | Total Time 14.00(14.00)\n",
      "Iter 8030 | Time 21.2550(21.4572) | Bit/dim 3.4469(3.4370) | Xent 2.3026(2.3026) | Loss 3.4469(3.4370) | Error 0.9011(0.9000) Steps 898(892.41) | Grad Norm 3.1645(2.2769) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0146 | Time 112.6373, Epoch Time 1311.1364(1263.3706), Bit/dim 3.4412(best: 3.4374), Xent 2.3026, Loss 3.4412, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8040 | Time 21.6588(21.4339) | Bit/dim 3.4336(3.4359) | Xent 2.3026(2.3026) | Loss 3.4336(3.4359) | Error 0.8844(0.8999) Steps 892(891.23) | Grad Norm 2.0529(2.1327) | Total Time 14.00(14.00)\n",
      "Iter 8050 | Time 21.0438(21.4560) | Bit/dim 3.4368(3.4370) | Xent 2.3026(2.3026) | Loss 3.4368(3.4370) | Error 0.9056(0.9002) Steps 874(891.97) | Grad Norm 2.8203(2.1552) | Total Time 14.00(14.00)\n",
      "Iter 8060 | Time 22.0454(21.4452) | Bit/dim 3.4323(3.4393) | Xent 2.3026(2.3026) | Loss 3.4323(3.4393) | Error 0.9033(0.9001) Steps 904(890.96) | Grad Norm 2.4793(2.1776) | Total Time 14.00(14.00)\n",
      "Iter 8070 | Time 21.6464(21.4400) | Bit/dim 3.3956(3.4386) | Xent 2.3026(2.3026) | Loss 3.3956(3.4386) | Error 0.8789(0.8990) Steps 904(890.60) | Grad Norm 2.7924(2.0787) | Total Time 14.00(14.00)\n",
      "Iter 8080 | Time 20.7245(21.3649) | Bit/dim 3.4285(3.4360) | Xent 2.3026(2.3026) | Loss 3.4285(3.4360) | Error 0.8789(0.9002) Steps 880(890.50) | Grad Norm 1.5322(2.0719) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0147 | Time 112.5214, Epoch Time 1307.7614(1264.7023), Bit/dim 3.4433(best: 3.4374), Xent 2.3026, Loss 3.4433, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8090 | Time 21.5357(21.3790) | Bit/dim 3.4300(3.4343) | Xent 2.3026(2.3026) | Loss 3.4300(3.4343) | Error 0.9144(0.9001) Steps 886(891.36) | Grad Norm 1.1207(2.0603) | Total Time 14.00(14.00)\n",
      "Iter 8100 | Time 22.0881(21.4404) | Bit/dim 3.4258(3.4381) | Xent 2.3026(2.3026) | Loss 3.4258(3.4381) | Error 0.9000(0.9002) Steps 892(893.41) | Grad Norm 1.7032(2.0983) | Total Time 14.00(14.00)\n",
      "Iter 8110 | Time 22.1211(21.5595) | Bit/dim 3.4390(3.4367) | Xent 2.3026(2.3026) | Loss 3.4390(3.4367) | Error 0.8944(0.8997) Steps 886(893.79) | Grad Norm 2.6426(2.0915) | Total Time 14.00(14.00)\n",
      "Iter 8120 | Time 21.3139(21.5849) | Bit/dim 3.4146(3.4361) | Xent 2.3026(2.3026) | Loss 3.4146(3.4361) | Error 0.9089(0.9003) Steps 886(892.25) | Grad Norm 1.5295(2.1128) | Total Time 14.00(14.00)\n",
      "Iter 8130 | Time 20.9892(21.5590) | Bit/dim 3.4301(3.4348) | Xent 2.3026(2.3026) | Loss 3.4301(3.4348) | Error 0.9011(0.9001) Steps 892(891.92) | Grad Norm 1.5218(2.1088) | Total Time 14.00(14.00)\n",
      "Iter 8140 | Time 22.0120(21.5313) | Bit/dim 3.4491(3.4334) | Xent 2.3026(2.3026) | Loss 3.4491(3.4334) | Error 0.9078(0.9000) Steps 892(891.66) | Grad Norm 1.4774(1.9443) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0148 | Time 113.5237, Epoch Time 1318.2466(1266.3087), Bit/dim 3.4358(best: 3.4374), Xent 2.3026, Loss 3.4358, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8150 | Time 20.7886(21.4469) | Bit/dim 3.4121(3.4341) | Xent 2.3026(2.3026) | Loss 3.4121(3.4341) | Error 0.8889(0.9000) Steps 874(891.76) | Grad Norm 2.2395(2.0870) | Total Time 14.00(14.00)\n",
      "Iter 8160 | Time 21.4855(21.4830) | Bit/dim 3.3810(3.4319) | Xent 2.3026(2.3026) | Loss 3.3810(3.4319) | Error 0.8900(0.9001) Steps 922(893.50) | Grad Norm 2.1573(2.1935) | Total Time 14.00(14.00)\n",
      "Iter 8170 | Time 21.4071(21.4507) | Bit/dim 3.4382(3.4330) | Xent 2.3026(2.3026) | Loss 3.4382(3.4330) | Error 0.9200(0.8998) Steps 880(893.38) | Grad Norm 1.6723(1.9867) | Total Time 14.00(14.00)\n",
      "Iter 8180 | Time 21.5902(21.4967) | Bit/dim 3.4451(3.4333) | Xent 2.3026(2.3026) | Loss 3.4451(3.4333) | Error 0.9011(0.8992) Steps 886(894.10) | Grad Norm 2.1177(1.9859) | Total Time 14.00(14.00)\n",
      "Iter 8190 | Time 21.1354(21.5418) | Bit/dim 3.4483(3.4326) | Xent 2.3026(2.3026) | Loss 3.4483(3.4326) | Error 0.9067(0.8993) Steps 886(895.22) | Grad Norm 2.2729(2.1144) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0149 | Time 109.7590, Epoch Time 1309.8759(1267.6157), Bit/dim 3.4386(best: 3.4358), Xent 2.3026, Loss 3.4386, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8200 | Time 21.2270(21.5254) | Bit/dim 3.4106(3.4331) | Xent 2.3026(2.3026) | Loss 3.4106(3.4331) | Error 0.8967(0.9005) Steps 898(894.18) | Grad Norm 1.5154(2.1046) | Total Time 14.00(14.00)\n",
      "Iter 8210 | Time 21.5515(21.5679) | Bit/dim 3.4538(3.4350) | Xent 2.3026(2.3026) | Loss 3.4538(3.4350) | Error 0.9056(0.9004) Steps 886(895.14) | Grad Norm 2.2865(2.1303) | Total Time 14.00(14.00)\n",
      "Iter 8220 | Time 21.6562(21.5526) | Bit/dim 3.3938(3.4336) | Xent 2.3026(2.3026) | Loss 3.3938(3.4336) | Error 0.8967(0.9005) Steps 898(896.33) | Grad Norm 1.6240(2.1598) | Total Time 14.00(14.00)\n",
      "Iter 8230 | Time 21.9560(21.4950) | Bit/dim 3.4156(3.4344) | Xent 2.3026(2.3026) | Loss 3.4156(3.4344) | Error 0.8944(0.9005) Steps 904(895.44) | Grad Norm 2.8184(2.2315) | Total Time 14.00(14.00)\n",
      "Iter 8240 | Time 21.5055(21.4589) | Bit/dim 3.4187(3.4327) | Xent 2.3026(2.3026) | Loss 3.4187(3.4327) | Error 0.9056(0.8998) Steps 886(893.64) | Grad Norm 1.5370(2.1076) | Total Time 14.00(14.00)\n",
      "Iter 8250 | Time 21.4853(21.4654) | Bit/dim 3.3920(3.4325) | Xent 2.3026(2.3026) | Loss 3.3920(3.4325) | Error 0.8967(0.8995) Steps 892(893.55) | Grad Norm 2.8711(2.1055) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0150 | Time 112.1588, Epoch Time 1311.2273(1268.9240), Bit/dim 3.4363(best: 3.4358), Xent 2.3026, Loss 3.4363, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8260 | Time 22.2313(21.5502) | Bit/dim 3.4022(3.4321) | Xent 2.3026(2.3026) | Loss 3.4022(3.4321) | Error 0.9011(0.9001) Steps 904(895.21) | Grad Norm 1.5165(2.0291) | Total Time 14.00(14.00)\n",
      "Iter 8270 | Time 22.0919(21.6278) | Bit/dim 3.4019(3.4322) | Xent 2.3026(2.3026) | Loss 3.4019(3.4322) | Error 0.8989(0.9011) Steps 904(897.20) | Grad Norm 2.9490(2.0757) | Total Time 14.00(14.00)\n",
      "Iter 8280 | Time 21.5358(21.6466) | Bit/dim 3.4278(3.4322) | Xent 2.3026(2.3026) | Loss 3.4278(3.4322) | Error 0.8967(0.9010) Steps 898(897.24) | Grad Norm 2.3346(2.0956) | Total Time 14.00(14.00)\n",
      "Iter 8290 | Time 21.0573(21.6401) | Bit/dim 3.4386(3.4311) | Xent 2.3026(2.3026) | Loss 3.4386(3.4311) | Error 0.9044(0.8987) Steps 880(896.31) | Grad Norm 1.4791(2.0547) | Total Time 14.00(14.00)\n",
      "Iter 8300 | Time 21.4713(21.5905) | Bit/dim 3.4435(3.4330) | Xent 2.3026(2.3026) | Loss 3.4435(3.4330) | Error 0.9089(0.8995) Steps 886(895.18) | Grad Norm 2.3937(2.0579) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0151 | Time 111.9461, Epoch Time 1323.3043(1270.5554), Bit/dim 3.4338(best: 3.4358), Xent 2.3026, Loss 3.4338, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8310 | Time 21.0303(21.5527) | Bit/dim 3.4270(3.4329) | Xent 2.3026(2.3026) | Loss 3.4270(3.4329) | Error 0.8967(0.8993) Steps 886(894.06) | Grad Norm 1.2418(2.0804) | Total Time 14.00(14.00)\n",
      "Iter 8320 | Time 21.1585(21.5028) | Bit/dim 3.4385(3.4327) | Xent 2.3026(2.3026) | Loss 3.4385(3.4327) | Error 0.9089(0.8998) Steps 880(893.80) | Grad Norm 2.3674(2.1334) | Total Time 14.00(14.00)\n",
      "Iter 8330 | Time 21.0751(21.4854) | Bit/dim 3.3922(3.4308) | Xent 2.3026(2.3026) | Loss 3.3922(3.4308) | Error 0.8978(0.8988) Steps 886(892.33) | Grad Norm 1.8345(2.0299) | Total Time 14.00(14.00)\n",
      "Iter 8340 | Time 22.1557(21.6633) | Bit/dim 3.3986(3.4307) | Xent 2.3026(2.3026) | Loss 3.3986(3.4307) | Error 0.8911(0.8988) Steps 916(897.31) | Grad Norm 2.5239(2.1966) | Total Time 14.00(14.00)\n",
      "Iter 8350 | Time 21.0586(21.6642) | Bit/dim 3.4387(3.4321) | Xent 2.3026(2.3026) | Loss 3.4387(3.4321) | Error 0.9011(0.8999) Steps 892(897.29) | Grad Norm 1.9413(2.1462) | Total Time 14.00(14.00)\n",
      "Iter 8360 | Time 21.7116(21.6832) | Bit/dim 3.4320(3.4328) | Xent 2.3026(2.3026) | Loss 3.4320(3.4328) | Error 0.9178(0.9004) Steps 898(897.84) | Grad Norm 2.1813(2.0377) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0152 | Time 113.5129, Epoch Time 1321.5011(1272.0838), Bit/dim 3.4355(best: 3.4338), Xent 2.3026, Loss 3.4355, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8370 | Time 21.9452(21.7023) | Bit/dim 3.4587(3.4359) | Xent 2.3026(2.3026) | Loss 3.4587(3.4359) | Error 0.8967(0.8990) Steps 916(899.67) | Grad Norm 2.9303(2.1851) | Total Time 14.00(14.00)\n",
      "Iter 8380 | Time 21.1324(21.6694) | Bit/dim 3.4565(3.4338) | Xent 2.3026(2.3026) | Loss 3.4565(3.4338) | Error 0.9189(0.8993) Steps 898(899.98) | Grad Norm 2.6142(2.2432) | Total Time 14.00(14.00)\n",
      "Iter 8390 | Time 21.9616(21.6768) | Bit/dim 3.4440(3.4324) | Xent 2.3026(2.3026) | Loss 3.4440(3.4324) | Error 0.9056(0.8998) Steps 898(900.55) | Grad Norm 1.7027(2.0958) | Total Time 14.00(14.00)\n",
      "Iter 8400 | Time 21.3856(21.6142) | Bit/dim 3.4059(3.4316) | Xent 2.3026(2.3026) | Loss 3.4059(3.4316) | Error 0.9056(0.9001) Steps 916(899.62) | Grad Norm 2.8922(2.1065) | Total Time 14.00(14.00)\n",
      "Iter 8410 | Time 21.9829(21.6341) | Bit/dim 3.4339(3.4307) | Xent 2.3026(2.3026) | Loss 3.4339(3.4307) | Error 0.8978(0.8998) Steps 880(898.52) | Grad Norm 1.6829(2.1060) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0153 | Time 113.5245, Epoch Time 1320.6857(1273.5419), Bit/dim 3.4356(best: 3.4338), Xent 2.3026, Loss 3.4356, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8420 | Time 21.7616(21.6491) | Bit/dim 3.4364(3.4306) | Xent 2.3026(2.3026) | Loss 3.4364(3.4306) | Error 0.9133(0.9008) Steps 886(897.04) | Grad Norm 1.9366(2.0402) | Total Time 14.00(14.00)\n",
      "Iter 8430 | Time 22.0344(21.6658) | Bit/dim 3.4320(3.4302) | Xent 2.3026(2.3026) | Loss 3.4320(3.4302) | Error 0.8944(0.9007) Steps 886(895.80) | Grad Norm 1.9606(2.0404) | Total Time 14.00(14.00)\n",
      "Iter 8440 | Time 22.0673(21.6787) | Bit/dim 3.4153(3.4310) | Xent 2.3026(2.3026) | Loss 3.4153(3.4310) | Error 0.9044(0.9005) Steps 916(896.33) | Grad Norm 2.3540(2.0937) | Total Time 14.00(14.00)\n",
      "Iter 8450 | Time 21.4739(21.6613) | Bit/dim 3.4091(3.4291) | Xent 2.3026(2.3026) | Loss 3.4091(3.4291) | Error 0.9000(0.8990) Steps 904(897.27) | Grad Norm 2.0608(2.2083) | Total Time 14.00(14.00)\n",
      "Iter 8460 | Time 21.9092(21.7959) | Bit/dim 3.4303(3.4306) | Xent 2.3026(2.3026) | Loss 3.4303(3.4306) | Error 0.9011(0.8984) Steps 898(900.53) | Grad Norm 1.7371(2.2055) | Total Time 14.00(14.00)\n",
      "Iter 8470 | Time 22.3384(21.8501) | Bit/dim 3.4495(3.4310) | Xent 2.3026(2.3026) | Loss 3.4495(3.4310) | Error 0.9200(0.9010) Steps 910(901.23) | Grad Norm 1.7849(2.1899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0154 | Time 114.0471, Epoch Time 1333.0372(1275.3267), Bit/dim 3.4300(best: 3.4338), Xent 2.3026, Loss 3.4300, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8480 | Time 22.2024(21.8430) | Bit/dim 3.4680(3.4316) | Xent 2.3026(2.3026) | Loss 3.4680(3.4316) | Error 0.9144(0.9009) Steps 922(902.45) | Grad Norm 2.3408(2.1831) | Total Time 14.00(14.00)\n",
      "Iter 8490 | Time 21.9516(21.7823) | Bit/dim 3.3976(3.4275) | Xent 2.3026(2.3026) | Loss 3.3976(3.4275) | Error 0.9078(0.9008) Steps 892(901.73) | Grad Norm 1.8246(2.1146) | Total Time 14.00(14.00)\n",
      "Iter 8500 | Time 21.3359(21.7647) | Bit/dim 3.4166(3.4282) | Xent 2.3026(2.3026) | Loss 3.4166(3.4282) | Error 0.8878(0.9010) Steps 886(901.37) | Grad Norm 1.4186(2.0832) | Total Time 14.00(14.00)\n",
      "Iter 8510 | Time 22.6513(21.8184) | Bit/dim 3.4424(3.4313) | Xent 2.3026(2.3026) | Loss 3.4424(3.4313) | Error 0.9000(0.9010) Steps 934(904.05) | Grad Norm 1.7590(2.0927) | Total Time 14.00(14.00)\n",
      "Iter 8520 | Time 21.2428(21.7961) | Bit/dim 3.4606(3.4301) | Xent 2.3026(2.3026) | Loss 3.4606(3.4301) | Error 0.9011(0.9008) Steps 904(903.31) | Grad Norm 2.5147(2.0468) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0155 | Time 113.4158, Epoch Time 1328.4375(1276.9201), Bit/dim 3.4314(best: 3.4300), Xent 2.3026, Loss 3.4314, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8530 | Time 21.3546(21.7435) | Bit/dim 3.4425(3.4316) | Xent 2.3026(2.3026) | Loss 3.4425(3.4316) | Error 0.8944(0.9001) Steps 892(903.44) | Grad Norm 2.3545(2.0932) | Total Time 14.00(14.00)\n",
      "Iter 8540 | Time 21.3868(21.7492) | Bit/dim 3.4286(3.4325) | Xent 2.3026(2.3026) | Loss 3.4286(3.4325) | Error 0.9022(0.8998) Steps 880(902.74) | Grad Norm 1.2010(1.9993) | Total Time 14.00(14.00)\n",
      "Iter 8550 | Time 21.2807(21.7449) | Bit/dim 3.4462(3.4317) | Xent 2.3026(2.3026) | Loss 3.4462(3.4317) | Error 0.9033(0.8999) Steps 904(904.07) | Grad Norm 4.1354(2.1125) | Total Time 14.00(14.00)\n",
      "Iter 8560 | Time 21.6470(21.7697) | Bit/dim 3.4152(3.4300) | Xent 2.3026(2.3026) | Loss 3.4152(3.4300) | Error 0.8656(0.8995) Steps 892(907.60) | Grad Norm 1.5865(2.1121) | Total Time 14.00(14.00)\n",
      "Iter 8570 | Time 21.9317(21.7993) | Bit/dim 3.3949(3.4254) | Xent 2.3026(2.3026) | Loss 3.3949(3.4254) | Error 0.8933(0.8995) Steps 904(908.49) | Grad Norm 2.0324(2.1536) | Total Time 14.00(14.00)\n",
      "Iter 8580 | Time 22.1432(21.8724) | Bit/dim 3.4328(3.4278) | Xent 2.3026(2.3026) | Loss 3.4328(3.4278) | Error 0.8944(0.9002) Steps 910(910.09) | Grad Norm 1.6382(2.0558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0156 | Time 112.6045, Epoch Time 1331.7026(1278.5635), Bit/dim 3.4303(best: 3.4300), Xent 2.3026, Loss 3.4303, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8590 | Time 21.8688(21.8167) | Bit/dim 3.4007(3.4279) | Xent 2.3026(2.3026) | Loss 3.4007(3.4279) | Error 0.8978(0.9000) Steps 886(906.55) | Grad Norm 1.9959(2.0194) | Total Time 14.00(14.00)\n",
      "Iter 8600 | Time 21.5729(21.7619) | Bit/dim 3.4449(3.4311) | Xent 2.3026(2.3026) | Loss 3.4449(3.4311) | Error 0.9200(0.9012) Steps 910(905.14) | Grad Norm 2.9190(1.9920) | Total Time 14.00(14.00)\n",
      "Iter 8610 | Time 22.2342(21.7552) | Bit/dim 3.4453(3.4280) | Xent 2.3026(2.3026) | Loss 3.4453(3.4280) | Error 0.9078(0.9009) Steps 904(904.69) | Grad Norm 1.9326(2.0422) | Total Time 14.00(14.00)\n",
      "Iter 8620 | Time 21.0967(21.7204) | Bit/dim 3.4224(3.4284) | Xent 2.3026(2.3026) | Loss 3.4224(3.4284) | Error 0.8967(0.9003) Steps 916(905.34) | Grad Norm 2.3041(2.0337) | Total Time 14.00(14.00)\n",
      "Iter 8630 | Time 22.4270(21.9054) | Bit/dim 3.4389(3.4284) | Xent 2.3026(2.3026) | Loss 3.4389(3.4284) | Error 0.9056(0.9001) Steps 910(907.72) | Grad Norm 2.2892(2.0620) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0157 | Time 114.5998, Epoch Time 1333.8797(1280.2230), Bit/dim 3.4324(best: 3.4300), Xent 2.3026, Loss 3.4324, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8640 | Time 21.8834(21.9546) | Bit/dim 3.4125(3.4235) | Xent 2.3026(2.3026) | Loss 3.4125(3.4235) | Error 0.8978(0.8987) Steps 916(907.82) | Grad Norm 2.0233(2.0067) | Total Time 14.00(14.00)\n",
      "Iter 8650 | Time 22.2924(21.9548) | Bit/dim 3.3863(3.4253) | Xent 2.3026(2.3026) | Loss 3.3863(3.4253) | Error 0.8867(0.8996) Steps 916(910.35) | Grad Norm 1.6987(2.1562) | Total Time 14.00(14.00)\n",
      "Iter 8660 | Time 22.3531(22.0161) | Bit/dim 3.4231(3.4260) | Xent 2.3026(2.3026) | Loss 3.4231(3.4260) | Error 0.8922(0.8996) Steps 910(911.31) | Grad Norm 1.8708(2.1058) | Total Time 14.00(14.00)\n",
      "Iter 8670 | Time 22.0905(22.0071) | Bit/dim 3.4240(3.4284) | Xent 2.3026(2.3026) | Loss 3.4240(3.4284) | Error 0.9000(0.8999) Steps 898(909.62) | Grad Norm 1.8663(2.0497) | Total Time 14.00(14.00)\n",
      "Iter 8680 | Time 22.1015(21.9922) | Bit/dim 3.4344(3.4288) | Xent 2.3026(2.3026) | Loss 3.4344(3.4288) | Error 0.9067(0.8998) Steps 928(909.69) | Grad Norm 2.3170(1.9793) | Total Time 14.00(14.00)\n",
      "Iter 8690 | Time 22.3027(22.0467) | Bit/dim 3.4413(3.4274) | Xent 2.3026(2.3026) | Loss 3.4413(3.4274) | Error 0.9011(0.9005) Steps 898(909.27) | Grad Norm 2.8531(2.0891) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0158 | Time 113.6202, Epoch Time 1344.6051(1282.1545), Bit/dim 3.4311(best: 3.4300), Xent 2.3026, Loss 3.4311, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8700 | Time 22.3520(22.0995) | Bit/dim 3.4506(3.4251) | Xent 2.3026(2.3026) | Loss 3.4506(3.4251) | Error 0.9111(0.9005) Steps 916(908.15) | Grad Norm 2.6858(2.0966) | Total Time 14.00(14.00)\n",
      "Iter 8710 | Time 22.4741(22.0898) | Bit/dim 3.4619(3.4266) | Xent 2.3026(2.3026) | Loss 3.4619(3.4266) | Error 0.8956(0.9004) Steps 910(907.82) | Grad Norm 2.4050(2.1336) | Total Time 14.00(14.00)\n",
      "Iter 8720 | Time 21.4208(22.0567) | Bit/dim 3.4126(3.4252) | Xent 2.3026(2.3026) | Loss 3.4126(3.4252) | Error 0.8922(0.9002) Steps 880(908.41) | Grad Norm 1.5012(2.1384) | Total Time 14.00(14.00)\n",
      "Iter 8730 | Time 22.4669(21.9501) | Bit/dim 3.4224(3.4282) | Xent 2.3026(2.3026) | Loss 3.4224(3.4282) | Error 0.9144(0.9002) Steps 910(906.35) | Grad Norm 1.5095(2.0634) | Total Time 14.00(14.00)\n",
      "Iter 8740 | Time 22.2019(22.0134) | Bit/dim 3.4133(3.4276) | Xent 2.3026(2.3026) | Loss 3.4133(3.4276) | Error 0.8911(0.9001) Steps 928(909.54) | Grad Norm 1.9855(2.1469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0159 | Time 114.3412, Epoch Time 1344.0429(1284.0111), Bit/dim 3.4283(best: 3.4300), Xent 2.3026, Loss 3.4283, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8750 | Time 21.8962(22.0415) | Bit/dim 3.4106(3.4275) | Xent 2.3026(2.3026) | Loss 3.4106(3.4275) | Error 0.9078(0.9009) Steps 928(911.86) | Grad Norm 1.9507(2.0713) | Total Time 14.00(14.00)\n",
      "Iter 8760 | Time 21.8664(22.0014) | Bit/dim 3.3947(3.4251) | Xent 2.3026(2.3026) | Loss 3.3947(3.4251) | Error 0.8967(0.8999) Steps 886(910.79) | Grad Norm 2.3335(2.1445) | Total Time 14.00(14.00)\n",
      "Iter 8770 | Time 21.7202(21.9486) | Bit/dim 3.4100(3.4264) | Xent 2.3026(2.3026) | Loss 3.4100(3.4264) | Error 0.9100(0.8995) Steps 904(909.93) | Grad Norm 1.9411(2.1057) | Total Time 14.00(14.00)\n",
      "Iter 8780 | Time 21.7911(21.9650) | Bit/dim 3.4315(3.4261) | Xent 2.3026(2.3026) | Loss 3.4315(3.4261) | Error 0.9011(0.8992) Steps 916(910.28) | Grad Norm 2.1570(2.1188) | Total Time 14.00(14.00)\n",
      "Iter 8790 | Time 21.2786(21.8802) | Bit/dim 3.4243(3.4259) | Xent 2.3026(2.3026) | Loss 3.4243(3.4259) | Error 0.8933(0.8989) Steps 904(909.97) | Grad Norm 2.6923(2.1251) | Total Time 14.00(14.00)\n",
      "Iter 8800 | Time 21.9778(21.9260) | Bit/dim 3.4114(3.4270) | Xent 2.3026(2.3026) | Loss 3.4114(3.4270) | Error 0.9111(0.9007) Steps 916(911.15) | Grad Norm 1.6035(2.1439) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0160 | Time 113.2373, Epoch Time 1336.1329(1285.5748), Bit/dim 3.4330(best: 3.4283), Xent 2.3026, Loss 3.4330, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8810 | Time 21.3688(21.8845) | Bit/dim 3.4302(3.4253) | Xent 2.3026(2.3026) | Loss 3.4302(3.4253) | Error 0.8911(0.8988) Steps 910(910.75) | Grad Norm 2.6687(2.1314) | Total Time 14.00(14.00)\n",
      "Iter 8820 | Time 21.5552(21.9524) | Bit/dim 3.4443(3.4263) | Xent 2.3026(2.3026) | Loss 3.4443(3.4263) | Error 0.8989(0.8994) Steps 910(913.09) | Grad Norm 2.0997(2.1152) | Total Time 14.00(14.00)\n",
      "Iter 8830 | Time 21.6320(21.9474) | Bit/dim 3.4225(3.4249) | Xent 2.3026(2.3026) | Loss 3.4225(3.4249) | Error 0.9089(0.8988) Steps 916(914.07) | Grad Norm 1.5811(2.0709) | Total Time 14.00(14.00)\n",
      "Iter 8840 | Time 22.5821(21.9474) | Bit/dim 3.4327(3.4247) | Xent 2.3026(2.3026) | Loss 3.4327(3.4247) | Error 0.8933(0.8997) Steps 916(914.42) | Grad Norm 1.8635(1.9402) | Total Time 14.00(14.00)\n",
      "Iter 8850 | Time 22.1708(21.9928) | Bit/dim 3.4340(3.4274) | Xent 2.3026(2.3026) | Loss 3.4340(3.4274) | Error 0.9044(0.9011) Steps 916(914.75) | Grad Norm 0.9668(1.8839) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0161 | Time 111.8948, Epoch Time 1339.1740(1287.1828), Bit/dim 3.4328(best: 3.4283), Xent 2.3026, Loss 3.4328, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8860 | Time 22.4040(21.9781) | Bit/dim 3.4381(3.4255) | Xent 2.3026(2.3026) | Loss 3.4381(3.4255) | Error 0.9056(0.9013) Steps 910(915.05) | Grad Norm 2.9625(2.0706) | Total Time 14.00(14.00)\n",
      "Iter 8870 | Time 22.1886(21.9813) | Bit/dim 3.4374(3.4269) | Xent 2.3026(2.3026) | Loss 3.4374(3.4269) | Error 0.9067(0.9007) Steps 892(914.59) | Grad Norm 2.4569(2.0026) | Total Time 14.00(14.00)\n",
      "Iter 8880 | Time 22.5451(22.0372) | Bit/dim 3.4292(3.4269) | Xent 2.3026(2.3026) | Loss 3.4292(3.4269) | Error 0.9133(0.9008) Steps 922(914.49) | Grad Norm 1.3195(2.0835) | Total Time 14.00(14.00)\n",
      "Iter 8890 | Time 22.3292(21.9771) | Bit/dim 3.3959(3.4273) | Xent 2.3026(2.3026) | Loss 3.3959(3.4273) | Error 0.9100(0.9010) Steps 928(913.76) | Grad Norm 1.7180(2.0094) | Total Time 14.00(14.00)\n",
      "Iter 8900 | Time 22.0164(21.9076) | Bit/dim 3.4122(3.4254) | Xent 2.3026(2.3026) | Loss 3.4122(3.4254) | Error 0.8967(0.9000) Steps 910(913.28) | Grad Norm 2.0941(2.0370) | Total Time 14.00(14.00)\n",
      "Iter 8910 | Time 22.4033(21.8973) | Bit/dim 3.4507(3.4250) | Xent 2.3026(2.3026) | Loss 3.4507(3.4250) | Error 0.9111(0.9004) Steps 934(915.25) | Grad Norm 1.5954(2.0387) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0162 | Time 112.3765, Epoch Time 1335.1943(1288.6231), Bit/dim 3.4267(best: 3.4283), Xent 2.3026, Loss 3.4267, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8920 | Time 22.1109(21.9366) | Bit/dim 3.4317(3.4239) | Xent 2.3026(2.3026) | Loss 3.4317(3.4239) | Error 0.9022(0.9018) Steps 916(914.68) | Grad Norm 2.0296(2.0256) | Total Time 14.00(14.00)\n",
      "Iter 8930 | Time 21.7686(21.9067) | Bit/dim 3.4147(3.4219) | Xent 2.3026(2.3026) | Loss 3.4147(3.4219) | Error 0.8844(0.9002) Steps 910(913.94) | Grad Norm 3.2469(2.0325) | Total Time 14.00(14.00)\n",
      "Iter 8940 | Time 21.3371(21.9463) | Bit/dim 3.4030(3.4196) | Xent 2.3026(2.3026) | Loss 3.4030(3.4196) | Error 0.9089(0.8992) Steps 916(915.32) | Grad Norm 1.7573(2.0515) | Total Time 14.00(14.00)\n",
      "Iter 8950 | Time 22.2783(21.9365) | Bit/dim 3.4332(3.4229) | Xent 2.3026(2.3026) | Loss 3.4332(3.4229) | Error 0.9056(0.9001) Steps 898(914.98) | Grad Norm 2.0543(2.0630) | Total Time 14.00(14.00)\n",
      "Iter 8960 | Time 21.8527(21.9066) | Bit/dim 3.4144(3.4253) | Xent 2.3026(2.3026) | Loss 3.4144(3.4253) | Error 0.9011(0.9001) Steps 916(914.03) | Grad Norm 2.1270(2.1248) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0163 | Time 114.7094, Epoch Time 1337.9041(1290.1015), Bit/dim 3.4248(best: 3.4267), Xent 2.3026, Loss 3.4248, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 8970 | Time 20.8615(21.8699) | Bit/dim 3.4145(3.4264) | Xent 2.3026(2.3026) | Loss 3.4145(3.4264) | Error 0.8978(0.9004) Steps 904(911.62) | Grad Norm 1.4000(2.0776) | Total Time 14.00(14.00)\n",
      "Iter 8980 | Time 22.3418(21.8175) | Bit/dim 3.4354(3.4275) | Xent 2.3026(2.3026) | Loss 3.4354(3.4275) | Error 0.9078(0.8995) Steps 922(910.93) | Grad Norm 2.8286(1.9528) | Total Time 14.00(14.00)\n",
      "Iter 8990 | Time 22.4386(21.8179) | Bit/dim 3.4399(3.4263) | Xent 2.3026(2.3026) | Loss 3.4399(3.4263) | Error 0.9144(0.8995) Steps 922(910.33) | Grad Norm 2.8750(2.0239) | Total Time 14.00(14.00)\n",
      "Iter 9000 | Time 21.5009(21.8033) | Bit/dim 3.4001(3.4258) | Xent 2.3026(2.3026) | Loss 3.4001(3.4258) | Error 0.9067(0.8998) Steps 922(910.07) | Grad Norm 1.3999(1.9482) | Total Time 14.00(14.00)\n",
      "Iter 9010 | Time 21.8393(21.8106) | Bit/dim 3.4231(3.4263) | Xent 2.3026(2.3026) | Loss 3.4231(3.4263) | Error 0.8900(0.9011) Steps 904(910.32) | Grad Norm 2.3113(2.0424) | Total Time 14.00(14.00)\n",
      "Iter 9020 | Time 21.5318(21.8146) | Bit/dim 3.4213(3.4216) | Xent 2.3026(2.3026) | Loss 3.4213(3.4216) | Error 0.9100(0.8999) Steps 916(913.07) | Grad Norm 1.6972(2.0899) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0164 | Time 113.1462, Epoch Time 1329.0976(1291.2714), Bit/dim 3.4278(best: 3.4248), Xent 2.3026, Loss 3.4278, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9030 | Time 21.6606(21.8573) | Bit/dim 3.4398(3.4236) | Xent 2.3026(2.3026) | Loss 3.4398(3.4236) | Error 0.9256(0.9010) Steps 910(913.05) | Grad Norm 1.5516(1.9295) | Total Time 14.00(14.00)\n",
      "Iter 9040 | Time 21.2851(21.8662) | Bit/dim 3.4000(3.4201) | Xent 2.3026(2.3026) | Loss 3.4000(3.4201) | Error 0.8822(0.8999) Steps 940(916.68) | Grad Norm 3.1434(2.0517) | Total Time 14.00(14.00)\n",
      "Iter 9050 | Time 21.9827(21.9221) | Bit/dim 3.4179(3.4218) | Xent 2.3026(2.3026) | Loss 3.4179(3.4218) | Error 0.8944(0.8992) Steps 916(914.53) | Grad Norm 1.3522(2.0941) | Total Time 14.00(14.00)\n",
      "Iter 9060 | Time 21.9192(21.9095) | Bit/dim 3.4192(3.4217) | Xent 2.3026(2.3026) | Loss 3.4192(3.4217) | Error 0.9044(0.8998) Steps 904(913.77) | Grad Norm 1.8603(1.9960) | Total Time 14.00(14.00)\n",
      "Iter 9070 | Time 21.5103(21.8939) | Bit/dim 3.4578(3.4237) | Xent 2.3026(2.3026) | Loss 3.4578(3.4237) | Error 0.9044(0.9002) Steps 898(913.37) | Grad Norm 1.6018(1.9239) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0165 | Time 110.7368, Epoch Time 1335.5879(1292.6009), Bit/dim 3.4301(best: 3.4248), Xent 2.3026, Loss 3.4301, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9080 | Time 21.8729(21.8678) | Bit/dim 3.3975(3.4211) | Xent 2.3026(2.3026) | Loss 3.3975(3.4211) | Error 0.8911(0.8986) Steps 910(912.34) | Grad Norm 2.3399(1.9918) | Total Time 14.00(14.00)\n",
      "Iter 9090 | Time 21.8217(21.8945) | Bit/dim 3.4309(3.4208) | Xent 2.3026(2.3026) | Loss 3.4309(3.4208) | Error 0.8933(0.8995) Steps 904(912.43) | Grad Norm 1.1995(1.8898) | Total Time 14.00(14.00)\n",
      "Iter 9100 | Time 21.2191(21.8758) | Bit/dim 3.4296(3.4211) | Xent 2.3026(2.3026) | Loss 3.4296(3.4211) | Error 0.9178(0.8989) Steps 910(912.18) | Grad Norm 1.6424(1.8342) | Total Time 14.00(14.00)\n",
      "Iter 9110 | Time 22.0889(21.9567) | Bit/dim 3.4427(3.4231) | Xent 2.3026(2.3026) | Loss 3.4427(3.4231) | Error 0.9078(0.8994) Steps 940(913.57) | Grad Norm 1.5976(1.8773) | Total Time 14.00(14.00)\n",
      "Iter 9120 | Time 22.5722(21.9861) | Bit/dim 3.3642(3.4211) | Xent 2.3026(2.3026) | Loss 3.3642(3.4211) | Error 0.9022(0.9000) Steps 916(912.35) | Grad Norm 2.2754(1.9247) | Total Time 14.00(14.00)\n",
      "Iter 9130 | Time 21.9417(21.9617) | Bit/dim 3.4239(3.4218) | Xent 2.3026(2.3026) | Loss 3.4239(3.4218) | Error 0.9011(0.9007) Steps 904(911.95) | Grad Norm 2.0284(2.1230) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0166 | Time 112.7115, Epoch Time 1338.7685(1293.9859), Bit/dim 3.4245(best: 3.4248), Xent 2.3026, Loss 3.4245, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9140 | Time 22.7364(22.0125) | Bit/dim 3.3948(3.4239) | Xent 2.3026(2.3026) | Loss 3.3948(3.4239) | Error 0.8744(0.9008) Steps 910(911.11) | Grad Norm 1.8752(2.0371) | Total Time 14.00(14.00)\n",
      "Iter 9150 | Time 21.8256(22.0078) | Bit/dim 3.4317(3.4256) | Xent 2.3026(2.3026) | Loss 3.4317(3.4256) | Error 0.9100(0.9015) Steps 892(911.01) | Grad Norm 2.2464(1.9327) | Total Time 14.00(14.00)\n",
      "Iter 9160 | Time 21.8845(21.9913) | Bit/dim 3.4260(3.4225) | Xent 2.3026(2.3026) | Loss 3.4260(3.4225) | Error 0.9000(0.9009) Steps 922(912.19) | Grad Norm 2.2884(1.9562) | Total Time 14.00(14.00)\n",
      "Iter 9170 | Time 21.5372(21.9864) | Bit/dim 3.4409(3.4238) | Xent 2.3026(2.3026) | Loss 3.4409(3.4238) | Error 0.9067(0.9014) Steps 928(912.77) | Grad Norm 2.1478(1.9901) | Total Time 14.00(14.00)\n",
      "Iter 9180 | Time 22.1203(21.9603) | Bit/dim 3.4315(3.4198) | Xent 2.3026(2.3026) | Loss 3.4315(3.4198) | Error 0.9000(0.8995) Steps 898(913.16) | Grad Norm 2.0303(2.0463) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0167 | Time 113.1255, Epoch Time 1340.3529(1295.3770), Bit/dim 3.4319(best: 3.4245), Xent 2.3026, Loss 3.4319, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9190 | Time 22.0572(21.9135) | Bit/dim 3.4347(3.4194) | Xent 2.3026(2.3026) | Loss 3.4347(3.4194) | Error 0.8967(0.8993) Steps 904(913.51) | Grad Norm 2.7279(2.0643) | Total Time 14.00(14.00)\n",
      "Iter 9200 | Time 22.1703(21.9219) | Bit/dim 3.4334(3.4229) | Xent 2.3026(2.3026) | Loss 3.4334(3.4229) | Error 0.9000(0.9000) Steps 928(915.71) | Grad Norm 1.9593(2.0470) | Total Time 14.00(14.00)\n",
      "Iter 9210 | Time 21.9238(21.9561) | Bit/dim 3.4083(3.4233) | Xent 2.3026(2.3026) | Loss 3.4083(3.4233) | Error 0.9144(0.9012) Steps 910(915.89) | Grad Norm 2.0425(2.1125) | Total Time 14.00(14.00)\n",
      "Iter 9220 | Time 22.4644(22.0191) | Bit/dim 3.4118(3.4195) | Xent 2.3026(2.3026) | Loss 3.4118(3.4195) | Error 0.8833(0.8997) Steps 904(916.24) | Grad Norm 1.6506(2.0637) | Total Time 14.00(14.00)\n",
      "Iter 9230 | Time 21.3682(22.1086) | Bit/dim 3.3833(3.4188) | Xent 2.3026(2.3026) | Loss 3.3833(3.4188) | Error 0.8911(0.8994) Steps 910(916.07) | Grad Norm 1.5146(2.0565) | Total Time 14.00(14.00)\n",
      "Iter 9240 | Time 21.6259(22.0979) | Bit/dim 3.4266(3.4191) | Xent 2.3026(2.3026) | Loss 3.4266(3.4191) | Error 0.9067(0.9001) Steps 904(915.97) | Grad Norm 2.2381(2.1141) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0168 | Time 112.4103, Epoch Time 1344.6228(1296.8543), Bit/dim 3.4272(best: 3.4245), Xent 2.3026, Loss 3.4272, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9250 | Time 22.3406(22.1324) | Bit/dim 3.4129(3.4191) | Xent 2.3026(2.3026) | Loss 3.4129(3.4191) | Error 0.8967(0.9004) Steps 928(918.87) | Grad Norm 1.5319(2.0764) | Total Time 14.00(14.00)\n",
      "Iter 9260 | Time 22.2755(22.0888) | Bit/dim 3.4325(3.4211) | Xent 2.3026(2.3026) | Loss 3.4325(3.4211) | Error 0.8967(0.8997) Steps 922(917.43) | Grad Norm 1.6423(2.0901) | Total Time 14.00(14.00)\n",
      "Iter 9270 | Time 22.1033(22.1093) | Bit/dim 3.4064(3.4198) | Xent 2.3026(2.3026) | Loss 3.4064(3.4198) | Error 0.8989(0.8994) Steps 892(918.14) | Grad Norm 1.9173(2.1390) | Total Time 14.00(14.00)\n",
      "Iter 9280 | Time 21.6560(22.1113) | Bit/dim 3.4240(3.4202) | Xent 2.3026(2.3026) | Loss 3.4240(3.4202) | Error 0.8944(0.9001) Steps 910(916.19) | Grad Norm 2.6282(2.0354) | Total Time 14.00(14.00)\n",
      "Iter 9290 | Time 21.5095(22.0495) | Bit/dim 3.3845(3.4194) | Xent 2.3026(2.3026) | Loss 3.3845(3.4194) | Error 0.9022(0.9003) Steps 910(913.02) | Grad Norm 2.3223(2.1225) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0169 | Time 112.2092, Epoch Time 1343.4932(1298.2535), Bit/dim 3.4221(best: 3.4245), Xent 2.3026, Loss 3.4221, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9300 | Time 22.6309(22.0612) | Bit/dim 3.3806(3.4170) | Xent 2.3026(2.3026) | Loss 3.3806(3.4170) | Error 0.8867(0.9002) Steps 934(912.51) | Grad Norm 2.6303(2.1114) | Total Time 14.00(14.00)\n",
      "Iter 9310 | Time 22.1891(21.9723) | Bit/dim 3.4047(3.4168) | Xent 2.3026(2.3026) | Loss 3.4047(3.4168) | Error 0.9111(0.8997) Steps 910(911.62) | Grad Norm 1.5496(2.0709) | Total Time 14.00(14.00)\n",
      "Iter 9320 | Time 22.0495(21.9207) | Bit/dim 3.4315(3.4198) | Xent 2.3026(2.3026) | Loss 3.4315(3.4198) | Error 0.9100(0.8997) Steps 892(910.13) | Grad Norm 2.2859(2.1160) | Total Time 14.00(14.00)\n",
      "Iter 9330 | Time 22.2637(21.9519) | Bit/dim 3.4075(3.4190) | Xent 2.3026(2.3026) | Loss 3.4075(3.4190) | Error 0.9078(0.8995) Steps 916(910.24) | Grad Norm 1.4501(2.1201) | Total Time 14.00(14.00)\n",
      "Iter 9340 | Time 21.4136(22.0356) | Bit/dim 3.4286(3.4210) | Xent 2.3026(2.3026) | Loss 3.4286(3.4210) | Error 0.9011(0.9003) Steps 916(912.81) | Grad Norm 2.2628(2.1225) | Total Time 14.00(14.00)\n",
      "Iter 9350 | Time 22.1069(22.0639) | Bit/dim 3.4351(3.4207) | Xent 2.3026(2.3026) | Loss 3.4351(3.4207) | Error 0.9089(0.9002) Steps 904(913.24) | Grad Norm 1.7392(2.0124) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0170 | Time 113.3158, Epoch Time 1342.2614(1299.5737), Bit/dim 3.4186(best: 3.4221), Xent 2.3026, Loss 3.4186, Error 0.9000(best: 0.9000)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 9360 | Time 22.0688(22.0122) | Bit/dim 3.4260(3.4188) | Xent 2.3026(2.3026) | Loss 3.4260(3.4188) | Error 0.8933(0.8993) Steps 916(912.95) | Grad Norm 1.2742(2.0885) | Total Time 14.00(14.00)\n",
      "Iter 9370 | Time 21.6550(22.0073) | Bit/dim 3.4165(3.4200) | Xent 2.3026(2.3026) | Loss 3.4165(3.4200) | Error 0.9033(0.8999) Steps 892(911.70) | Grad Norm 1.9307(2.0362) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_03125_drop_0_5_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode unsup --log_freq 10 --weight_y 0.5 --condition_ratio 0.03125 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
