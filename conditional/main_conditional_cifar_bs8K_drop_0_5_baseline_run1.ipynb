{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_drop_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz = modules.GaussianDiag.logp(mean, logs, z).view(-1,1)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        z = model.module.dropout(z)\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(z)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z = modules.GaussianDiag.sample(mean, logs)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=8000, batch_size_schedule='', begin_epoch=1, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=1, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_run1/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=5000, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000.0, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=6144, bias=True)\n",
      "  (project_class): LinearZeros(in_features=3072, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1469494\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 1501 | Time 127.5391(78.3046) | Bit/dim 4.0424(4.0337) | Xent 1.1731(0.8495) | Loss 4.6289(4.4584) | Error 0.4106(0.3035) Steps 820(822.61) | Grad Norm 30.9822(7.4288) | Total Time 14.00(14.00)\n",
      "Iter 1502 | Time 77.3683(78.2765) | Bit/dim 4.0245(4.0334) | Xent 1.0255(0.8548) | Loss 4.5373(4.4608) | Error 0.3646(0.3053) Steps 838(823.07) | Grad Norm 23.6008(7.9140) | Total Time 14.00(14.00)\n",
      "Iter 1503 | Time 78.8022(78.2923) | Bit/dim 4.0067(4.0326) | Xent 0.8441(0.8544) | Loss 4.4288(4.4598) | Error 0.3043(0.3053) Steps 808(822.62) | Grad Norm 8.8993(7.9435) | Total Time 14.00(14.00)\n",
      "Iter 1504 | Time 74.0701(78.1656) | Bit/dim 4.0098(4.0319) | Xent 0.7986(0.8528) | Loss 4.4091(4.4583) | Error 0.2890(0.3048) Steps 814(822.36) | Grad Norm 6.0821(7.8877) | Total Time 14.00(14.00)\n",
      "Iter 1505 | Time 76.5556(78.1173) | Bit/dim 4.0120(4.0313) | Xent 0.8454(0.8525) | Loss 4.4347(4.4576) | Error 0.3043(0.3048) Steps 820(822.29) | Grad Norm 12.8670(8.0371) | Total Time 14.00(14.00)\n",
      "Iter 1506 | Time 74.8553(78.0195) | Bit/dim 4.0291(4.0313) | Xent 0.8620(0.8528) | Loss 4.4601(4.4577) | Error 0.3133(0.3050) Steps 814(822.04) | Grad Norm 13.6270(8.2048) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 41.2157, Epoch Time 566.6514(503.7925), Bit/dim 4.0283(best: inf), Xent 1.3144, Loss 4.6855, Error 0.4418(best: inf)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1507 | Time 84.1211(78.2025) | Bit/dim 4.0235(4.0310) | Xent 0.8570(0.8530) | Loss 4.4520(4.4575) | Error 0.3066(0.3051) Steps 826(822.16) | Grad Norm 12.3092(8.3279) | Total Time 14.00(14.00)\n",
      "Iter 1508 | Time 76.9728(78.1656) | Bit/dim 4.0225(4.0308) | Xent 0.8631(0.8533) | Loss 4.4540(4.4574) | Error 0.3169(0.3054) Steps 832(822.46) | Grad Norm 10.4824(8.3925) | Total Time 14.00(14.00)\n",
      "Iter 1509 | Time 74.8913(78.0674) | Bit/dim 4.0215(4.0305) | Xent 0.8164(0.8522) | Loss 4.4297(4.4566) | Error 0.2950(0.3051) Steps 814(822.20) | Grad Norm 7.8267(8.3756) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 75.2105(77.9817) | Bit/dim 4.0157(4.0301) | Xent 0.8248(0.8513) | Loss 4.4281(4.4557) | Error 0.2965(0.3048) Steps 820(822.14) | Grad Norm 5.2380(8.2814) | Total Time 14.00(14.00)\n",
      "Iter 1511 | Time 75.5355(77.9083) | Bit/dim 4.0035(4.0293) | Xent 0.7952(0.8496) | Loss 4.4011(4.4541) | Error 0.2887(0.3044) Steps 826(822.25) | Grad Norm 4.1527(8.1576) | Total Time 14.00(14.00)\n",
      "Iter 1512 | Time 79.2767(77.9494) | Bit/dim 4.0067(4.0286) | Xent 0.7756(0.8474) | Loss 4.3945(4.4523) | Error 0.2785(0.3036) Steps 826(822.37) | Grad Norm 5.0117(8.0632) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 29.6691, Epoch Time 511.5409(504.0250), Bit/dim 4.0139(best: 4.0283), Xent 1.2517, Loss 4.6398, Error 0.4204(best: 0.4418)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1513 | Time 76.1889(77.8965) | Bit/dim 4.0043(4.0279) | Xent 0.7595(0.8448) | Loss 4.3841(4.4502) | Error 0.2755(0.3027) Steps 826(822.48) | Grad Norm 6.6607(8.0211) | Total Time 14.00(14.00)\n",
      "Iter 1514 | Time 75.7697(77.8327) | Bit/dim 4.0036(4.0271) | Xent 0.7785(0.8428) | Loss 4.3929(4.4485) | Error 0.2789(0.3020) Steps 826(822.58) | Grad Norm 6.8431(7.9858) | Total Time 14.00(14.00)\n",
      "Iter 1515 | Time 78.5052(77.8529) | Bit/dim 4.0185(4.0269) | Xent 0.7570(0.8402) | Loss 4.3970(4.4470) | Error 0.2731(0.3012) Steps 832(822.86) | Grad Norm 5.8761(7.9225) | Total Time 14.00(14.00)\n",
      "Iter 1516 | Time 79.2735(77.8955) | Bit/dim 4.0079(4.0263) | Xent 0.7706(0.8381) | Loss 4.3932(4.4454) | Error 0.2782(0.3005) Steps 814(822.60) | Grad Norm 5.0670(7.8368) | Total Time 14.00(14.00)\n",
      "Iter 1517 | Time 76.5466(77.8551) | Bit/dim 4.0167(4.0260) | Xent 0.7506(0.8355) | Loss 4.3920(4.4438) | Error 0.2721(0.2996) Steps 832(822.88) | Grad Norm 4.1551(7.7264) | Total Time 14.00(14.00)\n",
      "Iter 1518 | Time 76.9104(77.8267) | Bit/dim 4.0247(4.0260) | Xent 0.7234(0.8321) | Loss 4.3865(4.4420) | Error 0.2536(0.2982) Steps 826(822.97) | Grad Norm 3.0488(7.5860) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 29.7815, Epoch Time 508.6339(504.1633), Bit/dim 4.0151(best: 4.0139), Xent 1.2806, Loss 4.6554, Error 0.4147(best: 0.4204)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1519 | Time 74.3210(77.7215) | Bit/dim 4.0182(4.0257) | Xent 0.7278(0.8290) | Loss 4.3821(4.4402) | Error 0.2608(0.2971) Steps 832(823.24) | Grad Norm 2.6318(7.4374) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 80.0485(77.7914) | Bit/dim 3.9920(4.0247) | Xent 0.7336(0.8262) | Loss 4.3588(4.4378) | Error 0.2611(0.2960) Steps 832(823.51) | Grad Norm 3.2260(7.3111) | Total Time 14.00(14.00)\n",
      "Iter 1521 | Time 76.8753(77.7639) | Bit/dim 4.0014(4.0240) | Xent 0.7178(0.8229) | Loss 4.3603(4.4355) | Error 0.2544(0.2948) Steps 838(823.94) | Grad Norm 3.9879(7.2114) | Total Time 14.00(14.00)\n",
      "Iter 1522 | Time 79.8312(77.8259) | Bit/dim 4.0171(4.0238) | Xent 0.7224(0.8199) | Loss 4.3783(4.4338) | Error 0.2605(0.2938) Steps 826(824.00) | Grad Norm 3.9478(7.1135) | Total Time 14.00(14.00)\n",
      "Iter 1523 | Time 75.3912(77.7528) | Bit/dim 4.0094(4.0234) | Xent 0.7401(0.8175) | Loss 4.3795(4.4321) | Error 0.2620(0.2928) Steps 832(824.24) | Grad Norm 3.1986(6.9960) | Total Time 14.00(14.00)\n",
      "Iter 1524 | Time 73.0797(77.6127) | Bit/dim 4.0185(4.0232) | Xent 0.7383(0.8151) | Loss 4.3877(4.4308) | Error 0.2652(0.2920) Steps 814(823.94) | Grad Norm 2.9064(6.8733) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 29.2887, Epoch Time 504.5529(504.1749), Bit/dim 4.0108(best: 4.0139), Xent 1.2690, Loss 4.6454, Error 0.4180(best: 0.4147)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1525 | Time 77.1245(77.5980) | Bit/dim 4.0007(4.0226) | Xent 0.7109(0.8120) | Loss 4.3562(4.4286) | Error 0.2608(0.2910) Steps 820(823.82) | Grad Norm 2.4608(6.7410) | Total Time 14.00(14.00)\n",
      "Iter 1526 | Time 74.1323(77.4940) | Bit/dim 4.0140(4.0223) | Xent 0.7293(0.8095) | Loss 4.3787(4.4271) | Error 0.2606(0.2901) Steps 826(823.88) | Grad Norm 2.3840(6.6103) | Total Time 14.00(14.00)\n",
      "Iter 1527 | Time 76.9434(77.4775) | Bit/dim 4.0126(4.0220) | Xent 0.7133(0.8066) | Loss 4.3692(4.4253) | Error 0.2600(0.2892) Steps 832(824.13) | Grad Norm 2.1993(6.4779) | Total Time 14.00(14.00)\n",
      "Iter 1528 | Time 78.7131(77.5146) | Bit/dim 4.0112(4.0217) | Xent 0.7244(0.8042) | Loss 4.3734(4.4238) | Error 0.2602(0.2884) Steps 826(824.18) | Grad Norm 2.1457(6.3480) | Total Time 14.00(14.00)\n",
      "Iter 1529 | Time 76.4458(77.4825) | Bit/dim 3.9959(4.0209) | Xent 0.7059(0.8012) | Loss 4.3488(4.4215) | Error 0.2480(0.2871) Steps 832(824.42) | Grad Norm 2.6589(6.2373) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 77.8122(77.4924) | Bit/dim 3.9998(4.0203) | Xent 0.7178(0.7987) | Loss 4.3587(4.4196) | Error 0.2604(0.2863) Steps 826(824.47) | Grad Norm 2.8545(6.1358) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 29.5687, Epoch Time 506.3997(504.2417), Bit/dim 4.0111(best: 4.0108), Xent 1.2647, Loss 4.6435, Error 0.4123(best: 0.4147)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1531 | Time 74.8187(77.4122) | Bit/dim 4.0167(4.0202) | Xent 0.6941(0.7956) | Loss 4.3638(4.4180) | Error 0.2511(0.2853) Steps 820(824.33) | Grad Norm 1.9606(6.0105) | Total Time 14.00(14.00)\n",
      "Iter 1532 | Time 76.8829(77.3963) | Bit/dim 4.0178(4.0201) | Xent 0.6932(0.7925) | Loss 4.3644(4.4164) | Error 0.2452(0.2841) Steps 814(824.02) | Grad Norm 1.8892(5.8869) | Total Time 14.00(14.00)\n",
      "Iter 1533 | Time 75.2090(77.3307) | Bit/dim 4.0065(4.0197) | Xent 0.6951(0.7896) | Loss 4.3541(4.4145) | Error 0.2510(0.2831) Steps 826(824.08) | Grad Norm 1.4445(5.7536) | Total Time 14.00(14.00)\n",
      "Iter 1534 | Time 76.8105(77.3151) | Bit/dim 4.0009(4.0191) | Xent 0.6854(0.7865) | Loss 4.3436(4.4124) | Error 0.2435(0.2819) Steps 820(823.96) | Grad Norm 1.8272(5.6358) | Total Time 14.00(14.00)\n",
      "Iter 1535 | Time 74.5297(77.2315) | Bit/dim 3.9990(4.0185) | Xent 0.7182(0.7844) | Loss 4.3581(4.4107) | Error 0.2588(0.2812) Steps 814(823.66) | Grad Norm 1.7916(5.5205) | Total Time 14.00(14.00)\n",
      "Iter 1536 | Time 75.9973(77.1945) | Bit/dim 3.9973(4.0179) | Xent 0.6934(0.7817) | Loss 4.3440(4.4087) | Error 0.2508(0.2803) Steps 814(823.37) | Grad Norm 2.1019(5.4180) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 29.4057, Epoch Time 499.2488(504.0919), Bit/dim 4.0104(best: 4.0108), Xent 1.2689, Loss 4.6448, Error 0.4102(best: 0.4123)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1537 | Time 78.3455(77.2290) | Bit/dim 4.0155(4.0178) | Xent 0.7008(0.7793) | Loss 4.3659(4.4074) | Error 0.2506(0.2794) Steps 814(823.09) | Grad Norm 2.1288(5.3193) | Total Time 14.00(14.00)\n",
      "Iter 1538 | Time 75.6575(77.1819) | Bit/dim 4.0053(4.0174) | Xent 0.6733(0.7761) | Loss 4.3420(4.4055) | Error 0.2415(0.2783) Steps 820(823.00) | Grad Norm 2.1000(5.2227) | Total Time 14.00(14.00)\n",
      "Iter 1539 | Time 76.4045(77.1586) | Bit/dim 4.0069(4.0171) | Xent 0.6879(0.7734) | Loss 4.3508(4.4038) | Error 0.2489(0.2774) Steps 820(822.91) | Grad Norm 1.8202(5.1206) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 74.6244(77.0825) | Bit/dim 4.0046(4.0167) | Xent 0.6942(0.7711) | Loss 4.3517(4.4023) | Error 0.2548(0.2767) Steps 814(822.64) | Grad Norm 1.5334(5.0130) | Total Time 14.00(14.00)\n",
      "Iter 1541 | Time 73.7431(76.9824) | Bit/dim 3.9982(4.0162) | Xent 0.6946(0.7688) | Loss 4.3455(4.4006) | Error 0.2466(0.2758) Steps 820(822.56) | Grad Norm 1.4406(4.9058) | Total Time 14.00(14.00)\n",
      "Iter 1542 | Time 72.8817(76.8593) | Bit/dim 4.0073(4.0159) | Xent 0.6834(0.7662) | Loss 4.3490(4.3990) | Error 0.2445(0.2749) Steps 826(822.66) | Grad Norm 1.1267(4.7925) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 29.5728, Epoch Time 496.9667(503.8781), Bit/dim 4.0101(best: 4.0104), Xent 1.2806, Loss 4.6504, Error 0.4106(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1543 | Time 73.5122(76.7589) | Bit/dim 4.0071(4.0157) | Xent 0.6873(0.7638) | Loss 4.3508(4.3976) | Error 0.2418(0.2739) Steps 814(822.40) | Grad Norm 1.3952(4.6906) | Total Time 14.00(14.00)\n",
      "Iter 1544 | Time 76.4826(76.7506) | Bit/dim 4.0022(4.0153) | Xent 0.6691(0.7610) | Loss 4.3368(4.3958) | Error 0.2350(0.2727) Steps 814(822.15) | Grad Norm 1.4803(4.5942) | Total Time 14.00(14.00)\n",
      "Iter 1545 | Time 72.0021(76.6082) | Bit/dim 4.0038(4.0149) | Xent 0.6819(0.7586) | Loss 4.3447(4.3942) | Error 0.2379(0.2717) Steps 826(822.27) | Grad Norm 1.6765(4.5067) | Total Time 14.00(14.00)\n",
      "Iter 1546 | Time 74.6976(76.5509) | Bit/dim 4.0077(4.0147) | Xent 0.6823(0.7563) | Loss 4.3489(4.3929) | Error 0.2470(0.2709) Steps 814(822.02) | Grad Norm 2.1764(4.4368) | Total Time 14.00(14.00)\n",
      "Iter 1547 | Time 72.3490(76.4248) | Bit/dim 4.0062(4.0144) | Xent 0.7035(0.7547) | Loss 4.3580(4.3918) | Error 0.2564(0.2705) Steps 820(821.96) | Grad Norm 1.4798(4.3481) | Total Time 14.00(14.00)\n",
      "Iter 1548 | Time 74.6480(76.3715) | Bit/dim 4.0053(4.0142) | Xent 0.6720(0.7523) | Loss 4.3413(4.3903) | Error 0.2408(0.2696) Steps 820(821.90) | Grad Norm 0.8338(4.2427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 29.6577, Epoch Time 489.0881(503.4344), Bit/dim 4.0088(best: 4.0101), Xent 1.2750, Loss 4.6463, Error 0.4113(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1549 | Time 78.2409(76.4276) | Bit/dim 3.9982(4.0137) | Xent 0.6766(0.7500) | Loss 4.3365(4.3887) | Error 0.2434(0.2688) Steps 820(821.84) | Grad Norm 1.2055(4.1515) | Total Time 14.00(14.00)\n",
      "Iter 1550 | Time 74.2049(76.3609) | Bit/dim 4.0037(4.0134) | Xent 0.6928(0.7483) | Loss 4.3501(4.3875) | Error 0.2478(0.2682) Steps 820(821.79) | Grad Norm 1.6962(4.0779) | Total Time 14.00(14.00)\n",
      "Iter 1551 | Time 72.5030(76.2452) | Bit/dim 4.0054(4.0132) | Xent 0.6719(0.7460) | Loss 4.3414(4.3861) | Error 0.2394(0.2673) Steps 814(821.55) | Grad Norm 1.4806(4.0000) | Total Time 14.00(14.00)\n",
      "Iter 1552 | Time 73.2376(76.1549) | Bit/dim 4.0045(4.0129) | Xent 0.6832(0.7441) | Loss 4.3461(4.3849) | Error 0.2466(0.2667) Steps 832(821.87) | Grad Norm 1.5577(3.9267) | Total Time 14.00(14.00)\n",
      "Iter 1553 | Time 77.5291(76.1962) | Bit/dim 4.0050(4.0127) | Xent 0.6956(0.7426) | Loss 4.3528(4.3840) | Error 0.2482(0.2661) Steps 820(821.81) | Grad Norm 1.2255(3.8457) | Total Time 14.00(14.00)\n",
      "Iter 1554 | Time 74.4607(76.1441) | Bit/dim 4.0098(4.0126) | Xent 0.6712(0.7405) | Loss 4.3453(4.3828) | Error 0.2424(0.2654) Steps 814(821.58) | Grad Norm 0.7983(3.7542) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 29.3975, Epoch Time 495.1728(503.1866), Bit/dim 4.0094(best: 4.0088), Xent 1.2892, Loss 4.6540, Error 0.4102(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1555 | Time 74.3511(76.0903) | Bit/dim 4.0050(4.0123) | Xent 0.6840(0.7388) | Loss 4.3470(4.3817) | Error 0.2462(0.2649) Steps 808(821.17) | Grad Norm 1.2917(3.6804) | Total Time 14.00(14.00)\n",
      "Iter 1556 | Time 75.7454(76.0800) | Bit/dim 4.0010(4.0120) | Xent 0.6990(0.7376) | Loss 4.3505(4.3808) | Error 0.2492(0.2644) Steps 820(821.13) | Grad Norm 1.1114(3.6033) | Total Time 14.00(14.00)\n",
      "Iter 1557 | Time 76.6154(76.0960) | Bit/dim 4.0012(4.0117) | Xent 0.6754(0.7357) | Loss 4.3389(4.3796) | Error 0.2434(0.2638) Steps 820(821.10) | Grad Norm 1.3419(3.5355) | Total Time 14.00(14.00)\n",
      "Iter 1558 | Time 76.6204(76.1118) | Bit/dim 4.0082(4.0116) | Xent 0.6619(0.7335) | Loss 4.3392(4.3783) | Error 0.2410(0.2631) Steps 808(820.71) | Grad Norm 1.1023(3.4625) | Total Time 14.00(14.00)\n",
      "Iter 1559 | Time 75.4900(76.0931) | Bit/dim 4.0090(4.0115) | Xent 0.6747(0.7318) | Loss 4.3463(4.3774) | Error 0.2415(0.2624) Steps 820(820.69) | Grad Norm 0.9700(3.3877) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 77.0236(76.1210) | Bit/dim 3.9944(4.0110) | Xent 0.6864(0.7304) | Loss 4.3376(4.3762) | Error 0.2451(0.2619) Steps 808(820.31) | Grad Norm 0.8388(3.3112) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 29.2789, Epoch Time 500.8806(503.1174), Bit/dim 4.0083(best: 4.0088), Xent 1.2837, Loss 4.6502, Error 0.4110(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1561 | Time 74.0221(76.0581) | Bit/dim 4.0055(4.0108) | Xent 0.6932(0.7293) | Loss 4.3521(4.3755) | Error 0.2530(0.2616) Steps 820(820.30) | Grad Norm 1.2881(3.2505) | Total Time 14.00(14.00)\n",
      "Iter 1562 | Time 74.4446(76.0097) | Bit/dim 4.0046(4.0106) | Xent 0.6966(0.7283) | Loss 4.3529(4.3748) | Error 0.2522(0.2614) Steps 826(820.47) | Grad Norm 0.9331(3.1810) | Total Time 14.00(14.00)\n",
      "Iter 1563 | Time 73.5010(75.9344) | Bit/dim 4.0055(4.0105) | Xent 0.6720(0.7266) | Loss 4.3415(4.3738) | Error 0.2444(0.2608) Steps 820(820.45) | Grad Norm 0.7798(3.1090) | Total Time 14.00(14.00)\n",
      "Iter 1564 | Time 73.3931(75.8582) | Bit/dim 4.0092(4.0104) | Xent 0.6672(0.7248) | Loss 4.3427(4.3729) | Error 0.2405(0.2602) Steps 814(820.26) | Grad Norm 1.1216(3.0493) | Total Time 14.00(14.00)\n",
      "Iter 1565 | Time 72.5611(75.7592) | Bit/dim 3.9943(4.0100) | Xent 0.6886(0.7237) | Loss 4.3386(4.3718) | Error 0.2480(0.2599) Steps 814(820.07) | Grad Norm 0.8251(2.9826) | Total Time 14.00(14.00)\n",
      "Iter 1566 | Time 75.7968(75.7604) | Bit/dim 4.0026(4.0097) | Xent 0.6587(0.7218) | Loss 4.3320(4.3706) | Error 0.2359(0.2591) Steps 814(819.89) | Grad Norm 1.1172(2.9267) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 29.3590, Epoch Time 488.5856(502.6815), Bit/dim 4.0081(best: 4.0083), Xent 1.2876, Loss 4.6519, Error 0.4140(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1567 | Time 75.8529(75.7631) | Bit/dim 4.0045(4.0096) | Xent 0.6755(0.7204) | Loss 4.3423(4.3698) | Error 0.2452(0.2587) Steps 814(819.71) | Grad Norm 1.0251(2.8696) | Total Time 14.00(14.00)\n",
      "Iter 1568 | Time 72.2743(75.6585) | Bit/dim 4.0010(4.0093) | Xent 0.6628(0.7187) | Loss 4.3325(4.3687) | Error 0.2434(0.2583) Steps 826(819.90) | Grad Norm 0.6974(2.8044) | Total Time 14.00(14.00)\n",
      "Iter 1569 | Time 75.0174(75.6392) | Bit/dim 4.0139(4.0095) | Xent 0.6670(0.7171) | Loss 4.3474(4.3680) | Error 0.2399(0.2577) Steps 820(819.90) | Grad Norm 1.1510(2.7548) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 76.8778(75.6764) | Bit/dim 3.9948(4.0090) | Xent 0.6781(0.7160) | Loss 4.3339(4.3670) | Error 0.2472(0.2574) Steps 820(819.91) | Grad Norm 0.8796(2.6986) | Total Time 14.00(14.00)\n",
      "Iter 1571 | Time 73.9010(75.6231) | Bit/dim 4.0112(4.0091) | Xent 0.6803(0.7149) | Loss 4.3514(4.3665) | Error 0.2475(0.2571) Steps 826(820.09) | Grad Norm 0.9489(2.6461) | Total Time 14.00(14.00)\n",
      "Iter 1572 | Time 74.8490(75.5999) | Bit/dim 3.9886(4.0085) | Xent 0.6665(0.7134) | Loss 4.3219(4.3652) | Error 0.2386(0.2566) Steps 814(819.91) | Grad Norm 0.7818(2.5902) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 29.1736, Epoch Time 493.7798(502.4144), Bit/dim 4.0083(best: 4.0081), Xent 1.3018, Loss 4.6592, Error 0.4119(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1573 | Time 73.6270(75.5407) | Bit/dim 4.0014(4.0083) | Xent 0.6682(0.7121) | Loss 4.3355(4.3643) | Error 0.2392(0.2560) Steps 820(819.91) | Grad Norm 0.8864(2.5390) | Total Time 14.00(14.00)\n",
      "Iter 1574 | Time 72.7147(75.4559) | Bit/dim 4.0139(4.0084) | Xent 0.6616(0.7106) | Loss 4.3447(4.3637) | Error 0.2356(0.2554) Steps 808(819.55) | Grad Norm 1.0452(2.4942) | Total Time 14.00(14.00)\n",
      "Iter 1575 | Time 76.6558(75.4919) | Bit/dim 4.0002(4.0082) | Xent 0.6503(0.7088) | Loss 4.3254(4.3626) | Error 0.2371(0.2549) Steps 820(819.57) | Grad Norm 0.6963(2.4403) | Total Time 14.00(14.00)\n",
      "Iter 1576 | Time 74.6068(75.4654) | Bit/dim 3.9995(4.0079) | Xent 0.6678(0.7075) | Loss 4.3334(4.3617) | Error 0.2401(0.2544) Steps 814(819.40) | Grad Norm 0.7975(2.3910) | Total Time 14.00(14.00)\n",
      "Iter 1577 | Time 77.2654(75.5194) | Bit/dim 4.0050(4.0078) | Xent 0.6681(0.7063) | Loss 4.3391(4.3610) | Error 0.2414(0.2540) Steps 820(819.42) | Grad Norm 0.9069(2.3465) | Total Time 14.00(14.00)\n",
      "Iter 1578 | Time 77.5708(75.5809) | Bit/dim 4.0025(4.0077) | Xent 0.6761(0.7054) | Loss 4.3406(4.3604) | Error 0.2442(0.2537) Steps 820(819.43) | Grad Norm 0.7729(2.2993) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 29.2024, Epoch Time 497.2923(502.2607), Bit/dim 4.0078(best: 4.0081), Xent 1.2970, Loss 4.6563, Error 0.4131(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1579 | Time 75.6596(75.5833) | Bit/dim 4.0124(4.0078) | Xent 0.6484(0.7037) | Loss 4.3366(4.3597) | Error 0.2310(0.2531) Steps 808(819.09) | Grad Norm 1.0340(2.2613) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 74.8267(75.5606) | Bit/dim 3.9902(4.0073) | Xent 0.6610(0.7024) | Loss 4.3207(4.3585) | Error 0.2339(0.2525) Steps 814(818.94) | Grad Norm 0.9173(2.2210) | Total Time 14.00(14.00)\n",
      "Iter 1581 | Time 73.7012(75.5048) | Bit/dim 4.0052(4.0072) | Xent 0.6582(0.7011) | Loss 4.3343(4.3578) | Error 0.2368(0.2520) Steps 820(818.97) | Grad Norm 0.7898(2.1781) | Total Time 14.00(14.00)\n",
      "Iter 1582 | Time 76.3072(75.5289) | Bit/dim 4.0052(4.0072) | Xent 0.6839(0.7006) | Loss 4.3472(4.3575) | Error 0.2478(0.2519) Steps 814(818.82) | Grad Norm 1.2159(2.1492) | Total Time 14.00(14.00)\n",
      "Iter 1583 | Time 76.0966(75.5459) | Bit/dim 4.0091(4.0072) | Xent 0.6677(0.6996) | Loss 4.3429(4.3570) | Error 0.2389(0.2515) Steps 814(818.68) | Grad Norm 1.0025(2.1148) | Total Time 14.00(14.00)\n",
      "Iter 1584 | Time 76.3515(75.5701) | Bit/dim 3.9921(4.0068) | Xent 0.6739(0.6988) | Loss 4.3290(4.3562) | Error 0.2400(0.2512) Steps 814(818.54) | Grad Norm 0.8690(2.0774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 29.5189, Epoch Time 497.9837(502.1324), Bit/dim 4.0077(best: 4.0078), Xent 1.2981, Loss 4.6567, Error 0.4137(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1585 | Time 77.6381(75.6321) | Bit/dim 3.9984(4.0065) | Xent 0.6690(0.6979) | Loss 4.3329(4.3555) | Error 0.2408(0.2508) Steps 808(818.22) | Grad Norm 1.0755(2.0474) | Total Time 14.00(14.00)\n",
      "Iter 1586 | Time 77.3581(75.6839) | Bit/dim 3.9993(4.0063) | Xent 0.6668(0.6970) | Loss 4.3327(4.3548) | Error 0.2364(0.2504) Steps 814(818.09) | Grad Norm 0.7112(2.0073) | Total Time 14.00(14.00)\n",
      "Iter 1587 | Time 76.0572(75.6951) | Bit/dim 4.0020(4.0062) | Xent 0.6820(0.6966) | Loss 4.3430(4.3545) | Error 0.2441(0.2502) Steps 814(817.97) | Grad Norm 0.7412(1.9693) | Total Time 14.00(14.00)\n",
      "Iter 1588 | Time 74.3680(75.6553) | Bit/dim 4.0045(4.0061) | Xent 0.6485(0.6951) | Loss 4.3288(4.3537) | Error 0.2355(0.2498) Steps 820(818.03) | Grad Norm 0.7805(1.9336) | Total Time 14.00(14.00)\n",
      "Iter 1589 | Time 75.5257(75.6514) | Bit/dim 4.0124(4.0063) | Xent 0.6594(0.6940) | Loss 4.3421(4.3533) | Error 0.2376(0.2494) Steps 814(817.91) | Grad Norm 0.9269(1.9034) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 75.0210(75.6325) | Bit/dim 3.9982(4.0061) | Xent 0.6631(0.6931) | Loss 4.3298(4.3526) | Error 0.2386(0.2491) Steps 814(817.79) | Grad Norm 0.8318(1.8713) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 29.9287, Epoch Time 501.8717(502.1246), Bit/dim 4.0067(best: 4.0077), Xent 1.2912, Loss 4.6523, Error 0.4102(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1591 | Time 74.1828(75.5890) | Bit/dim 4.0050(4.0060) | Xent 0.6527(0.6919) | Loss 4.3314(4.3520) | Error 0.2325(0.2486) Steps 826(818.04) | Grad Norm 0.7727(1.8383) | Total Time 14.00(14.00)\n",
      "Iter 1592 | Time 74.4352(75.5544) | Bit/dim 3.9967(4.0058) | Xent 0.6715(0.6913) | Loss 4.3325(4.3514) | Error 0.2381(0.2483) Steps 814(817.92) | Grad Norm 0.8964(1.8101) | Total Time 14.00(14.00)\n",
      "Iter 1593 | Time 75.7742(75.5610) | Bit/dim 4.0008(4.0056) | Xent 0.6585(0.6903) | Loss 4.3300(4.3508) | Error 0.2356(0.2479) Steps 826(818.16) | Grad Norm 0.9357(1.7838) | Total Time 14.00(14.00)\n",
      "Iter 1594 | Time 76.9606(75.6030) | Bit/dim 4.0034(4.0055) | Xent 0.6533(0.6892) | Loss 4.3300(4.3501) | Error 0.2339(0.2475) Steps 808(817.86) | Grad Norm 0.8969(1.7572) | Total Time 14.00(14.00)\n",
      "Iter 1595 | Time 76.9779(75.6442) | Bit/dim 4.0094(4.0057) | Xent 0.6694(0.6886) | Loss 4.3441(4.3500) | Error 0.2444(0.2474) Steps 820(817.92) | Grad Norm 0.8996(1.7315) | Total Time 14.00(14.00)\n",
      "Iter 1596 | Time 75.9064(75.6521) | Bit/dim 3.9958(4.0054) | Xent 0.6633(0.6878) | Loss 4.3274(4.3493) | Error 0.2318(0.2469) Steps 820(817.98) | Grad Norm 1.0454(1.7109) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 29.3083, Epoch Time 499.5429(502.0472), Bit/dim 4.0073(best: 4.0067), Xent 1.3000, Loss 4.6573, Error 0.4101(best: 0.4102)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1597 | Time 74.6878(75.6232) | Bit/dim 3.9950(4.0051) | Xent 0.6749(0.6875) | Loss 4.3325(4.3488) | Error 0.2400(0.2467) Steps 820(818.04) | Grad Norm 1.1222(1.6933) | Total Time 14.00(14.00)\n",
      "Iter 1598 | Time 73.7419(75.5667) | Bit/dim 4.0065(4.0051) | Xent 0.6488(0.6863) | Loss 4.3309(4.3482) | Error 0.2350(0.2464) Steps 820(818.10) | Grad Norm 0.7456(1.6648) | Total Time 14.00(14.00)\n",
      "Iter 1599 | Time 74.1881(75.5254) | Bit/dim 4.0109(4.0053) | Xent 0.6703(0.6858) | Loss 4.3460(4.3482) | Error 0.2349(0.2460) Steps 814(817.98) | Grad Norm 1.2504(1.6524) | Total Time 14.00(14.00)\n",
      "Iter 1600 | Time 76.2357(75.5467) | Bit/dim 4.0034(4.0052) | Xent 0.6411(0.6845) | Loss 4.3239(4.3474) | Error 0.2321(0.2456) Steps 820(818.04) | Grad Norm 1.0539(1.6344) | Total Time 14.00(14.00)\n",
      "Iter 1601 | Time 75.8391(75.5554) | Bit/dim 3.9962(4.0049) | Xent 0.6577(0.6837) | Loss 4.3251(4.3468) | Error 0.2356(0.2453) Steps 826(818.28) | Grad Norm 0.8052(1.6096) | Total Time 14.00(14.00)\n",
      "Iter 1602 | Time 74.6477(75.5282) | Bit/dim 4.0036(4.0049) | Xent 0.6707(0.6833) | Loss 4.3390(4.3465) | Error 0.2394(0.2451) Steps 814(818.15) | Grad Norm 1.0876(1.5939) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 29.5733, Epoch Time 494.7058(501.8269), Bit/dim 4.0065(best: 4.0067), Xent 1.3068, Loss 4.6599, Error 0.4135(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1603 | Time 74.3498(75.4929) | Bit/dim 3.9952(4.0046) | Xent 0.6651(0.6827) | Loss 4.3277(4.3460) | Error 0.2338(0.2448) Steps 814(818.03) | Grad Norm 1.3709(1.5872) | Total Time 14.00(14.00)\n",
      "Iter 1604 | Time 79.6134(75.6165) | Bit/dim 4.0093(4.0048) | Xent 0.6592(0.6820) | Loss 4.3389(4.3458) | Error 0.2349(0.2445) Steps 808(817.72) | Grad Norm 1.4352(1.5827) | Total Time 14.00(14.00)\n",
      "Iter 1605 | Time 73.4916(75.5527) | Bit/dim 4.0024(4.0047) | Xent 0.6572(0.6813) | Loss 4.3310(4.3453) | Error 0.2400(0.2443) Steps 826(817.97) | Grad Norm 1.9576(1.5939) | Total Time 14.00(14.00)\n",
      "Iter 1606 | Time 75.5921(75.5539) | Bit/dim 4.0016(4.0046) | Xent 0.6823(0.6813) | Loss 4.3427(4.3452) | Error 0.2435(0.2443) Steps 814(817.85) | Grad Norm 0.8734(1.5723) | Total Time 14.00(14.00)\n",
      "Iter 1607 | Time 72.9464(75.4757) | Bit/dim 3.9951(4.0043) | Xent 0.6759(0.6812) | Loss 4.3330(4.3449) | Error 0.2435(0.2443) Steps 802(817.38) | Grad Norm 1.1668(1.5601) | Total Time 14.00(14.00)\n",
      "Iter 1608 | Time 75.5305(75.4773) | Bit/dim 4.0015(4.0042) | Xent 0.6335(0.6797) | Loss 4.3182(4.3441) | Error 0.2306(0.2439) Steps 808(817.10) | Grad Norm 1.2658(1.5513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 29.9735, Epoch Time 497.2120(501.6885), Bit/dim 4.0056(best: 4.0065), Xent 1.3008, Loss 4.6560, Error 0.4126(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1609 | Time 75.9805(75.4924) | Bit/dim 4.0115(4.0044) | Xent 0.6495(0.6788) | Loss 4.3362(4.3438) | Error 0.2395(0.2438) Steps 814(817.00) | Grad Norm 0.8420(1.5300) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 74.5598(75.4644) | Bit/dim 3.9909(4.0040) | Xent 0.6611(0.6783) | Loss 4.3215(4.3432) | Error 0.2404(0.2437) Steps 832(817.45) | Grad Norm 1.3070(1.5233) | Total Time 14.00(14.00)\n",
      "Iter 1611 | Time 78.7086(75.5618) | Bit/dim 4.0035(4.0040) | Xent 0.6753(0.6782) | Loss 4.3411(4.3431) | Error 0.2411(0.2436) Steps 820(817.53) | Grad Norm 1.1086(1.5109) | Total Time 14.00(14.00)\n",
      "Iter 1612 | Time 75.3538(75.5555) | Bit/dim 4.0033(4.0040) | Xent 0.6567(0.6776) | Loss 4.3316(4.3428) | Error 0.2389(0.2434) Steps 826(817.78) | Grad Norm 1.2292(1.5024) | Total Time 14.00(14.00)\n",
      "Iter 1613 | Time 75.5651(75.5558) | Bit/dim 4.0023(4.0039) | Xent 0.6626(0.6771) | Loss 4.3336(4.3425) | Error 0.2392(0.2433) Steps 814(817.67) | Grad Norm 1.0568(1.4891) | Total Time 14.00(14.00)\n",
      "Iter 1614 | Time 72.2448(75.4565) | Bit/dim 4.0003(4.0038) | Xent 0.6616(0.6766) | Loss 4.3311(4.3421) | Error 0.2351(0.2431) Steps 808(817.38) | Grad Norm 0.9663(1.4734) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 29.5844, Epoch Time 497.6332(501.5668), Bit/dim 4.0058(best: 4.0056), Xent 1.3059, Loss 4.6587, Error 0.4102(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1615 | Time 75.9679(75.4718) | Bit/dim 4.0046(4.0039) | Xent 0.6563(0.6760) | Loss 4.3328(4.3419) | Error 0.2361(0.2429) Steps 814(817.28) | Grad Norm 0.8870(1.4558) | Total Time 14.00(14.00)\n",
      "Iter 1616 | Time 76.2642(75.4956) | Bit/dim 3.9972(4.0037) | Xent 0.6582(0.6755) | Loss 4.3263(4.3414) | Error 0.2360(0.2427) Steps 814(817.18) | Grad Norm 1.0957(1.4450) | Total Time 14.00(14.00)\n",
      "Iter 1617 | Time 74.5659(75.4677) | Bit/dim 3.9973(4.0035) | Xent 0.6559(0.6749) | Loss 4.3252(4.3409) | Error 0.2304(0.2423) Steps 814(817.09) | Grad Norm 0.7449(1.4240) | Total Time 14.00(14.00)\n",
      "Iter 1618 | Time 72.6585(75.3834) | Bit/dim 4.0024(4.0034) | Xent 0.6466(0.6741) | Loss 4.3257(4.3405) | Error 0.2338(0.2420) Steps 814(816.99) | Grad Norm 0.9790(1.4106) | Total Time 14.00(14.00)\n",
      "Iter 1619 | Time 76.1167(75.4054) | Bit/dim 4.0090(4.0036) | Xent 0.6437(0.6731) | Loss 4.3309(4.3402) | Error 0.2278(0.2416) Steps 820(817.08) | Grad Norm 0.6894(1.3890) | Total Time 14.00(14.00)\n",
      "Iter 1620 | Time 75.2807(75.4017) | Bit/dim 3.9976(4.0034) | Xent 0.6577(0.6727) | Loss 4.3265(4.3398) | Error 0.2374(0.2415) Steps 808(816.81) | Grad Norm 1.1376(1.3815) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 29.4131, Epoch Time 496.0852(501.4024), Bit/dim 4.0063(best: 4.0056), Xent 1.3102, Loss 4.6614, Error 0.4126(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1621 | Time 74.5345(75.3757) | Bit/dim 4.0090(4.0036) | Xent 0.6678(0.6725) | Loss 4.3429(4.3399) | Error 0.2381(0.2414) Steps 820(816.91) | Grad Norm 0.8965(1.3669) | Total Time 14.00(14.00)\n",
      "Iter 1622 | Time 75.7573(75.3871) | Bit/dim 3.9965(4.0034) | Xent 0.6544(0.6720) | Loss 4.3237(4.3394) | Error 0.2368(0.2412) Steps 820(817.00) | Grad Norm 0.9580(1.3546) | Total Time 14.00(14.00)\n",
      "Iter 1623 | Time 75.0803(75.3779) | Bit/dim 3.9998(4.0033) | Xent 0.6564(0.6715) | Loss 4.3280(4.3390) | Error 0.2331(0.2410) Steps 814(816.91) | Grad Norm 0.8702(1.3401) | Total Time 14.00(14.00)\n",
      "Iter 1624 | Time 74.3384(75.3467) | Bit/dim 4.0031(4.0033) | Xent 0.6542(0.6710) | Loss 4.3302(4.3388) | Error 0.2351(0.2408) Steps 814(816.82) | Grad Norm 0.9665(1.3289) | Total Time 14.00(14.00)\n",
      "Iter 1625 | Time 74.5626(75.3232) | Bit/dim 3.9973(4.0031) | Xent 0.6675(0.6709) | Loss 4.3311(4.3385) | Error 0.2320(0.2405) Steps 826(817.10) | Grad Norm 1.0005(1.3191) | Total Time 14.00(14.00)\n",
      "Iter 1626 | Time 74.2854(75.2921) | Bit/dim 3.9969(4.0029) | Xent 0.6502(0.6703) | Loss 4.3220(4.3380) | Error 0.2292(0.2402) Steps 820(817.18) | Grad Norm 0.8762(1.3058) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 29.6331, Epoch Time 494.1006(501.1833), Bit/dim 4.0059(best: 4.0056), Xent 1.3110, Loss 4.6614, Error 0.4143(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1627 | Time 75.2443(75.2906) | Bit/dim 4.0033(4.0029) | Xent 0.6497(0.6697) | Loss 4.3281(4.3377) | Error 0.2272(0.2398) Steps 820(817.27) | Grad Norm 0.8732(1.2928) | Total Time 14.00(14.00)\n",
      "Iter 1628 | Time 77.8478(75.3674) | Bit/dim 4.0063(4.0030) | Xent 0.6632(0.6695) | Loss 4.3379(4.3377) | Error 0.2389(0.2398) Steps 814(817.17) | Grad Norm 1.2985(1.2930) | Total Time 14.00(14.00)\n",
      "Iter 1629 | Time 74.6407(75.3456) | Bit/dim 3.9949(4.0028) | Xent 0.6483(0.6688) | Loss 4.3190(4.3372) | Error 0.2354(0.2397) Steps 826(817.44) | Grad Norm 0.8875(1.2808) | Total Time 14.00(14.00)\n",
      "Iter 1630 | Time 73.4862(75.2898) | Bit/dim 3.9993(4.0027) | Xent 0.6585(0.6685) | Loss 4.3286(4.3369) | Error 0.2358(0.2395) Steps 814(817.33) | Grad Norm 1.0270(1.2732) | Total Time 14.00(14.00)\n",
      "Iter 1631 | Time 77.2178(75.3476) | Bit/dim 4.0016(4.0026) | Xent 0.6384(0.6676) | Loss 4.3208(4.3364) | Error 0.2322(0.2393) Steps 826(817.59) | Grad Norm 0.9537(1.2636) | Total Time 14.00(14.00)\n",
      "Iter 1632 | Time 74.4442(75.3205) | Bit/dim 4.0005(4.0026) | Xent 0.6408(0.6668) | Loss 4.3209(4.3360) | Error 0.2299(0.2390) Steps 814(817.48) | Grad Norm 0.9978(1.2556) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 29.2060, Epoch Time 497.8679(501.0839), Bit/dim 4.0060(best: 4.0056), Xent 1.3198, Loss 4.6659, Error 0.4103(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1633 | Time 73.8897(75.2776) | Bit/dim 3.9999(4.0025) | Xent 0.6499(0.6663) | Loss 4.3249(4.3356) | Error 0.2321(0.2388) Steps 814(817.38) | Grad Norm 0.7876(1.2416) | Total Time 14.00(14.00)\n",
      "Iter 1634 | Time 74.3035(75.2484) | Bit/dim 3.9948(4.0023) | Xent 0.6460(0.6657) | Loss 4.3178(4.3351) | Error 0.2332(0.2387) Steps 820(817.46) | Grad Norm 0.9633(1.2332) | Total Time 14.00(14.00)\n",
      "Iter 1635 | Time 75.3666(75.2519) | Bit/dim 3.9989(4.0022) | Xent 0.6473(0.6651) | Loss 4.3225(4.3347) | Error 0.2358(0.2386) Steps 808(817.18) | Grad Norm 0.9520(1.2248) | Total Time 14.00(14.00)\n",
      "Iter 1636 | Time 75.1404(75.2486) | Bit/dim 3.9979(4.0020) | Xent 0.6491(0.6647) | Loss 4.3224(4.3344) | Error 0.2354(0.2385) Steps 820(817.26) | Grad Norm 1.0600(1.2199) | Total Time 14.00(14.00)\n",
      "Iter 1637 | Time 75.0953(75.2440) | Bit/dim 4.0084(4.0022) | Xent 0.6526(0.6643) | Loss 4.3347(4.3344) | Error 0.2381(0.2385) Steps 820(817.34) | Grad Norm 1.5058(1.2284) | Total Time 14.00(14.00)\n",
      "Iter 1638 | Time 74.4670(75.2207) | Bit/dim 4.0005(4.0022) | Xent 0.6413(0.6636) | Loss 4.3211(4.3340) | Error 0.2252(0.2381) Steps 820(817.42) | Grad Norm 0.8440(1.2169) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 28.9838, Epoch Time 492.7571(500.8341), Bit/dim 4.0051(best: 4.0056), Xent 1.3102, Loss 4.6602, Error 0.4114(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1639 | Time 72.1420(75.1283) | Bit/dim 4.0007(4.0021) | Xent 0.6682(0.6637) | Loss 4.3348(4.3340) | Error 0.2365(0.2380) Steps 826(817.68) | Grad Norm 0.8227(1.2051) | Total Time 14.00(14.00)\n",
      "Iter 1640 | Time 72.8431(75.0597) | Bit/dim 4.0031(4.0022) | Xent 0.6380(0.6630) | Loss 4.3221(4.3336) | Error 0.2279(0.2377) Steps 814(817.57) | Grad Norm 1.0313(1.1999) | Total Time 14.00(14.00)\n",
      "Iter 1641 | Time 73.7835(75.0215) | Bit/dim 3.9999(4.0021) | Xent 0.6511(0.6626) | Loss 4.3254(4.3334) | Error 0.2318(0.2375) Steps 826(817.82) | Grad Norm 1.5351(1.2099) | Total Time 14.00(14.00)\n",
      "Iter 1642 | Time 76.9889(75.0805) | Bit/dim 3.9997(4.0020) | Xent 0.6412(0.6620) | Loss 4.3203(4.3330) | Error 0.2292(0.2373) Steps 832(818.25) | Grad Norm 1.1626(1.2085) | Total Time 14.00(14.00)\n",
      "Iter 1643 | Time 77.6245(75.1568) | Bit/dim 3.9917(4.0017) | Xent 0.6561(0.6618) | Loss 4.3197(4.3326) | Error 0.2380(0.2373) Steps 814(818.12) | Grad Norm 0.8972(1.1992) | Total Time 14.00(14.00)\n",
      "Iter 1644 | Time 75.9520(75.1807) | Bit/dim 4.0003(4.0017) | Xent 0.6383(0.6611) | Loss 4.3195(4.3322) | Error 0.2296(0.2371) Steps 826(818.36) | Grad Norm 1.0017(1.1932) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 29.5408, Epoch Time 494.4005(500.6411), Bit/dim 4.0059(best: 4.0051), Xent 1.3134, Loss 4.6626, Error 0.4133(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1645 | Time 73.8477(75.1407) | Bit/dim 3.9945(4.0014) | Xent 0.6516(0.6608) | Loss 4.3203(4.3319) | Error 0.2294(0.2369) Steps 820(818.41) | Grad Norm 0.9297(1.1853) | Total Time 14.00(14.00)\n",
      "Iter 1646 | Time 75.2906(75.1452) | Bit/dim 3.9988(4.0014) | Xent 0.6613(0.6608) | Loss 4.3294(4.3318) | Error 0.2409(0.2370) Steps 820(818.45) | Grad Norm 1.0359(1.1808) | Total Time 14.00(14.00)\n",
      "Iter 1647 | Time 73.0918(75.0836) | Bit/dim 4.0060(4.0015) | Xent 0.6417(0.6602) | Loss 4.3268(4.3316) | Error 0.2284(0.2367) Steps 814(818.32) | Grad Norm 0.9051(1.1726) | Total Time 14.00(14.00)\n",
      "Iter 1648 | Time 74.7590(75.0738) | Bit/dim 3.9866(4.0011) | Xent 0.6555(0.6601) | Loss 4.3143(4.3311) | Error 0.2324(0.2366) Steps 814(818.19) | Grad Norm 1.1213(1.1710) | Total Time 14.00(14.00)\n",
      "Iter 1649 | Time 76.5040(75.1167) | Bit/dim 4.0061(4.0012) | Xent 0.6554(0.6600) | Loss 4.3338(4.3312) | Error 0.2362(0.2366) Steps 814(818.06) | Grad Norm 1.4984(1.1809) | Total Time 14.00(14.00)\n",
      "Iter 1650 | Time 73.6449(75.0726) | Bit/dim 4.0058(4.0013) | Xent 0.6432(0.6595) | Loss 4.3274(4.3311) | Error 0.2280(0.2363) Steps 820(818.12) | Grad Norm 1.1430(1.1797) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 29.6840, Epoch Time 492.7161(500.4033), Bit/dim 4.0058(best: 4.0051), Xent 1.3174, Loss 4.6645, Error 0.4131(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1651 | Time 74.0257(75.0412) | Bit/dim 4.0053(4.0015) | Xent 0.6494(0.6592) | Loss 4.3300(4.3310) | Error 0.2340(0.2362) Steps 820(818.18) | Grad Norm 1.1397(1.1785) | Total Time 14.00(14.00)\n",
      "Iter 1652 | Time 74.3845(75.0215) | Bit/dim 4.0016(4.0015) | Xent 0.6361(0.6585) | Loss 4.3196(4.3307) | Error 0.2255(0.2359) Steps 814(818.05) | Grad Norm 1.5422(1.1894) | Total Time 14.00(14.00)\n",
      "Iter 1653 | Time 73.2942(74.9697) | Bit/dim 4.0024(4.0015) | Xent 0.6421(0.6580) | Loss 4.3234(4.3305) | Error 0.2350(0.2359) Steps 832(818.47) | Grad Norm 1.2737(1.1920) | Total Time 14.00(14.00)\n",
      "Iter 1654 | Time 74.9070(74.9678) | Bit/dim 3.9875(4.0011) | Xent 0.6504(0.6577) | Loss 4.3127(4.3300) | Error 0.2324(0.2358) Steps 814(818.34) | Grad Norm 0.8856(1.1828) | Total Time 14.00(14.00)\n",
      "Iter 1655 | Time 76.0915(75.0015) | Bit/dim 4.0036(4.0012) | Xent 0.6497(0.6575) | Loss 4.3285(4.3299) | Error 0.2295(0.2356) Steps 814(818.21) | Grad Norm 1.0028(1.1774) | Total Time 14.00(14.00)\n",
      "Iter 1656 | Time 71.7657(74.9044) | Bit/dim 4.0031(4.0012) | Xent 0.6462(0.6572) | Loss 4.3262(4.3298) | Error 0.2336(0.2355) Steps 814(818.08) | Grad Norm 1.2791(1.1804) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0276 | Time 29.5792, Epoch Time 489.7088(500.0825), Bit/dim 4.0042(best: 4.0051), Xent 1.3171, Loss 4.6628, Error 0.4097(best: 0.4101)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1657 | Time 73.8537(74.8729) | Bit/dim 3.9901(4.0009) | Xent 0.6475(0.6569) | Loss 4.3138(4.3293) | Error 0.2331(0.2355) Steps 814(817.96) | Grad Norm 1.4248(1.1877) | Total Time 14.00(14.00)\n",
      "Iter 1658 | Time 72.1708(74.7918) | Bit/dim 4.0063(4.0010) | Xent 0.6454(0.6565) | Loss 4.3290(4.3293) | Error 0.2321(0.2354) Steps 820(818.02) | Grad Norm 0.9811(1.1815) | Total Time 14.00(14.00)\n",
      "Iter 1659 | Time 77.0296(74.8590) | Bit/dim 4.0038(4.0011) | Xent 0.6356(0.6559) | Loss 4.3216(4.3291) | Error 0.2275(0.2351) Steps 826(818.26) | Grad Norm 0.9523(1.1747) | Total Time 14.00(14.00)\n",
      "Iter 1660 | Time 74.9669(74.8622) | Bit/dim 4.0043(4.0012) | Xent 0.6496(0.6557) | Loss 4.3291(4.3291) | Error 0.2312(0.2350) Steps 802(817.77) | Grad Norm 0.9226(1.1671) | Total Time 14.00(14.00)\n",
      "Iter 1661 | Time 74.6868(74.8569) | Bit/dim 3.9827(4.0007) | Xent 0.6555(0.6557) | Loss 4.3104(4.3285) | Error 0.2381(0.2351) Steps 814(817.66) | Grad Norm 1.2112(1.1684) | Total Time 14.00(14.00)\n",
      "Iter 1662 | Time 77.0705(74.9233) | Bit/dim 4.0053(4.0008) | Xent 0.6617(0.6559) | Loss 4.3362(4.3287) | Error 0.2348(0.2351) Steps 814(817.55) | Grad Norm 1.1916(1.1691) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0277 | Time 29.2463, Epoch Time 494.7394(499.9222), Bit/dim 4.0036(best: 4.0042), Xent 1.3118, Loss 4.6595, Error 0.4109(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1663 | Time 73.7370(74.8878) | Bit/dim 4.0043(4.0009) | Xent 0.6465(0.6556) | Loss 4.3275(4.3287) | Error 0.2252(0.2348) Steps 802(817.08) | Grad Norm 0.9222(1.1617) | Total Time 14.00(14.00)\n",
      "Iter 1664 | Time 75.1913(74.8969) | Bit/dim 4.0000(4.0009) | Xent 0.6395(0.6551) | Loss 4.3198(4.3284) | Error 0.2282(0.2346) Steps 820(817.17) | Grad Norm 1.1484(1.1613) | Total Time 14.00(14.00)\n",
      "Iter 1665 | Time 74.8385(74.8951) | Bit/dim 3.9918(4.0006) | Xent 0.6525(0.6550) | Loss 4.3181(4.3281) | Error 0.2381(0.2347) Steps 820(817.25) | Grad Norm 0.8818(1.1529) | Total Time 14.00(14.00)\n",
      "Iter 1666 | Time 74.6388(74.8874) | Bit/dim 4.0002(4.0006) | Xent 0.6426(0.6547) | Loss 4.3214(4.3279) | Error 0.2309(0.2346) Steps 820(817.34) | Grad Norm 1.4190(1.1609) | Total Time 14.00(14.00)\n",
      "Iter 1667 | Time 77.1456(74.9552) | Bit/dim 3.9947(4.0004) | Xent 0.6478(0.6545) | Loss 4.3186(4.3277) | Error 0.2381(0.2347) Steps 838(817.96) | Grad Norm 1.1041(1.1592) | Total Time 14.00(14.00)\n",
      "Iter 1668 | Time 74.7814(74.9500) | Bit/dim 4.0025(4.0005) | Xent 0.6446(0.6542) | Loss 4.3248(4.3276) | Error 0.2325(0.2346) Steps 814(817.84) | Grad Norm 1.3574(1.1652) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0278 | Time 29.3456, Epoch Time 495.7464(499.7969), Bit/dim 4.0040(best: 4.0036), Xent 1.3265, Loss 4.6673, Error 0.4103(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1669 | Time 74.9342(74.9495) | Bit/dim 3.9991(4.0004) | Xent 0.6427(0.6538) | Loss 4.3204(4.3273) | Error 0.2281(0.2344) Steps 820(817.90) | Grad Norm 0.9718(1.1594) | Total Time 14.00(14.00)\n",
      "Iter 1670 | Time 74.3181(74.9305) | Bit/dim 3.9983(4.0004) | Xent 0.6554(0.6539) | Loss 4.3260(4.3273) | Error 0.2312(0.2343) Steps 820(817.97) | Grad Norm 1.0939(1.1574) | Total Time 14.00(14.00)\n",
      "Iter 1671 | Time 73.3699(74.8837) | Bit/dim 3.9971(4.0003) | Xent 0.6455(0.6536) | Loss 4.3199(4.3271) | Error 0.2292(0.2342) Steps 808(817.67) | Grad Norm 0.9796(1.1521) | Total Time 14.00(14.00)\n",
      "Iter 1672 | Time 72.0034(74.7973) | Bit/dim 4.0024(4.0003) | Xent 0.6281(0.6529) | Loss 4.3164(4.3268) | Error 0.2315(0.2341) Steps 814(817.56) | Grad Norm 1.1297(1.1514) | Total Time 14.00(14.00)\n",
      "Iter 1673 | Time 72.7610(74.7362) | Bit/dim 3.9914(4.0001) | Xent 0.6210(0.6519) | Loss 4.3018(4.3260) | Error 0.2202(0.2337) Steps 820(817.63) | Grad Norm 0.9891(1.1465) | Total Time 14.00(14.00)\n",
      "Iter 1674 | Time 75.8721(74.7703) | Bit/dim 4.0004(4.0001) | Xent 0.6360(0.6514) | Loss 4.3184(4.3258) | Error 0.2311(0.2336) Steps 820(817.70) | Grad Norm 1.1199(1.1457) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0279 | Time 29.3542, Epoch Time 488.3166(499.4525), Bit/dim 4.0034(best: 4.0036), Xent 1.3161, Loss 4.6614, Error 0.4125(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1675 | Time 77.0004(74.8372) | Bit/dim 4.0045(4.0002) | Xent 0.6311(0.6508) | Loss 4.3200(4.3256) | Error 0.2329(0.2336) Steps 808(817.41) | Grad Norm 0.7458(1.1337) | Total Time 14.00(14.00)\n",
      "Iter 1676 | Time 75.9608(74.8709) | Bit/dim 3.9861(3.9998) | Xent 0.6242(0.6500) | Loss 4.2982(4.3248) | Error 0.2264(0.2334) Steps 820(817.49) | Grad Norm 0.9697(1.1288) | Total Time 14.00(14.00)\n",
      "Iter 1677 | Time 73.4989(74.8297) | Bit/dim 4.0099(4.0001) | Xent 0.6485(0.6500) | Loss 4.3342(4.3251) | Error 0.2329(0.2334) Steps 814(817.38) | Grad Norm 1.0806(1.1274) | Total Time 14.00(14.00)\n",
      "Iter 1678 | Time 75.0542(74.8365) | Bit/dim 3.9996(4.0001) | Xent 0.6437(0.6498) | Loss 4.3214(4.3250) | Error 0.2314(0.2333) Steps 814(817.28) | Grad Norm 1.0467(1.1249) | Total Time 14.00(14.00)\n",
      "Iter 1679 | Time 74.6577(74.8311) | Bit/dim 3.9947(3.9999) | Xent 0.6639(0.6502) | Loss 4.3266(4.3250) | Error 0.2384(0.2335) Steps 814(817.18) | Grad Norm 0.9332(1.1192) | Total Time 14.00(14.00)\n",
      "Iter 1680 | Time 76.6912(74.8869) | Bit/dim 3.9925(3.9997) | Xent 0.6487(0.6502) | Loss 4.3169(4.3248) | Error 0.2322(0.2334) Steps 826(817.45) | Grad Norm 1.0166(1.1161) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0280 | Time 29.2236, Epoch Time 497.5195(499.3945), Bit/dim 4.0032(best: 4.0034), Xent 1.3298, Loss 4.6681, Error 0.4138(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1681 | Time 76.3555(74.9310) | Bit/dim 3.9962(3.9996) | Xent 0.6581(0.6504) | Loss 4.3253(4.3248) | Error 0.2350(0.2335) Steps 820(817.52) | Grad Norm 1.0835(1.1151) | Total Time 14.00(14.00)\n",
      "Iter 1682 | Time 75.0429(74.9343) | Bit/dim 3.9876(3.9992) | Xent 0.6298(0.6498) | Loss 4.3025(4.3241) | Error 0.2288(0.2333) Steps 814(817.42) | Grad Norm 1.2900(1.1204) | Total Time 14.00(14.00)\n",
      "Iter 1683 | Time 75.7037(74.9574) | Bit/dim 4.0082(3.9995) | Xent 0.6431(0.6496) | Loss 4.3298(4.3243) | Error 0.2279(0.2332) Steps 814(817.32) | Grad Norm 0.9625(1.1156) | Total Time 14.00(14.00)\n",
      "Iter 1684 | Time 75.3117(74.9680) | Bit/dim 3.9925(3.9993) | Xent 0.6364(0.6492) | Loss 4.3107(4.3239) | Error 0.2229(0.2329) Steps 820(817.40) | Grad Norm 0.8027(1.1063) | Total Time 14.00(14.00)\n",
      "Iter 1685 | Time 76.5828(75.0165) | Bit/dim 3.9956(3.9992) | Xent 0.6322(0.6487) | Loss 4.3117(4.3235) | Error 0.2309(0.2328) Steps 826(817.65) | Grad Norm 1.0902(1.1058) | Total Time 14.00(14.00)\n",
      "Iter 1686 | Time 75.8137(75.0404) | Bit/dim 4.0057(3.9994) | Xent 0.6326(0.6482) | Loss 4.3221(4.3235) | Error 0.2304(0.2327) Steps 820(817.73) | Grad Norm 0.9303(1.1005) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0281 | Time 29.2067, Epoch Time 499.8167(499.4072), Bit/dim 4.0026(best: 4.0032), Xent 1.3268, Loss 4.6660, Error 0.4134(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1687 | Time 76.7758(75.0925) | Bit/dim 3.9931(3.9992) | Xent 0.6444(0.6481) | Loss 4.3153(4.3232) | Error 0.2349(0.2328) Steps 826(817.97) | Grad Norm 0.8577(1.0932) | Total Time 14.00(14.00)\n",
      "Iter 1688 | Time 74.9727(75.0889) | Bit/dim 3.9944(3.9990) | Xent 0.6343(0.6477) | Loss 4.3116(4.3229) | Error 0.2301(0.2327) Steps 808(817.67) | Grad Norm 0.9144(1.0879) | Total Time 14.00(14.00)\n",
      "Iter 1689 | Time 77.4332(75.1592) | Bit/dim 4.0047(3.9992) | Xent 0.6382(0.6474) | Loss 4.3238(4.3229) | Error 0.2245(0.2325) Steps 814(817.56) | Grad Norm 0.8523(1.0808) | Total Time 14.00(14.00)\n",
      "Iter 1690 | Time 76.9506(75.2129) | Bit/dim 4.0059(3.9994) | Xent 0.6423(0.6472) | Loss 4.3270(4.3230) | Error 0.2290(0.2324) Steps 820(817.64) | Grad Norm 0.8799(1.0748) | Total Time 14.00(14.00)\n",
      "Iter 1691 | Time 76.2065(75.2427) | Bit/dim 3.9997(3.9994) | Xent 0.6308(0.6467) | Loss 4.3151(4.3228) | Error 0.2230(0.2321) Steps 808(817.35) | Grad Norm 0.9685(1.0716) | Total Time 14.00(14.00)\n",
      "Iter 1692 | Time 75.5601(75.2523) | Bit/dim 3.9878(3.9991) | Xent 0.6531(0.6469) | Loss 4.3143(4.3225) | Error 0.2344(0.2321) Steps 814(817.25) | Grad Norm 0.8417(1.0647) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0282 | Time 29.2439, Epoch Time 503.2304(499.5219), Bit/dim 4.0027(best: 4.0026), Xent 1.3254, Loss 4.6654, Error 0.4123(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1693 | Time 77.7506(75.3272) | Bit/dim 4.0090(3.9994) | Xent 0.6231(0.6462) | Loss 4.3206(4.3225) | Error 0.2211(0.2318) Steps 832(817.69) | Grad Norm 0.7414(1.0550) | Total Time 14.00(14.00)\n",
      "Iter 1694 | Time 75.0526(75.3190) | Bit/dim 3.9904(3.9991) | Xent 0.6526(0.6464) | Loss 4.3168(4.3223) | Error 0.2319(0.2318) Steps 820(817.76) | Grad Norm 0.8834(1.0498) | Total Time 14.00(14.00)\n",
      "Iter 1695 | Time 78.3311(75.4093) | Bit/dim 3.9915(3.9989) | Xent 0.6403(0.6462) | Loss 4.3117(4.3220) | Error 0.2299(0.2318) Steps 820(817.83) | Grad Norm 0.9793(1.0477) | Total Time 14.00(14.00)\n",
      "Iter 1696 | Time 73.5125(75.3524) | Bit/dim 4.0025(3.9990) | Xent 0.6342(0.6459) | Loss 4.3196(4.3219) | Error 0.2252(0.2316) Steps 820(817.89) | Grad Norm 0.8715(1.0424) | Total Time 14.00(14.00)\n",
      "Iter 1697 | Time 73.6819(75.3023) | Bit/dim 3.9968(3.9989) | Xent 0.6297(0.6454) | Loss 4.3117(4.3216) | Error 0.2294(0.2315) Steps 820(817.96) | Grad Norm 1.0530(1.0427) | Total Time 14.00(14.00)\n",
      "Iter 1698 | Time 72.7755(75.2265) | Bit/dim 3.9952(3.9988) | Xent 0.6268(0.6448) | Loss 4.3086(4.3212) | Error 0.2271(0.2314) Steps 826(818.20) | Grad Norm 1.6515(1.0610) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0283 | Time 29.5728, Epoch Time 496.2019(499.4223), Bit/dim 4.0030(best: 4.0026), Xent 1.3392, Loss 4.6726, Error 0.4122(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1699 | Time 75.3623(75.2306) | Bit/dim 4.0020(3.9989) | Xent 0.6447(0.6448) | Loss 4.3243(4.3213) | Error 0.2302(0.2313) Steps 820(818.25) | Grad Norm 1.0960(1.0621) | Total Time 14.00(14.00)\n",
      "Iter 1700 | Time 74.2331(75.2007) | Bit/dim 3.9956(3.9988) | Xent 0.6418(0.6447) | Loss 4.3165(4.3212) | Error 0.2276(0.2312) Steps 808(817.94) | Grad Norm 1.1180(1.0637) | Total Time 14.00(14.00)\n",
      "Iter 1701 | Time 75.7566(75.2173) | Bit/dim 4.0054(3.9990) | Xent 0.6130(0.6438) | Loss 4.3119(4.3209) | Error 0.2252(0.2310) Steps 820(818.00) | Grad Norm 0.8694(1.0579) | Total Time 14.00(14.00)\n",
      "Iter 1702 | Time 77.6193(75.2894) | Bit/dim 3.9872(3.9986) | Xent 0.6538(0.6441) | Loss 4.3142(4.3207) | Error 0.2372(0.2312) Steps 808(817.70) | Grad Norm 1.0856(1.0587) | Total Time 14.00(14.00)\n",
      "Iter 1703 | Time 75.0252(75.2815) | Bit/dim 3.9958(3.9986) | Xent 0.6313(0.6437) | Loss 4.3114(4.3204) | Error 0.2284(0.2311) Steps 814(817.59) | Grad Norm 0.7405(1.0492) | Total Time 14.00(14.00)\n",
      "Iter 1704 | Time 74.8058(75.2672) | Bit/dim 3.9977(3.9985) | Xent 0.6161(0.6429) | Loss 4.3057(4.3200) | Error 0.2185(0.2308) Steps 814(817.49) | Grad Norm 1.2336(1.0547) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0284 | Time 29.8064, Epoch Time 498.4282(499.3924), Bit/dim 4.0025(best: 4.0026), Xent 1.3274, Loss 4.6662, Error 0.4093(best: 0.4097)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1705 | Time 73.9978(75.2291) | Bit/dim 4.0002(3.9986) | Xent 0.6317(0.6425) | Loss 4.3161(4.3199) | Error 0.2271(0.2307) Steps 820(817.56) | Grad Norm 0.8031(1.0472) | Total Time 14.00(14.00)\n",
      "Iter 1706 | Time 74.7461(75.2146) | Bit/dim 4.0096(3.9989) | Xent 0.6294(0.6421) | Loss 4.3243(4.3200) | Error 0.2272(0.2306) Steps 820(817.63) | Grad Norm 0.8929(1.0425) | Total Time 14.00(14.00)\n",
      "Iter 1707 | Time 74.2270(75.1850) | Bit/dim 3.9903(3.9987) | Xent 0.6279(0.6417) | Loss 4.3042(4.3195) | Error 0.2261(0.2304) Steps 820(817.71) | Grad Norm 0.9562(1.0400) | Total Time 14.00(14.00)\n",
      "Iter 1708 | Time 74.4215(75.1621) | Bit/dim 3.9905(3.9984) | Xent 0.6399(0.6417) | Loss 4.3104(4.3192) | Error 0.2210(0.2301) Steps 808(817.41) | Grad Norm 1.2294(1.0456) | Total Time 14.00(14.00)\n",
      "Iter 1709 | Time 74.7955(75.1511) | Bit/dim 3.9958(3.9983) | Xent 0.6121(0.6408) | Loss 4.3018(4.3187) | Error 0.2226(0.2299) Steps 826(817.67) | Grad Norm 1.0995(1.0473) | Total Time 14.00(14.00)\n",
      "Iter 1710 | Time 76.4397(75.1898) | Bit/dim 3.9932(3.9982) | Xent 0.6355(0.6406) | Loss 4.3109(4.3185) | Error 0.2339(0.2300) Steps 826(817.92) | Grad Norm 1.0343(1.0469) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0285 | Time 29.5789, Epoch Time 493.6871(499.2213), Bit/dim 4.0018(best: 4.0025), Xent 1.3445, Loss 4.6740, Error 0.4170(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1711 | Time 75.1329(75.1881) | Bit/dim 3.9975(3.9982) | Xent 0.6252(0.6401) | Loss 4.3101(4.3182) | Error 0.2246(0.2299) Steps 814(817.80) | Grad Norm 1.0484(1.0469) | Total Time 14.00(14.00)\n",
      "Iter 1712 | Time 73.4117(75.1348) | Bit/dim 3.9891(3.9979) | Xent 0.6207(0.6396) | Loss 4.2995(4.3177) | Error 0.2299(0.2299) Steps 826(818.05) | Grad Norm 1.2702(1.0536) | Total Time 14.00(14.00)\n",
      "Iter 1713 | Time 75.9400(75.1589) | Bit/dim 4.0015(3.9980) | Xent 0.6394(0.6396) | Loss 4.3212(4.3178) | Error 0.2286(0.2298) Steps 820(818.11) | Grad Norm 2.0133(1.0824) | Total Time 14.00(14.00)\n",
      "Iter 1714 | Time 72.8942(75.0910) | Bit/dim 3.9888(3.9977) | Xent 0.6498(0.6399) | Loss 4.3137(4.3176) | Error 0.2348(0.2300) Steps 820(818.16) | Grad Norm 1.0164(1.0804) | Total Time 14.00(14.00)\n",
      "Iter 1715 | Time 73.9512(75.0568) | Bit/dim 4.0009(3.9978) | Xent 0.6284(0.6395) | Loss 4.3151(4.3176) | Error 0.2280(0.2299) Steps 820(818.22) | Grad Norm 1.0204(1.0786) | Total Time 14.00(14.00)\n",
      "Iter 1716 | Time 74.9128(75.0525) | Bit/dim 3.9988(3.9978) | Xent 0.6259(0.6391) | Loss 4.3118(4.3174) | Error 0.2278(0.2299) Steps 814(818.09) | Grad Norm 1.0384(1.0774) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0286 | Time 29.5708, Epoch Time 491.5722(498.9918), Bit/dim 4.0025(best: 4.0018), Xent 1.3303, Loss 4.6676, Error 0.4138(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1717 | Time 72.6003(74.9789) | Bit/dim 3.9949(3.9978) | Xent 0.6333(0.6389) | Loss 4.3115(4.3172) | Error 0.2259(0.2297) Steps 826(818.33) | Grad Norm 1.2913(1.0838) | Total Time 14.00(14.00)\n",
      "Iter 1718 | Time 74.4165(74.9620) | Bit/dim 3.9907(3.9975) | Xent 0.6529(0.6394) | Loss 4.3172(4.3172) | Error 0.2350(0.2299) Steps 820(818.38) | Grad Norm 1.1841(1.0868) | Total Time 14.00(14.00)\n",
      "Iter 1719 | Time 76.5640(75.0101) | Bit/dim 3.9945(3.9975) | Xent 0.6632(0.6401) | Loss 4.3261(4.3175) | Error 0.2379(0.2301) Steps 820(818.43) | Grad Norm 0.9213(1.0819) | Total Time 14.00(14.00)\n",
      "Iter 1720 | Time 75.7016(75.0308) | Bit/dim 3.9934(3.9973) | Xent 0.6209(0.6395) | Loss 4.3039(4.3171) | Error 0.2270(0.2300) Steps 820(818.48) | Grad Norm 1.2956(1.0883) | Total Time 14.00(14.00)\n",
      "Iter 1721 | Time 73.9829(74.9994) | Bit/dim 4.0028(3.9975) | Xent 0.6110(0.6386) | Loss 4.3083(4.3168) | Error 0.2175(0.2297) Steps 814(818.34) | Grad Norm 1.1188(1.0892) | Total Time 14.00(14.00)\n",
      "Iter 1722 | Time 74.7978(74.9933) | Bit/dim 3.9978(3.9975) | Xent 0.6195(0.6381) | Loss 4.3076(4.3165) | Error 0.2238(0.2295) Steps 808(818.03) | Grad Norm 0.8250(1.0813) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0287 | Time 29.0233, Epoch Time 492.7478(498.8045), Bit/dim 4.0015(best: 4.0018), Xent 1.3340, Loss 4.6685, Error 0.4136(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1723 | Time 77.2684(75.0616) | Bit/dim 3.9985(3.9975) | Xent 0.6108(0.6372) | Loss 4.3039(4.3162) | Error 0.2134(0.2290) Steps 808(817.73) | Grad Norm 0.7183(1.0704) | Total Time 14.00(14.00)\n",
      "Iter 1724 | Time 75.7038(75.0809) | Bit/dim 4.0021(3.9977) | Xent 0.6349(0.6372) | Loss 4.3196(4.3163) | Error 0.2329(0.2291) Steps 832(818.16) | Grad Norm 1.2238(1.0750) | Total Time 14.00(14.00)\n",
      "Iter 1725 | Time 76.7265(75.1302) | Bit/dim 3.9996(3.9977) | Xent 0.6145(0.6365) | Loss 4.3069(4.3160) | Error 0.2252(0.2290) Steps 814(818.03) | Grad Norm 1.0066(1.0729) | Total Time 14.00(14.00)\n",
      "Iter 1726 | Time 73.4533(75.0799) | Bit/dim 3.9966(3.9977) | Xent 0.6355(0.6365) | Loss 4.3143(4.3159) | Error 0.2274(0.2290) Steps 802(817.55) | Grad Norm 1.0128(1.0711) | Total Time 14.00(14.00)\n",
      "Iter 1727 | Time 74.3726(75.0587) | Bit/dim 3.9904(3.9975) | Xent 0.6200(0.6360) | Loss 4.3004(4.3155) | Error 0.2261(0.2289) Steps 832(817.99) | Grad Norm 1.1346(1.0730) | Total Time 14.00(14.00)\n",
      "Iter 1728 | Time 73.2134(75.0033) | Bit/dim 3.9930(3.9973) | Xent 0.6320(0.6358) | Loss 4.3090(4.3153) | Error 0.2216(0.2287) Steps 820(818.05) | Grad Norm 1.2720(1.0790) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0288 | Time 29.3480, Epoch Time 495.4946(498.7052), Bit/dim 4.0018(best: 4.0015), Xent 1.3410, Loss 4.6723, Error 0.4111(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1729 | Time 76.5585(75.0500) | Bit/dim 3.9996(3.9974) | Xent 0.6262(0.6356) | Loss 4.3127(4.3152) | Error 0.2272(0.2286) Steps 814(817.93) | Grad Norm 1.2440(1.0840) | Total Time 14.00(14.00)\n",
      "Iter 1730 | Time 75.6753(75.0688) | Bit/dim 4.0008(3.9975) | Xent 0.6108(0.6348) | Loss 4.3062(4.3149) | Error 0.2204(0.2284) Steps 820(817.99) | Grad Norm 0.8204(1.0761) | Total Time 14.00(14.00)\n",
      "Iter 1731 | Time 77.7460(75.1491) | Bit/dim 3.9985(3.9975) | Xent 0.6227(0.6345) | Loss 4.3099(4.3148) | Error 0.2234(0.2282) Steps 826(818.23) | Grad Norm 1.5024(1.0888) | Total Time 14.00(14.00)\n",
      "Iter 1732 | Time 76.0798(75.1770) | Bit/dim 3.9881(3.9973) | Xent 0.6264(0.6342) | Loss 4.3013(4.3144) | Error 0.2234(0.2281) Steps 832(818.64) | Grad Norm 1.5452(1.1025) | Total Time 14.00(14.00)\n",
      "Iter 1733 | Time 74.3848(75.1532) | Bit/dim 3.9942(3.9972) | Xent 0.6162(0.6337) | Loss 4.3024(4.3140) | Error 0.2201(0.2278) Steps 814(818.50) | Grad Norm 1.4297(1.1123) | Total Time 14.00(14.00)\n",
      "Iter 1734 | Time 72.1991(75.0646) | Bit/dim 3.9977(3.9972) | Xent 0.6518(0.6342) | Loss 4.3236(4.3143) | Error 0.2354(0.2281) Steps 814(818.37) | Grad Norm 0.9035(1.1061) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0289 | Time 29.2878, Epoch Time 497.3873(498.6657), Bit/dim 4.0016(best: 4.0015), Xent 1.3381, Loss 4.6707, Error 0.4136(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1735 | Time 72.3673(74.9837) | Bit/dim 3.9918(3.9970) | Xent 0.6371(0.6343) | Loss 4.3104(4.3142) | Error 0.2272(0.2280) Steps 820(818.42) | Grad Norm 1.3202(1.1125) | Total Time 14.00(14.00)\n",
      "Iter 1736 | Time 75.7858(75.0077) | Bit/dim 3.9994(3.9971) | Xent 0.6273(0.6341) | Loss 4.3130(4.3141) | Error 0.2250(0.2279) Steps 808(818.10) | Grad Norm 1.9177(1.1367) | Total Time 14.00(14.00)\n",
      "Iter 1737 | Time 75.5177(75.0230) | Bit/dim 3.9965(3.9971) | Xent 0.6169(0.6336) | Loss 4.3050(4.3139) | Error 0.2271(0.2279) Steps 820(818.16) | Grad Norm 1.4710(1.1467) | Total Time 14.00(14.00)\n",
      "Iter 1738 | Time 73.7116(74.9837) | Bit/dim 3.9942(3.9970) | Xent 0.6320(0.6335) | Loss 4.3102(4.3138) | Error 0.2309(0.2280) Steps 814(818.04) | Grad Norm 1.2406(1.1495) | Total Time 14.00(14.00)\n",
      "Iter 1739 | Time 73.6648(74.9441) | Bit/dim 3.9932(3.9969) | Xent 0.6350(0.6336) | Loss 4.3108(4.3137) | Error 0.2256(0.2279) Steps 802(817.55) | Grad Norm 1.1705(1.1501) | Total Time 14.00(14.00)\n",
      "Iter 1740 | Time 75.1990(74.9518) | Bit/dim 3.9999(3.9970) | Xent 0.6242(0.6333) | Loss 4.3120(4.3136) | Error 0.2259(0.2279) Steps 820(817.63) | Grad Norm 1.1467(1.1500) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0290 | Time 29.7678, Epoch Time 491.7592(498.4585), Bit/dim 4.0010(best: 4.0015), Xent 1.3436, Loss 4.6728, Error 0.4129(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1741 | Time 75.6655(74.9732) | Bit/dim 3.9949(3.9969) | Xent 0.6232(0.6330) | Loss 4.3065(4.3134) | Error 0.2226(0.2277) Steps 814(817.52) | Grad Norm 1.2048(1.1517) | Total Time 14.00(14.00)\n",
      "Iter 1742 | Time 74.9621(74.9729) | Bit/dim 4.0004(3.9970) | Xent 0.6188(0.6326) | Loss 4.3098(4.3133) | Error 0.2186(0.2274) Steps 814(817.41) | Grad Norm 1.0122(1.1475) | Total Time 14.00(14.00)\n",
      "Iter 1743 | Time 75.0320(74.9746) | Bit/dim 3.9959(3.9970) | Xent 0.6177(0.6321) | Loss 4.3047(4.3130) | Error 0.2228(0.2273) Steps 826(817.67) | Grad Norm 0.9162(1.1406) | Total Time 14.00(14.00)\n",
      "Iter 1744 | Time 74.9095(74.9727) | Bit/dim 3.9957(3.9969) | Xent 0.6160(0.6316) | Loss 4.3037(4.3128) | Error 0.2174(0.2270) Steps 832(818.10) | Grad Norm 0.9636(1.1352) | Total Time 14.00(14.00)\n",
      "Iter 1745 | Time 76.9622(75.0324) | Bit/dim 3.9938(3.9968) | Xent 0.6393(0.6319) | Loss 4.3134(4.3128) | Error 0.2312(0.2271) Steps 826(818.34) | Grad Norm 1.2123(1.1376) | Total Time 14.00(14.00)\n",
      "Iter 1746 | Time 74.5079(75.0166) | Bit/dim 3.9915(3.9967) | Xent 0.6404(0.6321) | Loss 4.3116(4.3127) | Error 0.2270(0.2271) Steps 814(818.21) | Grad Norm 0.8034(1.1275) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0291 | Time 29.2202, Epoch Time 496.7078(498.4059), Bit/dim 4.0010(best: 4.0010), Xent 1.3525, Loss 4.6772, Error 0.4134(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1747 | Time 74.7525(75.0087) | Bit/dim 4.0004(3.9968) | Xent 0.6235(0.6319) | Loss 4.3121(4.3127) | Error 0.2232(0.2270) Steps 814(818.08) | Grad Norm 1.0609(1.1255) | Total Time 14.00(14.00)\n",
      "Iter 1748 | Time 73.8787(74.9748) | Bit/dim 3.9952(3.9967) | Xent 0.6214(0.6315) | Loss 4.3059(4.3125) | Error 0.2251(0.2270) Steps 814(817.96) | Grad Norm 0.9171(1.1193) | Total Time 14.00(14.00)\n",
      "Iter 1749 | Time 77.1449(75.0399) | Bit/dim 3.9882(3.9965) | Xent 0.6233(0.6313) | Loss 4.2999(4.3121) | Error 0.2205(0.2268) Steps 814(817.84) | Grad Norm 0.8760(1.1120) | Total Time 14.00(14.00)\n",
      "Iter 1750 | Time 73.7594(75.0015) | Bit/dim 4.0067(3.9968) | Xent 0.6354(0.6314) | Loss 4.3244(4.3125) | Error 0.2315(0.2269) Steps 826(818.09) | Grad Norm 1.0988(1.1116) | Total Time 14.00(14.00)\n",
      "Iter 1751 | Time 76.0909(75.0342) | Bit/dim 3.9856(3.9965) | Xent 0.6127(0.6309) | Loss 4.2920(4.3119) | Error 0.2192(0.2267) Steps 820(818.14) | Grad Norm 1.5436(1.1246) | Total Time 14.00(14.00)\n",
      "Iter 1752 | Time 74.5445(75.0195) | Bit/dim 3.9931(3.9964) | Xent 0.6351(0.6310) | Loss 4.3106(4.3119) | Error 0.2252(0.2266) Steps 814(818.02) | Grad Norm 1.3160(1.1303) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0292 | Time 29.7535, Epoch Time 495.6058(498.3219), Bit/dim 4.0011(best: 4.0010), Xent 1.3501, Loss 4.6761, Error 0.4144(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1753 | Time 74.8376(75.0140) | Bit/dim 3.9975(3.9964) | Xent 0.6214(0.6307) | Loss 4.3082(4.3117) | Error 0.2220(0.2265) Steps 808(817.72) | Grad Norm 0.9352(1.1244) | Total Time 14.00(14.00)\n",
      "Iter 1754 | Time 76.4761(75.0579) | Bit/dim 3.9961(3.9964) | Xent 0.6431(0.6311) | Loss 4.3177(4.3119) | Error 0.2332(0.2267) Steps 820(817.79) | Grad Norm 0.8697(1.1168) | Total Time 14.00(14.00)\n",
      "Iter 1755 | Time 73.9638(75.0251) | Bit/dim 3.9996(3.9965) | Xent 0.6287(0.6310) | Loss 4.3139(4.3120) | Error 0.2240(0.2266) Steps 820(817.85) | Grad Norm 1.3513(1.1238) | Total Time 14.00(14.00)\n",
      "Iter 1756 | Time 74.3830(75.0058) | Bit/dim 3.9985(3.9965) | Xent 0.6100(0.6304) | Loss 4.3035(4.3117) | Error 0.2139(0.2262) Steps 808(817.56) | Grad Norm 1.2100(1.1264) | Total Time 14.00(14.00)\n",
      "Iter 1757 | Time 72.8022(74.9397) | Bit/dim 3.9900(3.9963) | Xent 0.6306(0.6304) | Loss 4.3053(4.3115) | Error 0.2266(0.2262) Steps 820(817.63) | Grad Norm 1.0493(1.1241) | Total Time 14.00(14.00)\n",
      "Iter 1758 | Time 76.1381(74.9756) | Bit/dim 3.9844(3.9960) | Xent 0.6143(0.6299) | Loss 4.2916(4.3109) | Error 0.2214(0.2261) Steps 826(817.88) | Grad Norm 1.3973(1.1323) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0293 | Time 29.7709, Epoch Time 493.8458(498.1877), Bit/dim 4.0001(best: 4.0010), Xent 1.3497, Loss 4.6749, Error 0.4125(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1759 | Time 72.2105(74.8927) | Bit/dim 4.0083(3.9964) | Xent 0.6191(0.6296) | Loss 4.3179(4.3111) | Error 0.2260(0.2261) Steps 814(817.76) | Grad Norm 0.8348(1.1234) | Total Time 14.00(14.00)\n",
      "Iter 1760 | Time 74.7116(74.8873) | Bit/dim 3.9891(3.9961) | Xent 0.6290(0.6296) | Loss 4.3036(4.3109) | Error 0.2206(0.2259) Steps 808(817.47) | Grad Norm 1.4597(1.1335) | Total Time 14.00(14.00)\n",
      "Iter 1761 | Time 74.8709(74.8868) | Bit/dim 3.9974(3.9962) | Xent 0.6081(0.6289) | Loss 4.3014(4.3106) | Error 0.2156(0.2256) Steps 820(817.55) | Grad Norm 1.0966(1.1324) | Total Time 14.00(14.00)\n",
      "Iter 1762 | Time 73.8487(74.8556) | Bit/dim 3.9875(3.9959) | Xent 0.6268(0.6288) | Loss 4.3009(4.3103) | Error 0.2214(0.2255) Steps 808(817.26) | Grad Norm 1.3104(1.1377) | Total Time 14.00(14.00)\n",
      "Iter 1763 | Time 76.5466(74.9064) | Bit/dim 3.9924(3.9958) | Xent 0.6174(0.6285) | Loss 4.3011(4.3101) | Error 0.2177(0.2253) Steps 820(817.34) | Grad Norm 1.1539(1.1382) | Total Time 14.00(14.00)\n",
      "Iter 1764 | Time 74.3167(74.8887) | Bit/dim 3.9964(3.9958) | Xent 0.6255(0.6284) | Loss 4.3092(4.3100) | Error 0.2269(0.2253) Steps 820(817.42) | Grad Norm 1.1935(1.1398) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0294 | Time 29.4699, Epoch Time 491.3811(497.9835), Bit/dim 3.9999(best: 4.0001), Xent 1.3550, Loss 4.6774, Error 0.4143(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1765 | Time 76.2697(74.9301) | Bit/dim 3.9917(3.9957) | Xent 0.6170(0.6281) | Loss 4.3002(4.3097) | Error 0.2231(0.2252) Steps 808(817.14) | Grad Norm 1.6298(1.1545) | Total Time 14.00(14.00)\n",
      "Iter 1766 | Time 77.2458(74.9996) | Bit/dim 3.9791(3.9952) | Xent 0.6031(0.6273) | Loss 4.2807(4.3089) | Error 0.2208(0.2251) Steps 820(817.23) | Grad Norm 0.9763(1.1492) | Total Time 14.00(14.00)\n",
      "Iter 1767 | Time 73.2370(74.9467) | Bit/dim 3.9986(3.9953) | Xent 0.6268(0.6273) | Loss 4.3120(4.3090) | Error 0.2240(0.2251) Steps 820(817.31) | Grad Norm 1.0791(1.1471) | Total Time 14.00(14.00)\n",
      "Iter 1768 | Time 75.2343(74.9553) | Bit/dim 4.0029(3.9955) | Xent 0.6307(0.6274) | Loss 4.3182(4.3092) | Error 0.2282(0.2252) Steps 808(817.03) | Grad Norm 1.1245(1.1464) | Total Time 14.00(14.00)\n",
      "Iter 1769 | Time 74.8441(74.9520) | Bit/dim 4.0013(3.9957) | Xent 0.6111(0.6269) | Loss 4.3068(4.3092) | Error 0.2202(0.2250) Steps 826(817.30) | Grad Norm 1.0942(1.1449) | Total Time 14.00(14.00)\n",
      "Iter 1770 | Time 75.5079(74.9687) | Bit/dim 3.9921(3.9956) | Xent 0.6194(0.6267) | Loss 4.3018(4.3089) | Error 0.2206(0.2249) Steps 814(817.20) | Grad Norm 0.9276(1.1383) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0295 | Time 29.5805, Epoch Time 498.0200(497.9846), Bit/dim 3.9999(best: 3.9999), Xent 1.3456, Loss 4.6727, Error 0.4120(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1771 | Time 73.7865(74.9332) | Bit/dim 3.9997(3.9957) | Xent 0.6276(0.6267) | Loss 4.3135(4.3091) | Error 0.2250(0.2249) Steps 820(817.28) | Grad Norm 1.2769(1.1425) | Total Time 14.00(14.00)\n",
      "Iter 1772 | Time 74.6743(74.9254) | Bit/dim 3.9815(3.9953) | Xent 0.6279(0.6268) | Loss 4.2954(4.3087) | Error 0.2260(0.2249) Steps 820(817.37) | Grad Norm 1.1671(1.1432) | Total Time 14.00(14.00)\n",
      "Iter 1773 | Time 76.7587(74.9804) | Bit/dim 3.9917(3.9952) | Xent 0.6060(0.6261) | Loss 4.2946(4.3083) | Error 0.2196(0.2248) Steps 826(817.62) | Grad Norm 0.9798(1.1383) | Total Time 14.00(14.00)\n",
      "Iter 1774 | Time 77.3539(75.0516) | Bit/dim 3.9948(3.9952) | Xent 0.6110(0.6257) | Loss 4.3003(4.3080) | Error 0.2150(0.2245) Steps 838(818.24) | Grad Norm 1.2539(1.1418) | Total Time 14.00(14.00)\n",
      "Iter 1775 | Time 73.8698(75.0162) | Bit/dim 3.9973(3.9952) | Xent 0.6200(0.6255) | Loss 4.3073(4.3080) | Error 0.2229(0.2244) Steps 826(818.47) | Grad Norm 1.6101(1.1558) | Total Time 14.00(14.00)\n",
      "Iter 1776 | Time 76.5505(75.0622) | Bit/dim 3.9938(3.9952) | Xent 0.6061(0.6249) | Loss 4.2968(4.3077) | Error 0.2135(0.2241) Steps 820(818.51) | Grad Norm 1.9494(1.1796) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0296 | Time 29.7413, Epoch Time 498.2247(497.9918), Bit/dim 3.9994(best: 3.9999), Xent 1.3530, Loss 4.6759, Error 0.4127(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1777 | Time 77.2885(75.1290) | Bit/dim 3.9899(3.9950) | Xent 0.6079(0.6244) | Loss 4.2938(4.3072) | Error 0.2173(0.2239) Steps 814(818.38) | Grad Norm 0.8916(1.1710) | Total Time 14.00(14.00)\n",
      "Iter 1778 | Time 75.5580(75.1419) | Bit/dim 4.0015(3.9952) | Xent 0.6152(0.6241) | Loss 4.3091(4.3073) | Error 0.2224(0.2238) Steps 820(818.43) | Grad Norm 1.2964(1.1748) | Total Time 14.00(14.00)\n",
      "Iter 1779 | Time 75.3712(75.1487) | Bit/dim 3.9971(3.9953) | Xent 0.6353(0.6245) | Loss 4.3147(4.3075) | Error 0.2279(0.2240) Steps 820(818.48) | Grad Norm 1.7936(1.1933) | Total Time 14.00(14.00)\n",
      "Iter 1780 | Time 71.9937(75.0541) | Bit/dim 3.9900(3.9951) | Xent 0.6240(0.6245) | Loss 4.3020(4.3074) | Error 0.2242(0.2240) Steps 820(818.52) | Grad Norm 1.5649(1.2045) | Total Time 14.00(14.00)\n",
      "Iter 1781 | Time 75.5087(75.0677) | Bit/dim 3.9897(3.9950) | Xent 0.6212(0.6244) | Loss 4.3003(4.3071) | Error 0.2281(0.2241) Steps 826(818.75) | Grad Norm 1.3050(1.2075) | Total Time 14.00(14.00)\n",
      "Iter 1782 | Time 75.8914(75.0924) | Bit/dim 3.9952(3.9950) | Xent 0.6031(0.6237) | Loss 4.2968(4.3068) | Error 0.2175(0.2239) Steps 814(818.60) | Grad Norm 1.7393(1.2234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0297 | Time 29.3973, Epoch Time 496.6082(497.9502), Bit/dim 3.9994(best: 3.9994), Xent 1.3593, Loss 4.6791, Error 0.4144(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1783 | Time 74.2567(75.0674) | Bit/dim 3.9884(3.9948) | Xent 0.6109(0.6233) | Loss 4.2939(4.3064) | Error 0.2196(0.2238) Steps 826(818.82) | Grad Norm 1.7957(1.2406) | Total Time 14.00(14.00)\n",
      "Iter 1784 | Time 76.1692(75.1004) | Bit/dim 3.9943(3.9948) | Xent 0.6321(0.6236) | Loss 4.3103(4.3066) | Error 0.2289(0.2239) Steps 826(819.04) | Grad Norm 1.2442(1.2407) | Total Time 14.00(14.00)\n",
      "Iter 1785 | Time 75.6345(75.1165) | Bit/dim 3.9900(3.9946) | Xent 0.6122(0.6233) | Loss 4.2961(4.3062) | Error 0.2192(0.2238) Steps 808(818.71) | Grad Norm 1.0022(1.2336) | Total Time 14.00(14.00)\n",
      "Iter 1786 | Time 76.5717(75.1601) | Bit/dim 3.9963(3.9947) | Xent 0.6114(0.6229) | Loss 4.3020(4.3061) | Error 0.2224(0.2237) Steps 814(818.57) | Grad Norm 1.4039(1.2387) | Total Time 14.00(14.00)\n",
      "Iter 1787 | Time 75.8647(75.1812) | Bit/dim 4.0009(3.9949) | Xent 0.6292(0.6231) | Loss 4.3155(4.3064) | Error 0.2245(0.2238) Steps 808(818.25) | Grad Norm 1.7472(1.2539) | Total Time 14.00(14.00)\n",
      "Iter 1788 | Time 74.2462(75.1532) | Bit/dim 3.9906(3.9947) | Xent 0.6126(0.6228) | Loss 4.2969(4.3061) | Error 0.2174(0.2236) Steps 820(818.30) | Grad Norm 1.3856(1.2579) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0298 | Time 29.5947, Epoch Time 498.0730(497.9539), Bit/dim 3.9983(best: 3.9994), Xent 1.3613, Loss 4.6790, Error 0.4119(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1789 | Time 75.8847(75.1751) | Bit/dim 3.9892(3.9946) | Xent 0.6167(0.6226) | Loss 4.2975(4.3059) | Error 0.2196(0.2235) Steps 814(818.17) | Grad Norm 1.0676(1.2522) | Total Time 14.00(14.00)\n",
      "Iter 1790 | Time 75.2077(75.1761) | Bit/dim 3.9987(3.9947) | Xent 0.6254(0.6227) | Loss 4.3114(4.3060) | Error 0.2251(0.2235) Steps 808(817.87) | Grad Norm 1.4162(1.2571) | Total Time 14.00(14.00)\n",
      "Iter 1791 | Time 74.9546(75.1695) | Bit/dim 3.9944(3.9947) | Xent 0.6030(0.6221) | Loss 4.2959(4.3057) | Error 0.2151(0.2233) Steps 820(817.93) | Grad Norm 1.4107(1.2617) | Total Time 14.00(14.00)\n",
      "Iter 1792 | Time 72.9910(75.1041) | Bit/dim 3.9907(3.9946) | Xent 0.5964(0.6213) | Loss 4.2890(4.3052) | Error 0.2125(0.2229) Steps 814(817.81) | Grad Norm 1.2425(1.2611) | Total Time 14.00(14.00)\n",
      "Iter 1793 | Time 77.1748(75.1662) | Bit/dim 3.9807(3.9941) | Xent 0.6228(0.6214) | Loss 4.2921(4.3048) | Error 0.2216(0.2229) Steps 802(817.34) | Grad Norm 1.0375(1.2544) | Total Time 14.00(14.00)\n",
      "Iter 1794 | Time 75.6547(75.1809) | Bit/dim 4.0009(3.9943) | Xent 0.6074(0.6209) | Loss 4.3046(4.3048) | Error 0.2143(0.2226) Steps 808(817.06) | Grad Norm 1.2996(1.2558) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0299 | Time 29.4586, Epoch Time 497.0040(497.9254), Bit/dim 3.9989(best: 3.9983), Xent 1.3562, Loss 4.6770, Error 0.4100(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1795 | Time 73.4223(75.1281) | Bit/dim 3.9931(3.9943) | Xent 0.6197(0.6209) | Loss 4.3030(4.3048) | Error 0.2211(0.2226) Steps 820(817.15) | Grad Norm 1.5383(1.2643) | Total Time 14.00(14.00)\n",
      "Iter 1796 | Time 74.3269(75.1041) | Bit/dim 3.9873(3.9941) | Xent 0.6092(0.6206) | Loss 4.2919(4.3044) | Error 0.2218(0.2226) Steps 808(816.87) | Grad Norm 0.8587(1.2521) | Total Time 14.00(14.00)\n",
      "Iter 1797 | Time 77.3120(75.1703) | Bit/dim 3.9928(3.9941) | Xent 0.6181(0.6205) | Loss 4.3018(4.3043) | Error 0.2249(0.2226) Steps 826(817.15) | Grad Norm 0.7268(1.2363) | Total Time 14.00(14.00)\n",
      "Iter 1798 | Time 73.2086(75.1115) | Bit/dim 4.0018(3.9943) | Xent 0.6202(0.6205) | Loss 4.3120(4.3045) | Error 0.2312(0.2229) Steps 826(817.41) | Grad Norm 1.0808(1.2317) | Total Time 14.00(14.00)\n",
      "Iter 1799 | Time 75.2188(75.1147) | Bit/dim 3.9908(3.9942) | Xent 0.6121(0.6202) | Loss 4.2968(4.3043) | Error 0.2240(0.2229) Steps 814(817.31) | Grad Norm 1.2577(1.2324) | Total Time 14.00(14.00)\n",
      "Iter 1800 | Time 74.0281(75.0821) | Bit/dim 3.9945(3.9942) | Xent 0.6152(0.6201) | Loss 4.3021(4.3042) | Error 0.2224(0.2229) Steps 820(817.39) | Grad Norm 1.1820(1.2309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0300 | Time 29.4061, Epoch Time 492.5593(497.7645), Bit/dim 3.9985(best: 3.9983), Xent 1.3595, Loss 4.6783, Error 0.4127(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1801 | Time 73.9256(75.0474) | Bit/dim 3.9963(3.9943) | Xent 0.6068(0.6197) | Loss 4.2997(4.3041) | Error 0.2180(0.2228) Steps 814(817.29) | Grad Norm 0.8383(1.2191) | Total Time 14.00(14.00)\n",
      "Iter 1802 | Time 74.1426(75.0203) | Bit/dim 3.9882(3.9941) | Xent 0.6195(0.6197) | Loss 4.2979(4.3039) | Error 0.2212(0.2227) Steps 826(817.55) | Grad Norm 1.1925(1.2183) | Total Time 14.00(14.00)\n",
      "Iter 1803 | Time 75.7449(75.0420) | Bit/dim 3.9893(3.9939) | Xent 0.6259(0.6199) | Loss 4.3023(4.3039) | Error 0.2221(0.2227) Steps 820(817.62) | Grad Norm 1.2900(1.2205) | Total Time 14.00(14.00)\n",
      "Iter 1804 | Time 78.0034(75.1308) | Bit/dim 3.9971(3.9940) | Xent 0.6044(0.6194) | Loss 4.2993(4.3037) | Error 0.2188(0.2226) Steps 826(817.88) | Grad Norm 0.9663(1.2129) | Total Time 14.00(14.00)\n",
      "Iter 1805 | Time 77.3303(75.1968) | Bit/dim 4.0056(3.9944) | Xent 0.6041(0.6189) | Loss 4.3077(4.3038) | Error 0.2166(0.2224) Steps 802(817.40) | Grad Norm 1.9280(1.2343) | Total Time 14.00(14.00)\n",
      "Iter 1806 | Time 74.6769(75.1812) | Bit/dim 3.9755(3.9938) | Xent 0.6004(0.6184) | Loss 4.2757(4.3030) | Error 0.2184(0.2223) Steps 814(817.30) | Grad Norm 1.0614(1.2291) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0301 | Time 29.4927, Epoch Time 498.8220(497.7962), Bit/dim 3.9977(best: 3.9983), Xent 1.3549, Loss 4.6751, Error 0.4148(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1807 | Time 73.7105(75.1371) | Bit/dim 3.9974(3.9939) | Xent 0.5950(0.6177) | Loss 4.2949(4.3028) | Error 0.2115(0.2220) Steps 820(817.38) | Grad Norm 1.6854(1.2428) | Total Time 14.00(14.00)\n",
      "Iter 1808 | Time 75.3682(75.1440) | Bit/dim 3.9915(3.9938) | Xent 0.6048(0.6173) | Loss 4.2939(4.3025) | Error 0.2201(0.2219) Steps 814(817.28) | Grad Norm 1.5632(1.2524) | Total Time 14.00(14.00)\n",
      "Iter 1809 | Time 74.7201(75.1313) | Bit/dim 3.9908(3.9938) | Xent 0.6120(0.6171) | Loss 4.2968(4.3023) | Error 0.2208(0.2219) Steps 808(817.00) | Grad Norm 1.0213(1.2455) | Total Time 14.00(14.00)\n",
      "Iter 1810 | Time 77.8719(75.2135) | Bit/dim 3.9943(3.9938) | Xent 0.6016(0.6167) | Loss 4.2951(4.3021) | Error 0.2169(0.2217) Steps 826(817.27) | Grad Norm 0.8962(1.2350) | Total Time 14.00(14.00)\n",
      "Iter 1811 | Time 73.5445(75.1635) | Bit/dim 3.9847(3.9935) | Xent 0.6133(0.6166) | Loss 4.2914(4.3018) | Error 0.2199(0.2217) Steps 814(817.17) | Grad Norm 1.5690(1.2450) | Total Time 14.00(14.00)\n",
      "Iter 1812 | Time 75.8471(75.1840) | Bit/dim 3.9916(3.9934) | Xent 0.6085(0.6163) | Loss 4.2958(4.3016) | Error 0.2184(0.2216) Steps 838(817.80) | Grad Norm 0.7794(1.2311) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0302 | Time 29.9270, Epoch Time 496.4372(497.7554), Bit/dim 3.9977(best: 3.9977), Xent 1.3715, Loss 4.6835, Error 0.4131(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1813 | Time 73.2539(75.1261) | Bit/dim 3.9980(3.9936) | Xent 0.6119(0.6162) | Loss 4.3039(4.3017) | Error 0.2191(0.2215) Steps 808(817.50) | Grad Norm 1.2050(1.2303) | Total Time 14.00(14.00)\n",
      "Iter 1814 | Time 75.6461(75.1417) | Bit/dim 3.9968(3.9937) | Xent 0.5933(0.6155) | Loss 4.2935(4.3014) | Error 0.2140(0.2213) Steps 826(817.76) | Grad Norm 1.2452(1.2307) | Total Time 14.00(14.00)\n",
      "Iter 1815 | Time 76.6738(75.1876) | Bit/dim 3.9940(3.9937) | Xent 0.6095(0.6153) | Loss 4.2988(4.3013) | Error 0.2146(0.2211) Steps 820(817.82) | Grad Norm 1.0725(1.2260) | Total Time 14.00(14.00)\n",
      "Iter 1816 | Time 73.3378(75.1321) | Bit/dim 3.9913(3.9936) | Xent 0.5936(0.6147) | Loss 4.2881(4.3009) | Error 0.2147(0.2209) Steps 814(817.71) | Grad Norm 0.8392(1.2144) | Total Time 14.00(14.00)\n",
      "Iter 1817 | Time 72.0281(75.0390) | Bit/dim 3.9917(3.9936) | Xent 0.6158(0.6147) | Loss 4.2996(4.3009) | Error 0.2254(0.2210) Steps 820(817.78) | Grad Norm 0.9851(1.2075) | Total Time 14.00(14.00)\n",
      "Iter 1818 | Time 75.3009(75.0469) | Bit/dim 3.9879(3.9934) | Xent 0.6210(0.6149) | Loss 4.2984(4.3008) | Error 0.2238(0.2211) Steps 814(817.66) | Grad Norm 1.2437(1.2086) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0303 | Time 29.6962, Epoch Time 491.4978(497.5677), Bit/dim 3.9982(best: 3.9977), Xent 1.3756, Loss 4.6860, Error 0.4136(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1819 | Time 74.8237(75.0402) | Bit/dim 3.9908(3.9933) | Xent 0.6110(0.6148) | Loss 4.2963(4.3007) | Error 0.2188(0.2210) Steps 808(817.37) | Grad Norm 1.3996(1.2143) | Total Time 14.00(14.00)\n",
      "Iter 1820 | Time 77.0200(75.0996) | Bit/dim 4.0058(3.9937) | Xent 0.6071(0.6145) | Loss 4.3094(4.3010) | Error 0.2185(0.2209) Steps 814(817.27) | Grad Norm 0.9348(1.2059) | Total Time 14.00(14.00)\n",
      "Iter 1821 | Time 74.5383(75.0827) | Bit/dim 3.9868(3.9935) | Xent 0.6149(0.6146) | Loss 4.2942(4.3008) | Error 0.2153(0.2208) Steps 808(817.00) | Grad Norm 2.0534(1.2314) | Total Time 14.00(14.00)\n",
      "Iter 1822 | Time 75.0524(75.0818) | Bit/dim 3.9909(3.9934) | Xent 0.5971(0.6140) | Loss 4.2894(4.3004) | Error 0.2131(0.2205) Steps 820(817.09) | Grad Norm 1.4791(1.2388) | Total Time 14.00(14.00)\n",
      "Iter 1823 | Time 75.8000(75.1034) | Bit/dim 3.9951(3.9935) | Xent 0.6089(0.6139) | Loss 4.2996(4.3004) | Error 0.2199(0.2205) Steps 808(816.81) | Grad Norm 0.8849(1.2282) | Total Time 14.00(14.00)\n",
      "Iter 1824 | Time 77.6114(75.1786) | Bit/dim 3.9896(3.9933) | Xent 0.6062(0.6136) | Loss 4.2927(4.3002) | Error 0.2215(0.2206) Steps 826(817.09) | Grad Norm 2.0494(1.2528) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0304 | Time 29.7149, Epoch Time 500.4640(497.6546), Bit/dim 3.9981(best: 3.9977), Xent 1.3662, Loss 4.6812, Error 0.4121(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1825 | Time 74.8146(75.1677) | Bit/dim 3.9937(3.9933) | Xent 0.6041(0.6134) | Loss 4.2958(4.3000) | Error 0.2159(0.2204) Steps 808(816.82) | Grad Norm 1.7391(1.2674) | Total Time 14.00(14.00)\n",
      "Iter 1826 | Time 72.5952(75.0905) | Bit/dim 3.9909(3.9933) | Xent 0.6048(0.6131) | Loss 4.2933(4.2998) | Error 0.2175(0.2203) Steps 820(816.91) | Grad Norm 1.1270(1.2632) | Total Time 14.00(14.00)\n",
      "Iter 1827 | Time 72.1821(75.0033) | Bit/dim 3.9881(3.9931) | Xent 0.6024(0.6128) | Loss 4.2893(4.2995) | Error 0.2160(0.2202) Steps 832(817.36) | Grad Norm 1.7877(1.2789) | Total Time 14.00(14.00)\n",
      "Iter 1828 | Time 75.4065(75.0154) | Bit/dim 3.9920(3.9931) | Xent 0.6104(0.6127) | Loss 4.2972(4.2994) | Error 0.2174(0.2201) Steps 808(817.08) | Grad Norm 2.4177(1.3131) | Total Time 14.00(14.00)\n",
      "Iter 1829 | Time 75.7190(75.0365) | Bit/dim 3.9964(3.9932) | Xent 0.6159(0.6128) | Loss 4.3043(4.2996) | Error 0.2262(0.2203) Steps 814(816.99) | Grad Norm 1.5209(1.3193) | Total Time 14.00(14.00)\n",
      "Iter 1830 | Time 76.3940(75.0772) | Bit/dim 3.9853(3.9929) | Xent 0.6072(0.6126) | Loss 4.2889(4.2993) | Error 0.2204(0.2203) Steps 820(817.08) | Grad Norm 1.1236(1.3135) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0305 | Time 29.8230, Epoch Time 492.6880(497.5056), Bit/dim 3.9976(best: 3.9977), Xent 1.3793, Loss 4.6873, Error 0.4124(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1831 | Time 77.0882(75.1375) | Bit/dim 3.9820(3.9926) | Xent 0.6037(0.6124) | Loss 4.2839(4.2988) | Error 0.2230(0.2204) Steps 814(816.99) | Grad Norm 2.7469(1.3565) | Total Time 14.00(14.00)\n",
      "Iter 1832 | Time 74.4818(75.1179) | Bit/dim 3.9999(3.9928) | Xent 0.6062(0.6122) | Loss 4.3030(4.2989) | Error 0.2215(0.2204) Steps 808(816.72) | Grad Norm 2.9001(1.4028) | Total Time 14.00(14.00)\n",
      "Iter 1833 | Time 74.8201(75.1089) | Bit/dim 3.9946(3.9929) | Xent 0.5949(0.6117) | Loss 4.2921(4.2987) | Error 0.2150(0.2203) Steps 826(817.00) | Grad Norm 1.1896(1.3964) | Total Time 14.00(14.00)\n",
      "Iter 1834 | Time 74.8524(75.1012) | Bit/dim 3.9931(3.9929) | Xent 0.5893(0.6110) | Loss 4.2877(4.2984) | Error 0.2085(0.2199) Steps 826(817.27) | Grad Norm 1.9502(1.4130) | Total Time 14.00(14.00)\n",
      "Iter 1835 | Time 75.3865(75.1098) | Bit/dim 3.9889(3.9928) | Xent 0.6006(0.6107) | Loss 4.2892(4.2981) | Error 0.2175(0.2198) Steps 826(817.53) | Grad Norm 2.2942(1.4394) | Total Time 14.00(14.00)\n",
      "Iter 1836 | Time 74.3580(75.0872) | Bit/dim 3.9865(3.9926) | Xent 0.6123(0.6107) | Loss 4.2926(4.2980) | Error 0.2186(0.2198) Steps 826(817.78) | Grad Norm 1.9111(1.4536) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0306 | Time 29.5122, Epoch Time 496.1919(497.4662), Bit/dim 3.9975(best: 3.9976), Xent 1.3743, Loss 4.6847, Error 0.4136(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1837 | Time 73.6811(75.0451) | Bit/dim 3.9882(3.9925) | Xent 0.6059(0.6106) | Loss 4.2912(4.2978) | Error 0.2156(0.2197) Steps 826(818.03) | Grad Norm 1.0283(1.4408) | Total Time 14.00(14.00)\n",
      "Iter 1838 | Time 73.9368(75.0118) | Bit/dim 3.9910(3.9924) | Xent 0.5925(0.6100) | Loss 4.2872(4.2974) | Error 0.2125(0.2195) Steps 820(818.09) | Grad Norm 1.3664(1.4386) | Total Time 14.00(14.00)\n",
      "Iter 1839 | Time 75.3317(75.0214) | Bit/dim 3.9918(3.9924) | Xent 0.6060(0.6099) | Loss 4.2948(4.2974) | Error 0.2198(0.2195) Steps 814(817.97) | Grad Norm 1.8194(1.4500) | Total Time 14.00(14.00)\n",
      "Iter 1840 | Time 74.6557(75.0104) | Bit/dim 4.0030(3.9927) | Xent 0.6030(0.6097) | Loss 4.3045(4.2976) | Error 0.2126(0.2193) Steps 820(818.03) | Grad Norm 1.6678(1.4565) | Total Time 14.00(14.00)\n",
      "Iter 1841 | Time 74.6632(75.0000) | Bit/dim 3.9907(3.9927) | Xent 0.5900(0.6091) | Loss 4.2857(4.2972) | Error 0.2156(0.2191) Steps 826(818.27) | Grad Norm 0.8909(1.4396) | Total Time 14.00(14.00)\n",
      "Iter 1842 | Time 75.4223(75.0127) | Bit/dim 3.9760(3.9922) | Xent 0.6054(0.6090) | Loss 4.2787(4.2967) | Error 0.2163(0.2191) Steps 814(818.14) | Grad Norm 1.2639(1.4343) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0307 | Time 29.7907, Epoch Time 492.8993(497.3292), Bit/dim 3.9968(best: 3.9975), Xent 1.3801, Loss 4.6869, Error 0.4126(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1843 | Time 74.9876(75.0119) | Bit/dim 3.9907(3.9921) | Xent 0.6087(0.6090) | Loss 4.2950(4.2966) | Error 0.2153(0.2189) Steps 814(818.01) | Grad Norm 2.0654(1.4532) | Total Time 14.00(14.00)\n",
      "Iter 1844 | Time 74.2535(74.9892) | Bit/dim 3.9883(3.9920) | Xent 0.5936(0.6085) | Loss 4.2851(4.2963) | Error 0.2155(0.2188) Steps 820(818.07) | Grad Norm 1.4638(1.4536) | Total Time 14.00(14.00)\n",
      "Iter 1845 | Time 74.0295(74.9604) | Bit/dim 3.9811(3.9917) | Xent 0.5938(0.6081) | Loss 4.2780(4.2957) | Error 0.2145(0.2187) Steps 820(818.13) | Grad Norm 1.1846(1.4455) | Total Time 14.00(14.00)\n",
      "Iter 1846 | Time 73.7024(74.9226) | Bit/dim 3.9972(3.9918) | Xent 0.6086(0.6081) | Loss 4.3015(4.2959) | Error 0.2175(0.2187) Steps 832(818.55) | Grad Norm 2.1588(1.4669) | Total Time 14.00(14.00)\n",
      "Iter 1847 | Time 71.5501(74.8215) | Bit/dim 3.9960(3.9920) | Xent 0.5997(0.6079) | Loss 4.2958(4.2959) | Error 0.2163(0.2186) Steps 820(818.59) | Grad Norm 1.2331(1.4599) | Total Time 14.00(14.00)\n",
      "Iter 1848 | Time 76.7540(74.8795) | Bit/dim 3.9892(3.9919) | Xent 0.6000(0.6076) | Loss 4.2892(4.2957) | Error 0.2140(0.2185) Steps 808(818.27) | Grad Norm 1.0710(1.4482) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0308 | Time 29.5243, Epoch Time 490.5262(497.1251), Bit/dim 3.9971(best: 3.9968), Xent 1.3806, Loss 4.6874, Error 0.4137(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1849 | Time 75.7088(74.9043) | Bit/dim 3.9831(3.9916) | Xent 0.6192(0.6080) | Loss 4.2927(4.2956) | Error 0.2240(0.2186) Steps 820(818.33) | Grad Norm 1.3869(1.4464) | Total Time 14.00(14.00)\n",
      "Iter 1850 | Time 74.2355(74.8843) | Bit/dim 3.9887(3.9915) | Xent 0.5863(0.6073) | Loss 4.2819(4.2952) | Error 0.2073(0.2183) Steps 826(818.56) | Grad Norm 0.9508(1.4315) | Total Time 14.00(14.00)\n",
      "Iter 1851 | Time 72.2487(74.8052) | Bit/dim 3.9918(3.9915) | Xent 0.6002(0.6071) | Loss 4.2918(4.2951) | Error 0.2219(0.2184) Steps 820(818.60) | Grad Norm 0.9909(1.4183) | Total Time 14.00(14.00)\n",
      "Iter 1852 | Time 77.1400(74.8752) | Bit/dim 3.9858(3.9914) | Xent 0.5977(0.6068) | Loss 4.2846(4.2948) | Error 0.2143(0.2183) Steps 832(819.00) | Grad Norm 0.9777(1.4051) | Total Time 14.00(14.00)\n",
      "Iter 1853 | Time 74.4571(74.8627) | Bit/dim 4.0005(3.9916) | Xent 0.6100(0.6069) | Loss 4.3055(4.2951) | Error 0.2179(0.2183) Steps 814(818.85) | Grad Norm 0.9388(1.3911) | Total Time 14.00(14.00)\n",
      "Iter 1854 | Time 74.6390(74.8560) | Bit/dim 3.9909(3.9916) | Xent 0.5961(0.6066) | Loss 4.2889(4.2949) | Error 0.2175(0.2182) Steps 814(818.71) | Grad Norm 1.0618(1.3812) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0309 | Time 29.7312, Epoch Time 493.8813(497.0278), Bit/dim 3.9966(best: 3.9968), Xent 1.3753, Loss 4.6842, Error 0.4153(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1855 | Time 75.4548(74.8740) | Bit/dim 3.9881(3.9915) | Xent 0.6022(0.6065) | Loss 4.2892(4.2947) | Error 0.2166(0.2182) Steps 808(818.38) | Grad Norm 0.8609(1.3656) | Total Time 14.00(14.00)\n",
      "Iter 1856 | Time 72.5875(74.8054) | Bit/dim 3.9863(3.9913) | Xent 0.5990(0.6062) | Loss 4.2858(4.2945) | Error 0.2123(0.2180) Steps 814(818.25) | Grad Norm 0.9191(1.3522) | Total Time 14.00(14.00)\n",
      "Iter 1857 | Time 76.7236(74.8629) | Bit/dim 3.9902(3.9913) | Xent 0.6002(0.6061) | Loss 4.2903(4.2943) | Error 0.2133(0.2179) Steps 820(818.31) | Grad Norm 1.0869(1.3442) | Total Time 14.00(14.00)\n",
      "Iter 1858 | Time 74.6323(74.8560) | Bit/dim 4.0004(3.9916) | Xent 0.5929(0.6057) | Loss 4.2968(4.2944) | Error 0.2175(0.2179) Steps 814(818.18) | Grad Norm 1.0555(1.3356) | Total Time 14.00(14.00)\n",
      "Iter 1859 | Time 77.8013(74.9443) | Bit/dim 3.9922(3.9916) | Xent 0.5946(0.6053) | Loss 4.2895(4.2943) | Error 0.2155(0.2178) Steps 808(817.87) | Grad Norm 1.1165(1.3290) | Total Time 14.00(14.00)\n",
      "Iter 1860 | Time 77.9181(75.0336) | Bit/dim 3.9857(3.9914) | Xent 0.5839(0.6047) | Loss 4.2777(4.2938) | Error 0.2085(0.2175) Steps 814(817.75) | Grad Norm 1.6758(1.3394) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0310 | Time 29.7662, Epoch Time 500.4316(497.1299), Bit/dim 3.9961(best: 3.9966), Xent 1.3794, Loss 4.6858, Error 0.4131(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1861 | Time 76.8166(75.0870) | Bit/dim 3.9878(3.9913) | Xent 0.6021(0.6046) | Loss 4.2889(4.2936) | Error 0.2139(0.2174) Steps 820(817.82) | Grad Norm 1.2240(1.3359) | Total Time 14.00(14.00)\n",
      "Iter 1862 | Time 75.4276(75.0973) | Bit/dim 3.9914(3.9913) | Xent 0.6045(0.6046) | Loss 4.2937(4.2936) | Error 0.2221(0.2175) Steps 814(817.71) | Grad Norm 1.0863(1.3285) | Total Time 14.00(14.00)\n",
      "Iter 1863 | Time 76.4350(75.1374) | Bit/dim 3.9892(3.9913) | Xent 0.6062(0.6047) | Loss 4.2922(4.2936) | Error 0.2176(0.2175) Steps 838(818.32) | Grad Norm 0.9088(1.3159) | Total Time 14.00(14.00)\n",
      "Iter 1864 | Time 74.3487(75.1137) | Bit/dim 3.9928(3.9913) | Xent 0.6046(0.6047) | Loss 4.2951(4.2936) | Error 0.2145(0.2175) Steps 808(818.01) | Grad Norm 1.8078(1.3306) | Total Time 14.00(14.00)\n",
      "Iter 1865 | Time 73.4556(75.0640) | Bit/dim 3.9916(3.9913) | Xent 0.5830(0.6040) | Loss 4.2831(4.2933) | Error 0.2119(0.2173) Steps 808(817.71) | Grad Norm 1.2042(1.3268) | Total Time 14.00(14.00)\n",
      "Iter 1866 | Time 76.0333(75.0931) | Bit/dim 3.9913(3.9913) | Xent 0.6027(0.6040) | Loss 4.2927(4.2933) | Error 0.2126(0.2171) Steps 808(817.42) | Grad Norm 1.6371(1.3361) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0311 | Time 29.7960, Epoch Time 498.0001(497.1560), Bit/dim 3.9948(best: 3.9961), Xent 1.3857, Loss 4.6877, Error 0.4154(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1867 | Time 72.4348(75.0133) | Bit/dim 3.9960(3.9915) | Xent 0.5961(0.6037) | Loss 4.2940(4.2933) | Error 0.2151(0.2171) Steps 808(817.13) | Grad Norm 1.1758(1.3313) | Total Time 14.00(14.00)\n",
      "Iter 1868 | Time 76.3446(75.0533) | Bit/dim 3.9864(3.9913) | Xent 0.5891(0.6033) | Loss 4.2809(4.2929) | Error 0.2123(0.2169) Steps 820(817.22) | Grad Norm 1.4070(1.3336) | Total Time 14.00(14.00)\n",
      "Iter 1869 | Time 73.6156(75.0101) | Bit/dim 3.9865(3.9912) | Xent 0.5944(0.6030) | Loss 4.2837(4.2927) | Error 0.2115(0.2168) Steps 832(817.66) | Grad Norm 1.0288(1.3245) | Total Time 14.00(14.00)\n",
      "Iter 1870 | Time 73.7621(74.9727) | Bit/dim 3.9856(3.9910) | Xent 0.5881(0.6026) | Loss 4.2797(4.2923) | Error 0.2084(0.2165) Steps 814(817.55) | Grad Norm 0.8769(1.3110) | Total Time 14.00(14.00)\n",
      "Iter 1871 | Time 72.9620(74.9124) | Bit/dim 3.9858(3.9908) | Xent 0.5988(0.6025) | Loss 4.2852(4.2921) | Error 0.2114(0.2164) Steps 820(817.63) | Grad Norm 0.8634(1.2976) | Total Time 14.00(14.00)\n",
      "Iter 1872 | Time 74.7825(74.9085) | Bit/dim 3.9951(3.9910) | Xent 0.5893(0.6021) | Loss 4.2897(4.2920) | Error 0.2101(0.2162) Steps 814(817.52) | Grad Norm 0.9181(1.2862) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0312 | Time 29.7511, Epoch Time 489.4168(496.9238), Bit/dim 3.9956(best: 3.9948), Xent 1.3889, Loss 4.6900, Error 0.4135(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1873 | Time 73.2195(74.8578) | Bit/dim 3.9879(3.9909) | Xent 0.5990(0.6020) | Loss 4.2873(4.2919) | Error 0.2184(0.2162) Steps 814(817.41) | Grad Norm 0.7714(1.2708) | Total Time 14.00(14.00)\n",
      "Iter 1874 | Time 74.8129(74.8565) | Bit/dim 3.9803(3.9906) | Xent 0.5963(0.6018) | Loss 4.2785(4.2915) | Error 0.2170(0.2163) Steps 814(817.31) | Grad Norm 1.0076(1.2629) | Total Time 14.00(14.00)\n",
      "Iter 1875 | Time 73.8188(74.8253) | Bit/dim 3.9916(3.9906) | Xent 0.6057(0.6019) | Loss 4.2944(4.2915) | Error 0.2208(0.2164) Steps 808(817.03) | Grad Norm 0.9215(1.2526) | Total Time 14.00(14.00)\n",
      "Iter 1876 | Time 75.6533(74.8502) | Bit/dim 3.9985(3.9908) | Xent 0.5832(0.6014) | Loss 4.2901(4.2915) | Error 0.2055(0.2161) Steps 808(816.76) | Grad Norm 1.3416(1.2553) | Total Time 14.00(14.00)\n",
      "Iter 1877 | Time 74.2380(74.8318) | Bit/dim 3.9939(3.9909) | Xent 0.5909(0.6010) | Loss 4.2893(4.2914) | Error 0.2159(0.2161) Steps 802(816.32) | Grad Norm 1.0526(1.2492) | Total Time 14.00(14.00)\n",
      "Iter 1878 | Time 77.6655(74.9168) | Bit/dim 3.9859(3.9908) | Xent 0.5908(0.6007) | Loss 4.2813(4.2911) | Error 0.2130(0.2160) Steps 808(816.07) | Grad Norm 1.3416(1.2520) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0313 | Time 29.1605, Epoch Time 494.2207(496.8427), Bit/dim 3.9944(best: 3.9948), Xent 1.3876, Loss 4.6882, Error 0.4176(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1879 | Time 72.5850(74.8469) | Bit/dim 3.9857(3.9906) | Xent 0.6030(0.6008) | Loss 4.2872(4.2910) | Error 0.2195(0.2161) Steps 826(816.36) | Grad Norm 2.0105(1.2747) | Total Time 14.00(14.00)\n",
      "Iter 1880 | Time 74.0095(74.8217) | Bit/dim 3.9919(3.9906) | Xent 0.5772(0.6001) | Loss 4.2805(4.2907) | Error 0.2086(0.2159) Steps 820(816.47) | Grad Norm 1.4199(1.2791) | Total Time 14.00(14.00)\n",
      "Iter 1881 | Time 76.2448(74.8644) | Bit/dim 3.9894(3.9906) | Xent 0.5890(0.5998) | Loss 4.2839(4.2905) | Error 0.2136(0.2158) Steps 832(816.94) | Grad Norm 1.1010(1.2738) | Total Time 14.00(14.00)\n",
      "Iter 1882 | Time 77.5033(74.9436) | Bit/dim 3.9853(3.9905) | Xent 0.5892(0.5994) | Loss 4.2800(4.2902) | Error 0.2137(0.2157) Steps 808(816.67) | Grad Norm 1.4176(1.2781) | Total Time 14.00(14.00)\n",
      "Iter 1883 | Time 74.3384(74.9254) | Bit/dim 3.9932(3.9905) | Xent 0.5898(0.5992) | Loss 4.2881(4.2901) | Error 0.2119(0.2156) Steps 814(816.59) | Grad Norm 1.6029(1.2878) | Total Time 14.00(14.00)\n",
      "Iter 1884 | Time 76.0840(74.9602) | Bit/dim 3.9889(3.9905) | Xent 0.6027(0.5993) | Loss 4.2902(4.2901) | Error 0.2173(0.2157) Steps 814(816.51) | Grad Norm 0.9984(1.2791) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0314 | Time 29.7299, Epoch Time 496.0720(496.8196), Bit/dim 3.9950(best: 3.9944), Xent 1.3906, Loss 4.6902, Error 0.4149(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1885 | Time 72.7908(74.8951) | Bit/dim 3.9845(3.9903) | Xent 0.5806(0.5987) | Loss 4.2749(4.2897) | Error 0.2130(0.2156) Steps 820(816.62) | Grad Norm 0.9535(1.2694) | Total Time 14.00(14.00)\n",
      "Iter 1886 | Time 74.5351(74.8843) | Bit/dim 3.9947(3.9904) | Xent 0.5983(0.5987) | Loss 4.2939(4.2898) | Error 0.2134(0.2155) Steps 814(816.54) | Grad Norm 1.7087(1.2825) | Total Time 14.00(14.00)\n",
      "Iter 1887 | Time 74.3870(74.8694) | Bit/dim 3.9949(3.9906) | Xent 0.6028(0.5988) | Loss 4.2963(4.2900) | Error 0.2202(0.2157) Steps 820(816.64) | Grad Norm 1.1842(1.2796) | Total Time 14.00(14.00)\n",
      "Iter 1888 | Time 72.9463(74.8117) | Bit/dim 3.9898(3.9905) | Xent 0.5856(0.5984) | Loss 4.2826(4.2898) | Error 0.2086(0.2155) Steps 814(816.56) | Grad Norm 1.0796(1.2736) | Total Time 14.00(14.00)\n",
      "Iter 1889 | Time 73.5885(74.7750) | Bit/dim 3.9889(3.9905) | Xent 0.5821(0.5979) | Loss 4.2799(4.2895) | Error 0.2159(0.2155) Steps 832(817.03) | Grad Norm 1.4771(1.2797) | Total Time 14.00(14.00)\n",
      "Iter 1890 | Time 75.3936(74.7936) | Bit/dim 3.9822(3.9903) | Xent 0.5942(0.5978) | Loss 4.2794(4.2892) | Error 0.2140(0.2154) Steps 808(816.76) | Grad Norm 1.0677(1.2733) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0315 | Time 29.4338, Epoch Time 488.4473(496.5684), Bit/dim 3.9949(best: 3.9944), Xent 1.3940, Loss 4.6919, Error 0.4150(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1891 | Time 72.7439(74.7321) | Bit/dim 3.9880(3.9902) | Xent 0.5734(0.5971) | Loss 4.2747(4.2887) | Error 0.2065(0.2152) Steps 832(817.21) | Grad Norm 1.2083(1.2714) | Total Time 14.00(14.00)\n",
      "Iter 1892 | Time 73.8631(74.7060) | Bit/dim 3.9852(3.9900) | Xent 0.5630(0.5961) | Loss 4.2667(4.2881) | Error 0.2020(0.2148) Steps 826(817.48) | Grad Norm 1.6563(1.2829) | Total Time 14.00(14.00)\n",
      "Iter 1893 | Time 75.3845(74.7264) | Bit/dim 3.9884(3.9900) | Xent 0.5894(0.5959) | Loss 4.2831(4.2879) | Error 0.2143(0.2147) Steps 814(817.37) | Grad Norm 1.3201(1.2841) | Total Time 14.00(14.00)\n",
      "Iter 1894 | Time 76.3454(74.7749) | Bit/dim 3.9927(3.9901) | Xent 0.5915(0.5957) | Loss 4.2885(4.2879) | Error 0.2121(0.2147) Steps 820(817.45) | Grad Norm 0.8790(1.2719) | Total Time 14.00(14.00)\n",
      "Iter 1895 | Time 77.4399(74.8549) | Bit/dim 3.9885(3.9900) | Xent 0.5922(0.5956) | Loss 4.2846(4.2878) | Error 0.2124(0.2146) Steps 808(817.17) | Grad Norm 1.5681(1.2808) | Total Time 14.00(14.00)\n",
      "Iter 1896 | Time 72.5045(74.7844) | Bit/dim 3.9860(3.9899) | Xent 0.6047(0.5959) | Loss 4.2884(4.2878) | Error 0.2190(0.2147) Steps 820(817.25) | Grad Norm 1.3342(1.2824) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0316 | Time 29.9224, Epoch Time 493.7780(496.4847), Bit/dim 3.9950(best: 3.9944), Xent 1.4056, Loss 4.6978, Error 0.4177(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1897 | Time 77.9491(74.8793) | Bit/dim 3.9927(3.9900) | Xent 0.5902(0.5957) | Loss 4.2878(4.2878) | Error 0.2144(0.2147) Steps 820(817.34) | Grad Norm 1.5888(1.2916) | Total Time 14.00(14.00)\n",
      "Iter 1898 | Time 76.7342(74.9350) | Bit/dim 3.9835(3.9898) | Xent 0.5908(0.5956) | Loss 4.2789(4.2876) | Error 0.2125(0.2147) Steps 826(817.60) | Grad Norm 2.6331(1.3318) | Total Time 14.00(14.00)\n",
      "Iter 1899 | Time 76.6682(74.9870) | Bit/dim 3.9831(3.9896) | Xent 0.5776(0.5950) | Loss 4.2719(4.2871) | Error 0.2069(0.2144) Steps 814(817.49) | Grad Norm 1.3043(1.3310) | Total Time 14.00(14.00)\n",
      "Iter 1900 | Time 73.3841(74.9389) | Bit/dim 3.9887(3.9896) | Xent 0.5996(0.5952) | Loss 4.2885(4.2871) | Error 0.2180(0.2145) Steps 802(817.02) | Grad Norm 1.6785(1.3414) | Total Time 14.00(14.00)\n",
      "Iter 1901 | Time 73.5163(74.8962) | Bit/dim 3.9873(3.9895) | Xent 0.5887(0.5950) | Loss 4.2816(4.2870) | Error 0.2115(0.2144) Steps 808(816.75) | Grad Norm 1.7680(1.3542) | Total Time 14.00(14.00)\n",
      "Iter 1902 | Time 77.6858(74.9799) | Bit/dim 3.9957(3.9897) | Xent 0.5920(0.5949) | Loss 4.2917(4.2871) | Error 0.2114(0.2143) Steps 802(816.31) | Grad Norm 1.6704(1.3637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0317 | Time 29.0510, Epoch Time 500.2577(496.5979), Bit/dim 3.9940(best: 3.9944), Xent 1.3973, Loss 4.6927, Error 0.4163(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1903 | Time 75.8107(75.0048) | Bit/dim 3.9879(3.9896) | Xent 0.5851(0.5946) | Loss 4.2804(4.2869) | Error 0.2121(0.2143) Steps 802(815.88) | Grad Norm 1.5936(1.3706) | Total Time 14.00(14.00)\n",
      "Iter 1904 | Time 74.0611(74.9765) | Bit/dim 4.0023(3.9900) | Xent 0.5966(0.5947) | Loss 4.3005(4.2873) | Error 0.2085(0.2141) Steps 808(815.64) | Grad Norm 1.9752(1.3887) | Total Time 14.00(14.00)\n",
      "Iter 1905 | Time 73.3290(74.9271) | Bit/dim 3.9880(3.9899) | Xent 0.5827(0.5943) | Loss 4.2794(4.2871) | Error 0.2089(0.2139) Steps 802(815.23) | Grad Norm 1.8555(1.4027) | Total Time 14.00(14.00)\n",
      "Iter 1906 | Time 72.5712(74.8564) | Bit/dim 3.9902(3.9900) | Xent 0.5690(0.5935) | Loss 4.2747(4.2867) | Error 0.2039(0.2136) Steps 814(815.20) | Grad Norm 1.4778(1.4050) | Total Time 14.00(14.00)\n",
      "Iter 1907 | Time 75.4851(74.8753) | Bit/dim 3.9816(3.9897) | Xent 0.5829(0.5932) | Loss 4.2730(4.2863) | Error 0.2126(0.2136) Steps 808(814.98) | Grad Norm 1.7006(1.4139) | Total Time 14.00(14.00)\n",
      "Iter 1908 | Time 75.3679(74.8900) | Bit/dim 3.9810(3.9894) | Xent 0.6009(0.5934) | Loss 4.2815(4.2862) | Error 0.2129(0.2136) Steps 808(814.77) | Grad Norm 2.2489(1.4389) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0318 | Time 29.5120, Epoch Time 492.0079(496.4602), Bit/dim 3.9948(best: 3.9940), Xent 1.4039, Loss 4.6967, Error 0.4137(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1909 | Time 74.2925(74.8721) | Bit/dim 3.9863(3.9893) | Xent 0.5809(0.5931) | Loss 4.2767(4.2859) | Error 0.2096(0.2135) Steps 832(815.29) | Grad Norm 1.6547(1.4454) | Total Time 14.00(14.00)\n",
      "Iter 1910 | Time 77.4190(74.9485) | Bit/dim 3.9925(3.9894) | Xent 0.5810(0.5927) | Loss 4.2829(4.2858) | Error 0.2063(0.2133) Steps 814(815.25) | Grad Norm 1.2405(1.4392) | Total Time 14.00(14.00)\n",
      "Iter 1911 | Time 78.3706(75.0512) | Bit/dim 3.9828(3.9892) | Xent 0.5852(0.5925) | Loss 4.2754(4.2855) | Error 0.2084(0.2131) Steps 814(815.21) | Grad Norm 2.1527(1.4606) | Total Time 14.00(14.00)\n",
      "Iter 1912 | Time 75.1919(75.0554) | Bit/dim 3.9908(3.9893) | Xent 0.5767(0.5920) | Loss 4.2791(4.2853) | Error 0.2031(0.2128) Steps 814(815.18) | Grad Norm 1.1179(1.4504) | Total Time 14.00(14.00)\n",
      "Iter 1913 | Time 77.4233(75.1264) | Bit/dim 3.9855(3.9892) | Xent 0.5895(0.5919) | Loss 4.2803(4.2851) | Error 0.2134(0.2128) Steps 826(815.50) | Grad Norm 1.2413(1.4441) | Total Time 14.00(14.00)\n",
      "Iter 1914 | Time 74.9021(75.1197) | Bit/dim 3.9902(3.9892) | Xent 0.5830(0.5917) | Loss 4.2817(4.2850) | Error 0.2059(0.2126) Steps 802(815.10) | Grad Norm 1.0052(1.4309) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0319 | Time 29.6452, Epoch Time 502.8093(496.6507), Bit/dim 3.9934(best: 3.9940), Xent 1.3974, Loss 4.6921, Error 0.4110(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1915 | Time 73.9238(75.0838) | Bit/dim 3.9801(3.9889) | Xent 0.5869(0.5915) | Loss 4.2735(4.2847) | Error 0.2149(0.2127) Steps 820(815.24) | Grad Norm 1.0500(1.4195) | Total Time 14.00(14.00)\n",
      "Iter 1916 | Time 74.8270(75.0761) | Bit/dim 3.9892(3.9889) | Xent 0.5861(0.5914) | Loss 4.2823(4.2846) | Error 0.2116(0.2127) Steps 808(815.03) | Grad Norm 1.2178(1.4135) | Total Time 14.00(14.00)\n",
      "Iter 1917 | Time 75.4719(75.0880) | Bit/dim 3.9955(3.9891) | Xent 0.5929(0.5914) | Loss 4.2920(4.2848) | Error 0.2146(0.2127) Steps 814(815.00) | Grad Norm 1.2761(1.4093) | Total Time 14.00(14.00)\n",
      "Iter 1918 | Time 76.1153(75.1188) | Bit/dim 3.9852(3.9890) | Xent 0.5901(0.5914) | Loss 4.2803(4.2847) | Error 0.2130(0.2127) Steps 808(814.79) | Grad Norm 0.8485(1.3925) | Total Time 14.00(14.00)\n",
      "Iter 1919 | Time 75.8669(75.1413) | Bit/dim 3.9931(3.9891) | Xent 0.5667(0.5906) | Loss 4.2764(4.2845) | Error 0.2066(0.2125) Steps 814(814.76) | Grad Norm 1.4258(1.3935) | Total Time 14.00(14.00)\n",
      "Iter 1920 | Time 72.5262(75.0628) | Bit/dim 3.9814(3.9889) | Xent 0.5781(0.5902) | Loss 4.2704(4.2840) | Error 0.2111(0.2125) Steps 808(814.56) | Grad Norm 1.1164(1.3852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0320 | Time 29.6639, Epoch Time 494.0342(496.5722), Bit/dim 3.9938(best: 3.9934), Xent 1.3989, Loss 4.6933, Error 0.4135(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1921 | Time 74.0251(75.0317) | Bit/dim 3.9898(3.9889) | Xent 0.5926(0.5903) | Loss 4.2861(4.2841) | Error 0.2107(0.2124) Steps 820(814.72) | Grad Norm 2.2813(1.4121) | Total Time 14.00(14.00)\n",
      "Iter 1922 | Time 76.1248(75.0645) | Bit/dim 3.9924(3.9890) | Xent 0.5723(0.5898) | Loss 4.2785(4.2839) | Error 0.2077(0.2123) Steps 814(814.70) | Grad Norm 1.5372(1.4158) | Total Time 14.00(14.00)\n",
      "Iter 1923 | Time 73.7051(75.0237) | Bit/dim 3.9924(3.9891) | Xent 0.5837(0.5896) | Loss 4.2843(4.2839) | Error 0.2147(0.2124) Steps 814(814.68) | Grad Norm 1.1278(1.4072) | Total Time 14.00(14.00)\n",
      "Iter 1924 | Time 75.0680(75.0250) | Bit/dim 3.9818(3.9889) | Xent 0.5689(0.5890) | Loss 4.2662(4.2834) | Error 0.2055(0.2122) Steps 826(815.02) | Grad Norm 1.0031(1.3951) | Total Time 14.00(14.00)\n",
      "Iter 1925 | Time 74.2322(75.0012) | Bit/dim 3.9807(3.9887) | Xent 0.5883(0.5890) | Loss 4.2749(4.2832) | Error 0.2135(0.2122) Steps 808(814.81) | Grad Norm 1.9223(1.4109) | Total Time 14.00(14.00)\n",
      "Iter 1926 | Time 74.5986(74.9892) | Bit/dim 3.9833(3.9885) | Xent 0.5945(0.5891) | Loss 4.2806(4.2831) | Error 0.2124(0.2122) Steps 820(814.96) | Grad Norm 1.4560(1.4122) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0321 | Time 29.6380, Epoch Time 492.9499(496.4635), Bit/dim 3.9931(best: 3.9934), Xent 1.3970, Loss 4.6916, Error 0.4119(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1927 | Time 77.3820(75.0609) | Bit/dim 3.9940(3.9887) | Xent 0.5707(0.5886) | Loss 4.2793(4.2830) | Error 0.2079(0.2121) Steps 820(815.12) | Grad Norm 1.0548(1.4015) | Total Time 14.00(14.00)\n",
      "Iter 1928 | Time 74.1287(75.0330) | Bit/dim 3.9888(3.9887) | Xent 0.5897(0.5886) | Loss 4.2836(4.2830) | Error 0.2077(0.2120) Steps 808(814.90) | Grad Norm 0.9385(1.3876) | Total Time 14.00(14.00)\n",
      "Iter 1929 | Time 75.9393(75.0602) | Bit/dim 3.9917(3.9888) | Xent 0.5789(0.5883) | Loss 4.2811(4.2829) | Error 0.2031(0.2117) Steps 814(814.87) | Grad Norm 0.9742(1.3752) | Total Time 14.00(14.00)\n",
      "Iter 1930 | Time 72.7970(74.9923) | Bit/dim 3.9756(3.9884) | Xent 0.5762(0.5880) | Loss 4.2637(4.2824) | Error 0.2023(0.2114) Steps 820(815.03) | Grad Norm 1.3296(1.3739) | Total Time 14.00(14.00)\n",
      "Iter 1931 | Time 75.5148(75.0079) | Bit/dim 3.9812(3.9882) | Xent 0.5789(0.5877) | Loss 4.2707(4.2820) | Error 0.2095(0.2113) Steps 814(815.00) | Grad Norm 0.9312(1.3606) | Total Time 14.00(14.00)\n",
      "Iter 1932 | Time 75.3079(75.0169) | Bit/dim 3.9918(3.9883) | Xent 0.5669(0.5871) | Loss 4.2753(4.2818) | Error 0.2003(0.2110) Steps 808(814.79) | Grad Norm 1.7482(1.3722) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0322 | Time 29.7462, Epoch Time 496.4084(496.4619), Bit/dim 3.9930(best: 3.9931), Xent 1.4131, Loss 4.6996, Error 0.4205(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1933 | Time 73.1198(74.9600) | Bit/dim 3.9961(3.9885) | Xent 0.5853(0.5870) | Loss 4.2888(4.2820) | Error 0.2167(0.2112) Steps 814(814.76) | Grad Norm 1.1853(1.3666) | Total Time 14.00(14.00)\n",
      "Iter 1934 | Time 75.3485(74.9717) | Bit/dim 3.9914(3.9886) | Xent 0.5802(0.5868) | Loss 4.2815(4.2820) | Error 0.2050(0.2110) Steps 802(814.38) | Grad Norm 1.1573(1.3603) | Total Time 14.00(14.00)\n",
      "Iter 1935 | Time 74.1064(74.9457) | Bit/dim 3.9774(3.9883) | Xent 0.5696(0.5863) | Loss 4.2622(4.2814) | Error 0.2064(0.2109) Steps 826(814.73) | Grad Norm 1.5021(1.3646) | Total Time 14.00(14.00)\n",
      "Iter 1936 | Time 75.9765(74.9766) | Bit/dim 3.9813(3.9880) | Xent 0.5898(0.5864) | Loss 4.2762(4.2812) | Error 0.2110(0.2109) Steps 820(814.89) | Grad Norm 1.2513(1.3612) | Total Time 14.00(14.00)\n",
      "Iter 1937 | Time 75.4175(74.9899) | Bit/dim 3.9865(3.9880) | Xent 0.5661(0.5858) | Loss 4.2696(4.2809) | Error 0.2063(0.2107) Steps 802(814.50) | Grad Norm 1.5433(1.3666) | Total Time 14.00(14.00)\n",
      "Iter 1938 | Time 74.8573(74.9859) | Bit/dim 3.9854(3.9879) | Xent 0.5914(0.5860) | Loss 4.2811(4.2809) | Error 0.2126(0.2108) Steps 820(814.67) | Grad Norm 1.2537(1.3632) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0323 | Time 29.3867, Epoch Time 493.7220(496.3797), Bit/dim 3.9934(best: 3.9930), Xent 1.4103, Loss 4.6985, Error 0.4143(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1939 | Time 72.2056(74.9025) | Bit/dim 3.9971(3.9882) | Xent 0.5784(0.5857) | Loss 4.2863(4.2811) | Error 0.2116(0.2108) Steps 814(814.65) | Grad Norm 1.2406(1.3596) | Total Time 14.00(14.00)\n",
      "Iter 1940 | Time 73.7649(74.8684) | Bit/dim 3.9863(3.9881) | Xent 0.5850(0.5857) | Loss 4.2788(4.2810) | Error 0.2129(0.2109) Steps 808(814.45) | Grad Norm 2.1786(1.3841) | Total Time 14.00(14.00)\n",
      "Iter 1941 | Time 76.4352(74.9154) | Bit/dim 3.9875(3.9881) | Xent 0.5650(0.5851) | Loss 4.2700(4.2807) | Error 0.2033(0.2106) Steps 808(814.25) | Grad Norm 1.9876(1.4022) | Total Time 14.00(14.00)\n",
      "Iter 1942 | Time 75.2233(74.9246) | Bit/dim 3.9771(3.9878) | Xent 0.5726(0.5847) | Loss 4.2634(4.2801) | Error 0.2023(0.2104) Steps 808(814.07) | Grad Norm 1.1496(1.3947) | Total Time 14.00(14.00)\n",
      "Iter 1943 | Time 73.3900(74.8786) | Bit/dim 3.9848(3.9877) | Xent 0.5796(0.5846) | Loss 4.2746(4.2800) | Error 0.2106(0.2104) Steps 814(814.06) | Grad Norm 2.0108(1.4132) | Total Time 14.00(14.00)\n",
      "Iter 1944 | Time 73.9143(74.8496) | Bit/dim 3.9879(3.9877) | Xent 0.5696(0.5841) | Loss 4.2727(4.2798) | Error 0.2053(0.2102) Steps 814(814.06) | Grad Norm 1.8752(1.4270) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0324 | Time 29.5185, Epoch Time 490.1062(496.1915), Bit/dim 3.9913(best: 3.9930), Xent 1.4140, Loss 4.6983, Error 0.4170(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1945 | Time 77.0662(74.9161) | Bit/dim 3.9883(3.9877) | Xent 0.5673(0.5836) | Loss 4.2720(4.2795) | Error 0.2005(0.2100) Steps 820(814.24) | Grad Norm 1.3881(1.4258) | Total Time 14.00(14.00)\n",
      "Iter 1946 | Time 73.5340(74.8747) | Bit/dim 3.9828(3.9876) | Xent 0.5842(0.5836) | Loss 4.2749(4.2794) | Error 0.2054(0.2098) Steps 826(814.59) | Grad Norm 1.3905(1.4248) | Total Time 14.00(14.00)\n",
      "Iter 1947 | Time 74.2237(74.8551) | Bit/dim 3.9816(3.9874) | Xent 0.5702(0.5832) | Loss 4.2667(4.2790) | Error 0.2081(0.2098) Steps 820(814.76) | Grad Norm 1.5844(1.4296) | Total Time 14.00(14.00)\n",
      "Iter 1948 | Time 74.9716(74.8586) | Bit/dim 3.9865(3.9874) | Xent 0.5669(0.5827) | Loss 4.2699(4.2787) | Error 0.2066(0.2097) Steps 832(815.27) | Grad Norm 1.4586(1.4304) | Total Time 14.00(14.00)\n",
      "Iter 1949 | Time 75.6108(74.8812) | Bit/dim 3.9930(3.9875) | Xent 0.5971(0.5832) | Loss 4.2915(4.2791) | Error 0.2143(0.2098) Steps 820(815.41) | Grad Norm 1.3640(1.4285) | Total Time 14.00(14.00)\n",
      "Iter 1950 | Time 77.2780(74.9531) | Bit/dim 3.9814(3.9874) | Xent 0.5678(0.5827) | Loss 4.2653(4.2787) | Error 0.2049(0.2097) Steps 814(815.37) | Grad Norm 1.6332(1.4346) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0325 | Time 30.0936, Epoch Time 498.3237(496.2554), Bit/dim 3.9919(best: 3.9913), Xent 1.4144, Loss 4.6990, Error 0.4159(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1951 | Time 71.6396(74.8537) | Bit/dim 3.9827(3.9872) | Xent 0.5886(0.5829) | Loss 4.2769(4.2787) | Error 0.2099(0.2097) Steps 808(815.15) | Grad Norm 1.2459(1.4289) | Total Time 14.00(14.00)\n",
      "Iter 1952 | Time 75.0802(74.8605) | Bit/dim 3.9877(3.9872) | Xent 0.5684(0.5824) | Loss 4.2719(4.2784) | Error 0.2033(0.2095) Steps 826(815.48) | Grad Norm 1.7483(1.4385) | Total Time 14.00(14.00)\n",
      "Iter 1953 | Time 76.0349(74.8957) | Bit/dim 3.9804(3.9870) | Xent 0.5574(0.5817) | Loss 4.2591(4.2779) | Error 0.1999(0.2092) Steps 820(815.61) | Grad Norm 1.1500(1.4299) | Total Time 14.00(14.00)\n",
      "Iter 1954 | Time 74.1808(74.8743) | Bit/dim 3.9863(3.9870) | Xent 0.5830(0.5817) | Loss 4.2778(4.2779) | Error 0.2083(0.2092) Steps 808(815.38) | Grad Norm 1.2729(1.4252) | Total Time 14.00(14.00)\n",
      "Iter 1955 | Time 76.2120(74.9144) | Bit/dim 3.9886(3.9871) | Xent 0.5619(0.5811) | Loss 4.2695(4.2776) | Error 0.2016(0.2089) Steps 814(815.34) | Grad Norm 2.6580(1.4621) | Total Time 14.00(14.00)\n",
      "Iter 1956 | Time 76.5330(74.9630) | Bit/dim 3.9917(3.9872) | Xent 0.5801(0.5811) | Loss 4.2817(4.2777) | Error 0.2085(0.2089) Steps 808(815.12) | Grad Norm 1.0816(1.4507) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0326 | Time 29.9595, Epoch Time 495.1676(496.2228), Bit/dim 3.9921(best: 3.9913), Xent 1.4249, Loss 4.7045, Error 0.4151(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1957 | Time 77.1866(75.0297) | Bit/dim 3.9834(3.9871) | Xent 0.5657(0.5806) | Loss 4.2662(4.2774) | Error 0.2063(0.2088) Steps 802(814.73) | Grad Norm 1.3136(1.4466) | Total Time 14.00(14.00)\n",
      "Iter 1958 | Time 76.5881(75.0764) | Bit/dim 3.9919(3.9872) | Xent 0.5536(0.5798) | Loss 4.2687(4.2771) | Error 0.2029(0.2087) Steps 808(814.53) | Grad Norm 1.1920(1.4390) | Total Time 14.00(14.00)\n",
      "Iter 1959 | Time 73.9465(75.0425) | Bit/dim 3.9826(3.9871) | Xent 0.5683(0.5795) | Loss 4.2667(4.2768) | Error 0.1983(0.2083) Steps 832(815.05) | Grad Norm 1.7554(1.4485) | Total Time 14.00(14.00)\n",
      "Iter 1960 | Time 75.2444(75.0486) | Bit/dim 3.9788(3.9868) | Xent 0.5629(0.5790) | Loss 4.2603(4.2763) | Error 0.2053(0.2083) Steps 820(815.20) | Grad Norm 1.0730(1.4372) | Total Time 14.00(14.00)\n",
      "Iter 1961 | Time 75.9659(75.0761) | Bit/dim 3.9871(3.9868) | Xent 0.5882(0.5793) | Loss 4.2812(4.2765) | Error 0.2151(0.2085) Steps 808(814.98) | Grad Norm 1.7584(1.4468) | Total Time 14.00(14.00)\n",
      "Iter 1962 | Time 74.2445(75.0512) | Bit/dim 3.9861(3.9868) | Xent 0.5686(0.5789) | Loss 4.2704(4.2763) | Error 0.2039(0.2083) Steps 826(815.31) | Grad Norm 0.8692(1.4295) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0327 | Time 29.9149, Epoch Time 498.6171(496.2946), Bit/dim 3.9914(best: 3.9913), Xent 1.4291, Loss 4.7060, Error 0.4156(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1963 | Time 75.4773(75.0639) | Bit/dim 3.9935(3.9870) | Xent 0.5646(0.5785) | Loss 4.2758(4.2763) | Error 0.2054(0.2082) Steps 814(815.27) | Grad Norm 1.0482(1.4181) | Total Time 14.00(14.00)\n",
      "Iter 1964 | Time 75.9933(75.0918) | Bit/dim 3.9906(3.9871) | Xent 0.5632(0.5781) | Loss 4.2722(4.2762) | Error 0.2026(0.2081) Steps 802(814.88) | Grad Norm 0.8854(1.4021) | Total Time 14.00(14.00)\n",
      "Iter 1965 | Time 77.1992(75.1550) | Bit/dim 3.9853(3.9871) | Xent 0.5611(0.5775) | Loss 4.2659(4.2758) | Error 0.2004(0.2078) Steps 814(814.85) | Grad Norm 0.9872(1.3896) | Total Time 14.00(14.00)\n",
      "Iter 1966 | Time 74.7275(75.1422) | Bit/dim 3.9784(3.9868) | Xent 0.5606(0.5770) | Loss 4.2587(4.2753) | Error 0.1990(0.2076) Steps 814(814.82) | Grad Norm 1.5693(1.3950) | Total Time 14.00(14.00)\n",
      "Iter 1967 | Time 74.3845(75.1195) | Bit/dim 3.9920(3.9870) | Xent 0.5664(0.5767) | Loss 4.2752(4.2753) | Error 0.2070(0.2076) Steps 808(814.62) | Grad Norm 1.4575(1.3969) | Total Time 14.00(14.00)\n",
      "Iter 1968 | Time 74.2267(75.0927) | Bit/dim 3.9705(3.9865) | Xent 0.5821(0.5769) | Loss 4.2616(4.2749) | Error 0.2027(0.2074) Steps 814(814.60) | Grad Norm 1.2585(1.3927) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0328 | Time 30.2512, Epoch Time 497.8384(496.3409), Bit/dim 3.9914(best: 3.9913), Xent 1.4335, Loss 4.7082, Error 0.4128(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1969 | Time 72.6666(75.0199) | Bit/dim 3.9883(3.9865) | Xent 0.5740(0.5768) | Loss 4.2753(4.2749) | Error 0.2083(0.2074) Steps 814(814.58) | Grad Norm 1.0607(1.3828) | Total Time 14.00(14.00)\n",
      "Iter 1970 | Time 74.8862(75.0159) | Bit/dim 3.9782(3.9863) | Xent 0.5746(0.5767) | Loss 4.2655(4.2746) | Error 0.2105(0.2075) Steps 820(814.75) | Grad Norm 1.1110(1.3746) | Total Time 14.00(14.00)\n",
      "Iter 1971 | Time 72.1779(74.9308) | Bit/dim 3.9870(3.9863) | Xent 0.5719(0.5766) | Loss 4.2729(4.2746) | Error 0.2034(0.2074) Steps 820(814.90) | Grad Norm 0.9736(1.3626) | Total Time 14.00(14.00)\n",
      "Iter 1972 | Time 72.1944(74.8487) | Bit/dim 3.9857(3.9863) | Xent 0.5750(0.5765) | Loss 4.2732(4.2745) | Error 0.2051(0.2073) Steps 820(815.06) | Grad Norm 1.2663(1.3597) | Total Time 14.00(14.00)\n",
      "Iter 1973 | Time 76.1491(74.8877) | Bit/dim 3.9848(3.9862) | Xent 0.5530(0.5758) | Loss 4.2613(4.2742) | Error 0.2024(0.2072) Steps 814(815.02) | Grad Norm 1.6397(1.3681) | Total Time 14.00(14.00)\n",
      "Iter 1974 | Time 74.2696(74.8691) | Bit/dim 3.9879(3.9863) | Xent 0.5712(0.5757) | Loss 4.2735(4.2741) | Error 0.2069(0.2072) Steps 814(814.99) | Grad Norm 1.3074(1.3663) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0329 | Time 29.6088, Epoch Time 487.5145(496.0761), Bit/dim 3.9908(best: 3.9913), Xent 1.4294, Loss 4.7055, Error 0.4167(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1975 | Time 76.1418(74.9073) | Bit/dim 3.9833(3.9862) | Xent 0.5538(0.5750) | Loss 4.2602(4.2737) | Error 0.1961(0.2068) Steps 820(815.14) | Grad Norm 1.6714(1.3754) | Total Time 14.00(14.00)\n",
      "Iter 1976 | Time 74.8490(74.9056) | Bit/dim 3.9926(3.9864) | Xent 0.5510(0.5743) | Loss 4.2681(4.2735) | Error 0.1965(0.2065) Steps 808(814.93) | Grad Norm 1.4534(1.3778) | Total Time 14.00(14.00)\n",
      "Iter 1977 | Time 73.2371(74.8555) | Bit/dim 3.9875(3.9864) | Xent 0.5554(0.5737) | Loss 4.2652(4.2733) | Error 0.2046(0.2065) Steps 820(815.08) | Grad Norm 1.3797(1.3778) | Total Time 14.00(14.00)\n",
      "Iter 1978 | Time 74.7658(74.8528) | Bit/dim 3.9811(3.9863) | Xent 0.5779(0.5739) | Loss 4.2700(4.2732) | Error 0.2105(0.2066) Steps 826(815.41) | Grad Norm 1.4002(1.3785) | Total Time 14.00(14.00)\n",
      "Iter 1979 | Time 76.8599(74.9130) | Bit/dim 3.9829(3.9862) | Xent 0.5602(0.5735) | Loss 4.2630(4.2729) | Error 0.1999(0.2064) Steps 814(815.37) | Grad Norm 1.3880(1.3788) | Total Time 14.00(14.00)\n",
      "Iter 1980 | Time 77.7125(74.9970) | Bit/dim 3.9808(3.9860) | Xent 0.5798(0.5737) | Loss 4.2707(4.2728) | Error 0.2090(0.2065) Steps 814(815.33) | Grad Norm 2.2825(1.4059) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0330 | Time 30.0316, Epoch Time 499.2339(496.1709), Bit/dim 3.9916(best: 3.9908), Xent 1.4331, Loss 4.7082, Error 0.4151(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1981 | Time 74.3090(74.9764) | Bit/dim 3.9856(3.9860) | Xent 0.5576(0.5732) | Loss 4.2644(4.2726) | Error 0.2009(0.2063) Steps 820(815.47) | Grad Norm 1.7500(1.4162) | Total Time 14.00(14.00)\n",
      "Iter 1982 | Time 74.0680(74.9491) | Bit/dim 3.9843(3.9859) | Xent 0.5666(0.5730) | Loss 4.2676(4.2724) | Error 0.2095(0.2064) Steps 820(815.60) | Grad Norm 1.1187(1.4073) | Total Time 14.00(14.00)\n",
      "Iter 1983 | Time 73.5709(74.9078) | Bit/dim 3.9794(3.9857) | Xent 0.5703(0.5729) | Loss 4.2645(4.2722) | Error 0.2086(0.2065) Steps 808(815.37) | Grad Norm 1.4527(1.4087) | Total Time 14.00(14.00)\n",
      "Iter 1984 | Time 75.3573(74.9213) | Bit/dim 3.9891(3.9858) | Xent 0.5755(0.5730) | Loss 4.2768(4.2723) | Error 0.2051(0.2064) Steps 808(815.15) | Grad Norm 1.9068(1.4236) | Total Time 14.00(14.00)\n",
      "Iter 1985 | Time 74.3147(74.9031) | Bit/dim 3.9766(3.9856) | Xent 0.5505(0.5723) | Loss 4.2519(4.2717) | Error 0.2006(0.2063) Steps 808(814.94) | Grad Norm 1.1296(1.4148) | Total Time 14.00(14.00)\n",
      "Iter 1986 | Time 76.3528(74.9466) | Bit/dim 3.9904(3.9857) | Xent 0.5736(0.5723) | Loss 4.2772(4.2719) | Error 0.2084(0.2063) Steps 832(815.45) | Grad Norm 1.1305(1.4063) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0331 | Time 29.9529, Epoch Time 493.6743(496.0960), Bit/dim 3.9923(best: 3.9908), Xent 1.4395, Loss 4.7121, Error 0.4134(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1987 | Time 73.6861(74.9088) | Bit/dim 3.9932(3.9859) | Xent 0.5774(0.5725) | Loss 4.2819(4.2722) | Error 0.2043(0.2063) Steps 814(815.41) | Grad Norm 0.9769(1.3934) | Total Time 14.00(14.00)\n",
      "Iter 1988 | Time 73.9165(74.8790) | Bit/dim 3.9877(3.9860) | Xent 0.5518(0.5719) | Loss 4.2636(4.2719) | Error 0.2009(0.2061) Steps 802(815.00) | Grad Norm 1.5156(1.3970) | Total Time 14.00(14.00)\n",
      "Iter 1989 | Time 74.1283(74.8565) | Bit/dim 3.9907(3.9861) | Xent 0.5612(0.5715) | Loss 4.2712(4.2719) | Error 0.2009(0.2059) Steps 826(815.33) | Grad Norm 1.2300(1.3920) | Total Time 14.00(14.00)\n",
      "Iter 1990 | Time 72.4002(74.7828) | Bit/dim 3.9767(3.9858) | Xent 0.5774(0.5717) | Loss 4.2654(4.2717) | Error 0.2133(0.2062) Steps 814(815.29) | Grad Norm 1.0281(1.3811) | Total Time 14.00(14.00)\n",
      "Iter 1991 | Time 72.4951(74.7141) | Bit/dim 3.9833(3.9858) | Xent 0.5538(0.5712) | Loss 4.2601(4.2714) | Error 0.1996(0.2060) Steps 820(815.44) | Grad Norm 1.2299(1.3766) | Total Time 14.00(14.00)\n",
      "Iter 1992 | Time 73.9131(74.6901) | Bit/dim 3.9767(3.9855) | Xent 0.5668(0.5710) | Loss 4.2601(4.2710) | Error 0.2079(0.2060) Steps 820(815.57) | Grad Norm 1.8042(1.3894) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0332 | Time 29.5678, Epoch Time 485.7016(495.7841), Bit/dim 3.9908(best: 3.9908), Xent 1.4284, Loss 4.7050, Error 0.4156(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1993 | Time 75.0107(74.6997) | Bit/dim 3.9741(3.9851) | Xent 0.5682(0.5710) | Loss 4.2582(4.2706) | Error 0.2086(0.2061) Steps 808(815.35) | Grad Norm 1.6890(1.3984) | Total Time 14.00(14.00)\n",
      "Iter 1994 | Time 73.7376(74.6709) | Bit/dim 3.9907(3.9853) | Xent 0.5457(0.5702) | Loss 4.2636(4.2704) | Error 0.1945(0.2057) Steps 820(815.48) | Grad Norm 0.9828(1.3859) | Total Time 14.00(14.00)\n",
      "Iter 1995 | Time 75.2974(74.6897) | Bit/dim 3.9927(3.9855) | Xent 0.5599(0.5699) | Loss 4.2727(4.2705) | Error 0.2053(0.2057) Steps 814(815.44) | Grad Norm 1.3066(1.3835) | Total Time 14.00(14.00)\n",
      "Iter 1996 | Time 76.1068(74.7322) | Bit/dim 3.9849(3.9855) | Xent 0.5633(0.5697) | Loss 4.2666(4.2704) | Error 0.2061(0.2057) Steps 820(815.58) | Grad Norm 1.1243(1.3758) | Total Time 14.00(14.00)\n",
      "Iter 1997 | Time 74.4260(74.7230) | Bit/dim 3.9872(3.9856) | Xent 0.5759(0.5699) | Loss 4.2752(4.2705) | Error 0.2106(0.2059) Steps 814(815.53) | Grad Norm 1.8424(1.3898) | Total Time 14.00(14.00)\n",
      "Iter 1998 | Time 76.6936(74.7821) | Bit/dim 3.9773(3.9853) | Xent 0.5683(0.5698) | Loss 4.2615(4.2702) | Error 0.2047(0.2059) Steps 826(815.84) | Grad Norm 1.3721(1.3892) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0333 | Time 29.8991, Epoch Time 496.6941(495.8114), Bit/dim 3.9916(best: 3.9908), Xent 1.4394, Loss 4.7112, Error 0.4174(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 1999 | Time 75.9071(74.8159) | Bit/dim 3.9832(3.9853) | Xent 0.5548(0.5694) | Loss 4.2606(4.2700) | Error 0.1996(0.2057) Steps 808(815.61) | Grad Norm 1.6878(1.3982) | Total Time 14.00(14.00)\n",
      "Iter 2000 | Time 75.8981(74.8483) | Bit/dim 3.9827(3.9852) | Xent 0.5624(0.5692) | Loss 4.2639(4.2698) | Error 0.2044(0.2056) Steps 814(815.56) | Grad Norm 1.0745(1.3885) | Total Time 14.00(14.00)\n",
      "Iter 2001 | Time 72.5324(74.7788) | Bit/dim 3.9792(3.9850) | Xent 0.5580(0.5688) | Loss 4.2581(4.2694) | Error 0.2006(0.2055) Steps 814(815.51) | Grad Norm 1.3691(1.3879) | Total Time 14.00(14.00)\n",
      "Iter 2002 | Time 76.2884(74.8241) | Bit/dim 3.9863(3.9850) | Xent 0.5691(0.5688) | Loss 4.2709(4.2695) | Error 0.2047(0.2055) Steps 820(815.65) | Grad Norm 1.4448(1.3896) | Total Time 14.00(14.00)\n",
      "Iter 2003 | Time 74.3166(74.8089) | Bit/dim 3.9879(3.9851) | Xent 0.5654(0.5687) | Loss 4.2706(4.2695) | Error 0.2011(0.2053) Steps 808(815.42) | Grad Norm 1.4378(1.3911) | Total Time 14.00(14.00)\n",
      "Iter 2004 | Time 74.4883(74.7993) | Bit/dim 3.9792(3.9849) | Xent 0.5599(0.5685) | Loss 4.2592(4.2692) | Error 0.1996(0.2052) Steps 808(815.20) | Grad Norm 1.0152(1.3798) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0334 | Time 29.7657, Epoch Time 494.7788(495.7805), Bit/dim 3.9902(best: 3.9908), Xent 1.4333, Loss 4.7068, Error 0.4143(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2005 | Time 73.7376(74.7674) | Bit/dim 3.9823(3.9849) | Xent 0.5590(0.5682) | Loss 4.2618(4.2690) | Error 0.2045(0.2051) Steps 814(815.16) | Grad Norm 1.3940(1.3802) | Total Time 14.00(14.00)\n",
      "Iter 2006 | Time 71.3221(74.6641) | Bit/dim 3.9829(3.9848) | Xent 0.5582(0.5679) | Loss 4.2621(4.2688) | Error 0.2027(0.2051) Steps 808(814.95) | Grad Norm 1.4356(1.3819) | Total Time 14.00(14.00)\n",
      "Iter 2007 | Time 75.4242(74.6869) | Bit/dim 3.9862(3.9849) | Xent 0.5429(0.5671) | Loss 4.2577(4.2684) | Error 0.1921(0.2047) Steps 820(815.10) | Grad Norm 2.2272(1.4072) | Total Time 14.00(14.00)\n",
      "Iter 2008 | Time 74.4090(74.6785) | Bit/dim 3.9804(3.9847) | Xent 0.5494(0.5666) | Loss 4.2551(4.2680) | Error 0.1981(0.2045) Steps 820(815.24) | Grad Norm 1.2863(1.4036) | Total Time 14.00(14.00)\n",
      "Iter 2009 | Time 72.5243(74.6139) | Bit/dim 3.9896(3.9849) | Xent 0.5747(0.5669) | Loss 4.2770(4.2683) | Error 0.2039(0.2045) Steps 808(815.03) | Grad Norm 1.6571(1.4112) | Total Time 14.00(14.00)\n",
      "Iter 2010 | Time 75.9539(74.6541) | Bit/dim 3.9776(3.9847) | Xent 0.5531(0.5664) | Loss 4.2541(4.2679) | Error 0.1984(0.2043) Steps 814(815.00) | Grad Norm 2.1715(1.4340) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0335 | Time 29.7439, Epoch Time 488.7787(495.5704), Bit/dim 3.9900(best: 3.9902), Xent 1.4468, Loss 4.7134, Error 0.4161(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2011 | Time 72.1633(74.5794) | Bit/dim 3.9865(3.9847) | Xent 0.5505(0.5660) | Loss 4.2618(4.2677) | Error 0.2027(0.2042) Steps 826(815.33) | Grad Norm 2.0981(1.4539) | Total Time 14.00(14.00)\n",
      "Iter 2012 | Time 75.6755(74.6123) | Bit/dim 3.9798(3.9846) | Xent 0.5699(0.5661) | Loss 4.2648(4.2676) | Error 0.2047(0.2043) Steps 808(815.11) | Grad Norm 1.1314(1.4443) | Total Time 14.00(14.00)\n",
      "Iter 2013 | Time 74.4111(74.6062) | Bit/dim 3.9796(3.9844) | Xent 0.5540(0.5657) | Loss 4.2566(4.2673) | Error 0.2013(0.2042) Steps 820(815.25) | Grad Norm 1.7497(1.4534) | Total Time 14.00(14.00)\n",
      "Iter 2014 | Time 73.4897(74.5727) | Bit/dim 3.9744(3.9841) | Xent 0.5539(0.5654) | Loss 4.2514(4.2668) | Error 0.2013(0.2041) Steps 826(815.58) | Grad Norm 1.8092(1.4641) | Total Time 14.00(14.00)\n",
      "Iter 2015 | Time 73.6879(74.5462) | Bit/dim 3.9842(3.9841) | Xent 0.5620(0.5653) | Loss 4.2652(4.2667) | Error 0.2029(0.2040) Steps 826(815.89) | Grad Norm 1.6345(1.4692) | Total Time 14.00(14.00)\n",
      "Iter 2016 | Time 75.8243(74.5845) | Bit/dim 3.9935(3.9844) | Xent 0.5669(0.5653) | Loss 4.2769(4.2671) | Error 0.2060(0.2041) Steps 820(816.01) | Grad Norm 1.5167(1.4706) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0336 | Time 29.9327, Epoch Time 490.8599(495.4291), Bit/dim 3.9894(best: 3.9900), Xent 1.4470, Loss 4.7130, Error 0.4182(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2017 | Time 73.9318(74.5650) | Bit/dim 3.9818(3.9843) | Xent 0.5663(0.5653) | Loss 4.2649(4.2670) | Error 0.2033(0.2041) Steps 820(816.13) | Grad Norm 1.2464(1.4639) | Total Time 14.00(14.00)\n",
      "Iter 2018 | Time 75.9573(74.6067) | Bit/dim 3.9891(3.9845) | Xent 0.5476(0.5648) | Loss 4.2629(4.2669) | Error 0.1930(0.2037) Steps 808(815.89) | Grad Norm 1.4665(1.4640) | Total Time 14.00(14.00)\n",
      "Iter 2019 | Time 74.9156(74.6160) | Bit/dim 3.9868(3.9845) | Xent 0.5528(0.5644) | Loss 4.2632(4.2668) | Error 0.1980(0.2036) Steps 820(816.01) | Grad Norm 1.8620(1.4759) | Total Time 14.00(14.00)\n",
      "Iter 2020 | Time 74.1660(74.6025) | Bit/dim 3.9654(3.9840) | Xent 0.5672(0.5645) | Loss 4.2489(4.2662) | Error 0.2059(0.2036) Steps 820(816.13) | Grad Norm 1.8999(1.4887) | Total Time 14.00(14.00)\n",
      "Iter 2021 | Time 76.8759(74.6707) | Bit/dim 3.9929(3.9842) | Xent 0.5553(0.5643) | Loss 4.2705(4.2663) | Error 0.2001(0.2035) Steps 820(816.25) | Grad Norm 1.1557(1.4787) | Total Time 14.00(14.00)\n",
      "Iter 2022 | Time 74.6513(74.6701) | Bit/dim 3.9846(3.9842) | Xent 0.5556(0.5640) | Loss 4.2623(4.2662) | Error 0.2060(0.2036) Steps 820(816.36) | Grad Norm 1.3385(1.4745) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0337 | Time 29.5708, Epoch Time 495.4254(495.4290), Bit/dim 3.9899(best: 3.9894), Xent 1.4522, Loss 4.7160, Error 0.4148(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2023 | Time 73.7306(74.6419) | Bit/dim 3.9925(3.9845) | Xent 0.5587(0.5638) | Loss 4.2719(4.2664) | Error 0.1981(0.2034) Steps 832(816.83) | Grad Norm 1.3882(1.4719) | Total Time 14.00(14.00)\n",
      "Iter 2024 | Time 73.3896(74.6044) | Bit/dim 3.9714(3.9841) | Xent 0.5459(0.5633) | Loss 4.2443(4.2657) | Error 0.1991(0.2033) Steps 820(816.92) | Grad Norm 1.1998(1.4637) | Total Time 14.00(14.00)\n",
      "Iter 2025 | Time 78.0472(74.7077) | Bit/dim 3.9797(3.9840) | Xent 0.5390(0.5626) | Loss 4.2492(4.2652) | Error 0.1996(0.2032) Steps 820(817.02) | Grad Norm 1.1521(1.4544) | Total Time 14.00(14.00)\n",
      "Iter 2026 | Time 71.0729(74.5986) | Bit/dim 3.9861(3.9840) | Xent 0.5561(0.5624) | Loss 4.2642(4.2652) | Error 0.1965(0.2030) Steps 808(816.75) | Grad Norm 1.5050(1.4559) | Total Time 14.00(14.00)\n",
      "Iter 2027 | Time 74.1975(74.5866) | Bit/dim 3.9827(3.9840) | Xent 0.5475(0.5619) | Loss 4.2565(4.2649) | Error 0.1983(0.2029) Steps 820(816.84) | Grad Norm 1.2423(1.4495) | Total Time 14.00(14.00)\n",
      "Iter 2028 | Time 71.8094(74.5033) | Bit/dim 3.9872(3.9841) | Xent 0.5669(0.5621) | Loss 4.2707(4.2651) | Error 0.2023(0.2028) Steps 814(816.76) | Grad Norm 0.9404(1.4342) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0338 | Time 29.9675, Epoch Time 487.7050(495.1973), Bit/dim 3.9887(best: 3.9894), Xent 1.4473, Loss 4.7123, Error 0.4164(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2029 | Time 77.1346(74.5822) | Bit/dim 3.9781(3.9839) | Xent 0.5407(0.5614) | Loss 4.2484(4.2646) | Error 0.1931(0.2025) Steps 814(816.67) | Grad Norm 0.9786(1.4205) | Total Time 14.00(14.00)\n",
      "Iter 2030 | Time 71.9943(74.5046) | Bit/dim 3.9850(3.9839) | Xent 0.5484(0.5610) | Loss 4.2592(4.2645) | Error 0.1989(0.2024) Steps 826(816.95) | Grad Norm 1.7728(1.4311) | Total Time 14.00(14.00)\n",
      "Iter 2031 | Time 74.3831(74.5009) | Bit/dim 3.9923(3.9842) | Xent 0.5433(0.5605) | Loss 4.2640(4.2644) | Error 0.1956(0.2022) Steps 814(816.87) | Grad Norm 1.3904(1.4299) | Total Time 14.00(14.00)\n",
      "Iter 2032 | Time 72.2070(74.4321) | Bit/dim 3.9794(3.9840) | Xent 0.5554(0.5604) | Loss 4.2571(4.2642) | Error 0.1969(0.2021) Steps 820(816.96) | Grad Norm 2.0029(1.4471) | Total Time 14.00(14.00)\n",
      "Iter 2033 | Time 75.6413(74.4684) | Bit/dim 3.9840(3.9840) | Xent 0.5616(0.5604) | Loss 4.2648(4.2642) | Error 0.2025(0.2021) Steps 808(816.69) | Grad Norm 1.1285(1.4375) | Total Time 14.00(14.00)\n",
      "Iter 2034 | Time 76.6094(74.5326) | Bit/dim 3.9801(3.9839) | Xent 0.5673(0.5606) | Loss 4.2638(4.2642) | Error 0.2034(0.2021) Steps 832(817.15) | Grad Norm 1.9868(1.4540) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0339 | Time 29.6687, Epoch Time 493.6689(495.1514), Bit/dim 3.9896(best: 3.9887), Xent 1.4632, Loss 4.7212, Error 0.4176(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2035 | Time 74.6811(74.5371) | Bit/dim 3.9782(3.9837) | Xent 0.5481(0.5602) | Loss 4.2523(4.2639) | Error 0.1973(0.2020) Steps 820(817.24) | Grad Norm 1.4356(1.4534) | Total Time 14.00(14.00)\n",
      "Iter 2036 | Time 74.9404(74.5492) | Bit/dim 3.9785(3.9836) | Xent 0.5421(0.5597) | Loss 4.2496(4.2634) | Error 0.1951(0.2018) Steps 808(816.96) | Grad Norm 1.3719(1.4510) | Total Time 14.00(14.00)\n",
      "Iter 2037 | Time 77.4647(74.6366) | Bit/dim 3.9841(3.9836) | Xent 0.5477(0.5593) | Loss 4.2579(4.2633) | Error 0.1954(0.2016) Steps 826(817.23) | Grad Norm 1.9757(1.4667) | Total Time 14.00(14.00)\n",
      "Iter 2038 | Time 73.6028(74.6056) | Bit/dim 3.9766(3.9834) | Xent 0.5471(0.5590) | Loss 4.2502(4.2629) | Error 0.2039(0.2016) Steps 814(817.13) | Grad Norm 1.6758(1.4730) | Total Time 14.00(14.00)\n",
      "Iter 2039 | Time 74.3555(74.5981) | Bit/dim 3.9849(3.9834) | Xent 0.5460(0.5586) | Loss 4.2579(4.2627) | Error 0.1961(0.2015) Steps 820(817.22) | Grad Norm 1.8365(1.4839) | Total Time 14.00(14.00)\n",
      "Iter 2040 | Time 74.4545(74.5938) | Bit/dim 3.9904(3.9837) | Xent 0.5518(0.5584) | Loss 4.2663(4.2628) | Error 0.2004(0.2014) Steps 820(817.30) | Grad Norm 2.8840(1.5259) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0340 | Time 30.0278, Epoch Time 495.1835(495.1524), Bit/dim 3.9891(best: 3.9887), Xent 1.4541, Loss 4.7161, Error 0.4152(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2041 | Time 74.8074(74.6002) | Bit/dim 3.9879(3.9838) | Xent 0.5511(0.5581) | Loss 4.2634(4.2629) | Error 0.1961(0.2013) Steps 826(817.56) | Grad Norm 2.0967(1.5430) | Total Time 14.00(14.00)\n",
      "Iter 2042 | Time 73.9903(74.5819) | Bit/dim 3.9779(3.9836) | Xent 0.5407(0.5576) | Loss 4.2483(4.2624) | Error 0.1974(0.2012) Steps 826(817.82) | Grad Norm 1.0508(1.5283) | Total Time 14.00(14.00)\n",
      "Iter 2043 | Time 77.1161(74.6579) | Bit/dim 3.9807(3.9835) | Xent 0.5551(0.5576) | Loss 4.2582(4.2623) | Error 0.1959(0.2010) Steps 820(817.88) | Grad Norm 2.1548(1.5471) | Total Time 14.00(14.00)\n",
      "Iter 2044 | Time 71.7266(74.5700) | Bit/dim 3.9896(3.9837) | Xent 0.5478(0.5573) | Loss 4.2635(4.2623) | Error 0.2021(0.2010) Steps 814(817.77) | Grad Norm 2.9498(1.5891) | Total Time 14.00(14.00)\n",
      "Iter 2045 | Time 76.4498(74.6264) | Bit/dim 3.9737(3.9834) | Xent 0.5593(0.5573) | Loss 4.2534(4.2621) | Error 0.2044(0.2011) Steps 820(817.83) | Grad Norm 1.0498(1.5730) | Total Time 14.00(14.00)\n",
      "Iter 2046 | Time 74.3489(74.6181) | Bit/dim 3.9779(3.9832) | Xent 0.5600(0.5574) | Loss 4.2579(4.2619) | Error 0.2039(0.2012) Steps 808(817.54) | Grad Norm 1.7673(1.5788) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0341 | Time 29.8455, Epoch Time 494.0790(495.1202), Bit/dim 3.9898(best: 3.9887), Xent 1.4617, Loss 4.7206, Error 0.4182(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2047 | Time 76.3636(74.6704) | Bit/dim 3.9801(3.9831) | Xent 0.5531(0.5573) | Loss 4.2567(4.2618) | Error 0.1961(0.2011) Steps 808(817.25) | Grad Norm 1.5317(1.5774) | Total Time 14.00(14.00)\n",
      "Iter 2048 | Time 78.0990(74.7733) | Bit/dim 3.9792(3.9830) | Xent 0.5439(0.5569) | Loss 4.2512(4.2615) | Error 0.1931(0.2008) Steps 814(817.15) | Grad Norm 1.3882(1.5717) | Total Time 14.00(14.00)\n",
      "Iter 2049 | Time 73.9966(74.7500) | Bit/dim 3.9759(3.9828) | Xent 0.5601(0.5570) | Loss 4.2559(4.2613) | Error 0.2029(0.2009) Steps 820(817.24) | Grad Norm 1.3467(1.5650) | Total Time 14.00(14.00)\n",
      "Iter 2050 | Time 74.6824(74.7480) | Bit/dim 3.9832(3.9828) | Xent 0.5422(0.5565) | Loss 4.2542(4.2611) | Error 0.1984(0.2008) Steps 808(816.96) | Grad Norm 1.5738(1.5652) | Total Time 14.00(14.00)\n",
      "Iter 2051 | Time 77.4988(74.8305) | Bit/dim 3.9813(3.9828) | Xent 0.5714(0.5570) | Loss 4.2671(4.2613) | Error 0.2029(0.2009) Steps 814(816.87) | Grad Norm 2.5919(1.5960) | Total Time 14.00(14.00)\n",
      "Iter 2052 | Time 74.8504(74.8311) | Bit/dim 3.9872(3.9829) | Xent 0.5443(0.5566) | Loss 4.2594(4.2612) | Error 0.1946(0.2007) Steps 814(816.79) | Grad Norm 1.4378(1.5913) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0342 | Time 29.8233, Epoch Time 501.2223(495.3032), Bit/dim 3.9878(best: 3.9887), Xent 1.4521, Loss 4.7139, Error 0.4181(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2053 | Time 75.8756(74.8624) | Bit/dim 3.9868(3.9830) | Xent 0.5691(0.5570) | Loss 4.2713(4.2615) | Error 0.2067(0.2009) Steps 814(816.70) | Grad Norm 2.1805(1.6090) | Total Time 14.00(14.00)\n",
      "Iter 2054 | Time 72.0890(74.7792) | Bit/dim 3.9838(3.9830) | Xent 0.5587(0.5570) | Loss 4.2631(4.2616) | Error 0.2007(0.2009) Steps 808(816.44) | Grad Norm 2.4140(1.6331) | Total Time 14.00(14.00)\n",
      "Iter 2055 | Time 71.4415(74.6791) | Bit/dim 3.9830(3.9830) | Xent 0.5355(0.5564) | Loss 4.2507(4.2612) | Error 0.1917(0.2006) Steps 826(816.73) | Grad Norm 3.3270(1.6839) | Total Time 14.00(14.00)\n",
      "Iter 2056 | Time 72.0698(74.6008) | Bit/dim 3.9759(3.9828) | Xent 0.5493(0.5562) | Loss 4.2505(4.2609) | Error 0.1969(0.2005) Steps 814(816.65) | Grad Norm 2.2045(1.6995) | Total Time 14.00(14.00)\n",
      "Iter 2057 | Time 73.1506(74.5573) | Bit/dim 3.9863(3.9829) | Xent 0.5475(0.5559) | Loss 4.2601(4.2609) | Error 0.1996(0.2005) Steps 802(816.21) | Grad Norm 3.7637(1.7615) | Total Time 14.00(14.00)\n",
      "Iter 2058 | Time 75.8842(74.5971) | Bit/dim 3.9802(3.9829) | Xent 0.5595(0.5560) | Loss 4.2599(4.2609) | Error 0.2057(0.2006) Steps 814(816.14) | Grad Norm 2.5512(1.7852) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0343 | Time 29.8201, Epoch Time 485.9882(495.0238), Bit/dim 3.9878(best: 3.9878), Xent 1.4637, Loss 4.7197, Error 0.4168(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2059 | Time 73.9618(74.5781) | Bit/dim 3.9768(3.9827) | Xent 0.5436(0.5556) | Loss 4.2486(4.2605) | Error 0.1997(0.2006) Steps 826(816.44) | Grad Norm 1.4277(1.7744) | Total Time 14.00(14.00)\n",
      "Iter 2060 | Time 74.0548(74.5624) | Bit/dim 3.9800(3.9826) | Xent 0.5362(0.5551) | Loss 4.2481(4.2601) | Error 0.1903(0.2003) Steps 820(816.54) | Grad Norm 2.5086(1.7965) | Total Time 14.00(14.00)\n",
      "Iter 2061 | Time 72.8342(74.5105) | Bit/dim 3.9766(3.9824) | Xent 0.5490(0.5549) | Loss 4.2510(4.2598) | Error 0.1980(0.2002) Steps 796(815.93) | Grad Norm 2.9683(1.8316) | Total Time 14.00(14.00)\n",
      "Iter 2062 | Time 75.8694(74.5513) | Bit/dim 3.9849(3.9825) | Xent 0.5485(0.5547) | Loss 4.2591(4.2598) | Error 0.2020(0.2003) Steps 808(815.69) | Grad Norm 1.2178(1.8132) | Total Time 14.00(14.00)\n",
      "Iter 2063 | Time 74.3130(74.5441) | Bit/dim 3.9907(3.9827) | Xent 0.5310(0.5540) | Loss 4.2562(4.2597) | Error 0.1910(0.2000) Steps 808(815.46) | Grad Norm 2.5078(1.8340) | Total Time 14.00(14.00)\n",
      "Iter 2064 | Time 72.9378(74.4959) | Bit/dim 3.9738(3.9825) | Xent 0.5548(0.5540) | Loss 4.2512(4.2595) | Error 0.1956(0.1999) Steps 814(815.42) | Grad Norm 1.6979(1.8299) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0344 | Time 29.7782, Epoch Time 489.2926(494.8518), Bit/dim 3.9875(best: 3.9878), Xent 1.4719, Loss 4.7234, Error 0.4183(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2065 | Time 76.4784(74.5554) | Bit/dim 3.9724(3.9822) | Xent 0.5468(0.5538) | Loss 4.2458(4.2590) | Error 0.2044(0.2000) Steps 820(815.55) | Grad Norm 1.3494(1.8155) | Total Time 14.00(14.00)\n",
      "Iter 2066 | Time 75.4045(74.5809) | Bit/dim 3.9827(3.9822) | Xent 0.5455(0.5535) | Loss 4.2554(4.2589) | Error 0.1936(0.1998) Steps 820(815.69) | Grad Norm 1.1714(1.7962) | Total Time 14.00(14.00)\n",
      "Iter 2067 | Time 72.3461(74.5138) | Bit/dim 3.9843(3.9822) | Xent 0.5450(0.5533) | Loss 4.2568(4.2589) | Error 0.1990(0.1998) Steps 814(815.64) | Grad Norm 1.3470(1.7827) | Total Time 14.00(14.00)\n",
      "Iter 2068 | Time 72.5347(74.4545) | Bit/dim 3.9826(3.9822) | Xent 0.5438(0.5530) | Loss 4.2545(4.2587) | Error 0.1930(0.1996) Steps 820(815.77) | Grad Norm 0.9840(1.7588) | Total Time 14.00(14.00)\n",
      "Iter 2069 | Time 72.9426(74.4091) | Bit/dim 3.9809(3.9822) | Xent 0.5642(0.5533) | Loss 4.2630(4.2589) | Error 0.2049(0.1997) Steps 814(815.71) | Grad Norm 1.3012(1.7450) | Total Time 14.00(14.00)\n",
      "Iter 2070 | Time 74.0230(74.3975) | Bit/dim 3.9831(3.9822) | Xent 0.5375(0.5529) | Loss 4.2519(4.2587) | Error 0.1959(0.1996) Steps 826(816.02) | Grad Norm 1.2117(1.7290) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0345 | Time 29.5316, Epoch Time 488.7893(494.6700), Bit/dim 3.9873(best: 3.9875), Xent 1.4758, Loss 4.7252, Error 0.4171(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2071 | Time 74.6629(74.4055) | Bit/dim 3.9854(3.9823) | Xent 0.5531(0.5529) | Loss 4.2619(4.2588) | Error 0.2007(0.1997) Steps 814(815.96) | Grad Norm 1.1690(1.7122) | Total Time 14.00(14.00)\n",
      "Iter 2072 | Time 74.3044(74.4025) | Bit/dim 3.9817(3.9823) | Xent 0.5404(0.5525) | Loss 4.2519(4.2585) | Error 0.2020(0.1997) Steps 820(816.08) | Grad Norm 1.3423(1.7011) | Total Time 14.00(14.00)\n",
      "Iter 2073 | Time 76.9926(74.4802) | Bit/dim 3.9756(3.9821) | Xent 0.5409(0.5521) | Loss 4.2460(4.2582) | Error 0.1949(0.1996) Steps 814(816.02) | Grad Norm 0.9244(1.6778) | Total Time 14.00(14.00)\n",
      "Iter 2074 | Time 72.5721(74.4229) | Bit/dim 3.9793(3.9820) | Xent 0.5160(0.5511) | Loss 4.2373(4.2575) | Error 0.1844(0.1991) Steps 820(816.14) | Grad Norm 1.1620(1.6624) | Total Time 14.00(14.00)\n",
      "Iter 2075 | Time 76.7648(74.4932) | Bit/dim 3.9852(3.9821) | Xent 0.5502(0.5510) | Loss 4.2603(4.2576) | Error 0.1965(0.1990) Steps 820(816.26) | Grad Norm 1.4003(1.6545) | Total Time 14.00(14.00)\n",
      "Iter 2076 | Time 77.3474(74.5788) | Bit/dim 3.9802(3.9821) | Xent 0.5580(0.5512) | Loss 4.2592(4.2577) | Error 0.2024(0.1991) Steps 820(816.37) | Grad Norm 1.3489(1.6453) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0346 | Time 29.8950, Epoch Time 498.1098(494.7732), Bit/dim 3.9876(best: 3.9873), Xent 1.4746, Loss 4.7249, Error 0.4146(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2077 | Time 74.5948(74.5793) | Bit/dim 3.9943(3.9824) | Xent 0.5400(0.5509) | Loss 4.2643(4.2579) | Error 0.1911(0.1989) Steps 814(816.30) | Grad Norm 1.2851(1.6345) | Total Time 14.00(14.00)\n",
      "Iter 2078 | Time 75.2125(74.5983) | Bit/dim 3.9760(3.9822) | Xent 0.5250(0.5501) | Loss 4.2384(4.2573) | Error 0.1891(0.1986) Steps 826(816.59) | Grad Norm 1.8337(1.6405) | Total Time 14.00(14.00)\n",
      "Iter 2079 | Time 75.7272(74.6321) | Bit/dim 3.9644(3.9817) | Xent 0.5645(0.5505) | Loss 4.2467(4.2570) | Error 0.1991(0.1986) Steps 826(816.87) | Grad Norm 1.0158(1.6218) | Total Time 14.00(14.00)\n",
      "Iter 2080 | Time 74.2653(74.6211) | Bit/dim 3.9850(3.9818) | Xent 0.5211(0.5497) | Loss 4.2455(4.2566) | Error 0.1853(0.1982) Steps 802(816.42) | Grad Norm 0.9479(1.6015) | Total Time 14.00(14.00)\n",
      "Iter 2081 | Time 76.6112(74.6808) | Bit/dim 3.9814(3.9818) | Xent 0.5534(0.5498) | Loss 4.2581(4.2567) | Error 0.2006(0.1983) Steps 814(816.35) | Grad Norm 1.3337(1.5935) | Total Time 14.00(14.00)\n",
      "Iter 2082 | Time 72.7615(74.6233) | Bit/dim 3.9851(3.9819) | Xent 0.5271(0.5491) | Loss 4.2486(4.2564) | Error 0.1870(0.1980) Steps 814(816.28) | Grad Norm 1.6357(1.5948) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0347 | Time 29.7515, Epoch Time 494.4817(494.7644), Bit/dim 3.9875(best: 3.9873), Xent 1.4873, Loss 4.7312, Error 0.4218(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2083 | Time 72.5505(74.5611) | Bit/dim 3.9822(3.9819) | Xent 0.5543(0.5493) | Loss 4.2594(4.2565) | Error 0.2014(0.1981) Steps 814(816.21) | Grad Norm 1.0322(1.5779) | Total Time 14.00(14.00)\n",
      "Iter 2084 | Time 74.2220(74.5509) | Bit/dim 3.9723(3.9816) | Xent 0.5492(0.5493) | Loss 4.2469(4.2562) | Error 0.1984(0.1981) Steps 814(816.15) | Grad Norm 1.1912(1.5663) | Total Time 14.00(14.00)\n",
      "Iter 2085 | Time 74.5176(74.5499) | Bit/dim 3.9785(3.9815) | Xent 0.5287(0.5486) | Loss 4.2428(4.2558) | Error 0.1896(0.1978) Steps 820(816.26) | Grad Norm 1.2788(1.5577) | Total Time 14.00(14.00)\n",
      "Iter 2086 | Time 74.6096(74.5517) | Bit/dim 3.9857(3.9816) | Xent 0.5264(0.5480) | Loss 4.2489(4.2556) | Error 0.1953(0.1977) Steps 808(816.01) | Grad Norm 2.8589(1.5967) | Total Time 14.00(14.00)\n",
      "Iter 2087 | Time 73.5233(74.5208) | Bit/dim 3.9663(3.9812) | Xent 0.5386(0.5477) | Loss 4.2356(4.2550) | Error 0.1957(0.1977) Steps 820(816.13) | Grad Norm 1.3501(1.5893) | Total Time 14.00(14.00)\n",
      "Iter 2088 | Time 76.0242(74.5659) | Bit/dim 3.9824(3.9812) | Xent 0.5407(0.5475) | Loss 4.2527(4.2549) | Error 0.1986(0.1977) Steps 808(815.89) | Grad Norm 1.6409(1.5909) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0348 | Time 29.7970, Epoch Time 490.8263(494.6463), Bit/dim 3.9864(best: 3.9873), Xent 1.4725, Loss 4.7227, Error 0.4205(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2089 | Time 76.5590(74.6257) | Bit/dim 3.9846(3.9813) | Xent 0.5268(0.5469) | Loss 4.2480(4.2547) | Error 0.1955(0.1976) Steps 826(816.19) | Grad Norm 1.8123(1.5975) | Total Time 14.00(14.00)\n",
      "Iter 2090 | Time 75.4701(74.6511) | Bit/dim 3.9746(3.9811) | Xent 0.5171(0.5460) | Loss 4.2332(4.2541) | Error 0.1849(0.1973) Steps 802(815.77) | Grad Norm 1.0967(1.5825) | Total Time 14.00(14.00)\n",
      "Iter 2091 | Time 71.1206(74.5452) | Bit/dim 3.9799(3.9811) | Xent 0.5303(0.5455) | Loss 4.2450(4.2538) | Error 0.1964(0.1972) Steps 826(816.07) | Grad Norm 1.4800(1.5794) | Total Time 14.00(14.00)\n",
      "Iter 2092 | Time 76.2085(74.5951) | Bit/dim 3.9894(3.9813) | Xent 0.5492(0.5456) | Loss 4.2640(4.2541) | Error 0.1951(0.1972) Steps 820(816.19) | Grad Norm 1.6541(1.5816) | Total Time 14.00(14.00)\n",
      "Iter 2093 | Time 71.9418(74.5155) | Bit/dim 3.9787(3.9812) | Xent 0.5470(0.5456) | Loss 4.2522(4.2541) | Error 0.1927(0.1970) Steps 808(815.95) | Grad Norm 1.3825(1.5757) | Total Time 14.00(14.00)\n",
      "Iter 2094 | Time 75.6002(74.5480) | Bit/dim 3.9704(3.9809) | Xent 0.5377(0.5454) | Loss 4.2392(4.2536) | Error 0.1955(0.1970) Steps 820(816.07) | Grad Norm 0.9822(1.5579) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0349 | Time 29.9066, Epoch Time 492.3209(494.5765), Bit/dim 3.9868(best: 3.9864), Xent 1.4860, Loss 4.7298, Error 0.4191(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2095 | Time 75.4945(74.5764) | Bit/dim 3.9724(3.9807) | Xent 0.5461(0.5454) | Loss 4.2454(4.2534) | Error 0.1986(0.1970) Steps 820(816.19) | Grad Norm 1.8984(1.5681) | Total Time 14.00(14.00)\n",
      "Iter 2096 | Time 74.7867(74.5827) | Bit/dim 3.9776(3.9806) | Xent 0.5169(0.5446) | Loss 4.2360(4.2529) | Error 0.1865(0.1967) Steps 814(816.12) | Grad Norm 1.5656(1.5680) | Total Time 14.00(14.00)\n",
      "Iter 2097 | Time 71.4133(74.4876) | Bit/dim 3.9746(3.9804) | Xent 0.5233(0.5439) | Loss 4.2362(4.2524) | Error 0.1887(0.1965) Steps 814(816.06) | Grad Norm 1.6627(1.5709) | Total Time 14.00(14.00)\n",
      "Iter 2098 | Time 75.2330(74.5100) | Bit/dim 3.9793(3.9804) | Xent 0.5530(0.5442) | Loss 4.2559(4.2525) | Error 0.1981(0.1965) Steps 826(816.35) | Grad Norm 1.7413(1.5760) | Total Time 14.00(14.00)\n",
      "Iter 2099 | Time 71.7938(74.4285) | Bit/dim 3.9781(3.9803) | Xent 0.5488(0.5443) | Loss 4.2526(4.2525) | Error 0.2017(0.1967) Steps 826(816.64) | Grad Norm 2.0725(1.5909) | Total Time 14.00(14.00)\n",
      "Iter 2100 | Time 75.0643(74.4476) | Bit/dim 3.9903(3.9806) | Xent 0.5570(0.5447) | Loss 4.2688(4.2530) | Error 0.2007(0.1968) Steps 826(816.92) | Grad Norm 1.4387(1.5863) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0350 | Time 30.0452, Epoch Time 489.3368(494.4193), Bit/dim 3.9869(best: 3.9864), Xent 1.4860, Loss 4.7299, Error 0.4177(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2101 | Time 76.2081(74.5004) | Bit/dim 3.9751(3.9804) | Xent 0.5356(0.5444) | Loss 4.2429(4.2527) | Error 0.1917(0.1967) Steps 820(817.02) | Grad Norm 1.3284(1.5786) | Total Time 14.00(14.00)\n",
      "Iter 2102 | Time 75.7421(74.5376) | Bit/dim 3.9782(3.9804) | Xent 0.5318(0.5441) | Loss 4.2441(4.2524) | Error 0.1904(0.1965) Steps 808(816.75) | Grad Norm 1.6068(1.5794) | Total Time 14.00(14.00)\n",
      "Iter 2103 | Time 74.1675(74.5265) | Bit/dim 3.9867(3.9806) | Xent 0.5264(0.5435) | Loss 4.2499(4.2523) | Error 0.1893(0.1963) Steps 826(817.02) | Grad Norm 1.7078(1.5833) | Total Time 14.00(14.00)\n",
      "Iter 2104 | Time 77.4455(74.6141) | Bit/dim 3.9764(3.9804) | Xent 0.5360(0.5433) | Loss 4.2444(4.2521) | Error 0.1909(0.1961) Steps 814(816.93) | Grad Norm 1.4061(1.5779) | Total Time 14.00(14.00)\n",
      "Iter 2105 | Time 72.5306(74.5516) | Bit/dim 3.9731(3.9802) | Xent 0.5376(0.5431) | Loss 4.2419(4.2518) | Error 0.1985(0.1962) Steps 814(816.85) | Grad Norm 1.7588(1.5834) | Total Time 14.00(14.00)\n",
      "Iter 2106 | Time 76.9896(74.6247) | Bit/dim 3.9777(3.9801) | Xent 0.5374(0.5430) | Loss 4.2464(4.2516) | Error 0.1946(0.1961) Steps 808(816.58) | Grad Norm 2.6971(1.6168) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0351 | Time 29.9653, Epoch Time 498.7603(494.5496), Bit/dim 3.9860(best: 3.9864), Xent 1.4870, Loss 4.7295, Error 0.4180(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2107 | Time 77.9892(74.7257) | Bit/dim 3.9716(3.9799) | Xent 0.5227(0.5424) | Loss 4.2329(4.2511) | Error 0.1886(0.1959) Steps 808(816.32) | Grad Norm 1.2943(1.6071) | Total Time 14.00(14.00)\n",
      "Iter 2108 | Time 75.1641(74.7388) | Bit/dim 3.9802(3.9799) | Xent 0.5462(0.5425) | Loss 4.2533(4.2511) | Error 0.2015(0.1961) Steps 820(816.43) | Grad Norm 2.4285(1.6317) | Total Time 14.00(14.00)\n",
      "Iter 2109 | Time 73.2806(74.6951) | Bit/dim 3.9803(3.9799) | Xent 0.5256(0.5420) | Loss 4.2431(4.2509) | Error 0.1895(0.1959) Steps 820(816.54) | Grad Norm 2.0669(1.6448) | Total Time 14.00(14.00)\n",
      "Iter 2110 | Time 73.7793(74.6676) | Bit/dim 3.9885(3.9802) | Xent 0.5129(0.5411) | Loss 4.2450(4.2507) | Error 0.1875(0.1956) Steps 814(816.46) | Grad Norm 1.8204(1.6501) | Total Time 14.00(14.00)\n",
      "Iter 2111 | Time 73.4280(74.6304) | Bit/dim 3.9748(3.9800) | Xent 0.5390(0.5410) | Loss 4.2443(4.2505) | Error 0.1946(0.1956) Steps 820(816.57) | Grad Norm 2.1407(1.6648) | Total Time 14.00(14.00)\n",
      "Iter 2112 | Time 74.7735(74.6347) | Bit/dim 3.9717(3.9797) | Xent 0.5396(0.5410) | Loss 4.2415(4.2502) | Error 0.1915(0.1955) Steps 814(816.49) | Grad Norm 1.5617(1.6617) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0352 | Time 30.2731, Epoch Time 494.3186(494.5426), Bit/dim 3.9861(best: 3.9860), Xent 1.4925, Loss 4.7323, Error 0.4143(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2113 | Time 73.9993(74.6156) | Bit/dim 3.9788(3.9797) | Xent 0.5365(0.5409) | Loss 4.2470(4.2501) | Error 0.1917(0.1954) Steps 802(816.06) | Grad Norm 1.6777(1.6622) | Total Time 14.00(14.00)\n",
      "Iter 2114 | Time 74.0767(74.5995) | Bit/dim 3.9753(3.9796) | Xent 0.5297(0.5405) | Loss 4.2401(4.2498) | Error 0.1927(0.1953) Steps 826(816.36) | Grad Norm 2.2677(1.6803) | Total Time 14.00(14.00)\n",
      "Iter 2115 | Time 73.7561(74.5742) | Bit/dim 3.9741(3.9794) | Xent 0.5514(0.5408) | Loss 4.2498(4.2498) | Error 0.2006(0.1954) Steps 820(816.47) | Grad Norm 1.8650(1.6859) | Total Time 14.00(14.00)\n",
      "Iter 2116 | Time 74.0941(74.5598) | Bit/dim 3.9773(3.9794) | Xent 0.5249(0.5404) | Loss 4.2397(4.2495) | Error 0.1913(0.1953) Steps 826(816.75) | Grad Norm 1.1690(1.6704) | Total Time 14.00(14.00)\n",
      "Iter 2117 | Time 74.0451(74.5443) | Bit/dim 3.9796(3.9794) | Xent 0.5204(0.5398) | Loss 4.2398(4.2493) | Error 0.1839(0.1950) Steps 820(816.85) | Grad Norm 2.5664(1.6973) | Total Time 14.00(14.00)\n",
      "Iter 2118 | Time 75.2516(74.5656) | Bit/dim 3.9827(3.9795) | Xent 0.5419(0.5398) | Loss 4.2537(4.2494) | Error 0.1927(0.1949) Steps 814(816.76) | Grad Norm 1.7133(1.6977) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0353 | Time 29.5174, Epoch Time 490.2698(494.4144), Bit/dim 3.9847(best: 3.9860), Xent 1.5003, Loss 4.7348, Error 0.4167(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2119 | Time 75.2923(74.5874) | Bit/dim 3.9681(3.9791) | Xent 0.5401(0.5398) | Loss 4.2381(4.2490) | Error 0.1946(0.1949) Steps 808(816.50) | Grad Norm 1.4253(1.6896) | Total Time 14.00(14.00)\n",
      "Iter 2120 | Time 72.5231(74.5254) | Bit/dim 3.9821(3.9792) | Xent 0.5245(0.5394) | Loss 4.2443(4.2489) | Error 0.1887(0.1947) Steps 808(816.25) | Grad Norm 1.9130(1.6963) | Total Time 14.00(14.00)\n",
      "Iter 2121 | Time 74.3259(74.5194) | Bit/dim 3.9783(3.9792) | Xent 0.5158(0.5387) | Loss 4.2362(4.2485) | Error 0.1883(0.1945) Steps 808(816.00) | Grad Norm 1.6952(1.6962) | Total Time 14.00(14.00)\n",
      "Iter 2122 | Time 73.9778(74.5032) | Bit/dim 3.9695(3.9789) | Xent 0.5389(0.5387) | Loss 4.2390(4.2482) | Error 0.1953(0.1945) Steps 814(815.94) | Grad Norm 1.5215(1.6910) | Total Time 14.00(14.00)\n",
      "Iter 2123 | Time 72.4436(74.4414) | Bit/dim 3.9767(3.9788) | Xent 0.5395(0.5387) | Loss 4.2464(4.2482) | Error 0.1947(0.1945) Steps 826(816.24) | Grad Norm 1.0193(1.6708) | Total Time 14.00(14.00)\n",
      "Iter 2124 | Time 71.2383(74.3453) | Bit/dim 3.9917(3.9792) | Xent 0.5188(0.5381) | Loss 4.2511(4.2483) | Error 0.1861(0.1943) Steps 808(815.99) | Grad Norm 1.5302(1.6666) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0354 | Time 29.7976, Epoch Time 485.0865(494.1346), Bit/dim 3.9853(best: 3.9847), Xent 1.4862, Loss 4.7284, Error 0.4181(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2125 | Time 76.0232(74.3956) | Bit/dim 3.9817(3.9793) | Xent 0.5188(0.5375) | Loss 4.2411(4.2481) | Error 0.1850(0.1940) Steps 814(815.93) | Grad Norm 1.2765(1.6549) | Total Time 14.00(14.00)\n",
      "Iter 2126 | Time 76.6379(74.4629) | Bit/dim 3.9689(3.9790) | Xent 0.5169(0.5369) | Loss 4.2273(4.2474) | Error 0.1900(0.1939) Steps 826(816.24) | Grad Norm 1.5190(1.6508) | Total Time 14.00(14.00)\n",
      "Iter 2127 | Time 74.4089(74.4613) | Bit/dim 3.9735(3.9788) | Xent 0.5274(0.5366) | Loss 4.2372(4.2471) | Error 0.1926(0.1939) Steps 820(816.35) | Grad Norm 1.5314(1.6473) | Total Time 14.00(14.00)\n",
      "Iter 2128 | Time 76.6450(74.5268) | Bit/dim 3.9810(3.9789) | Xent 0.5251(0.5363) | Loss 4.2436(4.2470) | Error 0.1873(0.1937) Steps 808(816.10) | Grad Norm 1.2356(1.6349) | Total Time 14.00(14.00)\n",
      "Iter 2129 | Time 74.9074(74.5382) | Bit/dim 3.9822(3.9790) | Xent 0.5397(0.5364) | Loss 4.2520(4.2472) | Error 0.1903(0.1936) Steps 820(816.21) | Grad Norm 1.5258(1.6316) | Total Time 14.00(14.00)\n",
      "Iter 2130 | Time 75.3922(74.5638) | Bit/dim 3.9775(3.9789) | Xent 0.5284(0.5361) | Loss 4.2417(4.2470) | Error 0.1894(0.1934) Steps 808(815.97) | Grad Norm 1.3584(1.6234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0355 | Time 29.6198, Epoch Time 499.1883(494.2862), Bit/dim 3.9856(best: 3.9847), Xent 1.4987, Loss 4.7349, Error 0.4168(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2131 | Time 71.7763(74.4802) | Bit/dim 3.9891(3.9792) | Xent 0.5246(0.5358) | Loss 4.2514(4.2471) | Error 0.1890(0.1933) Steps 820(816.09) | Grad Norm 1.2554(1.6124) | Total Time 14.00(14.00)\n",
      "Iter 2132 | Time 74.1850(74.4714) | Bit/dim 3.9761(3.9791) | Xent 0.5271(0.5355) | Loss 4.2397(4.2469) | Error 0.1920(0.1933) Steps 814(816.03) | Grad Norm 1.3662(1.6050) | Total Time 14.00(14.00)\n",
      "Iter 2133 | Time 75.3421(74.4975) | Bit/dim 3.9729(3.9790) | Xent 0.5291(0.5353) | Loss 4.2375(4.2466) | Error 0.1921(0.1932) Steps 820(816.15) | Grad Norm 1.5219(1.6025) | Total Time 14.00(14.00)\n",
      "Iter 2134 | Time 75.9590(74.5413) | Bit/dim 3.9763(3.9789) | Xent 0.5279(0.5351) | Loss 4.2403(4.2464) | Error 0.1911(0.1932) Steps 814(816.08) | Grad Norm 0.9055(1.5816) | Total Time 14.00(14.00)\n",
      "Iter 2135 | Time 74.7978(74.5490) | Bit/dim 3.9761(3.9788) | Xent 0.5384(0.5352) | Loss 4.2453(4.2464) | Error 0.1961(0.1932) Steps 808(815.84) | Grad Norm 1.9222(1.5918) | Total Time 14.00(14.00)\n",
      "Iter 2136 | Time 74.1660(74.5375) | Bit/dim 3.9753(3.9787) | Xent 0.5337(0.5352) | Loss 4.2422(4.2463) | Error 0.1923(0.1932) Steps 820(815.96) | Grad Norm 2.2142(1.6105) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0356 | Time 29.5842, Epoch Time 491.2782(494.1960), Bit/dim 3.9842(best: 3.9847), Xent 1.5063, Loss 4.7374, Error 0.4219(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2137 | Time 76.1797(74.5868) | Bit/dim 3.9811(3.9788) | Xent 0.5112(0.5345) | Loss 4.2367(4.2460) | Error 0.1857(0.1930) Steps 814(815.90) | Grad Norm 1.1307(1.5961) | Total Time 14.00(14.00)\n",
      "Iter 2138 | Time 72.8657(74.5352) | Bit/dim 3.9791(3.9788) | Xent 0.5389(0.5346) | Loss 4.2485(4.2461) | Error 0.1873(0.1928) Steps 814(815.85) | Grad Norm 2.1371(1.6123) | Total Time 14.00(14.00)\n",
      "Iter 2139 | Time 79.2906(74.6778) | Bit/dim 3.9770(3.9787) | Xent 0.5313(0.5345) | Loss 4.2426(4.2460) | Error 0.1880(0.1927) Steps 814(815.79) | Grad Norm 1.2292(1.6008) | Total Time 14.00(14.00)\n",
      "Iter 2140 | Time 76.1309(74.7214) | Bit/dim 3.9726(3.9785) | Xent 0.5384(0.5346) | Loss 4.2418(4.2458) | Error 0.1970(0.1928) Steps 814(815.74) | Grad Norm 2.1186(1.6164) | Total Time 14.00(14.00)\n",
      "Iter 2141 | Time 73.9931(74.6996) | Bit/dim 3.9657(3.9781) | Xent 0.5297(0.5345) | Loss 4.2305(4.2454) | Error 0.1929(0.1928) Steps 814(815.69) | Grad Norm 1.4338(1.6109) | Total Time 14.00(14.00)\n",
      "Iter 2142 | Time 76.2744(74.7468) | Bit/dim 3.9856(3.9784) | Xent 0.5171(0.5339) | Loss 4.2441(4.2453) | Error 0.1803(0.1924) Steps 814(815.64) | Grad Norm 2.0063(1.6228) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0357 | Time 29.8646, Epoch Time 500.3245(494.3798), Bit/dim 3.9852(best: 3.9842), Xent 1.5089, Loss 4.7397, Error 0.4188(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2143 | Time 74.8869(74.7510) | Bit/dim 3.9751(3.9783) | Xent 0.5340(0.5339) | Loss 4.2421(4.2452) | Error 0.1936(0.1925) Steps 814(815.59) | Grad Norm 1.3680(1.6151) | Total Time 14.00(14.00)\n",
      "Iter 2144 | Time 74.9836(74.7580) | Bit/dim 3.9774(3.9782) | Xent 0.5276(0.5338) | Loss 4.2412(4.2451) | Error 0.1910(0.1924) Steps 814(815.54) | Grad Norm 1.0685(1.5987) | Total Time 14.00(14.00)\n",
      "Iter 2145 | Time 72.8358(74.7003) | Bit/dim 3.9782(3.9782) | Xent 0.5274(0.5336) | Loss 4.2419(4.2450) | Error 0.1933(0.1924) Steps 826(815.85) | Grad Norm 2.1577(1.6155) | Total Time 14.00(14.00)\n",
      "Iter 2146 | Time 74.7503(74.7018) | Bit/dim 3.9824(3.9784) | Xent 0.5091(0.5328) | Loss 4.2369(4.2448) | Error 0.1796(0.1921) Steps 814(815.80) | Grad Norm 1.4375(1.6101) | Total Time 14.00(14.00)\n",
      "Iter 2147 | Time 73.6006(74.6688) | Bit/dim 3.9703(3.9781) | Xent 0.5294(0.5327) | Loss 4.2350(4.2445) | Error 0.1879(0.1919) Steps 820(815.92) | Grad Norm 1.8144(1.6163) | Total Time 14.00(14.00)\n",
      "Iter 2148 | Time 74.3210(74.6584) | Bit/dim 3.9812(3.9782) | Xent 0.5300(0.5326) | Loss 4.2462(4.2445) | Error 0.1940(0.1920) Steps 820(816.05) | Grad Norm 1.4204(1.6104) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0358 | Time 29.8243, Epoch Time 490.8534(494.2740), Bit/dim 3.9847(best: 3.9842), Xent 1.5153, Loss 4.7424, Error 0.4164(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2149 | Time 73.3190(74.6182) | Bit/dim 3.9861(3.9785) | Xent 0.5427(0.5329) | Loss 4.2575(4.2449) | Error 0.1937(0.1921) Steps 808(815.80) | Grad Norm 1.3521(1.6026) | Total Time 14.00(14.00)\n",
      "Iter 2150 | Time 74.7282(74.6215) | Bit/dim 3.9739(3.9783) | Xent 0.5290(0.5328) | Loss 4.2383(4.2447) | Error 0.1909(0.1920) Steps 802(815.39) | Grad Norm 1.6887(1.6052) | Total Time 14.00(14.00)\n",
      "Iter 2151 | Time 75.9700(74.6619) | Bit/dim 3.9817(3.9784) | Xent 0.5147(0.5323) | Loss 4.2391(4.2446) | Error 0.1894(0.1919) Steps 820(815.53) | Grad Norm 1.2724(1.5952) | Total Time 14.00(14.00)\n",
      "Iter 2152 | Time 78.4958(74.7769) | Bit/dim 3.9733(3.9783) | Xent 0.5253(0.5321) | Loss 4.2359(4.2443) | Error 0.1889(0.1918) Steps 820(815.66) | Grad Norm 2.4753(1.6216) | Total Time 14.00(14.00)\n",
      "Iter 2153 | Time 72.5011(74.7087) | Bit/dim 3.9629(3.9778) | Xent 0.5331(0.5321) | Loss 4.2295(4.2439) | Error 0.1970(0.1920) Steps 820(815.79) | Grad Norm 1.3803(1.6144) | Total Time 14.00(14.00)\n",
      "Iter 2154 | Time 74.8639(74.7133) | Bit/dim 3.9839(3.9780) | Xent 0.5094(0.5314) | Loss 4.2386(4.2437) | Error 0.1847(0.1918) Steps 826(816.10) | Grad Norm 1.5517(1.6125) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0359 | Time 29.5442, Epoch Time 495.2553(494.3035), Bit/dim 3.9842(best: 3.9842), Xent 1.5160, Loss 4.7422, Error 0.4195(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2155 | Time 74.0225(74.6926) | Bit/dim 3.9768(3.9780) | Xent 0.5241(0.5312) | Loss 4.2389(4.2436) | Error 0.1843(0.1916) Steps 826(816.40) | Grad Norm 1.7577(1.6169) | Total Time 14.00(14.00)\n",
      "Iter 2156 | Time 73.8046(74.6660) | Bit/dim 3.9805(3.9780) | Xent 0.5227(0.5310) | Loss 4.2419(4.2435) | Error 0.1910(0.1915) Steps 814(816.32) | Grad Norm 1.7902(1.6221) | Total Time 14.00(14.00)\n",
      "Iter 2157 | Time 74.3773(74.6573) | Bit/dim 3.9789(3.9781) | Xent 0.5164(0.5305) | Loss 4.2371(4.2433) | Error 0.1909(0.1915) Steps 814(816.25) | Grad Norm 3.0183(1.6640) | Total Time 14.00(14.00)\n",
      "Iter 2158 | Time 74.5688(74.6546) | Bit/dim 3.9744(3.9779) | Xent 0.5376(0.5307) | Loss 4.2432(4.2433) | Error 0.1951(0.1916) Steps 814(816.19) | Grad Norm 2.5711(1.6912) | Total Time 14.00(14.00)\n",
      "Iter 2159 | Time 75.0117(74.6654) | Bit/dim 3.9671(3.9776) | Xent 0.5179(0.5303) | Loss 4.2260(4.2428) | Error 0.1873(0.1915) Steps 814(816.12) | Grad Norm 2.8891(1.7271) | Total Time 14.00(14.00)\n",
      "Iter 2160 | Time 75.5630(74.6923) | Bit/dim 3.9783(3.9776) | Xent 0.5138(0.5298) | Loss 4.2352(4.2426) | Error 0.1826(0.1912) Steps 820(816.24) | Grad Norm 2.4802(1.7497) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0360 | Time 29.9568, Epoch Time 492.9753(494.2636), Bit/dim 3.9834(best: 3.9842), Xent 1.5181, Loss 4.7424, Error 0.4218(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2161 | Time 72.0164(74.6120) | Bit/dim 3.9846(3.9778) | Xent 0.5088(0.5292) | Loss 4.2390(4.2425) | Error 0.1810(0.1909) Steps 802(815.81) | Grad Norm 1.6469(1.7466) | Total Time 14.00(14.00)\n",
      "Iter 2162 | Time 74.2303(74.6006) | Bit/dim 3.9748(3.9778) | Xent 0.5164(0.5288) | Loss 4.2330(4.2422) | Error 0.1899(0.1909) Steps 814(815.76) | Grad Norm 4.1422(1.8185) | Total Time 14.00(14.00)\n",
      "Iter 2163 | Time 76.4945(74.6574) | Bit/dim 3.9736(3.9776) | Xent 0.5166(0.5285) | Loss 4.2319(4.2419) | Error 0.1833(0.1907) Steps 808(815.52) | Grad Norm 1.5556(1.8106) | Total Time 14.00(14.00)\n",
      "Iter 2164 | Time 75.5269(74.6835) | Bit/dim 3.9670(3.9773) | Xent 0.5124(0.5280) | Loss 4.2232(4.2413) | Error 0.1867(0.1905) Steps 820(815.66) | Grad Norm 2.4398(1.8295) | Total Time 14.00(14.00)\n",
      "Iter 2165 | Time 73.3767(74.6443) | Bit/dim 3.9751(3.9772) | Xent 0.5287(0.5280) | Loss 4.2395(4.2413) | Error 0.1935(0.1906) Steps 808(815.43) | Grad Norm 4.0506(1.8961) | Total Time 14.00(14.00)\n",
      "Iter 2166 | Time 74.5364(74.6410) | Bit/dim 3.9765(3.9772) | Xent 0.5155(0.5276) | Loss 4.2342(4.2410) | Error 0.1861(0.1905) Steps 820(815.57) | Grad Norm 1.2760(1.8775) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0361 | Time 29.8744, Epoch Time 492.0482(494.1972), Bit/dim 3.9838(best: 3.9834), Xent 1.5204, Loss 4.7440, Error 0.4186(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2167 | Time 74.9910(74.6515) | Bit/dim 3.9748(3.9772) | Xent 0.5374(0.5279) | Loss 4.2435(4.2411) | Error 0.1987(0.1907) Steps 814(815.52) | Grad Norm 3.5859(1.9288) | Total Time 14.00(14.00)\n",
      "Iter 2168 | Time 74.1644(74.6369) | Bit/dim 3.9909(3.9776) | Xent 0.5125(0.5275) | Loss 4.2472(4.2413) | Error 0.1859(0.1906) Steps 820(815.65) | Grad Norm 3.3038(1.9700) | Total Time 14.00(14.00)\n",
      "Iter 2169 | Time 75.2838(74.6563) | Bit/dim 3.9758(3.9775) | Xent 0.5240(0.5274) | Loss 4.2378(4.2412) | Error 0.1894(0.1906) Steps 826(815.96) | Grad Norm 2.5585(1.9877) | Total Time 14.00(14.00)\n",
      "Iter 2170 | Time 73.7187(74.6282) | Bit/dim 3.9808(3.9776) | Xent 0.5230(0.5272) | Loss 4.2423(4.2412) | Error 0.1874(0.1905) Steps 814(815.90) | Grad Norm 2.9155(2.0155) | Total Time 14.00(14.00)\n",
      "Iter 2171 | Time 75.1896(74.6450) | Bit/dim 3.9601(3.9771) | Xent 0.5173(0.5269) | Loss 4.2188(4.2406) | Error 0.1866(0.1904) Steps 820(816.03) | Grad Norm 1.6862(2.0056) | Total Time 14.00(14.00)\n",
      "Iter 2172 | Time 75.0922(74.6584) | Bit/dim 3.9736(3.9770) | Xent 0.5052(0.5263) | Loss 4.2262(4.2401) | Error 0.1813(0.1901) Steps 814(815.97) | Grad Norm 2.1329(2.0094) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0362 | Time 29.7594, Epoch Time 493.7429(494.1835), Bit/dim 3.9843(best: 3.9834), Xent 1.5334, Loss 4.7510, Error 0.4212(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2173 | Time 74.6212(74.6573) | Bit/dim 3.9828(3.9772) | Xent 0.5343(0.5265) | Loss 4.2499(4.2404) | Error 0.1860(0.1900) Steps 796(815.37) | Grad Norm 1.6181(1.9977) | Total Time 14.00(14.00)\n",
      "Iter 2174 | Time 74.3258(74.6474) | Bit/dim 3.9725(3.9770) | Xent 0.5192(0.5263) | Loss 4.2320(4.2402) | Error 0.1847(0.1898) Steps 820(815.51) | Grad Norm 1.7263(1.9896) | Total Time 14.00(14.00)\n",
      "Iter 2175 | Time 74.8563(74.6536) | Bit/dim 3.9648(3.9766) | Xent 0.5135(0.5259) | Loss 4.2215(4.2396) | Error 0.1877(0.1897) Steps 814(815.46) | Grad Norm 3.1078(2.0231) | Total Time 14.00(14.00)\n",
      "Iter 2176 | Time 76.2367(74.7011) | Bit/dim 3.9792(3.9767) | Xent 0.5019(0.5252) | Loss 4.2301(4.2393) | Error 0.1785(0.1894) Steps 820(815.60) | Grad Norm 1.0998(1.9954) | Total Time 14.00(14.00)\n",
      "Iter 2177 | Time 72.3707(74.6312) | Bit/dim 3.9804(3.9768) | Xent 0.5149(0.5249) | Loss 4.2378(4.2393) | Error 0.1897(0.1894) Steps 820(815.73) | Grad Norm 1.9362(1.9936) | Total Time 14.00(14.00)\n",
      "Iter 2178 | Time 74.8941(74.6391) | Bit/dim 3.9759(3.9768) | Xent 0.5159(0.5246) | Loss 4.2338(4.2391) | Error 0.1904(0.1894) Steps 808(815.50) | Grad Norm 2.5934(2.0116) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0363 | Time 29.5691, Epoch Time 492.3883(494.1297), Bit/dim 3.9822(best: 3.9834), Xent 1.5062, Loss 4.7353, Error 0.4195(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2179 | Time 73.9462(74.6183) | Bit/dim 3.9761(3.9768) | Xent 0.5256(0.5246) | Loss 4.2389(4.2391) | Error 0.1903(0.1895) Steps 796(814.91) | Grad Norm 2.0398(2.0125) | Total Time 14.00(14.00)\n",
      "Iter 2180 | Time 76.6909(74.6805) | Bit/dim 3.9829(3.9770) | Xent 0.5167(0.5244) | Loss 4.2412(4.2392) | Error 0.1854(0.1893) Steps 826(815.25) | Grad Norm 3.3790(2.0535) | Total Time 14.00(14.00)\n",
      "Iter 2181 | Time 77.4247(74.7628) | Bit/dim 3.9670(3.9767) | Xent 0.4989(0.5236) | Loss 4.2165(4.2385) | Error 0.1781(0.1890) Steps 814(815.21) | Grad Norm 1.2816(2.0303) | Total Time 14.00(14.00)\n",
      "Iter 2182 | Time 75.7616(74.7928) | Bit/dim 3.9751(3.9766) | Xent 0.5211(0.5236) | Loss 4.2357(4.2384) | Error 0.1867(0.1889) Steps 802(814.81) | Grad Norm 3.1608(2.0642) | Total Time 14.00(14.00)\n",
      "Iter 2183 | Time 76.2494(74.8365) | Bit/dim 3.9708(3.9764) | Xent 0.5129(0.5232) | Loss 4.2273(4.2381) | Error 0.1871(0.1889) Steps 814(814.79) | Grad Norm 2.3000(2.0713) | Total Time 14.00(14.00)\n",
      "Iter 2184 | Time 74.6643(74.8313) | Bit/dim 3.9779(3.9765) | Xent 0.5263(0.5233) | Loss 4.2411(4.2382) | Error 0.1927(0.1890) Steps 820(814.94) | Grad Norm 1.8909(2.0659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0364 | Time 29.4620, Epoch Time 499.8685(494.3018), Bit/dim 3.9815(best: 3.9822), Xent 1.5210, Loss 4.7420, Error 0.4195(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2185 | Time 72.0934(74.7492) | Bit/dim 3.9590(3.9760) | Xent 0.5287(0.5235) | Loss 4.2234(4.2377) | Error 0.1926(0.1891) Steps 820(815.10) | Grad Norm 3.0529(2.0955) | Total Time 14.00(14.00)\n",
      "Iter 2186 | Time 74.5033(74.7418) | Bit/dim 3.9801(3.9761) | Xent 0.5062(0.5230) | Loss 4.2332(4.2376) | Error 0.1843(0.1890) Steps 814(815.06) | Grad Norm 1.1345(2.0667) | Total Time 14.00(14.00)\n",
      "Iter 2187 | Time 72.5515(74.6761) | Bit/dim 3.9681(3.9759) | Xent 0.5124(0.5227) | Loss 4.2243(4.2372) | Error 0.1841(0.1888) Steps 808(814.85) | Grad Norm 1.8901(2.0614) | Total Time 14.00(14.00)\n",
      "Iter 2188 | Time 74.0597(74.6576) | Bit/dim 3.9837(3.9761) | Xent 0.5035(0.5221) | Loss 4.2354(4.2371) | Error 0.1805(0.1886) Steps 814(814.83) | Grad Norm 1.7912(2.0533) | Total Time 14.00(14.00)\n",
      "Iter 2189 | Time 75.8898(74.6946) | Bit/dim 3.9785(3.9762) | Xent 0.5123(0.5218) | Loss 4.2347(4.2371) | Error 0.1839(0.1884) Steps 820(814.98) | Grad Norm 1.4763(2.0359) | Total Time 14.00(14.00)\n",
      "Iter 2190 | Time 76.0986(74.7367) | Bit/dim 3.9805(3.9763) | Xent 0.5068(0.5213) | Loss 4.2339(4.2370) | Error 0.1833(0.1883) Steps 802(814.59) | Grad Norm 1.8421(2.0301) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0365 | Time 29.8837, Epoch Time 490.7169(494.1943), Bit/dim 3.9829(best: 3.9815), Xent 1.5418, Loss 4.7538, Error 0.4172(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2191 | Time 77.1695(74.8097) | Bit/dim 3.9785(3.9764) | Xent 0.5088(0.5210) | Loss 4.2329(4.2368) | Error 0.1801(0.1880) Steps 802(814.21) | Grad Norm 1.5889(2.0169) | Total Time 14.00(14.00)\n",
      "Iter 2192 | Time 73.4652(74.7693) | Bit/dim 3.9748(3.9763) | Xent 0.4979(0.5203) | Loss 4.2237(4.2364) | Error 0.1847(0.1879) Steps 820(814.39) | Grad Norm 1.8633(2.0123) | Total Time 14.00(14.00)\n",
      "Iter 2193 | Time 76.0793(74.8086) | Bit/dim 3.9766(3.9763) | Xent 0.5269(0.5205) | Loss 4.2401(4.2366) | Error 0.1897(0.1880) Steps 808(814.20) | Grad Norm 1.8812(2.0084) | Total Time 14.00(14.00)\n",
      "Iter 2194 | Time 76.6724(74.8646) | Bit/dim 3.9702(3.9761) | Xent 0.5325(0.5208) | Loss 4.2364(4.2365) | Error 0.1973(0.1883) Steps 826(814.55) | Grad Norm 2.5107(2.0234) | Total Time 14.00(14.00)\n",
      "Iter 2195 | Time 76.4281(74.9115) | Bit/dim 3.9752(3.9761) | Xent 0.5116(0.5206) | Loss 4.2310(4.2364) | Error 0.1883(0.1883) Steps 790(813.81) | Grad Norm 1.8695(2.0188) | Total Time 14.00(14.00)\n",
      "Iter 2196 | Time 75.3195(74.9237) | Bit/dim 3.9732(3.9760) | Xent 0.5011(0.5200) | Loss 4.2237(4.2360) | Error 0.1790(0.1880) Steps 796(813.28) | Grad Norm 2.0948(2.0211) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0366 | Time 29.7414, Epoch Time 500.5351(494.3845), Bit/dim 3.9825(best: 3.9815), Xent 1.5350, Loss 4.7500, Error 0.4220(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2197 | Time 76.3853(74.9676) | Bit/dim 3.9716(3.9759) | Xent 0.5166(0.5199) | Loss 4.2299(4.2358) | Error 0.1877(0.1880) Steps 814(813.30) | Grad Norm 2.3123(2.0298) | Total Time 14.00(14.00)\n",
      "Iter 2198 | Time 74.6346(74.9576) | Bit/dim 3.9698(3.9757) | Xent 0.5095(0.5196) | Loss 4.2245(4.2355) | Error 0.1831(0.1878) Steps 814(813.32) | Grad Norm 1.4622(2.0128) | Total Time 14.00(14.00)\n",
      "Iter 2199 | Time 76.8671(75.0148) | Bit/dim 3.9731(3.9756) | Xent 0.5089(0.5192) | Loss 4.2276(4.2352) | Error 0.1813(0.1876) Steps 796(812.80) | Grad Norm 3.1226(2.0461) | Total Time 14.00(14.00)\n",
      "Iter 2200 | Time 75.9303(75.0423) | Bit/dim 3.9756(3.9756) | Xent 0.5134(0.5191) | Loss 4.2323(4.2352) | Error 0.1873(0.1876) Steps 814(812.84) | Grad Norm 1.3122(2.0241) | Total Time 14.00(14.00)\n",
      "Iter 2201 | Time 73.4239(74.9938) | Bit/dim 3.9812(3.9758) | Xent 0.5064(0.5187) | Loss 4.2344(4.2351) | Error 0.1794(0.1874) Steps 820(813.05) | Grad Norm 2.2080(2.0296) | Total Time 14.00(14.00)\n",
      "Iter 2202 | Time 72.5075(74.9192) | Bit/dim 3.9716(3.9757) | Xent 0.5104(0.5184) | Loss 4.2268(4.2349) | Error 0.1827(0.1872) Steps 814(813.08) | Grad Norm 3.5898(2.0764) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0367 | Time 29.9977, Epoch Time 495.4813(494.4174), Bit/dim 3.9819(best: 3.9815), Xent 1.5437, Loss 4.7537, Error 0.4233(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2203 | Time 72.0168(74.8321) | Bit/dim 3.9781(3.9757) | Xent 0.5173(0.5184) | Loss 4.2368(4.2349) | Error 0.1865(0.1872) Steps 820(813.29) | Grad Norm 1.3050(2.0533) | Total Time 14.00(14.00)\n",
      "Iter 2204 | Time 77.7768(74.9204) | Bit/dim 3.9711(3.9756) | Xent 0.5180(0.5184) | Loss 4.2301(4.2348) | Error 0.1861(0.1872) Steps 814(813.31) | Grad Norm 2.8823(2.0781) | Total Time 14.00(14.00)\n",
      "Iter 2205 | Time 75.7473(74.9452) | Bit/dim 3.9734(3.9755) | Xent 0.4942(0.5177) | Loss 4.2205(4.2344) | Error 0.1763(0.1869) Steps 826(813.69) | Grad Norm 2.4372(2.0889) | Total Time 14.00(14.00)\n",
      "Iter 2206 | Time 75.9202(74.9745) | Bit/dim 3.9701(3.9754) | Xent 0.5007(0.5172) | Loss 4.2205(4.2340) | Error 0.1834(0.1867) Steps 820(813.88) | Grad Norm 1.2560(2.0639) | Total Time 14.00(14.00)\n",
      "Iter 2207 | Time 74.3432(74.9556) | Bit/dim 3.9768(3.9754) | Xent 0.5090(0.5169) | Loss 4.2313(4.2339) | Error 0.1824(0.1866) Steps 820(814.06) | Grad Norm 1.6716(2.0521) | Total Time 14.00(14.00)\n",
      "Iter 2208 | Time 75.9649(74.9858) | Bit/dim 3.9813(3.9756) | Xent 0.5226(0.5171) | Loss 4.2426(4.2341) | Error 0.1869(0.1866) Steps 814(814.06) | Grad Norm 2.0320(2.0515) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0368 | Time 29.7873, Epoch Time 497.5888(494.5126), Bit/dim 3.9817(best: 3.9815), Xent 1.5290, Loss 4.7462, Error 0.4170(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2209 | Time 74.3617(74.9671) | Bit/dim 3.9759(3.9756) | Xent 0.4999(0.5166) | Loss 4.2258(4.2339) | Error 0.1789(0.1864) Steps 796(813.52) | Grad Norm 1.4583(2.0337) | Total Time 14.00(14.00)\n",
      "Iter 2210 | Time 74.3572(74.9488) | Bit/dim 3.9784(3.9757) | Xent 0.5214(0.5167) | Loss 4.2391(4.2340) | Error 0.1855(0.1864) Steps 814(813.53) | Grad Norm 3.2560(2.0704) | Total Time 14.00(14.00)\n",
      "Iter 2211 | Time 74.3654(74.9313) | Bit/dim 3.9712(3.9755) | Xent 0.5255(0.5170) | Loss 4.2339(4.2340) | Error 0.1890(0.1864) Steps 814(813.55) | Grad Norm 1.7234(2.0600) | Total Time 14.00(14.00)\n",
      "Iter 2212 | Time 74.3962(74.9153) | Bit/dim 3.9722(3.9754) | Xent 0.5089(0.5167) | Loss 4.2266(4.2338) | Error 0.1879(0.1865) Steps 820(813.74) | Grad Norm 1.9950(2.0581) | Total Time 14.00(14.00)\n",
      "Iter 2213 | Time 75.1796(74.9232) | Bit/dim 3.9763(3.9755) | Xent 0.5017(0.5163) | Loss 4.2272(4.2336) | Error 0.1827(0.1864) Steps 814(813.75) | Grad Norm 1.9931(2.0561) | Total Time 14.00(14.00)\n",
      "Iter 2214 | Time 74.9736(74.9247) | Bit/dim 3.9729(3.9754) | Xent 0.4982(0.5157) | Loss 4.2220(4.2333) | Error 0.1767(0.1861) Steps 814(813.76) | Grad Norm 2.0600(2.0562) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0369 | Time 29.8154, Epoch Time 493.0422(494.4685), Bit/dim 3.9807(best: 3.9815), Xent 1.5375, Loss 4.7495, Error 0.4207(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2215 | Time 73.8977(74.8939) | Bit/dim 3.9787(3.9755) | Xent 0.5093(0.5155) | Loss 4.2333(4.2333) | Error 0.1803(0.1859) Steps 802(813.40) | Grad Norm 2.2543(2.0622) | Total Time 14.00(14.00)\n",
      "Iter 2216 | Time 75.4739(74.9113) | Bit/dim 3.9666(3.9752) | Xent 0.5014(0.5151) | Loss 4.2173(4.2328) | Error 0.1789(0.1857) Steps 826(813.78) | Grad Norm 1.2656(2.0383) | Total Time 14.00(14.00)\n",
      "Iter 2217 | Time 75.1044(74.9171) | Bit/dim 3.9759(3.9753) | Xent 0.5119(0.5150) | Loss 4.2319(4.2328) | Error 0.1827(0.1856) Steps 814(813.79) | Grad Norm 1.8702(2.0332) | Total Time 14.00(14.00)\n",
      "Iter 2218 | Time 75.5519(74.9361) | Bit/dim 3.9763(3.9753) | Xent 0.5044(0.5147) | Loss 4.2285(4.2326) | Error 0.1859(0.1856) Steps 814(813.79) | Grad Norm 1.7533(2.0248) | Total Time 14.00(14.00)\n",
      "Iter 2219 | Time 77.0807(75.0005) | Bit/dim 3.9723(3.9752) | Xent 0.5071(0.5145) | Loss 4.2258(4.2324) | Error 0.1816(0.1855) Steps 814(813.80) | Grad Norm 1.9491(2.0226) | Total Time 14.00(14.00)\n",
      "Iter 2220 | Time 72.2725(74.9186) | Bit/dim 3.9766(3.9752) | Xent 0.4978(0.5140) | Loss 4.2255(4.2322) | Error 0.1810(0.1854) Steps 802(813.45) | Grad Norm 2.4679(2.0359) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0370 | Time 29.9550, Epoch Time 495.0248(494.4851), Bit/dim 3.9811(best: 3.9807), Xent 1.5427, Loss 4.7525, Error 0.4182(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2221 | Time 74.3945(74.9029) | Bit/dim 3.9749(3.9752) | Xent 0.5018(0.5136) | Loss 4.2258(4.2320) | Error 0.1745(0.1850) Steps 808(813.28) | Grad Norm 1.8498(2.0303) | Total Time 14.00(14.00)\n",
      "Iter 2222 | Time 74.8879(74.9025) | Bit/dim 3.9786(3.9753) | Xent 0.5170(0.5137) | Loss 4.2371(4.2322) | Error 0.1936(0.1853) Steps 802(812.95) | Grad Norm 1.6817(2.0199) | Total Time 14.00(14.00)\n",
      "Iter 2223 | Time 72.2983(74.8243) | Bit/dim 3.9716(3.9752) | Xent 0.4881(0.5129) | Loss 4.2156(4.2317) | Error 0.1756(0.1850) Steps 820(813.16) | Grad Norm 2.1602(2.0241) | Total Time 14.00(14.00)\n",
      "Iter 2224 | Time 74.7799(74.8230) | Bit/dim 3.9677(3.9750) | Xent 0.5205(0.5132) | Loss 4.2280(4.2316) | Error 0.1877(0.1851) Steps 814(813.18) | Grad Norm 2.2453(2.0307) | Total Time 14.00(14.00)\n",
      "Iter 2225 | Time 74.2432(74.8056) | Bit/dim 3.9725(3.9749) | Xent 0.4857(0.5123) | Loss 4.2154(4.2311) | Error 0.1787(0.1849) Steps 832(813.75) | Grad Norm 1.9034(2.0269) | Total Time 14.00(14.00)\n",
      "Iter 2226 | Time 74.2657(74.7894) | Bit/dim 3.9767(3.9750) | Xent 0.5139(0.5124) | Loss 4.2337(4.2312) | Error 0.1855(0.1849) Steps 820(813.93) | Grad Norm 1.9171(2.0236) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0371 | Time 30.0859, Epoch Time 490.5726(494.3678), Bit/dim 3.9809(best: 3.9807), Xent 1.5499, Loss 4.7559, Error 0.4219(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2227 | Time 73.8232(74.7604) | Bit/dim 3.9785(3.9751) | Xent 0.5043(0.5122) | Loss 4.2306(4.2312) | Error 0.1791(0.1847) Steps 802(813.58) | Grad Norm 1.3464(2.0033) | Total Time 14.00(14.00)\n",
      "Iter 2228 | Time 74.2828(74.7461) | Bit/dim 3.9737(3.9750) | Xent 0.4903(0.5115) | Loss 4.2189(4.2308) | Error 0.1787(0.1846) Steps 808(813.41) | Grad Norm 1.2481(1.9806) | Total Time 14.00(14.00)\n",
      "Iter 2229 | Time 75.7033(74.7748) | Bit/dim 3.9724(3.9750) | Xent 0.5133(0.5115) | Loss 4.2290(4.2307) | Error 0.1795(0.1844) Steps 808(813.25) | Grad Norm 1.9516(1.9798) | Total Time 14.00(14.00)\n",
      "Iter 2230 | Time 74.8599(74.7774) | Bit/dim 3.9689(3.9748) | Xent 0.4990(0.5112) | Loss 4.2184(4.2304) | Error 0.1836(0.1844) Steps 808(813.09) | Grad Norm 1.2599(1.9582) | Total Time 14.00(14.00)\n",
      "Iter 2231 | Time 73.8886(74.7507) | Bit/dim 3.9711(3.9747) | Xent 0.5080(0.5111) | Loss 4.2251(4.2302) | Error 0.1850(0.1844) Steps 820(813.30) | Grad Norm 1.0675(1.9314) | Total Time 14.00(14.00)\n",
      "Iter 2232 | Time 74.2534(74.7358) | Bit/dim 3.9691(3.9745) | Xent 0.4885(0.5104) | Loss 4.2134(4.2297) | Error 0.1825(0.1843) Steps 820(813.50) | Grad Norm 2.2992(1.9425) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0372 | Time 29.9163, Epoch Time 492.3320(494.3067), Bit/dim 3.9798(best: 3.9807), Xent 1.5547, Loss 4.7571, Error 0.4210(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2233 | Time 72.1063(74.6569) | Bit/dim 3.9722(3.9744) | Xent 0.5099(0.5104) | Loss 4.2271(4.2296) | Error 0.1853(0.1844) Steps 808(813.33) | Grad Norm 1.5012(1.9292) | Total Time 14.00(14.00)\n",
      "Iter 2234 | Time 73.7585(74.6299) | Bit/dim 3.9805(3.9746) | Xent 0.4794(0.5095) | Loss 4.2202(4.2293) | Error 0.1764(0.1841) Steps 808(813.17) | Grad Norm 1.2627(1.9092) | Total Time 14.00(14.00)\n",
      "Iter 2235 | Time 72.1317(74.5550) | Bit/dim 3.9611(3.9742) | Xent 0.5136(0.5096) | Loss 4.2179(4.2290) | Error 0.1911(0.1843) Steps 820(813.38) | Grad Norm 1.0245(1.8827) | Total Time 14.00(14.00)\n",
      "Iter 2236 | Time 74.4239(74.5511) | Bit/dim 3.9742(3.9742) | Xent 0.4967(0.5092) | Loss 4.2226(4.2288) | Error 0.1805(0.1842) Steps 802(813.04) | Grad Norm 1.3320(1.8662) | Total Time 14.00(14.00)\n",
      "Iter 2237 | Time 75.0090(74.5648) | Bit/dim 3.9644(3.9739) | Xent 0.4971(0.5088) | Loss 4.2129(4.2283) | Error 0.1813(0.1841) Steps 820(813.25) | Grad Norm 1.4634(1.8541) | Total Time 14.00(14.00)\n",
      "Iter 2238 | Time 73.1970(74.5238) | Bit/dim 3.9815(3.9741) | Xent 0.4871(0.5082) | Loss 4.2251(4.2282) | Error 0.1754(0.1839) Steps 808(813.09) | Grad Norm 1.6980(1.8494) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0373 | Time 29.8303, Epoch Time 485.9913(494.0572), Bit/dim 3.9822(best: 3.9798), Xent 1.5722, Loss 4.7683, Error 0.4241(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2239 | Time 72.4084(74.4603) | Bit/dim 3.9678(3.9740) | Xent 0.4983(0.5079) | Loss 4.2169(4.2279) | Error 0.1774(0.1837) Steps 814(813.12) | Grad Norm 2.4509(1.8675) | Total Time 14.00(14.00)\n",
      "Iter 2240 | Time 74.9863(74.4761) | Bit/dim 3.9745(3.9740) | Xent 0.4941(0.5075) | Loss 4.2216(4.2277) | Error 0.1787(0.1835) Steps 802(812.78) | Grad Norm 1.5325(1.8574) | Total Time 14.00(14.00)\n",
      "Iter 2241 | Time 74.9109(74.4891) | Bit/dim 3.9711(3.9739) | Xent 0.4869(0.5068) | Loss 4.2145(4.2273) | Error 0.1810(0.1835) Steps 802(812.46) | Grad Norm 2.4290(1.8746) | Total Time 14.00(14.00)\n",
      "Iter 2242 | Time 75.3379(74.5146) | Bit/dim 3.9779(3.9740) | Xent 0.4987(0.5066) | Loss 4.2272(4.2273) | Error 0.1819(0.1834) Steps 808(812.32) | Grad Norm 1.1437(1.8526) | Total Time 14.00(14.00)\n",
      "Iter 2243 | Time 72.2325(74.4461) | Bit/dim 3.9754(3.9740) | Xent 0.5045(0.5065) | Loss 4.2276(4.2273) | Error 0.1834(0.1834) Steps 802(812.01) | Grad Norm 1.7109(1.8484) | Total Time 14.00(14.00)\n",
      "Iter 2244 | Time 74.1175(74.4363) | Bit/dim 3.9691(3.9739) | Xent 0.4945(0.5062) | Loss 4.2164(4.2270) | Error 0.1737(0.1831) Steps 808(811.89) | Grad Norm 1.2757(1.8312) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0374 | Time 29.8892, Epoch Time 489.2606(493.9133), Bit/dim 3.9804(best: 3.9798), Xent 1.5682, Loss 4.7646, Error 0.4195(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2245 | Time 73.7740(74.4164) | Bit/dim 3.9734(3.9739) | Xent 0.4974(0.5059) | Loss 4.2221(4.2268) | Error 0.1784(0.1830) Steps 808(811.78) | Grad Norm 1.7585(1.8290) | Total Time 14.00(14.00)\n",
      "Iter 2246 | Time 73.7632(74.3968) | Bit/dim 3.9654(3.9736) | Xent 0.4960(0.5056) | Loss 4.2134(4.2264) | Error 0.1776(0.1828) Steps 814(811.84) | Grad Norm 1.4056(1.8163) | Total Time 14.00(14.00)\n",
      "Iter 2247 | Time 73.0496(74.3564) | Bit/dim 3.9693(3.9735) | Xent 0.5073(0.5057) | Loss 4.2230(4.2263) | Error 0.1843(0.1829) Steps 802(811.55) | Grad Norm 2.8896(1.8485) | Total Time 14.00(14.00)\n",
      "Iter 2248 | Time 73.6830(74.3362) | Bit/dim 3.9739(3.9735) | Xent 0.4972(0.5054) | Loss 4.2225(4.2262) | Error 0.1767(0.1827) Steps 814(811.62) | Grad Norm 1.9885(1.8527) | Total Time 14.00(14.00)\n",
      "Iter 2249 | Time 75.7475(74.3785) | Bit/dim 3.9768(3.9736) | Xent 0.4972(0.5052) | Loss 4.2255(4.2262) | Error 0.1799(0.1826) Steps 814(811.69) | Grad Norm 2.4334(1.8701) | Total Time 14.00(14.00)\n",
      "Iter 2250 | Time 74.4386(74.3803) | Bit/dim 3.9697(3.9735) | Xent 0.4940(0.5048) | Loss 4.2167(4.2259) | Error 0.1765(0.1824) Steps 796(811.22) | Grad Norm 1.1367(1.8481) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0375 | Time 29.6455, Epoch Time 489.6828(493.7864), Bit/dim 3.9794(best: 3.9798), Xent 1.5605, Loss 4.7597, Error 0.4231(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2251 | Time 75.8379(74.4241) | Bit/dim 3.9814(3.9737) | Xent 0.4850(0.5042) | Loss 4.2239(4.2259) | Error 0.1817(0.1824) Steps 802(810.95) | Grad Norm 2.0329(1.8537) | Total Time 14.00(14.00)\n",
      "Iter 2252 | Time 76.1938(74.4772) | Bit/dim 3.9651(3.9735) | Xent 0.5059(0.5043) | Loss 4.2180(4.2256) | Error 0.1834(0.1824) Steps 802(810.68) | Grad Norm 1.8152(1.8525) | Total Time 14.00(14.00)\n",
      "Iter 2253 | Time 73.8014(74.4569) | Bit/dim 3.9659(3.9732) | Xent 0.5149(0.5046) | Loss 4.2234(4.2255) | Error 0.1865(0.1825) Steps 814(810.78) | Grad Norm 2.9631(1.8858) | Total Time 14.00(14.00)\n",
      "Iter 2254 | Time 74.6987(74.4641) | Bit/dim 3.9719(3.9732) | Xent 0.4963(0.5044) | Loss 4.2201(4.2254) | Error 0.1829(0.1826) Steps 814(810.87) | Grad Norm 2.4318(1.9022) | Total Time 14.00(14.00)\n",
      "Iter 2255 | Time 75.1590(74.4850) | Bit/dim 3.9846(3.9735) | Xent 0.4788(0.5036) | Loss 4.2240(4.2253) | Error 0.1715(0.1822) Steps 808(810.79) | Grad Norm 2.0680(1.9072) | Total Time 14.00(14.00)\n",
      "Iter 2256 | Time 75.1147(74.5039) | Bit/dim 3.9607(3.9732) | Xent 0.5136(0.5039) | Loss 4.2175(4.2251) | Error 0.1836(0.1823) Steps 814(810.88) | Grad Norm 2.7759(1.9333) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0376 | Time 29.6923, Epoch Time 496.1131(493.8562), Bit/dim 3.9797(best: 3.9794), Xent 1.5632, Loss 4.7613, Error 0.4224(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2257 | Time 73.5234(74.4745) | Bit/dim 3.9684(3.9730) | Xent 0.4920(0.5035) | Loss 4.2144(4.2248) | Error 0.1817(0.1822) Steps 796(810.44) | Grad Norm 1.8241(1.9300) | Total Time 14.00(14.00)\n",
      "Iter 2258 | Time 74.6965(74.4811) | Bit/dim 3.9716(3.9730) | Xent 0.4784(0.5028) | Loss 4.2108(4.2244) | Error 0.1704(0.1819) Steps 802(810.18) | Grad Norm 1.5445(1.9184) | Total Time 14.00(14.00)\n",
      "Iter 2259 | Time 74.8567(74.4924) | Bit/dim 3.9802(3.9732) | Xent 0.5022(0.5028) | Loss 4.2313(4.2246) | Error 0.1809(0.1819) Steps 808(810.12) | Grad Norm 2.2823(1.9293) | Total Time 14.00(14.00)\n",
      "Iter 2260 | Time 72.1450(74.4220) | Bit/dim 3.9656(3.9730) | Xent 0.4946(0.5025) | Loss 4.2129(4.2242) | Error 0.1791(0.1818) Steps 808(810.06) | Grad Norm 2.2643(1.9394) | Total Time 14.00(14.00)\n",
      "Iter 2261 | Time 77.3599(74.5101) | Bit/dim 3.9706(3.9729) | Xent 0.4973(0.5024) | Loss 4.2192(4.2241) | Error 0.1756(0.1816) Steps 808(809.99) | Grad Norm 0.9651(1.9101) | Total Time 14.00(14.00)\n",
      "Iter 2262 | Time 76.2673(74.5628) | Bit/dim 3.9736(3.9729) | Xent 0.4956(0.5022) | Loss 4.2214(4.2240) | Error 0.1800(0.1815) Steps 796(809.57) | Grad Norm 1.3247(1.8926) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0377 | Time 29.7792, Epoch Time 494.2416(493.8678), Bit/dim 3.9794(best: 3.9794), Xent 1.5745, Loss 4.7667, Error 0.4243(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2263 | Time 75.6790(74.5963) | Bit/dim 3.9681(3.9728) | Xent 0.5119(0.5025) | Loss 4.2240(4.2240) | Error 0.1853(0.1817) Steps 808(809.53) | Grad Norm 2.1134(1.8992) | Total Time 14.00(14.00)\n",
      "Iter 2264 | Time 74.2087(74.5847) | Bit/dim 3.9659(3.9726) | Xent 0.4936(0.5022) | Loss 4.2127(4.2237) | Error 0.1800(0.1816) Steps 802(809.30) | Grad Norm 1.5658(1.8892) | Total Time 14.00(14.00)\n",
      "Iter 2265 | Time 73.0490(74.5386) | Bit/dim 3.9757(3.9727) | Xent 0.4981(0.5021) | Loss 4.2247(4.2237) | Error 0.1764(0.1815) Steps 808(809.26) | Grad Norm 2.2549(1.9002) | Total Time 14.00(14.00)\n",
      "Iter 2266 | Time 71.8207(74.4571) | Bit/dim 3.9644(3.9724) | Xent 0.4927(0.5018) | Loss 4.2107(4.2233) | Error 0.1783(0.1814) Steps 802(809.04) | Grad Norm 1.3904(1.8849) | Total Time 14.00(14.00)\n",
      "Iter 2267 | Time 72.9467(74.4118) | Bit/dim 3.9798(3.9726) | Xent 0.4816(0.5012) | Loss 4.2206(4.2232) | Error 0.1763(0.1812) Steps 808(809.01) | Grad Norm 1.2839(1.8669) | Total Time 14.00(14.00)\n",
      "Iter 2268 | Time 74.7002(74.4204) | Bit/dim 3.9714(3.9726) | Xent 0.5106(0.5015) | Loss 4.2267(4.2233) | Error 0.1865(0.1814) Steps 796(808.62) | Grad Norm 1.8115(1.8652) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0378 | Time 29.7742, Epoch Time 487.7517(493.6843), Bit/dim 3.9795(best: 3.9794), Xent 1.5905, Loss 4.7747, Error 0.4249(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2269 | Time 75.4757(74.4521) | Bit/dim 3.9739(3.9726) | Xent 0.5029(0.5015) | Loss 4.2254(4.2234) | Error 0.1807(0.1813) Steps 808(808.60) | Grad Norm 1.4527(1.8528) | Total Time 14.00(14.00)\n",
      "Iter 2270 | Time 72.9976(74.4084) | Bit/dim 3.9654(3.9724) | Xent 0.4870(0.5011) | Loss 4.2090(4.2230) | Error 0.1729(0.1811) Steps 814(808.77) | Grad Norm 2.1444(1.8616) | Total Time 14.00(14.00)\n",
      "Iter 2271 | Time 76.4234(74.4689) | Bit/dim 3.9723(3.9724) | Xent 0.4843(0.5006) | Loss 4.2144(4.2227) | Error 0.1760(0.1809) Steps 802(808.56) | Grad Norm 1.4036(1.8478) | Total Time 14.00(14.00)\n",
      "Iter 2272 | Time 74.6687(74.4749) | Bit/dim 3.9722(3.9724) | Xent 0.5035(0.5007) | Loss 4.2240(4.2227) | Error 0.1825(0.1810) Steps 814(808.73) | Grad Norm 2.1140(1.8558) | Total Time 14.00(14.00)\n",
      "Iter 2273 | Time 73.7664(74.4536) | Bit/dim 3.9637(3.9721) | Xent 0.4811(0.5001) | Loss 4.2042(4.2222) | Error 0.1783(0.1809) Steps 808(808.70) | Grad Norm 2.2729(1.8683) | Total Time 14.00(14.00)\n",
      "Iter 2274 | Time 74.0073(74.4402) | Bit/dim 3.9790(3.9724) | Xent 0.4896(0.4998) | Loss 4.2238(4.2222) | Error 0.1710(0.1806) Steps 820(809.04) | Grad Norm 2.9069(1.8995) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0379 | Time 29.8975, Epoch Time 492.9223(493.6614), Bit/dim 3.9785(best: 3.9794), Xent 1.5726, Loss 4.7648, Error 0.4235(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2275 | Time 76.5111(74.5024) | Bit/dim 3.9648(3.9721) | Xent 0.4896(0.4995) | Loss 4.2096(4.2219) | Error 0.1751(0.1804) Steps 814(809.19) | Grad Norm 2.4638(1.9164) | Total Time 14.00(14.00)\n",
      "Iter 2276 | Time 77.5868(74.5949) | Bit/dim 3.9665(3.9720) | Xent 0.5036(0.4996) | Loss 4.2183(4.2217) | Error 0.1843(0.1806) Steps 802(808.98) | Grad Norm 1.9858(1.9185) | Total Time 14.00(14.00)\n",
      "Iter 2277 | Time 72.9644(74.5460) | Bit/dim 3.9794(3.9722) | Xent 0.5025(0.4997) | Loss 4.2306(4.2220) | Error 0.1830(0.1806) Steps 808(808.95) | Grad Norm 3.1710(1.9561) | Total Time 14.00(14.00)\n",
      "Iter 2278 | Time 75.5581(74.5763) | Bit/dim 3.9717(3.9722) | Xent 0.4753(0.4989) | Loss 4.2094(4.2216) | Error 0.1690(0.1803) Steps 820(809.28) | Grad Norm 1.1067(1.9306) | Total Time 14.00(14.00)\n",
      "Iter 2279 | Time 73.1963(74.5349) | Bit/dim 3.9721(3.9722) | Xent 0.4957(0.4988) | Loss 4.2200(4.2216) | Error 0.1811(0.1803) Steps 802(809.06) | Grad Norm 3.2092(1.9689) | Total Time 14.00(14.00)\n",
      "Iter 2280 | Time 74.6038(74.5370) | Bit/dim 3.9711(3.9721) | Xent 0.4854(0.4984) | Loss 4.2138(4.2214) | Error 0.1783(0.1802) Steps 826(809.57) | Grad Norm 1.3288(1.9497) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0380 | Time 29.7935, Epoch Time 495.9468(493.7300), Bit/dim 3.9781(best: 3.9785), Xent 1.5793, Loss 4.7677, Error 0.4237(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2281 | Time 76.0516(74.5825) | Bit/dim 3.9679(3.9720) | Xent 0.4988(0.4984) | Loss 4.2173(4.2212) | Error 0.1806(0.1803) Steps 814(809.70) | Grad Norm 1.3882(1.9329) | Total Time 14.00(14.00)\n",
      "Iter 2282 | Time 73.7148(74.5564) | Bit/dim 3.9802(3.9723) | Xent 0.4645(0.4974) | Loss 4.2125(4.2210) | Error 0.1691(0.1799) Steps 820(810.01) | Grad Norm 1.7995(1.9289) | Total Time 14.00(14.00)\n",
      "Iter 2283 | Time 73.8413(74.5350) | Bit/dim 3.9657(3.9721) | Xent 0.4603(0.4963) | Loss 4.1958(4.2202) | Error 0.1675(0.1795) Steps 814(810.13) | Grad Norm 1.0664(1.9030) | Total Time 14.00(14.00)\n",
      "Iter 2284 | Time 77.0509(74.6104) | Bit/dim 3.9692(3.9720) | Xent 0.4854(0.4960) | Loss 4.2119(4.2200) | Error 0.1771(0.1795) Steps 808(810.07) | Grad Norm 2.3863(1.9175) | Total Time 14.00(14.00)\n",
      "Iter 2285 | Time 74.0171(74.5926) | Bit/dim 3.9606(3.9716) | Xent 0.5070(0.4963) | Loss 4.2141(4.2198) | Error 0.1830(0.1796) Steps 814(810.18) | Grad Norm 1.8024(1.9141) | Total Time 14.00(14.00)\n",
      "Iter 2286 | Time 74.2403(74.5821) | Bit/dim 3.9738(3.9717) | Xent 0.5047(0.4966) | Loss 4.2261(4.2200) | Error 0.1819(0.1796) Steps 802(809.94) | Grad Norm 2.7982(1.9406) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0381 | Time 29.7711, Epoch Time 494.1296(493.7420), Bit/dim 3.9786(best: 3.9781), Xent 1.6006, Loss 4.7788, Error 0.4208(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2287 | Time 71.2851(74.4832) | Bit/dim 3.9730(3.9717) | Xent 0.4788(0.4960) | Loss 4.2124(4.2197) | Error 0.1729(0.1794) Steps 808(809.88) | Grad Norm 3.9166(1.9999) | Total Time 14.00(14.00)\n",
      "Iter 2288 | Time 73.8597(74.4645) | Bit/dim 3.9588(3.9713) | Xent 0.4891(0.4958) | Loss 4.2034(4.2193) | Error 0.1809(0.1795) Steps 820(810.18) | Grad Norm 2.7067(2.0211) | Total Time 14.00(14.00)\n",
      "Iter 2289 | Time 74.8886(74.4772) | Bit/dim 3.9831(3.9717) | Xent 0.4897(0.4956) | Loss 4.2279(4.2195) | Error 0.1731(0.1793) Steps 796(809.76) | Grad Norm 4.3150(2.0899) | Total Time 14.00(14.00)\n",
      "Iter 2290 | Time 75.9743(74.5221) | Bit/dim 3.9721(3.9717) | Xent 0.4923(0.4955) | Loss 4.2182(4.2195) | Error 0.1775(0.1792) Steps 814(809.89) | Grad Norm 2.1851(2.0927) | Total Time 14.00(14.00)\n",
      "Iter 2291 | Time 73.2085(74.4827) | Bit/dim 3.9711(3.9717) | Xent 0.4962(0.4956) | Loss 4.2192(4.2195) | Error 0.1819(0.1793) Steps 802(809.65) | Grad Norm 4.7403(2.1722) | Total Time 14.00(14.00)\n",
      "Iter 2292 | Time 77.1472(74.5626) | Bit/dim 3.9643(3.9715) | Xent 0.4912(0.4954) | Loss 4.2099(4.2192) | Error 0.1755(0.1792) Steps 820(809.96) | Grad Norm 3.4477(2.2104) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0382 | Time 29.8886, Epoch Time 491.9227(493.6874), Bit/dim 3.9778(best: 3.9781), Xent 1.5698, Loss 4.7627, Error 0.4244(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2293 | Time 72.4501(74.4993) | Bit/dim 3.9779(3.9717) | Xent 0.4868(0.4952) | Loss 4.2213(4.2192) | Error 0.1774(0.1792) Steps 808(809.90) | Grad Norm 2.5508(2.2206) | Total Time 14.00(14.00)\n",
      "Iter 2294 | Time 73.8955(74.4811) | Bit/dim 3.9728(3.9717) | Xent 0.4813(0.4948) | Loss 4.2135(4.2191) | Error 0.1747(0.1790) Steps 814(810.02) | Grad Norm 5.1702(2.3091) | Total Time 14.00(14.00)\n",
      "Iter 2295 | Time 77.7218(74.5784) | Bit/dim 3.9648(3.9715) | Xent 0.4914(0.4947) | Loss 4.2106(4.2188) | Error 0.1713(0.1788) Steps 814(810.14) | Grad Norm 1.9883(2.2995) | Total Time 14.00(14.00)\n",
      "Iter 2296 | Time 73.2093(74.5373) | Bit/dim 3.9641(3.9713) | Xent 0.4967(0.4947) | Loss 4.2124(4.2186) | Error 0.1805(0.1788) Steps 808(810.08) | Grad Norm 4.4202(2.3631) | Total Time 14.00(14.00)\n",
      "Iter 2297 | Time 74.3821(74.5326) | Bit/dim 3.9666(3.9711) | Xent 0.4850(0.4944) | Loss 4.2091(4.2183) | Error 0.1761(0.1788) Steps 808(810.02) | Grad Norm 2.7743(2.3755) | Total Time 14.00(14.00)\n",
      "Iter 2298 | Time 74.8871(74.5433) | Bit/dim 3.9717(3.9711) | Xent 0.4932(0.4944) | Loss 4.2183(4.2183) | Error 0.1766(0.1787) Steps 820(810.32) | Grad Norm 4.2860(2.4328) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0383 | Time 29.7327, Epoch Time 491.8713(493.6329), Bit/dim 3.9779(best: 3.9778), Xent 1.5977, Loss 4.7768, Error 0.4249(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2299 | Time 73.6168(74.5155) | Bit/dim 3.9606(3.9708) | Xent 0.4915(0.4943) | Loss 4.2063(4.2180) | Error 0.1761(0.1786) Steps 808(810.25) | Grad Norm 3.9431(2.4781) | Total Time 14.00(14.00)\n",
      "Iter 2300 | Time 74.7630(74.5229) | Bit/dim 3.9703(3.9708) | Xent 0.4893(0.4942) | Loss 4.2150(4.2179) | Error 0.1787(0.1786) Steps 814(810.36) | Grad Norm 2.9580(2.4925) | Total Time 14.00(14.00)\n",
      "Iter 2301 | Time 71.3269(74.4270) | Bit/dim 3.9786(3.9710) | Xent 0.4870(0.4939) | Loss 4.2221(4.2180) | Error 0.1734(0.1785) Steps 814(810.47) | Grad Norm 4.1781(2.5431) | Total Time 14.00(14.00)\n",
      "Iter 2302 | Time 73.0352(74.3853) | Bit/dim 3.9625(3.9708) | Xent 0.4816(0.4936) | Loss 4.2033(4.2176) | Error 0.1743(0.1783) Steps 814(810.57) | Grad Norm 1.2387(2.5039) | Total Time 14.00(14.00)\n",
      "Iter 2303 | Time 76.9953(74.4636) | Bit/dim 3.9778(3.9710) | Xent 0.4784(0.4931) | Loss 4.2169(4.2176) | Error 0.1743(0.1782) Steps 796(810.14) | Grad Norm 3.5437(2.5351) | Total Time 14.00(14.00)\n",
      "Iter 2304 | Time 75.5630(74.4965) | Bit/dim 3.9687(3.9709) | Xent 0.4705(0.4924) | Loss 4.2040(4.2171) | Error 0.1684(0.1779) Steps 814(810.25) | Grad Norm 2.4116(2.5314) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0384 | Time 29.8720, Epoch Time 491.0961(493.5568), Bit/dim 3.9784(best: 3.9778), Xent 1.5896, Loss 4.7732, Error 0.4236(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2305 | Time 73.5054(74.4668) | Bit/dim 3.9701(3.9709) | Xent 0.4777(0.4920) | Loss 4.2090(4.2169) | Error 0.1717(0.1777) Steps 820(810.55) | Grad Norm 2.5180(2.5310) | Total Time 14.00(14.00)\n",
      "Iter 2306 | Time 75.2652(74.4908) | Bit/dim 3.9753(3.9710) | Xent 0.4741(0.4915) | Loss 4.2123(4.2168) | Error 0.1760(0.1777) Steps 814(810.65) | Grad Norm 2.4520(2.5286) | Total Time 14.00(14.00)\n",
      "Iter 2307 | Time 73.9636(74.4750) | Bit/dim 3.9622(3.9708) | Xent 0.4774(0.4910) | Loss 4.2009(4.2163) | Error 0.1725(0.1775) Steps 802(810.39) | Grad Norm 2.6439(2.5321) | Total Time 14.00(14.00)\n",
      "Iter 2308 | Time 75.4422(74.5040) | Bit/dim 3.9658(3.9706) | Xent 0.4915(0.4910) | Loss 4.2116(4.2161) | Error 0.1806(0.1776) Steps 790(809.78) | Grad Norm 2.6055(2.5343) | Total Time 14.00(14.00)\n",
      "Iter 2309 | Time 74.9153(74.5163) | Bit/dim 3.9711(3.9706) | Xent 0.4857(0.4909) | Loss 4.2139(4.2161) | Error 0.1757(0.1776) Steps 814(809.90) | Grad Norm 1.9141(2.5157) | Total Time 14.00(14.00)\n",
      "Iter 2310 | Time 73.2673(74.4788) | Bit/dim 3.9649(3.9705) | Xent 0.4742(0.4904) | Loss 4.2020(4.2157) | Error 0.1761(0.1775) Steps 814(810.03) | Grad Norm 2.7290(2.5221) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0385 | Time 29.6532, Epoch Time 491.2901(493.4888), Bit/dim 3.9780(best: 3.9778), Xent 1.6093, Loss 4.7827, Error 0.4211(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2311 | Time 75.2797(74.5029) | Bit/dim 3.9690(3.9704) | Xent 0.4714(0.4898) | Loss 4.2048(4.2153) | Error 0.1691(0.1773) Steps 814(810.15) | Grad Norm 2.1866(2.5120) | Total Time 14.00(14.00)\n",
      "Iter 2312 | Time 74.4417(74.5010) | Bit/dim 3.9646(3.9702) | Xent 0.4738(0.4893) | Loss 4.2015(4.2149) | Error 0.1723(0.1771) Steps 814(810.26) | Grad Norm 3.5355(2.5427) | Total Time 14.00(14.00)\n",
      "Iter 2313 | Time 75.6877(74.5366) | Bit/dim 3.9739(3.9704) | Xent 0.4805(0.4891) | Loss 4.2142(4.2149) | Error 0.1730(0.1770) Steps 814(810.37) | Grad Norm 1.4763(2.5107) | Total Time 14.00(14.00)\n",
      "Iter 2314 | Time 76.7154(74.6020) | Bit/dim 3.9722(3.9704) | Xent 0.4714(0.4885) | Loss 4.2079(4.2147) | Error 0.1676(0.1767) Steps 814(810.48) | Grad Norm 3.2013(2.5315) | Total Time 14.00(14.00)\n",
      "Iter 2315 | Time 71.7603(74.5167) | Bit/dim 3.9693(3.9704) | Xent 0.4873(0.4885) | Loss 4.2130(4.2146) | Error 0.1777(0.1767) Steps 814(810.59) | Grad Norm 2.0146(2.5160) | Total Time 14.00(14.00)\n",
      "Iter 2316 | Time 75.5790(74.5486) | Bit/dim 3.9697(3.9704) | Xent 0.4730(0.4880) | Loss 4.2061(4.2144) | Error 0.1750(0.1767) Steps 808(810.51) | Grad Norm 2.8334(2.5255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0386 | Time 29.9864, Epoch Time 494.9440(493.5325), Bit/dim 3.9773(best: 3.9778), Xent 1.6055, Loss 4.7800, Error 0.4212(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2317 | Time 74.8242(74.5569) | Bit/dim 3.9680(3.9703) | Xent 0.4638(0.4873) | Loss 4.1999(4.2139) | Error 0.1693(0.1765) Steps 808(810.44) | Grad Norm 1.1359(2.4838) | Total Time 14.00(14.00)\n",
      "Iter 2318 | Time 74.6625(74.5600) | Bit/dim 3.9686(3.9702) | Xent 0.4869(0.4873) | Loss 4.2121(4.2139) | Error 0.1803(0.1766) Steps 802(810.18) | Grad Norm 1.1197(2.4429) | Total Time 14.00(14.00)\n",
      "Iter 2319 | Time 75.6326(74.5922) | Bit/dim 3.9696(3.9702) | Xent 0.4746(0.4869) | Loss 4.2069(4.2137) | Error 0.1715(0.1764) Steps 808(810.12) | Grad Norm 1.7545(2.4222) | Total Time 14.00(14.00)\n",
      "Iter 2320 | Time 75.9426(74.6327) | Bit/dim 3.9739(3.9703) | Xent 0.4700(0.4864) | Loss 4.2089(4.2135) | Error 0.1699(0.1762) Steps 814(810.23) | Grad Norm 1.3644(2.3905) | Total Time 14.00(14.00)\n",
      "Iter 2321 | Time 73.0909(74.5865) | Bit/dim 3.9691(3.9703) | Xent 0.4675(0.4858) | Loss 4.2029(4.2132) | Error 0.1677(0.1760) Steps 802(809.99) | Grad Norm 1.0201(2.3494) | Total Time 14.00(14.00)\n",
      "Iter 2322 | Time 75.1030(74.6020) | Bit/dim 3.9634(3.9701) | Xent 0.4716(0.4854) | Loss 4.1991(4.2128) | Error 0.1701(0.1758) Steps 808(809.93) | Grad Norm 1.6652(2.3288) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0387 | Time 29.7757, Epoch Time 494.6475(493.5659), Bit/dim 3.9774(best: 3.9773), Xent 1.6324, Loss 4.7935, Error 0.4203(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2323 | Time 73.1935(74.5597) | Bit/dim 3.9833(3.9705) | Xent 0.4775(0.4852) | Loss 4.2220(4.2131) | Error 0.1706(0.1756) Steps 808(809.87) | Grad Norm 1.0919(2.2917) | Total Time 14.00(14.00)\n",
      "Iter 2324 | Time 73.6515(74.5325) | Bit/dim 3.9726(3.9705) | Xent 0.4798(0.4850) | Loss 4.2125(4.2131) | Error 0.1721(0.1755) Steps 802(809.63) | Grad Norm 1.6121(2.2714) | Total Time 14.00(14.00)\n",
      "Iter 2325 | Time 71.5254(74.4423) | Bit/dim 3.9792(3.9708) | Xent 0.4763(0.4848) | Loss 4.2174(4.2132) | Error 0.1693(0.1754) Steps 808(809.58) | Grad Norm 0.9540(2.2318) | Total Time 14.00(14.00)\n",
      "Iter 2326 | Time 77.3146(74.5284) | Bit/dim 3.9565(3.9704) | Xent 0.4697(0.4843) | Loss 4.1913(4.2125) | Error 0.1639(0.1750) Steps 826(810.08) | Grad Norm 1.9936(2.2247) | Total Time 14.00(14.00)\n",
      "Iter 2327 | Time 74.3888(74.5242) | Bit/dim 3.9638(3.9702) | Xent 0.4819(0.4842) | Loss 4.2048(4.2123) | Error 0.1763(0.1750) Steps 790(809.47) | Grad Norm 1.1899(2.1936) | Total Time 14.00(14.00)\n",
      "Iter 2328 | Time 75.5742(74.5557) | Bit/dim 3.9562(3.9698) | Xent 0.4673(0.4837) | Loss 4.1898(4.2116) | Error 0.1690(0.1749) Steps 814(809.61) | Grad Norm 1.3886(2.1695) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0388 | Time 29.6540, Epoch Time 491.1596(493.4937), Bit/dim 3.9760(best: 3.9773), Xent 1.6020, Loss 4.7770, Error 0.4202(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2329 | Time 75.3655(74.5800) | Bit/dim 3.9614(3.9695) | Xent 0.4759(0.4835) | Loss 4.1994(4.2113) | Error 0.1765(0.1749) Steps 820(809.92) | Grad Norm 2.2549(2.1721) | Total Time 14.00(14.00)\n",
      "Iter 2330 | Time 76.0448(74.6240) | Bit/dim 3.9702(3.9695) | Xent 0.4929(0.4838) | Loss 4.2166(4.2114) | Error 0.1803(0.1751) Steps 802(809.68) | Grad Norm 1.7888(2.1606) | Total Time 14.00(14.00)\n",
      "Iter 2331 | Time 76.1537(74.6699) | Bit/dim 3.9654(3.9694) | Xent 0.4734(0.4835) | Loss 4.2021(4.2111) | Error 0.1701(0.1749) Steps 802(809.45) | Grad Norm 3.2531(2.1933) | Total Time 14.00(14.00)\n",
      "Iter 2332 | Time 75.7084(74.7010) | Bit/dim 3.9677(3.9694) | Xent 0.4736(0.4832) | Loss 4.2045(4.2109) | Error 0.1689(0.1747) Steps 802(809.23) | Grad Norm 2.0378(2.1887) | Total Time 14.00(14.00)\n",
      "Iter 2333 | Time 74.9652(74.7090) | Bit/dim 3.9571(3.9690) | Xent 0.4640(0.4826) | Loss 4.1891(4.2103) | Error 0.1707(0.1746) Steps 808(809.19) | Grad Norm 1.8843(2.1795) | Total Time 14.00(14.00)\n",
      "Iter 2334 | Time 73.2853(74.6662) | Bit/dim 3.9803(3.9693) | Xent 0.4518(0.4817) | Loss 4.2062(4.2102) | Error 0.1637(0.1743) Steps 808(809.16) | Grad Norm 1.9549(2.1728) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0389 | Time 29.2827, Epoch Time 496.6377(493.5881), Bit/dim 3.9760(best: 3.9760), Xent 1.6051, Loss 4.7785, Error 0.4221(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2335 | Time 75.0969(74.6792) | Bit/dim 3.9683(3.9693) | Xent 0.4651(0.4812) | Loss 4.2009(4.2099) | Error 0.1707(0.1742) Steps 814(809.30) | Grad Norm 2.1460(2.1720) | Total Time 14.00(14.00)\n",
      "Iter 2336 | Time 73.7169(74.6503) | Bit/dim 3.9651(3.9692) | Xent 0.4729(0.4809) | Loss 4.2016(4.2096) | Error 0.1747(0.1742) Steps 790(808.72) | Grad Norm 1.5757(2.1541) | Total Time 14.00(14.00)\n",
      "Iter 2337 | Time 73.9313(74.6287) | Bit/dim 3.9658(3.9691) | Xent 0.4624(0.4804) | Loss 4.1970(4.2093) | Error 0.1665(0.1740) Steps 814(808.88) | Grad Norm 1.3531(2.1301) | Total Time 14.00(14.00)\n",
      "Iter 2338 | Time 75.8497(74.6654) | Bit/dim 3.9629(3.9689) | Xent 0.4677(0.4800) | Loss 4.1968(4.2089) | Error 0.1700(0.1739) Steps 802(808.68) | Grad Norm 2.3715(2.1373) | Total Time 14.00(14.00)\n",
      "Iter 2339 | Time 71.0110(74.5557) | Bit/dim 3.9670(3.9688) | Xent 0.4792(0.4800) | Loss 4.2066(4.2088) | Error 0.1749(0.1739) Steps 802(808.47) | Grad Norm 1.9292(2.1311) | Total Time 14.00(14.00)\n",
      "Iter 2340 | Time 75.6845(74.5896) | Bit/dim 3.9808(3.9692) | Xent 0.4710(0.4797) | Loss 4.2163(4.2090) | Error 0.1676(0.1737) Steps 802(808.28) | Grad Norm 1.2620(2.1050) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0390 | Time 29.7563, Epoch Time 490.7568(493.5031), Bit/dim 3.9754(best: 3.9760), Xent 1.6202, Loss 4.7855, Error 0.4234(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2341 | Time 70.8452(74.4773) | Bit/dim 3.9725(3.9693) | Xent 0.4688(0.4794) | Loss 4.2069(4.2090) | Error 0.1733(0.1737) Steps 802(808.09) | Grad Norm 3.0354(2.1329) | Total Time 14.00(14.00)\n",
      "Iter 2342 | Time 73.6650(74.4529) | Bit/dim 3.9660(3.9692) | Xent 0.4819(0.4794) | Loss 4.2070(4.2089) | Error 0.1710(0.1736) Steps 808(808.09) | Grad Norm 1.8914(2.1257) | Total Time 14.00(14.00)\n",
      "Iter 2343 | Time 73.0642(74.4112) | Bit/dim 3.9641(3.9690) | Xent 0.4488(0.4785) | Loss 4.1885(4.2083) | Error 0.1635(0.1733) Steps 802(807.91) | Grad Norm 1.2863(2.1005) | Total Time 14.00(14.00)\n",
      "Iter 2344 | Time 75.9590(74.4577) | Bit/dim 3.9713(3.9691) | Xent 0.4788(0.4785) | Loss 4.2107(4.2084) | Error 0.1731(0.1733) Steps 808(807.91) | Grad Norm 1.7060(2.0887) | Total Time 14.00(14.00)\n",
      "Iter 2345 | Time 75.0405(74.4751) | Bit/dim 3.9723(3.9692) | Xent 0.4522(0.4777) | Loss 4.1985(4.2081) | Error 0.1620(0.1730) Steps 790(807.37) | Grad Norm 1.4398(2.0692) | Total Time 14.00(14.00)\n",
      "Iter 2346 | Time 73.0798(74.4333) | Bit/dim 3.9647(3.9691) | Xent 0.4811(0.4778) | Loss 4.2052(4.2080) | Error 0.1749(0.1730) Steps 802(807.21) | Grad Norm 1.8562(2.0628) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0391 | Time 29.8480, Epoch Time 487.1972(493.3139), Bit/dim 3.9761(best: 3.9754), Xent 1.6403, Loss 4.7962, Error 0.4243(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2347 | Time 72.1094(74.3636) | Bit/dim 3.9788(3.9694) | Xent 0.4634(0.4774) | Loss 4.2105(4.2081) | Error 0.1625(0.1727) Steps 802(807.05) | Grad Norm 1.2234(2.0376) | Total Time 14.00(14.00)\n",
      "Iter 2348 | Time 73.3890(74.3343) | Bit/dim 3.9579(3.9690) | Xent 0.4785(0.4774) | Loss 4.1971(4.2077) | Error 0.1741(0.1727) Steps 796(806.72) | Grad Norm 2.1058(2.0397) | Total Time 14.00(14.00)\n",
      "Iter 2349 | Time 74.9702(74.3534) | Bit/dim 3.9719(3.9691) | Xent 0.4635(0.4770) | Loss 4.2036(4.2076) | Error 0.1729(0.1727) Steps 796(806.40) | Grad Norm 1.2789(2.0168) | Total Time 14.00(14.00)\n",
      "Iter 2350 | Time 72.7938(74.3066) | Bit/dim 3.9611(3.9689) | Xent 0.4723(0.4769) | Loss 4.1973(4.2073) | Error 0.1731(0.1728) Steps 814(806.63) | Grad Norm 1.3670(1.9973) | Total Time 14.00(14.00)\n",
      "Iter 2351 | Time 76.3651(74.3684) | Bit/dim 3.9714(3.9689) | Xent 0.4660(0.4766) | Loss 4.2045(4.2072) | Error 0.1691(0.1726) Steps 802(806.49) | Grad Norm 1.3311(1.9774) | Total Time 14.00(14.00)\n",
      "Iter 2352 | Time 74.0031(74.3574) | Bit/dim 3.9721(3.9690) | Xent 0.4588(0.4760) | Loss 4.2015(4.2070) | Error 0.1639(0.1724) Steps 814(806.72) | Grad Norm 1.1072(1.9513) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0392 | Time 29.8315, Epoch Time 489.0907(493.1872), Bit/dim 3.9755(best: 3.9754), Xent 1.6303, Loss 4.7906, Error 0.4231(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2353 | Time 75.0524(74.3783) | Bit/dim 3.9713(3.9691) | Xent 0.4769(0.4760) | Loss 4.2097(4.2071) | Error 0.1735(0.1724) Steps 808(806.75) | Grad Norm 2.8440(1.9780) | Total Time 14.00(14.00)\n",
      "Iter 2354 | Time 74.7277(74.3887) | Bit/dim 3.9665(3.9690) | Xent 0.4718(0.4759) | Loss 4.2024(4.2070) | Error 0.1706(0.1724) Steps 796(806.43) | Grad Norm 1.7993(1.9727) | Total Time 14.00(14.00)\n",
      "Iter 2355 | Time 71.5415(74.3033) | Bit/dim 3.9509(3.9685) | Xent 0.4586(0.4754) | Loss 4.1802(4.2062) | Error 0.1643(0.1721) Steps 802(806.30) | Grad Norm 2.6474(1.9929) | Total Time 14.00(14.00)\n",
      "Iter 2356 | Time 73.2388(74.2714) | Bit/dim 3.9658(3.9684) | Xent 0.4645(0.4751) | Loss 4.1980(4.2059) | Error 0.1671(0.1720) Steps 796(805.99) | Grad Norm 1.6280(1.9820) | Total Time 14.00(14.00)\n",
      "Iter 2357 | Time 73.8294(74.2581) | Bit/dim 3.9772(3.9687) | Xent 0.4708(0.4749) | Loss 4.2126(4.2061) | Error 0.1711(0.1719) Steps 802(805.87) | Grad Norm 1.5032(1.9676) | Total Time 14.00(14.00)\n",
      "Iter 2358 | Time 74.1143(74.2538) | Bit/dim 3.9687(3.9687) | Xent 0.4552(0.4744) | Loss 4.1963(4.2058) | Error 0.1630(0.1717) Steps 808(805.93) | Grad Norm 1.7612(1.9614) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0393 | Time 29.7576, Epoch Time 488.0855(493.0342), Bit/dim 3.9758(best: 3.9754), Xent 1.6328, Loss 4.7922, Error 0.4184(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2359 | Time 73.2015(74.2223) | Bit/dim 3.9748(3.9688) | Xent 0.4710(0.4743) | Loss 4.2103(4.2060) | Error 0.1659(0.1715) Steps 790(805.46) | Grad Norm 2.2671(1.9706) | Total Time 14.00(14.00)\n",
      "Iter 2360 | Time 74.8052(74.2397) | Bit/dim 3.9656(3.9687) | Xent 0.4596(0.4738) | Loss 4.1954(4.2057) | Error 0.1653(0.1713) Steps 802(805.35) | Grad Norm 1.3134(1.9509) | Total Time 14.00(14.00)\n",
      "Iter 2361 | Time 72.3576(74.1833) | Bit/dim 3.9766(3.9690) | Xent 0.4509(0.4731) | Loss 4.2020(4.2055) | Error 0.1639(0.1711) Steps 802(805.25) | Grad Norm 1.4569(1.9360) | Total Time 14.00(14.00)\n",
      "Iter 2362 | Time 74.2312(74.1847) | Bit/dim 3.9675(3.9689) | Xent 0.4647(0.4729) | Loss 4.1998(4.2054) | Error 0.1701(0.1711) Steps 796(804.97) | Grad Norm 1.8713(1.9341) | Total Time 14.00(14.00)\n",
      "Iter 2363 | Time 73.2428(74.1565) | Bit/dim 3.9529(3.9685) | Xent 0.4726(0.4729) | Loss 4.1892(4.2049) | Error 0.1726(0.1711) Steps 802(804.88) | Grad Norm 2.7067(1.9573) | Total Time 14.00(14.00)\n",
      "Iter 2364 | Time 73.4583(74.1355) | Bit/dim 3.9656(3.9684) | Xent 0.4704(0.4728) | Loss 4.2008(4.2048) | Error 0.1707(0.1711) Steps 808(804.98) | Grad Norm 2.1652(1.9635) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0394 | Time 29.6331, Epoch Time 486.5910(492.8409), Bit/dim 3.9752(best: 3.9754), Xent 1.6331, Loss 4.7918, Error 0.4200(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2365 | Time 74.5889(74.1491) | Bit/dim 3.9624(3.9682) | Xent 0.4621(0.4725) | Loss 4.1934(4.2044) | Error 0.1667(0.1710) Steps 802(804.89) | Grad Norm 3.0065(1.9948) | Total Time 14.00(14.00)\n",
      "Iter 2366 | Time 74.4890(74.1593) | Bit/dim 3.9517(3.9677) | Xent 0.4668(0.4723) | Loss 4.1852(4.2038) | Error 0.1640(0.1708) Steps 796(804.62) | Grad Norm 1.7379(1.9871) | Total Time 14.00(14.00)\n",
      "Iter 2367 | Time 72.7896(74.1182) | Bit/dim 3.9716(3.9678) | Xent 0.4552(0.4718) | Loss 4.1992(4.2037) | Error 0.1680(0.1707) Steps 802(804.54) | Grad Norm 2.1917(1.9932) | Total Time 14.00(14.00)\n",
      "Iter 2368 | Time 73.8582(74.1104) | Bit/dim 3.9797(3.9682) | Xent 0.4652(0.4716) | Loss 4.2123(4.2040) | Error 0.1724(0.1707) Steps 796(804.29) | Grad Norm 1.8525(1.9890) | Total Time 14.00(14.00)\n",
      "Iter 2369 | Time 76.6884(74.1878) | Bit/dim 3.9641(3.9680) | Xent 0.4724(0.4716) | Loss 4.2002(4.2039) | Error 0.1693(0.1707) Steps 796(804.04) | Grad Norm 1.5934(1.9771) | Total Time 14.00(14.00)\n",
      "Iter 2370 | Time 73.0690(74.1542) | Bit/dim 3.9687(3.9681) | Xent 0.4639(0.4714) | Loss 4.2006(4.2038) | Error 0.1657(0.1705) Steps 796(803.80) | Grad Norm 2.0826(1.9803) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0395 | Time 30.0565, Epoch Time 491.2751(492.7939), Bit/dim 3.9744(best: 3.9752), Xent 1.6389, Loss 4.7939, Error 0.4248(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2371 | Time 73.0631(74.1215) | Bit/dim 3.9613(3.9679) | Xent 0.4736(0.4714) | Loss 4.1981(4.2036) | Error 0.1707(0.1705) Steps 796(803.56) | Grad Norm 1.1719(1.9561) | Total Time 14.00(14.00)\n",
      "Iter 2372 | Time 75.6501(74.1673) | Bit/dim 3.9649(3.9678) | Xent 0.4599(0.4711) | Loss 4.1948(4.2033) | Error 0.1659(0.1704) Steps 802(803.52) | Grad Norm 2.0110(1.9577) | Total Time 14.00(14.00)\n",
      "Iter 2373 | Time 75.0213(74.1929) | Bit/dim 3.9664(3.9677) | Xent 0.4555(0.4706) | Loss 4.1941(4.2030) | Error 0.1664(0.1703) Steps 808(803.65) | Grad Norm 1.3490(1.9394) | Total Time 14.00(14.00)\n",
      "Iter 2374 | Time 73.5613(74.1740) | Bit/dim 3.9687(3.9678) | Xent 0.4633(0.4704) | Loss 4.2004(4.2030) | Error 0.1693(0.1703) Steps 808(803.78) | Grad Norm 2.4945(1.9561) | Total Time 14.00(14.00)\n",
      "Iter 2375 | Time 72.7892(74.1324) | Bit/dim 3.9724(3.9679) | Xent 0.4618(0.4702) | Loss 4.2033(4.2030) | Error 0.1679(0.1702) Steps 802(803.73) | Grad Norm 1.7370(1.9495) | Total Time 14.00(14.00)\n",
      "Iter 2376 | Time 72.4159(74.0809) | Bit/dim 3.9619(3.9677) | Xent 0.4736(0.4703) | Loss 4.1987(4.2028) | Error 0.1729(0.1703) Steps 802(803.68) | Grad Norm 2.9158(1.9785) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0396 | Time 30.0125, Epoch Time 488.0979(492.6530), Bit/dim 3.9739(best: 3.9744), Xent 1.6352, Loss 4.7915, Error 0.4226(best: 0.4093)\n",
      "===> Using batch size 8000. Total 6 iterations/epoch.\n",
      "Iter 2377 | Time 74.4192(74.0911) | Bit/dim 3.9563(3.9674) | Xent 0.4709(0.4703) | Loss 4.1918(4.2025) | Error 0.1694(0.1702) Steps 808(803.81) | Grad Norm 1.9545(1.9778) | Total Time 14.00(14.00)\n",
      "Iter 2378 | Time 73.7315(74.0803) | Bit/dim 3.9685(3.9674) | Xent 0.4609(0.4700) | Loss 4.1989(4.2024) | Error 0.1645(0.1701) Steps 808(803.93) | Grad Norm 2.4702(1.9926) | Total Time 14.00(14.00)\n",
      "Iter 2379 | Time 74.2698(74.0860) | Bit/dim 3.9642(3.9673) | Xent 0.4373(0.4690) | Loss 4.1829(4.2018) | Error 0.1595(0.1697) Steps 802(803.87) | Grad Norm 1.3270(1.9726) | Total Time 14.00(14.00)\n",
      "Iter 2380 | Time 75.8076(74.1376) | Bit/dim 3.9716(3.9674) | Xent 0.4488(0.4684) | Loss 4.1960(4.2016) | Error 0.1633(0.1696) Steps 796(803.64) | Grad Norm 1.6680(1.9635) | Total Time 14.00(14.00)\n",
      "Iter 2381 | Time 73.5791(74.1209) | Bit/dim 3.9663(3.9674) | Xent 0.4693(0.4684) | Loss 4.2010(4.2016) | Error 0.1699(0.1696) Steps 796(803.41) | Grad Norm 2.6905(1.9853) | Total Time 14.00(14.00)\n",
      "Iter 2382 | Time 74.1503(74.1218) | Bit/dim 3.9665(3.9674) | Xent 0.4649(0.4683) | Loss 4.1990(4.2015) | Error 0.1711(0.1696) Steps 808(803.55) | Grad Norm 2.3273(1.9955) | Total Time 14.00(14.00)\n",
      "validating...\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_drop_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 8000 --test_batch_size 5000 --save ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_run1 --resume ../experiments_published/cnf_conditional_cifar10_8K_drop_0_5_baseline_run1/epoch_250_checkpt.pth --seed 1 --conditional True --controlled_tol False --train_mode semisup --lr 0.0001 --warmup_iters 1000 --atol 1e-5  --rtol 1e-5 --weight_y 0.5 --dropout_rate 0.5\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
