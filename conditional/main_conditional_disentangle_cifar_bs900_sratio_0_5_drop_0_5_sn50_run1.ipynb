{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn50_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=50, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 14.3023(32.2069) | Bit/dim 9.9827(10.8682) | Xent 2.2822(2.3001) | Loss 11.1238(12.0182) | Error 0.7600(0.8645) Steps 574(574.00) | Grad Norm 43.0926(65.6833) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 14.5367(27.5263) | Bit/dim 8.9183(10.4495) | Xent 2.2381(2.2874) | Loss 10.0373(11.5932) | Error 0.7722(0.8369) Steps 574(574.00) | Grad Norm 13.4743(54.5737) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 14.4620(24.0942) | Bit/dim 8.4760(9.9741) | Xent 2.1724(2.2652) | Loss 9.5622(11.1067) | Error 0.7389(0.8177) Steps 574(574.00) | Grad Norm 11.0478(43.6730) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 14.5461(21.5503) | Bit/dim 8.1471(9.5359) | Xent 2.1392(2.2365) | Loss 9.2167(10.6541) | Error 0.7311(0.7966) Steps 574(574.00) | Grad Norm 7.4014(34.4396) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 14.2821(19.6675) | Bit/dim 7.8447(9.1274) | Xent 2.1348(2.2086) | Loss 8.9120(10.2317) | Error 0.7089(0.7772) Steps 574(574.00) | Grad Norm 5.0278(27.0142) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 92.0271, Epoch Time 927.4641(927.4641), Bit/dim 7.6721(best: inf), Xent 2.0965, Loss 8.7203, Error 0.6904(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 14.4348(18.3658) | Bit/dim 7.5843(8.7493) | Xent 2.1001(2.1810) | Loss 8.6344(9.8398) | Error 0.7089(0.7584) Steps 574(574.00) | Grad Norm 4.3350(21.1324) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 14.5165(17.3476) | Bit/dim 7.3218(8.4016) | Xent 2.1140(2.1598) | Loss 8.3788(9.4815) | Error 0.6911(0.7393) Steps 574(574.00) | Grad Norm 2.8980(16.4853) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 14.4403(16.6084) | Bit/dim 7.1866(8.0960) | Xent 2.0966(2.1430) | Loss 8.2349(9.1675) | Error 0.6867(0.7274) Steps 574(574.00) | Grad Norm 2.1582(12.8685) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 15.2471(16.1420) | Bit/dim 7.0934(7.8411) | Xent 2.1209(2.1324) | Loss 8.1539(8.9073) | Error 0.7278(0.7227) Steps 592(576.17) | Grad Norm 1.8368(10.0638) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 15.2626(15.9190) | Bit/dim 6.9994(7.6316) | Xent 2.0780(2.1191) | Loss 8.0384(8.6911) | Error 0.6944(0.7148) Steps 592(580.32) | Grad Norm 3.5682(7.9759) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 16.4813(15.8991) | Bit/dim 6.9634(7.4603) | Xent 2.0749(2.1080) | Loss 8.0009(8.5143) | Error 0.6978(0.7121) Steps 610(585.59) | Grad Norm 1.3047(6.3912) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 92.4404, Epoch Time 933.6051(927.6483), Bit/dim 6.9541(best: 7.6721), Xent 2.0644, Loss 7.9863, Error 0.6916(best: 0.6904)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 16.8367(16.0915) | Bit/dim 6.8794(7.3192) | Xent 2.0538(2.0965) | Loss 7.9064(8.3675) | Error 0.7144(0.7091) Steps 610(592.00) | Grad Norm 3.4727(5.4102) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.8756(16.2907) | Bit/dim 6.8278(7.1985) | Xent 2.0543(2.0886) | Loss 7.8549(8.2428) | Error 0.7044(0.7078) Steps 622(598.87) | Grad Norm 1.2167(5.1556) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 17.0448(16.4821) | Bit/dim 6.7455(7.0898) | Xent 2.0483(2.0826) | Loss 7.7697(8.1311) | Error 0.7056(0.7097) Steps 622(604.94) | Grad Norm 2.6856(6.1680) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 17.2293(16.6377) | Bit/dim 6.6656(6.9896) | Xent 2.1238(2.0801) | Loss 7.7275(8.0296) | Error 0.7722(0.7146) Steps 628(610.72) | Grad Norm 33.9815(9.2725) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 17.5868(16.7699) | Bit/dim 6.5640(6.8862) | Xent 2.1333(2.0804) | Loss 7.6306(7.9264) | Error 0.7711(0.7202) Steps 634(615.43) | Grad Norm 39.0674(13.5615) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 96.0432, Epoch Time 1048.3233(931.2685), Bit/dim 6.4454(best: 6.9541), Xent 2.0432, Loss 7.4670, Error 0.6890(best: 0.6904)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 17.4324(16.9716) | Bit/dim 6.3928(6.7737) | Xent 2.0646(2.0801) | Loss 7.4251(7.8138) | Error 0.7078(0.7246) Steps 628(619.85) | Grad Norm 14.7717(16.5866) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 17.7394(17.1182) | Bit/dim 6.2013(6.6429) | Xent 2.0373(2.0696) | Loss 7.2200(7.6777) | Error 0.6733(0.7176) Steps 628(622.13) | Grad Norm 13.8918(15.5175) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 17.7505(17.2333) | Bit/dim 6.4658(6.5163) | Xent 2.6460(2.0922) | Loss 7.7888(7.5624) | Error 0.8556(0.7252) Steps 634(624.02) | Grad Norm 174.1784(26.5095) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.3808(17.3288) | Bit/dim 5.9709(6.3952) | Xent 2.0507(2.1076) | Loss 6.9962(7.4490) | Error 0.7211(0.7392) Steps 628(625.66) | Grad Norm 23.0025(32.7415) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 17.6725(17.3578) | Bit/dim 5.8436(6.2666) | Xent 2.0795(2.1035) | Loss 6.8834(7.3183) | Error 0.7122(0.7397) Steps 628(625.80) | Grad Norm 15.3285(28.9132) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 17.3702(17.3617) | Bit/dim 5.7579(6.1491) | Xent 2.0406(2.0965) | Loss 6.7782(7.1973) | Error 0.7189(0.7345) Steps 628(625.59) | Grad Norm 7.7617(24.2348) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 96.3375, Epoch Time 1076.3378(935.6206), Bit/dim 5.7728(best: 6.4454), Xent 2.0496, Loss 6.7976, Error 0.7092(best: 0.6890)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.4981(17.4237) | Bit/dim 5.7623(6.0430) | Xent 2.0425(2.0838) | Loss 6.7836(7.0849) | Error 0.6878(0.7257) Steps 628(626.86) | Grad Norm 8.0591(21.1243) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.0122(17.4009) | Bit/dim 5.6911(5.9566) | Xent 2.0416(2.0724) | Loss 6.7119(6.9927) | Error 0.7033(0.7232) Steps 628(627.63) | Grad Norm 12.5140(20.6698) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 17.2666(17.3325) | Bit/dim 5.6322(5.8773) | Xent 2.0615(2.0671) | Loss 6.6630(6.9109) | Error 0.7556(0.7220) Steps 628(627.73) | Grad Norm 38.3931(22.4012) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.8422(17.2406) | Bit/dim 5.6395(5.8200) | Xent 2.0201(2.0620) | Loss 6.6496(6.8510) | Error 0.7033(0.7227) Steps 622(626.80) | Grad Norm 3.9584(22.1428) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.7905(17.1423) | Bit/dim 5.5445(5.7707) | Xent 2.0679(2.0563) | Loss 6.5785(6.7988) | Error 0.7300(0.7186) Steps 622(625.54) | Grad Norm 15.2436(20.1028) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 97.2531, Epoch Time 1059.1106(939.3253), Bit/dim 5.6034(best: 5.7728), Xent 1.9977, Loss 6.6022, Error 0.6766(best: 0.6890)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.9780(17.0733) | Bit/dim 5.6002(5.7249) | Xent 2.0063(2.0440) | Loss 6.6034(6.7470) | Error 0.6833(0.7079) Steps 622(624.61) | Grad Norm 7.2925(17.1904) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 16.7675(16.9892) | Bit/dim 5.5999(5.6894) | Xent 1.9309(2.0249) | Loss 6.5654(6.7018) | Error 0.6356(0.6964) Steps 622(623.93) | Grad Norm 12.9352(15.1233) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 16.9796(16.9402) | Bit/dim 5.6095(5.6591) | Xent 1.9935(2.0139) | Loss 6.6063(6.6661) | Error 0.7200(0.6944) Steps 622(623.09) | Grad Norm 44.8034(18.3049) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 16.9375(16.8833) | Bit/dim 5.5602(5.6290) | Xent 1.9532(2.0031) | Loss 6.5368(6.6306) | Error 0.6911(0.6941) Steps 628(622.70) | Grad Norm 19.9529(19.1501) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 16.2348(16.9134) | Bit/dim 5.5193(5.6021) | Xent 1.9380(1.9919) | Loss 6.4883(6.5981) | Error 0.6633(0.6870) Steps 628(624.74) | Grad Norm 9.3528(17.3627) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 16.9171(16.9866) | Bit/dim 5.4548(5.5700) | Xent 1.9540(1.9850) | Loss 6.4318(6.5625) | Error 0.6589(0.6836) Steps 646(629.58) | Grad Norm 13.2603(18.2277) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 98.8603, Epoch Time 1045.4137(942.5080), Bit/dim 5.4706(best: 5.6034), Xent 1.9381, Loss 6.4396, Error 0.6680(best: 0.6766)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 17.3801(17.0824) | Bit/dim 5.4169(5.5411) | Xent 1.9353(1.9770) | Loss 6.3846(6.5296) | Error 0.6667(0.6816) Steps 646(633.62) | Grad Norm 14.4739(18.0597) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 17.9880(17.1803) | Bit/dim 5.3800(5.5071) | Xent 1.9184(1.9677) | Loss 6.3392(6.4910) | Error 0.6400(0.6768) Steps 658(638.24) | Grad Norm 3.7727(16.2981) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 17.9252(17.3754) | Bit/dim 5.3455(5.4735) | Xent 1.9663(1.9644) | Loss 6.3286(6.4557) | Error 0.6756(0.6787) Steps 664(644.56) | Grad Norm 9.5644(16.4927) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 18.3647(17.5725) | Bit/dim 5.3362(5.4401) | Xent 1.9538(1.9654) | Loss 6.3131(6.4228) | Error 0.6978(0.6817) Steps 682(652.92) | Grad Norm 18.7945(17.2510) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.6162(17.7704) | Bit/dim 5.2637(5.4032) | Xent 1.9911(1.9579) | Loss 6.2593(6.3822) | Error 0.7233(0.6802) Steps 682(661.97) | Grad Norm 20.9402(16.5398) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 107.5977, Epoch Time 1109.8136(947.5271), Bit/dim 5.2665(best: 5.4706), Xent 1.8869, Loss 6.2099, Error 0.6410(best: 0.6680)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 18.3248(17.9680) | Bit/dim 5.2230(5.3737) | Xent 1.8961(1.9477) | Loss 6.1710(6.3475) | Error 0.6744(0.6763) Steps 682(670.11) | Grad Norm 10.1718(15.3617) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 18.4624(18.1342) | Bit/dim 5.2574(5.3491) | Xent 1.9024(1.9519) | Loss 6.2086(6.3250) | Error 0.6556(0.6778) Steps 700(677.06) | Grad Norm 4.8529(18.9938) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 18.0146(18.2479) | Bit/dim 5.3620(5.3476) | Xent 2.0732(1.9787) | Loss 6.3986(6.3370) | Error 0.7278(0.6894) Steps 688(682.29) | Grad Norm 16.1051(21.6232) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 18.1862(18.2375) | Bit/dim 5.2552(5.3380) | Xent 1.9266(1.9840) | Loss 6.2185(6.3300) | Error 0.7033(0.6972) Steps 694(685.16) | Grad Norm 9.3869(19.3699) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 17.8506(18.1115) | Bit/dim 5.1933(5.3087) | Xent 1.9652(1.9812) | Loss 6.1759(6.2994) | Error 0.7056(0.6969) Steps 670(682.42) | Grad Norm 7.7627(16.0725) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 17.3583(18.0089) | Bit/dim 5.2061(5.2795) | Xent 1.9494(1.9724) | Loss 6.1808(6.2657) | Error 0.6711(0.6933) Steps 670(679.50) | Grad Norm 7.4028(13.5192) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 104.0833, Epoch Time 1122.9069(952.7885), Bit/dim 5.1686(best: 5.2665), Xent 1.9107, Loss 6.1239, Error 0.6439(best: 0.6410)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 18.0414(17.9452) | Bit/dim 5.1393(5.2495) | Xent 1.8742(1.9561) | Loss 6.0763(6.2276) | Error 0.6578(0.6862) Steps 670(676.83) | Grad Norm 6.1764(11.5506) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 17.9623(17.8894) | Bit/dim 5.2318(5.2148) | Xent 1.9599(1.9399) | Loss 6.2117(6.1848) | Error 0.6733(0.6791) Steps 688(676.10) | Grad Norm 7.5135(10.4895) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 18.7037(17.9674) | Bit/dim 5.0621(5.1832) | Xent 1.8549(1.9245) | Loss 5.9895(6.1454) | Error 0.6178(0.6727) Steps 694(679.12) | Grad Norm 8.8002(10.0317) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 17.8366(18.0353) | Bit/dim 5.0326(5.1547) | Xent 1.8781(1.9165) | Loss 5.9716(6.1130) | Error 0.6656(0.6687) Steps 676(680.24) | Grad Norm 8.0519(9.5141) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.3455(18.0916) | Bit/dim 5.1212(5.1399) | Xent 1.9084(1.9276) | Loss 6.0753(6.1037) | Error 0.6611(0.6736) Steps 682(681.56) | Grad Norm 14.0903(13.8068) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 104.3020, Epoch Time 1113.8892(957.6216), Bit/dim 5.0275(best: 5.1686), Xent 1.8795, Loss 5.9673, Error 0.6353(best: 0.6410)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 17.9769(18.0909) | Bit/dim 5.0668(5.1144) | Xent 1.8887(1.9282) | Loss 6.0112(6.0784) | Error 0.6778(0.6752) Steps 670(680.83) | Grad Norm 11.7519(13.4886) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 18.3227(18.0768) | Bit/dim 5.0179(5.1003) | Xent 1.9689(1.9191) | Loss 6.0023(6.0599) | Error 0.7133(0.6709) Steps 682(679.75) | Grad Norm 17.7021(13.7546) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 18.7681(18.1004) | Bit/dim 4.9853(5.0718) | Xent 1.8577(1.9073) | Loss 5.9142(6.0254) | Error 0.6522(0.6663) Steps 682(679.89) | Grad Norm 8.0751(12.8622) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.7703(18.2167) | Bit/dim 4.9290(5.0446) | Xent 1.7935(1.8906) | Loss 5.8257(5.9899) | Error 0.6233(0.6610) Steps 694(682.82) | Grad Norm 7.6618(12.5515) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.4809(18.2831) | Bit/dim 5.1477(5.0599) | Xent 2.1945(1.9207) | Loss 6.2450(6.0202) | Error 0.7567(0.6725) Steps 700(686.53) | Grad Norm 34.7786(17.6125) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.8187(18.2997) | Bit/dim 5.0078(5.0539) | Xent 2.0094(1.9407) | Loss 6.0125(6.0242) | Error 0.7178(0.6816) Steps 688(685.74) | Grad Norm 5.6744(15.6794) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 108.0649, Epoch Time 1131.6961(962.8438), Bit/dim 4.9854(best: 5.0275), Xent 1.9233, Loss 5.9471, Error 0.6762(best: 0.6353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 19.0767(18.4815) | Bit/dim 4.9412(5.0263) | Xent 1.8965(1.9380) | Loss 5.8895(5.9953) | Error 0.6700(0.6818) Steps 706(692.35) | Grad Norm 3.4892(12.7430) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 18.6356(18.6545) | Bit/dim 4.9203(4.9967) | Xent 1.8945(1.9256) | Loss 5.8676(5.9596) | Error 0.6744(0.6772) Steps 724(698.49) | Grad Norm 16.9482(12.3146) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 19.7784(18.7624) | Bit/dim 5.2244(5.0108) | Xent 1.9746(1.9384) | Loss 6.2117(5.9800) | Error 0.6922(0.6845) Steps 718(701.86) | Grad Norm 58.1730(16.9725) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.3256(18.6637) | Bit/dim 5.1553(5.0733) | Xent 1.9201(1.9543) | Loss 6.1154(6.0504) | Error 0.6767(0.6900) Steps 706(700.44) | Grad Norm 10.2844(16.7923) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 18.5159(18.5697) | Bit/dim 4.9824(5.0601) | Xent 1.9649(1.9548) | Loss 5.9649(6.0375) | Error 0.6822(0.6902) Steps 682(695.24) | Grad Norm 6.7097(14.4631) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 104.3219, Epoch Time 1149.2479(968.4359), Bit/dim 4.9133(best: 4.9854), Xent 1.8449, Loss 5.8358, Error 0.6355(best: 0.6353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 18.2905(18.4111) | Bit/dim 4.8870(5.0226) | Xent 1.8503(1.9326) | Loss 5.8121(5.9889) | Error 0.6378(0.6817) Steps 688(689.41) | Grad Norm 6.9083(12.0434) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 18.0885(18.4114) | Bit/dim 4.8463(4.9797) | Xent 1.8055(1.9095) | Loss 5.7490(5.9345) | Error 0.6144(0.6743) Steps 688(687.81) | Grad Norm 7.3461(10.3620) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 18.8407(18.5386) | Bit/dim 4.7964(4.9387) | Xent 1.8141(1.8911) | Loss 5.7034(5.8843) | Error 0.6500(0.6677) Steps 706(692.57) | Grad Norm 17.7021(9.7505) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 19.4088(18.7445) | Bit/dim 4.8348(4.9248) | Xent 1.9545(1.8930) | Loss 5.8120(5.8713) | Error 0.6989(0.6686) Steps 718(700.97) | Grad Norm 19.6150(14.0252) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 19.4765(18.8810) | Bit/dim 4.7943(4.8972) | Xent 1.9302(1.8960) | Loss 5.7594(5.8452) | Error 0.6578(0.6693) Steps 730(706.02) | Grad Norm 6.0245(13.1883) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 19.8398(19.0225) | Bit/dim 4.7592(4.8663) | Xent 1.9180(1.8905) | Loss 5.7182(5.8115) | Error 0.6722(0.6674) Steps 742(712.23) | Grad Norm 12.1993(11.7780) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 111.0124, Epoch Time 1171.9863(974.5424), Bit/dim 4.7621(best: 4.9133), Xent 1.7890, Loss 5.6566, Error 0.6143(best: 0.6353)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 19.3326(19.1588) | Bit/dim 4.8325(4.8430) | Xent 1.7867(1.8658) | Loss 5.7258(5.7759) | Error 0.6589(0.6597) Steps 736(718.71) | Grad Norm 19.3479(12.0175) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 20.0802(19.3249) | Bit/dim 4.7910(4.8485) | Xent 1.7515(1.8474) | Loss 5.6667(5.7722) | Error 0.6456(0.6526) Steps 730(724.46) | Grad Norm 15.1043(14.2219) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 19.0375(19.3385) | Bit/dim 4.9335(4.8778) | Xent 1.8270(1.8625) | Loss 5.8470(5.8091) | Error 0.6344(0.6588) Steps 724(725.22) | Grad Norm 14.5238(15.3605) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 18.9307(19.3247) | Bit/dim 4.7673(4.8615) | Xent 1.7775(1.8578) | Loss 5.6561(5.7904) | Error 0.6433(0.6573) Steps 718(723.59) | Grad Norm 7.5102(14.1612) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 19.5126(19.3100) | Bit/dim 4.7421(4.8289) | Xent 1.7996(1.8375) | Loss 5.6419(5.7476) | Error 0.6367(0.6518) Steps 742(725.54) | Grad Norm 18.8604(13.1839) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 111.4614, Epoch Time 1199.2530(981.2837), Bit/dim 4.7210(best: 4.7621), Xent 1.7223, Loss 5.5822, Error 0.6013(best: 0.6143)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 19.9030(19.3673) | Bit/dim 4.6971(4.8029) | Xent 1.7569(1.8252) | Loss 5.5755(5.7155) | Error 0.6422(0.6486) Steps 742(729.16) | Grad Norm 5.7108(13.8629) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 20.0424(19.5195) | Bit/dim 4.6447(4.7716) | Xent 1.6891(1.7986) | Loss 5.4892(5.6708) | Error 0.6156(0.6390) Steps 748(734.14) | Grad Norm 8.7584(12.0900) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 20.3627(19.5988) | Bit/dim 4.6712(4.7473) | Xent 1.7534(1.7818) | Loss 5.5479(5.6382) | Error 0.6167(0.6329) Steps 766(739.62) | Grad Norm 8.4974(12.8662) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 19.8998(19.6553) | Bit/dim 4.6591(4.7221) | Xent 1.7104(1.7722) | Loss 5.5143(5.6082) | Error 0.6189(0.6321) Steps 760(744.52) | Grad Norm 16.7255(12.7840) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.9838(19.7900) | Bit/dim 4.6377(4.7036) | Xent 1.7208(1.7571) | Loss 5.4981(5.5822) | Error 0.6278(0.6273) Steps 772(749.89) | Grad Norm 10.1418(12.9952) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 20.6596(19.9497) | Bit/dim 4.6244(4.6872) | Xent 1.6749(1.7444) | Loss 5.4618(5.5594) | Error 0.5978(0.6238) Steps 784(757.49) | Grad Norm 12.0755(13.3340) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 118.4051, Epoch Time 1235.1577(988.9000), Bit/dim 4.6151(best: 4.7210), Xent 1.6574, Loss 5.4438, Error 0.5849(best: 0.6013)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 20.5145(20.1350) | Bit/dim 4.6478(4.6723) | Xent 1.7178(1.7350) | Loss 5.5067(5.5398) | Error 0.6078(0.6219) Steps 790(764.78) | Grad Norm 22.2638(14.8624) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 20.4749(20.2496) | Bit/dim 4.6294(4.6534) | Xent 1.7770(1.7257) | Loss 5.5178(5.5162) | Error 0.6511(0.6181) Steps 790(770.15) | Grad Norm 25.1109(14.4222) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 19.6930(20.3716) | Bit/dim 5.0528(4.7063) | Xent 1.7796(1.7368) | Loss 5.9426(5.5747) | Error 0.6200(0.6218) Steps 766(775.49) | Grad Norm 15.2754(16.9868) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 19.7913(20.2570) | Bit/dim 4.8079(4.7454) | Xent 1.8021(1.7724) | Loss 5.7090(5.6316) | Error 0.6333(0.6325) Steps 736(767.82) | Grad Norm 13.1842(16.9455) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 19.8658(20.1741) | Bit/dim 4.6570(4.7349) | Xent 1.7830(1.7681) | Loss 5.5485(5.6190) | Error 0.6411(0.6314) Steps 760(765.60) | Grad Norm 5.9574(14.3234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 120.6319, Epoch Time 1258.8351(996.9980), Bit/dim 4.6123(best: 4.6151), Xent 1.6321, Loss 5.4284, Error 0.5765(best: 0.5849)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 20.5377(20.2858) | Bit/dim 4.5887(4.6998) | Xent 1.6836(1.7486) | Loss 5.4305(5.5741) | Error 0.6089(0.6255) Steps 778(769.75) | Grad Norm 4.0159(12.2279) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 20.9450(20.3814) | Bit/dim 4.5628(4.6648) | Xent 1.6497(1.7250) | Loss 5.3877(5.5273) | Error 0.5567(0.6148) Steps 784(772.13) | Grad Norm 5.8330(10.9502) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 20.5555(20.5087) | Bit/dim 4.5512(4.6307) | Xent 1.6611(1.7094) | Loss 5.3817(5.4854) | Error 0.6178(0.6102) Steps 778(777.40) | Grad Norm 8.9747(11.1280) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 20.7202(20.6250) | Bit/dim 4.5250(4.5969) | Xent 1.6904(1.6947) | Loss 5.3702(5.4443) | Error 0.6144(0.6050) Steps 796(782.40) | Grad Norm 6.5330(10.6649) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 21.2061(20.6715) | Bit/dim 4.5227(4.5724) | Xent 1.7172(1.6902) | Loss 5.3812(5.4175) | Error 0.6267(0.6044) Steps 778(785.27) | Grad Norm 18.2485(10.8466) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 21.0068(20.7557) | Bit/dim 4.4913(4.5508) | Xent 1.6734(1.6817) | Loss 5.3280(5.3916) | Error 0.6067(0.6018) Steps 796(788.97) | Grad Norm 8.5311(10.5584) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 118.4374, Epoch Time 1282.0085(1005.5483), Bit/dim 4.4756(best: 4.6123), Xent 1.5468, Loss 5.2490, Error 0.5548(best: 0.5765)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 21.6942(20.8895) | Bit/dim 4.4768(4.5391) | Xent 1.6443(1.6986) | Loss 5.2989(5.3884) | Error 0.5867(0.6068) Steps 826(794.37) | Grad Norm 5.9708(12.0990) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 20.9287(21.0120) | Bit/dim 4.4666(4.5254) | Xent 1.6732(1.6836) | Loss 5.3033(5.3672) | Error 0.5933(0.6015) Steps 796(799.41) | Grad Norm 9.4395(10.8831) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 21.1738(21.0285) | Bit/dim 4.4845(4.5068) | Xent 1.5722(1.6635) | Loss 5.2707(5.3385) | Error 0.5722(0.5961) Steps 808(800.47) | Grad Norm 7.3969(9.9024) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 21.5271(21.0864) | Bit/dim 4.4396(4.4932) | Xent 1.6032(1.6624) | Loss 5.2412(5.3244) | Error 0.5833(0.5972) Steps 832(804.15) | Grad Norm 14.2445(11.0828) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 21.3655(21.1308) | Bit/dim 4.4638(4.4843) | Xent 1.7889(1.6755) | Loss 5.3583(5.3220) | Error 0.6211(0.5994) Steps 826(806.92) | Grad Norm 20.4996(12.9290) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 124.3255, Epoch Time 1310.6089(1014.7002), Bit/dim 4.4500(best: 4.4756), Xent 1.5914, Loss 5.2457, Error 0.5744(best: 0.5548)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 21.4837(21.2171) | Bit/dim 4.4052(4.4763) | Xent 1.6217(1.6722) | Loss 5.2160(5.3124) | Error 0.5867(0.5981) Steps 802(810.40) | Grad Norm 6.5700(12.2504) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 20.7108(21.1451) | Bit/dim 4.4203(4.4614) | Xent 1.5443(1.6493) | Loss 5.1925(5.2860) | Error 0.5600(0.5912) Steps 796(807.81) | Grad Norm 8.0632(10.6193) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 20.9232(21.0995) | Bit/dim 4.3857(4.4477) | Xent 1.5709(1.6262) | Loss 5.1711(5.2608) | Error 0.5700(0.5827) Steps 802(807.66) | Grad Norm 7.5801(9.6797) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 21.6967(21.1866) | Bit/dim 4.4334(4.4376) | Xent 1.6708(1.6217) | Loss 5.2688(5.2485) | Error 0.5822(0.5793) Steps 832(810.59) | Grad Norm 8.5821(10.2199) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 21.5832(21.3233) | Bit/dim 4.3825(4.4320) | Xent 1.6313(1.6310) | Loss 5.1981(5.2475) | Error 0.5811(0.5822) Steps 826(814.82) | Grad Norm 14.3968(11.2911) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 20.8562(21.2545) | Bit/dim 4.4877(4.4303) | Xent 1.6861(1.6429) | Loss 5.3308(5.2517) | Error 0.5822(0.5863) Steps 796(814.13) | Grad Norm 27.0013(12.8441) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 117.8090, Epoch Time 1304.2216(1023.3858), Bit/dim 4.4822(best: 4.4500), Xent 1.6561, Loss 5.3102, Error 0.5752(best: 0.5548)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 21.6211(21.1950) | Bit/dim 4.4498(4.4285) | Xent 1.5722(1.6422) | Loss 5.2359(5.2495) | Error 0.5633(0.5859) Steps 814(812.37) | Grad Norm 14.7049(13.0147) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 21.3449(21.2702) | Bit/dim 4.4011(4.4202) | Xent 1.5704(1.6266) | Loss 5.1863(5.2336) | Error 0.5811(0.5818) Steps 808(814.98) | Grad Norm 11.3601(11.9437) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 21.6896(21.3426) | Bit/dim 4.3701(4.4036) | Xent 1.5285(1.6091) | Loss 5.1343(5.2081) | Error 0.5522(0.5766) Steps 820(818.78) | Grad Norm 9.7472(10.9605) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 22.1131(21.4300) | Bit/dim 4.3859(4.3932) | Xent 1.5831(1.6038) | Loss 5.1774(5.1951) | Error 0.5622(0.5755) Steps 850(824.16) | Grad Norm 16.7222(11.7213) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 21.6190(21.5047) | Bit/dim 4.3625(4.3951) | Xent 1.5656(1.6134) | Loss 5.1453(5.2019) | Error 0.5400(0.5785) Steps 856(827.37) | Grad Norm 6.7320(12.3672) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 125.7634, Epoch Time 1327.5940(1032.5120), Bit/dim 4.4753(best: 4.4500), Xent 1.5834, Loss 5.2670, Error 0.5558(best: 0.5548)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 21.8255(21.5850) | Bit/dim 4.3940(4.3969) | Xent 1.6012(1.6179) | Loss 5.1947(5.2059) | Error 0.5922(0.5787) Steps 814(831.13) | Grad Norm 7.4709(13.4643) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 22.2163(21.6969) | Bit/dim 4.3603(4.3939) | Xent 1.5140(1.6133) | Loss 5.1173(5.2006) | Error 0.5522(0.5777) Steps 874(835.15) | Grad Norm 4.7601(11.8385) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 22.4230(21.7634) | Bit/dim 4.3090(4.3845) | Xent 1.4978(1.5988) | Loss 5.0579(5.1839) | Error 0.5400(0.5716) Steps 868(839.53) | Grad Norm 9.7123(11.3749) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 21.7771(21.8205) | Bit/dim 4.3370(4.3720) | Xent 1.5425(1.5822) | Loss 5.1082(5.1631) | Error 0.5467(0.5665) Steps 832(839.69) | Grad Norm 11.1166(10.7799) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 22.4710(21.8854) | Bit/dim 4.2983(4.3543) | Xent 1.5157(1.5668) | Loss 5.0561(5.1378) | Error 0.5511(0.5614) Steps 868(840.22) | Grad Norm 9.1309(9.7084) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 22.2867(21.9435) | Bit/dim 4.3246(4.3438) | Xent 1.5735(1.5642) | Loss 5.1113(5.1259) | Error 0.5778(0.5605) Steps 844(842.61) | Grad Norm 15.8869(11.1659) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 126.0726, Epoch Time 1353.4273(1042.1395), Bit/dim 4.3114(best: 4.4500), Xent 1.4980, Loss 5.0604, Error 0.5242(best: 0.5548)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 22.2111(21.9393) | Bit/dim 4.2889(4.3345) | Xent 1.4465(1.5448) | Loss 5.0121(5.1068) | Error 0.5222(0.5525) Steps 850(842.27) | Grad Norm 8.6478(10.9157) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 23.1319(22.0301) | Bit/dim 4.3556(4.3329) | Xent 1.5543(1.5363) | Loss 5.1327(5.1010) | Error 0.5500(0.5506) Steps 868(843.89) | Grad Norm 14.2117(11.4040) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 21.9117(22.0205) | Bit/dim 4.3490(4.3292) | Xent 1.4892(1.5465) | Loss 5.0937(5.1025) | Error 0.5356(0.5544) Steps 832(842.62) | Grad Norm 9.5844(11.9055) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 21.6163(21.9725) | Bit/dim 4.2817(4.3196) | Xent 1.4822(1.5352) | Loss 5.0228(5.0872) | Error 0.5378(0.5513) Steps 838(841.26) | Grad Norm 5.1034(10.9728) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 21.6500(21.9470) | Bit/dim 4.2388(4.3076) | Xent 1.4519(1.5179) | Loss 4.9648(5.0666) | Error 0.5367(0.5471) Steps 844(841.01) | Grad Norm 4.2591(9.5183) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 128.0019, Epoch Time 1353.3644(1051.4762), Bit/dim 4.3199(best: 4.3114), Xent 1.4466, Loss 5.0432, Error 0.5097(best: 0.5242)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 22.0564(21.9530) | Bit/dim 4.2618(4.3061) | Xent 1.5249(1.5206) | Loss 5.0242(5.0664) | Error 0.5433(0.5454) Steps 844(841.95) | Grad Norm 11.8842(11.1093) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 21.8828(21.9496) | Bit/dim 4.2513(4.3003) | Xent 1.4563(1.5122) | Loss 4.9795(5.0564) | Error 0.5256(0.5417) Steps 844(840.40) | Grad Norm 8.7250(10.7040) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 22.3830(22.0025) | Bit/dim 4.2561(4.2893) | Xent 1.3557(1.4951) | Loss 4.9339(5.0369) | Error 0.4978(0.5363) Steps 844(842.76) | Grad Norm 10.3488(10.4417) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 21.9958(22.0368) | Bit/dim 4.2618(4.2804) | Xent 1.4550(1.4785) | Loss 4.9893(5.0196) | Error 0.5167(0.5315) Steps 826(842.37) | Grad Norm 14.0178(10.2655) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 21.5895(21.9848) | Bit/dim 4.2647(4.2778) | Xent 1.4545(1.4843) | Loss 4.9920(5.0200) | Error 0.5300(0.5326) Steps 814(841.25) | Grad Norm 11.3373(10.7499) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 21.9402(21.9912) | Bit/dim 4.2462(4.2719) | Xent 1.4557(1.4858) | Loss 4.9740(5.0148) | Error 0.5256(0.5315) Steps 862(843.56) | Grad Norm 6.3413(11.0302) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 125.6995, Epoch Time 1355.2729(1060.5901), Bit/dim 4.2550(best: 4.3114), Xent 1.3563, Loss 4.9331, Error 0.4827(best: 0.5097)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 22.7162(22.0194) | Bit/dim 4.2471(4.2627) | Xent 1.4127(1.4648) | Loss 4.9534(4.9951) | Error 0.4922(0.5239) Steps 874(846.40) | Grad Norm 9.5160(10.5130) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 22.3878(22.0229) | Bit/dim 4.2321(4.2545) | Xent 1.4290(1.4480) | Loss 4.9466(4.9785) | Error 0.5122(0.5174) Steps 850(846.46) | Grad Norm 6.5737(10.3469) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 22.0631(22.0075) | Bit/dim 4.2369(4.2467) | Xent 1.5076(1.4430) | Loss 4.9906(4.9682) | Error 0.5367(0.5160) Steps 844(845.85) | Grad Norm 14.9385(10.6141) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 22.3494(22.0757) | Bit/dim 4.2118(4.2360) | Xent 1.4145(1.4337) | Loss 4.9190(4.9529) | Error 0.5156(0.5147) Steps 850(847.81) | Grad Norm 11.8029(9.9887) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 22.6089(22.1573) | Bit/dim 4.1949(4.2279) | Xent 1.3267(1.4191) | Loss 4.8583(4.9374) | Error 0.4711(0.5077) Steps 874(850.08) | Grad Norm 4.9080(9.2136) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 127.5373, Epoch Time 1364.7094(1069.7137), Bit/dim 4.2597(best: 4.2550), Xent 1.5942, Loss 5.0568, Error 0.5413(best: 0.4827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 21.9987(22.0920) | Bit/dim 4.2406(4.2389) | Xent 1.4963(1.4532) | Loss 4.9887(4.9655) | Error 0.5433(0.5186) Steps 832(847.76) | Grad Norm 9.2115(11.1590) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 22.4013(22.0470) | Bit/dim 4.2287(4.2434) | Xent 1.4397(1.4592) | Loss 4.9486(4.9730) | Error 0.5011(0.5197) Steps 856(846.59) | Grad Norm 10.1119(11.1330) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 22.4114(21.9665) | Bit/dim 4.2064(4.2368) | Xent 1.4028(1.4437) | Loss 4.9078(4.9587) | Error 0.4978(0.5135) Steps 850(844.29) | Grad Norm 12.7224(9.9218) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 21.8660(21.9989) | Bit/dim 4.2062(4.2261) | Xent 1.3575(1.4237) | Loss 4.8849(4.9380) | Error 0.4800(0.5088) Steps 850(846.03) | Grad Norm 7.3601(9.6795) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 22.1727(22.0307) | Bit/dim 4.1702(4.2149) | Xent 1.3268(1.4080) | Loss 4.8336(4.9188) | Error 0.4800(0.5021) Steps 844(846.77) | Grad Norm 5.3777(9.3294) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 22.3794(22.0502) | Bit/dim 4.3429(4.2127) | Xent 1.6960(1.4093) | Loss 5.1909(4.9174) | Error 0.5800(0.5027) Steps 862(849.06) | Grad Norm 26.7706(10.6030) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 125.5509, Epoch Time 1350.7538(1078.1449), Bit/dim 4.2064(best: 4.2550), Xent 1.3154, Loss 4.8641, Error 0.4792(best: 0.4827)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 22.2114(22.0868) | Bit/dim 4.2409(4.2163) | Xent 1.4452(1.4148) | Loss 4.9635(4.9237) | Error 0.4811(0.5032) Steps 868(851.68) | Grad Norm 16.1714(11.1038) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 21.9107(22.0183) | Bit/dim 4.1674(4.2115) | Xent 1.4025(1.4073) | Loss 4.8686(4.9152) | Error 0.5089(0.5021) Steps 862(850.99) | Grad Norm 10.1676(10.5611) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 22.0660(22.0319) | Bit/dim 4.1809(4.2033) | Xent 1.3531(1.3933) | Loss 4.8574(4.9000) | Error 0.4778(0.4973) Steps 868(852.00) | Grad Norm 5.3850(10.1253) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 22.2810(22.0654) | Bit/dim 4.1615(4.1911) | Xent 1.3943(1.3758) | Loss 4.8586(4.8790) | Error 0.5133(0.4938) Steps 856(853.64) | Grad Norm 6.1678(9.0158) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 22.3060(22.1186) | Bit/dim 4.1938(4.1882) | Xent 1.2497(1.3648) | Loss 4.8186(4.8706) | Error 0.4500(0.4914) Steps 862(854.53) | Grad Norm 5.9404(9.3263) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 125.8029, Epoch Time 1359.7936(1086.5944), Bit/dim 4.1669(best: 4.2064), Xent 1.2221, Loss 4.7779, Error 0.4370(best: 0.4792)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 22.2501(22.1301) | Bit/dim 4.1474(4.1803) | Xent 1.2647(1.3436) | Loss 4.7797(4.8521) | Error 0.4444(0.4812) Steps 868(856.02) | Grad Norm 5.4378(9.3461) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 22.2745(22.1612) | Bit/dim 4.1496(4.1705) | Xent 1.3982(1.3325) | Loss 4.8487(4.8367) | Error 0.4789(0.4760) Steps 868(856.29) | Grad Norm 13.8792(9.2864) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 22.4644(22.2264) | Bit/dim 4.1361(4.1684) | Xent 1.3569(1.3375) | Loss 4.8146(4.8372) | Error 0.4756(0.4765) Steps 868(857.83) | Grad Norm 19.4768(10.7151) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 22.2298(22.2790) | Bit/dim 4.1422(4.1636) | Xent 1.3291(1.3299) | Loss 4.8068(4.8285) | Error 0.4767(0.4743) Steps 856(859.06) | Grad Norm 6.5856(10.1715) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 22.3484(22.2743) | Bit/dim 4.1558(4.1581) | Xent 1.3632(1.3271) | Loss 4.8374(4.8217) | Error 0.4867(0.4721) Steps 862(858.00) | Grad Norm 11.3537(10.0476) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 21.9417(22.2526) | Bit/dim 4.1411(4.1553) | Xent 1.3687(1.3235) | Loss 4.8254(4.8170) | Error 0.4833(0.4722) Steps 850(857.64) | Grad Norm 13.6236(9.9868) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 127.3718, Epoch Time 1371.1555(1095.1312), Bit/dim 4.1434(best: 4.1669), Xent 1.2355, Loss 4.7612, Error 0.4382(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 22.5032(22.2516) | Bit/dim 4.1282(4.1535) | Xent 1.4251(1.3141) | Loss 4.8408(4.8106) | Error 0.4956(0.4693) Steps 874(858.45) | Grad Norm 16.1620(10.0224) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 22.6685(22.2331) | Bit/dim 4.1442(4.1485) | Xent 1.2409(1.3103) | Loss 4.7647(4.8037) | Error 0.4433(0.4675) Steps 874(857.39) | Grad Norm 4.9744(9.7931) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 22.7859(22.2381) | Bit/dim 4.0945(4.1402) | Xent 1.2182(1.3003) | Loss 4.7036(4.7904) | Error 0.4356(0.4638) Steps 856(857.99) | Grad Norm 10.5773(9.5756) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 22.5852(22.2342) | Bit/dim 4.1293(4.1333) | Xent 1.3069(1.2990) | Loss 4.7827(4.7828) | Error 0.4711(0.4624) Steps 874(857.41) | Grad Norm 9.7296(10.3122) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 22.6859(22.2369) | Bit/dim 4.1374(4.1337) | Xent 1.3860(1.3074) | Loss 4.8304(4.7874) | Error 0.5000(0.4648) Steps 868(856.64) | Grad Norm 11.0138(10.9015) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 123.6764, Epoch Time 1364.4895(1103.2120), Bit/dim 4.1226(best: 4.1434), Xent 1.1944, Loss 4.7198, Error 0.4228(best: 0.4370)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1500 | Time 22.2254(22.3079) | Bit/dim 4.1018(4.1242) | Xent 1.3009(1.2882) | Loss 4.7523(4.7683) | Error 0.4622(0.4587) Steps 862(860.12) | Grad Norm 9.6258(10.2309) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 21.9675(22.3167) | Bit/dim 4.0989(4.1244) | Xent 1.3263(1.2946) | Loss 4.7620(4.7717) | Error 0.4756(0.4611) Steps 832(858.65) | Grad Norm 7.0867(10.6083) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 22.4956(22.3324) | Bit/dim 4.1085(4.1189) | Xent 1.2589(1.2846) | Loss 4.7379(4.7612) | Error 0.4378(0.4571) Steps 856(856.91) | Grad Norm 8.6377(10.3650) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 22.0484(22.2732) | Bit/dim 4.1113(4.1105) | Xent 1.2772(1.2752) | Loss 4.7499(4.7481) | Error 0.4456(0.4538) Steps 838(856.74) | Grad Norm 6.8071(9.4130) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 22.1827(22.2717) | Bit/dim 4.0910(4.1082) | Xent 1.2985(1.2686) | Loss 4.7403(4.7425) | Error 0.4567(0.4519) Steps 862(856.76) | Grad Norm 12.8885(9.3530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 125.1691, Epoch Time 1369.7162(1111.2071), Bit/dim 4.1051(best: 4.1226), Xent 1.2891, Loss 4.7497, Error 0.4647(best: 0.4228)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 22.1220(22.3327) | Bit/dim 4.0639(4.1039) | Xent 1.2491(1.2574) | Loss 4.6884(4.7326) | Error 0.4722(0.4495) Steps 838(857.42) | Grad Norm 4.6337(9.1449) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 22.6524(22.3212) | Bit/dim 4.0562(4.0961) | Xent 1.1726(1.2452) | Loss 4.6425(4.7187) | Error 0.4133(0.4447) Steps 862(857.28) | Grad Norm 6.1317(8.9112) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 22.3065(22.2946) | Bit/dim 4.1079(4.0904) | Xent 1.2079(1.2427) | Loss 4.7118(4.7118) | Error 0.4233(0.4423) Steps 862(856.93) | Grad Norm 13.2675(8.8608) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 22.4457(22.2316) | Bit/dim 4.0551(4.0830) | Xent 1.1959(1.2299) | Loss 4.6530(4.6980) | Error 0.4167(0.4380) Steps 868(853.96) | Grad Norm 7.5927(8.5865) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 22.9768(22.2220) | Bit/dim 4.0752(4.0816) | Xent 1.2676(1.2394) | Loss 4.7090(4.7013) | Error 0.4511(0.4428) Steps 880(855.09) | Grad Norm 14.2441(9.6603) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 127.9048, Epoch Time 1370.9997(1119.0009), Bit/dim 4.1519(best: 4.1051), Xent 1.1912, Loss 4.7475, Error 0.4267(best: 0.4228)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn50_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 50\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
