{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn10_run1', seed=1, solver='dopri5', spectral_norm=True, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Adding spectral norm to Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 14.3283(30.6459) | Bit/dim 9.9844(10.8650) | Xent 2.2821(2.3001) | Loss 11.1254(12.0151) | Error 0.7522(0.8642) Steps 574(574.00) | Grad Norm 43.0966(65.6712) | Total Time 14.00(14.00)\n",
      "Iter 0020 | Time 14.3255(26.3767) | Bit/dim 8.9204(10.4469) | Xent 2.2379(2.2874) | Loss 10.0393(11.5906) | Error 0.7711(0.8361) Steps 574(574.00) | Grad Norm 13.4924(54.5645) | Total Time 14.00(14.00)\n",
      "Iter 0030 | Time 14.5640(23.2194) | Bit/dim 8.4763(9.9722) | Xent 2.1719(2.2652) | Loss 9.5623(11.1048) | Error 0.7367(0.8171) Steps 574(574.00) | Grad Norm 11.0562(43.6661) | Total Time 14.00(14.00)\n",
      "Iter 0040 | Time 14.2588(20.9033) | Bit/dim 8.1479(9.5345) | Xent 2.1385(2.2364) | Loss 9.2172(10.6527) | Error 0.7322(0.7959) Steps 574(574.00) | Grad Norm 7.4062(34.4354) | Total Time 14.00(14.00)\n",
      "Iter 0050 | Time 14.2637(19.1634) | Bit/dim 7.8429(9.1259) | Xent 2.1337(2.2085) | Loss 8.9097(10.2302) | Error 0.7144(0.7774) Steps 574(574.00) | Grad Norm 5.0397(27.0100) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 94.9213, Epoch Time 923.8957(923.8957), Bit/dim 7.6742(best: inf), Xent 2.0963, Loss 8.7224, Error 0.6908(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0060 | Time 14.0526(17.9084) | Bit/dim 7.5855(8.7484) | Xent 2.1007(2.1808) | Loss 8.6359(9.8389) | Error 0.7156(0.7586) Steps 574(574.00) | Grad Norm 4.3316(21.1314) | Total Time 14.00(14.00)\n",
      "Iter 0070 | Time 14.2124(16.9277) | Bit/dim 7.3223(8.4005) | Xent 2.1134(2.1597) | Loss 8.3790(9.4803) | Error 0.6944(0.7401) Steps 574(574.00) | Grad Norm 2.8558(16.4833) | Total Time 14.00(14.00)\n",
      "Iter 0080 | Time 14.4148(16.2394) | Bit/dim 7.1851(8.0947) | Xent 2.0967(2.1430) | Loss 8.2335(9.1662) | Error 0.6833(0.7280) Steps 574(574.00) | Grad Norm 2.1572(12.8639) | Total Time 14.00(14.00)\n",
      "Iter 0090 | Time 14.8871(15.7896) | Bit/dim 7.0961(7.8406) | Xent 2.1197(2.1320) | Loss 8.1559(8.9066) | Error 0.7367(0.7229) Steps 592(576.02) | Grad Norm 1.9568(10.0575) | Total Time 14.00(14.00)\n",
      "Iter 0100 | Time 14.8167(15.6003) | Bit/dim 6.9992(7.6314) | Xent 2.0778(2.1185) | Loss 8.0381(8.6907) | Error 0.7011(0.7158) Steps 592(580.22) | Grad Norm 3.6297(7.9813) | Total Time 14.00(14.00)\n",
      "Iter 0110 | Time 16.0516(15.5665) | Bit/dim 6.9672(7.4610) | Xent 2.0736(2.1073) | Loss 8.0040(8.5147) | Error 0.7011(0.7119) Steps 610(584.69) | Grad Norm 1.4640(6.3932) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 94.5536, Epoch Time 919.7718(923.7720), Bit/dim 6.9588(best: 7.6742), Xent 2.0627, Loss 7.9901, Error 0.6914(best: 0.6908)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0120 | Time 16.1670(15.7151) | Bit/dim 6.8827(7.3209) | Xent 2.0532(2.0955) | Loss 7.9093(8.3687) | Error 0.7178(0.7087) Steps 610(591.33) | Grad Norm 3.2853(5.3934) | Total Time 14.00(14.00)\n",
      "Iter 0130 | Time 16.8204(15.9639) | Bit/dim 6.8315(7.2004) | Xent 2.0571(2.0879) | Loss 7.8601(8.2443) | Error 0.7033(0.7068) Steps 622(598.38) | Grad Norm 1.4141(5.1306) | Total Time 14.00(14.00)\n",
      "Iter 0140 | Time 16.8263(16.1327) | Bit/dim 6.7485(7.0924) | Xent 2.0460(2.0818) | Loss 7.7715(8.1334) | Error 0.7056(0.7090) Steps 622(604.58) | Grad Norm 2.1565(6.0959) | Total Time 14.00(14.00)\n",
      "Iter 0150 | Time 16.6510(16.2878) | Bit/dim 6.6763(6.9929) | Xent 2.1316(2.0797) | Loss 7.7421(8.0327) | Error 0.7822(0.7137) Steps 622(609.93) | Grad Norm 34.8326(9.3061) | Total Time 14.00(14.00)\n",
      "Iter 0160 | Time 17.1563(16.4430) | Bit/dim 6.5695(6.8905) | Xent 2.1558(2.0800) | Loss 7.6474(7.9305) | Error 0.7822(0.7195) Steps 634(614.27) | Grad Norm 42.5120(13.5972) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 97.2675, Epoch Time 1029.9836(926.9583), Bit/dim 6.4657(best: 6.9588), Xent 2.0501, Loss 7.4907, Error 0.6986(best: 0.6908)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0170 | Time 17.1384(16.6196) | Bit/dim 6.4074(6.7803) | Xent 2.0677(2.0799) | Loss 7.4412(7.8203) | Error 0.7289(0.7244) Steps 628(618.36) | Grad Norm 14.6808(16.6018) | Total Time 14.00(14.00)\n",
      "Iter 0180 | Time 17.2031(16.7664) | Bit/dim 6.2377(6.6540) | Xent 2.0484(2.0701) | Loss 7.2619(7.6891) | Error 0.6933(0.7178) Steps 628(621.17) | Grad Norm 19.8531(15.8574) | Total Time 14.00(14.00)\n",
      "Iter 0190 | Time 16.8964(16.8878) | Bit/dim 6.4222(6.5238) | Xent 2.7686(2.0876) | Loss 7.8066(7.5676) | Error 0.8500(0.7222) Steps 628(623.13) | Grad Norm 173.6822(23.6511) | Total Time 14.00(14.00)\n",
      "Iter 0200 | Time 17.3227(16.9775) | Bit/dim 6.0514(6.4224) | Xent 2.0606(2.1091) | Loss 7.0817(7.4770) | Error 0.7300(0.7359) Steps 628(625.00) | Grad Norm 25.1828(31.7214) | Total Time 14.00(14.00)\n",
      "Iter 0210 | Time 16.9008(16.9801) | Bit/dim 5.8666(6.2957) | Xent 2.0843(2.1046) | Loss 6.9087(7.3480) | Error 0.7111(0.7347) Steps 628(625.16) | Grad Norm 3.5234(27.1454) | Total Time 14.00(14.00)\n",
      "Iter 0220 | Time 16.9580(16.9374) | Bit/dim 5.7843(6.1772) | Xent 2.0449(2.0965) | Loss 6.8067(7.2254) | Error 0.6933(0.7295) Steps 628(624.68) | Grad Norm 8.5079(22.7692) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 97.0439, Epoch Time 1055.2037(930.8057), Bit/dim 5.7946(best: 6.4657), Xent 2.0386, Loss 6.8139, Error 0.6813(best: 0.6908)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0230 | Time 17.0467(16.9690) | Bit/dim 5.8186(6.0703) | Xent 2.1685(2.0880) | Loss 6.9029(7.1143) | Error 0.7689(0.7236) Steps 634(626.63) | Grad Norm 53.5654(22.1227) | Total Time 14.00(14.00)\n",
      "Iter 0240 | Time 17.2382(17.0509) | Bit/dim 5.7173(5.9848) | Xent 2.1000(2.0945) | Loss 6.7673(7.0321) | Error 0.7400(0.7312) Steps 628(627.77) | Grad Norm 20.5383(24.9116) | Total Time 14.00(14.00)\n",
      "Iter 0250 | Time 16.9062(16.9886) | Bit/dim 5.6301(5.8996) | Xent 2.0188(2.0825) | Loss 6.6396(6.9409) | Error 0.6611(0.7229) Steps 628(627.83) | Grad Norm 6.5079(21.0998) | Total Time 14.00(14.00)\n",
      "Iter 0260 | Time 16.5029(16.9143) | Bit/dim 5.6503(5.8360) | Xent 2.0352(2.0694) | Loss 6.6679(6.8707) | Error 0.6889(0.7142) Steps 622(627.35) | Grad Norm 6.8707(17.3436) | Total Time 14.00(14.00)\n",
      "Iter 0270 | Time 16.1930(16.7751) | Bit/dim 5.5433(5.7829) | Xent 2.0476(2.0578) | Loss 6.5670(6.8118) | Error 0.7133(0.7082) Steps 622(625.94) | Grad Norm 9.0030(15.3873) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 98.6410, Epoch Time 1041.0477(934.1129), Bit/dim 5.6119(best: 5.7946), Xent 2.0001, Loss 6.6120, Error 0.6978(best: 0.6813)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0280 | Time 16.4509(16.6844) | Bit/dim 5.6111(5.7359) | Xent 2.0089(2.0448) | Loss 6.6156(6.7583) | Error 0.7111(0.7028) Steps 622(624.91) | Grad Norm 15.2765(14.9625) | Total Time 14.00(14.00)\n",
      "Iter 0290 | Time 16.4178(16.6332) | Bit/dim 5.6139(5.6994) | Xent 1.9393(2.0239) | Loss 6.5836(6.7114) | Error 0.6489(0.6924) Steps 622(624.14) | Grad Norm 25.0358(13.9369) | Total Time 14.00(14.00)\n",
      "Iter 0300 | Time 16.7319(16.6065) | Bit/dim 5.5746(5.6650) | Xent 1.9921(2.0102) | Loss 6.5706(6.6700) | Error 0.7189(0.6888) Steps 622(623.58) | Grad Norm 18.2476(14.7793) | Total Time 14.00(14.00)\n",
      "Iter 0310 | Time 16.6185(16.5653) | Bit/dim 5.5637(5.6366) | Xent 1.9667(2.0043) | Loss 6.5471(6.6387) | Error 0.6811(0.6922) Steps 622(622.88) | Grad Norm 15.6674(18.1569) | Total Time 14.00(14.00)\n",
      "Iter 0320 | Time 16.7336(16.5691) | Bit/dim 5.5317(5.6084) | Xent 1.9361(1.9920) | Loss 6.4997(6.6044) | Error 0.6389(0.6840) Steps 628(624.11) | Grad Norm 7.3401(15.9675) | Total Time 14.00(14.00)\n",
      "Iter 0330 | Time 16.9411(16.6163) | Bit/dim 5.5144(5.5882) | Xent 2.0888(2.0040) | Loss 6.5588(6.5902) | Error 0.7578(0.6919) Steps 646(626.78) | Grad Norm 40.5573(22.4614) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 101.0667, Epoch Time 1028.6162(936.9480), Bit/dim 5.5409(best: 5.6119), Xent 1.9903, Loss 6.5360, Error 0.7120(best: 0.6813)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0340 | Time 16.8446(16.7240) | Bit/dim 5.4534(5.5645) | Xent 1.9505(1.9987) | Loss 6.4286(6.5638) | Error 0.6589(0.6922) Steps 646(631.56) | Grad Norm 7.7857(21.2757) | Total Time 14.00(14.00)\n",
      "Iter 0350 | Time 17.2395(16.8300) | Bit/dim 5.4145(5.5327) | Xent 1.9773(1.9916) | Loss 6.4031(6.5285) | Error 0.6589(0.6868) Steps 646(635.35) | Grad Norm 4.8663(18.0347) | Total Time 14.00(14.00)\n",
      "Iter 0360 | Time 18.0624(17.0145) | Bit/dim 5.3788(5.4991) | Xent 2.0057(1.9818) | Loss 6.3816(6.4900) | Error 0.6844(0.6841) Steps 664(640.32) | Grad Norm 15.9650(15.6353) | Total Time 14.00(14.00)\n",
      "Iter 0370 | Time 18.1192(17.2425) | Bit/dim 5.3550(5.4638) | Xent 2.0018(1.9754) | Loss 6.3559(6.4515) | Error 0.7000(0.6820) Steps 676(647.62) | Grad Norm 15.9163(14.6215) | Total Time 14.00(14.00)\n",
      "Iter 0380 | Time 17.9519(17.4913) | Bit/dim 5.2681(5.4238) | Xent 1.9735(1.9686) | Loss 6.2549(6.4082) | Error 0.6967(0.6812) Steps 688(657.02) | Grad Norm 4.3396(12.6989) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 108.7289, Epoch Time 1094.6514(941.6791), Bit/dim 5.2863(best: 5.5409), Xent 1.9022, Loss 6.2374, Error 0.6507(best: 0.6813)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0390 | Time 18.6648(17.7196) | Bit/dim 5.2486(5.3932) | Xent 1.9290(1.9585) | Loss 6.2131(6.3724) | Error 0.6722(0.6779) Steps 700(667.01) | Grad Norm 16.1297(11.7842) | Total Time 14.00(14.00)\n",
      "Iter 0400 | Time 18.7376(18.0626) | Bit/dim 5.3030(5.3652) | Xent 1.9679(1.9628) | Loss 6.2869(6.3466) | Error 0.7133(0.6823) Steps 706(677.69) | Grad Norm 21.2913(14.0122) | Total Time 14.00(14.00)\n",
      "Iter 0410 | Time 18.3610(18.1996) | Bit/dim 5.3600(5.3416) | Xent 2.1731(1.9618) | Loss 6.4466(6.3225) | Error 0.7389(0.6821) Steps 700(685.04) | Grad Norm 75.6364(16.3306) | Total Time 14.00(14.00)\n",
      "Iter 0420 | Time 17.9236(18.1308) | Bit/dim 5.2421(5.3546) | Xent 1.9198(2.0062) | Loss 6.2020(6.3577) | Error 0.6611(0.6996) Steps 688(686.75) | Grad Norm 6.7874(19.8246) | Total Time 14.00(14.00)\n",
      "Iter 0430 | Time 18.1179(18.0997) | Bit/dim 5.2129(5.3334) | Xent 2.0032(2.0042) | Loss 6.2145(6.3356) | Error 0.7133(0.7012) Steps 682(687.69) | Grad Norm 6.1237(16.7901) | Total Time 14.00(14.00)\n",
      "Iter 0440 | Time 17.8645(18.0352) | Bit/dim 5.2417(5.3064) | Xent 1.9446(1.9947) | Loss 6.2140(6.3037) | Error 0.6911(0.6981) Steps 682(686.24) | Grad Norm 4.9930(14.0741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 110.8921, Epoch Time 1135.1438(947.4831), Bit/dim 5.2043(best: 5.2863), Xent 1.9344, Loss 6.1715, Error 0.6687(best: 0.6507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0450 | Time 18.2073(18.0233) | Bit/dim 5.1571(5.2779) | Xent 1.9138(1.9791) | Loss 6.1140(6.2675) | Error 0.6733(0.6921) Steps 706(688.43) | Grad Norm 5.7054(11.6861) | Total Time 14.00(14.00)\n",
      "Iter 0460 | Time 18.1271(18.0111) | Bit/dim 5.2682(5.2439) | Xent 2.0001(1.9641) | Loss 6.2683(6.2259) | Error 0.7178(0.6867) Steps 688(690.52) | Grad Norm 15.0922(10.6100) | Total Time 14.00(14.00)\n",
      "Iter 0470 | Time 17.9064(18.0262) | Bit/dim 5.0886(5.2162) | Xent 1.9110(1.9673) | Loss 6.0441(6.1998) | Error 0.6389(0.6897) Steps 688(691.02) | Grad Norm 6.3171(12.3010) | Total Time 14.00(14.00)\n",
      "Iter 0480 | Time 17.3373(17.9624) | Bit/dim 5.0638(5.1883) | Xent 1.9368(1.9642) | Loss 6.0322(6.1704) | Error 0.6944(0.6880) Steps 688(689.17) | Grad Norm 7.8481(11.5082) | Total Time 14.00(14.00)\n",
      "Iter 0490 | Time 18.3199(17.9815) | Bit/dim 5.1555(5.1622) | Xent 1.9516(1.9565) | Loss 6.1314(6.1405) | Error 0.6700(0.6854) Steps 682(688.58) | Grad Norm 22.6805(11.0118) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 109.2967, Epoch Time 1115.2866(952.5172), Bit/dim 5.0894(best: 5.2043), Xent 1.9056, Loss 6.0422, Error 0.6628(best: 0.6507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0500 | Time 17.9581(17.9896) | Bit/dim 5.1822(5.1455) | Xent 1.9868(1.9530) | Loss 6.1756(6.1221) | Error 0.6878(0.6853) Steps 676(687.50) | Grad Norm 32.7540(13.1374) | Total Time 14.00(14.00)\n",
      "Iter 0510 | Time 18.3086(17.9814) | Bit/dim 5.0769(5.1301) | Xent 1.9529(1.9483) | Loss 6.0534(6.1043) | Error 0.6744(0.6821) Steps 682(686.01) | Grad Norm 12.8726(12.8989) | Total Time 14.00(14.00)\n",
      "Iter 0520 | Time 17.8667(18.0348) | Bit/dim 5.0315(5.1026) | Xent 1.8933(1.9392) | Loss 5.9781(6.0723) | Error 0.6700(0.6785) Steps 694(685.67) | Grad Norm 9.4087(11.6188) | Total Time 14.00(14.00)\n",
      "Iter 0530 | Time 18.3987(18.0757) | Bit/dim 5.1114(5.0896) | Xent 1.8692(1.9267) | Loss 6.0460(6.0530) | Error 0.6644(0.6746) Steps 694(687.71) | Grad Norm 22.7025(12.9902) | Total Time 14.00(14.00)\n",
      "Iter 0540 | Time 18.3448(18.1050) | Bit/dim 4.9757(5.0633) | Xent 1.8507(1.9239) | Loss 5.9010(6.0253) | Error 0.6389(0.6740) Steps 694(686.63) | Grad Norm 5.3364(12.6893) | Total Time 14.00(14.00)\n",
      "Iter 0550 | Time 18.7315(18.2656) | Bit/dim 4.9607(5.0363) | Xent 1.9317(1.9147) | Loss 5.9265(5.9936) | Error 0.6689(0.6725) Steps 700(689.10) | Grad Norm 9.0778(11.5900) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 107.5494, Epoch Time 1127.6153(957.7701), Bit/dim 4.9379(best: 5.0894), Xent 1.8587, Loss 5.8672, Error 0.6453(best: 0.6507)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0560 | Time 18.4596(18.3181) | Bit/dim 5.0292(5.0193) | Xent 1.9262(1.9072) | Loss 5.9924(5.9728) | Error 0.6978(0.6712) Steps 694(692.39) | Grad Norm 39.2190(13.6759) | Total Time 14.00(14.00)\n",
      "Iter 0570 | Time 17.1867(18.2739) | Bit/dim 4.9778(5.0342) | Xent 1.8917(1.9109) | Loss 5.9237(5.9896) | Error 0.6700(0.6718) Steps 670(690.74) | Grad Norm 11.4447(15.4728) | Total Time 14.00(14.00)\n",
      "Iter 0580 | Time 18.3529(18.2709) | Bit/dim 4.9198(5.0150) | Xent 1.8618(1.9055) | Loss 5.8507(5.9678) | Error 0.6400(0.6699) Steps 688(689.06) | Grad Norm 7.7860(13.4549) | Total Time 14.00(14.00)\n",
      "Iter 0590 | Time 18.9128(18.3148) | Bit/dim 4.8643(4.9909) | Xent 1.8141(1.8973) | Loss 5.7714(5.9396) | Error 0.6489(0.6662) Steps 706(691.40) | Grad Norm 5.7672(12.1464) | Total Time 14.00(14.00)\n",
      "Iter 0600 | Time 17.9979(18.3358) | Bit/dim 5.1517(5.0010) | Xent 1.9637(1.9015) | Loss 6.1336(5.9517) | Error 0.7067(0.6682) Steps 706(694.34) | Grad Norm 19.5756(15.5040) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 105.8707, Epoch Time 1133.0654(963.0290), Bit/dim 4.9303(best: 4.9379), Xent 1.8468, Loss 5.8537, Error 0.6458(best: 0.6453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0610 | Time 18.8779(18.3686) | Bit/dim 4.9244(4.9885) | Xent 1.8520(1.8973) | Loss 5.8504(5.9372) | Error 0.6389(0.6652) Steps 706(695.24) | Grad Norm 10.1144(14.8105) | Total Time 14.00(14.00)\n",
      "Iter 0620 | Time 18.8714(18.5017) | Bit/dim 4.8527(4.9591) | Xent 1.8075(1.8831) | Loss 5.7565(5.9007) | Error 0.6133(0.6612) Steps 718(699.53) | Grad Norm 5.5742(12.9409) | Total Time 14.00(14.00)\n",
      "Iter 0630 | Time 19.0264(18.6701) | Bit/dim 4.8423(4.9281) | Xent 1.8578(1.8722) | Loss 5.7712(5.8642) | Error 0.6456(0.6575) Steps 712(704.79) | Grad Norm 32.5130(12.7872) | Total Time 14.00(14.00)\n",
      "Iter 0640 | Time 18.1503(18.6591) | Bit/dim 5.1928(5.0402) | Xent 1.9036(1.8944) | Loss 6.1447(5.9874) | Error 0.6778(0.6676) Steps 694(709.58) | Grad Norm 12.4941(16.4767) | Total Time 14.00(14.00)\n",
      "Iter 0650 | Time 17.2631(18.4038) | Bit/dim 5.0681(5.0694) | Xent 1.9938(1.9164) | Loss 6.0650(6.0276) | Error 0.7067(0.6749) Steps 658(699.27) | Grad Norm 10.4606(16.0569) | Total Time 14.00(14.00)\n",
      "Iter 0660 | Time 18.0925(18.2531) | Bit/dim 4.9283(5.0456) | Xent 1.9272(1.9171) | Loss 5.8919(6.0041) | Error 0.6567(0.6745) Steps 664(689.47) | Grad Norm 8.0442(13.7004) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 104.7098, Epoch Time 1136.8951(968.2450), Bit/dim 4.9228(best: 4.9303), Xent 1.8440, Loss 5.8448, Error 0.6378(best: 0.6453)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0670 | Time 17.9090(18.1997) | Bit/dim 4.8634(5.0051) | Xent 1.8515(1.8982) | Loss 5.7892(5.9542) | Error 0.6722(0.6681) Steps 682(685.91) | Grad Norm 3.5230(11.3533) | Total Time 14.00(14.00)\n",
      "Iter 0680 | Time 18.3451(18.2787) | Bit/dim 4.7993(4.9577) | Xent 1.7645(1.8792) | Loss 5.6815(5.8973) | Error 0.6167(0.6602) Steps 700(687.54) | Grad Norm 6.0257(10.2578) | Total Time 14.00(14.00)\n",
      "Iter 0690 | Time 18.9095(18.4067) | Bit/dim 4.9315(4.9548) | Xent 2.0395(1.9504) | Loss 5.9512(5.9299) | Error 0.7011(0.6799) Steps 712(692.29) | Grad Norm 10.6408(15.5018) | Total Time 14.00(14.00)\n",
      "Iter 0700 | Time 18.5605(18.4663) | Bit/dim 4.8300(4.9330) | Xent 2.0089(1.9646) | Loss 5.8344(5.9154) | Error 0.7211(0.6859) Steps 688(693.92) | Grad Norm 14.4657(14.8142) | Total Time 14.00(14.00)\n",
      "Iter 0710 | Time 18.7486(18.5397) | Bit/dim 4.7811(4.8968) | Xent 1.9284(1.9629) | Loss 5.7453(5.8782) | Error 0.6933(0.6881) Steps 718(695.70) | Grad Norm 7.3670(12.7835) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 109.7392, Epoch Time 1146.5595(973.5944), Bit/dim 4.7561(best: 4.9228), Xent 1.8653, Loss 5.6888, Error 0.6489(best: 0.6378)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0720 | Time 19.2800(18.5938) | Bit/dim 4.7761(4.8650) | Xent 1.8675(1.9513) | Loss 5.7098(5.8406) | Error 0.6744(0.6853) Steps 712(698.53) | Grad Norm 16.0095(12.0086) | Total Time 14.00(14.00)\n",
      "Iter 0730 | Time 19.3023(18.7133) | Bit/dim 4.6829(4.8294) | Xent 1.8269(1.9283) | Loss 5.5963(5.7936) | Error 0.6556(0.6777) Steps 730(706.42) | Grad Norm 6.7400(10.1462) | Total Time 14.00(14.00)\n",
      "Iter 0740 | Time 19.2853(18.8214) | Bit/dim 4.8954(4.8483) | Xent 1.9339(1.9251) | Loss 5.8623(5.8109) | Error 0.6789(0.6785) Steps 730(713.48) | Grad Norm 18.6533(12.9133) | Total Time 14.00(14.00)\n",
      "Iter 0750 | Time 18.6803(18.8901) | Bit/dim 4.8485(4.8577) | Xent 1.9045(1.9329) | Loss 5.8007(5.8241) | Error 0.6844(0.6811) Steps 718(715.93) | Grad Norm 8.0055(12.8128) | Total Time 14.00(14.00)\n",
      "Iter 0760 | Time 19.3032(18.9872) | Bit/dim 4.7397(4.8335) | Xent 1.8677(1.9155) | Loss 5.6736(5.7913) | Error 0.6500(0.6755) Steps 736(719.28) | Grad Norm 4.5659(11.1860) | Total Time 14.00(14.00)\n",
      "Iter 0770 | Time 19.2750(19.1218) | Bit/dim 4.6974(4.8023) | Xent 1.7507(1.8959) | Loss 5.5728(5.7503) | Error 0.6322(0.6695) Steps 742(725.22) | Grad Norm 3.8542(9.4741) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 114.2354, Epoch Time 1187.1004(979.9996), Bit/dim 4.6794(best: 4.7561), Xent 1.7674, Loss 5.5631, Error 0.6143(best: 0.6378)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0780 | Time 19.3180(19.2313) | Bit/dim 4.6554(4.7681) | Xent 1.7679(1.8654) | Loss 5.5394(5.7008) | Error 0.6344(0.6612) Steps 760(730.81) | Grad Norm 5.8395(8.2827) | Total Time 14.00(14.00)\n",
      "Iter 0790 | Time 19.5517(19.3558) | Bit/dim 4.6687(4.7418) | Xent 1.7850(1.8396) | Loss 5.5612(5.6616) | Error 0.6322(0.6509) Steps 766(738.45) | Grad Norm 11.3420(8.6204) | Total Time 14.00(14.00)\n",
      "Iter 0800 | Time 19.8306(19.4885) | Bit/dim 4.6536(4.7176) | Xent 1.7773(1.8206) | Loss 5.5423(5.6279) | Error 0.6322(0.6452) Steps 766(744.95) | Grad Norm 7.8732(8.7270) | Total Time 14.00(14.00)\n",
      "Iter 0810 | Time 19.8544(19.6453) | Bit/dim 4.6450(4.6934) | Xent 1.7794(1.8058) | Loss 5.5347(5.5963) | Error 0.6344(0.6387) Steps 784(751.86) | Grad Norm 13.1526(9.4924) | Total Time 14.00(14.00)\n",
      "Iter 0820 | Time 20.0917(19.8046) | Bit/dim 4.6737(4.6768) | Xent 1.8342(1.7958) | Loss 5.5909(5.5747) | Error 0.6256(0.6345) Steps 772(759.06) | Grad Norm 24.1185(10.3834) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 118.6826, Epoch Time 1233.0858(987.5922), Bit/dim 4.7343(best: 4.6794), Xent 1.8601, Loss 5.6644, Error 0.6679(best: 0.6143)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0830 | Time 20.8823(19.9529) | Bit/dim 4.6808(4.6797) | Xent 1.8510(1.8007) | Loss 5.6063(5.5801) | Error 0.6411(0.6382) Steps 802(766.23) | Grad Norm 18.4523(12.7552) | Total Time 14.00(14.00)\n",
      "Iter 0840 | Time 20.5677(20.0593) | Bit/dim 4.6365(4.6674) | Xent 1.7416(1.7928) | Loss 5.5073(5.5638) | Error 0.5989(0.6348) Steps 784(770.54) | Grad Norm 8.3816(12.2738) | Total Time 14.00(14.00)\n",
      "Iter 0850 | Time 21.0545(20.1986) | Bit/dim 4.5744(4.6443) | Xent 1.6771(1.7777) | Loss 5.4129(5.5331) | Error 0.5878(0.6305) Steps 814(778.30) | Grad Norm 4.0825(11.9426) | Total Time 14.00(14.00)\n",
      "Iter 0860 | Time 20.7392(20.3595) | Bit/dim 4.5918(4.6179) | Xent 1.7448(1.7595) | Loss 5.4642(5.4977) | Error 0.6111(0.6237) Steps 814(787.45) | Grad Norm 14.9544(10.7141) | Total Time 14.00(14.00)\n",
      "Iter 0870 | Time 21.3133(20.5553) | Bit/dim 4.5691(4.6018) | Xent 1.8365(1.7551) | Loss 5.4874(5.4794) | Error 0.6600(0.6235) Steps 820(796.20) | Grad Norm 20.8625(11.3625) | Total Time 14.00(14.00)\n",
      "Iter 0880 | Time 21.1899(20.7921) | Bit/dim 4.5109(4.5791) | Xent 1.7736(1.7606) | Loss 5.3977(5.4594) | Error 0.6200(0.6272) Steps 838(805.51) | Grad Norm 5.9982(11.5498) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 130.9665, Epoch Time 1292.6384(996.7436), Bit/dim 4.5015(best: 4.6794), Xent 1.6643, Loss 5.3336, Error 0.5934(best: 0.6143)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0890 | Time 21.5554(20.9519) | Bit/dim 4.5288(4.5592) | Xent 1.6813(1.7481) | Loss 5.3695(5.4333) | Error 0.6078(0.6227) Steps 826(812.79) | Grad Norm 15.6761(10.7397) | Total Time 14.00(14.00)\n",
      "Iter 0900 | Time 20.7613(20.9809) | Bit/dim 4.6369(4.5756) | Xent 1.8631(1.7625) | Loss 5.5685(5.4569) | Error 0.6856(0.6266) Steps 808(815.39) | Grad Norm 22.2284(13.2723) | Total Time 14.00(14.00)\n",
      "Iter 0910 | Time 21.3128(21.0802) | Bit/dim 4.5422(4.5630) | Xent 1.7691(1.7640) | Loss 5.4268(5.4450) | Error 0.6200(0.6289) Steps 832(819.04) | Grad Norm 12.6225(12.2041) | Total Time 14.00(14.00)\n",
      "Iter 0920 | Time 21.9199(21.1335) | Bit/dim 4.5123(4.5500) | Xent 1.7576(1.7644) | Loss 5.3911(5.4322) | Error 0.6289(0.6288) Steps 832(818.94) | Grad Norm 12.5601(12.7622) | Total Time 14.00(14.00)\n",
      "Iter 0930 | Time 21.2528(21.1290) | Bit/dim 4.4574(4.5334) | Xent 1.6926(1.7525) | Loss 5.3037(5.4096) | Error 0.6078(0.6233) Steps 814(820.15) | Grad Norm 6.0722(12.0680) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 125.2028, Epoch Time 1312.1112(1006.2046), Bit/dim 4.4952(best: 4.5015), Xent 1.6089, Loss 5.2996, Error 0.5675(best: 0.5934)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 0940 | Time 20.7376(21.1234) | Bit/dim 4.4310(4.5160) | Xent 1.6620(1.7348) | Loss 5.2620(5.3834) | Error 0.5844(0.6175) Steps 814(819.19) | Grad Norm 6.3987(11.1025) | Total Time 14.00(14.00)\n",
      "Iter 0950 | Time 21.3494(21.1271) | Bit/dim 4.4419(4.5010) | Xent 1.6270(1.7137) | Loss 5.2554(5.3579) | Error 0.5856(0.6094) Steps 838(818.76) | Grad Norm 6.5144(10.8482) | Total Time 14.00(14.00)\n",
      "Iter 0960 | Time 21.2523(21.1579) | Bit/dim 4.4484(4.4866) | Xent 1.6502(1.6995) | Loss 5.2736(5.3363) | Error 0.5944(0.6071) Steps 814(820.11) | Grad Norm 11.4582(10.9758) | Total Time 14.00(14.00)\n",
      "Iter 0970 | Time 21.5952(21.2910) | Bit/dim 4.4601(4.4741) | Xent 1.7228(1.6968) | Loss 5.3215(5.3225) | Error 0.5922(0.6059) Steps 838(824.15) | Grad Norm 11.0778(11.4583) | Total Time 14.00(14.00)\n",
      "Iter 0980 | Time 21.2801(21.3657) | Bit/dim 4.4417(4.4652) | Xent 1.6186(1.6983) | Loss 5.2509(5.3143) | Error 0.5700(0.6062) Steps 832(827.89) | Grad Norm 12.0372(11.5295) | Total Time 14.00(14.00)\n",
      "Iter 0990 | Time 21.7555(21.3479) | Bit/dim 4.4427(4.4596) | Xent 1.7438(1.7193) | Loss 5.3146(5.3193) | Error 0.6144(0.6122) Steps 826(826.15) | Grad Norm 8.1686(12.7131) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 124.9419, Epoch Time 1316.2449(1015.5058), Bit/dim 4.4422(best: 4.4952), Xent 1.7040, Loss 5.2942, Error 0.6167(best: 0.5675)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1000 | Time 21.3331(21.3471) | Bit/dim 4.4326(4.4482) | Xent 1.6784(1.7173) | Loss 5.2718(5.3068) | Error 0.5844(0.6098) Steps 814(824.22) | Grad Norm 11.7521(11.4378) | Total Time 14.00(14.00)\n",
      "Iter 1010 | Time 20.8285(21.2625) | Bit/dim 4.3913(4.4369) | Xent 1.6220(1.6983) | Loss 5.2022(5.2861) | Error 0.5811(0.6037) Steps 808(820.67) | Grad Norm 9.0238(10.2326) | Total Time 14.00(14.00)\n",
      "Iter 1020 | Time 21.5793(21.3453) | Bit/dim 4.3747(4.4203) | Xent 1.6379(1.6822) | Loss 5.1937(5.2615) | Error 0.5811(0.5990) Steps 826(823.58) | Grad Norm 7.8917(9.3792) | Total Time 14.00(14.00)\n",
      "Iter 1030 | Time 20.2297(21.3515) | Bit/dim 4.4606(4.4204) | Xent 1.7692(1.6996) | Loss 5.3452(5.2703) | Error 0.6144(0.6038) Steps 802(825.67) | Grad Norm 12.9722(11.6369) | Total Time 14.00(14.00)\n",
      "Iter 1040 | Time 21.5928(21.3914) | Bit/dim 4.3775(4.4215) | Xent 1.7045(1.7161) | Loss 5.2297(5.2795) | Error 0.6178(0.6089) Steps 826(828.56) | Grad Norm 10.5352(11.4721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 127.0994, Epoch Time 1320.7076(1024.6619), Bit/dim 4.3812(best: 4.4422), Xent 1.5852, Loss 5.1738, Error 0.5567(best: 0.5675)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1050 | Time 21.8448(21.4165) | Bit/dim 4.3986(4.4102) | Xent 1.6572(1.6970) | Loss 5.2272(5.2587) | Error 0.5811(0.6050) Steps 844(829.16) | Grad Norm 17.5695(11.0324) | Total Time 14.00(14.00)\n",
      "Iter 1060 | Time 21.6694(21.5203) | Bit/dim 4.3602(4.3991) | Xent 1.5583(1.6756) | Loss 5.1393(5.2369) | Error 0.5767(0.5979) Steps 850(833.53) | Grad Norm 8.2497(10.1336) | Total Time 14.00(14.00)\n",
      "Iter 1070 | Time 22.0441(21.5718) | Bit/dim 4.3159(4.3864) | Xent 1.5694(1.6596) | Loss 5.1006(5.2162) | Error 0.5711(0.5937) Steps 850(836.09) | Grad Norm 3.9733(10.5131) | Total Time 14.00(14.00)\n",
      "Iter 1080 | Time 21.4758(21.5824) | Bit/dim 4.3735(4.3774) | Xent 1.5628(1.6484) | Loss 5.1549(5.2016) | Error 0.5633(0.5906) Steps 826(836.63) | Grad Norm 13.8624(11.0514) | Total Time 14.00(14.00)\n",
      "Iter 1090 | Time 21.8150(21.6201) | Bit/dim 4.3619(4.3776) | Xent 1.5542(1.6503) | Loss 5.1390(5.2027) | Error 0.5622(0.5917) Steps 838(836.53) | Grad Norm 6.9515(11.8512) | Total Time 14.00(14.00)\n",
      "Iter 1100 | Time 21.8293(21.6930) | Bit/dim 4.3420(4.3675) | Xent 1.5732(1.6416) | Loss 5.1287(5.1883) | Error 0.5600(0.5890) Steps 838(837.20) | Grad Norm 10.2436(11.4476) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 127.7356, Epoch Time 1340.4247(1034.1347), Bit/dim 4.3375(best: 4.3812), Xent 1.5146, Loss 5.0948, Error 0.5458(best: 0.5567)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1110 | Time 22.2529(21.8063) | Bit/dim 4.3047(4.3569) | Xent 1.5293(1.6197) | Loss 5.0694(5.1668) | Error 0.5400(0.5815) Steps 850(841.43) | Grad Norm 4.2148(11.1874) | Total Time 14.00(14.00)\n",
      "Iter 1120 | Time 22.2414(21.8537) | Bit/dim 4.3254(4.3483) | Xent 1.6139(1.6020) | Loss 5.1324(5.1493) | Error 0.5667(0.5752) Steps 862(844.05) | Grad Norm 8.7532(10.0940) | Total Time 14.00(14.00)\n",
      "Iter 1130 | Time 22.1442(21.8586) | Bit/dim 4.4129(4.3646) | Xent 1.9456(1.6939) | Loss 5.3858(5.2116) | Error 0.7122(0.6041) Steps 844(845.23) | Grad Norm 8.1522(12.9616) | Total Time 14.00(14.00)\n",
      "Iter 1140 | Time 21.0970(21.6909) | Bit/dim 4.3605(4.3690) | Xent 1.7147(1.7119) | Loss 5.2179(5.2250) | Error 0.6367(0.6118) Steps 814(836.47) | Grad Norm 8.0187(11.2904) | Total Time 14.00(14.00)\n",
      "Iter 1150 | Time 21.2404(21.5688) | Bit/dim 4.2931(4.3584) | Xent 1.6540(1.7008) | Loss 5.1201(5.2088) | Error 0.6056(0.6089) Steps 826(831.97) | Grad Norm 4.4419(9.5637) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 126.7610, Epoch Time 1335.1140(1043.1641), Bit/dim 4.3063(best: 4.3375), Xent 1.5396, Loss 5.0761, Error 0.5506(best: 0.5458)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1160 | Time 21.3715(21.5066) | Bit/dim 4.2343(4.3429) | Xent 1.6208(1.6786) | Loss 5.0447(5.1821) | Error 0.5767(0.6013) Steps 826(830.41) | Grad Norm 9.3562(8.5527) | Total Time 14.00(14.00)\n",
      "Iter 1170 | Time 21.7657(21.5588) | Bit/dim 4.2567(4.3319) | Xent 1.5758(1.6556) | Loss 5.0446(5.1597) | Error 0.5844(0.5940) Steps 826(831.99) | Grad Norm 6.6722(8.9668) | Total Time 14.00(14.00)\n",
      "Iter 1180 | Time 21.6932(21.6297) | Bit/dim 4.2642(4.3180) | Xent 1.4892(1.6302) | Loss 5.0088(5.1331) | Error 0.5556(0.5850) Steps 820(834.28) | Grad Norm 6.2429(8.7549) | Total Time 14.00(14.00)\n",
      "Iter 1190 | Time 21.5424(21.6354) | Bit/dim 4.2634(4.3077) | Xent 1.5434(1.6104) | Loss 5.0351(5.1129) | Error 0.5511(0.5775) Steps 832(834.83) | Grad Norm 10.0780(8.9706) | Total Time 14.00(14.00)\n",
      "Iter 1200 | Time 21.8813(21.6692) | Bit/dim 4.2670(4.2970) | Xent 1.5154(1.5919) | Loss 5.0247(5.0929) | Error 0.5278(0.5685) Steps 844(836.86) | Grad Norm 10.9783(8.8762) | Total Time 14.00(14.00)\n",
      "Iter 1210 | Time 22.9553(21.8253) | Bit/dim 4.2577(4.2901) | Xent 1.5334(1.5906) | Loss 5.0244(5.0854) | Error 0.5533(0.5691) Steps 886(844.09) | Grad Norm 10.4482(9.6902) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 131.9062, Epoch Time 1348.5025(1052.3243), Bit/dim 4.2553(best: 4.3063), Xent 1.4344, Loss 4.9725, Error 0.5178(best: 0.5458)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1220 | Time 22.3640(21.9367) | Bit/dim 4.2692(4.2803) | Xent 1.5487(1.5709) | Loss 5.0435(5.0658) | Error 0.5678(0.5634) Steps 850(849.35) | Grad Norm 11.0260(9.6660) | Total Time 14.00(14.00)\n",
      "Iter 1230 | Time 22.3344(21.9921) | Bit/dim 4.2545(4.2718) | Xent 1.5339(1.5537) | Loss 5.0215(5.0486) | Error 0.5444(0.5557) Steps 856(852.31) | Grad Norm 9.7013(9.3904) | Total Time 14.00(14.00)\n",
      "Iter 1240 | Time 21.7027(21.9678) | Bit/dim 4.3310(4.2818) | Xent 1.8830(1.5739) | Loss 5.2725(5.0687) | Error 0.6678(0.5617) Steps 844(852.06) | Grad Norm 24.6491(11.8747) | Total Time 14.00(14.00)\n",
      "Iter 1250 | Time 22.8767(22.0569) | Bit/dim 4.2620(4.2805) | Xent 1.5158(1.5821) | Loss 5.0199(5.0715) | Error 0.5367(0.5642) Steps 874(857.10) | Grad Norm 5.0096(11.2463) | Total Time 14.00(14.00)\n",
      "Iter 1260 | Time 22.4659(22.1085) | Bit/dim 4.2620(4.2739) | Xent 1.6023(1.5760) | Loss 5.0632(5.0619) | Error 0.5800(0.5630) Steps 874(859.31) | Grad Norm 22.7838(10.8963) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 130.2509, Epoch Time 1368.3180(1061.8041), Bit/dim 4.2307(best: 4.2553), Xent 1.4455, Loss 4.9535, Error 0.5200(best: 0.5178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1270 | Time 21.8818(22.1066) | Bit/dim 4.2141(4.2634) | Xent 1.4978(1.5629) | Loss 4.9630(5.0448) | Error 0.5367(0.5596) Steps 850(858.56) | Grad Norm 7.7146(9.7977) | Total Time 14.00(14.00)\n",
      "Iter 1280 | Time 22.9997(22.0635) | Bit/dim 4.2274(4.2553) | Xent 1.4088(1.5412) | Loss 4.9318(5.0259) | Error 0.5178(0.5511) Steps 874(856.02) | Grad Norm 5.2409(9.9924) | Total Time 14.00(14.00)\n",
      "Iter 1290 | Time 22.4250(22.0425) | Bit/dim 4.2070(4.2464) | Xent 1.5377(1.5277) | Loss 4.9758(5.0103) | Error 0.5389(0.5460) Steps 862(855.18) | Grad Norm 7.4341(9.2841) | Total Time 14.00(14.00)\n",
      "Iter 1300 | Time 22.4954(22.0444) | Bit/dim 4.2402(4.2436) | Xent 1.4920(1.5270) | Loss 4.9862(5.0071) | Error 0.5422(0.5454) Steps 868(856.33) | Grad Norm 7.1413(10.8102) | Total Time 14.00(14.00)\n",
      "Iter 1310 | Time 21.9736(22.0435) | Bit/dim 4.2002(4.2357) | Xent 1.5592(1.5282) | Loss 4.9798(4.9998) | Error 0.5333(0.5462) Steps 844(855.98) | Grad Norm 12.4032(10.8782) | Total Time 14.00(14.00)\n",
      "Iter 1320 | Time 21.9106(22.0654) | Bit/dim 4.2169(4.2282) | Xent 1.5791(1.5187) | Loss 5.0064(4.9875) | Error 0.5478(0.5421) Steps 838(856.83) | Grad Norm 17.2056(10.6530) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 128.8681, Epoch Time 1356.7301(1070.6519), Bit/dim 4.2157(best: 4.2307), Xent 1.3864, Loss 4.9089, Error 0.4974(best: 0.5178)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1330 | Time 22.2516(22.0867) | Bit/dim 4.1948(4.2204) | Xent 1.4512(1.5019) | Loss 4.9204(4.9713) | Error 0.5011(0.5369) Steps 874(856.88) | Grad Norm 5.7332(10.4838) | Total Time 14.00(14.00)\n",
      "Iter 1340 | Time 21.7701(22.0722) | Bit/dim 4.2578(4.2210) | Xent 1.6492(1.4995) | Loss 5.0824(4.9707) | Error 0.5567(0.5360) Steps 850(857.33) | Grad Norm 29.9904(11.8764) | Total Time 14.00(14.00)\n",
      "Iter 1350 | Time 21.7930(22.0120) | Bit/dim 4.2442(4.2286) | Xent 1.5306(1.5187) | Loss 5.0096(4.9879) | Error 0.5456(0.5423) Steps 868(855.85) | Grad Norm 8.3286(12.3636) | Total Time 14.00(14.00)\n",
      "Iter 1360 | Time 22.2534(22.0051) | Bit/dim 4.1817(4.2186) | Xent 1.4674(1.5105) | Loss 4.9154(4.9739) | Error 0.5300(0.5412) Steps 862(854.77) | Grad Norm 4.2998(10.7922) | Total Time 14.00(14.00)\n",
      "Iter 1370 | Time 22.0186(22.0556) | Bit/dim 4.1998(4.2094) | Xent 1.4170(1.4964) | Loss 4.9083(4.9576) | Error 0.5056(0.5364) Steps 844(852.38) | Grad Norm 8.2537(9.6471) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 127.7073, Epoch Time 1358.0615(1079.2741), Bit/dim 4.1732(best: 4.2157), Xent 1.3575, Loss 4.8519, Error 0.4916(best: 0.4974)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1380 | Time 22.0930(22.0196) | Bit/dim 4.1815(4.2029) | Xent 1.4151(1.4880) | Loss 4.8891(4.9469) | Error 0.4978(0.5320) Steps 862(852.05) | Grad Norm 7.0227(10.6450) | Total Time 14.00(14.00)\n",
      "Iter 1390 | Time 22.1722(22.0180) | Bit/dim 4.1683(4.1986) | Xent 1.5304(1.4937) | Loss 4.9335(4.9454) | Error 0.5478(0.5331) Steps 850(852.59) | Grad Norm 10.4932(10.9695) | Total Time 14.00(14.00)\n",
      "Iter 1400 | Time 21.9547(22.0212) | Bit/dim 4.1286(4.1909) | Xent 1.3966(1.4799) | Loss 4.8269(4.9309) | Error 0.5044(0.5287) Steps 850(853.70) | Grad Norm 3.6388(10.3040) | Total Time 14.00(14.00)\n",
      "Iter 1410 | Time 21.6278(22.0288) | Bit/dim 4.2725(4.1871) | Xent 1.5301(1.4698) | Loss 5.0376(4.9220) | Error 0.5622(0.5257) Steps 820(853.88) | Grad Norm 21.5908(10.3847) | Total Time 14.00(14.00)\n",
      "Iter 1420 | Time 22.0957(22.0998) | Bit/dim 4.1861(4.1838) | Xent 1.4452(1.4713) | Loss 4.9086(4.9194) | Error 0.5178(0.5263) Steps 856(853.80) | Grad Norm 7.6805(10.8881) | Total Time 14.00(14.00)\n",
      "Iter 1430 | Time 22.2804(22.0843) | Bit/dim 4.1248(4.1796) | Xent 1.4448(1.4735) | Loss 4.8472(4.9163) | Error 0.5356(0.5278) Steps 880(855.39) | Grad Norm 4.8795(11.0427) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 128.9755, Epoch Time 1360.1053(1087.6991), Bit/dim 4.1495(best: 4.1732), Xent 1.3545, Loss 4.8267, Error 0.4874(best: 0.4916)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1440 | Time 22.3855(22.1100) | Bit/dim 4.1395(4.1742) | Xent 1.4299(1.4559) | Loss 4.8544(4.9021) | Error 0.5011(0.5214) Steps 874(854.08) | Grad Norm 11.1743(10.0762) | Total Time 14.00(14.00)\n",
      "Iter 1450 | Time 22.0962(22.1739) | Bit/dim 4.1585(4.1652) | Xent 1.3630(1.4450) | Loss 4.8400(4.8877) | Error 0.4989(0.5171) Steps 850(856.48) | Grad Norm 4.8948(9.9141) | Total Time 14.00(14.00)\n",
      "Iter 1460 | Time 22.3481(22.2415) | Bit/dim 4.1209(4.1573) | Xent 1.3992(1.4315) | Loss 4.8205(4.8731) | Error 0.4878(0.5119) Steps 856(858.85) | Grad Norm 14.6145(10.0352) | Total Time 14.00(14.00)\n",
      "Iter 1470 | Time 22.4552(22.2438) | Bit/dim 4.1490(4.1564) | Xent 1.4895(1.4424) | Loss 4.8938(4.8776) | Error 0.5467(0.5142) Steps 880(859.90) | Grad Norm 7.8458(10.5888) | Total Time 14.00(14.00)\n",
      "Iter 1480 | Time 22.5536(22.2901) | Bit/dim 4.1405(4.1533) | Xent 1.4207(1.4370) | Loss 4.8509(4.8718) | Error 0.4778(0.5122) Steps 862(861.28) | Grad Norm 8.3712(9.9845) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 127.8314, Epoch Time 1371.7150(1096.2196), Bit/dim 4.1283(best: 4.1495), Xent 1.3003, Loss 4.7785, Error 0.4625(best: 0.4874)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1490 | Time 21.8968(22.2372) | Bit/dim 4.0856(4.1444) | Xent 1.3901(1.4261) | Loss 4.7806(4.8575) | Error 0.5156(0.5098) Steps 856(858.87) | Grad Norm 4.3043(9.6086) | Total Time 14.00(14.00)\n",
      "Iter 1500 | Time 22.3689(22.2514) | Bit/dim 4.1246(4.1373) | Xent 1.4461(1.4183) | Loss 4.8476(4.8464) | Error 0.5122(0.5072) Steps 868(858.97) | Grad Norm 13.5910(10.3973) | Total Time 14.00(14.00)\n",
      "Iter 1510 | Time 22.5370(22.2702) | Bit/dim 4.1203(4.1437) | Xent 1.4484(1.4222) | Loss 4.8445(4.8548) | Error 0.5300(0.5093) Steps 856(858.83) | Grad Norm 5.8887(10.9491) | Total Time 14.00(14.00)\n",
      "Iter 1520 | Time 22.1156(22.2440) | Bit/dim 4.1284(4.1373) | Xent 1.3513(1.4183) | Loss 4.8040(4.8465) | Error 0.4711(0.5075) Steps 856(856.36) | Grad Norm 8.6359(10.9206) | Total Time 14.00(14.00)\n",
      "Iter 1530 | Time 21.9197(22.1702) | Bit/dim 4.1286(4.1290) | Xent 1.3748(1.4044) | Loss 4.8160(4.8312) | Error 0.4778(0.5029) Steps 850(853.99) | Grad Norm 9.3655(9.7608) | Total Time 14.00(14.00)\n",
      "Iter 1540 | Time 21.5576(22.1208) | Bit/dim 4.0990(4.1236) | Xent 1.3681(1.3945) | Loss 4.7831(4.8209) | Error 0.4678(0.4993) Steps 856(852.73) | Grad Norm 11.1611(9.6255) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 125.4608, Epoch Time 1361.8037(1104.1871), Bit/dim 4.1170(best: 4.1283), Xent 1.4221, Loss 4.8280, Error 0.5101(best: 0.4625)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1550 | Time 22.0131(22.0993) | Bit/dim 4.0739(4.1179) | Xent 1.3672(1.3853) | Loss 4.7576(4.8106) | Error 0.5067(0.4949) Steps 850(852.69) | Grad Norm 4.1360(9.4507) | Total Time 14.00(14.00)\n",
      "Iter 1560 | Time 22.2641(22.0629) | Bit/dim 4.1250(4.1250) | Xent 1.5300(1.4150) | Loss 4.8900(4.8325) | Error 0.5256(0.5038) Steps 868(853.11) | Grad Norm 9.4234(11.1008) | Total Time 14.00(14.00)\n",
      "Iter 1570 | Time 22.5946(22.0959) | Bit/dim 4.1470(4.1256) | Xent 1.4256(1.4228) | Loss 4.8598(4.8370) | Error 0.5000(0.5073) Steps 868(853.72) | Grad Norm 12.2668(10.0813) | Total Time 14.00(14.00)\n",
      "Iter 1580 | Time 21.2195(22.0432) | Bit/dim 4.1026(4.1195) | Xent 1.3291(1.4151) | Loss 4.7671(4.8270) | Error 0.4900(0.5049) Steps 826(849.63) | Grad Norm 12.0854(10.3863) | Total Time 14.00(14.00)\n",
      "Iter 1590 | Time 22.0905(21.9551) | Bit/dim 4.0868(4.1144) | Xent 1.3283(1.4017) | Loss 4.7510(4.8153) | Error 0.4744(0.5013) Steps 850(847.37) | Grad Norm 5.7644(9.5809) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 126.9823, Epoch Time 1352.1188(1111.6250), Bit/dim 4.0832(best: 4.1170), Xent 1.2410, Loss 4.7037, Error 0.4453(best: 0.4625)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 1600 | Time 21.9941(21.9538) | Bit/dim 4.0366(4.1070) | Xent 1.2740(1.3764) | Loss 4.6736(4.7952) | Error 0.4511(0.4919) Steps 850(847.62) | Grad Norm 4.4271(8.6414) | Total Time 14.00(14.00)\n",
      "Iter 1610 | Time 22.1189(21.9826) | Bit/dim 4.0575(4.0970) | Xent 1.3415(1.3575) | Loss 4.7282(4.7758) | Error 0.5000(0.4851) Steps 844(849.11) | Grad Norm 9.1009(8.1858) | Total Time 14.00(14.00)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_drop_0_5_sn10_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5 --dropout_rate 0.5 --spectral_norm True --spectral_norm_niter 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
