{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='4,5,6,7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_2cond.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import torch.utils.data as data\n",
      "from torch.utils.data import Dataset\n",
      "\n",
      "from PIL import Image\n",
      "import os.path\n",
      "import errno\n",
      "import codecs\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"colormnist\", \"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "parser.add_argument(\"--y_class\", type=int, default=10)\n",
      "parser.add_argument(\"--y_color\", type=int, default=10)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional_2cond as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "class ColorMNIST(data.Dataset):\n",
      "    \"\"\"\n",
      "    ColorMNIST\n",
      "    \"\"\"\n",
      "    urls = [\n",
      "        'http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz',\n",
      "        'http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz',\n",
      "    ]\n",
      "    raw_folder = 'raw'\n",
      "    processed_folder = 'processed'\n",
      "    training_file = 'training.pt'\n",
      "    test_file = 'test.pt'\n",
      "\n",
      "    def __init__(self, root, train=True, transform=None, target_transform=None, download=False):\n",
      "        self.root = os.path.expanduser(root)\n",
      "        self.transform = transform\n",
      "        self.target_transform = target_transform\n",
      "        self.train = train  # training set or test set\n",
      "\n",
      "        if download:\n",
      "            self.download()\n",
      "\n",
      "        if not self._check_exists():\n",
      "            raise RuntimeError('Dataset not found.' +\n",
      "                               ' You can use download=True to download it')\n",
      "\n",
      "        if self.train:\n",
      "            self.train_data, self.train_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.training_file))\n",
      "            \n",
      "            self.train_data = np.tile(self.train_data[:, :, :, np.newaxis], 3)\n",
      "        else:\n",
      "            self.test_data, self.test_labels = torch.load(\n",
      "                os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "            \n",
      "            self.test_data = np.tile(self.test_data[:, :, :, np.newaxis], 3)\n",
      "        \n",
      "        self.pallette = [[31, 119, 180],\n",
      "                         [255, 127, 14],\n",
      "                         [44, 160, 44],\n",
      "                         [214, 39, 40],\n",
      "                         [148, 103, 189],\n",
      "                         [140, 86, 75],\n",
      "                         [227, 119, 194],\n",
      "                         [127, 127, 127],\n",
      "                         [188, 189, 34],\n",
      "                         [23, 190, 207]]\n",
      "\n",
      "    def __getitem__(self, index):\n",
      "        \"\"\"\n",
      "        Args:\n",
      "            index (int): Index\n",
      "\n",
      "        Returns:\n",
      "            tuple: (image, target) where target is index of the target class.\n",
      "        \"\"\"\n",
      "        if self.train:\n",
      "            img, target = self.train_data[index].copy(), self.train_labels[index]\n",
      "        else:\n",
      "            img, target = self.test_data[index].copy(), self.test_labels[index]\n",
      "        \n",
      "        # doing this so that it is consistent with all other datasets\n",
      "        # to return a PIL Image\n",
      "        y_color_digit = np.random.randint(0, args.y_color)\n",
      "        c_digit = self.pallette[y_color_digit]\n",
      "        \n",
      "        img[:, :, 0] = img[:, :, 0] / 255 * c_digit[0]\n",
      "        img[:, :, 1] = img[:, :, 1] / 255 * c_digit[1]\n",
      "        img[:, :, 2] = img[:, :, 2] / 255 * c_digit[2]\n",
      "        \n",
      "        img = Image.fromarray(img)\n",
      "\n",
      "        if self.transform is not None:\n",
      "            img = self.transform(img)\n",
      "\n",
      "        if self.target_transform is not None:\n",
      "            target = self.target_transform(target)\n",
      "\n",
      "        return img, [target,torch.from_numpy(np.array(y_color_digit))]\n",
      "\n",
      "    def __len__(self):\n",
      "        if self.train:\n",
      "            return len(self.train_data)\n",
      "        else:\n",
      "            return len(self.test_data)\n",
      "\n",
      "    def _check_exists(self):\n",
      "        return os.path.exists(os.path.join(self.root, self.processed_folder, self.training_file)) and \\\n",
      "            os.path.exists(os.path.join(self.root, self.processed_folder, self.test_file))\n",
      "\n",
      "    def download(self):\n",
      "        \"\"\"Download the MNIST data if it doesn't exist in processed_folder already.\"\"\"\n",
      "        from six.moves import urllib\n",
      "        import gzip\n",
      "\n",
      "        if self._check_exists():\n",
      "            return\n",
      "\n",
      "        # download files\n",
      "        try:\n",
      "            os.makedirs(os.path.join(self.root, self.raw_folder))\n",
      "            os.makedirs(os.path.join(self.root, self.processed_folder))\n",
      "        except OSError as e:\n",
      "            if e.errno == errno.EEXIST:\n",
      "                pass\n",
      "            else:\n",
      "                raise\n",
      "\n",
      "        for url in self.urls:\n",
      "            print('Downloading ' + url)\n",
      "            data = urllib.request.urlopen(url)\n",
      "            filename = url.rpartition('/')[2]\n",
      "            file_path = os.path.join(self.root, self.raw_folder, filename)\n",
      "            with open(file_path, 'wb') as f:\n",
      "                f.write(data.read())\n",
      "            with open(file_path.replace('.gz', ''), 'wb') as out_f, \\\n",
      "                    gzip.GzipFile(file_path) as zip_f:\n",
      "                out_f.write(zip_f.read())\n",
      "            os.unlink(file_path)\n",
      "\n",
      "        # process and save as torch files\n",
      "        print('Processing...')\n",
      "\n",
      "        training_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 'train-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 'train-labels-idx1-ubyte'))\n",
      "        )\n",
      "        test_set = (\n",
      "            read_image_file(os.path.join(self.root, self.raw_folder, 't10k-images-idx3-ubyte')),\n",
      "            read_label_file(os.path.join(self.root, self.raw_folder, 't10k-labels-idx1-ubyte'))\n",
      "        )\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.training_file), 'wb') as f:\n",
      "            torch.save(training_set, f)\n",
      "        with open(os.path.join(self.root, self.processed_folder, self.test_file), 'wb') as f:\n",
      "            torch.save(test_set, f)\n",
      "\n",
      "        print('Done!')\n",
      "\n",
      "    def __repr__(self):\n",
      "        fmt_str = 'Dataset ' + self.__class__.__name__ + '\\n'\n",
      "        fmt_str += '    Number of datapoints: {}\\n'.format(self.__len__())\n",
      "        tmp = 'train' if self.train is True else 'test'\n",
      "        fmt_str += '    Split: {}\\n'.format(tmp)\n",
      "        fmt_str += '    Root Location: {}\\n'.format(self.root)\n",
      "        tmp = '    Transforms (if any): '\n",
      "        fmt_str += '{0}{1}\\n'.format(tmp, self.transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        tmp = '    Target Transforms (if any): '\n",
      "        fmt_str += '{0}{1}'.format(tmp, self.target_transform.__repr__().replace('\\n', '\\n' + ' ' * len(tmp)))\n",
      "        return fmt_str\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"colormnist\":\n",
      "        im_dim = 3\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = ColorMNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = ColorMNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, y_color, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "    y_onehot_color = thops.onehot(y_color, num_classes=model.module.y_color).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "    mean_color, logs_color = model.module._prior_color(y_onehot_color)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_color_sup = modules.GaussianDiag.logp(mean_color, logs_color, z[:, dim_sup:(2*dim_sup)]).view(-1,1)  # logp(z)_color_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, (2*dim_sup):]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_color_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "        zcolorsup = model.module.dropout_color(z[:, dim_sup:(2*dim_sup)])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "        zcolorsup = z[:, dim_sup:(2*dim_sup)]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "    \n",
      "    y_logits_color = model.module.project_color(zcolorsup)\n",
      "    loss_xent_color = model.module.loss_class(y_logits_color, y_color.to(x.get_device()))\n",
      "    y_color_predicted = np.argmax(y_logits_color.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, loss_xent_color, y_predicted, y_color_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,\n",
      "            y_class = args.y_class,\n",
      "            y_color = args.y_color)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    xent_color_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    error_color_meter = utils.RunningAverageMeter(0.97)\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        xent_color_meter.set(checkpt['xent_train_color'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        error_color_meter.set(checkpt['error_train_color'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        \n",
      "        fixed_y_color = torch.from_numpy(np.arange(model.module.y_color)).repeat(model.module.y_color).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot_color = thops.onehot(fixed_y_color, num_classes=model.module.y_color)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            mean_color, logs_color = model.module._prior_color(fixed_y_onehot_color)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_color_sup = modules.GaussianDiag.sample(mean_color, logs_color)\n",
      "            dim_unsup = np.prod(data_shape) - np.prod(fixed_z_sup.shape[1:]) - np.prod(fixed_z_color_sup.shape[1:])\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_color_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    best_error_score_color = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y_all) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            \n",
      "            y = y_all[0]\n",
      "            y_color = y_all[1]\n",
      "            \n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy()) \n",
      "                error_score_color = 1. - np.mean(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                \n",
      "            else:\n",
      "                loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, loss_xent_color, error_score, error_score_color = loss, 0., 0., 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "                xent_color_meter.update(loss_xent_color.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "                xent_color_meter.update(loss_xent_color)\n",
      "            error_meter.update(error_score)\n",
      "            error_color_meter.update(error_score_color)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('xent_color', {'train_iter': xent_color_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('error_color', {'train_iter': error_color_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Xent Color {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) | Error Color {:.4f}({:.4f}) |\"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, xent_color_meter.val, xent_color_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, error_color_meter.val, error_color_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent_color', {'train_epoch': xent_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('error_color', {'train_epoch': error_color_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses_xent_color = []; losses = []\n",
      "                total_correct = 0\n",
      "                total_correct_color = 0\n",
      "                \n",
      "                for (x, y_all) in test_loader:\n",
      "                    y = y_all[0]\n",
      "                    y_color = y_all[1]\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, loss_xent_color, y_predicted, y_color_predicted = compute_bits_per_dim_conditional(x, y, y_color, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * 0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  0.5 * (loss_xent + loss_xent_color)\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                        total_correct_color += np.sum(y_color_predicted.astype(int) == y_color.numpy())\n",
      "                    else:\n",
      "                        loss, atol, rtol, logp_actions, nfe = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent, loss_xent_color = loss, 0., 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                        losses_xent_color.append(loss_xent_color.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                        losses_xent_color.append(loss_xent_color)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss_xent_color = np.mean(losses_xent_color); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                error_score_color =  1. - total_correct_color / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('xent_color', {'validation': loss_xent_color}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                writer.add_scalars('error_color', {'validation': error_score_color}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Xent Color {:.4f}. Loss {:.4f}, Error {:.4f}(best: {:.4f}), Error Color {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss_xent_color, loss, error_score, best_error_score, error_score_color, best_error_score_color)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"error_color\": error_score_color,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"xent_color\": loss_xent_color,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"best_error_score_color\": best_error_score_color,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"error_train_color\": error_color_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"xent_train_color\": xent_color_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "                    if error_score_color < best_error_score_color:\n",
      "                        best_error_score_color = error_score_color\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"error_color\": error_score_color,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"xent_color\": loss_xent_color,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"best_error_score_color\": best_error_score_color,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"error_train_color\": error_color_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"xent_train_color\": xent_color_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_color_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.375, conditional=True, controlled_tol=False, conv=True, data='colormnist', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.5, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume=None, rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_375_drop_0_5_2cond_linear_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5, y_class=10, y_color=10)\n",
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=1764, bias=True)\n",
      "  (project_ycond_color): LinearZeros(in_features=10, out_features=1764, bias=True)\n",
      "  (project_class): LinearZeros(in_features=882, out_features=10, bias=True)\n",
      "  (project_color): LinearZeros(in_features=882, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5)\n",
      "  (dropout_color): Dropout(p=0.5)\n",
      ")\n",
      "Number of trainable parameters: 952502\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 0010 | Time 28.7928(47.2572) | Bit/dim 21.7506(23.4154) | Xent 2.2795(2.3002) | Xent Color 2.2993(2.3021) | Loss 22.8953(24.5659) | Error 0.8767(0.8992) | Error Color 0.9067(0.8886) |Steps 410(410.00) | Grad Norm 129.8668(137.5990) | Total Time 10.00(10.00)\n",
      "Iter 0020 | Time 28.7322(42.5259) | Bit/dim 17.1271(22.3110) | Xent 2.2378(2.2898) | Xent Color 2.2870(2.2992) | Loss 18.2583(23.4582) | Error 0.8733(0.8964) | Error Color 0.8100(0.8841) |Steps 410(410.00) | Grad Norm 106.7126(132.1896) | Total Time 10.00(10.00)\n",
      "Iter 0030 | Time 29.1122(38.9397) | Bit/dim 11.9004(20.1466) | Xent 2.1598(2.2644) | Xent Color 2.2604(2.2919) | Loss 13.0054(21.2857) | Error 0.4678(0.8134) | Error Color 0.7300(0.8555) |Steps 410(410.00) | Grad Norm 75.9952(120.9113) | Total Time 10.00(10.00)\n",
      "Iter 0040 | Time 30.3490(36.4077) | Bit/dim 8.1679(17.3718) | Xent 2.0572(2.2216) | Xent Color 2.2281(2.2788) | Loss 9.2392(18.4969) | Error 0.3189(0.6943) | Error Color 0.7289(0.8238) |Steps 410(410.00) | Grad Norm 42.0217(103.9019) | Total Time 10.00(10.00)\n",
      "Iter 0050 | Time 30.7143(34.6117) | Bit/dim 6.4357(14.6500) | Xent 1.9656(2.1641) | Xent Color 2.2144(2.2615) | Loss 7.4807(15.7564) | Error 0.2822(0.5908) | Error Color 0.7478(0.7887) |Steps 410(410.00) | Grad Norm 13.2959(83.0415) | Total Time 10.00(10.00)\n",
      "Iter 0060 | Time 28.4396(33.1155) | Bit/dim 5.9224(12.4000) | Xent 1.8720(2.0990) | Xent Color 2.1532(2.2390) | Loss 6.9287(13.4845) | Error 0.2944(0.5164) | Error Color 0.7111(0.7766) |Steps 410(410.00) | Grad Norm 13.8476(64.3404) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0001 | Time 78.9779, Epoch Time 2054.3730(2054.3730), Bit/dim 5.4669(best: inf), Xent 1.7960, Xent Color 2.1227. Loss 6.4465, Error 0.2125(best: inf), Error Color 0.5332(best: inf)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0070 | Time 29.4004(32.1477) | Bit/dim 5.2431(10.5886) | Xent 1.7797(2.0271) | Xent Color 2.1187(2.2120) | Loss 6.2177(11.6484) | Error 0.2611(0.4509) | Error Color 0.6189(0.7380) |Steps 410(410.00) | Grad Norm 7.7416(50.3102) | Total Time 10.00(10.00)\n",
      "Iter 0080 | Time 29.8431(31.4464) | Bit/dim 4.8228(9.1265) | Xent 1.7081(1.9486) | Xent Color 2.0569(2.1793) | Loss 5.7641(10.1584) | Error 0.3056(0.4074) | Error Color 0.5311(0.6991) |Steps 410(410.00) | Grad Norm 8.2090(39.0507) | Total Time 10.00(10.00)\n",
      "Iter 0090 | Time 31.0181(30.9034) | Bit/dim 4.3461(7.9317) | Xent 1.6387(1.8730) | Xent Color 2.0212(2.1427) | Loss 5.2611(8.9357) | Error 0.2456(0.3748) | Error Color 0.5678(0.6650) |Steps 410(410.00) | Grad Norm 6.2119(30.7252) | Total Time 10.00(10.00)\n",
      "Iter 0100 | Time 28.3961(30.4455) | Bit/dim 3.9583(6.9284) | Xent 1.6074(1.8060) | Xent Color 1.9593(2.1040) | Loss 4.8499(7.9059) | Error 0.2511(0.3439) | Error Color 0.5178(0.6355) |Steps 410(410.00) | Grad Norm 4.9976(24.1039) | Total Time 10.00(10.00)\n",
      "Iter 0110 | Time 30.6447(30.1043) | Bit/dim 3.5484(6.0794) | Xent 1.6103(1.7527) | Xent Color 1.9175(2.0603) | Loss 4.4303(7.0326) | Error 0.3089(0.3230) | Error Color 0.4611(0.5929) |Steps 416(410.69) | Grad Norm 4.3963(18.9558) | Total Time 10.00(10.00)\n",
      "Iter 0120 | Time 32.4215(30.7492) | Bit/dim 3.1338(5.3486) | Xent 1.5937(1.7143) | Xent Color 1.8797(2.0159) | Loss 4.0022(6.2811) | Error 0.2567(0.3122) | Error Color 0.5111(0.5653) |Steps 422(414.27) | Grad Norm 3.8661(15.0857) | Total Time 10.00(10.00)\n",
      "Iter 0130 | Time 31.0985(31.0216) | Bit/dim 2.8689(4.7242) | Xent 1.6795(1.6960) | Xent Color 1.8077(1.9690) | Loss 3.7407(5.6405) | Error 0.3433(0.3129) | Error Color 0.4211(0.5360) |Steps 422(416.30) | Grad Norm 3.0298(12.0119) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0002 | Time 77.6492, Epoch Time 2099.7802(2055.7353), Bit/dim 2.7905(best: 5.4669), Xent 1.6789, Xent Color 1.7731. Loss 3.6535, Error 0.2893(best: 0.2125), Error Color 0.3213(best: 0.5332)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0140 | Time 32.1802(31.3843) | Bit/dim 2.6305(4.1987) | Xent 1.7458(1.7028) | Xent Color 1.7478(1.9176) | Loss 3.5039(5.1038) | Error 0.3644(0.3223) | Error Color 0.4056(0.4992) |Steps 428(418.95) | Grad Norm 2.5085(9.5493) | Total Time 10.00(10.00)\n",
      "Iter 0150 | Time 35.9662(31.9331) | Bit/dim 2.4962(3.7639) | Xent 1.8547(1.7283) | Xent Color 1.6773(1.8637) | Loss 3.3792(4.6618) | Error 0.4644(0.3423) | Error Color 0.3767(0.4712) |Steps 440(422.70) | Grad Norm 1.9240(7.6052) | Total Time 10.00(10.00)\n",
      "Iter 0160 | Time 37.6471(33.0857) | Bit/dim 2.3755(3.4103) | Xent 1.8672(1.7612) | Xent Color 1.6140(1.8075) | Loss 3.2458(4.3024) | Error 0.4267(0.3655) | Error Color 0.3611(0.4471) |Steps 446(428.09) | Grad Norm 1.9536(6.0751) | Total Time 10.00(10.00)\n",
      "Iter 0170 | Time 37.8904(34.0852) | Bit/dim 2.2903(3.1269) | Xent 1.8779(1.7901) | Xent Color 1.5572(1.7484) | Loss 3.1491(4.0115) | Error 0.4478(0.3869) | Error Color 0.3611(0.4268) |Steps 446(432.79) | Grad Norm 2.1543(4.9390) | Total Time 10.00(10.00)\n",
      "Iter 0180 | Time 37.2308(34.7760) | Bit/dim 2.2590(2.9024) | Xent 1.8346(1.8105) | Xent Color 1.4602(1.6854) | Loss 3.0827(3.7764) | Error 0.4367(0.4042) | Error Color 0.3356(0.4053) |Steps 446(436.26) | Grad Norm 1.5096(4.0907) | Total Time 10.00(10.00)\n",
      "Iter 0190 | Time 35.9626(35.2018) | Bit/dim 2.2368(2.7274) | Xent 1.8414(1.8160) | Xent Color 1.4116(1.6198) | Loss 3.0500(3.5864) | Error 0.4389(0.4092) | Error Color 0.3144(0.3827) |Steps 446(438.82) | Grad Norm 1.3553(3.4315) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0003 | Time 86.3437, Epoch Time 2466.9935(2068.0730), Bit/dim 2.2113(best: 2.7905), Xent 1.7509, Xent Color 1.3000. Loss 2.9740, Error 0.3416(best: 0.2125), Error Color 0.2710(best: 0.3213)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0200 | Time 38.6259(35.8017) | Bit/dim 2.2047(2.5917) | Xent 1.7691(1.8075) | Xent Color 1.3096(1.5502) | Loss 2.9744(3.4311) | Error 0.4067(0.4069) | Error Color 0.2989(0.3648) |Steps 446(440.70) | Grad Norm 2.2541(3.0244) | Total Time 10.00(10.00)\n",
      "Iter 0210 | Time 36.3912(36.0727) | Bit/dim 2.1977(2.4863) | Xent 1.7302(1.7890) | Xent Color 1.2767(1.4821) | Loss 2.9494(3.3041) | Error 0.3756(0.4006) | Error Color 0.3600(0.3528) |Steps 446(442.09) | Grad Norm 19.1386(3.7456) | Total Time 10.00(10.00)\n",
      "Iter 0220 | Time 36.1695(36.0811) | Bit/dim 2.2125(2.4090) | Xent 1.6420(1.7583) | Xent Color 1.2046(1.4144) | Loss 2.9241(3.2022) | Error 0.3800(0.3913) | Error Color 0.3333(0.3454) |Steps 440(442.94) | Grad Norm 20.1617(6.5356) | Total Time 10.00(10.00)\n",
      "Iter 0230 | Time 35.9519(36.2200) | Bit/dim 2.1838(2.3489) | Xent 1.5858(1.7195) | Xent Color 1.1371(1.3432) | Loss 2.8646(3.1146) | Error 0.3444(0.3787) | Error Color 0.3222(0.3306) |Steps 440(442.17) | Grad Norm 19.0907(8.4547) | Total Time 10.00(10.00)\n",
      "Iter 0240 | Time 36.6266(36.1327) | Bit/dim 2.2130(2.3060) | Xent 1.5027(1.6679) | Xent Color 1.0191(1.2734) | Loss 2.8435(3.0414) | Error 0.3200(0.3610) | Error Color 0.2244(0.3149) |Steps 440(441.60) | Grad Norm 4.9984(10.0506) | Total Time 10.00(10.00)\n",
      "Iter 0250 | Time 36.6268(36.1043) | Bit/dim 2.1954(2.2746) | Xent 1.4260(1.6107) | Xent Color 1.1200(1.2168) | Loss 2.8319(2.9815) | Error 0.2944(0.3436) | Error Color 0.4367(0.3140) |Steps 440(441.18) | Grad Norm 35.7349(13.7538) | Total Time 10.00(10.00)\n",
      "Iter 0260 | Time 36.1128(35.8393) | Bit/dim 2.1997(2.2526) | Xent 1.3254(1.5476) | Xent Color 0.9941(1.1540) | Loss 2.7796(2.9280) | Error 0.2811(0.3287) | Error Color 0.2433(0.2905) |Steps 434(439.72) | Grad Norm 23.9286(14.3573) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0004 | Time 81.4739, Epoch Time 2482.8366(2080.5159), Bit/dim 2.1867(best: 2.2113), Xent 1.2435, Xent Color 0.8940. Loss 2.7211, Error 0.2052(best: 0.2125), Error Color 0.1087(best: 0.2710)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0270 | Time 30.8476(35.2489) | Bit/dim 2.1845(2.2356) | Xent 1.2473(1.4793) | Xent Color 0.9951(1.1156) | Loss 2.7451(2.8843) | Error 0.2656(0.3132) | Error Color 0.3222(0.2953) |Steps 428(438.04) | Grad Norm 29.1202(17.8571) | Total Time 10.00(10.00)\n",
      "Iter 0280 | Time 31.7972(34.4490) | Bit/dim 2.1673(2.2200) | Xent 1.1778(1.4072) | Xent Color 0.8320(1.0563) | Loss 2.6698(2.8358) | Error 0.2400(0.2976) | Error Color 0.1044(0.2641) |Steps 428(435.40) | Grad Norm 6.1682(17.1814) | Total Time 10.00(10.00)\n",
      "Iter 0290 | Time 33.9016(33.9718) | Bit/dim 2.1985(2.2076) | Xent 1.0884(1.3328) | Xent Color 0.9946(1.0116) | Loss 2.7193(2.7937) | Error 0.2322(0.2799) | Error Color 0.4244(0.2558) |Steps 428(433.46) | Grad Norm 40.0686(18.1966) | Total Time 10.00(10.00)\n",
      "Iter 0300 | Time 31.4855(33.6083) | Bit/dim 2.1888(2.1991) | Xent 1.0131(1.2626) | Xent Color 0.8292(0.9780) | Loss 2.6494(2.7592) | Error 0.2022(0.2669) | Error Color 0.1633(0.2519) |Steps 428(432.02) | Grad Norm 22.6063(20.2092) | Total Time 10.00(10.00)\n",
      "Iter 0310 | Time 32.4206(33.3475) | Bit/dim 2.1953(2.1901) | Xent 0.9697(1.1952) | Xent Color 0.8513(0.9301) | Loss 2.6506(2.7214) | Error 0.2311(0.2564) | Error Color 0.2478(0.2259) |Steps 428(430.81) | Grad Norm 33.7239(18.9679) | Total Time 10.00(10.00)\n",
      "Iter 0320 | Time 31.5659(32.9744) | Bit/dim 2.1819(2.1934) | Xent 0.9525(1.1311) | Xent Color 0.7923(0.9991) | Loss 2.6181(2.7259) | Error 0.2256(0.2454) | Error Color 0.1633(0.2866) |Steps 422(428.50) | Grad Norm 8.4282(28.4363) | Total Time 10.00(10.00)\n",
      "Iter 0330 | Time 32.2897(32.6216) | Bit/dim 2.1495(2.1858) | Xent 0.9074(1.0743) | Xent Color 0.8639(0.9733) | Loss 2.5924(2.6977) | Error 0.2211(0.2371) | Error Color 0.1900(0.2774) |Steps 422(426.79) | Grad Norm 12.5207(25.9491) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0005 | Time 76.5639, Epoch Time 2235.1120(2085.1538), Bit/dim 2.1575(best: 2.1867), Xent 0.8530, Xent Color 0.8310. Loss 2.5785, Error 0.1650(best: 0.2052), Error Color 0.1344(best: 0.1087)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0340 | Time 31.1886(32.3831) | Bit/dim 2.1133(2.1715) | Xent 0.9030(1.0305) | Xent Color 0.8249(0.9390) | Loss 2.5453(2.6639) | Error 0.2222(0.2311) | Error Color 0.1978(0.2566) |Steps 422(425.53) | Grad Norm 18.2746(23.0592) | Total Time 10.00(10.00)\n",
      "Iter 0350 | Time 33.3582(32.4168) | Bit/dim 2.1220(2.1607) | Xent 0.8091(0.9832) | Xent Color 0.9003(0.9112) | Loss 2.5493(2.6343) | Error 0.1856(0.2242) | Error Color 0.2800(0.2473) |Steps 422(424.61) | Grad Norm 32.4060(22.6859) | Total Time 10.00(10.00)\n",
      "Iter 0360 | Time 31.4076(32.2079) | Bit/dim 2.1431(2.1540) | Xent 0.7909(0.9359) | Xent Color 0.7379(0.8752) | Loss 2.5253(2.6068) | Error 0.2011(0.2176) | Error Color 0.1722(0.2362) |Steps 422(423.92) | Grad Norm 23.2726(21.9900) | Total Time 10.00(10.00)\n",
      "Iter 0370 | Time 31.3988(32.1159) | Bit/dim 2.1133(2.1478) | Xent 0.7496(0.8955) | Xent Color 0.6479(0.8284) | Loss 2.4627(2.5788) | Error 0.1811(0.2140) | Error Color 0.1011(0.2130) |Steps 422(423.42) | Grad Norm 4.5226(19.7601) | Total Time 10.00(10.00)\n",
      "Iter 0380 | Time 30.4978(32.0050) | Bit/dim 2.1273(2.1417) | Xent 0.7246(0.8578) | Xent Color 0.6954(0.7804) | Loss 2.4823(2.5512) | Error 0.1744(0.2088) | Error Color 0.1989(0.1917) |Steps 422(423.05) | Grad Norm 25.6104(18.0415) | Total Time 10.00(10.00)\n",
      "Iter 0390 | Time 33.3654(31.9549) | Bit/dim 2.1284(2.1430) | Xent 0.6940(0.8201) | Xent Color 0.7868(0.8592) | Loss 2.4986(2.5628) | Error 0.1756(0.2037) | Error Color 0.2400(0.2504) |Steps 422(422.77) | Grad Norm 26.6836(26.5828) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0006 | Time 79.5895, Epoch Time 2208.4748(2088.8534), Bit/dim 2.1083(best: 2.1575), Xent 0.6498, Xent Color 0.5941. Loss 2.4193, Error 0.1410(best: 0.1650), Error Color 0.0867(best: 0.1087)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0400 | Time 32.7357(31.9391) | Bit/dim 2.1034(2.1342) | Xent 0.7734(0.8025) | Xent Color 0.6681(0.8238) | Loss 2.4638(2.5408) | Error 0.2022(0.2014) | Error Color 0.1778(0.2400) |Steps 428(423.54) | Grad Norm 11.0801(24.1968) | Total Time 10.00(10.00)\n",
      "Iter 0410 | Time 32.3812(31.8387) | Bit/dim 2.0908(2.1246) | Xent 0.6978(0.7846) | Xent Color 0.6469(0.7711) | Loss 2.4270(2.5136) | Error 0.1922(0.1991) | Error Color 0.1256(0.2078) |Steps 422(423.59) | Grad Norm 9.8435(20.2524) | Total Time 10.00(10.00)\n",
      "Iter 0420 | Time 31.2779(31.8690) | Bit/dim 2.1038(2.1194) | Xent 0.6687(0.7547) | Xent Color 0.5521(0.7175) | Loss 2.4090(2.4874) | Error 0.1756(0.1952) | Error Color 0.1033(0.1764) |Steps 428(424.47) | Grad Norm 7.3939(16.3615) | Total Time 10.00(10.00)\n",
      "Iter 0430 | Time 33.5603(32.0486) | Bit/dim 2.1016(2.1122) | Xent 0.6224(0.7298) | Xent Color 0.5053(0.6661) | Loss 2.3835(2.4612) | Error 0.1644(0.1913) | Error Color 0.0856(0.1518) |Steps 428(425.40) | Grad Norm 8.7023(13.4597) | Total Time 10.00(10.00)\n",
      "Iter 0440 | Time 33.7294(32.1746) | Bit/dim 2.2207(2.1114) | Xent 0.6491(0.7116) | Xent Color 2.7859(0.7108) | Loss 3.0794(2.4670) | Error 0.1833(0.1900) | Error Color 0.8144(0.1746) |Steps 428(426.08) | Grad Norm 140.5500(18.7511) | Total Time 10.00(10.00)\n",
      "Iter 0450 | Time 33.4215(32.1108) | Bit/dim 2.0610(2.1306) | Xent 0.8324(0.6965) | Xent Color 1.0186(0.9938) | Loss 2.5237(2.5532) | Error 0.2522(0.1877) | Error Color 0.4533(0.2749) |Steps 428(426.58) | Grad Norm 22.9112(27.6896) | Total Time 10.00(10.00)\n",
      "Iter 0460 | Time 35.6722(33.0753) | Bit/dim 2.0583(2.1170) | Xent 0.8201(0.7218) | Xent Color 0.7763(0.9618) | Loss 2.4574(2.5379) | Error 0.2133(0.1953) | Error Color 0.1989(0.2828) |Steps 446(431.68) | Grad Norm 5.2155(23.7309) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0007 | Time 85.6544, Epoch Time 2272.0232(2094.3485), Bit/dim 2.0733(best: 2.1083), Xent 0.5921, Xent Color 0.7343. Loss 2.4049, Error 0.1405(best: 0.1410), Error Color 0.2032(best: 0.0867)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0470 | Time 33.2223(33.3176) | Bit/dim 2.0501(2.1047) | Xent 0.6109(0.7122) | Xent Color 0.7104(0.9064) | Loss 2.3805(2.5094) | Error 0.1544(0.1944) | Error Color 0.1844(0.2637) |Steps 434(432.51) | Grad Norm 4.6620(19.1003) | Total Time 10.00(10.00)\n",
      "Iter 0480 | Time 35.0656(33.3687) | Bit/dim 2.0407(2.0920) | Xent 0.6957(0.6899) | Xent Color 0.6376(0.8436) | Loss 2.3741(2.4753) | Error 0.1933(0.1891) | Error Color 0.1333(0.2358) |Steps 434(432.01) | Grad Norm 3.0184(14.8770) | Total Time 10.00(10.00)\n",
      "Iter 0490 | Time 34.9603(33.6974) | Bit/dim 2.0448(2.0811) | Xent 0.6165(0.6749) | Xent Color 0.6176(0.7848) | Loss 2.3534(2.4460) | Error 0.1656(0.1870) | Error Color 0.1300(0.2084) |Steps 434(432.54) | Grad Norm 1.6581(11.6707) | Total Time 10.00(10.00)\n",
      "Iter 0500 | Time 33.8236(33.7672) | Bit/dim 2.0471(2.0705) | Xent 0.5745(0.6513) | Xent Color 0.5363(0.7268) | Loss 2.3248(2.4151) | Error 0.1700(0.1832) | Error Color 0.1133(0.1827) |Steps 434(432.92) | Grad Norm 2.4035(9.4900) | Total Time 10.00(10.00)\n",
      "Iter 0510 | Time 33.5204(33.9256) | Bit/dim 2.0258(2.0601) | Xent 0.5836(0.6371) | Xent Color 0.5083(0.6709) | Loss 2.2988(2.3871) | Error 0.1533(0.1805) | Error Color 0.1089(0.1608) |Steps 434(433.20) | Grad Norm 5.9406(7.8179) | Total Time 10.00(10.00)\n",
      "Iter 0520 | Time 34.0921(34.0133) | Bit/dim 2.0328(2.0520) | Xent 0.5185(0.6216) | Xent Color 0.4467(0.6182) | Loss 2.2741(2.3619) | Error 0.1667(0.1784) | Error Color 0.0722(0.1399) |Steps 434(433.41) | Grad Norm 1.6292(6.5279) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0008 | Time 82.7436, Epoch Time 2351.0660(2102.0501), Bit/dim 2.0294(best: 2.0733), Xent 0.4636, Xent Color 0.3695. Loss 2.2377, Error 0.1213(best: 0.1405), Error Color 0.0287(best: 0.0867)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0530 | Time 35.4189(34.0187) | Bit/dim 2.0269(2.0447) | Xent 0.5339(0.6113) | Xent Color 0.4436(0.5706) | Loss 2.2713(2.3402) | Error 0.1489(0.1764) | Error Color 0.0889(0.1239) |Steps 434(433.57) | Grad Norm 11.7710(6.0963) | Total Time 10.00(10.00)\n",
      "Iter 0540 | Time 35.5000(34.3291) | Bit/dim 2.0131(2.0376) | Xent 0.5983(0.6060) | Xent Color 0.4089(0.5286) | Loss 2.2649(2.3213) | Error 0.1744(0.1767) | Error Color 0.0867(0.1112) |Steps 434(434.03) | Grad Norm 7.8433(6.2507) | Total Time 10.00(10.00)\n",
      "Iter 0550 | Time 34.6294(34.2952) | Bit/dim 2.0027(2.0294) | Xent 0.6076(0.5958) | Xent Color 0.5662(0.4973) | Loss 2.2962(2.3027) | Error 0.1867(0.1750) | Error Color 0.2122(0.1057) |Steps 434(434.37) | Grad Norm 36.2846(7.7888) | Total Time 10.00(10.00)\n",
      "Iter 0560 | Time 34.6146(34.1288) | Bit/dim 2.0649(2.0576) | Xent 0.7954(0.6029) | Xent Color 1.2273(0.9115) | Loss 2.5706(2.4362) | Error 0.2300(0.1793) | Error Color 0.4844(0.2222) |Steps 446(434.86) | Grad Norm 24.9293(22.8138) | Total Time 10.00(10.00)\n",
      "Iter 0570 | Time 35.9388(34.6139) | Bit/dim 2.0546(2.0583) | Xent 0.6763(0.6340) | Xent Color 0.6826(0.8828) | Loss 2.3943(2.4375) | Error 0.2044(0.1893) | Error Color 0.2278(0.2460) |Steps 440(436.99) | Grad Norm 6.4249(19.5775) | Total Time 10.00(10.00)\n",
      "Iter 0580 | Time 33.6867(34.3782) | Bit/dim 2.0167(2.0499) | Xent 0.5877(0.6210) | Xent Color 0.5936(0.8134) | Loss 2.3120(2.4085) | Error 0.1767(0.1855) | Error Color 0.1589(0.2280) |Steps 446(436.93) | Grad Norm 4.1496(15.7082) | Total Time 10.00(10.00)\n",
      "Iter 0590 | Time 35.5611(34.7415) | Bit/dim 2.0177(2.0384) | Xent 0.5612(0.6109) | Xent Color 0.5068(0.7416) | Loss 2.2847(2.3765) | Error 0.1722(0.1835) | Error Color 0.1311(0.2026) |Steps 434(437.30) | Grad Norm 4.1461(12.4928) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0009 | Time 82.3420, Epoch Time 2398.4034(2110.9406), Bit/dim 2.0065(best: 2.0294), Xent 0.4316, Xent Color 0.3956. Loss 2.2133, Error 0.1183(best: 0.1213), Error Color 0.0346(best: 0.0287)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0600 | Time 34.7925(34.7065) | Bit/dim 1.9899(2.0245) | Xent 0.5836(0.5989) | Xent Color 0.4410(0.6714) | Loss 2.2460(2.3421) | Error 0.1756(0.1808) | Error Color 0.0944(0.1773) |Steps 446(436.80) | Grad Norm 1.5530(9.8530) | Total Time 10.00(10.00)\n",
      "Iter 0610 | Time 34.5635(34.6986) | Bit/dim 1.9860(2.0151) | Xent 0.5207(0.5837) | Xent Color 0.3890(0.6013) | Loss 2.2135(2.3114) | Error 0.1656(0.1764) | Error Color 0.0656(0.1523) |Steps 446(438.20) | Grad Norm 1.9854(7.7075) | Total Time 10.00(10.00)\n",
      "Iter 0620 | Time 35.9689(34.9415) | Bit/dim 2.0039(2.0053) | Xent 0.6275(0.5728) | Xent Color 0.3467(0.5383) | Loss 2.2474(2.2831) | Error 0.1756(0.1735) | Error Color 0.0711(0.1307) |Steps 446(440.25) | Grad Norm 3.5315(6.2438) | Total Time 10.00(10.00)\n",
      "Iter 0630 | Time 33.1685(35.0241) | Bit/dim 1.9701(1.9963) | Xent 0.5035(0.5608) | Xent Color 0.3375(0.4847) | Loss 2.1804(2.2577) | Error 0.1600(0.1699) | Error Color 0.0711(0.1147) |Steps 446(441.93) | Grad Norm 2.1975(5.2177) | Total Time 10.00(10.00)\n",
      "Iter 0640 | Time 36.0346(35.1759) | Bit/dim 1.9761(1.9878) | Xent 0.5993(0.5576) | Xent Color 0.2777(0.4342) | Loss 2.1953(2.2357) | Error 0.1733(0.1690) | Error Color 0.0511(0.0986) |Steps 446(443.14) | Grad Norm 3.4809(4.5210) | Total Time 10.00(10.00)\n",
      "Iter 0650 | Time 35.2540(35.3292) | Bit/dim 1.9483(1.9777) | Xent 0.5649(0.5532) | Xent Color 0.2599(0.3924) | Loss 2.1545(2.2141) | Error 0.1878(0.1689) | Error Color 0.0422(0.0861) |Steps 452(444.70) | Grad Norm 1.9740(4.4751) | Total Time 10.00(10.00)\n",
      "Iter 0660 | Time 35.6754(35.4355) | Bit/dim 1.9474(1.9701) | Xent 0.5188(0.5461) | Xent Color 0.2679(0.3547) | Loss 2.1441(2.1953) | Error 0.1533(0.1654) | Error Color 0.0589(0.0760) |Steps 446(445.68) | Grad Norm 14.1534(4.9008) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0010 | Time 92.1713, Epoch Time 2451.7093(2121.1637), Bit/dim 1.9514(best: 2.0065), Xent 0.4041, Xent Color 0.2038. Loss 2.1033, Error 0.1128(best: 0.1183), Error Color 0.0190(best: 0.0287)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0670 | Time 34.3321(35.5347) | Bit/dim 1.9414(1.9620) | Xent 0.5250(0.5358) | Xent Color 0.2735(0.3420) | Loss 2.1410(2.1814) | Error 0.1744(0.1630) | Error Color 0.0700(0.0806) |Steps 446(446.43) | Grad Norm 14.9173(8.4360) | Total Time 10.00(10.00)\n",
      "Iter 0680 | Time 36.4846(35.6400) | Bit/dim 1.9345(1.9546) | Xent 0.5227(0.5295) | Xent Color 0.3356(0.3285) | Loss 2.1491(2.1691) | Error 0.1689(0.1613) | Error Color 0.1211(0.0823) |Steps 452(447.74) | Grad Norm 22.2918(10.2185) | Total Time 10.00(10.00)\n",
      "Iter 0690 | Time 36.9210(35.8353) | Bit/dim 1.9317(1.9460) | Xent 0.4964(0.5256) | Xent Color 0.2402(0.3075) | Loss 2.1158(2.1543) | Error 0.1533(0.1603) | Error Color 0.0600(0.0770) |Steps 452(448.86) | Grad Norm 13.1104(10.3343) | Total Time 10.00(10.00)\n",
      "Iter 0700 | Time 38.7105(35.9146) | Bit/dim 1.9366(1.9391) | Xent 0.5165(0.5231) | Xent Color 0.1949(0.2838) | Loss 2.1144(2.1409) | Error 0.1556(0.1595) | Error Color 0.0422(0.0694) |Steps 452(449.53) | Grad Norm 1.1080(9.4556) | Total Time 10.00(10.00)\n",
      "Iter 0710 | Time 35.8480(35.9203) | Bit/dim 1.9148(1.9326) | Xent 0.4880(0.5221) | Xent Color 0.2131(0.2590) | Loss 2.0901(2.1279) | Error 0.1556(0.1603) | Error Color 0.0633(0.0620) |Steps 452(450.18) | Grad Norm 11.1992(8.3447) | Total Time 10.00(10.00)\n",
      "Iter 0720 | Time 36.0961(36.0797) | Bit/dim 1.9134(1.9263) | Xent 0.5513(0.5193) | Xent Color 0.1715(0.2352) | Loss 2.0941(2.1150) | Error 0.1611(0.1594) | Error Color 0.0356(0.0543) |Steps 452(450.66) | Grad Norm 8.7621(7.3329) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0011 | Time 84.7249, Epoch Time 2492.5486(2132.3053), Bit/dim 2.2056(best: 1.9514), Xent 0.3710, Xent Color 3.2667. Loss 3.1151, Error 0.1103(best: 0.1128), Error Color 0.6196(best: 0.0190)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0730 | Time 37.3792(35.9742) | Bit/dim 2.2880(2.0049) | Xent 0.7255(0.5288) | Xent Color 1.1935(1.1297) | Loss 2.7677(2.4196) | Error 0.2300(0.1623) | Error Color 0.4567(0.1746) |Steps 446(448.86) | Grad Norm 19.2378(23.0419) | Total Time 10.00(10.00)\n",
      "Iter 0740 | Time 35.3632(36.3595) | Bit/dim 2.1130(2.0386) | Xent 0.6236(0.5851) | Xent Color 0.9444(1.1191) | Loss 2.5050(2.4646) | Error 0.1967(0.1809) | Error Color 0.3422(0.2402) |Steps 446(451.02) | Grad Norm 7.5956(20.3501) | Total Time 10.00(10.00)\n",
      "Iter 0750 | Time 37.4285(36.1752) | Bit/dim 2.0227(2.0400) | Xent 0.6296(0.5882) | Xent Color 0.7302(1.0374) | Loss 2.3627(2.4464) | Error 0.2000(0.1827) | Error Color 0.2589(0.2581) |Steps 458(450.84) | Grad Norm 4.9336(16.3161) | Total Time 10.00(10.00)\n",
      "Iter 0760 | Time 34.8508(36.0011) | Bit/dim 1.9498(2.0259) | Xent 0.5342(0.5886) | Xent Color 0.5887(0.9319) | Loss 2.2305(2.4060) | Error 0.1600(0.1827) | Error Color 0.2111(0.2486) |Steps 452(451.84) | Grad Norm 2.7760(13.0753) | Total Time 10.00(10.00)\n",
      "Iter 0770 | Time 35.2713(35.7877) | Bit/dim 1.9880(2.0130) | Xent 0.5905(0.5739) | Xent Color 0.4882(0.8319) | Loss 2.2577(2.3644) | Error 0.1800(0.1793) | Error Color 0.1700(0.2321) |Steps 452(451.09) | Grad Norm 3.6111(10.4774) | Total Time 10.00(10.00)\n",
      "Iter 0780 | Time 35.5562(35.8818) | Bit/dim 1.9643(2.0005) | Xent 0.6015(0.5688) | Xent Color 0.4258(0.7373) | Loss 2.2211(2.3270) | Error 0.1767(0.1780) | Error Color 0.1300(0.2108) |Steps 476(453.36) | Grad Norm 2.9205(9.1765) | Total Time 10.00(10.00)\n",
      "Iter 0790 | Time 38.7424(36.1086) | Bit/dim 1.9570(1.9886) | Xent 0.5565(0.5566) | Xent Color 0.3348(0.6448) | Loss 2.1798(2.2889) | Error 0.1733(0.1737) | Error Color 0.1000(0.1865) |Steps 482(459.26) | Grad Norm 3.9762(7.7194) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0012 | Time 91.8763, Epoch Time 2499.7677(2143.3291), Bit/dim 1.9542(best: 1.9514), Xent 0.3783, Xent Color 0.2590. Loss 2.1136, Error 0.1086(best: 0.1103), Error Color 0.0411(best: 0.0190)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0800 | Time 40.6584(36.7600) | Bit/dim 1.9335(1.9751) | Xent 0.4729(0.5453) | Xent Color 0.3202(0.5617) | Loss 2.1318(2.2519) | Error 0.1489(0.1705) | Error Color 0.1000(0.1632) |Steps 488(466.91) | Grad Norm 1.8570(6.4565) | Total Time 10.00(10.00)\n",
      "Iter 0810 | Time 38.5680(37.2440) | Bit/dim 1.9347(1.9659) | Xent 0.5334(0.5388) | Xent Color 0.2505(0.4878) | Loss 2.1307(2.2226) | Error 0.1689(0.1671) | Error Color 0.0767(0.1424) |Steps 476(472.27) | Grad Norm 3.5695(5.5151) | Total Time 10.00(10.00)\n",
      "Iter 0820 | Time 40.4686(37.7704) | Bit/dim 1.9524(1.9576) | Xent 0.5421(0.5314) | Xent Color 0.2483(0.4239) | Loss 2.1500(2.1964) | Error 0.1689(0.1650) | Error Color 0.0811(0.1233) |Steps 482(477.03) | Grad Norm 8.5000(5.0527) | Total Time 10.00(10.00)\n",
      "Iter 0830 | Time 39.5428(38.2095) | Bit/dim 1.9271(1.9461) | Xent 0.5162(0.5207) | Xent Color 0.1985(0.3715) | Loss 2.1058(2.1692) | Error 0.1600(0.1620) | Error Color 0.0467(0.1079) |Steps 494(481.18) | Grad Norm 4.3275(5.4832) | Total Time 10.00(10.00)\n",
      "Iter 0840 | Time 38.4754(38.3764) | Bit/dim 1.9195(1.9360) | Xent 0.4795(0.5155) | Xent Color 0.2314(0.3291) | Loss 2.0972(2.1472) | Error 0.1600(0.1599) | Error Color 0.0867(0.0963) |Steps 482(482.51) | Grad Norm 13.1171(6.0046) | Total Time 10.00(10.00)\n",
      "Iter 0850 | Time 39.2348(38.3660) | Bit/dim 1.9031(1.9283) | Xent 0.5101(0.5103) | Xent Color 0.2442(0.2963) | Loss 2.0917(2.1300) | Error 0.1500(0.1590) | Error Color 0.0900(0.0884) |Steps 482(483.13) | Grad Norm 14.9321(6.8981) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0013 | Time 93.0736, Epoch Time 2677.2609(2159.3471), Bit/dim 1.9023(best: 1.9514), Xent 0.3634, Xent Color 0.1174. Loss 2.0225, Error 0.1077(best: 0.1086), Error Color 0.0150(best: 0.0190)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0860 | Time 39.2801(38.4203) | Bit/dim 1.8967(1.9214) | Xent 0.5013(0.5066) | Xent Color 0.2465(0.2834) | Loss 2.0837(2.1189) | Error 0.1578(0.1582) | Error Color 0.0889(0.0884) |Steps 482(482.86) | Grad Norm 16.3587(8.7261) | Total Time 10.00(10.00)\n",
      "Iter 0870 | Time 39.3290(38.4666) | Bit/dim 1.8897(1.9141) | Xent 0.4911(0.5006) | Xent Color 0.1811(0.2589) | Loss 2.0578(2.1040) | Error 0.1478(0.1560) | Error Color 0.0500(0.0804) |Steps 470(481.95) | Grad Norm 9.6336(8.7560) | Total Time 10.00(10.00)\n",
      "Iter 0880 | Time 38.3955(38.6235) | Bit/dim 1.8859(1.9061) | Xent 0.5348(0.5071) | Xent Color 0.1447(0.2299) | Loss 2.0558(2.0904) | Error 0.1744(0.1581) | Error Color 0.0456(0.0703) |Steps 488(482.80) | Grad Norm 5.7898(7.8998) | Total Time 10.00(10.00)\n",
      "Iter 0890 | Time 39.8632(38.8068) | Bit/dim 1.8687(1.8961) | Xent 0.4838(0.5019) | Xent Color 0.1236(0.2035) | Loss 2.0206(2.0724) | Error 0.1511(0.1564) | Error Color 0.0300(0.0604) |Steps 488(484.17) | Grad Norm 3.8212(6.9502) | Total Time 10.00(10.00)\n",
      "Iter 0900 | Time 39.9673(38.9493) | Bit/dim 1.8733(1.8897) | Xent 0.4760(0.5005) | Xent Color 0.1081(0.1801) | Loss 2.0193(2.0599) | Error 0.1433(0.1569) | Error Color 0.0244(0.0520) |Steps 494(485.69) | Grad Norm 2.0179(5.9059) | Total Time 10.00(10.00)\n",
      "Iter 0910 | Time 40.0663(38.9624) | Bit/dim 1.8550(1.8826) | Xent 0.5073(0.4951) | Xent Color 0.1028(0.1603) | Loss 2.0075(2.0465) | Error 0.1522(0.1541) | Error Color 0.0278(0.0451) |Steps 494(486.62) | Grad Norm 2.8806(4.8619) | Total Time 10.00(10.00)\n",
      "Iter 0920 | Time 37.7553(38.9558) | Bit/dim 1.8543(1.8760) | Xent 0.4979(0.4936) | Xent Color 0.0998(0.1430) | Loss 2.0037(2.0352) | Error 0.1600(0.1541) | Error Color 0.0211(0.0391) |Steps 482(487.73) | Grad Norm 6.9086(4.4150) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0014 | Time 93.0951, Epoch Time 2688.8549(2175.2323), Bit/dim 1.8511(best: 1.9023), Xent 0.3649, Xent Color 0.0609. Loss 1.9576, Error 0.1047(best: 0.1077), Error Color 0.0127(best: 0.0150)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 0930 | Time 37.5458(38.7217) | Bit/dim 1.8583(1.8684) | Xent 0.5805(0.4968) | Xent Color 0.0874(0.1326) | Loss 2.0253(2.0258) | Error 0.1689(0.1552) | Error Color 0.0211(0.0367) |Steps 488(488.40) | Grad Norm 3.1023(4.9716) | Total Time 10.00(10.00)\n",
      "Iter 0940 | Time 38.9902(38.4490) | Bit/dim 1.8285(1.8604) | Xent 0.5592(0.4978) | Xent Color 0.0844(0.1205) | Loss 1.9894(2.0150) | Error 0.1722(0.1557) | Error Color 0.0244(0.0325) |Steps 488(488.14) | Grad Norm 5.9190(5.1291) | Total Time 10.00(10.00)\n",
      "Iter 0950 | Time 41.4031(38.3832) | Bit/dim 2.1523(1.9140) | Xent 0.9020(0.5153) | Xent Color 1.3958(0.8041) | Loss 2.7267(2.2438) | Error 0.2900(0.1619) | Error Color 0.4456(0.1429) |Steps 488(485.22) | Grad Norm 27.6134(20.2263) | Total Time 10.00(10.00)\n",
      "Iter 0960 | Time 35.3458(38.0102) | Bit/dim 2.0020(1.9478) | Xent 0.5058(0.5382) | Xent Color 0.8056(0.8354) | Loss 2.3299(2.2912) | Error 0.1633(0.1704) | Error Color 0.3133(0.1999) |Steps 458(483.44) | Grad Norm 8.7345(18.4479) | Total Time 10.00(10.00)\n",
      "Iter 0970 | Time 36.3214(37.5222) | Bit/dim 1.9121(1.9431) | Xent 0.5427(0.5454) | Xent Color 0.5124(0.7744) | Loss 2.1759(2.2730) | Error 0.1767(0.1726) | Error Color 0.1944(0.2103) |Steps 464(478.60) | Grad Norm 5.3888(15.4582) | Total Time 10.00(10.00)\n",
      "Iter 0980 | Time 35.7356(37.2140) | Bit/dim 1.8759(1.9300) | Xent 0.4903(0.5383) | Xent Color 0.3949(0.6796) | Loss 2.0972(2.2345) | Error 0.1556(0.1700) | Error Color 0.1433(0.1950) |Steps 458(474.10) | Grad Norm 2.5887(12.2866) | Total Time 10.00(10.00)\n",
      "Iter 0990 | Time 37.3340(36.9197) | Bit/dim 1.8616(1.9156) | Xent 0.5176(0.5338) | Xent Color 0.2842(0.5827) | Loss 2.0620(2.1947) | Error 0.1700(0.1688) | Error Color 0.1022(0.1717) |Steps 464(470.88) | Grad Norm 1.7205(9.6140) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0015 | Time 89.0694, Epoch Time 2549.9686(2186.4744), Bit/dim 1.8727(best: 1.8511), Xent 0.3663, Xent Color 0.1749. Loss 2.0079, Error 0.1058(best: 0.1047), Error Color 0.0314(best: 0.0127)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1000 | Time 35.5802(36.8150) | Bit/dim 1.8318(1.8992) | Xent 0.4958(0.5278) | Xent Color 0.2116(0.4929) | Loss 2.0087(2.1544) | Error 0.1478(0.1663) | Error Color 0.0678(0.1469) |Steps 464(468.77) | Grad Norm 1.4171(7.5143) | Total Time 10.00(10.00)\n",
      "Iter 1010 | Time 37.0630(36.8692) | Bit/dim 1.8628(1.8859) | Xent 0.4613(0.5174) | Xent Color 0.1814(0.4139) | Loss 2.0235(2.1188) | Error 0.1444(0.1623) | Error Color 0.0511(0.1234) |Steps 470(468.94) | Grad Norm 0.8338(5.8342) | Total Time 10.00(10.00)\n",
      "Iter 1020 | Time 35.3366(36.8201) | Bit/dim 1.8144(1.8719) | Xent 0.5628(0.5164) | Xent Color 0.1403(0.3496) | Loss 1.9902(2.0884) | Error 0.1800(0.1631) | Error Color 0.0300(0.1032) |Steps 470(470.18) | Grad Norm 1.7027(4.6404) | Total Time 10.00(10.00)\n",
      "Iter 1030 | Time 41.0326(37.3239) | Bit/dim 1.8002(1.8571) | Xent 0.5152(0.5154) | Xent Color 0.1575(0.2975) | Loss 1.9684(2.0603) | Error 0.1600(0.1627) | Error Color 0.0500(0.0872) |Steps 476(472.23) | Grad Norm 0.9962(3.7712) | Total Time 10.00(10.00)\n",
      "Iter 1040 | Time 40.3399(37.7710) | Bit/dim 1.7912(1.8422) | Xent 0.5238(0.5102) | Xent Color 0.1208(0.2538) | Loss 1.9524(2.0332) | Error 0.1633(0.1612) | Error Color 0.0244(0.0729) |Steps 488(474.20) | Grad Norm 3.1254(3.2925) | Total Time 10.00(10.00)\n",
      "Iter 1050 | Time 38.2249(38.1563) | Bit/dim 1.7833(1.8283) | Xent 0.5126(0.5085) | Xent Color 0.1037(0.2184) | Loss 1.9374(2.0100) | Error 0.1489(0.1602) | Error Color 0.0189(0.0621) |Steps 476(475.55) | Grad Norm 2.1077(3.2432) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0016 | Time 94.5741, Epoch Time 2622.4634(2199.5541), Bit/dim 1.7915(best: 1.8511), Xent 0.3542, Xent Color 0.0668. Loss 1.8968, Error 0.1073(best: 0.1047), Error Color 0.0105(best: 0.0127)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1060 | Time 37.0706(38.3804) | Bit/dim 1.7731(1.8160) | Xent 0.5093(0.5094) | Xent Color 0.1001(0.1890) | Loss 1.9254(1.9906) | Error 0.1667(0.1612) | Error Color 0.0256(0.0530) |Steps 488(476.65) | Grad Norm 2.7136(3.0244) | Total Time 10.00(10.00)\n",
      "Iter 1070 | Time 39.8896(38.5185) | Bit/dim 1.7639(1.8047) | Xent 0.5208(0.5053) | Xent Color 0.0914(0.1654) | Loss 1.9170(1.9724) | Error 0.1689(0.1609) | Error Color 0.0244(0.0460) |Steps 476(477.76) | Grad Norm 1.0157(3.0920) | Total Time 10.00(10.00)\n",
      "Iter 1080 | Time 38.9711(38.7108) | Bit/dim 1.7561(1.7927) | Xent 0.5038(0.5002) | Xent Color 0.0709(0.1443) | Loss 1.8998(1.9538) | Error 0.1633(0.1589) | Error Color 0.0133(0.0392) |Steps 488(479.85) | Grad Norm 1.2100(2.7727) | Total Time 10.00(10.00)\n",
      "Iter 1090 | Time 41.3743(38.8291) | Bit/dim 1.7467(1.7810) | Xent 0.5349(0.4986) | Xent Color 0.0787(0.1269) | Loss 1.9001(1.9374) | Error 0.1544(0.1575) | Error Color 0.0211(0.0338) |Steps 476(480.82) | Grad Norm 1.8663(2.5015) | Total Time 10.00(10.00)\n",
      "Iter 1100 | Time 38.8531(39.0217) | Bit/dim 1.7269(1.7710) | Xent 0.5102(0.4998) | Xent Color 0.0698(0.1131) | Loss 1.8719(1.9242) | Error 0.1500(0.1571) | Error Color 0.0211(0.0298) |Steps 488(481.60) | Grad Norm 2.8515(2.7630) | Total Time 10.00(10.00)\n",
      "Iter 1110 | Time 40.3685(39.3198) | Bit/dim 1.7331(1.7610) | Xent 0.5430(0.4956) | Xent Color 0.0474(0.1026) | Loss 1.8807(1.9106) | Error 0.1589(0.1553) | Error Color 0.0044(0.0265) |Steps 482(481.96) | Grad Norm 3.5993(2.9756) | Total Time 10.00(10.00)\n",
      "Iter 1120 | Time 39.3964(39.3107) | Bit/dim 1.7451(1.7498) | Xent 0.5011(0.4960) | Xent Color 0.0553(0.0918) | Loss 1.8842(1.8968) | Error 0.1622(0.1562) | Error Color 0.0133(0.0232) |Steps 488(482.77) | Grad Norm 2.5879(2.8018) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0017 | Time 94.0676, Epoch Time 2709.1524(2214.8420), Bit/dim 1.7181(best: 1.7915), Xent 0.3433, Xent Color 0.0306. Loss 1.8116, Error 0.0999(best: 0.1047), Error Color 0.0050(best: 0.0105)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1130 | Time 38.5006(39.3807) | Bit/dim 1.7087(1.7397) | Xent 0.4545(0.5004) | Xent Color 0.0516(0.0834) | Loss 1.8352(1.8856) | Error 0.1544(0.1578) | Error Color 0.0122(0.0209) |Steps 482(482.70) | Grad Norm 2.4152(2.7464) | Total Time 10.00(10.00)\n",
      "Iter 1140 | Time 41.5492(39.4374) | Bit/dim 1.6865(1.7294) | Xent 0.4765(0.4924) | Xent Color 0.0783(0.0768) | Loss 1.8252(1.8717) | Error 0.1389(0.1555) | Error Color 0.0200(0.0189) |Steps 476(482.63) | Grad Norm 8.8286(3.1320) | Total Time 10.00(10.00)\n",
      "Iter 1150 | Time 39.4281(39.4621) | Bit/dim 1.6708(1.7181) | Xent 0.4652(0.4907) | Xent Color 0.0584(0.0751) | Loss 1.8017(1.8595) | Error 0.1422(0.1538) | Error Color 0.0189(0.0191) |Steps 470(482.56) | Grad Norm 6.9788(4.2021) | Total Time 10.00(10.00)\n",
      "Iter 1160 | Time 42.0132(39.4841) | Bit/dim 1.6736(1.7084) | Xent 0.5310(0.4916) | Xent Color 0.0522(0.0702) | Loss 1.8194(1.8489) | Error 0.1711(0.1546) | Error Color 0.0100(0.0176) |Steps 488(482.90) | Grad Norm 2.5070(4.2244) | Total Time 10.00(10.00)\n",
      "Iter 1170 | Time 40.6719(39.5161) | Bit/dim 1.6548(1.6982) | Xent 0.4766(0.4894) | Xent Color 0.0630(0.0638) | Loss 1.7896(1.8365) | Error 0.1589(0.1537) | Error Color 0.0200(0.0155) |Steps 488(483.78) | Grad Norm 9.2930(4.2846) | Total Time 10.00(10.00)\n",
      "Iter 1180 | Time 41.4243(39.9260) | Bit/dim 1.6515(1.6881) | Xent 0.4345(0.4885) | Xent Color 0.0554(0.0685) | Loss 1.7739(1.8273) | Error 0.1356(0.1541) | Error Color 0.0122(0.0176) |Steps 488(485.70) | Grad Norm 6.6069(6.2836) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0018 | Time 101.5423, Epoch Time 2754.3580(2231.0275), Bit/dim 1.6510(best: 1.7181), Xent 0.3346, Xent Color 0.0273. Loss 1.7415, Error 0.1007(best: 0.0999), Error Color 0.0048(best: 0.0050)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1190 | Time 40.2206(40.0712) | Bit/dim 1.6518(1.6786) | Xent 0.4359(0.4884) | Xent Color 0.1047(0.0734) | Loss 1.7870(1.8191) | Error 0.1367(0.1538) | Error Color 0.0333(0.0201) |Steps 488(487.18) | Grad Norm 16.9868(7.5570) | Total Time 10.00(10.00)\n",
      "Iter 1200 | Time 39.7086(40.0271) | Bit/dim 1.6249(1.6694) | Xent 0.4982(0.4884) | Xent Color 0.0538(0.0764) | Loss 1.7629(1.8106) | Error 0.1522(0.1530) | Error Color 0.0122(0.0213) |Steps 482(488.92) | Grad Norm 2.8313(7.9712) | Total Time 10.00(10.00)\n",
      "Iter 1210 | Time 42.5885(40.1044) | Bit/dim 1.6393(1.6594) | Xent 0.4266(0.4748) | Xent Color 0.0535(0.0703) | Loss 1.7593(1.7957) | Error 0.1378(0.1495) | Error Color 0.0156(0.0189) |Steps 488(488.23) | Grad Norm 4.9393(7.2893) | Total Time 10.00(10.00)\n",
      "Iter 1220 | Time 38.9313(40.0654) | Bit/dim 1.6328(1.6501) | Xent 0.5162(0.4719) | Xent Color 0.0360(0.0627) | Loss 1.7709(1.7837) | Error 0.1489(0.1484) | Error Color 0.0022(0.0163) |Steps 488(487.52) | Grad Norm 2.5065(6.2803) | Total Time 10.00(10.00)\n",
      "Iter 1230 | Time 41.3101(40.0031) | Bit/dim 1.6146(1.6405) | Xent 0.4756(0.4720) | Xent Color 0.0419(0.0576) | Loss 1.7439(1.7729) | Error 0.1511(0.1496) | Error Color 0.0089(0.0146) |Steps 482(486.54) | Grad Norm 2.1926(5.5502) | Total Time 10.00(10.00)\n",
      "Iter 1240 | Time 40.4932(40.0425) | Bit/dim 1.6174(1.6292) | Xent 0.5169(0.4758) | Xent Color 0.0475(0.0546) | Loss 1.7585(1.7618) | Error 0.1489(0.1496) | Error Color 0.0089(0.0135) |Steps 488(486.30) | Grad Norm 6.0536(5.4178) | Total Time 10.00(10.00)\n",
      "Iter 1250 | Time 41.9701(40.1555) | Bit/dim 1.5776(1.6185) | Xent 0.4237(0.4729) | Xent Color 0.0475(0.0513) | Loss 1.6954(1.7496) | Error 0.1367(0.1482) | Error Color 0.0078(0.0125) |Steps 488(486.58) | Grad Norm 5.3100(5.4104) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0019 | Time 102.6629, Epoch Time 2770.5572(2247.2134), Bit/dim 1.5815(best: 1.6510), Xent 0.3283, Xent Color 0.0148. Loss 1.6673, Error 0.0962(best: 0.0999), Error Color 0.0010(best: 0.0048)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1260 | Time 40.2668(39.9847) | Bit/dim 1.5848(1.6087) | Xent 0.4864(0.4748) | Xent Color 0.0603(0.0486) | Loss 1.7214(1.7396) | Error 0.1456(0.1489) | Error Color 0.0178(0.0116) |Steps 488(486.80) | Grad Norm 8.0874(5.2841) | Total Time 10.00(10.00)\n",
      "Iter 1270 | Time 38.0567(39.9928) | Bit/dim 1.5774(1.5994) | Xent 0.4655(0.4729) | Xent Color 0.0266(0.0459) | Loss 1.7004(1.7291) | Error 0.1500(0.1474) | Error Color 0.0044(0.0109) |Steps 488(486.96) | Grad Norm 3.1791(5.1086) | Total Time 10.00(10.00)\n",
      "Iter 1280 | Time 39.0687(39.9784) | Bit/dim 1.5761(1.5899) | Xent 0.4506(0.4761) | Xent Color 0.0306(0.0424) | Loss 1.6964(1.7196) | Error 0.1322(0.1469) | Error Color 0.0067(0.0098) |Steps 488(487.07) | Grad Norm 7.0956(5.0115) | Total Time 10.00(10.00)\n",
      "Iter 1290 | Time 37.9678(39.8671) | Bit/dim 1.5398(1.5796) | Xent 0.5062(0.4680) | Xent Color 0.0675(0.0438) | Loss 1.6832(1.7075) | Error 0.1656(0.1459) | Error Color 0.0178(0.0102) |Steps 482(486.97) | Grad Norm 11.9786(5.9494) | Total Time 10.00(10.00)\n",
      "Iter 1300 | Time 38.6452(39.9971) | Bit/dim 2.2799(1.6834) | Xent 0.6870(0.4958) | Xent Color 5.7919(0.7762) | Loss 3.8996(2.0014) | Error 0.2278(0.1547) | Error Color 0.8133(0.1200) |Steps 512(488.88) | Grad Norm 137.7459(21.6349) | Total Time 10.00(10.00)\n",
      "Iter 1310 | Time 63.1188(45.2597) | Bit/dim 2.2042(1.8978) | Xent 0.8931(0.6501) | Xent Color 0.8589(0.9747) | Loss 2.6423(2.3040) | Error 0.3000(0.2065) | Error Color 0.2867(0.2069) |Steps 686(531.37) | Grad Norm 6.5882(20.9288) | Total Time 10.00(10.00)\n",
      "Iter 1320 | Time 66.5297(50.3327) | Bit/dim 2.0391(1.9510) | Xent 0.5519(0.6579) | Xent Color 0.5739(0.8805) | Loss 2.3205(2.3356) | Error 0.1867(0.2095) | Error Color 0.2256(0.2159) |Steps 728(580.31) | Grad Norm 10.8575(17.4559) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0020 | Time 128.7216, Epoch Time 3230.5691(2276.7141), Bit/dim 2.0427(best: 1.5815), Xent 0.3954, Xent Color 0.4221. Loss 2.2471, Error 0.1172(best: 0.0962), Error Color 0.1700(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1330 | Time 58.0339(53.1623) | Bit/dim 1.9824(1.9644) | Xent 0.5979(0.6358) | Xent Color 0.4161(0.7640) | Loss 2.2359(2.3143) | Error 0.1900(0.2024) | Error Color 0.1667(0.2029) |Steps 644(606.29) | Grad Norm 9.7478(15.4122) | Total Time 10.00(10.00)\n",
      "Iter 1340 | Time 60.9402(54.6226) | Bit/dim 1.9029(1.9565) | Xent 0.4586(0.6043) | Xent Color 0.2717(0.6417) | Loss 2.0855(2.2680) | Error 0.1533(0.1921) | Error Color 0.0933(0.1771) |Steps 620(612.06) | Grad Norm 4.7627(12.5461) | Total Time 10.00(10.00)\n",
      "Iter 1350 | Time 58.9922(55.6551) | Bit/dim 1.8316(1.9335) | Xent 0.5042(0.5838) | Xent Color 0.2126(0.5337) | Loss 2.0108(2.2129) | Error 0.1556(0.1843) | Error Color 0.0633(0.1500) |Steps 596(611.35) | Grad Norm 2.8125(10.3499) | Total Time 10.00(10.00)\n",
      "Iter 1360 | Time 54.9324(55.8416) | Bit/dim 1.7940(1.9010) | Xent 0.5173(0.5622) | Xent Color 0.1534(0.4392) | Loss 1.9617(2.1513) | Error 0.1456(0.1768) | Error Color 0.0467(0.1247) |Steps 584(604.60) | Grad Norm 5.3856(8.7607) | Total Time 10.00(10.00)\n",
      "Iter 1370 | Time 54.9595(55.6113) | Bit/dim 1.7663(1.8663) | Xent 0.4669(0.5434) | Xent Color 0.1429(0.3631) | Loss 1.9187(2.0929) | Error 0.1589(0.1706) | Error Color 0.0444(0.1037) |Steps 578(598.84) | Grad Norm 5.2128(7.6807) | Total Time 10.00(10.00)\n",
      "Iter 1380 | Time 52.2060(54.8924) | Bit/dim 1.7198(1.8302) | Xent 0.4543(0.5240) | Xent Color 0.1239(0.3003) | Loss 1.8643(2.0362) | Error 0.1278(0.1641) | Error Color 0.0356(0.0850) |Steps 560(591.15) | Grad Norm 5.1725(6.7813) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0021 | Time 110.5049, Epoch Time 3866.4969(2324.4076), Bit/dim 1.7017(best: 1.5815), Xent 0.3327, Xent Color 0.0488. Loss 1.7971, Error 0.1008(best: 0.0962), Error Color 0.0060(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1390 | Time 48.5233(53.7112) | Bit/dim 1.6864(1.7940) | Xent 0.5583(0.5144) | Xent Color 0.0986(0.2497) | Loss 1.8506(1.9850) | Error 0.1711(0.1615) | Error Color 0.0322(0.0707) |Steps 548(581.14) | Grad Norm 1.9706(5.8233) | Total Time 10.00(10.00)\n",
      "Iter 1400 | Time 47.3550(52.6307) | Bit/dim 1.6715(1.7612) | Xent 0.5185(0.5054) | Xent Color 0.0796(0.2095) | Loss 1.8210(1.9399) | Error 0.1567(0.1585) | Error Color 0.0178(0.0590) |Steps 542(571.75) | Grad Norm 1.7621(5.2767) | Total Time 10.00(10.00)\n",
      "Iter 1410 | Time 49.7501(51.5934) | Bit/dim 1.6541(1.7316) | Xent 0.4367(0.4942) | Xent Color 0.1292(0.1819) | Loss 1.7955(1.9006) | Error 0.1389(0.1549) | Error Color 0.0456(0.0516) |Steps 548(564.12) | Grad Norm 13.6634(5.7672) | Total Time 10.00(10.00)\n",
      "Iter 1420 | Time 46.1143(50.2094) | Bit/dim 1.6143(1.7052) | Xent 0.5074(0.4868) | Xent Color 0.0931(0.1745) | Loss 1.7644(1.8705) | Error 0.1467(0.1524) | Error Color 0.0289(0.0521) |Steps 536(556.08) | Grad Norm 6.6949(7.6664) | Total Time 10.00(10.00)\n",
      "Iter 1430 | Time 46.1128(49.0259) | Bit/dim 1.5812(1.6806) | Xent 0.5257(0.4774) | Xent Color 0.0641(0.1487) | Loss 1.7286(1.8371) | Error 0.1478(0.1482) | Error Color 0.0122(0.0436) |Steps 536(549.52) | Grad Norm 3.6362(6.8627) | Total Time 10.00(10.00)\n",
      "Iter 1440 | Time 47.6602(48.2353) | Bit/dim 1.5634(1.6556) | Xent 0.4086(0.4706) | Xent Color 0.0769(0.1271) | Loss 1.6847(1.8050) | Error 0.1311(0.1470) | Error Color 0.0222(0.0361) |Steps 524(543.74) | Grad Norm 4.4642(5.8059) | Total Time 10.00(10.00)\n",
      "Iter 1450 | Time 44.2109(47.5784) | Bit/dim 1.5572(1.6331) | Xent 0.4406(0.4650) | Xent Color 0.0642(0.1093) | Loss 1.6834(1.7767) | Error 0.1411(0.1452) | Error Color 0.0144(0.0300) |Steps 530(540.02) | Grad Norm 3.2211(5.1430) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0022 | Time 104.5319, Epoch Time 3239.8751(2351.8716), Bit/dim 1.5607(best: 1.5815), Xent 0.3187, Xent Color 0.0255. Loss 1.6468, Error 0.0949(best: 0.0962), Error Color 0.0026(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1460 | Time 47.7554(47.3390) | Bit/dim 1.5362(1.6116) | Xent 0.4268(0.4583) | Xent Color 0.0827(0.0981) | Loss 1.6636(1.7506) | Error 0.1400(0.1434) | Error Color 0.0233(0.0268) |Steps 536(537.76) | Grad Norm 8.1416(5.0552) | Total Time 10.00(10.00)\n",
      "Iter 1470 | Time 43.7303(46.6873) | Bit/dim 1.5238(1.5922) | Xent 0.4381(0.4507) | Xent Color 0.0481(0.0871) | Loss 1.6453(1.7267) | Error 0.1378(0.1408) | Error Color 0.0122(0.0237) |Steps 524(535.03) | Grad Norm 6.2829(5.3482) | Total Time 10.00(10.00)\n",
      "Iter 1480 | Time 45.1250(46.1491) | Bit/dim 1.5503(1.5755) | Xent 0.4834(0.4458) | Xent Color 0.3934(0.0985) | Loss 1.7695(1.7116) | Error 0.1522(0.1398) | Error Color 0.1589(0.0298) |Steps 524(533.37) | Grad Norm 45.1481(7.9556) | Total Time 10.00(10.00)\n",
      "Iter 1490 | Time 43.7396(45.8830) | Bit/dim 2.1749(1.7135) | Xent 0.8486(0.5114) | Xent Color 2.0246(1.4354) | Loss 2.8932(2.2002) | Error 0.2656(0.1615) | Error Color 0.6611(0.2125) |Steps 674(552.69) | Grad Norm 13.0218(21.3609) | Total Time 10.00(10.00)\n",
      "Iter 1500 | Time 48.4726(46.5759) | Bit/dim 1.9385(1.8109) | Xent 0.6086(0.5725) | Xent Color 0.9009(1.4305) | Loss 2.3159(2.3116) | Error 0.1956(0.1827) | Error Color 0.3678(0.2861) |Steps 734(594.26) | Grad Norm 5.8604(19.5220) | Total Time 10.00(10.00)\n",
      "Iter 1510 | Time 48.8161(47.0957) | Bit/dim 1.8503(1.8336) | Xent 0.5478(0.5805) | Xent Color 0.5436(1.2468) | Loss 2.1231(2.2904) | Error 0.1678(0.1852) | Error Color 0.2133(0.2892) |Steps 728(630.54) | Grad Norm 2.8702(15.8514) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0023 | Time 110.4967, Epoch Time 3188.4728(2376.9696), Bit/dim 1.8071(best: 1.5607), Xent 0.3778, Xent Color 0.3329. Loss 1.9848, Error 0.1167(best: 0.0949), Error Color 0.0824(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1520 | Time 44.8096(46.7221) | Bit/dim 1.7944(1.8337) | Xent 0.5019(0.5617) | Xent Color 0.4833(1.0529) | Loss 2.0407(2.2374) | Error 0.1633(0.1792) | Error Color 0.1733(0.2643) |Steps 674(650.92) | Grad Norm 1.7357(12.3973) | Total Time 10.00(10.00)\n",
      "Iter 1530 | Time 43.0536(46.4050) | Bit/dim 1.7500(1.8125) | Xent 0.5292(0.5436) | Xent Color 0.4250(0.8917) | Loss 1.9886(2.1713) | Error 0.1556(0.1727) | Error Color 0.1522(0.2381) |Steps 572(646.76) | Grad Norm 1.9099(9.7317) | Total Time 10.00(10.00)\n",
      "Iter 1540 | Time 44.7440(45.9774) | Bit/dim 1.6502(1.7797) | Xent 0.4589(0.5300) | Xent Color 0.3498(0.7537) | Loss 1.8523(2.1007) | Error 0.1489(0.1688) | Error Color 0.1100(0.2082) |Steps 572(626.63) | Grad Norm 1.5630(7.7333) | Total Time 10.00(10.00)\n",
      "Iter 1550 | Time 43.0128(45.4828) | Bit/dim 1.6378(1.7458) | Xent 0.4555(0.5169) | Xent Color 0.2935(0.6345) | Loss 1.8251(2.0336) | Error 0.1422(0.1654) | Error Color 0.1111(0.1794) |Steps 560(609.54) | Grad Norm 1.7355(6.1350) | Total Time 10.00(10.00)\n",
      "Iter 1560 | Time 45.1999(45.3677) | Bit/dim 1.5954(1.7104) | Xent 0.4426(0.5099) | Xent Color 0.2535(0.5328) | Loss 1.7694(1.9711) | Error 0.1411(0.1619) | Error Color 0.0844(0.1530) |Steps 548(596.70) | Grad Norm 2.8277(5.1057) | Total Time 10.00(10.00)\n",
      "Iter 1570 | Time 46.8804(45.4611) | Bit/dim 1.5959(1.6774) | Xent 0.4811(0.4974) | Xent Color 0.1737(0.4499) | Loss 1.7596(1.9142) | Error 0.1589(0.1568) | Error Color 0.0544(0.1305) |Steps 554(585.95) | Grad Norm 3.8698(4.6156) | Total Time 10.00(10.00)\n",
      "Iter 1580 | Time 44.6698(45.5814) | Bit/dim 1.5565(1.6472) | Xent 0.4317(0.4857) | Xent Color 0.1547(0.3783) | Loss 1.7031(1.8633) | Error 0.1344(0.1528) | Error Color 0.0344(0.1089) |Steps 548(578.30) | Grad Norm 0.7753(4.1151) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0024 | Time 102.2580, Epoch Time 3106.4052(2398.8527), Bit/dim 1.5513(best: 1.5607), Xent 0.3298, Xent Color 0.0837. Loss 1.6547, Error 0.0996(best: 0.0949), Error Color 0.0100(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1590 | Time 45.1832(45.5294) | Bit/dim 1.5632(1.6213) | Xent 0.4405(0.4862) | Xent Color 0.1548(0.3199) | Loss 1.7121(1.8228) | Error 0.1333(0.1516) | Error Color 0.0467(0.0923) |Steps 548(570.69) | Grad Norm 3.5304(3.9272) | Total Time 10.00(10.00)\n",
      "Iter 1600 | Time 44.2391(45.6168) | Bit/dim 1.5213(1.5974) | Xent 0.4311(0.4806) | Xent Color 0.1806(0.2764) | Loss 1.6742(1.7866) | Error 0.1400(0.1506) | Error Color 0.0700(0.0805) |Steps 554(565.67) | Grad Norm 10.4804(4.5623) | Total Time 10.00(10.00)\n",
      "Iter 1610 | Time 46.7185(45.5776) | Bit/dim 1.5314(1.5773) | Xent 0.4173(0.4726) | Xent Color 0.1558(0.2444) | Loss 1.6747(1.7566) | Error 0.1300(0.1490) | Error Color 0.0522(0.0718) |Steps 554(562.54) | Grad Norm 9.7204(5.1917) | Total Time 10.00(10.00)\n",
      "Iter 1620 | Time 46.5744(45.6068) | Bit/dim 1.4770(1.5587) | Xent 0.4312(0.4683) | Xent Color 0.1279(0.2109) | Loss 1.6167(1.7285) | Error 0.1411(0.1482) | Error Color 0.0322(0.0615) |Steps 548(560.55) | Grad Norm 6.9961(5.0935) | Total Time 10.00(10.00)\n",
      "Iter 1630 | Time 46.8099(45.8866) | Bit/dim 1.4901(1.5415) | Xent 0.3949(0.4602) | Xent Color 0.1054(0.1826) | Loss 1.6152(1.7022) | Error 0.1389(0.1466) | Error Color 0.0278(0.0528) |Steps 554(557.43) | Grad Norm 5.9506(5.0757) | Total Time 10.00(10.00)\n",
      "Iter 1640 | Time 45.4671(45.7778) | Bit/dim 1.4941(1.5267) | Xent 0.4660(0.4621) | Xent Color 0.0786(0.1583) | Loss 1.6302(1.6818) | Error 0.1367(0.1458) | Error Color 0.0211(0.0451) |Steps 548(556.68) | Grad Norm 1.7389(4.8872) | Total Time 10.00(10.00)\n",
      "Iter 1650 | Time 47.3546(45.8623) | Bit/dim 1.4578(1.5120) | Xent 0.4714(0.4582) | Xent Color 0.0665(0.1370) | Loss 1.5923(1.6608) | Error 0.1600(0.1449) | Error Color 0.0111(0.0381) |Steps 548(555.13) | Grad Norm 2.2867(4.6055) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0025 | Time 98.4574, Epoch Time 3145.5822(2421.2546), Bit/dim 1.4692(best: 1.5513), Xent 0.3116, Xent Color 0.0282. Loss 1.5542, Error 0.0940(best: 0.0949), Error Color 0.0022(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1660 | Time 42.4470(45.7719) | Bit/dim 1.4874(1.4994) | Xent 0.4543(0.4589) | Xent Color 0.1239(0.1200) | Loss 1.6319(1.6442) | Error 0.1411(0.1453) | Error Color 0.0411(0.0330) |Steps 548(554.60) | Grad Norm 15.1020(4.5359) | Total Time 10.00(10.00)\n",
      "Iter 1670 | Time 44.2223(45.6861) | Bit/dim 1.4742(1.4919) | Xent 0.4508(0.4547) | Xent Color 0.1056(0.1409) | Loss 1.6133(1.6409) | Error 0.1489(0.1442) | Error Color 0.0356(0.0429) |Steps 572(559.40) | Grad Norm 5.9490(6.6844) | Total Time 10.00(10.00)\n",
      "Iter 1680 | Time 48.1049(45.8768) | Bit/dim 1.4546(1.4824) | Xent 0.4269(0.4529) | Xent Color 0.1010(0.1348) | Loss 1.5865(1.6293) | Error 0.1378(0.1437) | Error Color 0.0278(0.0402) |Steps 578(564.78) | Grad Norm 6.1563(6.8463) | Total Time 10.00(10.00)\n",
      "Iter 1690 | Time 47.1683(46.1592) | Bit/dim 1.4384(1.4710) | Xent 0.3826(0.4475) | Xent Color 0.0599(0.1184) | Loss 1.5491(1.6124) | Error 0.1256(0.1426) | Error Color 0.0133(0.0344) |Steps 578(567.94) | Grad Norm 2.5281(5.9869) | Total Time 10.00(10.00)\n",
      "Iter 1700 | Time 45.6025(46.3450) | Bit/dim 1.4246(1.4608) | Xent 0.4588(0.4398) | Xent Color 0.0465(0.1008) | Loss 1.5509(1.5960) | Error 0.1533(0.1413) | Error Color 0.0078(0.0282) |Steps 578(569.97) | Grad Norm 2.6905(4.9505) | Total Time 10.00(10.00)\n",
      "Iter 1710 | Time 47.4052(46.1829) | Bit/dim 1.4260(1.4502) | Xent 0.4549(0.4353) | Xent Color 0.0513(0.0875) | Loss 1.5525(1.5809) | Error 0.1411(0.1408) | Error Color 0.0089(0.0235) |Steps 572(568.98) | Grad Norm 4.3433(4.4664) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0026 | Time 115.0240, Epoch Time 3192.7927(2444.4007), Bit/dim 1.4123(best: 1.4692), Xent 0.3042, Xent Color 0.0261. Loss 1.4949, Error 0.0951(best: 0.0940), Error Color 0.0029(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1720 | Time 47.6156(46.5903) | Bit/dim 1.4168(1.4406) | Xent 0.4543(0.4375) | Xent Color 0.0491(0.0782) | Loss 1.5426(1.5695) | Error 0.1467(0.1406) | Error Color 0.0122(0.0203) |Steps 578(571.99) | Grad Norm 2.9677(4.2997) | Total Time 10.00(10.00)\n",
      "Iter 1730 | Time 43.8350(46.6007) | Bit/dim 1.3937(1.4310) | Xent 0.4227(0.4308) | Xent Color 0.0423(0.0707) | Loss 1.5100(1.5564) | Error 0.1367(0.1391) | Error Color 0.0122(0.0180) |Steps 572(571.33) | Grad Norm 0.9363(4.1887) | Total Time 10.00(10.00)\n",
      "Iter 1740 | Time 47.6773(46.7831) | Bit/dim 1.3906(1.4210) | Xent 0.4575(0.4293) | Xent Color 0.0723(0.0663) | Loss 1.5231(1.5449) | Error 0.1378(0.1374) | Error Color 0.0222(0.0167) |Steps 578(571.23) | Grad Norm 11.1429(4.5956) | Total Time 10.00(10.00)\n",
      "Iter 1750 | Time 48.2061(47.1977) | Bit/dim 1.6127(1.4750) | Xent 0.5436(0.4479) | Xent Color 0.5026(0.5653) | Loss 1.8742(1.7283) | Error 0.1889(0.1443) | Error Color 0.1878(0.1260) |Steps 704(600.06) | Grad Norm 5.3385(13.4135) | Total Time 10.00(10.00)\n",
      "Iter 1760 | Time 50.0127(47.6751) | Bit/dim 1.5250(1.5008) | Xent 0.5336(0.4561) | Xent Color 0.3564(0.5415) | Loss 1.7475(1.7502) | Error 0.1722(0.1479) | Error Color 0.1311(0.1423) |Steps 716(627.09) | Grad Norm 3.0308(11.1214) | Total Time 10.00(10.00)\n",
      "Iter 1770 | Time 49.6473(48.2504) | Bit/dim 1.4654(1.4991) | Xent 0.4526(0.4597) | Xent Color 0.2332(0.4704) | Loss 1.6369(1.7316) | Error 0.1367(0.1478) | Error Color 0.0700(0.1266) |Steps 656(640.78) | Grad Norm 3.0861(9.0744) | Total Time 10.00(10.00)\n",
      "Iter 1780 | Time 51.4460(48.9427) | Bit/dim 1.4166(1.4808) | Xent 0.4861(0.4546) | Xent Color 0.1460(0.3958) | Loss 1.5746(1.6934) | Error 0.1678(0.1457) | Error Color 0.0344(0.1065) |Steps 632(640.26) | Grad Norm 1.2174(7.1829) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0027 | Time 119.0849, Epoch Time 3354.0724(2471.6909), Bit/dim 1.4160(best: 1.4123), Xent 0.3054, Xent Color 0.0751. Loss 1.5111, Error 0.0931(best: 0.0940), Error Color 0.0084(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1790 | Time 50.3569(49.5872) | Bit/dim 1.3978(1.4606) | Xent 0.4548(0.4514) | Xent Color 0.1242(0.3247) | Loss 1.5425(1.6547) | Error 0.1311(0.1438) | Error Color 0.0378(0.0868) |Steps 620(635.81) | Grad Norm 1.7513(5.7764) | Total Time 10.00(10.00)\n",
      "Iter 1800 | Time 52.5237(50.0692) | Bit/dim 1.3759(1.4418) | Xent 0.4495(0.4443) | Xent Color 0.0994(0.2680) | Loss 1.5131(1.6199) | Error 0.1489(0.1425) | Error Color 0.0200(0.0708) |Steps 620(631.72) | Grad Norm 1.3509(4.6071) | Total Time 10.00(10.00)\n",
      "Iter 1810 | Time 50.8139(50.3124) | Bit/dim 1.3898(1.4251) | Xent 0.4360(0.4374) | Xent Color 0.0790(0.2200) | Loss 1.5186(1.5895) | Error 0.1556(0.1410) | Error Color 0.0189(0.0573) |Steps 614(626.68) | Grad Norm 1.3213(3.7073) | Total Time 10.00(10.00)\n",
      "Iter 1820 | Time 49.7355(50.6001) | Bit/dim 1.3393(1.4096) | Xent 0.4261(0.4299) | Xent Color 0.0800(0.1830) | Loss 1.4659(1.5628) | Error 0.1400(0.1386) | Error Color 0.0200(0.0467) |Steps 620(622.81) | Grad Norm 1.0031(3.0422) | Total Time 10.00(10.00)\n",
      "Iter 1830 | Time 50.2351(50.7081) | Bit/dim 1.3468(1.3939) | Xent 0.4440(0.4248) | Xent Color 0.0577(0.1527) | Loss 1.4722(1.5383) | Error 0.1544(0.1364) | Error Color 0.0078(0.0376) |Steps 608(619.41) | Grad Norm 0.8289(2.4950) | Total Time 10.00(10.00)\n",
      "Iter 1840 | Time 50.9165(50.7065) | Bit/dim 1.3410(1.3805) | Xent 0.4180(0.4220) | Xent Color 0.0572(0.1278) | Loss 1.4597(1.5180) | Error 0.1356(0.1351) | Error Color 0.0133(0.0305) |Steps 608(616.36) | Grad Norm 1.2328(2.1331) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0028 | Time 118.7658, Epoch Time 3503.8572(2502.6559), Bit/dim 1.3336(best: 1.4123), Xent 0.2698, Xent Color 0.0224. Loss 1.4067, Error 0.0818(best: 0.0931), Error Color 0.0007(best: 0.0010)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1850 | Time 47.6295(50.2789) | Bit/dim 1.3406(1.3691) | Xent 0.4143(0.4128) | Xent Color 0.0469(0.1088) | Loss 1.4559(1.4994) | Error 0.1256(0.1321) | Error Color 0.0056(0.0251) |Steps 596(613.79) | Grad Norm 1.4146(1.9916) | Total Time 10.00(10.00)\n",
      "Iter 1860 | Time 51.3630(50.0957) | Bit/dim 1.3223(1.3580) | Xent 0.3674(0.4039) | Xent Color 0.0360(0.0933) | Loss 1.4232(1.4822) | Error 0.1233(0.1299) | Error Color 0.0011(0.0209) |Steps 590(611.33) | Grad Norm 1.2163(1.8287) | Total Time 10.00(10.00)\n",
      "Iter 1870 | Time 50.7450(50.1167) | Bit/dim 1.3039(1.3479) | Xent 0.3644(0.3962) | Xent Color 0.0546(0.0811) | Loss 1.4086(1.4672) | Error 0.1200(0.1269) | Error Color 0.0100(0.0174) |Steps 608(609.54) | Grad Norm 2.5497(1.8323) | Total Time 10.00(10.00)\n",
      "Iter 1880 | Time 49.6896(49.8755) | Bit/dim 1.3068(1.3380) | Xent 0.3602(0.3888) | Xent Color 0.0464(0.0718) | Loss 1.4084(1.4531) | Error 0.1156(0.1249) | Error Color 0.0067(0.0150) |Steps 602(606.93) | Grad Norm 1.2501(1.8581) | Total Time 10.00(10.00)\n",
      "Iter 1890 | Time 50.2176(49.9007) | Bit/dim 1.3028(1.3284) | Xent 0.3753(0.3879) | Xent Color 0.0414(0.0635) | Loss 1.4070(1.4412) | Error 0.1167(0.1249) | Error Color 0.0044(0.0127) |Steps 614(604.75) | Grad Norm 1.5193(1.8610) | Total Time 10.00(10.00)\n",
      "Iter 1900 | Time 50.6619(49.8796) | Bit/dim 1.2868(1.3177) | Xent 0.3409(0.3796) | Xent Color 0.0379(0.0578) | Loss 1.3815(1.4270) | Error 0.1056(0.1222) | Error Color 0.0067(0.0112) |Steps 590(603.97) | Grad Norm 1.6667(1.8369) | Total Time 10.00(10.00)\n",
      "Iter 1910 | Time 48.9714(49.7166) | Bit/dim 1.2748(1.3079) | Xent 0.3782(0.3761) | Xent Color 0.0397(0.0530) | Loss 1.3792(1.4152) | Error 0.1133(0.1210) | Error Color 0.0089(0.0104) |Steps 590(602.56) | Grad Norm 1.0258(1.7436) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0029 | Time 118.4065, Epoch Time 3409.3064(2529.8554), Bit/dim 1.2731(best: 1.3336), Xent 0.2366, Xent Color 0.0124. Loss 1.3353, Error 0.0711(best: 0.0818), Error Color 0.0006(best: 0.0007)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1920 | Time 48.7879(49.4393) | Bit/dim 1.2513(1.2977) | Xent 0.3645(0.3725) | Xent Color 0.0386(0.0482) | Loss 1.3521(1.4029) | Error 0.1133(0.1196) | Error Color 0.0056(0.0091) |Steps 590(600.42) | Grad Norm 2.3773(1.6962) | Total Time 10.00(10.00)\n",
      "Iter 1930 | Time 46.9328(49.1944) | Bit/dim 1.2590(1.2890) | Xent 0.3513(0.3645) | Xent Color 0.0350(0.0444) | Loss 1.3556(1.3912) | Error 0.1067(0.1175) | Error Color 0.0056(0.0082) |Steps 584(596.92) | Grad Norm 4.0906(1.9943) | Total Time 10.00(10.00)\n",
      "Iter 1940 | Time 47.3723(48.7164) | Bit/dim 1.2534(1.2800) | Xent 0.3460(0.3582) | Xent Color 0.0329(0.0411) | Loss 1.3482(1.3799) | Error 0.1222(0.1158) | Error Color 0.0067(0.0073) |Steps 584(593.67) | Grad Norm 1.9033(2.1015) | Total Time 10.00(10.00)\n",
      "Iter 1950 | Time 47.5422(48.5972) | Bit/dim 1.2309(1.2695) | Xent 0.3293(0.3512) | Xent Color 0.0363(0.0386) | Loss 1.3223(1.3670) | Error 0.1078(0.1138) | Error Color 0.0067(0.0066) |Steps 578(590.74) | Grad Norm 2.4020(2.0520) | Total Time 10.00(10.00)\n",
      "Iter 1960 | Time 45.9731(48.3677) | Bit/dim 1.2424(1.2613) | Xent 0.3216(0.3423) | Xent Color 0.0323(0.0361) | Loss 1.3309(1.3559) | Error 0.1067(0.1106) | Error Color 0.0056(0.0060) |Steps 566(587.63) | Grad Norm 3.0570(2.1993) | Total Time 10.00(10.00)\n",
      "Iter 1970 | Time 46.6276(48.1627) | Bit/dim 1.2342(1.2531) | Xent 0.3894(0.3449) | Xent Color 0.0317(0.0342) | Loss 1.3395(1.3479) | Error 0.1167(0.1109) | Error Color 0.0056(0.0056) |Steps 578(584.17) | Grad Norm 3.0122(2.3184) | Total Time 10.00(10.00)\n",
      "Iter 1980 | Time 47.8180(48.0134) | Bit/dim 1.2241(1.2457) | Xent 0.3464(0.3417) | Xent Color 0.0290(0.0327) | Loss 1.3180(1.3393) | Error 0.1111(0.1097) | Error Color 0.0033(0.0053) |Steps 578(581.13) | Grad Norm 3.0881(2.3926) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0030 | Time 115.9948, Epoch Time 3300.3321(2552.9697), Bit/dim 1.2257(best: 1.2731), Xent 0.2045, Xent Color 0.0096. Loss 1.2792, Error 0.0647(best: 0.0711), Error Color 0.0003(best: 0.0006)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 1990 | Time 49.4812(48.1633) | Bit/dim 1.2241(1.2380) | Xent 0.2634(0.3341) | Xent Color 0.0230(0.0315) | Loss 1.2958(1.3294) | Error 0.0900(0.1085) | Error Color 0.0011(0.0052) |Steps 566(579.31) | Grad Norm 2.8044(2.5769) | Total Time 10.00(10.00)\n",
      "Iter 2000 | Time 45.8669(48.0369) | Bit/dim 1.1831(1.2292) | Xent 0.3041(0.3307) | Xent Color 0.0244(0.0297) | Loss 1.2652(1.3193) | Error 0.1011(0.1075) | Error Color 0.0033(0.0047) |Steps 584(578.84) | Grad Norm 2.1352(2.7870) | Total Time 10.00(10.00)\n",
      "Iter 2010 | Time 46.9897(47.9038) | Bit/dim 1.1946(1.2211) | Xent 0.3406(0.3270) | Xent Color 0.0253(0.0285) | Loss 1.2861(1.3100) | Error 0.1144(0.1056) | Error Color 0.0044(0.0048) |Steps 572(577.47) | Grad Norm 3.5003(2.6867) | Total Time 10.00(10.00)\n",
      "Iter 2020 | Time 47.4614(47.7409) | Bit/dim 1.1840(1.2139) | Xent 0.3145(0.3238) | Xent Color 0.0352(0.0275) | Loss 1.2714(1.3018) | Error 0.1133(0.1050) | Error Color 0.0078(0.0046) |Steps 578(576.47) | Grad Norm 6.1144(2.7528) | Total Time 10.00(10.00)\n",
      "Iter 2030 | Time 45.8047(47.6801) | Bit/dim 1.1903(1.2072) | Xent 0.2641(0.3202) | Xent Color 0.0217(0.0281) | Loss 1.2618(1.2943) | Error 0.0867(0.1049) | Error Color 0.0033(0.0051) |Steps 584(576.54) | Grad Norm 5.4283(3.4076) | Total Time 10.00(10.00)\n",
      "Iter 2040 | Time 49.4062(47.6260) | Bit/dim 1.1619(1.2004) | Xent 0.3425(0.3146) | Xent Color 0.0207(0.0269) | Loss 1.2527(1.2858) | Error 0.1178(0.1032) | Error Color 0.0022(0.0048) |Steps 578(574.59) | Grad Norm 2.8988(3.4961) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0031 | Time 116.7846, Epoch Time 3279.1393(2574.7548), Bit/dim 1.1820(best: 1.2257), Xent 0.1888, Xent Color 0.0106. Loss 1.2319, Error 0.0607(best: 0.0647), Error Color 0.0008(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2050 | Time 47.2443(47.4283) | Bit/dim 1.1897(1.1957) | Xent 0.2834(0.3093) | Xent Color 0.0262(0.0262) | Loss 1.2671(1.2796) | Error 0.0944(0.1012) | Error Color 0.0056(0.0046) |Steps 566(572.68) | Grad Norm 5.0910(3.4590) | Total Time 10.00(10.00)\n",
      "Iter 2060 | Time 46.5938(47.3640) | Bit/dim 1.1605(1.1880) | Xent 0.2761(0.3025) | Xent Color 0.0311(0.0258) | Loss 1.2373(1.2701) | Error 0.0922(0.0987) | Error Color 0.0089(0.0047) |Steps 560(569.35) | Grad Norm 1.9889(3.4805) | Total Time 10.00(10.00)\n",
      "Iter 2070 | Time 46.2178(47.1312) | Bit/dim 1.1511(1.1810) | Xent 0.3526(0.3005) | Xent Color 0.0227(0.0247) | Loss 1.2449(1.2623) | Error 0.1111(0.0981) | Error Color 0.0056(0.0043) |Steps 554(565.81) | Grad Norm 2.8241(3.2507) | Total Time 10.00(10.00)\n",
      "Iter 2080 | Time 52.1275(47.1816) | Bit/dim 1.3765(1.1866) | Xent 0.4170(0.3004) | Xent Color 3.6031(0.1776) | Loss 2.3816(1.3061) | Error 0.1278(0.0978) | Error Color 0.6467(0.0333) |Steps 608(565.03) | Grad Norm 62.8312(7.4648) | Total Time 10.00(10.00)\n",
      "Iter 2090 | Time 50.9291(47.9946) | Bit/dim 1.9651(1.3980) | Xent 0.3650(0.3643) | Xent Color 1.2644(0.8840) | Loss 2.3724(1.7101) | Error 0.1111(0.1143) | Error Color 0.4122(0.1577) |Steps 824(610.84) | Grad Norm 6.5394(14.4587) | Total Time 10.00(10.00)\n",
      "Iter 2100 | Time 54.2939(49.4078) | Bit/dim 1.8096(1.5163) | Xent 0.3169(0.3648) | Xent Color 0.6491(0.8677) | Loss 2.0511(1.8244) | Error 0.1089(0.1158) | Error Color 0.2589(0.2025) |Steps 776(659.99) | Grad Norm 3.7419(11.6862) | Total Time 10.00(10.00)\n",
      "Iter 2110 | Time 50.4331(49.5798) | Bit/dim 1.7062(1.5745) | Xent 0.3379(0.3663) | Xent Color 0.4394(0.7780) | Loss 1.9005(1.8606) | Error 0.1078(0.1160) | Error Color 0.1722(0.2037) |Steps 728(682.58) | Grad Norm 2.3647(9.2519) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0032 | Time 124.6349, Epoch Time 3379.5737(2598.8993), Bit/dim 1.6974(best: 1.1820), Xent 0.2198, Xent Color 0.2456. Loss 1.8138, Error 0.0684(best: 0.0607), Error Color 0.0384(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2120 | Time 53.0414(50.3737) | Bit/dim 1.6456(1.6010) | Xent 0.3217(0.3555) | Xent Color 0.2798(0.6674) | Loss 1.7960(1.8568) | Error 0.1000(0.1124) | Error Color 0.0889(0.1826) |Steps 722(694.53) | Grad Norm 1.1617(7.2514) | Total Time 10.00(10.00)\n",
      "Iter 2130 | Time 51.0452(50.7262) | Bit/dim 1.5872(1.6020) | Xent 0.3794(0.3485) | Xent Color 0.2299(0.5614) | Loss 1.7395(1.8295) | Error 0.1200(0.1106) | Error Color 0.0733(0.1566) |Steps 692(695.66) | Grad Norm 1.4700(5.7249) | Total Time 10.00(10.00)\n",
      "Iter 2140 | Time 48.8487(50.3281) | Bit/dim 1.5240(1.5865) | Xent 0.2935(0.3358) | Xent Color 0.1870(0.4681) | Loss 1.6441(1.7875) | Error 0.0922(0.1073) | Error Color 0.0489(0.1310) |Steps 668(688.30) | Grad Norm 0.8651(4.5102) | Total Time 10.00(10.00)\n",
      "Iter 2150 | Time 49.8637(49.9470) | Bit/dim 1.4801(1.5647) | Xent 0.3282(0.3275) | Xent Color 0.1591(0.3916) | Loss 1.6019(1.7444) | Error 0.1033(0.1051) | Error Color 0.0500(0.1100) |Steps 662(680.39) | Grad Norm 1.6279(3.6198) | Total Time 10.00(10.00)\n",
      "Iter 2160 | Time 48.7444(49.9175) | Bit/dim 1.4423(1.5352) | Xent 0.3201(0.3216) | Xent Color 0.1373(0.3271) | Loss 1.5566(1.6974) | Error 0.1067(0.1043) | Error Color 0.0322(0.0909) |Steps 644(672.94) | Grad Norm 1.2350(2.9777) | Total Time 10.00(10.00)\n",
      "Iter 2170 | Time 50.3612(49.7361) | Bit/dim 1.4052(1.5035) | Xent 0.2912(0.3154) | Xent Color 0.1192(0.2758) | Loss 1.5078(1.6513) | Error 0.0900(0.1021) | Error Color 0.0289(0.0766) |Steps 644(665.91) | Grad Norm 1.4065(2.5055) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0033 | Time 123.2334, Epoch Time 3457.4301(2624.6553), Bit/dim 1.3870(best: 1.1820), Xent 0.1858, Xent Color 0.0511. Loss 1.4463, Error 0.0603(best: 0.0607), Error Color 0.0036(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2180 | Time 48.0889(49.7022) | Bit/dim 1.3850(1.4729) | Xent 0.2833(0.3081) | Xent Color 0.1109(0.2340) | Loss 1.4836(1.6084) | Error 0.0978(0.1008) | Error Color 0.0278(0.0643) |Steps 644(660.81) | Grad Norm 0.9114(2.3182) | Total Time 10.00(10.00)\n",
      "Iter 2190 | Time 49.3709(49.4483) | Bit/dim 1.3571(1.4450) | Xent 0.2714(0.2984) | Xent Color 0.0936(0.1985) | Loss 1.4484(1.5692) | Error 0.0856(0.0976) | Error Color 0.0144(0.0532) |Steps 638(656.02) | Grad Norm 2.9388(2.1645) | Total Time 10.00(10.00)\n",
      "Iter 2200 | Time 48.5190(49.6236) | Bit/dim 1.3346(1.4190) | Xent 0.2985(0.2989) | Xent Color 0.0909(0.1694) | Loss 1.4319(1.5361) | Error 0.0911(0.0983) | Error Color 0.0189(0.0444) |Steps 638(651.29) | Grad Norm 2.4191(2.1846) | Total Time 10.00(10.00)\n",
      "Iter 2210 | Time 48.1638(49.5487) | Bit/dim 1.2515(1.3859) | Xent 0.3134(0.2979) | Xent Color 0.0919(0.1477) | Loss 1.3529(1.4973) | Error 0.1078(0.0976) | Error Color 0.0233(0.0384) |Steps 608(644.32) | Grad Norm 2.8150(2.1751) | Total Time 10.00(10.00)\n",
      "Iter 2220 | Time 48.5927(49.3789) | Bit/dim 1.2525(1.3523) | Xent 0.2557(0.2972) | Xent Color 0.1094(0.1307) | Loss 1.3438(1.4592) | Error 0.0789(0.0969) | Error Color 0.0267(0.0337) |Steps 620(636.79) | Grad Norm 5.4040(2.5225) | Total Time 10.00(10.00)\n",
      "Iter 2230 | Time 47.8586(49.2970) | Bit/dim 1.2235(1.3212) | Xent 0.2729(0.2926) | Xent Color 0.0638(0.1156) | Loss 1.3077(1.4233) | Error 0.0900(0.0959) | Error Color 0.0156(0.0290) |Steps 614(631.65) | Grad Norm 1.6247(2.5591) | Total Time 10.00(10.00)\n",
      "Iter 2240 | Time 51.4686(50.1510) | Bit/dim 1.2037(1.2934) | Xent 0.2784(0.2905) | Xent Color 0.0568(0.1019) | Loss 1.2875(1.3915) | Error 0.1000(0.0951) | Error Color 0.0089(0.0252) |Steps 638(631.27) | Grad Norm 2.3515(2.3292) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0034 | Time 127.5367, Epoch Time 3440.0449(2649.1169), Bit/dim 1.2089(best: 1.1820), Xent 0.1646, Xent Color 0.0208. Loss 1.2552, Error 0.0512(best: 0.0603), Error Color 0.0011(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2250 | Time 53.6230(50.4786) | Bit/dim 1.2109(1.2698) | Xent 0.3041(0.2863) | Xent Color 0.0531(0.0907) | Loss 1.3002(1.3641) | Error 0.1011(0.0937) | Error Color 0.0089(0.0219) |Steps 620(629.02) | Grad Norm 2.3450(2.2834) | Total Time 10.00(10.00)\n",
      "Iter 2260 | Time 50.3515(50.6881) | Bit/dim 1.1990(1.2502) | Xent 0.2707(0.2852) | Xent Color 0.0572(0.0810) | Loss 1.2810(1.3417) | Error 0.0878(0.0927) | Error Color 0.0067(0.0191) |Steps 626(627.60) | Grad Norm 1.1940(2.0876) | Total Time 10.00(10.00)\n",
      "Iter 2270 | Time 49.7609(50.4656) | Bit/dim 1.1703(1.2322) | Xent 0.2983(0.2807) | Xent Color 0.0390(0.0724) | Loss 1.2547(1.3205) | Error 0.0956(0.0907) | Error Color 0.0056(0.0169) |Steps 632(625.30) | Grad Norm 1.7091(2.2082) | Total Time 10.00(10.00)\n",
      "Iter 2280 | Time 49.8790(50.3707) | Bit/dim 1.1613(1.2172) | Xent 0.3109(0.2794) | Xent Color 0.0465(0.0654) | Loss 1.2506(1.3034) | Error 0.0967(0.0903) | Error Color 0.0133(0.0150) |Steps 620(622.63) | Grad Norm 2.6866(2.3017) | Total Time 10.00(10.00)\n",
      "Iter 2290 | Time 49.6249(50.1564) | Bit/dim 1.1534(1.2032) | Xent 0.2902(0.2774) | Xent Color 0.0353(0.0598) | Loss 1.2348(1.2875) | Error 0.0911(0.0898) | Error Color 0.0078(0.0137) |Steps 620(620.39) | Grad Norm 2.8049(2.4152) | Total Time 10.00(10.00)\n",
      "Iter 2300 | Time 50.3283(50.0627) | Bit/dim 1.1537(1.1915) | Xent 0.2549(0.2714) | Xent Color 0.0446(0.0550) | Loss 1.2286(1.2731) | Error 0.0878(0.0880) | Error Color 0.0122(0.0123) |Steps 614(618.48) | Grad Norm 3.7153(2.6277) | Total Time 10.00(10.00)\n",
      "Iter 2310 | Time 51.1509(50.0966) | Bit/dim 1.1509(1.1822) | Xent 0.2745(0.2679) | Xent Color 0.0418(0.0519) | Loss 1.2300(1.2622) | Error 0.0744(0.0872) | Error Color 0.0078(0.0115) |Steps 608(616.00) | Grad Norm 3.1205(2.7363) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0035 | Time 127.6695, Epoch Time 3464.0492(2673.5649), Bit/dim 1.1468(best: 1.1820), Xent 0.1647, Xent Color 0.0122. Loss 1.1910, Error 0.0527(best: 0.0512), Error Color 0.0008(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2320 | Time 50.9335(50.2074) | Bit/dim 1.1477(1.1731) | Xent 0.2529(0.2660) | Xent Color 0.0353(0.0473) | Loss 1.2197(1.2514) | Error 0.0789(0.0865) | Error Color 0.0078(0.0102) |Steps 608(612.78) | Grad Norm 0.9008(2.6089) | Total Time 10.00(10.00)\n",
      "Iter 2330 | Time 52.6694(50.1630) | Bit/dim 1.1403(1.1635) | Xent 0.2248(0.2601) | Xent Color 0.0300(0.0440) | Loss 1.2040(1.2395) | Error 0.0689(0.0853) | Error Color 0.0067(0.0093) |Steps 608(609.44) | Grad Norm 1.7515(2.6307) | Total Time 10.00(10.00)\n",
      "Iter 2340 | Time 50.3384(50.1583) | Bit/dim 1.1378(1.1560) | Xent 0.2370(0.2564) | Xent Color 0.0354(0.0420) | Loss 1.2059(1.2306) | Error 0.0711(0.0835) | Error Color 0.0033(0.0087) |Steps 596(605.71) | Grad Norm 5.7893(3.2290) | Total Time 10.00(10.00)\n",
      "Iter 2350 | Time 49.4431(50.1238) | Bit/dim 1.1273(1.1487) | Xent 0.2460(0.2540) | Xent Color 0.0310(0.0401) | Loss 1.1966(1.2222) | Error 0.0767(0.0820) | Error Color 0.0100(0.0086) |Steps 590(602.12) | Grad Norm 3.8370(3.3489) | Total Time 10.00(10.00)\n",
      "Iter 2360 | Time 48.3013(50.0804) | Bit/dim 1.1134(1.1414) | Xent 0.1982(0.2519) | Xent Color 0.0252(0.0380) | Loss 1.1693(1.2139) | Error 0.0600(0.0806) | Error Color 0.0033(0.0083) |Steps 596(600.64) | Grad Norm 3.8823(3.5234) | Total Time 10.00(10.00)\n",
      "Iter 2370 | Time 47.8802(50.2022) | Bit/dim 1.1227(1.1342) | Xent 0.2304(0.2530) | Xent Color 0.0426(0.0390) | Loss 1.1910(1.2072) | Error 0.0778(0.0816) | Error Color 0.0122(0.0091) |Steps 590(598.94) | Grad Norm 7.4854(4.1556) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0036 | Time 98.7058, Epoch Time 3437.9564(2696.4967), Bit/dim 2.5746(best: 1.1468), Xent 0.5756, Xent Color 5.0140. Loss 3.9720, Error 0.1612(best: 0.0512), Error Color 0.6396(best: 0.0003)\n",
      "===> Using batch size 900. Total 66 iterations/epoch.\n",
      "Iter 2380 | Time 49.4915(49.9138) | Bit/dim 2.2800(1.3140) | Xent 0.6401(0.3177) | Xent Color 1.7864(1.1523) | Loss 2.8866(1.6815) | Error 0.1922(0.1003) | Error Color 0.4833(0.1257) |Steps 710(605.77) | Grad Norm 16.7935(17.6337) | Total Time 10.00(10.00)\n",
      "Iter 2390 | Time 56.0534(52.4788) | Bit/dim 1.8250(1.4807) | Xent 0.4151(0.4503) | Xent Color 0.8181(1.0809) | Loss 2.1333(1.8635) | Error 0.1300(0.1415) | Error Color 0.2956(0.1795) |Steps 926(676.49) | Grad Norm 8.8225(15.8934) | Total Time 10.00(10.00)\n",
      "Iter 2400 | Time 55.4789(53.7870) | Bit/dim 1.6337(1.5383) | Xent 0.3912(0.4370) | Xent Color 0.5483(0.9608) | Loss 1.8686(1.8878) | Error 0.1322(0.1377) | Error Color 0.2322(0.1951) |Steps 818(731.34) | Grad Norm 7.8225(13.3727) | Total Time 10.00(10.00)\n",
      "Iter 2410 | Time 55.6682(53.8709) | Bit/dim 1.5094(1.5419) | Xent 0.3626(0.4219) | Xent Color 0.3115(0.8037) | Loss 1.6779(1.8483) | Error 0.1267(0.1355) | Error Color 0.1156(0.1788) |Steps 764(744.51) | Grad Norm 3.3965(10.8502) | Total Time 10.00(10.00)\n",
      "Iter 2420 | Time 57.0740(54.4965) | Bit/dim 1.4391(1.5216) | Xent 0.3483(0.3973) | Xent Color 0.2350(0.6610) | Loss 1.5849(1.7862) | Error 0.1011(0.1275) | Error Color 0.0789(0.1556) |Steps 776(750.18) | Grad Norm 2.5511(8.6130) | Total Time 10.00(10.00)\n",
      "Iter 2430 | Time 55.1046(55.1220) | Bit/dim 1.3664(1.4929) | Xent 0.3052(0.3768) | Xent Color 0.2053(0.5376) | Loss 1.4940(1.7215) | Error 0.1067(0.1215) | Error Color 0.0767(0.1314) |Steps 752(756.04) | Grad Norm 2.3350(6.9877) | Total Time 10.00(10.00)\n",
      "Iter 2440 | Time 54.2639(55.4686) | Bit/dim 1.3346(1.4588) | Xent 0.3039(0.3613) | Xent Color 0.1867(0.4404) | Loss 1.4573(1.6592) | Error 0.0944(0.1166) | Error Color 0.0689(0.1112) |Steps 746(755.63) | Grad Norm 5.5312(5.9436) | Total Time 10.00(10.00)\n",
      "validating...\n",
      "Epoch 0037 | Time 126.6223, Epoch Time 3856.8691(2731.3078), Bit/dim 1.3362(best: 1.1468), Xent 0.1918, Xent Color 0.0635. Loss 1.4001, Error 0.0605(best: 0.0512), Error Color 0.0086(best: 0.0003)\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_2cond.py --data colormnist --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_colormnist_bs900_sratio_0_375_drop_0_5_2cond_linear_run1 --seed 1 --lr 0.001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.375 --dropout_rate 0.5 --y_color 10 --y_class 10\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
