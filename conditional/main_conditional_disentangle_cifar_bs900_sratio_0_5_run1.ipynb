{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tancode/repos/tan-ffjord/train_cnf_disentangle_cifar.py\n",
      "import argparse\n",
      "import os\n",
      "import time\n",
      "import numpy as np\n",
      "\n",
      "import torch\n",
      "import torch.optim as optim\n",
      "import torchvision.datasets as dset\n",
      "import torchvision.transforms as tforms\n",
      "from torchvision.utils import save_image\n",
      "\n",
      "import lib.layers as layers\n",
      "import lib.utils as utils\n",
      "import lib.multiscale_parallel as multiscale_parallel\n",
      "import lib.modules as modules\n",
      "import lib.thops as thops\n",
      "\n",
      "from train_misc import standard_normal_logprob\n",
      "from train_misc import set_cnf_options, count_nfe, count_parameters, count_total_time\n",
      "from train_misc import add_spectral_norm, spectral_norm_power_iteration\n",
      "from train_misc import create_regularization_fns, get_regularization, append_regularization_to_log\n",
      "\n",
      "from tensorboardX import SummaryWriter\n",
      "\n",
      "# go fast boi!!\n",
      "torch.backends.cudnn.benchmark = True\n",
      "SOLVERS = [\"dopri5\", \"bdf\", \"rk4\", \"midpoint\", 'adams', 'explicit_adams']\n",
      "parser = argparse.ArgumentParser(\"Continuous Normalizing Flow\")\n",
      "parser.add_argument(\"--data\", choices=[\"mnist\", \"svhn\", \"cifar10\", 'lsun_church'], type=str, default=\"mnist\")\n",
      "parser.add_argument(\"--dims\", type=str, default=\"8,32,32,8\")\n",
      "parser.add_argument(\"--strides\", type=str, default=\"2,2,1,-2,-2\")\n",
      "parser.add_argument(\"--num_blocks\", type=int, default=1, help='Number of stacked CNFs.')\n",
      "\n",
      "parser.add_argument(\"--conv\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\n",
      "    \"--layer_type\", type=str, default=\"ignore\",\n",
      "    choices=[\"ignore\", \"concat\", \"concat_v2\", \"squash\", \"concatsquash\", \"concatcoord\", \"hyper\", \"blend\"]\n",
      ")\n",
      "parser.add_argument(\"--divergence_fn\", type=str, default=\"approximate\", choices=[\"brute_force\", \"approximate\"])\n",
      "parser.add_argument(\n",
      "    \"--nonlinearity\", type=str, default=\"softplus\", choices=[\"tanh\", \"relu\", \"softplus\", \"elu\", \"swish\"]\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--seed\", type=int, default=0)\n",
      "\n",
      "parser.add_argument('--solver', type=str, default='dopri5', choices=SOLVERS)\n",
      "parser.add_argument('--atol', type=float, default=1e-5)\n",
      "parser.add_argument('--rtol', type=float, default=1e-5)\n",
      "parser.add_argument(\"--step_size\", type=float, default=None, help=\"Optional fixed step size.\")\n",
      "\n",
      "parser.add_argument('--test_solver', type=str, default=None, choices=SOLVERS + [None])\n",
      "parser.add_argument('--test_atol', type=float, default=None)\n",
      "parser.add_argument('--test_rtol', type=float, default=None)\n",
      "\n",
      "parser.add_argument(\"--imagesize\", type=int, default=None)\n",
      "parser.add_argument(\"--alpha\", type=float, default=1e-6)\n",
      "parser.add_argument('--time_length', type=float, default=1.0)\n",
      "parser.add_argument('--train_T', type=eval, default=True)\n",
      "\n",
      "parser.add_argument(\"--num_epochs\", type=int, default=1000)\n",
      "parser.add_argument(\"--batch_size\", type=int, default=200)\n",
      "parser.add_argument(\n",
      "    \"--batch_size_schedule\", type=str, default=\"\", help=\"Increases the batchsize at every given epoch, dash separated.\"\n",
      ")\n",
      "parser.add_argument(\"--test_batch_size\", type=int, default=200)\n",
      "parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
      "parser.add_argument(\"--warmup_iters\", type=float, default=1000)\n",
      "parser.add_argument(\"--weight_decay\", type=float, default=0.0)\n",
      "parser.add_argument(\"--spectral_norm_niter\", type=int, default=10)\n",
      "parser.add_argument(\"--weight_y\", type=float, default=0.5)\n",
      "\n",
      "parser.add_argument(\"--add_noise\", type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument(\"--batch_norm\", type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--residual', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--autoencode', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--rademacher', type=eval, default=True, choices=[True, False])\n",
      "parser.add_argument('--spectral_norm', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--multiscale', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--parallel', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--conditional', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument('--controlled_tol', type=eval, default=False, choices=[True, False])\n",
      "parser.add_argument(\"--train_mode\", choices=[\"semisup\", \"sup\", \"unsup\"], type=str, default=\"semisup\")\n",
      "parser.add_argument(\"--condition_ratio\", type=float, default=0.5)\n",
      "parser.add_argument(\"--dropout_rate\", type=float, default=0.0)\n",
      "\n",
      "\n",
      "# Regularizations\n",
      "parser.add_argument('--l1int', type=float, default=None, help=\"int_t ||f||_1\")\n",
      "parser.add_argument('--l2int', type=float, default=None, help=\"int_t ||f||_2\")\n",
      "parser.add_argument('--dl2int', type=float, default=None, help=\"int_t ||f^T df/dt||_2\")\n",
      "parser.add_argument('--JFrobint', type=float, default=None, help=\"int_t ||df/dx||_F\")\n",
      "parser.add_argument('--JdiagFrobint', type=float, default=None, help=\"int_t ||df_i/dx_i||_F\")\n",
      "parser.add_argument('--JoffdiagFrobint', type=float, default=None, help=\"int_t ||df/dx - df_i/dx_i||_F\")\n",
      "\n",
      "parser.add_argument(\"--time_penalty\", type=float, default=0, help=\"Regularization on the end_time.\")\n",
      "parser.add_argument(\n",
      "    \"--max_grad_norm\", type=float, default=1e10,\n",
      "    help=\"Max norm of graidents (default is just stupidly high to avoid any clipping)\"\n",
      ")\n",
      "\n",
      "parser.add_argument(\"--begin_epoch\", type=int, default=1)\n",
      "parser.add_argument(\"--resume\", type=str, default=None)\n",
      "parser.add_argument(\"--save\", type=str, default=\"experiments/cnf\")\n",
      "parser.add_argument(\"--val_freq\", type=int, default=1)\n",
      "parser.add_argument(\"--log_freq\", type=int, default=1)\n",
      "\n",
      "args = parser.parse_args()\n",
      "\n",
      "if args.controlled_tol:\n",
      "    import lib.odenvp_conditional_tol as odenvp\n",
      "else:\n",
      "    import lib.odenvp_conditional as odenvp\n",
      "    \n",
      "# set seed\n",
      "torch.manual_seed(args.seed)\n",
      "np.random.seed(args.seed)\n",
      "\n",
      "# logger\n",
      "utils.makedirs(args.save)\n",
      "logger = utils.get_logger(logpath=os.path.join(args.save, 'logs'), filepath=os.path.abspath(__file__)) # write to log file\n",
      "writer = SummaryWriter(os.path.join(args.save, 'tensorboard')) # write to tensorboard\n",
      "\n",
      "if args.layer_type == \"blend\":\n",
      "    logger.info(\"!! Setting time_length from None to 1.0 due to use of Blend layers.\")\n",
      "    args.time_length = 1.0\n",
      "\n",
      "logger.info(args)\n",
      "\n",
      "\n",
      "def add_noise(x):\n",
      "    \"\"\"\n",
      "    [0, 1] -> [0, 255] -> add noise -> [0, 1]\n",
      "    \"\"\"\n",
      "    if args.add_noise:\n",
      "        noise = x.new().resize_as_(x).uniform_()\n",
      "        x = x * 255 + noise\n",
      "        x = x / 256\n",
      "    return x\n",
      "\n",
      "\n",
      "def update_lr(optimizer, itr):\n",
      "    iter_frac = min(float(itr + 1) / max(args.warmup_iters, 1), 1.0)\n",
      "    lr = args.lr * iter_frac\n",
      "    for param_group in optimizer.param_groups:\n",
      "        param_group[\"lr\"] = lr\n",
      "\n",
      "\n",
      "def get_train_loader(train_set, epoch):\n",
      "    if args.batch_size_schedule != \"\":\n",
      "        epochs = [0] + list(map(int, args.batch_size_schedule.split(\"-\")))\n",
      "        n_passed = sum(np.array(epochs) <= epoch)\n",
      "        current_batch_size = int(args.batch_size * n_passed)\n",
      "    else:\n",
      "        current_batch_size = args.batch_size\n",
      "    train_loader = torch.utils.data.DataLoader(\n",
      "        dataset=train_set, batch_size=current_batch_size, shuffle=True, drop_last=True, pin_memory=True\n",
      "    )\n",
      "    logger.info(\"===> Using batch size {}. Total {} iterations/epoch.\".format(current_batch_size, len(train_loader)))\n",
      "    return train_loader\n",
      "\n",
      "\n",
      "def get_dataset(args):\n",
      "    trans = lambda im_size: tforms.Compose([tforms.Resize(im_size), tforms.ToTensor(), add_noise])\n",
      "\n",
      "    if args.data == \"mnist\":\n",
      "        im_dim = 1\n",
      "        im_size = 28 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.MNIST(root=\"../data\", train=True, transform=trans(im_size), download=True)\n",
      "        test_set = dset.MNIST(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == \"svhn\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.SVHN(root=\"../data\", split=\"train\", transform=trans(im_size), download=True)\n",
      "        test_set = dset.SVHN(root=\"../data\", split=\"test\", transform=trans(im_size), download=True)\n",
      "    elif args.data == \"cifar10\":\n",
      "        im_dim = 3\n",
      "        im_size = 32 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CIFAR10(\n",
      "            root=\"../data\", train=True, transform=tforms.Compose([\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ]), download=True\n",
      "        )\n",
      "        test_set = dset.CIFAR10(root=\"../data\", train=False, transform=trans(im_size), download=True)\n",
      "    elif args.data == 'celeba':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.CelebA(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.CelebA(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    elif args.data == 'lsun_church':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_train'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.LSUN(\n",
      "            '../data', ['church_outdoor_val'], transform=tforms.Compose([\n",
      "                tforms.Resize(96),\n",
      "                tforms.RandomCrop(64),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )    \n",
      "    elif args.data == 'imagenet_64':\n",
      "        im_dim = 3\n",
      "        im_size = 64 if args.imagesize is None else args.imagesize\n",
      "        train_set = dset.ImageFolder(\n",
      "            train=True, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.RandomHorizontalFlip(),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "        test_set = dset.ImageFolder(\n",
      "            train=False, transform=tforms.Compose([\n",
      "                tforms.ToPILImage(),\n",
      "                tforms.Resize(im_size),\n",
      "                tforms.ToTensor(),\n",
      "                add_noise,\n",
      "            ])\n",
      "        )\n",
      "    \n",
      "    data_shape = (im_dim, im_size, im_size)\n",
      "    if not args.conv:\n",
      "        data_shape = (im_dim * im_size * im_size,)\n",
      "\n",
      "    test_loader = torch.utils.data.DataLoader(\n",
      "        dataset=test_set, batch_size=args.test_batch_size, shuffle=False, drop_last=True\n",
      "    )\n",
      "    return train_set, test_loader, data_shape\n",
      "\n",
      "\n",
      "def compute_bits_per_dim(x, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "\n",
      "    logpz = standard_normal_logprob(z).view(z.shape[0], -1).sum(1, keepdim=True)  # logp(z)\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "\n",
      "    return bits_per_dim\n",
      "\n",
      "def compute_bits_per_dim_conditional(x, y, model):\n",
      "    zero = torch.zeros(x.shape[0], 1).to(x)\n",
      "    y_onehot = thops.onehot(y, num_classes=model.module.y_class).to(x)\n",
      "\n",
      "    # Don't use data parallelize if batch size is small.\n",
      "    # if x.shape[0] < 200:\n",
      "    #     model = model.module\n",
      "    \n",
      "    z, delta_logp = model(x, zero)  # run model forward\n",
      "    \n",
      "    dim_sup = int(args.condition_ratio * np.prod(z.size()[1:]))\n",
      "    \n",
      "    # prior\n",
      "    mean, logs = model.module._prior(y_onehot)\n",
      "\n",
      "    logpz_sup = modules.GaussianDiag.logp(mean, logs, z[:, 0:dim_sup]).view(-1,1)  # logp(z)_sup\n",
      "    logpz_unsup = standard_normal_logprob(z[:, dim_sup:]).view(z.shape[0], -1).sum(1, keepdim=True)\n",
      "    logpz = logpz_sup + logpz_unsup\n",
      "    logpx = logpz - delta_logp\n",
      "\n",
      "    logpx_per_dim = torch.sum(logpx) / x.nelement()  # averaged over batches\n",
      "    bits_per_dim = -(logpx_per_dim - np.log(256)) / np.log(2)\n",
      "    \n",
      "    # dropout\n",
      "    if args.dropout_rate > 0:\n",
      "        zsup = model.module.dropout(z[:, 0:dim_sup])\n",
      "    else:\n",
      "        zsup = z[:, 0:dim_sup]\n",
      "    \n",
      "    # compute xentropy loss\n",
      "    y_logits = model.module.project_class(zsup)\n",
      "    loss_xent = model.module.loss_class(y_logits, y.to(x.get_device()))\n",
      "    y_predicted = np.argmax(y_logits.cpu().detach().numpy(), axis=1)\n",
      "\n",
      "    return bits_per_dim, loss_xent, y_predicted\n",
      "\n",
      "def create_model(args, data_shape, regularization_fns):\n",
      "    hidden_dims = tuple(map(int, args.dims.split(\",\")))\n",
      "    strides = tuple(map(int, args.strides.split(\",\")))\n",
      "\n",
      "    if args.multiscale:\n",
      "        model = odenvp.ODENVP(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            nonlinearity=args.nonlinearity,\n",
      "            alpha=args.alpha,\n",
      "            cnf_kwargs={\"T\": args.time_length, \"train_T\": args.train_T, \"regularization_fns\": regularization_fns, \"solver\": args.solver, \"atol\": args.atol, \"rtol\": args.rtol},\n",
      "            condition_ratio=args.condition_ratio,\n",
      "            dropout_rate=args.dropout_rate,)\n",
      "    elif args.parallel:\n",
      "        model = multiscale_parallel.MultiscaleParallelCNF(\n",
      "            (args.batch_size, *data_shape),\n",
      "            n_blocks=args.num_blocks,\n",
      "            intermediate_dims=hidden_dims,\n",
      "            alpha=args.alpha,\n",
      "            time_length=args.time_length,\n",
      "        )\n",
      "    else:\n",
      "        if args.autoencode:\n",
      "\n",
      "            def build_cnf():\n",
      "                autoencoder_diffeq = layers.AutoencoderDiffEqNet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.AutoencoderODEfunc(\n",
      "                    autoencoder_diffeq=autoencoder_diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "        else:\n",
      "\n",
      "            def build_cnf():\n",
      "                diffeq = layers.ODEnet(\n",
      "                    hidden_dims=hidden_dims,\n",
      "                    input_shape=data_shape,\n",
      "                    strides=strides,\n",
      "                    conv=args.conv,\n",
      "                    layer_type=args.layer_type,\n",
      "                    nonlinearity=args.nonlinearity,\n",
      "                )\n",
      "                odefunc = layers.ODEfunc(\n",
      "                    diffeq=diffeq,\n",
      "                    divergence_fn=args.divergence_fn,\n",
      "                    residual=args.residual,\n",
      "                    rademacher=args.rademacher,\n",
      "                )\n",
      "                cnf = layers.CNF(\n",
      "                    odefunc=odefunc,\n",
      "                    T=args.time_length,\n",
      "                    train_T=args.train_T,\n",
      "                    regularization_fns=regularization_fns,\n",
      "                    solver=args.solver,\n",
      "                )\n",
      "                return cnf\n",
      "\n",
      "        chain = [layers.LogitTransform(alpha=args.alpha)] if args.alpha > 0 else [layers.ZeroMeanTransform()]\n",
      "        chain = chain + [build_cnf() for _ in range(args.num_blocks)]\n",
      "        if args.batch_norm:\n",
      "            chain.append(layers.MovingBatchNorm2d(data_shape[0]))\n",
      "        model = layers.SequentialFlow(chain)\n",
      "    return model\n",
      "\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "\n",
      "    # get deivce\n",
      "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
      "    cvt = lambda x: x.type(torch.float32).to(device, non_blocking=True)\n",
      "\n",
      "    # load dataset\n",
      "    train_set, test_loader, data_shape = get_dataset(args)\n",
      "\n",
      "    # build model\n",
      "    regularization_fns, regularization_coeffs = create_regularization_fns(args)\n",
      "    model = create_model(args, data_shape, regularization_fns)\n",
      "\n",
      "    if args.spectral_norm: add_spectral_norm(model, logger)\n",
      "    set_cnf_options(args, model)\n",
      "\n",
      "    logger.info(model)\n",
      "    logger.info(\"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "    \n",
      "    writer.add_text('info', \"Number of trainable parameters: {}\".format(count_parameters(model)))\n",
      "\n",
      "    # optimizer\n",
      "    optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
      "    \n",
      "    # set initial iter\n",
      "    itr = 1\n",
      "    \n",
      "    # set the meters\n",
      "    time_epoch_meter = utils.RunningAverageMeter(0.97)\n",
      "    time_meter = utils.RunningAverageMeter(0.97)\n",
      "    loss_meter = utils.RunningAverageMeter(0.97) # track total loss\n",
      "    nll_meter = utils.RunningAverageMeter(0.97) # track negative log-likelihood\n",
      "    xent_meter = utils.RunningAverageMeter(0.97) # track xentropy score\n",
      "    error_meter = utils.RunningAverageMeter(0.97) # track error score\n",
      "    steps_meter = utils.RunningAverageMeter(0.97)\n",
      "    grad_meter = utils.RunningAverageMeter(0.97)\n",
      "    tt_meter = utils.RunningAverageMeter(0.97)\n",
      "\n",
      "    # restore parameters\n",
      "    if args.resume is not None:\n",
      "        checkpt = torch.load(args.resume, map_location=lambda storage, loc: storage)\n",
      "        model.load_state_dict(checkpt[\"state_dict\"])\n",
      "        if \"optim_state_dict\" in checkpt.keys():\n",
      "            optimizer.load_state_dict(checkpt[\"optim_state_dict\"])\n",
      "            # Manually move optimizer state to device.\n",
      "            for state in optimizer.state.values():\n",
      "                for k, v in state.items():\n",
      "                    if torch.is_tensor(v):\n",
      "                        state[k] = cvt(v)\n",
      "        args.begin_epoch = checkpt['epoch'] + 1\n",
      "        itr = checkpt['iter'] + 1\n",
      "        time_epoch_meter.set(checkpt['epoch_time_avg'])\n",
      "        time_meter.set(checkpt['time_train'])\n",
      "        loss_meter.set(checkpt['loss_train'])\n",
      "        nll_meter.set(checkpt['bits_per_dim_train'])\n",
      "        xent_meter.set(checkpt['xent_train'])\n",
      "        error_meter.set(checkpt['error_train'])\n",
      "        steps_meter.set(checkpt['nfe_train'])\n",
      "        grad_meter.set(checkpt['grad_train'])\n",
      "        tt_meter.set(checkpt['total_time_train'])\n",
      "\n",
      "    if torch.cuda.is_available():\n",
      "        model = torch.nn.DataParallel(model).cuda()\n",
      "\n",
      "    # For visualization.\n",
      "    if args.conditional:\n",
      "        dim_unsup = int((1.0 - args.condition_ratio) * np.prod(data_shape))\n",
      "        fixed_y = torch.from_numpy(np.arange(model.module.y_class)).repeat(model.module.y_class).type(torch.long).to(device, non_blocking=True)\n",
      "        fixed_y_onehot = thops.onehot(fixed_y, num_classes=model.module.y_class)\n",
      "        with torch.no_grad():\n",
      "            mean, logs = model.module._prior(fixed_y_onehot)\n",
      "            fixed_z_sup = modules.GaussianDiag.sample(mean, logs)\n",
      "            fixed_z_unsup = cvt(torch.randn(model.module.y_class**2, dim_unsup))\n",
      "            fixed_z = torch.cat((fixed_z_sup, fixed_z_unsup),1)\n",
      "    else:\n",
      "        fixed_z = cvt(torch.randn(100, *data_shape))\n",
      "    \n",
      "\n",
      "    if args.spectral_norm and not args.resume: spectral_norm_power_iteration(model, 500)\n",
      "\n",
      "    best_loss_nll = float(\"inf\")\n",
      "    best_error_score = float(\"inf\")\n",
      "    \n",
      "    for epoch in range(args.begin_epoch, args.num_epochs + 1):\n",
      "        start_epoch = time.time()\n",
      "        model.train()\n",
      "        train_loader = get_train_loader(train_set, epoch)\n",
      "        for _, (x, y) in enumerate(train_loader):\n",
      "            start = time.time()\n",
      "            update_lr(optimizer, itr)\n",
      "            optimizer.zero_grad()\n",
      "\n",
      "            if not args.conv:\n",
      "                x = x.view(x.shape[0], -1)\n",
      "\n",
      "            # cast data and move to device\n",
      "            x = cvt(x)\n",
      "            \n",
      "            # compute loss\n",
      "            if args.conditional:\n",
      "                loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                if args.train_mode == \"semisup\":\n",
      "                    loss =  loss_nll + args.weight_y * loss_xent\n",
      "                elif args.train_mode == \"sup\":\n",
      "                    loss =  loss_xent\n",
      "                elif args.train_mode == \"unsup\":\n",
      "                    loss =  loss_nll\n",
      "                else:\n",
      "                    raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                error_score = 1. - np.mean(y_predicted.astype(int) == y.numpy())   \n",
      "                \n",
      "            else:\n",
      "                loss = compute_bits_per_dim(x, model)\n",
      "                loss_nll, loss_xent, error_score = loss, 0., 0.\n",
      "            \n",
      "            if regularization_coeffs:\n",
      "                reg_states = get_regularization(model, regularization_coeffs)\n",
      "                reg_loss = sum(\n",
      "                    reg_state * coeff for reg_state, coeff in zip(reg_states, regularization_coeffs) if coeff != 0\n",
      "                )\n",
      "                loss = loss + reg_loss\n",
      "            total_time = count_total_time(model)\n",
      "            loss = loss + total_time * args.time_penalty\n",
      "\n",
      "            loss.backward()\n",
      "            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
      "\n",
      "            optimizer.step()\n",
      "\n",
      "            if args.spectral_norm: spectral_norm_power_iteration(model, args.spectral_norm_niter)\n",
      "            \n",
      "            time_meter.update(time.time() - start)\n",
      "            loss_meter.update(loss.item())\n",
      "            nll_meter.update(loss_nll.item())\n",
      "            if args.conditional:\n",
      "                xent_meter.update(loss_xent.item())\n",
      "            else:\n",
      "                xent_meter.update(loss_xent)\n",
      "            error_meter.update(error_score)\n",
      "            steps_meter.update(count_nfe(model))\n",
      "            grad_meter.update(grad_norm)\n",
      "            tt_meter.update(total_time)\n",
      "            \n",
      "            # write to tensorboard\n",
      "            writer.add_scalars('time', {'train_iter': time_meter.val}, itr)\n",
      "            writer.add_scalars('loss', {'train_iter': loss_meter.val}, itr)\n",
      "            writer.add_scalars('bits_per_dim', {'train_iter': nll_meter.val}, itr)\n",
      "            writer.add_scalars('xent', {'train_iter': xent_meter.val}, itr)\n",
      "            writer.add_scalars('error', {'train_iter': error_meter.val}, itr)\n",
      "            writer.add_scalars('nfe', {'train_iter': steps_meter.val}, itr)\n",
      "            writer.add_scalars('grad', {'train_iter': grad_meter.val}, itr)\n",
      "            writer.add_scalars('total_time', {'train_iter': tt_meter.val}, itr)\n",
      "\n",
      "            if itr % args.log_freq == 0:\n",
      "                log_message = (\n",
      "                    \"Iter {:04d} | Time {:.4f}({:.4f}) | Bit/dim {:.4f}({:.4f}) | Xent {:.4f}({:.4f}) | Loss {:.4f}({:.4f}) | Error {:.4f}({:.4f}) \"\n",
      "                    \"Steps {:.0f}({:.2f}) | Grad Norm {:.4f}({:.4f}) | Total Time {:.2f}({:.2f})\".format(\n",
      "                        itr, time_meter.val, time_meter.avg, nll_meter.val, nll_meter.avg, xent_meter.val, xent_meter.avg, loss_meter.val, loss_meter.avg, error_meter.val, error_meter.avg, steps_meter.val, steps_meter.avg, grad_meter.val, grad_meter.avg, tt_meter.val, tt_meter.avg\n",
      "                    )\n",
      "                )\n",
      "                if regularization_coeffs:\n",
      "                    log_message = append_regularization_to_log(log_message, regularization_fns, reg_states)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, itr)\n",
      "\n",
      "            itr += 1\n",
      "        \n",
      "        # compute test loss\n",
      "        model.eval()\n",
      "        if epoch % args.val_freq == 0:\n",
      "            with torch.no_grad():\n",
      "                # write to tensorboard\n",
      "                writer.add_scalars('time', {'train_epoch': time_meter.avg}, epoch)\n",
      "                writer.add_scalars('loss', {'train_epoch': loss_meter.avg}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'train_epoch': nll_meter.avg}, epoch)\n",
      "                writer.add_scalars('xent', {'train_epoch': xent_meter.avg}, epoch)\n",
      "                writer.add_scalars('error', {'train_epoch': error_meter.avg}, epoch)\n",
      "                writer.add_scalars('nfe', {'train_epoch': steps_meter.avg}, epoch)\n",
      "                writer.add_scalars('grad', {'train_epoch': grad_meter.avg}, epoch)\n",
      "                writer.add_scalars('total_time', {'train_epoch': tt_meter.avg}, epoch)\n",
      "                \n",
      "                start = time.time()\n",
      "                logger.info(\"validating...\")\n",
      "                writer.add_text('info', \"validating...\", epoch)\n",
      "                losses_nll = []; losses_xent = []; losses = []\n",
      "                total_correct = 0\n",
      "                \n",
      "                for (x, y) in test_loader:\n",
      "                    if not args.conv:\n",
      "                        x = x.view(x.shape[0], -1)\n",
      "                    x = cvt(x)\n",
      "                    if args.conditional:\n",
      "                        loss_nll, loss_xent, y_predicted = compute_bits_per_dim_conditional(x, y, model)\n",
      "                        if args.train_mode == \"semisup\":\n",
      "                            loss =  loss_nll + args.weight_y * loss_xent\n",
      "                        elif args.train_mode == \"sup\":\n",
      "                            loss =  loss_xent\n",
      "                        elif args.train_mode == \"unsup\":\n",
      "                            loss =  loss_nll\n",
      "                        else:\n",
      "                            raise ValueError('Choose supported train_mode: semisup, sup, unsup')\n",
      "                        total_correct += np.sum(y_predicted.astype(int) == y.numpy())\n",
      "                    else:\n",
      "                        loss = compute_bits_per_dim(x, model)\n",
      "                        loss_nll, loss_xent = loss, 0.\n",
      "                    losses_nll.append(loss_nll.cpu().numpy()); losses.append(loss.cpu().numpy())\n",
      "                    if args.conditional: \n",
      "                        losses_xent.append(loss_xent.cpu().numpy())\n",
      "                    else:\n",
      "                        losses_xent.append(loss_xent)\n",
      "                \n",
      "                loss_nll = np.mean(losses_nll); loss_xent = np.mean(losses_xent); loss = np.mean(losses)\n",
      "                error_score =  1. - total_correct / len(test_loader.dataset)\n",
      "                time_epoch_meter.update(time.time() - start_epoch)\n",
      "                \n",
      "                # write to tensorboard\n",
      "                test_time_spent = time.time() - start\n",
      "                writer.add_scalars('time', {'validation': test_time_spent}, epoch)\n",
      "                writer.add_scalars('epoch_time', {'validation': time_epoch_meter.val}, epoch)\n",
      "                writer.add_scalars('bits_per_dim', {'validation': loss_nll}, epoch)\n",
      "                writer.add_scalars('xent', {'validation': loss_xent}, epoch)\n",
      "                writer.add_scalars('loss', {'validation': loss}, epoch)\n",
      "                writer.add_scalars('error', {'validation': error_score}, epoch)\n",
      "                \n",
      "                log_message = \"Epoch {:04d} | Time {:.4f}, Epoch Time {:.4f}({:.4f}), Bit/dim {:.4f}(best: {:.4f}), Xent {:.4f}, Loss {:.4f}, Error {:.4f}(best: {:.4f})\".format(epoch, time.time() - start, time_epoch_meter.val, time_epoch_meter.avg, loss_nll, best_loss_nll, loss_xent, loss, error_score, best_error_score)\n",
      "                logger.info(log_message)\n",
      "                writer.add_text('info', log_message, epoch)\n",
      "                \n",
      "                for name, param in model.named_parameters():\n",
      "                    writer.add_histogram(name, param.clone().cpu().data.numpy(), epoch)\n",
      "                    \n",
      "                \n",
      "                utils.makedirs(args.save)\n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"epoch_%i_checkpt.pth\"%epoch))\n",
      "                \n",
      "                torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"current_checkpt.pth\"))\n",
      "                \n",
      "                if loss_nll < best_loss_nll:\n",
      "                    best_loss_nll = loss_nll\n",
      "                    utils.makedirs(args.save)\n",
      "                    torch.save({\n",
      "                        \"args\": args,\n",
      "                        \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                        \"optim_state_dict\": optimizer.state_dict(),\n",
      "                        \"epoch\": epoch,\n",
      "                        \"iter\": itr-1,\n",
      "                        \"error\": error_score,\n",
      "                        \"loss\": loss,\n",
      "                        \"xent\": loss_xent,\n",
      "                        \"bits_per_dim\": loss_nll,\n",
      "                        \"best_bits_per_dim\": best_loss_nll,\n",
      "                        \"best_error_score\": best_error_score,\n",
      "                        \"epoch_time\": time_epoch_meter.val,\n",
      "                        \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                        \"time\": test_time_spent,\n",
      "                        \"error_train\": error_meter.avg,\n",
      "                        \"loss_train\": loss_meter.avg,\n",
      "                        \"xent_train\": xent_meter.avg,\n",
      "                        \"bits_per_dim_train\": nll_meter.avg,\n",
      "                        \"total_time_train\": tt_meter.avg,\n",
      "                        \"time_train\": time_meter.avg,\n",
      "                        \"nfe_train\": steps_meter.avg,\n",
      "                        \"grad_train\": grad_meter.avg,\n",
      "                    }, os.path.join(args.save, \"best_nll_checkpt.pth\"))\n",
      "                    \n",
      "                if args.conditional:\n",
      "                    if error_score < best_error_score:\n",
      "                        best_error_score = error_score\n",
      "                        utils.makedirs(args.save)\n",
      "                        torch.save({\n",
      "                            \"args\": args,\n",
      "                            \"state_dict\": model.module.state_dict() if torch.cuda.is_available() else model.state_dict(),\n",
      "                            \"optim_state_dict\": optimizer.state_dict(),\n",
      "                            \"epoch\": epoch,\n",
      "                            \"iter\": itr-1,\n",
      "                            \"error\": error_score,\n",
      "                            \"loss\": loss,\n",
      "                            \"xent\": loss_xent,\n",
      "                            \"bits_per_dim\": loss_nll,\n",
      "                            \"best_bits_per_dim\": best_loss_nll,\n",
      "                            \"best_error_score\": best_error_score,\n",
      "                            \"epoch_time\": time_epoch_meter.val,\n",
      "                            \"epoch_time_avg\": time_epoch_meter.avg,\n",
      "                            \"time\": test_time_spent,\n",
      "                            \"error_train\": error_meter.avg,\n",
      "                            \"loss_train\": loss_meter.avg,\n",
      "                            \"xent_train\": xent_meter.avg,\n",
      "                            \"bits_per_dim_train\": nll_meter.avg,\n",
      "                            \"total_time_train\": tt_meter.avg,\n",
      "                            \"time_train\": time_meter.avg,\n",
      "                            \"nfe_train\": steps_meter.avg,\n",
      "                            \"grad_train\": grad_meter.avg,\n",
      "                        }, os.path.join(args.save, \"best_error_checkpt.pth\"))\n",
      "                        \n",
      "\n",
      "        # visualize samples and density\n",
      "        with torch.no_grad():\n",
      "            fig_filename = os.path.join(args.save, \"figs\", \"{:04d}.jpg\".format(epoch))\n",
      "            utils.makedirs(os.path.dirname(fig_filename))\n",
      "            generated_samples = model(fixed_z, reverse=True).view(-1, *data_shape)\n",
      "            save_image(generated_samples, fig_filename, nrow=10)\n",
      "            if args.data == \"mnist\":\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,3,1,1), epoch)\n",
      "            else:\n",
      "                writer.add_images('generated_images', generated_samples.repeat(1,1,1,1), epoch)\n",
      "\n",
      "Namespace(JFrobint=None, JdiagFrobint=None, JoffdiagFrobint=None, add_noise=True, alpha=1e-06, atol=1e-05, autoencode=False, batch_norm=False, batch_size=900, batch_size_schedule='', begin_epoch=1, condition_ratio=0.5, conditional=True, controlled_tol=False, conv=True, data='cifar10', dims='64,64,64', divergence_fn='approximate', dl2int=None, dropout_rate=0.0, imagesize=None, l1int=None, l2int=None, layer_type='concat', log_freq=10, lr=0.0001, max_grad_norm=10000000000.0, multiscale=True, nonlinearity='softplus', num_blocks=2, num_epochs=1000, parallel=False, rademacher=True, residual=False, resume='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run1/epoch_250_checkpt.pth', rtol=1e-05, save='../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run1', seed=1, solver='dopri5', spectral_norm=False, spectral_norm_niter=10, step_size=None, strides='1,1,1,1', test_atol=None, test_batch_size=500, test_rtol=None, test_solver=None, time_length=1.0, time_penalty=0, train_T=True, train_mode='semisup', val_freq=1, warmup_iters=1000, weight_decay=0.0, weight_y=0.5)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ODENVP(\n",
      "  (transforms): ModuleList(\n",
      "    (0): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): LogitTransform()\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(4, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (3): SqueezeLayer()\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (5): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(7, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(13, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (2): SqueezeLayer()\n",
      "        (3): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (4): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(49, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): StackedCNFLayers(\n",
      "      (chain): ModuleList(\n",
      "        (0): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): CNF(\n",
      "          (odefunc): RegularizedODEfunc(\n",
      "            (odefunc): ODEfunc(\n",
      "              (diffeq): ODEnet(\n",
      "                (layers): ModuleList(\n",
      "                  (0): ConcatConv2d(\n",
      "                    (_layer): Conv2d(25, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (1): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (2): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                  (3): ConcatConv2d(\n",
      "                    (_layer): Conv2d(65, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "                  )\n",
      "                )\n",
      "                (activation_fns): ModuleList(\n",
      "                  (0): Softplus(beta=1, threshold=20)\n",
      "                  (1): Softplus(beta=1, threshold=20)\n",
      "                  (2): Softplus(beta=1, threshold=20)\n",
      "                )\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (project_ycond): LinearZeros(in_features=10, out_features=3072, bias=True)\n",
      "  (project_class): LinearZeros(in_features=1536, out_features=10, bias=True)\n",
      ")\n",
      "Number of trainable parameters: 1414198\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "/tancode/repos/tan-ffjord/lib/layers/odefunc.py:286: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  t = torch.tensor(t).type_as(y)\n",
      "Iter 13760 | Time 25.0518(26.1580) | Bit/dim 3.5241(3.5459) | Xent 0.0273(0.0607) | Loss 3.5377(3.5762) | Error 0.0056(0.0213) Steps 1036(1041.69) | Grad Norm 1.2153(2.4071) | Total Time 14.00(14.00)\n",
      "Iter 13770 | Time 25.2280(25.9509) | Bit/dim 3.5717(3.5435) | Xent 0.0247(0.0536) | Loss 3.5840(3.5703) | Error 0.0067(0.0187) Steps 1054(1042.57) | Grad Norm 0.9708(2.1071) | Total Time 14.00(14.00)\n",
      "Iter 13780 | Time 25.2203(25.9599) | Bit/dim 3.5172(3.5378) | Xent 0.0289(0.0469) | Loss 3.5317(3.5612) | Error 0.0067(0.0159) Steps 1048(1042.50) | Grad Norm 0.9470(1.7957) | Total Time 14.00(14.00)\n",
      "Iter 13790 | Time 26.2025(25.9556) | Bit/dim 3.5384(3.5364) | Xent 0.0144(0.0415) | Loss 3.5456(3.5571) | Error 0.0000(0.0135) Steps 1060(1044.03) | Grad Norm 0.4810(1.5388) | Total Time 14.00(14.00)\n",
      "Iter 13800 | Time 25.9338(25.9052) | Bit/dim 3.5211(3.5334) | Xent 0.0252(0.0378) | Loss 3.5337(3.5523) | Error 0.0044(0.0119) Steps 1048(1043.83) | Grad Norm 0.6648(1.3460) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0251 | Time 129.5316, Epoch Time 1590.7856(1478.3838), Bit/dim 3.5295(best: inf), Xent 1.8707, Loss 4.4648, Error 0.2789(best: inf)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13810 | Time 25.9457(25.9024) | Bit/dim 3.5404(3.5307) | Xent 0.0251(0.0340) | Loss 3.5529(3.5477) | Error 0.0067(0.0106) Steps 1030(1041.53) | Grad Norm 0.8056(1.1934) | Total Time 14.00(14.00)\n",
      "Iter 13820 | Time 26.0681(25.9074) | Bit/dim 3.5182(3.5276) | Xent 0.0265(0.0314) | Loss 3.5315(3.5433) | Error 0.0078(0.0094) Steps 1054(1042.36) | Grad Norm 0.8286(1.0797) | Total Time 14.00(14.00)\n",
      "Iter 13830 | Time 26.0437(25.8874) | Bit/dim 3.5480(3.5266) | Xent 0.0294(0.0295) | Loss 3.5627(3.5414) | Error 0.0078(0.0086) Steps 1042(1043.49) | Grad Norm 0.8572(1.0104) | Total Time 14.00(14.00)\n",
      "Iter 13840 | Time 26.1460(25.8688) | Bit/dim 3.4816(3.5247) | Xent 0.0293(0.0281) | Loss 3.4963(3.5387) | Error 0.0078(0.0079) Steps 1048(1043.63) | Grad Norm 1.0629(0.9469) | Total Time 14.00(14.00)\n",
      "Iter 13850 | Time 26.2024(25.9005) | Bit/dim 3.5239(3.5222) | Xent 0.0292(0.0267) | Loss 3.5385(3.5356) | Error 0.0089(0.0074) Steps 1048(1044.91) | Grad Norm 0.8162(0.9098) | Total Time 14.00(14.00)\n",
      "Iter 13860 | Time 26.5848(25.9099) | Bit/dim 3.5312(3.5244) | Xent 0.0172(0.0258) | Loss 3.5398(3.5373) | Error 0.0044(0.0070) Steps 1072(1045.25) | Grad Norm 0.6114(0.8804) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0252 | Time 124.4457, Epoch Time 1567.1661(1481.0472), Bit/dim 3.5264(best: 3.5295), Xent 1.8794, Loss 4.4661, Error 0.2787(best: 0.2789)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13870 | Time 26.2576(25.9107) | Bit/dim 3.5114(3.5233) | Xent 0.0250(0.0246) | Loss 3.5239(3.5356) | Error 0.0089(0.0066) Steps 1024(1045.83) | Grad Norm 0.8720(0.8551) | Total Time 14.00(14.00)\n",
      "Iter 13880 | Time 25.3458(25.9418) | Bit/dim 3.4994(3.5201) | Xent 0.0294(0.0245) | Loss 3.5141(3.5323) | Error 0.0089(0.0063) Steps 1048(1046.29) | Grad Norm 0.7553(0.8339) | Total Time 14.00(14.00)\n",
      "Iter 13890 | Time 25.4243(25.9016) | Bit/dim 3.5298(3.5201) | Xent 0.0308(0.0242) | Loss 3.5452(3.5322) | Error 0.0089(0.0062) Steps 1042(1047.00) | Grad Norm 0.9008(0.8103) | Total Time 14.00(14.00)\n",
      "Iter 13900 | Time 26.0293(25.8880) | Bit/dim 3.5181(3.5224) | Xent 0.0156(0.0239) | Loss 3.5259(3.5344) | Error 0.0033(0.0061) Steps 1048(1046.12) | Grad Norm 0.5730(0.8069) | Total Time 14.00(14.00)\n",
      "Iter 13910 | Time 26.0379(25.8742) | Bit/dim 3.5448(3.5234) | Xent 0.0222(0.0231) | Loss 3.5559(3.5350) | Error 0.0044(0.0057) Steps 1036(1045.37) | Grad Norm 0.6582(0.7697) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0253 | Time 123.9969, Epoch Time 1564.9769(1483.5651), Bit/dim 3.5248(best: 3.5264), Xent 1.9032, Loss 4.4765, Error 0.2821(best: 0.2787)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13920 | Time 25.6438(25.8211) | Bit/dim 3.5259(3.5234) | Xent 0.0271(0.0227) | Loss 3.5394(3.5347) | Error 0.0044(0.0055) Steps 1054(1044.24) | Grad Norm 0.8006(0.7571) | Total Time 14.00(14.00)\n",
      "Iter 13930 | Time 25.3957(25.8157) | Bit/dim 3.5034(3.5231) | Xent 0.0233(0.0227) | Loss 3.5151(3.5344) | Error 0.0033(0.0055) Steps 1048(1046.18) | Grad Norm 0.8440(0.7517) | Total Time 14.00(14.00)\n",
      "Iter 13940 | Time 25.9035(25.8302) | Bit/dim 3.5341(3.5218) | Xent 0.0249(0.0232) | Loss 3.5466(3.5334) | Error 0.0067(0.0058) Steps 1054(1046.03) | Grad Norm 0.8480(0.7699) | Total Time 14.00(14.00)\n",
      "Iter 13950 | Time 26.2377(25.8496) | Bit/dim 3.4929(3.5209) | Xent 0.0340(0.0231) | Loss 3.5099(3.5324) | Error 0.0078(0.0056) Steps 1060(1046.93) | Grad Norm 1.1827(0.7805) | Total Time 14.00(14.00)\n",
      "Iter 13960 | Time 26.0122(25.8918) | Bit/dim 3.4971(3.5200) | Xent 0.0226(0.0231) | Loss 3.5084(3.5316) | Error 0.0044(0.0056) Steps 1036(1046.93) | Grad Norm 0.7247(0.7845) | Total Time 14.00(14.00)\n",
      "Iter 13970 | Time 25.6315(25.8896) | Bit/dim 3.5261(3.5184) | Xent 0.0157(0.0219) | Loss 3.5340(3.5294) | Error 0.0000(0.0052) Steps 1042(1046.30) | Grad Norm 0.5976(0.7543) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0254 | Time 124.1314, Epoch Time 1564.8970(1486.0051), Bit/dim 3.5235(best: 3.5248), Xent 1.9061, Loss 4.4765, Error 0.2833(best: 0.2787)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 13980 | Time 25.7775(25.8896) | Bit/dim 3.5540(3.5192) | Xent 0.0183(0.0217) | Loss 3.5631(3.5300) | Error 0.0067(0.0056) Steps 1048(1046.94) | Grad Norm 0.6517(0.7583) | Total Time 14.00(14.00)\n",
      "Iter 13990 | Time 26.5609(25.9295) | Bit/dim 3.5217(3.5193) | Xent 0.0237(0.0215) | Loss 3.5336(3.5301) | Error 0.0089(0.0054) Steps 1054(1046.64) | Grad Norm 0.9945(0.7585) | Total Time 14.00(14.00)\n",
      "Iter 14000 | Time 25.7869(25.9021) | Bit/dim 3.5438(3.5194) | Xent 0.0238(0.0210) | Loss 3.5557(3.5299) | Error 0.0067(0.0053) Steps 1042(1045.94) | Grad Norm 0.6305(0.7352) | Total Time 14.00(14.00)\n",
      "Iter 14010 | Time 25.6923(25.9507) | Bit/dim 3.5117(3.5174) | Xent 0.0193(0.0211) | Loss 3.5213(3.5280) | Error 0.0044(0.0053) Steps 1036(1047.02) | Grad Norm 0.7102(0.7375) | Total Time 14.00(14.00)\n",
      "Iter 14020 | Time 26.7105(25.9339) | Bit/dim 3.5476(3.5195) | Xent 0.0262(0.0213) | Loss 3.5607(3.5302) | Error 0.0078(0.0055) Steps 1048(1047.53) | Grad Norm 0.9770(0.7463) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0255 | Time 124.2202, Epoch Time 1568.8652(1488.4909), Bit/dim 3.5232(best: 3.5235), Xent 1.9030, Loss 4.4747, Error 0.2760(best: 0.2787)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14030 | Time 26.0310(25.9036) | Bit/dim 3.5344(3.5178) | Xent 0.0129(0.0208) | Loss 3.5408(3.5282) | Error 0.0022(0.0050) Steps 1036(1047.94) | Grad Norm 0.4309(0.7392) | Total Time 14.00(14.00)\n",
      "Iter 14040 | Time 26.5639(25.9099) | Bit/dim 3.5421(3.5175) | Xent 0.0157(0.0203) | Loss 3.5500(3.5276) | Error 0.0011(0.0047) Steps 1036(1048.01) | Grad Norm 0.5723(0.7282) | Total Time 14.00(14.00)\n",
      "Iter 14050 | Time 26.1638(25.8836) | Bit/dim 3.5329(3.5171) | Xent 0.0219(0.0200) | Loss 3.5439(3.5271) | Error 0.0044(0.0045) Steps 1048(1047.31) | Grad Norm 0.6561(0.7217) | Total Time 14.00(14.00)\n",
      "Iter 14060 | Time 26.3716(25.8402) | Bit/dim 3.5018(3.5152) | Xent 0.0213(0.0203) | Loss 3.5124(3.5254) | Error 0.0044(0.0047) Steps 1054(1046.56) | Grad Norm 0.6815(0.7037) | Total Time 14.00(14.00)\n",
      "Iter 14070 | Time 27.0179(25.9720) | Bit/dim 3.4812(3.5155) | Xent 0.0255(0.0204) | Loss 3.4939(3.5257) | Error 0.0100(0.0047) Steps 1036(1046.07) | Grad Norm 0.8253(0.7083) | Total Time 14.00(14.00)\n",
      "Iter 14080 | Time 26.1398(25.9564) | Bit/dim 3.5288(3.5183) | Xent 0.0265(0.0209) | Loss 3.5421(3.5288) | Error 0.0089(0.0052) Steps 1048(1047.26) | Grad Norm 0.8607(0.7413) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0256 | Time 124.7489, Epoch Time 1567.8124(1490.8705), Bit/dim 3.5218(best: 3.5232), Xent 1.9096, Loss 4.4766, Error 0.2799(best: 0.2760)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14090 | Time 26.1847(26.0184) | Bit/dim 3.5282(3.5175) | Xent 0.0212(0.0209) | Loss 3.5388(3.5279) | Error 0.0067(0.0053) Steps 1036(1047.84) | Grad Norm 0.8161(0.7303) | Total Time 14.00(14.00)\n",
      "Iter 14100 | Time 25.9649(26.0121) | Bit/dim 3.5030(3.5177) | Xent 0.0186(0.0208) | Loss 3.5123(3.5281) | Error 0.0033(0.0050) Steps 1066(1049.43) | Grad Norm 0.6340(0.7229) | Total Time 14.00(14.00)\n",
      "Iter 14110 | Time 26.1038(25.9434) | Bit/dim 3.5118(3.5175) | Xent 0.0185(0.0206) | Loss 3.5211(3.5278) | Error 0.0044(0.0049) Steps 1060(1049.58) | Grad Norm 0.7225(0.7263) | Total Time 14.00(14.00)\n",
      "Iter 14120 | Time 26.3445(25.9298) | Bit/dim 3.5172(3.5181) | Xent 0.0229(0.0206) | Loss 3.5286(3.5284) | Error 0.0056(0.0049) Steps 1054(1049.39) | Grad Norm 0.8365(0.7275) | Total Time 14.00(14.00)\n",
      "Iter 14130 | Time 26.5136(25.8967) | Bit/dim 3.5130(3.5163) | Xent 0.0233(0.0203) | Loss 3.5247(3.5265) | Error 0.0067(0.0050) Steps 1054(1049.85) | Grad Norm 0.8737(0.7214) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0257 | Time 124.6642, Epoch Time 1569.3476(1493.2249), Bit/dim 3.5219(best: 3.5218), Xent 1.9202, Loss 4.4820, Error 0.2777(best: 0.2760)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14140 | Time 26.1022(25.9367) | Bit/dim 3.4922(3.5192) | Xent 0.0178(0.0205) | Loss 3.5012(3.5294) | Error 0.0022(0.0051) Steps 1036(1050.13) | Grad Norm 0.5378(0.7488) | Total Time 14.00(14.00)\n",
      "Iter 14150 | Time 25.6703(25.9104) | Bit/dim 3.5197(3.5190) | Xent 0.0176(0.0203) | Loss 3.5285(3.5291) | Error 0.0078(0.0052) Steps 1048(1048.83) | Grad Norm 0.7598(0.7515) | Total Time 14.00(14.00)\n",
      "Iter 14160 | Time 25.4473(25.9167) | Bit/dim 3.5253(3.5211) | Xent 0.0133(0.0200) | Loss 3.5319(3.5311) | Error 0.0000(0.0051) Steps 1060(1049.83) | Grad Norm 0.4501(0.7407) | Total Time 14.00(14.00)\n",
      "Iter 14170 | Time 26.9560(25.9538) | Bit/dim 3.4835(3.5166) | Xent 0.0199(0.0204) | Loss 3.4934(3.5268) | Error 0.0067(0.0051) Steps 1078(1050.49) | Grad Norm 0.6918(0.7468) | Total Time 14.00(14.00)\n",
      "Iter 14180 | Time 25.7080(25.9171) | Bit/dim 3.5170(3.5137) | Xent 0.0265(0.0209) | Loss 3.5302(3.5242) | Error 0.0100(0.0054) Steps 1036(1051.00) | Grad Norm 0.7086(0.7759) | Total Time 14.00(14.00)\n",
      "Iter 14190 | Time 25.5234(25.8391) | Bit/dim 3.5166(3.5156) | Xent 0.0173(0.0205) | Loss 3.5252(3.5258) | Error 0.0044(0.0052) Steps 1030(1049.50) | Grad Norm 0.5996(0.7531) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0258 | Time 124.4672, Epoch Time 1565.0359(1495.3792), Bit/dim 3.5210(best: 3.5218), Xent 1.9235, Loss 4.4828, Error 0.2781(best: 0.2760)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14200 | Time 26.3360(25.8987) | Bit/dim 3.5403(3.5202) | Xent 0.0285(0.0199) | Loss 3.5545(3.5301) | Error 0.0056(0.0048) Steps 1054(1049.78) | Grad Norm 0.6826(0.7229) | Total Time 14.00(14.00)\n",
      "Iter 14210 | Time 25.7497(25.8660) | Bit/dim 3.5184(3.5179) | Xent 0.0204(0.0193) | Loss 3.5286(3.5276) | Error 0.0044(0.0045) Steps 1042(1048.32) | Grad Norm 0.6284(0.6961) | Total Time 14.00(14.00)\n",
      "Iter 14220 | Time 25.8931(25.8371) | Bit/dim 3.5040(3.5174) | Xent 0.0244(0.0197) | Loss 3.5163(3.5272) | Error 0.0089(0.0046) Steps 1048(1046.88) | Grad Norm 0.6985(0.6925) | Total Time 14.00(14.00)\n",
      "Iter 14230 | Time 25.5298(25.8232) | Bit/dim 3.5003(3.5153) | Xent 0.0260(0.0207) | Loss 3.5133(3.5256) | Error 0.0044(0.0049) Steps 1054(1048.11) | Grad Norm 0.7606(0.7214) | Total Time 14.00(14.00)\n",
      "Iter 14240 | Time 26.1588(25.8343) | Bit/dim 3.5316(3.5130) | Xent 0.0195(0.0204) | Loss 3.5413(3.5232) | Error 0.0033(0.0046) Steps 1060(1048.88) | Grad Norm 0.6527(0.7070) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0259 | Time 124.2060, Epoch Time 1564.4656(1497.4518), Bit/dim 3.5211(best: 3.5210), Xent 1.8982, Loss 4.4702, Error 0.2736(best: 0.2760)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14250 | Time 25.9685(25.8681) | Bit/dim 3.5145(3.5151) | Xent 0.0147(0.0197) | Loss 3.5219(3.5249) | Error 0.0033(0.0043) Steps 1048(1050.04) | Grad Norm 0.7040(0.6899) | Total Time 14.00(14.00)\n",
      "Iter 14260 | Time 26.2210(25.8904) | Bit/dim 3.5393(3.5182) | Xent 0.0137(0.0197) | Loss 3.5462(3.5281) | Error 0.0011(0.0046) Steps 1042(1048.89) | Grad Norm 0.5905(0.6944) | Total Time 14.00(14.00)\n",
      "Iter 14270 | Time 26.0447(25.9452) | Bit/dim 3.4917(3.5144) | Xent 0.0232(0.0202) | Loss 3.5033(3.5245) | Error 0.0067(0.0048) Steps 1054(1048.51) | Grad Norm 0.8898(0.7005) | Total Time 14.00(14.00)\n",
      "Iter 14280 | Time 24.8328(25.9078) | Bit/dim 3.5192(3.5130) | Xent 0.0155(0.0203) | Loss 3.5269(3.5232) | Error 0.0022(0.0049) Steps 1054(1048.63) | Grad Norm 0.6519(0.7107) | Total Time 14.00(14.00)\n",
      "Iter 14290 | Time 26.2537(25.9516) | Bit/dim 3.5303(3.5141) | Xent 0.0212(0.0202) | Loss 3.5409(3.5242) | Error 0.0044(0.0049) Steps 1060(1048.28) | Grad Norm 0.7740(0.6983) | Total Time 14.00(14.00)\n",
      "Iter 14300 | Time 26.1250(25.9256) | Bit/dim 3.5412(3.5158) | Xent 0.0178(0.0203) | Loss 3.5502(3.5260) | Error 0.0044(0.0051) Steps 1060(1049.01) | Grad Norm 0.8012(0.7198) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0260 | Time 125.1202, Epoch Time 1571.0812(1499.6607), Bit/dim 3.5209(best: 3.5210), Xent 1.9113, Loss 4.4765, Error 0.2785(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14310 | Time 26.4109(25.9664) | Bit/dim 3.5469(3.5160) | Xent 0.0175(0.0204) | Loss 3.5556(3.5262) | Error 0.0033(0.0051) Steps 1054(1048.96) | Grad Norm 0.7304(0.7258) | Total Time 14.00(14.00)\n",
      "Iter 14320 | Time 26.1113(26.0045) | Bit/dim 3.5201(3.5170) | Xent 0.0165(0.0200) | Loss 3.5283(3.5270) | Error 0.0022(0.0047) Steps 1036(1048.96) | Grad Norm 0.6157(0.7165) | Total Time 14.00(14.00)\n",
      "Iter 14330 | Time 25.4858(25.9269) | Bit/dim 3.4826(3.5160) | Xent 0.0223(0.0198) | Loss 3.4938(3.5259) | Error 0.0067(0.0046) Steps 1054(1048.27) | Grad Norm 0.8637(0.7136) | Total Time 14.00(14.00)\n",
      "Iter 14340 | Time 25.8531(25.9203) | Bit/dim 3.5102(3.5172) | Xent 0.0189(0.0196) | Loss 3.5196(3.5270) | Error 0.0033(0.0045) Steps 1054(1048.14) | Grad Norm 0.6882(0.7044) | Total Time 14.00(14.00)\n",
      "Iter 14350 | Time 26.2446(25.9668) | Bit/dim 3.5183(3.5137) | Xent 0.0221(0.0196) | Loss 3.5294(3.5235) | Error 0.0033(0.0044) Steps 1066(1047.59) | Grad Norm 0.9081(0.7093) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0261 | Time 124.9922, Epoch Time 1572.2701(1501.8389), Bit/dim 3.5196(best: 3.5209), Xent 1.9524, Loss 4.4958, Error 0.2790(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14360 | Time 26.0010(25.9587) | Bit/dim 3.5158(3.5128) | Xent 0.0152(0.0189) | Loss 3.5234(3.5223) | Error 0.0033(0.0042) Steps 1060(1046.87) | Grad Norm 0.6956(0.6958) | Total Time 14.00(14.00)\n",
      "Iter 14370 | Time 25.8470(25.9876) | Bit/dim 3.4792(3.5131) | Xent 0.0159(0.0191) | Loss 3.4872(3.5227) | Error 0.0033(0.0041) Steps 1036(1046.28) | Grad Norm 0.5201(0.7013) | Total Time 14.00(14.00)\n",
      "Iter 14380 | Time 25.8717(25.9598) | Bit/dim 3.5241(3.5129) | Xent 0.0193(0.0195) | Loss 3.5337(3.5226) | Error 0.0044(0.0043) Steps 1060(1047.70) | Grad Norm 0.6825(0.7060) | Total Time 14.00(14.00)\n",
      "Iter 14390 | Time 26.6430(25.9806) | Bit/dim 3.5127(3.5154) | Xent 0.0174(0.0189) | Loss 3.5214(3.5248) | Error 0.0044(0.0041) Steps 1054(1048.61) | Grad Norm 0.8143(0.7181) | Total Time 14.00(14.00)\n",
      "Iter 14400 | Time 25.6515(25.8809) | Bit/dim 3.5142(3.5159) | Xent 0.0223(0.0190) | Loss 3.5254(3.5254) | Error 0.0056(0.0042) Steps 1036(1046.99) | Grad Norm 0.7263(0.7090) | Total Time 14.00(14.00)\n",
      "Iter 14410 | Time 25.5905(25.8346) | Bit/dim 3.4920(3.5141) | Xent 0.0254(0.0197) | Loss 3.5047(3.5239) | Error 0.0056(0.0042) Steps 1042(1047.51) | Grad Norm 0.7406(0.7132) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0262 | Time 124.7559, Epoch Time 1565.0472(1503.7352), Bit/dim 3.5195(best: 3.5196), Xent 1.9303, Loss 4.4846, Error 0.2805(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14420 | Time 25.6236(25.8421) | Bit/dim 3.5199(3.5148) | Xent 0.0203(0.0195) | Loss 3.5300(3.5245) | Error 0.0056(0.0043) Steps 1060(1047.78) | Grad Norm 0.7649(0.7108) | Total Time 14.00(14.00)\n",
      "Iter 14430 | Time 25.8703(25.8871) | Bit/dim 3.5367(3.5165) | Xent 0.0149(0.0191) | Loss 3.5442(3.5261) | Error 0.0022(0.0042) Steps 1072(1047.49) | Grad Norm 0.6007(0.7016) | Total Time 14.00(14.00)\n",
      "Iter 14440 | Time 25.4059(25.9159) | Bit/dim 3.5083(3.5140) | Xent 0.0224(0.0188) | Loss 3.5195(3.5234) | Error 0.0056(0.0041) Steps 1036(1045.88) | Grad Norm 0.9109(0.7104) | Total Time 14.00(14.00)\n",
      "Iter 14450 | Time 25.3436(25.9111) | Bit/dim 3.4785(3.5144) | Xent 0.0298(0.0197) | Loss 3.4934(3.5243) | Error 0.0078(0.0046) Steps 1054(1045.59) | Grad Norm 0.9231(0.7266) | Total Time 14.00(14.00)\n",
      "Iter 14460 | Time 25.9262(25.8738) | Bit/dim 3.5208(3.5140) | Xent 0.0139(0.0197) | Loss 3.5278(3.5239) | Error 0.0033(0.0046) Steps 1048(1046.38) | Grad Norm 0.6840(0.7459) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0263 | Time 125.0391, Epoch Time 1567.2583(1505.6409), Bit/dim 3.5197(best: 3.5195), Xent 1.9481, Loss 4.4938, Error 0.2816(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14470 | Time 25.7805(25.8420) | Bit/dim 3.4681(3.5149) | Xent 0.0132(0.0191) | Loss 3.4747(3.5244) | Error 0.0011(0.0042) Steps 1048(1047.81) | Grad Norm 0.5525(0.7431) | Total Time 14.00(14.00)\n",
      "Iter 14480 | Time 25.9537(25.8759) | Bit/dim 3.5279(3.5156) | Xent 0.0219(0.0189) | Loss 3.5388(3.5251) | Error 0.0067(0.0041) Steps 1042(1047.36) | Grad Norm 0.9986(0.7387) | Total Time 14.00(14.00)\n",
      "Iter 14490 | Time 25.7585(25.9016) | Bit/dim 3.5118(3.5143) | Xent 0.0173(0.0187) | Loss 3.5205(3.5237) | Error 0.0033(0.0039) Steps 1048(1048.20) | Grad Norm 0.7694(0.7250) | Total Time 14.00(14.00)\n",
      "Iter 14500 | Time 25.9333(25.9299) | Bit/dim 3.5106(3.5146) | Xent 0.0188(0.0191) | Loss 3.5200(3.5241) | Error 0.0022(0.0043) Steps 1048(1048.60) | Grad Norm 0.7652(0.7360) | Total Time 14.00(14.00)\n",
      "Iter 14510 | Time 25.6958(25.9531) | Bit/dim 3.4782(3.5105) | Xent 0.0227(0.0196) | Loss 3.4896(3.5203) | Error 0.0089(0.0048) Steps 1030(1048.03) | Grad Norm 0.7742(0.7648) | Total Time 14.00(14.00)\n",
      "Iter 14520 | Time 25.2789(25.9289) | Bit/dim 3.5234(3.5126) | Xent 0.0137(0.0194) | Loss 3.5302(3.5223) | Error 0.0011(0.0047) Steps 1042(1046.99) | Grad Norm 0.4617(0.7575) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0264 | Time 125.5533, Epoch Time 1571.3575(1507.6124), Bit/dim 3.5188(best: 3.5195), Xent 1.9648, Loss 4.5012, Error 0.2823(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14530 | Time 25.6461(25.8165) | Bit/dim 3.4834(3.5116) | Xent 0.0152(0.0188) | Loss 3.4911(3.5210) | Error 0.0033(0.0045) Steps 1048(1045.63) | Grad Norm 0.7556(0.7461) | Total Time 14.00(14.00)\n",
      "Iter 14540 | Time 26.1154(25.7643) | Bit/dim 3.4998(3.5139) | Xent 0.0235(0.0188) | Loss 3.5116(3.5233) | Error 0.0067(0.0045) Steps 1060(1047.08) | Grad Norm 0.7490(0.7298) | Total Time 14.00(14.00)\n",
      "Iter 14550 | Time 25.9990(25.7595) | Bit/dim 3.5171(3.5138) | Xent 0.0135(0.0184) | Loss 3.5238(3.5229) | Error 0.0022(0.0042) Steps 1048(1047.25) | Grad Norm 0.6034(0.7160) | Total Time 14.00(14.00)\n",
      "Iter 14560 | Time 26.2443(25.8090) | Bit/dim 3.5326(3.5124) | Xent 0.0124(0.0183) | Loss 3.5388(3.5216) | Error 0.0033(0.0043) Steps 1072(1049.01) | Grad Norm 0.6097(0.7230) | Total Time 14.00(14.00)\n",
      "Iter 14570 | Time 26.6750(25.8697) | Bit/dim 3.5075(3.5117) | Xent 0.0153(0.0180) | Loss 3.5151(3.5207) | Error 0.0022(0.0040) Steps 1054(1049.46) | Grad Norm 0.5929(0.7098) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0265 | Time 126.3544, Epoch Time 1562.6989(1509.2650), Bit/dim 3.5169(best: 3.5188), Xent 1.9635, Loss 4.4987, Error 0.2845(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14580 | Time 25.6714(25.8565) | Bit/dim 3.5177(3.5125) | Xent 0.0137(0.0185) | Loss 3.5246(3.5218) | Error 0.0022(0.0041) Steps 1048(1049.89) | Grad Norm 0.7036(0.7269) | Total Time 14.00(14.00)\n",
      "Iter 14590 | Time 25.6663(25.8363) | Bit/dim 3.4690(3.5084) | Xent 0.0217(0.0187) | Loss 3.4799(3.5178) | Error 0.0067(0.0042) Steps 1042(1049.22) | Grad Norm 0.9083(0.7279) | Total Time 14.00(14.00)\n",
      "Iter 14600 | Time 25.8500(25.8148) | Bit/dim 3.5027(3.5107) | Xent 0.0178(0.0185) | Loss 3.5116(3.5200) | Error 0.0044(0.0041) Steps 1048(1049.41) | Grad Norm 0.9757(0.7375) | Total Time 14.00(14.00)\n",
      "Iter 14610 | Time 26.5556(25.8425) | Bit/dim 3.5115(3.5093) | Xent 0.0233(0.0183) | Loss 3.5231(3.5185) | Error 0.0056(0.0040) Steps 1042(1049.29) | Grad Norm 0.7828(0.7387) | Total Time 14.00(14.00)\n",
      "Iter 14620 | Time 25.7843(25.8972) | Bit/dim 3.5399(3.5120) | Xent 0.0186(0.0181) | Loss 3.5492(3.5210) | Error 0.0011(0.0037) Steps 1054(1049.90) | Grad Norm 0.6761(0.7234) | Total Time 14.00(14.00)\n",
      "Iter 14630 | Time 25.9544(25.8805) | Bit/dim 3.5420(3.5149) | Xent 0.0156(0.0181) | Loss 3.5498(3.5240) | Error 0.0044(0.0038) Steps 1042(1050.53) | Grad Norm 0.6969(0.7276) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0266 | Time 125.6043, Epoch Time 1566.0659(1510.9690), Bit/dim 3.5181(best: 3.5169), Xent 1.9719, Loss 4.5041, Error 0.2795(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14640 | Time 26.2081(25.9265) | Bit/dim 3.4879(3.5121) | Xent 0.0214(0.0186) | Loss 3.4986(3.5214) | Error 0.0044(0.0039) Steps 1060(1050.48) | Grad Norm 0.7977(0.7329) | Total Time 14.00(14.00)\n",
      "Iter 14650 | Time 26.1559(25.8798) | Bit/dim 3.5284(3.5127) | Xent 0.0136(0.0185) | Loss 3.5352(3.5220) | Error 0.0000(0.0040) Steps 1060(1050.39) | Grad Norm 0.4819(0.7216) | Total Time 14.00(14.00)\n",
      "Iter 14660 | Time 26.2500(25.8788) | Bit/dim 3.4734(3.5130) | Xent 0.0201(0.0187) | Loss 3.4835(3.5223) | Error 0.0022(0.0039) Steps 1048(1051.38) | Grad Norm 0.9191(0.7363) | Total Time 14.00(14.00)\n",
      "Iter 14670 | Time 25.8792(25.8570) | Bit/dim 3.5211(3.5143) | Xent 0.0193(0.0188) | Loss 3.5308(3.5237) | Error 0.0044(0.0042) Steps 1042(1049.87) | Grad Norm 0.9989(0.7680) | Total Time 14.00(14.00)\n",
      "Iter 14680 | Time 25.7490(25.8130) | Bit/dim 3.5080(3.5135) | Xent 0.0139(0.0185) | Loss 3.5149(3.5227) | Error 0.0022(0.0042) Steps 1036(1049.06) | Grad Norm 0.6376(0.7645) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0267 | Time 125.2571, Epoch Time 1562.7310(1512.5219), Bit/dim 3.5163(best: 3.5169), Xent 1.9580, Loss 4.4953, Error 0.2797(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14690 | Time 25.3382(25.7528) | Bit/dim 3.4912(3.5126) | Xent 0.0149(0.0185) | Loss 3.4986(3.5218) | Error 0.0033(0.0043) Steps 1042(1049.13) | Grad Norm 0.7006(0.7841) | Total Time 14.00(14.00)\n",
      "Iter 14700 | Time 25.0057(25.7394) | Bit/dim 3.4930(3.5120) | Xent 0.0181(0.0182) | Loss 3.5021(3.5211) | Error 0.0011(0.0039) Steps 1048(1048.69) | Grad Norm 0.8445(0.7833) | Total Time 14.00(14.00)\n",
      "Iter 14710 | Time 25.8130(25.7464) | Bit/dim 3.5034(3.5116) | Xent 0.0145(0.0178) | Loss 3.5107(3.5205) | Error 0.0044(0.0037) Steps 1054(1048.80) | Grad Norm 0.8605(0.7773) | Total Time 14.00(14.00)\n",
      "Iter 14720 | Time 26.3229(25.8317) | Bit/dim 3.5028(3.5091) | Xent 0.0173(0.0177) | Loss 3.5114(3.5179) | Error 0.0022(0.0036) Steps 1060(1049.38) | Grad Norm 0.7147(0.7645) | Total Time 14.00(14.00)\n",
      "Iter 14730 | Time 25.6375(25.9020) | Bit/dim 3.5295(3.5091) | Xent 0.0120(0.0173) | Loss 3.5355(3.5178) | Error 0.0000(0.0034) Steps 1054(1049.90) | Grad Norm 0.4619(0.7394) | Total Time 14.00(14.00)\n",
      "Iter 14740 | Time 26.2032(25.8793) | Bit/dim 3.5379(3.5132) | Xent 0.0128(0.0175) | Loss 3.5443(3.5219) | Error 0.0011(0.0035) Steps 1066(1050.55) | Grad Norm 0.4664(0.7403) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0268 | Time 125.6701, Epoch Time 1566.1546(1514.1308), Bit/dim 3.5174(best: 3.5163), Xent 1.9747, Loss 4.5048, Error 0.2784(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14750 | Time 26.7112(25.9805) | Bit/dim 3.5185(3.5131) | Xent 0.0191(0.0177) | Loss 3.5281(3.5220) | Error 0.0056(0.0037) Steps 1042(1051.07) | Grad Norm 0.6848(0.7463) | Total Time 14.00(14.00)\n",
      "Iter 14760 | Time 26.2630(26.1453) | Bit/dim 3.5047(3.5142) | Xent 0.0260(0.0175) | Loss 3.5177(3.5230) | Error 0.0044(0.0036) Steps 1042(1049.32) | Grad Norm 0.8407(0.7468) | Total Time 14.00(14.00)\n",
      "Iter 14770 | Time 25.3785(26.1289) | Bit/dim 3.5051(3.5133) | Xent 0.0198(0.0176) | Loss 3.5150(3.5221) | Error 0.0067(0.0038) Steps 1054(1050.54) | Grad Norm 0.7985(0.7402) | Total Time 14.00(14.00)\n",
      "Iter 14780 | Time 25.8257(26.0342) | Bit/dim 3.4884(3.5122) | Xent 0.0247(0.0176) | Loss 3.5008(3.5210) | Error 0.0044(0.0038) Steps 1036(1048.98) | Grad Norm 0.8276(0.7278) | Total Time 14.00(14.00)\n",
      "Iter 14790 | Time 25.7703(25.9988) | Bit/dim 3.4845(3.5091) | Xent 0.0176(0.0177) | Loss 3.4933(3.5180) | Error 0.0044(0.0037) Steps 1030(1049.26) | Grad Norm 0.6552(0.7262) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0269 | Time 124.3093, Epoch Time 1578.6192(1516.0655), Bit/dim 3.5156(best: 3.5163), Xent 1.9710, Loss 4.5011, Error 0.2807(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14800 | Time 26.3276(26.0040) | Bit/dim 3.4769(3.5094) | Xent 0.0150(0.0178) | Loss 3.4844(3.5183) | Error 0.0033(0.0038) Steps 1066(1049.42) | Grad Norm 1.0053(0.7442) | Total Time 14.00(14.00)\n",
      "Iter 14810 | Time 25.9322(26.0392) | Bit/dim 3.5509(3.5086) | Xent 0.0198(0.0176) | Loss 3.5608(3.5174) | Error 0.0044(0.0038) Steps 1048(1049.60) | Grad Norm 0.7626(0.7393) | Total Time 14.00(14.00)\n",
      "Iter 14820 | Time 25.8259(25.9116) | Bit/dim 3.5423(3.5118) | Xent 0.0198(0.0183) | Loss 3.5522(3.5209) | Error 0.0033(0.0041) Steps 1054(1050.39) | Grad Norm 0.8358(0.7578) | Total Time 14.00(14.00)\n",
      "Iter 14830 | Time 25.5887(25.9618) | Bit/dim 3.5421(3.5117) | Xent 0.0160(0.0178) | Loss 3.5501(3.5206) | Error 0.0011(0.0038) Steps 1060(1050.68) | Grad Norm 0.6372(0.7375) | Total Time 14.00(14.00)\n",
      "Iter 14840 | Time 25.9800(25.9282) | Bit/dim 3.4879(3.5115) | Xent 0.0123(0.0175) | Loss 3.4941(3.5202) | Error 0.0011(0.0038) Steps 1060(1050.45) | Grad Norm 0.6383(0.7329) | Total Time 14.00(14.00)\n",
      "Iter 14850 | Time 25.9385(25.9116) | Bit/dim 3.5118(3.5104) | Xent 0.0169(0.0170) | Loss 3.5202(3.5189) | Error 0.0033(0.0033) Steps 1042(1049.57) | Grad Norm 0.6207(0.7214) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0270 | Time 124.6787, Epoch Time 1568.1057(1517.6267), Bit/dim 3.5148(best: 3.5156), Xent 1.9713, Loss 4.5005, Error 0.2838(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14860 | Time 25.3757(25.8941) | Bit/dim 3.5191(3.5108) | Xent 0.0154(0.0174) | Loss 3.5268(3.5195) | Error 0.0044(0.0035) Steps 1036(1048.88) | Grad Norm 0.6640(0.7253) | Total Time 14.00(14.00)\n",
      "Iter 14870 | Time 25.0085(25.8781) | Bit/dim 3.5247(3.5112) | Xent 0.0177(0.0172) | Loss 3.5335(3.5198) | Error 0.0056(0.0034) Steps 1054(1049.64) | Grad Norm 0.9569(0.7266) | Total Time 14.00(14.00)\n",
      "Iter 14880 | Time 25.2968(25.8576) | Bit/dim 3.5324(3.5105) | Xent 0.0171(0.0174) | Loss 3.5409(3.5192) | Error 0.0000(0.0034) Steps 1054(1050.78) | Grad Norm 0.7815(0.7402) | Total Time 14.00(14.00)\n",
      "Iter 14890 | Time 25.8182(25.8874) | Bit/dim 3.5027(3.5099) | Xent 0.0188(0.0178) | Loss 3.5121(3.5188) | Error 0.0033(0.0037) Steps 1042(1051.22) | Grad Norm 0.6914(0.7398) | Total Time 14.00(14.00)\n",
      "Iter 14900 | Time 25.3469(25.9129) | Bit/dim 3.5304(3.5098) | Xent 0.0132(0.0176) | Loss 3.5370(3.5186) | Error 0.0011(0.0037) Steps 1036(1050.17) | Grad Norm 0.4957(0.7260) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0271 | Time 125.8275, Epoch Time 1570.1251(1519.2017), Bit/dim 3.5155(best: 3.5148), Xent 1.9603, Loss 4.4956, Error 0.2773(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14910 | Time 25.6417(25.9503) | Bit/dim 3.5182(3.5109) | Xent 0.0176(0.0175) | Loss 3.5271(3.5196) | Error 0.0033(0.0037) Steps 1072(1052.38) | Grad Norm 0.8885(0.7509) | Total Time 14.00(14.00)\n",
      "Iter 14920 | Time 25.4970(25.8872) | Bit/dim 3.4778(3.5101) | Xent 0.0186(0.0171) | Loss 3.4871(3.5187) | Error 0.0022(0.0035) Steps 1060(1051.58) | Grad Norm 1.1041(0.7460) | Total Time 14.00(14.00)\n",
      "Iter 14930 | Time 26.1314(25.9089) | Bit/dim 3.5078(3.5100) | Xent 0.0163(0.0170) | Loss 3.5159(3.5185) | Error 0.0011(0.0036) Steps 1036(1052.03) | Grad Norm 0.7641(0.7385) | Total Time 14.00(14.00)\n",
      "Iter 14940 | Time 26.5664(25.9238) | Bit/dim 3.4892(3.5089) | Xent 0.0125(0.0176) | Loss 3.4954(3.5177) | Error 0.0011(0.0037) Steps 1042(1050.71) | Grad Norm 0.6490(0.7583) | Total Time 14.00(14.00)\n",
      "Iter 14950 | Time 25.2062(25.8850) | Bit/dim 3.5058(3.5113) | Xent 0.0214(0.0180) | Loss 3.5165(3.5202) | Error 0.0033(0.0038) Steps 1048(1050.43) | Grad Norm 1.0152(0.7696) | Total Time 14.00(14.00)\n",
      "Iter 14960 | Time 25.9818(25.9895) | Bit/dim 3.4750(3.5096) | Xent 0.0142(0.0181) | Loss 3.4821(3.5186) | Error 0.0011(0.0038) Steps 1030(1050.21) | Grad Norm 0.6693(0.7721) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0272 | Time 126.3708, Epoch Time 1570.8025(1520.7497), Bit/dim 3.5149(best: 3.5148), Xent 1.9517, Loss 4.4907, Error 0.2827(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 14970 | Time 25.8229(25.9738) | Bit/dim 3.4886(3.5086) | Xent 0.0115(0.0177) | Loss 3.4943(3.5175) | Error 0.0011(0.0037) Steps 1048(1051.01) | Grad Norm 0.5947(0.7505) | Total Time 14.00(14.00)\n",
      "Iter 14980 | Time 25.9790(25.9808) | Bit/dim 3.4834(3.5083) | Xent 0.0118(0.0172) | Loss 3.4893(3.5169) | Error 0.0011(0.0034) Steps 1060(1051.22) | Grad Norm 0.5616(0.7281) | Total Time 14.00(14.00)\n",
      "Iter 14990 | Time 25.5945(25.9234) | Bit/dim 3.5292(3.5097) | Xent 0.0155(0.0167) | Loss 3.5369(3.5181) | Error 0.0022(0.0033) Steps 1054(1050.10) | Grad Norm 0.7647(0.7093) | Total Time 14.00(14.00)\n",
      "Iter 15000 | Time 25.9557(25.9102) | Bit/dim 3.5357(3.5103) | Xent 0.0189(0.0170) | Loss 3.5451(3.5187) | Error 0.0033(0.0034) Steps 1042(1050.54) | Grad Norm 0.7145(0.7284) | Total Time 14.00(14.00)\n",
      "Iter 15010 | Time 26.2215(25.9424) | Bit/dim 3.5207(3.5100) | Xent 0.0165(0.0169) | Loss 3.5290(3.5185) | Error 0.0044(0.0034) Steps 1042(1050.16) | Grad Norm 0.6537(0.7234) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0273 | Time 126.0580, Epoch Time 1570.4042(1522.2393), Bit/dim 3.5145(best: 3.5148), Xent 1.9880, Loss 4.5085, Error 0.2771(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15020 | Time 26.0057(25.9325) | Bit/dim 3.5028(3.5108) | Xent 0.0092(0.0167) | Loss 3.5074(3.5192) | Error 0.0000(0.0032) Steps 1036(1050.44) | Grad Norm 0.4103(0.7265) | Total Time 14.00(14.00)\n",
      "Iter 15030 | Time 25.3240(25.8385) | Bit/dim 3.4760(3.5078) | Xent 0.0137(0.0170) | Loss 3.4829(3.5163) | Error 0.0022(0.0033) Steps 1060(1050.18) | Grad Norm 0.5514(0.7280) | Total Time 14.00(14.00)\n",
      "Iter 15040 | Time 25.8701(25.8879) | Bit/dim 3.4987(3.5092) | Xent 0.0123(0.0172) | Loss 3.5048(3.5179) | Error 0.0011(0.0035) Steps 1036(1048.49) | Grad Norm 0.5683(0.7534) | Total Time 14.00(14.00)\n",
      "Iter 15050 | Time 25.8005(25.9083) | Bit/dim 3.5371(3.5072) | Xent 0.0248(0.0173) | Loss 3.5495(3.5159) | Error 0.0056(0.0036) Steps 1042(1047.87) | Grad Norm 0.9972(0.7795) | Total Time 14.00(14.00)\n",
      "Iter 15060 | Time 26.3888(25.8885) | Bit/dim 3.5328(3.5093) | Xent 0.0154(0.0175) | Loss 3.5405(3.5180) | Error 0.0033(0.0037) Steps 1072(1049.56) | Grad Norm 0.6786(0.7896) | Total Time 14.00(14.00)\n",
      "Iter 15070 | Time 25.3342(25.7812) | Bit/dim 3.5126(3.5091) | Xent 0.0172(0.0176) | Loss 3.5213(3.5178) | Error 0.0033(0.0039) Steps 1066(1047.76) | Grad Norm 0.6593(0.7686) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0274 | Time 125.4521, Epoch Time 1561.8722(1523.4283), Bit/dim 3.5151(best: 3.5145), Xent 1.9805, Loss 4.5053, Error 0.2799(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n",
      "Iter 15080 | Time 25.4149(25.8058) | Bit/dim 3.5031(3.5060) | Xent 0.0192(0.0174) | Loss 3.5127(3.5147) | Error 0.0033(0.0037) Steps 1042(1048.04) | Grad Norm 0.7159(0.7613) | Total Time 14.00(14.00)\n",
      "Iter 15090 | Time 26.1541(25.8052) | Bit/dim 3.5337(3.5085) | Xent 0.0143(0.0170) | Loss 3.5408(3.5170) | Error 0.0033(0.0035) Steps 1048(1048.72) | Grad Norm 0.8279(0.7403) | Total Time 14.00(14.00)\n",
      "Iter 15100 | Time 25.6157(25.7741) | Bit/dim 3.5344(3.5098) | Xent 0.0192(0.0171) | Loss 3.5440(3.5184) | Error 0.0067(0.0036) Steps 1066(1049.56) | Grad Norm 0.6704(0.7447) | Total Time 14.00(14.00)\n",
      "Iter 15110 | Time 25.0816(25.8290) | Bit/dim 3.5313(3.5092) | Xent 0.0172(0.0177) | Loss 3.5399(3.5180) | Error 0.0011(0.0037) Steps 1042(1051.00) | Grad Norm 0.7780(0.7575) | Total Time 14.00(14.00)\n",
      "Iter 15120 | Time 26.1070(25.8776) | Bit/dim 3.4961(3.5102) | Xent 0.0178(0.0181) | Loss 3.5050(3.5193) | Error 0.0044(0.0038) Steps 1066(1052.41) | Grad Norm 0.7965(0.7701) | Total Time 14.00(14.00)\n",
      "validating...\n",
      "Epoch 0275 | Time 124.9002, Epoch Time 1566.3278(1524.7153), Bit/dim 3.5155(best: 3.5145), Xent 2.0157, Loss 4.5233, Error 0.2807(best: 0.2736)\n",
      "===> Using batch size 900. Total 55 iterations/epoch.\n"
     ]
    }
   ],
   "source": [
    "%run -p ../train_cnf_disentangle_cifar.py --data cifar10 --dims 64,64,64 --strides 1,1,1,1 --num_blocks 2 --layer_type concat --multiscale True --rademacher True --batch_size 900 --test_batch_size 500 --save ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run1 --resume ../experiments_published/cnf_conditional_disentangle_cifar10_bs900_sratio_0_5_run1/epoch_250_checkpt.pth --seed 1 --lr 0.0001 --conditional True --controlled_tol False --train_mode semisup --log_freq 10 --weight_y 0.5 --condition_ratio 0.5\n",
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
